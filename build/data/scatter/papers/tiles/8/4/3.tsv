id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
fbb645d4c69a1c0d485b97a1c1bb8cdd7b238c0b	note on the behaviour of an improvement heuristic on permutation and blocking flow-shop scheduling	blocking flow shops;permutation flow shops;flow shop scheduling;heuristic algorithms;article	This work deals with the permutation flow-shop scheduling problem with and without storage space between stages, where the performance criterion is the makespan. Many proposed procedures to solve these problems have an improvement phase based on the search in the pair-wise interchange neighbourhood. The authors have observed large plateaus in the solutions domain of these problems defined for this type of neighbourhood that make it difficult for the heuristics to search for a road to the optimum. An improvement heuristic is proposed, which uses two tools in order to evade these difficulties: a stochastic exploration of the neighbourhood (revolver) and a special consideration of ties. The improvement heuristic is applied, in conjunction with three adapted well-known heuristics in the literature, to the direct and inverse instances. The performance of the procedures was evaluated on nine generated sets of a thousand instances and on 90 instances from Taillard (1993). The obtained results recommend applying always the constructive heuristic procedures on the direct and inverse instance. The computational experience proves the effectiveness of the two tools implemented in the improvement phase.	blocking (computing);computation;constructive heuristic;dinic's algorithm;flow shop scheduling;heuristic (computer science);makespan;randomness;scheduling (computing)	Ramón Companys;Imma Ribas;Manuel Mateo	2010	IJMTM	10.1504/IJMTM.2010.032905	mathematical optimization;flow shop scheduling;computer science;operations management;mathematics;algorithm	AI	20.264388606453426	2.7161323748718575	13090
42866cf5ddc108136d5286a065b2c21fd8872e75	profit forecasting using support vector regression for consulting engineering firms	kernel;consulting engineering firms;support vector machines;training;construction industry;support vector regression;contracts;companies;support system;accuracy;bidding support vector machines pattern classification decision support systems consulting;decision support system;forecasting theory;decision support systems;consulting engineering companies profit forecasting support vector regression consulting engineering firms decision support systems;profit forecasting;pattern classification;bidding;support vector machines construction industry consultancies forecasting theory profitability regression analysis;consulting;regression analysis;consultancies;profitability;support vector machine;consulting engineering companies;contracts macroeconomics economic forecasting design engineering civil engineering support vector machines decision support systems costs pricing artificial neural networks	This paper introduces Support Vector Machines (SVM) in the particular field of decision support systems for consulting engineering companies and studies the differences and particularities of the corresponding solutions. A detailed analysis has been performed in order to assess the suitability and adaptability of these methods for the particular task taking into account the risk/benefit tradeoff.	decision support system;support vector machine	Victor Yepes;Eugenio Pellicer;Francesc J. Ferri	2009	2009 Ninth International Conference on Intelligent Systems Design and Applications	10.1109/ISDA.2009.43	support vector machine;decision support system;computer science;machine learning;data mining	Robotics	3.721505433589927	-9.578633311441944	13092
dc202cc98da6612c7d37bc4aa76e174690604260	a unified solution in fuzzy capital budgeting		Since the mid-1980s, both academics and practitioners have proposed and discussed various solutions in fuzzy capital budgeting. Based primarily on traditional capital budgeting methods, these solutions present the same problems as their respective deterministic methods: the implicit assumptions of the reinvestment rates; the possibility of multiple rates of return; and the possibility of anomalous behavior of the net present value method. This paper presents a unified solution in fuzzy capital budgeting based on modified deterministic methods proposed in the financial literature. This unified approach eliminates those problems and has the property of matching decisions on acceptance or rejection of investment projects with same life horizons and same scales and therefore maximize shareholder wealth. An insight is provided into the advantages of these investment project appraisal methods by comparing and contrasting them with traditional fuzzy methods. A comprehensive case study, based on an investment project on exploration of an oil field under both deterministic and fuzzy conditions, is included to illustrate the use of these methods. Due to the complexity of the calculations involved, new MS-Excel financial functions are developed, by using Visual Basic for Applications. The main contribution of this paper is the development of a unifying approach to capital budgeting under uncertainty that emphasizes the strengths of the modified methods, while bypassing the individual conflicts and drawbacks of the conventional capital budgeting methods. Results confirm that the proposed solution has many advantages over other capital budgeting methods. © 2018 Elsevier Ltd. All rights reserved.	deterministic algorithm;fuzzy concept;rejection sampling;visual basic for applications	Antonio Carlos Sampaio Filho;Marley M. B. R. Vellasco;Ricardo Tanscheit	2018	Expert Syst. Appl.	10.1016/j.eswa.2018.01.002	fuzzy logic;data mining;management science;present value;computer science;project appraisal;shareholder;rate of return;capital budgeting	AI	-4.2418901716378485	-16.266132858008135	13112
998473fdfba21d458aa8fc52563fa67f978517f3	associativity in combination of belief functions; a derivation of minc combination	belief function;dempster shafer theory	The nature of a contradiction (conflict) between two belief functions is investigated. Alternative ways of distributing the contradiction among nonempty subsets of frame of discernment are studied. The paper employes a new approach to understanding contradictions and introduces an original notion of potential contradiction. A method of an associative combination of generalized belief functions – minC combination and its derivation – is presented as part of the new approach. A proportionalization of generalized results is suggested as well.	minc	Milan Daniel	2003	Soft Comput.	10.1007/s00500-002-0215-5	dempster–shafer theory;computer science;artificial intelligence;mathematics;algorithm	NLP	-2.726980610326753	-23.41416606565139	13134
b8e838f2abde048103cfbe09ef057c8500e69a81	using a simulation software to perform energy and exergy analyses of the sulfur-iodine thermochemical process	exergy analysis;sulfur iodine;thermochemical cycle;flowsheet;aspen plus	The objective of this work is to demonstrate the utilization of the power of simulation tools to perform an exergy analysis of a process. Exergy analysis, being a new and useful thermodynamics tool, will be applied to one of the newest research fields in hydrogen production. One of the many advantages of computer simulation is elimination of the need to construct a pilot plant. Presently, extensive research is underway to come up with the production and use of clean fuels. The research entails performing pilot studies and proof of concept experiments using validated models. The research is further extended to various analyses such as safety, economic sustainability and energy efficiency of the processes involved. The production of hydrogen through thermochemical water splitting processes is one of the newest technologies and is expected to compete with the existing technologies. Among a wide range of thermochemical cycles, the sulfur-iodine (SI) thermochemical cycle process has been proposed as a promising technology for the production of hydrogen. In this research, we demonstrate how a commercial simulator can be used to perform an energy and exergy analysis of the SI water splitting process. Using a commercial simulator, a process flowsheet is developed based on research findings presented by other authors and an energy-exergy analysis is carried out on the process. The method of energy–exergy analysis used in this presentation indicates that an energy and exergy efficiency of 17% and 24% can be attained, respectively, in the conceptual design of the SI cycle.	simulation software	Bothwell Nyoni;Bongibethu Msekeli Hlabano-Moyo;Clive Chimwe	2017	IJMSSC	10.1142/S1793962317500027	engineering;thermochemical cycle;thermodynamics;exergy;physics;mechanical engineering	SE	6.104842924644997	2.227224722861465	13156
d704d6b9796451b1e3db55c0737e343114844a55	relationship between generalization and diversity in coevolutionary learning	vertical science platform;empirical study;iterated prisoner s dilemma ipd coevolutionary learning diversity maintenance evolutionary computation ec generalization performance;generalization performance;evolutionary computation;generalization performance issue;game theory;neural networks;iterated prisoner dilemma game;computer science learning systems testing evolutionary computation stochastic processes computational intelligence artificial intelligence processor scheduling computer networks neural networks;processor scheduling;computational intelligence;testing;coevolution;computer networks;learning systems;strategic interaction;learning system;diversity issue;iterated prisoner s dilemma ipd;research paper;iterated prisoner s dilemma;evolutionary computation ec;stochastic processes;coevolutionary learning systems;patents;iterated prisoner dilemma game coevolutionary learning systems generalization performance issue diversity issue diversity maintenance approach;research platform;artificial intelligence;journals;computer science;learning artificial intelligence;learning artificial intelligence evolutionary computation game theory;diversity maintenance;diversity maintenance approach;coevolutionary learning;is strategy;game playing;use case;researchers network;evolutionary computing	Games have long played an important role in the development and understanding of coevolutionary learning systems. In particular, the search process in coevolutionary learning is guided by strategic interactions between solutions in the population, which can be naturally framed as game playing. We study two important issues in coevolutionary learning - generalization performance and diversity - using games. The first one is concerned with the coevolutionary learning of strategies with high generalization performance, that is, strategies that can outperform against a large number of test strategies (opponents) that may not have been seen during coevolution. The second one is concerned with diversity levels in the population that may lead to the search of strategies with poor generalization performance. It is not known if there is a relationship between generalization and diversity in coevolutionary learning. This paper investigates whether there is such a relationship in coevolutionary learning through a detailed empirical study. We systematically investigate the impact of various diversity maintenance approaches on the generalization performance of coevolutionary learning quantitatively using case studies. The problem of the iterated prisoner's dilemma (IPD) game is considered. Unlike past studies, we can measure both the generalization performance and the diversity level of the population of evolved strategies. Results from our case studies show that the introduction and maintenance of diversity do not necessarily lead to the coevolutionary learning of strategies with high generalization performance. However, if individual strategies can be combined (e.g., using a gating mechanism), there is the potential of exploiting diversity in coevolutionary learning to improve generalization performance. Specifically, when the introduction and maintenance of diversity lead to a speciated population during coevolution, where each specialist strategy is capable of outperforming different opponents, the population as a whole can have a significantly higher generalization performance compared to individual strategies.	authorization;experiment;fingerprint;ieee xplore;interaction;interpupillary distance;iteration;population;prisoner's dilemma;problem solving;quantum nonlocality;search algorithm;software bug;test strategy	Siang Yew Chong;Peter Tiño;Xin Yao	2009	IEEE Transactions on Computational Intelligence and AI in Games	10.1109/TCIAIG.2009.2034269	use case;simulation;coevolution;computer science;artificial intelligence;machine learning;software testing;empirical research;evolutionary computation	AI	21.37709962538015	-9.115724519687525	13186
c033fc10051322fce4988e385d8f92a088710d5b	an optimization methodology for machine learning strategies and regression problems in ballistic impact scenarios	neural networks;machine learning;genetic algorithm;optimization;methodology;ballistic impacts	In domains with limited data, such as ballistic impact, prior researches have proven that the optimization of artificial neural models is an efficient tool for improving the performance of a classifier based on MultiLayer Perceptron. In addition, this research aims to explore, in the ballistic domain, the optimization of other machine learning strategies and their application in regression problems. Therefore, this paper presents an optimization methodology to use with several approaches of machine learning in regression problems, maximizing the limited dataset and locating the best network topology and input vector of each network model. This methodology is tested in real regression scenarios of ballistic impact with different artificial neural models, obtaining substantial improvement in all the experiments. Furthermore, the quality stage, based on criteria of information theory, enables the determination of when the complexity of the network design does not penalize the fit over the data and thereby the selection of the best neural network model from a series of candidates. Finally, the results obtained show the relevance of this methodology and its application improves the performance and efficiency of multiple machine learning strategies in regression scenarios.	artificial neural network;experiment;information theory;machine learning;mathematical optimization;multilayer perceptron;network model;network planning and design;network topology;relevance	Israel González-Carrasco;Ángel García-Crespo;Belén Ruíz-Mezcua;José Luis López Cuadrado	2010	Applied Intelligence	10.1007/s10489-010-0269-5	simulation;genetic algorithm;computer science;artificial intelligence;online machine learning;machine learning;methodology;artificial neural network	AI	14.448202333892853	-23.93215970714716	13220
eeb0877f898e7fb15b61316339529d0471f25fad	controlling dominance area of solutions and its impact on the performance of moeas	search space;combinatorial optimization problem;multi objective optimization;knapsack problem	This work proposes a method to control the dominance area of solutions in order to induce appropriate ranking of solutions for the problem at hand, enhance selection, and improve the performance of MOEAs on combinatorial optimization problems. The proposed method can control the degree of expansion or contraction of the dominance area of solutions using a user-defined parameter S. Modifying the dominance area of solutions changes their dominance relation inducing a ranking of solutions that is different to conventional dominance. In this work we use 0/1 multiobjective knapsack problems to analyze the effects on solutions ranking caused by contracting and expanding the dominance area of solutions and its impact on the search performance of a multi-objective optimizer when the number of objectives, the size of the search space, and the complexity of the problems vary. We show that either convergence or diversity can be emphasized by contracting or expanding the dominance area. Also, we show that the optimal value of the area of dominance depends strongly on all factors analyzed here: number of objectives, size of the search space, and complexity of the problems.		Hiroyuki Sato;Hernán E. Aguirre;Kiyoshi Tanaka	2006		10.1007/978-3-540-70928-2_5	mathematical optimization;combinatorics;mathematics;mathematical economics	EDA	21.600930285807657	-4.167820497564596	13257
dfaa1adde337f77c2fc16e8e0e2647b731612fb1	0-1 fuzzy goal programming approach to multiobjective waste paper collection method selection?			goal programming	H. Ziya Ulukan;Yesim Kop	2009	Multiple-Valued Logic and Soft Computing		mathematical optimization;goal programming;management science	NLP	18.100287560679163	-2.7018891638612863	13290
791644c169fb4ca65d09f016f9de2232c1a80555	on exact and approximate stochastic dominance strategies for portfolio selection	stochastic dominance;index tracking;portfolio optimization;expected shortfall;applied probability	One recent and promising strategy for Enhanced Indexation is the selection of portfolios that stochastically dominate the benchmark. We propose here a new type of approximate stochastic dominance rule which implies other existing approximate stochastic dominance rules. We then use it to find the portfolio that approximately stochastically dominates a given benchmark with the best possible approximation. Our model is initially formulated as a Linear Program with exponentially many constraints, and then reformulated in a more compact manner so that it can be very efficiently solved in practice. This reformulation also reveals an interesting financial interpretation. We compare our approach with several exact and approximate stochastic dominance models for portfolio selection. An extensive empirical analysis on real and publicly available datasets shows very good out-of-sample performances of our model. © 2016 Elsevier B.V. All rights reserved.	approximation algorithm;benchmark (computing);linear programming;performance	Renato Bruni;Francesco Cesarone;Andrea Scozzari;Fabio Tardella	2017	European Journal of Operational Research	10.1016/j.ejor.2016.10.006	financial economics;mathematical optimization;economics;expected shortfall;stochastic dominance;portfolio optimization;mathematics;applied probability;welfare economics;statistics	AI	5.083110643126678	-4.863608290494022	13291
47535f44bb56f759748d233c20b05aeddd138f60	optimal control of information epidemics	continuous time;vehicular ad hoc networks continuous time systems optimal control social networking online telecommunication control telecommunication security;vehicular network;telecommunication control;optimal control;continuous time systems;disease epidemics information epidemics information propagation population social network wireless vehicular network vehicular network security mean field route continuous time deterministic optimal control problem;social network;mean field;vehicular ad hoc networks;social networking online;telecommunication security;optimal control vehicles social network services recruitment context wireless communication roads;optimal control problem	In this work we address two problems concerning information propagation in a population: a) how to maximize the spread of a given message in the population within the stipulated time and b) how to create a given level of buzz- measured by the fraction of the population engaged in conversation on a topic of interest- at a specified time horizon. Arising in the context of two rather disparate networks- social and wireless vehicular networks, their importance in campaigning on social networks and security in vehicular networks can only be understated. Taking a mean-field route, we pose the two problems as continuous-time deterministic optimal control problems. We characterize optimal controls, present some numerical results and provide practical insights. Interestingly, the problems we address are antithesis to those in disease epidemics, which need to be contained rather than actively spread.	emoticon;numerical analysis;optimal control;regular expression;social network;software propagation	Aditya Karnik;Pankaj Dayama	2012	2012 Fourth International Conference on Communication Systems and Networks (COMSNETS 2012)	10.1109/COMSNETS.2012.6151316	vehicular ad hoc network;optimal control;computer science;mean field theory;distributed computing;computer security;computer network;social network	ML	-3.789357743222709	3.8219542268692606	13293
8796fb2761fdb807840996659a250b3eba63d6a0	evolving reactive agents with signalgp			reactive planning	Alexander Lalejini;Charles Ofria	2018	PeerJ PrePrints	10.7287/peerj.preprints.26921v1	evolutionary computation;machine learning;genetic programming;event-driven programming;artificial intelligence;computer science	Logic	22.900945999714754	-10.187300626697118	13294
ab376c1279c1ae70959e3bd7ca49adec79cd8603	mathematical model for deadlock resolution in multiple agv scheduling and routing network: a case study	dynamic program;automated guided vehicles agv;autonomous robots;deadlock resolution;mathematical programming;conflict free	Purpose – This paper aims to propose and formulate a complicated routing/scheduling problem for multiple automated guided vehicles (AGVs) in a manufacturing system. Design/methodology/approach – Considering the due date of AGVs requiring for material handling among shops in a jobshop layout, their earliness and tardiness are significant in satisfying the expected cycle time and from an economic view point. Therefore, the authors propose a mathematical program to minimize the penalized earliness and tardiness for a conflict-free and just-in-time production. Findings – The model considers a new concept of turning point for deadlock resolution. As the mathematical program is difficult to solve with a conventional method, an optimization method in two stages, namely, searching the solution space and finding optimal solutions are proposed. The performance of the proposed mathematical model is tested in a numerical example. Practical implications – A case study in real industrial environment is conducted. The f...	deadlock;mathematical model;routing;scheduling (computing)	Hamed Fazlollahtabar;Mohammad Saidi-Mehrabad;Ellips Masehian	2015	Industrial Robot	10.1108/IR-12-2014-0437	real-time computing;simulation;computer science	Robotics	13.162861060785978	3.2965562294365007	13300
d3c0d229671207361a7bf8df498c6cce300b6d76	public-private partnership: a design issue		This paper presents a bi-level stochastic model to optimize availability payment design. We propose a discrete time-varying linear dynamical system with an affine controller to solve the corresponding NP-hard problem. We use an infrastructure public-private partnership project to validate the effectiveness and efficiency of the proposed model.	black and burst;dynamical system;genetic algorithm;heuristic;np-hardness;numerical analysis;point-to-point protocol;privilege escalation;software release life cycle	Qingbin Cui;Xinyuan Zhu;Alex D'Alessio	2017	2017 IEEE Symposium Series on Computational Intelligence (SSCI)	10.1109/SSCI.2017.8285351	control theory;stochastic modelling;stochastic programming;mathematical optimization;linear dynamical system;general partnership;public–private partnership;affine transformation;payment;computer science	Embedded	5.337950481868566	1.2205607182833769	13359
4eaee59110cad630b8fb02b0c6a8efa5efd6d274	stochastic models for bidding strategies on oligopoly electricity market	bidding strategy;power market;bidding strategies;cournot model;supply function equilibrium model;electricity market;forward futures contract;profitability;stochastic model;supply function equilibrium;asian type call option	In this paper we consider the forward/futures contracts and Asian-type call options for power delivery as important components of the bidding strategies of the players’ profits on the electricity market. We show how these derivatives can affect their profit. We use linear asymmetric supply function equilibrium (SFE) and Cournot models to develop firms’ optimal bidding strategies by including forward/futures contracts and Asian-type options. We extend the methodology proposed by Niu et al. (IEEE Trans Power Syst 20(4):1859–1867, 2005), where only forward contracts for power delivery were considered in the SFE model.	futures and promises;secure two-party computation	Magdalena Borgosz-Koczwara;Aleksander Weron;Agnieszka Wylomanska	2009	Math. Meth. of OR	10.1007/s00186-008-0252-7	cournot competition;electricity market;forward market;stochastic modelling;profitability index	AI	-1.391384994566155	-4.71704315945335	13366
11aa3dda84d82810821760af738d9ec2809b3045	the value of reputation in an online freelance marketplace	dynamic programming;reputation;dynamic selection;auction;marketing;online reputation systems;economia y empresa;freelance marketplaces;grupo a;mixture models;empirical io methods;dynamic structural models;structural models;auctions	1 Details of Second-Step Estimation All the estimates from state transitions (θp, θg, θt), ηis, and non-parametric joint distributions of bid attributes are assumed to be known at this point, i.e., consistent estimates of these parameters and distributions from Step 1 are used. Throughout we estimate CCPs using weighted nested logits that are flexible functions of {xit, η̂i, si}. When doing so, we treat ηis as if they were observed by substituting η̂i in their lieu. However, because si is unobserved, at the each step of the EM algorithm, we weigh the nested logits with current estimates of ρs. We use flexible logits to estimate CCPs because commonly used non-parametric estimators are infeasible in our context because we have many continuous state variables and an extremely large state space. With a sufficiently large sample and an exhaustive set of covariates, CCP estimates from weighted flexible logits are similar to those obtained using non-parametric estimators (Arcidiacono & Miller 2011, Bajari et. al., 2010). In our flexible logit, we incorporate all the state variables, their higher order terms, and interactions. Step 1: Let the number of unobserved types be 2, and the initial guess of population probabilities be π1 = {π1 1, π1 2} = {0.5, 0.5}. Let {α1, β1, δ1, σ1} be the first guess of the structural parameters. We start with estimates from the model without persistent unobserved heterogeneity and a random αb11. Step 2: Update the ρs as follows:	expectation–maximization algorithm;interaction;miller–rabin primality test;perturbation theory;state space	Hema Yoganarasimhan	2013	Marketing Science	10.1287/mksc.2013.0809	economics;reputation;marketing;dynamic programming;mixture model;microeconomics;commerce	ML	1.2021043882637237	-10.281105756018679	13372
cc5a03a26f0eda886869725c83369e755a16b18b	fused gromov-wasserstein distance for structured objects: theoretical foundations and mathematical properties		Optimal transport theory has recently found many applications in machine learning thanks to its capacity for comparing various machine learning objects considered as distributions. The Kantorovitch formulation, leading to the Wasserstein distance, focuses on the features of the elements of the objects but treat them independently, whereas the Gromov-Wasserstein distance focuses only on the relations between the elements, depicting the structure of the object, yet discarding its features. In this paper we propose to extend these distances in order to encode simultaneously both the feature and structure informations, resulting in the Fused Gromov-Wasserstein distance. We develop the mathematical framework for this novel distance, prove its metric and interpolation properties and provide a concentration result for the convergence of finite samples. We also illustrate and interpret its use in various contexts where structured objects are involved.	encode;fused filament fabrication;interpolation;light transport theory;machine learning;transportation theory (mathematics)	Titouan Vayer;Laetitia Chapel;Rémi Flamary;Romain Tavenard;Nicolas Courty	2018	CoRR			ML	2.8226466329113364	-21.03552414233066	13389
5a0f9e395046c01c4c5334a77fdad655149bf4d2	optimal long-run imperfect maintenance with asymptotic virtual age	preventive maintenance mathematical model joints analytical models cost function;analytical models;preventive maintenance;cost function;renewal type cycle optimal long run imperfect maintenance asymptotic virtual age imperfect preventive maintenance periodic preventive maintenance increasing failure rates minimal repairs kijima ii virtual age model age based preventive maintenance;virtual age age based preventive maintenance imperfect repair minimal repair periodic maintenance;joints;reliability theory ageing failure analysis optimisation preventive maintenance;mathematical model	We consider imperfect, periodic preventive maintenance of items with increasing failure rates, and minimal repairs in-between. The imperfect maintenance is performed in accordance with a Kijima II virtual age model. The obtained deterministic asymptotic value of virtual age defines the starting age of an item at the beginning of each cycle in the formulated and analyzed long-run optimization problem. A similar approach is used for the corresponding age-based preventive maintenance, where the starting age of the renewal-type cycle is already a random variable. Some meaningful examples are considered.	failure cause;kyoichi kijima;mathematical optimization;optimization problem	Ji Hwan Cha;Maxim Finkelstein	2016	IEEE Transactions on Reliability	10.1109/TR.2015.2451612	reliability engineering;preventive maintenance;engineering;mathematical model;mathematics;forensic engineering;statistics	DB	6.793519030215903	-0.7580934347726114	13418
fed1deae7c9e241189120d8f1a79b237f11a113b	an excel-based decision support system for supply chain design and management of biofuels	ethanol;biofuel;supply chain design;biomass;supply chain management;visual basic for applications;decision support systems dss	This article presents a Decision Support System (DSS) to aid managers with supply chain (SC) design and logistics management of biomass-for-biofuel production. These tools play a very important role in efficiently managing biomass-for-biofuel SCs and have the potential to reduce the cost of biofuels. The proposed model coordinates the long-term decisions of designing a SC with the medium term decisions of logistics management. This system has the ability to (a) identify locations and capacities for biorefineries, given the availability of biomass and costs; (b) estimate the minimum cost of delivering biofuels, which include transportation, investment, and processing costs; and (c) perform sensitivity analyses with respect to a number of parameters. Visual Basic for Applications (VBA) is used to create the interface of the DSS, and Excel’s CPLEX Add-In is used to solve the mathematical models. An Excel-Based Decision Support System for Supply Chain Design and Management of Biofuels	cplex;decision support system;emoticon;logistics;mathematical model;visual basic for applications	Ambarish M. Acharya;Daniela S. Gonzales;Sandra Duni Eksioglu;Sumesh Arora	2014	IJORIS	10.4018/ijoris.2014100102	supply chain management;visual basic for applications;biomass;systems engineering;engineering;operations management;ethanol fuel;transport engineering;biofuel	HCI	9.789314537922372	-4.164483340251346	13436
03b106975a84a17c5b052ee122c26a18018c013c	a novel hybrid approach for wind speed prediction	seasonal adjustment;hybrid approach;wind speed prediction;exponential smoothing;neural network	It is important to improve the accuracy of wind speed predictions for wind park management and for conversion of wind power to electricity. However, due to the chaotic and intrinsic complexity of weather parameters, the prediction of wind speed data using different patterns is difficult. A hybrid model known as SAM–ESM–RBFN is proposed for capturing these different patterns and obtaining better prediction performance. This model is based on the seasonal adjustment method (SAM), exponential smoothing method (ESM), and radial basis function neural network (RBFN). The mean hourly wind speed data from two meteorological stations in the Hexi Corridor of China were used as examples to evaluate the performance of the proposed approach. To avoid randomness due to the RBFN model or the RBFN component of the hybrid model, all of the simulations were repeated 30 times prior to averaging. The SAM–ESM–RBFN model numerically outperformed the following models: the Holt–Winters model (HWM), the multilayer perceptron neural network (MLP), the ESM, the RBFN, the hybrid SAM and ESM (SAM–ESM), the hybrid SAM and RBFN (SAM–RBFN), and the hybrid ESM and RBFN (ESM–RBFN). Overall, the proposed approach was effective in improving the prediction accuracy. 2014 Elsevier Inc. All rights reserved.	artificial neural network;chaos theory;hull–white model;multilayer perceptron;newton's method;numerical analysis;performance;quad flat no-leads package;radial (radio);radial basis function;randomness;sam coupé;simulation;smoothing;time complexity;time series	Jujie Wang;Wenyu Zhang;Jianzhou Wang;Tingting Han;Lingbin Kong	2014	Inf. Sci.	10.1016/j.ins.2014.02.159	exponential smoothing;seasonal adjustment;simulation;computer science;artificial intelligence;artificial neural network;statistics	AI	9.002522939844134	-19.64303455931141	13444
4a7f9519b155e0ed00cf0de3d89d14ca41ddd386	generalized cell formation: iterative versus simultaneous resolution with grouping genetic algorithm	lean manufacturing;genetic algorithm;flow analysis;cellular	For each industrial, lean manufacturing is “The method” to improve productivity and reduce cost. One of the tools for lean is cellular manufacturing. This technique reduces the factory to several small entities, which are easier to manage. The main difficulty of the cellular manufacturing is the creation of these small entities called cells. When the flexibility is used during the production process, two problems emerge. The first problem consists to solve a Resource Planning Problem in allocating the operations on a specific machine. The second problem concerns the Cell Formation Problem where the machines are grouped into cells. The algorithm proposed in this paper is based on a simultaneous resolution of two interdependent problems. This paper proves the efficiency of the simultaneous resolution comparing to the sequential resolution with iterations. To compare only the resolution way, a unique grouping genetic algorithm is used and adapted for both cases.	cell signaling;density functional theory;embedded system;enterprise resource planning;entity;genetic algorithm;heuristic (computer science);interdependence;iteration;mathematical model;power of two;regular expression;vehicle identification number;xslt/muenchian grouping	Emmanuelle Vin;Alain Delchambre	2014	J. Intelligent Manufacturing	10.1007/s10845-013-0749-7	simulation;genetic algorithm;computer science;engineering;data-flow analysis;engineering drawing;lean manufacturing	Robotics	18.74638094409174	0.5673558703700167	13473
1203fb24f052cf6c61d8972854a912f52cf5d4a9	using cooperative mediation to coordinate traffic lights: a case study	dynamic change;swarm intelligence;game theory;distributed constraint optimization;traffic control;constraint optimization problem;coordination;constraint optimization problems	Several approaches tackle the problem of reducing traffic jams. A class of these approaches deals with coordination of traffic lights in order to allow vehicles traveling in a given direction to pass an arterial without stopping at junctions. In short, classical approaches, which are mostly based on offline and centralized determination of the prioritized direction, are quite inflexible since they cannot cope with dynamic changes in the traffic volume. More flexible approaches have been proposed based on implicit coordination and implicit communication (e.g. derived from game theory and swarm intelligence). These have advantages as well as shortcomings. The present paper presents an approach based on cooperative mediation which is a compromise between totally autonomous coordination with implicit communication and the classical centralized solution. We use a distributed constraint optimization algorithm in a dynamic scenario, showing that the mediation is able to reduce the frequency of miscoordination.	algorithm;autonomous robot;centralized computing;constrained optimization;distributed constraint optimization;game theory;mathematical optimization;online and offline;swarm intelligence	Denise de Oliveira;Ana L. C. Bazzan;Victor R. Lesser	2005		10.1145/1082473.1082544	game theory;mathematical optimization;simulation;swarm intelligence;computer science;artificial intelligence	AI	12.924902868768278	-7.660768879099291	13482
796aefba8536c4460b7ce61b9822ab8222178021	financial analysis for developing cdm project in the coke plant	recycle sustainable development financial analysis cdq technology co2 emission reductions potential calculation;protocols;recycle sustainable development;new technology;gas emissions;investments;environmental legislation;atmospheric measurements;carbon dioxide;human health;financial management;co2 emission reductions;sustainable development air pollution control coke environmental legislation financial management quenching thermal steel industry;coke plant;waste heat;potential calculation;emission reduction;pollution measurement;iron;kyoto protocol coke plant financial analysis wet quenching method waste heat china human health safety air pollution coke dry quenching gas emissions clean development mechanism sustainable development;maintenance engineering;low temperature;air pollution control;coke;estimation;fuels;quenching thermal;air pollution;steel industry;kyoto protocol;co2 emission;financial analysis;clean development mechanism;developing country;electricity;wet quenching method;profitability;carbon emissions;humans;temperature;china;coke dry quenching;atmosphere;waste heat protocols atmosphere atmospheric measurements pollution measurement humans air pollution temperature cooling carbon dioxide;sustainable development;human health safety;cooling;cdq technology	In the coke plant, the traditional coke wet quenching method is to be used to cool down the red hot coke and waste heat is released to the atmosphere in China. There are a lot of smog, particles, cyanide, sulphide and etc. mixtures within the waste heat. It is need to take measure to deal with and to minimum harmful to human health. Coke wet quenching method is not only course heavy air pollution but also energy waste. In this paper, a new advanced technology--coke dry quenching (CDQ) technology is being introduced. CDQ is a process by which the red hot coke is cooled down with low temperature inert gas in a closed cooling unit. Using CDQ new technology, the harmful emissions (including SOx, NOx and floating particles) can be significantly reduced and local environment can be improved. At Feb. 16th 2005, Kyoto protocol had finally become effective on the international focus. In the Kyoto protocol, there is a Clean Development Mechanism (CDM) which helps developing countries to participate the international emission reduction market and gains biggest profit from CDM system. In this paper, the theoretical base has been put into practice that Chinese coke plants can take part in CDM. Using CDM approved methodology, calculation has been done for potential carbon emission reductions (CERs) of CDQ. It can break a route for Chinese enterprises to get foreign support from CERs, and to overcome the big upfront investment barriers of CDQ and improve the local recycle sustainable development.	focal (programming language);trionic		2009		10.1109/IFITA.2009.193	environmental engineering;engineering;forensic engineering;waste management	Logic	7.369747549746736	-7.376394447570427	13560
71f1830004256482bdc08d15fdf4c7cd3aabbc52	building low co2 solutions to the vehicle routing problem with time windows using an evolutionary algorithm	street topology;environmental factors;evolutionary computation;power based instantaneous fuel consumption model;junction layout;routing;vehicle routing problem;road traffic;drive cycle vehicle routing problem time windows evolutionary multiobjective algorithm street topology junction layout junction characteristic road class power based instantaneous fuel consumption model co 2 emission;road class;junctions;data association;fuel consumption;junction characteristic;evolutionary multiobjective algorithm;fuels;roads;co 2 emission;drive cycle;vehicle routing problem with time windows;transportation;transportation environmental factors evolutionary computation road traffic road vehicles;co2 emission;optimization;vehicles;evolutionary algorithm;time windows;road vehicles;vehicles routing fuels roads evolutionary computation junctions optimization;qa76 computer software	An evolutionary Multi-Objective Algorithm (MOA) is used to investigate the trade-off between CO2 savings, distance and number of vehicles used in a typical vehicle routing problem with Time Windows (VRPTW). A problem set is derived containing three problems based on accurate geographical data which encapsulates the topology of streets as well as layouts and characteristics of junctions. This is combined with realistic speed-flow data associated with road-classes and a power-based instantaneous fuel consumption model to calculate CO2 emissions, taking account of drive-cycles. Results obtained using a well-known MOA with twin objectives show that it is possible to save up to 10% CO2, depending on the problem instance and ranking criterion used.	evolutionary algorithm;moa;microsoft windows;vehicle routing problem	Neil Urquhart;Emma Hart;Cathy Scott	2010	IEEE Congress on Evolutionary Computation	10.1109/CEC.2010.5586088	driving cycle;transport;mathematical optimization;routing;simulation;computer science;vehicle routing problem;machine learning;evolutionary algorithm;fuel efficiency;evolutionary computation	Theory	16.86513568532027	0.12631580191025196	13595
70ed221d749099d8dd3b9fc7171f516c9f8559f6	study to short-term flow estimation at intersection base on genetic neural networks	traffic flow;traffic flow estimation;genetics;genetic algorithm;artificial neural network;neural network	The traffic flow data is the foundation of the transportation management and control. Inevitably there is data loss in traffic parameters acquisitions, so it needs traffic flow estimation to complete the traffic flow information when the data loss is serious. Proper estimation of traffic flow is an essential component of advanced management of dynamic traffic networks. The genetic nerve-network is developed, combined the nerve network and the genetic algorithm together, to estimate the short-term traffic volume. According to the experiment result, the method is effective to estimate traffic flow in the short term at intersection.	artificial neural network;genetic algorithm;long short-term memory	Zhina Zhou;Zhongke Shi;Yingfeng Li	2009		10.1145/1543834.1544010	traffic generation model;network traffic control;genetic algorithm;computer science;traffic congestion reconstruction with kerner's three-phase theory;machine learning;traffic flow;artificial neural network;computer network;network traffic simulation	AI	9.27639870504438	-21.917225622703484	13650
1cab623f9afb4522bc900c7c08f6ed1418adfcbe	learning value heuristics for constraint programming		Search heuristics are of paramount importance for finding good solutions to optimization problems quickly. Manually designing problem specific search heuristics is a time consuming process and requires expert knowledge from the user. Thus there is great interest in developing autonomous search heuristics which work well for a wide variety of problems. Various autonomous search heuristics already exist, such as first fail, domwdeg and impact based search. However, such heuristics are often more focused on the variable selection, i.e., picking important variables to branch on to make the search tree smaller, rather than the value selection, i.e., ordering the subtrees so that the good subtrees are explored first. In this paper, we define a framework for learning value heuristics, by combining a scoring function, feature selection, and machine learning algorithm. We demonstrate that we can learn value heuristics that perform better than random value heuristics, and for some problem classes, the learned heuristics are comparable in performance to manually designed value heuristics. We also show that value heuristics using features beyond a simple score can be valuable.	algorithm;autonomous robot;constraint programming;feature selection;heuristic (computer science);machine learning;mathematical optimization;polynomial;scoring functions for docking;search engine optimization;search tree;tree (data structure)	Geoffrey Chu;Peter J. Stuckey	2015		10.1007/978-3-319-18008-3_8	constraint logic programming;concurrent constraint logic programming;mathematical optimization;constraint programming;binary constraint;constraint satisfaction;reactive programming;constraint learning;brute-force search;inductive programming;constraint satisfaction problem;hybrid algorithm	AI	20.71198325312357	-7.544193595899278	13745
03802cfe22bd300e41d69844b6e0d25c260a21fc	swaf: swarm algorithm framework for numerical optimization	benchmark problem;agent based model;performance comparison;numerical optimization;cognitive architecture;fast and frugal heuristic;evolutionary computing	A swarm algorithm framework (SWAF), realized by agent-based modeling, is presented to solve numerical optimization problems. Each agent is a bare bones cognitive architecture, which learns knowledge by appropriately deploying a set of simple rules in fast and frugal heuristics. Two essential categories of rules, the generate-and-test and the problem-formulation rules, are implemented, and both of the macro rules by simple combination and subsymbolic deploying of multiple rules among them are also studied. Experimental results on benchmark problems are presented, and performance comparison between SWAF and other existing algorithms indicates that it is efficiently.	agent-based model;algorithm;artificial intelligence;artificial neural network;benchmark (computing);cognitive architecture;cognitive science;emoticon;experiment;heuristic (computer science);mpeg transport stream;mathematical optimization;norm (social);radio frequency;situated;software release life cycle;swarm intelligence	Xiao-Feng Xie;Wen-Jun Zhang	2004		10.1007/978-3-540-24854-5_21	cognitive architecture;computer science;artificial intelligence;machine learning;algorithm;evolutionary computation	AI	23.020774446845422	-9.94928042947317	13771
55e171737dc2ff082d1bdfd3c66f3cb0fad2765b	use of evolutionary robots as an auxiliary tool for developing behavioral models of rats in an elevated plus-maze	robot sensing systems;manipulators;evolutionary computation;rats;mobile robot;behavior modeling;multilayer perceptrons;robotics;multilayer perceptron;rat behavior evolutionary robot auxiliary tool behavioral model test hypothesis elevated plus maze elman multilayer perceptron neural network weight optimization genetic algorithm;artificial neural networks rats robot sensing systems gallium manipulators trajectory;artificial neural networks;elevated plus maze robotics exploratory behavior genetic algorithms;evolutionary robotics;trajectory;multilayer perceptrons evolutionary computation genetic algorithms;behavior genetics;elevated plus maze;genetic algorithm;genetic algorithms;exploratory behavior;gallium;neural network	This work proposes the use of evolutionary robots as an auxiliary tool for the development of models to describe rats’ behavior. Evolutionary robots can be useful to test hypothesis and models of behavior, what is demonstrated here in experiments where a mobile robot is trained to reproduce the behavior of the rat in an elevated plus-maze (EPM). A recurrent Elman multilayer perceptron is used to control the robot. Due to the impossibility of knowing each desired output of the neural network, the weight optimization is made by a genetic algorithm. In this way, each individual of the genetic algorithm represents a set of weights of the neural network. Experiments where the fitness of each individual was computed based on the difference between the trajectories of the evolutionary robot on a EPM replica and of the rats behavior on a real EPM are presented.	artificial neural network;electronic counter-countermeasure;evolutionary algorithm;evolutionary robotics;experiment;genetic algorithm;mathematical optimization;mobile robot;multilayer perceptron	Helder K. Shimo;Antônio C. Roque-da-Silva;Renato Tinós;Julian Tejada;Silvio Morato	2010	2010 Eleventh Brazilian Symposium on Neural Networks	10.1109/SBRN.2010.45	simulation;genetic algorithm;computer science;artificial intelligence;machine learning;artificial neural network	Robotics	17.688006403925716	-22.872532386469373	13807
92e0acccbfa35465291662dbd85681974f68b0bf	computational intelligence techniques for modelling the critical flashover voltage of insulators: from accuracy to comprehensibility		This paper copes with the problem of flashover voltage on polluted insulators, being one of the most important components of electric power systems. Α number of appropriately selected computational intelligence techniques are developed and applied for the modelling of the problem. Some of the applied techniques work as black-box models, but they are capable of achieving highly accurate results (artificial neural networks and gravitational search algorithms). Other techniques, on the contrary, obtain results somewhat less accurate, but highly comprehensible (genetic programming and inductive decision trees). However, all the applied techniques outperform standard data analysis approaches, such as regression models. The variables used in the analyses are the insulator’s maximum diameter, height, creepage distance, insulator’s manufacturing constant, and also the insulator’s pollution. In this research work the critical flashover voltage on a polluted insulator is expressed as a function of the aforementioned variables. The used database consists of 168 different cases of polluted insulators, created through both actual and simulated values. Results are encouraging, with room for further study, aiming towards the development of models for the proper inspection and maintenance of insulators.	computation;computational intelligence	Evangelos Karampotsis;Konstantinos Boulas;Alexandros Tzanetos;Vasilios P. Androvitsaneas;Ioannis F. Gonos;Georgios Dounias;Ioannis A. Stathopulos	2017		10.1007/978-3-319-60042-0_35	regression analysis;insulator (electricity);machine learning;electric power system;artificial neural network;genetic programming;search algorithm;computational intelligence;decision tree;computer science;artificial intelligence	AI	11.04417820271802	-15.325765982597725	13860
f1d6cdd0f1417b9921bf34c89e70360f8242b7da	sourcing decision with correlated supplier disruption: an mv framework	order quantity sourcing decision correlated supplier disruption mv framework sourcing strategy demand uncertainty supply disruption bankruptcy risky supplier geographic proximity natural disaster supplier disruption correlation markowitz mean variance framework mv formulation;supply chain management bankruptcy disasters order processing;mean variance;correlation supply chains resource management analytical models uncertainty contracts numerical models;sourcing correlation mean variance supply disruption;supply disruption;correlation;sourcing	This paper investigates the sourcing strategy of a buyer who faces demand uncertainty, as well as supply disruption. Under disruption, a bankruptcy at one supplier may cause collapse of other suppliers. Hence, it is important to highlight the modeling of codependence between two or more risky suppliers. If location of the suppliers is in the same geographic proximity, then one natural disaster can affect all the suppliers in that region. Therefore, it is important to consider the impact of supplier disruption correlation for deciding the sourcing strategy. In this context, we use Markowitz's mean-variance (MV) framework to develop an MV formulation. We illustrate the case of perfectly positive and negative correlated suppliers, their order quantity and MV objective through numerical study. The result shows that the buyer should allocate the order to negatively correlated supplier to maximize the MV objective.	denial-of-service attack;numerical analysis	Pritee Ray;Mamata Jenamani	2014	2014 IEEE International Conference on Industrial Engineering and Engineering Management	10.1109/IEEM.2014.7058762	operations management;mathematics;microeconomics;supplier relationship management;correlation;commerce	Robotics	-0.2825231327094583	-5.163189345078193	13867
1988d907f2f045b966c1cb61aab3dbcd7604a5d2	complexity and effective prediction	repeated game;finite state automata;zero sum game	Article history: Received 10 December 2007 Available online xxxx JEL classification: C73 D83 C44 Let G = 〈I, J , g〉 be a two-person zero-sum game. We examine the two-person zero-sum repeated game G(k,m) in which players 1 and 2 place down finite state automata with k,m states respectively and the payoff is the average per-stage payoff when the two automata face off. We are interested in the cases in which player 1 is “smart” in the sense that k is large but player 2 is “much smarter” in the sense that m k. Let S(g) be the value of G where the second player is clairvoyant, i.e., would know player 1’s move in advance. The threshold for clairvoyance is shown to occur for m near min(|I|, | J |)k . For m of roughly that size, in the exponential scale, the value is close to S(g). For m significantly smaller (for some stage payoffs g) the value does not approach S(g). © 2009 Elsevier Inc. All rights reserved.	automata theory;finite-state machine;game theory;time complexity	Abraham Neyman;Joel H. Spencer	2010	Games and Economic Behavior	10.1016/j.geb.2009.05.007	combinatorics;discrete mathematics;economics;repeated game;mathematics;stochastic game;microeconomics;zero-sum game;mathematical economics;finite-state machine	AI	-3.7944910374950176	0.7721850630129897	13875
608702fb6e3ecdc4a544021af2e2a043e0742eb1	a conditional game for comparing approximations		We present a “conditional game” to be played between two approximate inference algorithms. We prove that exact inference is an optimal strategy and demonstrate how the game can be used to estimate the relative accuracy of two different approximations in the absence of exact marginals.	approximation algorithm	Frederik Eaton	2011			mathematical optimization;machine learning;mathematics;statistics	ML	23.83531030784618	-19.57078216685737	13956
9ac279ad1cc9d322f03cf0752a8d907a111dbbc2	the fundamentals of computational intelligence: system approach		This monograph is dedicated to the systematic presentation of main trends, technologies and methods of computational intelligence (CI). The book pays big attention to novel important CI technology- fuzzy logic (FL) systems and fuzzy neural networks (FNN). Different FNN including new class of FNN- cascade neo-fuzzy neural networks are considered and their training algorithms are described and analyzed. The applications of FNN to the forecast in macroeconomics and at stock markets are examined. The book presents the problem of portfolio optimization under uncertainty, the novel theory of fuzzy portfolio optimization free of drawbacks of classical model of Markovitz as well as an application for portfolios optimization at Ukrainian, Russian and American stock exchanges. The book also presents the problem of corporations bankruptcy risk forecasting under incomplete and fuzzy information, as well as new methods based on fuzzy sets theory and fuzzy neural networks and results of their application for bankruptcy risk forecasting are presented and compared with Altman method. This monograph also focuses on an inductive modeling method of self-organization the so-called Group Method of Data Handling (GMDH) which enables to construct the structure of forecasting models almost automatically. The results of experimental investigations of GMDH for forecasting at stock exchanges are presented. The final chapters are devoted to theory and applications of evolutionary modeling (EM) and genetic algorithms. The distinguishing feature of this monograph is a great number of practical examples of CI technologies and methods application for solution of real problems in technology, economy and financial sphere, in particular forecasting, classification, pattern recognition, portfolio optimization, bankruptcy risk prediction under uncertainty which were developed by authors and published in this book for the first time. All CI methods and algorithms are presented from the general system approach and analysis of their properties, advantages and drawbacks that enables practitioners to choose the most adequate method for their own problems solution.	computation;computational intelligence	Mikhail Z. Zgurovsky;Yuriy P. Zaychenko	2017		10.1007/978-3-319-35162-9	computational intelligence	AI	6.575294861320276	-23.830687782643956	13966
d2edb43474153006f992cf42d5a1e98e1881f455	dynamic optimal control policy in advertising price and quality	cost function;life cycle;ncku 成功大學 成大 圖書館 機構典藏;optimal control theory;sensitivity analysis;approximate solution;dissertations and theses journal referred papers conference papers nsc reserach report patent nckur ir ncku institutional repostiory 博碩士論文 期刊論文 國科會研究報告 專利 成大機構典藏;genetic algorithm;profitability;diffusion model;optimality condition;dynamic optimization	This study presents an algorithm for deriving the long-term polices of quality level, price and advertisement for a product. The diffusion models and cost functions are combined to formulate profit functions capable of determining future profit trends. The algorithm first implements the optimal control theory to derive the optimal conditions of the profit function. Then the genetic algorithm is employed to search for the approximate solutions of quality level, price and advertising expenditure at each period on the planning horizon (life cycle). Examples of different scenarios of the model parameters are presented to describe the results obtained herein. Sensitivity analysis for the major parameters is performed to specify their effects on profits. Results in this study allow us, firstly, to obtain explicit solutions simultaneously with respect to quality, price and advertising policies, secondly, to propose an appropriate algorithm for solving the different scenarios of the dynamic profit function, which...	optimal control	Chinho Lin;Shin-Yu Shen;Yih-Jyh Yeh;Jieh-Rern Ding	2001	Int. J. Systems Science	10.1080/00207720118596	biological life cycle;mathematical optimization;genetic algorithm;optimal control;computer science;engineering;machine learning;diffusion;mathematics;mathematical economics;operations research;sensitivity analysis;profitability index	Logic	2.891266183200046	-4.1425784938561545	14017
3ad6bd5c34b0866019b54f5976d644326069cb3d	towards next generation touring: personalized group tours	pattern recognition and data mining	Recommending and planning tour itineraries are challenging and time-consuming for tourists, hence they may seek tour operators for help. Traditionally tour operators have offered standard tour packages of popular locations, but these packages may not cater to tourist’s interests. In addition, tourists may want to travel in a group, e.g., extended family, and want an operator to help them. We introduce the novel problem of group tour recommendation (GROUPTOURREC), which involves many challenges: forming tour groups whose members have similar interests; recommending Pointsof-Interests (POI) that form the tour itinerary and cater for the group’s interests; and assigning guides to lead these tours. For each challenge, we propose solutions involving: clustering for tourist groupings; optimizing a variant of the Orienteering problem for POI recommendations; and integer programming for tour guide assignments. Using a Flickr dataset of seven cities, we compare our proposed approaches against various baselines and observe significant improvements in terms of interest similarity, total/maximum/minimum tour interests and total tour guide expertise.	cluster analysis;flickr;hierarchical clustering;integer programming;jaccard index;k-means clustering;np-hardness;next-generation network	Kwan Hui Lim;Jeffrey Chan;Christopher Leckie;Shanika Karunasekera	2016			simulation;computer science	AI	16.375607844748142	-0.1960042366625836	14050
0070fea90cd6ef3337f640600f2d8a2d37e096f3	size and effort-based computational models for software cost prediction	computer model	Reliable and accurate software cost estimations have always been a challenge especially for people involved in project resource management. The challenge is amplified due to the high level of complexity and uniqueness of the software process. The majority of estimation methods proposed fail to produce successful cost forecasting and neither resolve to explicit, measurable and concise set of factors affecting productivity. Throughout the software cost estimation literature software size is usually proposed as one of the most important attributes affecting effort and is used to build cost models. This paper aspires to provide size and effort-based estimations for the required software effort of new projects based on data obtained from past completed projects. The modelling approach utilises Artificial Neural Networks (ANN) with a random sliding window input and output method using holdout samples and moreover, a Genetic Algorithm (GA) undertakes to evolve the inputs and internal hidden architectures and to reduce the Mean Relative Error (MRE). The obtained optimal ANN topologies and input and output methods for each dataset are presented, discussed and compared with a classic MLR model.	approximation error;artificial neural network;complexity;computation;computational model;genetic algorithm;high-level programming language;input/output;learning to rank;software development effort estimation;software development process;software release life cycle;software sizing	Efi Papatheocharous;Andreas S. Andreou	2008			software sizing;computer science;computational resource;computational model	SE	7.033677480217744	-22.854816190100724	14084
f901fbf0ab538d446d7b0052dfe08dfd8122311c	physician scheduling using goal programming-an application to a large hospital in saudi arabia		This paper proposes a goal programming model that provides Physician schedules with improved efficiency and staff satisfaction at an Emergency Department. Monthly schedules are currently manually produced by one physician using an Excel sheet. He is spending one entire week each month to build a schedule for physicians. The proposed model takes into account numerous constraints and goals. A comparison between the hospital and the proposed schedules shows major outperformance of the proposed one.	goal programming;programming model;schedule (computer science);scheduling (computing)	Anis Gharbi;Mohamed Aly Ould Louly;Mohamed Naceur Azaiez	2017	2017 4th International Conference on Control, Decision and Information Technologies (CoDIT)	10.1109/CoDIT.2017.8102715	emergency department;operations management;scheduling (computing);goal programming;simulation;schedule;computer science	Robotics	12.527780018687494	-1.3644050397480938	14101
1757aa6d104a155505ac549925807a50ac08508d	multiple colony bees algorithm for continuous spaces	functional optimization;colonization;similarity between honey bees and ants;bees algorithm	. Introduction Optimization problems can be modelled by means of a set of ecision variables with their domains and constraints concerning he variable settings. They naturally are divided into three cateories: (i) with exclusively discrete variables, (ii) with exclusively ontinuous variables, and (iii) with discrete as well as continuous ariables. The class (ii) is also called as continuous optimization roblems, and the main goal of this paper is to develop an effiient solution approach based on bees algorithm [1] for tackling he continuous optimization problems. Continuous optimization is a growing research field since umerous real world problems were handled within the scope of his research field, and involves finding the minimum or maxium value of a function of one or many real variables. Continuous ptimization problems consist of one or more continuous deciion variables each one may have an infinite number of values. ithin this context, continuous optimization problems have not nite search spaces as distinct from the combinatorial optimizaion problems. Numerous studies developed a great deal of solution pproaches, including meta-heuristics, in order to tackle continuus optimization problems in such a manner of this study. ∗ Corresponding author. Tel.: +90 232 301 76 32; fax: +90 232 301 76 08. E-mail addresses: sener.akpinar@deu.edu.tr (Ş . Akpinar), dil.baykasoglu@deu.edu.tr (A. Baykasoğlu). ttp://dx.doi.org/10.1016/j.asoc.2014.08.063 568-4946/© 2014 Elsevier B.V. All rights reserved. Nature is the most popular source of inspiration while designing effective meta-heuristic algorithms. The behaviours and communication systems of social insects are more appealing than the other sources of inspiration in nature, since their characteristics are attractive individually and holistically from the optimization point of view. Such that, the meta-heuristic algorithms based on the simulation of the mentioned individual and holistic behaviours of social insects named as swarm-based optimization algorithms. Ant Colony Optimization (ACO) [2], Particle Swarm Optimization (PSO) [3], Bee Colony Optimization (BCO) [4] and Bees Algorithm (BA) [1] are the most familiar approaches belonging to swarm-based group of meta-heuristics. Many recent variants of these algorithms were used to solve continuous optimization problems by researchers [5–15]. In this study, we attempt to solve continuous optimization problems via a multiple colony bees algorithm, each colony inspired by real honey bees and represents a hive, in which different bees live and constitute a population. BA basically depends on a combination of random search and a type of neighbourhood search, whose structural properties may differ according to the characteristics of the problem on hand. Most of the studies belonging to existing literature about the applications of BA tried to simulate the activities of honey bees only in a single colony. To the best of our knowledge, Akbari and Ziarati’s work is the first attempt proposed a multiple colony bees algorithm for functional optimization [16]. An analogy and a computational comparison between their study and the current paper will be given in Section 3 and in Section 4.1 respectively, since we also dealt with	ant colony optimization algorithms;apache hive;bees algorithm;business architecture;computation;continuous optimization;eusociality;fax;heuristic (computer science);holism;mathematical optimization;particle swarm optimization;program optimization;random search;rational synergy;simulation	Sener Akpinar;Adil Baykasoglu	2014	Appl. Soft Comput.	10.1016/j.asoc.2014.08.063	mathematical optimization;computer science;artificial intelligence;machine learning;bees algorithm	AI	22.72151990147725	-4.13684154245532	14160
5f8da82f5d5deefac0660e898b0bac29fc0f8356	sixthsense: fast and reliable recognition of dead ends in mdps	dead ends;machine learning;planning under uncertainty;mdp	The results of the latest International Probabilistic Planning Competition (IPPC-2008) indicate that the presence of dead ends, states with no trajectory to the goal, makes MDPs hard for modern probabilistic planners. Implicit dead ends, states with executable actions but no path to the goal, are particularly challenging; existing MDP solvers spend much time and memory identifying these states. As a first attempt to address this issue, we propose a machine learning algorithm called SIXTHSENSE. SIXTHSENSE helps existing MDP solvers by finding nogoods, conjunctions of literals whose truth in a state implies that the state is a dead end. Importantly, our learned nogoods are sound, and hence the states they identify are true dead ends. SIXTHSENSE is very fast, needs little training data, and takes only a small fraction of total planning time. While IPPC problems may have millions of dead ends, they may typically be represented with only a dozen or two no-goods. Thus, nogood learning efficiently produces a quick and reliable means for dead-end recognition. Our experiments show that the nogoods found by SIXTHSENSE routinely reduce planning space and time on IPPC domains, enabling some planners to solve problems they could not previously handle.	algorithm;executable;experiment;first-order logic;first-order predicate;machine learning;overhead (computing);real life;sixthsense	Andrey Kolobov;Mausam;Daniel S. Weld	2010			simulation;computer science;artificial intelligence;machine learning;algorithm	AI	19.864847004550587	-13.868299875555275	14198
2a40679355767c8fc955518f2d44b6b8ac19eb18	generalized multiobjective evolutionary algorithm guided by descent directions	multiobjective optimization;multiobjective evolutionary algorithms;performance assessment	This paper proposes a generalized descent directions-guided multiobjective algorithm (DDMOA2). DDMOA2 uses the scalarizing fitness assignment in its parent and environmental selection procedures. The population consists of leader and non-leader individuals. Each individual in the population is represented by a tuple containing its genotype as well as the set of strategy parameters. The main novelty and the primary strength of our algorithm is its reproduction operator, which combines the traditional local search and stochastic search techniques. To improve efficiency, when the number of objective is increased, descent directions are found only for two randomly chosen objectives. Furthermore, in order to increase the search pressure in high-dimensional objective space, we impose an additional condition for the acceptance of descent directions found for leaders during local search. The performance of the proposed approach is compared with those produced by representative state-of-the-art multiobjective evolutionary algorithms on a set of problems with up to 8 objectives. The experimental results reveal that our algorithm is able to produce highly competitive results with well-established multiobjective optimizers on all tested problems. Moreover, due to its hybrid reproduction operator, DDMOA2 demonstrates superior performance on multimodal problems.	computation;descent;descent direction;evolutionary algorithm;experiment;local search (optimization);mathematical optimization;multimodal interaction;randomness;stochastic optimization	Roman Denysiuk;Lino A. Costa;Isabel A. Espírito-Santo	2014	J. Math. Model. Algorithms in OR	10.1007/s10852-014-9255-y	mathematical optimization;computer science;artificial intelligence;multi-objective optimization;machine learning;mathematics	AI	24.276995564464425	-3.708543963504464	14236
aab11655e07e730823cccb564d7749cb7be2a6c2	a closer look at the short-term return reversal	grupo de excelencia;sentiment;liquidity;administracion de empresas;economia y empresa;short term return reversal;grupo a;fundamental news	S returns unexplained by “fundamentals,” such as cash flow news, are more likely to reverse in the short run than those linked to fundamental news. Making novel use of analyst forecast revisions to measure cash flow news, a simple enhanced reversal strategy generates a risk-adjusted return four times the size of the standard reversal strategy. Importantly, isolating the component of past returns not driven by fundamentals provides a cleaner setting for testing existing theories of short-term reversals. Using this approach, we find that both liquidity shocks and investor sentiment contribute to the observed short-term reversal, but in different ways: Specifically, the reversal profit is attributable to liquidity shocks on the long side because fire sales more likely demand liquidity, and it is attributable to investor sentiment on the short side because short-sale constraints prevent the immediate elimination of overvaluation.	pervasive informatics;proxy server;return statement;theory	Zhi Da;Qianqiu Liu;Ernst Schaumburg	2014	Management Science	10.1287/mnsc.2013.1766	financial economics;economics;marketing;finance;economy;market liquidity;management;commerce	Web+IR	-2.0784096864288015	-8.82158322531119	14255
562748d15a446a12f6cf718b2a67372e35154f77	simulation of river stage using artificial neural network and mike 11 hydrodynamic model	computadora;tratamiento datos;computers;goodness of fit;levenberg marquardt;errors;water level;crecida;erreur;lms algorithm;feed forward neural network;rivers;neural networks;ordinateur;orissa india;rugosidad;root mean square error;performance;simulacion numerica;orissa;geometry;data processing;geometrie;traitement donnee;testing;time series;hydraulique;hydraulics;canal;algorithme;model validation;lm algorithm;modelo;back propagation training algorithms;hydrodynamique;control structure;propagacion;asie;orisa;roughness;chenal;rio;indexation;simulation numerique;riviere;hydrodynamic model;rugosite;hydrology;hydrological data;simulation study;developing country;algorithms;geometria;modele;model test;performances;monsoons;floods;error;neural network model;mousson;reseau neuronal;experimentation;channels;monzon;back propagation;models;hidraulica;red neuronal;hydrodynamics;propagation;india;training algorithm;digital simulation;artificial neural network;asia;inde;hidrodinamica;algoritmo;simulation models;crue	Simulation of water levels at different sections of a river using physically based flood routing models is quite cumbersome, because it requires many types of data such as hydrologic time series, river geometry, hydraulics of existing control structures and channel roughness coefficients. Normally in developing countries like India it is not easy to collect these data because of poor monitoring and record keeping. Therefore, an artificial neural network (ANN) technique is used as an effective alternative in hydrologic simulation studies. The present study aims at comparing the performance of the ANN technique with a widely used physically based hydrodynamic model in the MIKE 11 environment. The MIKE 11 hydrodynamic model was calibrated and validated for the monsoon periods (June-September) of the years 2006 and 2001, respectively. Feed forward neural network architecture with Levenberg-Marquardt (LM) back propagation training algorithm was used to train the neural network model using hourly water level data of the period June-September 2006. The trained ANN model was tested using data for the same period of the year 2001. Simulated water levels by the MIKE 11HD were compared with the corresponding water levels predicted by the ANN model. The results obtained from the ANN model were found to be much better than that of the MIKE 11HD results as indicated by the values of the goodness of fit indices used in the study. The Nash-Sutcliffe index (E) and root mean square error (RMSE) obtained in case of the ANN model were found to be 0.8419 and 0.8939m, respectively, during model testing, whereas in case of MIKE 11HD, the values of E and RMSE were found to be 0.7836 and 1.00m, respectively, during model validation. The difference between the observed and simulated peak water levels obtained from the ANN model was found to be much lower than that of MIKE 11HD. The study reveals that the use of Levenberg-Marquardt algorithm with eight hidden neurons in the hidden layer is sufficient to produce satisfactory results.	artificial neural network;simulation	Rabindra K. Panda;Niranjan Pramanik;Biplab Bala	2010	Computers & Geosciences	10.1016/j.cageo.2009.07.012	performance;hydrology;computer science;artificial intelligence;machine learning;mathematics;artificial neural network;statistics	ML	10.019360034857733	-23.57344328460542	14323
0755b133e05bb19b546f76ed13d4601505dc8e7b	interactive tabu search vs. interactive genetic algorithm	generic algorithm;multiple solution;interactive genetic algorithm;tabu search;interactive evolutionary computation	We propose an interactive tabu search (ITS) to be used for the development support of a product that fits a human’s feeling. Interactive evolutionary computation (IEC) is one of the technologies used in the development support of products that fit a human’s feeling using a computer and person undergoing a communication. The interactive generic algorithm (IGA) is generally used in the IEC. A major problem with the use of the IEC is the increased burden on the IEC user to evaluate multiple solution candidates. Using the ITS instead of the IGA may reduce this burden, because the ITS user chooses only his most favorite solution candidate among multiple solution candidates. We performed a comparison of the search performance using simulations with the ITS and IGA. As a result of this simulation, the search performance of the ITS exceeded that of the IGA by a range from 2% to 10%.	fits;generic programming;genetic algorithm;in-game advertising;simulation;tabu search	Tatsuya Hirokata;Masataka Tokumaru;Noriaki Muranaka	2010		10.1007/978-3-642-15399-0_74	mathematical optimization;simulation;interactive evolutionary computation;computer science;theoretical computer science	AI	19.72704048831853	-5.527643133146627	14349
8f7b40004cc5f5dc41a1783de45d8f8f6cfceef5	a novel aggregation principle for hesitant fuzzy elements	hesitant fuzzy set;membership uncertainty;aggregation principle;aggregation operator	In this paper, we present and apply a new aggregation principle for hesitant fuzzy elements (HFEs). First, we introduce a membership uncertainty index of the HFEs. Then we present a new aggregation principle for aggregating hesitant fuzzy elements, which can effectively reduce the computational complexity specific to the conventional aggregation principle. By the virtue of the proposed aggregation principle, the number of terms (values) in the aggregation result is significantly reduced. Furthermore, a t-test confirms that the scores associated with the results of the aggregation under conventional and new principles have no significant difference. Finally, a series of simulations are provided to demonstrate the feasibility and validity of the proposed aggregation principle.		Zhiming Mu;Shouzhen Zeng;Tomas Balezentis	2015	Knowl.-Based Syst.	10.1016/j.knosys.2015.04.008	artificial intelligence;machine learning;fuzzy logic;computational complexity theory;computer science	DB	-3.4919356510943094	-20.702025462835458	14408
5f5611cd94798b3c0b6bed4a4ca7f417c023a5f2	an efficient method for evaluating origin-destination connectivity reliability of real-world lifeline networks	seismic risk;metodo analisis;reliability;modele origine destination;distribution network;reseau distribution;efficiency;simulacion numerica;network analysis planning;modelo origen destinacion;red distribucion;lifeline networks;origin destination;eficacia;methode analyse;water supply system network;origin destination model;reseau adduction eau;origin and destination;analysis method;conexion;simulation numerique;raccordement;efficacite;ejemplo;travel behavior;riesgo sismico;travel patterns;example;connection;risque sismique;numerical simulation;red aduccion agua;exemple	This paper proposes a method of analyzing origin-destination (O-D) connectivity reliability of real-world lifeline networks. This research is novel because, for real-world lifeline networks with thousands of vertices and links, evaluating O-D connectivity is computationally infeasible with traditional methods. It is postulated that the O-D connectivity problem for the actual network can be converted into a virtual problem, while the computation for this problem is fast even for large networks. Hence, it is applicable to real-world lifeline networks. When combined with reliability analysis methods, the proposed method can be used to efficiently solve realistic O-D connectivity reliability problems for real-wold lifeline networks. In addition, a rating method is proposed to rank critical links; that is, links that have great influence on the O-D connectivity. The proposed framework should be valuable for management and reliability analysis of real-world lifeline networks.		Jianye Ching;Wei-Chih Hsu	2007	Comp.-Aided Civil and Infrastruct. Engineering	10.1111/j.1467-8667.2007.00501.x	computer simulation;seismic risk;telecommunications;connection;civil engineering;reliability;mathematics;efficiency;travel behavior;operations research	DB	13.588792161952492	-10.050663574776165	14487
28dbd028ee5d11706d7b0421fd5f18b8ff4cc48a	a measure of decision flexibility	risk aversion;decision maker;decision analysis;model uncertainty;decision theoretic planning;flexibility	We propose a decision-analytical approach to comparing the flexibility of decision situations from the perspective of a decisionmaker who exhibits constant risk-aversion over a monetary value model. Our approach is simple yet seems to be consistent with a variety of flexibility concepts, including robust and adaptive alternatives. We try to compensate within the model for uncertainty that was not anticipated or not modeled. This approach not only allows one to compare the flexibility of plans, but also guides the search for new, more flexible alternatives.	risk aversion	Ross D. Shachter;Marvin Mandelbaum	1996			decision-making;optimal decision;risk aversion;decision analysis;management science;weighted sum model	AI	-4.041940181829651	-13.650697317031666	14496
80f8b444eaf86550248c073e3b7b8f57ad2e76c4	optimal surveillance of a failure system	random times	For a system subject to breakdown at some random time two continuous surveillance plans are available, diiering in eeciency and cost. The controller is allowed to switch from one plan to the other at any time. We nd the optimal surveillance strategy in closed form and apply this result in some examples.		Wolfgang Stadje;Dror Zuckerman	1999	Annals OR	10.1023/A:1018905908440	artificial intelligence;operations management;mathematics;operations research	Theory	4.999388459314898	-1.3555591124790587	14516
dc19ded85f0bbd7d02353a48c094c69513e6a8ba	co-evolving line drawings with hierarchical evolution		We use an adversarial approach inspired by biological coevolution to generate complex line drawings without human guidance. Artificial artists and critics work against each other in an iterative competitive framework, forcing each to become increasingly sophisticated to outplay the other. Both the artists and critics are implemented in hercl, a framework combining linear and stack-based Genetic Programming, which is well suited to coevolution because the number of competing agents is kept small while still preserving diversity. The aesthetic quality of the resulting images arises from the ability of the evolved hercl programs, making judicious use of register adjustments and loops, to produce repeated substructures with subtle variations, in the spirit of low-complexity art.	artificial neural network;convolutional neural network;generative adversarial networks;genetic programming;glossary of computer graphics;iteration;line drawing algorithm;low-complexity art;programming paradigm;stack-oriented programming language;subroutine;testament	Darwin Vickers;Jacob Soderlund;Alan Blair	2017		10.1007/978-3-319-51691-2_4	genetic programming;adversarial system;coevolution;biological coevolution;computational creativity;artificial intelligence;computer science	AI	17.130195885102683	-18.724818339922358	14541
6c4c572751e45df6f6c9323491fca8aab7421124	reliability of a system with n parallel and not identical elements with constant failure rates	system reliability;exponential distribution;reliability theory exponential distribution failure analysis markov processes;queuing system;reliability modeling;identical elements;constant failure rates;state dependence;reliability theory;failure analysis;electrical devices;failure rate;marcov chain;reliability models;markov processes;identical elements system reliability constant failure rates reliability models marcov chain electrical devices exponential distribution;exponential distribution computational intelligence industrial engineering web design steady state real time systems computational modeling automatic control intelligent agent competitive intelligence	Reliability models based on Marcov chain (Except queuing systems) have extensive application in calculate the reliability of electrical and electronically devices. In this article we consider a system with fl parallel and not necessary identical elements with constant failure rates (Failure rates had exponential distribution). The system works until entire elements fail. The elements are not repairable and the system could not come back to previous states. At the end, the system parameters like MTTF, are calculated and a numerical example with three parallel and not identical elements is solved.	mean time between failures;numerical analysis;queueing theory;time complexity	M. Sharifi;M. Ganjian;P. Shafiee	2006	2006 International Conference on Computational Inteligence for Modelling Control and Automation and International Conference on Intelligent Agents Web Technologies and International Commerce (CIMCA'06)	10.1109/CIMCA.2006.175	exponential distribution;failure analysis;real-time computing;reliability theory;computer science;failure rate;queue management system;markov process	Robotics	6.806398624049184	0.6106355564358614	14542
e173dfb6e6a6e406ca9f15cc732e8db132447ef4	similar product clustering for long-tail cross-sell recommendations		"""One of the main reasons of the rapid growth and development of e-commerce is the ability of on-line stores to provide large varieties of products, unlike their off-line counterparts, confined by stock spaces. This also means that on-line stores have products, which have few individual purchases but form a large chunk of revenue all together. They are known as """"long-tail"""" products. Long-tail data sparsity makes it challenging to apply recommender algorithms. In this paper, we consider cross-sell recommendations using association rule mining. We consider application of clustering techniques to tackle this problem, and compare different clustering and distance calculation methods. Behavioral data, such as product views, and content data, such as category tree and product names, are used to calculate product similarity. Also, we develop a cross-validation method that allows stable metric calculations for algorithms that focus on long-tail products. We show that product clustering that uses session-based distances improve cross-sell recommendations for long-tail items."""	adaptive filter;algorithm;association rule learning;cluster analysis;cross-validation (statistics);e-commerce;jaccard index;long tail;online and offline;recommender system;sparse matrix	Vladislav Grozin;Alla Levina	2017				ML	4.921760425862611	-18.056271155181296	14571
e1bfe3b3d82a4d76b59e065efd4ea1359d4c69a7	a hybrid genetic algorithm for the two-dimensional single large object placement problem	parallelisme;elitism;inmigracion;elitisme;algorithme glouton;heuristic method;metodo heuristico;cutting stock problem;algoritmo genetico;genetics;optimisation combinatoire;rendement matiere;parallelism;paralelismo;rendimiento materia;probleme decoupe;algorithme genetique;greedy algorithm;algoritmo gloton;genetic algorithm;genetic algorithms;problema troquelado;methode heuristique;two dimensional cutting and packing;combinatorial optimization;immigration;heuristic algorithm;yield material economy;hybrid genetic algorithm;optimizacion combinatoria;single large object placement problem	In the two-dimensional single large object placement problem, we are given a rectangular master surface which has to be cut into a set of smaller rectangular items, with the aim of maximizing the total value of the pieces cut. We consider the special case in which the items cannot be rotated and must be cut with their edges always parallel to the edges of the surface. We present new greedy algorithms and a hybrid genetic approach with elitist theory, immigration rate, heuristics online and tailored crossover operators. Extensive computational results for a large number of small and large benchmark test problems are presented. The results show that our approach outperforms existing heuristic algorithms. 2006 Elsevier B.V. All rights reserved.	benchmark (computing);cut (graph theory);genetic algorithm;greedy algorithm;heuristic (computer science);memetic algorithm	Eleni Hadjiconstantinou;Manuel Iori	2007	European Journal of Operational Research	10.1016/j.ejor.2005.11.061	mathematical optimization;genetic algorithm;combinatorial optimization;computer science;artificial intelligence;mathematics;algorithm	AI	19.759086050358963	3.702154164202076	14638
8d65397c2af2eacb0e8900ded68c389a4df6802a	on the selection of a reduced set of indexes	indexation			Michael Hatzopoulos;John G. Kollias	1985	Comput. J.	10.1093/comjnl/28.4.406	computer science	DB	17.26791132690339	-13.067598036709898	14666
0249858b415565a3c47cd653c185ce86692b3a70	a branch-and-regret heuristic for stochastic and dynamic vehicle routing problems	heuristic;regret	This paper describes a new Branch-and-Regret Heuristic for a class of dynamic and stochastic vehicle routing problems. This work is motivated by a real-life problem faced by a major transporter in Norway. The heuristic uses stochastic information during the solution process. The new method is shown to be superior to previous heuristics that are uniquely based on a pure dynamic approach. The proposed heuristic is well suited to problems containing stochastic customers, stochastic demands, or both. © 2007 Wiley Periodicals, Inc. NETWORKS, Vol. 49(4), 330–34	heuristic (computer science);john d. wiley;real life;stochastic process;vehicle routing problem	Lars Magnus Hvattum;Arne Løkketangen;Gilbert Laporte	2007	Networks	10.1002/net.20182	consistent heuristic;stochastic neural network;mathematical optimization;routing;heuristic;branching;computer science;artificial intelligence;stochastic optimization;vehicle routing problem;dynamic programming;mathematics;ramification;operations research;regret	AI	17.660420941849377	2.4947577749576837	14771
75d096b7e8053546a644b4152c6c6c2cfe53faf6	temporal planning while the clock ticks		One of the original motivations for domain-independent planning was to generate plans that would then be executed in the environment. However, most existing planners ignore the passage of time during planning. While this can work well when absolute time does not play a role, this approach can lead to plans failing when there are external timing constraints, such as deadlines. In this paper, we describe a new approach for time-sensitive temporal planning. Our planner is aware of the fact that plan execution will start only once planning finishes, and incorporates this information into its decision making, in order to focus the search on branches that are more likely to lead to plans that will be feasible when the planner finishes.	algorithm;automated planning and scheduling;autonomous system (internet);axosoft;baseline (configuration management);failure;heuristic;lazy evaluation;mathematical optimization;optimization problem;partial-order planning;proxy server;run time (program lifecycle phase);search tree;slack variable;super-twisted nematic display;tree (data structure)	Michael Cashmore;Andrew Coles;Bence Cserna;Erez Karpas;Daniele Magazzeni;Wheeler Ruml	2018			machine learning;artificial intelligence;computer science	AI	18.021281348015236	-10.6077390959812	14836
7a040a0df39d4c30656a66690e3962bf04a3f91b	an analytical model of a deferred and incremental update strategy for secondary indexes	indexation;analytical model	Without Abstract	windows update	Edward Omiecinski;Wei Liu;Ian F. Akyildiz	1989		10.1007/3-540-51295-0_129	computer science;operations management;data mining;mathematical economics	DB	3.049558692588786	-7.394397409717335	14855
4ac0384c883ff4a6920572b34372077ae6aa1da9	a multi-objective meta-model assisted memetic algorithm with non gradient-based local search	aggregation function;support vector machines;multi objective optimization;multi objective evolutionary algorithm;hooke jeeves algorithm;memetic algorithm;mathematical programming;hybrid algorithms;support vector machine;evolutionary process;local search;hybrid algorithm;meta model	In this paper, we present an approach in which a local search mechanism is coupled to a multi-objective evolutionary algorithm. The local search mechanism is assisted by a meta-model based on support vector machines. Such a mechanism consists of two phases: the first one involves the use of an aggregating function which is defined by different weighted vectors. For the (scalar) optimization task involved, we adopt a non-gradient mathematical programming technique: the Hooke-Jeeves method. The second phase computes new solutions departing from those obtained in the first phase. The local search engine generates a set of solutions which are used in the evolutionary process of our algorithm. The preliminary results indicate that our proposed approach is quite promising.	evolutionary algorithm;gradient;local search (optimization);mathematical optimization;memetic algorithm;memetics;metamodeling;multi-objective optimization;support vector machine;web search engine	Saúl Zapotecas Martínez;Carlos A. Coello Coello	2010		10.1145/1830483.1830581	evolutionary programming;support vector machine;mathematical optimization;computer science;artificial intelligence;local search;hill climbing;machine learning;mathematics;best-first search;difference-map algorithm;memetic algorithm;guided local search;search algorithm	AI	23.821090879239545	-3.1307663602441043	14882
de377035bd89a9a5a93d12aa024ddf58e1edfa53	evaluation of a simulated annealing metaheuristic to solve a forest planning problem	simulated annealing;forest planning;metaheuristic	This paper discusses a forest planning problem that aims to obtain a specific management plan for the planting and harvesting of wood. The problem has the objective of maximising the net present value. A simulated annealing (SA) metaheuristics was proposed to solve the problem. The SA performance is tested in a set of ten problems generated from real data. The results obtained by the SA were compared with solution found by a GRASP and a genetic algorithm reported in the literature.	grasp;genetic algorithm;heuristic;metaheuristic;semiconductor consolidation;simulated annealing;software release life cycle	Antonio Almeida De Barros Junior;Gustavo Willam Pereira;Geraldo R. Mauri;Robson De Souza Melo	2013	IJAOM	10.1504/IJAOM.2013.058895	mathematical optimization;simulation;parallel metaheuristic;simulated annealing;computer science;engineering;operations management;metaheuristic	AI	18.26110129661911	-0.4377548139079008	14889
b25dabeaee2ac0c18cce5d921617b4a4d01fc191	potential effect of internal markets on hospitals' waiting time	queueing;queuing model;economic efficiency;medical care;hospitals;relative prices;waiting list;travel cost;pilot project;waiting time;international marketing;cost effectiveness;waiting lists	The paper examines by means of a queuing model how an internal market can contribute to a cost-effective allocation of resources in a hospital sector with waiting lists. Cost-effectiveness is defined as the allocation of resources that minimises patients' waiting time. The cost-effective allocation of patients between hospitals implies specialisation between hospitals with moderate travel costs, even when hospitals are equally efficient. It is shown that, in theory, there are relative prices that encourage the implementation of a cost-effective allocation of resources. Specific problems of market implementation may occur in medical care. There is little experience with managed markets for hospital treatment. The potential benefit in economic efficiency calls for pilot projects to gain experience in this important area.		Tor Iversen	2000	European Journal of Operational Research	10.1016/S0377-2217(99)00044-2	cost-effectiveness analysis;actuarial science;relative price;economics;operations management;queueing theory;economic efficiency;commerce	Theory	8.26564192100909	-3.909823295121556	14923
5c8b81c2f53fdd9bfe747dbbb1db851bafa920e0	confident decision making and improved throughput for cereal manufacturing with simulation	digital simulation;food processing industry;industrial control;kellogg company;area manufacturing capacity;cereal manufacturing;confident decision making;line management;simulation;throughput improvement	In 1999, Kellogg Company needed to rationalize its area manufacturing capacity. A significant portion of the production was moved between Kellogg manufacturing plants. Simulation played an important role in two facets of this project. First, it helped determine if the proposed engineering changes would pick up the slack in production lost due to idling of assets. Second, simulation was used to develop line management setpoints for increasing the output from the installed capacity.	simulation;slack variable;throughput	Travis A. Dahl;Brian F. Jacob	2000			project management;packaging and labeling;throughput;line management;simulation;computer science;engineering;food processing;industrial engineering;manufacturing engineering	Robotics	9.660163042669055	3.7935589797445832	14972
30e26fa656a3a36f59b18c3e307b0b2300bf771d	tuning the performance of the mmas heuristic	response surface methodology;perforation;degree of freedom;max min ant system;performance analysis;travelling salesperson problem;design of experiment	This paper presents an in-depth Design of Experiments (DOE) methodology for the performance analysis of a stochastic heuristic. The heuristic under investigation is Max-Min Ant System (MMAS) for the Travelling Salesperson Problem (TSP). Specifically, the Response Surface Methodology is used to model and tune MMAS performance with regard to 10 tuning parameters, 2 problem characteristics and 2 performance metrics—solution quality and solution time. The accuracy of these predictions is methodically verified in a separate series of confirmation experiments. The two conflicting responses are simultaneously optimised using desirability functions. Recommendations on optimal parameter settings are made. The optimal parameters are methodically verified. The large number of degrees-of-freedom in the MMAS design are overcome with a Minimum Run Resolution V design. Publicly available algorithm and problem generator implementations are used throughout. The paper should therefore serve as an illustrative case study of the principled engineering of a stochastic heuristic.	algorithm;ant colony optimization algorithms;classification tree method;design of experiments;experiment;heuristic;recommender system;response surface methodology;stochastic gradient descent;travelling salesman problem	Enda Ridge;Daniel Kudenko	2007		10.1007/978-3-540-74446-7_4	mathematical optimization;engineering;operations management;operations research	AI	18.146275401024756	-5.425043916025467	15051
3ecc000219cc9e056d08a28f0725fc50d4ed7b06	load balancing in a parallel dynamic programming multi-method applied to the 0-1 knapsack problem	dynamic programming;knapsack problems;resource allocation;load management dynamic programming parallel algorithms concurrent computing supercomputers linear programming operations research data structures algorithm design and analysis computer displays;dynamic program;0 1 knapsack problem load balancing parallel dynamic programming multimethod;knapsack problem;resource allocation knapsack problems dynamic programming parallel algorithms;load balance;dy namic programming;parallel algorithms	The 0-1 knapsack problem is considered. A parallel dynamic programming multi-method using dominance technique and processor cooperation is proposed. Different load balancing approaches are studied. Computational experiments carried out on an Origin 3800 supercomputer are reported and analyzed.	computation;dynamic programming;experiment;knapsack problem;load balancing (computing);sgi origin 3000 and onyx 3000;supercomputer	Moussa Elkihel;Didier El Baz	2006	14th Euromicro International Conference on Parallel, Distributed, and Network-Based Processing (PDP'06)	10.1109/PDP.2006.46	continuous knapsack problem;mathematical optimization;reactive programming;resource allocation;computer science;load balancing;theoretical computer science;cutting stock problem;dynamic programming;distributed computing;parallel algorithm;knapsack problem	HPC	21.956060311285032	2.281517951150465	15078
48505fc12b7dc485dd44094381bf7d79c988dc38	milp formulations and an iterated local search algorithm with tabu thresholding for the order batching problem		In this work we deal with the Order Batching Problem (OBP) considering traversal, return and midpoint routing policies. For the first time, we introduce Mixed Integer Linear Programming (MILP) formulations for these three variants of the OBP. We also suggest an efficient Iterated Local Search Algorithm with Tabu Thresholding (ILST). According to our extensive computational experiments on standard and randomly generated instances we can say that the proposed ILST yields an outstanding performance in terms of both accuracy and efficiency.	iterated function;iterated local search;local search (optimization);search algorithm;tabu search;thresholding (image processing)	Temel Öncan	2015	European Journal of Operational Research	10.1016/j.ejor.2014.11.025	mathematical optimization;combinatorics;machine learning;mathematics	Robotics	20.316551291593623	2.3084655969573165	15082
0a89a0b184ddb332ee16827c4b7bf7df3c20a39d	a hybrid approach based on the combination of adaptive neuro-fuzzy inference system and imperialist competitive algorithm: oil flow rate of the wells prediction case study	anfis;imperialist competitive algorithm;gradient descent;oil flow rate	In this paper, a novel hybrid approach composed of adaptive neuro-fuzzy inference system (ANFIS) and imperialist competitive algorithm is proposed. The imperialist competitive algorithm (ICA) is used in this methodology to determine the most suitable initial membership functions of the ANFIS. The proposed model combines the global search ability of ICA with local search ability of gradient descent method. To illustrate the suitability and capability of the proposed model, this model is applied to predict oil flow rate of the wells utilizing data set of 31 wells in one of the northern Persian Gulf oil fields of Iran. The data set collected in a three month period for each well from Dec. 2002 to Nov. 2010. For the sake of performance evaluation, the results of the proposed model are compared with the conventional ANFIS model. The results show that the significant improvements are achievable using the proposed model in comparison with the results obtained by conventional ANFIS.	adaptive neuro fuzzy inference system;emoticon;francis;gradient descent;gulf of execution;imperialist competitive algorithm;independent computing architecture;inference engine;input/output;intermediate representation;local search (optimization);matthews correlation coefficient;maxima and minima;mean squared error;membership function (mathematics);nl (complexity);neuro-fuzzy;performance evaluation	Shahram Mollaiy-Berneti	2013	Int. J. Comput. Intell. Syst.	10.1080/18756891.2013.768430	gradient descent;simulation;adaptive neuro fuzzy inference system;computer science;artificial intelligence;machine learning;imperialist competitive algorithm	AI	8.937220104578696	-20.10810937602548	15172
13f1391fac3fe702549d1b9c43e398f015229ca0	experience-based identification and control via higher-level learning and context discernment	learning algorithm;reinforcement learning;optimal control;multi agent systems;system identification;identification;learning artificial intelligence;context awareness humans control systems learning optimal control system identification artificial intelligence context modeling computational intelligence laboratories;experience base;optimal control identification learning artificial intelligence multi agent systems;system identification experience based identification higher level learning algorithm context discernment ai system intelligent computing agent reinforcement learning algorithm optimal controller	"""In AI systems so far developed, more knowledge (typically stored as """"rules"""") entails slower processing; in the case of humans, the more knowledge attained (in the form of experience), the speed/efficiency of performing new related tasks is improved. Experience-based (EB) identification and control is explored with the objective of achieving more human-like processes for 'intelligent' computing agents. The notion of experience is being successfully addressed via a novel concept for applying reinforcement learning (RL), called HLLA -higher level learning algorithm. The key idea is to re-purpose the RL method (to a """"higher level"""") such that instead of creating an optimal controller for a given task, an already achieved collection of such solutions for a variety of related contexts is provided (as an experience repository), and HLLA creates a strategy for optimally selecting a solution from the repository. The selection process is triggered by the agent becoming aware that a change in context has occurred, followed by the agent seeking information about what changed -a process here called context discernment - and finally, by selection. Typically, context discernment entails a form of system identification (SID); substantial enhancement of SID is also achieved via the EB methods. Examples are given."""	algorithm;experience;humans;image scaling;iterative method;optimal control;reinforcement learning;system identification	George G. Lendaris	2006	The 2006 IEEE International Joint Conference on Neural Network Proceedings	10.1109/IJCNN.2006.247279	identification;error-driven learning;simulation;optimal control;system identification;computer science;artificial intelligence;machine learning;reinforcement learning	Robotics	19.163611402828092	-19.894510553434664	15188
ef93ad6f4e0380fbd23a934827ab4eba3a091f50	performance degradation analysis method using satellite telemetry big data			big data;elegant degradation	Feng Zhou;De-Chang Pi;Xu Kang;Hua-Dong Tian	2016		10.3233/978-1-61499-722-1-186	environmental science;big data;remote sensing;satellite;telemetry	ML	7.226156018523008	-12.104450500017572	15281
4411669f8b3fb2635237e7d0bbb94d2504ec0e0e	planning umts base station location using genetic algorithm with a dynamic trade-off parameter	umts;multi objective optimization;genetic algorithm	In this paper, we address the problem of planning the universal mobile telecommunication system UMTS base stations location for uplink direction. The objective is to maximize the total trafic covered f and minimize the total installation cost g. This problem is modelled in the form of multi-objective optimization problem that can be transformed into a mono-objective problem of the form f+λg, where λ>0 is a trade-off parameter between the objective functions f and g. Our aim here is to present a solution method to the problem based on a genetic algorithm GA, which automates the choice of the parameter λ by varying it at each iteration of the algorithm. To apply the GA to our problem, we have proposed a special coding that combines the binary and integer coding. To validate the proposed method some numerical examples are given. The obtained results show the efficiency of our approach.	genetic algorithm	Mohammed Gabli;El Miloud Jaâra;El Bekkaye Mermri	2013		10.1007/978-3-642-40148-0_9	mathematical optimization;simulation;engineering;operations management	HCI	15.608831782490345	-0.13553753573253222	15285
4470458961d95785f8b9b6cc37fa312511eb1897	area protection in adversarial path-finding scenarios with multiple mobile agents on graphs - a theoretical and experimental study of strategies for defense coordination		We address a problem of area protection in graph-based scenarios with multiple agents. The problem consists of two adversarial teams of agents that move in an undirected graph shared by both teams. Agents are placed in vertices of the graph; at most one agent can occupy a vertex; and they can move into adjacent vertices in a conflict free way. Teams have asymmetric goals: the aim of one team - attackers - is to invade into given area while the aim of the opponent team - defenders - is to protect the area from being entered by attackers by occupying selected vertices. We study strategies for allocating vertices to be occupied by the team of defenders to block attacking agents. We show that the decision version of the problem of area protection is PSPACE-hard under the assumption that agents can allocate their target vertices multiple times. Further we develop various on-line vertex-allocation strategies for the defender team in a simplified variant of the problem with single stage vertex allocation and evaluated their performance in multiple benchmarks. The success of a strategy is heavily dependent on the type of the instance, and so one of the contributions of this work is that we identify suitable vertex-allocation strategies for diverse instance types. In particular, we introduce a simulation-based method that identifies and tries to capture bottlenecks in the graph, that are frequently used by the attackers. Our experimental evaluation suggests that this method often allows a successful defense even in instances where the attackers significantly outnumber the defenders.	adversary (cryptography);benchmark (computing);bottleneck (software);decision problem;graph (discrete mathematics);mobile agent;neighbourhood (graph theory);online and offline;pspace-complete;vertex (geometry)	Marika Ivanová;Pavel Surynek;Katsutoshi Hirayama	2018		10.5220/0006583601840191	artificial intelligence;computer science;machine learning;vertex (geometry);adversary;adversarial system;graph	AI	14.89998835204411	-1.3047946005440643	15312
07b6a716b153eb8a0e52464de51df7226d726048	a game theoretic model for smart grids demand management	0 1 mixed linear programming approach game theoretic model smart grids demand management demand side management optimization process nash equilibrium;smart power grids demand side management game theory linear programming;smart grid advanced metering infrastructure ami demand side management dsm game theory nash equilibrium;games nash equilibrium smart grids linear programming vectors smart meters mathematical model	Demand-side management (DSM) plays a key role in the future of smart grids. Recently, DSM researchers have developed various mathematical models to optimize the demand response. Most of these works ignore the channel impairments' impact on the optimization process. In this paper, we propose a new noncooperative game theoretic model for the management of smart grid's demand considering the packet error rate in our formulation. We set the Nash equilibrium conditions for the proposed model. Under an assumption on the form of the utility functions, we develop a 0-1 mixed linear programming approach to compute nondominated extreme Nash equilibria. Results on a numerical example are provided and some useful insights are presented. Under some assumptions and a fully proven proposition, a feasible nondominated Nash equilibrium solution is found. Finally, we report and comment on computational experiments on randomly generated smart grid DSM game instances with different characteristics.	agent-based model;bit error rate;computation;experiment;game theory;grid computing;linear programming;mathematical model;mathematical optimization;nash equilibrium;network packet;numerical analysis;procedural generation	Slim Belhaiza;Uthman A. Baroudi	2015	IEEE Transactions on Smart Grid	10.1109/TSG.2014.2376632	simulation;economics;microeconomics;mathematical economics	Robotics	1.5939737313846658	2.4940023183732047	15314
684160d4642fc3335ecd360b29cd825e5c1f08c6	a forecasting model based on multi-valued neutrosophic sets and two-factor, third-order fuzzy fluctuation logical relationships		Making predictions according to historical values has long been regarded as common practice by many researchers. However, forecasting solely based on historical values could lead to inevitable over-complexity and uncertainty due to the uncertainties inside, and the random influence outside, of the data. Consequently, finding the inherent rules and patterns of a time series by eliminating disturbances without losing important details has long been a research hotspot. In this paper, we propose a novel forecasting model based on multi-valued neutrosophic sets to find fluctuation rules and patterns of a time series. The contributions of the proposed model are: (1) using a multi-valued neutrosophic set (MVNS) to describe the fluctuation patterns of a time series, the model could represent the fluctuation trend of up, equal, and down with degrees of truth, indeterminacy, and falsity which significantly preserve details of the historical values; (2) measuring the similarities of different fluctuation patterns by the Hamming distance could avoid the confusion caused by incomplete information from limited samples; and (3) introducing another related time series as a secondary factor to avoid warp and deviation in inferring inherent rules of historical values, which could lead to more comprehensive rules for further forecasting. To evaluate the performance of the model, we explored the Taiwan Stock Exchange Capitalization Weighted Stock Index (TAIEX) as the major factor we forecast, and the Dow Jones Index as the secondary factor to facilitate the predicting of the TAIEX. To show the universality of the model, we applied the proposed model to forecast the Shanghai Stock Exchange Composite Index (SHSECI) as well.	degree of truth;hamming distance;indeterminacy in concurrent computation;java hotspot virtual machine;jones calculus;multi-factor authentication;quantum fluctuation;time series;universal turing machine	Hongjun Guan;Jie He;Aiwu Zhao;Zongli Dai;Shuang Guan	2018	Symmetry	10.3390/sym10070245	fuzzy logic;hamming distance;econometrics;composite index;complete information;stock market index;combinatorics;confusion;stock exchange;falsity;mathematics	Web+IR	3.6049214323187706	-21.348722020372254	15344
06b1518e5dbe4c7b79a8f01cab17e76caf9fb9a0	evolving recurrent models using linear gp	genetic program;linear genetic programming;recurrent architectures;temporal properties;empirical evaluation	Turing complete Genetic Programming (GP) models introduce the concept of internal state, and therefore have the capacity for identifying interesting temporal properties. Surprisingly, there is little evidence of the application of such models to problems for prediction. An empirical evaluation is made of a simple recurrent linear GP model over standard prediction problems.	genetic programming;turing completeness	Xiao Luo;Malcolm I. Heywood;A. Nur Zincir-Heywood	2005		10.1145/1068009.1068311	computer science;artificial intelligence;machine learning;algorithm	ML	17.84842828623113	-23.527575466497044	15398
eb77ba5362dfa713b719c29bb407072037b76cb5	lower bounds for multi-stage vehicle routing	vehicle routing;picking up requests multistage vehicle routing universal lower bound average delay dynamic pickup and delivery problem delivering requests;routing relays delay vehicle dynamics unmanned aerial vehicles position measurement vehicle safety protective relaying network servers motion control;vehicles;vehicles delays;large classes;lower bound;delays	The primary purpose of this paper is to develop a new universal lower bound on performance for multi-stage problems that arise in many applications such as manufacturing, communication, inventory routing and vehicle routing. In this paper, we develop such bounds on average delay in the context of the dynamic pickup and delivery problem (DPDP), where a collection of vehicles are responsible for picking up and delivering requests that arrive at unknown times and locations in a certain geographic area. We show that our bounds are tight (order optimal) for a large class of batching policies.	vehicle routing problem	Holly A. Waisanen;Devavrat Shah;Munther A. Dahleh	2007	2007 46th IEEE Conference on Decision and Control	10.1109/CDC.2007.4434790	routing;static routing;real-time computing;simulation;zone routing protocol;engineering;dynamic source routing;multipath routing;destination-sequenced distance vector routing;vehicle routing problem;mathematics;upper and lower bounds	Robotics	11.616145163117352	2.250147017444497	15414
8b3224aecc97974ea2d0f306ded8e125dbabc5fa	an ant colony optimization for solving a hybrid flexible flowshop	ant colony optimization;flowshop;sequence dependent setup;makespan;hybridization	In this paper, we propose an ant colony optimization (ACO) to solve a realistic variant of flowshop problem. The considered scheduling problem is a hybrid flexible flowshop problem with sequence-dependent setup times under the objective of minimizing the makespan. The proposed approach uses concept from multi-objective evolutionary algorithms and look-ahead information to enhance solutions quality. We also introduce new constructive heuristic used in the ACO local improvement. Numerical experiments were performed to compare the performance of the ACO on different benchmarks from the literature. The results indicate that the ACO is very competitive and enhances solutions of the known reference sets.	ant colony optimization algorithms;benchmark (computing);constructive heuristic;evolutionary algorithm;experiment;makespan;mathematical optimization;numerical method;reference implementation;scheduling (computing)	Aymen Sioud;Caroline Gagné;Marc Gravel	2014		10.1145/2598394.2598402	mathematical optimization;engineering;artificial intelligence;operations management	AI	22.070157006888248	-0.34798872039832124	15452
d51e2fb8dbe625bb54e779862d4ead13b35a0b0a	a method based on extended fuzzy transforms to approximate fuzzy numbers in mamdani fuzzy rule-based system		We propose a new Mamdani fuzzy rule-based system in which the fuzzy sets in the antecedents and consequents are assigned in a discrete set of points and approximated by using the extended inverse fuzzy transforms, whose components are calculated by verifying that the dataset is sufficiently dense with respect to the uniform fuzzy partition. We test our system in the problem of spatial analysis consisting in the evaluation of the livability of residential housings in all the municipalities of the district of Naples (Italy). Comparisons are done with the results obtained by using trapezoidal fuzzy numbers in the fuzzy rules.		Ferdinando Di Martino;Salvatore Sessa	2018	Adv. Fuzzy Systems	10.1155/2018/8458916	fuzzy logic;discrete mathematics;fuzzy set;fuzzy rule;partition (number theory);fuzzy number;mathematics	AI	-1.2467089851866084	-21.265732772554	15458
816b05cdc584fc65cf82264e29524f68a22583b9	let smart ants help you reduce the delay penalty of multiple software projects		Delays often occur in real-world software development projects and may cause significant monetary penalties to software companies. Meanwhile, industry lessons have shown that adding inexperienced employees would cause further delays due to the learning curve and communication overhead. However, if employees with same or similar skills and domain knowledge can be rescheduled from other concurrent projects to help with the delayed projects, it may be possible to reduce or even eliminate delay penalties without requesting extra employees. Here, the big challenge is how to conduct employee rescheduling without having employees working overtime, which is an NP hard problem in nature. To address such a problem, this paper proposes a novel employee rescheduling strategy based on improved ant colony optimization algorithm. Specifically, three generic rules are proposed to improve the effectiveness in generating valid solutions. Preliminary results on benchmark projects show that our strategy can achieve much better effectiveness than its genetic algorithm based counterpart in reducing the overall delay penalty of multiple software projects.	ant colony optimization algorithms;benchmark (computing);experience;genetic algorithm;mathematical optimization;np-hardness;overhead (computing);software development;software industry	Wei Zhang;Xiao Qiao Liu;Yun Yang	2017	2017 IEEE/ACM 39th International Conference on Software Engineering Companion (ICSE-C)			SE	14.353173675585941	-2.316320314695561	15490
421c02ab467d53fc6f54675cb0c16397d93d60e0	car accidents at the intersection with speed limit zone and open boundary conditions		Using cellular automata model, we numerically study the traffic characteristics and the probability Pac of car accidents to occur at the intersection of two roads under open boundary conditions. Beside the free flow (phase I), jamming phase (phase III) and maximum current (phase IV) the phase diagram exhibits a new phase (phase II) which take place for β u003c 0.4 (β is the extracting rate). Moreover, the investigation of probability of the occurrence of car accidents has shown that the probability Pac increases with increasing the extraction rate β and exhibits a maximum at the transition point from the free flow to the congestion phase. However, when β exceeds the value 0.4, the probability Pac doesn’t depends on β. Furthermore, simulation results show that the speed limit zone (SLZ) can improve the road safety, where the probability Pac decreases with increasing length of SLZ. Likewise, we found that there is a critical injection rate αc, below which both roads must be equiprobable. However, above which, one road must always have priority in order to reduce the risk of collisions.		Rachid Marzoug;Hicham Echab;Noureddine Lakouari;Hamid Ez-Zahraouy	2016		10.1007/978-3-319-44365-2_30	jamming;cellular automaton;discrete mathematics;transition point;boundary value problem;computer science;phase diagram;speed limit;control theory	Vision	10.20997619675307	-10.162516369855158	15496
e99073f6c543c3cf0107353d488437ad76587a36	approximate fuzzy preorders and equivalences	finite element methods;fuzzy set theory approximation theory fuzzy logic;fuzzy relations approximate fuzzy preorders approximate fuzzy equivalences;fuzzy relation;data mining;boundary conditions fuzzy logic;fuzzy set theory;fuzzy sets;approximate fuzzy equivalences;fuzzy logic;upper bound;approximation theory;transforms;fuzzy relations;approximation methods;approximate fuzzy preorders	Although Fuzzy Preorders and Fuzzy Equivalences are well established types of fuzzy relations, they are defined in terms of properties to be fulfilled in a crisp way. In this paper we relax those requirements by making them gradual in some way. As a result, a number of relations which are not strictly reflexive, symmetric or transitive with respect to any t-norm T, would be regarded as (Approximate) Fuzzy Preorders or Equivalences.	fuzzy logic;requirement;t-norm	Dionís Boixader;Jordi Recasens	2009	2009 IEEE International Conference on Fuzzy Systems	10.1109/FUZZY.2009.5277247	combinatorics;mathematical analysis;discrete mathematics;type-2 fuzzy sets and systems;computer science;artificial intelligence;fuzzy number;mathematics;fuzzy set	Robotics	-0.8937190568635308	-23.2702749762729	15580
ae7d8b14b4320d0ac066c75861649b6a09dcc301	a fuzzy seasonal arima model for forecasting	forecasting;soft drinks;prevision;fuzzy regression;fuzzy time series;modele regression floue;modelo autorregresivo;algoritmo borroso;sarima;arma model;regression model;forecasting model;time series;decision maker;regresion;modelo arma;autoregressive model;arima model;modelo regresion;regression;serie temporelle floue;seasonality;regression floue;fuzzy algorithm;modele regression;serie temporelle;autoregression integrated moving average;serie temporal;algorithme flou;modele arma;fuzzy regression model;time series data;fuzzy sarima;modele autoregressif;article;possibility distribution;arim	This paper proposes a fuzzy seasonal ARIMA (FSARIMA) forecasting model, which combines the advantages of the seasonal time series ARIMA (SARIMA) model and the fuzzy regression model. It is used to forecast two seasonal time series data of the total production value of the Taiwan machinery industry and the soft drink time series. The intention of this paper is to provide business which are affected by diversified management with a new method to conduct short-term forecasting. This model includes both interval models with interval parameters and the possible distribution of future value. Based on the results of practical application, it can be shown that this model makes good forecasts and is realistic. Furthermore, this model makes it possible for decision makers to forecast the best and worst estimates based on fewer observations than the SARIMA model.	autoregressive integrated moving average	Fang-Mei Tseng;Gwo-Hshiung Tzeng	2002	Fuzzy Sets and Systems	10.1016/S0165-0114(01)00047-1	econometrics;artificial intelligence;time series;mathematics;statistics	AI	6.442588125541805	-20.5219027701188	15620
1f8f8003718e5cdb61f2d0cfe976658e165ec323	genetic algorithms for finding optimal strategies for a student's game	mixed integer linear program;computational intelligence genetic algorithm optimal strategies entertaining student game beer run game optimization algorithm metaheuristics milp solver;game theory;computational intelligence;optimal method;ease of use;real world application;genetic algorithms air pollution computational intelligence optimization methods computational modeling dairy products information systems linear programming simulated annealing educational institutions;games;genetic algorithms entertainment game theory;linear program;genetic algorithm;genetic algorithms;optimal algorithm;entertainment;genetic algorithms computational intelligence games	"""Important advantages of genetic algorithms (GAs) are their ease of use, their wide applicability, and their good performance for a wide range of different problems. GAs are able to find good solutions for many problems even if the problem is complicated and its properties are not well known. In contrast, classical optimization approaches like linear programming or mixed integer linear programs (MILP) can only be applied to restricted types of problems as non-linearities of a problem that occur in many real-world applications can be modeled appropriately. This paper illustrates for an entertaining student game that GAs can easily be adapted to a problem where only limited knowledge about its properties and complexity are available and are able to solve the problem easily. Modeling the problem as a MILP and trying to solve it by using a standard MILP solver reveals that it is not solvable within reasonable time whereas GAs can solve it in a few seconds. The game studied is known to students as the so-called """"beer-run"""". There are different teams that have to walk a certain distance and to carry a case of beer. When reaching the goal all beer must have been consumed by the group and the winner of the game is the fastest team. The goal of optimization algorithms is to determine a strategy that minimizes the time necessary to reach the goal. This problem was chosen as it is not well studied and allows to demonstrate the advantages of using metaheuristics like GAs in comparison to standard optimization methods like MILP solvers for problems of unknown structure and complexity"""	complexity;decision problem;fastest;genetic algorithm;integer programming;linear programming;mathematical optimization;metaheuristic;norm (social);solver;usability	Thomas Butter;Franz Rothlauf;Jörn Grahl;Tobias Hildenbrand;Jens Arndt	2007	2007 IEEE Symposium on Computational Intelligence and Games	10.1109/CIG.2007.368119	game theory;mathematical optimization;simulation;genetic algorithm;computer science;artificial intelligence;machine learning;computational intelligence	AI	21.078832082843352	-3.4105538067446863	15642
5cc6685e5221a069da5dc2b2d4bc956469c99482	approximately optimal monitoring of plan preconditions	partially observed markov decision process;condition monitoring	Monitoring plan preconditions can allow for replanning when a precondition fails, generally far in advance of the point in the plan where the precondition is relevant. However, monitoring is generally costly, and some precondition failures have a very small impact on plan quality. We formulate a model for optimal precondition monitoring, using partially-observable Markov decisions processes, and describe methods for solving this model effectively, though approximately. Specifically, we show that the single-precondition monitoring problem is generally tractable, and the multiple-precondition monitoring policies can be effectively approximated using single-precondition solutions.	approximation algorithm;cobham's thesis;markov chain;observable;precondition	Craig Boutilier	2000			computer science;data mining;management science	AI	20.538651246949918	-15.03522836045355	15645
78e4d0fb4399519e88e393d0f961fdaf6ba37157	decision trees for computer go features		Monte-Carlo Tree Search (MCTS) is currently the dominant algorithm in Computer Go. MCTS is an asymmetric tree search technique employing stochastic simulations to evaluate leaves and guide the search. Using features to further guide MCTS is a powerful approach to improving performance. In Computer Go, these features are typically comprised of a number of hand-crafted heuristics and a collection of patterns, with weights for these features usually trained using data from high-level Go games. This paper investigates the feasibility of using decision trees to generate features for Computer Go. Our experiments show that while this approach exhibits potential, our initial prototype is not as powerful as using traditional pattern features.	algorithm;computer go;decision tree;experiment;heuristic (computer science);high- and low-level;monte carlo tree search;prototype;simulation	Francois van Niekerk;Steve Kroon	2013		10.1007/978-3-319-05428-5_4	computer science;simulation;machine learning;heuristics;decision tree;artificial intelligence;computer go	AI	19.029434413079105	-17.81546767123968	15662
d1eb0666c794c02838ee91ec61a61373671b875c	maintenance strategy optimization for complex power systems susceptible to maintenance delays and operational dynamics		Maintenance is a necessity for most multicomponent systems, but its benefits are often accompanied by considerable costs. However, with the appropriate number of maintenance teams and a sufficiently tuned maintenance strategy, optimal system performance is attainable. Given system complexities and operational uncertainties, identifying the optimal maintenance strategy is a challenge. A robust computational framework, therefore, is proposed to alleviate these difficulties. The framework is particularly suited to systems with uncertainties in the use of spares during maintenance interventions, and where these spares are characterized by delayed availability. It is provided with a series of generally applicable multistate models that adequately define component behavior under various maintenance strategies. System operation is reconstructed from these models using an efficient hybrid load-flow and event-driven Monte Carlo simulation. The simulation's novelty stems from its ability to intuitively implement complex strategies involving multiple contrasting maintenance regimes. This framework is used to identify the optimal maintenance strategies for a hydroelectric power plant and the IEEE-24 RTS. In each case, the sensitivity of the optimal solution to cost level variations is investigated via a procedure requiring a single reliability evaluation, thereby reducing the computational costs significantly. The results show the usefulness of the framework as a rational decision-support tool in the maintenance of multicomponent multistate systems.	computation;event-driven programming;ibm power systems;monte carlo method;optimal maintenance;program optimization;rationality;simulation;java-gnome	Hindolo George-Williams;Edoardo Patelli	2017	IEEE Transactions on Reliability	10.1109/TR.2017.2738447	electric power system;reliability engineering;novelty;monte carlo method;maintenance engineering;optimal maintenance;engineering	Embedded	12.323372538932373	-6.513978539138033	15709
c36f775c742beb47fdef2392a3b668308c9d0381	a review of possibilistic approaches to reliability analysis and optimization in engineering design	epistemic uncertainty;engineering design;design optimization;incomplete information;possibility theory;reliability analysis;design methodology	A variety of analysis strategies and design methodologies are widely applied to accommodate uncertainties in engineering design. Generally there exist two different types of uncertainties in practice, aleatory uncertainty and epistemic uncertainty. When data and information are very limited, the probabilistic methodology may not be appropriate. Among several alternative tools, possibility theory is proved to be a computationally efficient and stable tool to handle incomplete information. In this paper, we first introduce two issues concerned with possibilistic approaches: reliability analysis and design optimization. Then the type of uncertainties in these issues are explained with emphasis on the epistemic uncertainty. After that, this paper presents both theoretical development and computational improvement of possibility theory in recent years. More details are given to reveal the capability and characteristics of quantified uncertainty from different aspects. In the end, future research directions are summarized.	algorithmic efficiency;engineering design process;existential quantification;holism;mathematical optimization;optimizing compiler;possibility theory;probabilistic database;reliability engineering	Li-Ping He;Hong-Zhong Huang;Li Du;Xu-Dong Zhang;Qiang Miao	2007		10.1007/978-3-540-73111-5_118	probabilistic-based design optimization;possibility theory;uncertainty quantification;multidisciplinary design optimization;probabilistic design;uncertainty analysis;design methods;computer science;artificial intelligence;data mining;management science;complete information;engineering design process	SE	14.072644333579902	-6.668957612587912	15710
87f3aa1386f2ac23f290aa7ac0d5967563c4f7fc	the dual of generalized fuzzy subspaces		In fuzzy algebra, fuzzy subspaces are basic concepts. They had been introduced by Katsaras and Liu 1 in 1977 as a generalization of the usual notion of vector spaces. Since then, many results of fuzzy subspaces had been obtained in the literature 1–4 . Moreover, many researches in fuzzy algebra are closely related to fuzzy subspaces, such as fuzzy subalgebras of an associative algebra 5 , fuzzy Lie ideals of a Lie algebra 6 , fuzzy subcoalgebras of a coalgebra 7 . Hence fuzzy subspaces play an important role in fuzzy algebra. In 1996, Abdukhalikov 8 defined the dual of fuzzy subspaces as a generalization of the dual of kvector spaces. This notion was also studied and applied in many branches 2, 7–9 , especially in the fuzzy subcoalgebras 7 and fuzzy bialgebras 9 . After the introduction of fuzzy sets by Zadeh 10 , there are a number of generalizations of this fundamental concept. So it is natural to study algebraic structures connecting with them. In this paper, we aim our attention at the dual of vector space in intuitionistic fuzzy sets, interval-valued fuzzy sets, and interval-valued intuitionistic fuzzy sets for our further researches.		Wenjuan Chen;Yanyong Guan	2012	J. Applied Mathematics	10.1155/2012/932014	fuzzy classification	AI	-0.919355978478482	-23.633597135986562	15714
410d4da5f2e65da93499deedca43ac9d638db400	capacity optimization in feedforward brownian networks	least recently used caching;average case analysis;hierarchical caching;zipf s law;cache fault probability;web caching	Stochastic networks have played a fundamental role as canon ical models for a wide variety of multi-resource applications, i cluding numerous types of computer and communication systems, manu facturing systems, multi-item inventory systems, call cen ters, and workforce management systems. The complexity of such appli cations continue to grow at a rapid pace, which in turn increa ses the technical difficulties of obtaining stochastic network solutions in the analysis, modeling and optimization of these applica tions. Moreover, the consistent stream of emerging multi-resourc e application areas continue to further exacerbate this growth trend in complexity. One particularly important example is busines s performance management, which is a key emerging technology pos itioned to enable optimization of business process operatio ns and information technology infrastructure through stochasti c network models in order to achieve business performance targets. The primary difficulty in obtaining stochastic network solu tions in the analysis, modeling and optimization of multi-resour ce applications concerns the complex dependencies among the queues in the network. Indeed, it is only under relatively strong rest rictions that the stationary joint distribution for the network has a product form in terms of the stationary distribution for each queue i n isolation. Although the requirements for product-form soluti ons may not often hold in practice, the use of these results as approx imations, possibly together with other heuristics, is a freque ntly employed approach in computer capacity planning application s; refer to, e.g., [5]. However, the accuracy of this approach withou any bounds on the errors is an important concern from both a theor etical and practical perspective. Due to the difficulty of obtainin g analytic solutions for non-product-form stochastic networks, alte rnative approaches are often based on various forms of stochastic simu lat on methods. In particular, there is essentially exclusive use of simulation for the analysis, modeling and optimization of stoc hastic network models of business processes [4], with few exceptio ns. On the other hand, simulation can be computationally expensive, especially when used to solve optimization problems i nvolving multidimensional stochastic systems. Simulation-bas ed techniques have been widely studied to address such stochastic o ptimization problems in general; see, e.g., [6]. While these re c nt advances can significantly improve upon the computational cos ts and accuracy of solving stochastic optimization problems with naive use of simulation, we believe that much greater improvement s can be obtained by establishing and exploiting fundamental pro perties of the stochastic network and by extending and exploiting no nlinear optimization methods, together with the recent adva nces in stochastic simulation-based and numerical techniques. Of particular interest from both theoretical and practical perspectives are Brownian models of feedforward stochastic networ ks and non-linear optimization based on convex programming and tr ustregion methods. Under the latter iterative methods, the obj ctive function is approximated in a neighborhood of the current it erate (the so-called trust region) by a surrogate model that is eas ier to deal with than the objective function; see, e.g., [1]. The it era ive procedure moves to the next iterate within the trust region t hat optimizes the surrogate model, where both the trust region and surrogate model are updated as more information about the true obj ective function is obtained. We propose a general methodology for the optimization of fee dforward Brownian networks. Our approach is based on combini ng and extending approaches and results from various fields inc ludi g simulation optimization, trust-region methods, numerica l methods, and fundamental properties of feedforward Brownian networ ks. By exploiting and extending results from each of these areas, w e hope that our methodology will converge faster and yield better s olutions than existing approaches. In what follows, we present a summ ary of our general methodology and some of our ongoing research i n this area, and then consider a few numerical experiments tha t highlight structural properties of our approach.	analysis of algorithms;approximation algorithm;brownian motion;business process;capacity optimization;converge;convex optimization;experiment;feedforward neural network;heuristic (computer science);iteration;iterative method;linear programming;mathematical optimization;net-centric enterprise services;nonlinear programming;nonlinear system;numerical analysis;optimization problem;queueing theory;requirement;simulation;stationary process;stochastic optimization;stochastic process;surrogate model;trust region;ical	A. B. Dieker;Soumyadip Ghosh;Mark S. Squillante	2008	SIGMETRICS Performance Evaluation Review	10.1145/1453175.1453209	zipf's law;computer science;theoretical computer science;database;world wide web;statistics	ML	3.342451967216694	-1.3816508443810218	15740
60bbf06fcda481ab0e0ecfa32d931628294cb88a	searching for risk in large complex spaces		Safety analysts are starting to worry that large complex systems are becoming too difficult to analyze when part of the system is changed or placed under stress. Traditional safety analysis techniques may miss safety hazards or (more likely) some of the circumstances that can cause them. To help analysts discover hazards in complex systems, ASHiCS has created a proof-of-concept tool that uses evolutionary search and fast-time air traffic control (ATC) simulation to uncover airspace hazards. We use a fast-time ATC simulation (using RAMS Plus 1 ) of an en-route air sector containing multiple flight paths and aircraft types, and into this we inject a serious incident (cabin pressure loss) that requires one aircraft to make an emergency descent. We then use a near-neighbor random hillclimber to search for high-risk variants of that situation: we run a wide range of variants, select the subset of variants that caused the most risk, and then mutate the aircraft entry times to create a new set of situation variants that will hopefully have even greater risk. Weighted heuristics are able to focus on specific events, flight paths or aircraft so that the search can effectively target incidents of interest. Air traffic is generated by specifying the characteristics of each aircraft entering the sector, namely aircraft type, aircraft entry time, its entry and exit flight level and the waypoints specifying its flight path and any level changes. The traffic input files are created using genetic algorithms with restrictions on the distribution of aircraft to predetermined flight paths and an enforcement of wake turbulence separation. Once the input files have been created, a non-graphic version of RAMS Plus (i.e. a version that runs without any visualization to speed up simulations) is executed and the outputs analyzed by heuristics in the ASHiCS software. The solution space is extremely large and cannot be exhaustively searched for the worst case; this is a problem for safety analysts who need a context to the search results so that they can determine event probabilities. Our initial approach has been to conduct a sensitivity analysis to try and discover more about the average fitness of the population during the evolutionary search. This provides some insight to the nature of the solution space, in terms of the frequency of other high risk scenarios and how sensitive such solutions are to mutation of their input configuration. Our initial results suggest that for very large solutions spaces, where high scoring solutions are relatively rare, the range of the mutation operator (i.e. the degree to which the mutation operator can change the original) has a significant effect on the average fitness of the population. From our experiments, mutation operators with large ranges that permit radical changes to the genotype perform significantly worse than operators with small ranges that permit gradual changes.	advanced transportation controller;best, worst and average case;complex systems;experiment;feasible region;genetic algorithm;hazard (computer architecture);heuristic (computer science);rams;simulation;spaces;turbulence	Kester Clegg;Robert Alexander	2014		10.1007/978-3-662-45523-4_61	operator (computer programming);data mining;air traffic control;heuristics;complex system;risk assessment;computer science	SE	12.909868869432282	-8.907317535859077	15748
4e2d5e7bb5693b52a4164f719131698ec4d7cb3a	bounded optimization of resource allocation among multiple agents using an organizational decision model	dynamic change;decision models;multi agent system;conflict;resource allocation;decision mode;garbage can model gcm;optimization problem;simulation experiment;bounded optimization;optimality criteria;global optimization;multi agent system mas;problem solving;distributed management	Multi-agent System (MAS) can be used to dispose bounded optimization problems with dynamically changing resources because its autonomous distributed management model aspect is fitted to dealing with such external disturbances. One of the problems in multi-agent optimizations is that it is difficult to rigorously define the optimization criteria with respect to the global optimization in advance. Rather, it may depend much on more situated factors such as temporal availability of resources and coexistence of current conflicts, conflicts among what has been already scheduled and what is to be scheduled. In this paper, an organizational model called Garbage Can Model (GCM) is introduced. In GCM, through its three decision-making strategies and the fluidities of problems and resources, solutions made by an individual agent are concerned with several agents that are co-existing in the environment. The problems allocated to each agent are solved not only by an agent's own efforts, but also by the change of problem solving status of other agents. Our simulation experiment shows that GCM is a preferred framework for multi-agent optimization problems in dealing with the above difficulties.	mathematical optimization	Qiang Wei;Tetsuo Sawaragi;Yajie Tian	2005	Advanced Engineering Informatics	10.1016/j.aei.2005.01.001	optimization problem;mathematical optimization;decision model;simulation;resource allocation;computer science;multi-objective optimization;management science;global optimization	AI	13.942193317449616	-3.2964462400129424	15753
8eef5ceae4c87e425a436b20b9f2d547fc2b2bc0	stochastic fuzzy multi-objective backbone selection and capacity allocation problem under tax-band pricing policy with different fuzzy operators		In this paper, we investigate a multi-objective optimization problem that a telecom bandwidth broker (BB) faces when acquiring and selling bandwidth in an uncertain market environment in which there exists several backbone providers (BPs) and end users. The proposed model incorporates two important goals: maximizing expected profit and minimizing expected loss capacity within realistic constraints such as BPsu0027 capacity, meeting the end usersu0027 bandwidth requests and satisfying the Quality of Service requests of end usersu0027, considering stochastic capacity loss rates of BPs. The fuzzy set theory and stochastic programming techniques are employed to handle the non-deterministic nature of telecommunication network setting due to the presence of vagueness and randomness of information. The model is formulated in such a way that it simultaneously considers the randomness in demand and determines the allocation of end usersu0027 bandwidth requests into purchased capacity based on tax-band pricing scheme. As solution strategies, two different fuzzy operators, namely max---min and weighted additive max---min, are integrated into a resulting two-stage multi-objective stochastic linear programming model. Then, algorithms are provided to solve and to compare methodologies with deterministic approaches. Finally, the proposed algorithms are tested on several randomly generated test scenarios to provide managerial insight to decision makers of BB companies.	internet backbone	Hasan Hüseyin Turan	2017	Soft Comput.	10.1007/s00500-016-2057-6	mathematical optimization;artificial intelligence;fuzzy logic;computer science;machine learning;quality of service;end user;randomness;fuzzy set;stochastic programming;bandwidth broker;optimization problem	ECom	4.734993184950948	-5.324250273149723	15755
2d74a60cede9277cb2b5ef94172ab8dc100035bc	simple auctions for supply contracts	optimal procurement mechanism;competing suppliers;grupo de excelencia;administracion de empresas;newsvendor;economia y empresa;open descending auction;grupo a	This paper studies an optimal procurement mechanism for a newsvendor-like problem where the buyer's (newsvendor's) purchase price of the supplies is not fixed, but determined through interaction with candidate suppliers. The buyer has priors on the suppliers' costs but does not know their costs exactly. Recent literature has shown how the buyer can implement the optimal procurement mechanism by announcing a revenue function (specifying a payment for each quantity the buyer may purchase), then auctioning off the supply contract with the specified revenue function. In this paper, we show that a simple modified version of the standard open-descending auction for a fixed quantity is also an optimal mechanism for obtaining supplies. What distinguishes this mechanism is its simplicity and familiarity for the suppliers---open-descending auctions are very easy to run and ubiquitous in practice, whereas auctioning supply contracts with a specified revenue function is much less observed and more difficult to explain to suppliers. Furthermore, we show that this simple mechanism can be easily generalized to ex ante asymmetric suppliers and a class of nonlinear production costs. This paper was accepted by Yossi Aviv, operations management.		Izak Duenyas;Bin Hu;Damian R. Beil	2013	Management Science	10.1287/mnsc.1120.1705	newsvendor model;economics;marketing;operations management;microeconomics;commerce	ECom	-0.740531820620396	-6.1149102499605945	15790
43bed40c33fc26a85d5c868980ba05b9b7bb69e9	a re-evaluation of the over-searching phenomenon in inductive rule learning.	search strategy;spectrum;rule learning;hill climbing;exhaustive search	Phenomenon of over-searching is well known [3] but not shown for most of the rule learning heuristics In [3] only one heuristic was used and no true Exhaustive Search was employed (approximation with a beam of 512) we extend their work to 9 different heuristics (some of them were tuned in [1,2]) and a true exhaustive search we want to answer the question whether Separate-and-conquer algorithms can improve from more extensive search	algorithm;approximation;brute-force search;heuristic (computer science)	Frederik Janssen;Johannes Fürnkranz	2008		10.1137/1.9781611972795.29	beam search;artificial intelligence;machine learning;mathematics;incremental heuristic search;iterative deepening depth-first search;best-first search;combinatorial search;algorithm;search algorithm	AI	20.432586225130734	-11.600075068723992	15796
307e1dc7db5bf168c77be201ec45863c021c8f8f	an intelligent autopilot system that learns flight emergency procedures by imitating human pilots	databases;circuit faults;data collection;training;artificial neural networks;aerospace control;fires	We propose an extension to the capabilities of the Intelligent Autopilot System (IAS) from our previous work, to be able to learn handling emergencies by observing and imitating human pilots. The IAS is a potential solution to the current problem of Automatic Flight Control Systems of being unable to handle flight uncertainties, and the need to construct control models manually. A robust Learning by Imitation approach is proposed which uses human pilots to demonstrate the task to be learned in a flight simulator while training datasets are captured from these demonstrations. The datasets are then used by Artificial Neural Networks to generate control models automatically. The control models imitate the skills of the human pilot when handling flight emergencies including engine(s) failure or fire, Rejected Take Off (RTO), and emergency landing, while a flight manager program decides which ANNs to be fired given the current condition. Experiments show that, even after being presented with limited examples, the IAS is able to handle such flight emergencies with high accuracy.	artificial neural network;autopilot;control system;experiment;flight simulator;international automated systems;internet authentication service;neural networks;recovery time objective;simulation	Haitham Baomar;Peter J. Bentley	2016	2016 IEEE Symposium Series on Computational Intelligence (SSCI)	10.1109/SSCI.2016.7849881	control engineering;simulation;engineering	AI	15.758009133086453	-16.875855970878984	15896
9586d6367063d47acf3ba1b22ea865c579f5a22c	maintaing quality, poductivity and profit in a changing bell system	profitability			Luis T. Burke	1982			computer science;real-time computing;operations management;profitability index	EDA	2.307203499540195	-7.348897069911427	15941
469b0928945c4df98203b19648feab04585d9f36	evolving cbr and data segmentation by som for flow time prediction in semiconductor manufacturing factory	case base reasoning;production process;due date assignment;hybrid approach;flow time prediction;self organizing map;genetic algorithm;genetic algorithms;self organized map;case based reasoning;simulation model;semiconductor manufacturing	Flow time of semiconductor manufacturing factory is highly related to the shop floor status; however, the processes are highly complicated and involve more than 100 production steps. Therefore, a simulation model with the production process of a real wafer fab located in Hsin-Chu Science-based Park of Taiwan is built for further studying of the relationship between the flow time and the various input variables. In this research, a hybrid approach by combining Self-Organizing Map (SOM) and Case-Based Reasoning (CBR) for flow time prediction in semiconductor manufacturing factory is developed. And Genetic Algorithm (GA) is applied to fine-tune the weights of features in the CBR model. The flow time and related shop floor status are collected and fed into the SOM for clustering. Then, a corresponding SGA-CBR method is selected and applied for flow time prediction. Finally, using the simulated data, the effectiveness of the proposed method (SGA-CBR) is shown by comparing with other approaches.	case-based reasoning;semiconductor device fabrication	Pei-Chann Chang;Chin-Yuan Fan;Yen-Wen Wang	2009	J. Intelligent Manufacturing	10.1007/s10845-008-0116-2	simulation;genetic algorithm;computer science;engineering;artificial intelligence;machine learning	Robotics	12.955977578051266	-18.497176515582222	15964
2e3fc49139bfb597f5d2d4fcd1ef279cdb6efb5e	chance constrained project scheduling under risk	optimisation;stochastic optimization p timed petri net initial time execution risk management project scheduling stochastic variable;project management;risk management;uncertain variable;scheduling petri nets risk management project management stochastic processes optimisation;optimization problem;stochastic optimization;time petri net;scheduling risk management stochastic processes constraint optimization disaster management project management petri nets costs contingency management programmable control;stochastic processes;scheduling;project scheduling;petri nets;chance constraint	This study uses a p-timed Petri net to represent the initial scheduling of a project: places are tasks and each place has assigned a initial cost and a initial time execution. Risk management is applied to project scheduling in order to identify and mitigate risks where uncertain variables are modelled as stochastic variables. The algorithm determines the set of mitigation actions that reduce the risk exposure stated in the chance constraints of the proposed optimization problem. This work shows how this problem can be modelled as a stochastic optimization problem requiring that constraints should be held with a probability exceeding /spl alpha/.	algorithm;mathematical optimization;optimization problem;petri net;risk management;scheduling (computing);stochastic optimization	Ascensión Zafra-Cabeza;Miguel A. Ridao;Eduardo F. Camacho	2004	2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No.04CH37583)	10.1109/ICSMC.2004.1399903	project management;nurse scheduling problem;optimization problem;real-time computing;risk management;dynamic priority scheduling;computer science;stochastic optimization;distributed computing;scheduling;schedule;petri net	Robotics	9.022465894940549	-0.6148905881815487	16010
1a40cdbf4628ee719a8301dfe1df142697cdbd81	fractional moments on bandit problems		Reinforcement learning addresses the dilemma between exploration to find profitable actions and exploitation to act according to the best observations already made. Bandit problems are one such class of problems in stateless environments that represent this explore/exploit situation. We propose a learning algorithm for bandit problems based on fractional expectation of rewards acquired. The algorithm is theoretically shown to converge on an -optimal arm and achieve O(n) sample complexity. Experimental results show the algorithm incurs substantially lower regrets than parameter-optimized -greedy and SoftMax approaches and other low sample complexity state-of-the-art techniques.	algorithm;converge;reinforcement learning;sample complexity;softmax function;stateless protocol	B. AnandaNarayanan;Balaraman Ravindran	2011			mathematical optimization;artificial intelligence;machine learning;mathematics	ML	22.842398717693126	-17.738010056520338	16029
30f26f01bf8daf9f63bff2a12c0c0b3a636ca7a0	using computer simulation to compare tool delivery systems in an fmc	control system analysis computing;digital simulation;flexible manufacturing systems;machine tools;scheduling;computer simulation;control strategies;efficiency;flexible manufacturing cell;job scheduling strategies;output screens;parts flow;shop floor operations;system adequacy;tool delivery systems;tool transfer strategies	In the implementation of a flexible manufacturing cell (FMC), it is uncertain whether the proposed tool delivery system will be able to perform adequately. In order to determine the system's adequ.acy, along with the resulting efficiency of shop floor operations, a s~mulation has been performed. Tool transfer and job scheduling strategies were developed to aid in improving the flow of parts and tools on the shop floor. In addition, a series of output screens were developed for use during the simulation runs to verify the simulation and control strategies being used. 1 . I N T R O D U C T I O N For automated manufacturing to be efficient, well designed system layouts, cell controllers and material handling systems are required to facilitate the flow of work pieces and associated manufacturing resources on the production floor. Detailed knowledge of the manufacturing process must be ascertained, and the strategies used by the cell controller to coordinate the movement of parts and tools must be well established and proven to work. The separate components of the system must individually work in an effective manner and, when operating together, must be capable of coordinated interaction. One of the best methods for analyzing manufacturing systems and evaluating proposed system layouts is computer simulation. This paper describes the use of simulation to model the complex manufacturing operations and controlling algorithms of an FMC. A fully automated FMC is in the process of being installed. When fully operational, the FMC is expected to process a large variety of parts, whose machining programs will each require a large number of different tools. In addition, the parts to be machined will be hardened steel, causing tool lives to be as short as 5 minutes. Therefore, a large number of tool transfers are foreseen and it is of extreme importance that a tool delivery system be installed which is able to keep the machines supplied with the essential tools. 2 . D E S C R I P T I O N O F T H E F M C The FMC being investigated is shown in Figure 1. It consists of seven CNC milling machines arranged in two rows and separated by a central tool delivery system. The rows of three and four machines will be referred to as the front and back machines respec t ive ly . Parts, mounted on fixture units, will be delivered to the front machines via a linear transporter. In the event that the machine required for processing is busy processing another job, fixtures of parts will await processing in a queueing area opposite the linear transporter from the three machines. Fixtures of parts to be processed on one of the back machines will be manually delivered to each machine. Each of these back machines will have a local queue for parts awaiting processing. Tools, the main focus of this investigation, will be assembled in a remote tool room upon being ordered by a cell controller. Once assembled, multiple tools will be placed on a rack which will be manually brought to the shop floor. The tools will then be 191aced in two rotating floor carousels, each capable of holding 140 tools. They will be stored there until they are required at a machine for processing. Additional tools will be stored at each of the machines in machine tool magazines. These tool magazines will have a capacity of either 50 or 68 tools. 3 . A L T E R N A T I V E T O O L D E L I V E R Y O P T I O N S Three tool delivery options are being considered for installation in the FMC. The first system uses an overhead monorail robot to transfer tools between the machines and floor carousels. This monorail robot, if installed, will travel on a single-rail loop as shown in Figure 1. The other alternatives differ from this system by having either 1) two monorail robots which would travel on the same monorail loop shown, or 2) a gantry-mounted rrobot which would travel on two parallel rails and be capable of both lateral and longitudinal movement. An additional variation to the system, a single floor tool carousel, was also modeled. Its capacity was subsequently varied and the effects of this change investigated. 4 . M O D E L I N G A P P R O A C H The simulation was performed on a PC compatible computer using the all-purpose simulation language SIMAN. Most of the logic used to control the cell is located in FORTRAN event subroutines. A system aspect contributing to its uniqueness is the large number of tools being tracked at any given moment. The considerable storage requirements forced the simulation to be conducted within the OS/2 system operating environment. The simulation model incorporates a level of detail that extends well beyond the tool delivery system. It was felt that simply modeling each of the proposed tool delivery systems would aid only in comparing the relative efficiencies of the different systems. More importantly, it was desired to know whether the chosen tool delivery system would be able to adequately supply the system of machines being installed. Without the detailed modeling of the system as a whole, this result could not be realized. Thus, by using as much information as possible about the jobs, tools and machines to be used, an accurate assessment of the system's behavior was found. Using this information, cell control logic was developed to assure that the entire system performs in a way to expedite part manufacturing. 5 . T O O L D E L I V E R Y S Y S T E M As previously mentioned, the focus of this study was to assess the capabilities of the proposed tool delivery (TD) systems. In order to best assess the capabilities of the TD systems, control logic has been developed to best adjust for situations anticipated in the FMC. As different situations were encountered in the simulation, control 'strategies were progressively developed to perform a number of functions. In particular, the effects of two types of control strategies were investigated: tool transfer and job scheduling. Tool transfer logic was developed for immediate control over the tool delivery system, and job scheduling strategies were selected to help reduce the amount of tool transfer within the cell.	algorithm;cell (microprocessor);computer simulation;dropbox carousel;earthbound;flight management system;fortran;fundamental modeling concepts;ibm pc compatible;job scheduler;job stream;lateral thinking;level of detail;material handling;os/2;operating environment;overhead (computing);requirement;robot;scheduling (computing);simulation language;subroutine;test fixture	Eric P. Hedlund;Wayne J. Davis;Peter L. Webster	1990			computer simulation;job shop scheduling;parallel manipulator;simulation;computer science;engineering;technical report;job scheduler;machine tool;automatic control;assembly;efficiency;scheduling;manufacturing engineering	Robotics	10.608415737982947	4.161371662508382	16069
2df28e4654a6ecbb74482a6749d41d25ad99bd2d	a decision support approach to automatic timetabling in higher education institutions	automatic timetabling;higher education;timetables;decision support system;heuristics;combinatorial optimization	At a time when the need to reduce costs has become part of the day-to-day reality of all educational institutions, it is unthinkable to continue to manually perform those tasks (i.e., the creation of timetables) that can be automated and optimized. The automatic creation of timetables for educational institutions is one of the most studied problems by the scientific community. However, almost all studies have been based on very simplified models of reality that have no practical application. A realistic model of the problem, robust algorithms that are able to find valid solutions in highly restricted environments, and optimization methods that are able to quickly provide quality results are key factors to consider when attempting to solve this (real) problem faced by educational institutions. This paper presents a summary of the work performed by Bullet Solutions over the last few years, from the first stage of understanding and modelling the problem to the final analysis of the results obtained using the developed software under real conditions.	decision support system;timeline	Pedro Fernandes;Carla Sofia Pereira;Armando Barbosa	2016	J. Scheduling	10.1007/s10951-015-0435-z	mathematical optimization;simulation;decision support system;combinatorial optimization;computer science;artificial intelligence;heuristics;management science;higher education;algorithm	HPC	14.788047274834515	-0.30404596299193154	16078
19aaa8cfd3cb4b2e8e4c54e05a81caff7fb34121	a constraint programming heuristic for a heterogeneous vehicle routing problem with split deliveries	benchmark problem;exact algorithm;constraint programming	This article considers fresh goods distribution of a retail chain store in Turkey. The problem is formulated as a vehicle routing problem with a heterogeneous fleet for which no exact algorithm has ever been designed to solve it. A fast and effective algorithm based on constraint programming is proposed for the solution. The procedure is tested on some of the benchmark problems in literature. The real-life case is first solved assuming that delivery of a customer cannot be split between vehicles. Then it is resolved considering split deliveries. Solutions of both strategies are compared with the current performance of the firm to determine a distribution strategy. Results indicate considerable improvement in the performance of the firm.	constraint programming;heuristic;vehicle routing problem	Pinar Mizrak Ozfirat;Irem Ozkarahan	2010	Applied Artificial Intelligence	10.1080/08839511003715196	mathematical optimization;constraint programming;simulation;constraint satisfaction;computer science	AI	15.799641734545474	1.9600904162853576	16106
0d19300a4d8a6ce91c348a9f82e62e88978c81f4	extending equilibrium markets	electronic commerce;electricity supply industry economic cybernetics combinatorial mathematics electronic commerce;electronic markets;electronic business confast equilibrium markets distributed information discontinuous supply and demand energy markets production consumption fast moving electronic markets market mechanism convex hulls final adjustment;automated markets;electricity supply industry;costs production power generation economics supply and demand power generation power control bandwidth transportation power markets computer networks;convex hull;combinatorial mathematics;economic cybernetics;energy markets;equilibrium markets;supply and demand	Confast (CONvex hull with Final AdjuSTment), which is a mechanism for equilibrium markets, handles large markets with distributed information and discontinuous supply and demand. Energy markets present a particularly appropriate application for this new mechanism, which can handle production and consumption and is suitable for fast-moving electronic markets.		Per Carlsson;Fredrik Ygge;Arne Andersson	2001	IEEE Intelligent Systems	10.1109/5254.941354	e-commerce;variable pricing;computer science;market system;convex hull;supply and demand;microeconomics	Robotics	1.7020214188097857	2.1982623467276516	16199
5b8381ee45a0302844139c9a3fec5b66d0c7aa17	a stochastic programming approach to cash management in banking	stochastic programming or in banking integer stochastic programming;carte credit;loi discrete;automatic teller machine;modelizacion;discrete distribution;fixed cost;ley discreta;optimal solution;compensacion;banking;tratamiento transaccion;closed form solution;programacion entera;costo fijo;gestion encaisse;tarjeta de credito;systeme aide decision;guichet automatique;cout fixe;temps lineaire;exact solution;programmation stochastique;secteur bancaire;solucion exacta;sistema ayuda decision;probabilistic approach;tiempo lineal;programmation en nombres entiers;integer stochastic programming;modelisation;systeme incertain;decision support system;compensation;programacion mixta entera;integer programming;cash management;enfoque probabilista;approche probabiliste;probability distribution;linear time;court terme;credit card;programmation partiellement en nombres entiers;mixed integer programming;stochastic model;solution exacte;transaction processing;stochastic programming;sistema incierto;modeling;modelo estocastico;uncertain system;programacion estocastica;modele stochastique;automatic teller machines;corto plazo;short term;or in banking;traitement transaction;credit cards	The treasurer of a bank is responsible for the cash management of several banking activities. In this work we focus on two of them: cash management in automatic teller machines (ATMs), and in the compensation of credit card transactions. In both cases a decision must be taken according to a future customers demand, which is uncertain. From historical data we can obtain a discrete probability distribution of this demand, which allows the application of stochastic programming techniques. We present stochastic programming models for each problem. Two short-term and one mid-term models are presented for ATMs. The short-term model with fixed costs results in an integer problem which is solved by a fast (i.e. linear running time) algorithm. The short-term model with fixed and staircase costs is solved through its MILP equivalent deterministic formulation. The mid-term model with fixed and staircase costs gives rise to a multistage stochastic problem, which is also solved by its MILP deterministic equivalent. The model for compensation of credit card transactions results in a closed form solution. The optimal solutions of those models are the best decisions to be taken by the bank, and provide the basis for a decision support system.	atm turbo;algorithm;decision support system;money;multistage amplifier;np-equivalent;nonlinear system;piecewise linear continuation;real-time clock;simulation;stochastic programming;time complexity	Jordi Castro	2009	European Journal of Operational Research	10.1016/j.ejor.2007.10.015	probability distribution;mathematical optimization;integer programming;computer science;operations management;mathematics;mathematical economics;algorithm;statistics	ML	5.38023755938078	-2.785087875020774	16235
5a5fac07cac687764dabe6c56b65b53c53cbf40f	calculating scale elasticity in dea models	elasticite;economie;forecasting;economia;elasticity;production function;mesure efficacite farrel;analisis envolvimiento datos;piecewise linear;reliability;scale elasticity;rendement echelle;project management;information systems;theorie production;piecewise linear techniques;maintenance;methode echelle multiple;elasticidad;scaling law;soft or;information technology;farrell efficiency measures;packing;metodo escala multiple;dea;operations research;location;theorie economique;return to scale;investment;journal;journal of the operational research society;technique lineaire par morceau;inventory;purchasing;economic theory;history of or;logistics;data envelopment analysis;ley escala;marketing;scheduling;information value;production;communications technology;teoria economica;multiscale method;economy;computer science;operational research;loi echelle;efficiency measurement;valor informacion;valeur information;teoria produccion;production theory;applications of operational research;or society;jors;management science;infrastructure;analyse enveloppement donnee;returns to scale	In the data envelopment analysis (DEA) efficiency literature, qualitative characterizations of returns to scale (increasing, constant, or decreasing) are most common. In economics it is standard to use the scale elasticity as a quantification of scale properties for a production function representing efficient operations. Our contributions are to review DEA practices, apply the concept of scale elasticity from economic multi-output production theory to DEA piecewise linear frontier production functions, and develop formulas for scale elasticity for radial projections of inefficient observations in the relative interior of fully dimensional facets. The formulas are applied to both constructed and real data and show the differences between scale elasticities for the two valid projections (input and output orientations). Instead of getting qualitative measures of returns to scale only as was done earlier in the DEA literature, we now get a quantitative range of scale elasticity values providing more information to policy-makers.	elasticity (data store)	Finn R. Førsund;L. Hjalmarsson	2004	JORS	10.1057/palgrave.jors.2601741	returns to scale;project management;economics;marketing;operations management;data envelopment analysis;mathematical economics;management;operations research;information technology	Vision	0.08890440533040463	-12.747997845593657	16241
ec66e4d87eed20d80c5ad38cf5b0dc2c6d04cf78	selecting a meta-heuristic technique for smart micro-grid optimization problem: a comprehensive analysis		In current epoch, the economic operation of micro-grid under soaring renewable energy integration has become a major concern in the smart grid environment. There are several meta-heuristic optimization techniques available under different categories in literature. One of the most difficult tasks in cost minimization of micro-grid is to select the best suitable optimization technique. To resolve the problem of selecting a suitable optimization technique, a rigorous review of six meta-heuristic algorithms (Whale Optimization, Fire Fly, Particle Swarm Optimization, Differential Evaluation, Genetic Algorithm, and Teaching Learning-based Optimization) selected from three categories (Swarm Intelligence, Evolutionary Algorithms, and Teaching Learning) is conducted. It presents, a comparative analysis using different performance indicators for standard benchmark functions and proposed a smart micro-grid (SMG) operation cost minimization problem. A proposed SMG is modeled which incorporates utility connected power resources, e.g., wind turbine, photovoltaic, fuel cell, micro-turbine, battery storage, electric vehicle technology, and diesel power generator. The proposed work will help researchers and engineers to select an appropriate optimization method to solve micro-grid optimization problems with constraints. This paper concludes with a detailed review of micro-grid operation cost minimization techniques based on an exhaustive survey and implementation.	benchmark (computing);diesel;evolutionary algorithm;genetic algorithm;heuristic (computer science);mathematical optimization;metaheuristic;optimization problem;optimizing compiler;particle swarm optimization;qualitative comparative analysis;swarm intelligence	Baseem Khan;Pawan Singh	2017	IEEE Access	10.1109/ACCESS.2017.2728683	engineering optimization;multi-swarm optimization;probabilistic-based design optimization;meta-optimization;computer science;metaheuristic;mathematical optimization;multi-objective optimization;test functions for optimization;parallel metaheuristic	EDA	18.590996900082146	-3.56194341073099	16262
57785266264ac706dd5f489cc14fddbf70900f9b	biased respondent group selection under limited budget for minority opinion survey		This paper discusses a new approach to use the information from a special social network with high homophily to select a survey respondent group under a limited budget such that the result of the survey is biased to the minority opinions. This approach has a wide range of potential applications, e.g. collecting complaints from the customers of a new product while most of them are satisfied. We formally define the problem of computing such group with better utilization as the pbiased-representative selection problem (p-BRSP). This problem has two separate objectives and is difficult to deal with. Thus, we also propose a new unified-objective which is a function of the two optimization objectives. Most importantly, we introduce two polynomial time heuristic algorithms for the problem, where each of which has an approximation ratio with respect to each of the objectives.	approximation algorithm;heuristic;mathematical optimization;polynomial;selection algorithm;social network;time complexity	Donghyun Kim;Wei Wang;Matthew Tetteh;Jun Liang;Soyoon Park;Wonjun Lee	2015		10.1007/978-3-319-21786-4_16	public relations;political science;social psychology;welfare economics	DB	-1.068239665719586	-0.1360275459402791	16289
f4b624d7f4d8571e0e9dbec9e52d4fbe615e572c	micro to macro models for income distribution in the absence and in the presence of tax evasion	tax evasion;income distribution;taxation and redistribution models	We investigate the effect of tax evasion on the income distribution and the inequality index of a society through a kinetic model described by a set of nonlinear ordinary differential equations. The model allows to compute the global outcome of binary and multiple microscopic interactions between individuals. When evasion occurs, both individuals involved in a binary interaction take advantage of it, while the rest of the society is deprived of a part of the planned redistribution. In general, the effect of evasion on the income distribution is to decrease the population of the middle classes and increase that of the poor and rich classes. We study the dependence of the Gini index on several parameters (mainly taxation rates and evasion rates), also in the case when the evasion rate increases proportionally to a taxation rate which is perceived by citizens as unfair. Finally, we evaluate the relative probability of class advancement of individuals due to direct interactions and welfare provisions, and some typical temporal rates of convergence of the income distribution to its equilibrium state. 2014 Elsevier Inc. All rights reserved.	bitwise operation;evasion (network security);interaction;nonlinear system;social inequality	Maria Letizia Bertotti;Giovanni Modanese	2014	Applied Mathematics and Computation	10.1016/j.amc.2014.07.055	income distribution	ECom	-4.533807039805026	-9.877499288093878	16302
5463ce2c2dac766c39dea3cbd4df56ad23e69e9d	pareto optimal solution analysis of convex multi-objective programming problem	m pareto optimal solution;weight;convex multi objective;pareto optimal solution	The main method of solving multi-objective programming is changing multi-objective programming problem into single objective programming problem, and then get Pareto optimal solution. Conversely, whether all Pareto optimal solutions can be obtained through appropriate method, generally the answer is negative. In this paper, the methods of norm ideal point and membership function are used to solve the multi-objective programming problem. In norm ideal point method, norm and ideal point are given to structure the corresponding single objective programming problem. Then prove that for any Pareto optimal solution there exist weights such that Pareto optimal solution is the optimal solution of the corresponding single objective programming problem. In membership function method, firstly construct membership function for every objective function, then establish the single objective programming problem, after then solve the single objective programming problem, finally prove that for any Pareto optimal solution there exist weights such that the Pareto optimal solution is the optimal solution of the corresponding single objective programming problem. At last, two examples are given to illustrate that the two methods are effective in getting Pareto optimal solution.	existential quantification;loss function;optimization problem;pareto efficiency	Li Guo Zhang;Hua Zuo	2013	JNW	10.4304/jnw.8.2.437-444	pareto analysis;mathematical optimization;basic solution;optimal substructure;weight	AI	-0.8731512576709459	-17.785065720729893	16440
7565c439c9b94ff48b537940ccf280726413cf88	adaptive structure metrics for automated feedback provision in intelligent tutoring systems	metric learning;algebraic dynamic programming;intelligent tutoring systems;learning vector quantization;sequential data	Typical intelligent tutoring systems rely on detailed domain-knowledge which is hard to obtain and difficult to encode. As a data-driven alternative to explicit domain-knowledge, one can present learners with feedback based on similar existing solutions from a set of stored examples. At the heart of such a data-driven approach is the notion of similarity. We present a general-purpose framework to construct structure metrics on sequential data and to adapt those metrics using machine learning techniques. We demonstrate that metric adaptation improves the classification of wrong versus correct learner attempts in a simulated data set from sports training, and the classification of the underlying learner strategy in a real Java programming dataset.	algorithm;approximation;autonomous robot;cognitive science;dynamic programming;encode;general-purpose modeling;gradient descent;java;large margin nearest neighbor;learning vector quantization;loss function;machine learning;mathematical optimization;nonlinear programming;nonlinear system;sequence alignment;sorting;usb on-the-go	Benjamin Paaßen;Bassam Mokbel;Barbara Hammer	2016	Neurocomputing	10.1016/j.neucom.2015.12.108	learning vector quantization;computer science;theoretical computer science;machine learning;data mining	ML	21.82704868534421	-22.531764626399085	16448
5ab61fdd00e5d395bf405fefe0947e469ab6546f	sharing rules in teams	limited liability;piecewise linear;moral hazard;production process;noncooperative games;journal of economic literature;sharing rule	Abstract   We examine the problem of output sharing in a moral hazard in team situation. Although we do not consider any particular procedure, we assume that the team uses some procedure to decide a sharing rule before actual production takes place (for example, this may be a bargaining process or a team welfare maximization problem). This must take into account that the team will play a noncooperative game in the production process conditional on the chosen sharing rule. We show that the procedure for deciding the sharing rule does not have to look for anything more complicated than simple linear sharing rules. We also show that, when there is limited liability, the procedure needs to consider only the slightly more complicated piecewise linear rules. As a consequence of the linear sharing rule result, we are also able to provide a characterization of implementable outcomes.  Journal of Economic Literature  Classification Numbers: D82, D2, C72, J54.		Shasikanta Nandeibam	2002	J. Economic Theory	10.1006/jeth.2001.2953	limited liability;actuarial science;economics;piecewise linear function;operations management;scheduling;microeconomics;mathematical economics;welfare economics	ECom	-3.7377449505215465	-3.6891244874922315	16463
3ca85fe0f376258c4baa5d83e517bcbd3777ce72	an optimal contact model for maximizing online panel response rates	modelizacion;online survey panels;optimal solution;horizon roulant;optimisation;online survey;market research;metodo monte carlo;horizonte rodante;decision tree;optimizacion;estrategia optima;customer relationship management;migration;algorithme glouton;predictive modeling;heuristic method;comercializacion;field test;methode monte carlo;metodo heuristico;ecuesta estadistica;arbol decision;demografia;horizonte finito;rolling horizon;commercialisation;modelisation;optimal strategy;sample survey;programacion lineal;horizon fini;marketing;monte carlo method;optimal contact strategy;linear programming;survey response rates;gestion relation client;programmation lineaire;greedy algorithm;linear program;finite horizon;algoritmo gloton;optimization;prediction model;methode heuristique;response rate;monte carlo simulation;demography;modeling;arbre decision;strategie optimale;sondage statistique;optimization model;migracion;demographie	We develop and test an optimization model for maximizing response rates for online marketing research survey panels. The model consists of (1) a decision tree predictive model that classifies panelists into “states” and forecasts the response rate for panelists in each state and (2) a linear program that specifies how many panelists should be solicited from each state to maximize response rate. The model is forward looking in that it optimizes over a finite horizon during which S studies are to be fielded. It takes into account the desired number of responses for each study, the likely migration pattern of panelists between states as they are invited and respond or do not respond, as well as demographic requirements. The model is implemented using a rolling horizon whereby the optimal solution for S successive studies is derived and implemented for the first study. Then, as results are observed, an optimal solution is derived for the next S studies, and the solution is implemented for the first of these studies, etc. The procedure is field tested and shown to increase response rates significantly compared to the heuristic currently being used by panel management. Further analysis suggests that the improvement was due to the predictive model and that a “greedy algorithm” would have done equally well in the field test. However, further Monte Carlo simulations suggest circumstances under which the model would outperform the greedy algorithm.	benchmark (computing);black box;computer simulation;decision tree;greedy algorithm;heuristic;linear programming;mathematical optimization;monte carlo method;predictive modelling	Scott A. Neslin;Thomas P. Novak;Kenneth R. Baker;Donna L. Hoffman	2009	Management Science	10.1287/mnsc.1080.0969	mathematical optimization;simulation;linear programming;operations management;mathematics;operations research;statistics;monte carlo method	AI	5.953071340465139	-3.8444805498409336	16601
9067fe0ba7a3866b0e19c59570167d5ccd6205d9	trade-off between inventory and repair capacity in spare part networks	forecasting;reliability;project management;information systems;queuing system;maintenance;soft or;information technology;packing;operations research;location;relative prices;investment;journal;journal of the operational research society;inventory;purchasing;history of or;logistics;computer experiment;marketing;scheduling;production;communications technology;computer science;operational research;spare parts;queuing;applications of operational research;or society;jors;management science;infrastructure	The availability of repairable technical systems depends on the availability of (repairable) spare parts, to be influenced by (1) inventory levels and (2) repair capacity. In this paper, we present a procedure for simultaneous optimisation of these two factors. Our method is based on a modification of the well-known VARI-METRIC procedure for determining near-optimal spare part inventory levels and results for multi-class, multi-server queuing systems representing repair shops. The modification is required to avoid non-convexity problems in the optimisation procedure. To include part-time and overtime working, we allow for a non-integer repair capacity. To this end, we develop a simple approximation for queuing systems with a non-integer number of servers. Our computational experiments show that the near-optimal utilisation rate of the repair servers is usually high (0.80–0.98) and depends mainly on the relative price of the servers compared with inventory items. Further, the size of the repair shop (the minimal number of servers required for a stable system) plays its part. We also show that our optimisation procedure is robust for the choice of the step size for the server capacity.	spare part	Andrei Sleptchenko;Matthieu van der Heijden;Aart van Harten	2003	JORS	10.1057/palgrave.jors.2601511	project management;logistics;computer experiment;inventory;economics;forecasting;investment;marketing;operations management;reliability;location;operations research;information technology;scheduling;statistics	Robotics	5.779141352463724	-3.700432056901073	16617
fbe5202735ce851acd223ec3070e9fad27926adb	software piracy: an empirical examination of the impact of network externalities and open source alternatives on willingness to pay	software piracy;network externality;willingness to pay;open source	Software piracy continues to threaten the profitability of the software development industry. Software publishers have employed both preventive and deterrent controls and made small gains in protecting intellectual property rights. Although deterrent measures are believed to have a positive affect on profits, preventive controls are reported to have a negative affect as the cost of engagement offsets company profits. Commercial software developers are aggressively seeking economical methods of control to prevent piracy through extensive licensing agreements, further elimination of physical distribution networks, and the use of encryption-based software protection systems. However, recent encryption-based efforts have received negative responses from consumers. With the tightening of controls, more users are turning to other alternatives, posing a secondary threat to company profits. In this paper we study the impact of network externalities and the availability of open source software on the commercial software market when extensive piracy controls are in place. We use experimental closed-end contingent valuation to explore how the value of commercial software is impacted by piracy controls and competing software availability. In the complete design, we conduct a 2x2x2 experiment and use survey methods to elicit consumer preferences in determining how much students would be willing to pay (WTP) for commercial software under the following conditions: 1) high and low levels of network externalities, 2) the absence or presence of piracy controls, and 3) when comparable open source software is available. Preliminary results to pretest the survey instrument and experiment are included in this paper.	commercial software;contingency (philosophy);copy protection;digital rights management;eclipse;encryption;open-source hardware;open-source software;software developer;software development;software publisher;value (ethics)	Orneita Burton;T. S. Raghu;Rajiv Sinha;Ajay S. Vinze	2003			marketing;business;computer security;commerce	SE	-2.8474329286920845	-9.054058886571982	16632
952dd099e386c374391dbed0c59021eb4a59c449	soft sets based symbiotic organisms search algorithm for resource discovery in cloud computing environment	symbiotic organisms search;resource selection;resource discovery;resource matching;soft set;vms resources;vms information system;cloud computing	The dynamicity, coupled with the uncertainty that occurs between advertised resources and users’ resource requirement queries, remains significant problems that hamper the discovery of candidate resources in a cloud computing environment. Network size and complexity continue to increase dynamically which makes resource discovery a complex, NP-hard problem that requires efficient algorithms for optimum resource discovery. Several algorithms have been proposed in literature but there is still room for more efficient algorithms especially as the size of the resources increases. This paper proposes a soft-set symbiotic organisms search (SSSOS) algorithm, a new hybrid resource discovery solution. Soft-set theory has been proved efficient for tackling uncertainty problems that arises in static systems while symbiotic organisms search (SOS) has shown strength for tackling dynamic relationships that occur in dynamic environments in search of optimal solutions among objects. The SSSOS algorithm innovatively combines the strengths of the underlying techniques to provide efficient management of tasks that need to be accomplished during resource discovery in the cloud. The effectiveness and efficiency of the proposed hybrid algorithm is demonstrated through empirical simulation study and benchmarking against recent techniques in literature. Results obtained reveal the promising potential of the proposed SSSOS algorithm for resource discovery in a cloud environment.	cloud computing;hybrid algorithm;np-hardness;search algorithm;set theory;simulation	Ezugwu E. Absalom;Aderemi Oluyinka Adewumi	2017	Future Generation Comp. Syst.	10.1016/j.future.2017.05.024	still room;benchmarking;soft set;cloud computing;hybrid algorithm;distributed computing;computer science;search algorithm	HPC	20.185446153246392	-2.865287883647086	16646
485db8cac96454748575575d8a50cb3becc5721f	on stochastic dynamic stackelberg strategies	control theory;theorems;game theory;set theory;separation;feedback;stochastic processes;decision theory;stochastic dynamics	In recent years, considerable attention has been given in the literature to differential and multistage games [1-3,5]; most of it has dealt with deterministic games of perfect information. Behn and Ho [3] and Rhodes and Luenberger [1] considered the solution of pursuit-evasion games where each player made noise-corrupted linear measurements of the state. The problem was studied in more detail by Willman [4]; without additional restrictions, the general formulation of the problem leads to solutions involving infinitedimensional estimators. The concept of non-zero sum games is well known in static competitive economics, and has been recently introduced to dynamic games [5-7]. The solution of non-zero sum games is defined in terms of the approach taken by the players in defining optimality. The Stackelberg solution to the game is obtained when one of the players is forced to wait until the other player announces his decision, before making his own decision. Chen and Cruz [7] and Cruz and Simaan [8] have discussed extensively the	entity–relationship model;evasion (network security);multistage amplifier;pursuit-evasion	David A. Castañón;Michael Athans	1976	Automatica	10.1016/0005-1098(76)90081-9	game theory;mathematical optimization;combinatorics;theorem;decision theory;continuous-time stochastic process;control theory;feedback;mathematics;mathematical economics;stochastic;statistics;set theory	ECom	0.42993805282079806	-1.063561313792411	16665
ccd0e31d7c61b056e6ca3133b1d97da955623175	optimization on production-inventory problem with multistage and varying demand		This paper addresses production-inventory problem for the manufacturer by explicitly taking into account multistage and varying demand. A nonlinear hybrid integer constrained optimization is modeled to minimize the total cost including setup cost and holding cost in the planning horizon. A genetic algorithm is developed for the problem. A series of computational experiments with different sizes is used to demonstrate the efficiency and universality of the genetic algorithm in terms of the running time and solution quality. At last the combination of crossover probability and mutation probability is tested for all problems and a law is found for large size.	computation;constrained optimization;experiment;genetic algorithm;mathematical optimization;multistage amplifier;nonlinear system;program optimization;software release life cycle;time complexity;universality probability	Gang Duan;Li Chen;Yinzhen Li;Song Jie-Yan;Akhtar Tanweer	2012	J. Applied Mathematics	10.1155/2012/648262	mathematical optimization;mathematics;mathematical economics	AI	16.129478503232193	-0.5206284643455406	16737
fdac478c2e404b7da724253312946580aeab18d2	help for highway maintenance administrators-a highway maintenance simulation model	order statistics;course of action;moments;technical report;systems of probability distributions;monte carlo;department of transportation;data fitting;simulation model	The functions related to highway maintenance are often conceptually simple (repair the highway) and administratively complex (alternatives related to priorities, approaches, resources, and many others). Highway maintenance administrators are often faced with questions about which little or no definitive information exists and asked to make the proper decision. For example, if some amount of money is available for equipment, which type of equipment should be purchased? How many such equipment units? Where should they be placed and so forth? The dilemma of wanting to do the job well (i.e., make the best decision) and not having sufficient data with which to work is disconcerting at best. The highway maintenance simulation model described in this paper is intended to help alleviate the highway maintenance administrator's problem by providing a flexible highway-maintenance-decision-laboratory in which alternative courses of action may be tested.  The simulation model is the result of a two-year project sponsored jointly by the Louisiana Department of Transportation and Development and the Federal Highway Administration, with a scheduled completion date of September, 1979.	money;simulation;system administrator	James M. Pruett;Rodolfo Perdomo	1979			order statistic;simulation;engineering;technical report;simulation modeling;mathematics;moment;transport engineering;statistics;curve fitting;monte carlo method	AI	6.31451023143142	-7.234470519918648	16786
737875149212054d883841c875d4d11f580e37b2	dynamic uniform scaling for multiobjective genetic algorithms	multiobjective evolutionary algorithm;objective function;uniform distribution;multiobjective genetic algorithm	Before Multiobjective Evolutionary Algorithms (MOEAs) can be used as a widespread tool for solving arbitrary real world problems there are some salient issues which require further investigation. One of these issues is how a uniform distribution of solutions along the Pareto non-dominated front can be obtained for badly scaled objective functions. This is especially a problem if the bounds for the objective functions are unknown, which may result in the nondominated solutions found by the MOEA to be biased towards one objective, thus resulting in a less diverse set of tradeoffs. In this paper, the issue of obtaining a diverse set of solutions for badly scaled objective functions will be investigated and the proposed solutions will be implemented using the NSGA-II algorithm.	bridging (networking);evolutionary algorithm;genetic algorithm;image scaling;loss function;moea framework;mathematical optimization;multi-objective optimization;optimization problem;pareto efficiency	Gerulf K. M. Pedersen;David E. Goldberg	2004		10.1007/978-3-540-24855-2_2	mathematical optimization;uniform distribution	AI	23.32595371225741	-5.025446432950222	16790
401e8d1721499154fb71cd3b3ab151b8430395e9	digital content provision and optimal copyright protection	grupo de excelencia;copyright;piracy;search;administracion de empresas;quality;economia y empresa;grupo a	Advances in digital technologies have led to an increasing concern about piracy for providers of digital content (e.g., e-books, games, music, software, videos). Yet controversies exist over the influence of copyright protection on firm profitability. The objective of this paper is to provide an alternative rationale for the growing anti-protection trend and to investigate optimal copyright enforcement and quality provision in a monopoly setting. The proposed economic mechanism centers on the influence of copyright protection on consumer search when consumers can get to know the firm’s actions (e.g., price, quality) only after costly search. We show that more stringent copyright protection can induce the consumers to rationally expect lower ex post surplus, thus exerting a negative strategic effect on the consumers' willingness to search. This strategic effect may outweigh the positive main effect on the relative attractiveness of the authorized versus the pirated product, which can hence explain the optimality of incomplete and even zero copyright protection policies in markets where consumer prepurchase search is important (e.g., information goods, digital products). It is also because of this strategic effect that the firm may provide lower quality as copyright enforcement increases. Interestingly, quality unobservability can moderate this strategic effect and thus lead to an increasing incentive for copyright protection. This paper was accepted by Pradeep Chintagunta, marketing.		Liang Guo;Xiangyi Meng	2015	Management Science	10.1287/mnsc.2014.1972	economics;marketing;microeconomics;advertising;management;commerce	Theory	-2.9215101659168607	-7.747699147955767	16791
95f3a3cd2c1f1323738bec96a0cb54f0eade4364	disaggregating user evaluations using the shapley value		We consider a market where final products or services are compositions of a number of basic services. Users are asked to evaluate the quality of the composed product after purchase. The quality of the basic service influences the performance of the composed services but cannot be observed directly. The question we pose is whether it is possible to use user evaluations on composed services to assess the quality of basic services. We discuss how to combine aggregation of evaluations across users and disaggregation of information on composed services to derive valuations for the single components. As a solution we propose to use the (weighted) average as aggregation device in connection with the Shapley value as disaggregation method, since this combination fulfills natural requirements in our context. In addition, we address some occurring computational issues: We give an approximate solution concept using only a limited number of evaluations which guarantees nearly optimal results with reduced running time. Lastly, we show that a slightly modified Shapley value and the weighted average are still applicable if the evaluation profiles are incomplete.	advance directive - proxy;approximation algorithm;composition;computation;evaluation function;game theory;machine learning;partial;policy;polynomial;requirement;stable marriage problem;time complexity;way to go;teams	Matthias Feldotto;Themes De;Claus-Jochen Haake;Alexander Skopalik	2018		10.1145/3230654.3230659	solution concept;sampling (statistics);distributed computing;valuation (finance);mathematical optimization;basic service;shapley value;computer science;weighted arithmetic mean;reputation	Web+IR	-1.0220146458979815	-0.4500074341366279	16846
b83dcf2b43584dd5771fac972f83365b3b7b77f8	preventive maintenance of a single machine system working under piecewise constant operating condition		Abstract Manufacturing machines usually work under piecewise constant operating condition (PCOC) and are subject to imperfect preventive maintenance (PM). In this paper, an extended imperfect maintenance (IM) model for a machine working under PCOC is developed by combining an age-based hybrid IM model and an accelerated failure time model (AFTM). Maximum likelihood method is provided for estimating the model parameters. A dynamic cost-effective PM policy based on a short-term production plan is proposed for a common situation where the production plan is updated dynamically and only the current operating condition (OC) is confirmed. A numerical example is conducted to demonstrate the use of the proposed policy in practice.		Jiawen Hu;Zuhua Jiang;Haitao Liao	2017	Rel. Eng. & Sys. Safety	10.1016/j.ress.2017.05.014	reliability engineering;accelerated failure time model;engineering;piecewise;maximum likelihood;preventive maintenance	Logic	7.618774511271243	-0.6332426403884746	16856
cab194ec785a9d3ff32c6060e39824c36f8fcb25	strategic decision support for the bi-objective location-arc routing problem	location arc routing;minimization;combinatorial optimizaton;snow;routing;multi objective optimization;routing vehicles minimization roads linear programming planning;logistics multi objective optimization location arc routing combinatorial optimizaton;logistics;roads;linear programming;planning;vehicles;vehicle routing cost reduction decision theory facility location road vehicles supply chain management;strategic decision support benchmark instances road network total cost minimization larp facility intelligent decision support tool complex supply chain management problem bi objective location arc routing problem	This article develops an intelligent decision support tool for a complex supply chain management problem. In order to solve the bi-objective Location-Arc Routing Problem (LARP) facilities have to be located and routes must be determined simultaneously. The first objective is the minimization of the total costs which are the fixed cost of opening the facility, the fixed cost for the vehicles as well as the travelled distances. Additionally, a second objective related to a service aspect is investigated: the total sum of the delivery times for servicing the required demands. This idea arises since e.g. in a snow plowing application lead times for satisfying the required demands play an important role for the safety of people using the road network. To the best of our knowledge, this paper presents the first study devoted to the bi-objective LARP. The trade-off between the proposed objectives is investigated on adapted benchmark instances.	approximation;arc routing;benchmark (computing);decision support system;experiment;loss function;objective-c;optimization problem	Sandra Huber	2016	2016 49th Hawaii International Conference on System Sciences (HICSS)	10.1109/HICSS.2016.178	planning;logistics;routing;snow;simulation;computer science;linear programming;operations management;multi-objective optimization	Robotics	15.780032204281936	1.3035279594613107	16865
17f0291c5235f4efdd7d2ee45416657b64ed357c	biogeography-based optimisation for road recovery problem considering value of delay after urban waterlog disaster		An urban waterlog disaster (UWD) is caused by a rainfall when the urban drainage system fails to drain off the water produced by the rainfall. To reduce the influence of the waterlog disaster on society, a road recovery planning approach is developed that performs water volume estimation, and pumps collection, assignment, and transportation after a waterlog disaster. A mixed-integer programming model that considers value of delay to travellers was formulated. A biogeography-based optimisation (BBO) algorithm was presented to solve the problem. A real-world example of waterlog (caused by torrential rain on August 12, 2011, in the Pudong district of Shanghai, China) was presented using numerical analysis. The proposed approach emerges as a decision-making tool to help decision makers evaluate diverse recovery strategies before an UWD occurs, with the aim of optimising trade-off between economic costs and value of delay.	mathematical optimization	Lingpeng Meng;Qi Kang;Chuanfeng Han;Shao-Long Hu	2017	IJBIC	10.1504/IJBIC.2017.10004359	economic cost;torrential rain;mathematical optimization;programming paradigm;mathematics;drainage system (geomorphology);integer programming	HCI	10.83585059362483	-5.1753410812980745	16907
84fea8f210541fc47419d52bcf275972cfef393b	a mixed-integer goal programming model for nursing service budgeting	4 expense budget for nursing service;276 hospital nursing service staffing;641 goal programming model;goal programming	This paper presents a mixed-integer goal programming model for expense budgeting in a hospital nursing department. The model incorporates several different objectives based upon such considerations as cost containment and providing appropriate nursing hours for delivering quality nursing care. Also considered are possible trade-offs among full-time, part-time and overtime nurses on weekdays as well as weekends. The budget includes vacation, sick leave, holiday, and seniority policies of a hospital and various constraints on a hospital nursing service imposed by nursing unions. The results are based upon data from a study hospital and indicate that the model is practical for budgeting in a hospital nursing department.	breast feeding;budgets;care given by nurses;cost control;goal programming;integer (number);nursing service, hospital;nursing services;programming model;hospital nurse	Vandankumar M. Trivedi	1981	Operations research	10.1287/opre.29.5.1019	mathematical optimization;operations management;goal programming;mathematics	DB	12.371168467863878	-1.3151958377669881	16926
043ceaa9cc1d8f00f668e4d3e62afb9c4165d928	a consideration on the learning performances of the hierarchical structure learning automata (hsla) operating in the general nonstationary multiteacher environment	hierarchical structure;learning automata;computer simulation	Learning behaviors of the hierarchical structure learning automata (HSLA) with the three representative algorithms under the nonstationary multiteacher environments are considered. Several computer simulations confirm the effectiveness of the newly developed relative reward strength algorithm (NRRSA).	automata theory;learning automata;performance	Norio Baba;Yoshio Mogami	2007		10.1007/978-3-540-74829-8_11	simulation;computer science;artificial intelligence;machine learning	ML	18.38066570276973	-22.690178856488007	16947
a19bbd96b824b3322855c566db3fbf6b8dcac083	revenue management for low-cost providers	modelizacion;dynamic programming;overhead line;politica optima;control optimo;programacion dinamica;control theory;revenue management;exogeno;bepress selected works;pricing;exogene;real time;porcentaje ganancia;taux profit;ligne aerienne;air transportation;randomness;probabilistic approach;optimal policy;fijacion precios;revenue management dynamic pricing control theory;optimal control;modelisation;transport aerien;transporte aereo;service industry;tariffication;dynamic pricing;service industries;industrie service;tarification;enfoque probabilista;approche probabiliste;commande optimale;rebajas;temps reel;programmation dynamique;tiempo real;caractere aleatoire;airline industry;linea aerea;discount;politique optimale;rabais;modeling;return rate;fixation prix;exogenous;tarificacion	Low-cost providers have emerged as important players in many service industries, the most predominant being low-cost, or the so-called discount airlines. This paper presents models and results leading toward understanding the revenue management outlook for a discount pricing firm. A framework and model is formulated specifically for the airline industry, but is generalizable to low-cost providers in similar revenue management settings. We formulate an optimal pricing control model for a firm that must underprice to capture a segment of exogenous demand. Two specific model formulations are considered: a continuous deterministic version, and a discrete stochastic version. Structural results are derived for the deterministic case, providing insight into the general form of optimal underpricing policies. The stochastic results support the structural insight from the deterministic solution, and illuminate the effect of randomness on the underpricing policies.		Benjamin Marcus;Chris K. Anderson	2008	European Journal of Operational Research	10.1016/j.ejor.2007.04.010	tertiary sector of the economy;mathematical optimization;simulation;economics;marketing;operations management;mathematics	HPC	4.307960071957097	-3.3581277660682756	16948
ddfe621103eabddacc6a521cc2675afbb6f248fe	multicriteria linguistic decision making based on hesitant fuzzy linguistic term sets and the aggregation of fuzzy sets	fuzzy set;hesitant fuzzy linguistic term set;hesitant fuzzy set;likelihood value;multicriteria linguistic decision making;preference relation	In this paper, we present a new method for multicriteria linguistic decision making based on hesitant fuzzy linguistic term sets using the pessimistic attitude and the optimistic attitude of the decision-maker. The proposed method aggregates the fuzzy sets in each hesitant fuzzy linguistic term set into a fuzzy set and performs the a-cut operations to these aggregated fuzzy sets to get intervals, respectively, where a 2 ð0;1 . For each alternative, it performs the minimum operations and the maximum operations among the obtained intervals to get the derived intervals, respectively, where the minimum operation and the maximum operation among intervals denote the pessimistic attitude and the optimistic attitude of the decision-maker, respectively. Then, for each alternative, it uses the likelihood method for ranking the priority between the obtained intervals to get the preference order of the alternatives for the decision-maker with the pessimistic attitude and the optimistic attitude, respectively. The proposed method is more flexible than the existing methods for multicriteria linguistic decision making due to the fact that it considers the pessimistic attitude and the optimistic attitude of the decision-maker. 2014 Published by Elsevier Inc.	entity–relationship model;fuzzy set;horner's method	Shyi-Ming Chen;Jia-An Hong	2014	Inf. Sci.	10.1016/j.ins.2014.06.020	defuzzification;type-2 fuzzy sets and systems;fuzzy classification;computer science;artificial intelligence;fuzzy number;machine learning;data mining;mathematics;fuzzy set;fuzzy set operations	NLP	-3.12697719065363	-20.36051081795289	16958
2a0df9c903a5ffbaaa6b748735e2c0582726dbf8	lightweight adaptation in model-based reinforcement learning		Reinforcement learning algorithms can train an agent to operate successfully in a stationary environment. Most real-world environments, however, are subject to change over time. Research in the areas of transfer learning and lifelong learning addresses this problem by developing new algorithms that allow agents to adapt to environment change. Current trends in this area include model-free learning and data-driven adaptation methods. This paper explores in the opposite direction of those trends. Arguing that model-based algorithms may be better suited to the problem, it looks at adaptation in the context of model-based learning. Noting that standard algorithms themselves have some built-in capability for adaptation, it analyzes when and why a standard algorithm struggles to adapt to environment change. Then it experiments with lightweight and straightforward methods for adapting effectively.	algorithm;experiment;machine learning;rl (complexity);reinforcement learning;stationary process	Lisa Torrey	2011			robot learning;instance-based learning;error-driven learning;simulation;artificial intelligence;machine learning	ML	19.975506682657613	-20.81019295669708	16983
eabb552c3d1f680ce2775bdc7eb9664db144b229	a dynamic non-direct implementation mechanism for interdependent value problems	incentive compatibility;interdependent values;ex post incentive compatibility;informational size;mechanism design;privacy;auctions	Article history: Received 2 March 2015 Available online xxxx JEL classification: C70 D44 D60 D82	angela mclean (biologist);existential quantification;interdependence;privacy	Richard P. McLean;Andrew Postlewaite	2017	Games and Economic Behavior	10.1016/j.geb.2015.06.007	industrial organization;mechanism design;economics;incentive compatibility;microeconomics;privacy;welfare economics	Theory	-4.379163533223344	-3.9364948049825323	17015
f580ae876d18178d76cbc5704efeb2423c9fcc35	modelling the brt station capacity and queuing for all stopping busway operation	micro simulation;bus terminals;bus rapid transit;queue;traffic flow;dwell time;mathematical models;bus transit operations;stopping;bus capacity;queuing	Stations on Bus Rapid Transit (BRT) lines ordinarily control line capacity because they act as bottlenecks. At stations with passing lanes, congestion may occur when buses maneuvering into and out of the platform stopping lane interfere with bus flow, or when a queue of buses forms upstream of the station blocking inflow. We contend that, as bus inflow to the station area approaches capacity, queuing will become excessive in a manner similar to operation of a minor movement on an unsignalized intersection. This analogy was used to treat BRT station operation and to analyze the relationship between station queuing and capacity. We conducted microscopic simulation to study and analyze operating characteristics of the station under near steady state conditions through output variables of capacity, degree of saturation and queuing. In the first of two stages, a mathematical model was developed for all stopping buses potential capacity with bus to bus interference and the model was validated. Secondly, a mathematical model was developed to estimate the relationship between average queue and degree of saturation and calibrated for a specified range of controlled scenarios of mean and coefficient of variation of dwell time.		Rakkitha Widanapathiranage;Jonathan M. Bunker;Ashish Bhaskar	2015	Public Transport	10.1007/s12469-014-0095-y	embedded system;real-time computing;computer science;engineering;traffic flow;mathematical model;dwell time;transport engineering;queueing theory;queue;statistics	Crypto	9.965740272715712	-10.05505885480348	17020
c443629d1d2ce80c0684b95ef2c9d28745c52f24	a quantile approach to integration with respect to non-additive measures	completely distributive lattice;choquet integral;quantile;sugeno integral	The aim of this paper is to introduce some classes of aggregation functionals when the evaluation scale is a complete lattice. We focus on the notion of quantile of a lattice-valued function which have several properties of its real-valued counterpart and we study a class of aggregation functionals that generalizes Sugeno integrals to the setting of complete lattices. Then we introduce in the real-valued case some classes of aggregation functionals that extend Choquet and Sugeno integrals by considering a multiple quantile model generalizing the approach proposed in [3].	utility functions on indivisible goods	Marta Cardin	2012		10.1007/978-3-642-34620-0_14	mathematical optimization;mathematical analysis;discrete mathematics;quantile;mathematics;choquet integral;statistics	Vision	0.389910027412403	-21.42564700764946	17036
f43de05c3888fe22110d05a90d73aec454c2d7f3	verification of the time evolution of cosmological simulations via hypothesis-driven comparative and quantitative visualization	verification;engineering;visualization assisted process;geophysics computing cosmology data visualisation;feature detection;classical and quantum mechanics general physics;data visualization iterative algorithms predictive models computational modeling dark energy system testing environmental management project management physics iterative methods;data;simulation;time evolution verification;orders;visual evidence;iterative hypothesis verification process;data visualisation;hypothesis driven comparative visualization;accuracy;visualization;amr methods;computational modeling;adaptation model;geophysics computing;feature detection and tracking;environmental sciences;n body simulations methods;three dimensional displays;feature extraction;hypothesis testing;data visualization;cosmology;mathematical model;feature detection and tracking visualization in earth space environmental sciences hypothesis testing visual evidence;algorithms;space;n body simulation methods time evolution verification hypothesis driven comparative visualization quantitative visualization visualization assisted process cosmological simulation code verification iterative hypothesis verification process amr methods;environmental science;quantitative visualization;cosmological simulation code verification;n body simulation methods;evolution;visualization in earth;hypothesis test	We describe a visualization-assisted process for the verification of cosmological simulation codes. The need for code verification stems from the requirement for very accurate predictions in order to interpret observational data confidently. We compare different simulation algorithms in order to reliably predict differences in simulation results and understand their dependence on input parameter settings. Our verification process consists of the integration of iterative hypothesis-verification with comparative, feature and quantitative visualization. We validate this process by verifying the time evolution results of three different cosmology simulation codes. The purpose of this verification is to study the accuracy of AMR methods versus other N-body simulation methods for cosmological simulations.	adaptive multi-rate audio codec;algorithm;code;feature extraction;formal verification;iteration;mathematical optimization;parameter (computer programming);simulation;tamara munzner;verification and validation	Chung-Hsing Hsu;James P. Ahrens;Katrin Heitmann	2010	2010 IEEE Pacific Visualization Symposium (PacificVis)	10.1109/PACIFICVIS.2010.5429606	verification and validation of computer simulation models;simulation;computer science;theoretical computer science	Visualization	14.62569028040328	-10.945679758638176	17052
357d72a3307ef6cdc43dd7d3803a755f0bef28ce	stressed value-at-risk	port portfolio system stressed value at risk market risk stressed market environments stressed var regulatory version scenario analysis mathematical framework gaussian probability formalism tail risk fat tail volatilities risk factors stressed correlations participant behavior standard monte carlo simulation bloomberg lp;gaussian processes;risk management;risk management gaussian processes marketing monte carlo methods;marketing;monte carlo methods	"""Stressed Value at Risk (Stressed VAR) in its advanced framework provides a realistic measure of market risk tailored for stressed market environments. The simpler regulatory version of Stressed VAR is a special case. Stressed VAR corrects various deficits of ordinary VAR in times of market stress. Stressed VAR incorporates scenario analysis in a VAR setting in a sophisticated and consistent fashion. The mathematical framework is familiar and simple, designed to be understandable in a practical way by risk managers, traders, and regulators. Specifically, the familiar Gaussian (normal) probability formalism is employed, but in a completely different way than for ordinary VAR, designed to account for tail risk and collective behavior. The two main ingredients for Stressed VAR are """"fat-tail volatilities"""" that account for outlier events in the risk factors, and stressed correlations between risk factors that account for collective market participant behavior in stressed markets. This information is provided as input to standard Monte Carlo simulation to determine stressed market risks for a given portfolio. The VAR with the inputs of the fat-tail volatilities and the stressed correlations is the Stressed VAR. Bloomberg LP is implementing Stressed VAR in the PORT portfolio system."""	bloomberg terminal;monte carlo method;risk aversion;scenario analysis;semantics (computer science);simulation;traders;value at risk	Jan Dash	2012	2012 IEEE Conference on Computational Intelligence for Financial Engineering & Economics (CIFEr)	10.1109/CIFEr.2012.6327832	financial economics;econometrics;economics;operations management	Theory	2.23451504763579	-10.365230968421468	17057
4be7ba771932fcc3d0b9ce6e81d58f605a496b99	optimal policy for profit maximising in an eoq model under non-linear holding cost and stock-dependent demand rate	optimal solution;inventory management;non linear holding cost;stock dependent demand rate;optimal policy;inventory model;maximum profit;necessary and sufficient condition;numerical algorithm;eoq models;lot sizing;for profit;profitability;existence and uniqueness	In this article, we integrate a non-linear holding cost with a stock-dependent demand rate in a maximising profit per unit time model, extending several inventory models studied by other authors. After giving the mathematical formulation of the inventory system, we prove the existence and uniqueness of the optimal policy. Relying on this result, we can obtain the optimal solution using different numerical algorithms. Moreover, we provide a necessary and sufficient condition to determine whether a system is profitable, and we establish a rule to check when a given order quantity is the optimal lot size of the inventory model. The results are illustrated through numerical examples and the sensitivity of the optimal solution with respect to changes in some values of the parameters is assessed.	economic order quantity;nonlinear system	Valentín Pando;Juan García-Laguna;Luis A. San-José	2012	Int. J. Systems Science	10.1080/00207721.2011.565134	mathematical optimization;profitability index	Logic	2.9007339535890213	-3.88747872480048	17059
73b753934d187cd0428ba7007b9ae67d5d4c6862	single-versus two-opportunity price postponement and ordering strategies of a seasonal product				Avi Herbon	2018	Decision Sciences	10.1111/deci.12299		Theory	2.4548200993373395	-8.185390583711314	17094
5f9f9779d2e87c3edd544d735b914ea9394d4c44	discussion on natural fuzzy extension and joint fuzzy extension of the rational function	interval analysis;joint fuzzy extension;natural fuzzy extension;structured element	According to the knowledge of interval analysis, relations between natural fuzzy extension and joint fuzzy extension of rational function are discussed in this paper . On the basis of the natural fuzzy extension of rational function , two solutions to the joint fuzzy extension of rational function are put forward. Based on the structured element method, the fuzzy arithmetic with equality constraints is turned into the operation of two monotone functions with the same monotonic form on [0,1]. From this transition we get analytical expression of joint fuzzy extension of the rational function. © 2010 Springer-Verlag Berlin Heidelberg.		Shihui Wang;Sizong Guo	2010		10.1007/978-3-642-14880-4_8	fuzzy logic;mathematical analysis;discrete mathematics;membership function;defuzzification;fuzzy mathematics;fuzzy classification;fuzzy subalgebra;fuzzy number;fuzzy measure theory;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations	AI	-0.6861205225045931	-23.8001247388399	17119
44dc99de6ed8138ded4136fda37b60e7aef50b27	online ranking with top-1 feedback		We consider a setting where a system learns to rank a fixed set of m items. The goal is produce good item rankings for users with diverse interests who interact online with the system for T rounds. We consider a novel top-1 feedback model: at the end of each round, the relevance score for only the top ranked object is revealed. However, the performance of the system is judged on the entire ranked list. We provide a comprehensive set of results regarding learnability under this challenging setting. For PairwiseLoss and DCG, two popular ranking measures, we prove that the minimax regret is Θ(T 2/3). Moreover, the minimax regret is achievable using an efficient strategy that only spends O(m logm) time per round. The same efficient strategy achieves O(T 2/3) regret for Precision@k. Surprisingly, we show that for normalized versions of these ranking measures, i.e., AUC, NDCG & MAP, no online ranking algorithm can have sublinear regret.	algorithm;definite clause grammar;learnability;minimax;regret (decision theory);relevance	Sougata Chaudhuri;Ambuj Tewari	2015	CoRR		mathematical optimization;machine learning;data mining;mathematics;statistics	ML	23.404559762883416	-17.19411661844529	17161
ed6547a4a7735fb38f62d7124853038b6d6b2619	plug-in electric vehicle charging congestion analysis using taxi travel data in the central area of beijing		Recharging a plug-in electric vehicle is more time-consuming than refueling an internal combustion engine vehicle. As a result, charging stations may face serious congestion problems during peak traffic hours in the near future with the rapid growth of plug-in electric vehicle population. Considering that drivers' time costs are usually expensive, charging congestion will be a dominant factor that affect a charging station's quality of service. Hence, it is indispensable to conduct adequate congestion analysis when designing charging stations in order to guarantee acceptable quality of service in the future. This paper proposes a data-driven approach for charging congestion analysis of plug-in electric vehicle charging stations. Based on a data-driven plug-in electric vehicle charging station planning model, we adopt the queuing theory to model and analyze the charging congestion phenomenon in these planning results. We simulate and analyze the proposed method for charging stations servicing shared-use electric taxis in the central area of Beijing leveraging real-world taxi travel data.	network congestion;plug-in (computing);quality of service;queueing theory;simulation	Huimiao Chen;Hongcai Zhang;Zechun Hu;Yunyi Liang;Haocheng Luo;Yinhai Wang	2017	CoRR		mathematics;quality of service;charging station;control theory;queueing theory;internal combustion engine;automotive engineering;population;electric vehicle;beijing;taxis	HCI	9.242631539312905	-9.289880711594433	17199
1a82cf371751b282cd9d152ed896db29377f64b2	dynamic trajectory routing using an adaptive search method	search method;genetic algorithm;genetic algorithms;dynamic vehicle routing	Introduction ‘lb vehicle routingproblem (VRP) is a Compiex “ ~ Optim@ion probkm * heiongs to the ciass of NMompiete psobkms[3]. Ftx problems in the IWcornpkte CkSS, optimal solutions with known algorithms can be obtained oniyinexpnenM timp]. The VRP arises ina wide may of practical M1OII making probiems such as retail distribution, schooi bus routing, mail and newspapcs deiivety, mtmieipd waste coikction, fuel oil ddivery, dial-a-ride service, ml airli= and railway &et muting and scheduling. Efficient muting and scheduling of vehicles cm save govern~nt and industry many miliions of doilars a y=. Many Of the Of vehick muting SY$tem$ tit have hoen deveioped to date attain soiutions based on the static nature of the problem. In reality, the vehick routing probkms are dynamic and the methods for solving such probkms must have adaptive eapabdities.	algorithm;scheduling (computing);vehicle routing problem	Sam R. Thangiah;Kendall E. Nygard	1993		10.1145/162754.162840	mathematical optimization;path vector protocol	Theory	15.59239508723783	1.0714461526391712	17229
664c1eec91de7de504f93cd5be8e7982ae940acd	a method of weather radar echo extrapolation based on convolutional neural networks		Weather radar echo extrapolation techniques possess wide application prospects in short-term forecasting (i.e., nowcasting). Traditional methods of radar echo extrapolation have difficulty obtaining long limitation period data and lack the utilization rate of radar. To solve this problem, this paper proposes a method of weather radar echo extrapolation based on convolutional neural networks (CNNs). To create a strong correlation among contiguous weather radar echo images from traditional CNNs, this method present a new CNN model: Recurrent Dynamic CNNs (RDCNN). RDCNN consists of a recurrent dynamic sub-network and a probability prediction layer, which constructs a cyclic structure in the convolution layer, improving the ability of RDCNN to process time-related images. Nanjing, Hangzhuo and Xiamen experimented with radar data, and compared with traditional methods, our method achieved higher accuracy of extrapolation and extended the limitation period effectively, meeting the requirements for application.	convolutional neural network;extrapolation;neural networks	En Shi;Qian Li;Daquan Gu;Zhangming Zhao	2018		10.1007/978-3-319-73603-7_2	weather radar;extrapolation;convolutional neural network;computer vision;computer science;deep learning;radar;nowcasting;convolution;artificial intelligence	AI	8.66551114393858	-22.084286870366853	17243
861807a7bb4f8100ee64614073e90760e3f180f5	use-based pricing and prepaid-based pricing for service products: analysis of an unlimited prepaid card		This paper analyses the optimal pricing strategies of two sales models for a service product: the use-based mode, in which a customer pays for each use, and the prepaid-based mode, in which a customer enjoys a service with a card that represents an up-front payment for a period of time. For the prepaid mode, we analyze a kind of unlimited card, such as a fitness card. With considerations of customers’ perceived value, transfer fees and the time-value of the funds, we establish generalized pricing models of a service product under these two sales modes in a monopoly, and discuss the pricing strategies between two companies using game theory in a duopoly. Research shows that in a monopoly, the optimal use-based price is identical whether the transfer fee exists or not, and the optimal prepaid-based price when the transfer fee exists is lower than when it does not exist. In a duopoly, according to game theory, we can derive two Nash equilibria from nine situations. Finally, through numerical examples, we verify the effectiveness of the pricing strategy and derive some managerial insights.	game theory;monopoly;nash equilibrium;numerical analysis;stored-value card	Chuan Zhang;Yu-Xin Tian;Zhaomei Shi;Yan-Qiu Li	2018	IEEE Access	10.1109/ACCESS.2018.2873771	game theory;duopoly;distributed computing;service product management;pricing strategies;payment;nash equilibrium;computer science;monopoly;microeconomics	Metrics	-0.7678564514546636	-5.224219756171199	17266
2abaad54a0ceaa49dfc2fbcc94449564f071bea6	a dynamic analysis of the single-item periodic stochastic inventory system with order capacity	dynamic programming;efficient algorithm;dynamic program;operations research;optimal policy;stochastic inventory system;band structure;α β convexity;dynamic analysis	Consider a single-item periodic review stochastic inventory system with positive setup cost and finite order capacity. Chen and Lambrecht [Operations Research 44 (1996) 1013] showed that the optimal policy has a systematic pattern called the X–Y band structure. However there is no clear pattern for inventory positions between X and Y. Some properties of the optimal order policy are provided when the inventory position falls between X and Y. As a consequence of the analysis, an efficient algorithm is provided to compute the optimal ordering policy parameters. 2002 Elsevier Science B.V. All rights reserved.	algorithm;electronic band structure;entity–relationship model;inventory;operations research	Gin Hor Chan;Yuyue Song	2003	European Journal of Operational Research	10.1016/S0377-2217(02)00256-4	mathematical optimization;economics;computer science;operations management;dynamic programming;mathematics;dynamic program analysis;welfare economics;electronic band structure	Robotics	4.139221792282713	-2.5101087299414244	17330
94bea6f00be284206418a4a541a45bbefa64f5b7	asymmetric information about rivals' types in standard auctions	bidding strategies;asymmetric information;allocative efficiency;first price auction;second price auction;regularity condition	Abstract   We study auctions in which bidders may know the types of some rival bidders but not others. This asymmetry in bidders' knowledge about rivals' types has different effects on the two standard auction formats. In a second-price auction, it is weakly dominant to bid one's valuation, so the knowledge of rivals' types has no effect, and the good is allocated efficiently. In a first-price auction, bidders refine their bidding strategies based on their knowledge of rivals' types, which yields an inefficient allocation. We show that the inefficient allocation in the first-price auction translates into a poor revenue performance. Given a standard regularity condition, the seller earns higher expected revenue from the second-price auction than from the first-price auction, whereas the bidders are better off from the latter.		Jinwoo Kim;Yeon-Koo Che	2004	Games and Economic Behavior	10.1016/S0899-8256(03)00126-X	spectrum auction;auction sniping;allocative efficiency;walrasian auction;financial economics;information asymmetry;eauction;vickrey auction;combinatorial auction;generalized second-price auction;economics;unique bid auction;reverse auction;vickrey–clarke–groves auction;proxy bid;common value auction;revenue equivalence;multiunit auction;english auction;microeconomics;bid shading;auction theory;commerce;forward auction;dutch auction	ECom	-2.423338520480608	-4.731409550756753	17407
34e71e93f19160613bd13a5f3b3e2ae11899a046	existence of stable matchings in large markets with complementarities	large market;stable matchings;stability;core		complementarity theory;matching (graph theory)	Eduardo M. Azevedo;John William Hatfield	2013		10.1145/2492002.2482579	financial economics;core;stability;economics;mathematics;microeconomics;welfare economics;statistics	ECom	-4.260653657751215	-2.9821256502783564	17431
ffc3c4ed5dd13ce97d313227714addaecae3854c	multi-level maintenance economic optimization model of electric multiple unit component based on shock damage interaction		In order to simulate the reliability evolution process of Electric Multiple Unit (EMU) components under external shock and improve maintenance economy. The multi-level preventive maintenance method is established and the influence of maintenance period and allocation of multi-level imperfect maintenance on the maintenance economy are discussed respectively. Numerical experiments show that the multi-phase preventive maintenance model can reduce the maintenance cost rate. The analysis of bi-level imperfect maintenance capacity indicates that two-level preventive maintenance can extend the mileage of four-level preventive maintenance and three-level preventive maintenance can reduce the maintenance cost rate. Finally, some recommendations for the allocation of maintenance efforts are provided according to the different railway route features.	program optimization	Hong Wang;Yong Jun He;Lv Xiong;Zuhua Jiang	2017		10.1007/978-981-10-6364-0_72	preventive maintenance;reliability engineering;engineering	AI	8.230314097930725	-0.707445768402913	17453
0eb44302d868b02c238fcd95781bb6428b9a82c8	applying genetic algorithms to wall street	gas;evolutionary computation;dow jones;support levels;trading systems;stock markets;financial markets;genetic algorithms;moving averages;resistance levels;support resistance levels	Genetic algorithms (GAs) can be applied to a wide range of problems in the field of finance. The purpose of this paper is to make GAs accessible to practitioners, academicians and students who are interested in financial markets. By describing a simple application consisting in tuning a technical trading system for the Dow Jones we illustrate step by step how the reader can implement its own trading system with the help of the powerful tool, the GA. To show how this technique can easily be extended to other type of applications in the financial domain, some examples are brought up at the end of the paper.	algorithmic trading;care-of address;genetic algorithm;jones calculus;software release life cycle	Laura Núñez-Letamendia;Joaquín A. Pacheco;Silvia Casado	2011	IJDMMM	10.1504/IJDMMM.2011.042932	simulation;genetic algorithm;gasoline;computer science;machine learning;data mining;moving average;operations research;financial market;evolutionary computation	Theory	17.547704897936253	-8.019814399542643	17454
52e4911c4cd30f1bc35b102635793357f51eac57	scheduling constraints in dial-a-ride problems with transfers: a metaheuristic approach incorporating a cross-route scheduling procedure with postponement opportunities		In a conventional dial-a-ride-system a passenger is moved by one vehicle between the specified pickup and the corresponding drop-off location. In a dial-a-ride-system with transfers, it is possible (or even necessary) that passengers change the vehicle once or several times during their ride from the specified pickup to the specified drop-off location. Transfer scheduling constraints (TSC) are imposed in order to ensure that the comfort of the transfer remains on an acceptable level by avoiding too short or too long vehicle changing times but also for limiting the total riding time between the initial pickup location to the final destination. In this contribution, we investigate the dial-a-ride-problem with transfer scheduling constraints as an example for routing scenarios with TSC. We provide initial insights into the consequences of introducing TSCs using computational experiments with a memetic algorithm metaheuristic. This search algorithm is enhanced by a schedule building procedure that postpones waiting times at selected locations if necessary in order to meet the TSCs.	metaheuristic;schedule (project management);scheduling (computing)	Jörn Schönberger	2017	Public Transport	10.1007/s12469-016-0139-6	real-time computing;simulation;engineering;operations management	ML	13.845217085981073	0.7042615170334591	17467
5986a68963e502175e51cab86e2090c70941c617	contribution to the parameter elicitation in multicriteria optimization			mathematical optimization	Noureddine Aribi	2015	4OR	10.1007/s10288-014-0273-4	mathematical optimization;mathematics;multi-objective optimization	EDA	18.8079128754542	-4.298983551903111	17484
129e08a53a09616c0cdadc776b814b02e6ac21e9	multi-objective evolutionary algorithms to investigate neurocomputational issues: the case study of basal ganglia models	basal ganglia;pareto front;multi objective evolutionary algorithm;satisfiability;action selection;objective function;winner take all	The basal ganglia (BG) are a set of subcortical nuclei involved in action selection processes. We explore here the automatic parameterization of two models of the basal ganglia (the GPR and the CBG) using multi-objective evolutionary algorithms. We define two objective functions characterizing the supposed winner-takes-all functionality of the BG and obtain a set of solutions lying on the Pareto front for each model. We show that the CBG architecture leads to solutions dominating the GPR ones, this highlights the usefulness of the CBG additional connections with regards to the GPR. We then identify the most satisfying solutions on the fronts in terms of both functionality and plausibility. We finally define critical and indifferent parameters by analyzing their variations and values on the fronts, helping us to understand the dynamics governing the selection process in the BG models.	action selection;automatic control;basal (phylogenetics);consistency model;evolutionary algorithm;ganglia;job control (unix);kriging;pareto efficiency;plausibility structure	Jean Liénard;Agnès Guillot;Benoît Girard	2010		10.1007/978-3-642-15193-4_56	winner-take-all;action selection;computer science;artificial intelligence;multi-objective optimization;machine learning;satisfiability	Comp.	24.518887906388077	-8.280969160632477	17498
87a2ec639d88d3ff0c1fda819ce7f70164e6b9d7	forecasting urban air pollution using hmm-fuzzy model	forecasting;hidden markov model;computational intelligence;urban air pollution;hybrid model;fuzzy logic;hybrid approach;air pollution;hidden markov model hmm;fuzzy model;artificial neural network	In this paper, we introduce a Computational Intelligence (CI)-based method to model an hourly air pollution forecasting system that can forecast concentrations of airborne pollutant variables. We have used a hybrid approach of Hidden Markov Model (HMM) with fuzzy logic (HMM-fuzzy) to model hourly air pollution at a location related to its traffic volume and meteorological variable. The forecasting performance of this hybrid model is compared with other common tool based on Artificial Neural Network (ANN) and other fuzzy tool where rules are extracted using subtractive clustering. This research demonstrates that the HMM-fuzzy approach is effectively able to model an hourly air pollution forecasting system.		M. Maruf Hossain;Md. Rafiul Hassan;Michael Kirley	2008		10.1007/978-3-540-68125-0_52	fuzzy logic;forecasting;computer science;artificial intelligence;machine learning;computational intelligence;artificial neural network;hidden markov model;air pollution	HCI	9.532224098061585	-19.089562946332425	17522
dc6912420c16d7f5734884f21094c5c043d83362	mitigating bankruptcy propagation through contractual incentive schemes	simulation;journal;multi agent systems;bankruptcy propagation;contractual incentive schemes;supply chain	With the increasing interdependence among supply chain members on material, information and capital, interactions and decisions characterized by operational parameters are important causes of bankruptcy propagation in supply chain. This paper investigates the methods for mitigating bankruptcy propagation through supply chain coordination. Based on a two-stage supply chain network that consists of multiple upstream manufacturers and multiple downstream retailers, the effectiveness of some typical contractual incentive schemes, including revenue sharing, price discount and quantity flexibility contracts, in mitigating bankruptcy propagation among supply chain members is examined. Through agent-based simulation experiments, it has been revealed that: 1) the three typical supply chain contracts with properly designed contract parameters are effective in mitigating bankruptcy propagation, but their effectiveness depends on operational parameters of the supply chain; 2) horizontal competition among retailers is an important factor in determining the effectiveness of these contracts; 3) revenue sharing contract turns out to be more effective in mitigating bankruptcy propagation than the other two contracts. By comparing the optimal contract parameters with and without considering bankruptcy risks, it has also been found that, a set of contract parameters that can maximize the profit of the supply chain may increase the occurrence of bankruptcy in supply chain, leading to the phenomenon of a risk-profit tradeoff.	software propagation	Yanhong Sun;Xiaoyan Xu	2012	Decision Support Systems	10.1016/j.dss.2012.02.003	computer science;artificial intelligence;marketing;multi-agent system;microeconomics;supply chain;commerce	ECom	-2.0792488149318604	-6.009167478688271	17528
680f43bc68ea6da0cba3e05c383f81aaa7d150ff	short-term line maintenance scheduling of distribution network with pv penetration considering uncertainties		In this paper, the methodology for short-term line maintenance scheduling in distribution network with PV penetration is proposed, in which the mixed randomness and fuzziness of solar power generation concerning the available cloud amount forecasting, along with other random or fuzzy factors, such as electricity demand and component historical failure rate, are considered. First, based on the obtained historical data from NASA, the empirical mapping from cloud amount to solar irradiance can be established where the uncertainty is represented by combining randomness and fuzziness. Second, the short-term line maintenance scheduling of distribution network with uncertainties is modeled by using random fuzzy chance-constrained programming aiming at minimizing the pessimistic value in terms of economics and reliability subject to the chance constraints. Finally, a hybrid intelligent algorithm with two-layer optimization is implemented to solve the model. Simulation experiments are carried out on the IEEE 33-Bus and IEEE RBTS 2-Bus systems, and the results demonstrate the effectiveness of the proposed short-term line maintenance scheduling solution for distribution network with the mixed uncertainties of randomness and fuzziness.	algorithm;experiment;failure rate;mathematical optimization;randomness;scheduling (computing);simulation	Zhejing Bao;Canzhi Gui;Xiaogang Guo	2018	IEEE Access	10.1109/ACCESS.2018.2838082	reliability engineering;randomness;solar irradiance;fuzzy logic;failure rate;scheduling (computing);distributed computing;cloud cover;maintenance engineering;computer science;solar power	Embedded	15.025351491623924	-3.035757672887183	17582
59d1e6330ef653e868efc004acd5724efc95da26	analysis of the effects of intermodal terminals for the solutions of urban logistics problems in istanbul city		We analyzed the urban logistics problems in Istanbul City, and we tried to show the solutions of these problems thanks to intermodal freight terminals. Istanbul is a center of trade, tourism, and industry; 60 % of total logistics activities take place in Istanbul. At the same time, capacities, infrastructures, and performance of logistics system are affecting the logistics systems and industries of European countries and neighborhoods. However, solving the urban logistics problems have become regional and global ones and especially European countries, industries, and their commercial actors cannot ignore these problems. In addition to urban freight flows, high volume of international and domestic cargo flows causes different problems related to urban logistics. On the other hand, Istanbul has the different logistics nodes as airports, organized industrial zones, business centers, whereas logistics networks and links between nodes are not required level. This study focuses on the logistics nodes, relationships, opportunities and facilities of transportation, and effects of intermodal freight terminals to solving problems of the insufficient connections between these nodes.		Ömer Faruk Görçün	2014		10.1007/978-3-319-23512-7_68	environmental engineering;geography;civil engineering;transport engineering	HCI	9.47969461003483	-7.030764810590298	17621
a9d26e73af51220bd7a12e2a22e85a59c7c056eb	online adaptable learning rates for the game connect-4	games vectors training indexes machine learning algorithms complexity theory coherence;learning artificial intelligence computational complexity computer games;temporal difference learning tdl board games learning rates machine learning online adaptation n tuple systems reinforcement learning self adaptation self play temporal coherence;board games online adaptable learning rates computational intelligence temporal difference learning connect 4 incremental delta bar delta idbd machine learning	Learning board games by self-play has a long tradition in computational intelligence for games. Based on Tesauro's seminal success with TD-Gammon in 1994, many successful agents use temporal difference learning today. But in order to be successful with temporal difference learning on game tasks, often a careful selection of features and a large number of training games is necessary. Even for board games of moderate complexity like Connect-4, we found in previous work that a very rich initial feature set and several millions of game plays are required. In this work we investigate different approaches of online-adaptable learning rates like Incremental Delta Bar Delta (IDBD) or temporal coherence learning (TCL) whether they have the potential to speed up learning for such a complex task. We propose a new variant of TCL with geometric step size changes. We compare those algorithms with several other state-of-the-art learning rate adaptation algorithms and perform a case study on the sensitivity with respect to their meta parameters. We show that in this set of learning algorithms those with geometric step size changes outperform those other algorithms with constant step size changes. Algorithms with nonlinear output functions are slightly better than linear ones. Algorithms with geometric step size changes learn faster by a factor of 4 as compared to previously published results on the task Connect-4.	algorithm;coherence (physics);collision detection;computation;computational intelligence;exptime;fastest;machine learning;nonlinear system;performance tuning;td-gammon;tcl;temporal difference learning;tracing (software);video game developer	Samineh Bagheri;Markus Thill;Patrick Koch;Wolfgang Konen	2016	IEEE Transactions on Computational Intelligence and AI in Games	10.1109/TCIAIG.2014.2367105	temporal difference learning;semi-supervised learning;unsupervised learning;robot learning;multi-task learning;instance-based learning;error-driven learning;algorithmic learning theory;simulation;computer science;artificial intelligence;online machine learning;machine learning;learning classifier system;stability;competitive learning;computational learning theory;reinforcement learning;active learning;synchronous learning;probably approximately correct learning;generalization error	AI	19.620171389512215	-20.4464300920371	17645
9ee2d9050d1f490d5650ccb5ec60d999399264a4	scatter search for an uncapacitated p-hub median problem	scatter search;path relinking;hub location;p hub;combinatorial optimization;r allocation	Scatter search is a population-based method that has been shown to yield high-quality outcomes for combinatorial optimization problems. It uses strategies for combining solution vectors that have proved effective in a variety of problem settings. In this paper, we present a scatter search implementation for an NP-hard variant of the classic p-hub median problem. Specifically, we tackle the uncapacitated r-allocation p-hub median problem, which consists of minimizing the cost of transporting the traffics between nodes of a network through special facilities that act as transshipment points. This problem has a significant number of applications in practice, such as the design of transportation and telecommunications	combinatorial optimization;mathematical optimization;usb hub	Rafael Martí;Ángel Corberán;Juanjo Peiró	2015	Computers & OR	10.1016/j.cor.2014.12.009	mathematical optimization;combinatorial optimization;mathematics;algorithm	AI	17.514577948135596	2.693161127854202	17652
f28e9bbf63301bbae6ea3fa994f49ac31fc3df79	continuous t-norms and t-conorms satisfying the principle of inclusion and exclusion		The classical principle of inclusion and exclusion is formulated for set-theoretic union and intersection. It is natural to ask if it can be extended to fuzzy sets. The answer depends on the choice of fuzzy logical operations (which belong to the larger class of aggregation operators). Further, the principle can be generalized to interval-valued fuzzy sets, resp. IF-sets (Atanassov’s intuitionistic fuzzy sets). The principle of inclusion and exclusion uses cardinality of sets (which has a natural extension to fuzzy sets, interval-valued fuzzy sets and IF sets) or, more generally, a measure, which can be defined in different ways. We also point up the question of the domain of the measure which has been neglected so far.	t-norm	Mária Kuková;Mirko Navara	2013		10.1007/978-3-642-39165-1_20	discrete mathematics;cardinality;fuzzy logic;operator (computer programming);inclusion–exclusion principle;fuzzy set;mathematics	Vision	-1.025050861636828	-22.4470120911696	17692
3a7dd1d35a8aadb53af7acab37830af47b2e3659	packing non-identical circles within a rectangle with open length	heuristic;decomposition;institute for integrated and intelligent systems;griffith business school;logistics and supply chain management;genetic algorithm;cutting and packing;150309;combinatorial optimization	Packing non-identical circles inside a rectangle witnesses a wide range of industrial applications. However, the non-convex constraints in this problem make it intractable using exact analytical approaches. Even via heuristic methods, the solution time for industrial-scale instances sometimes is too long to be acceptable. This article aims to challenge the existing methods for the benchmark instances. The most significant contributions of this work are: firstly, we proposed three types of packing positions for selection and used human intelligence to convert an arbitrary circle sequence into a feasible compact layout; secondly, diverse position selection criteria have been tested, and it is found that the criterion commonly used in the literature is not the best; thirdly, the traditional genetic algorithm is adapted with lower crossover rate but higher mutation rate particularly, and a minor-adjustment operator with the purpose of exploring the neighborhood of the current best solutions is introduced.	analysis of algorithms;apple a5;benchmark (computing);combinatorial optimization;feasible region;genetic algorithm;heuristic;in-game advertising;mathematical optimization;numerical analysis;numerical method;parallel computing;premature convergence;search algorithm;set packing;software bug;software release life cycle;twelve tricks;xfig	Yaohua He;Yong Wu	2013	J. Global Optimization	10.1007/s10898-012-9948-6	mathematical optimization;genetic algorithm;heuristic;combinatorial optimization;mathematics;decomposition	AI	23.944889785040882	0.874446874966317	17698
7ebacceca305585760480ac6f4cf7207300cd448	measuring instance difficulty for combinatorial optimization problems	traveling salesman problem;assignment problem;bin packing;timetabling;graph coloring;algorithm selection;instance difficulty;knapsack problem;phase transition;hardness prediction;landscape analysis;combinatorial optimization	Discovering the conditions under which an optimization algorithm or search heuristic will succeed or fail is critical for understanding the strengths and weaknesses of different algorithms, and for automated algorithm selection. Large scale experimental studies – studying the performance of a variety of optimization algorithms across a large collection of diverse problem instances – provide the resources to derive these conditions. Data mining techniques can be used to learn the relationships between the critical features of the instances and the performance of algorithms. This paper discusses how we can adequately characterize the features of a problem instance that have impact on difficulty in terms of algorithmic performance, and how such features can be defined and measured for various optimization problems. We provide a comprehensive survey of the research field with a focus on six combinatorial optimization problems: assignment, traveling salesman, and knapsack problems, binpacking, graph coloring, and timetabling. For these problems – which are important abstractions of many real-world problems – we review hardness-revealing features as developed over decades of research, and we discuss the suitability of more problem-independent landscape metrics. We discuss how the features developed for one problem may be transferred to study related problems exhibiting	algorithm selection;algorithm design;coherence (physics);combinatorial optimization;data mining;expect;graph coloring;hardness of approximation;heuristic;knapsack problem;mathematical optimization;maxima and minima;optimization problem	Kate Smith-Miles;Leo Lopes	2012	Computers & OR	10.1016/j.cor.2011.07.006	phase transition;continuous knapsack problem;optimization problem;2-opt;mathematical optimization;bin packing problem;cross-entropy method;combinatorial optimization;generalized assignment problem;machine learning;graph coloring;mathematics;assignment problem;weapon target assignment problem;travelling salesman problem;knapsack problem;l-reduction;algorithm;3-opt;bottleneck traveling salesman problem;quadratic assignment problem	AI	22.615767331575825	0.9500916583530518	17778
d2eec65a0a91e2a05af0d906f06a209fdcd01e86	application of artificial immune system to domestic energy management problem		The connection of devices in a smart home should be done optimally, this helps save energy and money. Numerous optimization models have been applied, they are based on fuzzy logic, linear programming or bio-inspired algorithms. The aim of this work is to solve an energy management problem in a domestic environment by applying an artificial immune system. We carried out a thorough analysis of the different strategies that optimize a domestic environment system, in order to demonstrate the ability of an artificial immune system to find a successful optima that satisfies the problem constraints.	algorithm;artificial immune system;autonomous robot;british informatics olympiad;energy systems language;evolutionary computation;fuzzy logic;genetic algorithm;home automation;linear programming;mathematical optimization;nonlinear system;optimization problem;program optimization;scheduling (computing);software release life cycle;vergence	María Navarro-Cáceres;Amin Shokri Gazafroudi;Francisco Prieto Castrillo;Kumar G. Venyagamoorthy;Juan Manuel Corchado	2017	2017 IEEE 17th International Conference on Ubiquitous Wireless Broadband (ICUWB)	10.1109/ICUWB.2017.8251010	fuzzy logic;electricity generation;artificial immune system;domestic environment;control engineering;home automation;computer science;linear programming;energy management	Robotics	18.804328462400125	-4.117228383373879	17808
0f78efb779f11a517f89dbe8bc3fc6a93c02961f	hospital readmissions reduction program: an economic and operational analysis	game theory;healthcare;incentives;grupo de excelencia;medicare;readmissions;administracion de empresas;regulation;public policy;economia y empresa;grupo a;health care	The Hospital Readmissions Reduction Program (HRRP), a part of the US Patient Protection and Affordable Care Act, requires the Centers for Medicare and Medicaid Services to penalize hospitals with excess readmissions. We take an economic and operational (patient flow) perspective to analyze the effectiveness of this policy in encouraging hospitals to reduce readmissions. We introduce a single-hospital model to capture the dependence of a hospital’s readmission-reduction decision on various hospital characteristics. We derive comparative statics that predict how changes in hospital characteristics impact the hospital’s readmissionreduction decision. We then proceed to develop a game-theoretic model that captures the competition between hospitals introduced by the HRRP policy’s benchmarking mechanism. We provide bounds that apply to any equilibrium of the game and show that the comparative statics derived from the single-hospital model remain valid after the introduction of competition. Importantly, the comparison of the single-hospitals and multi-hospital models shows that, while the competition among hospitals often encourages more hospitals to reduce readmissions, it can only increase the number of “worst offenders,” which are hospitals that prefer paying penalties over reducing readmissions in any equilibrium. We calibrate our model with a dataset of hospitals in California which allows us to quantify the results and insights derived from the model. Last, we validate our model with recent hospitals’ performance data collected since the policy was implemented.	game theory;offset binary;operations research	Dennis J. Zhang;Itay Gurvich;Jan A. Van Mieghem;Eric Park;Robert S. Young;Mark V. Williams	2016	Management Science	10.1287/mnsc.2015.2280	public policy;game theory;regulation;actuarial science;economics;operations management;law;health care	ML	0.8488848921761155	-10.89165369889757	17878
e6ee3af3a926cdd530cf2560510a3e9b37a95ee8	agency-based asset pricing	asset pricing;managerial trading;moral hazard;risk-sharing	We study an infinite-horizon Lucas tree model where a manager is hired to tend to the trees and is compensated with a fraction of the trees’ output. The manager trades shares with investors and makes an effort that determines the distribution of the output. When the manager is less risk-averse than the investors, managerial trading smoothes output and results in a less volatile stock price and a lower risk premium; when the manager is more risk-averse, output and the stock price become more volatile and the risk premium is higher. Trading between the manager and investors acts as an indirect renegotiation mechanism that dynamically modulates the manager’s incentives, and in the meantime, allocates risk and return, but its effectiveness is limited when the market consists of dispersed small investors. * We would like to thank Debbie Lucas, Randall Morck, Larry Samuelson, Feng Gao and seminar participants at Columbia University, the IMF, and Georgia State University for helpful comments and suggestions. § School of Management, Yale University, New Haven, CT 06520-8200. E-mail: gary.gorton@yale.edu. §§  Tsinghua University, School of Economics and Management, Beijing 100084, China. E-mail: heping@sem.tsinghua.edu.cn. §§§ J. Mack Robinson School of Business, Georgia State University, Atlanta, GA 30303. Email: lxhuang@gsu.edu.com. Gary B. Gorton Yale University and NBER Ping He Tsinghua University §§	columbia (supercomputer);data haven;email;larry stockmeyer;modulation;pamela samuelson;randall beer;risk aversion;smoothing;software release life cycle	Gary B. Gorton;Ping He;Lixin Huang	2014	J. Economic Theory	10.1016/j.jet.2012.09.017	financial economics;principal–agent problem;capital asset pricing model;economics;finance;financial system;microeconomics	Web+IR	-3.3283841004961467	-6.010814237535651	17905
75d7517d030e159c5812614f631f1967f67a665b	visualization of the uk stock market based on complex networks for company's revenue forecast				Ziyi Wang;Jingti Han	2015		10.1007/978-3-319-16274-4_19	finance;financial system;stock	NLP	3.9823477322584804	-13.974184716149278	17931
2658167fcffe1afda63a041a39643856c168e4f3	trade credit insurance, capital constraint, and the behavior of manufacturers and banks	trade credit;trade credit insurance;loss aversion;capital constraint	The manufacturer who is a supplier of trade credit may face non-payment risk from customers and a capital shortage problem simultaneously. Trade credit insurance, as one of the most important risk management tools, has been widely used in companies’ daily operation. In this study, the manufacturer who allows customers to delay payment for goods already delivered purchases trade credit insurance to transfer and reduce non-payment risk and borrows money from a bank to accommodate the capital constraint problem. The Stackelberg game and loss-averse theory are used to establish a newsboy model including trade credit insurance, and the optimal insurance coverage and total sales of the manufacturer are thereby investigated. Subsequently, the interest rate decision of the bank under different risk-averse situations is also characterized. We find that the interest rate set by a loss-averse bank is equal to or greater than that given by a risk-neutral bank. The use of trade credit insurance can help the manufacturer expand sales and dramatically reduce its default risk. Both the bank and the manufacturer are better off due to the use of trade credit insurance, but contrary to what one might expect, the bank prefers giving a higher interest rate to the manufacturer when the premium rate is in a reasonable region, which indicates that the manufacturer cannot use the insurance to negotiate better financing terms.		Yongjian Li;Xueping Zhen;Xiaoqiang Cai	2016	Annals OR	10.1007/s10479-014-1602-x	bond insurance;bancassurance;credit reference;actuarial science;economics;credit risk;key person insurance;credit history;credit card interest;general insurance;auto insurance risk selection;export credit agency;commerce	Robotics	-0.10121635882371484	-6.378290636334487	18035
5eadf607eebbb77a740e2e65b6294cc8f07811a3	online algorithms for the general k-search problem	online algorithm;time series search;time series;journal;one way trading;lower bound;competitive ratio	Abstract   This paper investigates the general  k -search problem, in which a player is to sell totally  k  units of some asset within  n  periods, aiming at maximizing the total revenue. At each period, the player observes a quoted price which expires before the next period, and decides irrecoverably the amount of the asset to be sold at the price. We present a deterministic online algorithm and prove it optimal for the case where   k  ⩽  n  −  1  . For the other case where   k  ⩾  n  , we show by numerical illustration that the gap between the upper and the lower bound of competitive ratio is quite small for many situations.	online algorithm;search problem	Wenming Zhang;Yin-Feng Xu;Feifeng Zheng;Ming Liu	2011	Inf. Process. Lett.	10.1016/j.ipl.2011.04.008	competitive analysis;online algorithm;simulation;computer science;time series;mathematics;upper and lower bounds;operations research	DB	3.80703345078981	-1.9358943357519238	18061
6448ecca83d380c0e11b15881b4fddb22d39922b	coaching advice and adaptation	empirical study;optimal policy;autonomous agent;learning problems	Our research on coaching refers to one autonomous agent providing advice to another autonomous agent about how to act. In past work, we dealt with advice-receiving agents with fixed strategies, and we now consider agents which are learning. Further, we consider agents which have various limitations, with the hypothesis that if the coach adapts its advice to those limitations, more effective learning will result. In this work, we systematically explore the effect of various limitations upon the effectiveness of the coach’s advice. We state the two learning problems faced by the coach and the coached agents, and empirically study these problems in a predator-prey environment. The coach has access to optimal policies for the environment, and advises the predator on which actions to take. We experiment with limitations on the predator agent’s actions, the bandwidth between the coach and agent, and the memory size of the agent. We analyze the results which show that coaching can improve agent performance in the face of all these limitations.	advice (programming);autonomous agent;autonomous robot;bandwidth (signal processing);lotka–volterra equations;prey;q-learning;web crawler	Patrick Riley;Manuela M. Veloso	2003		10.1007/978-3-540-25940-4_17	simulation;computer science;knowledge management;artificial intelligence;autonomous agent;management science;empirical research	AI	20.600166166512874	-18.99663689092603	18115
0dd37f27b451fce51e1937e0151e81568d0a4e62	the hiring problem and lake wobegon strategies	distribution;68w40;comportement;decision making under uncertainty;concentracion;lake wobegon strategies;incertidumbre;martingale;fonction repartition;uncertainty;68q87;lognormal distribution;prise de decision;mediane;median;moyenne;secretary problem;funcion distribucion;distribution function;martingale analysis;conducta;promedio;modificacion;average;lognormal distributions;mediana;incertitude;behavior;58a25;toma decision;68r05;distribucion;concentration;modification	We introduce the hiring problem, in which a growing company continuously interviews and decides whether to hire applicants. This problem is similar in spirit but quite different from the well-studied secretary problem. Like the secretary problem, it captures fundamental aspects of decision making under uncertainty and has many possible applications. We analyze natural strategies of hiring above the current average, considering both the mean and the median averages; we call these Lake Wobegon strategies. Like the hiring problem itself, our strategies are intuitive, simple to describe, and amenable to mathematically and economically significant modifications. We demonstrate several intriguing behaviors of the two strategies. Specifically, we show dramatic differences between hiring above the mean and above the median. We also show that both strategies are intrinsically connected to the lognormal distribution, leading to only very weak concentration results, and the marked importance of the first few hires on the overall outcome.	decision theory;kaby lake;mathematical model;mathematical structure;secretary problem;whole earth 'lectronic link	Andrei Z. Broder;Adam Kirsch;Ravi Kumar;Michael Mitzenmacher;Eli Upfal;Sergei Vassilvitskii	2008		10.1137/07070629X	distribution;econometrics;uncertainty;martingale;distribution function;mathematics;secretary problem;concentration;median;algorithm;statistics;behavior	Theory	-4.1516169685366995	-0.4045030910201916	18130
cc2bcc656f53a0a2d7fb0f6cb8c1c9d13cfed247	a robust multiple ant colony system for the capacitated vehicle routing problem	ant colony optimisation;metaheuristics;ant colony optimization;vehicle routing problem;vehicle routing ant colony optimisation decision making uncertainty handling;vehicle routing;uncertainty handling;robust optimization;robust multiple ant colony system metaheuristic approach robust optimization uncertainty handling linear formulations travel costs decision makers transportation problems capacitated vehicle routing problem;metaheuristics robust optimization vehicle routing problem ant colony optimization;vehicles robustness uncertainty optimization routing legged locomotion;computer science;operational research	In transportation problems like the vehicle routing problem, the decision makers are increasingly adopting the idea that the problem data can be subject to uncertainty. The uncertainty can be encountered because of events that are not exactly predictable, like weather conditions, traffic jams, etc. In this paper, we study vehicle routing problem with uncertain travel costs. Then, to solve the problem, we propose a robust multiple ant colony system: a metaheuristic in which multiple ant colonies work in parallel to generate a collection of solutions with different levels of protection against the uncertainty. The uncertainty is handled by incorporating linear formulations from the field of robust optimization into the metaheuristic approach.	ant colony;experiment;mathematical optimization;metaheuristic;robust optimization;vehicle routing problem	Nihat Engin Toklu;Roberto Montemanni;Luca Maria Gambardella	2013	2013 IEEE International Conference on Systems, Man, and Cybernetics	10.1109/SMC.2013.322	mathematical optimization;ant colony optimization algorithms;parallel metaheuristic;computer science;vehicle routing problem;operations research;metaheuristic	Robotics	16.60872646439888	0.580788761669724	18137
4a79bd1647c97ed32401d7d8c47aa53fb5edfea9	environmental policy and market structure: a case of asymmetric firms	nash equilibrium;abatement tax;moral hazard;joint venture;synergy;endogenous market structure	This study seeks to analyze the impact of governmental regulations on the pollution level in a duopoly framework with endogenous market structure. We consider a dirty industry which involves two asymmetric firms, an MNC and a domestic firm, producing a homogenous product, where the MNC is so efficient that in the absence of a joint venture the MNC firm will act as a monopolist. We use a game theoretic framework to demonstrate that an increase in abatement cost, arising out of stricter government regulations, could trigger a regime-switch from monopoly to joint venture (or from joint venture to monopoly), and consequently may increase pollution. Turning to the welfare analysis, we find that the constrained first best outcome always involves joint venture formation. Given a market structure, we find that the optimal emission tax is always less than the marginal social damage. We then demonstrate that if the synergistic effect (SE) is large and the industry is neither very dirty, nor very clean, then the constrained first best outcome can be implemented by setting the abatement tax appropriately. Otherwise the constrained first best outcome cannot be implemented.		Indrani Roy Chowdhury	2015	IGTR	10.1142/S0219198915400198	industrial organization;synergy;economics;microeconomics;mathematical economics;market economy;nash equilibrium	ECom	-1.3840840700210868	-7.550217465469108	18201
bb00e3425589d7616269fc192d60c855dc003bd9	equilibrium joining strategies in m/m/1 queues with working vacation and vacation interruptions	queueing;stationary distribution;working vacation;期刊论文;vacation interruptions;equilibrium strategies	We study the equilibrium joining strategies for customers in an M/M/1 queue with working vacations and vacation interruptions. The service rate switches between a low and a high value depending on system dynamics. The server will take a multiple working vacation when the system is empty, during which a low service rate is provided to the arriving customers if any. Upon completion of the first customer’s service, given that the system is not empty, the working vacation will be terminated which means the server comes back and serves the following customers with a higher service rate. Otherwise, if the system is found empty upon completion of the first service, the server will continue his working vacation. Arriving customers may or may not know the state of the server and/or the number of the customers upon arrival, but they have to decide whether to enter the system or balk based on a linear reward-cost structure. We investigate customer behavior according to different levels of information regarding the system state. The equilibrium strategies for the customers are derived and the stationary behavior of the system under these strategies are analyzed. Finally, the effect of different levels of information on equilibrium thresholds and equilibrium entrance probabilities is illustrated by several numerical examples.		Kaili Li;Jinting Wang;Yanjia Ren;Jingwei Chang	2016	RAIRO - Operations Research	10.1051/ro/2015027	stationary distribution;mathematics;queueing theory;statistics	Theory	2.219304072850081	0.2629332854488569	18226
5b45371ab771afff4e5024744ec23c4e4159787b	beating a moving target: optimal portfolio strategies for outperforming a stochastic benchmark	benchmarking;continuous time;stochastic process;expected utility maximization;diffusions;utility function;optimal policy;portfolio theory;active portfolio management;growth optimal policy;portfolio management;stochastic control;object relational;optimal portfolio;constant proportions	We consider the portfolio problem in continuous-time where the objective of the investor or money manager is to exceed the performance of a given stochastic benchmark, as is often the case in institutional money management. The benchmark is driven by a stochastic process that need not be perfectly correlated with the investment opportunities, and so the market is in a sense incomplete. We first solve a variety of investment problems related to the achievement of goals: for example, we find the portfolio strategy that maximizes the probability that the return of the investor’s portfolio beats the return of the benchmark by a given percentage without ever going below it by another predetermined percentage. We also consider objectives related to the minimization of the expected time until the investor beats the benchmark. We show that there are two cases to consider, depending upon the relative favorability of the benchmark to the investment opportunity the investor faces. The problem of maximizing the expected discounted reward of outperforming the benchmark, as well as minimizing the discounted penalty paid upon being outperformed by the benchmark is also discussed. We then solve a more standard expected utility maximization problem which allows new connections to be made between some specific utility functions and the nonstandard goal problems treated here.	atari portfolio;average-case complexity;benchmark (computing);entropy maximization;expected utility hypothesis;goal programming;stochastic process	Sid Browne	1999	Finance and Stochastics	10.1007/s007800050063	financial economics;post-modern portfolio theory;stochastic process;merton's portfolio problem;actuarial science;stochastic control;economics;replicating portfolio;modern portfolio theory;finance;portfolio optimization;active management;welfare economics;statistics;benchmarking	ML	1.4935271763980784	-3.561828565759842	18332
7708570c9aba9f32c32f4d184d1561272a267c93	a fuzzy-cautious owa approach with evidential reasoning	mcdm;payoff matrix;uncertainty;indexes;case based reasoning;cognition;fuzzy set theory;evidential reasoning;vectors	Multi-criteria decision making (MCDM) is to make decisions in the presence of multiple criteria. To make a decision in the framework of MCDM under uncertainty, a novel fuzzy - Cautious OWA with evidential reasoning (FCOWA-ER) approach is proposed in this paper. Payoff matrix and belief functions of states of nature are used to generate the expected payoffs, based on which, two Fuzzy Membership Functions (FMFs) representing optimistic and pessimistic attitude, respectively can be obtained. Two basic belief assignments (bba's) are then generated from the two FMFs. By evidence combination, a combined bba is obtained, which can be used to make the decision. There is no problem of weights selection in FCOWA-ER as in traditional OWA. When compared with other evidential reasoning-based OWA approaches such as COWA-ER, FCOWA-ER has lower computational cost and clearer physical meaning. Some experiments and related analyses are provided to justify our proposed FCOWA-ER.	algorithmic efficiency;computational complexity theory;entity–relationship model;erdős–rényi model;experiment;performance;systems design	Deqiang Han;Jean Dezert;Jean-Marc Tacnet;Chongzhao Han	2012	2012 15th International Conference on Information Fusion		artificial intelligence;machine learning;data mining;mathematics;evidential reasoning approach	Robotics	-4.165718990205606	-20.808776227095098	18417
6b2f098ef6a16dc7c7935392ce65351fbeef7340	quantifying the impact of microgrid location and behavior on transmission network congestion		This work presents a preliminary analysis considering impact of a grid-connected microgrid on network transmission of the power system. The locational marginal prices of the power system are used to strategically place the microgrid to avoid congestion problems. In addition, a Monte Carlo simulation approach is implemented to confirm that network congestion can be attenuated if appropriate price-based signals are set to define the import and export dynamic between the two systems.	best, worst and average case;dynamic dispatch;loss function;marginal model;mega man network transmission;microgrid;monte carlo method;network congestion;piecewise linear continuation;simulation	Jialin Liu;Gabriela Martinez;C. Lindsay Anderson	2016	2016 Winter Simulation Conference (WSC)		control engineering;simulation;engineering;mathematics;presentation of a group	HPC	3.863250496180889	3.2756315727334475	18437
4e32ba65a71838b0882daf74808ad92cfec142ee	convolutional neural networks for energy time series forecasting		We investigate the application of convolutional neural networks for energy time series forecasting. In particular, we consider predicting the photovoltaic solar power and electricity load for the next day, from previous solar power and electricity loads. We compare the performance of convolutional neural networks with multilayer perceptron neural networks, which are one of the most popular and successful methods used for these tasks, and also with long short-term memory recurrent neural networks and a persistence baseline. The evaluation is conducted using four solar and electricity time series from three countries. Our results showed that the convolutional and multilayer perceptron neural networks performed similarly in terms of accuracy and training time, and outperformed the other models. This highlights the potential of convolutional neural networks for energy time series forecasting.	artificial neural network;baseline (configuration management);convolutional neural network;long short-term memory;memory-level parallelism;multilayer perceptron;persistence (computer science);quad flat no-leads package;recurrent neural network;time series	Irena Koprinska;Ilyse Stempler;Zheng Wang	2018	2018 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2018.8489399	task analysis;time series;convolutional neural network;machine learning;artificial intelligence;artificial neural network;recurrent neural network;solar power;pattern recognition;computer science;photovoltaic system;multilayer perceptron	ML	9.048400998742542	-19.215420263641025	18451
eba59cd35c22bc9ec9eb450a631f27edb6cc0e00	a regression approach for developing mathematical models for management and operations training simulators	modelo dinamico;modelizacion;evaluation performance;performance evaluation;modele mathematique;gestion;resource allocation;evaluacion prestacion;dynamic model;linear regression;mathematical functions;modelo matematico;simulator;modelisation;funcion matematica;analisis regresion;simulador;movimiento traslacion;modele dynamique;regresion lineal;modele simulation;mathematical model;simulateur;mathematical function;management simulator;analyse regression;mouvement translation;regression analysis;fonction mathematique;asignacion recurso;modelo simulacion;allocation ressource;translation motion;management;modeling;simulation model;regression lineaire	This paper presents a new linear regression- based approach for developing mathematical models suitable for use in management and operations training simulators. This novel approach modifies static cost-independent regression models into dynamic accountable simulator models that translate input resource decisions into output performance feedback. Entirely new techniques have been developed for accounting for time, determining resource allocation cost rates, translating resource allo cation into technical performance, and incor porating probabilistic effects. These techniques offer both empirical and probabilistic advan tages over existing methods.		Christopher A. Chung	2000	Simulation	10.1177/003754970007400502	simulation;computer science;engineering;artificial intelligence;operations research;function	AI	5.577823144602074	-9.672741571219587	18456
64c0404b77320e21a4de50a4afcd1d8ded58956d	optimal investment with transaction costs and stochastic volatility part i: infinite horizon		In this companion paper to “Optimal Investment with Transaction Costs and Stochastic Volatility Part I: Infinite Horizon”, we give an accuracy proof for the finite time optimal investment and consumption problem under fast mean-reverting stochastic volatility of a joint asymptotic expansion in a time scale parameter and the small transaction cost. AMS subject classification 91G80, 60H30. JEL subject classification G11.	horizon effect;verilog-ams;volatility	Maxim Bichuch;Ronnie Sircar	2017	SIAM J. Control and Optimization	10.1137/140983859	financial economics;implied volatility;merton's portfolio problem;economics;volatility smile;microeconomics;mathematical economics;stochastic volatility	AI	1.2874053680383961	-2.3687517723334732	18494
7c94407d25a21845d62a4ed7b1df7ddcf249a0e1	on the limits of black-box reductions in mechanism design	performance guarantee;social welfare;approximate algorithm;explicit knowledge;makespan;algorithms;mechanism design;parameter optimization	We consider the problem of converting an arbitrary approximation algorithm for a single-parameter optimization problem into a computationally efficient truthful mechanism. We ask for reductions that are black-box, meaning that they require only oracle access to the given algorithm and in particular do not require explicit knowledge of the problem constraints. Such a reduction is known to be possible, for example, for the social welfare objective when the goal is to achieve Bayesian truthfulness and preserve social welfare in expectation. We show that a black-box reduction for the social welfare objective is not possible if the resulting mechanism is required to be truthful in expectation and to preserve the worst-case approximation ratio of the algorithm to within a subpolynomial factor. Further, we prove that for other objectives such as makespan, no black-box reduction is possible even if we only require Bayesian truthfulness and an average-case performance guarantee.	algorithmic efficiency;approximation algorithm;best, worst and average case;black box;hierarchical and recursive queries in sql;makespan;mathematical optimization;optimization problem	Shuchi Chawla;Nicole Immorlica;Brendan Lucier	2012		10.1145/2213977.2214019	mechanism design;job shop scheduling;mathematical optimization;combinatorics;computer science;explicit knowledge;social welfare;mathematics;mathematical economics;algorithm	Theory	-2.16079267707918	-0.5110835253649947	18516
a6ac649600cc352ba56d6f4588bdc1407384f7de	simultaneous learning of perceptions and actions in autonomous robots		This paper presents a new learning approach for autonomous robots. Our system will learn simultaneously the perception – the set of states relevant to the task – and the action to execute on each state for the task-robotenvironment triad. The objective is to solve two problems that are found when learning new tasks with robots: interpretability of the learning process and number of parameters; and the complex design of the state space. The former was solved using a new reinforcement learning algorithm that tries to maximize the time before failure in order to obtain a control policy suitable to the desired behavior. The state representation will be created dynamically, starting with an empty state space and adding new states as the robot finds them, this makes unnecessary the creation of a predefined state representation, which is a tedious task.	algorithm;autonomous robot;regular expression;reinforcement learning;state space	Pablo Quintía;Roberto Iglesias;Miguel A. Rodríguez;Carlos V. Regueiro	2010			control engineering;social robot;robot;simulation;engineering;perception;robot control;computer vision;autonomous robot;mobile robot;artificial intelligence	Robotics	19.425885083667108	-21.423481735329016	18522
9b8fddacf4b8f0437f82f480151ae8544e6a7e32	short-term load forecasting using bayesian neural networks learned by hybrid monte carlo algorithm	neural networks;hybrid monte carlo;short term load forecasting;posterior distribution;hamilton dynamical system;cauchy distribution	This paper presents a short term load forecasting model based on Bayesian neural network (shorted as BNN) learned by the Hybrid Monte Carlo (shorted as HMC) algorithm. The weight vector parameter of the Bayesian neural network is a multi-dimensional random variable. In learning process, the Bayesian neural network is considered as a special Hamiltonian dynamical system, and the weights vector as the system position variable. The HMC algorithm is used to learn the weight vector parameter with respect to Normal prior distribution and Cauchy prior distribution, respectively. The Bayesian neural networks learned by Laplace algorithm and HMC algorithm and the artificial neural network (ANN) learned by the BP algorithm were used to forecast the hourly load of 25 days of April (Spring), osterior distribution auchy distribution amilton dynamical system hort term load forecasting August (Summer), October (Autumn) and January (Winter), respectively. The roots mean squared error (RMSE) and the mean absolute percent errors (MAPE) were used to measured the forecasting performance. The experimental result shows that the BNNs learned by HMC algorithm have far better performance than the BNN learned by Laplace algorithm and the neural network learned BP algorithm and the BNN learned by HMC has powerful generalizing capability, it can welly solve the overfitting problem.	artificial neural network;backpropagation;bayesian network;dynamical system;hybrid memory cube;hybrid monte carlo;least mean squares filter;mean squared error;monte carlo algorithm;monte carlo method;overfitting	Dongxiao Niu;Hui-feng Shi;Desheng Dash Wu	2012	Appl. Soft Comput.	10.1016/j.asoc.2011.07.001	econometrics;mathematical optimization;hybrid monte carlo;computer science;artificial intelligence;machine learning;cauchy distribution;posterior probability;artificial neural network;statistics	ML	11.048458988355101	-22.473786367052725	18560
60a49ef6ef5516cc30a1fa46a44f73bb70800f70	value iteration with options and state aggregation.		This paper presents a way of solving Markov Decision Processes that combines state abstraction and temporal abstraction. Specifically, we combine state aggregation with the options framework and demonstrate that they work well together and indeed it is only after one combines the two that the full benefit of each is realized. We introduce a hierarchical value iteration algorithm where we first coarsely solve subgoals and then use these approximate solutions to exactly solve the MDP. This algorithm solved several problems faster than vanilla value iteration.	approximation algorithm;iteration;iterative method;markov chain;markov decision process;uml state machine	Kamil Ciosek;David Silver	2015	CoRR		computer science;artificial intelligence;machine learning;abstraction;mathematical optimization;markov decision process	AI	21.83185964070535	-16.690819469142927	18584
082e96d2eb080108b3bd63dbcd45cad211a6e9f6	the effect of landscape funnels in qaplib instances		The effectiveness of common metaheuristics on combinatorial optimisation problems can be limited by certain characteristics of the fitness landscape. We use the local optima network model to compress the 'inherent structure' of a problem space into a network whose structure relates to the empirical hardness of the underlying landscape. Monotonic sequences axe used on the local optima networks of a benchmark set of QAP instances (QAPLIB) to expose landscape funnels. The results suggest links between features of these structures and lowered metaheuristic performance.		Sarah L. Thomson;Gabriela Ochoa;Fabio Daolio;Nadarajen Veerapen	2017		10.1145/3067695.3082512	artificial intelligence;mathematical optimization;network model;fitness landscape;local optimum;machine learning;fold (higher-order function);monotonic function;quadratic assignment problem;metaheuristic;mathematics	ML	23.62779994782638	-7.810253780689478	18607
7119f9d9d6d20d9217e0fb9ce42238f14bf10c6e	using chaos and complexity theory and design of experiments to forecast stock prices: tools of artificial intelligence	design of experiment;artificial intelligent		artificial intelligence;chaos;design of experiments	Jason D'Cruz;Carmo D'Cruz;Muzaffar Shaikh	2005			machine learning;design of experiments;computer science;artificial intelligence	AI	6.8070026257399086	-19.221649865865817	18666
b6dde1cc617baa2fd1ee582672305bbd933a8085	self-organized criticality of individual companies: an empirical study	individual company;empirical study;fluctuations statistics power system economics power generation economics stock markets statistical distributions power system modeling statistical analysis fractals econophysics;stock market;multifractality self organized criticality stock market power law volatility statistics;self organized criticality;multifractality;stock markets;statistical distributions;classical sand pile model stock market self organized criticality individual company price volatility statistical distribution;stock markets statistical distributions;interactive system;classical sand pile model;price volatility statistical distribution;stock market volatility;power law;cumulant;volatility statistics;price volatility;self organized critical	"""The stock market is a typically complex and self-interacting system that in many aspects it shows the characteristic of self-organized criticality (SOC). In the present work we mainly investigate the SOC properties of individual companies. Price volatility can be served as an analogy of """"avalanche"""" in the classical sand-pile model and different volatility statistics have different power law behaviors. We mainly analyze the distributions of the return and its ramificate statistics, both theoretical and experimental. The empirical study of the Chinese 5-min trading data (from 2005/7/8 to 2006/2/8) shows that some volatility statistics did show significant power law behaviors while some other volatility statistics show asymptotic power law behaviors and do not accord with the classical SOC model, which might be SOC affected by noise, sub-critical or even chaos. Furthermore, we find that cumulative volatility statistics are much more significant than the original statistics and the high-order volatilty statistics present an inherent multifractality."""	interaction;multifractal system;ramification problem;self-organization;self-organized criticality;structured criticality;volatility	Bin Rao;Dong-yun Yi;Cheng-li Zhao	2007	Third International Conference on Natural Computation (ICNC 2007)	10.1109/ICNC.2007.654	industrial organization;financial economics;implied volatility;economics	ML	2.433732724547277	-12.371110372969744	18679
c004341d81d4627a64823c9d68b1b8e69a69f403	asset-liability management under benchmark and mean-variance criteria in a jump diffusion market	continuous time;hamilton jacobi bellman equation;closed form solution;jump diffusion market;mean variance;yan zeng zhongfei li 基准模型 资产 均值 方差 管理 负债 市场 扩散 asset liability management under benchmark and mean variance criteria in a jump diffusion market;duality theory;asset liability management;jump diffusion;benchmark and mean variance models;stochastic dynamic programming	This paper investigates continuous-time asset-liability management under benchmark and mean-variance criteria in a jump diffusion market. Specifically, the authors consider one risk-free asset, one risky asset and one liability, where the risky asset’s price is governed by an exponential Lévy process, the liability evolves according to a Lévy process, and there exists a correlation between the risky asset and the liability. Two models are established. One is the benchmark model and the other is the mean-variance model. The benchmark model is solved by employing the stochastic dynamic programming and its results are extended to the mean-variance model by adopting the duality theory. Closed-form solutions of the two models are derived.	bellman equation;benchmark (computing);dynamic programming;stochastic programming;time complexity	Yan Zeng;Zhongfei Li	2011	J. Systems Science & Complexity	10.1007/s11424-011-9105-1	stochastic programming;closed-form expression;mathematical optimization;hamilton–jacobi–bellman equation;duality;actuarial science;mathematics;mathematical economics	ML	1.7830758903627109	-2.435437457843713	18715
041e489fa3c54783ba42abfd7b8ae5d6e566bd44	multi-item auctions defying intuition?	multidimensional mechanism design;auctions	The best way to sell n items to a buyer who values each of them independently and uniformly randomly in [c, c+1] is to bundle them together, as long as c is large enough. Still, for any c, the grand bundling mechanism is never optimal for large enough n, despite the sharp concentration of the buyer's total value for the items as n grows. Optimal multi-item mechanisms are rife with unintuitive properties, making multi-item generalizations of Myerson's celebrated mechanism a daunting task. We survey recent work on the structure and computational complexity of revenue-optimal multi-item mechanisms, providing structural as well as algorithmic generalizations of Myerson's result to multi-item settings.	computational complexity theory;rife;randomness	Constantinos Daskalakis	2015	SIGecom Exchanges	10.1145/2845926.2845928	mathematical optimization;economics;operations management;common value auction;mathematics;microeconomics;mathematical economics;welfare economics	ECom	-2.622404613136493	-1.3007306356517692	18726
fcee703e85d513365b4a27d6fb1bcaf57d320518	negative libor rates in the swap market model	forward swap rates;market model;91b70;support theorem;forward libor rates;60h10	We apply Stroock and Varadhan’s support theorem to show that there is a positive probability that within the Swap Market Model the implied Libor rates become negative in finite time.	quantum gate	Mark H. A. Davis;Vicente Mataix-Pastor	2007	Finance and Stochastics	10.1007/s00780-006-0032-2	libor market model;financial economics;basis swap;conditional variance swap;variance swap;actuarial science;economics;finance;yield curve	Theory	0.6235957989482959	-2.490462608556861	18757
17256b2eb349ff1b546654c085ca7417161189f5	clustering univariate time series into stationary and non-stationary	autocorrelation function;clustering univariate time series;pattern clustering;time measurement;biological system modeling;nonlinear;square function;time series analysis correlation biological system modeling time measurement predictive models robustness economics;time series;nonlinear functions;moving average;nonlinear function clustering univariate time series stationarity analysis moving average model autocorrelation function square function logarithmic function;logarithmic function;nonlinear function;time series analysis;moving average processes;time series moving average processes nonlinear functions pattern clustering;robustness;predictive models;automatic mechanism time series stationarity nonlinear autocorrelation function;economics;moving average model;correlation;stationarity;stationarity analysis;automatic mechanism	Lots of researchers have paid attention to time series clustering in recent years. This paper studies the stationarity analysis for autoregressive and moving average models of time series with clustering, firstly presents a set of nonlinear functions, or rather the square function along with logarithmic function to better autocorrelation function, secondly clusters time series into stationary and non-stationary with Clustering, finally an automatic mechanism for prejudging the stationarity of time series is presented. The proposed approach has been tested using two datasets, one natural and one synthetic, and is shown to yield useful and robust result of stationarity analysis.	acf;autocorrelation;autoregressive model;cluster analysis;cross-correlation;data mining;nonlinear system;stationary process;synthetic intelligence;time series	Heshan Guan;Shuliang Zou;Mengya Liu;Tieli Wang	2010	2010 Seventh International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2010.5569228	correlation clustering;econometrics;mathematical optimization;nonlinear system;time series;moving-average model;mathematics;order of integration;statistics	ML	7.183492671749072	-17.75364733640184	18800
94126c6eb23cbc58799eabed7f7dfc0a6af3491e	association rules applied to credit card fraud detection	fraud prevention;association rules;data mining;association rule;fraud detection;credit cards;credit card fraud	Association rules are considered to be the best studied models for data mining. In this article, we propose their use in order to extract knowledge so that normal behavior patterns may be obtained in unlawful transactions from transactional credit card databases in order to detect and prevent fraud. The proposed methodology has been applied on data about credit card fraud in some of the most important retail companies in Chile. 2008 Elsevier Ltd. All rights reserved.	credit card fraud;data mining;database	Daniel Sánchez;M. Amparo Vila;L. Cerda;José-María Serrano	2009	Expert Syst. Appl.	10.1016/j.eswa.2008.02.001	association rule learning;computer science;credit card interest;data mining;chargeback;computer security	DB	3.547035275359731	-17.816422177547093	18803
e7b5647df15cd7b0d07d1471081e523da1a28996	optimization of injection molding process for car fender in consideration of energy efficiency and product quality		Energy efficiency is an essential consideration in sustainable manufacturing. This study presents the car fender-based injection molding process optimization that aims to resolve the trade-off between energy consumption and product quality at the same time in which process parameters are optimized variables. The process is specially optimized by applying response surface methodology and using nondominated sorting genetic algorithm II (NSGA II) in order to resolve multi-object optimization problems. To reduce computational cost and time in the problem-solving procedure, the combination of CAE-integration tools is employed. Based on the Pareto diagram, an appropriate solution is derived out to obtain optimal parameters. The optimization results show that the proposed approach can help effectively engineers in identifying optimal process parameters and achieving competitive advantages of energy consumption and product quality. In addition, the engineering analysis that can be employed to conduct holistic optimization of the injection molding process in order to increase energy efficiency and product quality was also mentioned in this paper.	computation;computational complexity theory;computer cooling;diagram;evolutionary algorithm;genetic algorithm;holism;mathematical optimization;metamodeling;pareto efficiency;problem solving;process optimization;response surface methodology;sorting;x/open	Hong-Seok Park;Trung Thanh Nguyen	2014	J. Computational Design and Engineering	10.7315/JCDE.2014.025	engineering;management science;engineering drawing	AI	16.751078346988912	-4.436715642001022	18843
b80b707e3cec12aa084c3b863bd7f5d9abc89d07	uncertainty propagation in wildland fire behaviour modelling	uncertainty propagation;wildland fire	Rothermel’s model is the most widely used  re behaviour model in wildland  re research and management. It is a complex model that considers 17 input variables describing fuel type, fuel moisture, terrain and wind. Uncertainties in the input variables can have a substantial impact on the resulting errors and have to be considered, especially when the results are used in spatial decision making. In this paper it is shown that the analysis of uncertainty propagation can be carried out with the Taylor series method. This method is computationally cheaper than Monte Carlo and oŒers easy-to-use, preliminary sensitivity estimations.	monte carlo method;propagation of uncertainty;software propagation	Andreas Bachmann;Britta Allgöwer	2002	International Journal of Geographical Information Science	10.1080/13658810110099080	simulation;computer science;propagation of uncertainty;wildfire modeling;statistics	Vision	10.424189652811686	-16.14041607054542	18878
ad53cbb776ee0d9f51bba01babc2d8e7855d8546	the analytic hierarchy process - an exposition	decision analysis multiple criteria	This exposition on the Analytic Hierarchy Process (AHP) has the following objectives: (1) to discuss why AHP is a general methodology for a wide variety of decision and other applications, (2) to present brief descriptions of successful applications of the AHP, and (3) to elaborate on academic discourses relevant to the efficacy and applicability of the AHP vis-a-vis competing methodologies. We discuss the three primary functions of the AHP: structuring complexity, measurement on a ratio scale, and synthesis, as well as the principles and axioms underlying these functions. Two detailed applications are presented in a linked document athttp://mdm.gwu.edu/FormanGass.pdf.	bottom-up parsing;ct scan;decision analysis;fo (complexity);iteration;level of measurement;nl (complexity);ordinal data;programming paradigm;relevance;vertex-transitive graph	Ernest H. Forman;Saul I. Gass	2001	Operations Research	10.1287/opre.49.4.469.11231	operations management;mathematics;management science	Vision	-3.492220109895014	-16.385153502631965	18896
36ca3aa2ba3aa6a7951b93f35111a6df233b8e72	letters to the editor: three letters on merging		"""Dear Editor: It is important to clear up two commonly misunderstood points about tape merging. In the """"glossary of Sorting and following is stated regarding the polyphase merge: """"The effective power of the merge varies between T-1 and T/2, depending on the amount of input data and the number of strings."""" For the cascade merge, we read: """"The effective power of the merge varies between T-1 and T/2, but in all cases is less than the power of the Polyphase Merge."""" Both of these statements are greatly in error, and they must have been made without any logical foundation , The writer of this letter has carried out extensive theoretical calculations regarding the """"effective power"""" of these two merge patterns, and computer simulation of the algorithms reveals perfect agreement with the theory. The details of the computations are too lengthy to be included here (they will eventually be published elsewhere), but since they have been confirmed by simulation there is no doubt as to their validity. Suppose we have a cascade merge process [B. K. Betz and W. C. Carter, ACM Nat'l Meeting, ]959] which uses at most a k-way merge. If there are S initial strings produced by the first pass, the total number of passes over the data (including this first pass) is given very closely by the formula uk In S + vk, where uk and vk are constants; hence the effective power of the merge is S I"""" 1/(uk In S + vk-1) and as S becomes large this is given approximately by e 1/~k. Values of uk and vk are tabulated below; the effective power of the merge for the cascade algorithm has the approximate form 2k/Tr + .321 (where the constant 2/~-is known from theoretical considerations, while the constant .32l is an empirical estimate). This formula is quite accurate for k ~ 3. In the polyphase merge JR. L. Gilstad, EJCC, 1960] with the same assumptions, the total number of times each string is read during the process is given approximately by p~S in S q-qkS, hence the effective number of passes is pk in S + %. As above, the effective power is e ~/pk, as S becomes large; and in the case of the polyphase merge this quantity rapidly approaches 4 as k gets larger. (The effective number of passes is quite accurately given by the formula log4 (S/(k-1)) + 2, 10 …"""	acm international collegiate programming contest;approximation algorithm;cascade algorithm;cascade merge sort;computation;computer simulation;glossary;merge algorithm;polyphase matrix;polyphase quadrature filter;sorting	Donald E. Knuth;Martin A. Goetz	1963	Commun. ACM	10.1145/367651.367654	programming language;merge (version control);computer science	Theory	14.812460288371826	-9.955450390095706	18915
b6b6d3117e672183d84aabccf1adaa8bb716e42a	a new approach with orthogonal array for global optimization in design of experiments	factorial;objective function;design of experiments;orthogonal array;global optimization;taguchi approach;design of experiment	In this paper, we develop an approach that determines the overall best parameter setting in design of experiments. The approach starts with successive orthogonal array experiments and ends with a full factorial experiment. The setup for the next orthogonal-array experiment is obtained from the previous one by either fixing a factor at a given level or by reducing the number of levels considered for all currently non-fixed factors. We illustrate this method using an industrial problem with seven parameters, each with three levels. In previous work, the full factorial of 37 = 2,187 points was evaluated and the best point was found. With the new method, we found the same point using 3% of these evaluations. As a further comparison, we obtained the optimum using a traditional Taguchi approach, and found it corresponded to the 366th of the 2,187 possibilities when sorted by the objective function. We conclude the proposed approach would provide an accurate, fast, and economic tool for optimization using design of experiments.	design of experiments;experiment;global optimization;mathematical optimization	Hsin-Chuan Kuo;Jeun-Len Wu	2009	J. Global Optimization	10.1007/s10898-008-9357-z	econometrics;mathematical optimization;fractional factorial design;mathematics;design of experiments;global optimization	EDA	17.960527663431844	-5.185425348163835	18919
d969c92720b1b55cc3768ebcfa53e115018a8d88	optimal signal-setting for road network with maximum capacity	a min max bilevel program;maximum capacity;signal controlled road network;optimization;equilibrium constraints	A signal-controlled road network with maximum capacity is considered while route choice of users is taken into account. This problem can be formulated as an optimization problem by taking user equilibrium as a constraint. A min–max bilevel program is formulated for a signal-setting problem. A new hybrid search heuristic is proposed to exploit the maximum capacity of road network using delay-minimizing signal settings. For a road network with maximum capacity, a tractable computation scheme is presented to effectively determine a delay-minimizing signal-setting. Numerical computations are performed at two benchmark signal-controlled road networks. The proposed heuristic has been demonstrated successfully to solve the min–max signal-setting problem. As compared to conventional alternatives, the proposed heuristic has been empirically demonstrated highly effective in terms of delay rate reduction. 2014 Elsevier Inc. All rights reserved.	algorithmic efficiency;approximation algorithm;benchmark (computing);bilevel optimization;cobham's thesis;computation;conjugate gradient method;control system;experiment;heuristic;interaction;local optimum;local search (optimization);mathematical optimization;maxima and minima;national supercomputer centre in sweden;numerical analysis;numerical linear algebra;optimization problem;time complexity;time-invariant system	Suh-Wen Chiou	2014	Inf. Sci.	10.1016/j.ins.2014.03.032	mathematical optimization;simulation;computer science	AI	16.057587801199592	-0.7531261067891437	18969
555ee77234e3cc53492358a040fde52af6b9b0c5	propagation of financial shocks: the case of venture capital	contagion;intermediation;venture capital;internet;lock in;technology bubble	In this paper, I investigate whether venture-backed companies can be negatively affected when others that share the same investor perform poorly. To this end, I examine the impact of the collapse of the technology bubble on non-information-technology (non-IT) companies held alongside internet companies in venture portfolios. Using a difference-in-differences framework, I find that the end of the bubble was associated with a 26% larger decline in the probability of raising continuation financing for these non-IT companies in comparison to others. This does not appear to be driven by unobservable company characteristics such as IT-relatedness; for the same portfolio company receiving capital from multiple venture firms, investors with greater internet exposure were significantly less likely to continue participating in follow-on rounds. Overall, these results suggest that structuring intermediaries to be less fragile, as venture intermediaries have been, does not eliminate the possibility of contagion among portfolio companies. JEL Classification: G11, G24	continuation;difference in differences;dot-com bubble;software propagation	Richard R. Townsend	2015	Management Science	10.1287/mnsc.2014.2110	the internet;actuarial science;social venture capital;economics;venture capital;marketing;finance;microeconomics;management;law;commerce	Metrics	-2.032424331789635	-8.915226670365888	19026
ab4e84a71d3f6b91df1f56b14598b71ee28501ab	editorial message: special track on applications of evolutionary computation	domain specific modeling;real time;biological evolution;reproductive success;evolutionary algorithm;quality of service;evolutionary computing	The mechanisms that drive biological evolution are reproduction with variation, reproductive success proportional to fitness, and repetition. In any system where these mechanisms are active, populations of objects-biological or artificial-will evolve. Among such systems are evolutionary algorithms, which apply techniques inspired by biological evolution to search for good solutions to computationally difficult problems.	evolution;evolutionary algorithm;evolutionary computation;population	Bryant A. Julstrom	2005		10.1145/1066677.1066886	evolutionary programming;evolutionary music;simulation;quality of service;interactive evolutionary computation;human-based evolutionary computation;computer science;artificial intelligence;machine learning;evolutionary algorithm;evolutionary acquisition of neural topologies;evolution strategy;reproductive success;evolutionary computation	Theory	24.31668370026966	-7.971735832376254	19072
d7b808fe8969f853ad9934e1980f81330bc7d075	decision rule of assignable causes removal under an spc-epc integration system	integrable system;decision rule	In order to obtain the advantages of eliminating both assignable causes and common causes, an effective method to integrate Statistical Process Control (SPC) and Engineering Process Control (EPC) under the continuous process is rousing increasing interest. This study found that an EPC feedback compensation mechanism affects SPC out-of-control detection and degrades the output quality once suddenly assignable causes are removed. Based on this finding, we propose a novel SPC control scheme that can effectively detect and identify the disturbance types. Further, we introduce a cost-oriented decision rule for removing assignable causes in an SPC and EPC integration system. Based on this decision rule, a more accurate selection methodology of SPC charts' control limits is also proposed.	electronic product code	Chung-Hsing Huang;Yi-Ni Lin	2002	Int. J. Systems Science	10.1080/00207720210167005	integrable system;engineering;artificial intelligence;decision rule;control theory;mathematics;statistics	Logic	13.992769689684566	-13.77939741953615	19082
3c406317ce538b5713fad3f0625c122ba2475993	universal learning of repeated matrix games	learning algorithm;artificial intelligent;conference paper;prediction with expert advice;computer sciences;science learning;bayesian learning	We study and compare the learning dynamics of two universal learning algorithms, one based on Bayesian learning and the other on prediction with expert advice. Both approaches have strong asymptotic performance guarantees. When confronted with the task of finding good long-term strategies in repeated 2 × 2 matrix games, they behave quite differently. We consider the case where the learning algorithms are not even informed about the game they are playing.	algorithm;machine learning	Jan Poland;Marcus Hutter	2005	CoRR		semi-supervised learning;unsupervised learning;robot learning;multi-task learning;instance-based learning;error-driven learning;algorithmic learning theory;simulation;wake-sleep algorithm;computer science;artificial intelligence;online machine learning;machine learning;inductive transfer;mathematics;learning classifier system;stability;competitive learning;computational learning theory;bayesian inference;active learning;statistics;generalization error	ML	20.911559196829803	-19.211445224265937	19083
cb0389daf7069637f61ae26ee141605dbfef65bb	a novel method of fleet deployment based on route risk evaluation	fleet deployment;firefly algorithm;route risk evaluation;ocean environment	In order to take full advantage of resources for navigation and reduce risks, this paper proposes a novel method of fleet deployment based on route risk evaluation. Dempster-Shafer(D-S) evidence theory is applied in order to construct the route risk evaluation model, considering the main environmental factors affecting navigational safety. A novel optimization model of fleet deployment is thus established and the firefly algorithm used to solve the problem. The experimental results show the effectiveness and practicality of the new model and algorithm.	software deployment	Yuxin Zhao;Renfeng Jia;Na Jin;Yongxu He	2016	Inf. Sci.	10.1016/j.ins.2016.08.065	simulation;computer science;artificial intelligence;firefly algorithm	DB	13.695444050849872	-8.019452880579935	19121
96041c48113da8b5b7c3a27ed2c443ec0195b55f	a multivariate robust parameter optimization approach based on principal component analysis with combined arrays	robust parameter design rpd;multivariate mean square error mmse;principal component analysis pca;multiple objective programming	Todayu0027s modern industries have found a wide array of applications for optimization methods based on modeling with Robust Parameter Designs (RPD). Methods of carrying out RPD have thus multiplied. However, little attention has been given to the multiobjective optimization of correlated multiple responses using response surface with combined arrays. Considering this gap, this paper presents a multiobjective hybrid approach combining response surface methodology (RSM) with Principal Component Analysis (PCA) to study a multi-response dataset with an embedded noise factor, using a DOE combined array. How this approach differs from the most common approaches to RPD is that it derives the mean and variance equations using the propagation of error principle (POE). This comes from a control-noise response surface equation written with the most significant principal component scores that can be used to replace the original correlated dataset. Besides the dimensional reduction, this multiobjective programming approach has the benefit of considering the correlation among the multiple responses while generating convex Pareto frontiers to mean square error (MSE) functions. To demonstrate the procedure of the proposed approach, we used a bivariate case of AISI 52100 hardened steel turning employing wiper mixed ceramic tools. Theoretical and experimental results are convergent and confirm the effectiveness of the proposed approach.	experiment;interaction;mathematical optimization;multi-objective optimization;pareto efficiency;principal component analysis;propagation of uncertainty;radiation hardening;software propagation	Anderson Paulo de Paiva;José Henrique F. Gomes;Rogério Santana Peruchi;Rafael Coradi Leme;Pedro Paulo Balestrassi	2014	Computers & Industrial Engineering	10.1016/j.cie.2014.05.018	econometrics;mathematical optimization;mathematics;statistics	AI	23.52951703240621	-23.66155282223117	19132
39ff3aa82dd0a19f17681947a27759f43893b412	modeling of river water temperatures using feed-forward artificial neural networks		Water temperature influences most physical, chemical and biological processes of the river environment. It plays an important role in the distribution of fishes and on the growth rates of many aquatic organisms. It is therefore important to develop water temperature models in order to effectively manage aquatic habitats, to study the thermal regime of rivers and to have effective tools for environmental impact studies. The objective of the present study was to develop a water temperature model based on artificial neural networks (ANN) for two thermally different watercourses. The ANN model performed best in summer and autumn and showed a poorer (but still good) performance in spring. The many advantages of ANN models are their simplicity, low data requirements, their capability of modelling long-term series as well as have an overall	aquatic ecosystem;artificial neural network;habitat;performance;requirement;time series	Cindie Hébert;Daniel Caissie;Mysore G. Satish;Nassir El-Jabi	2012			artificial intelligence;machine learning;computer science;feed forward;artificial neural network	AI	11.722748555675421	-18.58935826616988	19140
b357b77f708a73f49e261ebb773078378657ca36	domain reduction using grasp construction phase for transmission expansion planning problem	transportation model;integer variables;combinatorial search;construction phase;grasp cp;fast heuristic algorithms;greedy randomized adaptive search procedure;milp;mixed integer linear programming;heuristic algorithms;tm tep;test systems;linear programming;search spaces;trabalho apresentado em evento;branch and bounds;transmission expansion planning;combinatorial optimization;local optimal solution;problem solving	This paper proposes a new strategy to reduce the combinatorial search space of a mixed integer linear programming (MILP) problem. The construction phase of greedy randomized adaptive search procedure (GRASP-CP) is employed to reduce the domain of the integer variables of the transportation model of the transmission expansion planning (TM-TEP) problem. This problem is a MILP and very difficult to solve specially for large scale systems. The branch and bound (BB) algorithm is used to solve the problem in both full and the reduced search space. The proposed method might be useful to reduce the search space of those kinds of MILP problems that a fast heuristic algorithm is available for finding local optimal solutions. The obtained results using some real test systems show the efficiency of the proposed method.	grasp	Mohsen Rahmani;Ruben A. Romero;Marcos J. Rider;Miguel Paredes	2012		10.1007/978-3-642-29124-1_8	mathematical optimization;combinatorics;discrete mathematics;mathematics	Robotics	18.667204890091355	2.842659065192835	19188
cb29d614857ed829eea305aba8bbc3c5bd3ca684	reliability analysis of a renewable multiple cold standby system	survival function;reliability analysis;cold standby;stopping time;deterministic repair;cauchy integral	We present a general reliability analysis of the basic multiple cold standby system attended by a single repair facility. The particular case of deterministic repair provides some explicit results for the survival function illustrated by computer-plotted graphs.	reliability engineering	Edmond J. Vanderperre	2004	Oper. Res. Lett.	10.1016/j.orl.2003.10.002	stopping time;mathematics;survival function;cauchy's integral formula;statistics	Robotics	6.8171685645236995	-0.47052446780653606	19357
3ef608f2bfee1fcc09298cc58248d13ea4b942b4	self-regenerating environmental absorption efficiency and the soylent green scenario	environmental absorption efficiency;self regeneration capabilities;discounting rate;pollution	We consider a stock pollution problem where the biosphere can transform from a sink to a source of pollution in the presence of self-regenerating environmental absorption efficiency. We examine the problem of controlling pollution and the capacity of the biosphere to absorb pollution: the regulator can mitigate emissions and can invest to build-up the absorption capacity of pollution sinks. We examine conditions under which both measures, mitigation and absorption capacity investments, are substitute (complement) to each other, and the relative extent to which environmental self-regenerating capabilities affect these conditions. We also exhibit the possibility of an oscillatory approach to the steady state. Particular attention is paid to the situation where the social planner is impatient. Copyright Springer Science+Business Media New York 2016		Fouad El Ouardighi;Hassan Benchekroun;Dieter Grass	2016	Annals OR	10.1007/s10479-015-2096-x	economics;pollution;operations management;operations research	NLP	-0.6382394645698329	-8.325475814592808	19401
f45088110b693944e77175eb43da700f073e7d85	server-uncertain spectrum trading in cognitive radio networks: a queueing-theoretic modeling approach	service area;spectrum efficiency;cognitive radio networks;dynamic spectrum access;server-uncertain spectrum trading;optimizing strategies;server uncertainty;secondary user service cost minimization;cognitive radio;service price;profitability;queueing theory;average service rate;stacp queueing model;server role;spectrum trading;queueing theory-based spectrum trading model;queueing;queueing-theoretic modeling approach;service content;service time;service state;economics;primary user profit maximization;cost reduction;uncertainty;business;servers	Spectrum trading is the promising method to improve spectrum efficiency from the perspective of economics. In this paper we propose a queueing-theory based spectrum trading model, where the primary user plays the server role providing spectrum to the secondary user who acts as the customer. The most significant challenge is how to optimize the spectrum trading model considering the server uncertainty which includes service state, service time, service area, service content and service price. We design a STACP queueing model according to the server attributes, so that the secondary user can choose the right queue quickly and reasonably according to its demand. Moreover, we further analyze the optimizing strategies for STACP model which can maximize the profit of the primary user and minimize the service cost of the secondary user. The simulation results demonstrate the analysis results.	cognitive radio;data recovery;ibm systems network architecture;information-theoretic death;queueing theory;server (computing);simulation;spectral efficiency;superuser	Lixia Liu;Gang Hu;Ming Xu;Yuxing Peng	2012	2012 7th International ICST Conference on Cognitive Radio Oriented Wireless Networks and Communications (CROWNCOM)		service level requirement;marketing;operations management;business;computer network	Mobile	-0.2559975966124876	-4.673450264080005	19444
7d88190ee1430d4258dceeadc478bf112ec9568e	data mining via association rules for power ramps detected by clustering or optimization	power ramp;association rules;data mining;big data;clustering;apriori algorithm;optimization	Power ramp estimation has wide ranging implications for wind power plants and power systems which will be the focus of this paper. Power ramps are large swings in power generation within a short time window. This is an important problem in the power system that needs to maintain the load and generation at balance at all times. Any unbalance in the power system leads to price volatility, grid security issues that can create power stability problems that leads to financial losses. In addition, power ramps decrease the lifetime of turbine and increase the operation and maintenance expenses. In this study, power ramps are detected by data mining and optimization. For detection and prediction of power ramps, data mining K means clustering approach and optimisation scoring function approach are implemented [1]. Finally association rules of data mining algorithm is employed to analyze temporal ramp occurrences between wind turbines for both clustering and optimization approaches. Each turbine impact on the other turbines are analyzed as different transactions at each time step. Operational rules based on these transactions are discovered by an Apriori association rule algorithm for operation room decision making. Discovery of association rules from an Apriori algorithm will serve the power system operator for decision making.	data mining;program optimization	Nurseda Yildirim;Bahri Uzunoglu	2016	Trans. Computational Science	10.1007/978-3-662-53090-0_9	simulation;engineering;operations management;data mining;fsa-red algorithm	ML	11.15583665515311	-13.892112236590453	19451
8c716030cbea4a61c4bfdbc5203d823746fa0520	the direct use of likelihood for significance testing	predictive interpretation;postdictive interpretation;hypothesis test;likelihood inference	The importance of likelihood is recognized in widely differing approaches to inference. This paper is concerned with some natural but underdeveloped uses of likelihood which have interesting theoretical consequences and are potentially useful for statistical practice. The body of the paper develops a speci®c use of likelihoods as a substitute for tail-areas in traditional signi®cance testing, following the heuristic proposition that, if a null hypothesis H1 has likelihood 1/20 or less of the likelihood of an alternative hypothesis H2, then H1 can be rejected with much the same force as a null hypothesis rejected on the basis of a tail-area of size 1/20 or less. In order to motivate the general attitude taken here towards the concept of likelihood, consider the following pair of precepts for a working statistician making use of the tools of statistical inference. First, he should keep clearly in mind the distinction between probabilistic devices which yield directly interpretable probabilities or expectations and those which deal only tangentially with a particular situation. Second, he should recognize and identify in his work a mixture of forward and backward modes of operation, where forward operation means the use of adopted probability models for arriving, perhaps tentatively, at estimates, predictions or decisions, while backward operation means testing, evaluating and revising the probability models in use. It will be argued below that the direct interpretation of likelihood falls more naturally into backward operation than into forward operation. The contrast between direct and indirect roles in inference is plainly visible in the context of likelihood. Likelihood is fundamental to frequentist inference because the likelihood function is a sucient statistic and thence is associated with a broad array of optimality properties. But frequentist inference deals only with operating characteristics of decision procedures which are directly relevant to an ensemble of usually hypothetical data sets including the data set under analysis, and hence frequentist inference is direct inference only by covert misinterpretation. Similarly, likelihood is fundamental to Bayesian inference, but in the indirect technical role of a multiplier which converts prior odds to posterior odds. R.A. Fisher discussed indirect aspects of likelihood, especially in relation to eciency, suciency and exhaustive estimation, and in relation to the ®ducial argument. But Fisher also recognized direct consideration of the likelihood function as a variety of inference. As an illustration of how to interpret the likelihood function directly, he exhibited a range of parameter values with likelihoods at least 1/15 of the maximum of the likelihood. His verbalization of the meaning of the interval was: Reprinted, with kind permission of the author and editor of the original proceedings, from Memoirs No. 1, Proceedings of Conference on Foundational Questions in Statistical Inference, Aarhus, Denmark, 7±22 May 1973, pp. 335±54. (eds. O. BarndorNielsen, Preben Blaesild, and Geert Schou), Department of Theoretical Statistics, Institute of Mathematics, University of Aarhus. Statistics and Computing (1997) 7, 247±252	block cipher mode of operation;cr rao advanced institute of mathematics, statistics and computer science;heuristic	Arthur P. Dempster	1997	Statistics and Computing	10.1023/A:1018598421607	econometrics;statistical hypothesis testing;pattern recognition;mathematics;statistics	ML	1.7439182350363354	-14.52583314860645	19519
4993d3dbad134b36b9c03fe23c2d146bfda5b353	dual sourcing problem with real option	cybernetics;reliability;uncertainty;procurement;contracts;supply chains;conferences	This paper addresses a single-period dual sourcing problem with real option under uncertain supply and demand. A retailer, who faces the uncertain market demand, can source from a low-price but unreliable supplier and an expensive but reliable supplier. In order to manage the supply uncertainties and decrease the cost, the retailer orders product from the unreliable supplier and buys option from the reliable one at the beginning of the selling season. After the demand realizes and the order is delivered, the option is executed. We develop a model for this problem and characterize the optimal policy. Then we compare the new strategy with the traditional dual sourcing strategy and the instant replenishment strategy with capacity constraint. The conditions that the new strategy has an advantage over the other ones are identified. Numeric experiments are conducted to analyze the impact of the parameters on the optimal policy and the profit in the new strategy and to compare the performance of the traditional dual sourcing strategy and the instant replenishment strategy with capacity constraint.	decision problem;experiment;mathematical optimization;numerical analysis;optimization problem;procurement	Junjun Han;Jie Xiang;Juliang Zhang;Guowei Hua	2016	2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2016.7844286	uncertainty;procurement;cybernetics;computer science;reliability;supply chain;statistics	Robotics	2.3774671868564816	-5.965852399324958	19535
e5941b126c0d518aa7fc7892447213ac4f8a3ac0	innately adaptive robotics through embodied evolution	tecnologia industrial tecnologia mecanica;ucl;grupo de excelencia;discovery;theses;conference proceedings;embodied evolution;digital web resources;ucl discovery;open access;ucl library;book chapters;open access repository;evolutionary algorithm;tecnologias;autonomous robot;ucl research	Autonomous adaptation in robots has become recognised as crucial for devices deployed in remote or inhospitable environments. The aim of this work is to investigate autonomous robot adaptation, focussing on damage recovery and adaptation to unknown environments. An embodied evolutionary algorithm is introduced and its capabilities demonstrated with experimental results. This algorithm is shown to be able to control the motion of a robot snake effectively; this same algorithm inherently recovers the snake’s motion after damage. Another experiment shows that the algorithm is capable of contorting a shape-changing antenna in such a way as to minimise the affect of background noise on it, thus allowing the antenna to achieve a better signal.	autonomous robot;evolutionary algorithm;robotics	Siavash Haroun Mahdavi;Peter J. Bentley	2006	Auton. Robots	10.1007/s10514-006-5941-6	simulation;computer science;artificial intelligence;machine learning;evolutionary algorithm	Robotics	24.24436877260218	-12.909932037147549	19549
7be9d656a586743f07259d119910e96dd02e41c7	multiobjective optimization algorithm with objective-wise learning for continuous multiobjective problems		Most of multiobjective optimization algorithms consider multiple objectives as a whole when solving multiobjective optimization problems (MOPs). However, in MOPs, different objective functions may possess different properties. Hence, it can be beneficial to build objective-wise optimization strategy for each objective separately. In this paper, we firstly propose a single objective guided multiobjective optimization (SOGMO) framework to solve continuous MOPs. In SOGMO framework, a solution is selected from archive, and then objective-wise learning strategy is developed to promote the evolution of each objective of the selected solution. Thus, all the objectives of the considered solution can be simultaneously optimized in parallel by the cooperation of objective-wise learning process. A specific instantiation of the SOGMO framework is implemented, where a neighborhood field optimization (NFO) algorithm, as objectivewise learning strategy, and dominance archive are designed. The proposed SOGMO implementation, called SOGMO-NFO, is systematically compared with several state-of-the-art multiobjective evolutionary algorithms (MOEA). Simulation results on 13 benchmark problems from CEC 2009 competition show that SOGMO-NFO is better than the compared MOEAs.	.nfo;archive;benchmark (computing);evolutionary algorithm;local search (optimization);moea framework;mathematical optimization;multi-objective optimization;optimization problem;resultant;simulation;universal instantiation;word lists by frequency;yang	Jiahai Wang;Chenglin Zhong;Ying Zhou;Yalan Zhou	2015	J. Ambient Intelligence and Humanized Computing	10.1007/s12652-014-0218-y	mathematical optimization;simulation;computer science;management science	AI	24.326654969839076	-4.00357551955434	19745
6a699223ba29005508ced7fdf5eac8a3c8656f86	a hybrid estimation of distribution algorithm for flexible job-shop scheduling problems with process plan flexibility	multiobjective optimization;evolutionary optimization;estimation of distribution algorithm;flexible job-shop scheduling;mallows distribution;routing flexibility;process plan flexibility	The flexible job-shop environments have become increasingly significant because of rapid improvements on shop floors such as production technologies, manufacturing processes and systems. Several real manufacturing and service companies have had to use alternative machines or processes for each operation and the availability of alternative process plans for each job in order to achieve good performance on the shop floor where conflicting objectives are common, e.g. the overall completion time for all jobs and the workload of the most loaded machine. In this paper, we propose a Pareto approach based on the hybridization of an estimation of distribution algorithm and the Mallows distribution in order to build better sequences for flexible job-shop scheduling problems with process plan flexibility and to solve conflicting objectives. This hybrid approach exploits the Pareto-front information used as an input parameter in the Mallows distribution. Various instances and numerical experiments are presented to illustrate that shop floor performance can be noticeably improved using the proposed approach. In addition, statistical tests are executed to validate this novel research.	combinatorial optimization;estimation of distribution algorithm;experiment;general material designation;golm metabolome database;job shop scheduling;mathematical optimization;numerical analysis;parameter (computer programming);pareto efficiency;scheduling (computing);shutdown (computing);software system	Ricardo Pérez-Rodríguez;Arturo Hernández Aguirre	2018	Applied Intelligence	10.1007/s10489-018-1160-z	artificial intelligence;workload;machine learning;computer science;job shop scheduling;estimation of distribution algorithm;scheduling (computing);statistical hypothesis testing;pareto principle;multi-objective optimization;alternative process	HPC	18.792657749627217	-0.8814596593732511	19752
8e6fc85d44296c0e15ae604f77821c36e9ba5ae2	job scheduling using ranking fuzzy number method	fuzzy sets greedy algorithms sun educational institutions gravity testing np complete problem uncertainty fuzzy set theory optimization methods;membership value;center of gravity method;uncertainty;fuzzy number;gravity;greedy algorithms;job sequencing job scheduling ranking fuzzy number method center of gravity method membership value;job sequencing;testing;fuzzy set theory;fuzzy sets;computational complexity production control fuzzy set theory;general solution;production control;computational complexity;indexation;center of gravity;sun;greedy algorithm;profitability;ranking fuzzy number method;job scheduling;np complete problem;optimization methods	Proposes a fuzzy ranking method to solve job scheduling problems. The proposed fuzzy ranking method is based on the concept of the center of gravity method. For each job in a job scheduling problem, the profit is transferred into the membership value, and the deadline is transferred into the value of a fuzzy element X. Then, each job can be transferred into a fuzzy number, and we can find the centroid (center) of this fuzzy number by applying the proposed fuzzy ranking method. Based on these ranking values, a good job sequencing can be obtained. Simulation tests show that generated solutions by the ranking fuzzy number approach are better than by the traditional greedy algorithm for solving the complex job scheduling problems.	fuzzy number;job scheduler;job shop scheduling;scheduling (computing)	Koun-Tem Sun	1998		10.1109/KES.1998.725958	mathematical optimization;discrete mathematics;defuzzification;type-2 fuzzy sets and systems;fuzzy classification;fuzzy number;machine learning;mathematics;ranking svm;fuzzy set operations	HPC	-1.670017019769705	-18.94278607686306	19777
3a9d0c008f5b847325762aa7ba219906470b7ff1	neuro-adaptive traffic congestion control for urban road networks		The rapid increase of private vehicles combined with the limited capabilities of the urban road infrastructure has made congestion one of the main problems of major cities worldwide, having a severe impact on both the economy and the environment. In this work, we shall attempt to solve the traffic management problem by examining in a unified manner the traffic network, the route guidance of the vehicles and the regulation of the traffic lights, as the basic elements of a single controlled system. In particular, we propose a decentralized adaptive control system, comprised of three main modules: i) the network congestion estimator, ii) the reference travel time estimator, and iii) the rate controller, that is capable of efficiently regulating the travel time along the traffic network while avoiding congestion at the junctions. The design of decentralized control algorithms and their implementation as traffic management applications for portable computing devices (e.g., 3rd and 4 th generation mobile phones, tablets, computers embedded in “smart” vehicles) is expected to improve drastically the traffic condition of urban road networks. Meanwhile, in future traffic networks, where the navigation of the vehicles will be conducted by autopilots in the absence of human-drivers, the use of such a distributed autonomous management system will be essential.		Charalampos P. Bechlioulis;Kostas J. Kyriakopoulos	2018	2018 European Control Conference (ECC)	10.23919/ECC.2018.8550287		Mobile	9.549273808029907	-8.406513435585822	19786
aee91cb29f6fcd5b396e3519fa4d67b799eecdbe	inventory and investment in setup operations under return on investment maximization	optimal solution;global solution;inventory reduction;return on investment;cost function;maximization;inversion;financial performance;investment decision;reduction;optimum global;setup operations;prise decision;global optimum;funcion coste;investment;analyse;linear functionals;inventory;capital investment;modelo;investissement;setup operation;fonction cout;reduccion;global optimization;just in time;analysis;modele;solution globale;toma decision;optimo global;maximizacion;solucion global;models;inventaire;inventario;maximisation;analisis	In this paper, we construct and analyze inventory and investment in setup operations policies under return on investment (ROI) maximization. The key contributing features of this paper are the establishment of an ROI model and characterization of the unique global optimal solution when there exists an option to invest in setup operations. We also show how the inventory level is reduced when it is optimal to invest additional money in setup operations and derive the unique optimal solutions in closed-form when the setup cost is a rational or linear function of the level of investment. Various interesting managerial insights are provided. © 1999 Elsevier Science Ltd. All rights reserved. Scope and purpose Capital investment in setup operations has been widely investigated so as to reduce inventory in accordance with just-in-time (JIT) philosophy. Meanwhile, for finished goods, the return on investment (ROI) is widely used as a financial performance criterion. The purpose of this paper is to design and analyse a capital investment model under ROI maximization. Specifically, this paper analytically and quantitatively shows how inventory may be reduced via investment in setup operations and how optimal inventory and investment decisions can be derived and characterized.		Toshitsugu Otake;K. Jo Min;Cheng-Kang Chen	1999	Computers & OR	10.1016/S0305-0548(98)00095-1	mathematical optimization;return on investment;analysis;mathematics;operations research;global optimization	ML	4.269234122207211	-4.848002701600622	19853
abc36d2d6f55a8c7fb8f7c7ebb27ea0d76996dcc	performance evaluation of slow sand filters using fuzzy rule-based modelling	grain size;fuzzy set;total coliforms fuzzy rule based modelling;performance evaluation;risk analysis;possibility and necessity measures;removal efficiency;system performance;fuzzy logic;wastewater treatment plant;fuzzy rule base;saudi arabia;multiple regression model;if then rules;total coliform;wastewater treatment;slow sand filters	The main objective of this study is to evaluate and predict the performance of slow sand filters used for wastewater treatment. The uncertainties in the control parameters and processes require fuzzy sets to be used when modelling system performance. Fuzzy logic if–then rules were used to build a model for the removal efficiency (total coliforms) of slow sand filters. The data were collected from three pilot-scale slow sand filters at the Alkhobar (Saudi Arabia) wastewater treatment plant. The removal efficiency of filters was modelled using three input control parameters—filtration rate, sand bed depth and grain size. Based on available data, fuzzy logic if–then rules were established. The fuzzy rule-based model was validated using experimental data of three case studies reported in the literature. The results were also compared with a multiple regression model. A possibilistic risk analysis was performed using optimal removal efficiency of the slow sand filters. The risk is estimated with respect to non-compliance of unrestricted agricultural reuse standards (100 total coliform/100 ml). In addition to slow sand filters, postor pre-chlorination of wastewater is recommended to improve wastewater quality for conforming agricultural reuse standards. Crown Copyright 2003 Published by Elsevier Ltd. All rights reserved.	crown group;fuzzy concept;fuzzy logic;fuzzy rule;fuzzy set;it risk management;logic programming;performance evaluation;regression testing	Rehan Sadiq;Muhammad A. Al-Zahrani;Anwer K. Sheikh;Tahir Husain;Shaukat Farooq	2004	Environmental Modelling and Software	10.1016/S1364-8152(03)00165-8	fuzzy logic;risk analysis;environmental engineering;computer science;engineering;artificial intelligence;civil engineering;operations management;sewage treatment;computer performance;slow sand filter;fuzzy set;grain size	AI	3.6056516445089857	-20.267918489336882	19858
c27ae4d7d9e2e95393118157d52a89526a121f2e	credit scoring for profitability objectives	modelizacion;rentabilidad;selection problem;optimisation;banking;empirical study;problema seleccion;credit scoring;score function;methode empirique;scoring credit;prestamo;optimizacion;metodo empirico;empirical method;dette;non responders;secteur bancaire;credit;algoritmo genetico;classification;pret;modelisation;regle decision;profit;loans;credito;beneficio;consumer credit;sum of squares;valuacion credito;algorithme genetique;benefice;for profit;genetic algorithm;generating function;genetic algorithms;rentabilite;optimization;profitability;prediction model;debt;regla decision;or in banking credit scoring genetic algorithms profitability;modeling;clasificacion;decision rule;or in banking;methode score;probleme selection	In consumer credit markets lending decisions are usually represented as a set of classification problems. The objective is to predict the likelihood of customers ending up in one of a finite number of states, such as good/bad payer, responder/non-responder and transactor/non-transactor. Decision rules are then applied on the basis of the resulting model estimates. However, this represents a misspecification of the true objectives of commercial lenders, which are better described in terms of continuous financial measures such as bad debt, revenue and profit contribution. In this paper, an empirical study is undertaken to compare predictive models of continuous financial behaviour with binary models of customer default. The results show models of continuous financial behaviour to outperform classification approaches. They also demonstrate that scoring functions developed to specifically optimize profit contribution, using genetic algorithms, outperform scoring functions derived from optimizing more general functions such as sum of squared error.		Steven Finlay	2010	European Journal of Operational Research	10.1016/j.ejor.2009.05.025	financial economics;genetic algorithm;actuarial science;economics;computer science;marketing;mathematics;empirical research;welfare economics;statistics	Theory	3.155908607394138	-10.036604975657369	19885
76fd7948dc8b859065bb79fb3ef69623867cc85f	a hybrid particle swarm optimization algorithm for the redundancy allocation problem	system reliability;local search algorithm;particle swarm optimizer;particle swarm optimization;tabu search;hybrid strategy;particle swarm optimization algorithm;multiple choice;redundancy allocation problem;local search;series parallel;hybrid algorithm;adaptive penalty function;penalty function	The Redundancy Allocation Problem generally involves the selection of components with multiple choices and redundancy levels that produce maximum system reliability given various system level constraints as cost and weight. In this paper we investigate the series–parallel redundant reliability problems, when a mixing of components was considered. In this type of problem both the number of redundancy components and the corresponding component reliability in each subsystem are to be decided simultaneously so as to maximise the reliability of system. A hybrid algorithm is based on particle swarm optimization and local search algorithm. In addition, we propose an adaptive penalty function which encourages our algorithm to explore within the feasible region and near feasible region, and discourage search beyond that threshold. The effectiveness of our proposed hybrid PSO algorithm is proved on numerous variations of three different problems and compared to Tabu Search and Multiple Weighted Objectives solutions.	algorithm;particle swarm optimization	Noura Beji;Bassem Jarboui;Mansour Eddaly;Habib Chabchoub	2010	J. Comput. Science	10.1016/j.jocs.2010.06.001	mathematical optimization;multi-swarm optimization;tabu search;computer science;local search;theoretical computer science;machine learning;mathematics;metaheuristic	Theory	21.255722909796425	-1.8940439240572033	19887
7a07e36f4728b1a8e03a46dd943fbc7146a22cd6	optimizing inspection strategies for multi-stage manufacturing processes using simulation optimization	multi-stage manufacturing;simulation optimization;required output quality;paper deal;multi-stage production process;lowest total inspection cost;optimizing inspection strategy;optimal inspection strategy;multi-stage process subject;resulting inspection cost;numerical example;quality control;production process;sampling methods;simulation modeling;mean squared error;simulation;inspection	This paper deals with the problem of determining the optimal inspection strategy for a multi-stage production process using simulation optimization. An optimal inspection strategy is the one that results in the lowest total inspection cost, while still assuring a required output quality. Because of the complexity of the problem, simulation is used to model the multi-stage process subject to inspection and to calculate the resulting inspection costs. Simulation optimization is then used to find the optimal inspection strategy. The performance of the proposed method is presented through the use of a numerical example.	mathematical optimization;numerical analysis;optimizing compiler;simulation	Vahid Sarhangian;Abolfazl Vaghefi;Hamidreza Eskandari;Mostafa K. Ardakani	2008	2008 Winter Simulation Conference		sampling;quality control;simulation;inspection;engineering;simulation modeling;mean squared error;scheduling;engineering drawing;statistics;manufacturing engineering	Robotics	8.469726924449114	1.092101159783331	19889
f02e2ebba1def672acde962ea5ab97ebc189efea	the bioinspired algorithm of electronic computing equipment schemes elements placement		One of the important design problems - the problem of ECE schemes elements placement is considered in this article. It belongs to the class of NP-hard problems. Statement of a placement problem is made; the complex criterion considering boundary conditions and restrictions is entered. The modified hybrid architecture of the bioinspired search using multilevel evolution and migration mechanism is offered. The genetic and evolutionary algorithms allowing receiving sets of quasi-optimum decisions, for polynomial time are developed. The program environment is created and computing experiment is made. The series of tests and experiments have allowed specifying theoretical estimations of placement algorithms running time and their behavior for schemes of various structures. The best running time of algorithms is O (n log n), the worst one is O(n3).		Vladimir V. Kureichik;Daria V. Zaruba	2015		10.1007/978-3-319-18476-0_6	control engineering;electronic engineering;computer science;theoretical computer science	EDA	20.78238558836837	1.7624751419670965	19896
774ef8e0e78d39840a67f03ca6dbf53acc5f4e52	defence and attack of systems with variable attacker system structure detection probability	forecasting;information structure;aplicacion militar;detection probability;reliability;systeme redondant;application militaire;project management;information systems;structure information;defence;maintenance;redundancia;metodo minimax;juego cooperativo;soft or;sistema estructura variable;information technology;systems engineering;minimax method;packing;systeme structure variable;estructura informacion;variable structure system;endommagement;probabilistic approach;operations research;location;deterioracion;investment;journal;cooperative game;journal of the operational research society;inventory;false targets;purchasing;redundant system;elements;history of or;redundancy;logistics;marketing;jeu cooperatif;scheduling;enfoque probabilista;approche probabiliste;methode minimax;ingenierie systeme;military application;production;communications technology;attack;computer science;information system;operational research;sistema redundante;damaging;systeme information;system demand;applications of operational research;or society;redondance;jors;management science;infrastructure;sistema informacion	A system consists of identical elements. The cumulative performance of these elements should meet a demand. The defender applies three types of defensive actions to reduce a damage associated with system performance reduction caused by an external attack: deploying separated redundant genuine system elements, deploying false elements, and protecting genuine elements. If the attacker cannot distinguish between genuine and false elements, he chooses a number of elements to attack and then selects the elements at random, distributing his resources equally across these elements. By obtaining intelligence data, the attacker can get full information about the system structure and identify false and unprotected genuine elements. The defender estimates the probability that the attacker can identify all system elements. This paper analyses the influence of this probability in a non-cooperative two-period minmax game between the defender and the attacker. Journal of the Operational Research Society (2010) 61, 124–133. doi:10.1057/jors.2008.158 Published online 7 January 2009	minimax	Gregory Levitin;Kjell Hausken	2010	JORS	10.1057/jors.2008.158	project management;economics;computer science;marketing;operations management;management;operations research;information technology;computer security;information system	DB	7.04829048710085	-3.2480379498469216	19899
473b9d7e0a090bf391156ed5aff4dc32d5d9e892	a contribution to the theory of word length distribution based on a stochastic word length distribution model		Abstract We derive a stochastic word length distribution model based on the concept of compound distributions and show its relationships with and implications for Wimmer et al. ’s (1994) synergetic word length distribution model.		Steffen Eger	2013	Journal of Quantitative Linguistics	10.1080/09296174.2013.799910	arithmetic;natural language processing;speech recognition;mathematics	NLP	14.745724216179013	-9.309085999359729	19914
53b2f81cc6590fd22a88fb50c84e97e6c02b37f4	consultant-guided search algorithms for the quadratic assignment problem	traveling salesman problem;swarm intelligence;metaheuristics;search algorithm;combinatorial optimization problem;max min ant system;quadratic assignment problem;combinatorial optimization;local search;ant colony optimization algorithm	Consultant-Guided Search (CGS) is a recent swarm intelligence metaheuristic for combinatorial optimization problems, inspired by the way real people make decisions based on advice received from consultants. Until now, CGS has been successfully applied to the Traveling Salesman Problem. Because a good metaheuristic should be able to tackle efficiently a large variety of problems, it is important to see how CGS behaves when applied to other classes of problems. In this paper, we propose four CGS algorithms for the Quadratic Assignment Problem (QAP) and we compare their performance. Our experimental results show that CGS is able to compete with Ant Colony Optimization in terms of solution quality.	ant colony optimization algorithms;combinatorial optimization;mathematical optimization;metaheuristic;program optimization;quadratic assignment problem;search algorithm;swarm intelligence;travelling salesman problem	Serban Iordache	2010		10.1145/1830761.1830876	optimization problem;extremal optimization;2-opt;mathematical optimization;ant colony optimization algorithms;cross-entropy method;parallel metaheuristic;combinatorial optimization;tabu search;swarm intelligence;computer science;generalized assignment problem;artificial intelligence;local search;machine learning;mathematics;weapon target assignment problem;travelling salesman problem;3-opt;metaheuristic;bottleneck traveling salesman problem;quadratic assignment problem;search algorithm	AI	23.31392616795292	-0.03796317648949076	19928
c9618cb7c0b7beba7966e63498eec642e3acc7b7	eigenvector selection with stepwise regression techniques to construct eigenvector spatial filters	spatial autoregressive regression;spatial filtering;spatial autocorrelation;eigenvectors	Because eigenvector spatial filtering (ESF) provides a relatively simple and successful method to account for spatial autocorrelation in regression, increasingly it has been adopted in various fields. Although ESF can be easily implemented with a stepwise procedure, such as traditional stepwise regression, its computational efficiency can be further improved. Two major computational components in ESF are extracting eigenvectors and identifying a subset of these eigenvectors. This paper focuses on how a subset of eigenvectors can be efficiently and effectively identified. A simulation experiment summarized in this paper shows that, with a well-prepared candidate eigenvector set, ESF can effectively account for spatial autocorrelation and achieve computational efficiency. This paper further proposes a nonlinear equation for constructing an ideal candidate eigenvector set based on the results of the simulation experiment. Copyright Springer-Verlag Berlin Heidelberg 2016	stepwise regression	Yongwan Chun;Daniel A. Griffith;Monghyeon Lee;Parmanand Sinha	2016	Journal of Geographical Systems	10.1007/s10109-015-0225-3	econometrics;mathematical optimization;eigenvalues and eigenvectors;mathematics;spatial analysis;spatial filter;statistics	Logic	24.57662528029026	-23.76284762443032	19938
25397021554894c7b3932556c85279d1947c08d7	heuristic speciation for evolving neural network ensemble	network design;evolutionary design;evolving neural networks;neural net work;pattern recognition;mutual information;niching;evolutionary algorithm;neural network;evolutionary computing	Speciation is an important concept in evolutionary computation. It refers to an enhancements of evolutionary algorithms to generate a set ofdiverse solutions. The concept is studied intensively in the evolutionary design of neural network ensembles. Thediversity and cooperation of individual networks are among the essential criteria of the design.This paper proposes a speciation framework for ensemble design which integratesa collection of new techniques. Its characteristic features are:(a) the population of networks are speciated as such thatthe mutual information between the networks' outputs and genotypic representations is preserved. (b) The ensemble is designed incrementally,upon discovery of a species of networks which enhances the ensembleperformance. (c) Multiple species are evolved andindividual networks are evaluated according to therole of their respective species in the ensemble.This framework provides an implementation of evolutionary algorithm which performs simultaneous single-objective optimizations.The new algorithm is evaluated with a series of classification benchmarks andshows an improvement over other evolutionary training strategiesand a statistical algorithm.	artificial neural network;continuous design;emoticon;evolutionary algorithm;evolutionary computation;heuristic;mutual information	Shin Ando	2007		10.1145/1276958.1277315	evolutionary programming;evolutionary music;network planning and design;human-based evolutionary computation;computer science;artificial intelligence;machine learning;evolutionary algorithm;evolutionary acquisition of neural topologies;data mining;mutual information	AI	23.609175831576948	-9.034228009078037	19988
a3a63b6e0d8f1480640fe39294b0715ade6839f5	general queuing model for optimal seamless delivery of payload processing in multi-core processors	multi-core;queuing analysis;real-time;remote sensing;remotely piloted aircraft	Recent developments in unmanned aerial systems (UAS) provide new opportunities in remote sensing application. In contrast to satellite and conventional (manned) aerial tasks, UAS flights can be operated in a very short period of time. UAS can also be more specifically focused toward a given task such as crop reconnaissance or electric line tower inspection. For some applications, the delivery time of the remote sensing results is crucial. The current three-phase procedure of data acquisition, data downloading and data processing, performed sequentially in time, represents a drawback that reduces the benefits of using unmanned aerial systems. In this paper, we present a parallel processing strategy, based on queuing theory, in which the data processing phase is performed on board in parallel with data acquisition. The unmanned aerial system payload has been enlarged with low-cost, lightweight, multi-core boards to facilitate remote sensing data processing during flight. The storage of the raw sensing data is also done for possible further analysis; however, the ultimate decision support information can be seamless delivered to the customer upon landing. Furthermore, text alarms and limited imagery can also be provided during flight.	aerial photography;central processing unit;data acquisition;decision support system;download;multi-core processor;parallel computing;queueing theory;remote sensing application;seamless3d;unmanned aerial vehicle	Esther Salamí;Cristina Barrado;Antonia Gallardo;Enric Pastor	2017	The Journal of Supercomputing	10.1007/s11227-017-2109-4	computer science;queueing theory;embedded system;decision support system;payload;remote sensing application;multi-core processor;data acquisition;upload;data processing	HPC	15.531610817605547	-11.693551752394571	19994
18a335cfa3335b8c5ac432a8c38d0c6c6a0ce00d	crowddeliver: planning city-wide package delivery paths leveraging the crowd of taxis	public transportation;public transportation trajectory roads urban areas real time systems relays planning;urban areas;trajectory;roads;planning;relays;trajectory data mining package delivery hitchhiking rides route planning taxi scheduling;real time systems	Despite the great demand on and attempts at package express shipping services, online retailers have not yet had a practical solution to make such services profitable. In this paper, we propose an economical approach to express package delivery, i.e., exploiting relays of taxis with passengers to help transport package collectively, without degrading the quality of passenger services. Specifically, we propose a two-phase framework called crowddeliver  for the package delivery path planning. In the first phase, we mine the historical taxi trajectory data  offline to identify the shortest package delivery paths with estimated travel time given any Origin–Destination pairs. Using the paths and travel time as the reference, in the second phase we develop an online adaptive taxi scheduling algorithm to find the near-optimal delivery paths  iteratively upon real-time requests and direct the package routing accordingly. Finally, we evaluate the two-phase framework using the real-world data sets, which consist of a point of interest, a road network, and the large-scale trajectory data, respectively, that are generated by 7614 taxis in a month in the city of Hangzhou, China. Results show that over 85% of packages can be delivered within 8 hours, with around 4.2 relays of taxis on average.	algorithm;motion planning;online and offline;online shopping;point of interest;real-time clock;relay;routing;scheduling (computing);two-phase locking;universal quantification	Chao Chen;Daqing Zhang;Xiaojuan Ma;Bin Guo;Leye Wang;Yasha Wang;Edwin Hsing-Mean Sha	2017	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2016.2607458	planning;simulation;engineering;trajectory;transport engineering	Robotics	11.462011211004071	-7.270465436701746	20046
04957d8ab68738b5f71abf4101facf9fa0fc6975	a genetic algorithm for the economic lot scheduling problem under extended basic period approach and power-of-two policy	existing literature;infeasible solution;pot policy;genetic algorithm;economic lot scheduling problem;extended basic period approach;power-of-two policy;solution quality;proposed ga;pot multiplier;multi-chromosome solution representation	In this study, we propose a genetic algorithm (GA) for the economic lot scheduling problem (ELSP) under extended basic period (EBP) approach and power-of-two (PoT) policy. The proposed GA employs a multi-chromosome solution representation to encode PoT multipliers and the production positions separately. Both feasible and infeasible solutions are maintained in the population through the use of some sophisticated constraint handling methods. Furthermore, a variable neighborhood search (VNS) algorithm is also fused into GA to further enhance the solution quality. The experimental results show that the proposed GA is very competitive to the best performing algorithms from the existing literature under the EBP and PoT policy.	economic lot scheduling problem;genetic algorithm;power of two;scheduling (computing)	Önder Bulut;Mehmet Fatih Tasgetiren;Mehmet Murat Fadiloglu	2011		10.1007/978-3-642-25944-9_8	mathematical optimization;economic lot scheduling problem;dynamic priority scheduling;management science	AI	18.925481264146477	-1.3229506710635361	20078
61b271b6df7d554dfc616f35df062af8cb1419d4	modelling practical lot-sizing problems as mixed-integer programs	institutional repositories;fedora;valid inequalities;vital;mixed integer program;lot sizing;mixed integer programming;production planning;supply chain;reformulation;vtls;ils	In spite of the remarkable improvements in the quality of general purpose mixed-integer programming software, the effective solution of a variety of lot-sizing problems depends crucially on the development of tight formulations for the special problem features occurring in practice. After reviewing some of the basic preprocessing techniques for handling safety stocks and multilevel problems, we discuss a variety of aspects arising particularly in small and large bucket (time period) models such as start-ups, changeovers, minimum batch sizes, choice of one or two set-ups per period, etc. A set of applications is described that contains one or more of these special features, and some indicative computational results are presented. Finally, to show another technique that is useful, a slightly different (supply chain) application is presented, for which the a priori addition of some simple mixed-integer inequalities, based on aggregation, leads to important improvements in the results. (Lot-Sizing; Production Planning; Mixed-Integer Programming; Valid Inequalities; Reformulation)	computation;integer programming;linear programming;preprocessor;programming tool;safety stock	Gaetan Belvaux;Laurence A. Wolsey	2001	Management Science	10.1287/mnsc.47.7.993.9800	mathematical optimization;integer programming;economics;marketing;operations management;mathematics;supply chain;algorithm	AI	14.38178789853794	4.004359751507241	20101
81331cff66aa3e2aeca913fa8f456b09b24d99d2	maximizing deviation method for multiple attribute decision making in intuitionistic fuzzy setting	intuitionistic fuzzy set;score function;weighted averaging;fuzzy number;multiple attribute decision making;weight information;intuitionistic fuzzy number;intuitionistic fuzzy weighted averaging ifwa operator;optimization model	With respect to multiple attribute decision making problems with intuitionistic fuzzy information, some operational laws of intuitionistic fuzzy numbers, score function and accuracy function of intuitionistic fuzzy numbers are introduced. An optimization model based on the maximizing deviation method, by which the attribute weights can be determined, is established. For the special situations where the information about attribute weights is completely unknown, we establish another optimization model. By solving this model, we get a simple and exact formula, which can be used to determine the attribute weights. We utilize the intuitionistic fuzzy weighted averaging (IFWA) operator to aggregate the intuitionistic fuzzy information corresponding to each alternative, and then rank the alternatives and select the most desirable one(s) according to the score function and accuracy function. Finally, an illustrative example is given to verify the developed approach and to demonstrate its practicality and effectiveness. 2008 Elsevier B.V. All rights reserved.	aggregate data;attribute grammar;intuitionistic logic;mathematical optimization	Gui-Wu Wei	2008	Knowl.-Based Syst.	10.1016/j.knosys.2008.03.038	mathematical optimization;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy number;data mining;score;fuzzy set operations	AI	-2.7936618678891225	-20.283322282415522	20103
010f8037c82a53eeb195dd582ee6ddc3ceb24422	a scalability study of multi-objective particle swarm optimizers	scalability study;continuous optimization problems;market research;pcx;kts;knowledge transfer strategy;measurement;omopso;optimization technique;multiobjective particle swarm optimizers;single objective problems;speed constrained multiobjective pso;smpso;parent centric recombination;optimized multiobjective pso;vepso;vectors;vector evaluated pso;particle swarm optimization;optimization vectors particle swarm optimization scalability measurement market research computer science;optimization;multiobjective optimization problems;scalability;computer science;particle swarm optimisation;multiobjective pso algorithms;multiobjective particle swarm optimizers pcx parent centric recombination kts knowledge transfer strategy smpso speed constrained multiobjective pso omopso optimized multiobjective pso vepso vector evaluated pso multiobjective pso algorithms multiobjective optimization problems single objective problems continuous optimization problems optimization technique scalability study	Particle swarm optimization (PSO) is a well-known optimization technique originally proposed for solving single-objective, continuous optimization problems. However, PSO has been extended in various ways to handle multi-objective optimization problems (MOPs). The scalability of multi-objective PSO algorithms as the number of sub-objectives increases has not been well examined; most observations are for two to four objectives. It has been observed that the performance of multiobjective optimizers for a low number of sub-objectives can not be generalized to problems with higher numbers of sub-objectives. With this in mind, this paper presents a scalability study of three well-known multi-objective PSOs, namely vector evaluated PSO (VEPSO), optimized multi-objective PSO (oMOPSO), and speed-constrained multi-objective PSO (SMPSO) with up to eight sub-objectives. The study indicates that as the number of sub-objectives increases, SMPSO scaled the best, oMOPSO scaled the worst, while VEPSO's performance was dependent on the knowledge transfer strategy (KTS) employed, with parent centric recombination (PCX) based approaches scaling consistently better.	algorithm;continuous optimization;image scaling;mathematical optimization;multi-objective optimization;pcx;particle swarm optimization;phase-shift oscillator;scalability;wait-for graph	Kyle Robert Harrison;Andries Petrus Engelbrecht;Beatrice M. Ombuki-Berman	2013	2013 IEEE Congress on Evolutionary Computation	10.1109/CEC.2013.6557570	market research;mathematical optimization;scalability;computer science;artificial intelligence;machine learning;particle swarm optimization;measurement	SE	24.586188623923185	-3.8980084131280752	20178
b814f8edf16dad56edeba2b1c734d00168e3d19b	hybrid pso-svm method for short-term load forecasting during periods with significant temperature variations in city of burbank	support vector machines;load forecasting;particle swarm optimization	This paper proposes a practical new hybrid model for short term electrical load forecasting based on particle swarm optimization (PSO) and support vector machines (SVM). Proposed PSO–SVM model is targeted for forecast load during periods with significant temperature variations. The proposed model detects periods when temperature significantly changes based on weather (temperature) forecast and	electrical load;mathematical optimization;particle swarm optimization;support vector machine	A. Selakov;D. Cvijetinovic;L. Milovic;S. Mellon;D. Bekut	2014	Appl. Soft Comput.	10.1016/j.asoc.2013.12.001	support vector machine;mathematical optimization;simulation;computer science;artificial intelligence;machine learning;particle swarm optimization	AI	9.250734636723534	-18.96925709242172	20189
1baa30a67d34d40f8662022095b8b66fe1892258	bargaining with limited computation: deliberation equilibrium	multiagent system;pareto efficiency;game theory;resource bounded reasoning;bounded rationality;anytime algorithm;it value;bargaining;perfect bayesian equilibrium;solution concept;optimality condition;automated negotiation;multiagent systems	We develop a normative theory of interaction—negotiation in particular—among self-interested computationally limited agents where computational actions are game theoretically treated as part of an agent’s strategy. We focus on a 2-agent setting where each agent has an intractable individual problem, and there is a potential gain from pooling the problems, giving rise to an intractable joint problem. At any time, an agent can compute to improve its solution to its own problem, its opponent’s problem, or the joint problem. At a deadline the agents then decide whether to implement the joint solution, and if so, how to divide its value (or cost). We present a fully normative model for controlling anytime algorithms where each agent has statistical performance profiles which are optimally conditioned on the problem instance as well as on the path of results of the algorithm run so far. Using this model, we introduce a solution concept, which we call deliberation equilibrium. It is the perfect Bayesian equilibrium of the game where deliberation actions are part of each agent’s strategy. The equilibria differ based on whether the performance profiles are deterministic or stochastic, whether the deadline is known or not, and whether the proposer is known in advance or not. We present algorithms for finding the equilibria. Finally, we show that there exist instances of the deliberation–bargaining problem where no pure strategy equilibria exist and also instances where the unique equilibrium outcome is not Pareto efficient.  2001 Elsevier Science B.V. All rights reserved.	anytime algorithm;computation;existential quantification;nash equilibrium;pareto efficiency	Kate Larson;Tuomas Sandholm	2001	Artif. Intell.	10.1016/S0004-3702(01)00132-1	bargaining problem;bayesian game;game theory;mathematical optimization;computer science;artificial intelligence;management science;mathematical economics;equilibrium selection;solution concept;bounded rationality	AI	-3.2495527768304866	-1.1618564282657247	20213
7f4aaa13d995697f8680d18b49e5701314316c81	control strategies for transit priority based on queue modeling and surrogate testing	traffic simulation;queuing model;microscopic traffic simulation;queuing theory;traffic control;traffic flow;surrogate testing;queueing model;waiting time;traffic signal preemption;statistical sampling;traffic delays;bus priority;traffic signal control systems;sampling methods;traffic signal phases;control strategy;transit priority;queuing modeling	To explore the potential benefits of bus priority based on signal phasing strategy, this article proposes a simplified statistical sampling method to simulate vehicle delays at intersections instead of using conventional microscopic traffic simulations. This method greatly reduces the calculation cost and time while maintaining the estimation accuracy of the delay. This research provides a surrogate testing system and tests intuitive traffic signal phasing strategies that shorten average waiting time of queuing vehicles by letting certain buses pass the intersection with a higher priority. Interestingly, the results reveal that the length of the green time should be adaptive with the proportion of buses in the traffic flow. Moreover, a higher priority is suggested for the bus at certain positions in a queue so that it can pass the intersection in one green period with its preceding vehicles. The benefits of this approach include (1) reduced delay of both buses and other vehicles when the proportion of bus...		Danya Yao;Yuelong Su;Yi Zhang;Li Li;Sihan Cheng;Zheng Wei	2009	J. Intellig. Transport. Systems	10.1080/15472450903084287	sampling;real-time computing;simulation;computer science;mathematics;statistics;computer network	Metrics	10.308042081940108	-9.993989387619443	20292
fb5d3763016434e6ef6f349660713add986637c3	toward graded and nongraded variants of stochastic dominance	science general;stochastic dominance;independent random variables;random variable	Summary. We establish a pairwise comparison method for random variables. This comparison results in a probabilistic relation on a given set of random variables. The transitivity of this probabilistic relation is investigated in the case of independent random variables, as well as when these random variables are pairwisely coupled by means of a copula, more in particular the minimum operator or the ?Lukasiewicz t-norm. A deeper understanding of this transitivity, which can be captured only in the framework of cycle-transitivity, allows to identify appropriate strict or weak thresholds, depending upon the copula involved, turning the probabilistic relation into a strict order relation. Using 1/2 as a fixed weak threshold does not guarantee an acyclic relation, but is always one-way compatible with the classical concept of stochastic dominance. The proposed method can therefore also be seen as a way of generating graded as well as non-graded variants of that popular concept. 		Bernard De Baets;Hans De Meyer	2007		10.1007/978-3-540-36247-0_10	mathematical optimization;combinatorics;mathematics;statistics	ML	0.48527531054253276	-20.278358101649992	20299
7391f39d1922063cae3cd5269a94ef419c5343d1	the periodic capacitated arc routing problem with irregular services	periodic arc routing problem;matheuristics	The aim of this paper is to introduce the periodic capacitated arc routing problem with irregular services. Someapplications can be found in roadmaintenance operations and road network surveillance. The problem consists of determining a set of routes to cover a given network over a time horizon. The roads must be serviced a number of times in sub-periods over the time horizon, according to a hierarchy of arc classes. We present a mathematical model and a heuristic solution approach. © 2011 Elsevier B.V. All rights reserved.	arc routing;computer and network surveillance;heuristic (computer science);linear programming relaxation;mathematical model;mathematical optimization;metaheuristic;numerical linear algebra;time complexity;two-phase locking	I. M. Monroy;Ciro-Alberto Amaya;André Langevin	2013	Discrete Applied Mathematics	10.1016/j.dam.2011.05.014	mathematical optimization;combinatorics;mathematics	AI	16.825978464319096	1.9547285484808903	20338
621780c901882cfe89f37985426c53cbfa17b29c	a method for decision making with the owa operator	desenvolupament sostenible;jocs d estrategia matematica;logica borrosa;info eu repo semantics article;fuzzy logic;games of strategy mathematics;aggregation operator;teoria d operadors;social responsability of business;selection of strategies;responsabilitat social de l empresa;index of maximum and minimum level;operator theory;presa de decisions;owa operator;sustainable development;info eu repo semantics publishedversion	A new method for decision making that uses the ordered weighted averaging (OWA) operator in the aggregation of the information is presented. It is used a concept that it is known in the literature as the index of maximum and minimum level (IMAM). This index is based on distance measures and other techniques that are useful for decision making. By using the OWA operator in the IMAM, we form a new aggregation operator that we call the ordered weighted averaging index of maximum and minimum level (OWAIMAM) operator. The main advantage is that it provides a parameterized family of aggregation operators between the minimum and the maximum and a wide range of special cases. Then, the decision maker may take decisions according to his degree of optimism and considering ideals in the decision process. A further extension of this approach is presented by using hybrid averages and Choquet integrals. We also develop an application of the new approach in a multi-person decision-making problem regarding the selection of strategies.	maxima and minima;ordered weighted averaging aggregation operator	José M. Merigó;Anna Maria Gil Lafuente	2012	Comput. Sci. Inf. Syst.	10.2298/CSIS110206044M	fuzzy logic;ordered weighted averaging aggregation operator;operator theory;computer science;artificial intelligence;sustainable development;weighted sum model	DB	-2.088053579544021	-20.7014970249283	20347
c36bd7bd355a1e624d2f3a3ff9812b3dca6267d9	software adoption under network effects: optimal seeding, sequencing, and pricing	adoption sequencing;network effects;price discrimination;seeding;software	Understanding the process of software adoption is of paramount importance to software start-ups. We study a monopolistic seller’s optimal seeding, sequencing, and pricing strategies under network effects. We demonstrate the importance of adoption sequencing as well as controllability over the seeding process to seller’s profit, consumer surplus, and social welfare. Under multi-pricing, full information, and full control over the seeding process, we show that all segments contain only paying customers except the first one, which contains both seeded and paying customers; and segments are opened in order of the customer valuation. Further, the seller’s optimal strategy is socially optimal. Under single-pricing and limited seeding control, worst case seeding (where all seeds go to the high-valuation customers) leads to higher social welfare and consumer surplus than uniform seeding, as the former covers a larger portion of the market while charging a lower price.	best, worst and average case;goto;random seed;value (ethics)	Yifan Dou;Marius F. Niculescu;D. J. Wu	2011			marketing;controllability;seeding;software;valuation (finance);social welfare;monopolistic competition;computer science;pricing strategies;economic surplus	ECom	-0.44668119312897	-5.805247816109262	20363
5857ef5985d30e2911e09decdc660a9566a706ba	collective intelligence, data routing and braess' paradox	multi agent system;utility function;utility maximization;network routing;artificial intelligent;shortest path routing;side effect;machine learning;load balance;collective intelligence	We consider the problem of designing the the utility functions of the utility-maximizing agents in a multi-agent system (MAS) so that they work synergistically to maximize a global utility. The particular problem domain we explore is the control of network routing by placing agents on all the routers in the network. Conventional approaches to this task have the agents all use the Ideal Shortest Path routing Algorithm (ISPA). We demonstrate that in many cases, due to the side-effects of one agent’s actions on another agent’s performance, having agents use ISPA’s is suboptimal as far as global aggregate cost is concerned, even when they are only used to route in£nitesimally small amounts of traf£c. The utility functions of the individual agents are not “aligned” with the global utility, intuitively speaking. As a particular example of this we present an instance of Braess’ paradox in which adding new links to a network whose agents all use the ISPA results in a decrease in overall throughput. We also demonstrate that load-balancing, in which the agents’ decisions are collectively made to optimize the global cost incurred by all traf£c currently being routed, is suboptimal as far as global cost averaged across time is concerned. This is also due to “sideeffects”, in this case of current routing decision on future traf£c. The mathematics of Collective Intelligence (COIN) is concerned precisely with the issue of avoiding such deleterious side-effects in multi-agent systems, both over time and space. We present key concepts from that mathematics and use them to derive an algorithm whose ideal version should have better performance than that of having all agents use the ISPA, even in the in£nitesimal limit. We present experiments verifying this, and also showing that a machine-learning-based version of this COIN algorithm in which costs are only imprecisely estimated via empirical means (a version potentially applicable in the real world) also outperforms the ISPA, despite having access to less information than does the ISPA. In particular, this COIN algorithm almost always avoids Braess’ paradox.	aggregate data;collective intelligence;dijkstra's algorithm;experiment;load balancing (computing);machine learning;multi-agent system;problem domain;router (computing);routing;shortest path problem;synergy;throughput;verification and validation	Kagan Tumer;David H. Wolpert	2002	J. Artif. Intell. Res.	10.1613/jair.995	routing;simulation;computer science;artificial intelligence;load balancing;machine learning;multi-agent system;mathematics;collective intelligence;side effect	AI	20.030604145095108	-12.544221955370105	20365
e2bd719e0d4cb665a26a32fa87ef4d39519d12ed	review of current technologies and proposed intelligent methodologies for water distributed network leakage detection		Water is a precious resource that should be managed carefully. However, due to leakages in water distributed networks (WDNs), a large amount of water is lost each year that suggests the need for reliable and robust leak detection and localization system. This paper attempts to review the current technologies for leakage detection in WDN as well as several proposed intelligent methodologies (such as support vector machine, neural network, and convolution neural network) over the past few years. The current methodologies and their limitations are discussed. Uncertainties involved in the implementation of WDN leakage detection are also discussed, and several suggestions to overcome such uncertainties are provided for future implementations.		T. K. Chan;Cheng Siong Chin;Xionghu Zhong	2018	IEEE Access	10.1109/ACCESS.2018.2885444	convolutional neural network;support vector machine;implementation;distributed computing;artificial neural network;computer science;leakage (electronics);pipeline transport;water resources	Mobile	13.268235402332829	-16.547332157313893	20408
ab9463c158e2ce4a8ca71a46cbca33fe12e8912c	diversity-driven selection of exploration strategies in multi-armed bandits	multi agent systems dexterous manipulators;robot sensing systems;diversity;mirrors;evolutionary computation;active learning;developmental robotics;redundancy;multi armed bandits;statistics;exploration;robot sensing systems context redundancy mirrors sociology statistics;simulated planar robotic arm diversity driven selection exploration strategies strategy agnostic method multiarmed bandit problem;context;sociology	We consider a scenario where an agent has multiple available strategies to explore an unknown environment. For each new interaction with the environment, the agent must select which exploration strategy to use. We provide a new strategy-agnostic method that treat the situation as a Multi-Armed Bandits problem where the reward signal is the diversity of effects that each strategy produces. We test the method empirically on a simulated planar robotic arm, and establish that the method is both able discriminate between strategies of dissimilar quality, even when the differences are tenuous, and that the resulting performance is competitive with the best fixed mixture of strategies.	multi-armed bandit;robot;robotic arm	Fabien C. Y. Benureau;Pierre-Yves Oudeyer	2015	2015 Joint IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)	10.1109/DEVLRN.2015.7346130	simulation;engineering;artificial intelligence;machine learning	Robotics	24.256497526334112	-17.148285339923984	20418
cd83b454149b7e5ef60d7c825be4ed6f6d815f34	a research on the improvement of dual optimization on bp neural network		At present, the most widely used BP network model in traffic, agriculture and related data mining is still an important neural network algorithm model, but its performance has not been satisfactory. The convergence and prediction accuracy of the BP neural network is general and easy to fall into the local optimal solution, and these shortcomings still need to be improved continuously. Therefore, in view of the above mentioned problems, an improved BP network algorithm is proposed by combining the dynamic self-learning effect factor and the improved network activation function. Experimental results show that the proposed improved BP network scheme can greatly improve the convergence efficiency and accuracy of BP neural networks.	activation function;algorithm;artificial neural network;data mining;lagrange multiplier;network model	Ruliang Wang;Yang Xuan	2017	2017 13th International Conference on Computational Intelligence and Security (CIS)	10.1109/CIS.2017.00114	machine learning;computer science;artificial intelligence;artificial neural network;network model;computational intelligence;convergence (routing);activation function	ML	10.96543809467291	-22.2649673424249	20459
3f33703e8e13879067ece50a12b47287b20f18cf	increasing the efficiency of the investments to be made in a portfolio of it projects: a data envelopment analysis approach	point of view;data envelope analysis	This article introduces a method that not only allows managers to determine the efficiency of the investments to be made in portfolios of IT projects from a multi-criteria point of view, but also provides guidelines on how to improve their efficiency.	data envelopment analysis	Rui de Oliveira Victorio;Antonio J. Alencar;Eber A. Schmitz;Armando Leite Ferreira	2008		10.1007/978-3-540-88871-0_27	financial economics;actuarial science;economics;systems engineering	SE	-4.2789717893771195	-15.165261342623996	20460
2c3ce8bf7798e31bbc60a9a6848d9f421cd6d323	nurse preference rostering using agents and iterated local search	agent scheduling;iterated local search;staff scheduling;nurse rostering;nurse scheduling;agent programming	This study presents an iterative local search (ILS) framework used within an agentbased nurse rostering system. This agent based system with the ILS (CNR-ILS) is used to improve nurse rosters with respect to nurse preferences. The system is a heuristic that considers a variety of nurse considerations including informal requested days off, preferences for days-of-the-week off, and preferences for specific numbers of consecutive days off. Since our implementation is not subject to the complexity of mathematical programs, it includes more preference considerations. The system is extensively tested in a federal hospital and is compared to both an integer programming solution from the literature and the scheduling methodology currently used at the test facility. When nurses are surveyed, the solutions found by this system are consistently favored over the solutions from the integer program and hospital’s current scheduling practice. Furthermore the variation of those ratings is significantly less.	agent-based model;heuristic;integer programming;iterated local search;iteration;iterative method;local search (optimization);nurse scheduling problem;scheduling (computing)	Chiaramonte Michael;Cochran Jeffery;Caswell David	2015	Annals OR	10.1007/s10479-014-1701-8	nurse scheduling problem;mathematical optimization;real-time computing;simulation;computer science;operations management;iterated local search;mathematics	AI	13.530628711749968	-0.8570539157783154	20530
a7214e46cd4efadc91f56abaf85f9bc8108ba4a9	combining an evolutionary algorithm with the multilevel paradigm for the simulation of complex system		Evolutionary Algorithms have become an efficient tool to simulate large and complex systems that require a huge amount of computational resources. Nevertheless, evolutionary algorithms may still suffer from either slow or premature convergence preventing the search to visit more promising areas, and thus leading to solutions of poor quality. In this work, the multilevel paradigm is used in order to enhance the evolutionary algorithm’s performance for simulating large industrial instances. The multilevel paradigm refers to the process of dividing large and difficult problems into smaller ones, which are hopefully much easier to solve, and then work backward towards the solution of the original problem, using a solution from a previous level as a starting solution at the next level. Experimental results comparing the multilevel evolutionary algorithm against its single-level variant are presented.	complex system;complex systems;computation;computational resource;evolutionary algorithm;multi-level cell;premature convergence;programming paradigm;simulation	Noureddine Bouhmala;Karina Hjelmervik;Kjell Ivar Øvergård	2013		10.7148/2013-0753	evolutionary programming;machine learning	AI	22.857279721691114	-4.251256545902973	20569
aca4767340556e95fc5fa2e424519e23e55544f4	information percolation in segmented markets	market equilibrium;over the counter markets;information sources;learning;information transmission;double auctions;search;percolation;matching;information quality;developing country;profitability;segmented markets;equilibrium;private information;double auction;information	We calculate equilibria of dynamic double-auction markets in which agents are distinguished by their preferences and information. Over time, agents are privately informed by bids and offers. Investors are segmented into groups that differ with respect to characteristics determining information quality, including initial information precision as well as market “connectivity,” the expected frequency of their trading opportunities. Investors with superior information sources attain higher expected profits, provided their counterparties are unable to observe the quality of those sources. If, however, the quality of bidders’ information sources are commonly observable, then, under conditions, investors with superior information sources have lower expected profits. Duffie is at the Graduate School of Business, Stanford University and is an NBER Research Associate. Malamud is at Swiss Finance Institute at EPF Lausanne. Manso is at the Sloan School of Business, MIT. We are grateful for research assistance from Xiaowei Ding, Michelle Ton, and Sergey Lobanov, and for discussion with Daniel Andrei, Luciano I. de Castro, Julien Cujean, Eiiricho Kazumori, and Phil Reny. Malamud gratefully acknowledges financial support by the National Centre of Competence in Research “Financial Valuation and Risk Management” (NCCR FINRISK).	bilateral filter;eclipse process framework;information quality;percolation;precedence effect;rebecca hargrave malamud;risk management;uc browser;value (ethics)	Darrell Duffie;Semyon Malamud;Gustavo Manso	2014	J. Economic Theory	10.1016/j.jet.2014.05.006	financial economics;matching;private information retrieval;information;economics;developing country;finance;percolation;double auction;microeconomics;information quality;commerce;profitability index	ECom	-3.3745774260796364	-5.141101921676119	20580
040bba3e67ea3468b90f3b680bf4bbe9ce1274d8	a comparative study of regression and evolution-based stock selection models for investor sentiment	finance;behavioral finance evolution based stock selection models regression models machine learning investor sentiment indicators feature selection model parameters genetic algorithms ga based method regression based method;biological system modeling;portfolios finance optimization benchmark testing genetic algorithms biological system modeling;portfolios;investment;regression analysis genetic algorithms investment learning artificial intelligence;regression;machine learning;stock selection;genetic algorithms;regression analysis;optimization;learning artificial intelligence;benchmark testing;genetic algorithms stock selection machine learning regression	Stock selection has long been recognized as an important task in finance. Researchers and practitioners in this area often use regression models to tackle this problem due to their simplicity and effectiveness. Recent advances in machine learning (ML) are leading to significant opportunities to solve these problems more effectively. In this paper, we present a comparative study between the traditional regression-based and evolution-based models using investor sentiment indicators for stock selection. In the evolution-based models, Genetic Algorithms (GA) are used for optimization of model parameters and feature selection of input variables simultaneously. We will show that our proposed GA-based method significantly outperforms the traditional regression-based method as well as the benchmark. We thus expect this evolution-based methodology to advance the research in machine learning for behavioral finance.	benchmark (computing);evolution;feature selection;genetic algorithm;machine learning;mathematical optimization;sentiment analysis;software release life cycle	Chien-Feng Huang;Tsung-Nan Hsieh;Bao Rong Chang;Chih-Hsiang Chang	2012	2012 Third International Conference on Innovations in Bio-Inspired Computing and Applications	10.1109/IBICA.2012.67	financial economics;economics;machine learning;data mining	AI	6.169736149453788	-18.787023385084044	20585
11066f117bfd82a6a78d8e07161eb46eefbab1d8	on an eoq model for deteriorating items with time-varying demand and partial backlogging	forecasting;time varying;reliability;project management;information systems;maintenance;soft or;information technology;packing;shortages;operations research;location;investment;journal;journal of the operational research society;inventory;purchasing;history of or;logistics;marketing;scheduling;deterioration;lot sizing;production;communications technology;computer science;operational research;partial backlogging;applications of operational research;or society;jors;management science;infrastructure	For seasonal products, fashionable commodities and high-tech products with a short product life cycle, the willingness of a customer to wait for backlogging during a shortage period is diminishing with the length of waiting time. Recently, Chang and Dye developed an inventory model in which the backlogging rate declines as the waiting time increases. In this paper, we complement the shortcoming of their model by adding the non-constant purchase cost into the model. In addition, we show that the total cost is a convex function of the number of replenishments. We further simplify the search process by providing an intuitively good starting value, which reduces the computational complexity significantly. Finally, we characterize the influences of the demand patterns over the replenishment cycles and others.	economic order quantity	J.-T. Teng;H.-L. Yang;Liang-Yuh Ouyang	2003	JORS	10.1057/palgrave.jors.2601490	project management;logistics;inventory;economics;forecasting;investment;marketing;operations management;reliability;location;operations research;information technology;scheduling;commerce	ML	2.9807927767571964	-4.822675169953698	20607
84501df1df7acbd641945093dec4847655e5be72	the optimal assignment of facilities to locations by branch and bound	branch and bound	"""The problem of assigning facilities to locations consists of the following: in the general case there are n fixed locations to which n facilities must be assigned. Each facility may be assigned to one and only one location. There are n! feasible assignments. The """"distance"""" between any pair of locations is the cost of transporting a unit of material between the locations. The """"traffic intensity"""" is the rate at which units of material are transferred between a given pair of facilities in both directions. An optimal assignment is one in which the sum of the product of distance times traffic intensity for all pairs of facility-location assignments is a minimum. The branch-and-bound technique with modifications is used to give an optimal assignment."""	branch and bound	J. W. Gavett;Norman V. Plyter	1966	Operations Research	10.1287/opre.14.2.210	mathematical optimization;operations management;mathematics;branch and bound	DB	16.40144187404953	4.098163215052939	20650
d175407b3d40628aa550078add8cb8183bac310f	scheduling medical tests: a solution to the problem of overcrowding in a hospital emergency department	patient diagnosis;emergency department;hospitals;public domain software constraint handling emergency management hospitals patient diagnosis;jobshop problem;emergency management;public domain software;electrocardiography;minizinc medical test scheduling overcrowding problem hospital emergency department constraint logic programming problem open source software;magnetic resonance imaging;scheduling medical tests;magnetic resonance imaging electrocardiography;constraint handling;operational research scheduling medical tests emergency department jobshop problem constraint logic programming;constraint logic programming;operational research	To improve the performance of hospital emergency department (ED), we propose a scheduling strategy to reduce the total time of medical tests in ED for a given number of patients. We model the schedulig strategy as a constraint logic programming problem, and then solve this problem with an open source software: Minizinc. Also we compare the medical test time with the scheduling strategy and that without scheduling, which is viewed as a benchmark for our proposed scheduling strategy. The results show that the scheduling strategy can save 12%-25% of medical test time.	benchmark (computing);constraint logic programming;open-source software;scheduling (computing)	Di Lin;Fabrice Labeau;Xidong Zhang;Guixia Kang	2012	2012 IEEE 14th International Conference on e-Health Networking, Applications and Services (Healthcom)	10.1109/HealthCom.2012.6380074	fair-share scheduling;nurse scheduling problem;simulation;medicine;flow shop scheduling;dynamic priority scheduling;rate-monotonic scheduling;emergency medicine;medical emergency	EDA	13.021599378029906	-1.2753991165868095	20672
0b1b7e99071fbe662d7883f7fa1ade63ab893d62	optimal prices and trade-in rebates for durable, remanufacturable products	product age profile;life cycle;pricing;durable products;profitability;remanufacturing;price discrimination;trade in rebates	Most durable products have two distinct types of customers: first-time buyers and customers who already own the product, but are willing to replace it with a new one or purchase a second one. Firms usually adopt a price-discrimination policy by offering a trade-in rebate only to the replacement customers to hasten their purchase decisions. Any return flow of products induced by trade-in rebates has the potential to generate revenues through remanufacturing operations. In this paper, we study the optimal pricing/trade-in strategies for such durable, remanufacturable products. We focus on the scenario where the replacement customers are only interested in trade-ins. In this setting, we study three pricing schemes: (i) uniform price for all customers, (ii) age-independent price differentiation between new and replacement customers (i.e., constant rebate for replacement customers), and (iii) age-dependent price differentiation between new and replacement customers (i.e., age-dependent rebates for replacement customers). We characterize the roles that the durability of the product, the extent of return revenues, the age profile of existing products in the market, and the relative size of the two customer segments play in shaping the optimal prices and the amount of trade-in rebates offered. Throughout the paper we highlight the operational decisions that might influence the above factors, and we support our findings with real-life practices. In an extensive numerical study, we compare the profit potential of different pricing schemes and quantify the reward (penalty) associated with taking into account (ignoring) customer segmentation, the price-discrimination option, return revenues, and the age profile of existing products. On the basis of these results, we are able to identify the most favorable pricing strategy for the firm when faced with a particular market condition and discuss implications on the life-cycle pricing of durable, remanufacturable products.		Saibal Ray;Tamer Boyaci;Necati Aras	2005	Manufacturing & Service Operations Management	10.1287/msom.1050.0080	pricing;biological life cycle;economics;marketing;microeconomics;price discrimination;commerce;profitability index	Theory	-1.051942881769321	-7.138933882002431	20674
976f8ddd69fe6414c4ba51f4d3e7656a49a284e6	testing linearity in a cointegrating str model for the money demand function: international evidence from g-7 countries	smooth transition regression;money demand function;non linear cointegration;money demand;g 7 countries;economic theory	The motivation behind this paper is to re-investigate the stability of the long-run money demand function (MDF) in a non-linear cointegrating framework for G-7 countries. Previous studies on non-linearity in the MDF are only related to the short-run dynamics and assume that long-run cointegrating relations are linear, which according to economic theory need not be the case. Thus, we really need to focus on the variables in the long-run MDF and their determinants through the adoption of a cointegrating smooth transition regression (CSTR) test developed by [I. Choi, P. Saikkonen, Testing linearity in cointegrating smooth transition regressions, Economet. J. 7 (2004) 341-365]. The reason is due to this model being more general than the traditional STR model in that it may contain several transition functions and has more than a single transition variable. Our evidence demonstrates the existence of a non-linear cointegrating relationship, and as such several transition variables should be of more concern under the non-linear hypothesis. Overall, we propose more possibilities that will bring about the unstable phenomenon of the long-run MDF.		Chien-Chiang Lee;Pei-Fen Chen;Chun-Ping Chang	2007	Mathematics and Computers in Simulation	10.1016/j.matcom.2006.12.012	econometrics	Arch	-0.42536149132590495	-10.430231299583628	20717
4dec2eac67b5a3d4b74b700ad6532c9ca21ee45b	iterative distribution-aware sampling for probabilistic symbolic execution	monte carlo sampling;conference paper;symbolic execution;probabilistic analysis	Probabilistic symbolic execution aims at quantifying the probability of reaching program events of interest assuming that program inputs follow given probabilistic distributions. The technique collects constraints on the inputs that lead to the target events and analyzes them to quantify how likely it is for an input to satisfy the constraints. Current techniques either handle only linear constraints or only support continuous distributions using a “discretization” of the input domain, leading to imprecise and costly results. We propose an iterative distribution-aware sampling approach to support probabilistic symbolic execution for arbitrarily complex mathematical constraints and continuous input distributions. We follow a compositional approach, where the symbolic constraints are decomposed into sub-problems whose solution can be solved independently. At each iteration the convergence rate of the com- putation is increased by automatically refocusing the analysis on estimating the sub-problems that mostly affect the accuracy of the results, as guided by three different ranking strategies. Experiments on publicly available benchmarks show that the proposed technique improves on previous approaches in terms of scalability and accuracy of the results.	benchmark (computing);constraint (mathematics);discretization;experiment;iteration;rate of convergence;sampling (signal processing);scalability;symbolic execution	Mateus Borges;Antonio Filieri;Marcelo d'Amorim;Corina S. Pasareanu	2015		10.1145/2786805.2786832	mathematical optimization;probabilistic analysis of algorithms;computer science;theoretical computer science;machine learning;monte carlo method	SE	22.048018594216217	-14.784256054986113	20737
4146e1719ef6d2cc6bf0bac6e128b0b2cd328cd0	managing expected return of investors: convertible bonds in china	emerging market;expected returns;international comparison;convertible bonds;management of expected return	This article studies the market reaction surrounding the announcement of the convertible bonds issuance in Chinese market, and partly explains the difference of influence on underlying securities in mature market such as the US market and emerging market in China. Meanwhile, it reveals how insiders in Chinese market use convertible bonds as a tool to manage the expected return of external investors.		Ruyan Yang;Hui Meng;Feng Xu	2007	International Journal of Information Technology and Decision Making	10.1142/S0219622007002393	bond market;economics;bond market index;market neutral;finance;financial system;convertible arbitrage;emerging markets;convertible bond	Theory	-1.5036525505943725	-9.69076708096142	20796
1b1a8748ef6b83ec3a56f119ea58b6fd51cf0c07	"""evolutionary stability and dynamic stability in generalized """"rock-scissors-paper"""" games"""				Franz J. Weissing	1989				Robotics	17.756019876931408	-9.204479982968934	20828
6a3d63921f246f5ac799468bf0fa91c932ea33d4	a dynamic vehicle routing problem with multiple delivery routes	acceptance rule;adaptive large neighborhood search;multiple routes;scenarios;dynamic vehicle routing	This paper considers a vehicle routing problem where each vehicle performs delivery operations over multiple routes during its workday and where new customer requests occur dynamically. The proposed methodology for addressing the problem is based on an adaptive large neighborhood search heuristic, previously developed for the static version of the problem. In the dynamic case, multiple possible scenarios for the occurrence of future requests are considered to decide about the opportunity to include a new request into the current solution. It is worth noting that the real-time decision is about the acceptance of the new request, not about its service which can only take place in some future routes (a delivery route being closed as soon as a vehicle departs from the depot). In the computational results, a comparison is provided with a myopic approach which does not consider scenarios of future requests.	heuristic;real-time transcription;rejection sampling;vehicle routing problem	Nabila Azi;Michel Gendreau;Jean-Yves Potvin	2012	Annals OR	10.1007/s10479-011-0991-3	simulation;operations management	Robotics	13.73196467289991	0.8085389942255915	20850
44f6be85caf48246aeda7f2046d794092dfd4f01	learning bayesian networks with algebraic differential evolution		In this paper we introduce DEBN, a novel evolutionary algorithm for learning the structure of a Bayesian Network. DEBN is an instantiation of the Algebraic Differential Evolution which is designed and applied to a particular (product) group whose elements encode all the Bayesian Networks of a given set of random variables. DEBN has been experimentally investigated on a set of standard benchmarks and its effectiveness is compared with BFO-B, a recent and effective bacterial foraging algorithm for Bayesian Network learning. The experimental results show that DEBN largely outperforms BFO-B, thus validating our algebraic approach as a viable solution for learning Bayesian Networks.	bayesian network;differential evolution	Marco Baioletti;Alfredo Milani;Valentino Santucci	2018		10.1007/978-3-319-99259-4_35	encode;machine learning;algebraic number;differential evolution;evolutionary algorithm;computer science;foraging;bayesian network;random variable;artificial intelligence	Vision	23.424909431899707	-8.177006269430967	20886
66d6ccf6f673601de4fd9c21cd57fc2d860aae9f	construction of investor sentiment index in the chinese stock market		This paper focuses on improving the adaptability of the investor sentiment index introduced by Baker and Wurgler[1] in the Chinese stock market. Considering not all the original proxies for sentiment are suitable for Chinese stock market, a new combination of proxies to form the investor sentiment index is proposed. Based on this investor sentiment index, the relationship between investor sentiment and stock price index in Chinese stock market is found.	index (publishing);inverted index;linear model;nonlinear system	Yuxi Yang;Takashi Hasuike	2017	2017 6th IIAI International Congress on Advanced Applied Informatics (IIAI-AAI)	10.1109/IIAI-AAI.2017.93	stock market index;financial economics;stock market;adaptability;behavioral economics;price index;economics	NLP	3.9834100016861385	-13.920905195488855	20889
57578498d74011b1f4db62c6afb8bd95fc5fc0dd	the application of neural networks to the papermaking industry	neural networks pulp and paper industry pulp manufacturing particle measurements industrial control humidity manufacturing industries bayesian methods multilayer perceptrons printers;learning neural networks paper industry paper curl quality control probability bayesian inference collinearity reduction confidence measures multilayer perceptron symbolic data;estimation theory;pulp manufacturing;collinearity reduction;probability;neural networks;learning;printers;probability paper industry process control multilayer perceptrons estimation theory bayes methods quality control;symbolic data;particle measurements;bayes methods;multilayer perceptrons;bayesian inference;confidence measures;bayesian methods;manufacturing industries;multilayer perceptron;paper curl;pulp and paper industry;paper industry;humidity;industrial control;process control;quality control	"""This paper describes the application of neural network techniques to the papermaking industry, particularly for the prediction of paper """"curl."""" Paper curl is an important quality measure that can only be measured reliably off-line after manufacture, making it difficult to control. Here we predict, before paper manufacture from characteristics of the current reel, whether the paper curl will be acceptable and the level of curl. For both the case of predicting the probability that paper will be """"out-of-specification"""" and that of predicting the level of curl, we include confidence intervals indicating to the machine operator whether the predictions should be trusted. The results and the associated discussion describe a successful application of neural networks to a difficult, but important, real-world task taken from the papermaking industry. In addition the techniques described are widely applicable to industry where direct prediction of a quality measure and its acceptability are desirable, with a clear indication of prediction confidence."""		Peter J. Edwards;Alan F. Murray;Georgios A. Papadopoulos;A. Robin Wallace;John Barnard;Gordon Smith	1999	IEEE transactions on neural networks	10.1109/72.809090	quality control;bayesian probability;artificial intelligence;machine learning;humidity;process control;probability;mathematics;manufacturing;estimation theory;multilayer perceptron;bayesian inference;artificial neural network;statistics	ML	15.516220381656705	-18.09540682579695	20916
572a82d0123a0ef7d5af2b0b00a0e3d68d0444c8	using fuzzy and fractal methods for analyzing market time series	market timing	In this contribution, we investigate the possibilities of using fuzzy and fractal methods for analyzing time series of market data. First, we implemented and tested a fuzzy component that provides fuzzyfication by the Mamdani Larsen inference method with static rules using not only Gauss but also Cauchy and Mandelbrot distribution. Second, we implemented and tested a fractal component that provides fuzzy clustering by the Takagi Sugeno method with dynamic fuzzy rules. Looking for an optimum, we simulated many parameter combinations and compared the results. We present some interesting results of our experiments.	audio feedback;cluster analysis;colors of noise;document classification;expect;experiment;fractal analysis;fuzzy clustering;fuzzy logic;information privacy;mandelbrot set;randomness;super smash bros.;time series	Petr Kroha;Marcus Lauschke	2010			financial economics;econometrics;economics;marketing	ML	4.43824194708702	-15.557468516491525	20942
5dbb11a7355e99fa7aae59b0dcaa546f4e21b340	a genetic algorithm approach to the artillery target assignment problem		In this work a new assignment problem with the name “Artillery Target Assignment Problem (ATAP)” is defined. ATAP is about assigning artillery guns to targets at different time instances while some objective functions are to be optimized. Since an assignment made for any time instance effect the value of the shooting ATAP is harder than the classical assignment problem. As far as the objective functions are concerned we define a base case problem. We elaborate on some possible variations of the base case which are exceptionally interesting in the military domain. For two of such problems, genetic algorithm solutions with customized representations and genetic operators are developed and presented.	admissible heuristic;assignment problem;asynchronous array of simple processors;genetic algorithm;genetic operator;np (complexity);np-hardness;recursion;software release life cycle	Burçin Sapaz;Ismail Hakki Toroslu;Göktürk Üçoluk	2010		10.1007/978-90-481-9794-1_8	weapon target assignment problem	PL	24.123301490874724	3.4570220482029	20948
3ffd9b99e9c91106115fb28361fe3aafa88f55b9	equipment possession quantity modeling and particle swarm optimization	least squares approximations;least squares method;industrial plants;biological system modeling;particle swarm optimization equipment possession quantity modeling least squares method subsection;equipment possession quantity;least square method;production equipment equipment evaluation industrial plants least squares approximations particle swarm optimisation;equipment evaluation;particle swarm optimization mathematics mathematical model optimization methods particle production least squares methods machinery educational technology optimized production technology genetics;computational modeling;adaptation model;particle swarm optimizer;subsection;particle swarm optimization;production equipment;mathematical model;optimization;adaptive inertia weight model equipment possession quantity modeling particle swarm optimization reasonable manufactory equipment amount production demand least squares method linearly decreasing inertia weight model;particle swarm optimization algorithm;modeling;particle swarm optimisation;parameter optimization;basic particle swarm optimization	Equipment and facility are the important basis of the production for the manufactory. Equipment possession quantity is the reasonable manufactory equipment amount that meets the production demand. The relationship among equipment possession quantity, the ratio of equipment in good condition and equipment quantity on duty is introduced, and mathematics models of equipment possession quantity is described and presented. Least squares method is employed to make the equipment possession quantity parameter optimization model and particle swarm optimization algorithm is used to optimize the model. To improve the computational precision of the model, the equipment possession quantity model is analyzed by subsection according to the evaluation range of equipment amount. Furthermore, the basic particle swarm optimization model (PSO), the linearly decreasing inertia weight particle swarm optimization model (LDPSO) and the adaptive inertia weight particle swarm optimization model (APSO) are respectively employed to optimize the parameters of the equipment possession quantity model. The computational results show that all of the three algorithms efficiently optimize the model and the performance of LPSO is worse than PSO and APSO. Moreover, the precision of equipment possession quantity model by subsection is much better than the former research.	algorithm;computation;least squares;mathematical optimization;particle swarm optimization;phase-shift oscillator	Zhixiong Liu;Runjun Zhao	2009	2009 Third International Conference on Genetic and Evolutionary Computing	10.1109/WGEC.2009.8	mathematical optimization;multi-swarm optimization;simulation;computer science;least squares;statistics	Robotics	10.84587404651747	-17.65026050360427	20966
772858261600b0c98fb91209fbb33c70f3111cbc	enhanced index tracking based on multi-objective immune algorithm	artificial immune system;immune algorithm;transaction cost;multi objective optimization;indexation;portfolio management;enhanced index tracking	Enhanced index tracking is a popular strategy in portfolio management that focuses on adding reliable value relative to the index on the basis of mimicking the behavior of the benchmark index. In this paper, we propose a multi-objective optimization scheme for the enhanced index tracking problem, which provides the framework of defining the objectives as both maximizing the degree of beating the benchmark index and minimizing the accumulated error of underperforming the benchmark. Transaction costs are limited in the constraints. An immunity-based multi-objective optimization algorithm is presented to search for the solution of the enhanced index tracking problem. Treatment of infeasibility and solution selection are also presented. Our proposed approach is implemented to five data sets drawn from major world markets. The computational results compared with other published results show that our method has superior performance. 2010 Elsevier Ltd. All rights reserved.	algorithm;benchmark (computing);computation;database index;fitness function;mathematical optimization;multi-objective optimization;risk measure	Qian Li;Linyan Sun;Liang Bao	2011	Expert Syst. Appl.	10.1016/j.eswa.2010.11.001	mathematical optimization;transaction cost;computer science;multi-objective optimization;machine learning;artificial immune system;project portfolio management	AI	20.251190991909098	-3.8563437225348114	20972
1fe371b130797b364c7f42764659fdf80e7791c8	entropy-based sensor selection for condition monitoring and prognostics of aircraft engine		Abstract The condition of the aircraft engine directly affects the reliability and safety of the aircraft. To monitor its condition, many sensors are deployed on or in the aircraft engine. However, only parts of the sensors can provide useful information for failure prognostics. An entropy-based sensor selection method which can provide quantitative description of the information contained in sensor data is proposed in this paper. Selecting the appropriate sensors cannot only help reduce the amount of data processing, but also increase the performance of condition monitoring. The experiments implemented with sensor data sets from NASA Ames Research Center prove the effectiveness of the proposed method.	naruto shippuden: clash of ninja revolution 3;sensor	Liansheng Liu;Shaojun Wang;Datong Liu;Yujie Zhang;Yu Peng	2015	Microelectronics Reliability	10.1016/j.microrel.2015.06.076	reliability engineering;engineering;automotive engineering;forensic engineering	Logic	13.004705528917306	-14.761364758290766	20983
e55d302817903b68314bed7f2d660bb414581483	potentials and limitations of green light optimal speed advisory systems	road traffic carbon compounds intelligent transportation systems;road traffic;intelligent transportation systems;vehicles air pollution fuels acceleration gears conferences green products;co 2 green light optimal speed advisory systems glosa smart traffic lights co 2 emissions environmental related metrics traffic density;carbon compounds	The reduction of CO2 emissions is one of the most anticipated features of future transportation systems. Smart traffic lights are believed to contribute to achieving this by either adapting their signal program or by informing approaching drivers. In this paper we investigate the potentials and limitations of the latter, that is, Green Light Optimal Speed Advisory (GLOSA) systems in a realistic, large scale simulation study. We examine the impact of different equipment rates of both traffic lights and vehicles on environmental related metrics but also study how these systems can increase the comfort for drivers by reducing waiting times and the number of stops. We find that at low traffic densities these systems can meet all their goals and lower CO2 emissions by up to 11.5% whereas in dense traffic several side-effects could be observed, including overall longer waiting times and even higher CO2 emissions for unequipped vehicles.	approximation;device driver;simulation	David Eckhoff;Bastian Halmos;Reinhard German	2013	2013 IEEE Vehicular Networking Conference	10.1109/VNC.2013.6737596	intelligent transportation system;simulation;computer science;compounds of carbon	Metrics	9.178700972141673	-9.47709820216314	21021
062104c32e4ba336191aad280377ae6abf4b8633	particle swarm optimization algorithm as a tool for profile optimization	decision tree;data mining;particle swarm optimization;self organizing map;support vector machine;neural network;bayesian networks	Complex analytical environment is challenging environment for finding customer profiles. In situation where predictive model exists like Bayesian networks challenge became even bigger regarding combinatory explosion. Complex analytical environment can be caused by multiple modality of output variable, fact that each node of Bayesian network can potetnitaly be target variable for profiling, as well as from big data environment, which cause data complexity in way of data quantity. As an illustration of presented concept particle swarm optimization algorithm will be used as a tool, which will find profiles from developed predictive model of Bayesian network. This paper will show how partical swarm optimization algorithm can be powerfull tool for finding optimal customer profiles given target conditions as evidences within Bayesian networks.	algorithm;particle swarm optimization;program optimization	Goran Klepac	2015	IJNCR	10.4018/IJNCR.2015100101	support vector machine;mathematical optimization;multi-swarm optimization;self-organizing map;computer science;artificial intelligence;machine learning;decision tree;bayesian network;data mining;particle swarm optimization;artificial neural network	EDA	4.431255181538343	-18.90942111503463	21070
2d8f6552d3ccb937881b5ed89276e5e34e4520b1	a measurement tool to track drones battery consumption during flights				Luis Corral;Ilenia Fronza;Nabil El Ioini;Aristea Ibershimi	2016		10.1007/978-3-319-44215-0_28	embedded system;simulation;aeronautics	Logic	17.048848320631773	-13.01043412934197	21104
7ce2de36e040b48b8076c8871c11b49c5344eaaa	an integrated production-inventory system with lot streaming and complete backordering in a three-stage multi-firm supply chain solved algebraically	inventory;the methods of complete squares and perfect squares;production	We formulate an integrated model of a three-stage multi-firm supply chain based on an integer multiplier at each stage, lot streaming allowed for all suppliers and manufacturers, and complete backordering allowed for some/all retailers. Then we derive the optimal solution to the integrated model using the methods of complete squares and perfect squares. These are simple algebraic approaches so that ordinary readers unfamiliar with differential calculus can understand the optimal solution procedure with ease. For our model, we also need check that the optimal solution, which is algebraically derived, is a global one. We solve a numerical example to illustrate the procedure. We finally deduce Ben-Daya and Al-Nassar’s (2008) models and remark on extending to a higher stage multi-firm supply chain from the integrated model.	scrum (software development)	Kit-Nam Francis Leung	2012	IJBPSCM	10.1504/IJBPSCM.2012.044971	mathematical optimization;inventory;economics;marketing;operations management;mathematical economics	Robotics	2.9068313370667584	-5.628033679408967	21113
b658594573f0f6d16bf994c8bcd4da7f31ffce0a	genetic algorithm based approach to concept solving for mechanical product in conceptual design	iterative process;mechanical product;encoding method;design automation;genetic operator;genetic algorithms mechanical products product design algorithm design and analysis process design design optimization optimization methods encoding morphology iterative methods;design engineering;mechanical products combinatorial mathematics design engineering genetic algorithms iterative methods;genetics;iterative methods;morphology;conceptual design;functional analysis;genetic algorithm;genetic algorithms;optimization;combinatorial optimization;encoding;combinatorial mathematics;mechanical products;gallium;encoding method genetic algorithm conceptual design mechanical product combinatorial optimization iterative process	Concept generation in conceptual design is a process of combinatorial optimization in nature. In this paper, Genetic Algorithm (GA) is utilized as a feasible tool to solve the problem of combinatorial optimization in concept generation, in which an improved encoding method of morphology matrix based on function analysis is applied, and a sequence of optimal concepts are generated through the search and iterative process controlled by genetic operators, including selection,crossover, mutation, and reproduction in GA. Several crucial problems on GA are discussed, such as the calculation of fitness value and the criteria for heredity termination, which have a heavy effect on selection of better concepts. In this work, concept generation is implemented using GA, which can facilitate not only generating several better concepts, but also selecting the best concept. Thus optimal concepts can be conveniently developed and design efficiency can be greatly improved.	combinatorial optimization;crossover (genetic algorithm);fuzzy concept;genetic algorithm;genetic operator;iterative method;mathematical morphology;mathematical optimization	Rui-feng Bo	2009	2009 Fifth International Conference on Natural Computation	10.1109/ICNC.2009.408	mathematical optimization;computer science;systems engineering;algorithm	Robotics	19.71614080503876	-1.8024378242322427	21168
f79ec326db0394917682e3a23c15ba6f3b06571e	optimizing machine assignment and loop layout in tandem agv workshop by co-evolutionary methodology	transports cost tandem agv system co evolutionary methodology layout design machine assignment;layout;genetic algorithm machine assignment optimization loop layout optimization tandem agv workshop coevolutionary methodology automated guided vehicles cost reduction material transport manufacturing process;mathematical model;genetic algorithms;optimization;layout conferences mathematical model genetic algorithms optimization floors;floors;conferences;remotely operated vehicles cost reduction genetic algorithms manufacturing processes materials handling	This paper proposed a co-evolutionary methodology to optimize the layout of a practical tandem automated guided vehicles (AGV) workshop aiming to reduce the costs for material transporting in manufacturing process. This methodology provided a fresh line to address the machine assignment, internal and external loop layout, as well as the loops arrangement on the floor synthetically, the transfer station setting was also considered. This methodology is not like the previous method that solved these contents in sequence. Genetic algorithm (GA) was herein applied for the iteration after these contents were ascertained. A mathematical model was also established for this methodology. The optimization result illustrated the efficiency of proposed co-evolutionary methodology in decreasing the material transporting costs comparing with the non co-evolutionary method. The workshop layout design can apply our methodology, which involved an overall consideration, as guidance.	control flow;evolutionary algorithm;genetic algorithm;iteration;mathematical model;mathematical optimization;optimizing compiler;software release life cycle	Luyang Hou;Zhuang-Cheng Liu;Yanjun Shi;Xiaojun Zheng	2016	2016 IEEE 20th International Conference on Computer Supported Cooperative Work in Design (CSCWD)	10.1109/CSCWD.2016.7565999	layout;simulation;genetic algorithm;computer science;mathematical model	EDA	13.395882765541083	3.347491775413233	21199
586095b6fe34b06add07b85df0e216f1f8760608	the sequential framework for heat exchanger network synthesis - the minimum number of units sub-problem	sequential framework;discrete optimization;design process;heat exchanger network;set partitions;limiting factor;mathematical programming;heat exchanger network synthesis;transhipment problem;user interaction;lp relaxation	An overview of an iterative and sequential methodology, called the Sequential Framework for heat exchanger network synthesis (HENS), is presented in the paper. The main objective of the Sequential Framework is to solve industrial size problems. The subtasks of the design process are solved sequentially using mathematical programming. There are two main advantages of the methodology. First, the design procedure is, to a large extent, automated while keeping significant user interaction. Second, the subtasks of the framework (MILP and NLP problems) are much easier to solve numerically than the MINLP models that have been suggested for HENS.#R##N##R##N#One of the limiting factors in the methodology is related to the two MILP models where significant improvements are required to prevent combinatorial explosion. To ease this problem for the minimum number of units MILP sub-problem, two different approaches to the issue were studied. The minimum number of units problem is modified to reduce the gap using physical insights and heuristics. Another novel approach tested was to reformulate some parts of the model by use of some ideas from set partitioning problems. Results show that though both methods succeed in tightening the LP relaxation, the model solution times remain too long to be of interest in the Sequential Framework.	network synthesis filters	Rahul Anantharaman;I. Nastad;Bjørn Nygreen;T. Gundersen	2010	Computers & Chemical Engineering	10.1016/j.compchemeng.2009.12.002	discrete optimization;mathematical optimization;limiting factor;design process;computer science;engineering;linear programming relaxation;mathematics;engineering drawing	AI	12.698643227434312	2.746137284300076	21245
e12ddd49aec909df7a7ea38b22ddd2e818adee8f	using competence sets to analyze the consumer decision problem	decision models;decision support;empirical study;fuzzy measure;habitual domain;decision problem;consumer satisfaction;competence set;indexation;consumer decision making;consumer decision;problem solving	This study is intended to provide a different approach to complement the existing consumer decision models (CDMs). Based on the concept of habitual domains and competence sets, we supply a framework for helping a firm in expanding the benefits of its products to fully address the consumer’s needs. According to the features of consumers’ decision making, we use challenging problem types to explore extensive problem solving, fuzzy problem types for limited problem solving, and routine and mixed routine problem types for routine problem solving. In addition, several useful indexes are established using fuzzy measures in this study, including the possibility of successfully appealing to consumers, the degree of consumer satisfaction, the degree of compatibility, and the degree of uniqueness. These indexes can be a decision support for implementing competence set analysis in practical applications. Finally, an empirical study on children’s apparel was conducted to show the applicability and feasibility of our proposed method in practice.	decision problem	Ting-Yu Chen	2001	European Journal of Operational Research	10.1016/S0377-2217(99)00353-7	decision model;optimal decision;decision support system;decision analysis;computer science;marketing;decision problem;mathematics;management science;empirical research;welfare economics;business decision mapping	Theory	-3.904639292373224	-17.90521657732813	21267
722700ba89151802fecb44b96cb70b3004eb15f5	technical note - mathematical pitfalls in the one machine multiproduct economic lot scheduling problem	334 economic lot sizes;economic lot scheduling problem	We consider a group of products sharing the same piece of production equipment, and show that two mathematical problem formulations in the literature are ill-posed. In both cases, the infimum of the cost function occurs at a boundary point that is not feasible.	economic lot scheduling problem;schedule (project management)	Paul J. Schweitzer;Edward A. Silver	1983	Operations Research	10.1287/opre.31.2.401	mathematical optimization;economic lot scheduling problem;economics;operations management;mathematics;mathematical economics	Theory	11.30401705829558	2.7988783744701613	21329
738ed27027e6f20f9b34f4155d421ca037efd8cc	derivative of functions over lattices as a basis for the notion of interaction between attributes	discrete derivative;91a12;interaction index;data analysis;partially ordered set;shapley value;indexation;game;capacity;68r05;lattice	The paper proposes a general notion of interaction between attributes, which can be applied to many fields in decision making and data analysis. It generalizes the notion of interaction defined for criteria modelled by capacities, by considering functions defined on lattices. For a given problem, the lattice contains for each attribute the partially ordered set of remarkable points or levels. The interaction is based on the notion of derivative of a function defined on a lattice, and appears as a generalization of the Shapley value or other probabilistic values.		Michel Grabisch;Christophe Labreuche	2007	Annals of Mathematics and Artificial Intelligence	10.1007/s10472-007-9052-7	partially ordered set;games;mathematical optimization;combinatorics;discrete mathematics;lattice;mathematics;shapley value;data analysis	AI	0.6053843941433918	-20.505561647207074	21338
85caf47ab9dd7510f03250c322c552c168a78e8a	tackling the cognitive processes that underlie brands' assessments using artificial neural networks and whole brain fmri acquisitions	artificial neural networks integrated circuits accuracy independent component analysis brain modeling probabilistic logic training;probability backpropagation biomedical mri brain cognition data reduction feedforward neural nets independent component analysis;brain networks cognitive processes brand assessments artificial neural networks whole brain fmri acquisitions data reduction probabilistic independent component analysis backpropagation feedforward neural network hidden nodes;brain;probability;fmri;integrated circuit;training;independent component analysis;backpropagation;cognitive process;accuracy;artificial neural networks;brain modeling;brand;marketing;cognition;evaluation;feedforward neural nets;data reduction;evaluation fmri artificial neural networks marketing brand;probabilistic logic;integrated circuits;artificial neural network;biomedical mri	This exploratory study proposes the use of artificial neural networks to analyze whole brain fMRI data. Because fMRI data is dimensionally exorbitant, the first step is to reduce the amount of data to a tractable size, which is accomplished using probabilistic independent component analysis (PICA). Then data enters a simple back propagation feed forward neural network. This network outputs correct predictions above chance level in a different sample of subjects. More interestingly, it is found that hidden nodes segregate and concentrate different, but coherent, brain networks, which are the target of interpretations to support cognitive processes during the assessment of brands' logos.	artificial neural network;backpropagation;cobham's thesis;cognitive science;coherence (physics);independent component analysis;neural networks;software propagation	José Paulo Marques dos Santos;Luiz Moutinho	2011	2011 International Workshop on Pattern Recognition in NeuroImaging	10.1109/PRNI.2011.22	computer science;artificial intelligence;machine learning;data mining	ML	4.323790636711526	-21.53602968350689	21360
03d4da7ae4362dfc093c6c9fa510328bc8430f56	the aim/end-use model and its application to forecast japanese carbon dioxide emissions	energy;integrable model;greenhouse gas emissions;technology;simulation;carbon dioxide emission;environment;co2 emission;economics;living standards;carbon tax	AIM (Asian-Pacific Integrated Model) has been developed for predicting greenhouse gas emissions and evaluating policy measures to reduce them. Two socio-economic scenarios were assumed and CO2 emissions were predicted based on these scenarios and policy intervention assumptions. It is found that mitigating CO2 emissions by 6% to the 1990 level without scaling back productive activities or standards of living in Japan is possible. However, if one relies on the market mechanism alone, it cannot be done. The analysis has shown that it is indispensable to introduce new policies and measures such as carbon tax and subsidies.	aim alliance;image scaling	Mikiko Kainuma;Yuzuru Matsuoka;Tsuneyuki Morita	2000	European Journal of Operational Research	10.1016/S0377-2217(99)00243-X	carbon offset;low-carbon economy;energy;economics;carbon finance;carbon credit;economy;economic growth;technology	ML	5.816224429458167	-8.330258190464605	21368
1438b34cafb0296ef89b1d02500511cb3389d91d	a monte-carlo approach for the endgame of ms. pac-man	computational intelligence;video game;artificial intelligent;software agents;software agents computer games monte carlo methods;games monte carlo methods testing computational intelligence conferences artificial intelligence computers;intelligent agent monte carlo approach ms pac man video game artificial intelligence research computational intelligence research path testing endgame module;monte carlo;computer games;monte carlo methods	Ms. Pac-Man is a challenging video game which provides an interesting platform for artificial intelligence and computational intelligence research. This paper introduces the novel concept of path testing and reports an effective Monte-Carlo approach to develop an endgame module of an intelligent agent that plays the game. Our experimental results show that the proposed method often helps Ms. Pac-Man to eat pills effectively in the endgame. It enables the agent to advance to higher stages and earn more scores. Our agent with the endgame module has achieved a 20% increase in average score over the same agent without the module.	artificial intelligence;branching factor;computational intelligence;effective method;evaluation function;greedy algorithm;heuristic;intelligent agent;minimax;model-based testing;monte carlo method;monte carlo tree search;pattern matching;real-time computing;real-time transcription;simulation	Bruce Kwong-Bun Tong;Chun Man Ma;Chi Wan Sung	2011	2011 IEEE Conference on Computational Intelligence and Games (CIG'11)	10.1109/CIG.2011.6031983	simulation;computer science;artificial intelligence;computational intelligence;artificial intelligence, situated approach;monte carlo method	AI	18.524048122193765	-17.887851734750495	21398
6d081080f1d7067eadf34bffc42b543dd1a766df	market participant estimation by using artificial market		In designin gar ea listi ca rt ificial market ,o n e o ft he m ost impor- tan tp oints to consider i st he c ombination of agents used .I nthi ss tud y, we propose an estimation metho db asedon inverse simulation to estimate the combinations of traders who participate in the market .T he proposed method applies a simulation that estimates market paricipation in differen tm arkets . The simulation results indicate that the proposed method is capable o fe st i- mating market participants.		Fujio Toriumi;Kiyoshi Izumi;Hiroki Matsui	2009		10.1007/978-3-642-16098-1_13	simulation;engineering;operations management;advertising	HCI	-4.398614797487019	-9.09671504043644	21414
58bf4270ea8eabde92af47a7bb3cbcdf89ea3bc9	three label tags for special applications: attaching on small targets, long distance recognition, and stable performance with arbitrary objects	circular polarization;tag;stable reading performance;rfid;small antenna			Jaeyul Choo;Chihyun Cho;Hosung Choo	2014	IEICE Transactions	10.1587/transcom.E97.B.1022	radio-frequency identification;circular polarization;telecommunications;world wide web	Mobile	17.239752828720494	-12.535930071356614	21423
ddd657b7cc9a9cfd10da3b964f8034f0025e819e	development of intelligent models for ravelling using neural network	neural nets;asphalt;time series;road building neural nets time series sensitivity analysis asphalt roads;intelligent networks neural networks artificial neural networks traffic control asphalt predictive models artificial intelligence telecommunication traffic databases sensitivity analysis;roads;sensitivity analysis;heavy traffic;road building;color contours artificial neural network intelligent models porous asphalt time series raveling sensitivity analysis;artificial neural network;material properties;neural network	The most unacceptable damage observed on porous asphalt is raveling. Therefore it is important to predict this detriment accurately and understand it deeply. Artificial neural network (ANN) was employed to predict raveling using time-series raveling and climate, construction and traffic factors. The necessary data was obtained from SHRP-NL database. Model I is able to forecast raveling low, moderate and high with correlation factor of R/sup 2/=0.986, 0.926 and 0.976. Model II provided sensitivity analysis indicating the relative contribution of factors related to climate, traffic factor, thickness, roughness and age. Color contours illustrated lots of facts such as heavy traffic and low thickness cause raveling on old asphalt at cold rainy days. Model III and its optimized version were developed to analyze relation between material properties and raveling. ANN proved to be a powerful technique to predict and analyze raveling opening great opportunities for development of ANN models for other detriments.	artificial neural network;color;nl (complexity);thickness (graph theory);time series	Maryam Miradi	2004	2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No.04CH37583)	10.1109/ICSMC.2004.1400901	material properties;simulation;computer science;artificial intelligence;machine learning;time series;asphalt;sensitivity analysis;artificial neural network	Robotics	9.795048777502569	-20.008629319000683	21478
63f78169ab207fcb541fc0f5a6c602ed29a27af2	large-scale power system planning using enhanced benders decomposition	stochastic processes environmental factors government policies power system planning;large scale power system planning benders decomposition investments under uncertainty;convergence;investment;computational modeling;stochastic processes;mathematical model;climate policy power system planning benders decomposition stochastic lp european power system european power sector;europe;stochastic processes computational modeling investment convergence partitioning algorithms europe mathematical model;partitioning algorithms	An enhanced Benders decomposition algorithm for two-stage stochastic LPs is presented and applied to a large-scale dynamic generation and transmission expansion planning model for the European power system. The improved algorithm is a variation of the traditional multi-cut Benders decomposition algorithm where the scenario aggregation used for the optimality cuts is reduced at a given error threshold. Experimental results show that this technique improves convergence and reduces computation time. An analysis using the planning model to compute an optimal development of the European power sector under a global climate policy is also discussed.	algorithm;benders decomposition;computation;experiment;rate of convergence;simulation;time complexity;two-phase commit protocol	Christian Skar;Gerard Doorman;Asgeir Tomasgard	2014	2014 Power Systems Computation Conference	10.1109/PSCC.2014.7038297	mathematical optimization;economics;operations management;mathematical economics	Robotics	5.163086635945368	3.540712553852333	21487
2f86e0bbe7cbbb810dfef6e6043c2a26d0691069	a note on 'a replenishment policy for items with price-dependent demand, time-proportional deterioration and no shortages'		In a recent paper, Begum et al. (2012, International Journal of Systems Science, 43, 903–910) established pricing and replenishment policy for an inventory system with price-sensitive demand rate, time-proportional deterioration rate which follows three parameters, Weibull distribution and no shortages. In their model formulation, it is observed that the retailer's stock level reaches zero before the deterioration occurs. Consequently, the model resulted in traditional inventory model with price sensitive demand rate and no shortages. Hence, the main purpose of this note is to modify and present complete model formulation for Begum et al. (2012). The proposed model is validated by a numerical example and the sensitivity analysis of parameters is carried out.		Nita H. Shah;Hardik N. Soni;Jyoti Gupta	2014	Int. J. Systems Science	10.1080/00207721.2012.749434	actuarial science	Logic	2.6382759297966163	-4.7116371830591675	21497
0f8e0c2e36bfe2302d8cfbc956a03df7030b44f5	data trading with multiple owners, collectors, and users: an iterative auction mechanism	software;companies;mechanical factors;big data;privacy;data models	In the big data era, it is vital to allocate the vast amount of data to heterogeneous users with different interests. To clinch this goal, various agents including data owners, collectors, and users should cooperate to trade data efficiently. However, the data agents (data owners, collectors, and users) are selfish and seek to maximize their own utilities instead of the overall system efficiency. As such, a sophisticated mechanism is imperative to guide the agents to distribute data efficiently. In this paper, the data trading problem of a data market with multiple data owners, collectors, and users is formulated and an iterative auction mechanism is proposed to coordinate the trading. The proposed mechanism guides the selfish data agents to trade data efficiently in terms of social welfare and avoids direct access of the agents’ private information. We theoretically prove that the proposed mechanism can achieve the socially optimal operation point. Moreover, we demonstrate that the mechanism satisfies appealing economic properties such as individual rationality and weakly balanced budget. Then, we expand the mechanism to nonexclusive data trading, in which the same data can be dispensed to multiple collectors and users. Simulations as well as real data experiments validate the theoretical properties of the mechanism.	big data;computer simulation;data mart;experiment;imperative programming;iteration;personally identifiable information;random access;rationality;vergence	Xuanyu Cao;Yan Chen;K. J. Ray Liu	2017	IEEE Transactions on Signal and Information Processing over Networks	10.1109/TSIPN.2017.2668144	data mining;business;computer security;commerce	AI	-0.09328331859363398	3.8346226297240307	21511
4992b7e10562a81ba14f09ba4868fcfcf8ff4f2b	use of two-layer cause-effect model to select source of signal in plant alarm system	theoretical computer science;computer science all;two layer cause effect model;plant alarm system	Alarm systems provide an interface between operators and the machinery in a chemical plant because they allow abnormalities or faults caused by operators to be detected at an early stage. Alarms should be used to enable operators to diagnose faults and plan countermeasures, and nuisance alarms should be eliminated. We propose the use of a selection algorithm of sets of pairs of alarm variables and their signs. The signs mean the upper or lower limits of the alarm variables. The selected sets of pairs are theoretically guaranteed to be able to qualitatively distinguish all assumed faults. We propose using a two-layer cause-effect model for the algorithm, which represents the cause and effect relationship between state variables based on the topology of a plant. This model is applied to a simple process. The simulation results illustrate the usefulness of our method.		Kazuhiro Takeda;Takashi Hamaguchi;Masaru Noda;Naoki Kimura;Toshiaki Itoh	2010		10.1007/978-3-642-15390-7_39	simulation;engineering;artificial intelligence;computer security	Robotics	12.86175360396702	-13.899744733647493	21542
4169bc196953d78042126e6fec916b33d5c23123	the max k-armed bandit: a new model of exploration applied to search heuristic selection	boltzmann distribution;stochastic search	The multiarmed bandit is often used as an analogy for the tradeoff between exploration and exploitation in search problems. The classic problem involves allocating trials to the arms of a multiarmed slot machine to maximize the expected sum of rewards. We pose a new variation of the multiarmed bandit—the Max K-Armed Bandit—in which trials must be allocated among the arms to maximize the expected best single sample reward of the series of trials. Motivation for the Max K-Armed Bandit is the allocation of restarts among a set of multistart stochastic search algorithms. We present an analysis of this Max K-Armed Bandit showing under certain assumptions that the optimal strategy allocates trials to the observed best arm at a rate increasing double exponentially relative to the other arms. This motivates an exploration strategy that follows a Boltzmann distribution with an exponentially decaying temperature parameter. We compare this exploration policy to policies that allocate trials to the observed best arm at rates faster (and slower) than double exponentially. The results confirm, for two scheduling domains, that the double exponential increase in the rate of allocations to the observed best heuristic outperforms the other approaches.	approximation;coat of arms;heuristic (computer science);iteration;multi-armed bandit;nyquist–shannon sampling theorem;problem solving;rss bandit;robotics;sampling (signal processing);scheduling (computing);search algorithm;stochastic optimization;time complexity	Vincent A. Cicirello;Stephen F. Smith	2005			mathematical optimization;boltzmann distribution;computer science;artificial intelligence;machine learning	AI	23.16927959497401	-17.067769371299068	21547
b378b996c3f77a9d36aaad1429e4f8c921cb668f	selling or subscribing software under quality uncertainty and network externality effect	software;software subscription contract;software quality contracts dp industry;uncertainty;perpetual license;dp industry;biological system modeling;network effect;contracts;welfare effect;dynamic environment;hybrid approach;network externality effect;software vendor;network externality;licenses;consumer welfare effect;path dependence;schedules;subscriptions;quality uncertainty;profitability;quality uncertainty software vendor software license perpetual license software subscription contract consumer welfare effect network externality effect;software license;software quality;software quality uncertainty licenses subscriptions costs production computer industry contracts environmental management quality management	We examine the optimal way for a software vendor to license software: perpetual license at a posted price, subscription contract that subscribers receive automatic updates for periodic payment, or a hybrid approach that involves both. By addressing such specific issues in the software market as network effects, quality uncertainty, upgrade compatibility, and the vendor's ability to commit to future prices in a dynamic environment, we demonstrate how a software vendor can manage the trade-offs of perpetual licensing and subscription to optimize profit, as well as the corresponding welfare effect on consumers. Though the subscription model helps the vendor lock in consumers so as to increase profit when there is a great uncertainty associated with the next version software, it destroys the path dependence in creating network externalities.Therefore, when the network effect is sufficiently large, it is more profitable for a software vendor to provide both perpetual licensing and subscription.	multi-licensing;path dependence;vendor lock-in;windows update	Jie Jennifer Zhang;Abraham Seidmann	2010	2010 43rd Hawaii International Conference on System Sciences	10.1109/HICSS.2010.333	uncertainty;economics;schedule;marketing;network effect;software engineering;database;vendor finance;management;software quality;statistics;commerce;profitability index	SE	-0.278192477255689	-6.925359181177996	21559
0d64caab05811eb06a499b3df2f403e8be3ee1e3	initial evidence for self-organized criticality in electric power system blackouts	power distribution reliability;self-organised criticality;time series;hurst exponent;r/s statistics;electric power system blackouts;long time correlations;scaled window variance analysis;self-organized criticality;time series	We examine correlations in a time series of electric power system bIackout sizes using scaled window variance analysis and R/S statistics. The data shows some evidence of long time correlations and has Hurst exponent near 0.7. Large blackouts tend to correlate with further large blackouts afler a long time interval. Similar effects are also observed in many other complex systems exhibiting self-organized criticality. We discuss this initial evidence and possible explanations for selforganized criticali~ in power systems blackouts. Selforganized criticality, I~fuh’y confirmed in power systems, would suggest new approaches to understanding and possibly controlling blackouts.	complex systems;hurst exponent;ibm power systems;self-organization;self-organized criticality;time series	Benjamin A. Carreras;David E. Newman;Ian Dobson;A. B. Poole	2000		10.1109/HICSS.2000.926768	complex systems;electric power system;statistics	Metrics	4.698918769997146	-12.537728711817874	21681
1627cc0fb1738b45ea7a4720005510c98849c7ff	evolving ant colony optimization based unit commitment	traveling salesman problem;ant colony optimization;combinatorial optimization problem;genetic algorithm;evolving ant colony optimization;pheromone matrix;unit commitment;unit commitment problem;time constraint	Ant colony optimization (ACO) was inspired by the observation of natural behavior of real ants' pheromone trail formation and foraging. Ant colony optimization is more suitable for combinatorial optimization problems. ACO is successfully applied to the traveling salesman problem. Multistage decision making of ACO gives an edge over other conventional methods. This paper proposes evolving ant colony optimization (EACO) method for solving unit commitment (UC) problem. The EACO employs genetic algorithm (GA) for finding optimal set of ACO parameters, while ACO solves the UC problem. Problem formulation takes into consideration the minimum up and down time constraints, startup cost, spinning reserve, and generation limit constraints. The feasibility of the proposed approach is demonstrated on two different systems. The test results are encouraging and compared with those obtained by other methods.	ant colony optimization algorithms;mathematical optimization	K. Vaisakh;L. R. Srinivas	2011	Appl. Soft Comput.	10.1016/j.asoc.2010.11.019	extremal optimization;mathematical optimization;ant colony optimization algorithms;meta-optimization;genetic algorithm;computer science;artificial intelligence;power system simulation;travelling salesman problem;3-opt;metaheuristic	EDA	23.10874956485952	-1.6263622803260098	21723
5dc8fadf9fa8e1bad02c0431eb61495bd6c170ed	learning options from demonstrations: a pac-man case study	learning from demonstration reinforcement learning temporal difference learning options framework;games robots data mining learning artificial intelligence algorithm design and analysis training trajectory	Reinforcement learning (RL) is a machine learning paradigm behind many successes in games, robotics, and control applications. RL agents improve through trial-and-error, therefore undergoing a learning phase during which they perform suboptimally. Research effort has been put into optimizing behavior during this period, to reduce its duration and to maximize after-learning performance. We introduce a novel algorithm that extracts useful information from expert demonstrations (traces of interactions with the target environment) and uses it to improve performance. The algorithm detects unexpected decisions made by the expert and infers what goal the expert was pursuing. Goals are then used to bias decisions while learning. Our experiments in the video game Pac-Man provide statistically significant evidence that our method can improve final performance compared to a state-of-the-art approach.	algorithm;atrial premature complexes;experiment;interaction;machine learning;programming paradigm;reinforcement learning;robotics;stiff-person syndrome;tracing (software);video games	Marco Tamassia;Fabio Zambetta;William L. Raffe;Florian Müller;Xiaodong Li	2018	IEEE Transactions on Games	10.1109/TCIAIG.2017.2658659	reinforcement learning;machine learning;artificial intelligence;robotics;computer science	ML	20.3596114423689	-20.640443823217076	21730
483ad4925b59c4133867e6db4ff0cc0a461b87e3	optimal solution for the two-dimensional facility layout problem using a branch-and-bound algorithm	optimal solution;branch and bound algorithm;layout problem;branch and bound approach;mathematical programming;facility layout;genetic algorithm;tabu search;facility planning;branch and bound;manufacturing system;nonlinear model	Facilities layout problem is one of the important issues affecting the productivity of manufacturing systems. This problem deals with the determination of optimum arrangement of manufacturing facilities with respect to different layout patterns. A two-dimensional layout is an arrangement fashion in which the manufacturing facilities are laid in a planar area. In this paper, a mixed-integer nonlinear mathematical programming model is proposed for determining the optimum layout of machines in a two-dimensional area. The parameters considered by the proposed model are (a) production capacity of machines, (b) multiple machines of each type (machine redundancy), (c) processing route of parts, (d) dimensions of machines. A technique is used to linearize the formulated nonlinear model. An algorithm based on branch-and-bound approach is proposed to obtain the optimal solution of the proposed mathematical programming model. A simple illustrative example is discussed to demonstrate the technique, and then small-, medium-, and large-sized problems are solved. Comparison of the layout obtained from the proposed model indicates that the proposed model considerably reduces the total distance traveled by products as compared to an optimum process layout configuration for small- and medium-sized problems. The paper concludes that the proposed branch-and-bound approach performs inefficient for large-sized problems. For large-sized problems, the proposed mathematical programming model should be solved through meta-heuristics like genetic algorithms, tabu search, etc.	algorithm;branch and bound	Maghsud Solimanpur;Amir Jafari	2008	Computers & Industrial Engineering	10.1016/j.cie.2008.01.018	mathematical optimization;discontinuity layout optimization;computer science;engineering;operations management;engineering drawing;branch and bound	DB	16.375388745972202	2.672231371126764	21737
d4d185f314e87d243a69d0a53b564d64a8a8a7be	a multiobjective approach for multistage reliability growth planning by considering the timing of new technologies introduction	reliability evolutionary computation pareto optimisation particle swarm optimisation planning product development;reliability engineering;testing;upper bound;reliability engineering testing product development planning data models upper bound;next generation engine development multiobjective multiple stage reliability growth planning mo ms rgp model decision variables product new technology product new contents product development time limit product development time budget multiobjective evolutionary algorithm multipleobjective particle swarm optimization pareto optimal solutions clustering methods multiple criteria decision making method reliability growth planning optimization;planning;technique for order of preference by similarity to ideal solution topsis data clustering multiobjective particle swarm optimization mopso multistage new product development reliability growth rg;data models;product development	This paper proposes a new multiobjective multiple stage reliability growth planning (MO-MS-RGP) model. The model is based on multiobjective consideration of developing a new product, including the cost, time, and product reliability. The number of test units, test time, and the percentage of introduced new technologies are considered as decision variables in the model. Varying reliability growth rates are considered for each subsystem in each stage. Product new technologies or contents can be completely introduced in one stage or partially introduced to the product over multiple stages. New product development time limit and budget are considered as constraints in the MO-MS-RGP model. An integrated approach is developed to formulate and solve the proposed MO-MS-RGP problem. The approach starts with a multiobjective evolutionary algorithm, called multipleobjective particle swarm optimization to find a set of Pareto optimal solutions. Then, clustering methods are applied to cluster the solutions obtained by the evolutionary algorithm. Finally, the clustered solutions are ranked using a multiple criteria decision making method. A numerical example illustrates the application of the proposed MO-MS-RGP model for the reliability growth planning optimization of a next generation engine development.	cluster analysis;decision theory;evolutionary algorithm;mathematical optimization;microsoft windows;multistage amplifier;new product development;numerical analysis;pareto efficiency;particle swarm optimization	Mohammadsadegh Mobin;Zhaojun Li;G. M. Komaki	2017	IEEE Transactions on Reliability	10.1109/TR.2016.2638124	planning;reliability engineering;data modeling;mathematical optimization;engineering;artificial intelligence;mathematics;software testing;upper and lower bounds;new product development	AI	13.70930377176929	-4.280497834492703	21749
dfd718e4d6480dd6bffcffa38a4879adb490c6ab	an improved genetic algorithm for a parallel machine scheduling problem with energy consideration		In recent years, there has been growing interest in reducing energy consumption in manufacturing industry. This paper focuses on the parallel machine scheduling problem extracting from the high-energy heating process in iron and steel enterprises. We first present a mixed integer mathematic model with the objective of minimizing the total energy consumption. Next, we propose an improved genetic algorithm (IGA) to find high-quality solutions to this mathematic model. Since the scheduling problem is NP-hard, the proposed IGA improves standard genetic algorithm (SGA) in following aspects: crossover operation and mutation operation based on problem characteristics and adaptive adjustment. To evaluate the proposed algorithm, we select two comparison algorithms: SGA and adaptive genetic algorithm (AGA), and conduct a serial of experiments with the case scenarios generated according to real-world production process. The results show that the proposed IGA has superior performance to the other two algorithms.	experiment;genetic algorithm;in-game advertising;np-hardness;parallel computing;scheduling (computing);synthetic genetic array	Hong Lu;Fei Qiao	2017	2017 13th IEEE Conference on Automation Science and Engineering (CASE)	10.1109/COASE.2017.8256314	genetic algorithm;scheduling (production processes);mathematical optimization;energy consumption;crossover;job shop scheduling	Robotics	19.276483970732627	-0.7070915958689239	21867
24b1b4a2da2a44bdcb81d80b4234d99b6b6b7f6e	investigation of the impacts of shared autonomous vehicle operation in halifax, canada using a dynamic traffic microsimulation model		Abstract This study presents a novel sequential modeling framework of shared autonomous vehicle (SAV) operation in the Halifax transport network. The Halifax regional transport network model is used to generate business-as-usual traffic demand in the morning peak hours. A new module of SAV assignment upon trip request is introduced and integrated with the traffic microsimulation model to simulate the SAVs’ occupied and empty trips in the network. The proposed framework demonstrates the capability to evaluate service performance of SAVs with different level of fleet sizes in the network. Model results suggest that fleet size of 900 SAVs serves 20% of the total morning commute trip requests. Traffic condition is improved for the first hour of peak periods as average speed increases and total travel time requirement decreases during operation of SAV fleet in Halifax. The results provide insights into SAV system planning in accordance to anticipated challenges of SAV adoption in transportation network.	autonomous robot	M. D. Jahedul Alam;Muhammad Ahsanul Habib	2018		10.1016/j.procs.2018.04.066	data mining;transport network;simulation;trips architecture;flow network;microsimulation;business system planning;computer science	Robotics	12.303592040844258	0.1209795023219481	21903
6824df30b244c9549499b1d88392db6a85fd404f	assessment of nuclear waste repository options using the er approach	nuclear waste repository;multi criteria decision analysis;evidential reasoning;public opinion;sensitivity analysis;decision making process;participative approach;evidential reasoning approach;group decision making;public perception;multiple criteria decision analysis;nuclear waste	Two technically feasible nuclear waste repository options have been identified in Belgium. To select one for implementation, a study was carried out to compare the public perception and acceptance of the two options. In this paper, it is described how the study and selection process can be supported, and how the diversity and uncertainty in public opinions can be rationally modeled and analyzed by applying the Evidential Reasoning (ER) approach. The ER approach is a recent advancement for multi-criteria decision analysis (MCDA). By using belief decision matrices to model MCDA problems, both qualitative and quantitative information with various types of uncertainties can be taken into account in decision-making processes in a unified and logical format. Following an illustration of the ER approach and an outline of the selection problem, it is described how the ER approach can be used to model the problem, aggregate information, and facilitate sensitivity analysis. Some suggestions are made for future assessment studies.	aggregate data;decision analysis;erdős–rényi model;selection algorithm	Dong-Ling Xu	2009	International Journal of Information Technology and Decision Making	10.1142/S021962200900351X	computer science;knowledge management;data mining;mathematics;management science;mathematical economics;evidential reasoning approach;multiple-criteria decision analysis	SE	-3.786742400518199	-16.97864111978429	21910
3bb8cb1b1034ff8fbe8dfb599e9256198df12f5b	long term forecasting of groundwater levels with evidence of non-stationary and nonlinear characteristics	long term forecasting;groundwater level forecasting;wavelets	Groundwater systems are in general characterised by non-stationary and nonlinear features. Modelling of these systems and forecasting their future states requires identification and capture of these underlying features that seem to drive these processes. Recently, wavelets have been used extensively in the area of hydrologic and environmental time series forecasting owing to its ability to unravel these aforementioned component features. In this paper, dynamic wavelet based nonlinear model (Wavelet Volterra coupled model) is tested for its ability to yield reliable long term forecasts of groundwater levels at two sites in Canada. The model results are compared with the results from other recent techniques like wavelet neural network (WA-ANN), Wavelet linear regression (WLR), Artificial neural network and dynamic auto regressive (DAR) Models. The results of the study show the potential of wavelet Volterra coupled models in forecasting groundwater levels in addition to being more versatile and simpler to use when compared with other competing models. & 2012 Elsevier Ltd. All rights reserved.	artificial neural network;nonlinear system;stationary process;time series;wavelet;wholesale line rental	Rathinasamy Maheswaran;Rakesh Khosa	2013	Computers & Geosciences	10.1016/j.cageo.2012.09.030	wavelet;econometrics;hydrology;machine learning;mathematics;statistics	AI	6.506786245828592	-17.75656937423211	21917
f18ec92e2c04de491b9c6bfbd065a373799e5850	explainable deterministic mdps		We present a method for a certain class of Markov Decision Processes (MDPs) that can relate the optimal policy back to one or more reward sources in the environment. For a given initial state, without fully computing the value function, q-value function, or the optimal policy the algorithm can determine which rewards will and will not be collected, whether a given reward will be collected only once or continuously, and which local maximum within the value function the initial state will ultimately lead to. We demonstrate that the method can be used to map the state space to identify regions that are dominated by one reward source and can fully analyze the state space to explain all actions. We provide a mathematical framework to show how all of this is possible without first computing the optimal policy or value function.	algorithm;bellman equation;deterministic finite automaton;markov chain;markov decision process;maxima and minima;state space	Joshua R. Bertram;Peng Wei	2018	CoRR		mathematical optimization;mathematics;state space;machine learning;markov decision process;bellman equation;artificial intelligence	ML	21.665806432769728	-16.99723072614276	21940
16d4699eea8fb2bb178594ed41d3db65bb08c2a0	bounded rationality in service systems	queueing;behavioral operations;bounded rationality;consumer behavior;service operations	The traditional economics and queueing literature typically assume that customers are fully rational. In contrast, in this paper, we study canonical service models with boundedly rational customers. We capture bounded rationality using a framework in which better decisions are made more often, while the best decision needs not always be made. We investigate the impact of bounded rationality on social welfare and revenue of a profit maximizing firm when the queue is visible and not visible to the customers. For invisible queues, from the firm’s perspective, higher irrationality always leads to higher optimal prices and higher revenue when customers are sufficiently irrational. From the social planner’s perspective, there may be strictly positive social welfare losses when customers are sufficiently irrational. For visible queues with a fixed price, we prove that a little bit of irrationality can lead to strict social welfare improvement, and we provide a simple inequality under which this improvement happens. With the optimal prices, however, irrationality always decreases social welfare, and a little bit of irrationality always results in revenue losses.	queue (abstract data type);rationality;social inequality	Tingliang Huang;Gad Allon;Achal Bassamboo	2013	Manufacturing & Service Operations Management	10.1287/msom.1120.0417	ecological rationality;economics;public economics;marketing;social heuristics;microeconomics;queueing theory;welfare economics;consumer behaviour;bounded rationality;service system	ECom	-1.9322696317066157	-6.087613925861393	21956
d54c678b97ae1787204159aa61b1e2ac6f525824	a new model for prediction of the performance of a cappuccino pod	shrinking core model;dynamic mass balance;creamer;cappuccino pod;dissolution process	In recent years a lot of attention and efforts have been dedicated on optimizing the design and quality of coffee pods. These would be applicable on professional as well as on household machines such as Senseo or Nestle. By providing a new method to describe the mechanisms of the brewing process within the pod, the research presented in this paper gives deeper insights into the design related aspects to improve the pods. A model incorporating the function of dynamic mass balance and the shrinking core model is employed to describe the dissolution process of the creamer in the pod. This dissolution step is the most important in determining the milky taste wanted in the case of cappuccino. The model was tested by comparing its results with those of a series of experiments. With the model presented in this paper designing better pods should be easier and more efficient.		Linhua Jiang;Hua Pan;Haibin Cai	2013	Int. J. Comput. Intell. Syst.	10.1080/18756891.2013.816028	simulation;operations research	DB	5.668799214951328	2.5547097172909132	21961
014aecca794b52418553d8a7e90061adee861172	selection of the best with stochastic constraints	iterative methods;operations research;stochastic processes;heuristic iterative algorithm;multi-criteria problem;multiple selection criterion;stochastic constraints	When selecting the best design of a system among a finite set of possible designs, there may be multiple selection criterion. One formulation of such a multi-criteria problem is minimization (or maximization) of one of the criterions while constraining the others. In this paper, we assume the criteria are unobservable mean values of stochastic outputs of simulation. We propose a new heuristic iterative algorithm for finding the best in this situation and use a number of experiments to demonstrate the performance of the algorithm.	expectation–maximization algorithm;experiment;heuristic;iterative method;selection (user interface);simulation	Alireza Kabirian;Sigurdur Ólafsson	2009	Proceedings of the 2009 Winter Simulation Conference (WSC)		stochastic process;mathematical optimization;combinatorics;computer science;machine learning;mathematics;iterative method;statistics	EDA	15.882583233598444	-7.188603396820415	21997
33eaf638d32dbf8fcd0634dcafc573ec7a61c7c3	simulation optimization in manufacturing analysis: simulation based optimization for supply chain configuration design	simulation optimization;mixed integer programming model;facility location;supply chain configuration design;genetic algorithm;hybrid optimization approach;production capacity;supply chain configuration;new approach;mixed integer programming;production policy;policy variable	The design of a supply chain network as an integrated system with several tiers of suppliers is a difficult task. It consists of making strategic decisions on the facility location, stocking location, production policy, production capacity, distribution and transportation modes. This research develops a hybrid optimization approach to address the Supply Chain Configuration Design problem. The new approach combines simulation, mixed integer programming and genetic algorithm. The genetic algorithm provides a mechanism to optimize qualitative and policy variables. The mixed integer programming model reduces computing efforts by manipulating quantitative variables. Finally simulation is used to evaluate performance of each supply chain configuration with non-linear, complex relationships and under more realistic assumptions. The approach is designed to be robust and could handle the large scale of the real world problems.	genetic algorithm;integer programming;linear programming;mathematical optimization;nonlinear system;programming model;simulation;supply chain network	Tu Hoang Truong;Farhad Azadivar	2003		10.1145/1030818.1030987	simulation;systems engineering;engineering	AI	12.366331494761893	-3.348718147421578	22038
ffb4d06815a340db01b668942e04b181a6c24b57	financial performance analysis of e-collaboration supply chain under transportation disruptions	financial response characteristics;forecasting;system dynamics modeling;transportation disaster;transportation disruption;system dynamics;financial performance;transportation internet supply chain management supply chains;collaborative forecasting supply chain financial performance analysis e collaboration supply chain transportation disruption system dynamics modeling internet e collaboration tools collaborative planning supply chains financial response characteristics transportation disaster;performance analysis supply chains supply chain management risk management collaboration financial management road transportation disaster management information management computer science;collaboration;collaborative forecasting supply chain;financial performance analysis;materials;collaborative tools;supply chains;collaborative planning supply chains;collaborative planning;e collaboration supply chain;internet;financial flow supply chain management disruption management system dynamics;financial flow;transport process;pipelines;transportation;disruption management;supply chain;supply chain management;marketing and sales;internet e collaboration tools	Financial performance analysis of e-collaboration supply chain under transportation disruption is carried out by system dynamics modeling and simulation. Financial performances of three different supply chains with Internet e-collaboration tools are compared with the assumption that a transportation disruption occurs in an inner transportation process of the retailer. Numerical results are shown to reveal the Non-collaborative, Collaborative Forecasting and Collaborative Planning supply chains have different financial response characteristics under certain transportation disaster, and Collaborative Forecasting supply chain has the best behavior in case of these events in contrast to the others.	profiling (computer programming)	Tian-jian Yang	2008		10.1109/ISCSCT.2008.372	supply chain risk management;service management;marketing;operations management;supply chain;business;commerce	Robotics	1.6238351510667592	-10.023084867508649	22048
a05bac4871b675aed22a31ca412649fcfd7ef51a	differentiation copayment design to improve the multi-tiered healthcare system in china		The Chinese healthcare system is a tiered system comprising hospitals at different levels. With limited resource, the system is expected to function well if care of various types can be delivered at appropriate tiers. However, without clear regulation, many patients visit upper-level comprehensive hospitals even for simple services while more ill patients may encounter denial of care there. On the other hand, lower-level community hospitals are often underutilized for the same reason. By the investigation in this paper, we found that the medical reimbursement policy affects patient's medical-care-seeking behavior to some extent. To guide patients to select the hospital reasonably, alleviate the congestion in the upper-level hospitals and improve the utilization of the lower-level hospitals, the differentiation copayment design between upper-and lower-level hospitals is proposed. From the perspective of China's macro-health environment, we conducted a systematic analysis of the system. We first analyzed the impact of the copayment on the patients' behavior of first-visit and reverse referral respectively. We then developed a queuing network model to capture various types of patient flows, and applied the BCMP theory to derive analytical results on queuing performance measures of hospitals at both levels. Finally, we combined the analyses and built an optimization model to design the final optimal copayment. Through this work, we expect to provide insights into the medical insurance policy to guide the government to manage the healthcare system effectively.	care-of address;expect;mathematical optimization;network congestion;network model	Yewen Deng;Na Li;Xuejun Xu;Zhibin Jiang	2017	2017 13th IEEE Conference on Automation Science and Engineering (CASE)	10.1109/COASE.2017.8256104		HPC	8.459758504070113	-4.008674785185671	22054
cbbc932b74ac94c80b819d468cf7fe4f92b297b6	an ant colony optimization model for parallel machine scheduling with human resource constraints		This model proposed an ant colony optimization model to tackle human resource constrained parallel machine scheduling problem  with precedence constraints. In this model, four subsystems are designed to solve the problem, including input subsystem which  conducts the problem and the ACO model related parameters and output subsystem which exports scheduling and analysis results,  sequence searching subsystem which constructs feasible sequence for each ant, human resource scheduling and sequence evaluation  subsystem which assigns human resource and determines the duration time of jobs under the allowable amount of operators. The  model is demonstrated to be valid using an example case.  	ant colony optimization algorithms;parallel computing;scheduling (computing)	Qiong Zhu;Yichao Gu;Gong Zhang;Jie Zhang;Xuefang Chen	2009		10.1007/978-3-642-10430-5_71	ant colony optimization algorithms;parallel metaheuristic;metaheuristic	AI	19.47058423128232	0.2676306721845456	22060
61fec90857d1c538922c4fe5e4bf718d8f7f8ace	what-if analysis in olap - with a case study in supermarket sales data		Today’s OnLine Analytical Processing (OLAP) or multi-dimensional databases have limited support for whatif or sensitivity analysis. What-if analysis is the analysis of how the variation in the output of a mathematical model can be assigned to different sources of variation in the model’s input. This functionality would give the OLAP analyst the possibility to play with “What if ...?”-questions in an OLAP cube. For example, with questions of the form: “What happens to an aggregated value in the dimension hierarchy if I change the value of this data cell by so much?” These types of questions are, for example, important for managers that want to analyse the effect of changes in sales on a product’s profitability in an OLAP supermarket sales cube. In this paper, we extend the functionality of the OLAP database with what-if analysis.	database;mathematical model;olap cube;online analytical processing	Emiel Caron;Hennie Daniels	2010			data mining	DB	-0.636878729112936	-11.521147940542896	22061
8ccd49079ac1f32dc4f8f46f0ef1c90efc4f6732	geology prediction based on operation data of tbm: comparison between deep neural network and statistical learning methods		Tunnel boring machine (TBM) is a complex engineering system widely used for tunnel construction. In view of the complicated construction environments, it is necessary to predict geology conditions prior to excavation. In recent years, massive operation data of TBM has been recorded, and mining these data can provide important references and useful information for designers and operators of TBM. In this work, a geology prediction approach is proposed based on deep neural network and operation data. It can provide relatively accurate geology prediction results ahead of the tunnel face compared with the other prediction models based on statistical learning methods. The application case study on a tunnel in China shows that the proposed approach can accurately estimate the geological conditions prior to excavation, especially for the short range ahead of training data. This work can be regarded as a good complement to the geophysical prospecting approach during the construction of tunnels, and also highlights the applicability and potential of deep neural networks for other data mining tasks of TBMs.	artificial neural network;data mining;dropout (neural networks);gradient descent;k-nearest neighbors algorithm;lr parser;mit engineering systems division;machine learning;mathematical optimization;overfitting;radio frequency;tunneling protocol	Maolin Shi;Xueguan Song;Wei Sun	2018	CoRR		seismology;predictive modelling;artificial neural network;data mining;geology;excavation;tunnel construction;training set	ML	9.880362256171031	-21.314443984224514	22142
543876fa51e1138c9778dc3b367bfa7f667afb53	a differential evolution based approach for the production scheduling of open pit mines with or without the condition of grade uncertainty		Abstract Production scheduling of open pit mines seeks to define such a temporal flow of ore and waste materials from a mine that maximizes a project’s Net Present value (NPV), while satisfying different physical and operational constraints. To achieve this objective, different mathematical formulations have been proposed in the technical literature. However, solving these formulations for a real sized open pit mine could be extremely difficult and computationally challenging job. In order to make this job computationally tractable, different heuristic and metaheuristic techniques are commonly used. This paper presents the results of a study where a real valued population based metaheuristic technique known as Differential Evolution (DE) algorithm has been used with the aim to solve the production scheduling problem of open pit mines with low to moderate computational cost with or without the condition of grade uncertainty. Three different case studies revealed the capabilities and efficiency of DE algorithm by producing sufficiently good solutions of the said with moderate computational cost.	differential evolution;scheduling (computing)	Asif Imtiaz Khan;Christian Niemann-Delius	2018	Appl. Soft Comput.	10.1016/j.asoc.2018.02.010	differential evolution;open-pit mining;mathematical optimization;mathematics;metaheuristic;heuristic;scheduling (production processes);net present value;population	Robotics	13.813408164147418	3.421282569591186	22217
ca74c874af464ae778b479b3a77191a72e63884b	short-term electricity demand forecasting with mars, svr and arima models using aggregated demand data in queensland, australia		Accurate and reliable forecasting models for electricity demand (G) are critical in engineering applications. They assist renewable and conventional energy engineers, electricity providers, end-users, and government entities in addressing energy sustainability challenges for the National Electricity Market (NEM) in Australia, including the expansion of distribution networks, energy pricing, and policy development. In this study, data-driven techniques for forecasting short-term (24-h) G-data are adopted using 0.5 h, 1.0 h, and 24 h forecasting horizons. These techniques are based on the Multivariate Adaptive Regression Spline (MARS), Support Vector Regression (SVR), and Autoregressive Integrated Moving Average (ARIMA) models. This study is focused in Queensland, Australia’s second largest state, where end-user demand for energy continues to increase. To determine the MARS and SVR model inputs, the partial autocorrelation function is applied to historical (area aggregated) G data in the training period to discriminate the significant (lagged) inputs. On the other hand, single input G data is used to develop the univariate ARIMA model. The predictors are based on statistically significant lagged inputs and partitioned into training (80%) and testing (20%) subsets to construct the forecasting models. The accuracy of the G forecasts, with respect to the measured G data, is assessed using statistical metrics such as the Pearson Product-Moment Correlation coefficient (r), Root Mean Square Error (RMSE), and Mean Absolute Error (MAE). Normalized model assessment metrics based on RMSE and MAE relative to observed means (View the MathML source), Willmott’s Index (WI ), Legates and McCabe Index (ELM), and Nash–Sutcliffe coefficients (ENS) are also utilised to assess the models’ preciseness. For the 0.5 h and 1.0 h short-term forecasting horizons, the MARS model outperforms the SVR and ARIMA models displaying the largest WI (0.993 and 0.990) and lowest MAE (45.363 and 86.502 MW), respectively. In contrast, the SVR model is superior to the MARS and ARIMA models for the daily (24 h) forecasting horizon demonstrating a greater WI (0.890) and MAE (162.363 MW). Therefore, the MARS and SVR models can be considered more suitable for short-term G forecasting in Queensland, Australia, when compared to the ARIMA model. Accordingly, they are useful scientific tools for further exploration of real-time electricity demand data forecasting.	autoregressive integrated moving average	Mohanad S. Al-Musaylh;Ravinesh C. Deo;Jan F. Adamowski;Yan Li	2018	Advanced Engineering Informatics	10.1016/j.aei.2017.11.002	electricity market;data mining;engineering;partial autocorrelation function;statistics;normalization (statistics);demand forecasting;multivariate statistics;autoregressive integrated moving average;mean squared error;government	AI	8.515150520157976	-17.47575411284893	22219
d583b8f40245ab2ccb20813cc2fd5f925d446a56	flaw selection strategies for value-directed planning	branch and bound algorithm;decision theoretic;value function;partial order	A central issue faced by partial-order, cansal-link (POCL) planning systems is how to select which flaw to resolve when generating the refinements of a partial plan. Domain-independent flaw selection strategies have been discussed extensively in the recent literature (Peot ~ Smith 1993; Joslin & Pollack 1994; Schubert & Gerevini 1995). The PYRRHUS planning system is a decision-theoretic extension to POCL planners that finds optimal plans for a class of goal-directed value functions. Although PYRRHUS uses a branch-and-bound algorithm instead of best-first satisficing search, it is faced with the same flaw selection decision as other POCL planners. This paper explains why popular domain-independent flawselection strategies are ineffective within an optimizing framework, and presents two new strategies that exploit the additional value information available to PYaam~s.	algorithm;branch and bound;computation;flaw hypothesis methodology;john d. wiley;mathematical optimization;overhead (computing);partial-order planning;pollack's rule;precondition;theory	Mike Williamson;Steve Hanks	1996			engineering;artificial intelligence;operations management;operations research	AI	18.724905193008397	-9.517700287752387	22221
86040ff22243bdd53c61d8b82febc4bfae9ece48	sequential versus simultaneous approach in the location and design of two new facilities using planar huff-like models	modelizacion;optimal solution;sequential design;competition;market share;localization;branch and bound algorithm;plant layout;optimum global;localizacion;probabilistic approach;global optimum;planning installation;modelisation;profit;branch and bound method;localisation;beneficio;metodo branch and bound;enfoque probabilista;approche probabiliste;exact algorithm;benefice;global optimization;aritmetica intervalo;profitability;methode separation et evaluation;interval arithmetic;arithmetique intervalle;equity participation;facility design;modeling;proyecto instalacion;optimo global;toma participacion;continuous location;prise participation;profit maximization;interval analysis;plan sequentiel;plan secuencial	A company frequently decides on location and design for new facilities in a sequential way. However, for a fixed number of new facilities (in this study we restrict ourselves to the case of two new facilities), the company might improve its profit by taking its decisions for all the facilities simultaneously. In this paper a profit maximization problem is considered, where the market share captured by each facility depends on both the distance to the customers (location) and its quality (design), and it is determined by a Huff-like model. Recent research on this type of models was aimed at finding global optima for a single new facility, holding quality fixed or variable, but no exact algorithm has been proposed to find optimal solutions for more than one facility. The existing algorithms coping with the simultaneous problem do a cycle search in which only one location is optimized independently, holding all other locations fixed; therefore it cannot guarantee that an optimal solution has been found. In this paper we propose an exact interval branch-and-bound algorithm to solve the simultaneous location and design 2-facilities problem. We present some computational results to compare the locations and qualities of the optimal solutions obtained by a sequential and two simultaneous approaches.	algorithmic efficiency;branch and bound;computation;entropy maximization;exact algorithm;expectation–maximization algorithm;global illumination;high-frequency direction finding	Boglárka G.-Tóth;José Fernández;Blas Pelegrín;Frank Plastria	2009	Computers & OR	10.1016/j.cor.2008.02.006	mathematical optimization;mathematics;interval arithmetic;1-center problem;global optimization	AI	5.814649106108518	-3.3844212954994286	22243
2a6c99bea9b33b8ec34bdd982ff6f3bf7fbf80cb	"""learning subjective """"cognitive maps"""" in the presence of sensory-motor errors"""	environmental influence;cognitive map;adaptive heuristic critic;rbf neural network;evolutionary strategy	In this paper we present a new version of our previous work on a maze learning animat. Its sensory/motor capabilities have been extended and modified so that they are more biologically plausible than before. The animat's learning architecture is based around a hybrid RBF Neural Network/Evolutionary Strategy implementation of an Adaptive Heuristic Critic. We conduct experiments in which the animat either acquires persistent but undetectable internal errors in its sensory equipment, or operates in an environment where undetectable factors influence motor actions. We also observe the effects of random sensory errors on the usefulness of the information which the animat acquires. Through interactions with its environment the animat learns a subjective cognitive map which is a fusion of the features in its surroundings, the path to a goal state, and the errors/environmental influences which it cannot directly detect. We find that despite the subjective nature of the map it remains useful under quite high levels of error/distortion in our experiments.	cognitive map	Anthony G. Pipe;Brian Carse;Terence C. Fogarty;Alan F. T. Winfield	1995		10.1007/3-540-59496-5_318	cognitive model;fuzzy cognitive map;cognitive map;computer science;artificial intelligence;machine learning;evolution strategy	HCI	17.460297880151938	-21.827165588578367	22333
51c1924ab1334e9c9be0cc50cdac3abc9ad6bad8	merger & acquisition and bubbles on chinese stock market	resource allocation;adaptive control;resource manager;concurrent execution environment;program execution;process termination mechanism;power aware computing;operating system;recyclable resource management method;execution environment;statistical testing corporate acquisitions econometrics securities trading;corporate acquisitions stock markets testing security economic forecasting control systems pricing power generation economics pervasive computing finance;concurrency control;reduced memory consumption;asset pricing bubbles merger acquisition chinese stock market duration dependence test shenzhen security market shanghai security market;high frequency;adaptive control mechanism;process creation mechanism;operating systems	Findings by duration dependence test indicate that there are significant bubbles in market response to merger & acquisition. This paper compares the different responses caused by the same event between Shenzhen security market and Shanghai security market.		Wang Ling;Liu Bo;Gao-feng Zou	2007	The 2007 International Conference on Intelligent Pervasive Computing (IPC 2007)	10.1109/IPC.2007.24	marketing;operations management;business;commerce	Robotics	3.0489520047176115	-12.069155886623125	22351
c506f699a081654c8a02f150f4aa4e58afeda9af	technical note - queueing systems with synergistic servers	flexible manufacturing line balancing;tandem;dynamic programming optimal control;production scheduling;queues;applications	We consider tandem lines with finite buffers and flexible, heterogeneous servers that are synergistic in that they work more effectively in teams than on their own. Our objective is to determine how the servers should be assigned dynamically to tasks in order to maximize the long-run average throughput. In particular, we investigate when it is better to take advantage of synergy among servers, rather than exploiting the servers' special skills, to achieve the best possible system throughput. We show that when there is no trade-off between server synergy and servers' special skills (because the servers are generalists who are equally skilled at all tasks), the optimal policy has servers working in teams of two or more at all times. Moreover, for Markovian systems with two stations and two servers, we provide a complete characterization of the optimal policy and show that, depending on how well the servers work together, the optimal policy either takes full advantage of servers' special skills, or full advantage of server synergy (and hence there is no middle ground in this case). Finally, for a class of larger Markovian systems, we provide sufficient conditions that guarantee that the optimal policy should take full advantage of server synergy at all times.	synergy	Sigrún Andradóttir;Hayriye Ayhan;Douglas G. Down	2011	Operations Research	10.1287/opre.1110.0934	real-time computing;computer science;operations management;distributed computing;scheduling;information technology;queue	Metrics	4.419635269692611	-0.12007553406259704	22355
173e283422111cb1381c03e64544717c6c2346fe	type-2 fuzzy sets made simple	type 2 fuzzy sets;fuzzy set representation;histograms;uncertainties minimization;uncertainty;computational intelligence;rule based;uoa 23 computer science and informatics;data mining;fuzzy set theory;fuzzy sets;fuzzy set theory fuzzy logic;fuzzy logic;rule base fuzzy logic systems;rae 2008;fuzzy sets uncertainty fuzzy logic histograms data mining computational intelligence computer science;membership function;type 2 fuzzy set;fuzzy logic system;extension principle;computer science;complement;union;intersection;article;membership function type 2 fuzzy sets rule base fuzzy logic systems fuzzy set representation union intersection complement uncertainties minimization	Type-2 fuzzy sets let us model and minimize the effects of uncertainties in rule-base fuzzy logic systems. However, they are difficult to understand for a variety of reasons which we enunciate. In this paper, we strive to overcome the difficulties by: 1) establishing a small set of terms that let us easily communicate about type-2 fuzzy sets and also let us define such sets very precisely, 2) presenting a new representation for type-2 fuzzy sets, and 3) using this new representation to derive formulas for union, intersection and complement of type-2 fuzzy sets without having to use the Extension Principle.	formal system;fuzzy logic;fuzzy set;type-2 fuzzy sets and systems	Jerry M. Mendel;Robert Ivor John	2002	IEEE Trans. Fuzzy Systems	10.1109/91.995115	fuzzy logic;discrete mathematics;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy subalgebra;fuzzy number;neuro-fuzzy;machine learning;computational intelligence;data mining;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations	Logic	-1.956326837564081	-23.55604000288464	22442
1fcc31ede29af96ef58b63edb5f00362fd940de6	consistency of pair-wise comparison matrix with fuzzy elements		 This paper deals with measuring inconsistency and incompatibility of pair-wise comparison matrix with fuzzy elements. Here we deal with some properties of such pair-wise comparisons, particularly, reciprocity consistency and compatibility. Moreover, we show how to measure it defining two new indices. The first index of inconsistency is based on the classical concept of consistency, however, instead of principle eigenvector method used in AHP we apply the logarithmic least squares method. Defining the second index of incompatibility we look for a consistent matrix in the form of the ratio matrix with the maximal membership grade “as close as possible” to the original fuzzy matrix. This leads to solving a nonlinear optimization problem which can be transformed to a sequence of LP ones. We compare properties and application areas of both indices. The first index FI is suitable for non-interactive elements of fuzzy matrices, particularly, when uncertainty of individual elements of the matrix can be reflected/measured. The second index GI is appropriate for interactive elements of fuzzy matrices. By GI a measure of compatibility of fuzzy matrix with the closest consistent matrix is expressed. Illustrating examples and simulations are supplied to characterize the concepts and derived properties.	consistency model;fuzzy associative matrix;fuzzy concept;interactivity;least squares;mathematical optimization;maximal set;nonlinear programming;nonlinear system;optimization problem;simulation;software incompatibility;the matrix	Jaroslav Ramík	2009			fuzzy logic;pure mathematics;matrix (mathematics);fuzzy associative matrix;pairwise comparison;mathematics	AI	-2.5678002808794766	-20.731988356090778	22462
ac01aee99e2f9fb103e621be09973e9e1bd3ddc7	a model for solving incompatible fuzzy goal programming: an application to portfolio selection	standard goal programming;infeasibility;socially responsible investing;portfolio selection;development sustainable indexes;fuzzy goal programming;membership function	Abstract#R##N#For many fuzzy goal programming (GP) approaches, in order to build the membership functions of fuzzy aspiration levels, a tolerance threshold for each one of them should be determined. In this paper, we address the case in which the decision maker proposes incompatible thresholds, which could lead to an infeasible problem. We propose an alternative algebraic formulation of the membership functions, which allows us to formulate models capable of providing solutions, although some tolerance thresholds are surpassed. The objective values that do not violate their corresponding threshold are evaluated positively according to the degree of achievement to their fuzzy target, and in turn those who violate the threshold are penalized according to their unwanted deviation with respect to the threshold. Thus, our model jointly uses the fuzzy GP approach and the standard GP approach, which also allows incorporating fuzzy and crisp targets into the same problem. The proposed procedure is applied to socially responsible portfolio selection problems.	goal programming	Mariano Jiménez;Amelia Bilbao-Terol;Mar Arenas Parra	2018	ITOR	10.1111/itor.12405	mathematical optimization;membership function;defuzzification;type-2 fuzzy sets and systems;computer science;artificial intelligence;fuzzy number;operations management;mathematics;welfare economics;fuzzy set operations	Logic	-1.8461613570665611	-17.218251262022232	22467
46f68cf2551471de844cfaefbaf54b378c864d2a	a multi-item continuous review inventory system with compound poisson demands	stock control;politica optima;chaine markov;cadena markov;numerical method;article synthese;relacion orden;ordering;assemblage mecanique;continuous system;joints;compounded structure;optimal policy;continuous review;estructura compuesta;systeme continu;iterative methods;livraison;structure composee;temps calcul;relation ordre;metodo numerico;policy evaluation;methode iterative;sistema continuo;processus markov;key words multi item inventory system;delivery;markov process;gestion stock;new algorithm;policy iteration;tiempo computacion;computation time;policy iteration method;reviews;politique optimale;methode numerique;markov chain	In this paper, we consider a multi-item continuous review inventory system with compound Poisson demands under a general cost structure. Excess demand is backlogged and a fixed delivery lag is assumed. A new algorithm for computing an exact optimal policy is derived based upon the policy iteration method (PIM) using properties of the optimal policy. This algorithm reduces substantially computation times in both the policy evaluation and the policy improvement routines of the PIM. In fact, numerical examples show that the computation times of the new algorithm are less than three percent of those of the PIM. Moreover, three joint ordering policies – the (s, c, S), the (R, T) and the (Q, S) policies – are compared with the optimal ordering policy computed by the new algorithm. It is shown that the (Q, S) policy is almost best among the three joint ordering policy and gives more than ten percent higher expected cost rate than the optimal ordering policy. Copyright Springer-Verlag Berlin Heidelberg 2001		Katsuhisa Ohno;Tomonori Ishigaki	2001	Math. Meth. of OR	10.1007/s001860000101	markov chain;stock control;numerical analysis;order theory;computer science;artificial intelligence;mathematics;iterative method;markov process;algorithm;statistics	Robotics	4.76749923285331	-2.976732384272625	22468
dc91da0dafbba9251ceb263d561a9ed60bc828d5	model for logistics capacity in the perishable food supply chain		Interest in fresh food has increased around the world. However, according to FAO losses of perishable food can reach 50%, depending on the logistics capabilities of the supply chain. A management model for transportation and warehouse capacities for the perishable food supply chain is proposed. The expansion of own capacity is evaluated in comparison to contracting 3PL in two scenarios: with and without cold chain. The model was developed within the system dynamics paradigm, modelled in iThink and evaluated through a case study of the supply chain of mango in Cundinamarca-Bogota. The seasonality of the supply and its discrepancy with the demand is included in the model. The model allows the study of the logistic performance, quality, costs and responsiveness of the Mango Supply Chain.		Javier Arturo Orjuela-Castro;Gina Lizeth Diaz Gamez;Maria Paula Bernal Celemín	2017		10.1007/978-3-319-66963-2_21	supply chain;cold chain;warehouse;business;microeconomics	ML	1.2270160094433509	-7.064573557202692	22493
5f9e951e3589f0f99e40015df7048c7cd026ca9d	modeling a microgrid as a single source using the timeframe capacity factor reliability model		This paper proposes a reliability model to convert a microgrid with several renewable energy sources (RESs) to a single source with an overall capacity factor. The proposed model takes into account the probabilistic behavior of solar and wind power generations. The model utilized the most recent RES capacity factor data available over the course of the study period. The timeframe capacity factor (TFCF) is considered for each renewable energy resource, over a considered timeframe (TF). The proposed method significantly reduces the prerequisite data and running time for reliability assessment compared to the existing probabilistic models.	microgrid;time complexity	Bamdad Falahati;Amin Kargarian Marvasti;Shahab Mehraeen;Yong Fu	2017	2017 IEEE Power & Energy Society Innovative Smart Grid Technologies Conference (ISGT)	10.1109/ISGT.2017.8086082	wind power;renewable energy;probabilistic logic;capacity factor;reliability engineering;microgrid;engineering	Metrics	5.892545389490116	3.3136179278355375	22507
31d4628897ffe8fb719d9013620acfdf9f8c1896	intelligent real-time fault diagnosis: methodology and application	coolants;training;condition monitoring fault diagnosis intelligent systems neural networks;artificial neural networks;pattern recognition;training neurons fault diagnosis valves coolants artificial neural networks pattern recognition;neurons;valves;isann training stage intelligent real time fault diagnosis intelligent diagnostic system intelligent system of artificial neural networks diagnostic test failure system deterioration degree;neural nets fault diagnosis learning artificial intelligence maintenance engineering;fault diagnosis	This paper presents a methodology for application of Intelligent Systems to Fault Diagnosis for Realtime applications. Following the introduction, design of the Intelligent Diagnostic System is discussed. A case study is then, described and the design methodology is applied to the case under investigation. Results of the application of case study are discussed in detail. In this approach an Intelligent System of Artificial Neural Networks ISANN has been trained to learn malfunctions of the test system as well as healthy or standard operational status. ISANN is capable of testing the system under investigation periodically, on a prescheduled plan or on request. Upon completion of the test, condition of operation will be identified and reported with accuracy as being either under normal operation status, or faulty. In the event of a system fault, the diagnostic test identifies the exact cause of failure and the degree of system deterioration with a precision in excess of 98%. The study provides provisions for optimization of training set resulting in an expedited training stage of ISANN.	artificial intelligence;artificial neural network;mathematical optimization;offset binary;real-time transcription;test set	Hosein Marzi	2015	2015 IEEE 28th Canadian Conference on Electrical and Computer Engineering (CCECE)	10.1109/CCECE.2015.7129403	control engineering;simulation;coolant;computer science;engineering;artificial intelligence;artificial neural network	Robotics	14.897125368194693	-16.893546259954046	22519
3ece1e95f6291f7be6150c8735e5a46c305cf2b6	on strong equilibria and improvement dynamics in network creation games		We study strong equilibria in network creation games. These form a classical and well-studied class of games where a set of players form a network by buying edges to their neighbors at a cost of a fixed parameter α. The cost of a player is defined to be the cost of the bought edges plus the sum of distances to all the players in the resulting graph. We identify and characterize various structural properties of strong equilibria, which lead to a characterization of the set of strong equilibria for all α in the range (0, 2). For α > 2, Andelman et al. (2009) prove that a star graph in which every leaf buys one edge to the center node is a strong equilibrium, and conjecture that in fact any star is a strong equilibrium. We resolve this conjecture in the affirmative. Additionally, we show that when α is large enough (≥ 2n) there exist non-star trees that are strong equilibria. For the strong price of anarchy, we provide precise expressions when α is in the range (0, 2), and we prove a lower bound of 3/2 when α ≥ 2. Lastly, we aim to characterize under which conditions (coalitional) improvement dynamics may converge to a strong equilibrium. To this end, we study the (coalitional) finite improvement property and (coalitional) weak acyclicity property. We prove various conditions under which these properties do and do not hold. Some of these results also hold for the class of pure Nash equilibria.	a* search algorithm;anarchy;converge;existential quantification;nash equilibrium	Tomasz Janus;Bart de Keijzer	2017		10.1007/978-3-319-71924-5_12	mathematical optimization;discrete mathematics;conjecture;a* search algorithm;mathematics;nash equilibrium;statistical physics;expression (mathematics);upper and lower bounds;price of anarchy;graph	Theory	-4.397582976258039	0.10295131138815403	22552
08e8ccf80a627ba93a07e1bf2d80516c216baf61	supply and demand equilibration algorithms for a class of market equilibrium problems	spatial price equilibrium;transportes;market equilibrium;markets;equilibrium economics;selected works;goods services market;algorithme;transports;algorithm;mathematical models;transportation;equilibre economique;freight service;prices;bepress;algorithms;supply;marche biens services;economics;demand;equilibrio economico;mercado bienes servicios;economic equilibrium;supply and demand;algoritmo	In this paper, we describe a family of progressive equilibration algorithms which can be used to solve a variety of market equilibrium problems such as the general spatial price equilibrium problem, the single price spatial price equilibrium problem, etc. They are relaxation-type algorithms which attempt to equilibrate the whole system by equilibrating successively each supply market (producer), or each demand market (consumer). One noteworthy feature of these algorithms is that, due to the special structure of the problem, the restricted equilibrium for each supply market (or demand market) can be obtained explicitly in closed form; another feature is that they are intuitive and straightforward to implement. Moreover, the computational results demonstrate that the algorithms are efficient and suitable for large-scale problems.	algorithm	Stella Dafermos;Anna Nagurney	1989	Transportation Science	10.1287/trsc.23.2.118	supply;transport;mathematical optimization;economics;mathematical model;supply and demand;microeconomics;economy;demand	Theory	0.5201234910351101	-3.953908312945919	22559
d9eb6828bf9788384867b9e40b0f582ae4fc62d7	multiobjective synthesis of robust vaccination policies	vaccination planning;robust synthesis;epidemics control;multiobjective optimization;genetic algorithms	This paper deals with the optimal planning of vaccination campaigns, using an evolutionary multiobjective optimization algorithm and a stochastic simulation of the epidemics dynamics in order to determine robust vaccination policies. A biobjective model is formulated, considering the minimization of control costs and number of infected individuals. The decision variables include number of campaigns, percentage of vaccination and time interval between each campaign. A SIR (Susceptible-Infected-Recovered) model and an IBM (Individual-Based Model) are employed for representing the epidemics. A two-stage optimization process is proposed: a set of nondominated steady-state regimes is obtained and one of them is selected to be concatenated to the transient regime vaccination policies. An evolutionary multiobjective optimization algorithm is proposed, with a local search procedure based on quadratic approximation supported by a hash table information storage. The resulting nondominated solutions are simulated in the IBM, in order to detect and discard the non-robust solutions. Final results show that optimal robust vaccination campaigns with different trade-offs can be designed, allowing policymakers to choose the best strategy according to the monetary cost and the expected efficacy.		André R. da Cruz;Rodrigo T. N. Cardoso;Ricardo H. C. Takahashi	2017	Appl. Soft Comput.	10.1016/j.asoc.2016.11.010	mathematical optimization;simulation;genetic algorithm;computer science;multi-objective optimization;mathematics	Logic	15.82323764931403	-2.005090007508475	22560
ea047c325cb5c33fcd2a4f16b5cbec9de6b91ca3	a two-stage r2 indicator based evolutionary algorithm for many-objective optimization		R2 indicator based multi-objective evolutionary algorithms (R2-MOEAs) have achieved promising performance on traditional multi-objective optimization problems (MOPs) with two and three objectives, but still cannot well handle manyobjective optimization problems (MaOPs) with more than three objectives. To address this issue, this paper proposes a two-stage R2 indicator based evolutionary algorithm (TS-R2EA) for many-objective optimization. In the proposed TS-R2EA, we first adopt an R2 indicator based achievement scalarizing function for the primary selection. In addition, by taking advantage of the reference vector guided objective space partition approach in diversity management for many-objective optimization, the secondary selection strategy is further applied. Such a two-stage selection strategy is expected to achieve a balance between convergence and diversity. Extensive experiments are conducted on a variety of benchmark test problems, and the experimental results demonstrate that the proposed algorithm has competitive performance in comparison with several tailored algorithms for many-objective optimization.	auditory processing disorder;benchmark (computing);evolutionary algorithm;experiment;global distance test;moea framework;mathematical optimization;multi-objective optimization;optimization problem;remote desktop services;utility	Fei Li;Ran Cheng;Jianchang Liu;Yaochu Jin	2018	Appl. Soft Comput.	10.1016/j.asoc.2018.02.048	mathematical optimization;machine learning;artificial intelligence;evolutionary algorithm;mathematics;convergence (routing);optimization problem	AI	24.402390987101697	-3.79128868157911	22561
4d693a00652dabb97047df24f13fe1c4e79dfdb6	particle swarm optimization in exploratory data analysis	particle swarm;projection pursuit;reinforcement learning;particle swarm optimizer;principal component analysis;topology preservation;exploratory data analysis	We discuss extensions of particle swarm based optimization (PSO) algorithms in the context of exploratory data analysis. In particular, we apply these extensions to principal component analysis, exploratory projection pursuit and topology preserving mappings. Our extensions include combining PSO algorithms with stochastic sampling and a form of reinforcement learning known as Q-learning. We illustrate on a variety of artificial data sets and show that our new results are better than previous results on such data sets.	exploratory testing;particle swarm optimization	Ying Wu;Colin Fyfe	2010		10.1007/978-3-642-13025-0_28	projection pursuit;mathematical optimization;multi-swarm optimization;computer science;machine learning;pattern recognition;exploratory data analysis;particle swarm optimization;reinforcement learning;principal component analysis	DB	21.030665199264625	-23.333254654834317	22583
9ef2b2b2cce7548e622f1578325e758815bff4c7	fast-converging tatonnement algorithms for one-time and ongoing market problems	rate of convergence;rate of change;tatonnement;asymptotic analysis;satisfiability;demand and supply;upper bound;market equilibria;distributed algorithm;lower bound	Why might markets tend toward and remain near equilibrium prices? In an effort to shed light on this question from an algorithmic perspective, this paper formalizes the setting of Ongoing Markets, by contrast with the classic market scenario, which we term One-Time Markets. The Ongoing Market allows trade at non-equilibrium prices, and, as its name suggests, continues over time. As such, it appears to be a more plausible model of actual markets.  For both market settings, this paper defines and analyzes variants of a simple tatonnement algorithm that differs from previous algorithms that have been subject to asymptotic analysis in three significant respects: the price update for a good depends only on the price, demand, and supply for that good, and on no other information; the price update for each good occurs distributively and asynchronously; the algorithms work (and the analyses hold) from an arbitrary starting point.  Our algorithm introduces a new and natural update rule. We show that this update rule leads to fast convergence toward equilibrium prices in a broad class of markets that satisfy the weak gross substitutes property. These are the first analyses for computationally and informationally distributed algorithms that demonstrate polynomial convergence.  Our analysis identifies three parameters characterizing the markets, which govern the rate of convergence of our protocols. These parameters are, broadly speaking: 1. A bound on the fractional rate of change of demand for each good with respect to fractional changes in its price. 2. A bound on the fractional rate of change of demand for each good with respect to fractional changes in wealth. 3. The closeness of the market to a Fisher market (a market with buyers starting with money alone).  We give two types of protocols. The first type assumes global knowledge of only (an upper bound on) the first parameter. For this protocol, we also provide a matching lower bound in terms of these parameters for the One-Time Market. Our second protocol, which is analyzed for the One-Time Market alone, assumes no global knowledge whatsoever.	centrality;distributed algorithm;fisher market;polynomial;rate of convergence;walrasian auction	Richard Cole;Lisa Fleischer	2008		10.1145/1374376.1374422	distributed algorithm;mathematical optimization;combinatorics;asymptotic analysis;mathematics;mathematical economics;upper and lower bounds;algorithm;statistics	Theory	-3.5111228594597	0.3963929069588907	22599
d806ef1464151c9459345e5be5643a28f6c93cb5	researches on flexible job-shop scheduling problem	population diversity;job shop scheduling combinatorial mathematics evolutionary computation;evolutionary computation;merge and split recombination crossover;symbiosis;combinatorial optimization problems;job shop scheduling;combinatorial optimization problem;individual fitness function;genetics;symbiosis evolutionary computation job shop scheduling process planning processor scheduling testing conference management energy management technology management educational institutions;biological cells;symbiotic evolutionary algorithm;job shop scheduling problem evolutionary algorithms;arithmetic average;evolutionary algorithms;search efficiency;process planning;evolutionary algorithm;job shop scheduling problem;individual fitness function evolutionary algorithms combinatorial optimization problems symbiotic evolutionary algorithm merge and split recombination crossover population diversity search efficiency arithmetic average;encoding;combinatorial mathematics;fitness function	Evolutionary algorithms (EAs) prove to be powerful in solving combinatorial optimization problems. A symbiotic evolutionary algorithm is applied to deal with complex job-shop scheduling problem (JSP). An efficient crossover, Merge and Split Recombination crossover (MSX) which always produces feasible offspring and enhances population diversity and search efficiency, is introduced for the JSP. Evaluation of individual’s contribution in a combination is well important and difficult in symbiotic evolutionary algorithm. A new individual fitness function is defined according to the arithmetic average over previous solution combinations that the individual has taken part in. The improved symbiotic algorithm is tested on a set of standard instances taken from the literature and compared with original approach. Experimental results validate the effectiveness of the improved algorithm, improving the solution quality and decreasing the computation time.	combinatorial optimization;computation;crossover (genetic algorithm);evolutionary algorithm;fitness function;job shop scheduling;mathematical optimization;scheduling (computing);time complexity	Zhaofeng Su;Hongze Qiu	2009	2009 Fifth International Conference on Natural Computation	10.1109/ICNC.2009.432	mathematical optimization;machine learning;evolutionary algorithm;mathematics;algorithm	EDA	21.46053939797641	-0.7643771059488976	22652
afa984a6bc390e0624ac814cc47c58db29cd16c0	special issue on algorithmic game theory	special issue;algorithmic game theory	This issue contains revised and expanded, journal versions of selected papers that were presented at the 4th Symposium on Algorithmic Game Theory that took place in Amalfi, Italy, October 17–19, 2011. The papers cover current topics in the field of Algorithmic Game Theory: mechanisms, games related to routing, auctions, and the study of the equilibria of general games. Two of the papers of the special issue study mechanisms. The paper Scheduling without payments considers the classical problem of designing mechanisms for scheduling jobs on unrelated machines but in a setting where payments are not allowed. Quite surprisingly, the paper shows that it is possible to obtain a non-trivial approximation. The paper A Truthful Mechanism for Value-Based Scheduling in Cloud Computing considers the scheduling of batch jobs on a cloud. Here, the owners of the jobs can specify their willingness to pay as a function of job due dates. Two papers study the structure of equilibria of games that model scenarii in communication networks. The paper Strategic Pricing in Next-hop Routing with Elastic Demands considers next-hop routing by self-interested agents. This model differs from the models in which the source of traffic determines the entire route from source to destination, as agents can only decide about the next hop destination. The paper Weakly-Acyclic (Internet) Routing Games shows that a class of routing games modeling important aspects of Internet routing are weakly-acyclic and thus they admit a pure Nash equilibria that can be found efficiently. The paper Repeated Budgeted Second Price Ad Auction abstracts existing mechanisms for repeated auctions of ads in sponsored search for agents with a fixed budget	algorithmic game theory;approximation;cloud computing;directed acyclic graph;internet;nash equilibrium;routing;scheduling (computing);search engine marketing;telecommunications network	Giuseppe Persiano	2013	Theory of Computing Systems	10.1007/s00224-013-9525-5	implementation theory;algorithmic mechanism design;game theory;algorithmic learning theory;algorithmic game theory	ECom	-2.4650756370392535	0.4866080894194852	22686
91852fad088f6a4d02cb8809ca2c5b233778bf0c	an effective pso-based hybrid algorithm for multiobjective permutation flow shop scheduling	simulated annealing combinatorial mathematics computational complexity flow shop scheduling pareto optimisation particle swarm optimisation probability search problems;manufacturing systems;job permutation;permutation flow shop scheduling;pareto optimisation;probability;premature convergence;job shop scheduling;pareto front;rule based;particle swarm optimization pso;searching behavior;combinatorial optimization problem;search methods;nawaz enscore ham heuristic;flow shop scheduling;remotely operated vehicles;scheduling algorithm job shop scheduling manufacturing systems particle swarm optimization simulated annealing search methods remotely operated vehicles aggregates robustness pareto optimization;simulated annealing;pareto optimization;permutation flow shop scheduling adaptive meta lamarckian learning hybrid algorithm local search multiobjective optimization moo pareto front particle swarm optimization pso;multiobjective local search;ranked order value rule;multiple objectives;scheduling algorithm;particle swarm optimizer;parallel evolution;random weighted linear sum function;computational complexity;aggregates;particle swarm optimization;adaptive local search;multiobjective permutation flow shop scheduling;scheduling problem;multiobjective optimization;robustness;search problems;adaptive meta lamarckian learning;np hard combinatorial optimization;multiobjective optimization moo;particle swarm optimisation;learning strategies;combinatorial mathematics;local search;random weighted linear sum function hybrid algorithm multiobjective permutation flow shop scheduling particle swarm optimization np hard combinatorial optimization adaptive local search ranked order value rule job permutation nawaz enscore ham heuristic probability searching behavior multiobjective local search simulated annealing metalamarckian learning pareto front;hybrid algorithm;metalamarckian learning	This paper proposes a hybrid algorithm based on particle swarm optimization (PSO) for a multiobjective permutation flow shop scheduling problem, which is a typical NP-hard combinatorial optimization problem with strong engineering backgrounds. Not only does the proposed multiobjective algorithm (named MOPSO) apply the parallel evolution mechanism of PSO characterized by individual improvement, population cooperation, and competition to effectively perform exploration but it also utilizes several adaptive local search methods to perform exploitation. First, to make PSO suitable for solving scheduling problems, a ranked-order value (ROV) rule based on a random key technique to convert the continuous position values of particles to job permutations is presented. Second, a multiobjective local search based on the Nawaz-Enscore-Ham heuristic is applied to good solutions with a specified probability to enhance the exploitation ability. Third, to enrich the searching behavior and to avoid premature convergence, a multiobjective local search based on simulated annealing with multiple different neighborhoods is designed, and an adaptive meta-Lamarckian learning strategy is employed to decide which neighborhood will be used. Due to the fusion of multiple different searching operations, good solutions approximating the real Pareto front can be obtained. In addition, MOPSO adopts a random weighted linear sum function to aggregate multiple objectives to a single one for solution evaluation and for guiding the evolution process in the multiobjective sense. Due to the randomness of weights, searching direction can be enriched, and solutions with good diversity can be obtained. Simulation results and comparisons based on a variety of instances demonstrate the effectiveness, efficiency, and robustness of the proposed hybrid algorithm.	aggregate data;combinatorial optimization;flow shop scheduling;heuristic;hybrid algorithm;local search (optimization);mathematical optimization;optimization problem;pareto efficiency;particle swarm optimization;premature convergence;randomness;scheduling (computing);simulated annealing;simulation	Bin-Bin Li;Ling Wang;Bo Liu	2008	IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans	10.1109/TSMCA.2008.923086	job shop scheduling;mathematical optimization;computer science;artificial intelligence;multi-objective optimization;machine learning;mathematics	AI	22.958785464226704	-1.3446859825247828	22703
6526da49e2f9674155bf75032a8dda21acbf6562	productivity growth and efficiency measurements in fuzzy environments with an application to health care	institutional repositories;fedora;fuzzy data;vital;data envelopment analysis;data envelopment analysis dea;malmquist productivity index;decision making units dmu;decision making units;vtls;article;ils;health care management;malmquist productivity index mpi	Health care organizations must continuously improve their productivity to sustain long-term growth and profitability. Sustainable productivity performance is mostly assumed to be a natural outcome of successful health care management. Data envelopment analysis (DEA) is a popular mathematical programming method for comparing the inputs and outputs of a set of homogenous decision making units (DMUs) by evaluating their relative efficiency. The Malmquist productivity index (MPI) is widely used for productivity analysis by relying on constructing a best practice frontier and calculating the relative performance of a DMU for different time periods. The conventional DEA requires accurate and crisp data to calculate the MPI. However, the real-world data are often imprecise and vague. In this study, the authors propose a novel productivity measurement approach in fuzzy environments with MPI. An application of the proposed approach in health care is presented to demonstrate the simplicity and efficacy of the procedures and algorithms in a hospital efficiency study conducted for a State Office of Inspector General in the United States. DOI: 10.4018/ijfsa.2012040101 2 International Journal of Fuzzy System Applications, 2(2), 1-35, April-June 2012 Copyright © 2012, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited. In addition to comparing the relative performance of a set of DMUs at a specific period, the conventional DEA can also be used to calculate the productivity change of a DMU over time using the Malmquist productivity index, hereafter referred to as MPI. The MPI was first introduced by Malmquist (1953). Cave et al. (1982a, 1982b) proposed a MPI, which calculated the relative performance of a DMU for different time periods using a parametric method. Färe et al. (1989) and Färe, Ggrosskopf, and Lovell (1994) proposed a non-parametric Malmquist index for productivity analysis that relied on constructing a best practice frontier and computing the distance of individual observations from the frontier. Productivity is measured by the MPI and defined as the ratio between efficiency, as calculated by the DEA, for the same DMU in two different time periods. Several modifications for calculating MPI have been proposed in the literature. Jacobs et al. (2006, Ch. 6) provided a comprehensive review of the MPI in health care. MPI is a very useful method for calculating the productivity change in the DMUs and many applications have been reported in the literature (Chang et al., 2009; Chen, 2003; Chen & Ali, 2004; Emrouznejad & Thanassoulis, 2010; Fiordelisi & Molyneux, 2010; Hashimoto et al., 2009; Kao, 2010; Liu & Wang, 2008; Odeck, 2000, 2006, 2009; Oliveira et al., 2009; Swanson Kazley & Ozcan, 2009; Tsekouras et al., 2004; Zhou et al., 2010). In health care, the growing trends of rising costs have forced the government agencies and health care providers to be more concerned with their profitability and productivity. MPI has been widely used in health care to evaluate productivity change in hospitals, nursing homes, dialysis providers, and pharmacies, among others (Chang et al., 2011; Färe et al., 1995; Ouellette & Vierstraete, 2004; Ozgen, 2006; Retzlaff-Roberts et al., 2004; Kirigia et al., 2004; O’Neill et al., 2008; Kirigia et al., 2008). Hollingsworth (2008) provides a comprehensive review of the DEA literature in health care. The conventional MPI requires precise measurement of the inputs and outputs. One of the main challenges associated with the application of the MPI is the difficulty in quantifying some of the input and output data in real-world problems where the observed values are often imprecise or vague. Imprecise or vague data may be the result of unquantifiable, incomplete and non obtainable information. One way to manipulate uncertain data for the MPI is to represent the imprecise or vague values by membership functions of the fuzzy sets theory. In this study, we propose a novel productivity measurement approach with MPI in Fuzzy Environments. We show the application of the proposed approach in a hospital efficiency study conducted for a State Office of Inspector General in the United States. The remainder of this paper is organized as follows. The next section presents the relative preliminaries and definitions including the basic DEA model, the MPI with precise data, and the Malmquist index under VRS and scale efficiency change. Fuzzy set definitions are given and the MPI under VRS and scale efficiency change with fuzzy data is presented. The applicability of the proposed framework in health care and exhibits the efficacy of the procedures and algorithms is then discussed. The final section consists of the conclusion and future research directions. PRELIMINARIES AND DEFINITIONS This section presents a review of the basic DEA models and MPI with precise data followed by some basic definitions of the fuzzy sets theory. The Basic DEA Models The DEA model was initially introduced by Charnes et al. (1978) as the CCR model to measure technical efficiency with the assumption of constant returns to scale (CRS). Banker et al. (1984) subsequently extended the CCR model to accommodate a more flexible variable returns to scale (VRS) by relaxing the constant returns to scale assumption in their model known as the BCC model. Since then, DEA has been International Journal of Fuzzy System Applications, 2(2), 1-35, April-June 2012 3 Copyright © 2012, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited. widely applied to measure the relative efficiency of a group of homogeneous DMUs. Let us consider n DMUs where each DMUj ( j n = 1 2 , ,..., ) uses a vector of inputs xij ( i m = 1 2 , ,..., ) to produce a vector of outputs yrj (r s = 1 2 , ,..., ). The CCR and BCC models are introduced by the following linear programming models for problems with precise input and output data: The precise CCR model (1a):	algorithm;best practice;bricx command center;data envelopment analysis;digital mockup;entity–relationship model;fuzzy control system;fuzzy logic;fuzzy set;input/output;linear programming;mathematical optimization;uncertain data;vagueness;virtual reference station;word lists by frequency	Adel Hatami-Marbini;Madjid Tavana;Ali Emrouznejad	2012	IJFSA	10.4018/ijfsa.2012040101	engineering;operations management;data mining;operations research	AI	-3.7504683223551396	-16.001568069841674	22740
42fe0c4fe07278677cb20ba03d5117314110792d	provably-good distributed algorithm for constrained multi-robot task assignment for grouped tasks	polynomials cost reduction distributed algorithms iterative methods mobile robots multi robot systems;distributed algorithms;nickel;nickel resource management robot kinematics distributed algorithms indexes algorithm design and analysis;resource management;auxiliary variable provably good distributed task assignment algorithms constrained multirobot task assignment grouped tasks heterogeneous multirobot system disjoint groups task allocation total payoff maximization total payoff minimization modified payoff function;indexes;task allocation auction algorithm distributed algorithm multi robot task assignment;algorithm design and analysis;robot kinematics	In this paper, we present provably-good distributed task assignment algorithms for a heterogeneous multi-robot system, in which the tasks form disjoint groups and there are constraints on the number of tasks a robot can do (both within the overall mission and within each task group). Each robot obtains a payoff (or incurs a cost) for each task and the overall objective for task allocation is to maximize (minimize) the total payoff (cost) of the robots. In general, existing algorithms for task allocation either assume that tasks are independent or do not provide performance guarantee for the situation, in which task constraints exist. We present a distributed algorithm to provide an almost optimal solution for our problem. The key aspect of our distributed algorithm is that the overall objective is (almost) maximized by each robot maximizing its own objective iteratively (using a modified payoff function based on an auxiliary variable, called price of a task). Our distributed algorithm is polynomial in the number of tasks, as well as the number of robots.	distributed algorithm;polynomial;robot	Lingzhi Luo;Nilanjan Chakraborty;Katia P. Sycara	2015	IEEE Transactions on Robotics	10.1109/TRO.2014.2370831	nickel;database index;algorithm design;distributed algorithm;mathematical optimization;real-time computing;computer science;artificial intelligence;resource management;distributed computing;robot kinematics	Robotics	-1.1283132998363212	2.8310844859599986	22753
ecfb9075e5666c442c6e5c61280e7f020f3a501d	calculating functions of interval type-2 fuzzy numbers for fault current analysis	fault currents fuzzy sets fuzzy systems uncertainty fuzzy set theory input variables reliability engineering availability equations shape;fuzzy number;power distribution faults;fault currents;type 2 fuzzy fault currents;fuzzy set theory;electric distribution system;work function;membership function;type 2 fuzzy set;fault current analysis;electric distribution system interval type 2 fuzzy numbers fault current analysis type 2 fuzzy fault currents;extension principle;electricity distribution;interval type 2 fuzzy numbers;interval type 2 fuzzy sets extension principle;power distribution faults fault currents fuzzy set theory;interval type 2 fuzzy sets	In this work, functions of type-2 fuzzy numbers are analyzed. For the special case of interval type-2 fuzzy numbers, the type-2 membership function of the output variable is calculated using the lower and upper membership functions of the input variables and the vertex method. This procedure is used in an application where the type-2 fuzzy fault currents of an electric distribution system are calculated. The results are shown and the advantages of the approach are discussed	approximation algorithm;defuzzification;embedded system;fuzzy logic;fuzzy set;type-2 fuzzy sets and systems;uncertain data	Julio Romero Agero;Alberto Vargas	2007	IEEE Transactions on Fuzzy Systems	10.1109/TFUZZ.2006.889757	mathematical analysis;discrete mathematics;membership function;work function;defuzzification;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy number;control theory;mathematics;fuzzy set;fuzzy set operations	Visualization	-0.8961611349048079	-22.251831495903197	22767
d5201c291450a278ed31a93c0bd9f8ca5477db1e	a fuzzy multi-criteria group decision making based on ranking interval type-2 fuzzy variables and an application to transportation mode selection problem	interval type-2 fuzzy variable;ranking fuzzy variables;fuzzy multi-criteria group decision making;generalized credibility;transportation mode selection	In this paper, we propose a fuzzy multi-criteria group decision-making method based on ranking interval type-2 fuzzy variables. First, we present a ranking method developed by relative preference index which we define using generalized credibility measure to rank interval type-2 fuzzy variables. Then, based on the proposed ranking method, we develop a new method to solve fuzzy multi-criteria group decision-making (FMCGDM) problems where linguistic ratings of the alternatives and the criteria weights are represented by interval type-2 fuzzy variables. The proposed FMCGDM method is applied to a transportation mode selection problem to find most preferable mode among available modes based on some selection criteria. The proposed method is simple in computation and important as it uses interval type-2 fuzzy variables which are more reasonable and sensible to represent linguistic terms rather than by type-1 fuzzy variables. A numerical experiment has been done to illustrate the proposed method.	selection algorithm	Pradip Kundu;Samarjit Kar;Manoranjan Maiti	2017	Soft Comput.	10.1007/s00500-015-1990-0	fuzzy logic;mathematical optimization;membership function;defuzzification;fuzzy transportation;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;fuzzy number;machine learning;fuzzy measure theory;data mining;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;fuzzy control system	AI	-2.7504749190979463	-20.263031816537122	22769
719e25f07b4a02945cb1bfb7aec4a0a2ef83299b	the effect of oil price on containership speed and fleet size	flotte;navio;modelizacion;forecasting;transportation network;consommation combustible;reliability;red transporte;project management;information systems;transporte maritimo;maintenance;maritime transportation;optimal speed;soft or;information technology;packing;operations research;location;effet dimensionnel;investment;journal;fuel consumption;journal of the operational research society;inventory;purchasing;modelisation;flota;minimizacion costo;consumo combustible;history of or;minimisation cout;logistics;reducteur vitesse;cost minimization;fuel oil;marketing;scheduling;size effect;fleet;conteneur;liner shipping;production;communications technology;operating cost;sea transport;containership fleet size;cout exploitation;ship;computer science;operational research;efecto dimensional;reductor velocidad;oil price;contenedor;modeling;speed reducer;applications of operational research;or society;jors;container;management science;infrastructure;reseau transport;costo explotacion;transport maritime;navire	The changing prices of bunker fuel open the door for substantial cost savings by adjusting the sailing speed of ships. A large ship may be burning up to 100 000 USD of bunker fuel per day, which may constitute more than 75% of its operating costs. Reducing the cruising speed by 20% reduces daily bunker consumption by 50%. However, in order to maintain liner service frequency and capacity, reducing the cruising speed may require additional ships to operate a route. We construct a cost model that we use to analyse the trade-off between speed reduction and adding vessels to a container line route, and devise a simple procedure to identify the sailing speed and number of vessels that minimize the annual operating cost of the route. Using published data, we demonstrate the potential for large-cost savings when one operates close to the minimal-cost speed. The presented methodology and procedure are applicable for any bunker fuel price. Journal of the Operational Research Society (2011) 62, 211–216. doi:10.1057/jors.2009.169 Published online 13 January 2010	analysis of algorithms;schedule (computer science)	D. Ronen	2011	JORS	10.1057/jors.2009.169	fuel oil;project management;logistics;inventory;economics;forecasting;investment;computer science;marketing;operations management;reliability;location;operations research;information technology;scheduling;container	Arch	8.436125736528185	-6.6234994940188665	22806
f5d1060a6f9db644f8094a6bc9f3d9f908671f79	reducing variance in gradient bandit algorithm using antithetic variates method		Policy gradient, which makes use of Monte Carlo method to get an unbiased estimation of the parameter gradients, has been widely used in reinforcement learning. One key issue in policy gradient is reducing the variance of the estimation. From the viewpoint of statistics, policy gradient with baseline, a successful variance reduction method for policy gradient, directly applies the control variates method, a traditional variance reduction technique used in Monte Carlo, to policy gradient. One problem with control variates method is that the quality of estimation heavily depends on the choice of the control variates. To address the issue and inspired by the antithetic variates method for variance reduction, we propose to combine the antithetic variates method with traditional policy gradient for the multi-armed bandit problem. Furthermore, we achieve a new policy gradient algorithm called Antithetic-Arm Bandit (AAB). In AAB, the gradient is estimated through coordinate ascent where at each iteration gradient of the target arm is estimated through: 1) constructing a sequence of arms which is approximately monotonic in terms of estimated gradients, 2) sampling a pair of antithetic arms over the sequence, and 3) re-estimating the target gradient based on the sampled pair. Theoretical analysis proved that AAB achieved an unbiased and variance reduced estimation. Experimental results based on a multi-armed bandit task showed that AAB can achieve state-of-the-art performances.	algorithm;antithetic variates;baseline (configuration management);coat of arms;control variates;coordinate descent;iteration;monte carlo method;multi-armed bandit;performance;reinforcement learning;sampling (signal processing);stochastic gradient descent;times ascent;variance reduction	Sihao Yu;Jun Xu;Yanyan Lan;Jiafeng Guo;Xueqi Cheng	2018		10.1145/3209978.3210068	control variates;unbiased estimation;computer science;reinforcement learning;monte carlo method;monotonic function;variance reduction;sampling (statistics);algorithm;antithetic variates	ML	23.937777081247145	-18.420807112012515	22821
41334472b72fa83409655cd6a066bdd6606d6414	similarity measures in scientometric research: the jaccard index versus salton's cosine formula	bibliometrie;scientometrics;modele mathematique;bibliometria;modelo matematico;scientometria;indexation;scientometrie;mathematical model;bibliometry;similarity measure	Abstract It is shown that in most practical cases Saltonu0027s cosine formula yields a numerical value that is twice Jaccardu0027s index.		Lieve Hamers;Yves Hemeryck;Guido Herweyers;Marc Janssen;Hans Keters;Ronald Rousseau;André Vanhoutte	1989	Inf. Process. Manage.	10.1016/0306-4573(89)90048-4	scientometrics;computer science;mathematical model;mathematics;world wide web;statistics	DB	1.894469363615702	-15.619561112342538	22891
6d44dadc14b961cda228075edb26a255d84fe80c	safety prediction of rail transit system based on deep learning		The safety prediction of rail transit system is a fundamental problem in rail transit modeling and management. In this paper, we propose a safety prediction model based on deep learning for rail transit safety, which has been implemented as a deep belief network (DBN). It can learn effective features for rail transit prediction in an unsupervised fashion, which has been examined and found to be effective for many areas such as image and audio classification. To increase the accuracy of prediction, we introduce user satisfaction and rare-event probability, the new input prediction factors, into safety prediction. The former takes account of human and the latter is computed by statistic model checking. To show proof of the model, a real-world subway data sets based on the Beijing Metro in China is presented to demonstrate the feasibility of the model. Experiments on data sets show good performance of our prediction. These positive results demonstrate that deep learning and new factors are promising in rail transit research.	bayesian network;complex systems;computer user satisfaction;deep belief network;deep learning;experiment;feature learning;model checking;system safety;unsupervised learning	Yan Zhang;Jiazhen Han;Weiping Li;Tingliang Zhou;Junfeng Sun;Juan Luo	2017	2017 IEEE/ACIS 16th International Conference on Computer and Information Science (ICIS)	10.1109/ICIS.2017.7960111	model checking;simulation;public transport;machine learning;statistic;deep belief network;deep learning;monte carlo method;artificial intelligence;beijing;data set;computer science	ML	8.613745515230487	-23.786958680563586	22908
640cc2e22a085b8c54fa4d7e302ebc82950bf056	a data-integrated tree-based simulation to predict financial market movement		The Standard and Poor’s 500 Index (S&P500) is one of the commonly used indices on the New York Stock Exchange. The 500 publicly traded companies that make up the index are chosen by a committee to best reflect the overall market of the United States. The broader objective of this research is to estimate the dynamics of the financial market movement in the United States. It is achieved by developing a data-integrated tree-based simulation model to predict S&P500 open and close values for a week. Classification and Regression Trees (CART) a data mining method is utilized to extract patterns of the financial market dynamics based on a data set collected from May 1, 2008 to November 30, 2009. The data set included the daily movement of financial markets in seven countries in Asia and Europe in relation to the daily movement of the S&P500. CART also utilized data on the currency exchange rates to capture the financial dynamics between the US and other countries. The simulation model repeatedly samples from four trees developed by CART to know how the opening and closing values of the S&P500 move in tandem with the other markets. DOI: 10.4018/joris.2012070105 International Journal of Operations Research and Information Systems, 3(3), 74-86, July-September 2012 75 Copyright © 2012, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited. One of the primary tasks of institutional fund managers and financial analysts is to predict how the market is going to move on a daily basis so that they can better reach their returns goals. To aid the fund managers and financial analysts, this research develops a data-integrated tree-based simulation model to predict how the opening and closing price of the S&P500 move in tandem with the other markets. Classification and Regression Tree (CART) model – a data mining tool for prediction and classification (Breiman, Friedman, Oishen, & Stone, 1984)is used to develop four regression tree structures: (1) “first” tree predicts S&P500 Open value for Monday mornings based on other market indices, currency exchange rates, and S&P500 open and close values of the prior Friday; (2) “second” tree predicts the S&P500 close value for Monday evenings based on other market indices, currency exchange rates, S&P500 open and close values of the prior Friday, and the predicted S&P500 open value for that morning; (3) “third” tree predicts the S&P500 open values for Tuesday through Friday based on previous day’s predicted S&P500 open and close values; and (4) “fourth” tree predicts the S&P500 close values for Tuesday through Friday based on the predicted value of that day’s S&P500 open value, and previous day’s open and close values. Simulation models developed by sampling from these four trees are better representation of the actual system and more efficient to execute.	closing (morphology);data mining;decision tree learning;information systems;operations research;sampling (signal processing);simulation;statistical classification;the new york times	Durai Sundaramoorthi;Andrew Coult;Dung Hai Nguyen	2012	IJORIS	10.4018/joris.2012070105	economics;marketing;operations management;economy	ML	3.9648819816633885	-15.75464348466814	22941
a714fe8929b9f1c1929e67a1301cd8b61888c55a	reconstruction of function block logic using metaheuristic algorithm: initial explorations	computers;manipulators;input variables;iec standards;iec standards input variables inference algorithms labeling algorithm design and analysis manipulators computers;inference algorithms;programmable controllers ant colony optimisation boolean algebra control charts evolutionary computation;algorithm design and analysis;labeling;boolean input output variable function block logic metaheuristic algorithm automation logic iec 61499 basic function execution control chart ant colony optimization evolutionary computation	This paper presents an approach for automatic reconstruction of automation logic from execution scenarios using a metaheuristic algorithm. The IEC 61499 basic function blocks is chosen as implementation language and reconstruction of Execution Control Charts for basic function blocks is addressed. The synthesis method is based on a metaheuristic algorithm most closely related to ant colony optimization and evolutionary computation. Execution scenarios can be recorded from testing legacy software solutions. At this stage results are only limited to generation of basic function blocks having only Boolean input/output variables.	algorithm;ant colony optimization algorithms;centralized computing;chart;evolutionary computation;input/output;legacy system;mathematical optimization;metaheuristic;object language;pick operating system;user interface	Daniil S. Chivilikhin;Anatoly Shalyto;Sandeep Patil;Valeriy Vyatkin	2015	2015 IEEE 13th International Conference on Industrial Informatics (INDIN)	10.1109/INDIN.2015.7281912	parallel metaheuristic;computer science;theoretical computer science;machine learning;function block diagram;algorithm	EDA	19.07269428595075	-7.892297767787654	22946
67e008de8317ce3fa8095b8206884ff5f1336058	artificial satellite heat pipe design using harmony search		The design of an artificial satellite requires an optimization of multiple objectives with respect to performance, reliability, and weight. In order to consider these objectives simultaneously, multi-objective optimization technique can be considered. In this chapter, a multi-objective method considering both thermal conductance and heat pipe mass is explained for the design of a satellite heat pipe. This method has two steps: at first, each single objective function is optimized; then multi-objective function, which is the sum of individual error between current function value and optimal value in terms of single objective, is minimized. Here, the multi-objective function, representing thermal conductance and heat pipe mass, has five design parameters such as 1) length of conduction fin, 2) cutting length of adhesive attached area, 3) thickness of fin, 4) adhesive thickness, and 5) operation temperature of the heat pipe. Study results showed that the approach using harmony search found better solution than traditional calculus-based algorithm, BFGS.	harmony search;heat pipe	Zong Woo Geem	2015		10.1007/978-3-662-47926-1_40	hydrology;artificial intelligence;remote sensing	Robotics	17.100471897004727	-5.26823107169646	22949
bfd34500f064db887d9392c90bdd634f57ecfebf	sequence mining based alarm suppression		Despite the high-pace improvement of industrial process automation, the management of abnormal events still requires human actions. Alarm systems are becoming crucial in providing situation-specific information to the decreasing number of operators. The key role of an alarm management system is to ensure that only the currently significant alarms are annunciated. The design of alarm suppression rules requires the systematic analysis of the process and its control system. We give an overview of the recently developed data-driven techniques and show that the widely applied correlation-based methods utilize a static view of the system. To provide more insight into the process dynamics and represent the temporal relationships among faults, control actions, and process variables, we propose of a multi-temporal sequence mining-based algorithm. The methodology starts with the generation of frequent temporal patterns of the alarm signals. We transform the multi-temporal sequences into Bayes classifiers. The obtained association rules can be used to define the alarm suppression rules. We analyze the data set of a laboratory-scale water treatment testbed to illustrate that multi-temporal sequences are applicable for the description of operation patterns. We extended the benchmark simulator of a vinyl acetate production technology to generate easily reproducible results and stimulate the development of alarm management algorithms. The results of detailed sensitivity analyses confirm the benefits of the application of temporal alarm suppression rules, which are reflecting the dynamical behavior of the process.	algorithm;association rule learning;benchmark (computing);control system;sequential pattern mining;testbed;zero suppression	Gyula Dorgo;János Abonyi	2018	IEEE Access	10.1109/ACCESS.2018.2797247	operator (computer programming);process control;algorithm design;alarm;real-time computing;alarm management;association rule learning;computer science;distributed computing;process automation system;control system	DB	14.172463458663946	-14.57974290152036	22968
ddafd3fad2d30eccb7c197d52acd460819d00b6c	an evolutionary algorithm based on a new decomposition scheme for nonlinear bilevel programming problems	optimal solutions;decomposition scheme;evolutionary algorithm;bilevel programming;nonlinear bilevel programming	In this paper, we focus on a class of nonlinear bilevel programming problems where the follower’s objective is a function of the linear expression of all variables, and the follower’s constraint functions are convex with respect to the follower’s variables. First, based on the features of the follower’s problem, we give a new decomposition scheme by which the follower’s optimal solution can be obtained easily. Then, to solve efficiently this class of problems by using evolutionary algorithm, novel evolutionary operators are designed by considering the best individuals and the diversity of individuals in the populations. Finally, based on these techniques, a new evolutionary algorithm is proposed. The numerical results on 20 test problems illustrate that the proposed algorithm is efficient and stable.	evolutionary algorithm;nonlinear programming;nonlinear system;numerical analysis;population	Hecheng Li;Yuping Wang	2010	IJCNS	10.4236/ijcns.2010.31013	evolutionary programming;mathematical optimization;computer science;evolutionary algorithm;mathematics;algorithm	Theory	23.650425064384915	-3.3508612702458374	22993
fb8d4613ae7e5450b9fb0e325da42fb07cd6d84b	$$(q, r, l)$$ ( q , r , l ) model for stochastic demand with lead-time dependent partial backlogging	lead time;backordering;reorder point;order quantity	The paper deals with an economic order quantity model for variable lead-time, order dependent purchasing cost, order size, reorder point and lead-time dependent partial backlogging. The average expected cost function is formulated by trading off setup cost, purchasing cost, lead-time crashing cost, inventory cost and costs of lost sale and partial backordering. In this cost function, order quantity, reorder point and lead-time are decision variables. The above average expected cost function is analysed by calculus method in light of both distribution-free and known distribution function. Numerical example is illustrated to justify our proposed model. Copyright Springer Science+Business Media New York 2015		Shib Sankar Sana;Suresh Kumar Goyal	2015	Annals OR	10.1007/s10479-014-1731-2	reorder point;economics;operations management;microeconomics;mathematical economics	AI	2.713726800644776	-4.122344658606531	23014
7a4a9ce636d99b0a04d3aab43e598605b821c78f	supply chain contracts for capacity decisions under symmetric and asymmetric information	supply chain;contract;capacity;asymmetric information;random demand	Production capacity decision under random demand is an important factor that significantly effects supply chain profits. It is realized in decentralized supply chains that the suppliers build capacity levels that are less than optimal for the total supply chain, since the supplier incurs all the cost and bears all the risk for the built capacity. To improve the supply chain performance, we analyze supply chain contracts considering capacity decisions in a two-party supply chain composed of a single manufacturer and a single supplier. We analyze and compare four well-known contracts, namely, simple wholesale price only contract, linear contract, cost sharing contract and revenue sharing contract under symmetric and asymmetric information about the supplier’s capacity building cost. The choice of the contract and determining the optimal contract parameters might be difficult for the manufacturer, especially if he has incomplete information about the supplier. In the asymmetric information models, we analyze the screening problem of the manufacturer when designing a menu of contracts without exact knowledge of the supplier’s capacity cost. We determine the optimal menu of contracts designed for both high and low cost suppliers and analyze their results through numerical experiments. Focusing on the capacity decisions under random demand, we aim to answer the three questions: (i) Which contracts coordinate the supply chain; (ii) Which contracts allow for any division of the supply chains profit among the firms; and (iii) Which contracts are worth adopting. We find the optimal contract parameters, determine the respective profits obtained by the supply chain members, and find which contracts would be better to use for the companies depending on the system parameters in different settings by analyzing and comparing the efficiencies of the contracts.		Onur Kaya;Serra Caner	2018	CEJOR	10.1007/s10100-017-0474-y	contract management;operations management;microeconomics;supply chain;completed-contract method;commerce	Theory	-0.535534542825205	-5.769218690389001	23045
9f30c4aa556430216ff456ea32319129dbeecc43	towards a distributed implementation of chemical reaction optimization for the multi-factory permutation flowshop scheduling problem		Abstract The Distributed Permutation Flowshop Scheduling Problem (DPFSP) is one of the most computationally complex problems. It has gained a wide attention not only in theoretical studies but also in manufacturing industry. In recent years, a lot of work has been done and many heuristics and metaheuristics have been proposed to tackle the DPFSP. Unfortunately, all the existing algorithms are centralized despite the fact that the distributed approaches are known to be more practical for the complex scheduling problems ones. Thus, we argue that distributed artificial intelligence techniques, namely Multi-Agent Systems (MAS), offer an appropriate tool to tackle problems of a distributed nature when they are properly designed and implemented. Thanks to their flexibility, adaptively and extensibility; MAS represents a promising variant to achieve a better performance. In this study, by combining the population-based evolutionary searching abilities of Chemical Reaction Optimization (CRO) metaheuristic with the capabilities of MAS in modeling hard combinatorial problems, we suggest an agent-based evolutionary algorithm called CROMAS to effectively solve the DPFSP. We tested our algorithm on well-known benchmark instances and compared its performance with respect to other recent methods. Experiments reveal that CROMAS is very effective and able to provide competitive results.	scheduling (computing)	Hafewa Bargaoui;Olfa Belkahla Driss;Khaled Ghédira	2017		10.1016/j.procs.2017.08.057	job shop scheduling;permutation;heuristics;metaheuristic;evolutionary algorithm;scheduling (computing);computer science;population;mathematical optimization;multi-agent system	Theory	23.27833607476722	-3.673425579966919	23047
9cda51b59de693f433b56c42d2a25ede76964101	analysis of a dynamic adverse selection model with asymptotic efficiency	principal agent model;computational geometry;dynamic adverse selection;markov decision processes	This paper studies an infinite horizon adverse selection model with an underlying two-state Markov decision process. It introduces a novel approach that constructs the continuation payoff frontier exactly, as the fixed point of a functional operator. If the model supports an incentive compatible first-best (ICFB) contract, the continuation payoff frontier can be efficiently constructed, and the principal’s optimal contracts converge to ICFB contracts over time. The existence of an ICFB contract is implied by the common assumption of private values and is a fairly general scenario. The paper generalizes some key findings in the dynamic adverse selection literature to this scenario.	continuation;converge;fixed point (mathematics);markov chain;markov decision process	Hao Zhang	2012	Math. Oper. Res.	10.1287/moor.1120.0541	markov decision process;mathematical optimization;computational geometry;mathematics;mathematical economics;statistics	PL	-1.1203060740032	-1.9222820441808923	23198
f08fa521e0e1768b89d40ff5749aa6e7f1690a70	a methodology and simulation of workload control in cellular manufacturing	cellular manufacturing		simulation	A. E. Batsis;Ilias P. Tatsiopoulos	1993			operations management;automotive engineering;manufacturing engineering	Robotics	10.331645640061884	4.032132321060489	23241
ca34991cd9ff3d96a9ee18c179f59f3bc3553c73	using lstm recurrent neural networks to predict excess vibration events in aircraft engines	microprocessors;vibrations;computer architecture;aircraft propulsion;engines;logic gates;biological neural networks	This paper examines building viable Recurrent Neural Networks (RNN) using Long Short Term Memory (LSTM) neurons to predict aircraft engine vibrations. The model is trained on a large database of flight data records obtained from an airline containing flights that suffered from excessive vibration. RNNs can provide a more generalizable and robust method for prediction over analytical calculations of engine vibration, as analytical calculations must be solved iteratively based on specific empirical engine parameters, and this database contains multiple types of engines. Further, LSTM RNNs provide a “memory” of the contribution of previous time series data which can further improve predictions of future vibration values. LSTM RNNs were used over traditional RNNs, as those suffer from vanishing/exploding gradients when trained with back propagation. The study managed to predict vibration values for 5, 10 and 20 seconds in the future, with 3.3%, 5.51% and 10.19% mean absolute error, respectively. These neural networks provide a promising means for the future development of warning systems so that suitable actions can be taken before the occurrence of excess vibration to avoid unfavorable situations during flight.	approximation error;artificial neural network;backpropagation;database;gradient;long short-term memory;neural networks;offset binary;random neural network;recurrent neural network;software propagation;time series	AbdElRahman ElSaid;Brandon Wild;James Higgins;Travis Desell	2016	2016 IEEE 12th International Conference on e-Science (e-Science)	10.1109/eScience.2016.7870907	structural engineering;control engineering;simulation;engineering	ML	10.63686264863671	-20.85640321198981	23316
a92b71283b92170e8e854d3d05939dbaeb5c2637	a tractable multiple agents protocol and algorithm for resource allocation under price rigidities	dynamic mechanism;resource allocation;constrained walrasian equilibrium;multiple agents system;price rigidity	In many resource allocation problems, economy efficiency must be taken into consideration together with social equality, and price rigidities are often made according to some economic and social needs. We investigate the computational issues of dynamic mechanisms for selling multiple indivisible objects under price rigidities. We propose a multiple agents protocol and algorithm with polynomial time complexity that can achieve the over-demanded sets of items, and then introduce a dynamic mechanism with rationing to discover constrainedWalrasian equilibria under price rigidities in polynomial time. We also address the computation of buyers’ expected profits and items’ expected prices, and discuss strategical issues in the sense of expected profits.	algorithm;cobham's thesis;computation;indivisible;polynomial;price point;real life;shadow price;social equality;time complexity;walrasian auction	Wei Huang;Hongbo Liu;Guangyao Dai;Ajith Abraham	2015	Applied Intelligence	10.1007/s10489-015-0663-0	resource allocation	AI	-2.753495285484651	-1.5584424087197044	23384
4433550f4c1166e7d4c4e8607bf7705c6c5e3965	on the convergence time of the best response dynamics in player-specific congestion games	game theory;nash equilibrium;best response dynamics;upper bound;convergence time;congestion game	We study the convergence time of the best response dynamics in player-specific singleton congestion games. It is well known that this dynamics can cycle, although from every state a short sequence of best responses to a Nash equilibrium exists. Thus, the random best response dynamics, which selects the next player to play a best response uniformly at random, terminates in a Nash equilibrium with probability one. In this paper, we are interested in the expected number of best responses until the random best response dynamics terminates. As a first step towards this goal, we consider games in which each player can choose between only two resources. These games have a natural representation as (multi-)graphs by identifying nodes with resources and edges with players. For the class of games that can be represented as trees, we show that the best-response dynamics cannot cycle and that it terminates after O(n) steps where n denotes the number of resources. For the class of games represented as cycles, we show that the best response dynamics can cycle. However, we also show that the random best response dynamics terminates after O(n) steps in expectation. Additionally, we conjecture that in general player-specific singleton congestion games there exists no polynomial upper bound on the expected number of steps until the random best response dynamics terminates. We support our conjecture by presenting a family of games for which simulations indicate a super-polynomial convergence time. ∗Parts of the results presented here already appeared in the Proceedings of the 4th Symposium on Stochastic Algorithms, Foundations, and Applications (SAGA) in 2007 [1].	algorithm;cycle (graph theory);nash equilibrium;network congestion;polynomial;simulation	Heiner Ackermann;Heiko Röglin	2008	CoRR		game theory;mathematical optimization;simulation;best response;economics;mathematics;microeconomics;mathematical economics;upper and lower bounds;nash equilibrium	Theory	-4.311142282539086	1.0807149338946176	23387
7a05002e8edb4c28e40c57e987c792f97bef71aa	recurrent fuzzy network design using hybrid evolutionary learning algorithms	network design;fuzzy neural network;dynamic plant control;continuous stirred tank reactor control;fuzzy rules;continuous stirred tank reactor;particle swarm optimizer;particle swam optimization;recurrent fuzzy neural networks;takagi sugeno kang;genetic algorithm;genetic algorithms;structure parameter learning;evolutionary learning	This paper proposes a recurrent fuzzy network design using the hybridization of a multigroup genetic algorithm and particle swarm optimization (R-MGAPSO). The recurrent fuzzy network designed here is the Takagi–Sugeno–Kang (TSK)-type recurrent fuzzy network (TRFN), in which each fuzzy rule comprises spatial and temporal sub-rules. Both the number of fuzzy rules and the parameters in a TRFN are designed simultaneously by R-MGAPSO. In R-MGAPSO, the techniques of variable-length individuals and the local version of particle swarm optimization are incorporated into a genetic algorithm, where individuals with the same length constitute the same group, and there are multigroups in a population. Population evolution consists of three major operations: elite enhancement by particle swarm optimization, sub-rule alignment-based crossover, and mutation. To verify the performance of R-MGAPSO, dynamic plant and a continuous-stirred tank reactor controls are simulated. R-MGAPSO performance is also compared with genetic algorithms in these simulations. r 2006 Elsevier B.V. All rights reserved.	artificial neural network;crossover (genetic algorithm);fuzzy rule;genetic algorithm;heuristic;machine learning;mathematical optimization;mutation (genetic algorithm);network planning and design;particle swarm optimization;reactor (software);recurrent neural network;simulation;software release life cycle	Chia-Feng Juang;I-Fang Chung	2007	Neurocomputing	10.1016/j.neucom.2006.08.010	multi-swarm optimization;genetic algorithm;adaptive neuro fuzzy inference system;computer science;artificial intelligence;neuro-fuzzy;machine learning;artificial neural network	AI	13.39577910030721	-23.211168811474685	23388
515edbb3952f089e2d103c68da4f8e16f928fcfa	an extended topsis method for multiple attribute group decision making based on single valued neutrosophic linguistic numbers	distance measure;topsis method;group decision making;single valued neutrosophic linguistic set;single valued neutrosophic linguistic number	The paper proposes the concepts of a single valued neutrosophic linguistic set (SVNLS) and a single valued neutrosophic linguistic number (SVNLN) as a further generalization of the concepts of a linguistic variable, an intuitionistic linguistic set and an intuitionistic linguistic number (ILN). Then, we introduce the basic operational rules of SVNLNs and define a generalized distance measure between SVNLNs. Furthermore, we develop an extended TOPSIS method for a multiple attribute group decision making problem based on the SVNLNs under single valued neutrosophic linguistic environment. Finally, an illustrative example of investment alternatives and the comparative analysis are given to demonstrate the application and effectiveness of the developed approach.		Jun Ye	2015	Journal of Intelligent and Fuzzy Systems	10.3233/IFS-141295	discrete mathematics;group decision-making;artificial intelligence;data mining;mathematics	Robotics	-2.815503667558135	-21.08990463681916	23389
00bac9d0f4f9a03978e7cf4db9a0e823f34f11d1	"""can neural networks learn the """"head and shoulders"""" technical analysis price pattern? towards a methodology for testing the efficient market hypothesis"""	asset prices;neural networks;efficient market hypothesis;rule based system;trading strategy;random walk;head and shoulders price pattern;technical analysis;neural network	Testing the validity of the Efficient Market Hypothesis (EMH) has been an unsolved argument for the investment community. The EMH states that the current market price incorporates all the information available, which leads to a conclusion that given the information available, no prediction of the future price changes can be made. On the other hand, technical analysis, which is essentially the search for recurrent and predictable patterns in asset prices, attempts to forecast future price changes. To the extend that the total return of a technical trading strategy can be regarded as a measure of predictability, technical analysis can be seen as a test of the EMH and in particular of the independent increments version of random walk. This paper is an initial attempt on creating an automated process, based on a combination of a rule-based system and a neural network, of recognizing one of the most common and reliable patterns in technical analysis, the head and shoulders pattern. The systematic application of this automated process on the identification of the head and shoulders pattern and the subsequent analysis of price behavior, in various markets can in principle work as a test of the EMH.	neural networks	Achilleas Zapranis;Evi Samolada	2007		10.1007/978-3-540-74695-9_53	actuarial science;computer science;artificial intelligence;efficient-market hypothesis;trading strategy;machine learning;technical analysis;random walk;artificial neural network	Theory	4.675692149830864	-17.225230071219187	23393
cfce07087fa2ef1ed5c9b0f06d10b6ac18e41da9	algorithm research for capacity-constrained stochastic traffic assignment model	successive equilibrium;incremental load;stochastic network;capacity constrained;traffic flow assignment;system optimal	The paper presents incremental load algorithm and successive equilibrium algorithm to solve the stochastic capacity-constrained traffic assignment model on the base of combining the Fisk's logit assignment optimal theory and Wardrop's optimisation principals. A system optimal quantitative indicator based on minimum O-D average cost principal is developed to evaluate the traffic flow assignment result. The simulation results for a small traffic network show that more reasonable traffic assignment is achieved when using more batches and optimal batch ratios with incremental load algorithm, while, the overall result is superior utilising successive equilibrium algorithm.	algorithm	Yong Yang;Yu-Song Yan;Zuo-An Hu;Pan Xie	2014	IJICT	10.1504/IJICT.2014.063214	mathematical optimization;computer science	Metrics	9.734398419309576	-0.3716906623716701	23497
d617446e93b265eb2cd8de88d06ec95329d66f78	analysis of the day-of-the-week anomaly for the case of emerging stock market	experimental analysis;stock market;statistical significance;financial data;investment strategies;stock exchange;day of the week effect;statistical analysis;profitability;emerging stock market;artificial neural network;neural network	The aim of the article is to explore the day-of-the-week effect in emerging stock markets. This effect relates to the attempts to find statistically significant dependences of stock trading anomalies, which occur in particular days of the week (usually the first or the last trading day), and which could be important for creating profitable investment strategies. The main question of the research is to define, if this anomalies affects the entire market, or it is applicable only for the specific groups of stocks, which could be recognized by identifying particular features. The investigation of the day-of-the-week effect is performed by applying two methods: traditional statistical analysis and artificial neural networks. The experimental analysis is based on financial data of the Vilnius Stock Exchange, as of the case of emerging stock market with relatively low turnover and small number of players. Application of numerous tests and methods reveals better effectiveness of the artificial neural networks for indicating significance of day-of-the-week effect.	anomaly detection	Virgilijus Sakalauskas;Dalia Kriksciuniene	2007		10.1007/978-3-540-77002-2_31	stock exchange;investment strategy;computer science;statistical significance;restricted stock;artificial neural network;profitability index;experimental analysis of behavior;algorithmic trading	ECom	6.171576001364534	-18.68281111984334	23616
47bcfcc6759bd7421b843a3c5e4726ac6d2c24d7	a decision support framework for evaluating revenue performance in sequential purchase contexts		This paper studies the product ordering problem in sequential purchase contexts where sellers aim to maximize their revenue faced with buyers with budget constraints. We propose a multilayered decision support framework that combines empirical data with simulation, optimization, and econometric methods to address this problem. Our framework allows sellers to: i) compare revenue performances of limited information sequencing strategies, ii) quantify benchmark revenue levels that can be achieved via the optimal sequence based on detailed buyer information, iii) determine the costs of limited information and strategic buyers to the seller, and iv) identify the moderators of sequencing strategy performance. We illustrate our framework through two applications in a business-to-business used-car auction setting. Contrary to previous studies reporting practitioners’ tendency to sequence items from the lowest value to the highest, our results suggest that the best-performing limited information sequencing strategy depends on buyers’ bidding behavior. We also find that the revenue difference between the optimal sequence and a limited information sequencing strategy can be substantial. Our results show that a significant portion of this revenue difference is associated with the seller’s limited information on buyers’ budgets and product valuations. Our applications also provide various sensitivity analyses and develop new propositions on the moderators of the relationship between the seller’s revenue and sequencing strategies.	benchmark (computing);decision support system;mathematical optimization;performance;simulation	O. Cem Ozturk;Selçuk Karabati	2017	European Journal of Operational Research	10.1016/j.ejor.2017.06.029	operations management;valuation (finance);decision support system;bidding;revenue;economics	Web+IR	-0.27708283944967893	-7.15543276182122	23649
d24b1e7192b0f86f230d255be68ab1dbcae57790	predictable variation and profitable trading of us equities: a trading simulation using neural networks	stock market;neural networks;transaction cost;investment management;trading strategy;regression;risk adjustment;predictability;predictive regression;profitability;cumulant;neural network	A switching rule conditioned on out-of-sample one-step-ahead predictions of returns is used to establish investment positions in either stocks or Treasury bills. The economic significance of any discernible patterns of predictability is assessed by incorporating transaction costs in the simulated trading strategies. We find that ANN models produce switching signals that could have been exploited by investors in an out-of-sample context to achieve superior cumulative and risk-adjusted returns when compared to either regression or a simple buy-and-hold strategy in the market indices. The robustness of these results across a large number of stock market indices is encouraging.#R##N#Scope and purpose#R##N#A large body of evidence has accumulated suggesting that stock returns are predictable by means of publicly available information on a number of financial and macroeconomic variables with an important business cycle component. Previous research has, for the most part, relied on standard statistical techniques (e.g., regression analysis) with unduly restrictive assumptions presumed to hold in the underlying data-generating process. This paper reexamines the evidence regarding predictable variation in US stock returns using both artificial neural network (ANN) and regression, and compares simulated trading results obtained from ANN models with those obtained from regression.	artificial neural network;simulation	Luvai Motiwalla;Mahmoud Wahab	2000	Computers & OR	10.1016/S0305-0548(99)00148-3	investment management;transaction cost;regression;actuarial science;predictability;trading strategy;artificial neural network;profitability index;cumulant;algorithmic trading	ML	2.3639980630030286	-11.583776932287893	23702
170a5f4a3ea37a782b3edd9852217826ad2191ac	the price of anarchy in hypergraph coloring games		The price of anarchy was introduced to measure the loss incurred by a society of agents who take actions in a decentralized manner instead of through a central authority. Hypergraph coloring has traditionally been studied in the context of a central designer who chooses colors. In this paper we study the price of anarchy when the choice of color is delegated to each of the vertices which are assumed self-interested.	anarchy;color;graph coloring	Rann Smorodinsky;Shakhar Smorodinsky	2017	CoRR		hypergraph;mathematical economics;combinatorics;price of anarchy;mathematics	AI	-3.8902320881908383	0.8967978870376593	23710
3d326c5aa3c38b432a8c41df376f800872326611	importing the computational neuroscience toolbox into neuro-evolution-application to basal ganglia	basal ganglia;neural networks;building block;evolving neural networks;action selection;computational neuroscience;evolutionary algorithms;neuro evolution;evolutionary algorithm;spatial organization;similarity function;cognitive function;artificial neural network;neural network	"""Neuro-evolution and computational neuroscience are two scientific domains that produce surprisingly different artificial neural networks. Inspired by the """"toolbox"""" used by neuroscientists to create their models, this paper argues two main points: (1) neural maps (spatially-organized identical neurons) should be the building blocks to evolve neural networks able to perform cognitive functions and (2) well-identified modules of the brain for which there exists computational neuroscience models provide well-defined benchmarks for neuro-evolution.  To support these claims, a method to evolve networks of neural maps is introduced then applied to evolve neural networks with a similar functionality to basal ganglia in animals (i.e. action selection). Results show that: (1) the map-based encoding easily achieves this task while a direct encoding never solves it; (2) this encoding is independent of the size of maps and can therefore be used to evolve large and brain-like neural networks; (3) the failure of direct encoding to solve the task validates the relevance of action selection as a benchmark for neuro-evolution"""	action selection;artificial neural network;basal (phylogenetics);benchmark (computing);cognition;computation;computational neuroscience;evolution;ganglia;map;relevance	Jean-Baptiste Mouret;Stéphane Doncieux;Benoît Girard	2010		10.1145/1830483.1830592	winner-take-all;stochastic neural network;nervous system network models;cognition;action selection;types of artificial neural networks;computer science;artificial intelligence;machine learning;physical neural network;time delay neural network;deep learning;spatial organization;artificial neural network;systems neuroscience	ML	16.004399631296412	-23.629809680201355	23719
492d5eddee7ef244371dfd69449b3c405dbdc796	soft time-windows for a bi-objective vendor selection problem under a multi-sourcing strategy: binary-continuous differential evolution	soft time window;multi sourcing strategy;binary continuous differential evolution;vendor selection;multiple objective programming	This paper introduces a novel and practical integration of the inventory control and vendor selection problems for a manufacturing system that provides multiple products for several stores located in different places. The replenishment policy of each store is the economic order quantity under a multi-sourcing strategy in which the demand rate decreases as the selling price increases. In this strategy, the ordered quantity of each store for each product can be replenished by a set of selected vendors among all. In addition, the selected vendors can deliver the required products within a certain time window based on a soft time-window mechanism. The aim is to minimize the total system cost and delivery schedule violations, simultaneously. A trade-off between the two objectives is generated using the min-max approach to obtain near fair non-dominated solutions. As the problem is known to be NP-hard, a novel meta-heuristic algorithm called binary-continuous differential evolution (BCDE) is developed to make the original differential evolution capable of solving both binary and continuous optimization problems. Moreover, an improved genetic algorithm with a multi-parent crossover operator is designed to solve the problem. While the applicability of the proposed approach and the solution methodologies are demonstrated, the solution algorithms are tuned and their performances are analyzed and compared statistically. Finally, sensitivity analyses on the size of the soft time-window and the bandwidth factor of the BCDE algorithm are conducted. Display Omitted A bi-objective vendor selection problem with inventory-related decisions is modeled.A soft time-window mechanism and a multi-sourcing strategy are studied.The objectives are minimization of the total cost and delivery schedule violation.A trade-off between the proposed objectives is established.A binary-continuous differential evolution is developed.	differential evolution;microsoft windows;selection algorithm	Amir Hossein Niknamfar;Seyed Taghi Akhavan Niaki	2016	Computers & OR	10.1016/j.cor.2016.06.003	mathematical optimization	ML	18.030421329918408	-1.2745623563227606	23734
fc49f797547491af001192351bf7b68741170306	online double auction mechanism for perishable goods	perishable goods;market design;multi agent simulation;online double auction	"""We investigate mechanism design for a spot market of perishable goods.We explain that failures of trading in the perishable goods damage social utility.We develop an online double auction that prioritizes time-critical bids.Multiagent simulations show the auction realizes efficient and fair allocations. One-sided auctions are used for market clearing in the spot markets for perishable goods because production cost in spot markets is already """"sunk."""" Moreover, the promptness and simplicity of one-sided auctions are beneficial for trading in perishable goods. However, sellers cannot participate in the price-making process in these auctions. A standard double auction market collects bids from traders and matches the higher bids of buyers and lower bids of sellers to find the most efficient allocation, assuming that the value of unsold items remains unchanged. Nevertheless, in the market for perishable goods, sellers suffer a loss when they fail to sell their goods, because their salvage values are lost when the goods perish. To solve this problem, we investigate the suitable design of an online double auction for perishable goods, where bids arrive dynamically with their time limits. Our market mechanism aims at improving the profitability of traders by reducing trade failures in the face of uncertainty of incoming/departing bids. We develop a heuristic market mechanism with an allocation policy that prioritizes bids of traders based on their time-criticality, and evaluate its performance experimentally using multi-agent simulation. We find out that our market mechanism realizes efficient and fair allocations among traders with approximately truthful behavior in different market situations."""		Kazuo Miyashita	2014	Electronic Commerce Research and Applications	10.1016/j.elerap.2014.06.004	mechanism design;combinatorial auction;double auction;microeconomics;market economy;commerce;forward auction	ECom	-1.2756201887658447	-5.899939838822481	23746
9be6bb06f6d1aa3adc99795d8dc5cbc7f97ae156	a linear order and owa operator for discrete gradual real numbers		In this paper we introduce a class of linear orders for discrete gradual real numbers. Based on the linear orders we propose an OWA operator on the set of discrete gradual numbers and discuss some its properties. This is a first step of our intentions to introduce a class of linear orders, and consequently also OWA operator, for the set of discrete gradual intervals. Because the set of all fuzzy intervals is included in the set of all gradual intervals, proposed linear orders and OWA operators would be applicable in the settings of fuzzy intervals too.	discretization;interval arithmetic	Zdenko Taka	2015			real number;operator (computer programming);discrete mathematics;mathematical optimization;mathematics	AI	-0.605419888205843	-22.030184540612726	23810
91d0f9dffe1ee3b66555f6ee638ec6f2f4c5fd27	integration of correlations with standard deviations for determining attribute weights in multiple attribute decision making	calcul scientifique;numerical stability;economic benefit;integrated approach;49k40;computer aided analysis;correlacion;analisis sensibilidad;alternatives;matematicas aplicadas;analyse assistee;modele mathematique;mathematiques appliquees;objective weights;estabilidad numerica;standard deviation;58xx;global sensitivity analysis;prise de decision;alternative ranking;preference information;modelo matematico;multiple attribute decision making;integration;interval;computacion cientifica;general methods;sensitivity analysis;integracion;mathematical model;analyse sensibilite;analisis asistido;stabilite numerique;correlation;scientific computation;attribute weights;applied mathematics;correlation coefficient;toma decision;28xx;rank correlation;article;models;criteria	This paper proposes a correlation coefficient (CC) and standard deviation (SD) integrated approach for determining the weights of attributes in multiple attribute decision making (MADM) and a global sensitivity analysis to the weights determined. The CCSD integrated approach determines the weights of attributes by considering SD of each attribute and their CCs with the overall assessment of decision alternatives, where CCs are determined by removing each attribute from the overall assessment of decision alternatives. If the CC for an attribute turns out to be very high, then the removal of this attribute has little effect on decision making; otherwise, the attribute should be given an important weight. The global sensitivity analysis to the weights of attributes is proposed to ensure the stability of the best decision alternative or alternative ranking. A numerical example about the economic benefit assessment of the industrial economyof China is investigated to illustrate the potential applications of the CCSD method in determining the weights of attributes. Comparisons with existing weight generation methods are also discussed. © 2009 Elsevier Ltd. All rights reserved.	coefficient;industrial pc;numerical analysis	Ying-Ming Wang;Ying Luo	2010	Mathematical and Computer Modelling	10.1016/j.mcm.2009.07.016	interval;econometrics;variable and attribute;mathematical model;data mining;mathematics;standard deviation;correlation;sensitivity analysis;rank correlation;numerical stability;statistics	AI	-0.3911109037675045	-15.919884562320934	23831
7432f7a3272f418be7857f2a36ed6e383098649a	detection of unsolvable temporal planning problems through the use of landmarks	deadline constraint;constraints domain;temporal landmark;new approach;feasible solution;unsolvable temporal planning problem;last planning competition show	Deadline constraints have been recently introduced in PDDL3.0. The results obtained in the constraints domains in the last Planning Competition show that planners are not yet fully competitive. When dealing with deadline constraints the number of feasible solutions for a problem is reduced and thus it is specially relevant the ability to detect unsolvability. In this paper we present a new approach, based on the use of temporal landmarks, for the detection of unsolvable temporal planning problems.	landmark point	Eliseo Marzal;Laura Sebastia;Eva Onaindia	2008		10.3233/978-1-58603-891-5-919	mathematical optimization;artificial intelligence;algorithm	Robotics	21.08205024226218	4.020817387316216	23863
fca1be36eaf5f15ae3c5b4989e9c89dadc015c72	predicting optimal solution cost with conditional probabilities	68t20;type systems;heuristic search;optimal solution cost prediction;68w25;68w20	Heuristic search algorithms are designed to return an optimal path from a start state to a goal state. They find the optimal solution cost as a side effect. However, there are applications in which all one wants to know is an estimate of the optimal solution cost. The actual path from start to goal is not initially needed. For instance, one might be interested in quickly assessing the monetary cost of a project for bidding purposes. In such cases only the cost of executing the project is required. The actual construction plan could be formulated later, after bidding. In this paper we propose an algorithm, named Solution Cost Predictor (SCP), that accurately and efficiently predicts the optimal solution cost of a problem instance without finding the actual solution. While SCP can be viewed as a heuristic function, it differs from a heuristic conceptually in that: 1) SCP is not required to be fast enough to guide search algorithms; 2) SCP is not required to be admissible; 3) our measure of effectiveness is the prediction accuracy, which is in contrast to the solution quality and number of nodes expanded used to measure the effectiveness of heuristic functions. We show empirically that SCP makes accurate predictions on several heuristic search benchmarks.	benchmark (computing);finite-state machine;heuristic (computer science)	Levi H. S. Lelis;Roni Stern;Ariel Felner;Sandra Zilles;Robert C. Holte	2014	Annals of Mathematics and Artificial Intelligence	10.1007/s10472-014-9432-8	consistent heuristic;mathematical optimization;heuristic;computer science;artificial intelligence;machine learning;data mining	AI	18.874867117273045	-9.998274678458133	23939
4078997e98a0ffe84960f1152e68c11a93039342	a multiagent-based framework for self-adaptive software with search-based optimization	software;synthetic aperture sonar;software architecture;planning;optimization;search problems;context	Planning a suitable solution to adapt to software changes is the most important and fundamental ability of self-adaptive software (SAS). However, with the increasing complexities of managed resources, context and user preferences, existing self-adaptive planning approaches need to be improved to deal with the complex changes which are multiple, interrelated and evolving. Search-based optimization (SBO) is well-suited to deal with multiple and complex problems. Hence, using SBO as a new self-adaptive planning approach may be a particularly promising research trajectory. This paper proposes a multi-agent framework for SAS with SBO to deal with complex changes, reduce maintenance time and cost, and enhance software quality. This framework defines a special software architecture of SAS to choose different planning approaches, uses the SBO to plan solutions for complex changes, and supports the online planning by multi agents. In addition, a corresponding workbench is being established to develop SAS according to this framework.	agent-based model;algorithm;fitness function;mathematical optimization;multi-agent system;overhead (computing);parallel computing;program optimization;sap business one;sas;scheduling (computing);search-based software engineering;software architecture;software quality;user (computing);workbench	Lu Wang;Qingshan Li	2016	2016 IEEE International Conference on Software Maintenance and Evolution (ICSME)	10.1109/ICSME.2016.16	planning;software architecture;synthetic aperture sonar;simulation;systems engineering;engineering;software framework;software engineering;software construction;data mining	Robotics	20.044352347462294	-2.1386575347980057	23961
5bf8faa53a3c11d0f11de836df8f2554ec8f1df8	learning human-aware path planning with fully convolutional networks		This work presents an approach to learn path planning for robot social navigation by demonstration. We make use of Fully Convolutional Neural Networks (FCNs) to learn from expert's path demonstrations a map that marks a feasible path to the goal as a classification problem. The use of FCNs allows us to overcome the problem of manually designing/identifying the cost-map and relevant features for the task of robot navigation. The method makes use of optimal Rapidly-exploring Random Tree planner (RRT*) to overcome eventual errors in the path prediction; the FCNs prediction is used as cost-map and also to partially bias the sampling of the configuration space, leading the planner to behave similarly to the learned expert behavior. The approach is evaluated in experiments with real trajectories and compared with Inverse Reinforcement Learning algorithms that use RRT* as underlying planner.	algorithm;artificial neural network;benchmark (computing);convolutional neural network;experiment;motion planning;neural networks;rapidly-exploring random tree;reinforcement learning;robotic mapping;sampling (signal processing)	Noé Pérez-Higueras;Fernando Caballero;Luis Merino	2018	2018 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2018.8460851	configuration space;simulation;convolutional neural network;task analysis;reinforcement learning;motion planning;feature extraction;machine learning;random tree;engineering;social navigation;artificial intelligence	Robotics	20.25749807904099	-20.665975894304346	23985
372600200befb7abac36be4d33bba87b5bd8df69	neuro-wavelet parametric modeling	parametric modeling;base method;forecasting theory;wavelet transforms;neuro-wavelet parametric modeling;industrial application;neural-based technique;optimal neuro-wavelet network;modeling;neural nets;forecast signal;forecast signals;space technology;parametric statistics;predictive models;economic forecasting;parametric model;computer networks;neural network	This work describes Neuro-Wavelet Parametric Modeling, a neural-based technique to classify, model and forecast signals or problems which are functions of either time or space. The paper presents the base method and discusses on the selection of the optimal neuro-wavelet network. An industrial application is also presented.	solid modeling;wavelet	Valentina Colla;Mirko Sgarbi;Leonardo Maria Reyneri	2000		10.1109/IJCNN.2000.10007	econometrics;parametric model;systems modeling;computer science;machine learning;mathematics;artificial neural network;statistics;wavelet transform	PL	8.249744022333514	-20.718440323757697	23989
2811d437fddc83ed3ca34bcccd67278d55527e9f	assessment of railway frequency converter performance and data quality using the ieee 762 standard		Reliability, availability and maintainability analysis is one of the most important tools for measuring system performance. The performance of a traction power supply system (TPSS) can be measured using the data collected from frequency converters, as these converters constitute the main part of the TPSS. The quality of the collected data should be good enough to provide the correct and complete information necessary for assessment of frequency converter performance. Many methods can be used to assess the performance of converters such as neural networks, fuzzy logic and standards. The IEEE 762 Standard offers a methodology that can provide key performance indicators for power generation units. This standard has been chosen for its widespread acceptance and applicability. To be able to evaluate a converter’s performance, IEEE 762 indexes should be calculated using data such as the downtime, reserve shutdown hours and service hours. Therefore, the purpose of this study is to assess the performance of the Swedish TPSS frequency converters using IEEE 762, and to assess the quality of data by inspecting their compatibility with this standard. In this study, an application has been developed to generate the missing information and to calculate the indexes provided by the standard, in order to evaluate the power converters’ performance. A case with sample data is also discussed in this paper.	data quality;frequency changer	Mustafa Aljumaili;Yasser A. Mahmood;Ramin Karim	2014	Int. J. Systems Assurance Engineering and Management	10.1007/s13198-013-0216-z	reliability engineering;electronic engineering;engineering;operations management	DB	12.008457532208542	-12.712403779291781	24069
5a39599e6eaf1b2251a27e6ab1633f74be8b447f	optimal placement of pressure gauges for water distribution networks using entropy theory based on pressure dependent hydraulic simulation		This study proposed a pressure driven entropy method (PDEM) that determines a priority order of pressure gauge locations, which enables the impact of abnormal condition (e.g., pipe failures) to be quantitatively identified in water distribution networks (WDNs). The method developed utilizes the entropy method from information theory and pressure driven analysis (PDA), which is the latest hydraulic analysis method. The conventional hydraulic approach has problems in determining the locations of pressure gauges, attributable to unrealistic results under abnormal conditions (e.g., negative pressure). The proposed method was applied to two benchmark pipe networks and one real pipe network. The priority order for optimal locations was produced, and the result was compared to existing approach. The results of the conventional method show that the pressure reduction difference of each node became so excessive, which resulted in a distorted distribution. However, with the method developed, which considers the connectivity of a system and the influence among nodes based on PDA and entropy method results, pressure gauges can be more realistically and reasonably located.	benchmark (computing);information theory;personal digital assistant;simulation	Do Guen Yoo;Dong Eil Chang;Yang Ho Song;Jung Ho Lee	2018	Entropy	10.3390/e20080576	mathematical optimization;entropy (information theory);mathematics	Robotics	10.011234797854994	-6.907985788846233	24080
7569f3a35facab4d66aeea444884aab21aa14594	prediction model for warranty costs: a case study of a lcd monitor company	warranties;polynomial regression;financial management;data collection;liquid crystal displays;maintenance engineering;cost management prediction model warranty costs lcd monitor company oem odm business after sales service bill of material;predictive models warranties costs monitoring manufacturing industries bills of materials marketing and sales companies conference management electronic mail;companies;oem odm;liquid crystal displays financial management;monitoring;oem odm prediction model warranty cost polynomial regression;warranty cost;business;predictive models;prediction model;bill of material;cost management;spare parts;marketing and sales	In the OEM/ODM business, the total cost of a product and its after-sales service is always one of the top considerations for customers buying the product or service. The cost management on the bill of material (BOM) and warranty is therefore a very important task for gaining an OEM/ODM order. Based on the analysis of repair data collected over more than four years by an ODM company, this study generates a prediction model for the warranty service costs on LCD monitors. Once a prediction model is built, a company can apply it to figure out the volume prediction of warranty returns and to prepare the necessary spare parts and resources so as to maintain the warranty returns.	browser object model;oracle data mining;thin-film-transistor liquid-crystal display	Louis Y. Y. Lu;Chih-Chyi Chiang	2008	2008 IEEE Asia-Pacific Services Computing Conference	10.1109/APSCC.2008.56	maintenance engineering;computer science;predictive modelling;statistics	SE	6.439888196615083	-11.923526040230389	24083
53bf95f53b4cbad78e0b79e367558415c3390766	linear programming approach for solving fuzzy critical path problems with fuzzy parameters	lr flat fuzzy number;ranking function;fully fuzzy critical path problem	To the best of our knowledge, there is no method in the literature to find the fuzzy optimal solution of fully fuzzy critical path (FFCP) problems i.e., critical path problems in which all the parameters are represented by LR flat fuzzy numbers. In this paper, a new method is proposed for the same. Also, it is shown that it is better to use JMD representation of LR flat fuzzy numbers in the proposed method as compared to the other representation of LR flat fuzzy numbers. © 2014 Elsevier B.V. All rights reserved.	critical path method;lr parser;linear programming	Parmpreet Kaur;Amit Kumar	2014	Appl. Soft Comput.	10.1016/j.asoc.2014.03.017	fuzzy logic;mathematical optimization;combinatorics;discrete mathematics;membership function;defuzzification;adaptive neuro fuzzy inference system;fuzzy transportation;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;fuzzy number;neuro-fuzzy;fuzzy measure theory;mathematics;fuzzy associative matrix;fuzzy set operations	AI	-0.6840980611230784	-19.30062858634126	24142
9412e588187d2f942d7211ce5c1af6cd36493be3	developing a procedure to obtain knowledge of optimum solutions in a travelling salesman problem	travelling salesman problem	Travelling salesman problem (TSP) is an NP-hard optimization problem. So it is necessary to use intelligent and heuristic methods to solve such a hard problem in a less computational time. In this paper, a novel data mining-based approach is presented. The purpose of the proposed approach is to extract a number of rules from optimum tours of small TSPs. The obtained rules can be used for solving larger TSPs. Our proposed approach is mentioned in a standard data mining framework, called CRISP-DM. For rule extracting, generalized rule induction (GRI) as a powerful association rule mining algorithm is used. The results of this approach are stated as if-then rules. This approach is performed on two standard examples of TSPs. The obtained rules from these examples are compared, and it is shown that the rules form two examples have much similarity. This issue shows that it is possible to use from extracted rules to solve larger TSPs.	travelling salesman problem	Abdorrahman Haeri;Reza Tavakkoli-Moghaddam	2010		10.1007/978-3-642-14831-6_21	traveling purchaser problem;2-opt;mathematical optimization;christofides algorithm;computer science;lin–kernighan heuristic;travelling salesman problem;3-opt;bottleneck traveling salesman problem	EDA	23.397307843753374	0.47715637004279177	24152
2dc6f4aedfe7e57172c5d5f8b017eecc2b2681d6	explore no more: improved high-probability regret bounds for non-stochastic bandits		This work addresses the problem of regret minimization in non-stochastic multi-armed bandit problems, focusing on performance guarantees that hold with high probability. Such results are rather scarce in the literature since proving them requires a large deal of technical effort and significant modifications to the standard, more intuitive algorithms that come only with guarantees that hold on expectation. One of these modifications is forcing the learner to sample the losses of every arm at least Ω( √ T ) times over T rounds, which can adversely affect performance if many of the arms are obviously suboptimal. While it is widely conjectured that this property is essential for proving high-probability regret bounds, we show in this paper that it is possible to achieve such strong results without this undesirable exploration component. Our result relies on a simple and intuitive loss-estimation strategy called Implicit eXploration (IX) that allows a remarkably clean analysis. To demonstrate the flexibility of our technique, we derive several improved high-probability bounds for various extensions of the standard multi-armed bandit framework. Finally, we conduct a simple experiment that illustrates the robustness of our implicit exploration technique.	algorithm;coat of arms;experiment;multi-armed bandit;regret (decision theory);with high probability	Gergely Neu	2015			mathematical optimization;machine learning;mathematics;mathematical economics;statistics	ML	23.317132916208685	-17.450238591066864	24185
df3bb89bfb9457b89d25cd48cc11620344547f33	robust placement and tuning of upfc via a new multiobjective scheme-based fuzzy theory	fuzzy theory;voltage violation;multiobjective;upfc;moihbmo	This article addresses a new modified honey bee mating optimization namely multiobjective honey bee mating optimization (MOIHBMO) based fuzzy multiobjective methodology for optimal locating and parameter setting of unified power flow controller (UPFC) in a power system for a long-term period. One of the profits obtained by UPFC placement in a transmission network is the reduction in total generation cost due to its ability to change the power flow pattern in the network. Considering this potential, UPFC can be also used to remove or at least mitigate the congestion in transmission networks. The other issue in a power system is voltage violation which could even render the optimal power flow problem infeasible to be solved. Voltage violation could be also mitigated by proper application of UPFC in a transmission system. These objectives are considered simultaneously in a unified objective function for the proposed optimization algorithm. At first, these objectives are fuzzified and designed to be comparable against each other and then they are integrated and introduced to a MOIHBMO method to find the solution which maximizes the value of integrated objective function in a 3-year planning horizon, considering the load growth. A power injection model is adopted for UPFC. Unlike, the most previous works in this field the parameters of UPFC are set for each load level to avoid inconvenient rejection of more optimal solutions. IEEE reliability test system is used as an illustrative example to show the effectiveness of the proposed method. VC 2014 Wiley Periodicals, Inc. Complexity 000: 00–00, 2014	algorithm;flow network;fuzzy logic;john d. wiley;loss function;mathematical optimization;network congestion;optimization problem;rejection sampling	Hadi Aghazadeh;Masoud Bakhshi Germi;Behzad Esazadeh Khiav;Noradin Ghadimi	2015	Complexity	10.1002/cplx.21548	mathematical optimization;control theory	EDA	16.84334136675206	-3.498037756774577	24202
9d5f411629601ea990c763d005ec0f5c0e85e0e5	a knowledge-based approach to problem formulation for product model-based multidisciplinary design optimization in aec	energy efficient buildings;design automation;multidisciplinary design optimization mdo;building simulation;problem formulation;artificial intelligence;conceptual building design;knowledge based systems	The cost-effectiveness and accuracy of a multidisciplinary design optimization (MDO) process is highly dependent on designers’ ability to flexibly formulate the optimization problem for specific challenges. Designers need to rapidly modify how object parameters are assigned to groupings of objects in the product model. Our research has developed a Reference-Based Optimization Method (RBOM) to enable this type of flexible problem formulation. However, the responsibility still falls on the designer to manage the problem formulation and MDO process, which can lead to inefficient and costly design decisions. By means of artificial intelligence, in particular knowledge-based systems, these potential barriers to MDO adoption in the Architecture, Engineering, and Construction (AEC) industry could be mitigated, resulting in more efficient design processes and, ultimately, energy-efficient built environments.	artificial intelligence;echo suppression and cancellation;engineering design process;engineering informatics;genetic algorithm;knowledge-based systems;mathematical optimization;multidisciplinary design optimization;norm (social);optimization problem;partial element equivalent circuit;program optimization;programming paradigm;simulation;software design;spatial variability	Benjamin Ross Welle;John Riker Haymaker	2011			electronic design automation;computer science;artificial intelligence;knowledge-based systems	AI	13.684416909987073	-5.7111712430403045	24274
91c3b56c8a9a9cb229bdccd30aa23ded3e56dfb5	a telecommunications call volume forecasting system based on a recurrent fuzzy neural network	fuzzy neural nets;telecommunication congestion control fuzzy neural nets recurrent neural nets technological forecasting;telecommunication congestion control;predictive models forecasting computational modeling telecommunications recurrent neural networks market research neurons;internal feedback telecommunications call volume forecasting dynamic fuzzy neural model small block diagonal recurrent neural networks;recurrent neural nets;technological forecasting	The problem of telecommunications call volume forecasting is addressed to in this work. In particular, a foreacasting system is proposed, that is based on a dynamic fuzzy-neural model, where the consequent parts of the fuzzy rules are small Block-Diagonal Recurrent Neural Networks with internal feedback. The forecasting characteristics are highlighted and the prediction performance is evaluated by use of real-world telecommunications data. An extensive comparative analysis with a series of existing forecasters is conducting, including both traditional models as well as fuzzy and neurofuzzy approaches.	algorithm;analysis of algorithms;artificial neural network;block cipher mode of operation;computation;computational intelligence;fuzzy control system;fuzzy logic;neural networks;neuro-fuzzy;nonlinear system;qualitative comparative analysis;recurrent neural network;time series	Paris A. Mastorocostas;Constantinos S. Hilas;Dimitris N. Varsamis;Stergiani C. Dova	2013	The 2013 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2013.6707102	technology forecasting;artificial intelligence;neuro-fuzzy;machine learning;data mining;time delay neural network;statistics	ML	10.124232875259816	-22.538759427852007	24316
4fa396ead8a0dc74cc0bfd0e467b084a6c0ba152	group decision support under intuitionistic fuzzy relations: the role of weak transitivity and consistency			consistency model;vertex-transitive graph	Barbara Pekala;Eulalia Szmidt;Janusz Kacprzyk	2018	Int. J. Intell. Syst.	10.1002/int.21923		Web+IR	-0.30483720420722943	-19.82233264513049	24336
0654aa417f9c7c79f265fffca2380cb5014fa678	generalized knapsack solvers for multi-unit combinatorial auctions: analysis and application to computational resource allocation	high dimensionality;resource allocation;e commerce;utility function;operations research;knapsack problem;winner determination problem;combinatorial auction	The problem of allocating discrete computational resources motivates interest in general multi-unit combinatorial exchanges. This paper considers the problem of computing optimal (surplus-maximizing) allocations, assuming unrestricted quasi-linear preferences. We present a solver whose pseudo-polynomial time and memory requirements are linear in three of four natural measures of problem size: number of agents, length of bids, and units of each resource. In applications where the number of resource types is inherently a small constant, e.g., computational resource allocation, such a solver offers advantages over more elaborate approaches developed for high-dimensional problems.  We also describe the deep connection between auction winner determination problems and generalized knapsack problems, which has received remarkably little attention in the literature. This connection leads directly to pseudo-polynomial solvers, informs solver benchmarking by exploiting extensive research on hard knapsack problems, and allows E-Commerce research to leverage a large and mature body of literature.	analysis of algorithms;computation;computational resource;e-commerce payment system;institute for operations research and the management sciences;knapsack problem;polynomial;pseudo-polynomial time;requirement;solver;time complexity	Terence Kelly	2004		10.1007/11575726_6	continuous knapsack problem;mathematical optimization;computer science;generalized assignment problem;operations management;cutting stock problem;mathematical economics;knapsack problem	AI	-2.497866838948739	-0.5366988342534039	24352
85bdb70962147cd3586d99f50328025c82c9ac8e	neural-guided deductive search for real-time program synthesis from examples		Synthesizing user-intended programs from a small number of input-output examples is a challenging problem with several important applications like spreadsheet manipulation, data wrangling and code refactoring. Existing synthesis systems either completely rely on deductive logic techniques that are extensively handengineered or on purely statistical models that need massive amounts of data, and in general fail to provide real-time synthesis on challenging benchmarks. In this work, we propose Neural Guided Deductive Search (NGDS), a hybrid synthesis technique that combines the best of both symbolic logic techniques and statistical models. Thus, it produces programs that satisfy the provided specifications by construction and generalize well on unseen examples, similar to data-driven systems. Our technique effectively utilizes the deductive search framework to reduce the learning problem of the neural component to a simple supervised learning setup. Further, this allows us to both train on sparingly available real-world data and still leverage powerful recurrent neural network encoders. We demonstrate the effectiveness of our method by evaluating on real-world customer scenarios by synthesizing accurate programs with up to 12× speed-up compared to state-ofthe-art systems.	artificial neural network;code refactoring;encoder;program synthesis;real-time clock;real-time transcription;recurrent neural network;spreadsheet;statistical model;supervised learning	Ashwin J. Vijayakumar;Abhishek Mohta;Oleksandr Polozov;Dhruv Batra;Prateek Jain;Sumit Gulwani	2018	CoRR		supervised learning;deductive reasoning;program synthesis;artificial intelligence;machine learning;mathematical logic;code refactoring;deep learning;computer science;recurrent neural network;small number	ML	21.719309783545185	-22.61085872251071	24370
5068549db4ed7df8679d4e742f42ff6fdce99a6b	gaussian convex evidence theory for ordered and fuzzy evidence fusion				Yungang Zhu;Hongying Duan;Xinhua Wang;Baokui Zhou;Guodong Wang;Radu Grosu	2017	Journal of Intelligent and Fuzzy Systems	10.3233/JIFS-169333	mathematics;convex analysis;discrete mathematics;fuzzy logic;fusion;regular polygon;gaussian	NLP	1.001992134296746	-21.650382421717524	24384
25effc2bba6b8b79b6d98e56fab5d15de5d6d4f2	on the role of acyclicity in the study of rationality of fuzzy choice functions	acyclicity;fuzzy preference relations;fuzzy sets educational institutions context economics intelligent systems presses software agents;rationality fuzzy preference relations fuzzy choice functions acyclicity;fuzzy set theory;fuzzy preference relation;rational choice;t norm fuzzy choice functions automated negotiation systems fuzzy preference relations fuzzy acyclicity rational choice function;fuzzy set theory decision making;fuzzy choice functions;choice function;rationality;automated negotiation	Fuzzy choice functions obtained from preference relations have been recently used to develop automated negotiation systems. This contribution contains a theoretical study of coherence conditions in the process of creating a fuzzy choice function from a preference relation. In particular, the role of the acyclicity property of fuzzy preference relations is studied in the framework of rationality of fuzzy choice functions. Two different definitions of fuzzy acyclicity are compared. The two classical ways of constructing a fuzzy choice function from a given fuzzy preference relation are considered and properties such as acyclicity and completeness are proved to be sufficient conditions to ensure that the constructed function is a rational choice function. Special attention has been paid to the choice of the t-norm too. The results obtained are also compared to the classical results on rationality in the theory of crisp choice functions.	coherence (physics);fuzzy concept;fuzzy logic;fuzzy set;online marketplace;rationality;scott continuity;software agent;t-norm	Davide Martinetti;Bernard De Baets;Susana Díaz;Susana Montes	2011	2011 11th International Conference on Intelligent Systems Design and Applications	10.1109/ISDA.2011.6121680	fuzzy logic;discrete mathematics;choice function;membership function;defuzzification;rationality;type-2 fuzzy sets and systems;fuzzy classification;computer science;fuzzy subalgebra;fuzzy number;fuzzy measure theory;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;rational choice theory	Robotics	-1.5493895326752678	-22.40742831814555	24386
4c964f14da11a0446feb6fe7bccfdf35c954bb8f	on the robust application of stochastic optimisation technology for the synthesis of reaction/separation systems	system engineering;optimisation;stochastic algorithm;multiphase reactions;systems engineering;simulation framework;simulated annealing;process synthesis;process design;conceptual design;global optimization;tabu search;separations;state transition	Stochastic optimisation technology is presented for the synthesis of integrated reaction and separation process superstructures. The optimisation techniques enable the systematic exploration of process design options hidden in the network formulations. The implementation of two stochastic algorithms, simulated annealing and Tabu search, is presented in the context of the synthesis problem. The paper presents a design transition framework at the interface between the optimisation technology and the process synthesis representation as well as customisation aspects that enable an efficient search of the vast number of processing candidates generally comprising the solution space of the optimisation problem. Both stochastic algorithms are easily implemented as they utilise the same state transition and simulation framework. Asymptotic convergence to the globally optimal domain is observed for simulated annealing as well as Tabu search. In view of the numerical evidence obtained, Tabu search appears to arrive at highquality solutions on more efficient search paths as compared with simulated annealing. # 2002 Elsevier Science Ireland Ltd. All rights reserved.	algorithm;feasible region;mathematical optimization;maxima and minima;numerical analysis;personalization;simulated annealing;simulation;stochastic optimization;tabu search	Patrick Linke;Antonis C. Kokossis	2003	Computers & Chemical Engineering	10.1016/S0098-1354(02)00253-3	process design;mathematical optimization;simulation;simulated annealing;tabu search;computer science;engineering;hill climbing;machine learning;conceptual design;mathematics;global optimization	AI	12.934042510229409	-5.994168554475124	24448
3e7d259deceedf5674e3a7b14f0abc486e03ef3d	on the payoff mechanisms in peer-assisted services with multiple content providers: rationality and fairness	game theory;cost function;peer to peer network;profit sharing;incentive;informations och kommunikationsteknik;coalitional games;cost reduction;incentive coalition game peer to peer network shapley value;internet architecture;coalition game;information and communication technology;shapley value;content distribution;profitability;telekommunikation;telecommunications	"""This paper studies an incentive structure for cooperation and its stability in peer-assisted services when there exist multiple content providers, using a coalition game-theoretic approach. We first consider a generalized coalition structure consisting of multiple providers with many assisting peers, where peers assist providers to reduce the operational cost in content distribution. To distribute the profit from cost reduction to players (i.e, providers and peers), we then establish a generalized formula for individual payoffs when a """"Shapley-like"""" payoff mechanism is adopted. We show that the grand coalition is unstable, even when the operational cost functions are concave, which is in sharp contrast to the recently studied case of a single provider where the grand coalition is stable. We also show that irrespective of stability of the grand coalition, there always exist coalition structures that are not convergent to the grand coalition under a dynamic among coalition structures. Our results give us an incontestable fact that a provider does not tend to cooperatewith other providers in peer-assisted services and is separated from them. Three facets of the noncooperative (selfish) providers are illustrated: 1) underpaid peers; 2) service monopoly; and 3) oscillatory coalition structure. Lastly, we propose a stable payoff mechanism that improves fairness of profit sharing by regulating the selfishness of the players as well as grants the content providers a limited right of realistic bargaining. Our study opens many new questions such as realistic and efficient incentive structures and the tradeoffs between fairness and individual providers' competition in peer-assisted services."""	concave function;control theory;digital distribution;existential quantification;fairness measure;game theory;monopoly;rationality	Jeong-woo Cho;Yung Yi	2014	IEEE/ACM Transactions on Networking	10.1109/TNET.2013.2259637	game theory;information and communications technology;telecommunications;incentive;shapley value;profitability index	ECom	-3.3728714794471175	-5.163795996484761	24512
5af729e39ab9723d4983c04f1be55afaba14e05c	robust airline schedule design in a dynamic scheduling environment	airline schedule design;re timing;re fleeting;robust planning;dynamic scheduling	In the past decade, major airlines in the U.S. have moved from banked hub-and-spoke operations to de-banked hub-and-spoke operations to lower operating costs. In [1], it is shown that dynamic airline scheduling, an approach that makes minor adjustments to flight schedules in the booking period by re-fleeting and re-timing flight legs, can significantly improve utilization of capacity and hence increase profit. In this paper, we develop robust schedule design models and algorithms to generate schedules that facilitate the application of dynamic scheduling in de-banked hub-and-spoke operations. Such schedule design approaches are robust in the sense that the schedules produced can more easily be manipulated in response to demand variability when embedded in a dynamic scheduling environment. In our robust schedule design model, we maximize the number of potentially connecting itineraries weighted by their respective revenues. We provide two equivalent formulations of the robust schedule design model and develop a decomposition-based solution approach involving a variable reduction technique and a variant of column generation. We demonstrate, through experiments using data from a major U.S. airline, that the schedule generated can improve profitability when dynamic scheduling is applied. It is also observed that the greater the demand variability, the more profit our robust schedules achieve when compared to existing ones.		Hai Jiang;Cynthia Barnhart	2013	Computers & OR	10.1016/j.cor.2011.06.018	real-time computing;simulation;dynamic priority scheduling;computer science	Mobile	12.816219137720928	3.63491842841036	24524
5e6c221799370a5951e160b5b7266c5195209d27	distributed constraint optimization for addressing vessel rotation planning problems	vessel rotation planning;multi agent systems;distributed constraint satisfaction	A distributed constraint optimization problem (DCOP) is a description of constraint optimization problem where variables and constraints are distributed among a group of agents, and where each agent can only interact with agents that share constraints. Even though DCOPs have been studied since the 1990s, there are only a few attempts to address real world problems using this formalism, mainly because of the complexity of the solution algorithms. In this paper, we compare 4 state-of-the-art DCOP approaches to solve the vessel rotation planning problem (VRPP), which concerns deciding on the optimal sequence of vessel visits to different terminals in a large seaport. We hereby also consider two agent structures: a single layer and a multi-layer structure. For each of the structures, we compare the four different algorithms for solving DCOPs, aiming at studying how the algorithms perform in VRPPs of increasing sizes. We assess the methods based on the size and quantity of messages exchanged, computation time, and quality of solutions. & 2015 Elsevier Ltd. All rights reserved.	accessibility;algorithm;computation;computational complexity theory;constrained optimization;dcop;defense in depth (computing);distributed constraint optimization;jade;layer (electronics);mathematical optimization;multi-agent system;optimization problem;personally identifiable information;processor register;semantics (computer science);throughput;time complexity;utility	Shijie Li;Rudy R. Negenborn;Gabriël Lodewijks	2016	Eng. Appl. of AI	10.1016/j.engappai.2015.11.001	mathematical optimization;simulation;computer science;artificial intelligence;multi-agent system	AI	15.466710697676252	-0.4843845865723743	24551
b38ef3793a160c3a3ce859952516ab04013c653d	networked optimization for demand side management based on non-cooperative game	optimisation demand side management game theory newton method;newton method demand side management noncooperative game networked optimization method nash equilibrium nikaido isoda function;decision support systems	In this paper, demand side management problem is reformulated by the jointly constrained noncooperative game. The corresponding networked optimization method that concentrates on seeking generalized Nash Equilibrium for noncooperative game is developed for the problem. Due to the large scale of users in demand side management, the noncooperative game based demand side management is divided into groups of sub games, which can be efficiently solved by Nikaido-Isoda function based Newton method. Simulation results verify that the effectiveness of the designed algorithm.	algorithm;mathematical optimization;nash equilibrium;newton's method;simulation	Chaojie Li;Xinghuo Yu;Wenwu Yu;Tingwen Huang	2015	2015 IEEE 13th International Conference on Industrial Informatics (INDIN)	10.1109/INDIN.2015.7281899	simulation;economics;microeconomics;mathematical economics	Robotics	1.993436971479061	2.6012258767471557	24557
e740d2e3db785fdea8a47b5a4998c6e51bf4d30c	strategic customer behavior in a queueing system with delayed observations	queueing;strategic customers;balking;reneging;observable queue;unobservable queue;delayed observation;periodic announcements;60k25;90b22	We consider the single-server Markovian queue with infinite waiting space and assume that there exists a certain reward-cost structure that reflects the customers’ desire for service and their dislike for waiting. The system is unobservable for the customers at their arrival instants, but the administrator provides them with periodic announcements of their current positions at rate θ , so that they may renege if it is preferable for them to do so. The customers are strategic, and their decision problem is whether to join or not the system upon arrival and whether to stay or renege later. Their strategies are specified by a join probability q and a reneging threshold n. We determine the equilibrium strategies (ne, qe) and study the socially optimal strategies (nsoc, qsoc). Extensive numerical experiments provide interesting qualitative insight about the model. In particular, the equilibrium throughput of the system is a unimodal function of θ . Moreover, despite the fact that we have an avoid-the-crowd situation, it is possible that qsoc > qe, in contrast to the classical unobservable model.	decision problem;experiment;knowledge society;ne (complexity);nash equilibrium;numerical analysis;observable;purchasing;queueing theory;server (computing);system administrator;throughput	Apostolos Burnetas;Antonis Economou;George Vasiliadis	2017	Queueing Syst.	10.1007/s11134-017-9522-5	real-time computing;simulation;mathematics	Metrics	2.1652439044925753	0.348637520482528	24634
4cdb2d427df469ec363c086740ec6680af1a887b	optimising television commercial air-time by means of a genetic algorithm	genetic algorithm	This paper provides an outline of a project that used genetic algorithms to solve a problem in scheduling commercial air-time. The resultant software has been in commercial use for a number of years.		Stephen C. Ottner	2000			simulation;genetic algorithm;computer science;artificial intelligence;machine learning;operations research	NLP	18.305523934498932	-1.0235950101889044	24638
2980d616a2db8a75b55f55f27097fef3bee09ae3	optimization of replacement times using imperfect monitoring information	optimisation;preventive maintenance;stochastic process;stopping rule;system monitoring;indexing terms;imperfect information;failure analysis;condition monitoring;stochastic processes;parallel systems;parallel machines failure analysis fault diagnosis preventive maintenance stochastic processes optimisation system monitoring condition monitoring;time use;failure rate;decision process;parallel machines;numerical experiment;condition monitoring cost function preventive maintenance electric shock robustness stochastic processes stochastic systems stress event detection face detection;imperfect monitoring;cost model;replacement times optimization two unit parallel system maintenance decision process cost model imperfect monitoring information preventive maintenance policy observed failure rate stochastic processes;fault diagnosis	We propose a state-based PM policy based on a stopping rule for an imperfectly monitored two-unit parallel system consisting of s-dependent units. The observed failure rate of the system is proposed as an efficient tool to integrate the imperfect monitoring information in the maintenance decision process : the possible nondetection of the failure of a unit and the monitoring quality are explicitly taken into account when optimizing the maintenance decisions. Using classical martingale results, a stochastic cost model is developed to assess the performance of the monitoring-maintenance policy for this system. Numerical experiments show that the PM policy based on the observed failure rate of the system is more robust to defective monitoring information and that it performs better than a policy which discards completely this imperfect information.		Anne Barros;Christophe Bérenguer;Antoine Grall	2003	IEEE Trans. Reliability	10.1109/TR.2003.821944	reliability engineering;stochastic process;system monitoring;preventive maintenance;failure analysis;real-time computing;index term;engineering;perfect information;failure rate;mathematics;statistics	Embedded	6.625713480567685	-0.8665321741122288	24668
0f9104599e56735f82a8dc4cb9e9dab144952975	a second-order uncertainty model of independent random variables: an example of the stress-strength reliability	previsions;stress-strength reliability;second-order uncertainty;linear programming;imprecise probabilities;natural extension;second order;imprecise probability;reliability analysis;linear program	A second-order hierarchical uncertainty model of a system of independent random variables is studied in the paper. It is shown that the complex nonlinear optimization problem for reducing the second-order model to the firstorder one can be represented as a finite set of simple linear programming problems with a finite number of constraints. The stress-strength reliability analysis by unreliable information about statistical parameters of the stress and strength exemplifies the model. Numerical examples illustrate the proposed algorithm for computing the stress-strength reliability.	am broadcasting;algorithm;anton (computer);linear programming;mathematical optimization;nonlinear programming;nonlinear system;numerical method;optimization problem	Lev V. Utkin	2003			statistics;econometrics;uncertainty analysis;illustration of the central limit theorem;sum of normally distributed random variables;exchangeable random variables;algebra of random variables;sensitivity analysis;independent and identically distributed random variables;pairwise independence;mathematics	ML	0.01281512345052192	-18.66311178530029	24730
e81de6db8a2feda6a6d4739bbac25829c7ace834	some new ranking criteria in data envelopment analysis under uncertain environment		Data Envelopment Analysis (DEA) is a very effective method to evaluate the relative efficiency of decision-making units (DMUs), which has been applied extensively to education, hospital, finance, etc. However, in real-world situations, the data of production processes can not be precisely measured in some cases, which leads to the research of DEA in uncertain environments. This paper will give some researches to uncertain DEA based on uncertainty theory. Due to the uncertain inputs and outputs, we will give three uncertain DEA models, as well as three types of fully ranking Criteria. For each uncertain DEA model, its crisp equivalent model is presented to simplify the computation of uncertain models. Finally, a numerical example is presented to illustrate the three ranking criteria.	computation;data envelopment analysis;effective method;mathematical model;numerical analysis;uncertainty theory	Meilin Wen;Qingyuan Zhang;Rui Kang;Yi Yang	2017	Computers & Industrial Engineering	10.1016/j.cie.2017.05.034	efficiency;mathematical optimization;econometrics;uncertainty theory;effective method;data envelopment analysis;economics;ranking	SE	-3.597119850849211	-17.2413420909297	24732
5970ed52ee13472729ee7403085274554b5cba2e	static scheduling algorithms for allocating directed task graphs to multiprocessors	tratamiento paralelo;graph theory;outil logiciel;grafo aciclico;static scheduling;evaluation performance;software tool;teoria grafo;programacion entera;performance evaluation;traitement parallele;dag;gestion labor;multiprocessor;time complexity;multiprocessor systems;probleme np complet;evaluacion prestacion;heuristic method;multiprocessors;graphe acyclique;metodo heuristico;paralelisacion;spectrum;algoritmo genetico;theorie graphe;programmation en nombres entiers;acyclic graph;aleatorizacion;scheduling algorithm;complexite temps;branch and bound method;gestion tâche;integer programming;metodo branch and bound;optimal scheduling;herramienta controlada por logicial;scheduling;directed graph;parallelisation;graphe oriente;parallelization;algorithme genetique;randomisation;algorithme evolutionniste;genetic algorithm;ordonamiento;grafo orientado;problema np completo;algoritmo evolucionista;software tools;methode heuristique;methode separation et evaluation;task graphs;evolutionary algorithm;task scheduling;multiprocesador;complejidad tiempo;integer program;randomization;branch and bound;article;parallel processing;ordonnancement;automatic parallelization;np complete problem;multiprocesseur	Static scheduling of a program represented by a directed task graph on a multiprocessor system to minimize the program completion time is a well-known problem in parallel processing. Since finding an optimal schedule is an NP-complete problem in general, researchers have resorted to devising efficient heuristics. A plethora of heuristics have been proposed based on a wide spectrum of techniques, including branch-and-bound, integer-programming, searching, graph-theory, randomization, genetic algorithms, and evolutionary methods. The objective of this survey is to describe various scheduling algorithms and their functionalities in a contrasting fashion as well as examine their relative merits in terms of performance and time-complexity. Since these algorithms are based on diverse assumptions, they differ in their functionalities, and hence are difficult to describe in a unified context. We propose a taxonomy that classifies these algorithms into different categories. We consider 27 scheduling algorithms, with each algorithm explained through an easy-to-understand description followed by an illustrative example to demonstrate its operation. We also outline some of the novel and promising optimization approaches and current research trends in the area. Finally, we give an overview of the software tools that provide scheduling/mapping functionalities.	branch and bound;evolutionary algorithm;genetic algorithm;graph theory;heuristic (computer science);integer programming;mathematical optimization;multiprocessing;np-completeness;parallel computing;scheduling (computing);taxonomy (general);time complexity	Yu-Kwong Kwok;Ishfaq Ahmad	1999	ACM Comput. Surv.	10.1145/344588.344618	fair-share scheduling;parallel processing;parallel computing;integer programming;dynamic priority scheduling;computer science;graph theory;theoretical computer science;evolutionary algorithm;two-level scheduling;scheduling;algorithm	EDA	21.703337445484514	2.748453715836822	24752
1e8bedde78ef3ddc4a4841d9bff6accf577b1baa	a ga-based parameter design for single machine turning process with high-volume production	factorial design;simulation;tool life;product model;lead time;production process;single machine;production control;simple genetic algorithm;product cycle;process parameters;sensitivity analysis;turning process;genetic algorithm;production cost;product quality;parameter design;cost model;production economics;fitness function;discrete event simulation	This paper presents a Parameter Design (PD) approach that provides near-optimal settings to the process parameters of a single lathe machine with high-volume production. Optimized process parameters include both machining parameters (cutting speed, feed rate, and depth of cut) and production parameters (material order size and inventory safety stock and reorder point). In high-volume production, machining parameters have amplified impacts on the machine performance in terms of productivity (cycle time), reliability (tool life), and product quality (surface finish). In addition, production parameters become critical in high-volume production since they directly impact the overall order fulfillment (production makespan and delivery reliability). Hence, this paper extends the conventional per-part machining cost model into a per-order production cost model by consolidating the production economics of both machining parameters and production controls. Discrete Event Simulation (DES) is utilized to capture the stochastic and dynamic production attributes and to transfer the static machine PD model into a dynamic PD-DES production model. The model is also utilized to accumulate the per-order running cost over production time while incorporating the impacts of process variability in tool life, labor efficiency, machining conditions, order lead time, and demand rate. Using the PD-DES model as a dynamic fitness function, a Simple Genetic Algorithm (SGA) is developed and applied to a CNC lathe machine to determine near-optimal settings to both machining and production process parameters so that the overall per-order production cost is minimized. Results showed effective SGA convergence profile with relatively low number of search generations. Sensitivity analysis with SGA parameters is conducted to demonstrate the search's robustness. The benefits of the per-order cost model are illustrated by repeating the SGA solution using machine productivity as a fitness criterion. The new SGA solution resulted in a better productivity but at a higher per-order cost. Finally, the effectiveness of SGA search is illustrated by outperforming the solutions obtained from two-level and three-level full factorial designs.	software release life cycle	Raid Al-Aomar;Ala'a Al-Okaily	2006	Computers & Industrial Engineering	10.1016/j.cie.2006.02.003	factorial experiment;simulation;genetic algorithm;computer science;engineering;operations management;product lifecycle;discrete event simulation;industrial engineering;scheduling;fitness function;engineering drawing;sensitivity analysis	HCI	9.727797714399905	1.73256225334796	24771
5cbe1abd07b53a22fd7190e865d33706a9075cc3	modeling water rights allocation in the south saskatchewan river basin in canada	rivers water resources resource management minimax techniques project management environmental economics hydroelectric power generation design engineering systems engineering and theory cybernetics;water resource;rivers;priority based maximal multiperiod network flow programming;water right;multiple objective optimization;lexicographic minimax water shortage ratios;water supply;south saskatchewan river basin;lexicographic minimax water shortage ratios water rights allocation south saskatchewan river basin canada multiple objective optimization methods priority based maximal multiperiod network flow programming population growth economic expansion;canada;mathematical programming;it adoption;economic expansion;water supply mathematical programming rivers;population growth;multiple objective optimization methods;river basin;network flow;water demand;water rights allocation	Two multiple objective optimization methods for water rights allocation, the priority-based maximal multiperiod network flow programming (PMMNF) method and lexicographic minimax water shortage ratios (LMWSR) method are applied to the South Saskatchewan river basin in southern Alberta, Canada. Population growth and economic expansion are straining water resources in this basin. Irrigation, municipal and industrial uses, and hydropower generation are the major water demands competing for limited water resources. The case study shows that the PMMNF method usually produces large differences in the satisfaction ratios among uses in times of water shortages since it is based on the priority concept. On the other hand, the allocations by LMWSR lead to fairer distributed satisfaction ratios because it adopts the lexicographic minimax equity concept and thus has water shortages shared among all the water demands of various types of uses.	equity crowdfunding;flow network;lexicography;mathematical optimization;maximal set;minimax;multi-objective optimization	Lizhong Wang;Liping Fang;Keith W. Hipel	2006	2006 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2006.385142	flow network;drainage basin;economic expansion;population growth;water supply	Robotics	6.623974924114348	-8.275679646066562	24775
cd5c63ae97253eff8b63afea67ac1e154802e92e	a hybrid genetic algorithm for no-wait job shop scheduling problems	genetic operator;no wait job shop;np hard problem;scheduling;waiting time;genetic algorithm;job shop;profitability;job shop scheduling problem;heuristic algorithm;asymmetric traveling salesman problem;hybrid genetic algorithm	A no-wait job shop (NWJS) describes a situation where every job has its own processing sequence with the constraint that no waiting time is allowed between operations within any job. A NWJS problem with the objective of minimizing total completion time is a NP-hard problem and this paper proposes a hybrid genetic algorithm (HGA) to solve this complex problem. A genetic operation is defined by cutting out a section of genes from a chromosome and treated as a subproblem. This subproblem is then transformed into an asymmetric traveling salesman problem (ATSP) and solved with a heuristic algorithm. Subsequently, this section with new sequence is put back to replace the original section of chromosome. The incorporation of this problem-specific genetic operator is responsible for the hybrid adjective. By doing so, the course of the search of the proposed genetic algorithm is set to more profitable regions in the solution space. The experimental results show that this hybrid genetic algorithm can accelerate the convergence and improve solution quality as well. 2008 Elsevier Ltd. All rights reserved.	feasible region;genetic algorithm;genetic operator;hercules graphics card;heuristic (computer science);job shop scheduling;memetic algorithm;np-hardness;scheduling (computing);travelling salesman problem	Jason Chao-Hsien Pan;Han-Chiang Huang	2009	Expert Syst. Appl.	10.1016/j.eswa.2008.07.005	heuristic;job shop scheduling;mathematical optimization;genetic algorithm;flow shop scheduling;computer science;artificial intelligence;genetic operator;machine learning;np-hard;scheduling;algorithm;profitability index	AI	18.75596854108485	1.2903961286852348	24783
d409d5ec2c4cbb381e4a46a43a559109c79c478f	the navigation process of mould-manufacturing scheduling optimisation by applying genetic algorithm	optimisation;ga;mould manufacturing;planning scheduling	The navigation process of mould-manufacturing scheduling optimisation by applying genetic algorithm Wen-Ren Jong & Po-Jung Lai To cite this article: Wen-Ren Jong & Po-Jung Lai (2015) The navigation process of mouldmanufacturing scheduling optimisation by applying genetic algorithm, International Journal of Computer Integrated Manufacturing, 28:12, 1331-1349, DOI: 10.1080/0951192X.2014.972461 To link to this article: http://dx.doi.org/10.1080/0951192X.2014.972461	computer-integrated manufacturing;genetic algorithm;mathematical optimization;scheduling (computing)	Wen-Ren Jong;Po-Jung Lai	2015	Int. J. Computer Integrated Manufacturing	10.1080/0951192X.2014.972461	fair-share scheduling;nurse scheduling problem;real-time computing;flow shop scheduling;dynamic priority scheduling;engineering;rate-monotonic scheduling;operations management;genetic algorithm scheduling;two-level scheduling;scheduling;engineering drawing	Robotics	19.06973206616314	0.10281528638428018	24804
39b70c28efa9031cad6eb109347db471b8f69ed4	online solar radiation forecasting under asymmetrie cost functions	forecasting;cost function;cost function forecasting barium equations mathematical model cities and towns solar radiation;barium;sunlight least squares approximations load forecasting;tracking ability online solar radiation forecasting asymmetrie cost functions linlin cost functions grid operator problem least mean squares algorithm lms algorithm linex cost functions;mathematical model;solar radiation;cities and towns	Grid operators are tasked to balance the electric grid such that generation equals load. In recent years renewable energy sources have become more popular. Because wind and solar power are intermittent, system operators must predict renewable generation and allocate some operating reserves to mitigate errors. If they overestimate the renewable generation during scheduling, they do not have enough generation available during operation, which can be very costly. On the other hand, if they underestimate the renewable generation, they face only the cost of keeping some generation capacity online and idle. So overestimation of resources create a more serious problem than underestimation. However, many researchers who study the solar radiation forecasting problem evaluate their methods using symmetric criteria like root mean square error (RMSE) or mean absolute error (MAE). In this paper, we use LinLin and LinEx which are asymmetric cost functions that are better fitted to the grid operator problem. We modify the least mean squares (LMS) algorithm according to LinLin and LinEx cost functions to create an online forecasting method. Due to tracking ability, the online methods gives better performance than their corresponding batch methods which is confirmed using simulation results.	algorithm;approximation error;business architecture;learning rule;least mean squares filter;loss function;mean squared error;scheduling (computing);simulation;sysop;gnulinex	Seyyed A. Fatemi;Anthony Kuh;Matthias Fripp	2014	Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2014 Asia-Pacific	10.1109/APSIPA.2014.7041755	mathematical optimization;simulation;engineering;operations management	HPC	10.62751773784421	-17.729799308163187	24818
bb9c3fe620451abdc52ad47a3dec7083ce77b987	two-period supply chain with flexible trade credit contract	trade credit;optimal order quantity;discounted wholesale price;two period supply chain;flexible early payment	We consider a fully flexible trade credit contract in a two-period supply chain.We design an incentive of a discounted wholesale price for the early payment.We propose a continuous newsvendor model to analyze the optimal decisions. This paper studies a two-period supply chain that consists of a retailer and a supplier. A newsvendor-like retailer is capital constrained and orders products using a supplier's trade credits to satisfy uncertain market demand. Most existing studies show that the retailer always postpones payment until the due date. To recall the loans earlier, we present a case in which the supplier, as a Stackelberg leader, offers an incentive of a discounted wholesale price in the second order to entice the retailer to choose flexible early payment. The proposed incentive is related to the retailer's early payment time in the first period. In the presence of bankruptcy risks for both the retailer and supplier, we propose a continuous newsvendor model of a two-period supply chain to analyze the decisions involved in the flexible trade credit contract. The analytic forms confirm that such an incentive can improve the decentralized supply chain efficiency and decreases the supplier's trade credit risk. The retailer always prefers early payment to payment around the due date to increase revenues. Furthermore, the action of paying early might help the retailer adjust cash flow between the two periods. We also find that a revenue sharing contract significantly affects the retailer's payment behavior and supplier's wholesale price. The numerical simulations support our results.	supply chain attack	Honglin Yang;Wenyan Zhuo;Yong Zha;Hong Wan	2016	Expert Syst. Appl.	10.1016/j.eswa.2016.08.056	economic order quantity	ECom	-0.9719839081430035	-5.760955430738073	24829
8447a3cc3b87928db86ae81c9618bcbc1233023f	gridmd: program architecture for distributed molecular simulation	simulation ordinateur;distributed system;molecular simulation;algoritmo paralelo;systeme reparti;parallel algorithm;execution time;distributed computing;molecular dynamics;portability;algorithme parallele;dynamique moleculaire;grid;sistema repartido;atomistic simulation;rejilla;portabilite;algorithme reparti;grille;calculo repartido;temps execution;algoritmo repartido;simulacion computadora;dinamica molecular;tiempo ejecucion;distributed algorithm;computer simulation;calcul reparti;portabilidad;free software	In the present work we describe architectural concepts of the distributed molecular simulation package GridMD. The main purpose of this work is to underline the construction patterns which may help to generalize the design of an application for extensive atomistic simulations. The issues such as design-time parallel execution implication, flexibility and extensions, portability to Grid environments and maximal adaptation of existing third-party codes and resources are addressed. The library is being currently developed, with gradually growing number of available components and tools. The basic GridMD engine is a free software and is distributed under the terms of wxWidgets library license [1]. 1 Motivation and Strategy The main subject of atomistic simulations is to study the microscopic behavior of a system of particles and to deduce physically important quantities from the microscopic model. Function of potential energy depending on particle coordinates may be taken from physical models of different kinds: classical, semi-empirical (with large number of parameters to be fitted for any specific system) or ab-initio (with much less number of fitting parameters and more generality). Having the potential defined, the next step is to extract system properties from it either by solving equations of motion (Molecular Dynamics) or by sampling the ensembles of phase space configurations selected by some criteria (Monte Carlo methods), or by searching the suitable configuration in phase space (transition state search, geometry optimization, ligand design, etc.). Sometimes the methods of exploring the system are combined in complicated numerical experiments. The most popular experiments in MD and MC have however relatively simple scenarios: take the system in some initial state and propagate it through the chain of other states by Newton equations solution or temperature-conditioned random process. Physically most important part of the model is the definition of the interaction potential. Looking at the simulation from the higher level as a tool to obtain physically significant results, researchers face the problem of process and data management which they must solve spending much effort on developing complex codes. Statistical averaging and variation of experiment parameters are always necessary to produce reliable data from numerical simulation [2]. Another fact is large simulation times for complex problems and the M. Hobbs, A. Goscinski, and W. Zhou. (Eds.): ICA3PP 2005, LNCS 3719, pp. 309–314, 2005. c © Springer-Verlag Berlin Heidelberg 2005	code;computer simulation;energy minimization;equation solving;experiment;hobbs meter;in-phase and quadrature components;lecture notes in computer science;mathematical optimization;maximal set;molecular dynamics;monte carlo method;newton;newton's method;numerical analysis;potential energy surface;sampling (signal processing);semiconductor industry;software architecture;software portability;springer (tank);stochastic process;wxwidgets	Ilya Valuev	2005		10.1007/11564621_35	computer simulation;distributed algorithm;molecular dynamics;parallel computing;simulation;computer science;theoretical computer science;distributed computing;parallel algorithm;grid	HPC	15.989953201760239	-10.332865924525837	24830
f6bc4e293f4ed101b66a7b0548325eee81781ae6	efficient matching of offers and requests in social-aware ridesharing		Ridesharing has been becoming increasingly popular in urban areas worldwide for its low cost and environmental friendliness. Much research attention has been drawn to the optimization of travel costs in shared rides. However, other important factors in ridesharing, such as the social comfort and trust issues, have not been fully considered in the existing works. In this paper, we formulate a new problem, named Assignment of Requests to Offers (ARO), that aims to maximize the number of served riders while satisfying the social comfort constraints as well as spatial-temporal constraints. We prove that the ARO problem is NP-hard. We then propose an exact algorithm for a simplified ARO problem. We further propose three pruning strategies to efficiently narrow down the searching space and speed up the assignment processing. Based on these pruning strategies, we develop two novel heuristic algorithms, the request-oriented approach and offer-oriented approach, to tackle the ARO problem. Through extensive experiments, we demonstrate the efficiency and effectiveness of our proposed approaches on real-world datasets.	approximation algorithm;exact algorithm;experiment;heuristic;linear programming;mathematical optimization;np-hardness;requests	Xiaoyi Fu;Ce Zhang;Hua Lu;Jianliang Xu	2018	2018 19th IEEE International Conference on Mobile Data Management (MDM)	10.1109/MDM.2018.00037	distributed computing;computer science;speedup;heuristic;schedule;exact algorithm	DB	16.922929851590265	0.031180336378167316	24831
65cfab80da8c3e243d97e71133fc39d4cfc56e10	knowledge transfer between automated planners		transfer problem-solving experience from previous tasks into the new task. Recently, the artificial intelligence community has attempted to model this transfer in an effort to improve learning on new tasks by using knowledge from related tasks. For example, classification and inference algorithms have been extended to support transfer of conceptual knowledge (for a survey see Torrey and Shavlik [2009]). Likewise, reinforcement learning has also been extended to support transfer (for a survey see Taylor and Stone [2009]; Torrey and Shavlik [2009]). This article presents an attempt to transfer structured knowledge in the framework of automated planning. Automated planning is the branch of artificial intelligence that studies the computational synthesis of ordered sets of actions that perform a given task (Ghallab, Nau, and Traverso 2004). A planner receives as input a collection of actions (that indicate how to modify the current state), a collection of goals to achieve, and a state. It then outputs a sequence of actions that achieve the goals from the initial state. Given that each action transforms the current state, planners may be viewed as searching for paths through the state space defined by the given actions. However, the search spaces can quickly become intractably large, such that the general problem of automated planning is PSpace-complete (Bylander 1994). The most common approach to coping with planning complexity involves defining heuristics that let the planner traverse the search space more efficiently. Current state-of-the-art planners use powerful domain-independent heuristics (Ghallab, Nau, and Traverso 2004; Nau 2007). These are not always sufficient, however, so an important research direction consists of defining manually or learning automatically domain-dependent heuristics (called control knowledge). In the latter case, Articles	algorithm;artificial intelligence;automated planning and scheduling;computation;computational complexity theory;heuristic (computer science);pspace-complete;problem solving;reinforcement learning;state space;traverse	Susana Fernández;Ricardo Aler;Daniel Borrajo	2011	AI Magazine		planning;simulation;computer science;artificial intelligence;operations research;search algorithm	AI	19.17765104092022	-13.68401763513135	24848
cd45e518b04b076b31d2762dac683898abf8e1d1	on forecasting amazon ec2 spot prices using time-series decomposition with hybrid look-backs		Abstract-In this paper, we study forecasting through time series decomposition to predict Amazon Elastic Compute Cloud (EC2) Spot prices. To achieve this, we first decompose the Spot price history into time series components; each component, which can exhibit deterministic or non-deterministic qualities, is then separately forecast using different standard forecasting techniques and look back periods; and finally, the individual forecasts are aggregated to form the Spot price forecast. We compare our approach with several standard forecasting methods, namely Na¨ıve, Seasonal Na¨ıve, ARIMA, ETS, STL, and TBATS. From experimental results we make two observations: (i) none of the evaluated forecasting techniques yields consistently good results across all Spot markets, even though Seasonal Na¨ıve can be considered the most robust when applied to markets with strong seasonal components, and (ii) our proposed technique performs comparably with, and in some cases, outperforms, the state-of-the-art forecasting techniques. The latter observation suggests that time series decompositionbased forecasting with hybrid look-backs warrants further investigation.	amazon elastic compute cloud (ec2);autoregressive integrated moving average;dtime;enterprise test software;ntime;non-deterministic turing machine;stl (file format);time series;tropical cyclone track forecasting	Mohan Baruwal Chhetri;Markus Lumpe;Quoc Bao Vo;Ryszard Kowalczyk	2017	2017 IEEE International Conference on Edge Computing (EDGE)	10.1109/IEEE.EDGE.2017.29	econometrics;time series;market research;elasticity (economics);cloud computing;financial economics;decomposition of time series;autoregressive integrated moving average;probabilistic forecasting;economics;spot contract	Robotics	5.170951652658046	-14.829583220800252	24869
b7cd0d976362268024ce70afd875094aaf1729c1	duality vs adjunction and general form for fuzzy mathematical morphology	fuzzy mathematical morphology	We establish in this paper the link between the two main approaches for fuzzy mathematical morphology, based on duality with respect to complementation and on the adjunction property, respectively. We also prove that the corresponding definitions of fuzzy dilation and erosion are the most general ones if a set of classical properties is required.	dilation (morphology);erosion (morphology);fuzzy logic;mathematical morphology	Isabelle Bloch	2005		10.1007/11676935_44	fuzzy mathematics;computer science;fuzzy number	Logic	0.06634967282800071	-23.89227759372917	24920
7cc12c0a6fe6565228e359f7f220d3b7995f4811	simpl: a system for integrating optimization techniques	constraint programming	In recent years, the Constraint Programming (CP) and Operations Research (OR) communities have explored the advantages of combining CP and OR techniques to formulate and solve combinatorial optimization problems. These advantages include a more versatile modeling framework and the ability to combine complementary strengths of the two solution technologies. This research has reached a stage at which further development would benefit from a general-purpose modeling and solution system. We introduce here a system for integrated modeling and solution called SIMPL. Our approach is to view CP and OR techniques as special cases of a single method rather than as separate methods to be combined. This overarching method consists of an infer-relax-restrict cycle in which CP and OR techniques may interact at any stage. We describe the main features of SIMPL and illustrate its usage with examples.	combinatorial optimization;computer programming;constraint programming;existential quantification;general-purpose modeling;hybrid algorithm;mathematical optimization;natural language processing;operations research;relax ng;simpl;solver;usability;vocabulary	Ionut D. Aron;John N. Hooker;Tallys H. Yunes	2004		10.1007/978-3-540-24664-0_2	mathematical optimization;constraint programming;constraint satisfaction;combinatorial optimization;computer science;artificial intelligence;mathematics;algorithm	AI	23.363253764768547	2.707708170756931	24954
88b39cbca160254204fa868172730b7308776d6b	operations policy for a supply chain system with fixed-interval delivery and linear demand	juste a temps;linear demand;modelizacion;forecasting;reliability;non linear programming;ciclo desarrollo;project management;entrega;programacion entera;information systems;delivery system;matrice intervalle;logistique;life cycle;estrategia optima;materia prima;raw materials;piecewise linear techniques;maintenance;matiere premiere;programacion no lineal;soft or;information technology;modele lineaire;packing;temps minimal;programmation non lineaire;modelo lineal;operations research;location;investment;programmation en nombres entiers;journal;horizonte finito;journal of the operational research society;technique lineaire par morceau;inventory;supply chain system;purchasing;modelisation;optimal strategy;livraison;planificacion;history of or;matriz intervalo;dependance du temps;logistics;time dependence;integer programming;tamano lote;horizon fini;linearisation morceau;marketing;taille lot;scheduling;linear model;interval matrix;cycle developpement;minimum time;linearizacion trozo;lot sizing;production;finite horizon;coaccion capacidad;communications technology;just in time delivery;planning;supply chain;just in time;contrainte capacite;justo en tiempo;computer science;sistema administracion;delivery good;operational research;planification;capacity constraint;fixed interval;tiempo minimo;modeling;systeme administration;dependencia del tiempo;strategie optimale;piecewise linearization;applications of operational research;or society;jors;management science;infrastructure;logistica	This research addresses a production-supply problem for a supply-chain system with fixed-interval delivery. A strategy that determines the optimal batch sizes, cycle times, numbers of orders of raw materials, and production start times is prescribed to minimize the total costs for a given finite planning horizon. The external demands are time-dependent following a life-cycle pattern and the shipment quantities follow the demand pattern. The shipment quantities to buyers follow various phases of the demand pattern in the planning horizon where demand is represented by piecewise linear model. The problem is formulated as an integer, non-linear programming problem. The model also incorporates the constraint of inventory capacity. The problem is represented using the network model where an optimal characteristic has been analysed. To obtain an optimal solution with N shipments in a planning horizon, an algorithm is proposed that runs with the complexity of Y(N) for problems with a single-phase demand and O(N) for problems with multi-phase demand. Journal of the Operational Research Society (2007) 58, 901–910. doi:10.1057/palgrave.jors.2602199 Published online 7 June 2006	algorithm;linear model;linear programming;network model;nonlinear programming;nonlinear system;piecewise linear continuation	Ahmad Diponegoro;Bhaba R. Sarker	2007	JORS	10.1057/palgrave.jors.2602199	project management;simulation;integer programming;economics;marketing;operations management;mathematics;supply chain;operations research;information technology	Robotics	5.770867423382283	-3.0871668502249596	25004
2f5a4a8013b585c03333313ef2e46d676a2892e4	compute cloud based weather detection and warning system	meterology;nowcasting meterology scientific computing cloud computing;weather forecasting;clouds meteorology meteorological radar computer architecture radar detection servers;weather detection algorithms cloud based weather detection warning system pay as you use model wastes resources commercial cloud services network capability real time operation;geophysics computing;scientific computing;atmospheric techniques;nowcasting;cloud computing;weather forecasting atmospheric techniques cloud computing geophysics computing	Compute cloud platforms pay-as-you-use model suits applications which require resources sporadically. Severe weather detection and prediction is one such application. Since severe weather events are rare, dedicating servers for such application wastes resources. In this paper, we present the feasibility of using commercial cloud services for severe weather detection and prediction. We show that commercial cloud services provide the required network capability to perform the real-time operation of weather detection and prediction from the radars to the cloud service instance. We automate the process of weather prediction on the cloud based on the results of our weather detection algorithms.	algorithm;cloud computing;radar;real-time clock	Dilip Kumar Krishnappa;Eric Lyons;David Emory Irwin;Michael Zink	2012	2012 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2012.6351000	meteorology;atmospheric sciences;weather forecasting;cloud computing;computer science;physics;weather research and forecasting model;remote sensing	Embedded	15.495604129696527	-11.627858372418789	25030
ca3cdb6c67cfe75dbf1e55cbf9f9a6eec9951964	road maintenance optimization through a discrete-time semi-markov decision process	road maintenance;asset management;semi markov process;optimization;infrastructure	Optimization models are necessary for efficient and cost-effective maintenance of a road network. In this regard, road deterioration is commonly modeled as a discrete-time Markov process such that an optimal maintenance policy can be obtained based on the Markov decision process, or as a renewal process such that an optimal maintenance policy can be obtained based on the renewal theory. However, the discrete-time Markov process cannot capture the real time at which the state transits while the renewal process considers only one state and one maintenance action. In this paper, road deterioration is modeled as a semi-Markov process in which the state transition has the Markov property and the holding time in each state is assumed to follow a discrete Weibull distribution. Based on this semi-Markov process, linear programming models are formulated for both infinite and finite planning horizons in order to derive optimal maintenance policies to minimize the life-cycle cost of a road network. A hypothetical road network is used to illustrate the application of the proposed optimization models. The results indicate that these linear programming models are practical for the maintenance of a road network having a large number of road segments and that they are convenient to incorporate various constraints on the decision process, for example, performance requirements and available budgets. Although the optimal maintenance policies obtained for the road network are randomized stationary policies, the extent of this randomness in decision making is limited. The maintenance actions are deterministic for most states and the randomness in selecting actions occurs only for a few states. & 2012 Elsevier Ltd. All rights reserved.	linear programming;markov chain;markov decision process;markov property;mathematical optimization;optimal maintenance;program optimization;randomized algorithm;randomness;requirement;semiconductor industry;state transition table;stationary process	Xueqing Zhang;Hui Gao	2012	Rel. Eng. & Sys. Safety	10.1016/j.ress.2012.03.011	reliability engineering;partially observable markov decision process;engineering;mathematics;transport engineering	AI	7.218828365796169	-1.2655777314392584	25057
41bea31f78f96117f43efa5037b677ba1f8a1760	malicious bayesian congestion games	liverpool;game theory;repository;computational complexity;bayesian nash equilibrium;university;congestion game	In this paper, we introduce malicious Bayesian congestion games as an extension to congestion games where players might act in a malicious way. In such a game each player has two types. Either the player is a rational player seeking to minimize her own delay, or – with a certain probability – the player is malicious in which case her only goal is to disturb the other players as much as possible. We show that such games do in general not possess a Bayesian Nash equilibrium in pure strategies (i.e. a pure Bayesian Nash equilibrium). Moreover, given a game, we show that it is NP-complete to decide whether it admits a pure Bayesian Nash equilibrium. This result even holds when resource latency functions are linear, each player is malicious with the same probability, and all strategy sets consist of singleton sets of resources. For a slightly more restricted class of malicious Bayesian congestion games, we provide easy checkable properties that are necessary and sufficient for the existence of a pure Bayesian Nash equilibrium. In the second part of the paper we study the impact of the malicious types on the overall performance of the system (i.e. the social cost). To measure this impact, we use the Price of Malice. We provide (tight) bounds on the Price of Malice for an interesting class of malicious Bayesian congestion games. Moreover, we show that for certain congestion games the advent of malicious types can also be beneficial to the system in the sense that the social cost of the worst case equilibrium decreases. We provide a tight bound on the maximum factor by which this happens.	bayesian network;best, worst and average case;malware;np-completeness;nash equilibrium;network congestion	Martin Gairing	2008		10.1007/978-3-540-93980-1_10	price of stability;bayesian game;game theory;epsilon-equilibrium;simulation;best response;sequential equilibrium;economics;repeated game;correlated equilibrium;microeconomics;mathematical economics;computational complexity theory;welfare economics;equilibrium selection;algorithm;nash equilibrium	ECom	-4.226027513328419	0.3756561839026482	25059
92c42d7e8229dc04161d03f7e4199340f8620ce2	the optimal portfolio model based on mean-cvar with linear weighted sum method	optimal portfolio model;linear weighted sum method the optimal portfolio var cvar multi objectives programming;mean cvar;investments;the optimal portfolio;risk analysis;linear weighted sum method;portfolios;investment;multi objectives programming;portfolio var;investors;portfolios reactive power investments modeling optimization gaussian distribution mathematical model;mathematical model;optimization;risk minimization;risk analysis investment;modeling;gaussian distribution;m v model;var;cvar;reactive power;investors optimal portfolio model mean cvar linear weighted sum method risk minimization portfolio var m v model	This paper proposed the optimal portfolio model maximizing returns and minimizing the risk expressed as CvaR under the assumption that the portfolio return subjects to heavy tail. With linear weighted sum method, we solved the multi-objectives model, and compared the model results to the case under the assumption of normal distribution portfolio return, which is based on the portfolio VAR. In an empirical research, it shows that the return in our model is approximate to that of M-V model, but risk is higher than M-V model. It is illustrated that when risk is described as CvaR, it will predict the potential risk of the portfolio, which is helpful for investors to raise awareness of risk.	approximation algorithm;cvar;v-model;weight function	Xing Yu;Yuling Tan;Liang Liu;Wenfeng Huang	2012	2012 Fifth International Joint Conference on Computational Sciences and Optimization	10.1109/CSO.2012.26	financial economics;post-modern portfolio theory;efficient frontier;actuarial science;economics;replicating portfolio;expected shortfall;investment;modern portfolio theory;finance;portfolio optimization;spectral risk measure;mathematics;distortion risk measure;rate of return on a portfolio;black–litterman model;roy's safety-first criterion;time consistency	ML	3.8540466692033877	-10.545762652547918	25083
089487f5ed5ceca487308fb204eb053da2f7925e	optimising maintenance: what are the expectations for cyber physical systems	reliability;wear;sensors;signal analysis;loading;cyber physical systems;condition based maintenance cbm;condition monitoring;monitoring;cyber physical systems wear condition monitoring condition based maintenance cbm sensors signal analysis diagnosis prognosis;sensors monitoring loading condition monitoring machinery reliability;wear condition monitoring cyber physical systems machinery maintenance engineering mechanical engineering computing;machinery;diagnosis;prognosis;wear monitoring cyber physical systems maintenance optimisation machinery component wear chemical wear optical methods varying load case	The need for maintenance is based on the wear of components of machinery. If this need can be defined reliably beforehand so that no unpredicted failures take place then the maintenance actions can be carried out economically with minimum disturbance to production. There are two basic challenges in solving the above. First understanding the development of wear and failures, and second managing the measurement and diagnosis of such parameters that can reveal the development of wear. In principle the development of wear and failures can be predicted through monitoring time, load or wear as such. Monitoring time is not very efficient, as there are only limited numbers of components that suffer from aging which as such is result of chemical wear i.e. changes in the material. In most cases the loading of components influences their wear. In principle the loading can be stable or varying in nature. Of these two cases the varying load case is much more challenging than the stable one. The monitoring of wear can be done either directly e.g. optical methods or indirectly e.g. vibration. Monitoring actual wear is naturally the most reliable approach, but it often means that additional investments are needed. The paper discusses the above issues and what are the requirements that follow from these for optimising maintenance based of the use of Cyber Physical Systems.	cyber-physical system;requirement	Erkki Jantunen;Urko Zurutuza;Luis Lino Ferreira;Pál Varga	2016	2016 3rd International Workshop on Emerging Ideas and Trends in Engineering of Cyber-Physical Systems (EITEC)	10.1109/EITEC.2016.7503697	structural engineering;engineering;forensic engineering;mechanical engineering	Embedded	12.157593099251988	-12.556374198446692	25098
deefd99bd40e008ced3c644d34b0db632e2154f3	covering-based fuzzy rough sets		Many researchers have combined rough set theory and fuzzy set theory in order to easily approach problems of imprecision and uncertainty. Covering-based rough sets are one of the important generalizations of classical rough sets. Naturally, covering-based fuzzy rough sets can be studied as a combination of covering-based rough set theory and fuzzy set theory. It is clear that Pawlak’s rough set model and fuzzy rough set model are special cases of the covering-based fuzzy rough set model. This paper investigates the properties of covering-based fuzzy rough sets. In addition, operations of intersection, union and complement on covering-based fuzzy rough sets are investigated. Finally, the corresponding algebraic properties are discussed in detail.	fuzzy set;linear algebra;rough set;set theory;turing completeness;monotone	Qingzhao Kong;Zengxin Wei	2015	Journal of Intelligent and Fuzzy Systems	10.3233/IFS-151940	combinatorics;discrete mathematics;rough set;membership function;defuzzification;fuzzy mathematics;fuzzy classification;fuzzy number;mathematics;fuzzy set;fuzzy set operations;dominance-based rough set approach	AI	-1.3715098286507126	-23.50233461885872	25149
50bf64802a8b80995ebaa20e152de3e3b8c9fb38	an intelligent multi-response off-line quality control for semiconductor manufacturing	fabrication;experimental design;n ratio;off line quality control;quality improvement;statistical process control;plasma etching;plasma etching process s n ratio production control quality control semiconductor manufacturing off line quality control statistical analysis performance characteristic transformation method optimal multi response process taguchi procedure;statistical process control production control quality control semiconductor device manufacture statistical analysis;process design;production control;statistical analysis;optimal multi response process;industrial control;process optimization;semiconductor device manufacture;performance characteristic transformation method;quality control semiconductor device manufacture plasma applications plasma immersion ion implantation costs fabrication etching optimization methods industrial control statistical analysis;plasma immersion ion implantation;cost effectiveness;quality control;plasma etching process;s n ratio;etching;plasma applications;taguchi procedure;semiconductor manufacturing;optimization methods	A Taguchi-based off-line quality control method is a cost-effective quality-improvement technique that uses experimental-design methods for efficient characterization of a product or process, combined with a statistical analysis of its variability, with the ultimate purpose of its minimization, so that more stable and higher quality products can be obtained. The paper presents a generalized performance statistic which is a summation of the expected quality losses of all responses. Since the units and orders of these responses may be quite different and not reasonable for process design, a normalization procedure called the performance characteristic transformation method (PCTM) is proposed to overcome these difficulties. In addition, a corresponded generalized signal-to-noise (S/N) ratio may be constructed based on the above concepts and would consider the importance of each response simultaneously. Therefore, the optimal multi-response process can be obtained by the traditional Taguchi procedure with respect to the new proposed S/N ratio. An experiment of the plasma-etching process is conducted to verify the performance of the new multiresponse process optimization technique. >		P.-R. Chang;Chin-Hui Hong;Share-Young Lee	1990		10.1109/IROS.1990.262461	process design;quality control;quality management;cost-effectiveness analysis;plasma etching;engineering;process optimization;etching;fabrication;design of experiments;semiconductor device fabrication;engineering drawing;statistical process control;statistics;manufacturing engineering	Robotics	14.367768461201871	-12.468242175600485	25188
3c18218ce674247f75b18e30255c633512797c42	"""corrigendum to """"tacit collusion in repeated auctions"""" [j. econ. theory 114 (2004) 153-169]"""	tacit collusion		tacit programming	Andrzej Skrzypacz;Hugo Hopenhayn	2004	J. Economic Theory	10.1016/j.jet.2003.12.001	industrial organization;economics;microeconomics;commerce	Theory	-3.9195610197479174	-3.281025025869149	25318
e70c716399a3e2a1545692954050cb9c685b12dd	exact solution of large-scale hub location problems with multiple capacity levels	traveling salesman problem;game theory;pareto optimal cuts;elimination tests;benders decomposition;hubs;hub location;algorithms;pareto optimum	This paper presents an extension of the classical capacitated hub location problem with multiple assignments in which the amount of capacity installed at the hubs is part of the decision process. An exact algorithm based on a Benders decomposition of a strong path-based formulation is proposed to solve large-scale instances of two variants of the problem: the splittable and nonsplittable commodities cases. The standard decomposition algorithm is enhanced through the inclusion of features such as the generation of strong optimality cuts and the integration of reduction tests. Given that in the nonsplittable case the resulting subproblem is an integer program, we develop an efficient enumeration algorithm. Extensive computational experiments are performed to evaluate the efficiency and robustness of the proposed algorithms. Computational results obtained on benchmark instances with up to 300 nodes and five capacity levels confirm their efficiency.		Ivan Contreras;Jean-François Cordeau;Gilbert Laporte	2012	Transportation Science	10.1287/trsc.1110.0398	benders' decomposition;game theory;mathematical optimization;combinatorics;computer science;mathematics;mathematical economics;travelling salesman problem;algorithm	AI	17.497984584814777	3.0653623377917056	25335
3134b7228aef44a990e6d5f3c168dac0c66fdd41	a new algorithm for adaptive online selection of auxiliary objectives	parameter control;evolutionary computation;standards;optimization evolutionary computation learning artificial intelligence benchmark testing standards radiation detectors problem solving;multi agent systems evolutionary computation feature selection learning artificial intelligence;reinforcement learning;radiation detectors;q learning auxiliary objectives target objective optimization problems adaptive online objective selection method reinforcement learning agent single objective evolutionary algorithms leadingones onemax mh iff problem;evolutionary algorithms;optimization;learning artificial intelligence;benchmark testing;parameter control reinforcement learning multi objectivization evolutionary algorithms;problem solving;multi objectivization	Consider optimization problems, where a target objective should be optimized. Some auxiliary objectives can be used to obtain the optimum of the target objective in less number of objective evaluations. We call such auxiliary objective a supporting one. Usually there is no prior knowledge about properties of auxiliary objectives, some objectives can be obstructive as well. What is more, an auxiliary objective can be both supporting and obstructive at different stages of the target objective optimization. Thus, an adaptive online method of objective selection is needed. Earlier, we proposed a method for doing that, which is based on reinforcement learning. In this paper, a new algorithm for adaptive online selection of optimization objectives is proposed. The algorithm meets the interface of a reinforcement learning agent, so it can be fit into the previously proposed framework. The new algorithm is applied for solving some benchmark problems with single-objective evolutionary algorithms. Specifically, Leading Ones with OneMax auxiliary objective is considered, as well as the MH-IFF problem. Experimental results are presented. The proposed algorithm outperforms Q-learning and random objective selection on the considered problems.	benchmark (computing);evolutionary algorithm;mathematical optimization;modified huffman coding;multi-objective optimization;optimization problem;q-learning;reinforcement learning	Arina Buzdalova;Maxim Buzdalov	2014	2014 13th International Conference on Machine Learning and Applications	10.1109/ICMLA.2014.100	benchmark;mathematical optimization;computer science;artificial intelligence;machine learning;particle detector;reinforcement learning;evolutionary computation	AI	24.045708050576923	-2.1195902128365183	25366
67e315f0e82f1c294363ea977946523958f20874	fuzzy and nonlinear programming approach for optimizing the performance of ubiquitous hotel recommendation	hotel;recommendation;ubiquitous computing;fuzzy weighted average;nonlinear programming	This study proposes a fuzzy and nonlinear programming approach for ubiquitous hotel recommendation. In the proposed approach, the weights of the attributes of a hotel differ among travelers, among locations, and over time. In addition, the weights assigned by a traveler are considered uncertain, and this uncertainty is resolved by defining these weights in fuzzy values. The overall performance of a hotel is then evaluated with the fuzzy weighted average of performance levels along all attribute dimensions. Subsequently, a nonlinear programming model is formulated and solved to derive the fuzzy values of weights that tailor the recommendation results to travelers’ choices. The proposed fuzzy and nonlinear programming approach was applied to a small region in the Seatwen District, Taichung City, Taiwan, and it satisfactorily explained travelers’ hotel choices in a ubiquitous environment.	nonlinear programming;nonlinear system	Toly Chen;Yu Hsuan Chuang	2018	J. Ambient Intelligence and Humanized Computing	10.1007/s12652-015-0335-2	fuzzy logic;machine learning;data mining;computer science;ubiquitous computing;artificial intelligence;nonlinear programming;weighted arithmetic mean	AI	-4.483829581109104	-19.18723638745701	25447
99663c54b30f122b3e1de59c263a170974387625	a multistage method for multiobjective route selection	dijkstra algorithm;genetic algorithm	The multiobjective route selection problem (m-RSP) is a key research topic in the car navigation system (CNS) for ITS (Intelligent Transportation System). In this paper, we propose an interactive multistage weight-based Dijkstra genetic algorithm (mwD-GA) to solve it. The purpose of the proposed approach is to create enough Pareto-optimal routes with good distribution for the car driver depending on his/her preference. At the same time, the routes can be recalculated according to the driveru0027s preferences by the multistage framework proposed. In the solution approach proposed, the accurate route searching ability of the Dijkstra algorithm and the exploration ability of the Genetic algorithm (GA) are effectively combined together for solving the m-RSP problems. Solutions provided by the proposed approach are compared with the current research to show the effectiveness and practicability of the solution approach proposed.	multistage amplifier	Feng Wen;Mitsuo Gen	2009	IEICE Transactions		mathematical optimization;suurballe's algorithm;simulation;genetic algorithm;dijkstra's algorithm;computer science;machine learning	Visualization	17.771459248106623	-0.042718132813735425	25455
d452f08ce39b2894626febb2ca782f3894e66afc	a new ranking method in the interval number complementary judgement matrix	mathematics;complexity theory;interval number complementary judgement matrix;matrix algebra;objective function;number theory;indexes;vectors;ranking;fuzzy systems mathematics decision making uncertainty;interval weight new ranking method interval number complementary judgement matrix;ranking interval number complementary judgement matrix consistency;linear program;interval weight;number theory matrix algebra;matlab;consistency;fuzzy systems;new ranking method	The paper gives the definition of consistency of interval number complementary judgement matrix, structuring object function, it gets the interval weight. Furthermore, it gets a simple method for checking the consistency of interval number complementary judgement matrix without having to solve linear program. Finally, a numerical example is given to illustrate the use of this method.	linear programming;numerical analysis	Wen-lei Shi;Xue-jun Peng;Juan Li;Yan-li Gao	2008	2008 Fifth International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2008.273	database index;combinatorics;number theory;discrete mathematics;ranking;mathematics;consistency;fuzzy control system	Robotics	-2.2708527659407327	-20.765988686648242	25471
12ae355b47086d1a4bcf23b06cced648acf15e54	the application of grey relational projection to rank and optimize risk control options	risk control option;grey relational analysis;combination weight	Risk control option (RCO) is a critical step for reducing the risk to a acceptable level in Formal Safety Assessment (FSA). Multi-attribute decision methods are introduced to measure quantity of RCOs. In this paper, grey relational methods and weights based on deviation square are combined to rank RCOs. Data information of RCOs on general cargo ships is calculated to select a suitable alternative, whose results show that grey relational methods are simple and practical for selecting of RCOs.		Jingxia Liu;Lidong Wang;Junzhong Bao	2015	2015 12th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)	10.1109/FSKD.2015.7382197	econometrics;grey relational analysis;data mining;mathematics	Robotics	-4.354291329636771	-18.00539774839881	25510
004daa1e30e37fb8c7b28bcbf0226993d644af75	computing contingent plans using online replanning		In contingent planning under partial observability with sensing actions, agents actively use sensing to discover meaningful facts about the world. For this class of problems the solution can be represented as a plan tree, branching on various possible observations. Recent successful approaches translate the partially observable contingent problem into a non-deterministic fully observable problem, and then use a planner for non-deterministic planning. While this approach has been successful in many domains, the translation may become very large, encumbering the task of the non-deterministic planner.rnrnIn this paper we suggest a different approach — using an online contingent solver repeatedly to construct a plan tree. We execute the plan returned by the online solver until the next observation action, and then branch on the possible observed values, and replan for every branch independently. In many cases a plan tree can be exponential in the number of state variables, but still, the tree has a structure that allows us to compactly represent it using a directed graph. We suggest a mechanism for tailoring such a graph that reduces both the computational effort and the storage space. Furthermore, unlike recent state of the art offline planners, our approach is not bounded to a specific class of contingent problems, such as limited problem width, or simple contingent problems. We present a set of experiments, showing our approach to scale better than state of the art offline planners.	contingency (philosophy)	Radimir Komarnitsky;Guy Shani	2016			machine learning;planner;state variable;mathematical optimization;observability;computer science;directed graph;observable;bounded function;solver;artificial intelligence;graph	AI	20.53063736133719	-14.594025550110501	25562
a119e35a010ac4b53be3595d4f10319e48641aa8	iam 2016 chairs' welcome & organization	metaheuristics;computing;industry;evolutionary;algorithms;real world	The workshop covers many aspects of the application of Metaheuristics in industrial environment and mainly the step from laboratory to the shop floor to meet the necessities of the industry in its real daily operations. The main areas of interest of the workshop are: success stories for industrial applications of metaheuristics, pitfalls of industrial applications of metaheuristics, metaheuristics to optimize dynamic industrial problems, multi-objective optimization in real-world industrial problems, meta-heuristics in very constraint industrial optimization problems, reduction of computing times through parameter tuning and surrogate modelling, parallelism and/or distributed design to accelerate computations, algorithm selection and configuration for complex problem solving, advantages and disadvantages of metaheuristics when compared to other techniques such as integer programming or constraint programming and new research topics for academic research inspired by real (algorithmic) needs in industrial applications.	algorithm selection;computation;constraint programming;heuristic (computer science);identity management;integer programming;mathematical optimization;metaheuristic;multi-objective optimization;parallel computing;problem solving;surrogate model	Silvino Fernandez;Thomas Stützle;Pablo Valledor Pellicer	2016		10.1145/2908961.2931645	computing;computer science;artificial intelligence;operations research;metaheuristic	EDA	21.46571374996756	0.5575709025608797	25596
afbf6dc7f70572b9b21a624dba6989a11856842c	methodology for fraud detection using rough sets	electronic mail;information systems;indexing terms;data mining;inspection;protection;energy consumption;energy distribution;spatial databases;pattern classification;rough sets;artificial intelligence;knowledge discovery in database;rough set;fraud detection;credit cards;historical data;rough sets inspection data mining artificial intelligence spatial databases energy consumption protection electronic mail credit cards information systems	This work proposes a methodology based on Rough Sets and KDD for fraud detection made by electrical energy consumers. This methodology does a detailed evaluation of the boundary region between normal and fraudulent costumers, identifying patterns of fraudulent behavior at historical data sets of electricity companies. Using these patterns, classification rules are derived, and they will permit the detection on the database of electricity companies of those clients that present fraudulent feature. When doing inspections with the proposed methodology, the rate of correctness and the quantity of detected frauds are increased, decreasing the losses with electricity fraud on Brazilian electrical energy distribution companies.	correctness (computer science);preprocessor;rough set;semiconductor consolidation	José Edison Cabral;João O. P. Pinto;Kathya S. C. Linares;Alexandra M. A. C. Pinto	2006	2006 IEEE International Conference on Granular Computing	10.1109/GRC.2006.1635791	rough set;computer science;machine learning;data mining;computer security	Robotics	11.236699473668121	-15.10583984599578	25636
79b19067bbe3c2c2370e1312fc9b550629694c78	freight transportation in railway networks with automated terminals: a mathematical model and mip heuristic approaches	freight transportation optimal planning mathematical programming mip heuristics;mathematical programming;mip heuristics;optimal planning;freight transportation	In this paper we propose a planning procedure for serving freight transportation requests in a railway network with fast transfer equipment at terminals. We consider a transportation system where different customers make their requests (orders) for moving boxes, i.e., either containers or swap bodies, between different origins and destinations, with specific requirements on delivery times. The decisions to be taken concern the route (and the corresponding sequence of trains) that each box follows in the network and the assignment of boxes to train wagons, taking into account that boxes can change more than one train and that train timetables are fixed. The planning procedure includes a pre-analysis step to determine all the possible sequences of trains for serving each order, followed by the solution of a 0-1 linear programming problem to find the optimal assignment of each box to a train sequence and to a specific wagon for each train in the sequence. This latter is a generalized assignment problem which is NP-hard. Hence, in order to find good solutions in acceptable computation times, two MIP heuristic approaches are proposed and tested through an experimental analysis considering realistic problem instances.	heuristic;mathematical model	Davide Anghinolfi;Massimo Paolucci;Simona Sacone;Silvia Siri	2011	European Journal of Operational Research	10.1016/j.ejor.2011.05.013	mathematical optimization;simulation;operations management;mathematics	Robotics	14.478977365509618	1.9165494236125988	25692
8213756452100246e48e4fc14e3f4af4a8b979d8	short term load forecasting using echo state networks	reservoirs;nonlinear mapping;training methods;echo state network;reservoirs training load forecasting artificial neural networks recurrent neural networks testing power system dynamics;neural nets;training;power system dynamics;reservoir size short term load forecasting echo state networks hourly load data average temperature nonlinear mapping training methods recurrent neural networks;testing;load forecasting;short term load forecasting;artificial neural networks;power engineering computing;average temperature;echo;reservoir size;recurrent neural networks;recurrent neural network;reservoirs echo load forecasting neural nets power engineering computing;echo state networks;neural network;hourly load data	In this paper a new algorithm is proposed for Short Term Load Forecasting (STLF) using Echo State Networks (ESN). Hourly load data along with only average temperature of each day and day type flag is fed to the ESN and nonlinear mapping is done using training methods. Despite conventional recurrent neural networks, ESN can be trained much easier and with great deal of accuracy. Simulation results show that this method successfully predicts load demands even using limited input data. Using several parallel ESN units with smaller reservoir sizes in which each ESN unit identifies the dynamics of a certain hour of the day throughout the training and testing process results in more efficient use of data. Using this method, there is no need to identify weak correlations between dynamics of certain hours by using bigger neural network.	algorithm;artificial neural network;echo state network;long short-term memory;nonlinear system;recurrent neural network;simulation	Hemen Showkati;Amir H. Hejazi;Sajad Elyasi	2010	The 2010 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2010.5596950	simulation;computer science;artificial intelligence;recurrent neural network;machine learning;artificial neural network	Robotics	10.461152675505534	-20.94572094059209	25732
b1fb9d84600bc3849171bebe71f7b80b66cd380a	the hybrid genetic algorithm with two local optimization strategies for traveling salesman problem	traveling salesman problem;local optimization strategies;journal;期刊论文;genetic algorithm	Traveling salesman problem (TSP) is proven to be NP-complete in most cases. The genetic algorithm (GA) is improved with two local optimization strategies for it. The first local optimization strategy is the four vertices and three lines inequality, which is applied to the local Hamiltonian paths to generate the shorter Hamiltonian circuits (HC). After the HCs are adjusted with the inequality, the second local optimization strategy is executed to reverse the local Hamiltonian paths with more than 2 vertices, which also generates the shorter HCs. It is necessary that the two optimization strategies coordinate with each other in the optimization process. The two optimization strategies are operated in two structural programs. The time complexity of the first and second local optimization strategies are O(n) and O(n), respectively. The two optimization strategies are merged into the traditional GA. The computation results show that the hybrid genetic algorithm (HGA) can find the better approximate solutions than the GA does within an acceptable computation time.	approximation algorithm;combinatorial optimization;computation;email;flow network;genetic algorithm;graph theory;hercules graphics card;heuristic;local search (optimization);mathematical optimization;memetic algorithm;np-completeness;reversion (software development);social inequality;time complexity;travelling salesman problem	Yong Wang	2014	Computers & Industrial Engineering	10.1016/j.cie.2014.01.015	optimization problem;extremal optimization;2-opt;mathematical optimization;multi-swarm optimization;combinatorics;meta-optimization;genetic algorithm;cross-entropy method;combinatorial optimization;computer science;derivative-free optimization;artificial intelligence;machine learning;mathematics;travelling salesman problem;algorithm;random optimization;3-opt;metaheuristic;bottleneck traveling salesman problem;global optimization	AI	24.19475908895498	0.6551867029409076	25765
44af2210eba2f244a83f5798aafb960ba0b210a0	genetic based discrete particle swarm optimization for elderly day care center timetabling	rovranked order value;engineering design;jspjob shop scheduling problem;cspconstraint satisfaction problem;fspflow shop scheduling problem;weighted max constraint satisfaction problem;max cspmaximum constraint satisfaction problem;gdpsogenetic based discrete particle swarm optimization;bcobee colony optimization;ggaguided genetic algorithm;min conflict random walk;manufactures;gagenetic algorithm;mcrwmin conflict random walk algorithm;tstabu search;edccelderly day care center;dpsodiscrete particle swarm optimization;psoparticle swarm optimization;genetic algorithm;tabu search;sasimulated annealing;discrete particle swarm optimization;timetabling problem;spsostandard particle swarm optimization;anovaanalysis of variance;np hardnon deterministic polynomial hard	The timetabling problem of local Elderly Day Care Centers (EDCCs) is formulated into a weighted maximum constraint satisfaction problem (Max-CSP) in this study. The EDCC timetabling problem is a multi-dimensional assignment problem, where users (elderly) are required to perform activities that require different venues and timeslots, depending on operational constraints. These constraints are categorized into two: hard constraints, which must be fulfilled strictly, and soft constraints, which may be violated but with a penalty. Numerous methods have been successfully applied to the weighted Max-CSP; these methods include exact algorithms based on branch and bound techniques and approximation methods based on repair heuristics, such as the min-conflict heuristic. This study aims to explore the potential of evolutionary algorithms by proposing a genetic-based discrete particle swarm optimization (GDPSO) to solve the EDCC timetabling problem. The proposed method is compared with the min-conflict random-walk algorithm (MCRW), Tabu search (TS), standard particle swarm optimization (SPSO), and a guided genetic algorithm (GGA). Computational evidence shows that GDPSO significantly outperforms the other algorithms in terms of solution quality and efficiency.	approximation;assignment problem;branch and bound;categorization;computation;constraint satisfaction problem;evolutionary algorithm;genetic algorithm;heuristic (computer science);mathematical optimization;maxima and minima;particle swarm optimization;tabu search	M. Y. Lin;Kwai-Sang Chin;Kwok Leung Tsui;T. C. Wong	2016	Computers & OR	10.1016/j.cor.2015.07.010	mathematical optimization;genetic algorithm;tabu search;computer science;artificial intelligence;machine learning;mathematics;engineering design process	AI	20.801904517293178	0.4156306937644224	25811
015d1aabae32efe2c30dfa32d8ce71d01bcac9c5	applications of approximation algorithms to cooperative games	broadcast news;convex polytope;approximate algorithm;service provider;cost function;convex distance function;polyhedron;complexity;cooperative game;lower envelope;bisector;voronoi diagram	"""The Internet, which is intrinsically a common playground for a large number of players with varying degrees of collaborative and sel sh motives, naturally gives rise to numerous new game theoretic issues. Computational problems underlying solutions to these issues, achieving desirable economic criteria, often turn out to be NP-hard. It is therefore natural to apply notions from the area of approximation algorithms to these problems. The connection is made more meaningful by the fact that the two areas of game theory and approximation algorithms share common methodology { both heavily use machinery from the theory of linear programming. Various aspects of this connection have been explored recently by researchers [8, 10, 15, 20, 21, 26, 27, 29]. In this paper we will consider the problem of sharing the cost of a jointly utilized facility in a \fair"""" manner. Consider a service providing company whose set of possible customers, also called users, is U . For each set S U C(S) denotes the cost incurred by the company to serve the users in S. The function C is known as the cost function. For concreteness, assume that the company broadcasts news of common interest, such as nancial news, on the net. Each user, i, has a utility, u0i, for receiving the news. This utility u 0 i is known only to user i. User i enjoys a bene t of u0i xi if she gets the news at the price xi. If she does not get the news then her bene t is 0. Each user is assumed to be sel sh, and hence in order to maximize bene t, may misreport her utility as some other number, say ui. For the rest of the discussion, the utility of user i will mean the number ui. A cost sharing mechanism determines which users receive the broadcast and at what price. The mechanism is strategyproof if the dominant strategy of each user is to reveal the"""	approximation algorithm;computation;game theory;internet;linear programming;list of code lyoko episodes;loss function;np-hardness	Kamal Jain;Vijay V. Vazirani	2001		10.1145/380752.380825	service provider;bisection;convex analysis;mathematical optimization;combinatorics;discrete mathematics;weighted voronoi diagram;complexity;convex polytope;voronoi diagram;convex hull;mathematics;proper convex function;polyhedron	Theory	-2.0987953660918466	1.1531433370519955	25861
601f20b50134c97b88af56a49f72f507519f3eaf	diagnosis of component failures in the space shuttlemain engines using bayesian belief network: a feasibility study	space shuttle main engine;feasibility study;bayesian belief networks;bayesian belief network;sensor validation;fault diagnosis	Although the Space Shuttle is a high reliability system, the health of the Space Shuttle must be accurately diagnosed in real-time. Two problems current plague the system, false alarms that may be costly, and missed alarms which may be not only expensive, but also dangerous to the crew. This paper describes the results of a feasibility study where a multivariate state estimation technique is coupled with a Bayesian Belief Network to provide both fault detection and fault diagnostic capabilities for the Space Shuttle Main Engines (SSME). Five component failure modes and several single sensor failures are simulated in our study and correctly diagnosed. The results indicate that this is a feasible fault detection and diagnosis technique and fault detection and diagnosis can be made earlier than standard redline methods allow.	bayesian network	Edwina Liu;Du Zhang	2003	International Journal on Artificial Intelligence Tools	10.1142/S0218213003001277	feasibility study;simulation;computer science;machine learning;bayesian network	Robotics	13.178865469247524	-14.618971721470423	25925
eb2c385c443fa479e59f4c25cc45dd55abd4e9f4	group tactics utilizing suppression and shelter	artificial intelligence group tactics tactical games squad movement global optimal strategy suppression fire graph theoretic approach pspace greedy algorithms;group tactics;video game;video game group tactics al;al;fires games artificial intelligence memory management real time systems optimization equations;greedy algorithms artificial intelligence computer games graph theory	In tactical games, a common strategy in squad movement has a portion of troops providing suppression fire while the rest of the squad advances to a new cover location safely. This represents a set of state changes as the entire squad moves towards the objective. The aim of this paper is to find the global optimal strategy where the total damage to the squad is minimized as they breach an objective. Traditional approaches to squad movement use greedy techniques and do not consider suppression fire. To better understand this problem and provide a firm foundation for other squad movement strategies that wish to employ suppression fire cover, we have developed a graph theoretic approach that solves for the globally optimal movement. We show that while this approach is PSPACE in terms of the number of cover locations, there are ways to mitigate this cost. We compare and contrast this approach to common greedy methods for squad movement, showing configurations where greedy algorithms fail. For this paper, we assume hardened targets.	graph theory;greedy algorithm;maxima and minima;pspace;zero suppression	Yinxuan Shi;Roger Crawfis	2014	2014 Computer Games: AI, Animation, Mobile, Multimedia, Educational and Serious Games (CGAMES)	10.1109/CGames.2014.6934139	simulation;artificial intelligence;machine learning;operations research	AI	7.555576527636161	2.469789299062379	25991
1ded347f85656ab710294821e9360aa9aa84e813	price systems constructed by optimal dynamic portfolios	dynamic programming;90 c 40;pricing of options;optimisation;programacion dinamica;funcion utilidad;marche financier;optimizacion;methode mesure;martingale;maximization;substitution;fonction utilite;utility function;metodo medida;discrete time;90 c 39;optimal dynamic portfolio;prix;utility maximization;regime prix;msc classification 1991 90 a 09;funcion logaritmica;martingale measure;logarithmic function;key words martingale measure;conexion;price system;60 g 42;raccordement;fonction logarithmique;programmation dynamique;financial market;portfolio management;equivalent martingale measure;optimization;gestion cartera;contingent claim;measurement method;gestion portefeuille;tiempo discreto;temps discret;price;connection;maximizacion;precio;substitucion;mercado financiero;maximisation;93 e 20	The paper studies connections between arbitrage and utility maximization in a discrete-time financial market. The market is incomplete. Thus one has several choices of equivalent martingale measures to price contingent claims. Davis determines a unique price for a contingent claim which is based on an optimal dynamic portfolio by use of a `marginal rate of substitution' argument. Here conditions will be given such that this price is determined by a martingale measure and thus by a consistent price system. The underlying utility function U is defined on the positive half-line. Then dynamic portfolios are admissible if the terminal wealth is positive. In case of the logarithmic utility function, the optimal dynamic portfolio is the numeraire portfolio.	price systems	Manfred Schäl	2000	Math. Meth. of OR	10.1007/s001860000049	mathematical optimization;logarithm;discrete time and continuous time;martingale;connection;dynamic programming;mathematics;superhedging price;risk-neutral measure;mathematical economics;financial market;statistics;project portfolio management	ECom	3.146522913819129	-2.748405765718047	26002
26b52c095270739dc25cde82711399bf7ab46c83	policy shaping: integrating human feedback with reinforcement learning	advise;values;reward shaping;proceedings;human feedback;direct policy labels;labels	A long term goal of Interactive Reinforcement Learning is to incorporate nonexpert human feedback to solve complex tasks. Some state-of -the-art methods have approached this problem by mapping human information t o rewards and values and iterating over them to compute better control polici es. In this paper we argue for an alternate, more effective characterization of human feedback: Policy Shaping. We introduce Advise, a Bayesian approach that attempts to maximize the information gained from human feedback by utilizing it a s direct policy labels. We compareAdvise to state-of-the-art approaches and show that it can outperf orm them and is robust to infrequent and inconsistent human feed back.	feedback;interactivity;noise shaping;programming paradigm;reinforcement learning	Shane Griffith;Kaushik Subramanian;Jonathan Scholz;Charles Lee Isbell;Andrea Lockerd Thomaz	2013			simulation;computer science;artificial intelligence;values;machine learning	ML	20.456525442189417	-19.765492270585625	26015
f48700c5a9401e5f5600a552a54252295723561a	unsupervised neural method for temperature forecasting	forecasting;time series forecasting;real time;linear regression;time series;arima model;negative feedback;adaptation;water masses;oceanography;artificial neural network	This article presents the results of using a novel Negative Feedback Artificial Neural Network for extraction of models of the thermal structure of oceanographic water masses and to forecast time series in real time. The results obtained using this model are compared with those obtained using a Linear Regression and an ARIMA model. The article presents the Negative Feedback Artificial Neural Network, shows how it extracts the model behind the data set and discuses the Artificial Neural Network’s forecasting abilities.	unsupervised learning	Juan Manuel Corchado;Colin Fyfe	1999	AI in Engineering	10.1016/S0954-1810(99)00007-2	computer science;engineering;artificial intelligence;machine learning;time series;artificial neural network	AI	9.994074544797764	-20.198210648402302	26070
bccf2e1c4acf81d2675ee25c2d3019aa71bbb062	multi-resolution selective ensemble extreme learning machine for electricity consumption prediction		We propose a multi-resolution selective ensemble extreme learning machine (MRSE-ELM) method for time-series prediction with the application to the next-step and next-day electricity consumption prediction. Specifically, at the current time stamp, the preceding time-series data is sampled at different time intervals (i.e. resolutions) to constitute the time windows used for the prediction. The value at each sampled point can be certain statistics calculated from its associated time interval. At each resolution, multiple extreme learning machines (ELMs) with different numbers of hidden neurons are first trained. Then, sequential forward selection and least square regression are used to select an optimal set of trained ELMs to constitute the final ensemble model. The experimental results demonstrate that the proposed MRSE-ELM outperforms the best single ELM model across all resolutions. Compared to three state-of-the-art prediction models, MRSE-ELM shows its superiority on the next-step and next-day electricity consumption prediction tasks.		Hui Song;A. Kai Qin;Flora Dilys Salim	2017		10.1007/978-3-319-70139-4_61	machine learning;timestamp;predictive modelling;pattern recognition;artificial intelligence;extreme learning machine;least squares;computer science;electricity;ensemble forecasting	AI	8.503630069254934	-21.269784121005937	26083
087e678d51e52a3e71fdcb7dcd9be593adb6c5c0	a quantum-inspired q-learning algorithm for indoor robot navigation	quantum inspired q learning algorithm;machine learning algorithms;control systems;learning process;learning algorithm;motion control;probability;concurrent computing;reinforcement learning;probability quantum inspired q learning algorithm indoor robot navigation control action dependent reinforcement learning state superposition principle quantum parallel computation;robot navigation;mobile robots;optimal policy;state superposition principle;intelligent control;fuzzy logic;navigation;computational modeling;machine learning;quantum computer;quantum mechanics;navigation quantum computing machine learning algorithms quantum mechanics concurrent computing computational modeling mobile robots machine learning control systems fuzzy logic;indoor robot navigation control;parallel computer;quantum parallel computation;learning artificial intelligence;quantum computing;action dependent reinforcement learning;quantum computing intelligent control learning artificial intelligence mobile robots motion control navigation probability	A quantum-inspired Q-learning (QIQL) algorithm is proposed for indoor robot navigation control. Q- learning is an action-dependent reinforcement learning method and has been widely used in robot navigation. Inspired by the fundamental characteristics of quantum computation, e.g. state superposition principle and quantum parallel computation, probability is introduced to Q-learning and along with the learning process the probability of each action to be selected at a certain state is updated, which leads to a natural exploration strategy instead of a pointed one with configured parameters. The simulated navigation experiments show that the proposed QIQL algorithm keeps a good balance of exploration and exploitation, which can avoid the local optimal policies and accelerate the learning process as well.	algorithm;computation;experiment;parallel computing;q-learning;quantum computing;reinforcement learning;robotic mapping;superposition principle;uncertainty principle	Chunlin Chen;Pei Yang;Xianzhong Zhou;Daoyi Dong	2008	2008 IEEE International Conference on Networking, Sensing and Control	10.1109/ICNSC.2008.4525476	robot learning;concurrent computing;computer science;control system;artificial intelligence;theoretical computer science;machine learning;quantum computer;reinforcement learning;mobile robot navigation;intelligent control	Robotics	18.421765305668778	-21.835118715493994	26117
74c5a1da34b44dbd514e6f3a83c202d94325ae32	a study on bunker fuel management for the shipping liner services	bunkering port;bunker fuel price;empirical model;sea transport;bunker fuel management;ship speed	In this paper, we consider a bunker fuel management strategy study for a single shipping liner service. The bunker fuel management strategy includes three components: bunkering ports selection (where to bunker), bunkering amounts determination (how much to bunker) and ship speeds adjustment (how to adjust the ship speeds along the service route). As these three components are interrelated, it is necessary to optimize them jointly in order to obtain an optimal bunker fuel management strategy for a single shipping liner service. As an appropriate model representing the relationship between bunker fuel consumption rate and ship speed is important in the bunker fuel management strategy, we first study in detail this empirical relationship. We find that the relationship can be different for different sizes of containerships and provide an empirical model to express this relationship for different sizes of containerships based on real data obtained from a shipping company. We further highlight the importance of using the appropriate consumption rate model in the bunker fuel management strategy as using a wrong or aggregated model can result in inferior or suboptimal strategies. We then develop a planning level model to determine the optimal bunker fuel management strategy, i.e. optimal bunkering ports, bunkering amounts and ship speeds, so as to minimize total bunker fuel related cost for a single shipping liner service. Based on the optimization model, we study the effects of port arrival time windows, bunker fuel prices, ship bunker fuel capacity and skipping port options on the bunker fuel management strategy of a single shipping liner service. We finally provide some insights obtained from two case studies. & 2011 Elsevier Ltd. All rights reserved.	emission intensity;mathematical optimization;microsoft windows;network congestion;quantum fluctuation;stochastic optimization;time of arrival	Zhishuang Yao;Szu Hui Ng;Loo Hay Lee	2012	Computers & OR	10.1016/j.cor.2011.07.012	computer science;empirical modelling	Metrics	8.88858171047233	-6.611499902659589	26131
ff7e17ea852490e945edb3a093bb6e322ab28c55	satisficing measure approach for vehicle routing problem with time windows under uncertainty	heuristic;routing;satisficing measure;stochastic demand;uncertain travel time	The complexity of evaluating chance constraints makes chance-constrained programming problem difficult to solve. One way to handle this complexity is by devising satisficing measures for the relevant uncertainties. This paper focuses on solving the stochastic vehicle routing problem with time windows (VRPTW) by Satisficing Measure Approach (SMA) that mitigates the dissatisfaction experienced by the customers. Satisficing measures are first proposed for the VRPTW with stochastic demand on various distributions to demonstrate the dependency of customers’ satisfaction towards lack of inventory based on the vehicle's capacity. Similar satisficing measures are extended to VRPTW with stochastic travel times. We integrate the proposed satisficing measures into an existing tabu-search heuristics to solve a set of generalized Solomon instances in a short amount of computation time. Compared with best-known results, the SMA saves the effort to design recourse actions, applicable to many popular probability distributions and produces very competitive results.	microsoft windows;vehicle routing problem	Viet Anh Nguyen;Jun Jiang;Kien Ming Ng;Kwong Meng Teo	2016	European Journal of Operational Research	10.1016/j.ejor.2015.07.041	mathematical optimization;routing;heuristic;computer science;operations management;mathematics;welfare economics	Robotics	16.31865693460176	0.7391109161502482	26157
cfb27c3b7d4636b410e7f5d6e1261cec322b95ee	choices of interacting positions on multiple team assembly	combinatorial optimization problem;complexity;team performance;organizational behavior;combinatorial optimization;computer simulation	This paper proposes a new method for choosing interacting positions that affect team performance on multiple team assembly in an organization. Various approaches for replacing team members are also reviewed and adjusted so that the resulting team obtained will be as effective and efficient as possible. This multiple team assembly is a combinatorial optimization problem that focuses on examining complexity in an organization. The objective of the problem is to achieve maximum performance of the team while at the same time trying to reduce the expected number of replacements and the expected number of trials needed to arrive at that performance level. Computer simulation is used to implement and demonstrate the proposed ideas.	algorithm;combinatorial optimization;computer simulation;isis/draw;interaction;mathematical optimization;maxima and minima;norm (social);one-way function;optimization problem;ski combinator calculus	Chartchai Leenawong;Nisakorn Wattanasiripong	2007		10.1007/978-3-540-74205-0_31	computer simulation;mathematical optimization;complexity;simulation;combinatorial optimization;computer science;management science;algorithm;organizational behavior	Robotics	23.69556504995945	-0.7900863279169037	26162
2f1432aa87f7fc0935c047fffb8cebf2d1841960	a benchmark solution for the risk-averse newsvendor problem	probleme vendeur journaux;ley uniforme;funcion demanda;closed form solution;funcion utilidad;expected utility maximization;fonction utilite;risk aversion;expected utility;utility function;newsvendor problem;prise decision;condicion optimalidad;condition optimalite;first order;theorie utilite;fonction demande;teoria utilidad;probabilistic demand;demanda aleatoria;utilite attendue;toma decision;utilidad espera;demande aleatoire;aversion riesgo;loi uniforme;newsboy problem;problema vendedor diarios;aversion risque;optimality condition;demand function;uniform distribution;utility theory	In this paper, we derive the first order conditions for optimality for the problem of a risk-averse expected-utility maximizer newsvendor. We use these conditions to solve a special case where the utility function is any increasing differentiable function, and the random demand is uniformly distributed. This special case has a simple closed form solution and therefore it provides an insightful and practical interpretation to the optimal point. We show some properties of the solution and also demonstrate how it can be used for assessing the newsvendor utility function parameters. 2005 Elsevier B.V. All rights reserved.	benchmark (computing);expected utility hypothesis;newsvendor model;risk aversion	Baruch Keren;Joseph S. Pliskin	2006	European Journal of Operational Research	10.1016/j.ejor.2005.03.047	closed-form expression;newsvendor model;extended newsvendor model;risk aversion;economics;expected utility hypothesis;operations management;first-order logic;uniform distribution;mathematical economics;welfare economics;utility;demand curve	AI	3.4402101859261203	-3.548178156920619	26224
7eb004c753c02c35db206de6ee908caceb41fc98	daily stock prediction using neuro-genetic hybrids	parallel genetic algorithm;linux cluster;genetics;genetic algorithm;prediction model;neural network	We propose a neuro-genetic daily stock prediction model. Traditional indicators of stock prediction are utilized to produce useful input features of neural networks. The genetic algorithm optimizes the neural networks under a 2D encoding and crossover. To reduce the time in processing mass data, a parallel genetic algorithm was used on a Linux cluster system. It showed notable improvement on the average over the buy-and-hold strategy. We also observed that some companies were more predictable than others.	artificial neural network;computer cluster;crossover (genetic algorithm);genetic algorithm;linux	Yung-Keun Kwon;Byung Ro Moon	2003		10.1007/3-540-45110-2_115	genetic algorithm;computer cluster;computer science;artificial intelligence;machine learning;data mining;predictive modelling;artificial neural network	ML	7.975116219564358	-18.943300141784388	26235
1d7a2f9c4fb41257e0f481ed8d62a0fb52fcaaf5	solving the periodic edge routing problem in the municipal waste collection		In many municipal waste collection systems, it is necessary to extend the planning horizon to more than one working day. This can happen, for example, in the collection of some recyclable articles. In this case, some of the streets must be served every day but others need only once every two days service. In this paper, we focus on planning the routing of the collection vehicles while extending the planning horizon to two working days. We propose a simple, but effective, heuristic approach and we carry out extensive computational experiments to evaluate its performance. We also apply our method to solve a real-case application related to the collection of recyclable wastes in a small Italian city.	routing	Chefi Triki	2017	APJOR	10.1142/S0217595917400152	periodic graph (geometry);operations management;mathematics;time horizon;heuristic;municipal solid waste	Theory	14.794835403590243	1.2664662664995208	26248
48fb734825c3616e1a82843a537ca3ac3b90f4c2	aggregate production planning utilizing a fuzzy linear programming	manufacturing;fuzzy linear programming;aggregate production planning	Uncertainties and imprecise information regarding customer demands, production, inventory, and MRP are very common in performing aggregate production-planning (APP) for the manufacturing in real world. This study presents a fuzzy linear programming approach for managing the uncertainties and imprecise information involved in industrial APP applications. Detailed discussions are given to the establishment of the Fuzzy Linear Programming approach with converting the fuzzy constraints of uncertain and imprecise items into deterministic equivalents. A mathematical model is developed for APP practice with the Fuzzy Linear Programming approach. For numerically performing an aggregate production planning with the Fuzzy Linear Programming developed, a computer simulation for an actual aggregate production-planning is presented. It is demonstrated in the study, the employment of the Fuzzy Linear Programming provides a great advantage in APP of manufacturing, if the parameters of the stochastic factors involved in the production planning are neither definitely reliable nor precise. The present study shows that the interrelated effects of the customer service level and facility capacity on the effectiveness and efficiency of aggregate production-planning is significant and should be taken into account in performing an aggregate production-planning	aggregate function;linear programming	Liming Dai;Lisa Fan;L. Sun	2003	Transactions of the SDPS		mathematical optimization;simulation;engineering;fuzzy number;operations management;fuzzy set operations	Robotics	10.22969198101748	-2.3364926776247463	26252
1e7d27ad8c0ceff99e4207f74cf0cf8cd472386d	discovering trading rules with genetic algorithms: an empirical study based on garch time series	time series;genetic algorithm;empirical study		genetic algorithm;time series	Shu-Heng Chen;Wei-Yuan Lin;Chueh-Yung Tsao	1999			genetic algorithm;empirical research;data mining;autoregressive conditional heteroskedasticity;economics;machine learning;artificial intelligence	SE	6.484611458581154	-18.930662089153877	26283
e85e6e045c4c56f2a5948d3015f6ea12216d557b	a portfolio approach to design in the presence of scenario-based uncertainty	uncertainty;pareto optimal;scenario;conceptual design;risk;dissertation;design;co evolutionary algorithm;portfolio			Kenneth Daniel Cooksey	2013			financial economics;mathematical optimization;economics;portfolio optimization;management science	EDA	12.857073935557208	-4.608140713810019	26299
861229056dfb03fe4b201caf36c3e343b3ccd016	just add pepper: extending learning algorithms for repeated matrix games to repeated markov games	game theory;multi agent learning;stochastic games	Learning in multi-agent settings has recently garnered much interest, the result of which has been the development of somewhat effective multi-agent learning (MAL) algorithms for repeated normal-form games. However, general-purpose MAL algorithms for richer environments, such as generalsum repeated stochastic (Markov) games (RSGs), are less advanced. Indeed, previously created MAL algorithms for RSGs are typically successful only when the behavior of associates meets specific game theoretic assumptions and when the game is of a particular class (such as zero-sum games). In this paper, we present a new algorithm, called Pepper, that can be used to extend MAL algorithms designed for repeated normal-form games to RSGs. We demonstrate that Pepper creates a family of new algorithms, each of whose asymptotic performance in RSGs is reminiscent of its asymptotic performance in related repeated normal-form games. We also show that some algorithms formed with Pepper outperform existing algorithms in an interesting RSG.	algorithm;game theory;general-purpose markup language;machine learning;markov chain;multi-agent system	Jacob W. Crandall	2012			randomized algorithms as zero-sum games;game theory;simulation;artificial intelligence;machine learning	ML	19.82645665806106	-17.57694355536476	26351
7b4f7ecdf337e112ade64b6f659a2599e1f4cee9	a two-phase approach to finding a better managerial solution for systems with addition-min fuzzy relational inequalities		In the relevant literature, fuzzy relational inequalities with addition-min composition have been proposed to model the data transmission mechanism in a BitTorrent-like peer-to-peer file-sharing system. In this paper, we present a two-phase approach to find an optimal solution to data transmission that minimizes an associated function while its components are controlled to result in reduced network congestion. Numerical examples are given to illustrate the procedures of the two-phase approach.	bittorrent;maxima and minima;network congestion;numerical method;peer-to-peer file sharing;two-phase commit protocol;two-phase locking	Sy-Ming Guu;Jiajun Yu;Yan-Kuen Wu	2018	IEEE Transactions on Fuzzy Systems	10.1109/TFUZZ.2017.2771406	associated function;inequality;control theory;fuzzy logic;mathematical optimization;mathematics;data transmission;network congestion;data modeling;linear programming	Visualization	0.28889385014055197	-17.383159765129285	26424
3c47dfe0a6531f0a5b028419dc00b48969851724	adaptive stochastic manpower scheduling	bayes methods;human resource management;scheduling;stochastic programming;bayesian forecasting models;adaptive stochastic manpower scheduling;distributional estimates;employee scheduling problem;nonstationary systems;random parameters;rolling-horizon approach;stochastic programming;stochastic programs;uncertain production rates;uncertain up-times	Bayesian forecasting models provide distributional est mates for random parameters, and relative to classic schemes, have the advantage that they can rapidly capt changes in nonstationary systems using limited historic data. Stochastic programs, unlike deterministic optimiza tion models, explicitly incorporate distributions for random parameters in the model formulation, and thus have th advantage that the resulting solutions more fully hedg against future contingencies. In this paper, we exploit th strengths of Bayesian prediction and stochastic program ming in a rolling-horizon approach that can be applied to solve real-world problems. We illustrate the methodolog on an employee scheduling problem with uncertain up times of manufacturing equipment and uncertain productio rates.	nurse scheduling problem;schedule;scheduling (computing);stochastic programming	Elmira Popova;David P. Morton	1998			stochastic programming;job shop scheduling;mathematical optimization;simulation;bayesian probability;computer science;technical report;stochastic optimization;production system;computational model;scheduling	AI	5.325618567862021	-2.657689615732517	26438
48dd9ece1d229a75429abc6fd28af5518f70181e	a three-stage approach to a multirow parallel machine layout problem		Facility layout is vital to save operational cost and enhance production efficiency. Multirow layout is a common pattern in practical manufacturing environment. Although parallel machines are frequently implemented in practice to enhance productivity, there lacks any in-depth study on multirow layout problem with parallel machines. In this paper, its mathematical programming formulation is established to minimize material flow cost. A three-stage approach is proposed to solve it. First, a Monte Carlo heuristic is devised to optimize the sequence of machines on multiple rows. Second, a linear program is used to determine the optimal exact location of each machine. Finally, an exchange heuristic is adopted to reassign material flows among parallel machines in different machine groups. An iterative optimization strategy is suggested to execute the three stages repeatedly to improve the solution quality. This approach is applied to a number of problem instances and compared against others. The experimental results show that it is able to effectively solve this new problem and significantly decrease material flow cost. Note to Practitioners—Multirow layout is common in practical manufacturing systems in which parallel machines are often used to improve productivity, shorten production time, and guarantee some flexibility. This paper studies a multirow parallel machine layout problem that involves machine groups, each of which contains parallel machines. Solving it is to locate all machines at multiple parallel rows to minimize material flow cost. It is challenging because one needs to decompose material flows and determine exact locations of machines simultaneously. A three-stage approach is proposed to do so. It is applied to many problem instances. The results demonstrate that it works well for such layout problems with parallel machines.		Xingquan Zuo;Shubing Gao;MengChu Zhou;Xin Yang;Xinchao Zhao	2019	IEEE Transactions on Automation Science and Engineering	10.1109/TASE.2018.2866377	computer science;mathematical optimization;material flow;row;monte carlo method;heuristic;linear programming	HPC	18.774931523862552	3.3162447704457025	26441
0d0c9864f1e70b7bd76a39ed2a39fbc7db33c13d	a stochastic model for planning swine facilities	farm design;swine production;swine facility;simulation model;practical difference;practical problem;different kind;big range;fit housing cost;stochastic model;classical approach;production management;stochastic processes;farming	A simulation model and its application for planning swine facilities are presented. Swine production is becoming more and more specialized, hence the sizing of a farm producing piglets is the main strategic decision concerning farmers who invest in sow production, since a farm comprises a big range of facilities with many possible sizes. The classical approach is deterministic, including sometimes some security margins without considering variations in future sow performance or in the management policy. The stochastic model presented here has revealed practical differences with respect to deterministic approaches. As result, simulation is useful to determine accurately the capacity, improve farm design, prevent practical problems and fit housing cost. Furthermore, the implementation in extend allows potential users to perform efficiently different kinds of analyses.	simulation;statement of work	Lluís M. Plà-Aragonés	2005	Proceedings of the Winter Simulation Conference, 2005.		stochastic process;agriculture;simulation;engineering;stochastic modelling;simulation modeling;management science;statistics	EDA	7.549642218892736	-5.36847202377274	26479
7da22ea30cb16157b0fe950c282bb2355dc67318	group decision making with incomplete intuitionistic preference relations based on quadratic programming models	quadratic program;group decision making;additive transitivity;completion;intuitionistic preference relation	Quadratic programs are developed to complete an incomplete intuitionistic preference relation (IPR).A parameterized formula is devised to convert normalized interval fuzzy weights into additively consistent IPRs.Two quadratic programs are established to generate interval fuzzy weights from a complete IPR.A procedure is proposed to solve group decision problems with incomplete IPRs. This paper presents a quadratic-program-based framework for group decision making with incomplete intuitionistic preference relations (IPRs). The framework starts with introducing a notion of additive consistency for incomplete IPRs, followed by a two-stage quadratic program model for estimating missing values in an incomplete IPR. The first stage aims to minimize inconsistency of the completed IPR and control hesitation margins of the estimated judgments within an acceptable threshold. The second stage is to find the most suitable estimates without changing the inconsistency level. Subsequently, a parameterized formula is proposed to transform normalized interval fuzzy weights into additively consistent IPRs. Two quadratic programs are developed to generate interval fuzzy weights from a complete IPR. The first model obtains interval fuzzy weight vectors by minimizing the squared deviation between the two sides of the transformation formula. By optimizing the parameter value, the second model finds the best weight vector based on the optimal solutions of the first model. A procedure is then developed to solve group decision problems with incomplete IPRs. A numerical example and a group selection problem for enterprise resource planning software products are provided to demonstrate the proposed models.	quadratic programming	Zhou-Jing Wang;Kevin W. Li	2016	Computers & Industrial Engineering	10.1016/j.cie.2016.01.001	mathematical optimization;discrete mathematics;group decision-making;completion;mathematics;quadratic programming;algorithm	AI	-2.441956523687246	-19.325006004079057	26516
368fb69d96763258e218451a835d432861c73334	runtime analysis of convex evolutionary search	geometric crossover;fitness landscape;runtime analysis;search algorithm;recombination;pure adaptive search;evolutionary algorithm;convex search	Geometric crossover formalises the notion of crossover operator across representations. In previous work, it was shown that all evolutionary algorithms with geometric crossover (but with no mutation) do a generalised form of convex search. Furthermore, it was suggested that these search algorithms could perform well on concave and approximately concave fitness landscapes. In this paper, we study the runtime of a generalised form of convex search on concave fitness landscapes. This is a first step towards linking a geometric theory of representations and runtime analysis in the attempt to (i) set the basis for a more general/unified approach for the runtime analysis of evolutionary algorithms across representations, and (ii) identify the essential matching features of evolutionary search behaviour and landscape topography that cause polynomial performance. Our convex search algorithm optimises LeadingOnes in O(n log n) fitness evaluations, which is faster than all unbiased unary black-box algorithms.	analysis of algorithms;black box;concave function;evolutionary algorithm;polynomial;search algorithm;topography;unary operation	Alberto Moraglio;Dirk Sudholt	2012		10.1145/2330163.2330255	mathematical optimization;combinatorics;fitness landscape;computer science;artificial intelligence;machine learning;evolutionary algorithm;mathematics;recombination;search algorithm	AI	23.99515735789067	-7.656584100540417	26531
a02d316ea3dd1f9c0adadde429c1c4a937504d83	fuzzy soft sets and fuzzy soft lattices	fuzzy soft set;fuzzy soft lattice;soft set;lattice	Abstract The theory of soft sets was introduced by Molodtsov in 1999 in order to deal with uncertainties. In this paper, the notion of fuzzy soft lattice is defined and some related properties are derived, which extends the notion of a fuzzy lattice to include the algebraic structures of soft sets. Then the lattice structure of fuzzy soft lattices are discussed. At last, the concept of fuzzy soft ideal over a lattice is presented and the lattice structures of fuzzy soft ideal over a lattice are discussed.		Yingchao Shao;Keyun Qin	2012	Int. J. Comput. Intell. Syst.	10.1080/18756891.2012.747667	combinatorics;discrete mathematics;topology;fuzzy number;lattice;mathematics;fuzzy set	Theory	-1.1708798407072547	-23.494187806182172	26539
8ce19e635595f091b287d7cf07c7b9740247e603	a stochastic optimization approach for robot scheduling	performance indicator;stochastic petri net;production system;objective function;stochastic optimization;optimality criteria;settore mat 09 ricerca operativa;conflict resolution	In this paper we are concerned with a stochastic optimization approach for determining the optimal job-sequencing in a robot-handler production system, such that the best value of some performance indices which depend on control parameters are obtained. The idea is to reflect the possible control policies of the system in its Stochastic Petri Net model (SPN) and to select a suitable conflict resolution rule whenever the transitions representing the possible actions of the robot are enabled. This rule would depend on a vectorx ∈ ℝn of control parameters, and the problem results in finding the values of those parameters which would be in some sense optimal for the system.	mathematical optimization;robot;scheduling (computing);stochastic optimization	Alexei A. Gaivoronski;E. Messina;Anna Sciomachen	1995	Annals OR	10.1007/BF02031703	stochastic programming;mathematical optimization;simulation;stochastic petri net;computer science;operations management;stochastic optimization;performance indicator;conflict resolution;mathematics;production system	Robotics	7.9979444215982465	-0.8085073828066044	26615
58a8c99a7eae895f61020b1a8f7bbf8774e45438	ethane: a heterogeneous parallel search algorithm for heterogeneous platforms	cluster computing;search algorithm;distributed algorithm;evolutionary computing	In this paper we present Ethane, a parallel search algorithm specifically designed for its execution on heterogeneous hardware environments. With Ethane we propose an algorithm inspired in the structure of the chemical compound of the same name, implementing a heterogeneous island model based in the structure of its chemical bonds. We also propose a schema for describing a family of parallel heterogeneous metaheuristics inspired by the structure of hydrocarbons in Nature, HydroCM (HydroCarbon inspired Metaheuristics), establishing a resemblance between atoms and computers, and between chemical bonds and communication links. Our goal is to gracefully match computers of different power to algorithms of different behavior (GA and SA in this study), all them collaborating to solve the same problem. The analysis will show that Ethane, though simple, can solve search problems in a faster and more robust way than well-known panmitic and distributed algorithms very popular in the literature.	computer;database schema;distributed algorithm;graceful exit;metaheuristic;numerical analysis;parallel algorithm;real life;reference model;search algorithm;search problem;software release life cycle;speedup	Julián Domínguez;Enrique Alba	2011	CoRR		distributed algorithm;simulation;computer cluster;computer science;artificial intelligence;theoretical computer science;machine learning;distributed computing;evolutionary computation;search algorithm	HPC	22.02007342163349	-9.10388023426802	26642
102f2994e321ac9594323a8123c43cefe1398779	congestion games with player-specific constants	abelian group;total order;nash equilibrium;nash equilibria;computational complexity;congestion game;network congestion	We consider a special case of weighted congestion games with player-specific latency functions where each player uses for each particular resource a fixed (non-decreasing) delay function together with a player-specific constant. For each particular resource, the resource-specific delay function and the player-specific constant (for that resource) are composed by means of a group operation (such as addition or multiplication) into a player-specific latency function. We assume that the underlying group is a totally ordered abelian group. In this way, we obtain the class of (weighted) congestion games with player-specific constants; we observe that this class is contained in the new intuitive class of dominance (weighted) congestion games. We focus on pure Nash equilibria for congestion games with player-specific constants; for these equilibria, we study questions of existence, computational complexity and convergence via improvement or best-reply steps of players. Our findings are as follows: – Games on parallel links: • Every unweighted congestion game has an ordinal potential; hence, it has the Finite Improvement Property and a pure Nash equilibrium. • There is a weighted congestion game with 3 players on 3 parallel links that does not have the Finite Best-Reply Property (and hence neither the Finite Improvement Property). • There is a particular best-reply cycle for general weighted congestion games with playerspecific latency functions and 3 players whose outlaw implies the existence of a pure Nash equilibrium. This cycle is indeed outlawed for dominance (weighted) congestion games with 3 players – and hence for (weighted) congestion games with player-specific constants and 3 players. Hence, (weighted) congestion games with player-specific constants and 3 players have a pure Nash equilibrium. – Network congestion games: For unweighted symmetric network congestion games with player-specific additive constants, it is PLS-complete to find a pure Nash equilibrium. – Arbitrary (non-network) congestion games: Every weighted congestion game with linear delay functions and player-specific additive constants (and hence with affine player-specific latency functions) has an ordinal potential; hence, it has the Finite Improvement Property and a pure Nash equilibrium. This work was partially supported by the IST Program of the European Union under contract number IST15964 (AEOLUS). Research supported by the International Graduate School of Dynamic Intelligent Systems (University of Paderborn, Germany). Congestion Games with Player-Specific Constants 1	computational complexity theory;nash equilibrium;network congestion;ordinal data;utility functions on indivisible goods	Marios Mavronicolas;Igal Milchtaich;Burkhard Monien;Karsten Tiemann	2007		10.1007/978-3-540-74456-6_56	mathematical optimization;combinatorics;best response;computer science;mathematics;mathematical economics;algorithm;nash equilibrium	ECom	-4.435398540087721	1.1630487999944321	26667
e92170f993f45c9f511e2884dc5e571f63680e1d	a new approach for sheet nesting problem using guided cuckoo search and pairwise clustering	cutting;nesting;guided local search;clustering;no fit polygon;cuckoo search	The nesting problem is commonly encountered in sheet metal, clothing and shoe-making industries. The nesting problem is a combinatorial optimization problem in which a given set of irregular polygons is required to be placed on a rectangular sheet. The objective is to minimize the length of the sheet while having all polygons inside the sheet without overlap. In this study, a methodology that hybridizes cuckoo search and guided local search optimization techniques is proposed.	cluster analysis;cuckoo search;cutting stock problem	Ahmed Elkeran	2013	European Journal of Operational Research	10.1016/j.ejor.2013.06.020	mathematical optimization;computer science;artificial intelligence;machine learning;cluster analysis;cutting;cuckoo search;guided local search	Vision	19.59428087191917	3.227462345309352	26686
9973e3fe792ff79ef05ad4894ccdadb5a8585ff1	emission mitigation via longitudinal control of intelligent vehicles in a congested platoon	travel time;pollutants;model predictive control;trajectory;traffic platooning;traffic congestion;intelligent vehicles;car following	The effectiveness of vehicle control on emission mitigation and traffic stabilization increases with the percentage (penetration rate) of intelligent vehicles in the congested vehicle platoon. A nonlinear model predictive control (MPC) approach for emission mitigation via longitudinal control of intelligent vehicles in a congested platoon is proposed in this article. Car-following behavior of drivers contributes to oscillations and significant emissions in congested highway traffic. The emergence of intelligent vehicles and modern communication technologies provides an opportunity to reduce adverse impacts of human factors and dynamically control car-following vehicles. To relieve the real-time optimization burden, the authors propose an instantaneous control model which is essentially a simplified MPC approach with a short and identical prediction and control horizon. The proposed vehicle control strategies are tested using a series of simulations and results verify that localized and instantaneous control of a few intelligent vehicles could reduce emissions of a platoon of vehicles. The models are also applied to field trajectory data and the results show that the instantaneous emission optimization model significantly reduces emissions without increasing travel time.		Zhaodong Wang;Xiqun Chen;Yanfeng Ouyang;Meng Li	2015	Comp.-Aided Civil and Infrastruct. Engineering	10.1111/mice.12130	pollutant;simulation;engineering;trajectory;automotive engineering;transport engineering;model predictive control	AI	9.607637315172148	-10.040673690662732	26702
997ccbd19978d4fcb15d3498b2c966e1d8b8d0dc	fuzzy preorders: conditional extensions, extensions and their representations	acyclicity;indicator;fuzzy relation;extension;transitivity;consistency	The crisp literature provides characterizations of the preorders that admit a total preorder extension when some pairwise order comparisons are imposed on the extended relation. It is also known that every preorder is the intersection of a collection of total preorders. In this contribution we generalize both approaches to the fuzzy case. We appeal to a construction for deriving the strict preference and the indifference relations from a weak preference relation, that allows to obtain full characterizations in the conditional extension problem. This improves the performance of the construction via generators studied earlier.	fuzzy logic	José Carlos Rodriguez Alcantud;Susana Díaz	2016	FO & DM	10.1007/s10700-016-9230-3	combinatorics;discrete mathematics;preorder;mathematics;transitive relation;consistency	NLP	0.19098012551266352	-20.454285163421968	26703
5561a44b92cb36b767f425935ffd8ea48a632a6d	optimal bidding in multi-item multislot sponsored search auctions	stochastic modeling;sponsored search;grupo de excelencia;search engine marketing;bid optimization;stochastic optimization;ciencias basicas y experimentales;matematicas;grupo a	We study optimal bidding strategies for advertisers in sponsored search auctions. In general, these auctions are run as variants of second-price auctions but have been shown to be incentive incompatible. Thus, advertisers have to be strategic about bidding. Uncertainty in the decision-making environment, budget constraints, and the presence of a large portfolio of keywords makes the bid optimization problem nontrivial. We present an analytical model to compute the optimal bids for keywords in an advertiser's portfolio. To validate our approach, we estimate the parameters of the model using data from an advertiser's sponsored search campaign and use the bids proposed by the model in a field experiment. The results of the field implementation show that the proposed bidding technique is very effective in practice. We extend our model to account for interactions between keywords, in the form of positive spillovers from generic keywords into branded keywords. The spillovers are estimated using a dynamic linear...	search engine marketing	Vibhanshu Abhishek;Kartik Hosanagar	2013	Operations Research	10.1287/opre.2013.1187	mathematical optimization;economics;search engine optimization;marketing;stochastic optimization;mathematics;microeconomics;stochastic;statistics;commerce	ECom	-0.6103242468013463	-4.263953792833706	26730
6be98a68da6662e72704c6cef80bd199c05c06ef	risk profile and consumer shopping behavior in electronic and traditional channels	consumidor;commerce electronique;comercio electronico;perfil;consommateur;e commerce;pricing;risk aversion;comportement consommateur;profile;economic model;online shopping;fijacion precios;consumer preference;modelo economico;profil risque consommateur;modele economique;shaped piece;consumer;comportamiento consumidor;retail channel switching;online shopping behavior;consumer behavior;consumer risk profile;fixation prix;electronic trade;comportement commercial en ligne	Empirical research of consumer online shopping behavior has generally established that risk associated with online shopping is an important factor when consumers consider whether to shop online or in a brick-and-mortar store. Literature also indicates the importance of information search and product evaluation in consumer purchasing decisions. Building on these results, this paper develops an economic model that captures consumer shopping channel choices based on shopping channel characteristics and consumer risk profiles—risk-neutral or riskaverse. Analyses of results show that after making purchases through one channel, electronic or traditional, risk-averse consumers tend to be more loyal customers than risk-neutral consumers. Observations from the model, confirmed by numerical examples, show that under certain channel characteristic values, the two types of consumers exhibit split channel behavior—riskneutral consumers prefer one channel and risk-averse consumers prefer the other. However, risk-neutral consumers are not always more likely to prefer electronic channel than risk-averse consumers. Implications for retailer pricing strategies are discussed.	information retrieval;mortar methods;numerical analysis;online shopping;purchasing;risk aversion	Alok Gupta;Bo-chiuan Su;Zhiping Walter	2004	Decision Support Systems	10.1016/j.dss.2003.08.002	e-commerce;pricing;risk aversion;consumer;marketing;economic model;advertising;consumer behaviour;commerce	ECom	-2.7390712918316065	-9.53816448004073	26750
127e011187078005d93bfac6aa31103554aa5f86	shared transport systems - a new chance for intermodal freight transport?				Aline Monika Gefeller;Jörn Schönberger	2014		10.1007/978-3-319-23512-7_3	transport engineering	Logic	13.19639687202177	-0.154863307374195	26764
4865d21646abc44cb8c5c8923aec7defb608f6b6	on the limitations of linear models in predicting travel times	travel time;road traffic;predictive models delay intelligent transportation systems cities and towns scalability road safety testing linear regression usa councils road vehicles;statistical analysis;traffic congestion;predict travel time;transportation;linear model;switching model;predict travel time linear model route guidance system driver assistance system switching model;route guidance system;driver assistance system;transportation road traffic statistical analysis	Traffic congestion is growing in major cities, and, consequently, delays are becoming more frequent. Route guidance systems can significantly reduce delays by assisting drivers in finding alternative routes. Due to simplicity and scalability, the linear predictors have been an essential part of route guidance systems in predicting the future travel times. This paper investigates the limitations of linear predictors, and proposes a switching model which can predict travel times with less error. Real world data is used to evaluate the proposed switching predictor and compare the results with commonly used linear predictors.	guidance system;kerrison predictor;network congestion;scalability	Erick J. Schmitt;Hossein Jula	2007	2007 IEEE Intelligent Transportation Systems Conference	10.1109/ITSC.2007.4357735	simulation;engineering;operations management;transport engineering	HPC	9.385880627132096	-9.54333914957424	26821
caad987a2b70d9b13b3e24108cd975e7f991815e	undesirable economic growth via agents' self-protection against environmental degradation	ecological damage;modelizacion;no determinismo;consumption;croissance economique;bien etre economique;acumulacion capital;welfare;agregat;fuel;capital accumulation;accumulation capital;substitut;efecto medio;endommagement;environmental effect;efecto medio ambiente;indeterminacy;deterioracion;degradation environnement;environmental protection;self protection choices;protection environnement;consumo;agregado;modelisation;ciencias economicas;undesirable economic growth;environmental deterioration;combustible;non determinism;non determinisme;medium effect;consommation;crecimiento economico;environmental degradation;sciences economiques;economics;proteccion medio ambiente;damaging;substituto;substitute;effet environnement;modeling;bienestar economico;economic growth;effet milieu;degradacion ecologica;aggregate	Our model analyzes the effects of the interplay between environmental degradation and consumption choices on economic growth dynamics and on economic agents’ welfare. We show that if private goods can be consumed as substitutes for environmental goods, then economic growth may be fueled by environmental deterioration. In this context, undesirable economic growth may occur, characterized by an inverse correlation between aggregate capital accumulation and economic agents’ welfare.	elegant degradation	Angelo Antoci;Marcello Galeotti;Paolo Russu	2007	J. Franklin Institute	10.1016/j.jfranklin.2006.02.037	aggregate;systems modeling;combustibility;environmental engineering;consumption;welfare;environmental degradation;capital accumulation	ECom	-0.4196916994237201	-9.176730003703625	26849
06a34ceeda4f0d683d36318b4620a10790b17cc9	crops selection for optimal soil planning using multiobjective evolutionary algorithms	multiobjective evolutionary algorithm	Farm managers have to deal with many conflicting objectives when planning which crop to cultivate. Soil characteristics are extremely important when determining yield potential. Fertilization and liming are commonly used to adequate soils to the nutritional requirements of the crops to be cultivated. Planting the crop that will best fit the soil characteristics is an interesting alternative to minimize the need of soil treatment, reducing costs and potential environmental damages. In addition, farmers usually look for investments that offer the greatest potential earnings with the least possible risks. According to the objectives to be considered the crop selection problem may become difficult to solve using traditional tools. Therefore, this work proposes an approach based on Multi-objective Evolutionary Algorithms to help in the selection of an appropriate cultivation plan considering five crop alternatives and five objectives simultaneously.	curve fitting;evolutionary algorithm;requirement;selection algorithm	Christian von Lücken;Ricardo Brunelli	2008			computer science	AI	9.620760680020478	-5.4640922997987875	26853
81bcd2ea236241b1b8de2453272fb0eb04ba3b7e	optimal mean-variance problem with constrained controls in a jump-diffusion financial market for an insurer	hjb equation;jump-diffusion process;mean-variance portfolio selection;optimal investment;verification theorem	In this paper, we study the optimal investment and optimal reinsurance problem for an insurer under the criterion of mean-variance. The insurer’s risk process is modeled by a compound Poisson process and the insurer can invest in a risk-free asset and a risky asset whose price follows a jump-diffusion process. In addition, the insurer can purchase new business (such as reinsurance). The controls (investment and reinsurance strategies) are constrained to take nonnegative values due to nonnegative new business and no-shorting constraint of the risky asset. We use the stochastic linear-quadratic (LQ) control theory to derive the optimal value and the optimal strategy. The corresponding Hamilton–Jacobi–Bellman (HJB) equation no longer has a classical solution. With the framework of viscosity solution, we give a new verification theorem, and then the efficient strategy (optimal investment strategy and optimal reinsurance strategy) and the efficient frontier are derived explicitly.		Junna Bi;Junyi Guo	2013	J. Optimization Theory and Applications	10.1007/s10957-012-0138-y	actuarial science	Theory	1.8706388198104809	-2.4732586062672426	26873
970a4fd2b6add66c15dae6aea99fdaaa459ff9e2	mobility allowance shuttle transit (mast) services: mip formulation and strengthening with logic constraints	type system;constraint satisfaction;scheduling problem	We study a hybrid transportation system referred to as Mobility Allowance Shuttle Transit (MAST) where vehicles may deviate from a fixed path consisting of a few mandatory checkpoints to serve demand distributed within a proper service area. In this paper we propose a Mixed Integer Programming (MIP) formulation for the static scheduling problem of a MAST type system. Since the problem is NP Hard, we develop sets of logic cuts, by using reasonable assumptions on passengers' behavior. The purpose of these constraints is to speed up the search for optimality by removing inefficient solutions from the original feasible region. Experiments show the effectiveness of the developed inequalities, achieving a reduction up to 90% of the CPU solving time for some of the instances.		Luca Quadrifoglio;Maged M. Dessouky;Fernando Ordóñez	2008		10.1007/978-3-540-68155-7_43	mathematical optimization;simulation	EDA	15.723974112428044	2.1974634416928085	26879
af305bb2619c9634c56b2bd7330e0d5b34a7b32c	an improved discrete migrating birds optimization for lot-streaming flow shop scheduling problem with blocking		Blocking lot-streaming flow shop (BLSFS) scheduling problems have considerable applications in various industrial systems, however, they have not yet been well studied. In this paper, an optimization model of BLSFS scheduling problems is formulated, and an improved migrating birds optimization (iMBO) algorithm is proposed to solve the above optimization problem with the objective of minimizing makespan. The proposed algorithm utilizes discrete job permutations to represent solutions, and applies multiple neighborhoods based on insert and swap operators to improve the leading solution. An estimation of distribution algorithm (EDA) is employed to obtain solutions for the rest migrating birds. A local search based on the insert neighborhood is embedded to improve the algorithm’s local exploitation ability. iMBO is compared with the existing discrete invasive weed optimization, estimation of distribution algorithm and modified MBO algorithms based on the well-known lot-streaming flow shop benchmark. The computational results and comparison demonstrate the superiority of the proposed iMBO algorithm for the BLSFS scheduling problems with makespan criterion.	flow shop scheduling;program optimization;scheduling (computing)	Yuyan Han;Junqing Li;Hongyan Sang;Tian Tian;Yun Bao;Qun Sun	2018		10.1007/978-3-319-95930-6_79	computer science;artificial intelligence;estimation of distribution algorithm;mathematical optimization;local search (optimization);permutation;machine learning;operator (computer programming);scheduling (computing);job shop scheduling;optimization problem;flow shop scheduling	AI	21.195805696682246	-0.08778700304900863	26918
da64933c4fca2f58dc01959a69ad618319a0dde3	inverse continuous casting problem solved by applying the artificial bee colony algorithm		The paper presents an application of the Artificial Bee Colony algorithm in solving the inverse continuous casting problem consisted in reconstruction of selected parameters characterizing the cooling conditions in crystallizer and in secondary cooling zone. In presented approach we propose to use the bee algorithm for minimization of appropriate functional representing the crucial part of the method.	artificial bee colony algorithm	Edyta Hetmaniok;Damian Slota;Adam Zielonka;Mariusz Pleszczynski	2013		10.1007/978-3-642-38610-7_40	computer science;machine learning;artificial intelligence;swarm intelligence;artificial bee colony algorithm;continuous casting	SE	18.262790365134023	-3.9948929683161487	26925
60a93cb6a1544035d11951215ac732d46d922dd7	wavelength converters placement in all optical networks using particle swarm optimization	optimal solution;wavelength routing;all optical network;search trees;particle swarm optimizer;genetic algorithm;mesh network;evolutionary algorithm;np complete problem	Placement of wavelength converters in an arbitrary mesh network is known to be a NP-complete problem. So far, this problem has been solved by heuristic strategies or by the application of optimization tools such as genetic algorithms. In this paper, we introduce a novel evolutionary algorithm: particle swarm optimization (PSO) to find the optimal solution to the converters placement problem. The major advantage of this algorithm is that does not need to build up a search tree or to create auxiliary graphs in find the optimal solutions. In addition, the computed results show that only a few particles are needed to search the optimal solutions of the placement of wavelength converters problem in an arbitrary network. Experiments have been conducted to demonstrate the effectiveness and efficiency of the proposed evolutionary algorithm. It was found that the efficiency of PSO can even exceed 90% under certain circumstances. In order to further improve the efficiency in obtaining the optimal solutions, four strategic initialization schemes are investigated and compared with the random initializations of PSO particles.	blocking (computing);computation;dspace;erlang (unit);evolutionary algorithm;experiment;genetic algorithm;heuristic (computer science);mathematical optimization;mesh networking;np-completeness;online and offline;particle swarm optimization;phase-shift oscillator;search tree;simulation;telecommunications network;wavelength-division multiplexing	Choon Fang Teo;Yun Cie Foo;Su Fong Chien;Andy Lock Yen Low;Ah Heng You;Gerardo Castañón	2005	Photonic Network Communications	10.1007/s11107-005-1693-z	mathematical optimization;genetic algorithm;np-complete;computer science;theoretical computer science;mesh networking;machine learning;evolutionary algorithm;metaheuristic;computer network	AI	23.980283918867443	0.8128838856620593	26960
ceb308b2aa0b7553b6b3b53462f7dc19091b8c2b	would good become better? an empirical investigation into the herding effect in china online retailing market	uncertainty;information asymmetry;chinese online retailing market;herding effect		online shopping	Alvin Zuyin Zheng;Wangsheng Zhu;Kanliang Wang	2016			information asymmetry;uncertainty;economics;marketing;statistics;commerce	NLP	-2.9976780534039222	-9.89970645638128	26962
bf980fa1463c5028742c9fad2bff522011242c21	games on large random interaction structures: information and complexity aspects	random processes computational complexity game theory graph theory graphs;graph theory;game theory;nonquadratic cost functions large random interaction structures game complexity aspects interacting agents probability measure approximate nash equilibrium random graphs high connectivity assumptions;graphs;computational complexity;games nash equilibrium cost function complexity theory equations random variables bayes methods;random processes	Games on large structures of interacting agents are considered. The participants of the game do not have a full knowledge of the interaction structure or the characteristics of the other players. Instead of that, an ensemble of possible interaction structures as well as a probability measure on that ensemble are assumed to be a common knowledge among the players. Furthermore, we assume that the agents have also local information. Specifically, they know the characteristics of some players, important for them. A new notion of equilibrium, describing approximate Nash equilibrium with high probability, is introduced. A concept of complexity of a game is also defined, as the minimum amount of information needed, in order to play almost optimally. Some special cases are then analyzed. Particularly, games on random graphs are considered and are shown to be simple, under high connectivity assumptions. Games on rings, under quadratic and non quadratic cost functions, are finally studied. Bounds on the complexity of the ring games are derived.	approximation algorithm;computational complexity theory;directed graph;information;interaction;nash equilibrium;random graph;with high probability	Ioannis Kordonis;George P. Papavassilopoulos	2013	52nd IEEE Conference on Decision and Control	10.1109/CDC.2013.6760134	combinatorial game theory;implementation theory;random graph;bayesian game;game theory;minimax;mathematical optimization;combinatorics;discrete mathematics;example of a game without a value;graph theory;folk theorem;repeated game;mathematics;stochastic game;normal-form game;outcome;graph;computational complexity theory;sequential game;equilibrium selection;symmetric game;solution concept;game complexity	Theory	-4.2200033465029145	0.0755666043844331	26988
970654bc5a815c341dcab1caee952c0813b938b0	solving 0-1 knapsack problem by continuous aco algorithm	0 1 knapsack problem;candidate group	This paper presents a continuous ACO approach to solve 0-1 knapsack problem. In this method, groups of candidate values of the components are constructed, and an amount of pheromone is initialised randomly for each candidate value (a real random number between 0.1 and 0.9) in each candidate group. To solve binary knapsack problem for each object a candidate group is constructed where candidate value is either 0 or 1. Each ant selects a value from each group to construct a path or a solution. After certain number of generation, store the best solution in a temporary population. When temporary population size is equal to the number of ants, then temporary population will be considered as initial population by re-initialising fresh set of pheromone. This procedure will continue until the maximum generation (defined) is reached. In experimental section, we compare the results of standard test functions and 0-1 knapsack problem with existing literature.	algorithm;distribution (mathematics);knapsack problem;linear programming;numerical method;path (graph theory);pixel;random number generation;randomness;shortest path problem;transportation theory (mathematics)	Chiranjit Changdar;G. S. Mahapatra;Rajat Kumar Pal	2013	IJCIStudies	10.1504/IJCISTUDIES.2013.057638	continuous knapsack problem;mathematical optimization;computer science;cutting stock problem;change-making problem;knapsack problem;algorithm	AI	24.48206164957922	-0.22080072918112922	27041
650ffc196f072a6ff36e441f7ada86c6b5d73070	k-additivity and c-decomposability of bi-capacities and its integral	engineering;fuzzy set;procesamiento informacion;conjunto difuso;ensemble flou;ingenierie;bi capacity;the bipolar mobius transform;choquet integral;information processing;ingenieria;k additivity;sistema difuso;systeme flou;traitement information;c decomposability;fuzzy system;51b10	k-Additivity is a convenient way to have less complex (bi-)capacities. This paper gives a new characterization of k-additivity, introduced by Grabisch and Labreuche, of bi-capacities and contrasts between the existing characterization and the new one, that differs from the one of Saminger and Mesiar. Besides, in the same way for capacities, a concept of C-decomposability, distinct from the proposal of Saminger and Mesiar, but closely-linked to k-additivity, is introduced for bi-capacities. Moreover, the concept of C-decomposability applies to the Choquet integral with respect to bi-capacities.		Katsushige Fujimoto;Toshiaki Murofushi;Michio Sugeno	2007	Fuzzy Sets and Systems	10.1016/j.fss.2007.03.002	information processing;computer science;artificial intelligence;mathematics;fuzzy set;choquet integral;algorithm;fuzzy control system	Theory	1.2351200170466254	-22.49448471519525	27066
15a694471c12ced35766f43a36a60e66e38c7c32	the pricing strategy of complete pre-ordered merchandise under discounted profit for maximizing	stock out;exponential function;linear functionals;price level;waiting time;mathematical model;profitability;discounted profit;price variability;complete pre ordered situation	When a customer steps into a complete pre-ordered store, he will review the merchandise and consider his demands based on the merchandise price levels and price variability at that point in time. However, after declaring his intention to purchase said merchandise, the store assistant informs him that the merchandise will not be available for a period of time. This is a typical stock-out merchandise scenario in which customers may only place an order for delivery at a later point in time. Therefore, whether or not customer purchases merchandise does not just depend on the price at that moment. It is also influenced by the expected future increase or decrease of the price of the merchandise and the length of time before the store can supply the merchandise. In this study, we will explore how to set price levels at each point in time during the stock-out period in order to maximize the discounted profit after considering the influence of the price level, price variability, and waiting time on customer demand. The main assumption in this study is that customers' potential demand rate function at a given point during the stock-out period is a linear function of the price level and price variability at that point. Also, the ratio function of customers willing to wait for the merchandise is an exponential function of the length of time before the merchandise will be delivered. Constructing a mathematical model that is concrete to discuss the above problem, to derive the optimal price function of the merchandise at each point in time, and to discuss the characteristics of this function are the main parts of this study.		Miao-Sheng Chen;Fu-Chien Tsai	2008	APJOR	10.1142/S0217595908001912	financial economics;economics;marketing;mid price;exponential function;mathematical model;mathematics;microeconomics;price level;stockout;profitability index	ML	1.6791487568991794	-4.981163923317458	27067
fa3374b237b87d48bb4ccd34bb311d288c3007bf	development and performance evaluation of flann based model for forecasting of stock markets	recursive least square;forecasting;least mean square;stock market;performance evaluation;performance index;mean absolute percentage error;stock price;indexation;data transformation;functional link artificial neural network flann;neural network model;artificial neural network	A trigonometric functional link artificial neural network (FLANN) model for short (one day) as well as long term (one month, two months) prediction of stock price of leading stock market indices: DJIA and S&P 500 is developed in this paper. The proposed FLANN model employs the least mean square (LMS) as well as the recursive least square (RLS) algorithms in different experiments to train the weights of the model. The historical index data transformed into various technical indicators as well as macro economic data as fundamental factors are considered as inputs to the proposed models. The mean absolute percentage error (MAPE) with respect to actual stock prices is selected as the performance index to gauge the quality of prediction of the models. Extensive simulation and test results show that the application of FLANN to the stock market prediction problem gives out results which are comparable to other neural network models. In addition the proposed models are structurally simple and requires less computation during training and testing as the model contains only one neuron and one layer. Between the two models proposed the FLANN-RLS requires substantially less experiments to train compared to the LMS based model. This feature makes the RLS-based FLANN model more suitable for online prediction.	performance evaluation	Ritanjali Majhi;Ganapati Panda;Gadadhar Sahoo	2009	Expert Syst. Appl.	10.1016/j.eswa.2008.08.008	econometrics;process performance index;mean absolute percentage error;least mean squares filter;forecasting;computer science;artificial intelligence;machine learning;data transformation;artificial neural network;statistics	Metrics	7.539796795754898	-19.938826738483396	27090
f8a17b15322ed2d7ec1a66b5a9002d9fddd43aee	beyond worst case sensitivity in private data analysis				Abhradeep Thakurta	2016		10.1007/978-1-4939-2864-4_547	case sensitivity;welfare economics;mathematics	Theory	-0.6222311522421776	-2.08384996653506	27126
c1ea2d0cb406171af1eb5427acb376417257f9eb	how to construct left-continuous triangular norms--state of the art	engineering;fuzzy set;procesamiento informacion;conjunto difuso;ensemble flou;triangular norm;ingenierie;information processing;ingenieria;sistema difuso;residuated lattice;systeme flou;traitement information;universal algebra;fuzzy system	Left-continuity of triangular norms is the characteristic property to make it a residuated lattice. Nowadays residuated lattices are subjects of intense investigation in the 2elds of universal algebra and nonclassical logic. The recently known construction methods resulting in left-continuous triangular norms are surveyed in this paper. c © 2003 Elsevier B.V. All rights reserved.	fuzzy set;residuated lattice;scott continuity;set theory;t-norm	Sándor Jenei	2004	Fuzzy Sets and Systems	10.1016/j.fss.2003.06.006	universal algebra;discrete mathematics;information processing;computer science;artificial intelligence;pure mathematics;mathematics;fuzzy set;fuzzy control system	AI	1.2009732675457476	-22.79768233593416	27127
6ed46edf8993d152580d5befe9144fde18bfdcc2	genetic evolution of programs		We present evolutionary approach to program development based on absorption of genetic strings at different metalevels. Introducing the principles of software evolution as processes of semantical stability, expansion and contraction, we continue with an example illustrating how it is possible to integrate metalevel with base level and how this integrated computation works when driven by genetic string of codons, determining decisions for program construction and the result of constructed program execution. Next, we analyse the evolutionary circle at metalevel and we show that mentioned expansion and contraction is possible. The main contribution of the paper is that it breaks classic boundaries between models and programs because formal binding of metalevels and base level have been reached.	automatic programming;computation;computer science;evolutionary algorithm;genetic algorithm;iterative and incremental development;software evolution	Ján Kollár;Emília Pietriková	2014	Central European Journal of Computer Science	10.2478/s13537-014-0214-5	simulation;artificial intelligence;mathematics;algorithm	Logic	24.263089542326345	-9.61602006698736	27145
a5f7a5a8d8051430df28a76e367129cf09c626e9	experimental study of the eligibility traces in complex valued reinforcement learning	eligibility traces;reinforcement learning;complex valued neural network eligibility traces complex valued reinforcement learning;learning artificial intelligence;learning mobile robots intelligent robots robot sensing systems intelligent sensors resonance light scattering intelligent actuators neural networks intelligent agent wheelchairs;complex valued reinforcement learning;complex valued neural network;neural network	Effectiveness of eligibility traces in complex valued reinforcement learning is studied. Complex valued reinforcement learning is a new method inspired by complex valued neural networks. In this study, it is desired that various approaches in the ordinally real valued reinforcement learning are applied to the complex valued reinforcement learning. This paper focuses attention on an experimental study of the eligibility traces. Simulation results infer that there is a possibility of overcoming tight perceptual aliasing with long trace back up.	algorithm;aliasing;artificial neural network;autonomous robot;backup;experiment;machine learning;reinforcement learning;simulation;tracing (software)	Takeshi Shibuya;Shingo Shimada;Tomoki Hamagami	2007	2007 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2007.4413989	error-driven learning;computer science;artificial intelligence;machine learning;learning classifier system;reinforcement learning;artificial neural network	Robotics	18.30917044614131	-22.214152853971868	27146
658a6f7ad25cdb7f22c1b500f1bcf0087b3f5783	compact classification of optimized boolean reasoning with particle swarm optimization	qa mathematics;discretization;boolean reasoning;particle swarm optimization;rough sets theory	Conventional cut selection in Boolean reasoning BR based discretization often produces under-optimistic prime cuts. This is due to the linearity of traditional heuristics in tackling high-dimensional space problem. We proposed a flexible yet compact and holistic solution by incorporating Particle Swarm Optimization PSO into the existing framework. The first challenge is to downsize the search space such that the probability of finding the global optimum is increased. The second task is to reconstruct the present fitness function so as to improve the classification performance of the induction algorithm, which in this case, C4.5. By injecting a filtration phase prior to the cut selection and introducing a tertiary term to the fitness function, the proposed extended BR with PSO EBRPSO discretizer is developed. Based on the evaluation using four real-world datasets i.e.: Heart, Breast, Iris and Wine, it is proven that EBRPSO outperforms the existing discretizers in terms of classification accuracy as well as reduction of the decision rules.	particle swarm optimization	D. F. Sameon;Siti Mariyam Hj. Shamsuddin;Roselina Sallehuddin;Anazida Zainal	2012	Intell. Data Anal.	10.3233/IDA-2012-00559	mathematical optimization;multi-swarm optimization;machine learning;discretization;data mining;mathematics;particle swarm optimization;algorithm	AI	19.87047903672976	-8.047325219058823	27160
050b508dfb1beda9dfae9132b05c26f7388fd3bc	improving performance of dispatch rules for daily scheduling of assembly and test operations	semiconductor assembly and test;machine setup;autosched;reentrant flow;grasp	We investigate the performance of an assembly and test semiconductor facility.New dispatch rules are developed for setting up and assigning lots to machines.The methodology combines metaheuristic logic and discrete event simulation.On-time delivery and throughput benefit greatly from the new rules. In recent years, there has been an increasing effort to improve the performance of semiconductor assembly and test facilities given their critical role in achieving on-time delivery. Using the simulation package AutoSched AP (ASAP) as the analytic tool, the goal of this paper is to show how the logic of intelligent heuristics can be combined with discrete event simulation to evaluate various dispatch rules for machine setup and scheduling in such facilities. The problem addressed is defined by a set of resources that includes machines and tooling, process plans for each product, and four hierarchical objectives: minimize the weighted sum of key device shortages, maximize weighted throughput, minimize the number of machines used, and minimize makespan for a given set of lots in queue.Three new dispatch rules are presented for configuring machines and assigning lots to them in assembly and test facilities. The first gives priority to hot lots containing key devices while using the setup frequency table obtained from our machine optimizer that takes the form of a greedy randomized adaptive search procedure (GRASP). The second embeds the more robust selection features of GRASP in the ASAP model through customization. This allows ASAP to explore a larger portion of the feasible region at each decision point by randomizing machine setups using adaptive probability distributions that are a function of solution quality. The third rule, which is a simplification of the second, always picks the setup for a particular machine that gives the greatest marginal improvement in the objective function among all candidates. The computational analysis showed that the three dispatch rules greatly improved ASAP performance with respect to the four objectives.	dynamic dispatch;scheduling (computing)	Shihui Jia;Jonathan F. Bard;Rodolfo Chacon;John Stuber	2015	Computers & Industrial Engineering	10.1016/j.cie.2015.08.016	mathematical optimization;real-time computing;engineering;artificial intelligence;operations management;operating system;machine learning;grasp;engineering drawing	SE	13.024410508920845	3.927855222228684	27161
816d136b7c5c718d57cab3c8b4b91a49e2c80c24	the undirected capacitated arc routing problem with profits	optimal solution;capacitated arc routing problem;travel time;satisfiability;undirected capacitated arc routing with profits;carrier;profitability;heuristics;auctions in transportation;branch and price	A profit and a demand are associated with each edge of a set of profitable edges of a given graph. A travel time is associated with each edge of the graph. A fleet of capacitated vehicles is given to serve the profitable edges. A maximum duration of the route of each vehicle is also given. The profit of an edge can be collected by one vehicle only that also serves the demand of the edge. The objective of this problem, which is called the undirected capacitated arc routing problem with profits (UCARPP), is to find a set of routes that satisfy the constraints on the duration of the route and on the capacity of the vehicle and maximize the total collected profit. We propose a branch-and-price algorithm and several heuristics. We can solve exactly instances with up to 97 profitable edges. The best heuristics find the optimal solution on most of instances where it is available.	arc routing;graph (discrete mathematics)	Claudia Archetti;Dominique Feillet;Alain Hertz;Maria Grazia Speranza	2010	Computers & OR	10.1016/j.cor.2009.05.005	mathematical optimization;branch and price;heuristics;mathematics;profitability index;satisfiability	Theory	15.927131259018719	3.0974722664075043	27175
d50ba6b2ae8771f38fe4c9a014d475aab88eeee8	flow profile comparison of a microscopic car-following model and a macroscopic platoon dispersion model for traffic simulation	civil engineering computing;digital simulation;road traffic;town and country planning;traffic computer control;us federal highway administration;macroscopic platoon dispersion model;microscopic car-following model;parameter calibration;traffic flow;traffic signal optimization;traffic simulation;traffic-signal-controlled intersection approach;urban planning	"""q ' t + T = F * qt + [(l-F) * q ' t+T-1] where q ' t + T qt = predicted flow rate at t ime interval t + T ; = flow rate of the initial platoon as it dischaxges from the stopline during t ime step t; T = 0.8 t imes the cruise travel t ime on the link (Cruise t ravel t ime is the t ime to t raverse a link assuming no need to slow down or stop for a signal. The value of 0.8 reflects those drivers who travel at a higher speed than the average.); F = 1/(1 + aT), a smoothing factor; and a = an empirically derived constant called the Pla toon Dispersion Factor normally defaulted to a value of 0.35 for typical u rban streets in the U.S. In general, this equat ion est imates the average flow ra te over t ime at which vehicles will arrive at a given point downstream of an intersection. Because drivers are not uniform in thei r behavior or desires, traffic platoons will disperse. An example of simple platoon dispersion is shown below in Figure 1 for two points on a roadway. The first location is at the stopline and shows the flow rate which would occur as the signal cycles from green to red. The second location is at a point fnr ther downstream. The plot of flow rate over t ime is also known as a """"Flow Profile"""". The downstream Flow Profile is characterizcd by aggressive (high speed) drivers leading the main body of traffic and slower drivers spreading the platoon to the r ight (r ight tail shift). 3. T R A F N E T S I M SIMULATION BASICS TRAF-NETSIM is a microscopic, one second timestepping, stochastic s imulat ion model of traffic flow for urban"""	computation;downstream (software development);network congestion;simulation;smoothing	Donald T. Gantz;James R. Mekemson	1990			computer simulation;calibration;simulation;microscopic traffic flow model;computer science;engineering;microscopy;technical report;civil engineering;traffic flow;urban planning;transport engineering;predictive modelling;solid modeling;computational model	Theory	9.836548275516847	-10.335659328916854	27223
e9f10856294b7bbe3b6338857a72653cb8c3a03f	optimizing the heliostat field layout by applying stochastic population-based algorithms		The heliostat field of Solar Central Receiver Systems takes up to 50% of the initial investment and can cause up to 40% of energetic loss in operation. Hence, it must be carefully optimized. Design procedures usually rely on particular heliostat distribution models. In this work, optimization of the promising biomimetic distribution model is studied. Two stochastic population-based optimizers are applied to maximize the optical efficiency of fields: a genetic algorithm, micraGA, and a memetic one, UEGO. As far as the authors know, they have not been previously applied to this problem. However, they could be a good option according to their structure. Additionally, a BruteForce Grid is used to estimate the global optimum and a Pure-Random Search is applied as a baseline reference. Our empirical results show that many different configurations of the distribution model lead to very similar solutions. Although micraGA exhibits poor performance, UEGO achieves the best results in a reduced time and seems appropriate for the problem at hand.	algorithmic efficiency;baseline (configuration management);biomimetics;brute-force attack;computation;control theory;dots per inch;floating-point unit;genetic algorithm;global optimization;loss function;mathematical optimization;memetics;multimodal interaction;optimization problem;optimizing compiler;procedural reasoning system;pylons project;random search;requirement	Nicolas C. Cruz;Juana López Redondo;José Domingo Álvarez;Manuel Berenguel;Pilar M. Ortigosa	2018	Informatica, Lith. Acad. Sci.		mathematical optimization;heliostat;computer science;population	AI	19.770310961336495	-3.9874989910410346	27245
f16a23959590a68ebaf44fdb5834806fa022dc40	modelling of weld-bead geometry and hardness profile in laser welding of plain carbon steel using neural networks and genetic algorithms	qa mathematics;laser welding;counter propagation neural network;back propagation neural network;weld bead quality;genetic algorithm;ts manufactures	An attempt was made to predict weld-bead geometry and its cross-sectional micro-hardness profile produced by laser welding of plain carbon steel (DC05) for a given set of process parameters. Welding was done using ytterbium fibre laser by considering laser power, weld speed and distance of the focal point from the sample surface as the input parameters. Microscopy was used to measure the weld dimensions. Micro-indentation was made to measure the corresponding Vickers’ hardness along the horizontal cross section. Two different models were developed. The first model had mean hardness and weld-bead geometry represented by four geometrical dimensions of the weld (that is, top width, depth, mid-width and heat-affected-zone width at mid-depth) as the modelling outputs. The second model had the hardness profile plot interpolation parameters as the modelling outputs. Two different designs of neural networks were used for process-based modelling, namely counter-propagation neural network (CPNN) and feed-forward ba...	artificial neural network;genetic algorithm	Anurag Singh;David E. Cooper;N. J. Blundell;Dilip Kumar Pratihar;Gregory J. Gibbons	2014	Int. J. Computer Integrated Manufacturing	10.1080/0951192X.2013.834469	structural engineering;genetic algorithm;computer science;artificial intelligence;laser beam welding;forensic engineering;engineering drawing	Robotics	13.067307205796295	-19.98731587991877	27258
cf555f210053cf06ec533383198b20e4d1dd6966	achieving customer service efficiency through web-based help desk system	customer relationship management;knowledge management;help desk;customer service	Abstract: We deal with hierarchical clustering for interval-valued functional data. Functional data is defined as the data which is function, or as the data approximated as a function. Functional clustering is proposed as clustering for functional data. Interval-valued functional data is defined as the functional data whose range corresponding to each value in the domain is interval-valued data. Interval-valued data is especially typical in symbolic data, and also intervalvalued functional data can be considered to be a kind of symbolic data. We propose some new dissimilarity criteria in hierarchical clustering for intervalvalued functional data as the extension of functional clustering method, and apply these criteria to real data.	approximation algorithm;cluster analysis;functional derivative;hierarchical clustering;is functions;web application	Ruey-Shun Chen;Chia-Chen Chen;S.-W. Chang	2003	IJCAT	10.1504/IJCAT.2003.000339	customer to customer;customer relationship management;computer science;knowledge management;marketing;customer intelligence;service desk;customer retention;management;customer advocacy	ML	-4.052136928590308	-22.636407846132663	27261
fe69b5f245d46b30e218ba6f400de4a797d88407	same-day delivery with heterogeneous fleets of drones and vehicles		In this paper, we analyze how drones can be combined with regular delivery vehicles to improve same-day delivery performance. To this end, we present a dynamic vehicle routing problem with heterogeneous fleets. Customers order goods over the course of the day. These goods are delivered either by a drone or by a regular transportation vehicle within a delivery deadline. Drones are faster but have a limited capacity as well as charging times. Vehicles capacities are unlimited but vehicles are slow due to urban traffic. To decide whether an order is delivered by a drone or by a vehicle, we present a policy function approximation based on geographical districting. Our computational study reveals two major implications: First, geographical districting is highly effective increasing the expected number of sameday deliveries. Second, a combination of drone and vehicle fleets may reduce routing costs significantly.	approximation;bellman equation;benchmark (computing);data recovery;predictive failure analysis;prime-factor fft algorithm;unmanned aerial vehicle;vehicle routing problem	Marlin W. Ulmer;Barrett W. Thomas	2018	Networks	10.1002/net.21855	mathematical optimization;mathematics;drone;transport engineering	Mobile	13.540807260955338	0.5401525161038048	27293
868c60d711635c135673b9176aee9c6b5d8feec8	improving greedy best-first search by removing unintended search bias (extended abstract)		Recent enhancements to greedy best-first search (GBFS) improve performance by occasionally adopting a non-greedy node expansion policy, resulting in more exploratory behavior. However, previous exploratory mechanisms do not address exploration within the space sharing the same heuristic estimate (plateau) and the search bias in a breadth direction. In this abstract, we briefly describe two modes of exploration (diversification), which work inter-(across) and intra-(within) plateau, and also introduce IP-diversification, a method combining Minimum Spanning Tree and randomization, which addresses “breadth”-bias instead of the “depth”-bias addressed by the existing methods.	best-first search;diversification (finance);greedy algorithm;heuristic;minimum spanning tree	Masataro Asai;Alex S. Fukunaga	2017			machine learning;artificial intelligence;computer science;data mining;best-first search	AI	20.45252703264136	-9.539132507317225	27334
1e20ca86aa8b8ef3a0876a0185c10bee18f14323	mind the gap: a heuristic study of subway tours	metaheuristics;evaluation of heuristics;one pass heuristics;urban railway;graph cycles	What is the minimum tour visiting all lines in a subway network? In this paper we study the problem of constructing the shortest tour visiting all lines of a city railway system. This combinatorial optimization problem has links with the classic graph circuit problems and operations research. A broad set of fast algorithms is proposed and evaluated on simulated networks and example cities of the world. We analyze the trade-off between algorithm runtime and solution quality. Time evolution of the trade-off is also captured. Then, the algorithms are tested on a range of instances with diverse features. On the basis of the algorithm performance, measured with various quality indicators, we draw conclusions on the nature of the above combinatorial problem and the tacit assumptions made while designing the algorithms.	heuristic	Maciej Drozdowski;Dawid Kowalski;Jan Mizgajski;Dariusz Mokwa;Grzegorz Pawlak	2014	J. Heuristics	10.1007/s10732-014-9252-3	mathematical optimization;simulation;computer science;lin–kernighan heuristic;artificial intelligence;operations research;metaheuristic	Logic	20.181195398880668	2.9536130534370204	27358
caf9e1004664339fc4b8a838c0352cf5f569f453	partitions based computational method for high-order fuzzy time series forecasting	fuzzy logical relations;time variant;fuzzy time series;time invariant;linguistic variables	In this paper, we present a computational method of forecasting based on multiple partitioning and higher order fuzzy time series. The developed computational method provides a better approach to enhance the accuracy in forecasted values. The objective of the present study is to establish the fuzzy logical relations of different order for each forecast. Robustness of the proposed method is also examined in case of external perturbation that causes the fluctuations in time series data. The general suitability of the developed model has been tested by implementing it in forecasting of student enrollments at University of Alabama. Further it has also been implemented in the forecasting the market price of share of State Bank of India (SBI) at Bombay Stock Exchange (BSE), India. In order to show the superiority of the proposed model over few existing models, the results obtained have been compared in terms of mean square and average forecasting errors.	time series	Sukhdev Singh Gangwar;Sanjay Kumar	2012	Expert Syst. Appl.	10.1016/j.eswa.2012.04.039	time-variant system;econometrics;lti system theory;computer science;artificial intelligence;machine learning;statistics	ML	6.842821025381913	-20.62960184336888	27379
24948a90e6086d55e66e1825868632525f604652	decision on economical rail grinding interval for controlling rolling contact fatigue	wear;rolling contact fatigue rcf;rolling contact fatigue;maintenance cost;rail grinding;mathematical modelling	Abstract#R##N##R##N#Rail players around the world have been increasing axle loads to improve the productivity of freight and heavy haul operations. This has increased the risk of surface cracks at curves because of rolling contact fatigue. Rail grinding has been considered an effective process for controlling these cracks and reducing risks of rail breaks. The complexity of deciding the optimal rail grinding intervals for improving the reliability and safety of rails is because of insufficient understanding of the various factors involved in the crack initiation and propagation process. This paper focuses on identifying the factors influencing rail degradation, developing models for rail failures and analyzing the costs of various grinding intervals for economic decision making. Various costs involved in rail maintenance, such as rail grinding, downtime, inspection, rail failures and derailment, and replacement of worn-out rails, are incorporated into the total cost model developed in this paper. Field data from the rail industry have been used for illustration.		G. Chattopadhyay;Venkatarami Reddy;P.-O. Larsson-Kråik	2005	ITOR	10.1111/j.1475-3995.2005.00525.x	mathematical model;mathematics;wear	Robotics	7.647986854327255	-5.133948148612052	27422
a488f378be17254409e250f2722167e7d5ba4b02	fuzzy objects in spaces with fuzzy partitions		A theory of fuzzy objects is derived in the category SpaceFP of spaces with fuzzy partitions, which generalize classical fuzzy sets and extensional maps in sets with similarity relations. It is proved that fuzzy objects in SpaceFP can be characterized by some morphisms in the category of sets with similarity relations. A powerset object functor \({\mathcal {F}}\) in the category SpaceFP is introduced and it is proved that \({\mathcal {F}}\) defines a CSLAT-powerset theory in the sense of Rodabaugh.		Jiri Mockor;Michal Holčapek	2017	Soft Comput.	10.1007/s00500-016-2431-4	mathematical analysis;discrete mathematics;topology;fuzzy mathematics;fuzzy classification;fuzzy number;category of sets;mathematics;fuzzy set;concrete category	ECom	-0.8446481518837475	-23.523438532011056	27473
b4e1430639304ebb587901913b4477b49874b843	prioritized aggregation operators and their applications	silicon;higher order criteria blocks prioritized criteria aggregation operators multicriteria decision problems prioritization relationship higher priority criteria lower priority criteria dependent weights prioritized scoring operator;cybernetics;silicon safety decision making educational institutions indexes barium cybernetics;statistical analysis mathematical operators operations research;multi criteria decision making;barium;operations research;aggregation;mathematical operators;indexes;compensation;statistical analysis;safety;compensation multi criteria decision making aggregation priority;priority	Multicriteria decision problems having a prioritization relationship between the criteria arises in situations in which a decrease in satisfaction to a higher priority criteria can't be readily compensated by an increase in satisfaction to a lower priority criteria. A typical example of this is the relationship between safety and cost. We consider criteria aggregation procedures for use in this kind of situation. We suggest that the prioritization between criteria can be modeled by making the weights associated with a criterion dependent upon the satisfaction of the higher priority criteria. We implement this using a prioritized scoring operator. We show how the lack of satisfaction to a higher order criteria blocks the possibility of compensation by lower priority criteria. We show that in the special case where the prioritization relationship among the criteria satisfies a linear ordering we can use a prioritized averaging operator.	decision problem;lexicographical order	Ronald R. Yager	2012	2012 6th IEEE International Conference Intelligent Systems	10.1109/IS.2012.6335107	operations management;mathematics;management science;operations research	Robotics	-3.194803329323886	-18.67451265037464	27505
021002a852fdc590fbfbad6027cf8342ff2ea975	the smallest grammar problem as constituents choice and minimal grammar parsing	optimal parsing;data discovery;smallest grammar problem;hierarchical structure inference	The smallest grammar problem—namely, finding a smallest context-free grammar that generates exactly one sequence—is of practical and theoretical importance in fields such as Kolmogorov complexity, data compression and pattern discovery. We propose a new perspective on this problem by splitting it into two tasks: (1) choosing which words will be the constituents of the grammar and (2) searching for the smallest grammar given this set of constituents. We show how to solve the second task in polynomial time parsing longer constituent with smaller ones. We propose new algorithms based on classical practical algorithms that use this optimization to find small grammars. Our algorithms consistently find smaller grammars on a classical benchmark reducing the size in 10% in some cases. Moreover, our formulation allows us to define interesting bounds on the number of small grammars and to empirically compare different grammars of small size.	algorithm;approximation;benchmark (computing);context-free grammar;context-free language;data compression;experiment;kolmogorov complexity;mathematical optimization;maxima and minima;mouse genetics project;online and offline;parsing;polynomial;run time (program lifecycle phase);smallest grammar problem;time complexity	Rafael Carrascosa;François Coste;Matthias Gallé;Gabriel G. Infante López	2011	Algorithms	10.3390/a4040262	natural language processing;context-sensitive grammar;id/lp grammar;tree-adjoining grammar;combinatorics;l-attributed grammar;link grammar;parsing expression grammar;grammar induction;machine learning;regular tree grammar;s-attributed grammar;mathematics;context-free grammar;attribute grammar;ambiguous grammar;adaptive grammar;grammar-based code;mildly context-sensitive grammar formalism;algorithm	ML	17.760342400995764	-13.593733917632328	27513
6d5af9486618f1d49c604fc79ea4927963659166	differences in trading and pricing between stock and index options	grupo de excelencia;sentiment;speculation;administracion de empresas;options;volatility smile;economia y empresa;behavior;grupo a	We find that the demand for stock options that increases exposure to the underlying is positively related to the individual investor sentiments and past market returns, whereas the demand for index options is invariant to these factors. These differences in trading patterns are also reflected in the differences in the composition of traders with different types of options---options on stocks are actively traded by individual investors, whereas trades in index options are more often motivated by the hedging demand of sophisticated investors. Consistent with a demand-based view of option pricing, the individual investor sentiments and past market returns are related to time-series variations in the slope of the implied volatility smile of stock options, but they have little impact on the prices of index options. The pricing impact is more pronounced in options with a higher concentration of unsophisticated investors and those with higher delta hedging costs. Our results provide evidence that factors not related to fundamentals also impact security prices.#R##N##R##N#This paper was accepted by Brad Barber, finance.	algorithmic trading;fo (complexity);nl (complexity);speculative execution;time series;traders;volatility	Michael Lemmon;Sophie Xiaoyan Ni	2014	Management Science	10.1287/mnsc.2013.1841	financial economics;speculation;economics;volatility smile;marketing;finance;non-qualified stock option;microeconomics;commerce;behavior	Theory	-1.6395980438192521	-9.019933723214539	27555
c3959fa8ca39e653cde11d9f61f67bf5450e2cfa	evolutionary mcts for multi-action adversarial games		Turn-based multi-action adversarial games are games in which each player turn consists of a sequence of atomic actions, resulting in an extremely high branching factor. Many strategy board, card, and video games fall into this category, for which the current state of the art is Online Evolutionary Planning (OEP) – an evolutionary algorithm (EA) that treats atomic actions as genes, and complete action sequences as genomes. In this paper, we introduce Evolutionary Monte Carlo Tree Search (EMCTS) to tackle this challenge, combining the tree search of MCTS with the sequence-based optimization of EAs. Experiments on the game Hero Academy show that EMCTS convincingly outperforms several baselines including OEP and an improved variant of OEP introduced in this paper, at different time settings and numbers of atomic actions per turn. EMCTS also scales better than any existing algorithm with the complexity of the problem.	academy;branching factor;evolutionary algorithm;linearizability;mathematical optimization;monte carlo method;monte carlo tree search;open educational practices	Hendrik Baier;Peter I. Cowling	2018	2018 IEEE Conference on Computational Intelligence and Games (CIG)	10.1109/CIG.2018.8490403	hero;machine learning;artificial intelligence;evolutionary computation;adversarial system;computer science;monte carlo method;evolutionary algorithm;branching factor;monte carlo tree search	AI	19.485104632736217	-17.792604346002502	27593
6bcfe81fa9a968356ec825f97c98a80a64bbc4e4	vulcan: a monte carlo algorithm for large chance constrained mdps with risk bounding functions		Chance Constrained Markov Decision Processes maximize reward subject to a bounded probability of failure, and have been frequently applied for planning with potentially dangerous outcomes or unknown environments. Solution algorithms have required strong heuristics or have been limited to relatively small problems with up to millions of states, because the optimal action to take from a given state depends on the probability of failure in the rest of the policy, leading to a coupled problem that is difficult to solve. In this paper we examine a generalization of a CCMDP that trades off probability of failure against reward through a functional relationship. We derive a constraint that can be applied to each state history in a policy individually, and which guarantees that the chance constraint will be satisfied. The approach decouples states in the CCMDP, so that large problems can be solved efficiently. We then introduce Vulcan, which uses our constraint in order to apply Monte Carlo Tree Search to CCMDPs. Vulcan can be applied to problems where it is unfeasible to generate the entire state space, and policies must be returned in an anytime manner. We show that Vulcan and its variants run tens to hundreds of times faster than linear programming methods, and over ten times faster than heuristic based methods, all without the need for a heuristic, and returning solutions with a mean suboptimality on the order of a few percent. Finally, we use Vulcan to solve for a chance constrained policy in a CCMDP with over 10 states in 3 minutes.	analysis of algorithms;anytime algorithm;concave function;constraint (mathematics);experiment;exploration problem;heuristic (computer science);linear programming;markov chain;markov decision process;monte carlo algorithm;monte carlo method;monte carlo tree search;norm (social);state space;dbase	Benjamin J. Ayton;Brian C. Williams	2018	CoRR		artificial intelligence;machine learning;monte carlo algorithm;bounding overwatch;mathematical optimization;computer science;state space;markov decision process;heuristics;heuristic;monte carlo tree search;linear programming	AI	21.429732383205657	-14.825057020101243	27606
7bb5a99315e100d3afcd7edf1b23007d142e6377	decentralized stochastic planning with anonymity in interactions	article	In this paper, we solve cooperative decentralized stochastic planning problems, where the interactions between agents (specified using transition and reward functions) are dependent on the number of agents (and not on the identity of the individual agents) involved in the interaction. A collision of robots in a narrow corridor, defender teams coordinating patrol activities to secure a target, etc. are examples of such anonymous interactions. Formally, we consider problems that are a subset of the well known Decentralized MDP (DECMDP) model, where the anonymity in interactions is specified within the joint reward and transition functions. In this paper, not only do we introduce a general model model called D-SPAIT to capture anonymity in interactions, but also provide optimization based optimal and local-optimal solutions for generalizable sub-categories of D-SPAIT. Introduction Decentralized Markov Decision Problem (Dec-MDP) model provides a rich framework to tackle decentralized decisionmaking problems. However, solving a Dec-MDP problem to create coordinated multi-agent policies in environments with uncertainty is NEXP-Hard (Bernstein et al. 2002). Researchers have typically employed two types of approaches to address this significant computational complexity: (1) approximate dynamic programming and policy iteration approaches (Seuken and Zilberstein 2007; Bernstein, Hansen, and Zilberstein 2005); (2) exploit static and dynamic sparsity in interactions (Becker et al. 2004; Nair et al. 2005; Velagapudi et al. 2011; Witwicki and Durfee 2012; Mostafa and Lesser 2012). In this paper, we pursue a third type of approach, where in we exploit anonymity in interactions. This is generalising on the notion of aggregate influences that has previously been considered in existing work (Witwicki and Durfee 2012; Mostafa and Lesser 2012; Varakantham et al. 2009). More specifically, we exploit the fact that in many decentralised stochastic planning problems, interactions between agents (specified as joint transition or reward functions) are not dependent on the identity of the agents involved. Instead they are dependent only on the number of agents involved in the interaction. For instance, in the navigation domains of (Melo and Veloso 2011) and (Varakantham et al. 2009), Copyright c © 2014, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. rewards (and transitions) in narrow corridors are dependent only on the number of agents entering the narrow corridor simultaneously and not on the specific agents. Similarly, in the coordination problem introduced by (Yin and Tambe 2011) for Autonomous Underwater and Surface Vehicles (AUVs and ASVs), the rewards are associated with the number of agents sampling the underwater samples simultaneously and not which specific agents. In fact, most sensor network problems (Kumar, Zilberstein, and Toussaint 2011; Nair et al. 2005) have coordination problems with anonymous interactions, where reward is dependent on the number of agents tracking a region (and not which specific sensors). Finally, in the context of coordinating defenders in security patrolling problems (Shieh et al. 2013), the reward for patrolling a target by multiple agents is dependent on the number of agents patrolling a target. While anonymity in interactions has been considered in the context of competitive games (Roughgarden and Tardos 2002; Varakantham et al. 2012; Ahmed, Varakantham, and Cheng 2012), it has not been considered in the context of decentralized and stochastic cooperative planning and that is the main focus of this paper. Concretely, we first provide a general model called Decentralized Stochastic Planning with Anonymous Interactions (D-SPAIT) to represent anonymity in interactions within the context of the DecMDPs framework. Secondly, we develop an optimization based formulation for solving the general D-SPAIT problems and in reference to this optimization, we prove a key theoretical result regarding the scalability of this formulation. Thirdly, we develop specific and scalable methods for generalizable sub-categories of the D-SPAIT problems and finally, we demonstrate the performance of these approaches on random D-SPAIT problems. In our experimental results, we also compare against Softmax based Flow Update (SMFU) algorithm, which was developed for competitive games and can be adapted to solve D-SPAIT problems.	aggregate data;approximation algorithm;artificial intelligence;computational complexity theory;decision problem;donald becker;dynamic programming;interaction;iteration;markov chain;markov decision process;mathematical optimization;mike lesser;multi-agent system;nexptime;phil bernstein;robot;sampling (signal processing);scalability;sensor;softmax function;sparse matrix;whole earth 'lectronic link	Pradeep Varakantham;Yossiri Adulyasak;Patrick Jaillet	2014			public relations;simulation;knowledge management;political science	AI	19.166303861299923	-15.502713538976353	27617
4897ff14e082c27d17418bde73ee6baec48072fd	a combined, equilibrium model of urban personal travel and goods movements	commodities;shippers;ecoulement trafic;transportation network;transportation supply;transporte pasajero;freight traffic;transport voyageur;red transporte;offre transport;trafic urbain;traffic control;passenger transportation;urban traffic;transport routier;traffic flow;prix;transporte mercaderia;demanda transporte;commodity flow;trafico urbano;transport marchandise;urban areas;oferta transporte;equilibre;commodity flow patterns;consumers;demande transport;travel habits;traffic managers;travel behavior;modele combine;equilibrium;regulation trafic;equilibrio;freight transportation;road transportation;price;transporte por carretera;flujo trafico;transport demand;regulacion trafico;precio;reseau transport	A combined equilibrium model of urban personal travel and goods movements is developed, in which commodity flows are generated by the consumption of a commodity, as part of the conduct of a given, generic urban activity undertaken by individual consumers/travelers. A fundamental feature of the model is the explicit, full representation of the interacting behaviors of commodity consumers/travelers and commodity suppliers/shippers, within the framework of a spatial, competitive economy. Concurrently, passenger and freight flows take place on a common, congestible network, which is also used for general travel. Travelers/consumers are assumed to minimize their costs activity plus travel through their joint choice of an activity site and travel itinerary to it. Activity suppliers also minimize the costs of buying and shipping goods, through their joint choice of wholesaler and freight shipping routes. In both cases, activity costs are perceived randomly, and travel costs are perceived deterministically. Commodity supply meets demand at activity sites. Locational commodity prices are determined endogenously. The transportation network, in which a given link may or may not be shared by private cars and freight trucks, is in user equilibrium, for all users. As a special case, a spatial price equilibrium for the commodity is obtained. It is shown that, under certain conditions, the model always possesses a unique solution. An algorithm for obtaining that solution is described. In conclusion, several areas for further extensions of the model are discussed.		Norbert Oppenheim	1993	Transportation Science	10.1287/trsc.27.2.161	economics;consumer;operations management;traffic flow;transport engineering;travel behavior;economy	Robotics	8.534825471031011	-7.062617047082878	27639
b498465c872e3be649dcaa483ef34d9dd5088d7e	are neural fields suitable for vector quantization?	dynamic neural fields;kernel;learning techniques;dynamic neural fields vector quantization learning techniques;neural fields;prototypes;empirical method;vector quantization prototypes topology machine learning concurrent computing couplings differential equations nonlinear equations design methodology unsupervised learning;vector quantisation data handling learning artificial intelligence;distance measurement;computational modeling;vector quantization;empirical methodology;mathematical model;self organization;vector quantizer;data handling;empirical methodology neural fields vector quantization;learning artificial intelligence;vector quantisation	This paper focuses on the possibility of enabling vector quantization learning techniques into dynamic neural fields, as an attempt to enrich their usage in bio-inspired applications. As mathematical approaches prove rather difficult to propose a practical solution, due to the non-linear character of the field equations, we adopt a different perspective in order to deal with this problem. This consists in simulating the evolution of the field and design an empirical method able to measure its quality. The developed benchmark framework implementing this approach is used to check whether a given field is capable to behave as expected in various situations, in particular those involving self-organization by vector quantization.	benchmark (computing);british informatics olympiad;multimodal interaction;nonlinear system;organizing (structure);self-organization;simulation;tandy video information system;vector quantization	Lucian Alecu;Hervé Frezza-Buet	2008	2008 Seventh International Conference on Machine Learning and Applications	10.1109/ICMLA.2008.21	kernel;self-organization;computer science;theoretical computer science;machine learning;group method of data handling;pattern recognition;mathematical model;mathematics;prototype;empirical research;computational model;vector quantization	Robotics	15.40100161000447	-23.449466835624477	27658
9bcdd09c44f100cf2704defd944c919ab41a70f0	a soft multi-criteria decision-making approach to assessing the goodness of typical reasoning systems based on empirical data	modelizacion;linguistic model;multicriteria analysis;methode empirique;coeficiente correlacion;multi criteria decision making;soft computing;metodo empirico;non linear regression;empirical method;analisis decision;multi criteria analysis;prise decision;validite;decision analysis;zz data;algorithme;modelisation;algorithm;modele linguistique;validity;validez;evaluation criteria;neuro fuzzy system;modelo linguistico;decision;analisis multicriterio;analyse multicritere;decision algorithm;correlation coefficient;toma decision;donnee zimmermann zysno;modeling;zimmermann zysno data;coefficient correlation;analyse decision;algorithme decision;linguistic modelling;algoritmo;empirical research	A soft multi-criteria decision-making system for the assessment of typical decision algorithms applying empiric data was constructed. The evaluation criteria were simplicity, correspondence with human reasoning, correspondence with reality, content validity and residual distribution. By virtue of our soft computing (SC) approach, we were able to computerize the assessments. The decision-making system was used to examine the Zimmermann–Zysno data of tiles by applying both conventional and those based on SC decision algorithms. The conventional algorithms comprised linear and non-linear regression analysis, whereas a fuzzy and a neuro-fuzzy system represented the SC approach. The SC approach seemed to yield better results within the bounds of the given criteria. c © 2001 Elsevier Science B.V. All rights reserved.	algorithm;fuzzy control system;neuro-fuzzy;nonlinear system;reasoning system;soft computing;zimmermann telegram	Vesa A. Niskanen	2002	Fuzzy Sets and Systems	10.1016/S0165-0114(01)00257-3	decision analysis;computer science;artificial intelligence;mathematics;soft computing;empirical research;algorithm;statistics	AI	-0.8196961631046189	-16.368271826379694	27700
c14cb400a0a74db10ab11a2ee631fdf4957c6107	neural networks approach to the detection of weekly seasonality in stock trading	mlp;day of the week anomaly;stock market;statistical significance;statistical method;rbf;investment strategies;stock exchange;artificial neural networks;mean return;sensitivity analysis;seasonality;profitability;artificial neural network;neural network	In this article we investigate the problem of detection the statistically significant dependences of stock trading return, which occur in particular days of the week (usually the first or the last trading day), and which could be important for creating profitable investment strategies. The identifying such days of the week (day-of-the-week effect) is performed by using artificial neural networks. The research results helped to conclude the effectiveness of application of neural networks, as compared to the traditional linear statistical methods for finding stock trading anomalies. The effectiveness of the method was confirmed by exploring impact of different variables to the day-of-the-week effect, evaluation of their influence and sensitivity analysis, and by selecting best performing neural network type. The experimental verification was implemented by using Vilnius Stock Exchange trading data.	neural networks;seasonality	Virgilijus Sakalauskas;Dalia Kriksciuniene	2008		10.1007/978-3-540-88906-9_56	stock exchange;investment strategy;computer science;machine learning;statistical significance;sensitivity analysis;artificial neural network;seasonality;statistics;profitability index	ML	6.158483039344939	-18.668350914375974	27703
d56df66188d3a5a7366fab94f3ccfeb54d04ba3d	optimum component test plans for phased-mission systems	experimental design;optimal test;phased mission systems;generation colonne;optimisation;fiabilidad;reliability;programacion semi infinita;exponential distribution;algorithmique;programmation semi definie;optimizacion;ley exponencial;generacion columna;porcentaje falla;loi exponentielle;plan experiencia;temps minimal;taux defaillance;semi infinite linear programming;component testing;test optimal;semi infinite programming;plan experience;algorithmics;algoritmica;fiabilite;minimum time;linear program;failure rate;optimization;programmation semi infinie;programacion semi definida;tiempo minimo;column generation;semi definite programming	We consider the component testing problem of a system that has to perform a mission consisting of a sequence of stages. Once a stage is over, all failed components of the system are replaced before the next stage starts in order to improve its reliability. The components have exponential life distributions where the failure rates depend on the stage of the mission. We formulate the optimal component testing problem as a semi-infinite linear program. We present an algorithmic procedure to compute optimal test times based on the column generation technique and illustrate with numerical examples.	test plan;unit testing	Orhan Feyzioglu;I. Kuban Altinel;Süleyman Özekici	2008	European Journal of Operational Research	10.1016/j.ejor.2007.01.053	column generation;exponential distribution;mathematical optimization;simulation;linear programming;operations management;failure rate;reliability;mathematics;unit testing;design of experiments;algorithmics	Robotics	7.748713665879304	-0.3388871308579966	27717
2e81231a0a5ac3f487c91a8fa006ba8dd88c92ae	simulation analysis of truck driver scheduling rules	freight handling;modelling;scheduling;simulation;transportation;2004 hours of service rules;north america;customer service;cycle times;fleet utilization;order-to-delivery process;random over-the-road trucking fleets;simulation analysis;truck driver scheduling rules	2004 brought a landmark event in the changes to regulations governing hours of service for truck drivers. This paper describes an effort utilizing modeling and simulation for evaluating the impact of the new 2004 Hours of Service (HOS) rules in scheduling and dispatching one of the largest random over-the-road (OTR) trucking fleets in North America. The model was comprehensive and enterprise-wide in nature, modeling unique order-to-delivery process characteristics for over 120,000 freight lanes and the continuous nature of the driver's work day. Model results provided quantification of the 2004 HOS impact on fleet utilization, cycle times and customer service. Results of the model were used to guide company strategy related to drivers, customers and operations. With five months of actual business performance collected regarding the new HOS in 2004, a post-mortem analysis has provided insight regarding the quality of simulation model forecasts done in 2003.	develop;device driver;scheduling (computing);simulation;libotr	Eric C. Ervin;Russell C. Harris	2004	Proceedings of the 2004 Winter Simulation Conference, 2004.		business model;transport;simulation;systems modeling;truck;cycle time variation;computer science;engineering;simulation modeling;empirical research;operations research;scheduling	HPC	8.736035745802997	-8.31705026829464	27748
bf763584f850ddd7e00fa0c7896268ebe1e41dfb	network pollution games	algorithmic mechanism design;approximation algorithms;planar graphs;pollution control	The problem of pollution control has been mainly studied in the environmental economics literature where the methodology of game theory is applied for the pollution control. To the best of our knowledge this is the first time this problem is studied from the computational point of view. We introduce a new network model for pollution control and present two applications of this model. On a high level, our model comprises a graph whose nodes represent the agents, which can be thought of as the sources of pollution in the network. The edges between agents represent the effect of spread of pollution. The government who is the regulator, is responsible for the maximization of the social welfare and sets bounds on the levels of emitted pollution in both local areas as well as globally in the whole network. We first prove that the above optimization problem is NP-hard even on some special cases of graphs such as trees. We then turn our attention on the classes of trees and planar graphs which model realistic scenarios of the emitted pollution in water and air, respectively. We derive approximation algorithms for these two kinds of networks and provide deterministic truthful and truthful in expectation mechanisms. In some settings of the problem that we study, we achieve the best possible approximation results under standard complexity theoretic assumptions. Our approximation algorithm on planar graphs is obtained by a novel decomposition technique to deal with constraints on vertices. We note that no known planar decomposition techniques can be used here and our technique can be of independent interest. For trees we design a two level dynamic programming approach to obtain an FPTAS. This approach is crucial to deal with the global pollution quota constraint. It uses a special multiple choice, multi-dimensional knapsack problem where coefficients of all constraints except one are bounded by a polynomial of the input size. We furthermore derive truthful in expectation mechanisms on general networks with bounded degree.	apx;approximation algorithm;computation;entropy maximization;graph (discrete mathematics);high-level programming language;network model;ptas reduction;planar graph	Eleftherios Anastasiadis;Xiaotie Deng;Piotr Krysta;Minming Li;Han Qiao;Jinshan Zhang	2016	Algorithmica	10.1007/s00453-018-0435-4	algorithmic mechanism design;mathematical optimization;simulation;computer science;artificial intelligence;approximation algorithm;planar graph	Theory	-2.394586730958195	1.9180758936992095	27775
e26d4d2ed5d9e932271b08fe1bd9a51a6ca0a556	the revised developmental approach to the uncapacitated examination timetabling problem	examination timetabling;biologically inspired algorithms;soft constraints;cell migration;cell biology	The developmental approach is a biologically-inspired method based on cell biology. Previous work applying this approach to the uncapacitated examination timetabling problem revealed areas for improvement. These included the need for a soft constraint measure in the heuristic used to assess the difficulties of allocating examinations and an investigation into the effectiveness of different application rates of cell migration and cell interaction. The study presented in this paper addresses these areas. The performance of the revised system is compared to the previous version and other biologically-inspired methodologies applied to the same problem set, as well as that of methods cited as producing one or more best results for the benchmarks.	benchmark (computing);constrained optimization;heuristic	Nelishia Pillay	2009		10.1145/1632149.1632173	simulation;computer science;artificial intelligence;operations research	Comp.	21.961690929373752	-0.15691736493739386	27779
ba0f923c25019ca81e554e513f2ed6b1ba5d63b7	towards a characterization of trapezoidal membership functions defined on extensive structures	fuzzy set;membership function;fuzzy set theory	In many applications of fuzzy set theory, the membership of an object is not defined directly. One of its attributes (like height, age, weight, . . . ) is first mapped on a real number (often by means of a physical instrument) and a parametric function then maps this real number on a membership degree in some fuzzy set (like ‘tall’, ‘old’, ‘heavy’, . . . ). A very common parametric function is the trapezoidal one. This paper presents some conditions guaranteeing the existence of such a trapezoidal membership function representing the knowledge of an expert.	fuzzy set;map;membership function (mathematics);set theory	Thierry Marchant	2005			fuzzy set operations;fuzzy mathematics;discrete mathematics;defuzzification;fuzzy set;type-2 fuzzy sets and systems;mathematical optimization;fuzzy number;fuzzy classification;membership function;mathematics	Vision	-0.9328184709444284	-22.823563350098453	27789
1d8a72c8af170e141c806b0d94415a72c9ac527c	considering decision maker ideas in product mix problems by goal programming	manufacturing systems;linear programming decision maker ideas product mix problem goal programming constraint theory production planning production control product mix decision manufacturing system;decision maker;decision making theory of constraints product mix;production control;theory of constraints;linear programming;linear program;production planning;goal programming;production planning decision making linear programming manufacturing systems production control;manufacturing system;product mix;production planning and control;throughput linear programming programming mathematical model genetic algorithms manufacturing systems	Theory of Constraints is an approach to production planning and control that emphasizes on the constraints to increase throughput. One of the applications in theory of constraints is product mix decision. This paper presents an alternative approach by using a goal programming to determine the product mix of the manufacturing system. The objective of this paper is to provide a methodology in order to make product mix decision. The key point of the proposed methodology is to allow decision makers to determine the importance of throughput and bottleneck machines via pair-wise comparison. Through an example, the inefficiency of theory of constraints in multiple bottleneck problems has been shown. Comparison of theory of constraints, linear programming and other methods has also been discussed to show the advantages of proposed method.	goal programming;linear programming;throughput	Fahimeh Tanhaie;Nasim Nahavandi	2011	2011 IEEE International Conference on Industrial Engineering and Engineering Management	10.1109/IEEM.2011.6118149	theory of constraints;decision-making;mathematical optimization;constraint programming;economics;systems engineering;engineering;linear programming;operations management;goal programming;mathematics	Robotics	13.015484684660938	-3.70468201566838	27848
67d4cb5f9501b5b9f98f446df43ef12cc83a79b6	trading off rewards and errors in multi-armed bandits		In multi-armed bandits, the most common objective is the maximization of the cumulative reward. Alternative settings include active exploration, where a learner tries to gain accurate estimates of the rewards of all arms. While these objectives are contrasting, in many scenarios it is desirable to trade off rewards and errors. For instance, in educational games the designer wants to gather generalizable knowledge about the behavior of the students and teaching strategies (small estimation errors) but, at the same time, the system needs to avoid giving a bad experience to the players, who may leave the system permanently (large reward). In this paper, we formalize this tradeoff and introduce the ForcingBalance algorithm whose performance is provably close to the best possible tradeoff strategy. Finally, we demonstrate on realworld educational data that ForcingBalance returns useful information about the arms without compromising the overall reward.	coat of arms;convex function;expectation–maximization algorithm;multi-armed bandit;regret (decision theory);sampling (signal processing);software incompatibility;synthetic intelligence;thompson sampling	Akram Erraqabi;Alessandro Lazaric;Michal Valko;Emma Brunskill;Yun-En Liu	2017			simulation;economics;operations management;social psychology	ML	23.366665129684993	-17.397503637200806	27849
365f2a317975bf4bbd31d0aca1cd37534905242c	international asset market, nonconvergence, and endogenous fluctuations	asset market;capital accumulation;overlapping generations model;capital flows;endogenous fluctuations;optimal portfolio;mutual fund	We develop an overlapping generations model with re-tradeable paper assets and capital accumulation to analyze the interaction between the real economy and an international asset market. The world consists of two homogeneous countries, which differ only in their initial levels of capital. Consumers who live for two periods transfer wealth over time and across countries by holding international mutual funds which pay stochastic dividends. The optimal portfolio decisions of consumers do not necessarily induce convergence of incomes between the two countries. Moreover, interaction through the asset market induces endogenous fluctuation of capital flows between the rich and the poor country. © 2007 Elsevier Inc. All rights reserved. JEL classification: E44; F43; O11	quantum fluctuation;tree accumulation	Tomoo Kikuchi	2008	J. Economic Theory	10.1016/j.jet.2007.05.008	cost of capital;financial economics;physical capital;economics;diversification;capital market line;capital intensity;finance;macroeconomics;financial system;microeconomics;consumption-based capital asset pricing model;security market line;capital accumulation;overlapping generations model	AI	-2.483347439122392	-7.2066982035322855	27850
5776707f2f5e0ca592b9dd8fa6a49eef7762bdb9	evolutionary techniques for optimization problems in integrated manufacturing system: state-of-the-art-survey	engineering design;evolutionary technique et;combinatorial optimization problem;manufacturing and processing;layout design ld;flexible manufacturing system fms;advanced planning and scheduling;optimization problem;np hard problem;integrated manufacturing system ims;efficient implementation;advanced planning and scheduling aps;flexible manufacturing system;next generation;mathematical model;process planning;evolutionary algorithm;job shop scheduling problem;manufacturing system;flexible job shop scheduling problem fjsp;multistage process planning mpp;quality management	Integrated manufacturing system (IMS) is a novel manufacturing environment which has been developed for the next generation of manufacturing and processing technologies. It consists of engineering design, process planning, manufacturing, quality management, and storage and retrieval functions. Improving the decision quality in those fields give rise to complex combinatorial optimization problems, unfortunately, most of them fall into the class of NP-hard problems. Find a satisfactory solution in an acceptable time play important roles. Evolutionary techniques (ET) have turned out to be potent methods to solve such kind of optimization problems. How to adapt evolutionary technique to the IMS is very challenging but frustrating. Many efforts have been made in order to give an efficient implementation of ET to optimize the specific problems in IMS. In this paper, we address four crucial issues in IMS, including design, planning, manufacturing, and distribution. Furthermore, some hot topics in these issues are selected to demonstrate the efficiency of ET’s application, such as layout design (LD) problem, flexible job-shop scheduling problem (fJSP), multistage process planning (MPP) problem, and advanced planning and scheduling (APS) problem. First, we formulate a generalized mathematic models for all those problems; several evolutionary algorithms which adapt to the problems have been proposed; some test instances based on the practical problems demonstrate the effectiveness and efficiency of our proposed approach. 2008 Elsevier Ltd. All rights reserved.	automated planning and scheduling;combinatorial optimization;decision quality;engineering design process;evolutionary algorithm;goodyear mpp;job shop scheduling;mathematical optimization;multistage amplifier;np-hardness;next-generation network;optimization problem;scheduling (computing)	Mitsuo Gen;Lin Lin;Haipeng Zhang	2009	Computers & Industrial Engineering	10.1016/j.cie.2008.09.034	optimization problem;mathematical optimization;quality management;simulation;computer science;engineering;operations management;evolutionary algorithm;np-hard;mathematical model;manufacturing engineering	AI	18.89702015176863	-1.7892681655394271	27862
e5ae7bcb6590c31dffd10da70e1a1fffbbc1c67a	a maximum-entropy approach to minimising resource contention in aircraft routing for optimisation of airport surface operations	resource contention;system engineering;optimisation;optimal airport surface operations;aircraft routing;taxiway routing;aircraft gates;aircraft taxiways;assigned runways;measure;information entropy;maximum entropy;industrial engineering	We propose a new application of information entropy and motivate this application with the problem of routing aircraft between their gates and assigned runways for optimising airport surface operations. In this context, the more clustered (i.e., the less scattered) the scheduled uses of taxiway links, the higher temporal resource contention, which in turn results in higher congestion probability and larger amounts of delays, wasted fuel and emissions. A measure of ‘scatteredness’ is needed. The connection between such a measure and entropy lies in the fact that the arrival times of the n aircraft that reach a given taxiway link within a given time interval partition the interval into n + 1 subintervals. The n + 1 corresponding subinterval-length proportions constitute a probability distribution, and the entropy of this distribution can be used to measure the scatteredness of the use pattern. Our numerical experience demonstrates the significant advantage of entropy maximisation in aircraft taxiway routing.	algorithm;entropy (information theory);experiment;high-level programming language;integer programming;linear programming;mathematical optimization;network congestion;numerical analysis;resource contention;routing;scheduling (computing)	Jacob H.-S. Tsao;Agus Pratama	2011	IJIDS	10.1504/IJIDS.2011.043028	simulation;measure;principle of maximum entropy;operations management;operations research;statistics;entropy	Embedded	12.55908323202662	-0.0681987940934249	27891
8de95aa3d1805bbe8bf889ec86cc144a4baf4efc	assessment of data suitability for machine prognosis using maximum mean discrepancy		As more and more data become available for machine prognostic analysis in the big data environment, effective data suitability assessment methods become highly desired to help locate data with sufficient quality for analysis. Driven by this purpose, this paper proposes a novel and systematic methodology for data suitability assessment based on the needs of prognostics and health management (PHM). In this study, the data suitability for PHM is assessed from the aspects of detectability, diagnosability, and trendability, which correspond to the three major tasks of PHM: fault detection, fault diagnosis, and degradation assessment. The proposed methodology is mainly built upon the recent research studies on maximum mean discrepancy in the field of machine learning, which include a family of test statistics that are used to test the difference between two data distributions. The effectiveness of the proposed methodology is demonstrated in diverse industrial applications, which include semiconductors, boring tool degradation, and sensorless drive diagnosis. The results in the case studies indicate that the proposed methodology can be a promising tool to evaluate whether the data under study or the extracted feature set is suitable for PHM tasks.	big data;discrepancy function;elegant degradation;fault detection and isolation;machine learning;semiconductor	Xiaodong Jia;Ming Zhao;Yuan Di;Qibo Yang;Jay Lee	2018	IEEE Transactions on Industrial Electronics	10.1109/TIE.2017.2777383	data visualization;statistical hypothesis testing;engineering;feature extraction;prognostics;big data;fault detection and isolation;reliability engineering	Metrics	13.911388226507967	-15.973399399303027	27925
253aeb4d0bb206b8b2f580683f00545c70fc28ee	or practice - modeling potential demand for supply-constrained drugs: a new hemophilia drug at bayer biological products	modelizacion;forecasting;trademark;prevision demande;variabilidad;industria farmaceutica;demanda terapeutica;marca comercial;decision analysis applications;medicament;marque commerciale;drug demand;demande therapeutique;salud publica;analisis decision;grupo de excelencia;prise de decision;industries;forecasting drug demand;industrie pharmaceutique;decision analysis;therapeutical request;epidemiology;modelisation;ciencias basicas y experimentales;epidemiologia;matematicas;industries pharmaceutical;prevision demanda;sante publique;medicamento;vente;sales;venta;drug;variability;pharmaceutical industry;grupo a;toma decision;variabilite;modeling;analyse decision;applications;public health;pharmaceutical;epidemiologie;demand forecasting	This paper describes the evolution and application of a novel approach for forecasting drug demand in markets where supply limitations have significantly curtailed sales volumes and thus reduced the usefulness of conventional sales-based forecasting methods. This occurs frequently with biological (biotech) drugs. We use methods from decision analysis to explicitly model the variability in epidemiological data together with the variability in treatment modalities to estimate latent therapeutic demand (LTD)---the underlying demand that captures how physicians would prescribe treatment and how patients would comply if ample supplies of drugs were available and affordable. Our approach evolved from efforts to help Bayer Biological Products with strategic decisions regarding its drug for treating hemophilia A, the future of which had been clouded for several years, primarily due to a lack of confidence in demand estimates. Use of the LTD model resulted in a better understanding of the therapeutic needs of the global hemophilia community and helped Bayer make good decisions. We believe this approach is widely applicable to forecasting potential demand for supply-constrained as well as brand-new drugs, and thus can be very useful in helping both drug manufacturers and health-care agencies worldwide to ensure adequate supplies of critical drugs.		Jeffrey S. Stonebraker;Donald L. Keefer	2009	Operations Research	10.1287/opre.1070.0506	systems modeling;epidemiology;public health;demand forecasting;decision analysis;forecasting;marketing;operations management;mathematics;operations research;statistics	Theory	4.369136564667809	-6.223575215615464	27927
f9e159b3b6b2f6d5b2d9f7e94fa0844b64d8ca2c	on-demand re-optimization of integration flows	integration flows;optimization;workload adaptation	Integration flows are used to propagate data between heterogeneous operational systems or to consolidate data into data warehouse infrastructures. In order to meet the increasing need of up-to-date information, many messages are exchanged over time. The efficiency of those integration flows is therefore crucial to handle the high load of messages and to are based on incremental statistic maintenance and periodic cost-based re-optimization. This also achieves adaptation to unknown statistics and changing workload characteristics, which is important since integration flows are deployed for long time horizons. However, the major drawbacks of periodic re-optimization are many unnecessary reoptimization steps and missed optimization opportunities due to adaptation delays. In this paper, we therefore propose the novel concept of on-demand re-optimization. We exploit optimality conditions from the optimizer in order to (1) monitor optimality of the current plan, and (2) trigger directed re-optimization only if necessary. Furthermore, we introduce the PlanOptimalityTree as a compact representation of optimality conditions that enables efficient monitoring and exploitation of these conditions. As a result and in contrast to existing work, re-optimization is immediately triggered but only if a new plan is certain to be found. Our experiments show that we achieve near-optimal re-optimization overhead and fast workload adaptation. & 2014 Elsevier Ltd. All rights reserved.	experiment;mathematical optimization;operational system;optimization problem;overhead (computing);system integration	Matthias Boehm;Dirk Habich;Wolfgang Lehner	2014	Inf. Syst.	10.1016/j.is.2014.03.005	real-time computing;simulation;computer science;operating system;data mining;database;computer security	DB	10.399644894390354	1.889786834028887	27932
687da38db3bade4a700e40b8524ba7d8ad0613ea	stochastic multi-objectives supply chain optimization with forecasting partial backordering rate: a novel hybrid method of meta goal programming and evolutionary algorithms		This study proposes a model for a multi-objective, multi-buyer, multi-vendor, multiproduct and multi-constraint supply chain. The buyers’ demand rates are stochastic variables with known probability distribution functions. The classical (R, Q) inventory control system is used to manage the inventories of all buyers where the lead times are production rate dependent. Shortage is permitted and is partially backordered where the partial back ordering rate is forecasted. The model is considered as a multi-objective integer nonlinear programming problem including cost, service level and lead time objectives and using a novel hybrid method, a hybrid of Meta Goal Programming (MGP) and Firefly Algorithm (FA) are solved. Numerical examples are given to illustrate the proposed method in the study. The results of the study are compared to other hybrid methods of Meta Goal Programming with other evolutionary algorithms such as Bees Algorithms (BA), Particle Swarm Optimization (PSO), Genetic Algorithm (GA) and Simulated Annealing (SA).	approximation algorithm;business architecture;central processing unit;control system;decision theory;evolutionary algorithm;firefly algorithm;genetic algorithm;goal programming;heuristic;inventory control;loss function;metaheuristic;mouse genetics project;nl (complexity);nonlinear programming;nonlinear system;numerical analysis;numerical aperture;numerical method;optimization problem;particle swarm optimization;phase-shift oscillator;recommender system;routing;sl (complexity);scrum (software development);simulated annealing;software release life cycle;stochastic process	Ata Allah Taleizadeh	2017	APJOR	10.1142/S021759591750021X	mathematical optimization;goal programming;nonlinear programming;firefly algorithm;genetic algorithm;metaheuristic;mathematics;evolutionary algorithm;supply chain optimization;multi-objective optimization	AI	17.743087197568855	-1.1170612230460353	27958
1893070609ba51aafe7cff177e2f21032951dc5d	memetic algorithms in constrained optimization		Memetic Algorithms (MAs) are a fairly recent breed of optimization algorithms created through a synergetic coupling of global and local search strategies [615]. While predecessors of MAs, i.e. Genetic Algorithms (GAs) and Evolutionary Algorithms (EAs) have had significant success in solving a number of real life complex optimization problems in the past, their performance can be greatly improved though a hybridization with other techniques [188]. GAs or EAs hybridized with local search strategies are commonly referred as memetic algorithms. These methods are inspired by models of natural systems that combine the evolutionary adaptation of a population with individual learning within the lifetimes of its members. While, the underlying GA/EA provides the ability for exploration, the local search aids in exploitation [492]. The exploitation schemes adopted in MAs include incorporation of heuristics, approximation algorithms, local search algorithms, specialized schemes for recombination etc. An excellent review of memetic algorithms has been presented by Ong, Lim and Chen [689]. The performance of a MA is largely dependent on the correct choice of the local search strategies (memes), identification of the sub-set undergoing local improvements and the convergence criterion used in local search strategies. In this chapter, first, we discuss constrained optimization and provide a brief review of using memetic algorithms in solving Constrained Optimization Problems (ConOPs). The representations and local search approaches used in memetic algorithms in solving different ConOPs are also described and reviewed. We also present two case studies to demonstrate the use memetic algorithms in solving ConOPs. The first case study is designed to solve constrained numerical optimization problems with traditional representation while the next is designed to solve a combinatorial	approximation algorithm;constrained optimization;crossover (genetic algorithm);entity–relationship model;evolutionary algorithm;expanded memory;genetic algorithm;heuristic (computer science);job shop scheduling;local search (optimization);mathematical optimization;meme;memetic algorithm;memetics;population;rate of convergence;real life;scheduling (computing);scott isaacs;search algorithm;sequential quadratic programming;software release life cycle;synergetics (haken)	Tapabrata Ray;Ruhul A. Sarker	2012		10.1007/978-3-642-23247-3_9	constrained optimization;memetic algorithm	AI	22.515304735954615	-4.7537174264371345	28015
9875ec7a7fcc4db33fa438e8321670d0568fdf0f	transferring evolved reservoir features in reinforcement learning tasks	echo state network;full autonomy;source task;reservoir property;target task;inter-task mapping;transfer learning;reservoir feature;rl testbed;initial population;transfer method	The major goal of transfer learning is to transfer knowledge acquired on a source task in order to facilitate learning on another, different, but usually related, target task. In this paper, we are using neuroevolution to evolve echo state networks on the source task and transfer the best performing reservoirs to be used as initial population on the target task. The idea is that any non-linear, temporal features, represented by the neurons of the reservoir and evolved on the source task, along with reservoir properties, will be a good starting point for a stochastic search on the target task. In a step towards full autonomy and by taking advantage of the random and fully connected nature of echo state networks, we examine a transfer method that renders any inter-task mappings of states and actions unnecessary. We tested our approach and that of intertask mappings in two RL testbeds: the mountain car and the server job scheduling domains. Under various setups the results we obtained in both cases are promising.	autonomy;echo state network;job scheduler;mountain car;neuroevolution;nonlinear system;randomness;reinforcement learning;rendering (computer graphics);scheduling (computing);server (computing);stochastic optimization	Kyriakos C. Chatzidimitriou;Ioannis Partalas;Pericles A. Mitkas;Ioannis P. Vlahavas	2011		10.1007/978-3-642-29946-9_22	simulation;computer science;artificial intelligence;machine learning	ML	20.141695335138525	-22.416025025101074	28030
d411a6f489d0d13ff93688455ca6bc59c2b02126	developing a time-cost trading-off model for construction projects in fuzzy environment (case study: weigh in motion system)		Completion time is an important issue on construction projects with limited practical duration usually studied comparing to construction or implementation cost. In this research work, the fuzzy behaviour of cost and time for project activities has been investigated based on predicted completion time followed by developing a mathematical model for making a trade-off approach for project planning. The objective function is to minimise project cost which is defined in discrete and continuous domains considering limited implementation time. According to the constraints, a nonlinear model was developed, so a linearisation technique has been utilised to convert that to linear. In addition, an acceptable range of changes (tolerance) is considered for the project completion time to achieve better solutions followed by making a sensitivity analysis process on results of time-cost trading-off approach. Installing a weigh in motion (WIM) system in the central Iranian road of Isfahan-Naein has been selected as case s...	motion system	Solmaz Salehzadeh;Abbas Mahmoudabadi	2018	IJAOM	10.1504/IJAOM.2018.10015011	engineering;weigh in motion;operations management;project management;reliability engineering;fuzzy logic;project planning	SE	9.39011042436082	0.9233686426240588	28044
cf72b17f667d771e233024ba30a9f63300c6d98c	multi-objective portfolio optimization based on fuzzy genetic algorithm	pareto optimisation;risk analysis;stock markets fuzzy set theory genetic algorithms investment pareto optimisation risk analysis;multi objective;investment combination proportion fuzzy genetic algorithm markowitz portfolio model risk asset liquidity minimum expected investor return rate inequality constraint semigradient fuzzy number penalty factor maximum liquidity minimum portfolio risk multiobjective portfolio optimization model weekly turnover rate weekly return rate stocks pareto optimal solution set risk asset investment proportion;investment;fuzzy set theory;stock markets;risk appetite;portfolios investment optimization mathematical model security genetic algorithms linear programming;fuzzy genetic algorithm;risk appetite multi objective fuzzy genetic algorithm pareto optimality;genetic algorithms;pareto optimality	Based on the Markowitz portfolio model, this paper considered the liquidity of the risk assets, set the minimum expected return rate of investors, and changed the inequality constraint to semi-gradient fuzzy number. Then used the penalty factor to adjust the objective function, taking the maximum liquidity and minimum risk of portfolio as the objectives, we established the Multi-objective portfolio optimization model. Using the weekly return rate and weekly turnover rate data of eight stocks that are typical in several industries, we can get the Pareto optimal solution set of risk assets investment proportion by fuzzy genetic algorithm. Investors can accord to the personal attitude toward return, risk, liquidity to choose better investment combination proportion of risk assets.	entropy maximization;fuzzy number;genetic algorithm;gradient;loss function;mathematical optimization;optimization problem;pareto efficiency;semiconductor industry;social inequality	Huilin Yi;Jianhui Yang	2013	2013 Ninth International Conference on Computational Intelligence and Security	10.1109/CIS.2013.26	investment strategy;actuarial science;risk analysis;investment;computer science;artificial intelligence;modern portfolio theory;machine learning;portfolio optimization;risk appetite	AI	3.8964114089943807	-10.348899375951117	28059
aecf169a1e4da1791e0b2f6345757b206c5728fa	modeling and simulation of railway traffic as a hybrid system	simulation costing modelling optimisation rail traffic railways scheduling;mathematical model atmospheric modeling matlab equations;mathematical model;hybrid system simulation and modeling;atmospheric modeling;cost railway traffic modeling railway traffic simulation hybrid system train arrivals train departures train scheduling people flow people movement people waiting schedule optimization;matlab	Purpose of the paper is to present a Model for the Railway traffic that consists of train arrivals and departures at different stations and also the people walking in stations, getting into trains and alighting. This is a good classical example of Hybrid system for which initially the existing models are looked at to discuss the drawbacks and the needs of models for this problem scenario. The model we consider here has been utilized to simulate the Railway traffic based on a schedule of the Train and People flow in the stations to analyze whether it can work without any conflicts, how it impacts the people movement and waiting, all in a single model and optimization of the schedule. With this, it is possible to calculate the ratio of the cost of running the train to the cost of people waiting and arrive at optimization for the scheduling of the train. All these are done with the assumed figures of people flow both on entry to the station till they get into the train and similarly the people flow from alighting the train till they walk out of the exits.	hybrid system;mathematical optimization;schedule (computer science);scheduling (computing);simulation	M. J. Venkatarangan;Yam Yeung	2014	2014 IEEE International Systems Conference Proceedings	10.1109/SysCon.2014.6819300	simulation;engineering;operations management;transport engineering	Robotics	10.249594427832442	-9.663546935204655	28074
a4b7275fe8e6d528cb0d9b4f5c499d0189aea813	developing a multi-objective, multi-item inventory model and three algorithms for its solution	multi item inventory model and three algorithms for its solution;ommolbanin yousefi mirbahadorgholi aryanezhad seyed jafar sadjadi arash shahin 联合补充问题 多客观的基因算法 边算法 developing a multi objective	We develop a multi-objective model in a multi-product inventory system. The proposed model is a joint replenishment problem (JRP) that has two objective functions. The first one is minimization of total ordering and inventory holding costs, which is the same objective function as the classic JRP. To increase the applicability of the proposed model, we suppose that transportation cost is independent of time, is not a part of holding cost, and is calculated based on the maximum of stored inventory, as is the case in many real inventory problems. Thus, the second objective function is minimization of total transportation cost. To solve this problem three efficient algorithms are proposed. First, the RAND algorithm, called the best heuristic algorithm for solving the JRP, is modified to be applicable for the proposed problem. A multi-objective genetic algorithm (MOGA) is developed as the second algorithm to solve the problem. Finally, the model is solved by a new algorithm that is a combination of the RAND algorithm and MOGA. The performances of these algorithms are then compared with those of the previous approaches and with each other, and the findings imply their ability in finding Pareto optimal solutions to 3200 randomly produced problems.	approximation algorithm;computation;genetic algorithm;hemispherical resonator gyroscope;heuristic (computer science);inventory theory;loss function;mathematical optimization;multi-objective optimization;np-hardness;optimization problem;pareto efficiency;particle swarm optimization;performance;powera;randomness;stochastic process;time complexity;turing test	Ommolbanin Yousefi;Mir-Bahador Aryanezhad;Seyed Jafar Sadjadi;Arash Shahin	2012	Journal of Zhejiang University SCIENCE C	10.1631/jzus.C1100384	mathematical optimization;computer science;operations research	AI	18.34552704524294	0.7616760304529694	28102
4e36382f7aab2e90750db2f41318828699ab19ae	enhancing the accuracy and fairness of human decision making		Societies often rely on human experts to take a wide variety of decisions affecting their members, from jail-or-release decisions taken by judges and stop-and-frisk decisions taken by police officers to accept-or-reject decisions taken by academics. In this context, each decision is taken by an expert who is typically chosen uniformly at random from a pool of experts. However, these decisions may be imperfect due to limited experience, implicit biases, or faulty probabilistic reasoning. Can we improve the accuracy and fairness of the overall decision making process by optimizing the assignment between experts and decisions?rnIn this paper, we address the above problem from the perspective of sequential decision making and show that, for different fairness notions from the literature, it reduces to a sequence of (cons-trained) weighted bipartite matchings, which can be solved efficiently using algorithms with approximation guarantees. Moreover, these algorithms also benefit from posterior sampling to actively trade off exploitation---selecting expert assignments which lead to accurate and fair decisions---and exploration---selecting expert assignments to learn about the expertsu0027 preferences and biases. We demonstrate the effectiveness of our algorithms on both synthetic and real-world data and show that they can significantly improve both the accuracy and fairness of the decisions taken by pools of experts.	algorithm;approximation;fairness measure;matching (graph theory);sampling (signal processing);synthetic intelligence	Isabel Valera;Adish Singla;Manuel Gomez Rodriguez	2018			artificial intelligence;machine learning;computer science;probabilistic logic;sampling (statistics);trade-off;decision-making	ML	20.350639305985386	-16.633308507808824	28183
dc8570fb578cf2c11363a77ecfbaed81f22bb4ff	a coordinated approach to hedge the risks in stochastic inventory-routing problem	assignment problem;long period;garch model;service level;and forward;forward option pricing model;net present value;risk aversion;particle swarm optimization pso;black scholes model;option pricing;decision maker;transport policy;particle swarm optimizer;stochastic demand;transport costs;point of view;particle swarm optimization algorithm;inventory routing problem irp;autoregressive conditional heteroskedasticity;optimal portfolio	The inventory routing problem (IRP) studied in this research involves repeated delivery of products from a depot to a set of retailers that face stochastic demands over a long period. The main objective in the IRP is to design the set of routes and delivery quantities that minimize transportation cost while controlling inventory costs. Traditional IRP focuses on risk-neutral decision makers, i.e., characterizing replenishment policies that maximize expected total net present value, or equivalently, minimize expected total cost over a planning horizon. In this research, for incorporating risk aversion, a hedge-based stochastic inventory-routing system (HSIRS) integrated with Generalized Autoregressive Conditional Heteroskedasticity (GARCH) model and Forward Option Pricing (FOP)model based on Black-Scholes model, from hedge point of view, is proposed to solve the multi-product multi-period inventory routing problem with stochastic demand. Computational results demonstrate the importance of this approach not only to risk-averse decision makers, but also to maximize the net present value at an acceptable service level. As a result, an optimal portfolio (R, s, S) system of product group can be generated to maximize the net present value under an acceptable service level in a given planning horizon. Meanwhile, the target group needed to be served and the relative transportation policy also can be determined accordingly based on the time required to be served as a priori partition to minimize the average transportation costs; hence, the routing assignment problem can be successfully optimized through a Predicting Particle Swarm Optimization algorithm.	routing	Yee Ming Chen;Chun-Ta Lin	2009	Computers & Industrial Engineering	10.1016/j.cie.2008.09.044	financial economics;autoregressive conditional heteroskedasticity;mathematical optimization;economics;marketing;operations management	SE	10.645538950911488	-2.182544393799816	28186
94f2b5436cf9af77bd69696d345f0398d17183d9	a fuzzy extension of analytic hierarchy process based on the constrained fuzzy arithmetic	constrained fuzzy arithmetic;fuzzy pairwise comparison matrices;triangular fuzzy numbers;fuzzy analytic hierarchy process;fuzzy weighted average	The aim of the paper is to highlight the necessity of applying the concept of constrained fuzzy arithmetic instead of the concept of standard fuzzy arithmetic in a fuzzy extension of Analytic Hierarchy Process (AHP). Emphasis is put on preserving the reciprocity of pairwise comparisons during the computations. For deriving fuzzy weights from a fuzzy pairwise comparison matrix, we consider a fuzzy extension of the geometric mean method and simplify the formulas proposed by Enea and Piazza (Fuzzy Optim Decis Mak 3:39–62, 2004). As for the computation of the overall fuzzy weights of alternatives, we reveal the inappropriateness of applying the concept of standard fuzzy arithmetic and propose the proper formulas where the interactions among the fuzzy weights are taken into account. The advantage of our approach is elimination of the false increase of uncertainty of the overall fuzzy weights. Finally, we advocate the validity of the proposed fuzzy extension of AHP; we show by an illustrative example that by neglecting the information about uncertainty of intensity Research has been supported by the project No. GA 14-02424S Methods of operations research for decision support under uncertainty of the Grant Agency of the Czech Republic. B Ondřej Pavlačka ondrej.pavlacka@upol.cz Jana Krejčí jana.krejci@unitn.it Jana Talašová jana.talasova@upol.cz 1 Department of Industrial Engineering, University of Trento, Via Sommarive 9, 38122 Trento, Italy 2 Department of Mathematical Analysis and Applications of Mathematics, Faculty of Science, Palacký University Olomouc, 17. listopadu 12, 771 46 Olomouc, Czech Republic	algorithm;computation;decision support system;fuzzy concept;industrial engineering;interaction;mathematical optimization;nonlinear system;norm (social);numerical analysis;operations research;software release life cycle	Jana Krejcí;Ondrej Pavlacka;Jana Talasová	2017	FO & DM	10.1007/s10700-016-9241-0	fuzzy logic;mathematical optimization;discrete mathematics;fuzzy cognitive map;membership function;defuzzification;adaptive neuro fuzzy inference system;fuzzy transportation;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;artificial intelligence;fuzzy subalgebra;fuzzy number;neuro-fuzzy;machine learning;fuzzy measure theory;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;fuzzy control system	AI	-2.0443480713797153	-20.394478521675843	28192
5c3439863713c352086ec5250d888cf61a7ab046	modeling of chloride concentration effect on reinforcement corrosion	modelizacion;reinforced concrete;acero;metodo analitico;reinforcing steel;concentracion;etude experimentale;simulacion numerica;simulation;reforzamiento;construccion de hormigon;reinforcement;cloruro;structural deterioration and defects;modelisation;chloride ingress;acier;durability;concrete construction;analytical method;automate cellulaire;simulation numerique;renforcement;construction beton;steel;beton arme;methode analytique;ejemplo;corrosive materials;chlorure;reinforcement engineering;concrete structures;modeling;example;estudio experimental;corrosion;concentration;cellular automaton;numerical simulation;automata celular;hormigon armado;chlorides;exemple	The corrosion of reinforcement is one of the major causes of deterioration of reinforced concrete (RC) structures, considerably affecting their durability and reliability. The rate of reinforcement corrosion is governed by, among other factors, the presence of chlorides on the surface of the steel. The assessment of such deteriorating effects necessitates the development of relevant models and the utilization of advanced simulation techniques to enable the probabilistic analysis of concrete structures. In this article an approach for the assessment of the durability and reliability of RC structures under attack from chlorides is introduced. The field of chloride concentration at different locations in the structure (represented in 2D space by chosen longitudinal or cross sections) is modeled as a function of time by a cellular automata (CA) technique. The results of this simulation are then utilized for the assessment of a steel corrosion prognosis using a probabilistic 1D model at chosen points, although the rate of corrosion is based on experimental results. The concentrations of chlorides and pH levels are reflected in this way. The described approach is applied to an illustrative example showing the feasibility of capturing the effect of chloride concentration on the steel ∗To whom correspondence should be addressed. E-mail: vorechovska. d@fce.vutbr.cz. corrosion rate and consequently on the assessment of the service life and/or reliability of the structure.	automata theory;cellular automaton;computer simulation;concrete security;controlled image base;durability (database systems);focused ion beam;numerical analysis;probabilistic analysis of algorithms;reinforcement learning;simulation;software release life cycle;structural element	Dita Vorechovská;Jan Podrouzek;Markéta Chromá;Pavla Rovnaníková;Bretislav Teplý	2009	Comp.-Aided Civil and Infrastruct. Engineering	10.1111/j.1467-8667.2009.00602.x	computer simulation;cellular automaton;structural engineering;reinforcement;simulation;corrosion;systems modeling;computer science;engineering;durability;forensic engineering;concentration	SE	13.52307815863206	-10.629219186638593	28203
178a4a64c45d1fc01dc135c47b2905eee51447f4	adaptive memory algorithm with the covering recombination operator		The adaptive memory algorithm (AMA) is a population-based metaheuristics initially developed in 1995 by Rochat and Taillard. AMA relies on a central memory M and consists in three steps: generate a new solution s from M with a recombination operator, improve s with a local search operator, and use s to update M with a memory update operator. In 1999, Galinier and Hao proposed the GPX recombination operator for the graph coloring problem. In this paper, AMC, a general type of evolutionary algorithm, is formalized and called Adaptive Memory Algorithm with the Covering Recombination Operator. It relies on a specific formulation of the considered problem and on a generalization of the GPX recombination operator. It will be showed that AMC has obtained promising results in various domains, such as graph coloring, satellite range scheduling and project management.	algorithm	Nicolas Zufferey	2015		10.1007/978-3-319-18167-7_14	mathematical optimization;machine learning;mathematics;algorithm	Theory	22.54860922031076	-0.3373678163575776	28207
9fc39878e6bfca2218c451079ecebf292600dd0a	developing game ai agent behaving like human by mixing reinforcement learning and supervised learning		Artificial intelligence (AI) agent created with Deep Q-Networks (DQN) can defeat human agents in video games. Despite its high performance, DQN often exhibits odd behaviors, which could be immersion-breaking against the purpose of creating game AI. Moreover, DQN is capable of reacting to the game environment much faster than humans, making itself invincible (thus not fun to play with) in certain types of games. On the other hand, supervised learning framework trains an AI agent using historical play data of human agents as training data. Supervised learning agent exhibits a more human-like behavior than reinforcement learning agents because of imitating training data. However, its performance is often no better than human agents. The ultimate purpose of AI agents is to entertain human players. A good performance and a humanlike behavior are important factors of the AI agents, and both of them should be achieved simultaneously. This study proposes frameworks combining reinforcement learning and supervised learning and we call then separated network model and shared network model. We evaluated their performances by the game scores and behaviors by Turing test. The experimental results demonstrate that the proposed frameworks develop an AI agent of better performance than human agent and natural behavior than reinforcement learning agents.	artificial intelligence (video games);experiment;humans;hyper-heuristic;immersion (virtual reality);mixed model;network model;performance;reinforcement learning;supervised learning;the invincible;turing test	Shohei Miyashita;Xinyu Lian;Xiao Zeng;Takashi Matsubara;Kuniaki Uehara	2017	2017 18th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)	10.1109/SNPD.2017.8022767	turing test;error-driven learning;network model;machine learning;supervised learning;computer science;artificial neural network;training set;reinforcement learning;artificial intelligence	AI	17.926406474798245	-20.484261635036937	28239
7acd9ec8adf557944ad65d96daab22169b3b7e3f	a new hierarchical approach for mopso based on dynamic subdivision of the population using pareto fronts	pareto optimisation;pareto front;ions;multi objective optimization;pareto dominance;iterative methods;lead;classification algorithms;comparative study;dynamic population;multiobjective optimization;e dominance hierarchical approach mopso pareto fronts pareto dominance multiobjective particle swarm optimization predefined iteration;optimization;pareto fronts;demography;particle swarm optimisation;benchmark testing;dynamic population multiobjective optimization pareto dominance pareto fronts;particle swarm optimisation demography iterative methods pareto optimisation;classification algorithms lead optimization ions benchmark testing	This paper introduces a new hierarchical architecture for multi-objective optimization. Based on the concept of Pareto dominance, the process of implementation of the algorithm consists of two stages. First, when executing a multiobjective Particle S warm Optimization (MOPSO), a ranking operator is applied to the population in a predefined iteration to build an initial archive Using ε-dominance. Second, several runs will be based on a dynamic number of sub-populations. Those populations, having a fixed size, are generated from the Pareto fronts witch are resulted from ranking operator. A comparative study with other algorithms existing in the literature has shown a better performance of our algorithm referring to some most used benchmarks.	algorithm;archive;benchmark (computing);iteration;mathematical optimization;multi-objective optimization;pareto efficiency;particle swarm optimization;population;subdivision surface	Raja Fdhila;Tarek M. Hamdani;Adel M. Alimi	2010	2010 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2010.5641884	statistical classification;mathematical optimization;computer science;multi-objective optimization;machine learning;mathematics;management science	EDA	24.40417383048061	-5.624327219116652	28242
b5a015353b4f0396582d9a8c62d13e3fb4be1fca	grasp and path relinking for the clustered prize-collecting arc routing problem	clustered prize collecting arc routing problem;arc routing;path relinking;grasp	The Clustered Prize-collecting Arc Routing Problem is an arc routing problem where each demand edge is associated with a profit which is collected once if the edge is serviced, independently of the number of times it is traversed. It is further required that if a demand edge is serviced, then all the demand edges of its component are also serviced.#R##N##R##N#This paper presents GRASP and Path Relinking heuristics for the Clustered Prize-collecting Arc Routing Problem. For the constructive phase of the GRASP two different strategies are considered. One of them follows a bottom-up style whereas the other one is a top-down procedure. The best solutions obtained with both strategies are used as elite solutions for the Path Relinking. The results of extensive computational experiments are presented and analyzed.	arc routing;grasp	Julián Aráoz;Elena Fernández;Carles Franquesa	2013	J. Heuristics	10.1007/s10732-011-9183-1	mathematical optimization;static routing;grasp;mathematics	Theory	18.35694328014838	2.328494846068913	28299
2d20d135bcc9ff7b7dc910b6a397afec83b57492	the role of relevance in explanation i: irrelevance as statistical independence	statistical independence	We evaluate current explanation schemes. These are either insufficiently general, or suffer from other serious drawbacks. A domain-independent explanation theory, based on ignoring irrelevant variables in a probabilistic setting, is proposed. Independencebased maximum aposteriori probability (IB-MAP) explanations, an instance of irrelevance-based explanation, has several interesting properties, which provide for simple algorithms for computing such explanations. A best-first algorithm that generates IB-MAP explanations is presented, and evaluated empirically. The algorithm shows reasonable performance for up to medium-size problems on a set of randomly generated belief networks. An alternate algorithm, based on linear systems of inequalities, is discussed. K E Y W O R D S : probabilistic reasoning, abduction, explanation under uncertainty, Bayesian belief networks, relevance	abductive reasoning;algorithm;bayesian network;linear system;procedural generation;randomness;relevance	Solomon Eyal Shimony	1993	Int. J. Approx. Reasoning	10.1016/0888-613X(93)90027-B	independence;econometrics;computer science;mathematics;algorithm;statistics	AI	21.197547182488204	-13.461204657917662	28363
099c76b82b16ca7d9f0a7c7233da1cb4a7c86e7f	dynamic chemical process modelling using a multiple basis function genetic programming algorithm	genetic program;process modelling;dynamic modelling;neural network	In recent years the process industries have been subjected to a change in emphasis with more importance being placed upon factors such as product quality, economic performance and environmental and safety issues. This has lead to an increase in demand for process monitoring, optimisation and advanced control methods. In general, however, the benefits of these techniques can only be fully realised with the aid of an accurate process model.	advanced process control;algorithm;basis function;genetic programming;mathematical optimization;process modeling	Mark P. Hinchliffe;Mark J. Willis;Ming T. Tham	1999			computer science;artificial intelligence;machine learning;process modeling;artificial neural network;algorithm;population-based incremental learning	AI	14.426265247971703	-6.416136613695742	28365
a4f6439e88671b27d2f2cb2c3e5b866072f58677	quality-related fault detection and diagnosis based on total principal component regression model		This paper investigates the issue of quality-related fault detection and diagnosis. A total principal component regression (TPCR) model is build, based on which process variables space is divided into two orthogonal subspaces. Subsequently, two statistical indices with different correlations with output space are designed in each subspace, respectively. An appropriate decision logic is used to determine whether a fault is quality-related or not. Once a fault is detected, it is necessary to explore the cause of the failure. Due to traditional contribution plots often provide inaccurate diagnostic result, this paper introduces an improved method without smearing effect, which is integrated into TPCR model for accurate fault diagnosis. Simulation results demonstrate the effectiveness of the proposed method.	fault detection and isolation;principal component analysis;principal component regression;simulation	Guang Wang;Jianfang Jiao	2018	IEEE Access	10.1109/ACCESS.2018.2793281	linear subspace;decision table;distributed computing;principal component analysis;computer science;fault detection and isolation;principal component regression;subspace topology;correlation;pattern recognition;artificial intelligence	EDA	13.437327322486093	-14.499909038137996	28393
e92594393c3f9a21369b95a69a108e7ad5b498a0	mathematical model and solution for land-use crop planning with cooperative work		Most farm work planning for land-use crops such as sugarcane belongs to flexible flow shop scheduling if neglecting cooperative work and other specific constraints. Because the conventional approaches to the flexible flow shop scheduling cannot formulate these specific constraints, we require a new approach for solving land-use crop planning problems that considers cooperative work. This paper describes a detailed mathematical model and a hybrid algorithm for solving the model, in which many practical constraints are taken into account, including cooperative work, optimum time windows, waiting time between operations, and moving time. The hybrid algorithm uses meta-heuristic simulated annealing and a mixed integer programming solver in Gurobi. In order to obtain good schedules in a reasonable time, we adopt a strategy of fixing partial work sequences in the simulated annealing procedure and optimizing the partial schedule using the solver. The results of the evaluation computation show that the proposed model is operative for the practical constraints, and that the hybrid algorithm is adaptable to scheduling computation. The strategy of fixing partial work sequences is applicable to reducing computation times for large-sized land-use crop planning problems.	computation;flow shop scheduling;gurobi;heuristic;hybrid algorithm;integer programming;linear programming;mathematical model;microsoft windows;scheduling (computing);simulated annealing;solver	Senlin Guan;Takeshi Shikanai;Morikazu Nakamura;Koichiro Fukami	2017	2017 6th IIAI International Congress on Advanced Applied Informatics (IIAI-AAI)	10.1109/IIAI-AAI.2017.110	mathematical optimization;hybrid algorithm;scheduling (computing);simulated annealing;computation;management science;schedule;integer programming;flow shop scheduling;computer science;solver	Robotics	14.661209911352513	3.3105306190264963	28401
021c6b186a5cada9d0fdafdbb78e08ccb75be319	local search with constraint propagation and conflict-based heuristics	conflict detection;constraint propagation;search space;search algorithm;constraint satisfaction;repair methods;conflict based search;scheduling problem;constraint satisfaction problem;hybrid search algorithm;local search	Search algorithms for solvingCSP (Constraint Satisfaction Problems) usually fall into one of two main families: local search algorithms and systematic algorithms. Both familie s have their advantages. Designing hybrid approaches seems promising since those advantages may be combined into a single approach. In this paper, we present a new hybrid technique. It performs a local search over partial assignments instead of complete assignments, and uses filtering techniques and conflict-based techniques to efficiently guide the search. This new technique benefits from both classical approaches: a priori pruning of the search space from filtering-based search and possible repair of early mistakes from local search. We focus on a specific version of this technique: tab decision-repair . Experiments done on open-shop scheduling problems show that our approach competes well with the best highly specialized algorithms.  2002 Elsevier Science B.V. All rights reserved.	constraint satisfaction;filter (signal processing);heuristic (computer science);local consistency;local search (constraint satisfaction);local search (optimization);open-shop scheduling;scheduling (computing);search algorithm;software propagation	Narendra Jussien;Olivier Lhomme	2000		10.1016/S0004-3702(02)00221-7	beam search;job shop scheduling;mathematical optimization;constraint satisfaction;beam stack search;tabu search;constraint learning;computer science;local search;theoretical computer science;hill climbing;machine learning;min-conflicts algorithm;brute-force search;iterated local search;mathematics;incremental heuristic search;iterative deepening depth-first search;best-first search;combinatorial search;constraint satisfaction problem;difference-map algorithm;hybrid algorithm;local consistency;guided local search;search algorithm	AI	23.346799663967072	3.5223483385121845	28405
3cdb18e0ffbd1086835b5a21560d630fa4e10b23	a genetic algorithm based control strategy for the energy management problem in phevs		Genetic algorithms have been applied to various optimization problems in the past. Our library GeneiAL implements a framework for genetic algorithms specially targeted to the area of hybrid electric vehicles. In a parallel hybrid electric vehicle (PHEV), an internal combustion engine and an electrical motor are coupled on the same axis in parallel. In the area of PHEVs, genetic algorithms have been extensively used for the optimization of parameter tuning of control strategies. We use GeneiAL to control the torque distribution between the engines directly. The objective function of this control strategy minimizes the weighted sum of functions that evaluate the fuel consumption, the battery state of charge, and drivability aspects over a prediction horizon of fixed finite length. We analyze the influence of these weights and dierent configurations for the genetic algorithm on the computation time, the convergence, and the quality of the optimization result. For promising configurations, we compare the results of our control strategy with common control strategies.	genetic algorithm	Johanna Nellen;Benedikt Wolters;Lukas Netz;Sascha Geulen;Erika Ábrahám	2015			control engineering;simulation;meta-optimization;engineering;operations management	EDA	17.202748208328913	-3.880981393824748	28438
ca0e68aefd1d208447e86ac80b31143939b13802	an analysis of communication policies for homogeneous multi-colony aco algorithms	traveling salesman problem;distributed memory;parallel algorithm;ant colony optimization;communication topologies;fixed effects model;parallelization;analysis of variance;parallel architecture;local search	The increasing availability of parallel hardware encourages the design and adoption of parallel algorithms. In this article, we present a study in which we analyze the impact that different communication policies have on the solution quality reached by a parallel homogeneous multi-colony ACO algorithm for the traveling salesman problem. We empirically test different configurations of each algorithm on a distributed-memory parallel architecture, and analyze the results with a fixed-effects model of the analysis of variance. We consider several factors that influence the performance of a multi-colony ACO algorithm: the number of colonies, migration schedules, communication strategies on different interconnection topologies, and the use of local search. We show that the importance of the communication strategy employed decreases with increasing search effort and stronger local search, and that the relative effectiveness of one communication strategy versus another changes with the addition of local search. 2010 Elsevier Inc. All rights reserved.	distributed memory;fixed effects model;interconnection;local search (optimization);parallel algorithm;parallel computing;travelling salesman problem	Colin Twomey;Thomas Stützle;Marco Dorigo;Max Manfrin;Mauro Birattari	2010	Inf. Sci.	10.1016/j.ins.2010.02.017	mathematical optimization;ant colony optimization algorithms;distributed memory;analysis of variance;computer science;fixed effects model;local search;theoretical computer science;distributed computing;parallel algorithm;travelling salesman problem	AI	21.89660093046742	2.6473411117229917	28443
49f60e692125aa3a30c957961640f0ad3bf9c9ff	data mining with neural networks for wheat yield prediction	yield prediction;neural networks;information technology;data mining;seasonality;industrial application;precision agriculture;prediction;neural network	Precision agriculture (PA) and information technology (IT) are closely interwoven. The former usually refers to the application of nowadays' technology to agriculture. Due to the use of sensors and GPS technology, in today's agriculture many data are collected. Making use of those data via IT often leads to dramatic improvements in efficiency. For this purpose, the challenge is to change these raw data into useful information. In this paper we deal with neural networks and their usage in mining these data. Our particular focus is whether neural networks can be used for predicting wheat yield from cheaply-available in-season data. Once this prediction is possible, the industrial application is quite straightforward: use data mining with neural networks for, e.g., optimizing fertilizer usage, in economic or environmental terms.	artificial neural network;data mining	Georg Ruß;Rudolf Kruse;Martin Schneider;Peter Wagner	2008		10.1007/978-3-540-70720-2_4	prediction;computer science;data science;machine learning;data mining;precision agriculture;operations research;artificial neural network;seasonality;statistics	ML	7.2774163952677915	-18.84224907221743	28464
15aa277b1054cdcdf7fc018e3a3abe2df7a1691b	temporal-difference networks for dynamical systems with continuous observations and actions	td network representation;predictive representation;predictive state representation;continuous observation;observable dynamical system;temporal-difference network;noisy continuous dynamical system;continuous dynamical system;dynamical system;td network;td method	y3 y7 y8 a a Temporal-difference (TD) networks • 1 are a class of predictive state representation (PSR)2 which represent state in partially observable dynamical systems in terms of a set of predictions about future observations. In a TD network these predictions, or questions, are represented by a question • network (see figure below). Each node in the network represents a prediction about the value a specific • observation will take some number of time steps in the future, possibly conditioned on one or more actions. For example, node y • 1 below makes a prediction about an observation one time step in the future, either unconditionally (left) or conditioned on action a (right). The values of all the nodes in the network at a given time constitute the state • of the system, and the values are computed by an answer network. The answer network is a function approximator that maps the current predic• tions, next action, and next observation to a new set of predictions. Links in the question network denote node targets, which specify the quantity • a node tries to predict. Some targets are in terms of other predictions, and so TD learning techniques can be used to update the parameters of the answer network at each time step. Eligibility traces can be used as in traditional TD learning to make more effi• cient use of data, and in certain systems must be used to guarantee learning.3	dynamical system;map;partially observable system;predictive state representation;temporal difference learning;tracing (software)	Christopher M. Vigorito	2009			linear dynamical system;discrete mathematics;dynamical systems theory;random dynamical system;machine learning;control theory;mathematics	ML	19.760603449771978	-23.37272387433006	28494
3fe3becc27961815892124d80a09f9604acbe493	soft rough approximation operators and related results	期刊论文	Soft set theory is a newly emerging tool to deal with uncertain problems. Based on soft sets, soft rough approximation operators are introduced, and soft rough sets are defined by using soft rough approximation operators. Soft rough sets, which could provide a better approximation than rough sets do, can be seen as a generalized rough set model. This paper is devoted to investigating soft rough approximation operations and relationships among soft sets, soft rough sets, and topologies. We consider four pairs of soft rough approximation operators and give their properties. Four sorts of soft rough sets are investigated, and their related properties are given. We show that Pawlak’s rough set model can be viewed as a special case of soft rough sets, obtain the structure of soft rough sets, give the structure of topologies induced by a soft set, and reveal that every topological space on the initial universe is a soft approximating space.	approximation;rough set;set theory	Zhaowen Li;Bin Qin;Zhangyong Cai	2013	J. Applied Mathematics	10.1155/2013/241485	combinatorics;mathematical analysis;discrete mathematics;rough set;mathematics	AI	-1.2886299614193932	-23.35898690404275	28495
d17451f8c1ee87e1aefa0a60723fa2d900ece564	pricing and inventory strategy of dual-channel supply chain under random demand and retailer's capital constraint	dual channel;random demand;deferred payment;pricing strategy;capital constraint	A dual-channel supply chain system composed of one manufacturer and one retailer is considered in this paper, which existed uncertainty demands in both distribution channels and capital constraint on retailer. It set up the profit model of manufacturer dominated dual-channel supply chain system, studied to the optimal pricing and inventory strategies of decentralized and centralized supply chain, obtained the optimal pricing and inventory strategy of the two kinds of supply chain system. The analysis to the optimal solution indicated that the demand uncertainty of two distribution channels, deferred-payment rate etc, are all have certain influence relation on the pricing strategies of manufacturer and retailer. Numerical experiment has showed the effectiveness of the conclusions. Pricing and Inventory Strategy of Dual-channel Supply Chain under Random Demand and Retailer’s Capital Constraint	centralized computing;multi-channel memory architecture	Hehua Fan;Yongwei Zhou	2015	IJCINI	10.4018/IJCINI.2015010103	computer science;multi-channel memory architecture	AI	-0.026564887028550734	-5.203024179442654	28508
b6f05c661a8062eb0226c44f4d7c758f95b40176	alternative pheromone laying strategy — an improvement for the aco algorithm		This paper gives a brief overview of the current state in the ant colony optimization (ACO) field of study. Furthermore, it introduces an alternative pheromone laying strategy for the ACO algorithm. In the paper, the newly introduced strategy is implemented, tested on a model problem and compared with the classical approach. A parameterized problem space generator has been introduced. The generator generates graphs along which ants are allowed to move freely on the Y axis, but constrained to increment the current value on the X axis by one with each move. In this way, a dynamic decision making optimization problem with the goal of minimizing the path from an arbitrary starting node to an arbitrary finish node has been simulated. Using the ACO algorithm, the generated problems are being solved with the classical pheromone laying approach and the modified approach, introduced in this paper. The obtained results unequivocally indicate that the introduced modification has the potential to serve as an improvement for the ACO algorithm in general.	algorithm;ant colony optimization algorithms;apache axis;iteration;mathematical optimization;optimization problem;parameterized complexity;problem domain;real life;routing	Kemal Lutvica;Samim Konjicija	2017	2017 XXVI International Conference on Information, Communication and Automation Technologies (ICAT)	10.1109/ICAT.2017.8171607	ant colony optimization algorithms;algorithm design;parameterized complexity;dynamic decision-making;laying;algorithm;computer science;graph;optimization problem	EDA	23.13783984621099	-0.9038191123588951	28557
4cf3b7bd10f3ae45f1ba7c55040ff761a58a93ed	an optimal deterministic control policy of servers in front and back rooms with a variable number of switching points and switching costs	queuing theory;exact solution;optimization;markov decision process;wang jiamin 控制服务器 切换点 客房 背面 成本 可变 服务设施 工人 an optimal deterministic control policy of servers in front and back rooms with a variable number of switching points and switching costs;switching cost;queuing control	In this paper we consider a retail service facility with cross-trained workers who can perform operations in both the front room and back room. Workers are brought from the back room to the front room and vice versa depending on the number of customers in the system. A loss of productivity occurs when a worker returns to the back room. Two problems are studied. In the first problem, given the number of workers available, we determine an optimal deterministic switching policy so that the expected number of customers in queue is minimized subject to a constraint ensuring that there is a sufficient workforce to fulfill the functions in the back room. In the second problem, the number of workers needed is minimized subject to an additional constraint requiring that the expected number of customers waiting in queue is bounded above by a given threshold value. Exact solution procedures are developed and illustrative numerical examples are presented.	numerical analysis;regular expression	JiaMin Wang	2009	Science in China Series F: Information Sciences	10.1007/s11432-009-0130-9	markov decision process;mathematical optimization;simulation;computer science;queueing theory	ML	4.502283138174491	-0.27606407756163637	28572
bdc20fea18bf38b472d65a8a8e842fa1e00edc61	distributed methods for solving the security-constrained optimal power flow problem	conventional approach;load flow;large-scale nonconvex nonlinear programming;security-constrained optimal power flow problem;power system;nonlinear programming;power generation dispatch;power system security;direction method;decomposition algorithms;electricity generation resources;concave programming;objective function value;precontingency constraints;post-contingency constraints;classical optimal power flow;optimal power flow;available electricity generation resource;scopf problem;security-constrained optimal power flow;benders cut;distributed methods;power generation reliability;power systems;electricity generation;economics;optimization;vectors;objective function;constrained optimization;mathematical model;power flow;generators;direct method	The optimal power flow is the problem of determining the most efficient, low-cost and reliable operation of a power system by dispatching the available electricity generation resources to the load on the system. Unlike the classical optimal power flow problem, the security-constrained optimal power flow (SCOPF) problem takes into account both the pre-contingency (base-case) constraints and post-contingency constraints. In the literature, the problem is formulated as a large-scale nonconvex nonlinear programming. We propose two decomposition algorithms based on the Benders cut and the alternating direction method of multipliers for solving this problem. Our algorithms often generate a solution, whose objective function value is smaller than conventional approaches.	algorithm;augmented lagrangian method;benders decomposition;contingency plan;display resolution;distributed algorithm;flow network;loss function;nonlinear programming;operating point;optimization problem	Dzung T. Phan;Jayant Kalagnanam	2012	2012 IEEE PES Innovative Smart Grid Technologies (ISGT)	10.1109/ISGT.2012.6175679	nonlinear programming;electricity generation;electric power system;minimum-cost flow problem;constrained optimization;multi-commodity flow problem;control engineering;mathematics;mathematical optimization	EDA	5.768663259435421	3.5276123723735022	28635
0063e13da2eb06317d3d12ba82d73841ee4270e5	wasserstein distance and the distributionally robust tsp		Recent research on the robust and stochastic travelling salesman problem and the vehicle routing problem has seen many different approaches for describing the region of ambiguity, such as taking convex combinations of observed demand vectors or imposing constraints on the moments of the spatial demand distribution. One approach that has been used outside the transportation sector is the use of statistical metrics that describe a distance function between two probability distributions. In this paper, we consider a distributionally robust version of the Euclidean travelling salesman problem in which we compute the worst-case spatial distribution of demand against all distributions whose Wasserstein distance to an observed demand distribution is bounded from above. This constraint allows us to circumvent common overestimation that arises when other procedures are used, such as fixing the center of mass and the covariance matrix of the distribution. Numerical experiments confirm that our new approach is useful as a decision support tool for dividing a territory into service districts for a fleet of vehicle when limited data is available.	approximation algorithm;attempt;best, worst and average case;decision support system;dopamine;entropy maximization;estimated;expectation maximization algorithm;experiment;numerical linear algebra;set tsp problem;thrombospondins;travelling salesman problem;vehicle routing problem	John Gunnar Carlsson	2018	Operations Research	10.1287/opre.2018.1746		ML	14.74793682266003	-1.0714646807892023	28670
9bdba50a16f6f9ce164a4e4fdc977e41a2796536	application of the nested rollout policy adaptation algorithm to the traveling salesman problem with time windows	nested rollout policy adaptation;communication conference;trav eling salesman problem with time windows;nested monte carlo;traveling salesman problem with time windows	In this paper, we are interested in the minimization of the travel cost of the traveling salesman problem with time windows. In order to do this minimization we use a Nested Rollout Policy Adaptation (NRPA) algorithm. NRPA has multiple levels and maintains the best tour at each level. It consists in learning a rollout policy at each level. We also show how to improve the original algorithm with a modified rollout policy that helps NRPA to avoid time windows violations.	algorithm;ambiguous name resolution;experiment;genetic algorithm;local optimum;microsoft windows;travelling salesman problem	Tristan Cazenave;Fabien Teytaud	2012		10.1007/978-3-642-34413-8_4	simulation;computer science;operations research	ML	18.862992695300047	2.1855217012167385	28671
13ebaf622d707af933a6c26ce1892318055f88e9	stock index trend analysis based on signal decomposition	adaptive fourier decomposition;rbf neural network;stock index trend forecasting;the dow theory			Liming Zhang;Defu Zhang;Weifeng Li	2014	IEICE Transactions		econometrics;actuarial science	ML	4.580661389160305	-14.333214837288315	28741
710f76d4b9054e15ebf64781577e9b9d89e82827	weighted voronoi diagrams for optimal location of goods and services in planar maps	weighted voronoi diagrams;geographic support;biological system modeling geographic information systems resource management additives economics industries transportation;facility location problem weighted voronoi diagrams planar maps geographic support supply and demand components unique global function multicriteria analysis;multicriteria analysis;goods distribution;geographic information system;resource manager;resource management;planar maps;biological system modeling;computational geometry;industries;optimal location;additives;objective function;community networks;supply and demand components;clustering;geographic information systems;transportation;unique global function;facility location problem;transport costs;economics;social benefit;optimal facilities location;optimal facilities location computational geometry weighted voronoi diagrams clustering;cost benefit analysis;goods distribution computational geometry cost benefit analysis facility location;quantitative evaluation;voronoi diagram;facility location;supply and demand	Location of industries or services is a classical problem with an overlapping of techniques involving a geographic support, a quantitative evaluation of supply and demand components, and communication networks. Traditional Cost-Benefit Analysis are focused towards maximization of economic and social benefits in terms of distance maps, minimization transport costs and desirability of goods and services. Economic, population and infrastructures models are variable along time, and consequently, sites used for their representation must be adequately weighted according to different criteria. The aggregation of different criteria in a unique objective function (depending on adjustable parameters) allows to identify a unique solution for the problem in terms of weighted distance maps linked to the objective function. Thus, Weighted Voronoi Diagrams (WVD) provide the most adequate framework for computing and representing the optimal localization linked to a unique global function. This paper considers the interaction between Weighted Voronoi Diagrams and Multicriteria Analysis in Facility Location Problem. First, we introduce a weighted Voronoi diagram based on the places to avoid filter the points of the scenario from infinite to a finite number. Then, Multicriteria Analysis chooses one point of the resulting set from a function that accumulates a list of given criteria. As a result of the process we get the ideal position of the new facility we want to locate in. The solution can afford both private and public facilities, desirable or undesirable as well with three levels of application relative to urban, metropolitan and regional areas.	accessibility;expectation–maximization algorithm;facility location problem;feedback;geographic information system;loss function;map;mathematical optimization;optimization problem;partial template specialization;population;simulation;telecommunications network;weighted voronoi diagram	Eduardo Riol Fernández;Julio Cesar Puche Regaliza;Francisco Javier Delgado del Hoyo;Javier Finat Codes;Rubén Martínez García	2011	2011 Eighth International Symposium on Voronoi Diagrams in Science and Engineering	10.1109/ISVD.2011.27	mathematical optimization;engineering;operations management;welfare economics	AI	9.803201955733183	-1.5508158201170488	28786
c8a57c94318417c528f90dd2e6140ae11fc03ca1	a stochastic modeling approach to real-time prediction of queue overflows	random process;queue prediction technology;lane changing;queue length prediction;conventional queue prediction method;queue overflow;real-time queue overflow;queue overflows;real-time prediction;stochastic modeling approach;queue-overflow occurrence;queue length;queue-overflow issue;real time information;kalman filtering;algorithms;stochastic processes;detectors;methodology;real time;stochastic model;traffic flow theory	Queue overflow is a critical issue in developing queue prediction technologies for applications in Advanced Transportation Management System (ATMS). Conventional queue prediction methods, however, are limited to incident-free queue length prediction where traffic arrivals can be readily obtained using detectors. Despite the problems posed by queue overflow, studies addressing queue-overflow issues, or for predicting queue overflows beyond detectors, appear inadequate. This paper describes an advanced methodology which uses a stochastic system modeling approach and random processes for predicting queue lengths beyond detectors in real time. Lane changing is taken into account in developing the queue-overflow prediction model because lane changing accompanies queue overflow in most cases. A discrete-time, nonlinear stochastic system is specified for modeling the queues and lane changes beyond detectors during queue-overflow occurrence. The noise terms of the recursive equations of the model account for the effects of queues and a variety of arriving volumes on vehicular lane-changing maneuvers during queue-overflow occurrence. The unknown traffic arrivals beyond detectors are predicted employing random processes. In addition, a recursive estimation algorithm for predicting real-time queue overflows is developed utilizing the extended Kalman filtering technique. Preliminary test results indicate that the proposed methodology is promising for real-time prediction of queue overflows. The predicted queue overflows can be used not only in understanding the phenomenon of lane traffic patterns during queue-overflow occurrence, but also in developing related advanced technologies such as real-time road traffic congestion control and management systems.	real-time transcription;stochastic modelling (insurance)	Jiuh-Biing Sheu	2003	Transportation Science	10.1287/trsc.37.1.97.12816	computer simulation;multilevel feedback queue;stochastic process;detector;simulation;real-time operating system;telecommunications;computer science;traffic flow;queue management system;fork–join queue;weighted random early detection;operations research;algorithm	Robotics	9.564784897547401	-11.900288622334227	28806
f9b494310d4623ae6b54803f995bcc259c0814c5	a multi-objective evolutionary algorithm based on decomposition and constraint programming for the multi-objective team orienteering problem with time windows		Abstract The team orienteering problem with time windows (TOPTW) is a well-known variant of the orienteering problem (OP) originated from the sports game of orienteering. Since the TOPTW has many applications in the real world such as disaster relief routing and home fuel delivery, it has been studied extensively. In the classical TOPTW, only one profit is associated with each checkpoint while in many practical applications each checkpoint can be evaluated from different aspects, which results in multiple profits. In this study, the multi-objective team orienteering problem with time windows (MOTOPTW), where checkpoints with multiple profits are considered, is introduced to find the set of Pareto optimal solutions to support decision making. Moreover, a multi-objective evolutionary algorithm based on decomposition and constraint programming (CPMOEA/D) is developed to solve the MOTOPTW. The advantages of decomposition approaches to handle multi-objective optimization problems and those of the constraint programming to deal with combinatorial optimization problems have been integrated in CPMOEA/D. Finally, the proposed algorithm is applied to solve public benchmark instances. The results are compared with the best-known solutions from the literature and show more improvement.	constraint programming;evolutionary algorithm;microsoft windows	Wanzhe Hu;M. Fathi;Panos M. Pardalos	2018	Appl. Soft Comput.	10.1016/j.asoc.2018.08.026	orienteering;mathematical optimization;machine learning;constraint programming;artificial intelligence;mathematics;combinatorial optimization;pareto principle;evolutionary algorithm;optimization problem	AI	17.22733006533472	1.1066383150808823	28866
50a4dafcb91c4149dd2a76ea0b71a36f7c82834f	efficient detection method for data integrity attacks in smart grid		With the developing of the Smart Grid, false data injection attacks (FDIAs) as a typical data integrity attack successfully bypass the traditional bad data detection and identification, has a serious influence on the power system safe and reliable operation. State estimation, which is an important process in smart grid, is used in system monitoring to get optimally estimate the power grid state through analysis of the monitoring data. However, FDIAs compromising data integrity will lead to wrong decision makings in power dispatch or electric power market transactions. In this paper, focusing on the power property, we introduce an index to quantitatively measure the node voltage stability and reflect the influence of FDIAs on the power system. Then, we use an improved clustering algorithm to identify the node vulnerability level, which helps operators take measures and detect the false data injection attacks timely. Besides, one effective state forecasting detection method is proposed, which is meaningful for real-time detection of false data injection attacks. Finally, the simulation result verifies the effectiveness and performance of the proposed method.	data integrity	Peixiu An;Zhitao Guan	2016		10.1007/978-3-319-49148-6_21	internet privacy;computer security;computer network	EDA	11.297144887346011	-13.984835937639149	28867
0d057bafb186387f9d44c9b62de9dad0dada59a8	a tolerance approach for unbalanced economic development policy-making in a fuzzy environment	policy making;unbalanced development policy and tolerance approach;fuzzy goal programming;human resource;developing country;economic development;economic policy;development policy;economic growth	In general, developing countries lack capital and human resources to develop all industrial sectors of their countries. Therefore, they will select some of the industrial sectors concerned and invest all the money to those sectors to make unbalanced economic progress. In making economic policies, they have to use imprecise [fuzzy] information to consider many development goals. This paper applies a fuzzy goal programming approach for the optimal planning of an unbalanced development policy for developing (or underdeveloped) countries. In particular, it presents how fuzzy objectives of economic planners can be quantified through the use of specific tolerance operators in various economic growth alternatives.	unbalanced circuit	Jong Soon Kim;Byung Ahm Sohn;Bong Gi Whang	2002	Inf. Sci.	10.1016/S0020-0255(02)00277-3	developing country;human resources;economic statistics	DB	4.655258509735619	-7.8849634783506035	28871
e3244d85155659e4201046ed541c209b9c7bdb52	parameter optimization of tandem queue systems with finite intermediate buffers via fuzzy simulation	fuzzy simulation;imprecise data;simulation optimization;tandem queue system;simulation methods;tandem queue;critical path;indexation;mathematical model;optimization;perturbation;uncertain data;parameter optimization	In this study, the issue of parameter optimization of tandem queue systems (TQS) is discussed. Since the data of such systems are regularly imprecise, a fuzzy logic modeling approach seems to be very necessary and more practical than conventional simulation or mathematical modeling. This issue has been ignored in the previous studies. Therefore, a fuzzy simulation based method is proposed and its performance is compared with the conventional simulation through a numerical example. In order to compare the results of these simulation methods, an efficiency index is proposed. It is seen that the proposed fuzzy simulation based method performs more efficient with regard to the efficiency index. Aside from this advantage, by using the fuzzy simulation approach of this study, we can have more realistic solutions when imprecision occurs in TQS. This is the first study that uses fuzzy simulation approach for TQS with imprecise and uncertain data.	mathematical optimization;simulation;tandem computers	Ali Azadeh;R. Mohammad Ebrahim;H. Eivazy	2010	Perform. Eval.	10.1016/j.peva.2009.10.004	mathematical optimization;real-time computing;perturbation;computer science;fuzzy number;critical path method;mathematical model;mathematics;statistics	Arch	13.994163191453511	-6.684783296908223	28878
1e725f4152e400a7104ae45414f8f30f46b8c5da	a simulated annealing algorithm for solving the bi-objective facility layout problem	simulated annealing algorithm;bi objective;facility layout problem;simulated annealing;facility layout	In this article, a bi-objective facility layout problem (BOFLP) is considered by combining the objectives of minimization of the total material handling cost (quantitative) and the maximization of total closeness rating scores (qualitative), with the predetermined weights are assigned to the respective objectives. A simulated annealing (SA) algorithm is proposed to solve the BOFLP, as well as a comparison of SA with the previous works is provided. It is shown that the SA algorithm works better than the previous works; thus proving the fact that the proposed SA algorithm is an efficient method for solving BOFLP.	algorithm;simulated annealing	Ramazan Sahin	2011	Expert Syst. Appl.	10.1016/j.eswa.2010.09.117	mathematical optimization;simulated annealing;computer science;machine learning;adaptive simulated annealing	Theory	18.969325660914173	1.080714763678983	28887
8553583921d1c8dfc09fd696ce8df1c15126bb20	decentralized task allocation in multi-robot systems via bipartite graph matching augmented with fuzzy clustering		Robotic systems, working together as a team, are becoming valuable players in different real-world applications, from disaster response to warehouse fulfillment services. Centralized solutions for coordinating multi-robot teams often suffer from poor scalability and vulnerability to communication disruptions. This paper develops a decentralized multi-agent task allocation (Dec-MATA) algorithm for multi-robot applications. The task planning problem is posed as a maximum-weighted matching of a bipartite graph, the solution of which using the blossom algorithm allows each robot to autonomously identify the optimal sequence of tasks it should undertake. The graph weights are determined based on a soft clustering process, which also plays a problem decomposition role seeking to reduce the complexity of the individual-agents’ task assignment problems. To evaluate the new Dec-MATA algorithm, a series of case studies (of varying complexity) are performed, with tasks being distributed randomly over an observable 2D environment. A centralized approach, based on a state-of-the-art MILP formulation of the multi-Traveling Salesman problem is used for comparative analysis. While getting within 7-28% of the optimal cost obtained by the centralized algorithm, the Dec-MATA algorithm is found to be 1-3 orders of magnitude faster and minimally sensitive to task-to-robot ratios, unlike the centralized algorithm.	algorithmic efficiency;analysis of algorithms;autonomous robot;benchmark (computing);blossom algorithm;centralized computing;cluster analysis;computation;fuzzy clustering;last mile;matching (graph theory);multi-agent system;observable;parallel computing;qualitative comparative analysis;randomized algorithm;randomness;relevance;robot;robotics;scalability;test case;time complexity;travelling salesman problem;vulnerability (computing)	Payam Ghassemi;Souma Chowdhury	2018	CoRR		real-time computing;fuzzy clustering;robot;scalability;bipartite graph;blossom algorithm;computer science;distributed computing;graph;observable	AI	19.131409424347044	-14.599943711123194	28896
166375906054b68c1281bad359ffdf4ad2954bc5	modeling exchange rates: smooth transitions, neural networks, and linear models	estimation theory;bayesian regularization;neural nets;bayes methods;exchange rates neural networks artificial neural networks predictive models vectors testing bayesian methods linearity feedforward neural networks mathematical model;foreign exchange trading;time series;indexing terms;forecasting model nonlinearities exchange rates time series neural network estimation theory bayesian regularization smooth transition autoregression;bayes methods foreign exchange trading neural nets time series forecasting theory autoregressive processes estimation theory;forecasting theory;autoregressive processes;random walk;smooth transition models;linear model;smooth transition;exchange rate;smooth transition autoregression;artificial neural network;neural network;time series model	The goal of this paper is to test and model nonlinearities in several monthly exchange rates time series. We apply two different nonlinear alternatives, namely: the artificial neural-network time series model estimated with Bayesian regularization; and a flexible smooth transition specification, called the neuro-coefficient smooth transition autoregression. The linearity test rejects the null hypothesis of linearity in 10 out of 14 series. We compare, using different measures, the forecasting performance of the nonlinear specifications with the linear autoregression and the random walk models.	artificial neural network;autoregressive model;coefficient;linear model;neural network simulation;nonlinear system;null value;projections and predictions;specification;time series	Marcelo C. Medeiros;Alvaro Veiga;Carlos Eduardo Pedreira	2001	IEEE transactions on neural networks	10.1109/72.935089	bayesian vector autoregression;econometrics;computer science;machine learning;time series;mathematics;artificial neural network;statistics	ML	8.226740339849163	-20.761260013452628	28898
669e5003c7db3bf9b5f14cebd42c329cfc621586	arterial coordination control signal transition optimization based on volatility analysis		In order to reduce the adverse effects of the arterial intersections signal control strategy dynamic changes during arterial coordination control signal transition. This paper analyzes the fluctuation characteristics during the transition of control signals. The phase offset variation volatility model, signal cycle fluctuation volatility model, green ratio volatility model are constructed, and the volatility model analysis is conducted on the two aspects of the multi-period transition and multi-intersection coordination cycle transitions. The particle swarm optimization algorithm was used to solve the multi-objective optimization problem in the arterial coordination transition, and the best transition control strategy was finally obtained. The simulation results show that the proposed algorithm is optimized compared with the traditional Add algorithm and the Subtract algorithm in terms of vehicle average delay, vehicle average stop times, and average queue length, which can make the signal control effect better.		Xiaoming Liu;Yuan Jiang;Chunlin Shang;Shaohu Tang	2018	2018 21st International Conference on Intelligent Transportation Systems (ITSC)	10.1109/ITSC.2018.8569838		Robotics	10.786108774746612	-8.996889322076823	28907
3755c2494d4ea8d689c9ebfa1603becb24a0530c	uncertainty and the specificity of human capital	labor contract;specific human capital;human capital;labor force;uncertainty labor contracts specific human capital	This paper studies the choice between general and specific human capital. A trade-off arises because general human capital, while less productive, can easily be reallocated across firms. Accordingly, the fraction of individuals with specific human capital depends on the amount of uncertainty in the economy. Our model implies that while economies with more specific human capital tend to be more productive, they also tend to be more vulnerable to turbulence. As such, our theory sheds some light on the experience of Japan, where human capital is notoriously specific: while Japan benefited from this predominately specific labor force in tranquil times, this specificity may also have been at the heart of its prolonged stagnation. © 2008 Elsevier Inc. All rights reserved. JEL classification: J24; J41; J62; D92	sensitivity and specificity;turbulence	Martin Gervais;Igor Livshits;Césaire Meh	2008	J. Economic Theory	10.1016/j.jet.2007.10.003	capital employed;capital good;capital deepening;physical capital;economics;financial capital;capital intensity;economic capital;microeconomics;market economy;capital consumption allowance;capital accumulation;capital formation;labour economics	AI	-4.014322706674686	-6.447231239446013	28943
6d7f850d6cfc91146daa6448d52cc5529b44ac7d	the weighted variance minimization for options pricing	option pricing	A problem of the weighted variance minimization for options pricing in the case of a diffusion model is considered. We estimate a number of options with different values of some parameter which can be a strike price, an exercise date, a barrier, etc. The optimal estimators in the general case and their approximations for some options are pointed out.		Anatoly A. Gormin;Yuri N. Kashtanov	2008	Monte Carlo Meth. and Appl.	10.1515/mcma.2007.018	actuarial science;trinomial tree;valuation of options;black–scholes model;mathematics;finite difference methods for option pricing;rational pricing;binomial options pricing model;monte carlo methods for option pricing	AI	3.1176191007434233	-10.784257341333827	28981
99aa77bf2de13b22a0178278f80d23a609cd4568	es-gp: an effective evolutionary regression framework with gaussian process and adaptive segmentation strategy		This paper proposes a novel evolutionary regression framework with Gaussian process and adaptive segmentation strategy (named ES-GP) for regression problems. The proposed framework consists of two components, namely, the outer DE and the inner DE. The outer DE focuses on finding the best segmentation scheme, while the inner DE focuses on optimizing the hyper-parameters of GP model for each segment. These two components work cooperatively to find a piecewise gaussian process solution which is flexible and effective for complicated regression problems. The proposed ES-GP has been tested on four artificial regression problems and two real-world time series regression problems. The experiment results show that ES-GP is capable of improving prediction performance over non-segmented or fixed-segmented solutions.	gaussian process;time series	Shijia Huang;Jinghui Zhong	2018		10.1007/978-3-319-93713-7_71	differential evolution;mathematical optimization;time series;piecewise;computer science;gaussian process;segmentation	ML	14.00103341635208	-22.96017377475017	29042
9427f98dce0cb670f0ffc4be923bfec5143288b3	a multi-variable grey model with a self-memory component and its application on engineering prediction	multi variable system;m model;grey prediction theory;期刊论文;foundation pit deformation;mgm 1 m model;subgrade settlement;self memory principle;article;mgm 1	This paper presents a novel multi-variable grey self-memory coupling prediction model (SMGM(1,m)) for use in multi-variable systems with interactional relationship under the condition of small sample size. The proposed model can uniformly describe the relationships among system variables and improve the modeling accuracy. The SMGM(1,m) model combines the advantages of the self-memory principle of dynamic system and traditional MGM(1,m) model through coupling of the above two prediction methods. The weakness of the traditional grey prediction model, i.e., being sensitive to initial value, can be overcome by using multi-time-point initial field instead of only single-time-point initial field in the system?s self-memorization equation. As shown in the two case studies of engineering settlement deformation prediction, the novel SMGM(1,m) model can take full advantage of the system?s multi-time historical monitoring data and accurately predict the system?s evolutionary trend. Three popular accuracy test criteria are adopted to test and verify the reliability and stability of the SMGM(1,m) model, and its superior predictive performance over other traditional grey prediction models. The results show that the proposed SMGM(1,m) model enriches grey prediction theory, and can be applied to other similar multi-variable engineering systems. The self-memory principle is introduced into the grey MGM(1,m) prediction model.We can uniformly describe the interactional relationship of multi-variable systems.The coupling prediction model can take full advantage of its multi-time historical data.Traditional grey model?s weakness of being sensitive to initial value can be overcome.The results of engineering example demonstrate its remarkable prediction performance.		Xiaojun Guo;Sifeng Liu;Li-Feng Wu;Yanbo Gao;Yingjie Yang	2015	Eng. Appl. of AI	10.1016/j.engappai.2015.03.014	artificial intelligence;operations research	AI	11.467143196201707	-18.052594327924183	29150
6d190db153d1b81c45501478cc8ab54653b3e2fb	two-node market under imperfect competition		In this paper, we consider a Cournot auction with uniform nodal prices for a two-node market. The structure of each local market is an oligopoly. We demonstrate how the type of Nash equilibrium depends on the throughput. Finally, we investigate the optimum throughput problem under an imperfect competition in the market.	nist hash function competition	Alexander A. Vasin;Ekaterina A. Daylova	2017	Automation and Remote Control	10.1134/S0005117917090144	throughput;cournot competition;oligopoly;mathematical optimization;mathematics;imperfect competition;nash equilibrium;microeconomics	Robotics	-1.8673299307530957	-4.685647937743464	29165
d161369be8255179444751f8e9459b34cdc0703f	smarandache bl-algebra	mv algebra;smarandache bl algebra;bl algebra;q smarandache implicative ideal filter;q smarandache ideal	Abstract In this paper we define the Smarandache BL -algebra, Q -Smarandache ideal and Q -Smarandache implicative ideal, we obtain some related results. After that by considering the notions of these ideals we determine relationships between ideals in BL -algebra and Q -Smarandache (implicative) ideals of BL -algebra. Finally we construct quotient of Smarandache BL -algebras via MV -algebra and prove some theorems.	bl (logic);boolean algebra;fuzzy control system;lattice (discrete subgroup);linear algebra;mv-algebra;soft computing	Arsham Borumand Saeid;Afsaneh Ahadpanah;Lida Torkzadeh	2010	J. Applied Logic	10.1016/j.jal.2010.06.001	discrete mathematics;pure mathematics;mathematics;algebra	AI	0.4143376598073264	-23.15271018254089	29172
07b51a0fd4c4faec8d183289f425372deeb78125	variable selection and data pre-processing in nn modelling of complex chemical processes	computacion informatica;neural networks;neural model;grupo de excelencia;variable selection;ciencias basicas y experimentales;process modelling;quimica;variable information;statistical techniques;network structure;neural network model;data preprocessing;information theoretic;variable entropy;neural network	The neural network models represent nowadays a powerful tool for complicated process identification. However, because of the fact that they belong to the category of data-driven “black box” models, they cannot avoid the consequences of the “garbage in–garbage out” rule. This work proposes a simultaneous data balancing-variable selection procedure, which is based on traditional statistical techniques and modern information theoretic approaches. It is implemented on a complicated dataset of restricted quality, which refers to a commercial aldol condensation unit (BASF). Based on the pre-processed database a neural model for the prediction of the process yield has been developed. The results verify the importance of the pre-processing stage in terms of generalization accuracy as well as of simpler network structure due to the data-variable selection procedure. Finally, an analysis of the model trends has been implemented to assess qualitative characteristics of the model, which was then used in industrial test runs and resulted in an improvement of the process operation.	data pre-processing;feature selection;preprocessor	Stavros Papadokonstantakis;Stephan Machefer;Klaus Schnitzlein;Argyrios I. Lygeros	2005	Computers & Chemical Engineering	10.1016/j.compchemeng.2005.01.004	computer science;artificial intelligence;machine learning;process modeling;data mining;data pre-processing;artificial neural network;statistics	DB	6.152714390927151	-21.820979980637976	29241
0ef7ad0d0c79d94fbe2c2e5c0eb7e86c57831f13	business-to-consumer platform strategy: how vendor certification changes platform and seller incentives		We build an economic model to study the problem of offering a new, high-certainty channel on an existing business-to-consumer platform such as Taobao and eBay. On this new channel, the platform owner exerts effort to reduce the uncertainty of service quality. Sellers can either sell through the existing low-certainty channel or go through additional screening to sell on this new channel. We model the problem as a Bertrand competition game where sellers compete on price and exert effort to provide better service to consumers. In this game, we consider a reputation spillover effect that refers to the impact of the high-certainty channel on the perceived service quality in the low-certainty channel. Counter-intuitively, we find that low-certainty channel demand will decrease as the reputation spillover effect increases, in the case of low inter-channel competition. Also, low-certainty channel demand increases as the quality uncertainty increases, in the case of intense inter-channel competition. Furthermore, the platform owner should offer a new high-certainty channel when (i) the perceived quality for this channel is sufficiently high, (ii) sellers in this channel are able to efficiently provide quality service, (iii) consumers in this channel are not so sensitive to the quality uncertainty, or (iv) the reputation spillover effect is high. In the one-channel case, the incentives of the platform owner and sellers are aligned for all model parameters. However, this is not the case for the two-channel solution, and our model reveals where tensions will arise between parties.	bertrand (programming language);knowledge spillover;quality of service;reputation management;reputation system;taobao marketplace	Can Sun;Yonghua Ji;Bora Kolfal;Raymond A. Patterson	2017	ACM Trans. Management Inf. Syst.	10.1145/3057273	consumer-to-business;vendor;service quality;spillover effect;incentive;economics;reputation;communication channel;commerce;bertrand competition	Security	-2.73185121690405	-7.372845299479918	29266
7631e90bbc0f8d177992bec665545c0cf7d37dc7	consistency in positive reciprocal matrices: an improvement in measurement methods		Consistency estimation of decision-makers’ judgments in decision-making processes is fundamental to generating agreements and making decisions. Analytic hierarchy process (AHP) is a widely used method to solve this type of problem, enabling evaluation of the consistency of judgments emitted by the decision-makers through the maximum eigenvalue of the matrix of judgments. In addition to the consistence index originally proposed in AHP, different indexes have been proposed in the literature, which use the minimum element of consistency. These indices that solve some of the original consistency index problems, present others that may question their usefulness. The purpose of this paper is to propose a new index, as an improvement of the previous indexes. Among other characteristics, this new index is intuitive and easy to use, is bounded in the interval [0, 1], proposes a critical value to accept or reject matrices, that depends on the size of the pairwise-comparison matrix, and can be extended to another type of pairwise-comparison scales. In addition, the probability distribution for the new index is defined, which enables calculating the probability of a matrix being consistent, as a function of the critical acceptance value.	cladogram;the matrix	Jos&#x00E9; Ignacio Pel&#x00E1;ez;Eustaquio A. Mart&#x00ED;nez;Luis G. Vargas	2018	IEEE Access	10.1109/ACCESS.2018.2829024	probability distribution;eigenvalues and eigenvectors;analytic hierarchy process;distributed computing;mathematical optimization;computer science;bounded function;critical value;matrix (mathematics);reciprocal	Web+IR	-3.7619368847902366	-19.87015973155726	29268
861bba27dd59a47025fd29405952615efb3779af	an end-quality assessment system for electronically commutated motors based on evidential reasoning	electric motor;decision analysis;electrical motors;evidential reasoning;quality assessment;mechanical faults;feature extraction;signal processing;fault detection;multi attribute decision analysis;fault isolation;end quality assessment	Automatic end-quality assessment is a mean that helps reaching zero-fault products at the end of the manufacturing process. In this paper we present a system for assessing the quality of electronically commutated motors. The system consists of two major parts: feature extraction and overall quality assessment. The feature extraction part consists of signal processing algorithms tailored for mechanical fault detection. The quality assessment part, aimed for fault isolation and final quality decision, employs evidential reasoning for multi-attribute decision analysis. A prototype version of the system is validated on a test batch of 130 electronically commutated motors, demonstrating high diagnostic resolution and accuracy.		Pavle Boskoski;Janko Petrovcic;Bojan Musizza;Dani Juricic	2011	Expert Syst. Appl.	10.1016/j.eswa.2011.04.185	electric motor;decision analysis;computer science;signal processing;fault detection and isolation	AI	12.966496230316539	-16.10073523943698	29280
71bc3b19517c8d56758ed0d5789bf5d90f00c93a	ant colony optimization for power plant maintenance scheduling optimization	bas;ga;mmas;sa;ant colony optimization;heuristics;optimum parameter;power plant maintenance scheduling;sensitivity analysis	In this paper, a formulation that enables ant colony optimization (ACO) algorithms to be applied to the power plant maintenance scheduling optimization (PPMSO) problem is developed and tested on a 21-unit case study. A heuristic formulation is introduced and its effectiveness in solving the problem is investigated. The results obtained indicate that the performance of ACO algorithms is significantly better than that of a number of other metaheuristics, such as genetic algorithms and simulated annealing, which have been applied to the same case study previously.	ant colony optimization algorithms;genetic algorithm;heuristic;mathematical optimization;metaheuristic;scheduling (computing);simulated annealing	Wai-Kuan Foong;Holger R. Maier;Angus R. Simpson	2005		10.1145/1102256.1102335	power station;mathematical optimization;ant colony optimization algorithms;genetic algorithm;electric power;parallel metaheuristic;simulated annealing;computer science;artificial intelligence;heuristics;sensitivity analysis;metaheuristic	AI	17.364491135386494	-2.362274084939735	29329
7f94e5c6398cd5e12ffe3bc327ccf9cb4ee6a01c	neural-optimal control algorithm for real-time regulation of in-line storage in combined sewer systems	urban stormwater management;neural networks;real time control;real time;hydraulic sewer models;dynamic routing;optimal policy;satisfiability;artificial intelligent;optimal control;sewer system;control system;combined sewers;adaptive learning;real time control system;cost effectiveness;artificial intelligence;combined sewer overflow;real time implementation;stormwater management;optimization model;coordinate system;neural network;dynamic optimization;time constraint	Attempts at implementing real-time control systems as a cost-effective means of minimizing the pollution impacts of untreated combined sewer overflows have largely been unsustained due to the complexity of the real-time control problem. Optimal real-time regulation of flows and in-line storage in combined sewer systems is challenging due to the need for complex optimization models integrated with urban stormwater runoff prediction and fully dynamic routing of sewer flows within 5-15min computational time increments. A neural-optimal control algorithm is presented that fully incorporates the complexities of dynamic, unsteady hydraulic modeling of combined sewer system flows and optimal coordinated, system-wide regulation of in-line storage. The neural-optimal control module is based on a recurrent Jordan neural network architecture that is trained using optimal policies produced by a dynamic optimal control module. The neural-optimal control algorithm is demonstrated in a simulated real-time control experiment for the King County combined sewer system, Seattle, Washington, USA. The algorithm exhibits an effective adaptive learning capability that results in near-optimal performance of the control system while satisfying the time constraints of real-time implementation.	algorithm;optimal control;real-time clock	Suseno Darsono;John W. Labadie	2007	Environmental Modelling and Software	10.1016/j.envsoft.2006.09.005	control engineering;simulation;real-time control system;environmental engineering;computer science;engineering;artificial intelligence;artificial neural network	Robotics	15.088133187875508	-19.011323797870702	29345
1811e1524552610997f6387c4da1f1fbe28eb15f	optimality and natural selection in markets	asymptotic optimality;market model;dynamic system;population dynamic;natural selection;dynamic equilibrium;market selection;pareto optimality;profit maximization;steady state	Evolutionary arguments are often used to justify the fundamental behavioral postulates of competitive equilibrium. Economists such as Milton Friedman have argued that natural selection favors profit maximizing firms over firms engaging in other behaviors. Consequently, producer efficiency, and therefore Pareto efficiency, are justified on evolutionary grounds. We examine these claims in an evolutionary general equilibrium model. If the economic environment were held constant, profitable firms would grow and unprofitable firms would shrink. In the general equilibrium model, prices change as factor demands and output supply evolves. Nonetheless, our model verifies Friedman’s claim that natural selection favors profit maximization. But we show that this does not imply that equilibrium allocations converge over time to efficient allocations. Consequently, Koopmans critique of Friedman is correct. The long-run outcomes of evolutionary market models are not well described by General Equilibrium analysis, even though the GE hypotheses on firm behavior emerge in markets through natural selection.	converge;expectation–maximization algorithm;nash equilibrium;nat friedman;pareto efficiency	Lawrence E. Blume;David A. Easley	2002	J. Economic Theory	10.1006/jeth.2000.2764	natural selection;economics;dynamic equilibrium;dynamical system;finance;macroeconomics;population dynamics;microeconomics;mathematical economics;steady state;welfare economics	ECom	-4.265260896324923	-5.446779808403993	29408
a6bdb668d7f7f75ce12a917eb7ed68ce2903eff1	multiple-attribute decision-making method based on the correlation coefficient between dual hesitant fuzzy linguistic term sets		Abstract Hesitant fuzzy linguistic term sets (HFLTS) and dual hesitant fuzzy sets (DHFS) are two important branches of fuzzy mathematics, both of which have been widely applied in multiple-attribute decision-making (MADM) problems under uncertain environments. To humanize the decision-making process, the former deals with hesitant fuzzy linguistic terms in line with people’s common sense, and to reflect the nature of people’s hesitancy, the latter deals with both the membership and nonmembership hesitancy functions of fuzzy sets (FS). However, as the decision-making environment is increasingly complex, the characteristics of these two sets need to be combined to more precisely represent the fuzzy linguistic information. Therefore, this paper proposes a new extension of the HFLTS concept, i.e., dual hesitant fuzzy linguistic term set (DHFLTS), to highlight the importance of the nonmembership degree for HFLTSs. Some properties for the DHFLTS are given. Motivated by information energy, the information energy for DHFLTS and the correlation coefficient between DHFLTSs as well as the weighted correlation coefficient are defined. Finally, a supplier selection problem is given to demonstrate the feasibility and superiority of this method.	coefficient	Ruichen Zhang;Zongmin Li;Huchang Liao	2018	Knowl.-Based Syst.	10.1016/j.knosys.2018.07.014	machine learning;fuzzy logic;fuzzy mathematics;correlation coefficient;artificial intelligence;fuzzy set;linguistics;computer science;rule-based machine translation	NLP	-3.4678126914180405	-21.580141454204824	29431
1b7c07b771ff1d5e214be478eab65d0ded3d3f23	risk-averse auction agents	e commerce;risk aversion;expected utility;dynamic program;production process;auction theory;probability distribution;exponential utility;production planning;profitability;supply chain management;utility theory;auctions	Auctions are an important means for purchasing material in the era of e-commerce. Research on auctions often studies them in isolation. In practice, however, auction agents are part of complete supply-chain management systems and have to make the same decisions as their human counterparts. To address this issue, we generalize results from auction theory in three ways. First, auction theory provides the optimal bidding function for the case where auction agents want to maximize the expected profit. Since companies are often risk-averse, we derive a closed form of the optimal bidding function for auction agents that maximize the expected utility of the profit for concave exponential utility functions. Second, auction theory often assumes that auction agents know the bidder's valuation of an auctioned item. However, the valuation depends on how the item can be used in the production process. We therefore develop theoretical results that enable us to integrate our auction agents into production-planning systems to derive the bidder's valuation automatically. Third, auction theory often assumes that the probability distribution over the competitors' valuations of the auctioned item is known. We use simulations of the combined auction- and production-planning system to obtain crude approximations of these probability distributions automatically. The resulting auction agents are part of a complete supply-chain management system and seamlessly combine ideas from auction theory, utility theory, and dynamic programming.	approximation;concave function;dynamic programming;e-commerce;expected utility hypothesis;exponential utility;heuristic;purchasing;risk aversion;simulation;time complexity;value (ethics)	Yaxin Liu;Richard Goodwin;Sven Koenig	2003		10.1145/860575.860632	walrasian auction;probability distribution;auction algorithm;eauction;supply chain management;vickrey auction;combinatorial auction;generalized second-price auction;risk aversion;expected utility hypothesis;unique bid auction;reverse auction;vickrey–clarke–groves auction;proxy bid;common value auction;revenue equivalence;english auction;scheduling;bid shading;auction theory;utility;profitability index;forward auction	AI	-2.127315400705868	-2.022241560563078	29435
daa95ed03bcb529e156d6024890be1f7c93dc4a8	crowdexpress: a probabilistic framework for on-time crowdsourced package deliveries		Speed and cost of logistics are two major concerns to on-line shoppers, but they generally conflict with each other in nature. To alleviate the contradiction, we propose to exploit existing taxis that are transporting passengers on the street to relay packages collaboratively, which can simultaneously lower the cost and accelerate the speed. Specifically, we propose a probabilistic framework containing two phases called CrowdExpress for the on-time package express deliveries. In the first phase, we mine the historical taxi GPS trajectory data offline to build the package transport network. In the second phase, we develop an online adaptive taxi scheduling algorithm to find the path with the maximum arriving-on-time probability “on-the-fly” upon realtime requests, and direct the package routing accordingly. Finally, we evaluate the system using the real-world taxi data generated by over 19,000 taxis in a month in the city of New York, US. Results show that around 9,500 packages can be delivered successfully on time per day with the success rate over 94%, moreover, the average computation time is within 25 milliseconds. Keywords—package delivery; hitchhiking rides; route planning; taxi scheduling; trajectory data mining	algorithm;baseline (configuration management);chao (sonic);computation;crowdsourcing;data mining;entity–relationship model;exploit (computer security);feedback;global positioning system;logistics;mathematical induction;motion planning;online and offline;relay;routing;scheduling (computing);throughput;time complexity;two-phase locking	Chao Chen;Sen Yang;Weichen Liu;Yasha Wang;Bin Guo;Daqing Zhang	2018	CoRR		transport network;computer science;theoretical computer science;real-time computing;global positioning system;scheduling (computing);computation;probabilistic logic;trajectory;relay;exploit	Mobile	11.461821169127946	-7.276315071113361	29441
4e90478fbd86cc4f0054d66731715e574c0ae727	large multi-unit auctions with a large bidder		We compare equilibrium bidding in uniform-price and discriminatory auctions when a single large bidder (i.e., with multi-unit demand) competes against many small bidders, each with single-unit demands. We show that the large bidder prefers the discriminatory auction over the uniform-price auction, and we provide general conditions under which small bidders have the reverse preference. We use examples to show that the efficiency and revenue rankings of the two auctions are ambiguous. JEL Codes: C72, D44, D47, D61, D82.	ambiguous grammar;auction algorithm;code;nash equilibrium	Brian Baisa;Justin Burkett	2018	J. Economic Theory	10.1016/j.jet.2017.11.010	financial economics;combinatorial auction;common value auction;microeconomics;business;commerce;forward auction	ECom	-2.577160677599494	-4.57704303595301	29510
d508a64ac7d7c17a577577e5a8c73271fe3c6620	knowledge propagation in model-based reinforcement learning tasks	reinforcement learning			Corinna Richter;Jörg Stachowiak	2000			error-driven learning;computer science;machine learning;learning classifier system;reinforcement learning	ML	19.27698408793011	-20.454329551595343	29534
22c373b3c2ccf4244238036bf32c07a450b02af0	the least square b-nucleolus for fuzzy cooperative games		The least square B-nucleolus for fuzzy cooperative games is proposed based on the bi-excess of fuzzy coalitions. The proposed solutions consider not only the size of fuzzy coalitions, but also the blocking and constructive powers. The uniqueness of the least square B-prenucleolus for fuzzy cooperative games is proved detailedly. Some quadratic programming models for generating the least square B-nucleolus of complete and incomplete fuzzy cooperative games are presented, respectively. The least square B-prenucleolus is extended to the multiplicative setting, and the logarithmic least square B-prenucleolus for multiplicative fuzzy cooperative games is derived.		Jian Lin;Qiang Zhang	2016	Journal of Intelligent and Fuzzy Systems	10.3233/IFS-151754	mathematical optimization;discrete mathematics;defuzzification;fuzzy number;mathematics;mathematical economics	Robotics	-0.5437523993019503	-21.54466428637353	29553
c07234467c3799dc6ccdedbd366718ac495925b4	studying dynamic equilibrium of cloud computing adoption with application of mean field games	government;client server systems;fokker planck equation;commerce;forward fokker planck equation dynamic equilibrium cloud computing adoption mean field games client server cloud infrastructures business world government agencies economic technological environment backward pde hamilton jacobi bellman equation;partial differential equations client server systems cloud computing commerce economics fokker planck equation government;partial differential equations;cloud computing computational modeling equations games mathematical model market research economics;economics;cloud computing	Computing is undergoing a substantial shift from client/server to the cloud. The enthusiasm for cloud infrastructures is not only present in the business world, but also extends to government agencies. Managers of both segments thus need to have a clear view of how this new era will evolve in the coming years, in order to appropriately react to a changing economic and technological environment. In this study, we explore the dynamic equilibrium of cloud computing adoption through the application of Mean Field Games. In our formulation, each agent (i.e., each firm or government agency) arbitrates between “continuing to implement the traditional on-site computing paradigm” and “moving to adopt the cloud computing paradigm”. To decide on his level of moving to the cloud computing paradigm, each agent will optimize a total cost that consists of two components: the effort cost of moving to the cloud computing paradigm and the adoption cost of implementing the cloud computing paradigm. In the formulation, the adoption cost is linked to the general trend of decisions on the computing paradigm adoption. Thus, an agent's optimal level of transition to the cloud computing paradigm is not only dependent on his own effort and adoption costs but also affected by the general trend of adoption decisions. The problem is solved by a system of partial differential equations (PDEs), that is, mean field games PDEs, which consists of a backward PDE, the Hamilton Jacobi Bellman equation for a controlled problem, and a forward Fokker-Planck equation transported by the optimal control from the backward HJB equation. Thus, the solution to the forward Fokker-Planck equation enables us to study the dynamic evolution of the density of the cloud computing adoption. It therefore allows us to investigate the impact of the general trend of technology adoption decisions on a firm's optimal decision of technology transition.	bellman equation;client–server model;cloud computing;hamilton–jacobi–bellman equation;jacobi method;level of detail;moose file system;norm (social);numerical partial differential equations;optimal control;programming paradigm;server (computing)	SingRu Celine Hoe;Murat Kantarcioglu;Alain Bensoussan	2012	2012 50th Annual Allerton Conference on Communication, Control, and Computing (Allerton)	10.1109/Allerton.2012.6483221	simulation;computer science;operations management;management science;utility computing	HPC	-0.16094823585405094	-9.310051790548263	29577
eaf91ee2e4190e28556818c3a01e33d2542e8e22	screening of port enterprise value chain routines based on evolution equilibrium	evolutionary game;port enterprises value chain routines;routine selection	In order to find the screening mechanism of port enterprises value chain routines, based on describing the selection of port enterprises value chain routines, an evolutionary game model is presented and constructed. Using this model, we analyze the strategy of port enterprises value chain routines when they achieve the stability of the evolution. The results show that port enterprise value chain routine selection is a dynamic and repeated game. The expected revenue and convention cost of routines taking part in the game playing directly correlate with the evolutionary stable strategy and the selection of port enterprises value chain routines tend to be conservative strategy. Introducing evolutionary game theory provides a new perspective for the study on the formation, search and selection of port enterprises value chain routines and provides favorable theoretical support for further research in related fields.	endogeneity (econometrics);evolution;exogenesis: perils of rebirth;game theory	Bing Han;Pengfei Zhang;Haibo Kuang;Min Wan	2018	Wireless Personal Communications	10.1007/s11277-017-5110-6	computer science;real-time computing;management science;enterprise value;evolutionary game theory;evolutionarily stable strategy;repeated game;revenue	Web+IR	-2.3417633215320977	-6.003606968401822	29592
c095d47d3e338a5b7a8d4c0d5ec4243341375927	a cooperative co-evolution pso for flow shop scheduling problem with uncertainty	fuzzy programming;cooperative co evolutionary particle swarm optimization algorithm;uncertainty;flow shop scheduling problem	Considering current situation of production scheduling with uncertainties in modern manufacturing enviroments, flow shop production scheduling model is established based on the theory of fuzzy programming, in which fuzzy processing time is considered and the duration time of intermediate is unlimited. The maximum membership function of mean value has been applied to solve the non-linear fuzzy scheduling model in order to convert the fuzzy optimization problem to the general optimization problem. Finally, a cooperative co-evolutionary particle swarm optimization algorithm based on catastrophe added to improve the diversity of the swarm (CCPSO) is adopted to solve flow shop production scheduling with uncertainty within infinite intermediate storage and the simulation results obtained are effective and satisfactory.	algorithm;catastrophe theory;flow shop scheduling;mathematical optimization;nonlinear system;optimization problem;particle swarm optimization;scheduling (computing);simulation	Bin Jiao;Qunxian Chen;Shaobin Yan	2011	JCP	10.4304/jcp.6.9.1955-1961	fair-share scheduling;job shop scheduling;mathematical optimization;multi-swarm optimization;uncertainty;flow shop scheduling;dynamic priority scheduling;rate-monotonic scheduling;artificial intelligence;mathematics;statistics	Embedded	19.507790295133592	-0.20756409921647515	29618
adedc3cf0f3d65db476c09cb21f1e71081845019	on logical, algebraic, and probabilistic aspects of fuzzy set theory		Fuzzy Logic and the Linz Seminar:Themes and some Personal Reminiscences.- How I Saw and How I See Fuzzy Sets.- Modules in the Category.- A Geometric Approach to MV-algebras.- On the Equational Characterization of Continuous t-norms.- The Semantics of Fuzzy Logics: Two Approaches to Finite Tomonoids.- Structure of Uninorms with Continuous Diagonal Functions.- The Notions of Overlap and Grouping Functions.- Asymmetric Copulas and Their Application in Design of Experiments.- Copulae of Processes Related to the Brownian Motion: a Brief Survey.- Extensions of Capacities.- Multi-source Information Fusion Using Measure Representations.- Bases and Transforms of Set Functions.- Conditioning for Boolean Subsets, Indicator Functions, and Fuzzy Subsets.- Multivalued Functions Integration from Additive to Arbitrary Non-Negative Set Function.	fuzzy set;set theory		2016		10.1007/978-3-319-28808-6	fuzzy logic;combinatorics;discrete mathematics;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;fuzzy number;mathematics;fuzzy set;fuzzy set operations	Theory	0.8322248979356776	-21.65938903424288	29619
057f7587d7fa0824c9e5056ca43cde3920120143	simulation of alternative approaches to relieving congestion at locks in a river transportion system	inland navigation;file attente;modelizacion;forecasting;scheduling and sequencing;congestion trafic;reliability;sequencage;project management;systeme evenement discret;information systems;cout capital;barges;rivers;congestion trafico;maintenance;hydrologie;soft or;information technology;packing;priorite;queue;operations research;location;investment;journal;transport system;journal of the operational research society;capital cost;inventory;sistema acontecimiento discreto;purchasing;modelisation;regle decision;sequencing;hidrologia;discrete event system;history of or;dependance du temps;logistics;time dependence;coste capital;traffic congestion;marketing;scheduling;rio;riviere;simulation application;hydrology;production;periode pointe;communications technology;lock operations;peak period;computer science;regla decision;operational research;series of queues;navigation interieure;priority;prioridad;modeling;fila espera;navegacion interior;dependencia del tiempo;ordonnancement;decision rule;applications of operational research;or society;reglamento;jors;management science;infrastructure;water transportation	JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org.. Palgrave Macmillan Journals and Operational Research Society are collaborating with JSTOR to digitize, preserve and extend access to The Journal of the Operational Research Society. We construct a discrete-event simulation model to investigate the impact of alternative decision rules and infrastructural improvements to relieve traffic congestion in a section of the Upper Mississippi River navigation system. The model covers a series of five locks that serve commercial tows with widely different barge configurations, as well as private recreational vessels. Mixes and intensities of vessel activity are highly dependent on the time of year, day of week and time of day. The model reveals that some improvement in performance (especially in peak periods) can be achieved by scheduling lock activity with priority given to vessels with shortest average processing and lock setup times (tempered by the time that vessels have spent in queue). Greater improvement occurs with the use of helper boats and greatest improvement occurs with enlarged locks. The alternative remedies must be evaluated with consideration of their dramatically different capital costs.	archive;lock (computer science);network congestion;scheduling (computing);simulation;the new palgrave dictionary of economics	L. Douglas Smith;Donald C. Sweeney;James F. Campbell	2009	JORS	10.1057/palgrave.jors.2602587	capital cost;project management;logistics;simulation;inventory;economics;forecasting;investment;computer science;marketing;operations management;sequencing;decision rule;reliability;location;management;operations research;information technology;scheduling;queue;statistics	OS	8.566897761864702	-7.032100341722784	29631
beecaed23207b49c8e8dbaef92cd3edaab9a9155	economic bios		Purpose – The purpose of this paper is to examine the possibility of biotic patterns. In economics, markets are thought to tend to equilibrium with random and unpredictable deviations. However, an explosion of empirical work searching for possible chaos show an enormous amount of unexplained nonlinear structure. These observations led the authors to examine the possibility of biotic patterns in economics. Design/methodology/approach – Bios is defined as a causally generated creative process. It is the causal counterpart to random walk, just as chaos is the causal equivalent to randomness. Economic data consisting of time series from several categories, including banking, employment and population, and gross domestic product and components, were studied for diversification, recurrence, and predictability patterns characteristic of bios. Diversification was quantified as increased variance with embedding, recurrences were measured using newly developed computer programs, and predictability was measured with a nonlinear prediction method. Findings – Dynamic analyses of the data show: episodic patterning and asymmetric statistical distribution, typical of bios; increase in variance with embedding (diversification), less recurrence than shuffled copies of the data (novelty), demonstrating creativity; consecutive recurrence; and patterning in the series of differences, indicating non-random causation. Originality/value – The demonstration of bios in empirical data indicates that the economy is non-stationary, causal, and creative. This contradicts the notion that markets regulate themselves and tend to equilibrium, and the characterization of market variation as random or chaotic. Further economic crises may be avoided by acknowledging that financial markets are not bound within limits and can be modified into new forms by human action.	bios;causal filter;causality;chaos theory;computer program;diversification (finance);nonlinear system;randomness;recurrence relation;stationary process;time series	Hector Sabelli;Lazar Kovacevic	2011	Kybernetes	10.1108/03684921111118022		ML	0.20340347417827914	-10.378624461764588	29664
011b5d2373a71679359c66979122b78bb52ccc4c	output space search for structured prediction		We consider a framework for structured prediction based on search in the space of complete structured outputs. Given a structured input, an output is produced by running a time-bounded search procedure guided by a learned cost function, and then returning the least cost output uncovered during the search. This framework can be instantiated for a wide range of search spaces and search procedures, and easily incorporates arbitrary structured-prediction loss functions. In this paper, we make two main technical contributions. First, we define the limited-discrepancy search space over structured outputs, which is able to leverage powerful classification learning algorithms to improve the search space quality. Second, we give a generic cost function learning approach, where the key idea is to learn a cost function that attempts to mimic the behavior of conducting searches guided by the true loss function. Our experiments on six benchmark domains demonstrate that using our framework with only a small amount of search is sufficient for significantly improving on state-of-the-art structuredprediction performance.	algorithm;benchmark (computing);discrepancy function;experiment;loss function;machine learning;structured prediction	Janardhan Rao Doppa;Alan Fern;Prasad Tadepalli	2012			interpolation search;beam search;mathematical optimization;machine learning;data mining;mathematics;incremental heuristic search;iterative deepening depth-first search;guided local search	ML	21.925571497680007	-21.541485005056302	29693
40f0da540e80eb8cfbfcae3954f468b1d181d722	the k-dissimilar vehicle routing problem	vehicle routing problem vrp;risk management;logistics;decision support systems;similarity;combinatorial optimization;security;metaheuristic	In this paper we define a new problem, the aim of which is to find a set of k dissimilar solutions for a vehicle routing problem (VRP) on a single instance. This problem has several practical applications in the cash-in-transit sector and in the transportation of hazardous materials. A min–max mathematical formulation is proposed which requires a maximum similarity threshold between VRP solutions, and the number k of dissimilar VRP solutions that need to be generated. An index to measure similarities between VRP solutions is defined based on the edges shared between pairs of alternative solutions. An iterative metaheuristic to generate k dissimilar alternative solutions is also presented. The solution approach is tested using large and medium size benchmark instances for the capacitated vehicle routing problem.		Luca Talarico;Kenneth Sörensen;Johan Springael	2015	European Journal of Operational Research	10.1016/j.ejor.2015.01.019	logistics;mathematical optimization;combinatorics;similarity;decision support system;risk management;combinatorial optimization;computer science;operations management;mathematics;metaheuristic	Theory	17.041777837593475	1.590654510847199	29710
1456c586a338c559281f133ae3ba90d81395403b	pareto clustering search applied for 3d container ship loading plan problem	hybrid heuristics;clustering search;pareto front;stowage planning	Pareto Clustering Search (PCS) is a hybrid method to solve multi-objective problems.PCS detects promising areas and applies local search heuristics only in these areas.We apply the PCS to solve the 3D Container ship Loading Plan Problem (CLPP).The PCS provides better solutions for the CLPP than mono-objective methods.Decision maker chooses the solution that best meets their interests in a situation. The 3D Container ship Loading Plan Problem (CLPP) is an important problem that appears in seaport container terminal operations. This problem consists of determining how to organize the containers in a ship in order to minimize the number of movements necessary to load and unload the container ship and the instability of the ship in each port. The CLPP is well known to be NP-hard. In this paper, the hybrid method Pareto Clustering Search (PCS) is proposed to solve the CLPP and obtain a good approximation to the Pareto Front. The PCS aims to combine metaheuristics and local search heuristics, and the intensification is performed only in promising regions. Computational results considering instances available in the literature are presented to show that PCS provides better solutions for the CLPP than a mono-objective Simulated Annealing.	cluster analysis;pareto efficiency	Eliseu Junio Araújo;Antonio Augusto Chaves;Luiz Leduíno de Salles Neto;Anibal Tavares de Azevedo	2016	Expert Syst. Appl.	10.1016/j.eswa.2015.09.005	simulation;multi-objective optimization;operations research	AI	17.613616819722232	0.9182966270921088	29723
98d8b44e10d911a07a62a5b2190ddb13f361d913	multilayer perceptron training optimization for high speed impacts classification	multilayer perceptron;high speed	The construction of structures subjected to impact was traditionally carried out empirically, relying on real impact tests.  The need for design tools to simulate this process triggered the development in recent years of a large number of models of  different types. Taking into account the difficulties of these methods, poor precision and high computational cost, a neural  network for the classification of the result of impacts on steel armours was designed. Furthermore, the numerical simulation  method was used to obtain a set of input patterns to probe the capacity of themodel development. In the problem tackled with,  the available data for the network designed include, the geometrical parameters of the solids involved — radius and length  of the projectile, thickness of the steel armour — and the impact velocity, while the response of the system is the prediction  about the plate perforation.  	multilayer perceptron	Ángel García-Crespo;Belén Ruíz-Mezcua;Israel González-Carrasco;José Luis López Cuadrado	2008		10.1007/978-90-481-2311-7_32	computer science;machine learning;multilayer perceptron	Vision	12.028159153806893	-20.99225604325328	29786
1614d08cdd9b96e054216f85d1d5617254ed632e	a portfolio optimization model for minimizing soft margin-based generalization bound	smoothed safety first;portfolio optimization model;soft margin based generalization bound;machine learning	Roy’s safety first (RSF) criterion aims to minimize the shortfall probability in portfolio selection. Smoothed safety first portfolio optimization model is a useful tool to realize RSF criterion by minimizing an approximation of the empirical shortfall probability. However, the generalization performance of the smoothed safety first portfolio optimization model may be poor when the number of the samples is finite. In this paper, a soft margin-based generalization bound on the shortfall probability is obtained firstly. Then, a portfolio optimization model is built by minimizing the soft margin-based generalization bound. Finally, the good generalization performance of the portfolio optimization model is verified by experiments.	mathematical optimization	Minghu Ha;Yang Yang;Chao Wang	2017	J. Intelligent Manufacturing	10.1007/s10845-014-1011-7	mathematical optimization;computer science;machine learning;portfolio optimization;mathematics;mathematical economics;roy's safety-first criterion	ML	4.643555333081016	-10.129762111429836	29803
7137c68841e2e75873abf3fcf75bebfe81e6ed9d	grasp and hybrid grasp-tabu heuristics to solve a maximal covering location problem with customer preference ordering		A maximal covering location problem with customer preferences is studied.A GRASP and a hybrid GRASP-Tabu heuristics to find lower bounds are proposed.The heuristics are tested with medium and large size instances.Despite the heuristics simplicity, they provide high quality solutions efficiently.The proposed heuristics are competitive against commercial optimization software. In this study, a maximal covering location problem is investigated. In this problem, we want to maximize the demand of a set of customers covered by a set of p facilities located among a set of potential sites. It is assumed that a set of facilities that belong to other firms exists and that customers freely choose allocation to the facilities within a coverage radius. The problem can be formulated as a bilevel mathematical programming problem, in which the leader locates facilities in order to maximize the demand covered and the follower allocates customers to the most preferred facility among those selected by the leader and facilities from other firms. We propose a greedy randomized adaptive search procedure (GRASP) heuristic and a hybrid GRASP-Tabu heuristic to find near optimal solutions. Results of the heuristic approaches are compared to solutions obtained with a single-level reformulation of the problem. Computational experiments demonstrate that the proposed algorithms can find very good quality solutions with a small computational burden. The most important feature of the proposed heuristics is that, despite their simplicity, optimal or near-optimal solutions can be determined very efficiently.	computation;experiment;grasp;greedy algorithm;greedy randomized adaptive search procedure;heuristic (computer science);mathematical optimization;maximal set;multi-level cell;randomized algorithm;tabu search	Juan A. Díaz;Dolores E. Luna;José Fernando Camacho Vallejo;Martha-Selene Casas-Ramírez	2017	Expert Syst. Appl.	10.1016/j.eswa.2017.04.002	machine learning;artificial intelligence;metaheuristic;software;tabu search;heuristics;computer science;greedy randomized adaptive search procedure;mathematical optimization;heuristic;bilevel optimization;grasp	AI	17.490701588647944	3.264548487316903	29845
9b6cea69f3c036b092fdc8c7d752df7bf3f0ecfd	kdd-cup 99: knowledge discovery in a charitable organization's donor database	decision tree;model performance;two stage prediction;neural network	"""1. I N T R O D U C T I O N 2. This report describes the results of our knowledge discovery and modeling on the data of the 1997 donation campaign of an 1. American charitable organization. The two data sets (training and evaluation) contained about 95000 customers each, with an average net donation of slightly over 11 cents per customer, hence a total net donation of around $10500 results from the """"mail to all"""" policy. The main tool we utilized for the knowledge discovery task is 2. Amdocs' Information Analysis Environment, which allows standard 2-class knowledge discovery and modeling, but also Value Weighted Analysis (VWA). In VWA, the discovered segments and models attempt to optimize the value and class membership simultaneously. Thus, our modeling was based on a 1-stage model rather than a separate analysis for donation probability and expected donation (the approach taken by all of KDD-Cup 98's reported modeling efforts except our own). 3. We concentrate the first two parts of the report on introducing the knowledge and models we have discovered. The third part deals with the methods, algorithms and comments about the results. In doing the analysis and modeling we used only the training data set of KDD-Cup 98, reserving the evaluation data set for final unbiased model evaluation for our 5 suggested models only. If our goal had been only knowledge discovery, it might have been useful to utilize the evaluation data too, especially the donors. It is probably possible to find more interesting phenomena with almost 10000 donors than with under 5000. MAIN RESULTS"""	algorithm;data mining;sigkdd;test set	Saharon Rosset;Aron Inger	2000	SIGKDD Explorations	10.1145/846183.846204	computer science;artificial intelligence;data science;machine learning;decision tree;data mining;artificial neural network	ML	4.936446183306855	-19.0290484683694	29855
83f4e9a10321d6044d5c3107d4e67170dae1a327	ilp-based heuristic method for the multi-period survivable network augmentation problem		This paper studies the optimization of a multi-period survivable network augmentation (MPSNA) problem. An integer linear programming (ILP) model is formulated. Because the MPSNA problem is very time-consuming to solve even for a very small network, a four-stage ILP-based heuristic approach is developed. Simulations on six test case networks of various sizes are carried out. The experimental results show that our approach is effective at solving the MPSNA problem.	computer simulation;heuristic;integer programming;linear programming;mathematical optimization;test case	Yali Wang;John Doucette	2016	2016 8th International Workshop on Resilient Networks Design and Modeling (RNDM)	10.1109/RNDM.2016.7608268	mathematical optimization;heuristic;computer science	AI	17.32296740666254	-2.0503011389284227	29936
7a388cabcf473721c5aab4b45c9ea6508737d784	a parallel algorithm for minimizing the number of routes in the vehicle routing problem with time windows	parallel algorithm;approximation algorithms;mpi library;guided local search;vehicle routing problem with time windows;heuristics	A parallel algorithm for minimizing the number of routes in the vehicle routing problem with time windows (VRPTW) is presented. The algorithm components cooperate periodically by exchanging their best solutions with the lowest number of routes found to date. The objective of the work is to analyze speedup, achieved accuracy of solutions and scalability of the MPI implementation. For comparisons the selected VRPTW tests are used. The derived results justify the proposed parallelization concept. By making use of the parallel algorithm the twelve new best-known solutions for Gehring and Homberger's benchmarking tests were found.	microsoft windows;parallel algorithm;vehicle routing problem	Miroslaw Blocho;Zbigniew J. Czech	2011		10.1007/978-3-642-31464-3_26	mathematical optimization;parallel computing;computer science;theoretical computer science;heuristics;distributed computing;parallel algorithm;approximation algorithm;guided local search	Theory	21.92145098828953	2.585867695147338	29937
255a47fdf82372ec1f3b94f96ca7a11bfb94315d	mature or emerging markets: competitive duopoly investment decisions	game theory;or in strategic planning;project portfolio management	We develop a competitive investment model wherein two competing firms consider investing into two projects targeting, separately, a mature and an emerging market. The returns firms obtain from investments into these markets are assumed to follow an S-shaped curve and depend on both firms’ actions. Considering symmetric environments (in terms of investment opportunities), we find that different forms of interactions may arise (e.g., Prisoner’s Dilemma and Game of Chicken) and outline corresponding strategies that offer higher returns by exploiting first-mover advantages, cooperation opportunities and aggressive choices. We also discuss the market conditions that can lead to these outcomes. Finally, considering non-symmetric environments, we show that a firm may be better off when its competitor’s budget increases. 2013 Elsevier B.V. All rights reserved.	competitive programming;interaction;prisoner's dilemma	Mark S. Zschocke;Benny Mantin;Elizabeth M. Jewkes	2013	European Journal of Operational Research	10.1016/j.ejor.2013.01.021	game theory;economics;marketing;microeconomics;market economy;commerce;project portfolio management	AI	-2.4187238148626378	-5.933753625698017	29978
74f4c2a066b809272a2de1a42b97017f09c067a9	sspmo: a scatter tabu search procedure for non-linear multiobjective optimization	scatter search;continuous variable;evolutionary multiobjective optimization;efficient frontier;multiobjective optimization;global optimization;tabu search;nonlinear multiobjective optimization;multiobjective metaheuristics	We describe the development and testing of a metaheuristic procedure, based on the scatter search methodology, for the problem of approximating the efficient frontier of nonlinear multiobjective optimization problems with continuous variables. Recent applications of scatter search have shown its merit as a global optimization technique for single-objective problems. However, the application of scatter search to multiobjective optimization problems has not been fully explored in the literature. We test the proposed procedure on a suite of problems that have been used extensively in multiobjective optimization. Additional tests are performed on instances that are an extension of those considered classic. The tests indicate that our extension of the basic scatter search framework is a viable alternative for multiobjective optimization.	approximation algorithm;computation;experiment;global optimization;linear function;mathematical optimization;metaheuristic;multi-objective optimization;nonlinear system;tabu search;test set	Julián Molina Luque;Manuel Laguna;Rafael Martí;Rafael Caballero	2007	INFORMS Journal on Computing	10.1287/ijoc.1050.0149	efficient frontier;mathematical optimization;tabu search;computer science;multi-objective optimization;machine learning;mathematics;management science;continuous optimization;metaheuristic;global optimization	AI	23.650457746087586	-3.328163592500162	29992
0f7477fa89e89b5d9c59565def3945511f3de44f	damage sharing may not be enough: an analysis of an ex-ante regulation policy for data breaches		Data breaches, occurring either on the customer’s PCs or on the service provider’s equipment, expose customers to significant economic losses. An ex-ante regulation policy that apportions a fraction of the losses to the service provider (a damage-sharing policy) may reduce the burden for the customer and lead the service provider to invest more in security. We analyse this regulation policy through a game-theoretic approach, where the customer acts on the amount of personal information it reveals, and the service provider acts on the amount of security investments. We show that the game exhibits a single Nash equilibrium in a realistic scenario. In order to optimize the social welfare, the regulator has to choose the fraction of damage apportioned to the service provider. We show that the policy is relatively ineffective unless the fraction of damage charged to the service provider is quite large, beyond 60%. On the other hand, if the policy is applied with a large damage-sharing factor, the overall social welfare falls heavily.		Giuseppe D'Acquisto;Marta Flamini;Maurizio Naldi	2012		10.1007/978-3-642-32287-7_13	data mining;computer security	HCI	-3.4558904930407377	-8.581052410373756	30006
5fdd78fb13a4e9af2eefc36a0af812186edc2f5d	a model for allocating retail outlet building resources across market areas	retail trade;management;working paper;store location	"""Many factors affect retail outlet profitability, including market potential, distribution and product costs, market pricing levels, cost (and availability) of land or space and the relationship between share of outlets and share of markets. A model is presented which was used to plan building decisions for outlets for a consumer product across time and across market arez The model has been in use for a number of years and has provided important input for budgetting and planning decisions. The implementation process for this model is also discussed. The model and its use provide an example of what the authors believe to be """"successful"""" management science application — the characteristics of and reasons for this success are discussed."""	management science	Gary L. Lilien;Ambar G. Rao	1976	Operations Research	10.1287/opre.24.1.1	market share;marketing;retail;market share analysis;commerce	ECom	1.2299069987180504	-7.365627241947833	30049
cb1dec534cf9315fe247d5efd8d691e93568be92	notes on possibilistic variances of fuzzy numbers	possibilistic variance;matematicas aplicadas;mathematiques appliquees;loi probabilite;ley probabilidad;fuzzy number;possibilistic covariance;probability distribution;probability theory;possibilistic theory;possibilistic mean;applied mathematics;variance;variancia	Abstract   The definitions of two crisp possibilistic variances of a fuzzy number   A  ,    Var    (  A  )    and      Var     ′     (  A  )   , were introduced by Carlsson and Fuller. In this work, we show that many properties of variance in probability theory are preserved by      Var     ′     (  A  )   . We also get the important relationships between    Var    (  A  )    and      Var     ′     (  A  )   .	fuzzy number	Wei-Guo Zhang;Ying-Luo Wang	2007	Appl. Math. Lett.	10.1016/j.aml.2007.03.002	probability distribution;probability theory;econometrics;mathematical optimization;fuzzy number;mathematics;variance;statistics	Theory	0.762699287638667	-20.396060176066324	30058
396de2b1796b9e863d7a87665b6c0cd502a8841d	particle filtering-based methods for time to failure estimation with a real-world prognostic application	machine learning;predictive maintenance;particle filtering;time to failure (ttf);modeling	One of core technologies for prognostics is to predict failures before they occur and estimate time to failure (TTF) by using built-in predictive models. The predictive model could be either physics-based model or machine learning-based model. Machine learning-based predictive modeling is an emerging application of machine learning to machinery maintenance. Accurate TTF estimation could help performing predictive action “just-in-time”. However, the developed predictive models sometimes fail to provide a precise TTF estimate. To address this issue, we propose a Particle Filtering (PF)-based method to estimate TTF. After introducing the PF-based algorithm, we present the implementation along with the experimental results obtained from a case study of Auxiliary Power Unit (APU) prognostics. To our best knowledge, this is the first application of PF-based method to APU prognostic. The results demonstrated that the PF-based method is useful for estimating TTF for predictive maintenance and it greatly improved TTF estimation precision for APU prognostics.	algorithm;apu nahasapeemapetilon;built-in self-test;experiment;importance sampling;just-in-time compilation;machine learning;particle filter;predictive modelling;rapid update cycle;sampling (signal processing);test template framework	Chunsheng Yang;Qingfeng Lou;Jie Liu;Yubin Yang;Qiang Qiang Cheng	2017	Applied Intelligence	10.1007/s10489-017-1083-0	machine learning;computer science;artificial intelligence;predictive maintenance;prognostics;particle filter	AI	13.706551724721816	-14.965974121332899	30068
1dfd6678b3651853cce3600aedf8a82387954878	real time scheduling by coordination for optimizing operations of equipments in a container terminal	mutual information portals feedback explosions artificial intelligence computer science mathematics web and internet services information analysis protocols;category information;distance function;cooking recipes data base;market basket data;cooking recipes mutual information based clustering market basket data profiling users web sites user satisfaction category information;profiling users;user profile;user profiling;mutual information based clustering;cooking recipes;marketing;web sites;categorical clustering;web sites marketing;mutual information;user satisfaction	To maximize the productivity of a container terminal, it is important to have the operations of the equipments of different types optimized and synchronized. However, any attempt to search for a globally optimum operation schedule is prohibitive because of the real time constraint. The real time scheduling method proposed in this paper first generates schedules of individual equipments by using simple heuristics. Subsequently, the method analyzes the schedules to locate the job that causes the longest delay and tries to reduce the delay by adjusting the individual schedules of the relevant equipments. This adjustment process repeats until the quality of the schedules is satisfactory. Simulation experiments have shown that our method produces more efficient operation schedules in real time than other methods based on simulated annealing or heuristic-only strategies.	experiment;heuristic (computer science);optimizing compiler;real-time computing;schedule (computer science);schedule (project management);scheduling (computing);simulated annealing;simulation	Eun Yeong Ahn;Kiyeok Park;Byoungho Kang;Kwang Ryel Ryu	2007	19th IEEE International Conference on Tools with Artificial Intelligence(ICTAI 2007)	10.1109/ICTAI.2007.138	metric;computer science;data mining;mutual information;world wide web;statistics	Robotics	8.168184242979589	3.6783574930020113	30186
124535b2933a9a8a482ad6d0ad601d6ce3af7e74	optimal mechanisms with simple menus	menu representation;optimal mechanism design;revenue maximization	We consider revenue-optimal mechanism design for the case with one buyer and two items. The buyer's valuations towards the two items are independent and additive. In this setting, optimal mechanism is unknown for general valuation distributions. We obtain two categories of structural results that shed light on the optimal mechanisms. These results can be summarized into one conclusion: under certain conditions, the optimal mechanisms have simple menus.  The first category of results state that, under a centain condition, the optimal mechanism has a monotone menu. In other words, in the menu that represents the optimal mechanism, as payment increases, the allocation probabilities for both items increase simultaneously. This theorem complements Hart and Reny's recent result regarding the nonmonotonicity of menu and revenue in multi-item settings. Applying this theorem, we derive a version of revenue monotonicity theorem that states stochastically superior distributions yield more revenue. Moreover, our theorem subsumes a previous result regarding sufficient conditions under which bundling is optimal[Hart and Nisan 2012].  The second category of results state that, under certain conditions, the optimal mechanisms have few menu items. Our first result in this category says that, for certain distributions, the optimal menu contains at most 4 items. The condition admits power (including uniform) density functions. Our second result in this category works for a weaker (hence more general) condition, under which the optimal menu contains at most 6 items. This condition is general enough to include a wide variety of density functions, such as exponential functions and any function whose Taylor series coefficients are nonnegative. Our last result in this category works for unit-demand setting. It states that, for uniform distributions, the optimal menu contains at most 5 items. All these results are in sharp contrast to Hart and Nisan's recent result that finite-sized menu cannot guarantee any positive fraction of optimal revenue for correlated valuation distributions.	coefficient;debug menu;time complexity;utility functions on indivisible goods;value (ethics);monotone	Zihe Wang;Pingzhong Tang	2014		10.1145/2600057.2602863	mathematical optimization;mathematics;microeconomics;mathematical economics;welfare economics	ECom	-2.953449099265613	-1.9193003350031301	30223
0f3194e98be558e3615b519d04b51738db05c0f1	variational inverse control with events: a general framework for data-driven reward definition		The design of a reward function often poses a major practical challenge to realworld applications of reinforcement learning. Approaches such as inverse reinforcement learning attempt to overcome this challenge, but require expert demonstrations, which can be difficult or expensive to obtain in practice. We propose variational inverse control with events (VICE), which generalizes inverse reinforcement learning methods to cases where full demonstrations are not needed, such as when only samples of desired goal states are available. Our method is grounded in an alternative perspective on control and reinforcement learning, where an agent’s goal is to maximize the probability that one or more events will happen at some point in the future, rather than maximizing cumulative rewards. We demonstrate the effectiveness of our methods on continuous control tasks, with a focus on highdimensional observations like images where rewards are hard or even impossible to specify.	reinforcement learning;variational principle	Justin Fu;Avi Singh;Dibya Ghosh;Larry Yang;Sergey Levine	2018			artificial intelligence;computer science;machine learning;inverse;reinforcement learning;data-driven	ML	21.92610559823017	-20.010671534559965	30374
8acb607637d2840e3afcabe3c59bdf0918936703	security constrained multi-period optimal power flow by a new enhanced artificial bee colony	inter temporal constraints;eabc;multi period framework;dscopf	Presenting a new operation function for power systems, i.e. DSCOPF.Proposing a new stochastic search method named EABC.Adapting the proposed EABC as the solution approach of DSCOPF.Extensive testing of the presented model and solution approach. Security constrained optimal power flow (SCOPF) is an important operation function for dispatching centers of current power systems. It optimizes operating conditions of the system, while saves its security. However, SCOPF in its present form focuses on a single time interval, which is known as static SCOPF. A multi-period SCOPF model, referred to as dynamic SCOPF (DSCOPF) in this paper, is presented. It extends static SCOPF to multi-period frameworks considering the inter-temporal constraints. The proposed DSCOPF is a more practical operation function and can optimize operating conditions of the system more realistically compared to traditional SCOPF. Moreover, to solve this DSCOPF model, considering its nonlinear and non-convex behavior, a new stochastic search method is presented, which is an enhanced version of artificial bee colony (ABC) algorithm. The proposed enhanced ABC (EABC) has high exploration capability and can discover different areas of the solution space. Also, it is a robust algorithm and has low sensitivity with respect to the initial population. Effectiveness of the proposed EABC is extensively illustrated on various test cases.	artificial bee colony algorithm	Saber Armaghani;Nima Amjady;Oveis Abedinia	2015	Appl. Soft Comput.	10.1016/j.asoc.2015.08.024	mathematical optimization;simulation;artificial intelligence;machine learning	ECom	17.497853162968205	-3.820041766277402	30390
5aa377596d458a17b350cf7fa8a09042059b422d	a study of the optimal use period and number of minimal repairs of a repairable product after the warranty expires	intervalo tiempo;modelizacion;occupation time;reliability;replacement;new product;remplacement;two phase medium;garantie contre risque;maintenance;numerical method;medio difasico;milieu diphasique;producto nuevo;time interval;lead time;modelisation;cout moyen;temps occupation;metodo numerico;average cost;warranty;coste medio;rupture;defaillance;tiempo ocupacion;mantenimiento;reparation;produit nouveau;garantia contra riesgo;reemplazo;tiempo puesta en marcha;failures;reparacion;modeling;temps mise en oeuvre;fallo;ruptura;methode numerique;repair;new products;intervalle temps	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;primary source	Jhy-Ping Jhang	2005	Int. J. Systems Science	10.1080/00207720500160191	mean time between failures;systems modeling;numerical analysis;engineering;reliability;mathematics;forensic engineering;algorithm;new product development;statistics	Robotics	6.360093429199164	-2.6041162973909255	30491
9645091d7f9f8c641a5971b34954898cd4b5c121	algorithms for uniform optimal strategies in two-player zero-sum stochastic games with perfect information	stochastic games multi agent markov decision processes perfect information uniform optimal strategies optimality;optimality range;perfect information;uniform optimal strategies;eurecom ecole d ingenieur telecommunication centre de recherche graduate school research center communication systems;multi agent markov decision processes;stochastic games;computation	In stochastic games with perfect information, in each state at m os one player has more than one action available. We propose two algorithms which find t e uniform optimal strategies for zero-sum two-player stochastic games with perfect information. Such strategies are optimal for the long term average criterio n as well. We prove the convergence for one algorithm, which presents a higher c omplexity than the other one, for which we provide numerical analysis.	algorithm;numerical analysis	Konstantin Avrachenkov;Laura Cottatellucci;Lorenzo Maggi	2012	Oper. Res. Lett.	10.1016/j.orl.2011.10.005	mathematical optimization;simulation;computer science;perfect information;computation;mathematics;mathematical economics;algorithm	ML	0.2537757975788621	-0.5591111856088539	30511
131c52c193def5c829d9daca04641042e629284e	evolutionary simulations of maternal effects in artificial developmental systems	dna;modelizacion;herencia;parite;pulga de dna;puce a dna;heritage;intelligence artificielle;nuclear dna;parity;algoritmo genetico;modelisation;vie artificielle;maternal effect;dna chip;algorithme genetique;paridad;artificial intelligence;algorithme evolutionniste;genetic algorithm;algoritmo evolucionista;inteligencia artificial;evolutionary algorithm;reseau neuronal;inheritance;artificial evolution;modeling;red neuronal;artificial life;neural network	Maternal influence on offspring goes beyond strict nuclear (DNA) inheritance: inherited maternal mRNA, mitochondria, caring and nurturing are all additional sources that affect offspring development, and they can be also shaped by evolution. These additional factors are called maternal effects, and their important role in evolution is well established experimentally. This paper presents two models for maternal effects, based on a genetic algorithm and simulated development of neural networks. We extended a model by Eggenberger by adding two mechanisms for maternal effects: the first mechanism attempts to replicate maternal cytoplasmic control, while the second mechanism replicates interactions between the fetus and the uterine environment. For examining the role of maternal effects in artificial evolution, we evolved networks for the odd-3-parity problem, using increasing rates of maternal influence. Experiments have shown that maternal effects increase adaptiveness in the latter model.	artificial neural network;computer simulation;evolutionary algorithm;experiment;genetic algorithm;interaction;self-replicating machine	Artur Matos;Reiji Suzuki;Takaya Arita	2005		10.1007/11553090_11	maternal effect;systems modeling;genetic algorithm;dna microarray;parity;computer science;artificial intelligence;nuclear dna;evolutionary algorithm;dna;artificial life	AI	23.162948634764653	-12.28041152056312	30549
1666c9f9313355b3c990f3d9a277a201529c28af	integrated production planning and preventive maintenance in deteriorating production systems	preventive maintenance;production system;product line;decision maker;mixed integer program;computer experiment;failure rate;production planning;failure prone manufacturing systems;manufacturing system	This paper discusses the issue of integrating production planning and preventive maintenance in manufacturing production systems. In particular, it tackles the problem of integrating production and preventive maintenance in a system composed of parallel failure-prone production lines. It is assumed that when a production line fails, a minimal repair is carried out to restore it to an ‘as-bad-as-old’ status. Preventive maintenance is carried out, periodically at the discretion of the decision maker, to restore the production line to an ‘as-good-as-new’ status. It is also assumed that any maintenance action, performed on a production line in a given period, reduces the available production capacity on the line during that period. The resulting integrated production and maintenance planning problem is modeled as a nonlinear mixed-integer program when each production line implements a cyclic preventive maintenance policy. When noncyclical preventive maintenance policies are allowed, the problem is modeled as a linear mixed-integer program. A Lagrangian-based heuristic procedure for the solution of the first planning model is proposed and discussed. Computational experiments are carried out to analyze the performance of the method for different failure rate distributions, and the obtained results are discussed in detail. 2008 Elsevier Inc. All rights reserved.	computation;experiment;failure rate;heuristic;integer programming;lagrangian (field theory);line level;linear programming;nonlinear system	El Houssaine Aghezzaf;Najib M. Najid	2008	Inf. Sci.	10.1016/j.ins.2008.05.007	preventive maintenance;decision-making;computer experiment;failure rate;production system	AI	7.543989548277724	-0.8174949249670861	30620
808f7a83a987fedf68304d655e4ce7b8a414d30e	analysis of toy model for protein folding based on particle swarm optimization algorithm	modelizacion;swarm intelligence;algoritmo busqueda;proteine;complexite calcul;algorithme recherche;juguete;search algorithm;biologia molecular;structure proteine;modelisation;protein structure;complejidad computacion;particle swarm optimizer;jouet;optimizacion enjambre particula;computational complexity;molecular biology;protein structure prediction;toy;ground state;optimisation essaim particule;algorithme evolutionniste;protein folding;algoritmo evolucionista;proteina;evolutionary algorithm;reseau neuronal;particle swarm optimization algorithm;protein;modeling;red neuronal;neural network;biologie moleculaire	One of the main problems of computational approaches to protein structure prediction is the computational complexity. Many researches use simplified models to represent protein structure. Toy model is one of the simplification models. Finding the ground state is critical to the toy model of protein. This paper applies Particle Swarm Optimization (PSO) Algorithm to search the ground state of toy model for protein folding, and performs experiments both on artificial data and real protein data to evaluate the PSO-based method. The results show that on one hand, the PSO method is feasible and effective to search for ground state of toy model; on the other hand, toy model just can simulate real protein to some extent, and need further improvements.	algorithm;computational complexity theory;experiment;ground state;interaction energy;lattice model (physics);level of detail;maxima and minima;particle swarm optimization;protein structure prediction;simulation	Juan Liu;Longhui Wang;Lianlian He;Feng Shi	2005		10.1007/11539902_78	protein folding;protein structure;systems modeling;swarm intelligence;computer science;bioinformatics;artificial intelligence;machine learning;evolutionary algorithm;protein structure prediction;mathematics;ground state;computational complexity theory;artificial neural network;algorithm;search algorithm	Comp.	23.739661861505965	-11.93980844192692	30635
72936f098d7b032ef0404a38bcc424d793207695	asset network planning: integration of environmental data and sensor performance for counter piracy	environmental forecasts;sensor performance surfaces;path planning;multi objective optimization;surveillance satellites optimization measurement planning marine vehicles sea surface;counter piracy environmental forecasts sensor performance surfaces multi objective optimization sensor networks path planning;sensor networks;counter piracy	An operation planning system, integrating dynamic environmental forecasts and satellite Automatic Identification System sensor performance surfaces, to improve maritime traffic situation awareness is proposed and tested. Multi-objective evolutionary algorithms are used to optimize a network of monitoring assets with respect to a combined surveillance and piracy activity risk metric, the network area coverage and the mission cost, under given spatial and kinematic constraints. Pareto efficient solutions are provided, each representing a tradeoff among mission objectives. Tests in a counter piracy operational scenario with real-world hindcast data and sensor performance surfaces show the effectiveness of the methodology in improving surveillance efficiency.	automatic identification and data capture;backtesting;evolutionary algorithm;pareto efficiency;ring counter;sensor	Raffaele Grasso;Paolo Braca;John C. Osler;Jim Hansen	2013	21st European Signal Processing Conference (EUSIPCO 2013)	10.5281/zenodo.43743	simulation;engineering;operations management;computer security	AI	13.71127006480944	-7.947955916979602	30674
b2bc4754aeef9e83f6620d538ae0ce4d09047f4a	challenges in scheduling electric vehicle charging with discrete charging rates in ac power networks		To meet the substantial demand for electrified transportation, a high level of penetration of electric vehicles (EVs) will be expected to incur considerable impacts on the reliability of electricity grid. Hence, there requires an intelligent scheduling mechanism for EV charging for maintaining the electricity grid within the operating limits and absorbing intermittent renewable energy. This note discusses the challenges in designing practical charging algorithms considering common discrete charging rates with minimum power requirements in various charging modes over alternating current (AC) power networks. We present the basic ideas of applying convex relaxation and proper rounding techniques to tackle the combinatorial optimization problem of EV charging.	algorithm;combinatorial optimization;convex optimization;extended validation certificate;high-level programming language;linear programming relaxation;mathematical optimization;optimization problem;requirement;rounding;schedule (project management);scheduling (computing);transcranial alternating current stimulation	Majid Khonji;Sid Chi-Kin Chau;Khaled M. Elbassioni	2018		10.1145/3208903.3208934	grid;renewable energy;combinatorial optimization;control engineering;alternating current;scheduling (computing);ac power;electric vehicle;electricity;computer science	Theory	4.1794051675056245	4.052894702933499	30694
0c1b51e478fdf554b896f15a8fd127ec84ec5f7b	ironing without control	d80;monotonicity constraint;mechanism design;c60;c70;ironing;optimization	I extend Myerson’s [R. Myerson, Optimal auction design, Math. Oper. Res. 6 (1981) 58–73] ironing technique to more general objective functions. The approach is based on a generalized notion of virtual surplus which can be maximized pointwise even when the monotonicity constraint implied by incentive compatibility binds. It is applicable to quasilinear principalagent models where the standard virtual surplus is weakly concave in the allocation or appropriately separable in the allocation and type. No assumptions on allocation rules are required beyond monotonicity.	concave function;norm (social)	Juuso Toikka	2011	J. Economic Theory	10.1016/j.jet.2011.06.003	mechanism design;principal–agent problem;adverse selection;economics;incentive compatibility;operations management;microeconomics;mathematical economics;iron;welfare economics	Theory	-4.359150285061171	-2.7934927703118992	30730
1e274d2bf5f0293d35e7e876980771cef9b89c1a	uav/ugv search and capture of goal-oriented uncertain targets*this research was supported in part by isf grant #1337/15 and part by a grant from most, israel and the jst japan		This paper considers a new, complex problem of UAV/UGV collaborative efforts to search and capture attackers under uncertainty. The goal of the defenders (UAV/UGV team) is to stop all attackers as quickly as possible, before they arrive at their selected goal. The uncertainty considered is twofold: the defenders do not know the attackers' location and destination, and there is also uncertainty in the defenders' sensing. We suggest a real-time algorithmic framework for the defenders, combining entropy and stochastic-temporal belief, that aims at optimizing the probability of a quick and successful capture of all of the attackers. We have empirically evaluated the algorithmic framework, and have shown its efficiency and significant performance improvement compared to other solutions.		Mor Sinay;Noa Agmon;Oleg Maksimov;Guy Levy;Moshe Bitan;Sarit Kraus	2018	2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2018.8594273	control engineering;task analysis;operations research;performance improvement;goal orientation;computer science	Robotics	19.536802057132707	-14.633847141053225	30759
dd1fae7c0960ee57622907c2c24a7afb94275b00	case mix classification and a benchmark set for surgery scheduling	benchmark set;classification;surgery scheduling;proximity	Numerous benchmark sets exist for combinatorial optimization problems. However, in healthcare scheduling, only a few benchmark sets are known, mainly focused on nurse rostering. One of the most studied topics in the healthcare scheduling literature is surgery scheduling, for which there is no widely used benchmark set. An effective benchmark set should be diverse, reflect the real world, contain large instances, and be extendable. This paper proposes a benchmark set for surgery scheduling algorithms, which satisfies these four criteria. Surgery scheduling instances are characterized by an underlying case mix, which describes the volume and properties of the surgery types. Given a case mix, unlimited random instances can be generated. A complete surgery scheduling benchmark set should encompass the diversity of prevalent case mixes. We therefore propose a case mix classification scheme, which we use to typify both real-life and theoretical case mixes that span the breadth of possible case mix types. Our full benchmark set contains 20,880 instances, with a small benchmark subset of 146 instances. The instances are generated based on real-life case mixes (11 surgical specialties), as well as theoretical instances. The instances were generated using a novel instance generation procedure, which introduces the concept of “instance proximity” to measure the similarity between two instances, and which uses this concept to generate sets of instances that are as diverse as possible. B Gréanne Leeftink a.g.leeftink@utwente.nl B Erwin W. Hans e.w.hans@utwente.nl 1 Centre for Healthcare Operations Improvement and Research (CHOIR), University of Twente, P.O. Box 217, 7500 AE Enschede, The Netherlands	algorithm;benchmark (computing);cellular automaton;combinatorial optimization;crew scheduling;extensibility;geforce 7 series;mathematical optimization;nurse scheduling problem;open-shop scheduling;real life;scheduling (computing)	Gréanne Leeftink;Erwin W. Hans	2018	J. Scheduling	10.1007/s10951-017-0539-8	mathematical optimization;classification scheme;combinatorial optimization;scheduling (computing);computer science;surgery;data mining;case mix index	ML	20.696068530491424	3.0149157684787538	30774
11f65dcd752827b5ddcfdc16308a06fff2e97fc2	analytics branching and selection for the capacitated multi-item lot sizing problem with nonidentical machines		We study a capacitated multi-item lot sizing problem with nonidentical machines. For the problem, we propose several mathematical formulations and their peritem and per-period Dantzig-Wolfe decompositions, followed by exploring their relative efficiency in obtaining lower and upper bounds. Additionally, we observe that the optimum has a correlation with the solution values of the pricing subproblems of Dantzig- Wolfe decompositions, along with the solution values of the uncapacitated problems and linear programming (LP) relaxation. Using these solution values, we build statistical estimation models (i.e., generalized linear models) that give insight on the optimal values, as well as information about how likely a setup variable is to take a value of 1 at an optimal point. We then develop an analytics branching and selection method where the information is utilized for an analytics-based branching and selection procedure to fix setup variables, which is, to our knowledge, the first research using likelihood information to improve solution qualities. This method differs from approaches that use solution values of LP relaxation (e.g., relaxation induced neighborhood search, feasibility pump, and LP and fix). The application is followed by extensive computational tests. Comparisons with other methods indicate that the optimization method is computationally tractable and can obtain better results. © 2018 INFORMS.		Tao Wu;Zhe Liang;Canrong Zhang	2018	INFORMS Journal on Computing	10.1287/ijoc.2017.0777	mathematical optimization;efficiency;dantzig–wolfe decomposition;mathematics;column generation;discrete mathematics;branching (version control);linear programming;generalized linear model;analytics	Theory	18.254075723798962	4.049036464229669	30796
d1e3219a4d827150019d9cf2596333e4fef3b4b6	router: a fast and flexible local search algorithm for a class of rich vehicle routing problems	local search algorithm;vehicle routing problem	We describe a flexible indirect search procedure which we have applied for solving a special pick-up and delivery vehicle routing problem with time windows. The heuristic is based on an encoding of a solution as a sequence/permutation of tasks, a cheapest insertion decoding procedure, and, a threshold-accepting like local search meta-heuristic.	heuristic;local search (optimization);microsoft windows;router (computing);search algorithm;vehicle routing problem	Ulrich Derigs;Thomas Döhmer	2004		10.1007/3-540-27679-3_18	policy-based routing;wireless routing protocol;routing table;diffusing update algorithm;mathematical optimization;routing;enhanced interior gateway routing protocol;static routing;zone routing protocol;equal-cost multi-path routing;dynamic source routing;local search;multipath routing;destination-sequenced distance vector routing;vehicle routing problem;distance-vector routing protocol;mathematics;distributed computing;best-first search;routing protocol;link-state routing protocol;computer network;guided local search	Robotics	24.237609098599496	2.823902272714773	30804
5137fd386eaf3885c0f0bc55eac69c3c43fc4628	a bayesian perspective on residential demand response using smart meter data		The widespread deployment of Advanced Metering Infrastructure has made granular data of residential electricity consumption available on a large scale. One field of research that relies on such granular consumption data is Residential Demand Response, where individual users are incentivized to temporarily reduce their consumption during periods of high marginal cost of electricity. To quantify the economic potential of Residential Demand Response, it is important to estimate the reductions during Demand Response hours, taking into account the heterogeneity of electricity users. In this paper, we incorporate latent variables representing behavioral archetypes of electricity users into the process of short-term load forecasting with Machine Learning methods, thereby differentiating between varying levels of energy consumption. The latent variables are constructed by fitting Conditional Mixture Models of Linear Regressions and Hidden Markov Models on smart meter data of a Residential Demand Response program in the western United States. We observe a notable increase in the accuracy of short-term load forecasts compared to the case without latent variables. We estimate the reductions during Demand Response events conditional on the latent variables and discover a higher DR reduction among users with automated smart home devices compared to those without.	hidden markov model;home automation;latent variable;machine learning;marginal model;markov chain;mixture model;smart meter;software deployment	Datong Zhou;Maximilian Balandat;Claire J. Tomlin	2016	2016 54th Annual Allerton Conference on Communication, Control, and Computing (Allerton)	10.1109/ALLERTON.2016.7852373	econometrics;simulation;engineering	AI	8.380836838279832	-16.406619996485706	30821
c72a3cdc5722aa817f3bd1ecf78b95e993744a68	fuzzy cardinality based evaluation of quantified sentences	fuzzy set;computacion informatica;ciencias basicas y experimentales;point of view;grupo a	Quanti®ed statements are used in the resolution of a great variety of problems. Several methods have been proposed to evaluate statements of types I and II. The objective of this paper is to study these methods, by comparing and generalizing them. In order to do so, we propose a set of properties that must be ful®lled by any method of evaluation of quanti®ed statements, we discuss some existing methods from this point of view and we describe a general approach for the evaluation of quanti®ed statements based on the fuzzy cardinality and fuzzy relative cardinality of fuzzy sets. In addition, we discuss some concrete methods derived from the mentioned approach. These new methods ful®ll all the properties proposed and, in some cases, they provide an interpretation or generalization of existing methods. Ó 2000 Elsevier Science Inc. All rights reserved.	fuzzy set	Miguel Delgado;Daniel Sánchez;M. Amparo Vila	2000	Int. J. Approx. Reasoning	10.1016/S0888-613X(99)00031-6	type-2 fuzzy sets and systems;computer science;artificial intelligence;fuzzy number;data mining;mathematics;fuzzy set;fuzzy set operations;algorithm	AI	-2.7445702805301253	-23.175275768064747	30826
37aae5812c50e57916c1a3096f41b17cdf07f43f	a new method multi-factor trend regression and its application to economy forecast in jiangxi	time series forecasting;predictive value;multifactor trend regression;time series;time series economics forecasting theory regression analysis sequential estimation;sequential regression multifactor trend regression economy forecast jiangxi province time series forecasting regression analysis simultaneous independent variables;simultaneous independent variables;economy forecast;forecasting theory;economic forecasting regression analysis predictive models linear regression equations multivariate regression data mining application software educational institutions data engineering;trend regression;regression analysis;economics;sequential estimation;jiangxi province;sequential regression	The principle of a new method called Trend Regression is introduced and applied to the economy forecast of Jiangxi Province. The method improved previous time series forecasting method in which only self-extension is done and multiple factors (variables) are not taken into consideration. Also, it got over the weakness of forecasting by general regression analysis that relies on simultaneous independent variables. A time series is the function of multiple factors. The values (independent variables) in a period may affect the value (dependent variable) to be predicated in the next period. The nearer the sample time to the predicted time, the more important the sample to the predict value. By shifting the dependent variable to establish models, sequential regression and prediction can be realized. In this way the trend of information can be minded.	dependent ml;sampling (signal processing);time series	Yuechao Ding	2008	First International Workshop on Knowledge Discovery and Data Mining (WKDD 2008)	10.1109/WKDD.2008.111	econometrics;time series;data mining;regression analysis;statistics;cross-sectional regression	ML	6.758259690259367	-17.99475861644806	30858
5865d231f3f5678c28593ad3a6343b65a4e0f574	towards enabling deep learning techniques for adaptive dynamic programming		Human-level control through deep learning and deep reinforcement learning have revealed the unique and powerful potentials through a very complex Go game. The AlphaGo, developed by Google DeepMind, has beat the top Go game player early this year. The scientific and technological advancement behind the success of AlphaGo attracted researchers from multiple areas, including machine learning, artificial intelligence, computational intelligence and so on. Adaptive dynamic programming (ADP) methods have the similar fundamental principle with reinforcement learning, and show strong performance for continuous time and continuous state systems. Deep learning techniques are also possible to be integrated for ADP designs. In this paper, we discuss the key techniques and components in deep reinforcement learning and then present the successful applications for computer games and maze navigation. Future opportunities for deep learning enabled ADP will be discussed at the end.	alphago;artificial intelligence;computational intelligence;deep learning;dynamic programming;machine learning;pc game;polystation;reinforcement learning;usb on-the-go	Zhen Ni;Naresh Malla;Xiangnan Zhong	2017	2017 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2017.7966205	instance-based learning;algorithmic learning theory;robot learning;machine learning;hyper-heuristic;artificial intelligence;competitive learning;simulation;computer science;reinforcement learning;computational learning theory;active learning (machine learning)	AI	19.232272209824043	-20.189222673437975	30874
5fec7315ef1a2d617bc16736f6b8e46a3a7232f8	cabins: a framework of knowledge acquisition and iterative revision for schedule improvement and reactive repair	optimisation sous contrainte;constrained optimization;iterative process;representacion conocimientos;user needs;learning algorithm;job shop scheduling;resource allocation;iterations;resource management;selection;response;algorithme apprentissage;searching;optimizacion con restriccion;proceso iterativo;processus iteratif;estudio caso;bias;heuristic methods;dynamics;quality;scheduling;knowledge acquisition;acquisition;etude cas;optimality criteria;reparation;scheduling problem;artificial intelligence;job shop;ordonamiento;context dependent;control;optimization;asignacion recurso;trade off analysis;reactivities;knowledge representation;allocation ressource;reparacion;representation connaissances;management;models;allocations;knowledge based systems;ordonnancement;adquisicion;repair;modification	Practical schcduling problems generally require allocation of resources in the prrsencr of a large. diverse and typically conflicting set of constraints and optimization criteria. 'The il-strucluredness of both the solution space and the desired objectives make scheduling problems difficult to formalize. This paper describes a chse-based learning method for acquiring context-dependent user oplirnization prcfwences and tradeoffs and using them to incrementally improve schedule quality in prdictive scheduling and reactive schedule management in response to unexpected execution events. Tlic approach, implemented in the CABINS system, uses acquired user preferences to dynamically modify search control to guide schedule improvement. During iterative repair, cases are exploited for: (1) repair action selection, (21 evaluation of intermediate repair results and (3) recovery from revision failures. The method allows the system to dynamically switch between repair heuristic actions, each of which operates wit,h respect 1.0 a particular local view of the problem and offers selective repair advantages. Application oi a repair action tunes the search proccdure to the characteristics of the local repair problem. This is achieved by dynamic modification of the search control bias. There is no a priori charactcrization of the amount or modification that may b e required by repair actions. However, initial experimental results show that the approach is able to (a) capture and effectively utilize user scheduling preferences that were not present in the scheduling model, (b) produce schedules with high quality, without unduly sacrificing efficiency i n predictive schedule generation and rext,ive response to unpredictable execution events along a variety of criteria that have been recognized as important in r ed operating environments.	action selection;belief revision;context-sensitive language;display resolution;emoticon;feasible region;heuristic;iterative and incremental development;iterative method;knowledge acquisition;mathematical optimization;schedule (computer science);scheduling (computing);user (computing)	Kazuo Miyashita;Katia P. Sycara	1995	Artif. Intell.	10.1016/0004-3702(94)00089-J	job shop scheduling;selection;dynamics;simulation;iteration;resource allocation;computer science;artificial intelligence;context-dependent memory;bias;iterative and incremental development;scheduling;scientific control	AI	15.989022829560897	-6.67649336281447	30876
628e08155a6551b01dcfd7a05508975ae991e995	phase diagram analysis based on a temporal-spatial queueing model	analytical models;ramping;temporal spatial queueing model;phase diagrams;automobiles;traffic queuing;queuing theory;road traffic;queueing theory;traffic control;traffic flow;traffic congestion phase diagram analysis temporal spatial queueing model macroscopic congestion patterns on off ramps ramping traffic scenarios jam queue ramping vehicle newell simplified car following model road flow rate q main ramping road flow rate q ramp macroscopic phase diagram;traffic control queueing analysis analytical models merging;traffic congestion;merging;phase diagram;road traffic automobiles phase diagrams queueing theory;traffic flow theory;traffic flow phase diagram ramping temporal spatial queueing model;queueing analysis;ramps interchanges	In this paper, we propose a simple temporal-spatial queueing model to quantitatively address some typical congestion patterns that were observed around on/off-ramps. In particular, we examine three prime factors that play important roles in ramping traffic scenarios: the time τ<sub>in</sub> for a vehicle to join a jam queue, the time τ<sub>out</sub> for this vehicle to depart from this jam queue, and the time interval <i>T</i> for the ramping vehicle to merge into the mainline. Based on Newell's simplified car-following model, we show how τ<sub>in</sub> changes with the main road flow rate <i>q</i><sub>main</sub>. Meanwhile, <i>T</i> is the reciprocal of the ramping road flow rate <i>q</i><sub>ramp</sub>. Thus, we analytically derive the macroscopic phase diagram plotted on the <i>q</i><sub>main</sub>-versus- <i>q</i><sub>ramp</sub> plane and τ<sub>in</sub>-versus-<i>T</i> plane based on the proposed model. Further study shows that the new queueing model not only reserves the merits of Newell's model on the microscopic level but helps quantify the contributions of these parameters in characterizing macroscopic congestion patterns as well. Previous approaches distinguished phases merely through simulations, but our model could derive analytical boundaries for the phases. The phase transition conditions obtained by this model agree well with simulations and empirical observations. These findings help reveal the origins of some well-known phenomena during traffic congestion.	dvd region code;jam;level of detail;multi-level cell;network congestion;nonlinear system;phase diagram;queueing theory;ramp simulation software for modelling reliability, availability and maintainability;velocity (software development);windows hardware certification kit	Xiqun Chen;Li Li;Zhiheng Li	2012	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2012.2203305	simulation;engineering;traffic flow;newell's car-following model;transport engineering;queueing theory;phase diagram;statistics	Metrics	9.88559963873228	-10.364091316591983	30902
35dd9615062031efd0219d8a14888b89da53ed26	a price forecast model of hydraulic cylinders through integration of case based reasoning and back-propagation neural networks	shapes structures backpropagation case based reasoning forecasting theory hydraulic systems maintenance engineering neural nets pricing procurement servomechanisms;price forecast;neural nets;bp neural networks;case based reasoning integrated bpnn model integrated cbr model model sensitivity producer price index supplier qualification servo cylinder pressure rating cylinder stroke cylinder bore diameter delphi method direct mathematical model economical evaluation procurement plans hydraulic cylinder price forecast model backpropagation neural networks;procurement;pricing;hydraulic cylinder price forecast model;shapes structures;direct mathematical model;cylinder stroke;producer price index;supplier qualification;maintenance engineering;integrated cbr model;backpropagation;model sensitivity;artificial neural networks;servo cylinder;forecasting theory;estimation;price forecast hydraulic cylinders case based reasoning bp neural networks;procurement plans;cylinder bore diameter;servomechanisms;qualifications;cognition;pressure rating;servomotors;predictive models;delphi method;backpropagation neural networks;integrated bpnn model;case based reasoning;economical evaluation;hydraulic systems;predictive models artificial neural networks cognition servomotors qualifications estimation;hydraulic cylinders	Various kinds of hydraulic cylinders are widely used and have large variations in price. It has great significance to establish a hydraulic cylinder price forecast model for the issues of making procurement plans and the economical evaluation of maintenance and remanufacturing. However, it is hard to establish a direct mathematical model because there are many factors influencing the hydraulic cylinder price. The objective of this paper is to develop a price forecast model for hydraulic cylinders through the application of case based reasoning (CBR) and back-propagation neural networks (BPNN). Seven critical factors related to the hydraulic cylinder price have been found out by means of the Delphi method. They are cylinder bore diameter, stroke, pressure rating, servo cylinder or not, installation type, supplier qualification, and Producer Price Index (PPI). The PPI introduced in the model can improve the model sensitivity to market change. Case study is used to prove that both the BPNN model and the integrated CBR and BPNN model are capable of forecasting hydraulic cylinder price. The forecast of the integrated CBR and BPNN model is more accuracy than that of BPNN model.	artificial neural network;backpropagation;case-based reasoning;cylinder seal;cylinder-head-sector;delphi method;embarcadero delphi;mathematical model;pixel density;procurement;servo;software propagation	Min Zhou;Yanxia Chen;Ting Gao	2013	2013 Ninth International Conference on Natural Computation (ICNC)	10.1109/ICNC.2013.6817954	simulation;engineering;operations management;operations research	AI	11.835674877250193	-18.638397010181762	30907
6ee17002ec39b3d371cab00b81f645b1b9987c16	reinforcement learning vs human programming in tetherball robot games	analytical models;trajectory robot kinematics mathematical model games learning artificial intelligence analytical models;abt scholkopf;trajectory;games;mathematical model;learning artificial intelligence;abt schaal;robot players reinforcement learning human programming tetherball robot games robot learning ability complex task solving motor learning framework motor skill learning dynamical motor primitives robot trajectory representation entropy policy search motor framework training high quality skill learning;robot kinematics;trajectory control entropy learning artificial intelligence learning systems mobile robots sport	Reinforcement learning of motor skills is an important challenge in order to endow robots with the ability to learn a wide range of skills and solve complex tasks. However, comparing reinforcement learning against human programming is not straightforward. In this paper, we create a motor learning framework consisting of state-of-the-art components in motor skill learning and compare it to a manually designed program on the task of robot tetherball. We use dynamical motor primitives for representing the robot's trajectories and relative entropy policy search to train the motor framework and improve its behavior by trial and error. These algorithmic components allow for high-quality skill learning while the experimental setup enables an accurate evaluation of our framework as robot players can compete against each other. In the complex game of robot tetherball, we show that our learning approach outperforms and wins a match against a high quality hand-crafted system.	display resolution;importance sampling;iteration;kullback–leibler divergence;mathematical model;opponent process;rl (complexity);reinforcement learning;robot;sample complexity;sampling (signal processing)	Simone Parisi;Hany Abdulsamad;Alexandros Paraschos;Christian Daniel;Jan Peters	2015	2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2015.7354296	temporal difference learning;unsupervised learning;robot learning;games;instance-based learning;challenge point framework;error-driven learning;algorithmic learning theory;simulation;computer science;artificial intelligence;trajectory;social robot;machine learning;mathematical model;learning classifier system;reinforcement learning;active learning;robot kinematics	Robotics	20.385534684115395	-20.549148442922053	30920
eec7f7829b1ec613cd6d9b676ea99fd3210bf44a	on a class of uninorms	increasing operation;triangular norm;idempotent operation;binary operations;triangular conorm;strong negation;uninorm	Binary operations in unit interval have diverse applications in fuzzy set theory as multivalued logical connectives (e.g. triangular norms are multivalued conjunctions and triangular conorms are multivalued disjunction cf. Klement, Mesiar, Pap). On the other hand, aggregation operators on real line can be reduced to an aggregation in unit interval (cf. Yager, Rybalov). This implies the great importance of examination and characterization of such operations. We discuss the structure of binary operations U : [0, 1] → [0, 1].	fuzzy set;logical connective;set theory	Józef Drewniak;Pawel Drygas	2002	International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems	10.1142/S021848850200179X	mathematical analysis;discrete mathematics;binary operation;mathematics;algebra	DB	-0.3722757418488284	-22.90011261024555	30928
120879f5bd48dcc9ddcf528fa3d9f117cd3c93b8	orthogonal wavelet support vector machine for predicting crude oil prices	t technology general;t technology	Previous studies mainly used radial basis, sigmoid, polynomial, linear, and hyperbolic functions as the kernel function for computation in the neurons of conventional support vector machine (CSVM) whereas orthogonal wavelet requires less number of iterations to converge than these listed kernel functions. We proposed an orthogonal wavelet support vector machine (OSVM) model for predicting the monthly prices of West Texas Intermediate crude oil prices. For evaluation purposes, we compared the performance of our results with that of the CSVM, and multilayer perceptron neural network (MLPNN). It was found to perform better than the CSVM, and the MLPNN. Moreover, the number of iterations, and time computational complexity of the OSVM model is less than that of CSVM, and MLPNN. Experimental results suggest that the OSVM is effective, robust, and can efficiently be used for crude oil price prediction. Our proposal has the potentials of advancing the prediction accuracy of crude oil prices, which makes it suitable for building intelligent decision support systems.	orthogonal wavelet;support vector machine	Haruna Chiroma;Sameem Abdul Kareem;Adamu I. Abubakar;Akram M. Zeki;Mohammed Joda Usman	2013		10.1007/978-981-4585-18-7_23	econometrics;mathematical optimization;machine learning;mathematics	ML	8.753489312388648	-20.197240661170245	30931
73c427b7ea2c3da223153a93061369fa3581acf3	akaike information criterion-based conjunctive belief rule base learning for complex system modeling		Abstract Nonlinear complex system modeling has become the basis of many theoretical and practical problems, which requires balancing the correlations between the modeling accuracy and the modeling complexity. However, the two objectives may not be consistent with each other under many practical conditions, especially for complex systems with multiple influential factors. The belief rule base (BRB) has shown advantages in nonlinear complex system modeling under uncertainty. However, most of current works on BRB has focused only on the modeling accuracy. As such, an Akaike Information Criterion (AIC)-based objective, AIC BRB , is deduced to represent both the modeling accuracy (denoted by the Mean Square Error (MSE)) and the modeling complexity (denoted by the number of the parameters). Based on the proposed AIC BRB , a bi-level optimization model and a corresponding bi-level optimization algorithm are developed. Moreover, an empirical optimization path search strategy is proposed for upper-level optimization. The optimization path is comprised of multiple solutions with optimal performance. After the BRB learning process, both the structure and the parameters of BRB are optimized, which identifies the best decision structure of BRB. A numerical multi-extreme function case and a practical pipeline leak detection case are studied. The results show that an optimization path could be identified with a series of optimal solutions. With AIC BRB as the objective, the jointly optimized BRB in the best decision structure can be obtained with an improved modeling accuracy as well as reduced modeling complexity.	akaike information criterion;complex system;rule-based system;systems modeling	Leilei Chang;Zhi-Jie Zhou;Yu-Wang Chen;Xiaobin Xu;Jianbin Sun;Tian-Jun Liao;Xu Tan	2018	Knowl.-Based Syst.	10.1016/j.knosys.2018.07.029	artificial intelligence;complex system;machine learning;complex system;computer science;akaike information criterion;nonlinear system;mean squared error	ML	15.051130049421186	-19.161538434212886	30939
c731a3d8ef9121de7ee6a14dc70e0e774792982c	"""accurate and diverse sampling of sequences based on a """"best of many"""" sample objective"""		"""For autonomous agents to successfully operate in the real world, anticipation of future events and states of their environment is a key competence. This problem has been formalized as a sequence extrapolation problem, where a number of observations are used to predict the sequence into the future. Real-world scenarios demand a model of uncertainty of such predictions, as predictions become increasingly uncertain - in particular on long time horizons. While impressive results have been shown on point estimates, scenarios that induce multi-modal distributions over future sequences remain challenging. Our work addresses these challenges in a Gaussian Latent Variable model for sequence prediction. Our core contribution is a """"Best of Many"""" sample objective that leads to more accurate and more diverse predictions that better capture the true variations in real-world sequence data. Beyond our analysis of improved model fit, our models also empirically outperform prior work on three diverse tasks ranging from traffic scenes to weather data."""		Apratim Bhattacharyya;Bernt Schiele;Mario Fritz	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2018.00885	extrapolation;latent variable model;task analysis;machine learning;artificial intelligence;sampling (statistics);point estimation;computer science;recurrent neural network;data modeling;pattern recognition;ranging	Vision	22.18687989525462	-20.73152073912954	30941
40cacc7ded922fc1b1459d07cb4806b3bb45c1a7	an investigation of lead-time effects in manufacturing/remanufacturing systems under simple push and pull control strategies	analisis numerico;order statistic;analisis sistema;push pull strategy;statistique ordre;manufacturing process;statistical re order point models;costo;lead time;analyse numerique;administracion deposito;numerical analysis;logistics;computer experiment;procedimiento fabricacion;manufacturing;estadistica orden;programacion produccion;gestion stock;production planning and inventory control;system analysis;production planning;tiempo puesta en marcha;analyse systeme;remanufacturing;procede fabrication;planification production;inventory control;temps mise en oeuvre;control strategy;computational experiments;cout	In this paper we extend the PUSH and PULL control strategies defined by van der Laan et al. (E.A. van der Laan, M. Salomon, R. Dekker, Production planning and inventory control for remanufacturable durable products, Working paper 9531/A, Econometric Institute, Erasmus University Rotterdam, The Netherlands, 1995) to evaluate numerically the effects of lead-time duration and lead-time variability on total expected costs in production/inventory systems with remanufacturing. Although both strategies are non-optimal, they are relatively easy to analyse numerically and, more importantly, they are actually used in practice. The most important outcomes of the study are, that for both control strategies: (i) manufacturing lead-times have a larger influence on system costs than remanufacturing lead-times; (ii) a larger remanufacturing lead-time may sometimes result in a cost decrease; and (iii) a larger variability in the manufacturing lead-time may sometimes result in a cost decrease.		Erwin van der Laan;Marc Salomon;Rommert Dekker	1999	European Journal of Operational Research	10.1016/S0377-2217(98)00108-8	inventory control;logistics;order statistic;simulation;computer experiment;input/output;numerical analysis;marketing;operations management;mathematics;system analysis;manufacturing;operations research	Robotics	4.349612837104059	-3.87848646026705	30963
968e3735bf844b8b14d3416f2b594b5ab9435c3a	a mobile application for real-time multimodal routing under a set of users' preferences	mobile communication systems;route guidance;routing;real time multimodal fastest path problem under constraints;shortest path algorithms;multimodal transportation;centralized routing;mobile applications;fastest path problem;intermodal networks;stated preferences;resource constrained shortest path problem	In this article an application for mobile navigation is introduced with the intention to serve users who either do not have access to a private vehicle or prefer combining a vehicle with a public transport mode provided that it yields a reduction to their total travel time. This application guides users to their destination by integrating the use of private and public transport and differs from in-vehicle route guidance systems that suggest routes only for private vehicle users, and from other conventional mobile route guidance systems that propose either a route for public transport users or a route for private vehicle users. In addition, the proposed application suggests intermodal routes that comply with each individual's preferences in an attempt to fulfill the user's needs in complex, urban networks. The data requirements for the mobile application are covered by a real-time database that is handled by a central server (processor) and contains information about traffic congestion, train and bus sched...	mobile app;multimodal interaction;real-time transcription;routing	Konstantinos Gkiotsalitis;Antony Stathopoulos	2015	J. Intellig. Transport. Systems	10.1080/15472450.2013.856712	routing;simulation;computer science;engineering;transport engineering;shortest path problem;computer network	Embedded	15.197693869109454	-0.7231293032315713	31028
639cc01afcc1c78063f7a6bbdae998cd147911c4	a robust utility learning framework via inverse optimization		In many smart infrastructure applications, flexibility in achieving sustainability goals can be gained by engaging end users. However, these users often have heterogeneous preferences that are unknown to the decision maker tasked with improving operational efficiency. Modeling user interaction as a continuous game between noncooperative players, we propose a robust parametric utility learning framework that employs constrained feasible generalized least squares estimation with heteroskedastic inference. To improve forecasting performance, we extend the robust utility learning scheme by employing bootstrapping with bagging, bumping, and gradient boosting ensemble methods. Moreover, we estimate the noise covariance, which provides approximated correlations between players, which we leverage to develop a novel correlated utility learning framework. We apply the proposed methods both to a toy example arising from Bertrand-Nash competition between two firms and to data from a social game experiment designed to encourage energy efficient behavior among smart building occupants. Using occupant voting data for shared resources such as lighting, we simulate the game defined by the estimated utility functions to demonstrate the performance of the proposed methods.	approximation algorithm;bertrand (programming language);ensemble learning;generalized least squares;gradient boosting;nash equilibrium;program optimization;simulation	Ioannis C. Konstantakopoulos;Lillian J. Ratliff;Ming Jin;S. Shankar Sastry;Costas J. Spanos	2018	IEEE Transactions on Control Systems Technology	10.1109/TCST.2017.2699163	econometrics;simulation;engineering;machine learning;statistics	ML	24.424154494493813	-17.497015376853636	31034
6357fec59507e57cffb6037fa04a1087f771a4ed	a study on the application of ga-bp neural network in the bridge reliability assessment	analytical models;neural networks bridges reliability engineering proposals rivers computational intelligence information security information science design engineering civil engineering;poor model adaptability;reliability;bridge reliability assessment genetic algorithms ga bp neural network;low calculation efficiency;neural nets;yangtze river;model adaptation;bridges;backpropagation;bridge reliability assessment;number coding system;civil engineering computing;reliability assessment;bridges structures;artificial neural networks;monitoring;structural beams;reliability backpropagation bridges structures civil engineering computing genetic algorithms neural nets;bp neural network;genetic algorithm;genetic algorithms;masangxi yangtze river bridge ga bp neural network bridge reliability assessment poor model adaptability low calculation efficiency number coding system;neural network model;encoding;ga bp neural network;neural network;masangxi yangtze river bridge;genetic algorithms ga	In the design of the bridge reliability assessment proposal, the application of the BP neural network model can help overcome some shortcomings in the traditional bridge reliability assessment, such as the poor model adaptability, the low calculation efficiency and so on. However, there are also some problems in the BP neural network model, for example, the uneasily determinable initial weights and easily local optimum. The BP neural network model optimized by the application of GA global optimum characteristics can get the optimum relation quickly. Therefore, this paper puts forward a GA-BP neural network model based on the real number coding system to analyze the bridge reliability assessment and applies it to the reliability assessment in Masangxi Yangtze River Bridge. The results of the application show that the GA-BP neural network model has a higher accuracy, higher reliability and application value in the assessment of the engineering works than the traditional BP neural network model.	algorithm;algorithmic efficiency;artificial neural network;global optimization;kiva;local optimum;network model;nonlinear system;software release life cycle	Jianxi Yang;Fan Wang	2008	2008 International Conference on Computational Intelligence and Security	10.1109/CIS.2008.29	genetic algorithm;computer science;artificial intelligence;machine learning;artificial neural network	AI	11.653660085491815	-21.63134458124559	31035
af0904e8074dba07c35bfa7b0217b306a5a02e38	fuzzy fractals and hyperfractals	fuzzy fractal;hausdorff dimension;visualization;fuzzy hyperfractal	In analogy of the relationship between crisp multivalued fractals and the associated hyperfractals studied recently by ourselves, the properties of fuzzy fractals are investigated by means of fuzzy hyperfractals. In particular, we focus on their address structure, Hausdorff dimension and visualization. Two illustrative examples are supplied. © 2016 Published by Elsevier B.V.	fractal;hausdorff dimension	Jan Andres;Miroslav Rypka	2016	Fuzzy Sets and Systems	10.1016/j.fss.2016.01.008	mathematical analysis;discrete mathematics;visualization;topology;computer science;hausdorff dimension;mathematics	SE	-0.06490194511782987	-23.693553101985312	31078
87809df578033ba747fa7dfae9f8a1055d56a749	particle swarm optimization with transition probability for timetabling problems		In this paper, we propose a new algorithm to solve university course timetabling problems using a Particle Swarm Optimization (PSO). PSOs are being increasingly applied to obtain near-optimal solutions to many numerical optimization problems. However, it is also being increasingly realized that PSOs do not solve constraint satisfaction problems as well as other meta-heuristics do. In this paper, we introduce transition probability into PSO to settle this problem. Experiments using timetables of the University of Tsukuba showed that this approach is a more effective solution than an Evolution Strategy.	best practice;cops (software);computation;constraint satisfaction problem;cryptographic service provider;evolution strategy;fitness function;genetic algorithm;heuristic (computer science);markov chain;mathematical optimization;particle swarm optimization;real life;schedule;time complexity;psos	Hitoshi Kanoh;Satoshi Chen	2013		10.1007/978-3-642-37213-1_27	mathematical optimization;artificial intelligence;mathematics;algorithm	AI	23.240043770727905	0.47908137388631095	31103
a97d4e9750bcf27b458e0d10203e719899f1fb1e	dg integrated distribution system expansion planning with uncertainties				Rahul Kumar Malee;Ashok Singh Chundawat;Niharika Maliwar;A. K. Sharma	2018	Journal of Intelligent and Fuzzy Systems	10.3233/JIFS-169784	mathematics;machine learning;artificial intelligence	Robotics	7.013409086468585	-10.382377653661747	31115
8fd6c8c1671f4f15a0e998547ccc0bc097ca272d	modeling and control of cstr using model based neural network predictive control		this paper presents a predictive control strategy based on neural network model of the plant is applied to Continuous Stirred Tank Reactor (CSTR). This system is a highly nonlinear process; therefore, a nonlinear predictive method, e.g., neural network predictive control, can be a better match to govern the system dynamics. In the paper, the NN model and the way in which it can be used to predict the behavior of the CSTR process over a certain prediction horizon are described, and some comments about the optimization procedure are made. Predictive control algorithm is applied to control the concentration in a continuous stirred tank reactor (CSTR), whose parameters are optimally determined by solving quadratic performance index using the optimization algorithm. An efficient control of the product concentration in cstr can be achieved only through accurate model. Here an attempt is made to alleviate the modeling difficulties using Artificial Intelligent technique such as Neural Network. Simulation results demonstrate the feasibility and effectiveness of the NNMPC technique. KeywordsContinuous Stirred Tank Reactor; Neural Network based Predictive Control; Nonlinear Auto Regressive with eXogenous signal.	algorithm;artificial neural network;control theory;mathematical optimization;network model;nonlinear system;reactor (software);simulation;system dynamics	Piyush Shrivastava	2012	CoRR		control theory	Robotics	15.049156105111047	-19.179053407543595	31143
3fa06090fb9a8412a3ab90222234899dc383113b	a systematic simulation-based process intensification method for shale gas processing and ngls recovery process systems under uncertain feedstock compositions	process intensification;techno economic analysis;shale gas processing;ngls recovery;feedstock uncertainty	Handling uncertainty in feedstock compositions is an important challenge for shale gas processing and natural gas liquids (NGL) recovery process systems. If the process system is designed without considering uncertain feedstock compositions, the product specifications could be easily violated. To address this challenge, we develop a systematic simulation-based process intensification method. This method consists of three steps, namely process simulation, capacity-oriented process intensification, and design validation. An iterative feature of the proposed method guarantees the intensified design hedged against hale gas processing GLs recovery eedstock uncertainty rocess intensification echno-economic analysis uncertain feedstock compositions. The proposed method is illustrated on a conventional process system and a novel condensation-based system. In the novel system, a condensation process is considered in the gas dehydration section and it is integrated with a turboexpander process to improve the overall energy utilization efficiency. The intensified design of the novel system shows a lower total annualized cost than that of the conventional system. © 2016 Elsevier Ltd. All rights reserved.	iterative method;ngl (programming language);process architecture;simulation	Jian Gong;Minbo Yang;Fengqi You	2017	Computers & Chemical Engineering	10.1016/j.compchemeng.2016.11.010	engineering;process engineering;waste management	DB	9.150761092446116	-5.115018265257542	31148
c1890c9f30b7a5083732a7e4566975301e51537d	stacked thompson bandits		We introduce Stacked Thompson Bandits (STB) for efficiently generating plans that are likely to satisfy a given bounded temporal logic requirement. STB uses a simulation for evaluation of plans, and takes a Bayesian approach to using the resulting information to guide its search. In particular, we show that stacking multiarmed bandits and using Thompson sampling to guide the action selection process for each bandit enables STB to generate plans that satisfy requirements with a high probability while only searching a fraction of the search space.	action selection;algorithm;automated planning and scheduling;maxima and minima;quality of service;requirement;sampling (signal processing);set-top box;simulation;stacking;temporal logic;thompson sampling;venue (sound system);with high probability	Lenz Belzner;Thomas Gabor	2017	2017 IEEE/ACM 3rd International Workshop on Software Engineering for Smart Cyber-Physical Systems (SEsCPS)	10.1109/SEsCPS.2017.4	thompson sampling;theoretical computer science;temporal logic;actor model;bayesian network;informatics;machine learning;bounded function;action selection;artificial intelligence;bayesian probability;computer science	SE	21.537981695305138	-16.725984831741997	31156
24dee919d65de42a8b495b07f8053656b823b1e5	ga-based discrete dynamic programming approach for scheduling in fms environments	dynamic programming;feasible job sequences;sequences;performance criteria;flexible manufacturing systems;dynamic programming dynamic scheduling job shop scheduling flexible manufacturing systems processor scheduling scheduling algorithm genetic algorithms law legal factors algorithm design and analysis;job shop scheduling;processor scheduling;flexible manufacturing system environment;dynamic program;computation efficiency genetic algorithm based discrete dynamic programming approach static schedule generation flexible manufacturing system environment scheduling sequence dependent schedule generation strategy feasible job sequences legal schedules performance criteria locally optimized partial schedules;law;sequence dependent schedule generation strategy;flexible manufacturing system fms;genetic algorithms gas;legal factors;scheduling algorithm;production control;flexible manufacturing system;scheduling;static schedule generation;locally optimized partial schedules;genetic algorithm;genetic algorithms;heuristics;computation efficiency;state explosion;legal schedules;computational efficiency;algorithm design and analysis;genetic algorithm based discrete dynamic programming approach;sequences flexible manufacturing systems production control dynamic programming genetic algorithms;dynamic scheduling	The paper presents a new genetic algorithm (GA)-based discrete dynamic programming (DDP) approach for generating static schedules in a flexible manufacturing system (FMS) environment. This GA-DDP approach adopts a sequence-dependent schedule generation strategy, where a GA is employed to generate feasible job sequences and a series of discrete dynamic programs are constructed to generate legal schedules for a given sequence of jobs. In formulating the GA, different performance criteria could be easily included. The developed DDF algorithm is capable of identifying locally optimized partial schedules and shares the computation efficiency of dynamic programming. The algorithm is designed In such a way that it does not suffer from the state explosion problem inherent in pure dynamic programming approaches in FMS scheduling. Numerical examples are reported to illustrate the approach.	cns disorder;computation;differential dynamic programming;disk data format;genetic algorithm;job stream;neurolinguistic programming;numerical method;occupations;schedule (computer science);schedule (document type);scheduling (computing);scheduling - hl7 publishing domain;software release life cycle	Jian-Bo Yang	2001	IEEE transactions on systems, man, and cybernetics. Part B, Cybernetics : a publication of the IEEE Systems, Man, and Cybernetics Society	10.1109/3477.956045	job shop scheduling;mathematical optimization;real-time computing;genetic algorithm;computer science;scheduling	Robotics	19.65759429361241	-0.14228781580726246	31196
ed5ddab8dd799353e98d6267efcec914e75c7574	fuzzy mathematical morphology: general concepts and decomposition properties	science general;mathematical morphology;mathematics;logical framework;image processing;uncertainty;image processing mathematical morphology fuzzy logic fuzzy set theory;morphological operation;gray scale morphology;discrete morphology;gray scale;fuzzy set theory;fuzzy sets;fuzzy morphological operations fuzzy mathematical morphology decomposition properties binary morphology gray scale morphology fuzzy set theory fuzzy logic general logical framework discrete morphology;fuzzy logic;morphology;morphological operations;fuzzy morphological operations;pixel;general logical framework;zinc;morphology morphological operations fuzzy logic gray scale pixel zinc uncertainty fuzzy set theory fuzzy sets mathematics;decomposition properties;binary morphology;fuzzy mathematical morphology	Fuzzy mathematical morphology is an alternative extension of binary morphology to gray-scale morphology, using techniques from fuzzy set theory and fuzzy logic. The authors discuss a general logical framework for discrete morphology and investigate the decomposition of the fuzzy morphological operations in this framework.	mathematical morphology	Mike Nachtegael;Etienne E. Kerre	1999		10.1109/KES.1999.820171	adaptive neuro fuzzy inference system;fuzzy classification;artificial intelligence;neuro-fuzzy;machine learning;mathematics;fuzzy associative matrix;algorithm	Vision	0.40521414914633114	-23.89093069560522	31201
dadffeea7011458d6634319372ce0eea2a245609	inverse reinforcement learning based on critical state		Inverse reinforcement learning is tried to search a reward function based on Markov Decision Process. In the IRL topics, experts produce some good traces to make agents learn and adjust the reward function. But the function is difficult to set in some complicate problems. In this paper, Inverse Reinforcement Learning based on Critical State (IRLCS) is proposed to search a succinct and meaningful reward function. IRLCS select a set of reward indexes from whole state space through comparing the difference between the good and bad demonstrations. According to the simulation results, IRLCS can search a good strategy that is similar to experts.	algorithm;algorithmic efficiency;computation;exponential backoff;markov chain;markov decision process;reinforcement learning;simulation;state space;time complexity;tracing (software)	Kao-Shing Hwang;Tien-Yu Cheng;Wei-Cheng Jiang	2015			temporal difference learning;reward-based selection;machine learning	AI	20.433842083286034	-19.675473575113173	31248
0e593e7060a107ea695936aa8c479b44c79e8bda	financial derivatives and partial differential equations		1. ASSETS AND DERIVATIVES. Assets of all sorts are traded in financial markets: stocks and stock indices, foreign currencies, loan contracts with various interest rates, energy in many forms, agricultural products, precious metals, etc. The prices of these assets fluctuate, sometimes wildly. As an example, Figure 1 shows the price of IBM stock within a single day. The picture would look more or less the same across a month, a year, or a decade, though the axis scales would be different. If you could anticipate the price fluctutations to any significant extent, then you could clearly make a great amount of money very quickly. The fact that many people are trying to do exactly that makes the fluctuations essentially unpredictable for practical purposes. A fundamental principle of finance, the efficient market hypothesis [9] asserts that all information available to anyone anywhere is instantly expressed in the current price, as market participants race to be the first to profit from new information. Thus successive price changes may be considered to be uncorrelated random variables, since they depend on as-yet unrevealed information. This principle is the subject of intensive analytical testing and some controversy [7], but is an excellent approximation for our purposes. Although the directions of the price motions are completely unpredictable, statistics can tell us a lot about their expected size. Figure 2 shows the distribution of percentage changes in IBM stock price across half hour time intervals. We can identify a typical size of the fluctuations, about half of one percent in this example. Since the fluctuations are uncorrelated and have mean near zero, this typical size is the single most important statistical quantity that we can extract from the price history. We may additionally ask about the form of this distribution, for example, whether or not it is a Gaussian. Again, this is the subject of active research [10]. ∗To appear in American Mathematical Monthly	apache axis;approximation;gaussian (software);money;price point;tag (game)	Robert Almgren	2002	The American Mathematical Monthly		first-order partial differential equation;derivative;brownian motion;mathematical economics;partial differential equation;financial market;project portfolio management	Theory	1.978692641773492	-3.833487804315535	31372
7cd4b0d31c4db40d9b2d55e4d158733e77edc391	daily electric load forecasting based on rbf neural network models	differential evolution;peak load;short term load forecast;radial basis function network;load curve	This paper presents a method of improving the performance of a day-ahead 24-h load curve and peak load forecasting. The next-day load curve is forecasted using radial basis function (RBF) neural network models built using the best design parameters. To improve the forecasting accuracy, the load curve forecasted using the RBF network models is corrected by the weighted sum of both the error of the current prediction and the change in the errors between the current and the previous prediction. The optimal weights (called “gains” in the error correction) are identified by differential evolution. The peak load forecasted by the RBF network models is also corrected by combining the load curve outputs of the RBF models by linear addition with 24 coefficients. The optimal coefficients for reducing both the forecasting mean absolute percent error (MAPE) and the sum of errors are also identified using differential evolution. The proposed models are trained and tested using four years of hourly load data obtained from the Korea Power Exchange. Simulation results reveal satisfactory forecasts: 1.230% MAPE for daily peak load and 1.128% MAPE for daily load curve.	approximation error;artificial neural network;coefficient;differential evolution;error detection and correction;load profile;mean squared error;radial (radio);radial basis function network;simulation;weight function	Hee-Soo Hwang	2013	Int. J. Fuzzy Logic and Intelligent Systems	10.5391/IJFIS.2013.13.1.39	econometrics;mathematical optimization;computer science;machine learning	Robotics	10.016086771389114	-18.49369614947775	31376
39a0716e45d14eb269bfa6fb60151dd6234cebed	multi-agent reinforcement learning-based approach for controlling signals through adaptation		In this paper, we present a multi-agent reinforcement learning-based approach for controlling traffic signals. The aim is to use a multi-agent system with learning abilities for controlling and optimising traffic lights. We consider in this study the Q-learning algorithm, where the states are computed from average queue length in approaching links. The action space is modelled offline by using different time splits. The adaptation of the considered learning optimal policy through online learning is introduced to deal with the change of the environment. The simulation results show the effectiveness of the proposed adaptive learning algorithm.	multi-agent system;reinforcement learning	Mohammed Tahifa;Jaouad Boumhidi;Ali Yahyaouy	2018	IJAACS	10.1504/IJAACS.2018.10013246	q-learning;reinforcement learning;adaptive learning;multi-agent system;distributed computing;queue;computer science	NLP	19.531486963458978	-19.39217437207081	31377
8d1d4446b919742457707552bbc0ec495e7177a7	dynamic estimation of the price-response of deadline-constrained electric loads under threshold policies	estimation load scheduling;aggregates history mathematical model vectors equations real time systems computational modeling;electricity market price response dynamic estimation deadline constrained electric loads unbiased estimator dynamic one step ahead prediction price signal exogenous signal discrete time intervals random consumption deadlines random demands random times threshold policy dynamic aggregate model consistent estimator aggregate consumption time series;pricing estimation theory power markets	The paper presents a consistent and unbiased estimator for dynamic, one-step-ahead prediction of the aggregate response of a large number of individual loads to a common price signal, using only aggregate past response data. The price per unit of consumption is an exogenous signal which is updated at discrete time intervals. It is assumed that individual loads arrive in the system at random times with random demands and random consumption deadlines, and may defer their consumption up to the deadline in order to minimize their total cost. It is further assumed that the individual loads adopt a threshold policy in the sense that they only consume when the price is below a certain threshold. A dynamic aggregate model is constructed from models of independent individual loads. A consistent and unbiased estimator which only uses aggregate data, i.e., the price and aggregate consumption time-series is presented for estimating the aggregate consumption as a function of price.	aggregate data;approximation error;kerrison predictor;map;population;sysop;time series	Mesrob I. Ohannessian;Mardavij Roozbehani;Donatello Materassi;Munther A. Dahleh	2014	2014 American Control Conference	10.1109/ACC.2014.6859473	econometrics;mathematical optimization	Metrics	5.326209829408754	-12.492033855953231	31415
424f4443c16ddbe22acbd3b6505ac58d99fa096f	mechanism design to the budget constrained buyer: a canonical mechanism approach	taxation principle;multi dimensional mechanism;revelation principle;budget constraint;indirect mechanism	The present paper studies the problem on multi-dimensional mechanisms in which the buyer’s taste and budget are his private information. The paper investigates the problem by way of a canonical mechanism in the traditional one-dimensional setting: function of one variable, the buyer’s taste. In our multi-dimensional context, this is an indirect mechanism. The paper characterizes the optimal canonical mechanism and shows that this approach loses no generality with respect to the direct (multi-dimensional) mechanism. Copyright Springer-Verlag Berlin Heidelberg 2014		Naoki Kojima	2014	Int. J. Game Theory	10.1007/s00182-013-0403-9	budget constraint;economics;public economics;microeconomics;mathematical economics;revelation principle;welfare economics	ECom	-3.9468755936972997	-3.2110270614948133	31450
14635b5ee05877d967ceb0812f26b3b7eeacbd9c	particle filter with operational-scalable takagi-sugeno fuzzy degradation model for filter-clogging prognosis		In this paper, filter clogging is used as an aerospace integrated vehicle health management case study to demonstrate the proposed prognostic approach. The focus of this paper is on a scalable data-driven degradation model and how it can improve the remaining useful life prediction performance in condition monitoring of a filter component. Instead of overall fitting of the data, a degradation pattern (a parameterized Takagi–Sugeno fuzzy model) is learned from experimental data collected under a range of operating conditions in the proposed approach. The parameter allows the model to scale to fit different degradation profiles, and hence a more accurate model. In real-time condition monitoring, the degradation and model parameter are simultaneously estimated online based on noisy measurement updates using a particle filter. The estimation results show close tracking of the degradation state and good convergence of the model parameter to its real value. The remaining useful life prediction results show low ...	elegant degradation;particle filter	Tarapong Sreenuch;Faisal Khan;Jincheng Li	2015	J. Aerospace Inf. Sys.	10.2514/1.I010385	simulation;engineering	AI	14.638192843037864	-15.402848682435744	31452
6802eb41243eb058c7faf58964124165cafb0d2b	simulation to scaled city: zero-shot policy transfer for traffic control via autonomous vehicles		Using deep reinforcement learning, we successfully train a set of two autonomous vehicles to lead a fleet of vehicles onto a round-about and then transfer this policy from simulation to a scaled city without fine-tuning. We use Flow, a library for deep reinforcement learning in microsimulators, to train two policies, (1) a policy with noise injected into the state and action space and (2) a policy without any injected noise. In simulation, the autonomous vehicles learn an emergent metering behavior for both policies which allows smooth merging. We then directly transfer this policy without any tuning to the University of Delawareu0027s Scaled Smart City (UDSSC), a 1:25 scale testbed for connected and automated vehicles. We characterize the performance of the transferred policy based on how thoroughly the ramp metering behavior is captured in UDSSC. We show that the noise-free policy results in severe slowdowns and only, occasionally, it exhibits acceptable metering behavior. On the other hand, the noise-injected policy consistently performs an acceptable metering behavior, implying that the noise eventually aids with the zero-shot policy transfer. Finally, the transferred, noise-injected policy leads to a 5% reduction of average travel time and a reduction of 22% in maximum travel time in the UDSSC. Videos of the proposed self-learning controllers can be found at https://sites.google.com/view/iccps-policy-transfer.		K. Jang;Logan E. Beaver;Behdad Chalaki;Ben Remer;Eugene Vinitsky;Andreas A. Malikopoulos;Alexandre Bayen	2018	CoRR		real-time computing;policy transfer;smart city;deep learning;cyber-physical system;reinforcement learning;flow (psychology);metering mode;testbed;computer science;artificial intelligence	AI	19.862012098220656	-18.554328183321964	31515
98a730f197d702f95cd56951437d4a71285e6e53	randomless as a critical point: simulation fitting better planning of distribution centers	distribution centers project;better planning;distribution center;simulation technology;ryder logistics;better resource;static analysis;critical point;simulation project;simulation fitting;equipment sizing;common sizing error;previous statics analysis;simulation model;logistics	The proposal of this paper is to show the importance of using simulation technology in logistic operations studies for a Distribution Center (DC). In addition, it will be presented a way as simulation technology can generate several benefits in distribution centers projects, such as providing a better resources and equipment sizing, number of docks for inbound and outbound, flow of materials, layout, etc., preventing common sizing errors when using only static analysis with spread sheets. In the last four years, besides the project made by Belge at Ryder Logistics, we developed several simulation projects for DCs in other companies like Unilever, Mclane, DHL-Exel and observed that previous statics analysis typically implies in errors varying between 20% to 60% when comparing with dynamic studies (simulated models) and the real implementations.	critical point (network science);inbound marketing;logistics;mathematical model;simulation;static program analysis	Marcelo K. Fugihara;Alain de Norman et d'Audenhove;Neuton T. Karassawa	2007	2007 Winter Simulation Conference		logistics;simulation;computer science;systems engineering;engineering;simulation modeling;critical point;static analysis	Arch	9.622929187085045	1.5383396832249838	31525
d792353d5eb2c8897072005fb0bc2f8b43a6be4a	reconstruction methods for ahp pairwise matrices: how reliable are they?	ahp;pairwise matrix reconstruction methods;firefly algorithm;decision support systems;ranking comparison;genetic algorithms	Habitually, decision-makers are exposed to situations that require a lot of knowledge and expertise. Therefore, they need tools to help them choose the best possible alternatives. Analytic hierarchical process (AHP) is one of those tools and it is widely used in many fields. While the use of AHP is very simple, there is a situation that becomes complex: the consistency of the pairwise matrices. In order to obtain the consistent pairwise matrix from the inconsistent one, reconstruction methods can be used, but they cannot guarantee getting the right matrix according to the judgments of the decision maker. This situation does not allow proper evaluation of methods reliability, i.e. it is not possible to obtain a reliable ranking of alternatives based on an inconsistent matrix. In this work, a new way to evaluate the reliability of matrix reconstruction methods is proposed. This technique uses a novel measure for alternatives ranking comparison (based on element positions and distances), which is introduced in order to compare several matrix reconstruction methods. Finally, in order to demonstrate the extensibility of this new reliability measure, two reconstruction methods based on bio-inspired models (a Genetic Algorithm and the Firefly Algorithm) are presented and evaluated by using the aforementioned reliability measure.		Marcelo Karanik;Leonardo Wanderer;José Antonio Gómez-Ruiz;José Ignacio Peláez	2016	Applied Mathematics and Computation	10.1016/j.amc.2016.01.008	mathematical optimization;analytic hierarchy process;genetic algorithm;decision support system;firefly algorithm;machine learning;data mining;mathematics	Vision	-4.0810205761953435	-19.626416146839784	31543
c56be33d349f3d696d3023ba30c06c39c8061ee6	minimizing quality costs given a fixed budget - a case study				K. K. Kuong-Lau;A. B. Hoadley	1986			real-time computing;reliability engineering;quality costs;computer science;variable cost	ECom	8.847344598528634	0.27761304398869396	31563
fdb06d545e65db4cdd9d68eefbb186a9d6291735	can internet-based tv succeed? towards a sequential framework for market entry	market entry;distribution channel;profitability;entry barrier	The advent of the Internet alleviates the access bottleneck to TV distribution channels and softens licensing requirements. This lowers entry barriers to TV markets via the Internet in various forms. This paper takes the German TV sector as an example to analyze the attractiveness of TV markets for new entrants. A sequential framework for entering TV markets is introduced. The skills set of an Internet-based TV provider for such an entry is examined. Technical feasibility, legal aspects, and potential sources of revenue are considered. Potential above average profits due to the market's oligopolistic structure, as well as an increased contestability thanks to lower market entry barriers render the German TV market attractive for new entrants. In early 2002, Internet-based TV still faced severe technical and legal constraints. The analysis suggests that once these constraints have been overcome, the Internet can be an attractive additional distribution channel for television. While existing revenue sources from the TV sector are expected to be transferable to a certain degree, the value of innovative revenue sources based on online sales cannot yet be determined. This value will depend on the future acceptance of interactivity by the viewer (e.g. Owen 1999). Claudia Loebbecke, Marcia Falkenberg	claudia neuhauser;interactivity;internet;requirement;technical standard	Marcia Falkenberg;Claudia Löbbecke	2002			industrial organization;marketing;advertising;business	ECom	-3.158106471836296	-7.986204612858947	31570
22377811862d9998ce4897c89f8941c4c88dc7c1	a multiobjective approach to resource management in smart grid	smart grids resource management microgrids vectors equations mathematical model symmetric matrices;single objective approach multiobjective approach smart grid resource management state space model resources distribution cost mo optimization problem h design grid disturbances linear matrix inequality stochastic mechanisms approximated pareto front apf;stochastic processes linear matrix inequalities pareto optimisation power system management smart power grids	A multiobjective (MO) approach is proposed for resource management in the smart grid. The resource management problem is formulated via a state-space model. The grid performance is optimized and the distribution cost of resources is minimized, yielding an MO optimization problem. An H∞ design is adopted to address grid disturbances, and linear matrix inequality approaches are used to render the MO problem numerically solvable. An algorithm that integrates deterministic and stochastic mechanisms is presented. By using the algorithm, an approximated Pareto front (APF) can be obtained. The resources in the smart grid can thus be managed by choosing one of the trade-off strategies represented by nondominated vectors on the APF. In contrast with a single-objective approach, the proposed MO approach provides a grid designer with a broad perspective on optimality, which clearly illustrates how one objective affects the other. Simulations show that by using the MO approach, the distribution cost can be significantly reduced with moderate degradation of the grid performance.	approximation algorithm;australian privacy foundation;computer simulation;decision problem;elegant degradation;linear matrix inequality;mathematical optimization;numerical analysis;optimization problem;pareto efficiency;social inequality;state space	Wei-Yu Chiu	2014	The 2014 International Conference on Control, Automation and Information Sciences (ICCAIS 2014)	10.1109/ICCAIS.2014.7020554	mathematical optimization;simulation;mathematics	Robotics	4.979634145944732	3.090882371915554	31605
c087d15d2d02a9ea6d0e02316cb825d6f57914a2	implementation of a two-tier double auction for on-line power purchasing in the simulation of a distributed intelligent cyber-physical system		The increasing penetration of distributed renewable generation brings new power producers to the market [1]. Rooftop photovoltaic (PV) panels allow home owners to generate more power than personally needed and this excess production could be voluntarily sold to nearby homes, alleviating additional transmission costs especially in rural areas [2]. Power is sold as a continuous quantity and power markets involve pricing that may change on a minute-to-minute basis. Forward markets assist with scheduling power in advance [3]. The speed and complexity of the calculations needed to support online distributed auctions is a good fit for intelligent agents [4]. This paper describes the simulation of a two-tier double auction for short-term forward power exchanges between participants at the outer edges of a power distribution system (PDS). The paper describes the double auction algorithms and demonstrates online auction execution in a simulated distributed system of intelligent agents assisting with voltage/var control near distributed renewable generation [5]. The agents were enhanced to autonomously create local power market organizations and execute the series of online power auctions using Advanced Message Queuing Protocol (AMQP).	advanced message queuing protocol;auction algorithm;autonomous robot;cyber-physical system;distributed computing;intelligent agent;iterative method;multitier architecture;offset binary;purchasing;recommender system;scheduling (computing);simulation	Denise M. Case;M. Nazif Faqiry;Bodhisattwa Prasad Majumder;Sanjoy Das;Scott A. DeLoach	2014	Research in Computing Science		simulation;operations management;business;commerce	ECom	0.7450044584196593	3.658458376227547	31683
9f3956f06de33088fe946a5335bc097ff7a01a86	master surgery scheduling with consideration of multiple downstream units	resource allocation;master surgery scheduling;or in health services;ward and icu occupancy	We consider a master surgery scheduling (MSS) problem in which block operating room (OR) time is assigned to different surgical specialties. While many MSS approaches in the literature consider only the impact of the MSS on operating theater and operating staff, we enlarge the scope to downstream resources, such as the intensive care unit (ICU) and the general wards required by the patients once they leave the OR. We first propose a stochastic analytical approach, which calculates for a given MSS the exact demand distribution for the downstream resources. We then discuss measures to define downstream costs resulting from the MSS and propose exact and heuristic algorithms to minimize these costs. 2014 Elsevier B.V. All rights reserved.	algorithm;approximation algorithm;branch and bound;computation;downstream (software development);extended validation certificate;heuristic;heuristic (computer science);international components for unicode;loss function;optimization problem;paging;schedule (computer science);scheduling (computing);simulated annealing;solver	Andreas Fügener;Erwin W. Hans;Rainer Kolisch;Nikky Kortbeek;Peter T. Vanberkel	2014	European Journal of Operational Research	10.1016/j.ejor.2014.05.009	simulation;economics;resource allocation;operations management	AI	12.731800201460638	-1.1637971285369533	31692
a221c4003c7f6482314ff084a6add32c4ba5c352	towards the theory of m-approcimate systems: fundamentals and examples	fuzzy set;procesamiento informacion;category;teoria sistema;cl monoid;m approximate operator;fuzzy topology;conjunto difuso;topologie l;ensemble flou;m fuzzy topology;info eu repo semantics article;topologia l;l;54a40;categorie;systems theory;theorie systeme;information processing;m approximate system;l topology;sistema difuso;systeme flou;traitement information;rough set;l rough set;fuzzy system;l m fuzzy topology;fuzzy bitopology	The concept of an M-approximate system is introduced. Basic properties of the category of M-approximate systems and in a natural way defined morphisms between them are studied. It is shown that categories related to fuzzy topology as well as categories related to rough sets can be described as special subcategories of the category of M-approximate systems.		Alexander P. Sostak	2010	Fuzzy Sets and Systems	10.1016/j.fss.2010.05.010	equivalence of categories;category of topological spaces;allegory;rough set;category;higher category theory;information processing;l;computer science;artificial intelligence;machine learning;mathematics;fuzzy set;systems theory;algorithm;fuzzy control system;category theory	Logic	1.4068898154608775	-22.842794038312736	31754
a30fecd2ab646bb11ccabb7a3c23f1faddcf113e	an extensive evaluation of seven machine learning methods for rainfall prediction in weather derivatives	q335 artificial intelligence	Regression problems provide some of the most challenging research opportunities in the area of machine learning, and more broadly intelligent systems, where the predictions of some target variables are critical to a specific application. Rainfall is a prime example, as it exhibits unique characteristics of high volatility and chaotic patterns that do not exist in other time series data. This work’s main impact is to show the benefit machine learning algorithms, and more broadly intelligent systems have over the current state-of-the-art techniques for rainfall prediction within rainfall derivatives. We apply and compare the predictive performance of the current state-of-the-art (Markov chain extended with rainfall prediction) and six other popular machine learning algorithms, namely: Genetic Programming, Support Vector Regression, Radial Basis Neural Networks, M5 Rules, M5 Model trees, and k-Nearest Neighbours. To assist in the extensive evaluation, we run tests using the rainfall time series across data sets for 42 cities, with very diverse climatic features. This thorough examination shows that the machine learning methods are able to outperform the current state-of-the-art. Another contribution of this work is to detect correlations between different climates and predictive accuracy. Thus, these results show the positive effect that machine learning-based intelligent systems have for predicting rainfall based on predictive accuracy and with minimal correlations existing across climates.	algorithm;artificial intelligence;chaos theory;error message;expert system;fits;genetic programming;machine learning;markov chain;neural networks;radial (radio);radial basis function;real life;reflections of signals on conducting lines;support vector machine;time series;tree accumulation;volatility	Sam Cramer;Michael Kampouridis;Alex Alves Freitas;Antonis K. Alexandridis	2017	Expert Syst. Appl.	10.1016/j.eswa.2017.05.029	time series;online machine learning;support vector machine;machine learning;artificial intelligence;intelligent decision support system;genetic programming;artificial neural network;computer science;data set;markov chain	ML	9.217696204923772	-20.552699730973774	31773
9e6f8c51d8370cd9259b2de7478da095bee808b2	pallet optimization and throughput estimation via simulation	automotive engineering;analytical models;manufacturing systems;manuals;optimized production technology;measurement;production system;testing;throughput production systems manufacturing systems analytical models manuals workstations measurement testing optimized production technology automotive engineering;workstations;production systems;cost effectiveness;production rate;technical report;process simulation;throughput	We describe a discrete-process simulation analysis of a production system at an automotive supply company. This simulation project was undertaken with the goals of demonstrating and confirming production rates of a manufacturing process based on a proposed design layout and operational data, and of identifying costeffective ways of improving the design to increase those production rates.	production system (computer science);simulation;throughput	Edward J. Williams;Andrew Gevaert	1997		10.1145/268437.268618	process simulation;computer science;systems engineering;engineering;artificial intelligence;production system;manufacturing engineering	EDA	9.136586800155813	2.467819718798157	31861
77a95c9d7382d5d42def73e0cdc3b5b241eb1397	a single-layer perceptron with promethee methods using novel preference indices	single layer perceptron;majority rule;weighted averaging;compromise operator;multi criteria decision aid;mcda;indexation;promethee methods;pattern classification;preference relation;computer simulation	The Preference Ranking Organization METHods for Enrichment Evaluations (PROMETHEE) methods, based on the outranking relation theory, are used extensively in multi-criteria decision aid (MCDA). In particular, preference indices with weighted average aggregation representing the intensity of preference for one pattern over another pattern are measured by various preference functions. The higher the intensity, the stronger the preference is indicated. For MCDA, to obtain the ranking of alternatives, compromise operators such as the weighted average aggregation, or the disjunctive operators are often employed to aggregate the performance values of criteria. The compromise operators express the group utility or the majority rule, whereas the disjunctive operators take into account the strongly opponent or agreeable minorities. Since these two types of operators have their own unique features, it is interesting to develop a novel aggregator by integrating them into a single aggregator for a preference index. This study aims to develop a novel PROMETHEE-based single-layer perceptron (PROSLP) for pattern classification using the proposed preference index. The assignment of a class label to a pattern is dependent on its net preference index, which is obtained by the proposed perceptron. Computer simulations involving several real-world data sets reveal the classification performance of the proposed PROMETHEE-based SLP. The proposed perceptron with the novel preference index performs well compared to that with the original one.	feedforward neural network;perceptron;preference ranking organization method for enrichment evaluation	Yi-Chung Hu	2010	Neurocomputing	10.1016/j.neucom.2010.08.002	computer simulation;majority rule;computer science;artificial intelligence;machine learning;data mining;mathematics;multiple-criteria decision analysis	NLP	-3.8428622894172215	-21.212908166641505	32040
34acd68de2968c3077ccc3a24552128ea7a03d14	platform-based information goods: the economics of exclusivity	libre mercado;modelizacion;rentabilidad;information good;informacion economica;prestation service;indirect network effects;economic information;competitividad;platform competition;prestacion servicio;videojuego;bien numerico;contrato;contracting service;video game;marche concurrentiel;jeu video;modelisation;exclusivity;economic order quantity;numerical analysis;contract;bien numerique;competitiveness;complements;digital goods;information economique;rentabilite;quantite economique a commander;profitability;contrat;cantidad economica pedida;open market;competitivite;modeling	This paper explores the role of exclusive contracting between vendors of platforms (such as video game consoles) and vendors of complements (such as video games). The main questions of interest are: When do we observe complement exclusivity, and what is the impact of exclusive contracting on prices, profits and efficiency? We answer these questions by developing a model of competition between platforms in an industry with indirect network effects, and deriving some insightful analytical and numerical results. While complement vendors have natural incentives to be available on all platforms, we establish conditions under which they can be contracted for exclusive supply on a single platform. Exclusivity eases competition in the platform market and can significantly help increase a platform's adoption. However, exclusivity choice presents a key trade-off for the complement vendor-a larger platform offers access to a larger market, but also more competition, as compared to a smaller platform. We find that exclusivity is more likely in the nascent and very mature stages of the platform market, whereas non-exclusivity is more likely in the intermediate stages. Interestingly, our numerical analysis suggests that a complement vendor might sometimes prefer being exclusive on the smaller platform, rather than the larger one.		Ravindra Mantena;Ramesh Sankaranarayanan;Siva Viswanathan	2010	Decision Support Systems	10.1016/j.dss.2010.07.004	contract;economic order quantity;economics;numerical analysis;artificial intelligence;marketing;operations management;economy;information good;management;law;commerce	ECom	-1.1813470159227224	-6.4352989167950865	32110
2036ef4597aa772a517d8ea95653b2e20fcc4a93	optimal preventive replacement under minimal repair and random repair cost	preventive replacement;repair cost limit policy;minimal repair;optimal stopping;jump processes	A repair/replacement problem for a single unit system with random repair cost is considered. When the unit fails, the repair cost is observed and a decision is made whether to replace the unit or repair it. We assume that the repair is minimal, i.e., the unit is restored to its functioning condition just prior to failure, without changing its age. The unit can be preventively replaced at any time. The problem is formulated as a continuous time decision problem and reduced to an optimal stopping problem in discrete time by applying results from the thoery of jump processes. The existence of the optimal policy is proved and its structure is found using semimartingale decomposition and ?-maximization technique. It is shown that the optimal policy is an age replacement, repair-cost-limit policy, and the optimal preventive replacement time and the repair cost limits can be obtained by solving a system of ordinary differential equations with boundary conditions.		V. Makis;Xiangjun Jiang;Kai Cheng	2000	Math. Oper. Res.	10.1287/moor.25.1.141.15207	mathematical optimization;optimal stopping;mathematics;statistics	Theory	6.4345700707722715	-1.1809865016016436	32183
c02efcbb11d6283e6d0482d844e0230754ab56cf	training agents with neural networks in systems with imperfect information		The paper deals with multi-agent system that represents trading agents acting in the environment with imperfect information. Fictitious play algorithm, first proposed by Brown in 1951, is a popular theoretical model of training agents. However, it is not applicable to larger systems with imperfect information due to its computational complexity. In this paper we propose a modification of the algorithm. We use neural networks for fast approximate calculation of the best responses. An important feature of the algorithm is the absence of agent’s a priori knowledge about the system. Agents’ learning goes through trial and error with winning actions being reinforced and entered into the training set and losing actions being cut from the strategy. The proposed algorithm has been used in a small game with imperfect information. And the ability of the algorithm to remove iteratively dominated strategies of agents' behavior has been demonstrated.	approximation algorithm;computational complexity theory;information system;multi-agent system;nash equilibrium;neural networks;test set	Yulia Korukhova;Sergey Kuryshev	2017		10.5220/0006242102960301	artificial intelligence;machine learning;data mining	AI	18.492209223380158	-18.781624106045815	32329
70258b68170447dcbe45b47fdb183e8c45fcf157	punctuality analysis using a microscopic simulation in which drivers' behaviour is considered	punctuality visualization;simulation;microscopic simulation;train operation;urban areas;robustness;punctuality analysis;railroads;japan;on time performance;railroad tracks;track circuit	One of the recent problems in urban railways in Japan is that small delays often happen during rush hours. Because trains are running very densely, even a small delay propagates to succeeding trains and the delay tends to expand. In order to precisely evaluate the robustness of a railway system, a detailed simulation so called microscopic simulation in which components of a railway system are modelled in detail is used. There exist a lot of research papers with regard to the microscopic simulation, but they have not explicitly dealt with the manipulation by the driver, which is yet crucial to obtain a realistic result of simulation. We have developed a microscopic simulator, in which drivers’ manipulation is explicitly expressed. In this paper, we introduce how the simulator is constructed and how the simulator was effective in the analysis we did when a part of the tracks were relocated.	electronic circuit simulation;simulation	Yasufumi Ochiai;Norio Tomii	2015	JRTPM	10.1016/j.jrtpm.2015.09.003	simulation;engineering;operations management;operations research	Robotics	9.920651036586301	-10.176963993293016	32399
203c588b31f00249ff9af750da1209ae96437bcc	revisiting risk-sensitive mdps: new algorithms and results		While Markov Decision Processes (MDPs) have been shown to be effective models for planning under uncertainty, the objective to minimize the expected cumulative cost is inappropriate for high-stake planning problems. As such, Yu, Lin, and Yan (1998) introduced the Risk-Sensitive MDP (RSMDP) model, where the objective is to find a policy that maximizes the probability that the cumulative cost is within some user-defined cost threshold. In this paper, we revisit this problem and introduce new algorithms that are based on classical techniques, such as depth-first search and dynamic programming, and a recently introduced technique called Topological Value Iteration (TVI). We demonstrate the applicability of our approach on randomly generated MDPs as well as domains from the ICAPS 2011 International Probabilistic Planning Competition (IPPC).	algorithm;davis–putnam algorithm;depth-first search;distributed file system (microsoft);dynamic programming;expected utility hypothesis;iteration;local interconnect network;markov chain;markov decision process;procedural generation;run time (program lifecycle phase);televideo	Ping Hou;William Yeoh;Pradeep Varakantham	2014			mathematical optimization;artificial intelligence;machine learning;mathematics;algorithm	AI	21.63985346455994	-16.28279489723243	32412
e284820c7d8f8378192760e1493a831cba549cf5	supply chain coordination with uncertainty in two-echelon yields	overproduction risk sharing;revenue sharing;coordination model;two echelon yields	This paper researches the coordination models in the supply chain where there are uncertain two-echelon yields and random demand. We analyzed three contracts of revenue sharing (RS), overproduction risk sharing (OS), and combination of RS and OS (RO), and contrasted them with uncoordinated model. We studied the optimal order decision for downstream manufacturer and the optimal production decision for upstream manufacturer. Numerical examples were presented to illustrate the results. The study showed that the RS contract and OS sharing contract both have their advantages and disadvantages and the RO contract could benefit the whole supply chain best. We found out that the OS contract gives the upstream manufacturer incentive to produce more so as to maximize the profit value, but the upstream manufacturer may receive less as the price of overproduced part increases. We also found out that under most scenarios, the supply chain benefits from the yields and demand risks reduction and generates a higher profit. But sometimes in the OS contract the downstream manufacturer profit can increase as yields randomness increases. And, in the uncoordinated case and OS contract, the upstream manufacturer profit can increase as demand randomness increases.	downstream (software development);numerical method;operating system;randomness;reed–solomon error correction;revenue sharing;row echelon form;upstream (software development)	Hongjun Peng;Meihua Zhou;Ling Qian	2013	APJOR	10.1142/S0217595912500443	operations management;microeconomics;commerce	ECom	-0.3086017356748609	-5.638687490722439	32422
633b536f08329d56ba82be3db6ac6a92e850b582	adaptive play q-learning with initial heuristic approximation	intelligent system design;learning process;multiagent system;state space methods;goal directed stochastic games;action penalty representation;learning agents;intelligent robots;curse of dimensionality;single agent problem;multiple autonomous robot coordination;orbital robotics;learning systems;autonomous agent;two robot grid world problem adaptive play q learning heuristic approximation multiple autonomous robot coordination multiagent system intelligent system design learning agents goal directed stochastic games action penalty representation single agent problem;stochastic processes;adaptive systems;multi robot systems;intelligent systems;intelligent system;intelligent agent;adaptive play q learning;exponential growth;multiagent learning;learning artificial intelligence;autonomous agents;robot kinematics multiagent systems stochastic processes state space methods autonomous agents intelligent systems intelligent agent intelligent robots robotics and automation orbital robotics;stochastic games adaptive systems intelligent robots learning artificial intelligence learning systems multi robot systems;joint action;robotics and automation;heuristic approximation;autonomous robot;stochastic games;two robot grid world problem;multiagent systems;robot kinematics	"""The problem of an effective coordination of multiple autonomous robots is one of the most important tasks of the modern robotics. In turn, it is well known that the learning to coordinate multiple autonomous agents in a multiagent system is one of the most complex challenges of the state-of-the-art intelligent system design. Principally, this is because of the exponential growth of the environment's dimensionality with the number of learning agents. This challenge is known as """"curse of dimensionality"""", and relates to the fact that the dimensionality of the multiagent coordination problem is exponential in the number of learning agents, because each state of the system is a joint state of all agents and each action is a joint action composed of actions of each agent. In this paper, we address this problem for the restricted class of environments known as goal-directed stochastic games with action-penalty representation. We use a single-agent problem solution as a heuristic approximation of the agents' initial preferences and, by so doing, we restrict to a great extent the space of multiagent learning. We show theoretically the correctness of such an initialization, and the results of experiments in a well-known two-robot grid world problem show that there is a significant reduction of complexity of the learning process."""	ap computer science a;admissible heuristic;agent-based model;algorithm;approximation;artificial intelligence;autonomous robot;cobham's thesis;correctness (computer science);curse of dimensionality;experiment;heuristic;linear programming relaxation;machine learning;multi-agent system;q-learning;real life;robotics;sensor;state space;systems design;time complexity	Andriy Burkov;Brahim Chaib-draa	2007	Proceedings 2007 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2007.363575	simulation;computer science;artificial intelligence;autonomous agent;machine learning;intelligent agent	Robotics	19.292995401038812	-19.461215486351332	32434
826e428ac0a2cbc318686ca2b7167f7f074edb7c	neural networks for water demand time series forecasting	distributed system;time series forecasting;water supply;optimization problem;transfer function;neural network model;water demand;neural network	One of the main problems in the management of large water supply and distribution systems is the forecasting of the daily demand in order to schedule the pumping effort and minimize the costs. This work presents the use of neural network models, with and without intervention series, to this forecasting task. The networks used have units with a variable gain in the transfer function and the training phase is performed like a standard optimization problem, in order to do it more efficiently.	mathematical optimization;neural networks;optimization problem;pumping (computer systems);time series;transfer function;variable-gain amplifier	Robert Griñó	1991		10.1007/BFb0035927	optimization problem;computer science;machine learning;time series;transfer function;water supply;artificial neural network	ML	10.085965570813384	-15.068831273328618	32613
fb50612b7e6583c2f3540ccbedb9669e5867fa27	research on safe driving scoring system and personalized ratemaking of vehicle insurance based on obd data		This paper was based on the latest domestic automobile insurance ratemaking model, brought drive behavior into the adjustment range of the independent underwriting coefficient (underwriting by company), explored the differential pricing problem of automobile insurance rate. The paper was based on OBD driving behavior data of vehicles, identified the key characteristics of driving behavior influencing accidents per year. In the next step, EW--AHP methods was taken to build a safe driving scoring system and according to driving safety latent variable, the score was turned into the adjustment factor of the independent underwriting coefficient (underwriting by company). After further case study, it turned out that the difference between premiums was remarkable and provided the basis to personalized Ratemaking of Vehicle Insurance.	anhaptoglobinemia;coefficient;drug vehicle;latent variable;personalization;score	Ch. S. Park;Chaofan Wang;José Gregorio Peña-Delgado;Zhijie Liu;Tianmei Wang	2018		10.1145/3265689.3265696	actuarial science;underwriting;computer science;latent variable	Mobile	4.609795833374845	-15.06512533705782	32732
ae310d35b36619799167c076888efb020da066f8	a hybrid model for optimal concurrent design of solid oxide fuel cell system considering functional performance and production cost	functional performance;multi objective optimization;software systems;concurrent design;hybrid model;solid oxide fuel cell sofc;system design;optimal design;production cost;solid oxide fuel cell;neural network	This research addresses the issues to identify the optimal design of solid oxide fuel cell (SOFC) system considering functional performance and production cost. In this research, modeling of the relations between design parameters and evaluation parameters is first discussed. Due to uncertainties of parameter relations, a hybrid model is introduced in this work to describe two types of parameter relations, mathematical relations and neural network relations, and associate these two types of relations through a parameter relation network. The optimal SOFC system design considering function performance and production cost is achieved by changing values of design parameters based on evaluation of performance and cost parameters through multi-objective optimization. A software system has been developed based on the introduced method. A case study has also been conducted to demonstrate the effectiveness of the optimal SOFC system design approach.	artificial neural network;cell (microprocessor);lazy evaluation;mathematical optimization;multi-objective optimization;optimal design;software system;systems design	Dong Zhao;Wei Dong;Deyi Xue	2008	Concurrent Engineering: R&A	10.1177/1063293X08092489	control engineering;simulation;computer science;engineering;optimal design;operations management;multi-objective optimization;artificial neural network;concurrent engineering;software system;systems design	SE	14.908828549657267	-6.257602825064107	32800
4af77d4267a8da198d1624cb71fc6b23a594e61d	resolution of fuzzy relational equations - method, algorithm and software with applications	fuzzy relational equations;inverse problem resolution	Analytical methods and algorithms for inverse problem resolution of fuzzy linear systems of equations in some BL-algebras (Gödel algebra in case of max–min and min–max compositions, and Goguen algebra in case of max–product composition) are presented. Algorithms with software realization for solving fuzzy linear systems of equations are proposed. Applications include fuzzy optimization with fuzzy linear systems of equation constraint, fuzzy machines and covering problem. 2011 Elsevier Inc. All rights reserved.	algorithm;bl (logic);covering problems;gödel;linear system;linked list;mathematical optimization;maxima and minima;source lines of code;t-norm	K. Peeva	2013	Inf. Sci.	10.1016/j.ins.2011.04.011	fuzzy logic;mathematical optimization;combinatorics;discrete mathematics;defuzzification;adaptive neuro fuzzy inference system;fuzzy mathematics;fuzzy classification;computer science;fuzzy subalgebra;fuzzy number;neuro-fuzzy;fuzzy measure theory;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;fuzzy control system	AI	0.5186840877813623	-22.909005185980728	32863
24baf16f8cf09ae6be4973302df600aa43cb3d1a	the risk assessment of bank based on multiple correlation analysis and three-valued logic neuron model	three valued logic;banking;history;neural networks;neural nets;risk analysis;uncertainty;multiple correlation analysis;uncertainty knowledge;risk evaluation;wearable computers;risk management;uncertainty knowledge bank risk assessment multiple correlation analysis three valued logic two value logic neuron neural network;random variables;uncertainty handling;correlation methods;power method;bank risk;inference rule;artificial neural networks;risk management risk analysis neurons neural networks uncertainty production multivalued logic random variables wearable computers information analysis;three valued logic neural network;uncertainty handling banking correlation methods neural nets risk management ternary logic;loans and mortgages;ternary logic;production;bank risk assessment;risk assessment;risk evaluation multiple correlation analysis three valued logic neural network;neurons;correlation;neural network model;multivalued logic;information analysis;neural network;correlation analysis;two value logic neuron	Most of questions have the uncertainty and the illegibility. Taking account of the insufficiency of two value logic neuron, the paper puts forward three-valued logic neural network, based on the multiple correlation analysis, which have been successfully used in the bank risk assessment. The paper, firstly has constructed the multiple correlation model. Secondly, puts forward the three-valued logic neural network and explains its basic operations such as inference rules. Finally, combining of the bank risk assessment and using the multiple correlation analysis and has determined the evaluation index and constructed an assessment of the neural network model. The result indicates that this model can be used in three-valued connection rules to realize three-valued logic reasoning. This has provided one powerful method for the uncertainty knowledge's expression and the inference.	artificial neural network;biological neuron model;convergence insufficiency;network model;risk assessment;three-valued logic	Peng Dong;Feng Dai;Yingjiang Li	2010	2010 Asia-Pacific Conference on Wearable Computing Systems	10.1109/APWCS.2010.12	risk assessment;random variable;risk analysis;uncertainty;wearable computer;power iteration;computer science;artificial intelligence;machine learning;data mining;data analysis;probabilistic logic network;correlation;artificial neural network;rule of inference	AI	4.104265489696104	-21.65211447440094	32879
e876d0e4925117041d7256565603b778fc5ff3e6	genetic algorithm for the job-shop scheduling with skilled operators		In this paper, we tackle the job shop scheduling problem (JSP) with skilled operators (JSPSO). This is an extension of the classic JSP in which the processing of a task in a machine has to be assisted by one operator skilled for the task. The JSPSO is a challenging problem because of its high complexity and because it models many real-life sit- uations in production environments. To solve the JSPSO, we propose a genetic algorithm that incorporates a new coding schema as well as ge- netic operators tailored to dealing with skilled operators. This algorithm is analyzed and evaluated over a benchmark set designed from conven- tional JSP instances. The results of the experimental study show that the proposed algorithm performs well and at the same time they allowed us to gain insight into the problem characteristics and to draw ideas for further improvements.	genetic algorithm;job shop scheduling;scheduling (computing)	Raúl Mencía;María R. Sierra;Ramiro Varela	2015		10.1007/978-3-319-18833-1_5	simulation;computer science;artificial intelligence;machine learning	AI	22.40798635682518	-1.879971473860269	32912
7e773f462e1bccc5e3ff8025727dd36be20a2ab3	toward co-evolutionary training of a multi-class classifier	genetic program;evolutionary computation;genetic algorithm multiclass classification genetic programming coevolutionary system;multi class classification;pattern classification;genetic algorithm;genetic programming genetic algorithms concurrent computing decision support systems evolution biology computer science hardware assembly clustering algorithms heuristic algorithms;pattern classification evolutionary computation learning artificial intelligence;learning artificial intelligence;evolutionary system	In this work the multi-class classification capabilities of genetic programming (GP) are explored in the context of a competitive co-evolutionary system, in which a population of GP classifiers is trained against an evolving population of trainers (exemplar selectors) with the goal of reducing GP training time for large multi-class classification problems. Moreover, the niche-enabling mechanisms established in the genetic algorithm (GA) literature, known as crowding and sharing, are implemented for the classifier population in order to provide multi-class solutions from a single population in the same trial. The results as presented in the paper indicate the appropriateness of the competitive co-evolutionary training approach under GP multi-class classification.	crowding;genetic algorithm;genetic programming;multiclass classification;niche blogging;norm (social);software release life cycle	Andrew R. McIntyre;Malcolm I. Heywood	2005	2005 IEEE Congress on Evolutionary Computation	10.1109/CEC.2005.1554958	evolutionary programming;genetic programming;genetic algorithm;cultural algorithm;computer science;artificial intelligence;machine learning;multiclass classification;genetic representation;pattern recognition;evolutionary computation	ML	24.503835919474536	-10.625318593207536	32923
12d1c0b17bfc0372938bc028c05a83e8dc64008e	effect of large solar power plant on locational marginal prices in oman		This manuscript investigates the effect of new investments on the locational marginal prices in the main interconnected system of Oman. The study includes modeling the existing transmission network and simulating system performance during peak period. In addition, the effect of renewable energy projects on locational marginal prices are simulated.	computer simulation;marginal model	Mohammed Albadi;Y. M. El-Rayani;Ehab F. El-Saadany;H. A. Al-Riyami	2017	2017 IEEE 30th Canadian Conference on Electrical and Computer Engineering (CCECE)	10.1109/CCECE.2017.7946741	geography;environmental protection;economy;natural resource economics	Visualization	5.767181813683119	-8.064284665082628	32954
71886c33a34c6effe14f465ebe2806383e0d76a3	regal: a regularization based algorithm for reinforcement learning in weakly communicating mdps	markov decision process;optimal regret rate;optimal bias vector;unknown weakly;previous regret bound;various diameter-like quantity;algorithm proceed	We provide an algorithm that achieves the optimal regret rate in an unknown weakly communicating Markov Decision Process (MDP). The algorithm proceeds in episodes where, in each episode, it picks a policy using regularization based on the span of the optimal bias vector. For an MDP with S states and A actions whose optimal bias vector has span bounded by H, we show a regret bound of Õ(HS √ AT ). We also relate the span to various diameter-like quantities associated with the MDP, demonstrating how our results improve on previous regret bounds.	algorithm;markov chain;markov decision process;regret (decision theory);reinforcement learning	Peter L. Bartlett;Ambuj Tewari	2009			mathematical optimization;discrete mathematics;machine learning;mathematics	ML	22.193070206252052	-17.30506467556895	33101
2fc1b3d6eba4f13b3abb315155686b66eac83910	prediction of pine wilt disease in jiangsu province based on web dataset and gis	classification and regression tree;environmental variables;pine wilt disease;seasonal variation;pine forest;logistic regression;receiver operating characteristic curve;web dataset;gis;genetic algorithm for rule set prediction;maximum entropy method;correlation coefficient;prediction;geographic distribution;environmental factor	80 pine wilt disease occurrence points with geographical coordinates in 2007 and 31 environmental variables from open web datasets were gathered as the main source of information. Four modeling methods of Classification and Regression Trees (CART), Genetic Algorithm for Rule-set prediction (GARP), maximum entropy method (Maxent), and Logistic Regression (LR) were introduced to generate potential geographic distribution maps of pine wood nematode in Jiangsu province, China. Then we calculated three statistical criteria of area under the Receiver Operating Characteristic Curve (AUC), Pearson correlation coefficient (COR) and Kappa to evaluate the performance of the models. The results showed that: CART outperformed other three models; slope, precipitation, seasonal variations (bio15), mean temperature of driest quarter (bio9), north-south aspect (northness), maximum temperature of warmest month (bio5) were the six enforcing environmental factors; future occurrence area of pine wilt disease will be 47.27% of total pine forest, tripling present infected area of the pest.	geographic information system	Mingyang Li;Milan Liu;Min Liu;Yunwei Ju	2010		10.1007/978-3-642-16515-3_19	geomatics;prediction;computer science;logistic regression;receiver operating characteristic;seasonality;statistics	Vision	9.873213063914791	-18.709064785270492	33154
b43ca9e23dd9d5caf91df6c15012855af48d7e14	development of a pso-sa hybrid metaheuristic for a new comprehensive regression model to time-series forecasting	forecasting;time series forecasting;curve fit;non linear regression;regression model;time series;simulated annealing;hybrid metaheuristic;hybrid approach;particle swarm optimizer;particle swarm optimization;indexation;curve fitting;fitness function;fitness efficiency index	Forecasting has always been a crucial challenge for organizations as they play an important role in making many critical decisions. Much effort has been devoted over the past several decades to develop and improve the time-series forecasting models. In these models most researchers assumed linear relationship among the past values of the forecast variable. Although the linear assumption makes it easier to manipulate the models mathematically, it can lead to inappropriate representation of many real-world patterns in which non-linear relationship is prevalent. This paper introduces a new time-series forecasting model based on non linear regression which has high flexibility to fit any number of data without pre-assumptions about real patterns of data and its fitness function. To estimate the model parameters, we have used hybrid metaheuristic which has the ability of estimating the optimal value of model parameters. The proposed hybrid approach is simply structured, and comprises two components: a particle swarm optimization (PSO) and a simulated annealing (SA). The hybridization of a PSO with SA, combining the advantages of these two individual components, is the key innovative aspect of the approach. The performance of the proposed method is evaluated using standard test problems and compared with those of related methods in literature, ARIMA and SARIMA models. The results in solving on 11 problems with different structure reveal that the proposed model yields lower errors for these data sets.	metaheuristic;phase-shift oscillator;time series	Javad Behnamian;Seyyed M. T. Fatemi Ghomi	2010	Expert Syst. Appl.	10.1016/j.eswa.2009.05.079	econometrics;mathematical optimization;artificial intelligence;machine learning;time series;statistics;curve fitting	DB	6.492905177646746	-18.550388456953854	33200
963516db4018ee86a5a2eb98718e51019f9bad79	sequential approximation method in multi-objective optimization using aspiration level approach	aspiration level;approximation method;multi objective optimization;support vector regression;decision maker;objective function;pareto optimal solution	One of main issues in multi-objective optimization is to support for choosing a final solution from Pareto frontier which is the set of solution to problem. For generating a part of Pareto optimal solution closest to an aspiration level of decision maker, not the whole set of Pareto optimal solutions, we propose a method which is composed of two steps; i) approximate the form of each objective function by using support vector regression on the basis of some sample data, and ii) generate Pareto frontier to the approximated objective functions based on given the aspiration level. In addition, we suggest to select additional data for approximating sequentially the forms of objective functions by relearning step by step. Finally, the effectiveness of the proposed method will be shown through some numerical examples.	approximation;multi-objective optimization	Yeboon Yun;Hirotaka Nakayama;Min Yoon	2006		10.1007/978-3-540-70928-2_26	mathematical optimization;multi-objective optimization;machine learning;mathematics;algorithm	Robotics	15.304573689481062	-4.51073017495867	33223
070f88e2f0e26f0b803219c927651d0302ef05a7	deliberation scheduling for time-critical sequential decision making	different aspect;time-critical decision;deliberation scheduling;computational resource;different model;precursor model;different circumstance;computational strategy;time-critical sequential decision;decision making;recurrent model;meta-level control problem	We describe a method for time-critical de­ cision making involving sequential tasks and stochastic processes. The method employs several iterative refinement routines for solv­ ing different aspects of the decision mak­ ing problem. This paper concentrates on the meta-level control problem of delibera­ tion scheduling, allocating computational re­ sources to these routines. We provide dif­ ferent models corresponding to optimization problems that capture the different circum­ stances and computational strategies for de­ cision making under time constraints. We consider precursor models in which all deci­ sion making is performed prior to execution and recurrent models in which decision mak­ ing is performed in parallel with execution, accounting for the states observed during ex­ ecution and anticipating future states. We describe algorithms for precursor and recur­ rent models and provide the results of our empirical investigations to date.	algorithm;computation;iterative method;iterative refinement;mathematical optimization;refinement (computing);schedule (computer science);scheduling (computing);sion's minimax theorem;stochastic process;window of opportunity	Thomas L. Dean;Leslie Pack Kaelbling;Jak Kirman;Ann E. Nicholson	1993			optimal decision;influence diagram;decision analysis;decision engineering;computer science;machine learning;data mining;management science;business decision mapping	AI	7.2617711305164026	1.9671238225247427	33294
8793e6999ffd42555a148ef1164ce4a03ae95979	shedding tiers for a la carte? an economic analysis of cable tv pricing		A new regulatory debate has sprung up around the pricing of TV networks on cable and satellite systems. Many argue that bundling networks on tiers, rather than selling channels individually, is anticonsumer and forces families to purchase programming they don’t value and often find offensive. The Federal Communications Commission, after issuing sharply conflicting reports on the subject, is considering measures to enforce a la carte pricing. This paper explains the economics of multi-channel video distribution, showing that network cost conditions dictate reliance on bundling. Consumers do, in fact, purchase programs they find valuable, with operators effectively throwing in additional content for free. This outcome is dictated not by market power, as competitive entrants bundle just as aggressively as do incumbents, but by the underlying economic conditions: cable TV networks are distributed to additional households at zero marginal cost. Restricting the basic tier from, say, 60 channels to just those, say, 20 channels a given subscriber prefers is actually more expensive than providing the large tier to all. The upshot is that the goal of reduced retail prices under a la carte is a chimera.	chimera (software library);linear algebra;marginal model;multitier architecture;product bundling	Thomas W. Hazlett	2006	JTHTL		industrial organization;economics;marketing;advertising	ECom	-2.0268462055496066	-7.402710055060254	33359
80b6ea2976fc47bab7e18bf9da56b0b3416d4a31	aggregation in the analytic hierarchy process: why weighted geometric mean should be used instead of weighted arithmetic mean		Abstract The main focus of this paper is the aggregation of local priorities into global priorities in the Analytic Hierarchy Process (AHP) method. We study two most frequently used aggregation approaches - the weighted arithmetic and weighted geometric means - and identify their strengths and weaknesses. We investigate the focus of the aggregation, the assumptions made on the way, and the effect of different normalizations of local priorities on the resulting global priorities and their ratios. We clearly show the superiority of the weighted geometric mean aggregation over the weighted arithmetic mean aggregation in AHP for the purpose of deriving global priorities of alternatives. We also contribute to the literature on rank reversal in AHP. In particular, we show that a change of the normalization condition for the local priorities of alternatives may result in different ranking when the weighted arithmetic mean aggregation is used for deriving global priorities of alternatives, and we demonstrate that the ranking obtained by the weighted geometric mean aggregation is not normalization dependent. Moreover, we prove that the ratios of global priorities of alternatives obtained by the weighted geometric mean aggregation are invariant under the normalization of local priorities of alternatives and weights of criteria. We also propose three alternative approaches to aggregating preference information contained in local pairwise comparison matrices of alternatives into a global consistent pairwise comparison matrix of alternatives and prove their equivalence.	analytical hierarchy;expert system;mean squared error	Jana Krejcí;Jan Stoklasa	2018	Expert Syst. Appl.	10.1016/j.eswa.2018.06.060	artificial intelligence;normalization (statistics);geometric mean;rank reversals in decision-making;mathematical optimization;machine learning;weighted geometric mean;computer science;analytic hierarchy process;pairwise comparison;ranking;weighted arithmetic mean	ML	-3.3878709185829563	-20.13004922145564	33442
2c1ceba201cc3bfd56ad802cdf7d85be756a410e	value-centric framework and pareto optimality for design and acquisition of communication satellites	uncertainty;npv;value;satellite;system design;pareto;optimization;communication;pareto optimality	Abstract#R##N##R##N#Investments in space systems are substantial, indivisible, and irreversible, characteristics of high-risk investments. Traditional approaches to system design, acquisition, and risk mitigation are derived from a cost-centric mindset, and as such they incorporate little information about the value of the spacecraft to its stakeholders. These traditional approaches are appropriate in stable environments. However, the current technical and economic conditions are distinctly uncertain and rapidly changing. Consequently, these traditional approaches have to be revisited and adapted to the current context.#R##N##R##N##R##N##R##N#We propose that in uncertain environments, decision-making with respect to design and acquisition choices should be value-based. We develop a value-centric framework, analytical tools, and an illustrative numerical example for communication satellites. Our two proposed metrics for decision-making are the system's expected value and value uncertainty. Expected value is calculated as the expected NPV of the satellite. The cash inflow is calculated as a function of the satellite loading, its transponder pricing, and market demand. The cash outflows are the various costs for owning and operating the satellite. Value uncertainty emerges due to uncertainties in the various cash flow streams, in particular because of market conditions. We propagate market uncertainty through Monte Carlo simulation, and translate it into value uncertainty for the satellite. The end result is a portfolio of Pareto-optimal satellite design alternatives.#R##N##R##N##R##N##R##N#By using value and value uncertainty as decision metrics in the down-selection process, decision-makers draw on more information about the system in its environment, and in making value-based design and acquisition choices, they ultimately make more informed and better choices. Copyright © 2009 John Wiley & Sons, Ltd.	communications satellite;pareto efficiency	Joy Brathwaite;Joseph H. Saleh	2009	Int. J. Satellite Communications Networking	10.1002/sat.956	pareto principle;net present value;simulation;uncertainty;telecommunications;management science;satellite;statistics;systems design	Mobile	0.750072597227704	-9.02991277629672	33488
f91cb29e6c2cbeeba0e247d71071584217c2786f	modelling load-settlement behaviour of piles using high-order neural network (hon-pile model)	theoretical model;soil pile interaction;faculty of science environment engineering and technology;civil geotechnical engineering;090501;high order neural network;soil properties;spt;load settlement behaviour;artificial neural network;neural network	An accurate estimation of pile response to loading is a challenging task due to the complexity of the soil-pile interactions and uncertainties in the soil properties. Conventional methods of predicting pile load-settlement relationship either oversimplify the problem or require the parameters that are difficult to determine in the laboratory. In this study, a high-order neural network (HON) is developed to simulate the pile load-settlement curve using properties of the pile and SPT data along the depth of pile embedment as inputs. The results indicated a significant improvement in the quality of HON predictions over that of BPN, RBF and GRNN models. Based on the comparisons with the predictions of elastic and hyperbolic models, the proposed HON model provides better predictions than existing theoretical models.	artificial neural network	Ahmed Taha Ismail;D.-S. Jeng	2011	Eng. Appl. of AI	10.1016/j.engappai.2011.02.008	simulation;computer science;machine learning;symmetry protected topological order;artificial neural network	AI	11.72778595278798	-20.76802639924787	33521
27e65a1f9e954128ad837e23502541cfaf00a544	provision of non-excludable public goods on networks: from equilibrium to centrality measures	optimized production technology;game theory;nash equilibrium;pollution measurement;investment;games nash equilibrium investment pollution measurement security optimized production technology numerical models;games;interdependence graph nonexcludable public goods centrality measures interdependent strategic users nash equilibrium socially optimal exit equilibrium profiles;numerical models;security	We consider the provision of non-excludable public goods on a network of interdependent strategic users. We study three different equilibria of these games, namely the Nash equilibrium, socially optimal, and exit equilibrium profiles. We identify properties of the interdependence graph that guarantee the existence and uniqueness of these equilibria. We further establish a connection between users' centralities in their interdependence network, and their efforts at different interior equilibria of these games. These characterizations separate the effects of incoming and outgoing dependencies, as well as the influence of paths of different length, on users' effort levels. We discuss some conceptual and practical implications of this centrality-effort connection.	centrality;interdependence;nash equilibrium;norm (social)	Parinaz Naghizadeh Ardabili;Mingyan Liu	2015	2015 53rd Annual Allerton Conference on Communication, Control, and Computing (Allerton)	10.1109/ALLERTON.2015.7446986	price of stability;markov perfect equilibrium;epsilon-equilibrium;best response;sequential equilibrium;economics;microeconomics;mathematical economics;welfare economics;equilibrium selection;nash equilibrium	ECom	-1.3070528777136303	-3.4417400575719803	33532
14e290da7f4495871964d179f75ce272f7a61ace	adjusting the errors of the gm(1, 2) grey model in the financial data series using an adaptive fuzzy controller		Purpose#R##N##R##N##R##N##R##N##R##N#The purpose of this paper is to focus on the adjustment of the GM(1, 2) errors for financial data series that measures changes in the public sector financial indicators, taking into account that the errors in grey models remain a key problem in reconstructing the original data series.#R##N##R##N##R##N##R##N##R##N#Design/methodology/approach#R##N##R##N##R##N##R##N##R##N#Adjusting the errors in grey models must follow some rules that most often cannot be determined based on the chaotic trends they register in reconstructing data series. In order to ensure the adjustment of these errors, for improving the robustness of GM(1, 2), was constructed an adaptive fuzzy controller which is based on two input variables and one output variable. The input variables in the adaptive fuzzy controller are: the absolute error ei0(k)[%] of GM(1, 2), and the distance between two values xi0(k)[%], while the output variable is the error adjustment Aei0(k)[%] determined with the help of the above-mentioned input variables.#R##N##R##N##R##N##R##N##R##N#Findings#R##N##R##N##R##N##R##N##R##N#The adaptive fuzzy controller has the advantage that sets the values for error adjustments by the intensity (size) of the errors, in this way being possible to determine the value adjustments for each element of the reconstructed financial data series.#R##N##R##N##R##N##R##N##R##N#Originality/value#R##N##R##N##R##N##R##N##R##N#To ensure a robust process of planning the financial resources, the available financial data are used for long periods of time, in order to notice the trend of the financial indicators that need to be planned. In this context, the financial data series could be reconstituted using grey models that are based on sequences of financial data that best describe the status of the analyzed indicators and the status of the relevant factors of influence. In this context, the present study proposes the construction of a fuzzy adaptive controller that with the help of the output variable will ensure the error’s adjustment in the reconstituted data series with GM(1, 2).		Marcel Bolos;Ioana Bradea;Camelia Delcea	2016	Grey Systems: T&A	10.1108/GS-12-2015-0079	econometrics;engineering;operations management;data mining	NLP	5.4835520634448	-15.65828203643614	33553
1f92ca5a2e3837f7aec48844ab9688bd9eed2b1e	the aircraft ground routing problem: analysis of industry punctuality indicators in a sustainable perspective	ground routing;airport operations management;mixed integer programming;sustainable development	The ground routing problem consists in scheduling the movements of aircraft on the ground between runways and parking positions while respecting operational and safety requirements in the most efficient way. We present a Mixed Integer Programming (MIP) formulation for routing aircraft along a predetermined path. This formulation is generalized to allow several possible paths. Our model takes into account the classical performance indicators of the literature (the average taxi and completion times) but also the main punctuality indicators of the air traffic industry (the average delay and the on time performance). Then we investigate their relationship through experiments based on real data from Copenhagen airport (CPH). We show that the industry punctuality indicators are in contradiction with the objective of reducing taxi times and therefore pollution emissions. We propose new indicators that are more sustainable, but also more relevant for stakeholders. We also show that the runway is the main bottleneck of CPH airport and that alternate paths cannot improve the performance indicators.	airport security;experiment;hardware random number generator;integer programming;jackson;linear programming;loss function;network congestion;numerical analysis;optimization problem;ramp simulation software for modelling reliability, availability and maintainability;regular language;requirement;routing;scheduling (computing);wikipedia;world3	Julien Guépet;Olivier Briant;Jean-Philippe Gayon;Rodrigo Acuna-Agost	2016	European Journal of Operational Research	10.1016/j.ejor.2015.08.041	simulation;integer programming;operations management;mathematics;sustainable development	AI	13.10971085965931	0.0006597921820337211	33565
4be5082502bf3aae25e4465482f6df541135d898	a parallel best-first b&b algorithm and its axiomatization	traveling salesman problem;distributed memory;g 2 1;d 4 2;f 2 2;i 6 4;axiomatization;branch and bound algorithm;distributed memory machine;f 1 2;probabilistic model;d 4 1;synchronization;information exchange;parallelization;combinatorial optimization;branch and bound;d 1 3	We present a new parallel best-first Branch and Bound algorithm designed for distributed memory machines. Starting from an axiomatization of the Branch and Bound pardigm, we develop the notion of fringes in the Branch and Bound tree which correspond to sets of equivalent problems. The algorithm proposed in this paper evaluates these sets of problems in parallel, during each phase of its execution. These computationally intensive phases alternate with control phases where synchronization and information exchange between processors lakes place. We use a probabilistic model for predicting the performances of this algorithm. Finally, we discuss the performances obtained on MIMD-DM multiprocessors for the asymmetric non-Euclidean Traveling Salesman Problem.	algorithm;axiomatic system	Marc Gengler;Giovanni Coray	1994	Parallel Algorithms Appl.	10.1080/10637199408915407	mathematical optimization;parallel computing;combinatorial optimization;computer science;theoretical computer science;mathematics;branch and bound;algorithm	AI	22.25955928712893	4.130232325678691	33617
b12b1970a3fd0498235dc0bb4aff8ff06cef3226	a partition based match making algorithm for taxi sharing	minimisation;singapore;taxi services;taxicabs;partitioning analysis;transportation minimisation road vehicles traffic engineering computing;transportation;ridesharing;algorithms;traffic engineering computing;simulation based approach partition based match making algorithm taxi sharing taxi utilisation improvement passenger inconvenience minimisation road network quality of service requirement constrained optimisation problem singapore;road vehicles	Taxi sharing can help to improve the utilisation of taxis. Passengers for sharing are always chosen with some objectives in mind, for example, to minimise the inconvenience caused due to de-tours when picking up or dropping off other passengers. In this paper, we describe a method that optimises the match making process in order to minimise the inconvenience imposed on passengers. This method is based on the idea of dividing the road network into several partitions so that a certain quality of service requirement regarding inconvenience is satisfied. More precisely, this can be considered as a constrained optimisation problem.We describe a procedure how to decide on optimal parameters for the partitioning algorithm. In addition, we analyse the theoretical maximum sharing potential of commuters in Singapore using a simulation-based approach.	algorithm;equivalence partitioning;experiment;mathematical optimization;privacy;quality of service;simulation	Jiajian Xiao;Heiko Aydt;Michael Lees;Alois Knoll	2013	16th International IEEE Conference on Intelligent Transportation Systems (ITSC 2013)	10.1109/ITSC.2013.6728585	simulation;engineering;operations management;transport engineering	Robotics	13.674536073208552	0.4443972686547727	33619
d95e091a5982bad976267a339c90185c2ea0475e	the vehicle routing problem with floating targets: formulation and solution approaches		This paper addresses a generalization of the vehicle routing problem in which the pick-up locations of the targets are nonstationary. We refer to this problem as the vehicle routing problem with floating targets and the main characteristic is that targets are allowed to move from their initial home locations while waiting for a vehicle. This problem models new applications in drone routing, ridesharing, and logistics where a vehicle agrees to meet another vehicle or a customer at a location that is away from the designated home location. We propose a Mixed Integer Second Order Cone Program (MISOCP) formulation for the problem, along with valid inequalities for strengthening the continuous relaxation. We further exploit the problem structure using a Lagrangian decomposition and propose an exact branch-and-price algorithm. Computational results on instances with varying characteristics are presented and the results are compared to the solution of the full problem using CPLEX. The proposed valid inequalities...	vehicle routing problem	Claudio Gambella;Joe Naoum-Sawaya;Bissan Ghaddar	2018	INFORMS Journal on Computing	10.1287/ijoc.2017.0800	discrete mathematics;mathematical optimization;mathematics;inequality;branch and price;vehicle routing problem;exploit;lagrangian;integer	EDA	16.25230195546499	1.6970969209161255	33721
0bb0c38b5e2d4cd41c84052821d47676219206ce	relicensing as a secondary market strategy	closed loop supply chain;secondary market;relicensing fee;remanufacturing;working paper;closed loop supply chains;durable goods	Secondary markets in the Information Technology (IT) industry, where used or refurbished equipment is traded, have been growing steadily. For Original Equipment Manufacturers (OEMs) in this industry, the importance of secondary markets has grown in parallel, not only as a source of revenue, but also because of their impact on these firms’ competitive advantage and market strategy. Recent articles in the press have severely criticized some OEMs who are perceived to be actively trying to eliminate the secondary market for their products. Others have policies that enhance their secondary markets. The goal of this paper is to understand how an OEM’s incentives and optimal strategies vis-à-vis the secondary market are shaped contingent on her relative competitive advantage, product characteristics and consumer preferences. The critical tradeoff that we examine is whether the indirect benefit from maintaining an active secondary market (the resale value effect) can outweigh the potentially negative effect of the sales of used products at the expense of new product sales (the cannibalization effect). To that end, we develop a model where the OEM can directly affect the resale value of her product through a relicensing fee charged to the buyer of the refurbished equipment. Moreover, we introduce a measure of the consumers’ willingness to return their used products to account for the fact that the higher the price offered by a third-party entrant, the higher the ratio of returned products at their end-of-use. We analyze the OEM’s decision in both the monopoly and the duopoly cases, characterize the optimal relicensing fee set by the OEM, and draw conclusions on the conditions that favor stimulating or deterring the secondary market.	contingency (philosophy);monopoly;refurbishment (electronics)	Nektarios Oraiopoulos;Mark E. Ferguson;L. Beril Toktay	2012	Management Science	10.1287/mnsc.1110.1456	secondary market;economics;marketing;operations management;finance;microeconomics;durable good;management;commerce	ECom	-2.031079010261974	-7.800541139409327	33737
631ad38b0b6776545fc5cacd479367f41b6b78e0	the max problem revisited: the importance of mutation in genetic programming	runtime analysis;genetic program;genetic programming;conference paper;computational complexity;theory;mutation	This paper contributes to the rigorous understanding of genetic programming algorithms by providing runtime complexity analyses of the well-studied Max problem. Several experimental studies have indicated that it is hard to solve the Max problem with crossover-based algorithms. Our analyses show that different variants of the Max problem can provably be solved using simple mutation-based genetic programming algorithms.  Our results advance the body of computational complexity analyses of genetic programming, indicate the importance of mutation in genetic programming, and reveal new insights into the behavior of mutation-based genetic programming algorithms.	algorithm;computational complexity theory;genetic programming;whole earth 'lectronic link	Timo Kötzing;Andrew M. Sutton;Frank Neumann;Una-May O'Reilly	2012		10.1145/2330163.2330348	evolutionary programming;mutation;quality control and genetic algorithms;genetic programming;mathematical optimization;genetic algorithm;computer science;bioinformatics;genetic operator;genetic representation;computational complexity theory;theory;algorithm	PL	23.801370367790604	-6.62557100022647	33751
d094f8bc446d113c9286b27eaf63ba10c49deccf	regression-based models for the prediction of unconfined compressive strength of artificially structured soil		Unconfined compressive strength (UCS) of soil is a critical and important geotechnical property which is widely used as input parameters for the design and practice of various geoengineering projects. UCS controls the deformational behavior of soil by measuring its strength and load bearing capacity. The laboratory determination of UCS is tedious, expensive and being a time-consuming process. Therefore, the present study is aimed to establish empirical equations for UCS using simple and multiple linear regression methods. The accuracy of the developed equations are tested by employing coefficient of determination (R 2), root mean square error (RMSE) and mean absolute percentage error (MAPE). It has been found that the developed equations are reliable and capable to predict UCS with acceptable degree of confidence. Among all the developed models, model-I consist of lime content, curing time, plastic limit, liquid limit, potential of hydrogen, primary ultrasonic wave velocity, optimum moisture content and maximum dry density as independent parameters shows highest prediction capacity with R 2, RMSE and MAPE are 0.96, 25.89 and 16.59, respectively.	approximation error;coefficient of determination;hydrogen;mean squared error;soft computing;velocity (software development);lime	L. K. Sharma;T. N. Singh	2017	Engineering with Computers	10.1007/s00366-017-0528-8	mathematical optimization;statistics;mathematics;compressive strength;mean absolute percentage error;coefficient of determination;linear regression;geotechnical engineering;atterberg limits;bearing capacity;water content;mean squared error	HCI	12.123035019010675	-19.73694476048192	33763
21a4f7db6bd01f0e4e4a157f8e14dbb5afdc6082	experimental comparison of btd and intelligent backtracking: towards an automatic per-instance algorithm selector		We consider a generic binary CSP solver parameterized by high-level design choices, i.e., backtracking mechanisms, constraint propagation levels, and variable ordering heuristics. We experimentally compare 24 different configurations of this generic solver on a benchmark of around a thousand instances. This allows us to understand the complementarity of the different search mechanisms, with an emphasis on Backtracking with Tree Decomposition (BTD). Then, we use a per-instance algorithm selector to automatically select a good solver for each new instance to be solved. We introduce a new strategy for selecting the solvers of the portfolio, which aims at maximizing the number of instances for which the portfolio contains a good solver, independently from a time limit.	backtracking;benchmark (computing);central processing unit;complementarity theory;experiment;greedy algorithm;heuristic (computer science);high- and low-level;justin (robot);level design;local consistency;local search (optimization);pierre wolper;simple features;software propagation;solver;tree decomposition;unified framework	Loïc Blet;Samba Ndiaye;Christine Solnon	2014		10.1007/978-3-319-10428-7_16	theoretical computer science;machine learning;algorithm	AI	24.073069882122976	3.3478479030765977	33764
d358902b201132817e5a4d916e56e5959baa3d94	self-organizing urban transportation systems	global solution;multiagent system;optimization technique;public transport;traffic flow;transport system;self organizing system;self organization;urban transport	Urban transportation is a complex phenomenon. Since many agents are constantly interacting in parallel, it is difficult to predict the future state of a transportation system. Because of this, optimization techniques tend to give obsolete solutions, as the problem changes before it can be optimized. An alternative lies in seeking adaptive solutions. This adaptation can be achieved with self-organization. In a self-organizing transportation system, the elements of the system follow local rules to achieve a global solution. Like this, when the problem changes the system can adapt by itself to the new configuration. In this chapter, I will review recent, current, and future work on self-organizing transportation systems. Self-organizing traffic lights have proven to improve traffic flow considerably over traditional methods. In public transportation systems, simple rules are being explored to prevent the “equal headway instability” phenomenon. The methods we have used can be also applied to other urban transportation systems and their generality is discussed.	instability;interaction;mathematical optimization;organizing (structure);self-organization	Carlos Gershenson	2009	CoRR	10.1007/978-3-642-24544-2_15	self-organization;simulation;artificial intelligence;traffic flow;public transport	AI	16.49665217108293	-8.630660529243	33824
b207ebe7afcb015ef027c6880328473609b921ac	self-consistency of decision rules for group decision making	incompatibility;game theory;arrow theorem;quiebra;systeme aide decision;social decision;teoria juego;theorie jeu;compatibilidad;sistema ayuda decision;prise decision;decision maker;incompatibilite;self consistent;symetrie;symmetry;regle decision;theoreme arrow;decision support system;self consistency;autocoherencia;arrow s impossibility theorem;compatibility;preferencia;autocoherence;compatibilite;preference;group decision making;regla decision;decision colectiva;simetria;simple game;decision collective;toma decision;group decision;teorema arrow;incompatibilidad;decision rule;faillite;bankruptcy	The author treats, in this paper, a group of decision makers, where each of them already has preference on a given set of alternatives but the group as a whole does not have a decision rule to make their group decision, yet. Then, the author examines which decision rules are appropriate. As a criterion of “appropriateness” the author proposes the concepts of self-consistency and universal self-consistency of decision rules. Examining the existence of universally self-consistent decision rules in two cases: (1) decision situations with three decision makers and two alternatives, and (2) those with three decision makers and three alternatives, the author has found that all decision rules are universally self-consistent in the case (1), whereas all universally self-consistent decision rules have one and just one vetoer in the essential cases in (2). The result in the case (2) implies incompatibility of universal self-consistency with symmetry. An example of applications of the concept of self-consistency to a bankruptcy problem is also provided in this paper, where compatibility of self-consistency with symmetry in a particular decision situation is shown.		Takehiro Inohara	2007	European Journal of Operational Research	10.1016/j.ejor.2006.05.013	game theory;decision model;optimal decision;influence diagram;decision support system;economics;arrow's impossibility theorem;decision theory;decision analysis;decision field theory;decision engineering;artificial intelligence;decision tree;decision rule;mathematics;admissible decision rule;evidential reasoning approach;evidential decision theory;welfare economics;weighted sum model;business decision mapping;decision matrix	ECom	-1.2437514116571933	-16.165156661524897	33875
329b565036473a5360cc5e1fcbc635b0fc81e37f	a hybrid method for prediction and assessment efficiency of decision making units: real case study: iranian poultry farms	decision tree;efficiency;data envelopment analysis;poultry meat farming;artificial neural network	The objective of this article is an evaluation and assessment efficiency of the poultry meat farm as a case study with the new method. As it is clear poultry farm industry is one of the most important sub-sectors in comparison to other ones. The purpose of this study is the prediction and assessment efficiency of poultry farms as decision making units DMUs. Although, several methods have been proposed for solving this problem, the authors strongly need a methodology to discriminate performance powerfully. Their methodology is comprised of data envelopment analysis and some data mining techniques same as artificial neural network ANN, decision tree DT, and cluster analysis CA. As a case study, data for the analysis were collected from 22 poultry companies in Iran. Moreover, due to a small data set and because of the fact that the authors must use large data set for applying data mining techniques, they employed k-fold cross validation method to validate the authors' model. After assessing efficiency for each DMU and clustering them, followed by applied model and after presenting decision rules, results in precise and accurate optimizing technique.	iranian.com	Iman Rahimi;Reza Behmanesh;Rosnah Mohd. Yusuff	2013	IJDSST	10.4018/jdsst.2013010104	computer science;operations management;machine learning;decision tree;data mining;data envelopment analysis;efficiency;operations research;artificial neural network	ML	4.471658920205466	-19.596943632145997	33925
1f696cd6f55b5106f36c22553730d211b2e3cbc4	optimizing inventory policies in process networks under uncertainty	process network;receding horizon;inventory policies;stochastic programming;inventory planning	We address the inventory planning problem in process networks under uncertainty through stochastic programming models. The scope of inventory planning requires the formulation of multiperiod models to represent the time-varying conditions of industrial process, but the multistage stochastic programming formulations are often too large to solve. We propose a policy-based approximation of the multistage stochastic formulation that avoids anticipativity by enforcing the same decision rule for all scenarios. The proposed formulation includes the logic modeling inventory policies, and it is used to find the parameters that offer the best expected performance. We propose policies for inventory planning in process networks with arrangements of inventories in parallel and in series. We compare the inventory planning strategies obtained from the policybased formulation with the analogous two-stage approximation of the multistage stochastic program. Sequential implementation of both planning strategies in receding horizon simulations show the advantages of the policy-based model, despite the increase in computational complexity.	approximation;computational complexity theory;inventory;kahn process networks;motion planning;multistage amplifier;optimizing compiler;series and parallel circuits;simulation;stochastic programming	Pablo García-Herreros;Anshul Agarwal;John M. Wassick;Ignacio E. Grossmann	2016	Computers & Chemical Engineering	10.1016/j.compchemeng.2016.05.014	stochastic programming;mathematical optimization;inventory theory;mathematics;management science	AI	9.313732000780016	-2.678794740618009	33930
739a763d7d19e184451a2b0aa1dbc5f06eed79f2	scheduling jobs and preventive maintenance on fuzzy job shop using genetic algorithm	fixed time intervals;cybernetics;fuzzy scheduling;maximum fuzzy completion time;preventive maintenance;decoding;job shop scheduling;processor scheduling;fuzzy job shop scheduling;fuzzy scheduling job scheduling preventive maintenance fuzzy job shop random key genetic algorithm fixed time intervals random key representation decoding strategy maintenance operation discrete crossover maximum fuzzy completion time;fixed time;maintenance engineering;random key genetic algorithm;decoding strategy;fuzzy set theory;fuzzy job shop;genetic algorithm fuzzy job shop scheduling preventive maintenance random key;preventive maintenance fuzzy set theory genetic algorithms job shop scheduling;discrete crossover;biological cells;random key;machine learning;maintenance engineering job shop scheduling decoding biological cells machine learning processor scheduling cybernetics;scheduling problem;random key representation;genetic algorithm;job shop;genetic algorithms;job scheduling;maintenance operation	Preventive maintenance (PM) has been considered on many scheduling problems, however, the problem of scheduling jobs and PM on fuzzy job shop are seldom investigated. This paper presents a random key genetic algorithm (RKGA) for the problem with resumable jobs and PM in the fixed time intervals. RKGA uses a novel random key representation, a new decoding strategy incorporating maintenance operation, and discrete crossover. RKGA is applied to some instances to minimize the maximum fuzzy completion time. Computational results show the optimization ability of RKGA on fuzzy scheduling with PM.	computation;crossover (genetic algorithm);genetic algorithm;job stream;mathematical optimization;schedule (project management);scheduling (computing)	You-Lian Zheng;Yuan-Xiang Li;De-Ming Lei	2010	2010 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2010.5580802	job shop scheduling;real-time computing;genetic algorithm;flow shop scheduling;computer science;artificial intelligence;machine learning;distributed computing	AI	19.662881000652582	-0.23795478424445907	33976
db59382728d7a9891a08a4eff63da99beb5d827f	fuzzy multicriteria programming in economic systems analysis	economic system	s I 1 ! Fuzzy Multicriteria Programming in Economic Systems Analysis L. D. Xu Information Systems, University of Wisconsin, La Crosse, Wisconsin 54601 In this paper, four main aspects of both the inquiring and operational systems of economic systems analysis are explored: (1) A new philosophical paradigm as the foundation of general methodology in place of Kantian-Newtonian inquiring systems is proposed. (2) A new problem formulation space--a multidimensional, synergetic, and autopoietic model--is proposed. (3) The new philosophical paradigm is characterized as a Singerian inquiring system, and Marglinu0027s multiobjective analysis is replaced by Singerian multiobjective analysis. (4) Fuzzy set theory and fuzzy Markov theory are introduced for multiobjective analysis. An interactive and iterative fuzzy programming method is proposed for solving a quasi-optimization problem under constraints involving a multiple objective function. Comparing this with the adapted gradient search method, surrogate worth trade-off method, and the Zionts-Wallenius method, the approximate preference structure is emphasized. The paper discusses how to specify a set of nondominated solutions and approach a nondominated extreme point from the perspective of the decision maker. One difference between the surrogate worth trade-off method and the proposed method is that the former indicates the marginal rates of substitution with the limitation of pair trade-off, while the latter shows an approximate preference structure for generating a new ideal point. The trade-off of pairs is substituted by multidimensional evaluations without assuming that certain conditions remain unchanged. Application of Possibility Theory to Learning in Knowledge-Based Systems with an Imperfect Teacher Maria Zemankova Computer Science Department, University of Tennessee, 107 Ayres Hall, Knoxville, Tennessee 37996-1301 The goal of the learning process is to build a knowledge base consisting of a set of concepts defined over an object set described by its attributes. The object set is be grouped into equivalence classes using similarity relations defined on attribute domains. The learning algorithm is based on finding a possibility function value for each equivalence class or the degree to which this class satisfies the concept being learned. This function is translated into an optimized description of the concept that can be easily read by users. During the learning process the teacher provides positive and negative examples, and the system computes the possibility function values for the boundary region that can be corrected by the teacher if necessary. The teacher may be imperfectu0027 u0027 --he or she does not have to know the concepts exactly--and the system will		L. D. Xu	1988	Int. J. Approx. Reasoning		computer science	Logic	-2.712077267450079	-17.2532227246498	34011
010cbe5ff93da2c0defb368828e5a90facf026a2	conditional value at risk methodology under fuzzy-stochastic approach	uncertainty modeling;conditional value at risk;fuzzy sets;portfolio	This paper describes methodology of dealing with financial modeling under uncertainty with risk and vagueness aspects. An approach to modeling risk by the Conditional Value at Risk methodology under imprecise and soft Conditions is solved. It is supposed that the input data and problem conditions are difficult to determine as real number or as some precise distribution function. Thus, vagueness is modeled through the fuzzy numbers of linear T-number type. The combination of risk and vagueness is solved by fuzzy-stochastic methodology. Illustrative example is introduced.	expected shortfall;value at risk	Shao-fang Tang;Ying-yu He	2013		10.1007/978-3-642-39479-9_20	econometrics;expected shortfall;computer science;data mining;mathematics;dynamic risk measure;fuzzy set;statistics	ECom	-0.43999816802234143	-18.660558640009707	34013
3f87b15920e0cdb053ad7c937e8283a24b2726f5	fair division of a graph		We consider fair allocation of indivisible items under an additional constraint: there is an undirected graph describing the relationship between the items, and each agent’s share must form a connected subgraph of this graph. This framework captures, e.g., fair allocation of land plots, where the graph describes the accessibility relation among the plots. We focus on agents that have additive utilities for the items, and consider several common fair division solution concepts, such as proportionality, envy-freeness and maximin share guarantee. While finding good allocations according to these solution concepts is computationally hard in general, we design efficient algorithms for special cases where the underlying graph has simple structure, and/or the number of agents—or, less restrictively, the number of agent types—is small. In particular, despite non-existence results in the general case, we prove that for acyclic graphs a maximin share allocation always exists and can be found efficiently.	accessibility relation;agent-based model;algorithm;approximation;computation;directed acyclic graph;directed graph;graph (discrete mathematics);minimax;multi-armed bandit;nash equilibrium;ordinal data;round-robin scheduling;utility functions on indivisible goods;word lists by frequency	Sylvain Bouveret;Katarína Cechlárová;Edith Elkind;Ayumi Igarashi;Dominik Peters	2017		10.24963/ijcai.2017/20	fair division;mathematical economics;accessibility relation;minimax;computer science;proportionality (mathematics);graph	AI	-3.047110421619794	0.032246945202885485	34020
27638e0c5e9d2ddddeb60c5349c4cf999a916ab1	optimality conditions for long-run average rewards with underselectivity and nonsmooth features	optimization;markov processes;transient analysis;dynamic programming;viscosity;generators;aerospace electronics	We study three existing issues associated with optimization of long-run average rewards of time-nonhomogeneous Markov processes in continuous time with continuous state spaces: 1) the underselectivity, i.e., the optimal policies do not depend on their actions in any finite time period; 2) its related issue, the bias optimality, i.e., policies that optimize both long-run average and transient total rewards, and 3) the effects of a nonsmooth point of a value function on performance optimization. These issues require considerations of the performance in the entire period with an infinite horizon, and therefore are not easily solvable by dynamic programming, which works backwards in time and takes a local view at a particular time instant. In this paper, we take a different approach called the  relative optimization theory, which is based on a direct comparison of the performance measures of any two policies. We derive tight necessary and sufficient optimality conditions that take the underselectivity into consideration; we derive bias optimality conditions for both long-run average and transient rewards; and we show that the effect of a wide class of nonsmooth points, called semismooth points, of a value function on the long-run average performance is zero and can be ignored.	bellman equation;best, worst and average case;decision problem;dynamic programming;markov property;mathematical optimization;performance tuning	Xi-Ren Cao	2017	IEEE Transactions on Automatic Control	10.1109/TAC.2017.2655487	horizon;mathematical optimization;control theory;mathematical economics;mathematics;dynamic programming;markov process;bellman equation;instant	Metrics	3.51008767968693	-1.6071590820204593	34129
349a9bab56a2e5e06c96f6cef1e94b5a58e387ba	pursuit on an organized crime network		We model the hierarchical evolution of an organized criminal network via antagonistic recruitment and pursuit processes. Within the recruitment phase, a criminal kingpin enlists new members into the network, who in turn seek out other affiliates. New recruits are linked to established criminals according to a probability distribution that depends on the current network structure. At the same time, law enforcement agents attempt to dismantle the growing organization using pursuit strategies that initiate on the lower level nodes and that unfold as self-avoiding random walks. The global details of the organization are unknown to law enforcement, who must explore the hierarchy node by node. We halt the pursuit when certain local criteria of the network are uncovered, encoding if and when an arrest is made; the criminal network is assumed to be eradicated if the kingpin is arrested. We first analyze recruitment and study the large scale properties of the growing network; later we add pursuit and use numerical simulations to study the eradication probability in the case of three pursuit strategies, the time to first eradication and related costs. Within the context of this model, we find that eradication becomes increasingly costly as the network increases in size and that the optimal way of arresting the kingpin is to intervene at the early stages of network formation. We discuss our results in the context of dark network disruption and their implications on possible law enforcement strategies.	dark fibre;denial-of-service attack;halting problem;network formation;numerical analysis;simulation	Charles Z. Marshak;M. Puck Rombach;Andrea L. Bertozzi;Maria R. D'Orsogna	2015	CoRR		operations research;computer security	ML	-4.114025649797367	3.8245006642226915	34133
496b32c4d19a4162ed21aa22ef743b58e36d6e5f	supply chain contracting in environments with volatile input prices and frictions	risk management;interface of operations and finance;supply chain management	"""We model of a bilateral monopoly supply chain with stochastic demand, stochastic input costs, production lead times, and working capital constraints. Both the upstream component supplier and the downstream buyer (final good assembler) are risk-neutral and have limited liability. The supplier announces a supply contract to which the assembler responds with an order quantity. The assembler, who is a price-setter, faces linear demand curve. The supply chain contract that we study is a general, multi-instrument contract into which many textbook supply contracts can be readily mapped. That is, textbook contracts such as wholesale price, revenue-sharing, or two-part tariff contract are special cases of our general contract. Our main result is a set of necessary and sufficient conditions that the instruments of the general contract must satisfy in order for the contract to induce first-best. For first-best to occur in the absence of any capital constraints, the contract terms must include a transfer price that adjusts to prevailing commodity prices (we refer to such a transfer price as """"index price"""") and default penalties, which become active when the firms' commodity input costs are sufficiently high. As an example, we identify two contracts that satisfy these conditions. The contracts are an index and penalty (I&P) revenue-sharing contract and an I&P two-part tariff contract. For first-best to occur in the presence of capital constraints (i.e., when firms must borrow working capital in order to produce) the terms of the coordinating contract must also include capital commitments. Moreover, I&P two-part tariff contract can no longer coordinate. In situations when the coordinating contract cannot be implemented (e.g., because firms are unable to contract on capital), further analysis reveals that the supplier may prefer to offer the assembler contracts that perform poorly when the supply chain operates without any budget constraints. One example of such contact is a pass-through contract under which the supplier passes 100% of his commodity input cost on the assembler and charges a fixed markup for each unit produced."""		Panagiotis Kouvelis;Danko Turcic;Wenhui Zhao	2018	Manufacturing & Service Operations Management	10.1287/msom.2017.0660	contract management;operations management;spot contract;microeconomics;business;forward contract;commerce	Robotics	-0.6543195327817208	-6.0987447073217576	34203
33d36012aed74782427d423490db0d19d7ec331c	using genetic programming to improve software effort estimation based on general data sets	genetic program;software effort estimation;genetics;public domain;ge netic programming	This paper investigates the use of various techniques including genetic programming, with public data sets, to attempt to model and hence estimate software project effort. The main research question is whether genetic programs can offer 'better' solution search using public domain metrics rather than company specific ones. Unlike most previous research, a realistic approach is taken, whereby predictions are made on the basis of the data available at a given date. Experiments are reported, designed to assess the accuracy of estimates made using data within and beyond a specific company. This research also offers insights into genetic programming’s performance, relative to alternative methods, as a problem solver in this domain. The results do not find a clear winner but, for this data, GP performs consistently well, but is harder to configure and produces more complex models. The evidence here agrees with other researchers that companies would do well to base estimates on in house data rather than incorporating public data sets. The complexity of the GP must be weighed against the small increases in accuracy to decide whether to use it as part of any effort prediction estimation.	experiment;genetic algorithm;genetic programming;global positioning system;information privacy;loose source routing;paging;randomness;software development effort estimation;software project management;solver	Martin Lefley;Martin J. Shepperd	2003		10.1007/3-540-45110-2_151	mathematical optimization;public domain;simulation;computer science;artificial intelligence;data science;machine learning;data mining	AI	19.669841477512655	-8.300743795539908	34214
70fa2cb9d909bd63f5c1c4975e04de2545c3d05e	bridging the gap between theory and practice of approximate bayesian inference	bayesian inference;parameterized complexity theory;approximate bayesian inference;approximation;article letter to editor;algorithms;computational explanation;np hard;article in monograph or in proceedings	In computational cognitive science, many cognitive processes seem to be successfully modeled as Bayesian computations. Yet, many such Bayesian computations has been proven to be computationally intractable (NP-hard) for unconstrained input domains, even if only an approximate solution is sought. This computational complexity result seems to be in strong contrast with the ease and speed with which humans can typically make the inferences that are modeled by Bayesian models. This contrast—between theory and practice—poses a considerable theoretical challenge for computational cognitive modelers: How can intractable Bayesian computations be transformed into computationally plausible `approximate’ models of human cognition? In this paper, three candidate notions of ‘approximation’ are discussed, each of which has been suggested in the cognitive science literature. We will sketch how (parameterized) computational complexity analyses can yield model variants that are tractable and which can serve as the basis of computationally plausible models of cognition.	approximation algorithm;bayesian approaches to brain function;bayesian network;bridging (networking);cobham's thesis;cognition;cognitive model;cognitive science;computation;computational complexity theory;fitness approximation;np (complexity)	Johan Kwisthout;Iris van Rooij	2013	Cognitive Systems Research	10.1016/j.cogsys.2012.12.008	variable-order bayesian network;computer science;artificial intelligence;theoretical computer science;machine learning;approximation;np-hard;bayesian statistics;bayesian inference	AI	21.217905631666675	-13.467946474156456	34230
19e9318bb1f67c40fe343e090c0a43885bb6d2fe	evolutionary robotics	genetic algorithms;intelligent control;mobile robots;behavioural competences;control system organisation;evolutionary robotics;fully autonomous robots;insects;mobile robots;species adaptation genetic algorithms;subsumption;survival skills;unknown environments	But here, you can get it easily this evolutionary robotics to read. As known, when you read a book, one to remember is not only the PDF, but also the genre of the book. You will see from the PDF that your book chosen is absolutely right. The proper book option will influence how you read the book finished or not. However, we are sure that everybody right here to seek for this book is a very fan of this kind of book.	evolutionary robotics;portable document format	Dario Floreano;Phil Husbands;Stefano Nolfi	2008		10.1007/978-3-540-30301-5_62	artificial intelligence;machine learning	ECom	24.262679282549225	-12.269682415197158	34239
2bfa6538957f790f9c2bfff51d26aaebdaf8a1fc	modelling urban transport sector: a methodology based on osemosys model generator		Transport sector could play a relevant role in future energy decarbonisation pathways contributing to energy consumption and pollutant emissions reduction. Planning in the transport sector requires that policy makers be supported with scientifically sound tools, which are able to take into account the peculiar aspects of transport system analyses, especially at urban scale. One of these peculiarities is the spatial dimension of origin-destination travels, that usually cannot be properly modelled by the common bottom-up energy model generators. These generators are used to build models of the energy systems at different geographic scales, including the urban one, and are suitable for forecasting scenario analyses over mid-long term time horizons, e.g. for studying the decarbonisation pathway feasibility. To face the spatial dimension issue, a new concept – the transport corridor – has been introduced into the open source optimization model generator OSeMOSYS, by acting on the structure of its equations. The proposed methodology, in particular, allows the definition of the minimum cost configuration of an urban transport system, with a competition among transport modes and technologies able to fulfil the mobility demand, under different technical and environmental constraints. This approach can comparatively evaluate the impacts of alternative policy options with a higher granularity with respect to more conventional modelling techniques. As such, it could be useful in assessing, under a mid-long term perspective, the effectiveness of specific local measures (like restricted traffic areas) or actions promoting sustainable mobility (as car-pooling/sharing initiatives), supporting the identification of issues and investment priorities.	bottom-up parsing;gene regulatory network;mathematical optimization;open-source software;pro tools	Daniele Grosso;Raffaella Gerboni;Dario Cotugno	2017	2017 IEEE 41st Annual Computer Software and Applications Conference (COMPSAC)	10.1109/COMPSAC.2017.171	transport corridor;energy consumption;pollutant;granularity;transport engineering;engineering	SE	9.528172529458757	-6.336430103429142	34244
d6c58c619be8e31c33d7cdcb3022d839c83feb82	model-based prognostics with concurrent damage progression processes	statistical distributions condition monitoring fault diagnosis pumps remaining life assessment;pumps;statistical distributions;mathematical model friction vectors joints impellers torque estimation;condition monitoring;variance control centrifugal pumps model based prognostics particle filters;concurrent damage progression processes damage progression models centrifugal pump physics based model variance control algorithm probability distribution remaining useful life end of life prediction state parameter estimation component degradation model based prognostics;remaining life assessment;fault diagnosis	Model-based prognostics approaches rely on physics-based models that describe the behavior of systems and their components. These models must account for the several different damage processes occurring simultaneously within a component. Each of these damage and wear processes contributes to the overall component degradation. We develop a model-based prognostics methodology that consists of a joint state-parameter estimation problem, in which the state of a system along with parameters describing the damage progression are estimated, followed by a prediction problem, in which the joint state-parameter estimate is propagated forward in time to predict end of life and remaining useful life. The state-parameter estimate is computed using a particle filter and is represented as a probability distribution, allowing the prediction of end of life and remaining useful life within a probabilistic framework that supports uncertainty management. We also develop a novel variance control algorithm that maintains an uncertainty bound around the unknown parameters to limit the amount of estimation uncertainty and, consequently, reduce prediction uncertainty. We construct a detailed physics-based model of a centrifugal pump that includes damage progression models, to which we apply our model-based prognostics algorithm. We illustrate the operation of the prognostic solution with a number of simulation-based experiments and demonstrate the performance of the approach when multiple damage mechanisms are active.	agent-based model;algorithm;centrifugal governor;color gradient;computation;computational complexity theory;elegant degradation;end-of-life (product);estimation theory;experiment;extrapolation;image noise;particle filter;simulation;uncertainty quantification	Matthew J. Daigle;Kai Goebel	2013	IEEE Transactions on Systems, Man, and Cybernetics: Systems	10.1109/TSMCA.2012.2207109	probability distribution;mathematics;statistics	ML	14.699475273606158	-14.985883617792856	34312
2b226f2b0ac26312ca78c87eb145abf54217fb7a	move acceptance in local search metaheuristics for cross-domain search		Abstract Metaheuristics provide high-level instructions for designing heuristic optimisation algorithms and have been successfully applied to a range of computationally hard real-world problems. Local search metaheuristics operate under a single-point based search framework with the goal of iteratively improving a solution in hand over time with respect to a single objective using certain solution perturbation strategies, known as move operators, and move acceptance methods starting from an initially generated solution. Performance of a local search method varies from one domain to another, even from one instance to another in the same domain. There is a growing number of studies on ‘more general’ search methods referred to as cross-domain search methods, or hyper-heuristics, that operate at a high-level solving characteristically different problems, preferably without expert intervention. This paper provides a taxonomy and overview of existing local search metaheuristics along with an empirical study into the effects that move acceptance methods, as components of single-point based local search metaheuristics, have on the cross-domain performance of such algorithms for solving multiple combinatorial optimisation problems. The experimental results across a benchmark of nine different computationally hard problems highlight the shortcomings of existing and well-known methods for use as components of cross-domain search methods, despite being re-tuned for solving each domain.	algorithm;benchmark (computing);combinatorial optimization;heuristic;high- and low-level;hyper-heuristic;local search (optimization);mathematical optimization;metaheuristic;taxonomy (general)	Warren G. Jackson;Ender Özcan;Robert Ivor John	2018	Expert Syst. Appl.	10.1016/j.eswa.2018.05.006	machine learning;empirical research;local search (optimization);artificial intelligence;metaheuristic;operator (computer programming);heuristic;computer science	AI	22.81672073425313	-2.7561364502242283	34387
ca72059331c164927fcf7688d7c21e4287575c34	combined planning and scheduling in a divergent production system with co-production: a case study in the lumber industry	production system;wood drying and finishing;planning and scheduling;production management;production process;anytime algorithm;mixed integer program;integrated planning and scheduling;mathematical programming;constraint programming;process planning;lumber industry	Many research initiatives carried out in production management consider process planning and operations scheduling as two separate and sequential functions. However, in certain contexts, the two functions must be better integrated. This is the case in divergent production systems with co-production (i.e. production of different products at the same time from a single product input) when alternative production processes are available. This paper studies such a context and focuses on the case of drying and finishing operations in a softwood lumber facility. The situation is addressed using a single model that simultaneously performs process planning and scheduling. We evaluate two alternative formulations. The first one is based on mixed integer programming (MIP) and the second on constraint programming (CP). We also propose a search procedure to improve the performance of the CP approach. Both approaches are compared with respect to their capacity to generate good solutions in short computation time.	automated planning and scheduling;computation;constraint programming;integer programming;linear programming;production system (computer science);scheduling (computing);time complexity	Jonathan Gaudreault;Jean-Marc Frayret;Alain Rousseau;Sophie D'Amours	2011	Computers & OR	10.1016/j.cor.2010.10.013	mathematical optimization;constraint programming;simulation;computer science;scheduling;production system	AI	13.644070010496211	3.9171293032534016	34405
377726f0c7eeb1a90adacc758f2e6863722dc48e	technical solutions of tsinghuaeolus for robotic soccer	source code;robot soccer;evaluation framework	TsinghuAeolus is the champion team for the latest two RoboCup simulation league competitions. While our binary and nearly full source code for RoboCup 2001 had been publicly available for the entire year, we won the champion again in Fukuka, with more obvious advantage. This paper describes the key innovations that bring this improvement. They include an advice-taking mechanism which aims to improve agents’ adaptability, a compact and effective option scoring policy which is crucial in the option-evaluation framework, and thorough analysis of interception problem which leads to more intelligent interception skill. Although not strongly interrelated, these innovations come together to form a set of solutions for problems across different levels.	adversary (cryptography);bitwise operation;entity–relationship model;intelligent control;lecture notes in computer science;online and offline;parsing;q-learning;requirement;robot;simulation;springer (tank);symbolically isolated linguistically variable intelligence algorithms;yao graph	Jinyi Yao;Lao Ni;Fan Yang;Yunpeng Cai;Zengqi Sun	2003		10.1007/978-3-540-25940-4_18	simulation;computer science;artificial intelligence;source code	AI	18.385437819910777	-17.52435972933255	34462
337ee35e1fc24b053e004b52cb14770a7196ac30	endogenous groups and dynamic selection in mechanism design	health research;uk clinical guidelines;biological patents;numerical technique;europe pubmed central;citation search;risk sharing;uk phd theses thesis;organizational form;life sciences;mechanism design;uk research reports;medical journals;europe pmc;biomedical research;transition region;steady state;bioinformatics	We create a dynamic theory of endogenous risk sharing groups, with good internal information, and their coexistence with relative performance, individualistic regimes, which are informationally more opaque. Inequality and organizational form are determined simultaneously. Numerical techniques and succinct re-formulations of mechanism design problems with suitable choice of promised utilities allow the computation of a stochastic steady state and its transitions. Regions of low inequality and moderate to high wealth (utility promises) produce the relative performance regime, while regions of high inequality and low wealth produce the risk sharing group regime. If there is a cost to prevent coalitions, risk sharing groups emerge at high wealth levels also. Transitions from the relative performance regime to the group regime tend to occur when rewards to observed outputs exacerbate inequality, while transitions from the group regime to the relative performance regime tend to come with a decrease in utility promises. Some regions of inequality and wealth deliver long term persistence of organization form and inequality, while other regions deliver high levels of volatility. JEL Classification Numbers: D23,D71,D85,O17.	coexist (image);computation;interaction technique;low back pain;persistence (computer science);rewards;social inequality;steady state;volatility	Gabriel A. Madeira;Robert M. Townsend	2008	Journal of economic theory	10.1016/j.jet.2007.03.007	mechanism design;actuarial science;economics;public economics;operations management;microeconomics;mathematical economics;steady state	Metrics	-3.900687051545751	-4.893973490295134	34508
9605e37e194cacb9c1b3401412abde8a77631384	the comparison of two procurement strategies in the presence of supply disruption	game theory;procurement;期刊论文;supply disruption;supply chain	Two types of procurement strategies under competition are investigated.The joint pricing and ordering decisions of duopoly are analyzed.The reliability threshold is derived to characterize the equilibrium outcomes.Solution procedures are proposed to determine the equilibrium outcomes. The emergency procurement strategy and the optimal allocation procurement strategy are widely used for managing supply disruption risks. In this paper, we investigate two competing manufacturers using these procurement strategies in the presence of supply disruption risks. The joint pricing and ordering decisions of both manufacturers are analyzed using the game theoretic framework. The structural property of the manufacturer with the optimal allocation procurement strategy is characterized by the reliability threshold value, which further determines the equilibrium outcomes for both manufacturers. We find the reliability threshold is a generalization of the supplier's reliability level, which involves all the critical factors that influence manufacturers' procurement decisions under a competitive scenario. The optimal allocation procurement strategy for manufacturer profit maximization in a non-competitive scenario does not necessarily generate competitive advantage in a competitive scenario; under a wide range of parameters, the emergency procurement strategy can produce larger profit for the manufacturer than the optimal allocation procurement strategy when all suppliers are unreliable. The effects of reliability level and costs on procurement decisions are examined using comparative studies and numerical computations.	denial-of-service attack;procurement	Bo He;He Huang;Kaifu Yuan	2015	Computers & Industrial Engineering	10.1016/j.cie.2015.03.019	game theory;procurement;marketing;operations management;microeconomics;supply chain;commerce	HCI	-2.250309580682462	-6.0160267722218705	34512
fa6893b2132e39e16a203d84222e6fc2a91fb0f2	echo state network and particle swarm optimization for prognostics of a complex system		To ensure complex systems reliability and to extent their life cycle, it is crucial to properly and timely prognose faults. In this context, this paper describes a new intelligent approach to estimate the remaining useful life in complex systems. This approach is based on the combination of several intelligent techniques. This approach is based on Echo State Network (ESN) and Particle Swarm Optimization (PSO) technique to set the ESN with optimal parameters. The input of this model are the measurements of signals correlated to the component degradation state, whereas the model output is the component RUL. To validate the feasibility of the proposed approach, real life fault historical data from turbofan engines system were analyzed and used to obtain the optimal prediction of RUL.	complex system;complex systems;echo state network;elegant degradation;particle swarm optimization;real life;reliability engineering;simulation;soft computing	Safa Ben Salah;Imtiez Fliss;Moncef Tagina	2017	2017 IEEE/ACS 14th International Conference on Computer Systems and Applications (AICCSA)	10.1109/AICCSA.2017.210	computer science;real-time computing;complex system;complex system;prognostics;echo state network;particle swarm optimization	Robotics	14.32385314352907	-15.216767017722155	34566
a49697290f3a86510f42f4f133744200261a8915	ensemble of optimized echo state networks for remaining useful life prediction		The use of Echo State Networks (ESNs) for the prediction of the Remaining Useful Life (RUL) of industrial components, i.e. the time left before the equipment will stop fulfilling its functions, is attractive because of their capability of handling the system dynamic behavior, the measurement noise, and the stochasticity of the degradation process. In particular, in this paper we originally resort to an ensemble of ESNs, for enhancing the performances of individual ESNs and providing also an estimation of the uncertainty affecting the RUL prediction. The main methodological novelties in our use of ESNs for RUL prediction are: i) the use of the individual ESN memory capacity within the dynamic procedure for aggregating of the ESNs outcomes; ii) the use of an additional ESN for estimating the RUL uncertainty, within the Mean Variance Estimation (MVE) approach. With these novelties, the developed approach outperforms a static ensemble and a standard MVE approach for uncertainty estimation in tests performed on a synthetic and two industrial datasets.	echo state network;elegant degradation;performance;synthetic intelligence;system dynamics	Marco Rigamonti;Piero Baraldi;Enrico Zio;Indranil Roychoudhury;Kai Goebel;Scott Poll	2018	Neurocomputing	10.1016/j.neucom.2017.11.062	artificial intelligence;machine learning;differential evolution;prediction interval;pattern recognition;mathematics;recurrent neural network	Robotics	14.667679842646308	-15.523176671487466	34580
4868b3a57db7dd882597be78fe90fe3027fd4fb2	an integrated model to evaluate the performance of solar pv firms		The use of renewable energy resources is being stressed in the 21st century due to the depletion of fossil fuels and the increasing consciousness about environmental degradation. Renewable energies, such as wind energy, fire energy, hydropower energy, geothermal energy, solar energy, biomass energy, ocean power and natural gas, are treated as alternative means of meeting global energy demands. After Japan's nuclear plant disaster in March 2011, people are aware that a good renewable energy resource not only needs to produce zero or little air pollutants and greenhouse gases, it also needs to have a high safety standard to prevent the chances of hazards from happening. Solar energy is one of the most promising renewable energy sources with an infinite sunlight resource and environmental sustainability. However, photovoltaic products currently still require a high production cost with low conversion efficiency. In addition, the solar industry has a rather versatile market cycle in response to economic conditions. Therefore, solar firms need to strengthen their competitiveness in order to survive and to acquire decent profits in the market. This research proposes a performance evaluation model by integrating data envelopment analysis (DEA) and analytic hierarchy process (AHP) to assess the business performance of the solar firms. From the analysis, the firms can understand their current positions in the market and to know how they can improve their business. A case study is performed on the crystalline silicon solar firms in Taiwan.	analytical hierarchy;competitive analysis (online algorithm);consciousness;data envelopment analysis;depletion region;elegant degradation;fossil;performance evaluation	Chun-Yu Lin;Amy Hsin-I Lee;He-Yau Kang;Wen Hsin Lee	2012			automotive engineering;photovoltaic system;business	AI	7.311969463475962	-7.073132273025359	34598
732c944e20f2b12d094b6cbca25a19f71afb8c88	a combined simulation optimization framework to improve operations in parcel logistics		Operators of parcel transshipment terminals face the challenge of sorting and transferring a large number of parcels efficiently. In order to meet customer expectations, short sorting intervals are required. In this paper we present a technical framework that combines metaheuristics with discrete-event simulation (DES) to provide robust solutions for problems at the operational level of parcel transshipment terminals. First, the framework applies metaheuristics such as local search solve the problem of scheduling incoming trucks as well as allocations at the loading gates taking into account the characteristics of the internal sorting system. Next, detailed conclusions on the real system behavior are drawn by testing the solutions in multiple DES experiments with stochastic processing times. The paper presents first results using our framework and investigates the procedure with regard to the solutions quality and runtime requirements.	assertion (software development);experiment;fifo (computing and electronics);heuristic (computer science);integer programming;internal sort;local search (optimization);logistics;mathematical optimization;metaheuristic;microsoft outlook for mac;requirement;scheduling (computing);search algorithm;simulation;sorting;stochastic process	Moritz Pöting;Jonas Rau;Uwe Clausen;Christin Schumacher	2017	2017 Winter Simulation Conference (WSC)	10.1109/WSC.2017.8248063	robustness (computer science);mathematical optimization;operator (computer programming);simulation;scheduling (computing);computer science;local search (optimization);metaheuristic;transshipment;linear programming;sorting	Robotics	13.849993992751436	2.1840016547865084	34614
dbac7280264746ae48c500fdc261896e0a59a70d	a novel aco algorithm with adaptive parameter	np hard problem;ant colony system;experience base	ACO has been proved to be one of the best performing algorithms for NP-hard problems as TSP. Many strategies for ACO have been studied, but little theoretical work has been done on ACO’s parameters α and β , which control the relative weight of pheromone trail and heuristic value. This paper describes the importance and functioning of α and β , and draws a conclusion that a fixed β may not enable ACO to use both heuristic and pheromone information for solution when 1 α = . Later, following the analysis, an adaptive β strategy is designed for improvement. Finally, a new ACO called adaptive weight ant colony system (AWACS) with the adaptive β and 1 α = is introduced, and proved to be more effective and steady than traditional ACS through the experiment based on TSPLIB test.	algorithm;ant colony;heuristic;np-hardness	Han Huang;Xiaowei Yang;Zhifeng Hao;Ruichu Cai	2006		10.1007/11816102_2	computer science;artificial intelligence;np-hard;operations research	AI	22.74642614103796	-0.8765978918684333	34616
2d9d66790987bee5991dba51e401dbc18730ee3e	a goal programming approach to deriving interval weights in analytic form from interval fuzzy preference relations based on multiplicative consistency		Abstract This paper focuses on how to find an analytic solution of optimal interval weights from consistent interval fuzzy preference relations (IFPRs) and obtain approximate-solution-based interval weights in analytic form from inconsistent IFPRs. The paper first analyzes the popularly used interval weight additive normalization model and illustrates its drawbacks on the existence and uniqueness for characterizing ]0, 1[-valued interval weights obtained from IFPRs. By examining equivalency of ]0, 1[-valued interval weight vectors, a novel framework of multiplicatively normalized interval fuzzy weights (MNIFWs) is then proposed and used to define multiplicatively consistent IFPRs. The paper presents significant properties for multiplicatively consistent IFPRs and their associated MNIFWs. These properties are subsequently used to establish two goal programming (GP) models for obtaining optimal MNIFWs from consistent IFPRs. By the Lagrangian multiplier method, analytic solutions of the two GP models are found for consistent IFPRs. The paper further devises a two-step procedure for deriving approximate-solution-based MNIFWs in analytic form from inconsistent IFPRs. Two visualized computation formulas are developed to determine the left and right bounds of approximate-solution-based MNIFWs of any IFPR. The paper shows that this approximate solution is an optimal solution if an IFPR is multiplicatively consistent. Three numerical examples including three IFPRs and comparative analyses are offered to demonstrate rationality and validity of the developed model.	goal programming	Zhou-Jing Wang	2018	Inf. Sci.	10.1016/j.ins.2018.06.006	mathematics;normalization model;fuzzy logic;discrete mathematics;normalization (statistics);lagrange multiplier;goal programming;multiplicative function;left and right	AI	-2.8903161933421377	-20.187491248751357	34650
dd786d7d3a57d83aa0d319ca5bb3126c463e0fd7	prediction of stress intensity factors in pavement cracking with neural networks based on semi-analytical fea	semi analytical fea;neural networks;stress intensity factor;pavement cracking;prediction	Computation of the stress intensity factors (SIFs) at the crack tip is the basis for pavement crack propagation analysis. Due to the three-dimensional (3-D) nature of cracked pavements and traffic loading, two-dimensional (2-D) finite element analysis (FEA) may be too simple to precisely predict SIFs, and the best choice for calculating the SIFs seems to be 3-D FEA programs. However, the 3-D FEA solutions are often computationally heavy. We had previously developed a semi-analytical FEA with multi-variable regression approach to fill this gap, but its accuracy still needs to be improved. To address this problem, a neural network approach based on semi-analytical FEA is presented in this paper: firstly, a SIFs database was generated through analyzing varieties of pavement structures using elastic semi-analytical FEA program; secondly, from the results in the database, neural network (NN) based SIF equations were developed for practical applications. The determination coefficients (R^2) of all the developed NN models were greater than 0.99 and mean square error (MSE) values were less than 1e-4. The comparisons between the prediction results from NN models and multivariable regression models also showed the advantage of NN over multivariable regression on the prediction accuracy. This proposed NN SA-FEA SIF prediction approach has been developed as a pavement crack propagation analysis tool.	artificial neural network;federal enterprise architecture;password cracking;semiconductor industry	Zhenhua Wu;Sheng Hu;Fujie Zhou	2014	Expert Syst. Appl.	10.1016/j.eswa.2013.07.063	prediction;computer science;machine learning;stress intensity factor;artificial neural network;statistics	ML	12.409475525207647	-20.150889669495278	34673
553c1e5f97838f248abdc4f1377177ec86ee9be7	exploiting multi-step sample trajectories for approximate value iteration	robotics;learning machines	Approximate value iteration methods for reinforcement learning (RL) generalize experience from limited samples across large stateaction spaces. The function approximators used in such methods typically introduce errors in value estimation which can harm the quality of the learned value functions. We present a new batch-mode, off-policy, approximate value iteration algorithm called Trajectory Fitted Q-Iteration (TFQI). This approach uses the sequential relationship between samples within a trajectory, a set of samples gathered sequentially from the problem domain, to lessen the adverse influence of approximation errors while deriving long-term value. We provide a detailed description of the TFQI approach and an empirical study that analyzes the impact of our method on two well-known RL benchmarks. Our experiments demonstrate this approach has significant benefits including: better learned policy performance, improved convergence, and some decreased sensitivity to the choice of function approximation.	approximation algorithm;batch processing;bellman equation;experiment;generalization error;ibm notes;iterative method;landweber iteration;markov decision process;model selection;problem domain;reinforcement learning;software propagation	Robert William Wright;Steven Loscalzo;Philip Dexter;Lei Yu	2013		10.1007/978-3-642-40988-2_8	econometrics;mathematical optimization;computer science;artificial intelligence;machine learning;data mining;mathematics;statistics	ML	23.083801082640576	-19.603098208720766	34759
cc949d93241ca0b09f37b71d6b3847689bde3f65	small and medium sized manufacturer performance on third party b2b electronic marketplaces: the role of enabling and it capabilities	electronic marketplaces;business to business;performance;resource based view;online manufacturer	a r t i c l e i n f o This research investigates the determinants of the performance of small and medium sized manufacturers on business-to-business electronic marketplaces (B2B EMs). Based on the resource-based view, the framework proposed suggests that a manufacturing firm's performance on a B2B EM is determined by EM enabling capabilities, namely the online marketing capability, flexible manufacturing capability and content management capability. Further, the framework posits that these EM enabling capabilities are in turn determined by the firm's IT capability. Data from 358 online manufacturers participating in a B2B EM is collected and analyzed. The results confirm our hypotheses that the online marketing capability, flexible manufacturing capability and content management capability fully mediate the impact of the IT capability on the firm's online performance. Furthermore, the online marketing capability is found to be a stronger factor in influencing the manufacturer's online performance than the others. With recent developments in electronic commerce, there has been a significant increase in the number of small and medium sized manufacturers using third party business-to-business (B2B) electronic marketplaces (EMs), such as Alibaba.com, to sell to their customers directly. Online EMs allow manufacturers to bypass middlemen and possibly compensate for a decline in sales through traditional channels [1]. B2B customers often prefer to purchase directly from manufacturers as well because manufacturers can offer lower prices and more stable supply compared to B2B intermediaries. Recognizing the unique value propositions that can be offered by manufacturers, many B2B EMs, such as DHgate.com [2], consider manufacturers as their primary clients. At the same time, a small or medium sized manufacturer can be in a disadvantageous position when it comes to managing its business on-line. Selling on B2B EMs requires it to build organizational capabilities different from those tailored to sell offline [3,4]. In the traditional market, such manufacturers mainly acquire their customers through personal relationships [5] and rely heavily on a small number of major clients, who tend to place large orders [6]. However, selling on the Inter-net forces manufacturers to engage with a larger number of customers who order in small batches. Besides, unlike using traditional customer relationship practices, manufacturers should be able to use various Internet-based media to market their products to customers in a larger geographic area. They also need to adjust their manufacturing practices to accommodate for varying order volumes. In other words, direct selling on EMs requires manufacturers to …	content management system;e-commerce;internet;online advertising;online and offline	Shan Wang;Hasan Cavusoglu	2015	Decision Support Systems	10.1016/j.dss.2015.08.006	performance;marketing;operations management;commerce	AI	-2.2698064737145485	-8.305577010355954	34771
f16e4d190a8db9cb681c971d8aaacbcd99cff776	"""erratum to """"evolving neural network using real coded genetic algorithm for permeability estimation of the reservoir"""" [expert systems with applications 38 (8) (2011) 9862-9866]"""	neural network;genetic algorithm;expert systems;permeability estimation	neural network;genetic algorithm;expert systems;permeability estimation	artificial neural network;emoticon;expert system;genetic algorithm	Rasoul Irani;Reza Nasimi	2013	Expert Syst. Appl.	10.1016/j.eswa.2013.02.024	artificial intelligence;machine learning	ML	12.053340257789301	-23.167868291904032	34783
4c79d8071611f9b183d77f07b17ebe94730b972c	berth allocation in an ore terminal with demurrage, despatch and maintenance	brazilian port;berth allocation problem;adaptive large neighborhood search;solid bulk	A berth allocation problem encountered in a Brazilian ore terminal is presented.Maintenances and extra fees (demurrage) or rewards (despatch) are considered.A mixed integer linear programming model and real instances are proposed.An adaptive large neighborhood search (ALNS) heuristic is developed.Computational experiments indicate that ALNS consistently yields good solutions. This paper presents a Berth Allocation Problem (BAP) encountered in an ore terminal located in a Brazilian port. The BAP consists of assigning ships to berthing positions along a quay in a port. In the Brazilian port considered here, maintenance activities have to be performed at the berths and extra fees (demurrage) or rewards (despatch) for the port administrator are incurred. We propose a mixed integer linear programming model for the problem and a set of instances based on real data in order to validate the model. An adaptive large neighborhood search (ALNS) heuristic is applied to solve the instances and computational experiments are performed. The results indicate that the ALNS heuristic yields good solutions on all instances.		Glaydston Mattos Ribeiro;Geraldo R. Mauri;Saulo de Castro Beluco;Luiz Antonio Nogueira Lorena;Gilbert Laporte	2016	Computers & Industrial Engineering	10.1016/j.cie.2016.03.005	mathematical optimization;engineering;operations management;operations research	DB	14.520159596210181	1.9885425709097038	34800
ed8ba3a10dec544018e899277bd86b37aaee2627	model learning for multistep backward prediction in dyna- ${q}$  learning		"""A model-based reinforcement learning (RL) method which interplays direct and indirect learning to update <inline-formula> <tex-math notation=""""LaTeX"""">${Q}$ </tex-math></inline-formula> functions is proposed. The environment is approximated by a virtual model that can predict the transition to the next state and the reward of the domain. This virtual model is used to train <inline-formula> <tex-math notation=""""LaTeX"""">${Q}$ </tex-math></inline-formula> functions to accelerate policy learning. Lookup table methods are usually used to establish such environmental models, but these methods need to collect tremendous amounts of experiences to enumerate responses of the environment. In this paper, a stochastic model learning method based on tree structures is presented. To model the transition probability, an online clustering method is applied to equip the model learning method with the abilities to evaluate the transition probability. By the virtual model, the RL method produces simulated experience in the stage of indirect learning. Since simulated transitions and backups are more usefully focused by working backward from the state-action, the pair estimated <inline-formula> <tex-math notation=""""LaTeX"""">${Q}$ </tex-math></inline-formula> value of which changes significantly, the useful one-step backups are actions that lead directly into the one state whose value has already obviously been changed. This, however, may induce a false positive; that is, a backup state may be an invalid state, such as an absorbing or terminal state, especially in cases where the changes of <inline-formula> <tex-math notation=""""LaTeX"""">${Q}$ </tex-math></inline-formula> values at the planning stage are still needed to put back for ranking even though they are based on a simulated experience and are possibly erroneous. It is obvious that when the agent is attracted to generate simulated experience around the area of these absorbing states, the learning efficiency is deteriorated. This paper proposes three detecting methods to solve this problem. Moreover, the policy learning can speed up. The effectiveness and generality of our method is further demonstrated in three numerical simulations. The simulation results demonstrate that the training rate of our method is obviously improved."""	3d modeling;approximation algorithm;backup;cluster analysis;computer simulation;dspace;enumerated type;iteration;lookup table;markov chain;numerical analysis;q-learning;reinforcement learning;sampling (signal processing);sensor;state (computer science);tree (data structure)	Kao-Shing Hwang;Wei-Cheng Jiang;Yu-Jen Chen;Iris Hwang	2018	IEEE Transactions on Systems, Man, and Cybernetics: Systems	10.1109/TSMC.2017.2671848		ML	20.36936847689	-19.344237460552375	34877
bb9ab10d7a853d766cea01e9bb2aa7643f97ce81	a revenue management approach for managing operating room capacity	financial management;health care;scheduling;surgery;belobaba emsrb algorithm;elective surgeries;multitier reimbursement system;operating room capacity management;patient scheduling;revenue management approach	The advanced scheduling of patients for elective surgeries is challenging when the operating room capacity usage by these procedures is uncertain. We study the application of some revenue management concepts and techniques to operating rooms for several surgical procedures performed in a multi-tier reimbursement system. Our approach focuses on booking requests for elective procedures, under the assumption that each request uses a random amount of time. We create and use a modified version of Belobaba's well-known EMSRb algorithm (Belobaba 1989) to decide on near-optimal protection levels for various classes of patients. Under the random resource utilization assumption, we decide, for each planning horizon, how much time to reserve for satisfying the demand coming from each class of patients, based on the type of surgical procedure requested and the patient's reimbursement level.	algorithm;multitier architecture;randomness;scheduling (computing)	Alia Stanciu;Luis G. Vargas;Jerrold H. May	2010	Proceedings of the 2010 Winter Simulation Conference		simulation;actuarial science;insurance;computer science;resource management;mathematical model;scheduling;health care;satisfiability	AI	12.446493450947749	-1.3031404086964877	34942
f40021078106ee11904c91a9b27097c3f796ecca	meta-learning with neural networks and landmarking for forecasting model selection an empirical evaluation of different feature sets applied to industry data	time series analysis predictive models forecasting computational modeling neural networks data models training;time series forecasting models meta learning landmarking model selection forecasting industry data artificial neural networks predictive accuracy neural network meta learner statistical forecasting models statistical tests time series meta features error based feature sets;industry data meta learning time series forecasting metafeatures;time series forecasting theory learning artificial intelligence neural nets statistical testing	Although artificial neural networks are occasionally used in forecasting future sales for manufacturing in industry, the majority of algorithms applied today are univariate statistical time series methods for level, seasonal, trend or trend-seasonal patterns. With different statistical methods created for different time series patterns, large scale applications on 10,000s of times series require automatic method selection, often done manually by human experts based on various time series characteristics, or automatically using error metrics of past performance. However, the task of selecting adequate forecasting methods can also be viewed as a supervised learning problem. For instance, a neural network can be trained as a meta-learner relating characteristic time series features to the ex post accuracy of forecasting methods for each time series. Past research has proposed different sets of time series features for meta-learning including simple statistical or information-theoretic as well as model-based features, but have neglected the use of past forecast errors. This paper studies the predictive accuracy of using different feature sets for a neural network meta-learner selecting between four statistical forecasting models, introducing error-based features (landmarkers) and statistical tests as time series meta-features. A large-scale empirical study on NN3 industry data shows promising results of including error-based feature sets in meta-learning for selecting time series forecasting models.	aggregate data;algorithm;artificial neural network;benchmark (computing);feature selection;information theory;meta learning (computer science);model selection;naivety;smoothing;supervised learning;time complexity;time series	Mirko Kuck;Sven F. Crone;Michael Freitag	2016	2016 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2016.7727376	computer science;machine learning;pattern recognition;data mining	ML	6.340412972281638	-19.543940393871576	34976
4f1bd1d93361d8dee8aac4b24ca4df5ea46ea19d	neural controller for business yield management	gestion entreprise;modele entreprise;high dimensionality;firm management;operations research;modelo empresa;back propagation neural network;yield;business model;neural control;recherche operationnelle;administracion empresa;rendimiento;reseau neuronal;curve fitting;inventory control;investigacion operacional;red neuronal;rendement;analytical model;historical data;neural network;yield management	In the business and operational research field there is a class of perishable inventory control problems called ‘yield management’. Examples are floating pricing strategies in air ticket reservation and hotel room booking. Due to the complex nature of yield management, there are few analytical models available for practical application. This paper presents a neural network approach to solving yield management problems. Using modified back propagation neural networks, a threshold band in the high-dimensional yield management space is generated based on historical data and/or management expertise. When the actual inventory level is outside the threshold band, prices should be adjusted to lead the inventory level back to the threshold band. The interval of the threshold band indicates the stability of the business system.	artificial neural network;backpropagation;inventory control;operations research;software propagation	Shouhong Wang	1996	Neural Computing & Applications	10.1007/BF01414879	inventory control;business model;yield;yield management;simulation;operations research;artificial neural network;curve fitting	ML	5.271939971421131	-11.361465988157649	35012
df940428b8dab60abb76936bf14e7b8b325e69dc	a novel hybridization of artificial neural networks and arima models for forecasting resource consumption in an iis web server	software reliability autoregressive moving average processes internet neural nets;predictive models memory management artificial neural networks aging data models software autoregressive processes;arima software aging hybrid model artificial neural network;hybrid model;autoregressive integrated moving average model hybridization software aging long running software application software rejuvenation software system hybrid arima resource consumption forecasting iis web server running commercial server data linear component artificial neural network model;arima;artificial neural network;software aging	Software aging has been observed in a long running software application. A technique named rejuvenation is proposed to counteract this problem. The key to the aging and rejuvenation problem is how to analyze/forecast the resource consumption of software system. In this paper, we propose a methodology of hybrid ARIMA and artificial neural networks to forecast resource consumption in an IIS web server which is a running commercial server and subjected to software aging. The proposed hybrid method consists of two steps. In the first step, an ARIMA model is used to analyze the linear component of the data. In the second step, an artificial neural network model is developed to model the residuals from ARIMA model. The results show that the proposed hybrid model can be a good trade-off to forecast resource consumption.	artificial neural network;autoregressive integrated moving average;internet information services;machine learning;mathematical optimization;network model;neural networks;nonlinear system;server (computing);software aging;software system;web server	Yongquan Yan;Ping Guo;Lifeng Liu	2014	2014 IEEE International Symposium on Software Reliability Engineering Workshops	10.1109/ISSREW.2014.27	autoregressive integrated moving average;simulation;computer science;engineering;artificial intelligence;data mining;software aging;artificial neural network	Embedded	8.223039513947207	-18.92140161171109	35029
c44fbf480bd0147da6d4980207ad723a488f5668	subpopulation diversity based accepting immigrant in distributed evolutionary algorithms	algorithm analysis;parallel evolutionary algorithm;current diversity;subpopulation diversity;concrete problem;proposed scheme;evolutionary algorithm;target subpopulation;better solution;evolutionary algorithms;salesman problem;parallel algorithms;evolutionary computation	As a popular type of parallel evolutionary algorithms, distributed evolutionary algorithms (DEAs) are widely used in a variety of fields. To get better solutions of concrete problems, a scheme of subpopulation diversity based accepting immigrant in DEAs is proposed in this paper. In migration with this scheme, an immigrant will be put into its target subpopulation only if its current diversity is lower than a threshold value. Algorithm analysis shows that the extra cost of time for this scheme is acceptable in many DEAs. Experiments are conducted on instances of the Traveling Salesman Problem from the TSPLIB. Results show that the DEA based on the proposed scheme can get better solutions than the one without it.	analysis of algorithms;evolutionary algorithm;travelling salesman problem	Chengjun Li;Jian Wang;Xianbin Yan;Guangdao Hu	2013	2013 International Conference on Parallel and Distributed Systems	10.1109/ICPADS.2013.66	embedded system;parallel computing;computer science;theoretical computer science;operating system;distributed computing;programming language;algorithm;field-programmable gate array;evolutionary computation	EDA	24.502045720963206	-1.0271406844112436	35042
ca5a4a825add2c1fb9f76faf46181db9b787b2e3	a population heuristic method used for resolving aircraft landing problem (alp).	heuristic method	A lot of research has been done concerning the airline scheduling problem (ALP) and several of the heuristic methods used show good results. Three methods for resolving ALP problem are presented with their advantages and disadvantages. And we present a new method combining between three methods which gives us best results.	heuristic;newton's method;scheduling (computing)	Adnan Yassine;Said Bourazza	2006			mathematical optimization;simulation;engineering;operations research	NLP	20.557634062933495	2.5989023084529603	35097
197e3c919c04c8f8122b3264f1b6cc08ae03e7ae	customer-specific transaction risk management in e-commerce	e commerce;risk management;payment;customer value;risk premium;risk adjustment;customer risk;online retailer	Increasing potential for turnover in e-commerce is inextricably linked with an increase in risk. Online retailers (e-tailers), aiming for a company-wide value orientation should manage this risk. However, current approaches to risk management either use average retail prices elevated by an overall risk premium or restrict the payment methods offered to customers. Thus, they neglect customer-specific value and risk attributes and leave turnover potentials unconsidered. To close this gap, an innovative valuation model is proposed in this contribution that integrates customer-specific risk and potential turnover. The approach presented evaluates different payment methods using their risk-turnover characteristic, provides a risk-adjusted decision basis for selecting payment methods and allows e-tailers to derive automated risk management decisions per customer and transaction without reducing turnover potential.	e-commerce payment system;online shopping;risk management;value (ethics)	Markus Ruch;Stefan Sackmann	2009		10.1007/978-3-642-03132-8_6	liquidity risk;financial risk;finance;business;customer retention;commerce;financial risk management	AI	-1.1241683423618267	-7.338812310249064	35109
e5331279178211250737641874abf04be311a2b5	combining phm information and system architecture to support aircraft maintenance planning	aircraft electrical system phm information prognostics and health management aircraft maintenance planning preventive maintenance predictive maintenance phm technology rul estimation remaining useful life estimation decision support methodology system level rul;reliability aerospace industry aircraft condition monitoring maintenance engineering planning;reliability;maintenance planning prognostics health monitoring system arquitecture;maintenance engineering fault trees prognostics and health management probability aircraft monitoring inspection;maintenance engineering;aerospace industry;condition monitoring;planning;aircraft	Aircraft are highly valuable assets and large budgets are spent in preventive and predictive maintenance programs. The application of PHM (Prognostics and Health Management) technologies can be a powerful decision support tool to help maintenance planners. The RUL (Remaining Useful Life) estimations obtained from a PHM system can be used in order to plan in advance for the repair of components before a failure occurs. However, when system architecture is not taken into account, the use of PHM information may lead the operator to rush to replace a component that would not affect immediately the operation of the system under consideration. This paper presents a methodology for decision support in maintenance planning with application in aeronautical systems. The methodology combines system architecture information and RUL estimations for all components comprised in the system under study, allowing the estimation of a RUL value for the whole system. This system level RUL (S-RUL) can be used as support information for identifying the best moment to repair a component. Also, when several components present high degradation levels, the proposed methodology can be used to define a set of components that, when repaired, will bring the whole system to a safe degradation level with lowest cost. A case study is used to illustrate the application of the methodology in a simplified aircraft electrical system.	decision support system;elegant degradation;systems architecture	Felipe Augusto Sviaghin Ferri;Leonardo Ramos Rodrigues;João Paulo Pordeus Gomes;Ivo Paixao de Medeiros;Roberto Kawakami Harrop Galvão;Cairo L. Nascimento	2013	2013 IEEE International Systems Conference (SysCon)	10.1109/SysCon.2013.6549859	reliability engineering;systems engineering;engineering;operations management;predictive maintenance;prognostics	Robotics	12.221224290065235	-12.508861758840418	35116
74c3edf4072d185829b7d9c135cb4dc481ed6479	dynamic churn prediction framework with more effective use of rare event data: the case of private banking	training data generation;dynamic churn prediction;customer relationship management;customer retention;data mining;sampling;private banking;rare event	Customer churn prediction literature has been limited to modeling churn in the next (feasible) time period. On the other hand, lead time specific churn predictions can help businesses to allocate retention efforts across time, as well as customers, and identify early triggers and indicators of customer churn. We propose a dynamic churn prediction framework for generating training data from customer records, and leverage it for predicting customer churn within multiple horizons using standard classifiers. Further, we empirically evaluate the proposed approach in a case study about private banking customers in a European bank. The proposed framework includes customer observations from different time periods, and thus addresses the absolute rarity issue that is relevant for the most valuable customer segment of many companies. It also increases the sampling density in the training data and allows the models to generalize across behaviors in different time periods while incorporating the impact of the environmental drivers. As a result, this framework significantly increases the prediction accuracy across prediction horizons compared to the standard approach of one observation per customer; even when the standard approach is modified with oversampling to balance the data, or lags of customer behavior features are added as additional predictors. The proposed approach to dynamic churn prediction involves a set of independently trained horizon-specific binary classifiers that use the proposed dataset generation framework. In the absence of predictive dynamic churn models, we had to benchmark survival analysis which is used predominantly as a descriptive tool. The proposed method outperforms survival analysis in terms of predictive accuracy for all lead times, with a much lower variability. Further, unlike Cox regression, it provides horizon specific ranking of customers in terms of churn probability which allows allocation of retention efforts across customers and time periods.	extreme value theory	Özden Gür Ali;Umut Aritürk	2014	Expert Syst. Appl.	10.1016/j.eswa.2014.06.018	sampling;customer relationship management;computer science;data mining;customer retention	ML	5.509186606077668	-14.891597060079848	35154
e54e4f4a999709d26be486694f30a0de00aedaaa	color image magnification with interval-valued fuzzy sets	pixel;magnification algorithm;interval valued fuzzy sets	In this work we present a simple magnification algorithm for color images. It uses Interval-Valued Fuzzy Sets in such a way that every pixel has an interval membership constructed from its original intensity and its neighbourhood's one. Based on that interval membership, a block is created for each pixel, so this is a block expansion method.	color image;fuzzy set	Aranzazu Jurio;Miguel Pagola;Humberto Bustince;Gleb Beliakov	2011		10.1007/978-3-642-24001-0_29	computer vision;discrete mathematics;mathematics;geometry	Vision	0.4318745070068533	-23.486484794621145	35170
985600f126500af883d5ae6f5da74070bb1a8bec	an axiomatic characterization of a fuzzy generalization of rough sets	approximation operators;fuzzy set;rough set theory;fuzzy relation;composition of approximation spaces;triangular norm;satisfiability;binary relation;difference set;rough set	In rough set theory, the lower and upper approximation operators defined by a fixed binary relation satisfy many interesting properties. Several authors have proposed various fuzzy generalizations of rough approximations. In this paper, we introduce the definitions for generalized fuzzy lower and upper approximation operators determined by a residual implication. Then we find the assumptions which permit a given fuzzy set-theoretic operator to represent a upper (or lower) approximation derived from a special fuzzy relation. Different classes of fuzzy rough set algebras are obtained from different types of fuzzy relations. And different sets of axioms of fuzzy set-theoretic operator guarantee the existence of different types of fuzzy relations which produce the same operator. Finally, we study the composition of two approximation spaces. It is proved that the approximation operators in the composition space are just the composition of the approximation operators in the two fuzzy approximation spaces.	rough set	Ju-Sheng Mi;Wen-Xiu Zhang	2004	Inf. Sci.	10.1016/j.ins.2003.08.017	fuzzy logic;combinatorics;mathematical analysis;discrete mathematics;rough set;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;fuzzy subalgebra;fuzzy number;machine learning;fuzzy measure theory;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations	DB	-0.7696502683006101	-23.072143624716606	35180
05dd4243c05db267f6c6690d0410817810800b6a	stochastic synapse reinforcement learning (ssrl)		Over the past several decades, reinforcement learning has emerged as one of the major paradigms in machine learning because it allows an agent to learn through interaction with its environment, so long as there is some mechanism by which the agent can gain evaluative feedback on the effects of its actions. However, there are still many open questions as to the most appropriate reinforcement learning approach, particularly for difficult problems such as those dealing with delayed reward, unknown reward structures, continuous state and/or action spaces, perceptual aliasing, and/or environmental change. Here we present a new learning algorithm for these types of difficult problems. It combines the eligibility traces approach to reinforcement learning with artificial neural networks for generalization and pushes the idea of stochastic computations down to the level of the synapse. A proof-of-concept experiment in the domain of robotics demonstrates that the approach has promise.	algorithm;aliasing;artificial neural network;computation;machine learning;reinforcement learning;robotics;synapse;tracing (software)	Syed Naveed Hussain Shah;Dean F. Hougen	2017	2017 IEEE Symposium Series on Computational Intelligence (SSCI)	10.1109/SSCI.2017.8285425	synapse;computation;artificial neural network;environmental change;perception;machine learning;reinforcement learning;artificial intelligence;computer science;robotics	ML	18.344226728445506	-22.995689572398287	35207
1e346b689d772d51b94738e17a652e26d0d278db	a quasi-concave dea model with an application for bank branch performance evaluation	performance evaluation;sampling error;bank branch evaluation;article letter to editor;quasi concavity;data envelopment analysis;data envelope analysis	In this paper, we present a data envelopment analysis (DEA) model that replaces the standard DEA assumption of concavity for the production frontier with the less restrictive microeconomic assumption of quasi-concavity. The resulting quasi-concave model circumvents the potential specification error that comes with CCR and BCC models, if the production technology is non-concave, e.g. because of increasing marginal productivity. In addition, the model reduces the potential finite sample error associated with FDH models in small and medium-sized samples. A real-life application for the branch network of a large Dutch bank illustrates the operation of the quasi-concave model and its potential for improving performance evaluation.	concave function;performance evaluation	David Dekker;Thierry Post	2001	European Journal of Operational Research	10.1016/S0377-2217(00)00153-3	econometrics;mathematical optimization;computer science;operations management;data envelopment analysis;mathematics;operations research;statistics	HPC	2.728194177163845	-10.155852624438277	35256
4969898de5f46c5ac89accf2834fe09d4b1543a6	rapid prediction of optimum population size in genetic programming using a novel genotype -: fitness correlation	landscape metric;genetic program;population size;control parameters;adaptive algorithm;landscape analysis;landscape;design theory;genotype fitness correlation;real world	The main aim of landscape analysis has been to quantify the 'hardness' of problems. Early steps have been made towards extending this into Genetic Programming. However, few attempts have been made to extend the use of landscape analysis into the prediction of ways to make a problem easy, through the optimal setting of control parameters. This paper introduces a new class of landscape metrics, which we call 'Genotype-Fitness Correlations'. An example of this family of metrics is applied to six real-world regression problems. It is demonstrated that genotype-fitness correlations may be used to estimate optimum population sizes for the six problems. We believe that this application of a landscape metric as guidance in the setting of control parameters is an important step towards the development of an adaptive algorithm that can respond to the perceived landscape in 'real-time', i.e. during the evolutionary search process itself.	adaptive algorithm;genetic programming;real-time locating system	David C. Wedge;Douglas B. Kell	2008		10.1145/1389095.1389346	population size;simulation;fitness landscape;bioinformatics;landscape;designtheory	AI	23.76449190498798	-7.268379982823149	35279
f74501d2d13cb6d37984881945dae9eb47cf217b	cross-bidding in simultaneous online auctions: antecedents and consequences	commerce electronique;comercio electronico;markets;methode empirique;mercado;price discount;metodo empirico;empirical method;online auction;by product;sous produit;cross bidding;subproducto;marche;subasta;bidding;online auctions;enchere;antecedent;simultaneous auctions;electronic trade;empirical research;antecedente	Cross-bidding is a new strategy used in online auctions. The bidder simultaneously monitors several identical auctions, taking advantage of their price differential. We examined the determinants and outcomes of cross-bidding behavior and the contingent factors that shape it. Using empirical data, we demonstrated that cross-bidders can realize significant price discounts compared to non-cross-bidders; the number of experienced bidders in an auction market contributes to more cross-bidding; and this effect is positively moderated by market liquidity of the product being auctioned. 2010 Elsevier B.V. All rights reserved.	contingency (philosophy)	Varol O. Kayhan;James A. McCart;Anol Bhattacherjee	2010	Information & Management	10.1016/j.im.2010.07.001	financial economics;social science;economics;marketing;common value auction;english auction;microeconomics;empirical research;reservation price;commerce	AI	-3.72174029158742	-8.615468156887838	35370
86a06d8778c45925f512dc895acdbe3a632de962	stochastic optimization for a mineral value chain with nonlinear recovery and forward contracts		When a new forward contract is signed between a mining company and a customer to hedge the risk incurred by the uncertainty in commodity market, the mining company needs to re-optimize the plans of the entire value chain to account for the change of risk level. A two-stage stochastic mixed integer nonlinear program is formulated to optimize a mineral value chain in consideration of both geological uncertainty and market uncertainty. A heuristic is developed to deal with the complexity incurred by the throughput- and head-grade-dependent recovery rate in the processing plant. Through a series of numerical tests, we show that the proposed heuristic is effective and efficient. The test results also show that ignoring the dynamic recovery rate will result in loss and severe misestimation in the mineral value chains profitability. Based on the proposed model and heuristic, an application in evaluating and designing a forward contract is demonstrated through a hypothetical case study.	mathematical optimization;nonlinear system;stochastic optimization	Jian Zhang;Roussos G. Dimitrakopoulos	2018	JORS	10.1057/s41274-017-0269-5	stochastic optimization;commodity market;forward contract;operations management;throughput;profitability index;financial economics;heuristic;nonlinear system;economics	AI	2.7976270137940715	-6.512917618794836	35460
1fe41414ca90be3db66f485d8bc3a7fc0ac48a80	short-term traffic flow forecasting: parametric and nonparametric approaches via emotional temporal difference learning		Information signal from real case and natural complex dynamical systems such as traffic flow are usually specified by irregular motions. Chaotic nonlinear dynamics approach is now the most powerful tool for scientists to deal with complexities in real cases, and neural networks and neuro-fuzzy models are widely used for their capabilities in nonlinear modeling of chaotic systems more than the traditional methods. As mentioned, the traffic flow conditions caused the forecasting values of traffic flow to lack robustness and accuracy. In this paper, the traffic flow forecasting is analyzed with emotional concepts and multi-agent systems (MASs) points of view as a new method in this field. The findings enabled the researchers to develop a newly object-oriented method of forecasting traffic flow. Its architecture is based on a temporal difference (TD) Q-learning with a neuro-fuzzy structure, which is the nonparametric approach. The performance of TD Q-learning is improved by emotional learning. The proposed method on the present conditions and the action of the system according to the criteria could forecast traffic signals so that the objectives are reached in minimum time. The ability of presented learning algorithm to prospect gains from future actions and obtain rewards from its past experiences allows emotional TD Q-learning algorithm to improve its decisions for the best possible actions. In addition, to study in a more practical situation, the neuro-fuzzy behaviors could be modeled by MAS. The proposed method (intelligent/nonparametric approach) is compared by parametric approach, autoregressive integrated moving average (ARIMA) method, which is implemented by multi-layer perceptron neural networks and called ARIMANN. Here, the ARIMANN is updated by backpropagation and temporal difference backpropagation for the first time. The simulation results revealed that the studied forecaster could discover the optimal forecasting by means of the Q-learning algorithm. Difficult to handle through parametric and classic methods, the real traffic flow signals used for fitting the algorithms is obtained from a two-lane street I-494 in Minnesota City.	artificial neural network;autoregressive fractionally integrated moving average;autoregressive integrated moving average;autoregressive model;backpropagation;benchmark (computing);chaos theory;cobham's thesis;comparison of command shells;complex dynamics;conjugate gradient method;decision support system;dynamical system;experience;gauss–newton algorithm;gradient descent;layer (electronics);levenberg–marquardt algorithm;lynx (web browser);machine learning;mathematical model;multi-agent system;multilayer perceptron;neuro-fuzzy;newton;nonlinear system;q-learning;rl (complexity);real-time clock;real-time computing;recursion (computer science);reinforcement learning;seasonality;simulation;stationary process;temporal difference learning;test data;time series;white noise	Javad Abdi;Behzad Moshiri;Baher Abdulhai;Ali Khaki-Sedigh	2012	Neural Computing and Applications	10.1007/s00521-012-0977-3	simulation;artificial intelligence;machine learning;statistics	AI	9.515745807236241	-21.3332795663483	35478
6a17b2ba75d8fca0b06fa5329b18694ed985959f	experience selection in deep reinforcement learning for control		Experience replay is a technique that allows off-policy reinforcement-learning methods to reuse past experiences. The stability and speed of convergence of reinforcement learning, as well as the eventual performance of the learned policy, are strongly dependent on the experiences being replayed. Which experiences are replayed depends on two important choices. The first is which and how many experiences to retain in the experience replay buffer. The second choice is how to sample the experiences that are to be replayed from that buffer. We propose new methods for the combined problem of experience retention and experience sampling. We refer to the combination as experience selection. We focus our investigation specifically on the control of physical systems, such as robots, where exploration is costly. To determine which experiences to keep and which to replay, we investigate different proxies for their immediate and long-term utility. These proxies include age, temporal difference error and the strength of the applied exploration noise. Since no currently available method works in all situations, we propose guidelines for using prior knowledge about the characteristics of the control problem at hand to choose the appropriate experience replay strategy.	buffer overflow;experience;rate of convergence;reinforcement learning;robot;sampling (signal processing);temporal difference learning	Tim de Bruin;Jens Kober;Karl Tuyls;Robert Babuska	2018	Journal of Machine Learning Research		machine learning;mathematics;robot;reuse;experience sampling method;deep learning;reinforcement learning;temporal difference learning;artificial intelligence;physical system;robotics	Robotics	21.715092697983756	-19.7417561578958	35503
0ea3a48428fa227f67803bb94029e355f7775514	an approach based on evaluation particle swarm optimization algorithm for 2d irregular cutting stock problem	cutting stock problem;epso;grid approximation	Cutting stock problem is an important problem that arises in a variety of industrial applications. An irregular-shaped nesting approach for two-dimensional cutting stock problem is constructed and Evolution Particle Swarm Optimization Algorithm (EPSO) is utilized to search optimal solution in this research. Furthermore, the proposed approach combines a grid approximation method with Bottom-Left-Fill heuristic to allocate irregular items. We evaluate the proposed approach using 15 revised benchmark problems available from the EURO Special Interest Group on Cutting and Packing. The performance illustrates the effectiveness and efficiency of our approach in solving irregular cutting stock problems.	algorithm;cutting stock problem;particle swarm optimization	Yanxin Xu;Genke Yang;Changchun Pan	2013		10.1007/978-3-642-38703-6_20	mathematical optimization;simulation;cutting stock problem;mathematics	Vision	20.535829772163332	0.8029743871114756	35530
c7717eb0eadaa59358111cebd913edbacc8d79b5	implication operators on the set of ∨-irreducible element in the linguistic truth-valued intuitionistic fuzzy lattice	journal	We construct a kind of linguistic truth-valued intuitionistic fuzzy lattice based on linguistic truth-valued lattice implication algebras to deal with linguistic truth values. We get some properties of implication operators on the set of ∨-irreducible elements. And furthermore the implication operators on the linguistic truth-valued intuitionistic fuzzy lattice are discussed. The proposed system can better express both comparable and incomparable information. Also it can deal with both positive and negative evidences which are represented by linguistic truth values at the same time during the information processing system.		Li Zou;Xin Liu;Zheng Pei;Degen Huang	2013	Int. J. Machine Learning & Cybernetics	10.1007/s13042-012-0100-1	discrete mathematics;mathematics;algorithm	NLP	-2.828732710982146	-22.04189341451787	35552
bcc693cbd728c91fd8d2feebd6c93ef227220b34	a modeling system for simulation of dial-a-ride services	other engineering and technologies not elsewhere specified;level of service;ovrig annan teknik;simulation;paratransit services;fleet management;passenger comfort	We present a modeling system for simulation of dial-a-ride s ervices. It can be used as a tool for understanding and study how different designs, a d different ways to operate a dial-a-ride service, affect the performance and e fficiency of the service. The system simulates the operation of a dynamic dial-a-ride service that operates with multiple fleets of vehicles with different capacities, schedules and depots. It can be used to investigate how the setting of service and cost parameters and the design of the service affect the total cost for the operator a nd level of service for the customer. We describe the different modules in the system an d the possible uses of the system. A short simulation study is performed to exempli fy how it can be used. In this study the effects of including costs for customer dis comfort are evaluated.		Carl H. Häll;Magdalena Högberg;Jan T. Lundgren	2012	Public Transport	10.1007/s12469-012-0052-6	simulation;computer science;engineering;civil engineering;automotive engineering;transport engineering;services computing;level of service	HCI	11.887413781150077	0.186523300309418	35611
5d4ebbaa1ef8feeac885e2869f45c0276c18834f	learning montezuma's revenge from a single demonstration		We propose a new method for learning from a single demonstration to solve hard exploration tasks like the Atari game Montezuma’s Revenge. Instead of imitating human demonstrations, as proposed in other recent works, our approach is to maximize rewards directly. Our agent is trained using off-the-shelf reinforcement learning, but starts every episode by resetting to a state from a demonstration. By starting from such demonstration states, the agent requires much less exploration to learn a game compared to when it starts from the beginning of the game at every episode. We analyze reinforcement learning for tasks with sparse rewards in a simple toy environment, where we show that the run-time of standard RL methods scales exponentially in the number of states between rewards. Our method reduces this to quadratic scaling, opening up many tasks that were previously infeasible. We then apply our method to Montezuma’s Revenge, for which we present a trained agent achieving a high-score of 74,500, better than any previously published result.		Tim Salimans;Richard Chen	2018	CoRR			ML	20.731563544802643	-19.79372024252884	35697
05b9630ba0d5964922c635a298696cd40201d663	linear approximation of karnik-mendel type reduction algorithm	engineering;systems;uncertainty;technology;interval type 2;fuzzy logic controller;science technology;yttrium;set;engineering electrical electronic;defuzzification	Karnik-Mendel (KM) algorithm is the most used and researched type reduction (TR) algorithm in literature. This algorithm is iterative in nature and despite consistent long term effort, no general closed form formula has been found to replace this computationally expensive algorithm. In this research work, we demonstrate that the outcome of KM algorithm can be approximated by simple linear regression techniques. Since most of the applications will have a fixed range of inputs with small scale variations, it is possible to handle those complexities in design phase and build a fuzzy logic system (FLS) with low run time computational burden. This objective can be well served by the application of regression techniques. This work presents an overview of feasibility of regression techniques for design of data-driven type reducers while keeping the uncertainty bound in FLS intact Simulation results demonstrates the approximation error is less than 2%. Thus our work preserve the essence of Karnik-Mendel algorithm and serves the requirement of low computational complexities.	analysis of algorithms;approximation algorithm;approximation error;computation;farthest-first traversal;free library of springfield township;fuzzy logic;genetic algorithm;hungarian algorithm;iterative method;linear approximation;run time (program lifecycle phase);simulation	Syed Moshfeq Salaken;Abbas Khosravi;Saeid Nahavandi;Dongrui Wu	2015	2015 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2015.7337811	set;mathematical optimization;uncertainty;defuzzification;computer science;artificial intelligence;yttrium;machine learning;mathematics;algorithm;statistics;technology	EDA	24.544697627900593	-15.35504267638921	35701
3449ef26816d8327061efe6a574f21d27b3fe8f9	statistical and neural network forecasts of apparel sales	neural network	In this paper, we are investigating both statistical and soft computing (e.g., neural networks) forecasting approaches. Using sales data from 1997 - 1999 to train our model, we forecasted sales for the year 2000. We found an average correlation of 90% between forecast and actual sales using statistical time series analysis, but only 70% correlation for the model based on neural networks. We are now working to convert standard input parameters into fuzzified inputs. We believe that fuzzy rules would help neural networks learn more efficiently and provide better forecasts.	artificial neural network	Les M. Sztandera;Celia Frank;Ashish Garg;Amar Raheja	2003			computer science;machine learning;data mining;artificial neural network	NLP	6.781874082518672	-19.745820505498997	35759
0a89bfe9a3135167c3ea8d78a413617f6cb4a05f	advances in computational intelligence		In the not so distant future, we expect analytic models to become a commodity. We envision having access to a large number of data-driven models, obtained by a combination of crowdsourcing, crowdservicing, cloud-based evolutionary algorithms, outsourcing, in-house development, and legacy models. In this new context, the critical question will be model ensemble selection and fusion, rather than model generation. We address this issue by proposing customized model ensembles on demand, inspired by Lazy Learning. In our approach, referred to as Lazy Meta-Learning, for a given query we find the most relevant models from a DB of models, using their meta-information. After retrieving the relevant models, we select a subset of models with highly uncorrelated errors. With these models we create an ensemble and use their meta-information for dynamic bias compensation and relevance weighting. The output is a weighted interpolation or extrapolation of the outputs of the models ensemble. Furthermore, the confidence interval around the output is reduced as we increase the number of uncorrelated models in the ensemble. We have successfully tested this approach in a power plant management application.	artificial neural network;canonical account;cloud computing;compiler;computation;computational intelligence;crowdsourcing;decibel;decision tree learning;evolutionary algorithm;experiment;extrapolation;feature vector;greedy algorithm;interpolation;lazy evaluation;lazy learning;legacy system;local search (optimization);model selection;outsourcing;relevance;surround sound;tree (data structure)	Josef Kittler;Alfred Kobsa;Moni Naor;Demetri Terzopoulos	2012		10.1007/978-3-642-30687-7		AI	10.341903928039063	-22.8294325215651	35762
57e38d545e52cdc3ed548403f29ffd262a2e4836	optimal closing of a momentum trade		There is an extensive academic literature that documents that stocks which have performed well in the past often continue to perform well over some holding period so called momentum. We study the optimal timing for an asset sale for an agent with a long position in a momentum trade. The asset price is modelled as a geometric Brownian motion with a drift that initially exceeds the discount rate, but with the opposite relation after an unobservable and exponentially distributed time. The problem of optimal selling of the asset is then formulated as an optimal stopping problem under incomplete information. Based on the observations of the asset, the agent wants to detect the unobservable change point as accurately as possible. Using filtering techniques and stochastic analysis, we reduce the problem to a one-dimensional optimal stopping problem, which we solve explicitly. We show also that the optimal boundary at which the investor should liquidate the trade depends monotonically on the model parameters.	brownian motion;closing (morphology);optimal stopping	Erik Ekström;Carl Lindberg	2013	J. Applied Probability	10.1017/S0021900200013425	mathematical optimization;optimal stopping;mathematics;mathematical economics;momentum;statistics	Theory	1.8681801003846572	-3.8704947526612643	35846
c13e4f93802eb087aa0a14a7cf6ba2af48dd0048	life circle cost minimizing modeling for highway alignment base on genetic algorithm	highway design;ga;life circle cost;multi objective related	Highway alignment design is always treated as a complicated multi-objective related question which can be solved by a life circle cost minimization model. Life circle cost minimization model, involves main cost-related factors considered in highway alignment design, is discussed in this paper. A new genetic algorithm with a powerful genetic operator for alignment design is proposed for this model. This algorithm, retaining the evolving stability of those superior individuals, fully enhances the converging speed, solution accuracy and make up the deficiencies in generality of conventional method in coping with prematurity. Experiments show that the new method makes remarkable progress in robustness and effectiveness compared with other methods and possesses significant values in engineering applications.	genetic algorithm;genetic operator;premature convergence;rate of convergence	Jianxin Chen;Mai-Xia Lv	2014	Journal of Multimedia	10.4304/jmm.9.4.514-521	simulation	AI	23.01020017448529	-5.323415394779675	35867
146f773adb153c798bede05b313b2204bdcf404b	a class of extremum problems related to agency models with imperfect monitoring	minimisation;monotone likelihood ratio;optimal solution;minimization;information systems;score function;key words convex order of distributions;probleme extremum;fonction repartition;agent modeling;principal agent problem;fenchel duality;inequality constraint;moral hazard;minimizacion;lagrange multiplier;dual problem;monotonie;dualite fenchel;function space;ordre convexe;funcion distribucion;distribution function;first order;cost minimization;independent and identically distributed;teoria agencia;monotonicity;extremum problem;theorie agence;monotonia;information system;agency theory;convex order;systeme information;imperfect monitoring;problema extremo;sistema informacion	The cost minimization problem in an agency model with imperfect monitoring is considered. Under the ®rst order approach, this can be stated as a convex minimization problem with linear inequality and equality constraints in a generally in®nite dimensional function space. We apply the Fenchel Duality Theorem, and obtain as a dual problem a concave maximization problem of ®nite dimension. In particular, a Lagrange multiplier description of the optimal solution to the cost minimization problem is derived, justifying and extending thus the approach of Kim (1995). By the duality, the dependence of the minimum cost value on the information system used becomes particularly visible. The minimum cost value behaves monotonically w.r.t. the convex ordering of certain distributions induced by the competing information systems. Under the standard inequality constraint, one is led to the distributions of the score functions of the information systems and their convex order relation. It is shown that also for multivariate actions, Blackwell su1⁄2ciency implies the convex order relation of the score function distributions. A further result refers to a multi-agents model recently considered by Budde (1997), when the maximum of n independent and identically distributed (i.i.d.) univariate output variables is focussed. If two univariate information systems have monotone likelihood ratios, then the convex ordering between the two score function distributions implies the weaker convex increasing ordering between the distributions of the same score functions under the maximum distributions.	blackwell (series);computer multitasking;concave function;convex optimization;duality (optimization);expectation–maximization algorithm;exponential utility;feasible region;fenchel's duality theorem;information system;lagrange multiplier;linear inequality;mathematical optimization;maxima and minima;optimization problem;social inequality;support vector machine;time complexity;x image extension;monotone	Jörg Budde;Norbert Gaffke	1999	Math. Meth. of OR	10.1007/s001860050038	convex function;convex analysis;subderivative;principal–agent problem;mathematical optimization;convex optimization;duality;convex combination;linear matrix inequality;calculus;mathematics;mathematical economics;logarithmically convex function;information system;statistics;proper convex function	AI	3.241982746357666	-2.7196150283721514	35923
fd7619feb312d227574cf47446fe96f86843b1a0	on information costs, short sales and the pricing of extendible options, steps and parisian options		This paper provides a simple framework for the valuation of exotic derivatives within shadow costs of incomplete information and short sales. The specific features of the OTC markets with comparison to the organized markets require an additional investment to obtain information about the financial products, to process data, to elaborate models, etc. The shadow cost includes two components. The first component is the product of pure information cost due to imperfect knowledge. The second component represents the additional cost caused by the short-selling constraint. Information costs are linked to Merton’s (J Fianance 42:483–510, 1987) model of capital market equilibrium with incomplete information, CAPMI. This model is extended by Wu et al. (Rev Quant Finance Account 7:119–136, 1996) who propose incomplete-information capital market equilibrium with heterogeneous expectations and short sale restrictions, GCAPM. This model is used in our paper to provide for the first time in the literature analytic solutions for derivatives in the presence of both shadow costs of incomplete information and short sales. Our methodology incorporates shadow costs of incomplete information and short sales in the options and their underlying securities. We provide formulas using the standard Black and Scholes method or the martingale method. Since shadow costs are important in the presence of illiquidity, the formulas are useful for the valuation of OTC derivatives.	extensibility	Mondher Bellalah	2018	Annals OR	10.1007/s10479-015-2050-y	financial economics;actuarial science;economics;shadow price;commerce	Theory	0.23286008136750552	-5.147620112246421	35943
1cdccf3083d2da6264a86dc364d0be793d32d807	a kohonen-like decomposition method for the traveling salesman problem: kniesdecompose	decomposition method;heuristic algorithm;artificial neural network;traveling salesman problem;traveling salesman	In addition to the classicalheuristicalgorithmsof operationsresearchtherehave alsobeenseveral approachesbasedon artificial neuralnetworks which solve the traveling salesmanproblem (TSP).Their efficiency, however, decreases astheproblemsize (numberof cities) increases. An ideato reducethe complexity of a large-scaleTSPinstanceis to decomposeor partition it into smaller subproblems, which areeasierto solve. In this paperwe introduce an all-neuraldecompositionheuristicthat is basedon a recentselforganizingmapcalledKNIES which hasbeensuccessfullyimplementedin solvingboththeEuclideanTSPandtheEuclideanHamiltonianpathproblem.	teuvo kohonen;travelling salesman problem	Necati Aras;I. Kuban Altinel;B. John Oommen	2000				EDA	24.591167049852757	1.4819857669119831	35960
947ef4dbeac35aaf5710e15a4277d1e05ae86d88	remaining useful life prediction using prognostic methodology based on logical analysis of data and kaplan-meier estimation	cbm;rul;logical analysis of data;prognostics;kaplan meier estimation	Most of the reported prognostic techniques use a small number of condition indicators and/or use a thresholding strategies in order to predict the remaining useful life (RUL). In this paper, we propose a reliability-based prognostic methodology that uses condition monitoring (CM) data which can deal with any number of condition indicators, without selecting the most significant ones, as many methods propose. Moreover, it does not depend on any thresholding strategies provided by the maintenance experts to separate normal and abnormal values of condition indicators. The proposed prognostic methodology uses both the age and CM data as inputs to estimate the RUL. The key idea behind this methodology is that, it uses Kaplan–Meier as a timedriven estimation technique, and logical analysis of data as an event-driven diagnostic technique to reflect the effect of the operating conditions on the age of the monitored equipment. The performance of the estimated RUL is measured in terms of the difference between the predicted and the actual RUL of the monitored equipment. A comparison between the proposed methodology and one of the common RUL prediction technique; Cox proportional hazard model, is given in this paper. A common dataset in the field of prognostics is employed to evaluate the proposed methodology. A. Ragab · M.-S. Ouali · S. Yacout (B) · H. Osman Industrial Engineering and Applied Mathematics Department, École Polytechnique de Montréal, Montréal, Canada e-mail: soumaya.yacout@polymtl.ca A. Ragab e-mail: ahmed.ragab@polymtl.ca M.-S. Ouali e-mail: msouali@polymtl.ca H. Osman e-mail: hany.osman@polymtl.ca	baseline (configuration management);email;event-driven programming;industrial engineering;jsp model 2 architecture;kaplan–meier estimator;least absolute deviations;mean time between failures;newton–cotes formulas;thresholding (image processing)	Ahmed Ragab;Mohamed-Salah Ouali;Soumaya Yacout;Hany Osman	2016	J. Intelligent Manufacturing	10.1007/s10845-014-0926-3	reliability engineering;engineering;data mining;statistics;prognostics	SE	13.670193504293682	-15.295566669929414	35961
6a3583d5034e6376cf7b04784e9770a4d1508d57	preference ranking schemes in multi-objective evolutionary algorithms	multi criteria decision making;multi objective evolutionary algorithm;user preferences;pareto optimal solution;multi objective optimization problem;large classes;pareto optimality	In recent years, multi-objective evolutionary algorithms have diversified their goal from finding an approximation of the complete efficient front of a multi-objective optimization problem, to integration of user preferences. These user preferences can be used to focus on a preferred region of the efficient front. Many such user preferences come from so called proper Pareto-optimality notions. Although, starting with the seminal work of Kuhn and Tucker in 1951, proper Pareto-optimal solutions have been around in the multi-criteria decision making literature, there are (surprisingly) very few studies in the evolutionary domain on this. In this paper, we introduce new ranking schemes of various state-of-the-art multi-objective evolutionary algorithms to focus on a preferred region corresponding to proper Pareto-optimal solutions. The algorithms based on these new ranking schemes are successfully tested on extensive benchmark test problems of varying complexity, with the aim to find the preferred region of the efficient front. This comprehensive study adequately demonstrates the efficiency of the developed multi-objective evolutionary algorithms in finding the complete preferred region for a large class of complex problems.	evolutionary algorithm	Marlon Alexander Braun;Pradyumn Kumar Shukla;Hartmut Schmeck	2011		10.1007/978-3-642-19893-9_16	mathematical optimization;multi-objective optimization;machine learning;welfare economics	AI	21.553789934324723	-4.304553311722542	36011
7005fa6f2a2b24e191f12fd3da827c4340ee996f	a model and algorithm for out-sourcing planning	dynamic programming;optimal solution;time varying;outsourcing;inventory management;complexity theory;outsourcing lot sizing production dynamic programming polynomials cost function laboratories automation heuristic algorithms contracts;dynamic programming algorithm;computational complexity dynamic lot sizing model outsourcing planning inventory capacity dynamic programming algorithm;outsourcing computational complexity dynamic programming inventory management lot sizing;outsourcing planning;computational complexity;inventory capacity;dynamic lot sizing model;polynomial time;lot sizing	This paper addresses a dynamic lot-sizing model with outsourcing. All the unsatisfied demands are out-sourced without backlogging, where the inventory capacity is limited and shortages are prohibited. Costs are concave and time varying. Some new properties are obtained in an optimal solution and a dynamic programming algorithm is developed to solve the problem in strongly polynomial time	algorithm;concave function;dynamic lot-size model;dynamic programming;outsourcing;time complexity	Xiao Liu;Chengen Wang;Xiaochuan Luo;Dingwei Wang	2005	IEEE International Conference on e-Business Engineering (ICEBE'05)	10.1109/ICEBE.2005.9	mathematical optimization;simulation;computer science;dynamic programming	Robotics	5.459809027523382	0.6975356917758464	36016
9724089c4d7fd83ce54f31a26be61cecfc6aaae9	branch-and-cut for the forest harvest scheduling subject to clearcut and core area constraints		Abstract Integrating forest fragmentation into forest harvest scheduling problems adds substantial complexity to the models and solution techniques. Forest fragmentation leads to shrinking of the core habitat area and to weakening of the inter-habitat connections. In this work, we study forest harvest scheduling problems with constraints on the clearcut area and constraints on the core area. We propose a mixed integer programming formulation where constraints on the clearcut area are the so-called cover constraints while constraints on the core area are new in the literature as far as we know. As the number of constraints can be exponentially large, the model is solved by branch-and-cut, where the spatial constraints are generated only as necessary or not before they are needed. Branch-and-cut was tested on real and hypothetical forest data sets ranging from 45 to 1363 stands and temporal horizons ranging from three to seven periods were employed. Results show that the solutions obtained by the proposed approach are within or slightly above 1% of the optimal solution within three hours at the most.	branch and cut;core (optical fiber);scheduling (computing);the forest	Miguel Constantino;Isabel Martins	2018	European Journal of Operational Research	10.1016/j.ejor.2017.07.060	operations management;mathematical optimization;cutting-plane method;mathematics;branch and cut;forest management;scheduling (computing);ranging;integer programming;forest fragmentation	Theory	15.119717017098857	3.202909686920175	36022
65d4cf5fb6b7d017fd039bac597d0c680946cfa1	on the markovian efficiency of bertrand and cournot equilibria	dynamic duopoly;differentiated duopoly;perfect nash equilibria;social sciences;investment;productive asset oligopoly;product differentiation;business economics;adjustment costs;productive asset;economics;quantity competition;market efficiency;price;oligopoly;closed loop nash equilibria;comparing cournot	We characterize and compare closed-loop (feedback) price and quantity strategies within a full-fledged dynamic model of oligopolistic competition in which production requires exploitation of a renewable productive asset. Unlike previous papers on the strategic exploitation of productive assets, we allow for imperfect product substitutability, which enables us to deal with price competition. We show that the traditional result that the Bertrand equilibrium is more efficient than the Cournot equilibrium does not necessarily hold in a Markovian environment, either in the short-run or at the stationary equilibrium, or using the discounted sum of welfare as a criterion for relative efficiency. © 2014 Elsevier Inc. All rights reserved. JEL classification: C73; D43; L13; Q20	bertrand (programming language);mathematical model;nash equilibrium;stationary process	Luca Colombo;Paola Labrecciosa	2015	J. Economic Theory	10.1016/j.jet.2014.11.007	industrial organization;bertrand competition;economics;bertrand paradox;investment;product differentiation;finance;macroeconomics;microeconomics;mathematical economics;market economy;oligopoly	AI	-2.845492440946732	-4.366516258812394	36041
8974559d9ce5fcb6739bb30e742f86d5cceda7f6	using a neural network for forecasting in an organic traffic control management system		Increasing mobility and rising traffic demands cause serious problems in urban road networks. Approaches to reduce the negative impacts of traffic include an improved control of traffic lights and the introduction of dynamic traffic guidance systems that take current conditions into account. One solution for the former aspect is Organic Traffic Control (OTC) which provides a self-organized and self-adaptive system founding on the principles of Organic Computing. This paper introduces further steps in enhancing the current OTC system with a forecasting technique based on neural networks. The prediction of short-term traffic conditions is an important component of an advanced traffic management system. It enables the system to prevent congestions and is able to react faster to changes in the traffic flow.	adaptive system;artificial neural network;computer multitasking;control system;experiment;feedforward neural network;guidance system;management system;organic computing;over-the-counter (finance);recurrent neural network;self-organization	Matthias Sommer;Sven Tomforde;Jörg Hähner	2013			advanced traffic management system;organic computing;management system;computer network;artificial neural network;traffic flow;business;network traffic simulation	Networks	9.305523298221605	-11.568878891303333	36083
92e88bb5d6547917f4a9d22e1ff36016688dca56	on the synergistic use of parallel exact solvers and heuristic/stochastic methods for combinatorial optimisation problems: a study case for the tsp on the meiko cs-2	stochastic method;branch and bound algorithm;exact solution;parallel implementation;combinatorial optimisation	This paper deals with the exact solutions of combinatorial optimisation problems We describe a parallel implementation of a Branch and Bound algorithm we complete such software with heuristic schemes and we show results with reference to the well known TSP problem We evaluate the performance by introducing the concept of implementation e ciency that allows to estimate the communication overhead notwith standing the over search problem	algorithm;branch and bound;combinatorial optimization;heuristic;mathematical optimization;overhead (computing);search problem;synergy;travelling salesman problem;whole earth 'lectronic link	Antonio d'Acierno;Salvatore Palma	1996		10.1007/3540617795_51	mathematical optimization;combinatorics;machine learning;mathematics	AI	22.341810023456134	3.825971934357671	36189
a26d4d5b1b5f056846714e56907cf0d69f22ae1d	supply chain systems coordination with multiple risk sensitive retail buyers	cybernetics;supply contracts supply chain system coordination multiple heterogeneous risk sensitive retail buyers information asymmetric settings risk averse retailer fair trade rule supply chain system optimization information symmetric setting markdown contract parameter;industries;contracts;trade rules coordination mean variance analysis risk analysis stackelberg game supply chain optimization;supply chains;upper bound;supply chains contracts optimization industries upper bound cybernetics;supply chains optimisation retailing risk analysis;optimization	This paper explores supply chain systems coordination challenges in the presence of multiple heterogeneous risk sensitive retail buyers using the commonly seen markdown contract under both information symmetric and asymmetric settings. For each setting, we explore two scenarios. The first scenario allows the upstream manufacturer to freely set a separate contract to each risk averse retailer, whereas the second scenario specifies that the manufacturer has to grant the same contract to each risk averse retailer under the fair trade rule. We analytically show that the markdown contract which can achieve “perfect coordination” only exists in the first scenario (without the fair trade rule) under the information symmetric setting. For all the other scenarios, we find that perfect coordination cannot be achieved by the markdown contract, and hence we develop the computational algorithms to help identify the markdown contract parameter(s) which can achieve the “best possible coordination.” In addition, we reveal that the manufacturer's risk attitude does significantly affect the achievability of perfect coordination. The findings of this paper also provide analytical evidence to show that the fair trade rule would do more harm than good for supply chain systems optimization under simple supply contracts.	algorithm;design by contract;mathematical optimization;risk aversion	Tsan-Ming Choi	2016	IEEE Transactions on Systems, Man, and Cybernetics: Systems	10.1109/TSMC.2015.2452894	cybernetics;computer science;artificial intelligence;supply chain;upper and lower bounds	Security	-2.2757045251649424	-4.5585216289736525	36199
dd797a08a9f29b7afb4e32dc1670c113c51707b7	an optimal algorithm for the stochastic bandits with knowing near-optimal mean reward		This paper studies a variation of stochastic multi-armed bandit (MAB) problem where the agent knows a prior knowledge named Near-optimal Mean Reward (NoMR). We show that the cumulative regret of this bandit variation has a lower bound of Ω (1/∆), where ∆ is the gap between the optimal and the second optimal mean reward. An algorithm called NoMR-Bandit is proposed to this variation, and we demonstrate that the cumulative regret of NoMRBandit has a uniform upper bound of O (∆). It is concluded that NoMR-Bandit is optimal in terms of the order of regret bounds.	algorithm;multi-armed bandit;regret (decision theory)	Shangdong Yang;Hao Wang;Yang Gao;Xingguo Chen	2018			computer science;artificial intelligence;machine learning;regret;multi-armed bandit;upper and lower bounds;algorithm	ML	22.828708205974955	-17.350386349607483	36205
04367a446144afe8c446cee533d00a641183a998	managing cost overrun risk in project funding allocation	project management;decision making under uncertainty;resource allocation;probability density function;project manager;decision maker;portfolio optimization;risk preference;weighted sums;cost overrun risk;project selection	This paper discusses decision making of project funding allocation under uncertain project costs. Because project costs are uncertain and funding allocations may not necessarily match the costs required, each project is inherently subject to a cost overrun risk (COR). In this paper, a model is proposed in which project cost is treated as a factor with a probability density function. The decision maker then allocates the total funding to the projects while minimizing a weighted sum of mean and variance of the COR of the project portfolio. Some properties of project COR are derived and interpreted. Optimal funding allocation, in relationship to factors such as various project sizes and riskiness, project interdependency, and the decision maker’s risk preference, is analyzed. The proposed funding allocation model can be integrated with project selection decision-making and provides a basis for more effective project control.	chain-of-responsibility pattern;interdependence;interpreted language;mathematical optimization;object lifetime;optimization problem;weight function	Chung-Li Tseng;Kyle Y. Lin;Satheesh K. Sundararajan	2005	Annals OR	10.1007/s10479-005-6238-4	basis of estimate;project management;cost contingency;decision-making;probability density function;actuarial science;resource allocation;environmental resource management;operations management;portfolio optimization;project management triangle;schedule;risk management plan;project portfolio management	AI	6.459086268129567	-5.37363830002506	36301
8d412b419eda6196a55c32da472a96c677a88da3	knowledge-based extraction of area of expertise for cooperation in learning	robot learning;multi robot learning;mobile robot;q learning;cooperation in learning;multi robot systems learning artificial intelligence mobile robots;mobile robots;areas of expertise;real world application;learning systems intelligent robots machine learning process control intelligent control data mining feature extraction testing mobile robots uncertainty;multi robot learning cooperation in learning area of expertise knowledge evaluation q learning;multi robot learning knowledge based extraction mobile robots areas of expertise q learning;information exchange;multi robot systems;knowledge evaluation;learning artificial intelligence;area of expertise;knowledge based extraction;knowledge base	Using each other's knowledge and expertise in learning - what we call cooperation in learning- is one of the major existing methods to reduce the number of learning trials, which is quite crucial for real world applications. In situated systems, robots become expert in different areas due to being exposed to different situations and tasks. As a consequence, areas of expertise (AOE) of the other agents must be detected before using their knowledge, especially when the exchanged knowledge is not abstract, and simple information exchange might result in incorrect knowledge, which is the case for Q-learning agents. In this paper we introduce an approach for extraction of AOE of agents for cooperation in learning using their Q-tables. The evaluating robot uses a behavioral measure to evaluate itself, in order to find a set of states it is expert in. That set is used, then, along with a Q-table-based feature for extraction of areas of expertise of other robots by means of a classifier. Extracted areas are merged in the last stage. The proposed method is tested both in extensive simulations and in real world experiments using mobile robots. The results show effectiveness of the introduced approach, both in accurate extraction of areas of expertise and increasing the quality of the combined knowledge, even when, there are uncertainty and perceptual aliasing in the application and the robot	ata over ethernet;aliasing;decision table;experiment;information exchange;markov chain;mobile robot;partially observable system;q-learning;simulation;situated;statistical classification	Majid Nili Ahmadabadi;Ahmad Imanipour;Babak Nadjar Araabi;Masoud Asadpour;Roland Siegwart	2006	2006 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2006.281730	mobile robot;knowledge base;computer science;knowledge management;artificial intelligence;machine learning	Robotics	18.45006055502771	-21.226306660433842	36302
805940c521c5e18c1f772f3d2d91fc24dc932c23	inclusion measure and its use in measuring similarity and distance measure between hesitant fuzzy sets	hesitant fuzzy set;fuzzy set theory;fuzzy sets hafnium decision making educational institutions measurement uncertainty cognition intelligent systems;distance hesitant fuzzy set inclusion measure similarity measure;hfs similarity measure distance measure hesitant fuzzy sets hybrid monotonic inclusion measures hesitant fuzzy elements;inclusion measure;similarity measure;distance	In this paper, we propose a variety of inclusion measures for hesitant fuzzy sets (HFSs), based on which the corresponding similarity and distance measures can be obtained. First, we investigate the hybrid monotonic inclusion measures between two hesitant fuzzy elements (HFEs) and present their properties. Second, the hybrid monotonic inclusion measure between two HFSs are proposed. Furthermore, similarity measures and distance are postulated by inclusion measures. Last, numerical examples are provided to illustrate the inclusion measures, similarity measures and distance.	fuzzy set;numerical analysis;similarity measure	Shu Yun Yang;Hong-Ying Zhang;Zhi Wei Yue	2013	2013 IEEE International Conference on Granular Computing (GrC)	10.1109/GrC.2013.6740441	mathematical analysis;discrete mathematics;computer science;fuzzy measure theory;data mining;mathematics;fuzzy set;distance;fuzzy set operations	Robotics	-2.403891167650065	-21.412808616732246	36312
f3b67c169080209f730bdcecd45f44f8f17acc13	approximating the optimal mapping for weapon target assignment by fuzzy reasoning	weapon target assignment wta;fuzzy decision maker fdm;grid partitioning	Weapon target assignment is a weapon assignment problem with the objective of minimizing the expected survival value of targets. This problem must be quickly solved on the battlefield (i.e., in real-time). Considering the combined complexity and the strict time constraints, a fuzzy decision maker is proposed as an alternative to aid commanders in deciding on proper weapon assignments. The concept builds a fuzzy decision maker for a given data set by using extended grid partitioning based on a sensitivity analysis of input variables. The proposed decision maker is implemented and tested with several randomly sampled assignment instances on realistic scenarios. In addition, a larger-scale scenario is also considered to demonstrate the scalability of the approach. The results show that the proposed system exhibits satisfactory performance and is serviceable on the battlefield.	analysis of algorithms;approximation algorithm;computation;finite difference method;logic programming;norm (social);rule induction;scalability;test set;time complexity;weapon target assignment problem	Mehmet Alper Sahin;Kemal Leblebicioglu	2014	Inf. Sci.	10.1016/j.ins.2013.08.004	mathematical optimization;artificial intelligence;machine learning;weapon target assignment problem;operations research;algorithm	AI	2.704158037915818	-18.623261215226663	36315
38cd095002347683957081c90a639bafe98efe01	a pso based integrated functional link net and interval type-2 fuzzy logic system for predicting stock market indices	interval type 2fls;particle swarm optimization;functional link artificial neural network;fuzzy logic system;backpropagation learning algorithm	This chapter maps out the development of the PSO based Functional Link Interval Type-2 Fuzzy Neural System (FLIT2FNS) model used to forecast the stock market indices. In the process, it discusses the architecture of Functional Link Artificial Neural Network (FLANN), FLANN & Type-1Fuzzy Logic System (Type1FLS) and the differences between Type-1FLS and Interval Type-2 Fuzzy Logic System (IT2FLS). The discussion aims at highlighting the relative advantages and disadvantages of all these models. It also shows the superiority of the hybrid model FLIT2FNS.	algorithm;artificial neural network;backpropagation;biological systems engineering;fuzzy logic;machine learning;map;maxima and minima;particle swarm optimization;phase-shift oscillator;quasiperiodicity;simulation	Sreejit Chakravarty;Pradipta Kishore Dash	2012	Appl. Soft Comput.	10.1016/j.asoc.2011.09.013	mathematical optimization;computer science;artificial intelligence;neuro-fuzzy;machine learning;mathematics;particle swarm optimization;algorithm	AI	7.226714093350824	-21.505065691187646	36318
de5dc11038dde1000a5852e0e89a35a909292355	on the relation between fuzzy closing morphological operators, fuzzy consequence operators induced by fuzzy preorders and fuzzy closure and co-closure systems	info eu repo semantics contributiontoperiodical;fuzzy consequence operator;fuzzy closure system;fuzzy preorder;fuzzy morphological operator;lattice	In a previous paper, Elorza and Burillo explored the coherence property in fuzzy consequence operators. In this paper we show that fuzzy closing operators of mathematical morphology are always coherent operators. We also show that the coherence property is the key to link the four following families: fuzzy closing morphological operators, fuzzy consequence operators, fuzzy preorders and fuzzy closure and co-closure systems. This will allow to translate important well-known properties from the field of approximate reasoning to the field of image processing.	approximation algorithm;closing (morphology);coherence (physics);emoticon;enumerated type;image processing;mathematical morphology;return-oriented programming;structuring element;vertex-transitive graph;whole earth 'lectronic link	Jorge Elorza;Ramón Fuentes-González;Jean Bragard;Pedro J. Burillo	2013	Fuzzy Sets and Systems	10.1016/j.fss.2012.08.010	fuzzy logic;t-norm fuzzy logics;discrete mathematics;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;fuzzy subalgebra;fuzzy number;pure mathematics;lattice;fuzzy measure theory;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;fuzzy control system;algebra	AI	0.04026050162734808	-23.887848832877584	36325
58f6723a4e39ce2047da2ef085eb6c1e094c4119	a genetic-based framework for solving (multi-criteria) weighted matching problems	multi criteria optimization;simulated annealing algorithm;multi objective evolutionary algorithm;computer experiment;genetic algorithm;genetic algorithms;weighted matching;multi objective evolutionary algorithms;pareto optimality	The purpose of this paper is to present a flexible genetic-based framework for solving the multi-criteria weighted matching problem (mc-WMP). In the first part of this paper, we design a genetic-based framework for solving the ordinary weighted matching problem. We present an extensive analysis of the quality of the results and introduce a methodology for tuning its parameters. In the second part, we develop a modified genetic-based algorithm for solving the mc-WMP. The algorithm generates a significant and representative portion of the Pareto optimal set. To assess the performance of the algorithm, we conduct computational experiments with two and three criteria. The potential of the proposed algorithm is demonstrated by comparing to a multi-objective simulated annealing algorithm. 2002 Elsevier Science B.V. All rights reserved.	computation;experiment;genetic algorithm;matching (graph theory);pareto efficiency;simulated annealing	Andrés L. Medaglia;Shu-Cherng Fang	2003	European Journal of Operational Research	10.1016/S0377-2217(02)00484-8	algorithm design;mathematical optimization;combinatorics;meta-optimization;genetic algorithm;cultural algorithm;computer science;machine learning;genetic representation;mathematics;population-based incremental learning	AI	23.44690769224591	-0.30113412658958677	36331
9d68b3b82a6141431377edd64247cea99b928e1d	population size matters: rigorous runtime results for maximizing the hypervolume indicator	evolutionary multi objective optimization;genetic programming;hypervolume indicator;theory;runtime time analysis	Evolutionary multi-objective optimization is one of the most successful areas in the field of evolutionary computation. Using the hypervolume indicator to guide the search of evolutionary multi-objective algorithms has become very popular in recent years. We contribute to the theoretical understanding of these algorithms by carrying out rigorous runtime analyses. We consider multi-objective variants of the problems OneMax and LeadingOnes called OneMinMax and LOTZ, respectively, and investigate hypervolume-based algorithms with population sizes that do not allow coverage of the entire Pareto front. Our results show that LOTZ is easier to optimize than OneMinMax for hypervolume-based evolutionary multi-objective algorithms, which is contrary to the results on their single-objective variants and the well-studied ( 1 + 1 ) EA. Furthermore, we study multi-objective genetic programming using the hypervolume indicator. We show that the classical ORDER problem is easy to optimize if the population size is large enough to cover the whole Pareto front and point out situations where a small population size leads to an exponential optimization time.	population	Anh Quang Nguyen;Andrew M. Sutton;Frank Neumann	2015	Theor. Comput. Sci.	10.1016/j.tcs.2014.06.023	genetic programming;mathematical optimization;computer science;machine learning;mathematics;theory;algorithm	ECom	23.844749782389197	-6.6462320055999635	36352
032efb420e716047409dbd8df9444a46c5bbcc5b	a parameter free algorithm of cooperative genetic algorithm for nurse scheduling problem	parameter free algorithm shift schedule schedule reoptimization nursing level nurse organization holidays attendance hospital penalty function penalty weight adjustment technique nurse scheduling problem cooperative genetic algorithm;hospitals;patient care;scheduling genetic algorithms hospitals patient care;scheduling;genetic algorithms;schedules scheduling optimization medical services sociology statistics computer aided software engineering	This paper describes a technique of penalty weight adjustment for the Cooperative Genetic Algorithm applied to the nurse scheduling problem. In this algorithm, coefficient and thresholds for each penalty function are automatically optimized. Therefore, this technique provides a parameter free algorithm of nurse scheduling. The nurse scheduling is very complex task, because many requirements must be considered. These requirements are implemented by a set of penalty function in this research. In real hospital, several changes of the schedule often happen. Such changes of the shift schedule yields various inconveniences, for example, imbalance of the number of the holidays and the number of the attendance. Such inconvenience causes the fall of the nursing level of the nurse organization. Reoptimization of the schedule including the changes is very hard task and requires very long computing time. We consider that this problem is caused by the solution space having many local minima. We propose a technique to adjust penalty weights and thresholds through the optimization to escape from the local minima.	coefficient;feasible region;genetic algorithm;mathematical optimization;maxima and minima;nurse scheduling problem;penalty method;requirement;scheduling (computing);shift jis	Makoto Ohki;Satoru Kishida	2013	2013 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2013.6637348	fair-share scheduling;nurse scheduling problem;mathematical optimization;simulation;genetic algorithm;dynamic priority scheduling;computer science;rate-monotonic scheduling;genetic algorithm scheduling;scheduling	Robotics	20.86677873138898	-0.058421678659952965	36360
c12dfaac9ace5a12c426a1e7c7df8372d3299d4d	discrete support vector decision trees via tabu search	analyse multivariable;nombre entier;decision tree;multivariate analysis;decision aid;support vector machines;analisis datos;comercializacion;ayuda decision;classification;support vector;algorithme;commercialisation;algorithm;integer;data analysis;mixed integer program;estimation erreur;62h30;marketing;error estimation;entero;statistical computation;estimacion error;calculo estadistico;aide decision;analisis multivariable;analyse donnee;tabu search;calcul statistique;support vector machine;decision trees;clasificacion;algoritmo	An algorithm is proposed for generating decision trees in which multivariate splitting rules are based on the new concept of discrete support vector machines. By this term a discrete version of SVMs is denoted in which the error is properly expressed as the count of misclassi0ed instances, in place of a proxy of the misclassi0cation distance considered by traditional SVMs. The resulting mixed integer programming problem formulated at each node of the decision tree is then e2ciently solved by a tabu search heuristic. Computational tests performed on both well-known benchmark and large marketing datasets indicate that the proposed algorithm consistently outperforms other classi0cation approaches in terms of accuracy, and is therefore capable of good generalization on validation sets. c © 2003 Published by Elsevier B.V.	algorithm;approximation algorithm;benchmark (computing);computation;decision tree;heuristic;integer programming;linear programming;structural risk minimization;support vector machine;tabu search	Carlotta Orsenigo;Carlo Vercellis	2004	Computational Statistics & Data Analysis	10.1016/j.csda.2003.11.005	support vector machine;mathematical optimization;machine learning;mathematics;algorithm	ML	0.488524150037915	-15.541508936148626	36361
4294adeec1fe8709425c60cae78b9236f1452905	application of relevance vector machine and survival probability to machine degradation assessment	probability density function;data collection;machine prognostics;censored data;condition monitoring;kaplan meier;relevance vector machine;intelligent system;uncensored data;operation and maintenance;survival probability	Condition monitoring (CM) of machines health or industrial components and systems that can detect, classify and predict the impending faults is critical in reducing operating and maintenance cost. Many papers have reported the valuable models and methods of prognostic systems. However, it was rarely found the papers deal with censored data, which was common in machine condition monitoring practice. This work deals with development of machine degradation assessment system that utilizes censored and complete data collected from CM routine. Relevance vector machine (RVM) is selected as intelligent system then trained by input data obtained from run-to-failure bearing data and target vectors of survival probability estimated by Kaplan–Meier (KM) and probability density function estimators. After validation process, RVM is employed to predict survival probability of individual unit of machine component. The plausibility of the proposed method is shown by applying the proposed method to bearing degradation data in predicting survival probability of individual unit. 2010 Elsevier Ltd. All rights reserved.	artificial intelligence;censoring (statistics);elegant degradation;kaplan–meier estimator;numerical weather prediction;overfitting;plausibility structure;portable document format;relevance vector machine;simulation	Achmad Widodo;Bo-Suk Yang	2011	Expert Syst. Appl.	10.1016/j.eswa.2010.08.049	probability density function;data science;data mining;relevance vector machine;censoring;statistics;data collection	AI	13.621570592650004	-15.501275058618146	36373
f9cee3a3687c0492d23a6b7524395f49462753d7	resource planning and scheduling of payload for satellite with genetic particles swarm optimization	automatic control;earth observing satellite;differential evolution;resource constraint;evolutionary computation;constraint optimization;resource planning;availability;scheduling artificial satellites combinatorial mathematics genetic algorithms particle swarm optimisation;earth;genetic reproduction mechanisms;planning and scheduling;genetic reproduction mechanisms resource planning satellite payload scheduling genetic particles swarm optimization automated control earth observing satellite constraint optimization problem combinatorial optimization;genetic particles swarm optimization;genetics;strategic planning;constraint optimization problem;technology planning;particle swarm optimizer;scheduling;particle swarm optimization;satellites;artificial satellites;genetic algorithm;payloads;genetic algorithms;earth observation;payload;automated control;genetic algorithm planning and scheduling payload particle swarm optimization;combinatorial optimization;particle swarm optimization algorithm;satellite payload scheduling;particle swarm optimisation;combinatorial mathematics	The resource planning and scheduling technology of payload is a key technology to realize an automated control for earth observing satellite with limited resources on satellite, which is implemented to arrange the works states of various payloads to carry out missions by optimizing the scheme of the resources. The scheduling task is a difficult constraint optimization problem with various and mutative requests and constraints. Based on the analysis of the satellite's functions and the payload's resource constraints, a proactive planning and scheduling strategy based on the availability of consumable and replenishable resources in time-order is introduced along with dividing the planning and scheduling period to several pieces, where then the planning and scheduling is modeled as a combinatorial optimization. The genetic particle swarm optimization algorithm (GPSO) is proposed to address the problem, which was derived from the original continuous particle swarm optimization (PSO) and incorporated with the genetic reproduction mechanisms, namely crossover and mutation. The simulation results have shown that GPSO significantly improved the search efficacy of PSO for the combinatorial optimizations.	automated planning and scheduling;combinatorial optimization;constrained optimization;enterprise resource planning;genetic algorithm;mathematical optimization;mutation (genetic algorithm);optimization problem;particle swarm optimization;scheduling (computing);simulation	Li Jian;Wang Cheng	2008	2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational Intelligence)	10.1109/CEC.2008.4630799	fair-share scheduling;payload;mathematical optimization;multi-swarm optimization;simulation;genetic algorithm;strategic planning;dynamic priority scheduling;combinatorial optimization;computer science;genetic algorithm scheduling;two-level scheduling;automatic control;satellite;evolutionary computation	Robotics	20.11792084604995	-0.9998488254942385	36387
c1ee810d77b8ca8f076e59b1a07a19be3f2a547f	commercial credit difference evaluation and prediction model: based on neural network	anti-interference prediction;artificial neural networks (ann);credit tendency differenc	The different capital configurations of bank accounts for different credit tendency which will affect the currency condition as well as whole finance system. The mercurial and nonlinear factors of economy usually brought the difficulty when predicts. This study adopts the feed-forward backprop network (BP), conceiving predicting modeling with the sample of various banks’. The simulation results indicated that the model performs well in anti-interference and accurate in prediction error (less than 2%). Moreover, we got the result that non state-own banks tend to be more cautious than state-own by 10% on average.	artificial neural network;backpropagation;interference (communication);mercurial;nonlinear system;simulation	Yi Wang;Huosong Xia;Jian Liu	2010	JCIT		probabilistic neural network;computer science;machine learning;predictive modelling;artificial neural network	ML	6.913958726935864	-18.288168474691265	36410
42d0b89de2d55dfdc51f5142887f01d661640df3	computational approaches for efficient scheduling of steel plants as demand response resource	energy intensive industrial processes demand response resource steel plants scheduling branch and bound algorithm industrial loads;demand response;steel plant scheduling;resource task network branch and bound algorithm mixed integer programming demand response steel plant scheduling;branch and bound algorithm;tree searching demand side management scheduling steel industry;mixed integer programming;metals heating;resource task network	Demand response is seeing increased popularity worldwide and industrial loads are actively taking part in this trend. As a host of energy-intensive industrial processes, steel plants have both the motivation and potential to provide demand response. However, the scheduling of steel plants is very complex and the involved computations are intense. In this paper, we focus on these difficulties and propose methods such as adding cuts and implementing an application-specific branch and bound algorithm to make the computations more tractable.	algorithm;branch and bound;cobham's thesis;computation;industrial pc;iteration;mathematical optimization;numerical analysis;rounding;scheduling (computing);steel;time complexity	Xiao Zhang;Gabriela Hug;J. Zico Kolter;Iiro Harjunkoski	2016	2016 Power Systems Computation Conference (PSCC)	10.1109/PSCC.2016.7540881	fair-share scheduling;mathematical optimization;simulation;dynamic priority scheduling;engineering;operations management	EDA	5.16788167533463	3.0681624516447163	36452
4d4e6c385f7cfd5d481c610651d717daf9c3dd74	a practical module-based simulation model for transshipment-inventory systems	dynamic programming;marine transportation;stock control data processing;module based simulation;pull system;power system modeling linear programming environmental economics production facilities marine vehicles marine transportation dynamic programming performance analysis joining processes costs;inventory policy;trans shipment inventory systems;transportation digital simulation goods dispatch data processing stock control data processing;order by a demand;goods dispatch data processing;marine vehicles;building simulation;transportation;production facilities;environmental economics;performance analysis;linear programming;joining processes;linear program;power system modeling;simulation model;digital simulation;supply and demand;pull system module based simulation trans shipment inventory systems linear programming transportation supply and demand order by a demand inventory policy	A method of modeling transshipment-inventory systems proposed in an attempt to describe the systems flexibly which a lot of kinds of items are ordered to transport a transship, transported, stored, and delivered to the tomers. The system consists of a number of supply, tra shipment and demand nodes. However, the problem c sidered in this study is totally different from the tradition transshipment problem in terms of linear programmin Firstly, any number of different kinds of items can be trea for analysis. Secondly, any size of transportation trucks be specified to transport items for any number of the tw node combinations. In other words, the capacity of transportation truck is to be specified in building a simu tion model. In addition, any number of supply, transsh ment and demand nodes can be specified in a simula model. Thirdly, the order by a demand node is made tow the associated transshipment node, based on the inve policy at the demand node, and the so-called the “p system” is adopted in the demand-supply environment. An efficient module-based modeling method proposed to generate simulation models for the abo mentioned transshipment-inventory systems. The p posed method is applied to the actual system. It is fo that the time to build simulation models could b drastically reduced. Furthermore, the proposed metho found to be both practical and powerful.	adaptive binary optimization;co-ment;fo (complexity);order by;p system;simula;simulation	Soemon Takakuwa;Tsukasa Fujii	1999		10.1145/324898.325091	transport;simulation;computer science;engineering;linear programming;dynamic programming;simulation modeling;mathematics;supply and demand;transport engineering	AI	11.78040556635589	0.8648390935329631	36486
64c2b9883e88ad54ff29ae8ada5cc6d701313c08	research on the optimal transit route selection model and automatic inquiry system	automatic query system;bus cluster;transit route selection	An optimal transit route selection problem is studied in this paper. Since the data are abundant, it is unfavorable to use conventional method. We put forward a concept of bus cluster and establish a network graph theory model by means of the radiation and sweeps methods. Based on the platform of the software Delphi and Object Pascal program, we develop an automatic query system which provides a user interface with complete function and convenient operation. Through this interface, the best route between any two transit stations can be quickly inquired in different needs. Consequently, the optimal transit route selection problem is solved.		Jianli Cao	2010		10.1007/978-3-642-16339-5_23	real-time computing;simulation;computer science;operations management	NLP	11.710484599205945	1.7151461120517966	36537
1bbe30cd2425b3b62d5b015f3c151a233088ce86	equilibrium and sensitivity analysis of dynamic ridesharing	traffic equilibrium;automobiles;equilibrium states sensitivity analysis dynamic ridesharing travel sharing car occupancy rate improvement operations research route similarity detouring rescheduling equilibrium modeling approach nested logit model behavioral factors matching criteria single link case complex multiple factor interaction;cib_traffic;sensitivity analysis;transportation;ridesharing;transportation automobiles;behavior;vehicle dynamics;vehicles fuels mathematical model analytical models sensitivity analysis vehicle dynamics cost function;logits	Sharing travels is an effective way to increase car occupancy rates and to reduce the number of cars for the same distances traveled. This is a problem involving operations research (how to best match travels in time and space, what level of similarity for the routes, ...) and behavior challenges (on which conditions users choose to share travels, which type of users is willing to share the travel, what is the accepted detour and rescheduling, ...). This research aims at filling the gap in the latter aspect by proposing an equilibrium modeling approach for (dynamic) ridesharing. By using a Nested logit model, greater insight into the impact of different policies, behavioral factors and matching criteria can be presented. We deal with a theoretically interesting basic model structure of a single-link case, in which we study the complex interaction of multiple factors, and the joint sensitivity to the most relevant parameters. This allows us to derive some basic recommendations from the equilibrium states reached.	complex network;discrete choice;logistic regression;matching (graph theory);operations research;point of view (computer hardware company)	Francesco Viti;Francesco Corman	2013	16th International IEEE Conference on Intelligent Transportation Systems (ITSC 2013)	10.1109/ITSC.2013.6728587	simulation;engineering;operations management;transport engineering	Robotics	9.294216571226162	-8.653722377191649	36548
cb4ec9d21a9f96c98fc8172192b552d5d0246d00	predicting global internet growth using augmented diffusion, fuzzy regression and neural network models	performance measure;forecasting;gross domestic product;new technology;fuzzy regression;artificial intelligent;economic indicator;global internet growth;neural network model;diffusion;forecast accuracy;diffusion model;neural network	Quantitative models explaining and forecasting the growth of new technology like the Internet in global business operation appear infrequently in the literature. This paper introduces two artificial intelligence (AI) models such as the neural network and fuzzy regression along with an augmented diffusion model to study and predict the Internet growth in several OECD nations. First, a linear version of an augmented diffusion model is designed. An augmented diffusion model is constructed by including an economic indicator, gross domestic product per capita, into the model. In the next step, two soft AI models are calibrated from the augmented diffusion model. Performance measures of predictions from these models on new samples show that these soft models provide improved forecast accuracy over the augmented diffusion model. The results confirm the major contribution of this research in predicting global Internet growth.		Kallol Kumar Bagchi;Somnath Mukhopadhyay	2006	International Journal of Information Technology and Decision Making	10.1142/S0219622006001861	gross domestic product;econometrics;simulation;economics;forecasting;computer science;artificial intelligence;machine learning;economic indicator;diffusion;diffusion;artificial neural network;statistics	Vision	6.1246392519897634	-17.798644791676228	36574
d1b360ff01b41d64328a6f6f8da22ab584b8db87	a new evolutionary algorithm based on contraction method for many-objective optimization problems	orthogonal crossover operator;many objective optimization;ranking	The convergence ability of Pareto-based evolutionary algorithms sharply reduces for many objective optimization problems because solutions are difficult to rank by the Pareto dominance due to large size of non-dominance area. In order to tackle the problem, a new contraction method on non-dominance area is proposed to rank solutions. The method makes the objective space as well as the no-dominance area be contracted into smaller regions. In order to get reasonable selection pressure and maintain the diversity of solutions, the contraction degree for different solutions is different, i.e., for solutions close to Pareto optimal solutions the degree of contraction is bigger. Then a new multi-objective evolutionary algorithm based on the new contraction method and orthogonal crossover operator with quantification technique (QOX) is designed for many-objective problems. Experimental results show that the proposed approach can guide the search to converge to the Pareto optimal front (PF) for many-objective optimization problems. 2014 Elsevier Inc. All rights reserved.	circuit complexity;converge;dominating set;evolutionary algorithm;experiment;mathematical optimization;optimization problem;pareto efficiency;quantization (signal processing)	Cai Dai;Yuping Wang;Ye Miao	2014	Applied Mathematics and Computation	10.1016/j.amc.2014.07.069	mathematical optimization;combinatorics;ranking;mathematics;algorithm;statistics	AI	23.417558176343835	-3.8875020405732523	36585
13d6876a6dfc1cc175cc1831e48726ee01ea652d	a learning classifier system for mazes with aliasing clones	learning process;learning algorithm;learning agents;aliasing;reinforcement learning;learning classifier systems;associative perception;learning classifier system;navigation;real world application;maze	Maze problems represent a simplified virtual model of the real environment and can be used for developing core algorithms of many real-world application related to the problem of navigation. Learning Classifier Systems (LCS) are the most widely used class of algorithms for reinforcement learning in mazes. However, LCSs best achievements in maze problems are still mostly bounded to non-aliasing environments, while LCS complexity seems to obstruct a proper analysis of the reasons for failure. Moreover, there is a lack of knowledge of what makes a maze problem hard to solve by a learning agent. To overcome this restriction we try to improve our understanding of the nature and structure of maze environments. In this paper we describe a new LCS agent that has a simpler and more transparent performance mechanism. We use the structure of a predictive LCS model, strip out the evolutionary mechanism, simplify the reinforcement learning procedure and equip the agent with the ability to Associative Perception, adopted from psychology. We then assess the new LCS with Associative Perception on an extensive set of mazes and analyse the results to discover which features of the environments play the most significant role in the learning process. We identify a particularly hard feature for learning in mazes, aliasing clones, which arise when groups of aliasing cells occur in similar patterns in different parts of the maze. We discuss the impact of aliasing clones and other types of aliasing on learning algorithms.	3d modeling;algorithm;aliasing;learning classifier system;machine learning;reinforcement learning	Zhanna V. Zatuchna;Anthony J. Bagnall	2007	Natural Computing	10.1007/s11047-007-9055-7	semi-supervised learning;aliasing;navigation;error-driven learning;simulation;computer science;artificial intelligence;machine learning;learning classifier system;reinforcement learning;algorithm	AI	17.624253570784067	-21.755321101788326	36597
4029675ff78fdb3b13e05967ec1208d7e109aac1	study on primary product logistics: demand prediction based on neural network theory	libraries;learning process;primary production;logistics neural networks;neural networks;mass customization;logistics neural networks economic forecasting demand forecasting predictive models artificial neural networks statistics transportation educational institutions production;economic forecasting;data stream;the demand of primary product logistics;biological system modeling;artificial neural networks;logistics;transportation;web sites;the demand of primary product logistics demand forecasting artificial neural networks;statistics;production;predictive models;prediction model;product design;back propagation;artificial neural network;neural network;demand forecasting	Primary product logistics shares the challenges of other logistical problems, but also possesses many unique features which preclude the application of usual methods of the logistics of primary products. In particular, it is not possible to accurately forecast demand. To overcome the limitations of single logistics demand forecasting techniques and the difficulties in primary products logistics that exist currently, this paper reports the use of neural network theory to establish a predictive model of the demand in primary products logistics based on a back-propagation (BP) neural network. The BP Algorithm used in the learning process includes two processes: forward computing of data stream and backward propagation of error signals, which make the output vector closer to the expected output vectors by continuous adjusting of weights, thus improving the accuracy of the logistics forecasting. Primary products demand and example Analysis verify the accuracy of this BP neural network based prediction model for primary product demand.	algorithm;artificial neural network;backpropagation;logistics;network theory;propagation of uncertainty;software propagation	Xin-li Wang;Kun Zhao	2010	2010 Third International Conference on Knowledge Discovery and Data Mining	10.1109/WKDD.2010.147	computer science;machine learning;predictive modelling;operations research;artificial neural network	ML	7.615379362905552	-19.22707936556309	36602
fa117ad1c7aca33e9891a063a0e2f6d35d572a17	online task scheduling for edge computing based on repeated stackelberg game		Abstract A key function of an edge service provider (ESP) is to dynamically allocate resources to tasks existing at the edges upon request. This is, however, a challenging task due to a number of several factors: real-time decision-making without any prior knowledge of future arrivals, tasks’ satisfactions provided by requests, and utilization of resources. To address these challenges, we propose an online scheduling that maps various tasks to the given relevant resources based on a repeated Stackelberg game. First, we model this problem as a long-term vs. short-term repeated Stackelberg game. In particular, for each round of the game, acting as a short-term leader, a user with a request first decides the unit prices for processing tasks within the relevant budget to maximize current total satisfaction of tasks. Then, based on the prices offered by different users in different rounds, to maximize the long-term profits earned from users, the ESP acts as the follower whose strategy is matching resources with tasks, and splitting those tasks among different edge centers owning various types of resources (edge mobile devices). The Stackelberg equilibrium between the ESP and the users is obtained using our proposed algorithms. Finally, we evaluate the effectiveness of our proposal, in terms of task distributions.	edge computing;scheduling (computing)	Yingmo Jie;Xinyu Tang;Kim-Kwang Raymond Choo;Tuba Korkmaz;Mingchu Li;Cheng Guo	2018	J. Parallel Distrib. Comput.	10.1016/j.jpdc.2018.07.019	service provider;distributed computing;profit (economics);stackelberg competition;scheduling (computing);mobile device;edge computing;computer science	HPC	3.6118613214456565	0.7653314280655467	36609
6b791c04d8312d53aadd0aa29ba69d923f610981	improved performance of computer networks by embedded pattern detection		"""Computer Networks are usually balanced appealing to personal expe- rience and heuristics, without taking advantage of the behavioral patterns em- bedded in their operation. In this work we report the application of tools of computational intelligence to find such patterns and take advantage of them to improve the network's performance. The traditional traffic flow for Computer Network is improved by the concatenated use of the following """"tools"""": a) Ap- plying intelligent agents, b) Forecasting the traffic flow of the network via Multi-Layer Perceptrons (MLP) and c) Optimizing the forecasted network's parameters with a genetic algorithm. We discuss the implementation and expe- rimentally show that every consecutive new tool introduced improves the beha- vior of the network. This incremental improvement can be explained from the characterization of the network's dynamics as a set of emerging patterns in time."""	embedded system	Ángel Fernando Kuri Morales;Iván Cortés-Arce	2014		10.1007/978-3-319-07491-7_3	computer science;artificial intelligence;machine learning;data mining;network simulation	Arch	7.97178715395885	-23.165409103614618	36612
88c51d4e075a530d7cf4ba541a9780fe2129b4ca	boosting active learning to optimality: a tractable monte-carlo, billiard-based algorithm	generalization error;active learning;reinforcement learning;optimal policy;query by committee;tree structure;uct;finite horizon;monte carlo;sampling strategy;multi armed bandit;proof of principle	This paper focuses on Active Learning with a limited number of queries; in application domains such as Numerical Engineering, the size of the training set might be limited to a few dozen or hundred examples due to computational constraints. Active Learning under bounded resources is formalized as a finite horizon Reinforcement Learning problem, where the sampling strategy aims at minimizing the expectation of the generalization error. A tractable approximation of the optimal (intractable) policy is presented, the Bandit-based Active Learner (BAAL) algorithm. Viewing Active Learning as a single-player game, BAAL combines UCT, the tree structured multi-armed bandit algorithm proposed by Kocsis and Szepesvári (2006), and billiard algorithms. A proof of principle of the approach demonstrates its good empirical convergence toward an optimal policy and its ability to incorporate prior AL criteria. Its hybridization with the Query-by-Committee approach is found to improve on both stand-alone BAAL and stand-alone QbC.	active learning (machine learning);algorithm;application domain;approximation;cobham's thesis;computation;computer go;experiment;generalization error;greedy algorithm;heuristic (computer science);kernel method;monte carlo method;multi-armed bandit;nonlinear system;numerical linear algebra;principle of maximum entropy;reinforcement learning;sampling (signal processing);search algorithm;test set	Philippe Rolet;Michèle Sebag;Olivier Teytaud	2009		10.1007/978-3-642-04174-7_20	mathematical optimization;multi-armed bandit;computer science;artificial intelligence;machine learning;data mining;mathematics;active learning;tree structure;proof of concept;reinforcement learning;active learning;statistics;monte carlo method;generalization error	ML	22.388696662361113	-18.222541205730625	36636
b14e0f35157347d073f256cf3ffdf172fa6b1ec5	an interactive genetic algorithm with c-means clustering for the unequal area facility layout problem	c means clustering facility layout interactive genetic algorithm unequal area facility layout problem;pattern clustering;pattern clustering decision making facilities layout genetic algorithms interactive systems;delta modulation layout optimization encoding clustering algorithms intelligent systems materials;layout;decision maker;materials;delta modulation;facilities layout;clustering method;interactive genetic algorithm;heuristic optimization;intelligent systems;clustering algorithms;facility layout;genetic algorithms;optimization;unequal area facility layout problem;decision maker interactive genetic algorithm c means clustering unequal area facility layout problem ua flp heuristic optimization;subjective evaluation;encoding;interactive systems;ua flp;c means clustering	Unequal Area Facility Layout Problem (UA-FLP) has been addressed by several methods. However, UA-FLP has only been solved regarding quantitative criteria. Our approach includes subjective features to UA-FLP, which are difficult to take into account with a classical heuristic optimization. For that, an Interactive Genetic Algorithm (IGA) is proposed that allows an interaction between the algorithm and the Decision Maker (DM). Involving the DM knowledge into the approach guides the search process, adjusting it to the DM's preferences at every iteration of the algorithm. The whole population is evaluated through the DM subjective evaluations of the representative solutions, which are different enough and are chosen by means of c-Means clustering method. The empirical test results show that the proposed IGA is capable of capturing DM preferences and that it can progress towards a good solution in a reasonable number of iterations.	cluster analysis;common criteria;dm-crypt;genetic algorithm;heuristic;in-game advertising;iteration;mathematical optimization;user agent;windows fundamentals for legacy pcs	Laura García-Hernández;Henri Pierreval;Lorenzo Salas-Morera;Antonio Arauzo-Azofra	2010	2010 10th International Conference on Intelligent Systems Design and Applications	10.1109/ISDA.2010.5687291	layout;decision-making;delta modulation;genetic algorithm;intelligent decision support system;computer science;artificial intelligence;machine learning;data mining;cluster analysis;encoding	AI	20.578640986837776	-5.1124664104537	36650
1275acd15ae10301d55d1fa9c8b64b4fbf77d9cf	searching for a sequence of adaptation operations utilizing ai-based planning techniques			automated planning and scheduling		2008		10.1007/978-0-387-78414-4_597	engineering;artificial intelligence;operations management;operations research	Robotics	12.262660516079828	-2.446441333996876	36669
0bb389dbb1c83c0ce99f6c2995978b0ab5400948	validation of a new multiclass mesoscopic simulator based on individual vehicles for dynamic network loading	network theory (graphs);road traffic;continuous-time link-based approach;demand discretization;dynamic traffic assignment;individual vehicles;laboratory tests;medium-size urban networks;multiclass mesoscopic simulator;multiclass multilane dynamic network loading model;multilane multiclass traffic behaviors;network flow propagation;simulation-based approach;variable traffic demand	The dynamic network loading problem is crucial for performing dynamic traffic assignment. It must reproduce the network flow propagation, while taking into account the time and a variable traffic demand on each path of the network. In this paper, we consider a simulation-based approach for dynamic network loading as the best-suited option. We present a multiclass multilane dynamic network loading model based on a mesoscopic scheme that uses a continuous-time link-based approach with a complete demand discretization. In order to demonstrate the correctness of the model, we computationally validate the proposed simulation model using a variety of laboratory tests. The obtained results look promising, showing the model's ability to reproduce multilane multiclass traffic behaviors for medium-size urban networks.	correctness (computer science);discretization;experiment;flow network;mesoscopic physics;multiclass classification;simulation;software propagation	M. Paz Linares;Carlos Carmona;Jaume Barceló;Cristina Montañola-Sales	2014	Proceedings of the Winter Simulation Conference 2014		traffic generation model;control engineering;simulation;engineering;transport engineering;network traffic simulation	Robotics	9.456689775533945	-10.820521369962021	36688
1cc193f353d2d6cd98d7c6558860440102023a74	using an arima-garch modeling approach to improve subway short-term ridership forecasting accounting for dynamic volatility		Subway short-term ridership forecasting plays an important role in intelligent transportation systems. However, limited efforts have been made to forecast the subway short-term ridership, accounting for dynamic volatility. The traditional forecasting methods can only provide point values that are unable to offer enough information on the volatility/uncertainty of the forecasting results. To fill this gap, the aim of this paper is to incorporate the dynamic volatility into the subway short-term ridership forecasting process that not only generates the expected value of the short-term ridership but also obtains the prediction interval. Four kinds of the integrated ARIMA and GARCH models are constructed to model the mean part and volatility part of the short-term ridership. The performance of the proposed method is investigated with the real subway short-term ridership data from three stations in Beijing. The model results show that the proposed model outperforms the traditional model for all three stations. The hybrid model can significantly improving the reliability of the predicted point value by reducing the mean prediction interval length of the ridership, and improve the prediction interval coverage probability. Considering the different traffic patterns between weekday and weekend, the short-term ridership is also modeled, respectively. This paper can help management understand the dynamic volatility of the subway short-term ridership, and have the potential to disseminate more reliable subway information to travelers through the information systems.	autoregressive integrated moving average;information system;network congestion;nonlinear system;smart card;tropical cyclone track forecasting;volatility	Chuan Ding;Jinxiao Duan;Yanru Zhang;Xinkai Wu;Guizhen Yu	2018	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2017.2711046	time series;econometrics;simulation;engineering;accounting;coverage probability;prediction interval;autoregressive integrated moving average;expected value;volatility (finance);autoregressive conditional heteroskedasticity;intelligent transportation system	Robotics	8.425504306908087	-13.878498135328416	36713
b299252b9f34427f5cf3b64b4a5f616c27afeaf7	enhanced decomposition-based many-objective optimization using supplemental weight vectors	evolutionary computation;approximation algorithms;information services;statistics;optimization;search problems;sociology	In evolutionary multi-objective optimization, each solution in the population generally has two roles. The first one is to approximate a part of the Pareto front, and the second one is to be a variable information resource to generate offspring. In many-objective optimization involving four or more conflicting objectives, solutions in the population have to be sparsely distributed in the objective space and the variable space to approximate a high-dimensional Pareto front, and each solution faces the difficulty to play the second role since variables are drastically individualized in the population. To overcome this problem, we focus on MOEA/D algorithm framework and propose a method to introduce supplemental weight vectors and solutions which maintain variable information resource to enhance the solution search for each part of the Pareto front. Experimental results using many-objective knapsack problems show that the supplemental weight vectors and solutions improves the search performance of MOEA/D by improving the diversity of the obtained solutions.	algorithmic efficiency;approximation algorithm;computation;human body weight;knapsack problem;moea framework;mathematical optimization;multi-objective optimization;pareto efficiency	Hiroyuki Sato;Satoshi Nakagawa;Minami Miyakawa;Keiki Takadama	2016	2016 IEEE Congress on Evolutionary Computation (CEC)	10.1109/CEC.2016.7743983	mathematical optimization;computer science;theoretical computer science;machine learning;mathematics;information system;evolutionary computation	AI	24.385041753024208	-3.400969595467792	36723
88a6e05cccd5374e41d18446d111afd6a3782b04	intercity bus scheduling model incorporating variable market share	transportation computational complexity integer programming nonlinear programming road vehicles scheduling share prices;nondeterministic polynomial time hard;intercity bus;intercity bus scheduling model nondeterministic polynomial time hard nonlinear mixed integer program competitive market passenger choice behavior variable market share;nonlinear programming;market share;routing np hard problem testing profitability civil engineering air transportation joining processes lagrangian functions humans;nonlinear mixed integer program;mixed integer program;integer programming;computational complexity;scheduling;variable market share;competitive market;transportation;polynomial time;adaptive scheduling;consumer behavior;share prices;intercity bus scheduling model;choice behavior;variable market shares intercity bus nonlinear mixed integer program scheduling;variable market shares;passenger choice behavior;road vehicles	In this paper, we develop an intercity bus scheduling model for a variable market share. The proposed model, unlike the past intercity bus scheduling models, does not assume a fixed market share. Instead, passenger choice behaviors in competitive markets are considered. The model is formulated as a nonlinear mixed integer program that is characterized as nondeterministic polynomial-time hard. We develop a solution algorithm to efficiently solve the model. The test results, which are based on a major Taiwan intercity bus carrier, show the good performance of the model and the solution algorithm.	algorithm;integer programming;linear programming;np (complexity);nonlinear system;scheduling (computing);time complexity	Shangyao Yan;Yu-Jung Tung;Ching-Hui Tang	2007	IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans	10.1109/TSMCA.2007.904824	time complexity;market share;transport;mathematical optimization;simulation;integer programming;nonlinear programming;computer science;perfect competition;computational complexity theory;scheduling	Metrics	14.998383235999098	2.93580334924366	36819
5987ead663c2aebd650fb67a7dfdc43653d9d87e	intuitionistic fuzzy rough approximation operators determined by intuitionistic fuzzy triangular norms		In this paper, relation-based intuitionistic fuzzy rough approximation operators determined by an intuitionistic fuzzy triangular norm T are investigated. By employing an intuitionistic fuzzy triangular norm T and its dual intuitionistic fuzzy triangular conorm, lower and upper approximations of intuitionistic fuzzy sets with respect to an in- tuitionistic fuzzy approximation space are first introduced. Properties of T-intuitionistic fuzzy rough approximation operators are then examined. Relationships between special types of intuitionistic fuzzy relations and properties of T-intuitionistic fuzzy rough approximation operators are further explored.	approximation;intuitionistic logic	Wei-Zhi Wu;Shen-Ming Gu;Tong-Jun Li;You-Hong Xu	2014		10.1007/978-3-319-11740-9_60	mathematical analysis;discrete mathematics;topology;defuzzification;fuzzy mathematics;fuzzy classification;fuzzy subalgebra;fuzzy number;mathematics;fuzzy set operations	Robotics	-0.916861772354631	-22.511765630641115	36835
f791ba830f1cb912ad61429beca355c575ed4943	optimal strategies of it consulting firms: the impact of license fee and open source	competition;cost saving;license fees;economic model;open source code;software package;pricing strategy;profit maximization;it consulting firms;open source	In this paper, we develop an economic model to study the impact of commercial software and open source codes on the solutions offered by IT consulting firms. Robust support and trust are the most important assets of the IT solution built on commercial software, whereas cost savings is the major reason for adopting the IT solution developed by open source codes. The study aims to characterize the difference between the IT solutions by the demand for the number of licenses and the distribution of IT knowledge level. We find that the distribution of IT knowledge level on the pricing strategy of the IT consulting firm offering the solution built on open source codes is not monotonic. Moreover, from the viewpoint of efficiency and profit maximization, we find that IT consulting firm should help enterprise customers enhance their IT knowledge as it can offer the solutions built on different software packages.	algorithmic efficiency;code;commercial software;entropy maximization;knowledge level;open-source software	Yung-Ming Li;Jhih-Hua Jhang-Li;Yen-Chun Liu	2008		10.1145/1409540.1409594	marketing;business;commerce	Metrics	-0.4244954244579559	-6.2955954266446446	36862
6a37bb9f383136568b114afeb458903d7b7f9599	evaluating a local genetic algorithm as context-independent local search operator for metaheuristics	intensification;iterated local search;objective function;variable neighborhood search;local search based metaheuristics;discrete parameter optimization;genetic algorithm;context independent local search;evolutionary algorithm;local search;parameter optimization	Local genetic algorithms have been designed with the aim of providing effective intensification. One of their most outstanding features is that they may help classical local search-based metaheuristics to improve their behavior. This paper focuses on experimentally investigating the role of a recent approach, the binary-coded local genetic algorithm (BLGA), as context-independent local search operator for three local search-based metaheuristics: random multi-start local search, iterated local search, and variable neighborhood search. These general-purpose models treat the objective function as a black box, allowing the search process to be context-independent. The results show that BLGA may provide an effective and efficient intensification, not only allowing these three metaheuristics to be enhanced, but also predicting successful applications in other local search-based algorithms. In addition, the empirical results reported here reveal relevant insights on the behavior of classical local search methods when they are performed as context-independent optimizers in these three well-known metaheuristics.	black box;experiment;general-purpose modeling;genetic algorithm;iterated local search;iteration;local search (optimization);loss function;metaheuristic;optimization problem;variable neighborhood search	Carlos García-Martínez;Manuel Lozano	2010	Soft Comput.	10.1007/s00500-009-0506-1	beam search;local optimum;mathematical optimization;genetic algorithm;tabu search;computer science;artificial intelligence;local search;hill climbing;machine learning;evolutionary algorithm;iterated local search;mathematics;incremental heuristic search;best-first search;line search;metaheuristic;guided local search;search algorithm	AI	23.838287917723406	-3.3063943122677895	36865
63d28dc76a7c7409d6a39b5b454fa5773207f61b	hybrid process neural network based on spatio-temporal similarities for short-term traffic flow prediction	hybrid process neural network;cross correlation function;neural networks;road network;neural nets;cross correlation;neural networks telecommunication traffic traffic control roads predictive models intelligent transportation systems spatiotemporal phenomena uncertainty artificial neural networks layout;short term road traffic flow prediction model;road traffic;intelligent transportation systems;traffic flow;correlation methods;time delay;spatio temporal similarity;prediction theory;comparative method;spatiotemporal phenomena;traffic engineering computing;spatiotemporal analysis;predictive models;prediction model;mathematical prediction;conferences;delays;neural network;time delay hybrid process neural network spatio temporal similarity short term road traffic flow prediction model cross correlation function;traffic engineering computing correlation methods delays neural nets prediction theory road traffic spatiotemporal phenomena	Spatio-temporal similarities, one of the characteristics to describe the relativity of traffic phenomenon, can be utilized to predict short-term traffic flow. These similarities not always appear at spatial adjacent road links because of complexity of road network. In this paper, we adopt Cross-Correlation Function to depict similarities between different traffic flow series according to the observed flow data. The process characteristic generalizes the evolvement rules of traffic flow which are essentials need to be tackled by a prediction model. After choosing the most correlative road links and their time delay instead of the upstream or downstream ones, a Hybrid Process Neural Network is constructed to predict shortterm traffic flow, which uses various scales to catch traffic features such as daily-periodicity, weekly-periodicity and spatiotemporal process, since a simple model is not good enough to depict all these rules. Application of the proposed method is demonstrated, and the experimental results show that our method outperforms other compared methods.	artificial neural network;broadcast delay;downstream (software development);principle of good enough;quasiperiodicity	Cheng Hu;Kunqing Xie;Guojie Song;Tianshu Wu	2008	2008 11th International IEEE Conference on Intelligent Transportation Systems	10.1109/ITSC.2008.4732609	traffic generation model;simulation;engineering;artificial intelligence;machine learning	Robotics	8.71843988218745	-22.530321908885988	36874
6a0c50f56f33476d0c2b375803baea335a8ee055	an envelope theorem and some applications to discounted markov decision processes	90c40;envelope theorem;93e20;economic model;optimal policy;value iteration;economic growth model;optimization problem;differentiability of the optimal policy;euclidean space;value function;infinite horizon;markov decision process;optimal value function;discounted markov decision process;differentiability of the optimal value function	In this paper, an Envelope Theorem (ET) will be established for optimization problems on Euclidean spaces. In general, the Envelope Theorems permit analyzing an optimization problem and giving the solution by means of differentiability techniques. The ET will be presented in two versions. One of them uses concavity assumptions, whereas the other one does not require such kind of assumptions. Thereafter, the ET established will be applied to the Markov Decision Processes (MDPs) on Euclidean spaces, discounted and with infinite horizon. As the first application, several examples (including some economic models) of discounted MDPs for which the et allows to determine the value iteration functions will be presented. This will permit to obtain the corresponding optimal value functions and the optimal policies. As the second application of the ET, it will be proved that under differentiability conditions in the transition law, in the reward function, and the noise of the system, the value function and the optimal policy of the problem are differentiable with respect to the state of the system. Besides, various examples to illustrate these differentiability conditions will be provided. Copyright Springer-Verlag 2008	envelope theorem;markov chain;markov decision process	Hugo Cruz-Suárez;Raúl Montes-de-Oca	2008	Math. Meth. of OR	10.1007/s00186-007-0155-z	markov decision process;mathematical optimization;mathematics;mathematical economics	Theory	1.5385922529374627	-1.9588201529008253	36933
a33d35b708ae84578b132a24cc8813c44dd7bf77	solving the quadratic assignment problem on heterogeneous environment (cpus and gpus) with the application of level 2 reformulation and linearization technique	tesla c2070;paper;heterogeneous systems;cuda;nvidia geforce gtx titan;nvidia;algorithms;computer science	The Quadratic Assignment Problem, QAP, is a classic combinatorial optimization problem , classified as NP-hard and widely studied. This problem consists in assigning N facilities to N locations obeying the relation of 1 to 1, aiming to minimize costs of the displacement between the facilities. The application of Reformulation and Linearization Technique, RLT, to the QAP leads to a tight linear relaxation but large and difficult to solve. Previous works based on level 3 RLT needed about 700GB of working memory to process one large instances (N = 30 facilities). We present a modified version of the algorithm proposed by Adams et al. which executes on heterogeneous systems (CPUs and GPUs), based on level 2 RLT. For some instances, our algorithm is up to 140 times faster and occupy 97% less memory than the level 3 RLT version. The proposed algorithm was able to solve by first time two instances: tai35b and tai40b. 1. Introduction The Quadratic Assignment Problem, or QAP, is one of the hardest and studied combi-natorial optimization problems of literature. Consists to find an allocation of N facilities to N locations obeying the ratio of 1 to 1 and aiming to minimize the cost obtained by the sum of flow-distance products. Its formulation was initially presented by Koopmans e Beckmann (1957) and has practical applications in object allocation into departments, electronic circuit boards design, layouts problems, construction planning, and others. For a detailed study on the QAP we suggest the following references: Pardalos et al. Exact methods to solve the QAP require high computational power, for example, in Hahn et al. (2013), one instance with 30 facilities (as nug30) spent 8 days in a supercomputer composed by 32 nodes of 2.2GHz Intel Xeon CPU and 700GB of main memory. The best known strategy to achieve the exact solution of QAP is using branch-and-bound algorithms where the main problem is divided into smaller sub-problems in trying to solve them exactly. The lower bound are essential components for branch-and-bound procedures because they allow discards a large number of alternatives in the search for the optimal solution. The limits reached by Burer e Vandenbussche (2006), Adams et al. (2007) and Hahn et al. (2012) stand out as the most tighter to the QAP. The proposed by Burer e Vandenbussche (2006) consists of relaxations lift-and-project for binary integer problems, Adams et al. (2007) presents a dual ascent algorithm based on level …	algorithm;anomaly detection at multiple scales;branch and bound;cpu cache;central processing unit;combinatorial optimization;computation;computer data storage;displacement mapping;electronic circuit;gigabyte;graphics processing unit;linear programming relaxation;mathematical optimization;np-hardness;obedience (human behavior);operations research;optimization problem;printed circuit board;quadratic assignment problem;relation (database);supercomputer;times ascent	Alexandre Domingues Gonçalves;Artur Alves Pessoa;Lúcia Maria de A. Drummond;Cristiana Bentes;Ricardo C. Farias	2015	CoRR		mathematical optimization;parallel computing;computer science;theoretical computer science;operating system;distributed computing;algorithm	AI	16.67550827469768	4.048768343681894	36936
24cb16cd0c305ad2362e9f80f7d071df5cccb891	a discrete simulation framework for part replenishment optimization		Supply Chains are difficult to plan as they involve complex relations and maintain dynamically changing variables that influence them. In this paper, we present a discrete event simulation framework for purpose of decision making in a replacement auto parts Supply Chain. Ford Motor’s Parts, Supply and Logistics (PS&L) department supports a Supply Chain that represents a trade-off where parts are either maintained at a central distribution facility or sent directly to local distribution center. This represents a compromise between inventory transportation costs and accessibility in parts distribution. To support decisions within this environment, we present a framework to characterize this scenario as a discrete simulation problem allowing for the means to evaluate controls for the determination of optimal inventory (on-hand inventory dollars), fill rate and labor costs. Our case study results demonstrate the necessary dynamics to support this decision making process.	accessibility;algorithm;centralized computing;economic order quantity;image scaling;interaction;logistics;mathematical optimization;program optimization;simultech;simulation	David A. Ostrowski;Bradley Graham;Oleg Yu. Gusikhin	2013		10.5220/0004633004670473	discrete optimization	AI	11.144301055136731	-2.719757666400714	36944
4a33287da1b82ba5d77bb188867524c2ed9f0337	a mixed r&d projects and securities portfolio selection model	modelizacion;metodo momento;selection problem;model selection;approximation lineaire;problema seleccion;project management;moment method;securite;bolsa valores;approximation method;portfolio selection;r d project portfolio selectionsemi absolute deviation risk function;gestion risque;risk management;seleccion cartera;linear approximation;knowledge society;selection modele;natural science foundation of china;probabilistic approach;long terme;journal;long term;bourse valeurs;selection portefeuille;stock exchange;scenario;modelisation;business environment;systeme incertain;selection projet;programacion lineal;largo plazo;programacion mixta entera;methode approximation;seleccion modelo;argumento;enfoque probabilista;approche probabiliste;methode moment;script;safety;mixed integer stochastic programming problem;aproximacion lineal;portfolio management;linear programming;programmation lineaire;programmation partiellement en nombres entiers;linear program;mixed integer programming;r d project portfolio selection;gestion projet;gestion cartera;gestion riesgo;stochastic model;gestion portefeuille;stochastic programming;scenario generation;sistema incierto;seguridad;modeling;modelo estocastico;seleccion proyecto;uncertain system;modele stochastique;project selection;gestion proyecto;semi absolute deviation risk function;probleme selection	The business environment is full of uncertainty. Allocating the wealth among various asset classes may lower the risk of overall portfolio and increase the potential for more benefit over the long term. In this paper, we propose a mixed single-stage R&D projects and multi-stage securities portfolio selection model. Specifically, we present a bi-objective mixed-integer stochastic programming model. Moreover, we use semi-absolute deviation risk functions to measure the risk of mixed asset portfolio. Based on the idea of moments approximation method via linear programming, we propose a scenario generation approach for the mixed single-stage R&D projects and multi-stage securities portfolio selection problem. The bi-objective mixed-integer stochastic programming problem can be solved by transforming it into a single objective mixed-integer stochastic programming problem. A numerical example is given to illustrate the behavior of the proposed mixed single stage R&D projects and multistage securities portfolio selection model. This work was partially supported by the Informatics Research Center for Development of Knowledge Society Infrastructure, Graduate School of Informatics, Kyoto University, Japan and the National Natural Science Foundation of China. Corresponding author. Tel: +86-10-62765141. Corresponding author. Tel: +81-75-753-5519; Fax: +81-75-753-4756. E-mail addresses: yfang@mail.amss.ac.cn (Yong Fang), chenlh@gsm.pku.edu.cn (Lihua Chen), fuku@i.kyoto-u.ac.jp (Masao Fukushima).	approximation;entity–relationship model;fax;informatics;integer programming;knowledge society;linear programming;loss function;multistage amplifier;numerical analysis;programming model;selection algorithm;semiconductor industry;stochastic programming;uniform resource identifier	Yong Fang;Lihua Chen;Masao Fukushima	2008	European Journal of Operational Research	10.1016/j.ejor.2007.01.002	stochastic programming;post-modern portfolio theory;econometrics;mathematical optimization;stock exchange;systems modeling;economics;replicating portfolio;linear programming;scenario;stochastic modelling;operations management;portfolio optimization;mathematics;mathematical economics;model selection;linear approximation	AI	4.8630194779612275	-4.967690165591454	36958
80476cdbee83a278b8f6ceddef42067a467a7d22	a hybrid intelligent system for short and mid-term forecasting for the celpe distribution utility	power distribution planning;optimisation;hybrid intelligent systems load forecasting artificial neural networks power system planning matlab information analysis power quality power industry statistical analysis time series analysis;mathematics computing;neural nets;time 45 day hybrid intelligent system short forecasting mid term forecasting energy company of pernambuco distribution utility artificial neural network heuristic rules electric load forecasting matlab 7 0 r14 load demand time 3 day time 7 day time 15 day time 30 day;hybrid intelligent system;load forecasting;power engineering computing;power engineering computing artificial intelligence load forecasting mathematics computing neural nets optimisation power distribution planning;artificial intelligence;artificial neural network	This paper presents the development of a hybrid intelligent system, joining an artificial neural network (ANN) based technique and heuristic rules to adjust the short and mid-term electric load forecasting in the 3, 7, 15, 30, and 45 days ahead. The study was based on load demand data of Energy Company of Pernambuco (CELPE), whose data contain the hourly load consumption in the period from January-2000 until December-2004. The proposed system forecasts a holiday as one Saturday or Sunday based on the specialist's information that analyzes the load behaviors of each holiday. The hybrid intelligent system presented an improvement in the load forecasts in relation to the results achieved by the ANN alone. The program was implemented in MATLAB 7.0 R14.	artificial intelligence;artificial neural network;electrical load;heuristic;hybrid intelligent system;load profile;matlab	Ronaldo R. B. de Aquino;Aida A. Ferreira;Milde M. S. Lira;Geane B. Silva;Otoni Nóbrega Neto;J. B. Oliveira;Carlos F. D. Diniz;J. Fideles	2006	The 2006 IEEE International Joint Conference on Neural Network Proceedings	10.1109/IJCNN.2006.247145	simulation;computer science;artificial intelligence;hybrid intelligent system;machine learning;artificial neural network	Robotics	9.234192278955948	-17.564263945769007	37054
216aa925abfef1f87eb684dbf77bbf4a78e58944	t-syntopogenous structures compatible with fuzzy t-uniformities and fuzzy t-neighbourhoods structures	fuzzy t uniform spaces;fuzzy t neighbourhood spaces;t syntopogenous spaces	In this paper, we continue the discussion of T-syntopogenous spaces, which were introduced by the second author in 2012. This manuscript shows that the T-syntopogenous spaces agree well with Hashem-Morsi fuzzy T-neighbourhood spaces (2003), fuzzy T-proximity spaces (2002) and Hohle fuzzy T-uniform spaces (1982). Also, we show that each syntopogenous space in the sense of Csaszar (1963), generates a T-syntopogenous space. Finally, we deduce the concept of fuzzy T-neighbourhood syntopogenous structures and some related results.		Nehad N. Morsi;Khaled A. Hashem	2014	Fuzzy Sets and Systems	10.1016/j.fss.2014.02.008	discrete mathematics;fuzzy mathematics;fuzzy classification;fuzzy subalgebra;fuzzy number;mathematics;geometry;t-norm	Logic	0.2321517053379005	-23.63389818697215	37075
e243a1a0f0c0e4841b925b2421223adfff8f1811	analysis of sequential failures for assessment of reliability and safety of manufacturing systems	manufacturing systems safety;reliability;failure analysis;petri nets;petri net;manufacturing system;sequential failures	Assessment of reliability and safety of a manufacturing system with sequential failures is an important issue in industry, since the reliability and safety of the system depend not only on all failed states of system components, but also on the sequence of occurrences of those failures. Methods that are currently available in sequential failure analysis always start with given sequences of the failures in the system, which is not the case in real life situations; therefore, the sequences of the failures should be identified and the probability of their occurrence should be determined. In this paper, we represent a methodology that can be used for identifying the failure sequences and assessing the probability of their occurrence in a manufacturing system. The method employs Petri net modeling and reachability trees constructed based on the Petri nets. The methodology is demonstrated on an example of an automated machining and assembly system.		Angela Adamyan;David He	2002	Rel. Eng. & Sys. Safety	10.1016/S0951-8320(02)00013-3	reliability engineering;systems engineering;engineering;mathematics;petri net;statistics	Robotics	7.334292922953335	0.5948599436249272	37076
4abd9f9dd1480f45d6ae2e96f8cc2968543df44f	approval voting and scoring rules with common values	approval voting;plurality rule;information aggregation;scoring rule	We compare approval voting with other scoring rules for environments with common values and private information. For finite electorates, the best equilibrium under approval voting is superior to plurality rule or negative voting. For large electorates, if any scoring rule yields a sequence of equilibria that aggregates information, then approval voting must do so as well.	personally identifiable information	David S. Ahn;Santiago Oliveros	2016	J. Economic Theory	10.1016/j.jet.2016.09.002	scoring rule;bullet voting;actuarial science;economics;approval voting;data mining;microeconomics;cardinal voting systems;positional voting system;preferential block voting;welfare economics;anti-plurality voting;condorcet method	AI	-4.3171780182941	-3.998131962820337	37088
3476e35d48988e1c31792916ce2362630f643968	an improved algorithm of fuzzy circuit for closed fuzzy matroids	fuzzy set;matrix algebra;finite element method;fuzzy set theory;dependent fuzzy set matroid fuzzy matroid closed fuzzy matroids fuzzy circuits algorithm;fuzzy set fuzzy circuit closed fuzzy matroids standardization;matrix algebra fuzzy set theory;educational institutions fuzzy sets telecommunications physics finite element methods presses	The algorithm of Fuzzy circuits for closed fuzzy matroids existes some disadvantages in [1]. We will improved the algorithm from four aspects: the standardization of fuzzy set μ, the simplification of its steps, the correction of its errors and the enhancement of its practicability.	algorithm;fuzzy set;level of detail;matroid	Yonghong Li;Sidong Xian;Dong Qiu	2011	2011 Eighth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)	10.1109/FSKD.2011.6019474	fuzzy logic;t-norm fuzzy logics;combinatorics;discrete mathematics;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy subalgebra;fuzzy number;neuro-fuzzy;fuzzy measure theory;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;fuzzy control system	EDA	-0.5196251952446965	-23.734173872302886	37140
0f01fd06cace8f719e79c6198601aefd6b38d211	effects of customer response to fashion product stockout on holding costs, order sizes, and profitability in omnichannel retailing			omnichannel	Berdymyrat Ovezmyradov;Hisashi Kurata	2019	ITOR	10.1111/itor.12511	operations management;omnichannel;profitability index;holding cost;stockout;mathematics	ECom	-0.3846411630508203	-5.271812310382206	37203
2aa5c5e356a08da61877529f703bfa4a62030961	suitable aggregation operator for a realistic supplier selection model based on risk preference of decision maker		In this paper, we propose (a) the realistic models for Supplier Selection and Order Allocation (SSOA) problem under fuzzy demand and volume/quantity discount constraints, and (b) how to select the suitable aggregation operator based on risk preference of the decision makers (DMs). The aggregation operators under consideration are additive, maximin, and augmented operators while the risk preferences are classified as risk-averse, risk-taking, and risk-neutral ones. The fitness of aggregation operators and risk preferences of DMs is determined by statistical analysis. The analysis shows that the additive, maximin, and augmented aggregation operators are consistently suitable for risk-taking, risk-averse, and risk-neutral DMs, respectively.		Sirin Suprasongsin;Pisal Yenradee;Van-Nam Huynh	2016		10.1007/978-3-319-45656-0_6	management science	AI	-3.109509062139736	-17.38963974882599	37221
f96f5e82023112e534cb762963c620dc64bbc7db	learning-by-doing; consequences for incentive design	learning by doing;incentive scheme;private information	Through a learning-by-doing process, a firm’s efficiency depends positively on the extent of its previous business activity. From a dynamic incentives perspective, therefore, efficiency is endogenous. In addition, the efficiency of the firm is likely to be subject to private information. The model captures some of the trade-offs principals face in designing incentive schemes in this context.  2002 Elsevier Science B.V. All rights reserved.	endogeneity (econometrics);personally identifiable information	Petter Osmundsen	2002	Information Economics and Policy	10.1016/S0167-6245(01)00050-6	private information retrieval;economics;incentive;public economics;marketing;microeconomics;welfare economics	AI	-3.2105991400782705	-7.01270771465101	37288
d4f6572016b4fff6b0b5e1ae55bfa005f847de13	intestinal broiler microflora estimation by artificial neural network	broiler;modeling;microflora population	Microflora population of poultry was affected by various factors. Many methods and techniques were developed to study microflora population. But, most of them confronted some problems. Moreover, being costly, laborious, and time-consuming made it impossible to measure microflora population several times. In this study, we tried to estimate intestinal microflora population using artificial neural network (ANN). Lactic acid bacteria were used as model of microflora population. Time and lactic acid bacteria were used as input and output variables, respectively. The best model of ANN was determined based on coefficient of determination, root mean square error, and mean absolute error criteria. The results of current study have shown that ANN is appropriate, cheap, and reliable tools to estimate intestinal microflora population (lactic acid bacteria) of broiler at different ages.	approximation error;artificial neural network;coefficient of determination;input/output;mean squared error	Hamid Reza Hemati Matin;Ali Asghar Saki;Hasan Aliarabi;Mojtaba Shadmani;Hamid Zare Abyane	2011	Neural Computing and Applications	10.1007/s00521-011-0553-2	systems modeling	ML	11.663960788664454	-20.326360158514973	37305
e6c20b3e9028ef9cd5cc8bcbcc13b2af161387b9	shadowed fuzzy sets: a framework with more freedom degrees for handling uncertainties than interval type-2 fuzzy sets and lower computational complexity than general type-2 fuzzy sets		In this chapter, the concept of Shadowed Fuzzy Set is introduced and some of its related operations are studied. Shadowed Fuzzy Set enables localization of the underlying uncertainty of fuzzy grades in type-2 fuzzy sets through exploitation of shadowed sets. It provides a capable framework that despite preserving the uncertainties of fuzzy grades in type-2 fuzzy sets, adheres the simplicity of the concept and operations of interval type-2 fuzzy sets.	computational complexity theory;fuzzy set;type-2 fuzzy sets and systems	Hooman Tahayori;Alireza Sadeghian	2013		10.1007/978-3-642-28959-0_6	mathematical optimization;combinatorics;discrete mathematics;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy classification;fuzzy number;fuzzy measure theory;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations	Vision	-1.6075672039239044	-23.40892231216045	37357
7cb4c3a2bef9a0d4fad3b9a68d0de482def1cd9c	bounds on uptime distribution based on aging for systems with finite lifetimes	aging;repairable and nonrepairable failures;bounds;uptime	Consider a system which is subject to occasional failures over its lifetime. Under general stochastic assumptions concerning the occurrence of these failures, the duration of the repair times, and the length of the system’s lifetime, the distribution of the total uptime the system accumulates over its lifetime may be determined. While the distribution is unfortunately complicated except in certain circumstances, it is possible to derive useful bounds based on the aging characteristics of the distributions of interfailure times, repair times, and system life. Bounds are developed for the cases where the system lifetime follows (1) a degenerate distribution (or constant mission duration), (2) an exponential distribution, and (3) a general distribution. These bounds may be interpreted as extensions of well-known bounds based on aging notions from renewal theory. Conditions are identified for when the bounds are sharp, and examples are used to show the computational tractability and usefulness of the results.	computation;object lifetime;time complexity;uptime	Charles E. Wells	2015	Annals OR	10.1007/s10479-015-1950-1	ageing;operations management;mathematics;statistics	Metrics	6.164814574486428	-1.0291092283773444	37394
8953df3c747684a481217f66c2bc482e29aa0b66	two natural heuristics for 3d packing with practical loading constraints	maximum touching area;freight transport;3d packing;capacitated vehicle routing problem;vehicle routing;three dimensional;travel cost;tabu search algorithm;tabu search;deepest bottom left fill	In this paper, we describe two heuristics for the Single Vehicle Loading Problem (SVLP), which can handle practical constraints that are frequently encountered in the freight transportation industry, such as the servicing order of clients; item fragility; and the stability of the goods. The two heuristics, Deepest-Bottom-Left-Fill and Maximum Touching Area, are 3D extensions of natural heuristics that have previously only been applied to 2D packing problems. We employ these heuristics as part of a two-phase tabu search algorithm for the Three-Dimensional Loading Capacitated Vehicle Routing Problem (3L-CVRP), where the task is to serve all customers using a homogeneous fleet of vehicles at minimum traveling cost. The resultant algorithm produces mostly superior solutions to existing approaches, and appears to scale better with problem size.	heuristic (computer science);set packing	Lei Wang;Songshan Guo;Shi Chen;Wenbin Zhu;Andrew Lim	2010		10.1007/978-3-642-15246-7_25	three-dimensional space;mathematical optimization;tabu search;computer science;vehicle routing problem	ML	16.494049069825945	1.9083975681524181	37434
beaa64f1158ca4c8d9a931e7938e33bf3b15060f	efficient local search in traffic assignment	uncertainty;space exploration;roads;games;genetic algorithms;search problems;vehicles	The traffic assignment problem (TAP) plays a key role in the context of efficient urban mobility. The TAP can be approached from various perspectives. One of the fundamental models to solve the TAP is the so-called User Equilibrium (UE), which assumes that drivers behave rationally aiming at minimising their travel costs. However, this is a complex optimisation problem. To this regard, in this paper we propose the use of the GRASP metaheuristic to provide approximate solutions to the UE. The path relinking mechanism is also used to increase the coverage of the solutions space. We advance the state-of-the-art by proposing a novel modelling for the TAP, through which one can adjust the granularity of the search space, thus making a more efficient, directed local search. We also devise an efficient assignment evaluation scheme that avoids redundant computations during the local search process. Additionally, we develop a novel greedy procedure for generating enhanced initial solutions for the GRASP algorithm. Based on experiments, we demonstrate that our approach outperforms classical algorithms, providing solutions that are significantly closer to the UE. Moreover, our empirical results show that the stability and fairness levels achieved by our approach are considerably better than those achieved by other methods.	anarchy;approximation algorithm;assignment problem;computation;experiment;fairness measure;grasp;greedy algorithm;iteration;local search (optimization);mathematical optimization;metaheuristic	Gabriel de Oliveira Ramos;Ana L. C. Bazzan	2016	2016 IEEE Congress on Evolutionary Computation (CEC)	10.1109/CEC.2016.7743966	games;mathematical optimization;simulation;genetic algorithm;uncertainty;computer science;artificial intelligence;space exploration;machine learning;mathematics;statistics	AI	21.976119578974746	-3.1750615947997334	37466
5082a640c706802413ab58e7dd4a43364e601c50	mechanism design for daily deals	daily deals;mechanism design	Daily deals are very popular in today’s e-commerce. In this work, we study the problem of mechanism design for a daily deal website to maximize its revenue and obtain the following results. (1) For the Bayesian setting, we first design a revenue-optimal incentive-compatible (IC) mechanism with pseudo-polynomial time complexity. Considering the high computational complexity of the mechanism, we then develop a greedy mechanism that is much more computationally efficient yet maintains a constant competitive ratio regarding the Bayesian optimal revenue in expectation. (2) For the prior-independent setting, we first propose a randomized IC mechanism with a pseudo-polynomial time complexity that can achieve a constant competitive ratio. Then, by leveraging the greedy mechanism designed for the Bayesian setting, we come up with a new mechanism that can achieve a good tradeoff between computational efficiency and competitive ratio. After that, we discuss the robustness issue regarding the two mechanisms (i.e., they both use the trick of random partition and may perform badly for the worst-case partition) and propose an effective way to guarantee a constant competitive ratio even for the worst-case partition.	algorithmic efficiency;best, worst and average case;competitive analysis (online algorithm);computational complexity theory;deal of the day;e-commerce;greedy algorithm;polynomial;pseudo-polynomial time;randomized algorithm;time complexity	Binyi Chen;Tao Qin;Tie-Yan Liu	2015			mechanism design;simulation;computer science	AI	-0.8999230418960267	0.1790958795786528	37507
d1e7f3f432801eaad8b9a0c0edf12f8d7f0c34dd	visual navigation with actor-critic deep reinforcement learning		Visual navigation in complex environments is crucial for intelligent agents. In this paper, we propose an efficient deep reinforcement learning (DRL) method to tackle visual navigation tasks. We present the synchronous advantage actor-critic (A2C) with generalized advantage estimator (GAE) algorithm. The A2C enables agents to learn from multiple processes, which significantly reduces the training time. The GAE used to estimate the advantage function improves the policy gradient estimates. We focus on visual navigation tasks in ViZDoom, and train agents in two health gathering scenarios. The experimental results show this method successfully teaches our agents to navigate in these scenarios. The A2C with GAE agent reaches the highest score in the first task, and a competitive score in the second task. In addition, this agent has better average scores and lower variances in both tasks.	actor model;algorithm;driven right leg circuit;google app engine;gradient;intelligent agent;machine vision;minecraft;performance;reinforcement learning	Kun Shao;Dongbin Zhao;Yuanheng Zhu;Qichao Zhang	2018	2018 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2018.8489185	intelligent agent;machine learning;estimator;task analysis;reinforcement learning;pattern recognition;visualization;artificial intelligence;computer science	Robotics	19.75627519838977	-20.162722251217275	37579
0488f08f652193ea07b3fccb7a5a47f26221bebb	process situation assessment: from a fuzzy partition to a finite state machine	state machine;dynamic system;fuzzy clustering;system evolution;complex system;clustering method;state space;data acquisition;situation assessment;finite state machine	Process situation assessment plays a major role in supervision of complex systems. The knowledge of the system behavior is relevant to support operators in their decision tasks. For complex industrial processes such as chemical or petrochemical ones, most of supervision approaches are based on data acquisition techniques and specifically on clustering methods to cope with the difficulty of modeling the process. Consequently, the system behavior can be characterized by a state space partition. This way, situation assessment is performed online through the tracking of the system evolution from one class to another. Furthermore, a finite state machine that is a support tool for process operators is elaborated to model the system behavior. This article presents theoretical aspects according to which the intuition that the trajectory observation of a dynamical system by a sequence of classes, to which the actual state belongs, gives valuable information about the real behavior of the system is substantiated. Thus, practical aspects are developed on the state machine construction and illustrated by two simple applications in the domain of chemical processes. r 2006 Elsevier Ltd. All rights reserved.	automaton;cluster analysis;complex systems;data acquisition;dynamical system;event (computing);fault detection and isolation;finite-state machine;genie;mind;online and offline;reactor (software);simulation;state space;steam;user interface	Tatiana Kempowsky-Hamon;Audine Subias;Joseph Aguilar-Martin	2006	Eng. Appl. of AI	10.1016/j.engappai.2005.12.012	fuzzy clustering;computer science;state space;artificial intelligence;dynamical system;machine learning;finite-state machine;data acquisition;situation analysis;virtual finite-state machine;state variable	AI	15.969587573760085	-15.675296510649906	37592
52ad3c75d58b776dd2edf36e7cbae9343f263e6d	evolutionary multiobjective design of an alternative energy supply system	investments;renewable energy sources;sensitivity analysis energy supply system renewable energy sources optimal design multiobjective optimization pareto optimality evolutionary algorithms;optimization batteries vectors aggregates photovoltaic systems investments;vectors;sensitivity analysis;aggregates;batteries;energy supply system;evolutionary algorithms;optimal design;multiobjective optimization;optimization;photovoltaic systems;pareto optimality	The purpose of alternative energy supply systems is to produce electrical energy at the location of its consumption, independently from the supply grid, and exploiting renewable energy sources, such as sunlight and wind. Rapidly changing conditions on energy markets and strengthening environmental requirements make alternative energy supply systems more and more favored. However, designing such systems is a hard optimization problem because of numerous decision variables, conflicting criteria, and complex evaluation of the candidate designs. There are reports on techno-economic optimization of alternative energy supply systems in the literature that search for optimal system configurations maximizing technical performance and minimizing the overall costs. The authors employ stochastic optimization methods integrated with numerical models of the energy supply systems, and handle multiobjective optimization problems in the singleobjective manner through the weighted sum approach or transformation of selected criteria into constraints. This paper describes a Pareto multiobjective approach to techno-economic optimization of an alternative energy supply system based on differential evolution for multiobjective optimization. The paper reviews the related work, introduces the applied methodology and the considered problem, describes the experimental setup and the performed numerical experiments, and reports on the optimization results.	approximation;computer simulation;design tool;differential evolution;evolutionary algorithm;experiment;level of detail;mathematical optimization;multi-objective optimization;numerical analysis;optimization problem;pareto efficiency;prototype;requirement;sensitivity and specificity;stochastic optimization;uninterruptible power supply;weight function	Bogdan Filipic;Ivan Lorencin	2012	2012 IEEE Congress on Evolutionary Computation	10.1109/CEC.2012.6256159	renewable energy;mathematical optimization;engineering optimization;computer science;optimal design;photovoltaic system;multi-objective optimization;evolutionary algorithm;mathematical economics;sensitivity analysis	AI	17.031802243440833	-3.5521746924275743	37636
3aea240a06a2148af07df603b7f1824016f48599	a new integrated fqfd approach for improving quality and reliability of solar drying systems		Saffron is the most expensive spice and is significantly valuable in non-oil export. Drying process of saffron is considered as a critical control point with major effects on quality and safety parameters. A suitable drying method covering standards and market requirements while it is costlty benefitial and saves energy is desirable. Solar drying could be introduced as an appropriate procedure in rural and collecting sites of saffron since major micorobial and chemical factors of saffron can be preserved and achieved by using a renewable energy source. So, a precise system taking advantage of management, engineering and food technology sciences could be developed. Since there was no published record of integrated methods of Analytical Hierarchy Process (AHP) and Fuzzy Quality Function Deployment (FQFD) applied to solar energy drying systems, in this paper, Fuzzy Quality Function Deployment as a quality management tool by emphasizing technical and customer requirements has been implemented in order to improve quality parameters, optimizing technological expenses and market expansion strategy. Subsequently, Analytical Hierarchy Process based on survey from customers and logical pair-wise comparison are employed to decrease costs and increase the efficiency and the effectiveness of economic indicators. Using the integrated approach of AHP and FQFD in solar drying systems in saffron industry will result in cost benefit, quality improvement, the customer satisfaction enhancement, and the increase in saffron exports.	analytical hierarchy;control point (mathematics);quality function deployment;requirement;spice 2;software deployment	Navid Akar;Hossein Lotfizadeh	2017	CoRR		renewable energy;analytic hierarchy process;reliability engineering;operations management;data mining;quality management;computer science;customer satisfaction;critical control point;quality function deployment;solar energy;economic indicator	EDA	7.829830993362306	-6.363340975087555	37676
080746395df11c2595c57d2221a15b43dca1eef7	analysis and forecasting on the relationship between port logistics capacity and coastal marine economic growth based on pknn in case of zhejiang province	the port logistics capacity;neural networks;neural nets;economic forecasting;marine economy in zhejiang province prior knowledge neural networks pknn the port logistics capacity;predictive models economics sea measurements logistics time series analysis neural networks;pknn port logistics capacity coastal marine economic growth coastal ocean economic development time series method multivariate method priori knowledge neural network zhejiang province;time series;marine economy in zhejiang province;logistics;time series analysis;time series economic forecasting logistics neural nets;predictive models;economics;prior knowledge neural networks pknn;sea measurements	A priori knowledge neural network is presented to model on the relationship between the port logistics capacity and coastal marine economic growth in Zhejiang province in China. Compared to other methods (such as general GM (1, 1) based time series model), the PKNN with multivariate and time series method has better performance. And, it is validated by predicting the level of coastal ocean economic development in Zhejiang from the year 2011 to 2020.	artificial neural network;logistics;network model;time series	Zhibin Li;Lingling Wang;Haichuan Lou	2012	2012 Eighth International Conference on Computational Intelligence and Security	10.1109/CIS.2012.62	computer science;machine learning;time series;operations research;artificial neural network;statistics	Robotics	8.420536572023869	-19.309487345951215	37684
241fc8afd33b7a02cf687ff3fc39f5eb6e46e478	a cooperative coevolutionary algorithm for design of platform-based mass customized products	mass customization;mass production;satisfiability;product variant;product platform;business model;computer experiment;platform product customization;cooperative coevolutionary algorithm;parametric optimization;evolutionary process;market segmentation;modular architecture;article;large scale problem	As a new business model, mass customization (MC) intends to enable enterprises to comply with customer requirements at mass production efficiencies. A widely advocated approach to implement MC is platform product customization (PPC). In this approach, a product variant is derived from a given product platform to satisfy customer requirements. Adaptive PPC is such a PPC mode in which the given product platform has a modular architecture where customization is achieved by swapping standard modules and/or scaling modular components to formulate multiple product variants according to market segments and customer requirements. Adaptive PPC optimization includes structural configuration and parametric optimization. This paper presents a new method, namely, a cooperative coevolutionary algorithm (CCEA), to solve the two interrelated problems of structural configuration and parametric optimization in adaptive PPC. The performance of the proposed algorithm is compared with other methods through a set of computational experiments. The results show that CCEA outperforms the existing hierarchical evolutionary approaches, especially for large-scale problems tested in the experiments. From the experiments, it is also noticed that CCEA is slow to converge at the beginning of evolutionary process. This initial slow convergence property of the method improves its searching capability and ensures a high quality solution. L. Li · G. Q. Huang (B) Department of Industrial and Manufacturing Systems Engineering, The University of Hong Kong, Hong Kong, People’s Republic of China e-mail: gqhuang@hku.hk S. T. Newman Department of Mechanical Engineering, University of Bath, Bath, UK	algorithm;computation;converge;display resolution;email;experiment;fitness function;genetic representation;heuristic (computer science);image scaling;industrial engineering;mathematical optimization;new product development;optimization problem;paging;population;requirement;scalability;systems engineering	Li Li;George Q. Huang;Stephen T. Newman	2008	J. Intelligent Manufacturing	10.1007/s10845-008-0137-x	business model;mathematical optimization;mass production;simulation;computer experiment;mass customization;systems engineering;engineering;artificial intelligence;marketing;machine learning;market segmentation;satisfiability	Robotics	17.59378681794192	-6.115350361077833	37691
f03d2eb61a0be5563929f06dcbe84451af50aaa1	discretization of optimal beamlet intensities in imrt: a binary integer programming approach	inverse planning;fluence map optimization;radiation therapy;combinatorial optimization	The intensity modulated radiation therapy (IMRT) treatment planning problem is usually divided into three smaller problems that are solved sequentially: the geometry problem, intensity problem, and realization problem. That division has the consequence of causing a plan quality deterioration arising from the transition between the intensity problem and the realization problem. Typically, on the beamlet-based approach, after the optimal beamlet intensities are determined, they are discretized over a range of values using a distance criterion (rounding). However, that decision criterion is not appropriate and we present empirical evidence that this can lead to a significant deterioration of the treatment plan quality regardless of the model used to tackle the intensity problem. We propose a combinatorial optimization approach and a probabilistic binary tabu search algorithm to enable an improved transition from optimized to delivery fluence maps in IMRT by minimizing the deterioration of the treatment plan quality and improving organ sparing at the same time. Four head and neck clinical examples were used to test the ability of the proposed formulation and resolution method to obtain improved plans compared to the usual rounding procedure. The results obtained present a clear improvement of the treatment plan quality both in terms of target coverage and also in terms of parotid sparing. © 2011 Elsevier Ltd. All rights reserved.	combinatorial optimization;discretization;eisenstein's criterion;image resolution;integer programming;linear programming;mathematical optimization;modulation;optimization problem;rounding;search algorithm;tabu search	Humberto Rocha;Joana Matos Dias;Brigida C. Ferreira;Maria do Carmo Lopes	2012	Mathematical and Computer Modelling	10.1016/j.mcm.2011.11.056	mathematical optimization;radiation therapy;combinatorial optimization;mathematics	AI	18.68687510642671	3.794494266054961	37709
1c005bd9d84ec042231e39aeff54544c0f4f9c01	interval-valued contractive fuzzy negations	electronic mail;lipschitz function;input variables;mathematics and statistics;standard zadeh negation interval valued contractive fuzzy negations lipschitz function;fuzzy sets upper bound length measurement fuzzy set theory electronic mail input variables;length measurement;fuzzy set theory;fuzzy sets;upper bound;standard zadeh negation;sets;interval valued contractive fuzzy negations;aggregation operators	In this work we consider the concept of contractive interval-valued fuzzy negation, as a negation such that it does not increase the length or amplitude of an interval. We relate this to the concept of Lipschitz function. In particular, we prove that the only strict (strong) contractive interval-valued fuzzy negation is the one generated from the standard (Zadeh's) negation.	contraction mapping;fuzzy set;vertical blanking interval	Benjamín R. C. Bedregal;Humberto Bustince;Javier Fernández;Glad Deschrijver;Radko Mesiar	2010	International Conference on Fuzzy Systems	10.1109/FUZZY.2010.5584635	fuzzy logic;mathematical analysis;discrete mathematics;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy number;mathematics;fuzzy set;fuzzy set operations	DB	-0.7345854362507103	-22.947095155405492	37737
6cf1126bf3fc6b715d6fe633bc8ca90678f38a60	an economic order quantity model with completely backordering and nondecreasing demand under two-level trade credit		In the traditional inventory system, it was implicitly assumed that the buyer pays to the seller as soon as he receives the items. In today’s competitive industry, however, the seller usually offers the buyer a delay period to settle the account of the goods. Not only the seller but also the buyer may apply trade credit as a strategic tool to stimulate his customers’ demands. This paper investigates the effects of the latter policy, two-level trade credit, on a retailer’s optimal ordering decisions within the economic order quantity framework and allowable shortages. Unlike most of the previous studies, the demand function of the customers is considered to increase with time.The objective of the retailer’s inventory model is to maximize the profit. The replenishment decisions optimally are obtained using genetic algorithm. Two special cases of the proposed model are discussed and the impacts of parameters on the decision variables are finally investigated. Numerical examples demonstrate the profitability of the developed two-level supply chain with backorder.	decision theory;economic order quantity;genetic algorithm;inventory theory;numerical method;scrum (software development)	Zohreh Molamohamadi;Rahman Arshizadeh;Napsiah B. Ismail;Amir Azizi	2014	ADS	10.1155/2014/340135	economics;operations management;credit note;microeconomics;commerce	AI	1.769084243087848	-5.145987309045516	37745
e8dfe67089edaffe99366965a7b52972d6e04e21	the new extension of topsis method for multiple criteria decision making with hesitant pythagorean fuzzy sets		Abstract Pythagorean fuzzy sets (PFSs) as a new generalization of fuzzy sets (FSs) can handle uncertain information more flexibly in the process of decision making. In our real life, we also may encounter a hesitant fuzzy environment. In view of the effective tool of hesitant fuzzy sets (HFSs) for expressing the hesitant situation, we introduce HFSs into PFSs and extend the existing research work of PFSs. Concretely speaking, this paper considers that the membership degree and the non-membership degree of PFSs are expressed as hesitant fuzzy elements. First, we propose a new concept of hesitant Pythagorean fuzzy sets (HPFSs) by combining PFSs with HFSs. It provides a new semantic interpretation for our evaluation. Meanwhile, the properties and the operators of HPFSs are studied in detail. For the sake of application, we focus on investigating the normalization method and the distance measures of HPFSs in advance. Then, we explore the application of HPFSs to multi-criteria decision making (MCDM) by employing the technique for order preference by similarity to ideal solution (TOPSIS) method. A new extension of TOPSIS method is further designed in the context of MCDM with HPFSs. Finally, an example of the energy project selection is presented to elaborate on the performance of our approach.	fuzzy set	Decui Liang;Zeshui Xu	2017	Appl. Soft Comput.	10.1016/j.asoc.2017.06.034	multiple-criteria decision analysis;operator (computer programming);fuzzy logic;mathematics;fuzzy set operations;artificial intelligence;machine learning;fuzzy set;type-2 fuzzy sets and systems;defuzzification;topsis	Robotics	-3.2818394978677907	-21.453622927326332	37752
541a46ae93b20edbb52eddda05db7aec1e5573ab	the multi-objective constrained assignment problem	military personnel;resource scheduling;assignment problem;generalized assignment problem;pareto front;moea;assignment problems;operations research;multi objective genetic algorithm;radio frequency;scheduling	There has been much research done in the area of assignment problems. These problems span many different disciplines and researchers from an array of different fields are exploring ways to solve them. As such, the assignment problem has been modified over the years to meet specific needs in different application areas. But how do the mathematical formulations of the various assignment problems compare with one another? By knowing the similarities and differences of these problems, a researcher can best fit a particular model to the problem he is attempting to solve. In [2], some of the basic differences between the various models are outlined and a baseline model that researchers can use as a starting point to build their specific model is presented. This abstract briefly describes the constrained assignment problem (CAP) and a specific problem called the Airman Assignment Problem (AAP) and then discusses the results of experiments using the NSGA-II.	aap;assignment problem;baseline (configuration management);curve fitting;experiment;multi-objective optimization	Mark P. Kleeman;Gary B. Lamont	2006		10.1145/1143997.1144127	mathematical optimization;linear bottleneck assignment problem;computer science;generalized assignment problem;multi-objective optimization;deadline-monotonic scheduling;assignment problem;weapon target assignment problem;operations research;scheduling;radio frequency;quadratic assignment problem	PL	21.15989679082367	1.1843383035675799	37759
d102721dd5f80f334735378fbf2539f2c290de1e	a novel neuro takagi sugeno modeling approach for an activated sludge reactor	topology;asm1 model artificial neural networks takagi sugeno activated sludge reactor;biological system modeling;sludge treatment approximation theory chemical reactors environmental degradation fuzzy control interpolation linear parameter varying systems multilayer perceptrons neurocontrollers;biological system modeling mathematical model nitrogen inductors data models topology water pollution;inductors;mathematical model;water pollution;approximation theory takagi sugeno modelling ts fuzzy model activated sludge reactor asm1 model artificial neural network ann multilayer perceptron mlp biological degradation process input output space interpolation nonlinear weighting function quasilpv approach;data models;nitrogen	This paper investigates the use of Artificial Neural Networks (ANN), more precisely Multi-Layer Perceptrons (MLP), as an alternative to the weighting function μ in a Multi-Model approach type Takagi Sugeno (TS) for an Activated Sludge Reactor. The reduced bio-reactor activated sludge ASM1 model, which describes the biological degradation of an activated sludge reactor, is designed based on several simplifications, as a TS fuzzy model, which structure is based on a set of linear sub models, covering the process input-output space, interpolated by a nonlinear weighting function. Using data gathered from the set of sub model outputs used as inputs of a nonlinear multi-step ANN predictor. The function μ, earlier obtained using a Quasi-LPV approach, is then approximated using the designed MLP where the training and validation of the latter is performed and results are compared with the original approach with and without input and parametric disturbances.	approximation algorithm;artificial neural network;british informatics olympiad;computation;elegant degradation;fuzzy logic;interpolation;kerrison predictor;memory-level parallelism;multilayer perceptron;nonlinear system;performance;quad flat no-leads package;reactor (software);sugeno integral;time complexity;weight function	Matoug Lamia;Khadir M. Tarek	2016	2016 International Conference on Control, Decision and Information Technologies (CoDIT)	10.1109/CoDIT.2016.7593550	control engineering;engineering;machine learning;control theory	Robotics	12.332273714387906	-20.862834388536747	37819
93dc1598ad8387c316e2d957eec2140e28a64d6f	inventory and internal logistics management as critical factors affecting the supply chain performances		This paper focuses on the inventory and internal logistics management problem within a specific Supply Chain (SC) node (a Distribution Centre (DC)). The objective is twofold: to monitor the performance of different inventory control policies under distinct operative scenarios and to reduce the Internal Logistic Costs (ILCs) by investigating the effect of some critical parameters (i.e., the number of incoming/outgoing trucks from suppliers/to retailers, the number of forklifts and lift trucks, etc.) for increasing the service level provided to final retailers and allocating internal resources efficiently. To this end, a simulation model of a real DC is implemented.	logistics;performance	Duilio Curcio;Francesco Longo	2009	IJSPM	10.1504/IJSPM.2009.032591	supply chain risk management;logistics;inventory theory;supply chain management;service level;resource allocation;service management;systems engineering;marketing;operations management;supply chain;design of experiments	ML	5.852219300438421	-6.6534668053073505	37824
80e1640b7628a931ecf126af90a4291be9da5f47	improved teaching-learning-based optimization algorithm with group learning			algorithm;mathematical optimization	Ming Li;Honglu Ma;Baijie Gu	2016	Journal of Intelligent and Fuzzy Systems	10.3233/JIFS-169049		Robotics	24.432181586635252	-5.221085984130428	37922
52b13480e6c9f1d77ebd9155c03b50b358d510a8	feasibility study on hysol csp	firm power;feasibility analysis;ccgt;hybridization;hysol;csp;steam turbine;storage;ocgt	Concentrating Solar Power (CSP) plants utilize thermal conversion of direct solar irradiation. A trough or tower configuration focuses solar radiation and heats up oil or molten salt that subsequently in high temperature heat exchangers generate steam for power generation. High temperature molten salt can be stored and the stored heat can thus increase the load factor and the usability for a CSP plant, e.g. to cover evening peak demand. In the HYSOL concept (HYbrid SOLar) such configuration is extended further to include a gas turbine fueled by upgraded biogas or natural gas. The optimized integrated HYSOL concept, therefore, becomes a fully dispatchable (offering firm power) and fully renewable energy source (RES) based power supply alternative, offering CO2-free electricity in regions with sufficient solar resources. The economic feasibility of HYSOL configurations is addressed in this paper. The analysis is performed from a socioand privateeconomic perspective. In the socio-economic analysis, the CO2 free HYSOL alternative is discussed relative to conventional reference firm power generation technologies. In particular, the HYSOL performance relative to new power plants based on natural gas (NG) such as open cycle or combined cycle gas turbines (OCGT or CCGT) are in focus. In the corporate-economic analysis the focus is on the uncertain technical and economic parameters. The core of the analyses is based on the LCOE economic indicator. In the corporate economic analysis, NPV and IRR are furthermore used to assess the feasibility. The feasibility of renewable based HYSOL power plant configurations attuned to specific electricity consumption patterns in selected regions with promising solar energy potentials are discussed.	economic complexity index;hash table;infinite impulse response;power supply;steam;usability	Lars Henrik Nielsen;Klaus Skytte;Cristian Hernán Cabrera Pérez;Eduardo Cerrajero García;Diego Lopez Barrio;Lucía González Cuadrado;Alberto Rodríguez Rocha	2016		10.1016/j.procs.2016.04.238	feasibility study;orbital hybridisation;combined cycle;computer science;communicating sequential processes;steam turbine	AI	5.80441635798139	1.9844316055020774	37928
f5c3b274b42fe84f1f032195efdfd84414997b19	a revisit to queueing-inventory system with positive service time	lead time;stochastic decomposition;queueing inventory;positive service time	A queueing-inventory system, with the item given with probability γ to a customer at his service completion epoch, is considered in this paper. Two control policies, (s,Q) and (s,S) are discussed. In both cases we obtain the joint distribution of the number of customers and the number of items in the inventory as the product of their marginals under the assumption that customers do not join when inventory level is zero. Optimization problems associated with both models are investigated and the optimal pairs (s,S) and (s,Q) and the corresponding expected minimum costs are obtained. Further we investigate numerically an expression for per unit time cost as a function of γ. This function exhibit convexity property. A comparison with Schwarz et al. (Queueing Syst. 54:55–78, 2006 ) is provided. The case of arbitrarily distributed service time is briefly indicated. Copyright Springer Science+Business Media New York 2015		Achyutha Krishnamoorthy;Ramamoorthy Manikandan;B. Lakshmy	2015	Annals OR	10.1007/s10479-013-1437-x	mathematical optimization;simulation;computer science;operations management;operations research;statistics	Theory	3.4185358197097644	-3.1925569069634196	37952
11693b65ccf90700fec0f99618c7f4720a522ab1	variable scaling for time series prediction: application to the estsp'07 and the nn3 forecasting competitions	least squares approximations;mathematics computing;support vector machines;nn3 forecasting competition;time series;variable selection;estsp 07 competition dataset time series prediction nn3 forecasting competition direct prediction methodology recursive methodology least square support vector machine k nn approximator;k nn approximator;least square support vector machine;time series least squares approximations mathematics computing support vector machines;least squares approximation economic forecasting predictive models input variables load forecasting support vector machines stock markets uncertainty testing yttrium;direct prediction methodology;estsp 07 competition dataset;recursive methodology;time series prediction;least squares support vector machine	In this paper, variable selection and variable scaling are used in order to select the best regressor for the problem of time series prediction. Direct prediction methodology is used instead of the classic recursive methodology. Least Squares Support Vector Machines (LS-SVM) and K-NN approximator are used in order to avoid local minimal in the training phase of the model. The global methodology is applied to the ESTSP'07 competition dataset and the dataset B of the NN3 Forecasting Competition.	feature selection;image scaling;k-nearest neighbors algorithm;least squares support vector machine;nist hash function competition;recursion;scalability;time series	Amaury Lendasse;Elia Liitiäinen	2007	2007 International Joint Conference on Neural Networks	10.1109/IJCNN.2007.4371405	econometrics;computer science;machine learning;time series;feature selection;statistics	ML	8.077637372888981	-20.511950050057216	37958
c8dd53bb51e0d1972fde43f8b3be5a8e22bd6e07	distributed unidirectional and bidirectional heuristic search: algorithm design and empirical assessment	distributed system;evaluation performance;cluster computing;systeme reparti;algoritmo busqueda;performance evaluation;racimo calculadura;algorithme recherche;evaluacion prestacion;heuristic method;search algorithm;distributed heuristic search;distributed computing;search method;metodo heuristico;heuristic search;grappe calculateur;performance improvement;sistema repartido;calculo repartido;methode heuristique;calcul reparti;algorithm design	Since its introduction three decades ago, bidirectional heuristic search did not deliver the expected performance improvement over unidirectional search methods. The problem of search frontiers passing each other is a widely accepted conjecture led to amendments to steer the search using computationally demanding heuristics. The computation cost associated with front-to-front evaluations crippled further investigation and hence bidirectional search was long neglected. However, recent findings demonstrate that the initial conjecture is wrong since the major search effort is spent after the frontiers have already met [7]. In this paper we reconsider bidirectional search by proposing a new generic approach based on cluster computing. The proposed approach is then evaluated and compared with its unidirectional counterparts. The obtained results reveal that cluster computing is a viable approach for distributed heuristic search. Particularly, clustered bidirectional search is capable of solving problems beyond unidirectional search capabilities and in the same time outperforms unidirectional approaches in terms of memory space and execution time.	a* search algorithm;algorithm design;bidirectional search;computation;computer cluster;dspace;heuristic (computer science);run time (program lifecycle phase)	Abdel Elah Al-Ayyoub	2005	The Journal of Supercomputing	10.1007/s11227-005-0165-7	beam search;algorithm design;bidirectional search;simulation;heuristic;computer cluster;computer science;artificial intelligence;distributed computing;incremental heuristic search;iterative deepening depth-first search;best-first search;algorithm;search algorithm	AI	21.993429190776716	3.832128132377786	37973
7872e7c0bbd20221d9f0311e9de565323c597386	mini-max regret strategy for robust capacity expansion decisions in semiconductor manufacturing	capacity planning;mini max regret;modeling and analysis for semiconductor manufacturing;manufacturing strategy	Semiconductor manufacturing is capital intensive and the capacity utilization significantly affects the capital effectiveness and profitability of semiconductor manufacturing companies. Semiconductor companies have to rely on various demand forecasts of the products fabricated by different technologies as a basis to determine capital investments for capacity expansion plans that require long-lead time. Moving into the consumer electronics era, semiconductor products have increasingly shortened product life cycles with demand fluctuation. However, given the involved uncertainties and long lead-time for capacity expansion, the capacity expansion decisions of semiconductor manufacturing companies are under both the risks of capacity shortage and surplus along with the time. This study aims to propose a mini---max regret strategy for capacity planning under demand uncertainty to improve capacity utilization and capital effectiveness in semiconductor manufacturing. This model considers each of possible outcomes of the multi-period demand forecasts from sales and marketing departments to generate robust capacity expansion plan to minimize the maximum regrets of capacity oversupply and shortage. An empirical study was conducted in a leading semiconductor company in Taiwan for the validation. The results have shown robustness and practical viability of the proposed mini---max strategy for capacity expansion planning under demand uncertainties.	regret (decision theory);semiconductor device fabrication	Chen-Fu Chien;Jia-Nian Zheng	2012	J. Intelligent Manufacturing	10.1007/s10845-011-0561-1	actuarial science;marketing	ML	3.759934765868671	-7.215020260062613	38058
0d26f93e043cc4f2705c105881ef9baed4842c66	is feedforward learning more efficient than feedback learning in smart meters of electricity consumption?		The most popular way to improve consumers’ control over their electricity cost is by providing frequent and detailed feedback with “in-home displays” (IHD). In this study, we examined alternative ways to train experimental participants to control and optimize their use of electricity by “feedforward” training to map energy consuming behaviors to costs. The participants were trained in one of four experimental conditions, one feedback (“IHD”) and three feedforward conditions before they had to control the electricity consumption in a simulated household. Results showed that one of the feedforward conditions produced somewhat higher utility and as good or better satisfaction of a monthly budget than the feedback training condition, despite never receiving any feedback about the monthly cost, but the generalization to a new budget constraint proved to be slightly poorer. Introduction The use of so-called “smart electricity meters” is rapidly becoming common. It has been estimated that within the European Union alone some 51 billion euro is being invested in smart meters (Faruqui, Harris, & Hledik, 2009). In many countries, household energy consumption is still billed once a month, but smart meters can offer feedback that is detailed and more frequent with so called In Home Displays (IHDs). Intuitively, the latter kind of feedback system seems more beneficial, and, indeed, many early studies suggested energy reductions up to 15%. However, more recent studies point at consumption reductions at 2-4%, few of them being significant (Klopfert & Wallenborn, 2011). In the present study, we focus at in-home displays (IHDs), which only display the electrical consumption at different time intervals, and, unlike smart meters, they do not have a two-way communication with the central system. In a previous laboratory experiment (Guath, Millroth, Elwin, & Juslin, 2012), we investigated how feedback about electricity consumption is best presented to electricity consumers in order to control and optimize their use of electricity. To measure a participant’s energy efficiency in an experimentally controlled environment, the participants took on the role of an inhabitant in a simulated household, performing different types of energy consuming behaviors within a given budget (Figure 1). The goal of decreasing electricity consumption is often emphasized, but the participant’s task is actually an optimization problem that requires an appropriate balance between the cost of the electricity consumed and the benefit or utility obtained. The problem is illustrated in Figure 2, where the utility of electricity consumption is plotted against cost. The maximum utility obtainable at a given cost, assumed to be a decelerating function of the cost, is illustrated by the curve in Figure 2. The hypothetical utility obtained at a cost by a consumer is illustrated with a dot. The task is to move closer to the line for “maximal utility”, however, this is associated with two constraints: achieving sufficient utility to make life bearable and not surpassing a constrained budget. Guath et al. (2012) showed that in a deterministic system, frequent and detailed feedback was advantageous, but in probabilistic system, feedback aggregated over time was better, because it filtered out random noise. The Present Study In the present study, we wanted to evaluate if the same improvement could be obtained by feedforward training, rather than feedback training (as in most IHDs), hence, minimizing the negative effects from feedback interventions as conceptualized in Kluger and DeNisi’s (1996) study. Specifically, we wanted to avoid the decrease of effectiveness when attention is moved away from the task to the self, thus, making the effects of the training short-term. Another motive was to make the mapping task more flexible, not being dependent on the simulated household (Figure 1). Detailed and frequent feedback (an IHD) was compared to three feedforward conditions. Feedforward is defined as a process where knowledge is used to act directly to control the system, thus anticipating the changes that will occur (Basso & Olivetti Belardinelli, 2006). In the present task, partici-	experiment;feed forward (control);feedback;feedforward neural network;harris affine region detector;inferring horizontal gene transfer;mathematical optimization;maximal set;noise (electronics);olivetti envision;optimization problem;smart card;smart meter;utility	Mona Guath;Peter Juslin;Philip Millroth	2013				HCI	1.253079086533936	3.5712709753398855	38077
68655b8d91ac782a65b4cfb3d57f60b910f4cfed	controlling multi-reservoir systems	policy;markov;reservoir control;processes;hd28 management industrial management;optimal policy;optimal;markov process;control;markov processes;reservoir	The paper extends the results of the form of the optimal policy for a hydroelectric reservoir problem from the one-reservoir case to multi-reservoir cases. The importance of these new results in practice is that they allow more efficient solution algorithms to be developed. Since multi-reservoir problems are extremely difficult to solve, such algorithms are of great value.		Thomas W. Archibald;C. S. Buchanan;Lyn C. Thomas;K. I. M. McKinnon	2001	European Journal of Operational Research	10.1016/S0377-2217(99)00450-6	mathematical optimization;operations management;mathematics;management science;markov process;statistics	Robotics	4.5171913317226915	-1.9028159562332045	38150
c6005b2bf776cc45538200cc39ba0202661f3060	threat evaluation of enemy air fighters via neural network-based markov chain modeling		Threat evaluation (TE) is a process used to assess the threat values (TVs) of air-breathing threats (ABTs), such as air fighters, that are approaching defended assets (DAs). This study proposes an automatic method for conducting TE using radar information when ABTs infiltrate into territory where DAs are located. The method consists of target asset (TA) prediction and TE. We divide a friendly territory into discrete cells based on the effective range of anti-aircraft missiles. The TA prediction identifies the TA of each ABT by predicting the ABT’s movement through cells in the territory via a Markov chain, and the cell transition is modeled by neural networks. We calculate the TVs of the ABTs based on the TA prediction results. A simulation-based experiment revealed that the proposed method outperformed TE based on the closest point of approach or the radial speed vector methods. © 2016 Elsevier B.V. All rights reserved.	adobe air;artificial neural network;markov chain;radar;radial (radio);simulation;test engineer	Hoyeop Lee;Byeong Ju Choi;Chang Ouk Kim;Jin Soo Kim;Ji Eun Kim	2017	Knowl.-Based Syst.	10.1016/j.knosys.2016.10.032	simulation;telecommunications;artificial intelligence;computer security	AI	11.253472230130543	-16.649797597053034	38218
2c225c5e1be37bcc1de1c3efeb6d05c713a9c435	vrp with minimum delivery latency using linear programming		Vehicle Routing Problems (VRPs) concern allocating multiple vehicles to service requests with routes that start and end at a depot, visit all requests, and minimize some operational cost. Many UAV situations can be modeled in a manner that represents tasks to be accomplished by service requests and UAVs as agents that fulfill these requests. Often, communications are restricted by available power, data rate, noise margins, and cost. These constraints cause information gathered at request locations to be delivered only when the UAV returns to the “depot.” In other words, UAVs must be allocated to services requests and then deliver information gathered at the request location to a communication range to be able to transmit information back to a command and control HQ. Because of limited communication ranges which result from limited power onboard small UAVs, information is not delivered until the UAV returns to the depot. Research on similar problems show some related work on minimizing the latency of the pickup of requests but not the delivery. This work attempts to study the VRP with minimum delivery latency and formulate a Linear Programming (LP) solution. The objective function is shown to be nonlinear, but it can be linearized at a cost: many new variables are added to linearize it. This in turn, makes the problem intractable, and only solutions to small problems are obtained. Nomenclature N = Number of Requests M = Number of Vehicles R = Number of Tours i, j = Request Indices r = Trip Index k = Vehicle Index = Arrival time of request i = Pickup time of request i = Delivery time of request i	interrupt latency;linear programming;loss function;noise margin;nonlinear system;optimization problem;uncompressed video;unmanned aerial vehicle;vehicle routing problem	Chelsea Sabo;Manish Kumar;Kelly Cohen;Derek Kingston	2012		10.2514/6.2012-2561	real-time computing;simulation;engineering;operations management	Theory	13.749826036796852	1.0104849768432842	38245
7152179045d48c7d449db4f19b7edcc481aa3aaa	two meta-heuristic algorithms for two-echelon location-routing problem with vehicle fleet capacity and maximum route length constraints	location-routing;location;routing;hybrid genetic algorithm;simulated annealing	In the present research, a two-echelon location-routing problem with constraints of vehicle fleet capacity and maximum route length is considered. The problem’s objective is to determine the location and number of two types of capacitated facilities, the size of two different vehicle fleets, and the related routes on each echelon. Two algorithms hybrid genetic algorithm and simulated annealing are then applied to solve the problem. Results of numerical experiments show that the applied hybrid genetic and simulated annealing algorithms are much more effective than the solutions of the solved examples by the software LINGO. Finally, solutions of simulated annealing and hybrid genetic algorithms were compared with each other.	experiment;genetic algorithm;hercules graphics card;heuristic;lingo (programming language);memetic algorithm;metaheuristic;np-hardness;nonlinear system;numerical analysis;routing;row echelon form;simulated annealing;strong np-completeness	Vahid Majazi Dalfard;Mojtaba Kaveh;Nasim Ekram Nosratian	2012	Neural Computing and Applications	10.1007/s00521-012-1190-0	mathematical optimization;simulation	AI	16.91577322383134	1.6329300472633914	38250
94305d65ee976e28e3ef34023df9685bae67fc3f	distances, norms and error propagation in idempotent semirings		Error propagation and perturbation theory are well-investigated areas of mathematics dealing with the influence of errors and perturbations of input quantities on output quantities. However, these methods are restricted to quantities relying on real numbers under traditional addition and multiplication. We aim to present first steps of an analogous theory on idempotent semirings, so we define distances and norms on idempotent semirings and matrices over them. These concepts are used to derive inequalities characterizing the influence of changes in the input quantities on the output quantities of some often used semiring expressions.	idempotence;propagation of uncertainty;schema (genetic algorithms)	Roland Glück	2018		10.1007/978-3-030-02149-8_4	idempotence;discrete mathematics;inequality;real number;semiring;expression (mathematics);matrix (mathematics);multiplication;mathematics;areas of mathematics	Vision	1.490214069823679	-21.748239764156907	38251
4c86176c5a01a43415b037db844b169932567d04	optimizing product launches in the presence of strategic consumers	new product introduction;new product development	A technology firm launches newer generations of a given product over time. At any moment, the firm decides whether to release a new version of the product that captures the current technology level at the expense of a fixed launch cost. Consumers are forward-looking and purchase newer models only when it maximizes their own future discounted surpluses. We start by assuming that consumers have a common valuation for the product and consider two product launch settings. In the first setting, the firm does not announce future release technologies and the equilibrium of the game is to release new versions cyclically with a constant level of technology improvement that is optimal for the firm. In the second setting, the firm is able to precommit to a schedule of technology releases and the optimal policy generally consists of alternating minor and major technology launch cycles. We verify that the difference in profits between the commitment and no-commitment scenarios can be significant, varying from 4% to 12%. Finally, we generalize our model to allow for multiple customer classes with different valuations for the product, demonstrating how to compute equilibria in this case and numerically deriving insights for different market compositions.	numerical analysis;optimizing compiler;purchasing;usb on-the-go;value (ethics)	Ilan Lobel;Jigar Patel;Gustavo J. Vulcano;Jiawei Zhang	2016	Management Science	10.1287/mnsc.2015.2189	economics;marketing;operations management;microeconomics;management;new product development;commerce	Theory	-0.34936421263244083	-5.830223425147526	38342
6f6ceb749b67156084b8c2daca8025dbfb6c7d69	meta-level control of anytime algorithms with online performance prediction		Anytime algorithms enable intelligent systems to trade computation time with solution quality. To exploit this crucial ability in real-time decisionmaking, the system must decide when to interrupt the anytime algorithm and act on the current solution. Existing meta-level control techniques, however, address this problem by relying on significant offline work that diminishes their practical utility and accuracy. We formally introduce an online performance prediction framework that enables metalevel control to adapt to each instance of a problem without any preprocessing. Using this framework, we then present a meta-level control technique and two stopping conditions. Finally, we show that our approach outperforms existing techniques that require substantial offline work. The result is efficient nonmyopic meta-level control that reduces the overhead and increases the benefits of using anytime algorithms in intelligent systems.	anytime algorithm;benchmark (computing);computation;mobile robot;on the fly;online and offline;overhead (computing);performance prediction;preprocessor;real-time transcription;time complexity	Justin Svegliato;Kyle Hollins Wray;Shlomo Zilberstein	2018		10.24963/ijcai.2018/208	performance prediction;machine learning;artificial intelligence;computer science	AI	19.50853314341718	-11.600321887240725	38368
9674a50c52f1cea8575ef58446b4b39a80bc1df2	a multiscaling test of causality effects among international stock markets	international stock markets;stock market;causal effect;wavelet decomposition;causality effect;school of accounting;respubid18692;multiresolution analysis;granger causality test;international	This paper investigates the causal links between the world's largest stock markets, namely the U.S. market, the U.K. market, and the Japanese market, over various time horizons. The major innovation of this paper is to apply the wavelet multiscaling method into the study of the international stock market linkage. The main empirical results from the wavelet multiscaling method support three conclusions. First, there is a significant bi-directional causal effect among the three markets when viewed in terms of high frequency behavior. Second, no consistent causality effects are observed in the frequency of 16 days, which we can view approximately as monthly data, between all three major stock markets. This empirical finding implies that the three major stock markets are strongly connected in their daily or weekly returns, while in the monthly returns level, there does not exist a consistent causality effect. Third, the empirical evidence we provide supports the findings of Ramsey & Lampart (1998) and Gencay et. al. (2002) that there is not one global causality relation prevailing over all time scales.	causality	Cheng Zhang;Francis In;Alan Farley	2004	Neural Parallel & Scientific Comp.		financial economics;stock market bubble;economics;international economics;commerce	Arch	3.479984809959989	-13.785753903213774	38387
58e553da3f5165240005ff1286ec506e8bb95530	analysis of heuristic techniques for controlling contagion		Many strategic actions carry a ‘contagious’ component beyond the immediate locale of the effort itself. Viral marketing and peacekeeping operations have both been observed to have a spreading effect. In this work, we use counterinsurgency as our illustrative domain. Defined as the effort to block the spread of support for an insurgency, such operations lack the manpower to defend the entire population and must focus on the opinions of a subset of local leaders. As past researchers of security resource allocation have done, we propose using game theory to develop such policies and model the interconnected network of leaders as a graph. Unlike this past work in security games, actions in these domains possess a probabilistic, non-local impact. To address this new class of security games, recent research has used novel heuristic oracles in a double oracle formulation to generate mixed strategies. However, these heuristic oracles were evaluated only on runtime and quality scaling with the graph size. Given the complexity of the problem, numerous other problem features and metrics must be considered to better inform practical application of such techniques. Thus, this work provides a thorough experimental analysis including variations of the contagion probability average and standard deviation. We extend the previous analysis to also examine the size of the action set constructed in the algorithms and the final mixed strategies themselves. Our results indicate that game instances featuring smaller graphs and low contagion probabilities converge slowly while games with larger graphs and medium contagion probabilities converge most quickly.	algorithm;approximation;blocking (computing);clustering coefficient;converge;degree distribution;experiment;game theory;general-purpose modeling;graph (discrete mathematics);heuristic;heuristic (computer science);image scaling;iteration;population;social network	Jason Tsai;Nicholas Weller;Milind Tambe	2012			game theory;computer science;oracle;standard deviation;probabilistic logic;viral marketing;heuristic;population;operations research;resource allocation	AI	5.839250181617363	-11.236636772791329	38395
ec409bcee39c11f994ae5709763dcc1eeaea267e	a big data based deep learning approach for vehicle speed prediction		Vehicle speed prediction plays an important role in Data-Driven Intelligent Transportation System (D2ITS) and electric vehicle energy management. Accurately predicting vehicle speed for an individual trip is a challenging topic because vehicle speed is subjected to various factors such as route types, route curvature, driver behavior, weather and traffic condition. A big data based deep learning vehicle speed prediction algorithm featuring big data analytics and Adaptive Neuro-Fuzzy Inference System (ANFIS) is presented in this paper. Big data analytics examines copious amounts of speed related data to identify the pattern and correlation between input factors and vehicle speed. ANFIS model is constructed and configured, based on the analytics. The proposed speed prediction algorithm is trained and evaluated using the actual driving data collected by one test driver. Experiment results indicate that the proposed algorithm is capable of accurately predicting vehicle speed for both freeway and urban traffic networks.	adaptive neuro fuzzy inference system;algorithm;big data;deep learning;freeway;neuro-fuzzy;u.s. route shield	Zheyuan Cheng;Mo-Yuen Chow;Daebong Jung;Jinyong Jeon	2017	2017 IEEE 26th International Symposium on Industrial Electronics (ISIE)	10.1109/ISIE.2017.8001278	deep learning;big data;adaptive neuro fuzzy inference system;electric vehicle;engineering;machine learning;inference;intelligent transportation system;artificial intelligence;analytics	Robotics	9.534895837921706	-19.547522091513137	38498
490229c235427d8682fd1b10b0dcb2b2a7cc2289	an interactive satisficing method for solving multiobjective mixed fuzzy-stochastic programming problems	optimisation sous contrainte;constrained optimization;multicriteria analysis;multiobjective programming;programmation multiobjectif;controle stochastique;fuzzy programming;fuzzy goals;fuzzy stochastic programming;stochastic control mathematics;programmation stochastique;interactive method;prise decision;decision maker;teoria decision;interactive methods;optimizacion con restriccion;mathematical programming;theorie decision;multiobjective mixed fuzzy stochastic programming;decision theory;uncertainty relation;fuzzification;control estocastico;analisis multicriterio;analyse multicritere;programmation floue;programmation stochastique floue;economia matematica;contrainte chance;stochastic programming;toma decision;programmation mathematique;fuzzified chance constraints;methode interactive;programacion estocastica;mathematical economy;programacion matematica;economie mathematique;programacion difusa;chance constraint;programacion multiobjetivo	In this paper an interactive satis cing method (named PRELIME) is proposed for solving mixed fuzzy-stochastic multiobjective programming problems. The proposed method can be used to solve linear as well as a class of nonlinear multiobjective problems in mixed fuzzy-stochastic environment wherein various kinds of uncertainties related to fuzziness and=or randomness are present. In this method a fuzzifying approach has been proposed which treats the stochastic objectives on the basis of extended E-model and the stochastic constraints as fuzzi ed chance constraints. As a result of this the stochastic objectives as well as the stochastic constraints are treated in a fuzzy environment providing an opportunity to the decision maker (DM) to trade-o fuzzy as well as stochastic objectives and constraints during the interactive process of search for a satis cing solution. The use of the method has been illustrated on some test examples taken from literature. c © 2001 Elsevier Science B.V. All rights reserved.	decision theory;multi-objective optimization;nonlinear system;randomness;stochastic programming	Chandra Mohan;Hung T. Nguyen	2001	Fuzzy Sets and Systems	10.1016/S0165-0114(98)00269-3	stochastic programming;decision-making;mathematical optimization;constrained optimization;decision theory;artificial intelligence;mathematics;fuzzy set;mathematical economics	AI	0.2107269497083061	-16.49376754297563	38501
78fd4a3bf91e38c186ab4f0d714bd330a6261f5d	assessing the potential of data-driven models for estimation of long-term monthly temperatures		Abstract Having information on air temperature components consisting minimum (T min ), maximum (T max ) and mean (T) temperatures plays a crucial role in various aspects of agriculture such as agricultural meteorology, soil science, agronomy, etc. The present study explores the performance of four data-driven models including artificial neural networks (ANN), adaptive neuro-fuzzy inference system (ANFIS), support vector machine (SVM) and multivariate adaptive regression splines (MARS) for estimation of long-term monthly T min , T max and T. For this purpose, the long-term monthly temperatures of 50 stations all over Iran were used. The data of 35 and 15 stations were utilized to train and test the models, respectively. To feed the models, the geographical information (latitude, longitude, altitude) and periodicity component (the number of months) were employed as input parameters. The obtained results demonstrated that the long-term monthly temperatures of the studied regions can be estimated as a function of geographical information and periodicity component. Comparing the overall performance of the models at training stage revealed that the ANN outperformed the other models for estimating the long-term monthly T min , T max and T. Thatu0027s while the SVM, ANN and ANFIS had superiority over the others at testing stage for estimation of the long-term monthly T min , T max and T, respectively. Furthermore, the MARS model presented the weakest performance for estimating the long-term monthly temperatures at both training and testing stages.		Saeid Mehdizadeh	2018	Computers and Electronics in Agriculture	10.1016/j.compag.2017.11.038	statistics;engineering;control engineering;artificial neural network;multivariate adaptive regression splines;adaptive neuro fuzzy inference system;support vector machine;altitude	ML	9.754268960787938	-18.600175186505414	38522
b646ca1ac0a0c9eadd274e2003375953aeb9af6b	flashover risk prediction on polluted insulators strings of high voltage transmission lines	preventive maintenance;fuzzy reasoning;neural networks;neural nets;risk analysis;sensor network;flashover;power engineering computing;sensor networks;preventive maintenance flashover risk prediction polluted insulator strings high voltage transmission lines partial discharges high voltage insulator surface partial discharge sensor network fuzzy inference system risk level extraction risk level signal neural network transmission tower;partial discharges poles and towers flashover insulators satellites sensor systems optical fiber sensors;high voltage;partial discharge;fuzzy inference system;power overhead lines;partial discharges;insulators;risk analysis flashover fuzzy reasoning insulators neural nets partial discharges power engineering computing power overhead lines preventive maintenance;sensor networks partial discharges insulators neural networks;neural network;transmission line	Partial discharges on high voltage insulator surfaces are directly related with the deposition of pollution over the insulators. A complete partial discharge sensor network was previously developed and has been in operation for approximately one year. A fuzzy inference system was applied on the results to extract a risk level. In this paper the risk level signal was applied as a neural network input to predict the flashover risk ten days ahead. Two sensors installed on transmission towers submitted to match environments provide the data sets used as training and test. The risk predicted follows the behavior of the observed risk and could be used to better plan the schedules for preventive maintenance.	artificial neural network;chemical vapor deposition;discharger;fuzzy logic;inference engine;sensor;string (computer science);topological insulator;transmission line	Hilton Oliveira de Lima;Sérgio Campello Oliveira;Eduardo Fontana	2011	2011 11th International Conference on Intelligent Systems Design and Applications	10.1109/ISDA.2011.6121688	wireless sensor network;computer science;machine learning;artificial neural network	Robotics	11.234385230045648	-17.343358786876642	38537
fa4f5d701711149913af347be12de46fd0a192ab	integrated particle swarm and evolutionary algorithm approaches to the quadratic assignment problem		This paper introduces three integrated hybrid approaches that apply a combination of Hierarchical Particle Swarm Optimization (HPSO) and Evolutionary Algorithms (EA) to the Quadratic Assignment Problem (QAP). The approaches maintain a single population. In the first approach, Alternating HPSO-EA (AHE), the population alternates between applying HPSO and EA in successive generations. In the second, more integrated approach, Integrated HPSO-EA (IHE), each population element chooses to apply one of the two algorithms in each generation with some probability. An element applying HPSO in a given generation can be influenced by an element applying EA in that generation, and vice versa. Thus, within the same generation, some elements act as HPSO particles and others as EA population members, and yet the entire population still cooperates. In the third approach, we present a Social Evolutionary Algorithm (SEA), in which the population applies EA, and each population element can choose to apply the PSO-style social mutation operator in each generation with some probability. The three approaches are compared to HPSO and EA using 31 instances of varying size from the QAP instance library.	adaptive histogram equalization;alternating turing machine;ant colony optimization algorithms;evolutionary algorithm;particle swarm optimization;quadratic assignment problem;simulated annealing	Ayah M. Helal;Enas Jawdat;Islam El-Nabarawy;Donald C. Wunsch;Ashraf M. Abdelbar	2017	2017 IEEE Symposium Series on Computational Intelligence (SSCI)	10.1109/SSCI.2017.8280797	operator (computer programming);mathematical optimization;probabilistic logic;quadratic assignment problem;combinatorial optimization;evolutionary algorithm;population;particle swarm optimization	PL	23.971609583013073	-1.8924333211103248	38576
725316c49e96b765d3a96db159ef695f6b80539a	estimating the performance of emergency medical service location models via discrete event simulation	location;emergency medical services;optimization;discrete event simulation	We address the problem of evaluating deterministic EMS (Emergency Medical Service) location models via a simulation approach. For deterministic set covering location models, the performance of the model is typically determined by an objective function representing a certain type of coverage. After determining the location of EMS stations by deterministic models, we propose to conduct a simulation analysis to evaluate the performance by estimating the “real” coverage of the population that takes into consideration the unavailability of the busy ambulances. By using optimization tools, we find the location of ambulances for each model by solving the mathematical models and then we simulate each setting for two different policies under the same parameters. The overall performance of the models is firstly tested on Istanbul data and then on randomly generated data with different problem size, different layout and different arrival rates. We demonstrate that the performance estimated by the simulation based assessment can vary with respect to the problem characteristics and that the performance of a seemingly good deterministic model may not be that good as measured by the simulation approach.	mathematical optimization;population;procedural generation;simulation;software performance testing	Tonguç Ünlüyurt;Yasir Tunçer	2016	Computers & Industrial Engineering	10.1016/j.cie.2016.03.029	mathematical optimization;simulation;computer science;engineering;operations management;discrete event simulation;emergency medical services;location;operations research	AI	13.883848385628239	-0.4824512164535047	38589
f85fa516989e0284034e2a7bfcc98a9541613f78	on risk minimizing portfolios under a markovian regime-switching black-scholes economy	change of measures;continuous time;change of measure;stochastic differential game;convex risk measure;regime switching;regime switching hjb equation;hamilton jacobi bellman;measure of risk;stochastic control;diffusion process;financial risk;risk measure;risk minimization;hjb equation;markov chain;finite state markov chain	Résumé/Abstract: We consider a risk minimization problem in a continuous-time Markovian regimeswitching financial model modulated by a continuous-time, observable and finitestate Markov chain whose states represent different market regimes. We adopt a particular form of convex risk measure, which includes the entropic risk measure as a particular case, as a proxy of risk. The riskminimization problem is formulated as a Markovian regime-switching version of a two-player, zerosum stochastic differential game. One important feature of our model is to allow the flexibility of controlling both the diffusion process representing the financial risk and the Markov chain representing macro-economic risk. This is novel and interesting from both the perspectives of stochastic differential games and stochastic control. A verification theorem for the Hamilton-Jacobi-Bellman (HJB) solution of the game is provided and some particular cases are discussed. (joint work with Tak Kuen Siu, Curtin University of Technology)	black–scholes model;coherent risk measure;entropic risk measure;financial modeling;hamilton–jacobi–bellman equation;jacobi method;markov chain;modulation;observable;stochastic control	Robert J. Elliott;Tak Kuen Siu	2010	Annals OR	10.1007/s10479-008-0448-5	financial economics;markov chain;mathematical optimization;stochastic control;economics;financial risk;entropic value at risk;diffusion process;mathematics;dynamic risk measure;mathematical economics;coherent risk measure;statistics	ML	1.5046861746412072	-1.67819263151214	38590
cc94954aedb56fb2d7fc72f1b989999ec2d17719	artificial neural network based modelling approach for municipal solid waste gasification in a fluidized bed reactor	geociencias medio ambiente;medio ambiente;grupo de excelencia	In this paper, multi-layer feed forward neural networks are used to predict the lower heating value of gas (LHV), lower heating value of gasification products including tars and entrained char (LHVp) and syngas yield during gasification of municipal solid waste (MSW) during gasification in a fluidized bed reactor. These artificial neural networks (ANNs) with different architectures are trained using the Levenberg-Marquardt (LM) back-propagation algorithm and a cross validation is also performed to ensure that the results generalise to other unseen datasets. A rigorous study is carried out on optimally choosing the number of hidden layers, number of neurons in the hidden layer and activation function in a network using multiple Monte Carlo runs. Nine input and three output parameters are used to train and test various neural network architectures in both multiple output and single output prediction paradigms using the available experimental datasets. The model selection procedure is carried out to ascertain the best network architecture in terms of predictive accuracy. The simulation results show that the ANN based methodology is a viable alternative which can be used to predict the performance of a fluidized bed gasifier.	abbreviations;activation function;architecture as topic;artificial neural network;backpropagation;broyden–fletcher–goldfarb–shanno algorithm;choose (action);computational fluid dynamics;conjugate gradient method;consistency model;cross-validation (statistics);curie;elemental;fletcher's checksum;genetic heterogeneity;gradient descent;heating;immunostimulating conjugate (antigen);layer (electronics);leucaena pulverulenta;levenberg–marquardt algorithm;linear function;local hidden variable theory;mimo;master of social work;mean squared error;miso;model selection;monte carlo method;mycobacterium phage marquardt;network architecture;newton;ninety nine;nonlinear system;numerous;phb2 gene;prekallikrein;programming paradigm;protocols documentation;qr code;reactor (software);reactor device component;sigmoid colon;sigmoid function;small;software propagation;solid waste;solid-phase synthesis techniques;system analysis;tars;anatomical layer;libgdx;triangulation	Daya Shankar Pandey;Saptarshi Das;Indranil Pan;James J. Leahy;Witold Kwapinski	2016	Waste management	10.1016/j.wasman.2016.08.023	environmental engineering;engineering;waste management	ML	13.21678052596631	-21.10613641128124	38622
8ef9f88806a8fe5c0e04cc86cff45a868eab0bcc	integration of improved grbfn with fuzzy clustering for electricity price forecasting		This paper proposes an efficient model with a hybrid intelligent system that consists of fuzzy c-Varieties of fuzzy clustering and GRBFN (Generalized Radial Basis Function Network) of ANN (Artificial Neural Network) for electricity price forecasting. In recent years, electric power system deregulation has been spread in the world so that electricity power markets become more complicated. Due to the highly nonlinearity of electricity price time-series, more accurate forecasting model are required to purchase and/or sell electricity in power markets. In this paper, a hybrid intelligent method is proposed to provide one-step ahead predicted values of electricity price with a couple of strategies; Fuzzy c-Varieties and improved GRBFN. The former is used to classify input data into some fuzzy clusters while the latter is employed to provide the predicted electricity price at each cluster by applying Deterministic Annealing clustering to the determination of the initial values of parameters in the Gaussian functions. The reason why Fuzzy c-Varieties are used is to increases the number of data by allowing each data to be belonged to more than two clusters. The proposed method is tested for real data of ISO New England, USA.	artificial neural network;cluster analysis;electricity price forecasting;fuzzy clustering;hybrid intelligent system;nonlinear system;radial (radio);radial basis function network;simulated annealing;time series	Hiroyuki Mori;Satoshi Itaba	2017	2017 Joint 17th World Congress of International Fuzzy Systems Association and 9th International Conference on Soft Computing and Intelligent Systems (IFSA-SCIS)	10.1109/IFSA-SCIS.2017.8023270	electric power system;fuzzy logic;artificial neural network;fuzzy clustering;artificial intelligence;electricity price forecasting;mathematical optimization;machine learning;radial basis function network;computer science;cluster analysis;hybrid intelligent system	Robotics	8.6851163285106	-18.6514391259912	38642
ea9ff20d5b74652d539610e2da202d1768f2b052	gridftp based real-time data movement architecture for x-ray photon correlation spectroscopy at the advanced photon source	detectors;x ray spectroscopy data acquisition data analysis grid computing materials science computing parallel processing photon correlation spectroscopy;high performance computing;gridftp;photon correlation spectroscopy;real time data transfer gridftp based real time data movement architecture x ray photon correlation spectroscopy advanced photon source dynamical properties xpcs speckle pattern x ray beam area detector images time series high performance computing techniques hpc techniques aps gridftp plugin data analysis data acquisition system;charge coupled devices;x ray spectroscopy;data analysis;servers;big data;synchrotron big data gridftp high performance computing x ray photon correlation spectroscopy;x ray photon correlation spectroscopy;materials science computing;synchrotron;correlation;servers detectors charge coupled devices correlation data acquisition real time systems;grid computing;data acquisition;parallel processing;real time systems	X-ray photon correlation spectroscopy (XPCS) is a unique tool to study the dynamical properties in a wide range of materials over a wide spatial and temporal range. XPCS measures the correlated changes in the speckle pattern, produced when a coherent x-ray beam is scattered from a disordered sample, over a time series of area detector images. The technique rides on “Big Data” and relies heavily on high performance computing (HPC) techniques. In this paper, we propose a highspeed data movement architecture for moving data within the Advanced Photon Source (APS) as well as between APS and the users' institutions. We describe the challenges involved in the internal data movement and a GridFTP-based solution that enables more efficient usage of the APS beam time. The implementation of GridFTP plugin as part of the data acquisition system at the Advanced Photon Source for real time data transfer to the HPC system for data analysis is discussed.	big data;coherence (physics);data acquisition;gridftp;plug-in (computing);real-time clock;real-time data;remote file sharing;supercomputer;time series	S. Narayanan;T. J. Madden;Alec R Sandy;Rajkumar Kettimuthu;Michael Link	2012	2012 IEEE 8th International Conference on E-Science	10.1109/eScience.2012.6404466	electronic engineering;analytical chemistry;optics;physics	HPC	16.494976013671707	-10.840201620105484	38725
a11be9390e4ff36796bc83983cdf20f1723f22db	solving quadratic assignment problems by differential evolution	restart strategy quadratic assignment problem differential evolution de population based stochastic search technique optimization problem continuous real valued function discrete decision variable qap np hard problem integer permutation crossover operator tabu list population diversity;search problems computational complexity evolutionary computation integer programming quadratic programming	Differential evolution (DE) was introduced by Stone and Price in 1995 as a population-based stochastic search technique for solving optimization problems in a continuous space. DE has been successfully applied to various real world numerical optimization problems. In recent years not only continuous real-valued function, the applications of DE on combinatorial optimization problems with discrete decision variables are reported. However, genetic operator in the standard DE can not directly applied to discrete space. In this paper, we propose a method to solve quadratic assignment problems (QAP) by DE. The QAP is a well-known combinatorial optimization problem with a wide variety of practical applications. It is NP-hard and is considered to be one of the most difficult problems. In the QAP, a candidate solution can represented a permutation of integer. The proposed method employs permutation representation for individuals in DE. Therefore, a individual vector is encoded directly as a permutation. In discrete space, to realize effcient solution search like standard DE which have continuous nature, we modify differential operator to handle permutation encoding. Additionally, in order to maintain diversity of population, restart strategy and tabu list are introduced to proposed method instead of crossover operator. Finally, we show the experimental results using instances of QAPLIB and the efficacy of proposed method.	combinatorial optimization;differential evolution;genetic operator;mathematical optimization;np-hardness;numerical analysis;optimization problem;quadratic assignment problem;stochastic optimization;tabu search	Jun-ichi Kushida;Kazuhisa Oba;Akira Hara;Tetsuyuki Takahama	2012	The 6th International Conference on Soft Computing and Intelligent Systems, and The 13th International Symposium on Advanced Intelligence Systems	10.1109/SCIS-ISIS.2012.6505170	optimization problem;mathematical optimization;combinatorics;discrete mathematics;computer science;mathematics;quadratic assignment problem	EDA	24.337089448635002	-0.4309405371936966	38751
5b83187ee513a2c318ea7c26665e8043012aab41	a corridor method-based algorithm for the pre-marshalling problem	container terminal;high performance	To ease the situation and to ensure a high performance of ship, train and truck operations at container terminals, containers sometimes are pre-stowed near to the loading place and in such an order that it fits the loading sequence. This is done after the stowage plan is finished and before ship loading starts. Such a problem may be referred to as pre-marshalling. Motivated by most recent publications on this problem we describe a metaheuristic approach which is able to solve this type of problem. The approach utilizes the paradigm of the corridor method.	algorithm;marshalling (computer science)	Marco Caserta;Stefan Voß	2009		10.1007/978-3-642-01129-0_89	simulation;engineering;operations management;transport engineering	Theory	14.362441103773909	1.4098118872224998	38763
ca2884c858291ab9bbb00c82f359e05d4f68f84b	indivisibilities, lotteries, and monetary exchange	bargaining power;theoretical model;e4 money and interest rates;monetary economics;fiat money;journal of economic literature;e5 monetary policy central banking and the supply of money and credit;indivisible good;monetary exchange;indivisibilities	We introduce lotteries (randomized trading) into search-theoretic models of money. In a model with indivisible goods and fiat money, we show goods trade with probability 1 and money trades with probability {, where {<1 iff buyers have sufficient bargaining power. With divisible goods, a nonrandom quantity q trades with probability 1 and, again, money trades with probability { where {<1 iff buyers have sufficient bargaining power. Moreover, q never exceeds the efficient quantity (not true without lotteries). We consider several extensions designed to get commodities as well as money to trade with probability less than 1, and to illuminate the efficiency role of lotteries. Journal of Economic Literature Classification Numbers: E40, D83. 2002 Elsevier Science (USA)	algorithmic trading;indivisible;money;randomized algorithm;theory	Aleksander Berentsen;Miguel Molico;Randall Wright	2002	J. Economic Theory	10.1006/jeth.2000.2689	bargaining power;velocity of money;economics;money measurement concept;endogenous money;finance;macroeconomics;microeconomics;financial transaction;economic policy;monetary economics	ECom	-3.206625928438454	-2.376630475828152	38797
3e9d7401fca583bcec13a8bf67bb42fdba9a890a	cost minimization and social fairness for spatial crowdsourcing tasks		Spatial crowdsourcing is an activity consisting in outsourcing spatial tasks to a community of online, yet on-ground and mobile, workers. A spatial task is characterized by the requirement that workers must move from their current location to a specified location to accomplish the task. We study the assignment of spatial tasks to workers. A sequence of sets of spatial tasks is assigned to workers as they arrive. We want to minimize the cost incurred by the movement of the workers to perform the tasks. In the meanwhile, we are seeking solutions that are socially fair. We discuss the competitiveness in terms of competitive ratio and social fairness of the Work Function Algorithm, the Greedy Algorithm, and the Randomized versions of the Greedy Algorithm to solve this problem. These online algorithms are memory-less and are either inefficient or unfair. In this paper, we devise two Distribution Aware Algorithms that utilize the distribution information of the tasks and that assign tasks to workers on the basis of the learned distribution. With realistic and synthetic datasets, we empirically and comparatively evaluate the performance of the three baseline and two Distribution Aware Algorithms.	baseline (configuration management);competitive analysis (online algorithm);crowdsourcing;digital distribution;ftc fair information practice;fairness measure;greedy algorithm;online algorithm;outsourcing;randomized algorithm;synthetic intelligence	Qing Liu;Talel Abdessalem;Huayu Wu;Zihong Yuan;Stéphane Bressan	2016		10.1007/978-3-319-32025-0_1	knowledge management;management science	HCI	10.032374649370492	-0.5170136180015432	38895
a185da1d9fe4dcdec5328c76d9dc6d3e0f0676c6	hydrologic uncertainty for bayesian probabilistic forecasting model based on bp ann	mcmc methods;neural nets;flood control hydrologic uncertainty bayesian probabilistic forecasting model bp ann quantitative rainfall forecasting linear distribution normal distribution prior density function likelihood function artificial neural network markov chain monte carlo method;normal distribution;bayes methods;weather forecasting;bayesian probabilistic forecasting model;markov chain monte carlo method;forecasting model;prior density function;backpropagation;decision maker;conference paper;posterior distribution;geophysics computing;forecasting theory;linear distribution;markov chain monte carlo;flood control;bp ann;uncertainty bayesian methods predictive models artificial neural networks reservoirs floods technology forecasting gaussian distribution civil engineering structural engineering;hydrologic uncertainty;rain;estimation risk;markov processes;floods;quantitative rainfall forecasting;weather forecasting backpropagation bayes methods floods forecasting theory geophysics computing markov processes monte carlo methods neural nets rain;likelihood function;monte carlo methods;artificial neural network	The Bayesian forecasting system (BFS) consists of three components which can be deal with independently. Considering the fact that the quantitative rainfall forecasting has not been fully developed in all catchment areas in China, the emphasis is given to the hydrologic uncertainty for Bayesian probabilistic forecasting. The procedure of determining the prior density and likelihood functions associated with hydrologic uncertainty is very complicated and there is a requirement to assume a linear and normal distribution within the framework of BFS. These pose severe limitation to its practical application to real-life situations. In this paper, a new prior density and likelihood function model is developed with BP artificial neural network (ANN) to study the hydrologic uncertainty of short-term reservoir stage forecasts based on the BFS framework. Markov chain Monte Carlo (MCMC) method is employed to solve the posterior distribution and statistics of reservoir stage. A case study is presented to investigate and illustrate these approaches using 3 hours rainfall-runoff data from the ShuangPai Reservoir in China. The results show that Bayesian probabilistic forecasting model based on BP ANN not only increases forecasting precision greatly but also offers more information for flood control, which makes it possible for decision makers consider the uncertainty of hydrologic forecasting during decisionmaking and estimate risks of different decisions quantitatively.	artificial neural network;bayesian network;function model;markov chain monte carlo;monte carlo method;real life;typset and runoff	Chun-tian Cheng;Kwok-wing Chau;Xiang-Yang Li	2007	Third International Conference on Natural Computation (ICNC 2007)	10.1109/ICNC.2007.425	probabilistic forecasting;econometrics;geography;machine learning;statistics	Robotics	9.904934692912878	-19.03362315906589	38900
0c9300df9837d6947149d6700ff0d11d0a003c0f	a fuzzy analysis of a richter theorem in fuzzy consumers	fuzzy relations;fuzzy con- sumers.	In this paper we prove that a transitive fuzzy relationRon a setX can be extended to a total transitive fuzzy relationQ on X preserving the irreflexivity ofR. This generalizes a classical theorem of Szpilrajn that asserts that any strict order onX can be extended to a total strict order onX. The extension theorem that we obtain will be used in the fuzzy analysis of a classical Richter theorem in consumer theory.		Irina Georgescu	2003			discrete mathematics;fuzzy logic;fuzzy mathematics;fuzzy set;fuzzy measure theory;fuzzy subalgebra;fuzzy number;fuzzy classification;mathematics;membership function	Theory	-0.401821956883282	-22.669160006232715	38905
2deba8b926fc71898f7290485d834b02a2cd9115	a self-configuring metaheuristic for control of multi-strategy evolutionary search		There exists a great variety of evolutionary algorithms (EAs) that represent different search strategies for many classes of optimization problems. Real-world problems may combine several optimization features that are not known beforehand, thus there is no information about what EA to choose and which EA settings to apply. This study presents a novel metaheuristic for designing a multi-strategy genetic algorithm (GA) based on a hybrid of the island model, cooperative and competitive coevolution schemes. The approach controls interactions of GAs and leads to the self-configuring solving of problems with a priori unknown structure. Two examples of implementations of the approach for multi-objective and non-stationary optimization are discussed. The results of numerical experiments for benchmark problems from CEC competitions are presented. The proposed approach has demonstrated efficiency comparable with other well-studied techniques. And it does not require the participation of the human-expert, because it operates in an automated, self-configuring way.		Evgenii Sopov	2015		10.1007/978-3-319-20469-7_4	evolutionary programming;parallel metaheuristic;tabu search;evolutionary algorithm;guided local search;evolutionary computation	Robotics	23.9073005524306	-3.9524737375684755	38945
db5c5aa0c72d610b3aa244c524a0179e3a1a20aa	handling swarm of uavs based on evolutionary multi-objective optimization		The fast technological improvements in unmanned aerial vehicles (UAVs) has created new scenarios where a swarm of UAVs could operate in a distributed way. This swarm of vehicles needs to be controlled from a set of ground control stations, and new reliable mission planning systems, which should be able to handle the large amount of variables and constraints. This paper presents a new approach where this complex problem has been modelled as a constraint satisfaction problem (CSP), and is solved using a multi-objective genetic algorithm (MOGA). The algorithm has been designed to minimize several variables of the mission, such as the fuel consumption or the makespan among others. The designed fitness function, used by the algorithm, takes into consideration, as a weighted penalty function, the number of constraints fulfilled for each solution. Therefore, the MOGA algorithm is able to manage the number of constraints fulfilled by the selected plan, so it is possible to maximize in the elitism phase of the MOGA the quality of the solutions found. This approach allows to alleviate the computational effort carried out by the CSP solver, finding new solutions from the Pareto front, and therefore reducing the execution time to obtain a solution. In order to test the performance of this new approach 16 different mission scenarios have been designed. The experimental results show that the approach outperforms the convergence of the algorithm in terms of number of generations and runtime.	aerial photography;computation;constraint satisfaction problem;fitness function;genetic algorithm;makespan;mathematical optimization;multi-objective optimization;pareto efficiency;penalty method;powera;run time (program lifecycle phase);solver;swarm;unmanned aerial vehicle	Cristian Ramírez-Atencia;María Dolores Rodríguez-Moreno;David Camacho	2017	Progress in Artificial Intelligence	10.1007/s13748-017-0123-7	job shop scheduling;genetic algorithm;swarm behaviour;computer science;penalty method;mathematical optimization;constraint satisfaction problem;multi-objective optimization;fitness function;solver	AI	20.28158331479654	-6.022227729632022	38982
dca8942115e5c471ba35c1082f4ca2eb8675d30c	quality and price effects on technology adoption	network externalities	We study the adoption patterns of two competing technologies as well as the effectiveness and optimality of viral pricing strategies. Our model considers two incompatible technologies of differing quality and a market in which valuations are heterogeneous and subject to externalities. We provide partial characterization results about the structure and robustness of equilibria and give conditions under which a technology purveyor can gains in market share. We show that myopic best-response dynamics are monotonic and convergent, and propose two pricing mechanisms using this insight to help a technology seller tip the market in its favor. In particular, we show that non-discriminatory pricing is less costly and just as effective as a discriminatory policy. Finally, we study endogenous pricing using simulations and now find, in contrast to our analytical results with exogenous prices, that a higher quality technology consistently holds a competitive advantage over the lower quality competitor, irrespective of its market share.	entropy maximization;heterogeneous computing;icis;information systems;instrumental convergence;np-hardness;nash equilibrium;simulation;social network	Jacomo Corbo;Yevgeniy Vorobeychik	2009			externality;competitive advantage;robustness (computer science);marketing;valuation (finance);network effect;market share;computer science;pricing strategies	ECom	-2.535339636663975	-6.423877041761756	39029
5535646ae3e287a1f5c3867db48bc9d1a58e1f5e	combining aggregation with pareto optimization: a case study in evolutionary molecular design	high resolution;pareto front;search method;design optimization;drug design;indexation;estrogen receptor;fuzzy constraints;multi objective optimization problem;pareto optimality;molecular structure	This paper is motivated by problem scenarios in automated drug design. It discusses a modeling approach for design optimization problems with many criteria that can be partitioned into objectives and fuzzy constraints. The purpose of this remodeling is to transform the original criteria such that, when using them in an evolutionary search method, a good view on the trade-off between the different objectives and the satisfaction of constraints is obtained.#R##N##R##N#Instead of reducing a many objective problem to a single-objective problem, it is proposed to reduce it to a multi-objective optimization problem with a low number of objectives, for which the visualization of the Pareto front is still possible and the size of a high-resolution approximation set is affordable. For design problems where it is reasonable to combine certain objectives and/or constraints into logical groups by means of desirability indexes, this method will yield good trade-off results with reduced computational effort. The proposed methodology is evaluated in a case-study on automated drug design where we aim to find molecular structures that could serve as estrogen receptor antagonists.	pareto efficiency	Johannes W. Kruisselbrink;Michael T. M. Emmerich;Thomas Bäck;Andreas Bender;Adriaan P. IJzerman;Eelke van der Horst	2009		10.1007/978-3-642-01020-0_36	mathematical optimization;operations management;multi-objective optimization;mathematics;management science;vector optimization	EDA	19.241386389610774	-6.454422545785037	39038
07cedd3e0334410f9fd9712c079c165f0186c0d1	an experimental analysis of schema creation, propagation and disruption in genetic programming	schema theory;experimental analysis	In this paper we first review the main results in the theory of schemata in Genetic Programming (GP) and summarise a new GP schema theory which is based on a new definition of schema. Then we study the creation, propagation and disruption of this new form of schemata in real runs, for standard crossover, one-point crossover and selection only. Finally, we discuss these results in the light our GP schema theorem.	denial-of-service attack;genetic programming;holland's schema theorem;schema (genetic algorithms);software propagation	Riccardo Poli;William B. Langdon	1997			genetic program;genetic algorithm;crossover;genetic programming;machine learning;schema (psychology);strengths and weaknesses;artificial intelligence;computer science	DB	22.894224305727597	-9.480804065810455	39107
eb938204a77aed562e22e5e928edf69144002b1a	coalitional security games	game theory;stackelberg games;optimization;security	Game theoretic models of security, and associated computational methods, have emerged as critical components of security posture across a broad array of domains, including airport security and coast guard. These approaches consider terrorists as motivated but independent entities. There is, however, increasing evidence that attackers, be it terrorists or cyber attackers, communicate extensively and form coalitions that can dramatically increase their ability to achieve malicious goals. To date, such cooperative decision making among attackers has been ignored in the security games literature. To address the issue of cooperation among attackers, we introduce a novel coalitional security game (CSG) model. A CSG consists of a set of attackers connected by a (communication or trust) network who can form coalitions as connected subgraphs of this network so as to attack a collection of targets. A defender in a CSG can delete a set of edges, incurring a cost for deleting each edge, with the goal of optimally limiting the attackers’ ability to form effective coalitions (in terms of successfully attacking high value targets). We first show that a CSG is, in general, hard to approximate. Nevertheless, we develop a novel branch and price algorithm, leveraging a combination of column generation, relaxation, greedy approximation, and stabilization methods to enable scalable high-quality approximations of CSG solutions on realistic problem instances.	airport security;approximation algorithm;blocking (computing);branch and price;column generation;constructive solid geometry;cyber spying;entity;greedy algorithm;integer programming;linear programming relaxation;max;np-hardness;poor posture;snp (complexity);scalability;theory	Qingyu Guo;Bo An;Yevgeniy Vorobeychik;Long Tran-Thanh;Jiarui Gan;Chunyan Miao	2016			game theory;simulation;computer science;artificial intelligence;distributed computing;computer security	AI	7.494833730865307	2.467518652969977	39123
ecb6b7958e0cfc26fb4830041bed04523d73968b	a fuzzy-ga decision support system for enhancing postponement strategies in supply chain management	performance measure;fuzzy set;fuzzy rules;performance metric;support system;decision support system;hybrid optimisation algorithms;membership function;supply chain;supply chain management;historical data;knowledge base	This paper aims to propose a knowledge-based Fuzzy - GA Decision Support System with performance metrics for better measuring postponement strategies. The Fuzzy - GA approach mainly consists of two stages: knowledge representation and knowledge assimilation. The relevant knowledge of deciding what type of postponement strategies to adopt is encoded as a string with a fuzzy rule set and the corresponding membership functions. The historical data on performance measures forming a combined string is used as the initial population for the knowledge assimilation stage afterwards. GA is then further incorporated to provide an optimal or nearly optimal fuzzy set and membership functions for related performance measures. The originality of this research is that the proposed system is equipped with the ability of assessing the loss caused by discrepancy away from the different supply chain parties, and therefore enabling the identification of the best set of decision variables.	decision support system;fuzzy concept;software release life cycle	Cassandra X. H. Tang;Henry C. W. Lau	2008		10.1007/978-3-540-89694-4_15	knowledge base;supply chain management;decision support system;membership function;defuzzification;fuzzy classification;computer science;knowledge management;artificial intelligence;fuzzy number;machine learning;data mining;supply chain;fuzzy set;fuzzy set operations	HCI	-4.472649218057008	-16.728036386122568	39189
725d25545c625b58e24505cf64ef69607d0d1c7c	ontogenetic and phylogenetic reinforcement learning	reinforcement learning	Reinforcement learning (RL) problems come in many flavours, as do the algorithms for solving them. It is currently not clear which of the commonly used RL benchmarks best measure an algorithm’s capacity for solving real-world problems. Similarly, it is not clear which types of RL algorithms are best suited to solve which kinds of RL problems. Here we present some dimensions along the axes o which RL problems and algorithms can be varied to help distinguish them from each other. Based on results and arguments in the literature, we present some conjectures as to what algorithms should work best for particular types of problems, and argue that tunable RL benchmarks are needed in order to further understand the capabilities of RL algorithms.	phylogenesis;phylogenetics;reinforcement learning	Tom Schaul;Daan Wierstra;Faustino J. Gomez;Jürgen Schmidhuber;Christian Igel;Julian Togelius	2009	KI		artificial intelligence;machine learning;mathematics;algorithm	ML	20.64932979985334	-11.39636022099355	39230
0f028a6dbd8e204fd4d19ad13f939e77abd1c7fd	a primary theoretical study on decomposition-based multiobjective evolutionary algorithms	evolutionary computation;complexity theory;scalar subproblem based search scheme decomposition based multiobjective evolutionary algorithm moea runtime complexity pareto front fitness space decision space weight parameter based decomposition serialized algorithm neighborhood coevolution scheme;search problems evolutionary computation pareto optimisation;runtime;theoretical study decomposition based multiobjective evolutionary algorithms moeas runtime analysis;期刊论文;statistics;optimization;algorithm design and analysis;sociology;runtime optimization evolutionary computation algorithm design and analysis complexity theory sociology statistics	Decomposition-based multiobjective evolutionary algorithms (MOEAs) have been studied a lot and have been widely and successfully used in practice. However, there are no related theoretical studies on this kind of MOEAs. In this paper, we theoretically analyze the MOEAs based on decomposition. First, we analyze the runtime complexity with two basic simple instances. In both cases the Pareto front have oneto-one map to the decomposed subproblems or not. Second, we analyze the runtime complexity on two difficult instances with bad neighborhood relations in fitness space or decision space. Our studies show that: 1) in certain cases, polynomialsized evenly distributed weight parameters-based decomposition cannot map each point in a polynomial sized Pareto front to a subproblem; 2) an ideal serialized algorithm can be very efficient on some simple instances; 3) the standard MOEA based on decomposition can benefit a runtime cut of a constant fraction from its neighborhood coevolution scheme; and 4) the standard MOEA based on decomposition performs well on difficult instances because both the Pareto domination-based and the scalar subproblem-based search schemes are combined in a proper way.	cloud computing;differential evolution;dominating set;evolutionary algorithm;loss function;moea framework;mathematical optimization;optimization problem;pareto efficiency;particle swarm optimization;polynomial;regular expression;run time (program lifecycle phase);time complexity	Yuan-Long Li;Yuren Zhou;Zhi-hui Zhan;Jun Zhang	2016	IEEE Transactions on Evolutionary Computation	10.1109/TEVC.2015.2501315	algorithm design;mathematical optimization;combinatorics;computer science;machine learning;mathematics;statistics;evolutionary computation	Web+IR	24.383341961761293	-2.98977010987655	39232
42835e13ca9b995e33c369e862d9582d4d469534	research on the effect of real estate price to intermediate objectives of monetary policy based on ecm	stability criteria;ecm monetary policy intermediate objective co integration;integration testing;co integration;intermediate objective;real estate;fluctuations;financial management;pricing;property market;endogenous money;controllability;money supply;real estate price index;enterprise content management ecm real estate price index interbank interest rate error correction model money supply controllability monetary policy;enterprise content management;interest rate;indexes;interbank interest rate;monetary policy;property market economic indicators financial management pricing;error correction;price index;mathematical model;fluctuations indexes error correction mathematical model stability criteria economic indicators;ecm;error correction model;economic indicators	This paper reviews some relevant literatures first, then takes a co-integration test on broad money supply, real estate price index and interbank interest rate, and establishes the error correction model. The conclusions are as follows, it is a negative relationship between interbank interest rate volatility and M2, the coefficient of elasticity is -0.0522; short-term fluctuation of real estate price index and volatility of M2 shows a positive relationship and its elasticity is 0.0327, which shows that the real estate as collateral for a loan, increase of their prices will increase the money supply, thereby enhancing endogenous money supply, which in turn reduces the controllability of the money supply as an intermediate objective; The coefficient of error correction term is 0.0033, indicating a long-term fluctuations in the real estate price index also plays a positive role on the volatility of broad money supply. When the short-term fluctuations deviate from the long-run equilibrium, adjustment of at the level of 0.0033 will be intensified, which makes it go back to the to the long run equilibrium.	coefficient;elasticity (cloud computing);elasticity (data store);error correction model;error detection and correction;integration testing;money;quantum fluctuation;volatility	Yueying Lv;Decun Guo	2010	2010 International Conference on E-Business and E-Government	10.1109/ICEE.2010.637	financial economics;pricing;financial management;error correction model;inflation;monetary policy;price elasticity of supply;controllability;economics;price index;integration testing;marketing;capitalization rate;endogenous money;interest rate;finance;mathematical model;classical dichotomy;monetary economics;real estate	Robotics	-0.6822955394531426	-8.982596735119603	39266
de5d816ab90a288efaabe9e4f0030e4c0f887598	branch-and-price-and-cut for the truck-and-trailer routing problem with time windows		In this paper, we present a new branch-and-price-and-cut algorithm to solve the truck-and-trailer routing problem with time windows (TTRPTW) and two real-world extensions. In all TTRPTW variants, the fleet consists of one or more trucks that may attach a trailer. Some customers are not accessible with a truckand-trailer combination, but can however be serviced by one if the trailer is previously detached and parked at a suitable location. In the first extension, the planning horizon comprises two days and customers may be visited either on both days or only once, in which case twice the daily supply must be collected. The second extension incorporates load transfer times depending on the quantity moved from a truck to its trailer. The exact branch-and-price-and-cut algorithm for the standard variant and the two new extensions is based on a set-partitioning formulation in which columns are routes describing the movement of a truck and its associated trailer. Linear relaxations of this formulation are solved by column generation where new routes are generated with a dynamic programming labeling algorithm. The effectiveness of this pricing procedure can be attributed to the adaptation of techniques such as bidirectional labeling, the ng-neighbourhood, and heuristic pricing using dynamically reduced networks and relaxed dominance. The cutting component of the branch-and-price-and-cut adds violated subset-row inequalities to strengthen the linear relaxation. Computational studies show that our algorithm outperforms existing approaches on TTRP and TTRPTW benchmark instances used in the literature.	algorithm;algorithmics;benchmark (computing);bidirectional reflectance distribution function;branch and bound;branch and price;central processing unit;column (database);column generation;computation;dynamic programming;experiment;german research centre for artificial intelligence;heuristic;interdependence;linear programming relaxation;microsoft windows;nethack;point of view (computer hardware company);printing;routing;semantic role labeling;synchronization (computer science)	Ann-Kathrin Rothenbächer;Michael Drexl;Stefan Irnich	2018	Transportation Science	10.1287/trsc.2017.0765	branch and price;mathematics;column generation;mathematical optimization;operations management;vehicle routing problem;time horizon;truck;trailer;dynamic programming;heuristic	AI	15.91431416382803	2.4185226108428908	39308
b2f0ae76aa09f44cac3279d557f5562ab78c03c8	ant-based vehicle congestion avoidance system using vehicular networks	vehicle traffic routing;vehicle congestion problem;ant colony optimization;vehicular networks;car navigation system	Vehicle traffic congestion leads to air pollution, driver frustration, and costs billions of dollars annually in fuel consumption. Finding a proper solution to vehicle congestion is a considerable challenge due to the dynamic and unpredictable nature of the network topology of vehicular environments, especially in urban areas. Instead of using static algorithms, e.g. Dijkstra and A*, we present a bio-inspired algorithm, food search behavior of ants, which is a promising way of solving traffic congestion in vehicular networks. We have called this the Antbased Vehicle Congestion Avoidance System (AVCAS). AVCAS combines the average travel speed prediction of traffic on roads with map segmentation to reduce congestion as much as possible by finding the least congested shortest paths in order to avoid congestion instead of recovering from it. AVCAS collects real-time traffic data from vehicles and road side units to predict the average travel speed of roads traffic. It utilizes this information to perform an ant-based algorithm on a segmented map resulting in avoidance of congestion. Simulation results conducted on various vehicle densities show that the proposed system outperforms the existing systems in terms of average travel time, which decreased by an average of 11.5%, and average travel speed which increased by an average of 13%. In addition, AVCAS handles accident conditions in a more efficient way and decreases congestion by using alternative paths.	algorithm;british informatics olympiad;evaporation;guidance system;map segmentation;network congestion;network topology;real-time transcription;shortest path problem;simulation;vehicle-to-vehicle	Mohammad Reza Jabbarpour Sattari;Ali Jalooli;Erfan Shaghaghi;Rafidah Md Noor;Léon J. M. Rothkrantz;Rashid Hafeez Khokhar;Nor Badrul Anuar	2014	Eng. Appl. of AI	10.1016/j.engappai.2014.08.001	vehicular ad hoc network;mathematical optimization;ant colony optimization algorithms;simulation;floating car data;vehicle information and communication system;computer science;traffic congestion reconstruction with kerner's three-phase theory;computer security;slow-start	Mobile	11.486600870262459	-8.652585618281213	39311
52485281e01eda0ec13474e677bc0a41765192d1	genetic algorithm approach for solving a cell formation problem in cellular manufacturing	optimal solution;group technology;cell formation;mathematical model;industrial application;genetic algorithm;group efficacy;cellular manufacturing	Please cite this article in press as: Mahdav with Applications (2008), doi:10.1016/j.esw Cellular manufacturing (CM) is an industrial application of group technology concept. One of the problems encountered in the implementation of CM is the cell formation problem (CFP). The CFP attempted here is to group machines and parts in dedicated manufacturing cells so that the number of voids and exceptional elements in cells are minimized. The proposed model, with nonlinear terms and integer variables, cannot be solved for real sized problems efficiently due to its NP-hardness. To solve the model for real-sized applications, a genetic algorithm is proposed. Numerical examples show that the proposed method is efficient and effective in searching for optimal solutions. The results also indicate that the proposed approach performs well in terms of group efficacy compared to the well-known existing cell formation methods. 2008 Elsevier Ltd. All rights reserved.	computers, freedom and privacy conference;genetic algorithm;np-hardness;nonlinear system;numerical method;whole earth 'lectronic link	Iraj Mahdavi;Mohammad Mahdi Paydar;Maghsud Solimanpur;Armaghan Heidarzade	2009	Expert Syst. Appl.	10.1016/j.eswa.2008.07.054	mathematical optimization;genetic algorithm;computer science;artificial intelligence;machine learning;mathematical model;mathematics;algorithm;statistics	Robotics	20.5270332319338	1.7021464997341191	39366
e11751ac7f7aeab2069c8624c4d17f8938d4766c	smart data-harnessing for financial value in short-term hire electric car schemes	smart monitoring;market research;environmental impact;pricing;noise pollution air pollution control automobiles electric vehicles;systems engineering and theory;cities and towns vehicles systems engineering and theory market research roads pricing;smart cities;big data;roads;environmental impact electric vehicles vehicle hire models smart technologies smart monitoring smart cities big data;vehicle hire models;electric vehicles;cities and towns;vehicles;internal combustion engines smart data harnessing financial value short term hire electric car schemes electric powertrain noise pollution air pollution carbon dioxide emissions outlay induced car residential parking social division smart cities;smart technologies	In the developed world, two distinct trends are emerging to shake-up the current dominance of privately-owned, combustion motor car transport. The first is the emergence of the electric powertrain for vehicles as an affordable and massmarketed means of transport. This carries with it the potential to address many of the immediate shortcomings of the current paradigm, especially CO2 emissions, air and noise pollution. The second is the rise of new hire models of car ownership - the concept of paying for the use of a car as and when you need it. This carries with it the potential to address many of the existing issues: outlay-induced car use, residential parking and social division. On a similar timescale, we are witnessing the rise of smart technologies and smart cities, concepts that use data about the state of a system or elements of it to create value. There have been relatively few examples of schemes that have combined the electric and hire-model concepts, despite the huge potential for synergy. Indeed, the majority is against them on both counts - cars are predominantly privately-owned and driven by internal combustion engines. Nevertheless, there is significant potential for this to change over the coming years.	emergence;extended validation certificate;network congestion;niche blogging;programming paradigm;routing;smart city;speculative execution;synergy;usb hub	Peter Cooper;Tom Crick;Theodore Tryfonas	2015	2015 10th System of Systems Engineering Conference (SoSE)	10.1109/SYSOSE.2015.7151928	market research;pricing;simulation;big data;computer science;engineering;automotive engineering;transport engineering;computer security;environmental impact assessment	HCI	8.885937927402187	-9.040693655548424	39381
6d2872f7ceefe14f372a8e1f4e8ee90ee33d10b2	a probabilistic model for estimating the effects of photovoltaic sources on the power systems reliability		Abstract As the power grid is continuously expanding, the increased amount of loading and the corresponding amount of generation is causing the grid to become more and more complex. The addition of intelligent devices for control and communications is also making the grid more vulnerable to failures. The addition of distributed renewable energy sources can potentially alleviate some of the stress on the grid, and help improve its reliability; however, their effects on the grid are not yet fully understood. In this paper, we study the effects of adding photovoltaic (PV) sources to the grid from a reliability perspective. We use monte carlo simulation to assess the reliability of the system in the presence of transmission line contingencies and potential cascading failures, and we use the particle swarm optimization to find the optimal placement of PV sources in the network that would maximize the system reliability. Our results show that while adding PV sources to the network can, under certain conditions, improve the system reliability, increasing the amount of PV indefinitely does not necessarily continue to improve it. There is a certain optimal amount of PV power that would improve the system reliability, and any further increase can have negative effects.	ibm power systems;reliability engineering;statistical model	Ayman Faza	2018	Rel. Eng. & Sys. Safety	10.1016/j.ress.2017.11.008	transmission line;reliability engineering;statistics;grid;cascading failure;engineering;renewable energy;electric power system;monte carlo method;photovoltaic system;particle swarm optimization	DB	6.113211739140977	3.6765644136734057	39409
7b06a2f0636a0dfdc8b1d590c1019e9194b69310	minimax chance constrained programming models for fuzzy decision systems	fuzzy simulation;fuzzy programming;spectrum;programming model;goal programming;genetic algorithm;fuzzy decision	The existing chance constrained programming for fuzzy decision systems is essentially a kind of maximax models (optimistic models) which maximize the maximum possible return. This paper presents a spectrum of minimax models as opposed to maximax models based on chance constrained programming as well as chance constrained multiobjective programming and chance constrained goal programming, in which the minimax models will select the alternative that provides the best of the worst possible return. Finally, a fuzzy simulation based genetic algorithm will be designed for solving minimax models and illustrated by some numerical examples. © 1998 Elsevier Science Inc. All rights reserved.	genetic algorithm;goal programming;minimax;multi-objective optimization;numerical analysis;simulation	Booding Liu	1998	Inf. Sci.	10.1016/S0020-0255(98)10015-4	spectrum;mathematical optimization;genetic algorithm;reactive programming;computer science;artificial intelligence;machine learning;goal programming;mathematics;programming paradigm;inductive programming;programming language	AI	0.5176095734675737	-16.766235728166194	39464
9d5d795a8c731263000cbcbcfcab45eb301f029d	parallel metaheuristic search		We present a general picture of the parallel meta-heuristic search for optimization. We recall the main concepts and strategies in designing parallel metaheuristics, pointing to a number of contributions that instantiated them for neighborhoodand populationbased meta-heuristics, and identify trends and promising research directions. We focus on cooperation-based strategies, which display remarkable performances, in particular on asynchronous cooperation and advanced cooperation strategies that create new information out of exchanged data to enhance the global guidance of the search.	heuristic (computer science);mathematical optimization;parallel metaheuristic;performance	Teodor Gabriel Crainic	2018		10.1007/978-3-319-07124-4_40	mathematical optimization;computer science;parallel metaheuristic	AI	21.61189581403723	-4.944338972271837	39514
c85abfbe5e61a416265138ffd5dd08841c684671	an interactive reservoir management system for lake kariba	management practice;forecasting;water management;prevision;management system;metodo box jenkins;systeme aide decision;course of action;sistema ayuda decision;time series;decision support system;amenagement hydraulique;scenario analysis;aprovechamiento hidraulico;box jenkins method;serie temporelle;serie temporal;user interaction;extreme event;deposito;reservoir;methode box jenkins;management policy;time series model	This paper presents a user-interactive decision support system (DSS) for the management of the Lake Kariba reservoir. Built in the fourth-generation computer language IFPS, the system takes into account relevant reservoir characteristics and parameters, such as the amount of hydropower generated, reservoir storage throughout the year, and the amount of water released for down-stream usage. The system blends water release rules determined previously using optimization and simulationbased scenario analyses with expert input from an experienced reservoir manager, yielding a n intuitive and realistic DSS with which the reservoir manager may easily identify. The DSS also includes a BoxJenkins time series model that forecasts future inflows. Each month, the system provides the manager with a proposed release schedule, which the manager then uses to explore and evaluate the consequences in terms of the decision criteria, over an extended period of time. The types of information provided to and sought from the manager correspond closely with actual reservoir management practice. An important characteristic of the system is that the manager can quickly explore various different potential release decisions a priori, for a variety of potential inflow scenarios, including predicted inflows for average hydrological years, as well as inflows reflecting extreme events such as drought and flood periods. The manager can compare the results of the release decisions made in the scenario analysis, both with the release strategy proposed by the system and with historical release decisions, thus aiding the manager in establishing effective reservoir management policies in practice. Thus, rather than a mechanical value, our DSS offers the manager a flexible problem analysis with suggested courses of action. We illustrate the system using example sessions with an experienced reservoir manager. While the system is designed specifically to support the management of Lake Kariba, its extension to a more general class of reservoir management problems is straightforward.	computer language;decision problem;decision support system;history of computing hardware;ifps;mathematical optimization;problem solving;rare events;scenario analysis;software release life cycle;time series	Antonie Stam;Kazimierz A. Salewicz;Jay E. Aronson	1998	European Journal of Operational Research	10.1016/S0377-2217(96)00365-7	simulation;decision support system;operations management;time series;mathematics;operations research;statistics	DB	6.958260815139909	-4.339506919845394	39516
3bbc1eebabc17b459611927cc232329d76e42ff7	a generic preprocessing optimization methodology when predicting time-series data	forecasting;ant colony optimization;genetic algorithms;preprocessing optimization methodology	A general Methodology referred to as Daphne is introduced which is used to find optimum combinations of methods to preprocess and forecast for time-series datasets. The Daphne Optimization Methodology (DOM) is based on the idea of quantifying the effect of each method on the forecasting performance, and using this information as a distance in a directed graph. Two optimization algorithms, Genetic Algorithms and Ant Colony Optimization, were used for the materialization of the DOM. Results show that the DOM finds a near optimal solution in relatively less time than using the traditional optimization algorithms.	ant colony optimization algorithms;brute-force search;computation;directed graph;document object model;genetic algorithm;mathematical optimization;preprocessor;software release life cycle;time series	Ioannis Kyriakidis;Kostas D. Karatzas;Andrew Ware;George Papadourakis	2016	Int. J. Comput. Intell. Syst.	10.1080/18756891.2016.1204113	multi-swarm optimization;ant colony optimization algorithms;test functions for optimization;meta-optimization;genetic algorithm;forecasting;computer science;artificial intelligence;machine learning;data mining;metaheuristic;statistics	Metrics	23.632969669661286	-1.603829330590593	39567
bb79a1b25c57088f6be4e7de67282a3756b1806d	genetic programming for evolving due-date assignment models in job shop environments	hyper heuristics;genetic programming;due date assignment;job shop;hyper heuristics genetic programming job shop due date assignment	Due-date assignment plays an important role in scheduling systems and strongly influences the delivery performance of job shops. Because of the stochastic and dynamic nature of job shops, the development of general due-date assignment models (DDAMs) is complicated. In this study, two genetic programming (GP) methods are proposed to evolve DDAMs for job shop environments. The experimental results show that the evolved DDAMs can make more accurate estimates than other existing dynamic DDAMs with promising reusability. In addition, the evolved operation-based DDAMs show better performance than the evolved DDAMs employing aggregate information of jobs and machines.	aggregate data;estimated;genetic programming;job stream;occupations;scheduling (computing);scheduling - hl7 publishing domain	Su Nguyen;Mengjie Zhang;Mark Johnston;Kay Chen Tan	2014	Evolutionary Computation	10.1162/EVCO_a_00105	genetic programming;job shop scheduling;mathematical optimization;real-time computing;flow shop scheduling;computer science;artificial intelligence;operations management;machine learning	AI	19.423145361339966	-0.6493764224320533	39610
1f229a7e3bd8bb550f7ce9554e653adef344ef83	an extended policy gradient algorithm for robot task learning	motion control;learning techniques;optimal parameter sets;reinforcement learning;convergence rate;robot kinematics machine learning cognitive robotics robot vision systems intelligent robots motion control design methodology robot sensing systems learning systems genetic programming;reinforcement learning algorithm;strategic decisional processes;robot task learning;machine learning;robots;extended policy gradient algorithm;gradient methods;decision process;learning artificial intelligence;robots gradient methods learning artificial intelligence;simulation environment;reinforcement learning algorithm extended policy gradient algorithm robot task learning strategic decisional processes optimal parameter sets learning techniques	In real-world robotic applications, many factors, both at low-level (e.g., vision and motion control parameters) and at high-level (e.g., the behaviors) determine the quality of the robot performance. Thus, for many tasks, robots require fine tuning of the parameters, in the implementation of behaviors and basic control actions, as well as in strategic decisional processes. In recent years, machine learning techniques have been used to find optimal parameter sets for different behaviors. However, a drawback of learning techniques is time consumption: in practical applications, methods designed for physical robots must be effective with small amounts of data. In this paper, we present a method for concurrent learning of best strategy and optimal parameters, by extending the policy gradient reinforcement learning algorithm. The results of our experimental work in a simulated environment and on a real robot show a very high convergence rate.	aibo;algorithm;gradient;high- and low-level;machine learning;mathematical optimization;maxima;rate of convergence;reinforcement learning;relevance;robot;virtual reality	Andrea Cherubini;Francesca Giannone;Luca Iocchi;Pier Francesco Palamara	2007	2007 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2007.4399219	robot;motion control;robot learning;error-driven learning;simulation;computer science;artificial intelligence;online machine learning;machine learning;rate of convergence;reinforcement learning;active learning	Robotics	18.356280097265543	-19.73255229808673	39671
4ee629a425d0b2d4fea864b331af91b573922a0c	a complete efficiency ranking of decision making units in data envelopment analysis	efficiency;optimization problem;ranking;data envelopment analysis;decision making unit;efficiency measurement;data envelope analysis	The efficiency measures provided by DEA can be used for ranking Decision Making Units (DMUs), however, this ranking procedure does not yield relative rankings for those units with 100% efficiency. Andersen and Petersen have proposed a modified efficiency measure for efficient units which can be used for ranking, but this ranking breaks down in some cases, and can be unstable when one of the DMUs has a relatively small value for some of its inputs. This paper proposes an alternative efficiency measure, based on a different optimization problem that removes the difficulties.	data envelopment analysis	S. Mehrabian;Mohammad R. Alirezaee;Gholam Reza Jahanshahloo	1999	Comp. Opt. and Appl.	10.1023/A:1008703501682	econometrics;data envelopment analysis;mathematics;statistics	DB	-2.2476025338208308	-16.1307045466447	39674
fecff2dfc8de07834c161fc0febda51b35ef4d7b	the berth allocation problem in terminals with irregular layouts		Abstract As international trade thrives, terminals attempt to obtain higher revenue while coping with an increased complexity with regard to terminal management operations. One of the most prevalent problems such terminals face is the Berth Allocation Problem (BAP), which concerns allocating vessels to a set of berths and time slots while simultaneously minimizing objectives such as total stay time or total assignment cost. Complex layouts of real terminals introduce spatial constraints which limit the mooring and departure of vessels. Although significant research has been conducted regarding the BAP, these real-world restrictions have not been taken into account in a general way. The present work proposes both a mixed integer linear programming formulation and a heuristic, which are capable of obtaining optimal or near-optimal solutions to this novel variant of the BAP. In order to assess the quality of the heuristic, which is being employed in a real tank terminal in Belgium, it is compared against the exact approach by way of randomly-generated instances and real-world benchmark sets derived from the tank terminal.	berth allocation problem	Juan Francisco Correcher;J. Adam Rondinone;Ramón Alvarez-Valdés;Greet Vanden Berghe	2019	European Journal of Operational Research	10.1016/j.ejor.2018.07.019	mathematical optimization;iterated local search;combinatorial optimization;mooring;berth allocation problem;integer programming;mathematics;revenue;heuristic	Theory	15.211446655302659	2.319577836825682	39735
98a74943db51739dcc1f1ae1ecd21affb2cac120	particle swarm optimization with fast local search for the blind traveling salesman problem	traveling salesman problem;control systems;brazil council;search space;travelling salesman problem;combinatorial problems;hybrid model;particle swarm optimizer;hybrid method;fast local search;particle swarm optimization;traveling salesman problems cities and towns particle swarm optimization robustness vehicles genetic algorithms control systems system testing brazil council costs;system testing;cities and towns;robustness;genetic algorithms;traveling salesman problems;vehicles;weighted graph;hybrid algorithm	The classical travelling salesman problem (TSP) is to determine a tour in a weighted graph (that is, a cycle that visits every vertex exactly once) such that the sum of the weights of the edges in this tour is minimal. Hybrid methods, based on nature inspired heuristics, have shown their ability to provide high quality solutions for the TSP. The success of a hybrid algorithm is due to its tradeoff between the exploration and exploitation abilities in search space. This work presents a new hybrid model, based on Particle Swarm Optimization and Fast Local Search, with concepts of Genetic Algorithms, for the blind TSP A detailed description of the model is provided, emphasizing its hybrid features. The control parameters were carefully adjusted and the implemented system was tested with instances from 76 to 2103 cities. For instances up to 439 cities, the best results were less than 1% in excess ofthe known optima. In the average, for all instances, results are 2.538% in excess. Simularion results indicated that the proposed hybrid model performs robustly. These results encourage further research and improvement of the hybrid model to tackle with hard combinatorial problems.	local search (optimization);particle swarm optimization;travelling salesman problem	Heitor Silvério Lopes;Leandro dos Santos Coelho	2005		10.1109/ICHIS.2005.86	mathematical optimization;lin–kernighan heuristic;artificial intelligence;machine learning;mathematics;3-opt	AI	23.8331240668448	1.1882072184587815	39840
ceae9ad6c590fa2f66098b9752b9d3172c76340b	analysing bidder performance in randomised and fixed-deadline automated auctions	performance measure;automated auctions;analytical models;stochastic process;stochastic processes;state space;stochastic model;analytical model	"""The rule with which automated computerised auctions are closed play an important role in determining bidder strategies and auction outcomes. In this paper we examine two such rules: auctions with randomised closing times and fixed deadlines. To this end, stochastic models of auctions with discrete state-space representing the prices attained are developed and analysed. The models allow us to determine the stationary probabilistic outcomes of the auctions, which are used to examine the bidder performance, measured as the savings it makes with respect to the maximum payable or its payoff. For this purpose, one bidder is singled out as the """"special bidder"""" (SB) and its performance is studied as a function of the speed with which it raises the price, or its bid rate. The results show that with random closures, the SB has incentives to place bids promptly to obtain high savings; on the other hand, with fixed deadline auctions, the SB should choose its bid rate with respect to the other system parameters in order to maximise payoffs."""		Kumaara Velan;Erol Gelenbe	2010		10.1007/978-3-642-13541-5_5	stochastic process;generalized second-price auction;state space;stochastic modelling;common value auction	AI	3.636672662882196	-1.843189942419813	39851
f0aba02c412efb0b7e990adeb93718843c87a054	minimizing waiting times in a route design problem with multiple use of a single vehicle		In this study we introduce a routing problem with multiple use of a single vehicle and service time in demand points (clients) with the aim of minimizing the sum of clients waiting time to receive service. This problem is relevant in the distribution of aid, in disaster stricken communities, in the recollection and/or delivery of perishable goods and personnel transportation, among other situations, where reaching clients to perform service, fast and fair, is a priority. We consider vehicle capacity and travel distance constraints which force multiple use of the vehicle in the planning horizon. This paper presents and compares two mixed integer formulations for this problem, based on a multi–level network.		Francisco Ángel-Bello;Iris Martínez-Salazar;Ada M. Alvarez	2013	Electronic Notes in Discrete Mathematics	10.1016/j.endm.2013.05.102	simulation;computer security	Theory	13.439837159198445	0.7653463538273245	39869
b54f9e05120d0e64e2e27ab24aef66ff7f1cd2cb	part dispatch in multi-stage card lines	printed circuits;job shop scheduling;uncertainty;buffer storage;linear functionals;feedback;model uncertainty;linear quadratic;lot sizing;assembly systems;production planning;circuit testing;uncertainty circuit testing job shop scheduling feedback printed circuits lot sizing production planning assembly systems buffer storage dispatching;dispatching	This paper describes a model for making lot sizing and part dispatch decisions in multistage tester areas. The model explicitly models uncertainties such as random yield. A variant of the Linear Quadratic problem results for the high volume systems under consideration. The resulting feedback law enables the explicit computation of part dispatch quantities as linear functions of the local in-process inventory and arrivals. The effectiveness of the feedback policy is evaluated under different conditions.		Ram Akella;Sampath Rajagopal;Praveen Kumar	1986		10.1109/ROBOT.1986.1087654	job shop scheduling;mathematical optimization;real-time computing;uncertainty;computer science;engineering;feedback;printed circuit board;statistics	Robotics	7.988558161486365	1.3667105309678016	39899
0790f289c3e598015380aa12220b44436e6cb885	modeling the ase 20 greek index using artificial neural networks combined with genetic algorithms	feedforward neural network;feedforward neural networks;quantitative trading strategies;transaction cost;fixed time;backpropagation;trading strategy;momentum and backpropagation;moving average;indexation;genetic algorithm;genetic algorithms;transaction costs;genetic algo rithm;training algorithm;neural network	The motivation for this paper is to investigate the use of alternative novel neural network architectures when applied to the task of forecasting and trading the ASE 20 Greek Index using only autoregressive terms as inputs. This is done by benchmarking the forecasting performance of 4 different neural network training algorithms with some traditional techniques, either statistical such as an autoregressive moving average model (ARMA), or technical such as a moving average convergence/divergence model (MACD), plus a naive strategy. For the best training algorithm found, we used a genetic algorithm to find the best feature set, in order to enhance the performance of our models. More specifically, the trading performance of all models is investigated in a forecast and trading simulation on ASE 20 fixing time series over the period 2001-2009 using the last one and half year for out-of-sample testing. As it turns out, the combination of the neural network with genetic algorithm, does remarkably well and outperforms all other models in a simple trading simulation exercise and when more sophisticated trading strategies as transaction costs were applied.	artificial neural network;genetic algorithm;neural networks	Andreas S. Karathanasopoulos;Konstantinos A. Theofilatos;Panagiotis M. Leloudas;Spiridon D. Likothanassis	2010		10.1007/978-3-642-15819-3_58	feedforward neural network;transaction cost;genetic algorithm;computer science;artificial intelligence;machine learning;data mining;artificial neural network	NLP	6.892753053080157	-19.799076901143817	39903
43524f73971315fc1e11671a6ee8d63cc30559a3	genetic algorithms for optimization of boids model	modelizacion;esquiva colision;linear combination;optimisation;aplicacion militar;application militaire;modele agrege;optimizacion;ingenierie connaissances;grupo escolar;agregat;modelo agregado;vertebrata;alimentacion maquina;intelligence artificielle;algoritmo genetico;agregado;modelisation;seguimiento modelo;poursuite modele;obstacle avoidance;combinacion lineal;school complex;pisces;machine feed;model following;military application;algorithme genetique;alimentation machine;aggregate model;artificial intelligence;genetic algorithm;optimization;collision avoidance;inteligencia artificial;groupe scolaire;esquive collision;modeling;combinaison lineaire;knowledge engineering;aggregate	In this paper, we present an extended boids model for simulating the aggregate moving of fish schools in a complex environment. Three behavior rules are added to the extended boids model: following a feed; avoiding obstacle; avoiding enemy boids. The moving vector is a linear combination of every behavior rule vector, and the coefficients should be optimized. We also proposed a genetic algorithm to optimize the coefficients. Experimental results show that by using the GA-based optimization, the aggregate motions of fish schools become more realistic and similar to behaviors of real fish world.	boids;genetic algorithm	Yen-Wei Chen;Kanami Kobayashi;Xinyin Huang;Zensho Nakao	2006		10.1007/11893004_7	simulation;geography;artificial intelligence;cartography	Theory	16.054793534725345	-19.393814433781696	39918
017af1164a8ec319277a61e88d64966d4db44c86	do promotions benefit manufacturers, retailers, or both?	performance measure;empirical generalizations;market research;manufacturers versus retailers;vector autoregressive model;weighted least square;margin;accounting;market structure;implications;product;large scale;category management;vector autoregressive models;marketing;generic point;effects;brands;profitability;market;long term profitability;cumulant;sales promotions	While there has been strong managerial and academic interest in price promotions, much of the focus has been on the impact of such promotions on category sales, brand sales and brand choice. In contrast, little is known about the long-run impact of price promotions on manufacturer and retailer revenues and margins, although both marketing researchers and practitioners consider this a priority area (Marketing Science Institute 2000). Do promotions generate additional revenue and for whom? Which brand, category and market conditions influence promotional benefits and their allocation across manufacturers and retailers? To answer these questions, we conduct a large-scale econometric investigation of the effects of price promotions on manufacturer revenues, retailer revenues and total profits (margins). This investigation proceeds in two steps. First, persistence modeling reveals the shortand long-run effects of price promotions on these performance measures. Second, weighted least-squares analysis shows to what extent brand characteristics and promotional policies, as well as market-structure and category characteristics, influence promotional impact. A first major finding of our analyses is that a price promotion typically does not have permanent monetary effects for either party. Second, in terms of the cumulative, over-time, promotional impact on their revenues, we find significant differences between the manufacturer and retailer. Price promotions have a predominantly positive impact on manufacturer revenues, but their effects on retailer revenues are mixed. Moreover, retailer category margins are typically reduced by price promotions. Even when accounting for cross-category and store-traffic effects, we still find evidence that price promotions are typically not beneficial to the retailer. Third, our results indicate that manufacturer revenue elasticities are higher for promotions of small-share brands, for national brands and for frequently promoted brands. Moreover, they are higher for impulse products and in categories with a low degree of brand proliferation and low private-label shares. Retailer revenue elasticities, in turn, are higher for brands with frequent and shallow promotions, for impulse products and in categories with a low degree of brand proliferation. As such, from a revenue-generating point of view, manufacturer and retailer interests are often aligned in terms of which categories and brands to promote. Finally, retailer margin elasticities are higher for promotions of small-share brands and for brands with infrequent and shallow promotions. Thus, the implications with respect to the frequency of promotions depend upon the performance measure the retailer chooses to emphasize. The paper discusses the managerial implications of our results for both manufacturers and retailers, and suggests various avenues for future research.	benchmark (computing);generalized least squares;least squares;log-space reduction;margin classifier;marginal model;marketing science;nonlinear system;persistence (computer science);point of view (computer hardware company);stationary process	Shuba Srinivasan;Koen H. Pauwels;Dominique M. Hanssens;Marnik G. Dekimpe	2004	Management Science	10.1287/mnsc.1040.0225	market research;product;margin;economics;marketing;generic point;brand;mathematics;market structure;microeconomics;commerce;profitability index;cumulant	Web+IR	-1.667831598150526	-8.849158812055789	39926
9603f6419b1db7cd7dd07afaa080be41003e8e14	application of stochastic control theory to the optimal portfolio selection problem	risk management;investment;stochastic systems investment partial differential equations risk management;hjb equation optimal portfolio stochastic control;partial differential equations;portfolios process control control theory equations yttrium markov processes;hjb theorem stochastic control theory optimal portfolio selection problem assets risk level power functions natural logarithmic utility function hamilton jacobi bellman theorem;stochastic control;stochastic systems;optimal portfolio;hjb equation	Application of stochastic control theory to the optimal portfolio selection problem, in the case when portfolio consists of two assets with different level of risk is illustrated. Choosing power functions and natural logarithmic for the utility function, and using a converse of Hamilton-Jacobi-Bellman (HJB) theorem, the formula for optimal portfolio is derived.	coefficient;control theory;diversification (finance);hamilton–jacobi–bellman equation;jacobi method;selection algorithm;stochastic control;utility	Milos Japundzic;Dragan Jocic;Ivan Pavkov	2012	2012 IEEE 10th Jubilee International Symposium on Intelligent Systems and Informatics	10.1109/SISY.2012.6339491	financial economics;mathematical optimization;merton's portfolio problem;economics;portfolio optimization;mathematical economics	Embedded	1.7151923508135485	-1.6682509064781612	39945
2ca63bc5fae025d3b104a67ffa5beba28f5ca9ba	tupaq: an efficient planner for large-scale predictive analytic queries.		The proliferation of massive datasets combined with the development of sophisticated analytical techniques have enabled a wide variety of novel applications such as improved product recommendations, automatic image tagging, and improved speech-driven interfaces. These and many other applications can be supported by Predictive Analytic Queries (PAQs). A major obstacle to supporting PAQs is the challenging and expensive process of identifying and training an appropriate predictive model. Recent efforts aiming to automate this process have focused on single node implementations and have assumed that model training itself is a black box, thus limiting the effectiveness of such approaches on largescale problems. In this work, we build upon these recent efforts and propose an integrated PAQ planning architecture that combines advanced model search techniques, bandit resource allocation via runtime algorithm introspection, and physical optimization via batching. The result is TUPAQ, a component of the MLbase system, which solves the PAQ planning problem with comparable quality to exhaustive strategies but an order of magnitude more efficiently than the standard baseline approach, and can scale to models trained on terabytes of data across hundreds of machines.	algorithm;baseline (configuration management);black box;introspection;mathematical optimization;paq;predictive modelling;terabyte	Evan R. Sparks;Ameet S. Talwalkar;Michael J. Franklin;Michael I. Jordan;Tim Kraska	2015	CoRR		database;architecture;data mining;implementation;planner;machine learning;computer science;limiting;resource allocation;artificial intelligence	ML	21.703250165041908	-20.492120585568276	39950
2b3c4340c744f51ebb58dcab239a3311781ad42d	evolution of voronoi-based fuzzy controllers	genetique;parallelisme;robot movil;metodo adaptativo;linguistique;genetic operator;fuzzy controller;representacion sistema;linguistic variable;desigualdad variacional;control difusa;diagramme voronoi;sintesis control;genetica;mobile robot;fuzzy rules;inegalite variationnelle;fuzzy control;logique floue;hombre;logica difusa;methode adaptative;development process;algoritmo genetico;genetics;fuzzy logic;resolucion problema;prevencion esquiva colision;parallelism;linguistica;paralelismo;obstacle avoidance;robot mobile;biomimetique;synthese commande;representation systeme;fonction appartenance;prevention esquive collision;adaptive method;human;system representation;membership function;algorithme genetique;variational inequality;reparation;algorithme evolutionniste;completitud;genetic algorithm;algoritmo evolucionista;collision avoidance;funcion pertenencia;completeness;evolutionary algorithm;reparacion;diagrama voronoi;completude;control synthesis;fuzzy system;moving robot;problem solving;resolution probleme;repair;voronoi diagram;commande floue;homme;biomimetics;linguistics	A fuzzy controller is usually designed by formulating the knowledge of a human expert into a set of linguistic variables and fuzzy rules. One of the most successful methods to automate the fuzzy controllers development process are evolutionary algorithms. In this work, we propose a so-called “approximative” representation for fuzzy systems, where the antecedent of the rules are determined by a multivariate membership function defined in terms of Voronoi regions. Such representation guarantees the -completeness property and provides a synergistic relation between the rules. An evolutionary algorithm based on this representation can evolve all the components of the fuzzy system, and due to the properties of the representation, the algorithm (1) can benefit from the use of geometric genetic operators, (2) does not need genetic repair algorithms, (3) guarantees the completeness property and (4) can implement previous knowledge in a simple way by using adaptive a priori rules. The proposed representation is evaluated on an obstacle avoidance problem with a simulated mobile robot.	evolutionary algorithm;experiment;fuzzy control system;fuzzy rule;genetic operator;mobile robot;obstacle avoidance;simulation;synergy;voronoi diagram	Carlos Kavka;Marc Schoenauer	2004		10.1007/978-3-540-30217-9_55	fuzzy logic;biomimetics;mobile robot;variational inequality;genetic algorithm;voronoi diagram;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;completeness;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;genetic operator;calculus;genetic representation;mathematics;obstacle avoidance;fuzzy associative matrix;fuzzy set operations;software development process;algorithm;fuzzy control system	AI	23.420997595455724	-12.669447644782204	39956
8dd187710c6d51f3ad4bd288f7c8bf07b8de302a	quantitative rough sets based on subsethood measures	three way decision;subsethood measure;inclusion degree;quantitative rough set;qualitative rough set	Subsethood measures, also known as set-inclusion measures, inclusion degrees, rough inclusions, and rough-inclusion functions, are generalizations of the set-inclusion relation for representing graded inclusion. This paper proposes a framework of quantitative rough sets based on subsethood measures. A specific quantitative rough set model is defined by a particular class of subsethood measures satisfying a set of axioms. Consequently, the framework enables us to classify and unify existing generalized rough set models (e.g., decision-theoretic rough sets, probabilistic rough sets, and variable precision rough sets), to investigate limitations of existing models, and to develop new models. Various models of quantitative rough sets are constructed from different classes of subsethood measures. Since subsethood measures play a fundamental role in the proposed framework, we review existing methods and introduce new methods for constructing and interpreting subsethood measures.	rough set	Yiyu Yao;Xiaofei Deng	2014	Inf. Sci.	10.1016/j.ins.2014.01.039	combinatorics;discrete mathematics;mathematics;dominance-based rough set approach	AI	-2.639281216944136	-22.829399555485807	39959
93aafd3ba8bdb54f0a3cf2939a48411b47255b2d	design of the control system for hydraulic experimental bench based on labview	control engineering education;virtual instrumentation;software control systems testing instruments data acquisition computers education;computer aided instruction;data processing;virtual instrument;control system;hydraulic control equipment;dynamic sensors control system design hydraulic experimental bench labview teaching process hydraulic online testing data acquisition card daq virtual instrument develop platform data saving data processing man machine interface;man machine interface;control engineering computing;dynamic characteristic;experiment control system;online testing;vi control system hydraulic bench;data acquisition;teaching;virtual instrumentation computer aided instruction control engineering computing control engineering education data acquisition hydraulic control equipment teaching	Hydraulic experimental benches are indispensable in teaching process. In order to improve the experiment effect, establish the concept of hydraulic online testing, a control system was designed, which mainly consist of computer, data acquisition card (DAQ), and several sensors. The experimental software is developed on the famous virtual instrument (VI) develop platform — LabVIEW, and provides several functions such as control, measure, data saving, data processing, result show and so on. With the experimental program customized in the system, general experiments can be done under the guide of control system by man-machine interface. The experiment control system uses dynamic sensors to measure all kinds parameters in experiment process, so it is more suitable for hydraulic dynamic characteristic tests.	control system;data acquisition;experiment;labview;sensor;sound quality;user interface;virtual instrumentation	Cui Zhang;Xiaobin Wang;Zhigang Wang	2011	Proceedings of 2011 International Conference on Electronic & Mechanical Engineering and Information Technology	10.1109/EMEIT.2011.6023521	human–machine interface;embedded system;simulation;data processing;computer science;engineering;control system;control theory;data acquisition;computer engineering	Robotics	16.465874593760596	-12.377847388086074	39961
b29f270b93051c70a41f5603f78f73c6c47f60ab	a hybrid tabu search algorithm for automatically assigning patients to beds	decision support;admission scheduling;scheduling algorithm;tabu search algorithm;itec;metaheuristic	OBJECTIVE We describe a patient admission scheduling algorithm that supports the operational decisions in a hospital. It involves efficiently assigning patients to beds in the appropriate departments, taking into account the medical needs of the patients as well as their preferences, while keeping the number of patients in the different departments balanced.   METHODS Due to the combinatorial complexity of the admission scheduling problem, there is a need for an algorithm that intelligently assists the admission scheduler in taking decisions fast. To this end a hybridized tabu search algorithm is developed to tackle the admission scheduling problem. For testing, we use a randomly generated data set. The performance of the algorithm is compared with an integer programming approach.   RESULTS AND CONCLUSION The metaheuristic allows flexible modelling and presents feasible solutions even when disrupted by the user at an early stage in the calculation. The integer programming approach is not able to find a solution in 1h of calculation time.	beds;integer (number);integer programming;limited stage (cancer stage);metaheuristic;patients;procedural generation;scheduling (computing);scheduling - hl7 publishing domain;search algorithm;solutions;tabu search	Peter Demeester;Wouter Souffriau;Patrick De Causmaecker;Greet Vanden Berghe	2010	Artificial intelligence in medicine	10.1016/j.artmed.2009.09.001	fair-share scheduling;mathematical optimization;dynamic priority scheduling;tabu search;computer science;artificial intelligence;theoretical computer science;machine learning;scheduling;metaheuristic;guided local search	AI	12.86542445242953	-1.2289390331846397	40026
13137f4081812c68b52dd04a35c6ce13f6037795	fuzzy heuristic solution approaches for the warm/cold lot sizing problem	warm cold process;lot sizing;heuristics;fuzzy systems	In this paper we introduce fuzzy versions some rule based lot sizing heuristics for the dynamic lot-sizing problem with warm/cold process. In our setting “the demand at each period” and “the warm system threshold” (production/order quantity required for keeping the system warm on to next period) are fuzzy numbers. Similar to the crisp counterpart setting of the problem, horizon length, production capacity at	dynamic lot-size model;fuzzy number;heuristic (computer science);horizon effect	A. Altay;Ayhan Özgür Toy;Yeliz Ekinci	2016	Appl. Soft Comput.	10.1016/j.asoc.2016.07.058	mathematical optimization;fuzzy transportation;computer science;artificial intelligence;fuzzy number;heuristics;operations research;fuzzy control system	AI	15.050958683850974	-3.4320726906489165	40035
127c3cce30bf7840e5d65914012aace3c724cc5a	pricing network edges for heterogeneous selfish users	game theory;travel time;nash equilibrium;nash equilibria;selfish routing;marginal cost pricing;network traffic;optimal routing;network pricing;heterogeneous network	We study the negative consequences of selfish behavior in a congested network and economic means of influencing such behavior. We consider a model of selfish routing in which the latency experienced by network traffic on an edge of the network is a function of the edge congestion, and network users are assumed to selfishly route traffic on minimum-latency paths. The quality of a routing of traffic is measured by the sum of travel times (the total latency).It is well known that the outcome of selfish routing (a Nash equilibrium) does not minimize the total latency. An ancient strategy for improving the selfish solution is the principle of marginal cost pricing, which asserts that on each edge of the network, each network user on the edge should pay a tax offsetting the congestion effects caused by its presence. By pricing network edges according to this principle, the inefficiency of selfish routing can always be eradicated.This result, while fundamental, assumes a very strong homogeneity property: all network users are assumed to trade off time and money in an identical way. The guarantee also ignores both the algorithmic aspects of edge pricing and the unfortunate possibility that an efficient routing of traffic might only be achieved with exorbitant taxes. Motivated by these shortcomings, we extend this classical work on edge pricing in several different directions and prove the following results.We prove that the edges of a single-commodity network can always be priced so that an optimal routing of traffic arises as a Nash equilibrium, even for very general heterogeneous populations of network users.When there are only finitely many different types of network users and all edge latency functions are convex, we show how to compute such edge prices efficiently.We prove that an easy-to-check mathematical condition on the population of heterogeneous network users is both necessary and sufficient for the existence of edge prices that induce an optimal routing while requiring only moderate taxes.	algorithm;convex function;marginal model;nash equilibrium;network congestion;network traffic control;population;routing	Richard Cole;Yevgeniy Dodis;Tim Roughgarden	2003		10.1145/780542.780618	game theory;static routing;mathematical economics;nash equilibrium	Theory	-3.1063614011547678	2.5543730970335154	40055
d6b89d20995ffd5ad8187383ec8ac3ffa139ec9c	hellinger distance for fuzzy measures	probability theory and statistics;mathematics;comunicacion de congreso;capacities;mathematical statistics;natural sciences;hellinger distancefuzzy measuresradon nikodym derivativechoquet integralcapacities;choquet integral;matematisk statistik;radon nikodym derivative;fuzzy measures;hellinger distance;sannolikhetsteori och statistik	Hellinger distance is a distance between two additive measures defined in terms of the RadonNikodym derivative of these two measures. This measure proposed in 1909 has been used in a large variety of contexts. In this paper we define an analogous measure for fuzzy measures. We discuss them for distorted probabilities and give two examples.	fuzzy logic;fuzzy measure theory;radon–nikodym theorem;utility functions on indivisible goods	Vicenç Torra;Yasuo Narukawa;Michio Sugeno;Michael Carlson	2013		10.2991/eusflat.2013.82	mathematical analysis;calculus;fuzzy measure theory;mathematics;statistics;hellinger distance	Vision	0.4853003704342065	-21.34253219657594	40058
af05bcf713102fd4e9eb1e3e03175e9a638e894e	optimal digital rights management with uncertain piracy	software;probability;apple itunes service optimal digital rights management uncertain piracy digital copy copyrighted materials legitimate customers probability pirated copy legal action firms drm protected copy observable firm behaviors price discrimination protected files weaker protection eventual abandonment drm protections;uncertainty;pricing;copy protection;copyright;industries;computer crime;materials;digital rights management;uncertainty digital rights management digital piracy pricing versioning;digital piracy;software industry;uncertainty software industries marketing and sales electronic publishing materials profitability;profitability;electronic publishing;versioning;price discrimination;digital right management;marketing and sales;organisational aspects;probability computer crime copy protection copyright digital rights management organisational aspects pricing	Many firms that sell digital copies of copyrighted materials online face a common dilemma: the use of Digital Rights Management to impede pirates often also has negative implications for legitimate customers. We introduce a two-period model in which the use of DRM in the first period affects the probability of consumers encountering pirated copies in the second period, the threat of legal action affects the probability of consumers obtaining pirated copies, and firms choose whether to sell, and at what prices, either strongly or weakly DRM protected copies, or both. We are able to explain a range of observable firm behaviors with this model, including the use of price discrimination to offer both strongly and weakly protected files simultaneously, with weaker protection commanding a higher price, and the eventual abandonment of DRM protections, both of which have been observed at various times, for example, with Apple's iTunes service.	digital recording;digital rights management;observable;stop online piracy act	Robert F. Easley;Byung Cho Kim;Daewon Sun	2012	2012 45th Hawaii International Conference on System Sciences	10.1109/HICSS.2012.460	pricing;uncertainty;economics;computer science;software versioning;marketing;probability;digital rights management;database;advertising;electronic publishing;management;law;world wide web;price discrimination;computer security;statistics;profitability index	DB	-3.5585049415039745	-7.785711856139665	40061
7fc0bd7ba894081fba5682a216ff4adcf1d09cbc	a simple dynamic model for the dispersion of motorway traffic emission	pollution dispersion models;qa75 electronic computers computer science szamitastechnika;traffic emission;szamitogeptudomany;gaussian processes;sensitivity analysis traffic emission emission dispersion;exhaust gases;dispersion mathematical model wind speed pollution boundary conditions computational modeling approximation methods;air pollution control;road traffic control;vehicle dynamics air pollution control distributed parameter systems gaussian processes road traffic control;sensitivity analysis;pollution aware traffic controllers dynamic model motorway traffic emission dispersion process model distributed parameter system conservation law balance volumes wind direction plug flow pollution absorption gaussian plume model boundary conditions macroscopic emission model sensitivity is control system structure;distributed parameter systems;emission dispersion;wind;vehicle dynamics;rural areas	In this work a modeling approach is introduced for the dispersion of motorway traffic emissions. The process model is developed for a distributed parameter system, and is derived based on the conservation law within the balance volumes between the road and the rural area, specified by the wind direction. Parallel to the wind, plug flow is considered, and for the absorption of pollution a simplified version of the Gaussian plume model is used. For the boundary conditions of the model, the output of the macroscopic emission model, introduced in [Csikós et al. (2012)] is substituted. A sensitivity analysis is performed on the proposed model which justifies the preconception on the future control system structure.	control system;mathematical model;plume (fluid dynamics);process modeling	Alfréd Csikós;István Varga;Katalin M. Hangos	2013	16th International IEEE Conference on Intelligent Transportation Systems (ITSC 2013)	10.1109/ITSC.2013.6728452	meteorology;simulation;environmental engineering;engineering	Robotics	9.511575610150185	-11.052462891789913	40076
1c0c9866de44c8920704bce6d852b38dfecfcc15	using a cooking operation simulator to improve cooking speed in a multiproduct japanese cuisine restaurant		This study was conducted to improve the cooking speed in multiproduct Japanese cuisine restaurants using a cooking operation simulator. Traditionally, restaurants improve cooking speed through menu and cooking operation simplification because the cooking speed depends strongly on customer satisfaction and productivity. In recent years, customer requirements for restaurant menus have become diverse. A restaurant must evolve the menu and cooking operations to adapt to customer needs. Cooking systems of multiproduct restaurants can produce diverse menus, but the cooking speed is low. They should improve the cooking speed to improve customer satisfaction and productivity.		Takeshi Shimmura;Yoshihiro Hisano;Syuichi Ohura;Tomoyuki Asakawa;Toshiya Kaihara;Nobutada Fujii;Tomomi Nonaka	2014		10.1007/978-3-662-44736-9_67	food science;engineering;operations management;advertising	NLP	6.010792393637735	-9.569184160848229	40088
845c7b82e79d6ddaa450011ff63d1ce96cf008bc	an extreme learning machine approach for modeling evapotranspiration using extrinsic inputs	limited data;extreme learning machine;least square support vector machine;evapotranspiration;arid region	Precise estimation of evapotranspiration is crucial for accurate crop-water estimation. Recently machine learning (ML) techniques like artificial neural network (ANN) are being widely used for modeling the process of evapotranspiration. However, ANN faces issues like trapping in local minima, slow learning and tuning of meta-parameters. In this study an improved extreme learning machine (ELM) algorithm was used to estimate weekly reference crop evapotranspiration (ETo). The study was carried out for Jodhpur and Pali meteorological weather stations located in the Thar Desert, India. The study evaluated the performance of three different input combinations. The first input combination used locally available maximum and minimum air temperature data while the second and third combination used ETo values from another station (extrinsic inputs) along with the locally available temperature data as inputs. The performance of ELM models was compared with the empirical Hargreaves equation, ANN and leastsquare support vector machine (LS-SVM) models. Root mean squared error (RMSE), Nash–Sutcliffe model efficiency coefficient (NSE) and threshold statistics (TS) were used for comparing the performance of the models. The performance of ELM model was found to be better than the Hargreaves and ANN model. The LS-SVM and ELM displayed similar performance. ELM3 models, with 36 and 33 neurons in hidden layer were found to be the best models (RMSE of 0.43 for Jodhpur and 0.33 for Pali station) for estimating weekly ETo at Jodhpur and Pali stations respectively. The results showed that ELM is a simple yet efficient algorithm which exhibited good performance; hence, can be recommended for estimating weekly ETo. Furthermore, it was also found that use of ETo values from another station can help in improving the efficiency of ML models in limited data scenario. 2016 Elsevier B.V. All rights reserved.	algorithm;artificial neural network;coefficient;elm;least squares;machine learning;maxima and minima;mean squared error;nash equilibrium;network search engine;sparse matrix;support vector machine;trap (computing)	Amit Prakash Patil;Paresh Chandra Deka	2016	Computers and Electronics in Agriculture	10.1016/j.compag.2016.01.016	evapotranspiration;arid;engineering;artificial intelligence;machine learning	ML	10.795645046761138	-19.485305889356322	40128
d89c5b43340a9a4a274bfd385d2f40500d8c8a0a	on the j-divergence of intuitionistic fuzzy sets with its application to pattern recognition	intuitionistic fuzzy set;distance measure;divergence;clustering;communication theory;pattern recognition;entropy;information theoretic;similarity measure	The importance of suitable distance measures between intuitionistic fuzzy sets (IFSs) arises because of the role they play in the inference problem. A concept closely related to one of distance measures is a divergence measure based on the idea of information-theoretic entropy that was first introduced in communication theory by Shannon (1949). It is known that Jdivergence is an important family of divergences. In this paper, we construct J-divergence between IFSs. The proposed Jdivergence can induce some useful distance and similarity measures between IFSs. Numerical examples demonstrate that the proposed measures perform well in clustering and pattern recognition. 2007 Elsevier Inc. All rights reserved.	algorithm;cluster analysis;embedded system;fuzzy set;information theory;installable file system;intuitionistic logic;pattern recognition;shannon (unit);yang	Wen-Liang Hung;Miin-Shen Yang	2008	Inf. Sci.	10.1016/j.ins.2007.11.006	entropy;discrete mathematics;computer science;machine learning;pattern recognition;mathematics;cluster analysis;divergence;communication theory	AI	-2.912188611403591	-22.998608891490548	40143
ffe64a8bbe91397c28c3fa9a35b87b7c1f065b7e	a new fuzzy programming method to derive the priority vector from an interval reciprocal comparison matrix	fuzzy programming;inconsistent interval reciprocal comparison matrix;priority vector;期刊论文;interval reciprocal comparison matrix	In this paper, we discuss the interval reciprocal comparison matrices and suggest a new fuzzy programming method (NFPM) to derive the priority vector from an interval reciprocal comparison matrix. Based on Mikhailovu0027s membership function, we first give a membership function, which is used to measure the decision makeru0027s satisfaction degree for the priority vector derived from each interval constraint. Then by using the membership function, we propose a new fuzzy programming method to get the most satisfaction degree of each priority vector. The NFPM has many prominent characteristics, especially, it is capable of deriving the priority vector from a consistent or inconsistent interval reciprocal comparison matrix. Moreover, the NFPM can overcome some drawbacks of Mikhailovu0027s fuzzy programming method (FPM) for inconsistent interval reciprocal comparison matrices. Finally, several examples are given to illustrate the applicability and advantage of the proposed method.		Liuhao Chen;Zeshui Xu	2015	Inf. Sci.	10.1016/j.ins.2015.04.015	mathematical optimization;discrete mathematics;mathematics;algorithm	AI	-3.1519699107099215	-20.269994217150394	40148
f2207d716d236bc34e5d8b585c098ecf374192a7	test economics - what can a board/system test engineer do to influence supply operation metrics	system testing systems engineering and theory cost function virtual manufacturing outsourcing contracts assembly bills of materials environmental economics investments;outsourcing;return on investment;test enhancements;investment;contract manufacturers;contract manufacturers test economics board system test engineer supply operation metrics outsourcing return on investment test enhancements roi product costs;supply chain management economics investment outsourcing production testing;board system test engineer;economics;production cost;product costs;production testing;roi;supply operation metrics;supply chain management;cost model;test economics	"""The outsourcing of manufacturing to low cost regions has significantly changed product cost models. The impact of test may be much more difficult to determine in lower cost regions. This paper presents some background on product cost models and the effect that outsourcing has on these cost models. In light of these affect the concept of return on investment for """"test enhancements"""" is discussed. The paper provides examples of how test enhancements can generate both positive and negative ROI. The paper concludes by presenting ideas on how test engineers can influence product costs, in both a positive and negative way. The reader should note that these examples use hypothetical data based on quotes received over the last several years from contract manufacturers. The intent of these examples is to illustrate the concepts presented"""	outsourcing;region of interest;system testing;test engineer;test strategy	Sylvain Tourangeau;Bill Eklow	2006	2006 IEEE International Test Conference	10.1109/TEST.2006.297650	reliability engineering;return on investment;supply chain management	SE	2.6785697383160576	0.9438623173614665	40267
6e904afaf13dd65f77b2ca6f03cdcb1856a3b807	instability of mixed nash equilibria in generalised hawk-dove game: a project conflict management scenario		This paper generalises the Hawk-Dove evolutionary game by introducing cost sharing ratios for both players, and applies the generalised Hawk-Dove model to conflict management in projects through investigating the stability of Nash equilibria. A model with clashing interests between a project owner and a contractor is considered to derive their strategy adaptation given the cost sharing ratios. As expected, the pure Nash equilibria are shown to be dominantly stable while the mixed strategy equilibrium is observed to be unstable, across the range of considered cost sharing ratios. In addition, simulations are conducted on the strategy adaptation and stability of the equilibria under noisy and latent conditions. The obtained results can be used by project managers in optimising their strategy in practice.	control theory;instability;nash equilibrium;simulation	Sheryl Le Chang;Mikhail Prokopenko	2017	Games	10.3390/g8040042	microeconomics;economics;welfare economics;epsilon-equilibrium;risk dominance;best response;correlated equilibrium;nash equilibrium;coordination game;matching pennies;trembling hand perfect equilibrium	AI	-4.00279216177117	-5.369793628523056	40293
10d23fca0348c357d95d6300536e545b44a59fe5	stability in a non-autonomous iterative system: an application to oligopoly	non autonomous systems;perfect competition;nationalekonomi;cournot equilibrium;autonomic system;stability;synchronization;iterative maps;economics;equilibrium state	This paper reconsiders the relation between oligopoly and perfect competition, more specifically the problem of emergent instability when the number of competitors increases, as pointed out by several authors. A process of mixed short and long run dynamics is set up. In the short run the competitors are subject to capacity limits due to fixed capital stocks, in the long run they may renew these stocks and so in the moments of reinvestment have access to a constant returns technology. The evolution of the system depends on the number of competitors, the interval between their entry on the market, and the durability of capital. The main result is a theorem showing that if capital has a durability of more periods than the spacing of reinvestment times among the firms, multiplied with their total number, then the system always contracts to the Cournot equilibrium state.	autonomous robot;iterative method	Anastasiia Panchuk;Tönu Puu	2009	Computers & Mathematics with Applications	10.1016/j.camwa.2009.06.048	synchronization;cournot competition;stability;thermodynamic equilibrium;perfect competition;statistics	HPC	-3.3331410622032447	-4.284282574790589	40306
d5ff926dc9de54d81c36373760fc209943bfd4c7	a bivariate fuzzy time series model to forecast the taiex	time series forecasting;stock index;fuzzy time series;neural networks;time series;indexation;futures index;bivariate models;neural network;time series model	Fuzzy time series models have been applied to forecast various domain problems and have been shown to forecast better than other models. Neural networks have been very popular in modeling nonlinear data. In addition, the bivariate models are believed to outperform the univariate models. Hence, this study intends to apply neural networks to fuzzy time series forecasting and to propose bivariate models in order to improve forecasting. The stock index and its corresponding index futures are taken as the inputs to forecast the stock index for the next day. Both in-sample estimation and out-of-sample forecasting are conducted. The proposed models are then compared with univariate models as well as other bivariate models. The empirical results show that one of the proposed models outperforms the many other models.	bivariate data;time series	Tiffany Hui-Kuang Yu;Kun-Huang Huarng	2008	Expert Syst. Appl.	10.1016/j.eswa.2007.05.016	econometrics;forecast error;computer science;time series;artificial neural network;statistics	DB	7.36737298674815	-19.91435011827449	40308
9726d5b01fcca5eaba19b713d0daa04862beea07	metaheuristics with local search techniques for retail shelf-space optimization	optimal solution;metaheuristics;shelf allocation;grupo de excelencia;retail industry;administracion de empresas;nonlinear problem;profitability;economia y empresa;network flow;grupo a;quantitative method;local search;retail;management science	E shelf-space allocation can provide retailers with a competitive edge. While there has been little study on this subject, there is great interest in improving product allocation in the retail industry. This paper examines a practicable linear allocation model for optimizing shelf-space allocation. It extends the model to address other requirements such as product groupings and nonlinear profit functions. Besides providing a network flow solution, we put forward a strategy that combines a strong local search with a metaheuristic approach to space allocation. This strategy is flexible and efficient, as it can address both linear and nonlinear problems of realistic size while achieving near-optimal solutions through easily implemented algorithms in reasonable timescales. It offers retailers opportunities for more efficient and profitable shelf management, as well as higher-quality planograms.	algorithm;flow network;local search (optimization);metaheuristic;nonlinear system;requirement	Andrew Lim;Brian Rodrigues;Xingwen Zhang	2004	Management Science	10.1287/mnsc.1030.0165	mathematical optimization;economics;marketing;operations management;retail;mathematics;microeconomics;management;metaheuristic;commerce	DB	0.6450301408720843	-4.385455613377849	40313
4d56b33c2980cbe796385b8da38c749982c0a37d	mission oriented robust multi-team formation and its application to robot rescue simulation		Team formation is the problem of selecting a group of agents, where each agent has a set of skills; the aim is to accomplish a given mission (a set of tasks), where each task is made precise by a skill necessary for managing it. In a dynamic environment that o↵ers the possibility of losing agents during a mission, e.g., some agents break down, the robustness of a team is crucial. In this paper, the focus is laid on the mission oriented robust multi-team formation problem. A formal framework is defined and two algorithms are provided to tackle this problem, namely, a complete and an approximate algorithm. In the experiments, these two algorithms are evaluated in RMASBench (a rescue multi-agent benchmarking platform used in the RoboCup Rescue Simulation League). We empirically show that (i) the approximate algorithm is more realistic for RMASBench compared to the complete algorithm and (ii) considering the robust mission multi-teams have a better control on the fire spread than the sophisticate solvers provided in RMASBench.	approximation algorithm;complete (complexity);experiment;heuristic (computer science);job shop scheduling;multi-agent system;performance;scheduling (computing);simulation;testbed	Tenda Okimoto;Tony Ribeiro;Damien Bouchabou;Katsumi Inoue	2016			robustness (computer science);simulation;artificial intelligence;machine learning;robot;benchmarking;computer science	AI	19.053421921751113	-14.586276097995738	40319
e7dea989b9fe257abd5d8f8e5b02df256617b655	structure features for sat instances classification	complex networks;sat solving;classifiers;portfolio	Abstract The success of portfolio approaches in SAT solving relies on the observation that different SAT solvers may dramatically change their performance depending on the class of SAT instances they are trying to solve. In these approaches, a set of features of the problem is used to build a prediction model, which classifies instances into classes, and computes the fastest algorithm to solve each of them. Therefore, the set of features used to build these classifiers plays a crucial role. Traditionally, portfolio SAT solvers include features about the structure of the problem and its hardness . Recently, there have been some attempts to better characterize the structure of industrial SAT instances. In this paper, we use some structure features of industrial SAT instances to build some classifiers of industrial SAT families of instances. Namely, they are the scale-free structure, the community structure and the self-similar structure. First, we measure the effectiveness of these classifiers by comparing them to other sets of SAT features commonly used in portfolio SAT solving approaches. Then, we evaluate the performance of this set of structure features when used in a real portfolio SAT solver. Finally, we analyze the relevance of these features on the analyzed classifiers.	boolean satisfiability problem;codec;fits;fractal dimension;relevance;solver	Carlos Ansótegui;Maria Luisa Bonet;Jesús Giráldez-Cru;Jordi Levy	2017	J. Applied Logic	10.1016/j.jal.2016.11.004	computer science;artificial intelligence;machine learning;mathematics;complex network;algorithm	Logic	20.66432981653585	-8.411599147129179	40333
beb7b4fac62c2a0ff16a4dcfe526b8dbf4df6f44	comparative analysis of mp-based solvers to optimize distribution problems in logistics		Distribution related problems in logistics have many decision variables and constraints that have to be considered simultaneously. Most often, these are the problems in the discrete optimization branch, modeled and optimized using operational research, in particular mathematical programming (MP) models and methods, such as mixed integer linear programming (MILP), integer programming (IP) and integer linear programming (ILP). These methods become ineffective very quickly in the case of larger size problems. Also, the number of decision variables and constraints may exceed the capacity of available MP solvers. To improve their efficiency and reduce the effective size of the solution space of a given problem, hybrid approaches are being developed, integrating MP with other environments. This article discusses the results of comparative analysis of several MP solvers (LINGO, SCIP, GUROBI) in the context of their use for optimization of selected distribution problems.	logistics	Jaroslaw Wikarek;Pawel Sitek;Tadeusz Stefanski	2018		10.1007/978-3-319-77179-3_10	constraint logic programming;integer programming;mathematical optimization;discrete optimization;computer science	Theory	15.93895629137081	2.119629269192849	40344
bb86e7ee953e517d5dd626904d8be81cddac743e	hybrid dynamic classifier for drift-like fault diagnosis in a class of hybrid dynamic systems: application to wind turbine converters	data mining;drift monitoring;drift like fault detection;machine learning;wind turbine;multicellular converters	Hybrid dynamic systems (HDS) combine both discrete and continuous dynamics. Discretely controlled continuous systems (DCCS) is an important class of HDS in which the system switches between several discrete modes in response to discrete control events issued by a discrete controller. Their continuous dynamics depend on the discrete mode in which the system is. Wind turbine converters are an example of DCCS. Faults in converters may impact significantly the availability and the production performance of wind turbines. These faults can occur as a gradual abnormal change in the values of parameters describing the system continuous dynamics in a discrete mode. In this case, they entail a drift in the system operating conditions until the failure takes over completely. Detecting this drift in early stage allows reducing the power production losses as well as the wind turbine unavailability and maintenance costs. However, this drift can be observed only when the system is in the discrete modes where the continuous dynamics described by the affected parameters are active. Consequently, this paper proposes an approach based on the use of hybrid dynamic classifier able to monitor a drift in normal operating conditions of the converter in discrete modes where the continuous dynamics are impacted by a parametric fault. This allows keeping the useful patterns representative of the drift and therefore to detect it in its early stage.	dynamical system	Houari Toubakh;Moamar Sayed Mouchaweh	2016	Neurocomputing	10.1016/j.neucom.2015.07.073	real-time computing;discrete event dynamic system;computer science;machine learning;control theory	AI	14.323421107367782	-14.960303350225198	40402
15b734d0a0859112ccb31d6bf8ea9b7e872b0c6f	induced uncertain pure linguistic hybrid averaging aggregation operator and its application to group decision making		In this paper, we propose a new aggregation operator under uncertain pure linguistic environment called the induced uncertain pure linguistic hybrid averaging aggregation (IUPLHAA) operator. Some of the main advantages and properties of the new operator are studied. Moreover, in the situations where the given arguments about all the attribute weights, the attribute values and the expert weights are expressed in the form of linguistic labels variables, we develop an approach based on the IUPLHAA operator for multiple attribute group decision making with uncertain pure linguistic environment. Finally, an illustrative example is given to verify the developed approach and to demonstrate its feasibility and practicality.	decision theory;fuzzy number;ibm systems network architecture;moe	Meirong Li;Bo Peng;Shouzhen Zeng	2015	Informatica, Lith. Acad. Sci.		artificial intelligence;data mining;mathematics	AI	-2.8258829471825058	-21.2711803607024	40417
e6225ce040ed7fda7a7b847ce4d09bc47d5486b5	rmars: robustification of multivariate adaptive regression spline under polyhedral uncertainty	mars;computacion informatica;finance;polyhedral uncertainty;robust optimization;rmars;ciencias basicas y experimentales;matematicas;grupo a	Since, with increased volatility and further uncertainties, financial crises translated a high ''noise'' within data from financial markets and economies into the related models, recent years' events in the financial world have led to radically untrustworthy representations of the future. Hence, robustification started to attract more attention in finance. The presence of noise and data uncertainty raises critical problems to be dealt with on the theoretical and computational side. For immunizing against parametric uncertainties, robust optimization has gained greatly in importance as a modeling framework from both a theoretical and a practical point of view. Consequently, we include the existence of uncertainty considering future scenarios in the multivariate adaptive regression spline (MARS) that has an apparent success in modeling real-life data in a variety of application fields, and robustify it through robust optimization proposed to cope with data and resulting model parameter uncertainty. We represent the new Robust MARS (RMARS) in theory and method and apply RMARS on financial market data. We demonstrate its good performance with a simulation study and a numerical experience that refers to basic economic indicators. Results indicate that models from RMARS have much less variability in parameter estimates and in accuracy measures, to the cost of just a slightly lower accuracy than MARS.	polyhedron;robustification;smoothing spline;spline (mathematics)	Ayse Özmen;Gerhard-Wilhelm Weber	2014	J. Computational Applied Mathematics	10.1016/j.cam.2013.09.055	econometrics;mathematical optimization;mars exploration program;robust optimization;calculus;control theory;mathematics;algorithm;statistics	Vision	5.819355234776001	-16.132596211991682	40434
3150f9cff189a1e1819f54e05425b173c82f8d88	a new method to predict short-term wind power in the renewable energy systems	renewable energy;uncertainty;short term wind power;simulation;anns;emd;artificial neural networks;wind energy;cbr;nonlinearity;case based reasoning;empirical mode decomposition;productivity prediction	The production of energy from renewable energy systems RES arrays has the properties of nonlinearity and uncertainty and usually fluctuates with the changes of weather and other factors. In this paper, an integrated method based on case-based reasoning CBR, empirical mode decomposition EMD and artificial neural network ANN is proposed to predict the productivity of the RES. At first, the non-stationary time series is pre-processed by CBR to find out the ten days with the most similarity to the predicted day. Then uses EMD decomposed times series of ten days we selected into a series of IMFs Intrinsic Mode Functions with features of stationarity and multiple time scale, for each IMF component, constructing models of ANN to predict. Finally would be straight line fit to final predict result. The simulation results show that the productivity predicted by using this integrated algorithm is more accurate than that of single artificial neutral network model.		Xiang Zheng;Xiong Chen	2016	IJWMC	10.1504/IJWMC.2016.078214	wind power;renewable energy;case-based reasoning;simulation;uncertainty;nonlinear system;computer science;artificial intelligence;hilbert–huang transform;machine learning;artificial neural network;statistics	EDA	8.94347267782435	-18.604949677976705	40439
a37ee530bdd67a6ab9a138118b647910da3c99d4	unpaced production lines with jointly unbalanced operation time means and buffer capacities - their behaviour and performance	unpaced lines;simulation;buffer storage size;line length;patterns of imbalance;idle time;imbalance patterns;average buffer level;uneven buffer capacities;unbalanced operation time means;unequal service time means;unpaced production lines	The performance of unpaced production lines that are unbalanced in terms of both their operation time means and buffer storage sizes is studied in this paper. The lines were simulated under their steady-state operational mode of operation with various values of line length, buffer storage capacity, degree of imbalance, and patterns of imbalance. Output data (principally idle time and average buffer level) were analysed utilising a number of statistical tools. In terms of idle time, it was found that the best unbalanced pattern is an MT bowl configuration, coupled with a distribution of buffer capacity as evenly as possible. With respect to ABL, the best pattern turned out to be a monotone decreasing MT order, together with an ascending buffer size order.	operation time;unbalanced circuit	Sabry Shaaban	2011	IJMTM	10.1504/IJMTM.2011.042108	real-time computing;computer science;engineering;operations management;engineering drawing	OS	8.92241978095388	3.322428307657015	40448
59f1f8603f0c54b5213e834ec73ca4dd0c05fa84	a hybrid ensemble of heterogeneous regressors for wind speed estimation in wind farms		This paper focuses on a problem of wind speed estimation in wind farms by proposing an ensemble of regressors in which the output of four different systems (Neural Networks (NNs), Suppor Vector Regressors (SVRs) and Gaussian Processes (GPRs)) will be the input of a final prediction system (An Extreme Learning Machine (ELM) in this case). Moreover, we propose to use variables from atmospheric reanalysis data as predictive inputs for the systems, which gives us the possibility of hybridizing numerical weather models with ML techniques for wind speed prediction in real systems. The experimental evaluation of the proposed system in real data from a wind farm in Spain has been carried out, with the subsequent discussion about the performance of the different ML regressors and the ensemble method tested in this wind speed prediction problem.		Laura Cornejo-Bueno;J. Acevedo-Rodríguez;Luis Prieto;Sancho Salcedo-Sanz	2018		10.1007/978-3-319-99626-4_9	extreme learning machine;artificial neural network;wind speed;machine learning;numerical weather prediction;gaussian process;computer science;artificial intelligence	ML	9.48515001486508	-19.3168612865251	40487
6cdff59b57c1808ad2fb5e4310bab598a69876f2	holdups and overinvestment in capital markets	holdup problems;investment;bargaining;trading frictions	This paper analyzes the problem of rms that need to make investment decisions in capital markets characterized by trading frictions and ex-post bargaining. Trading frictions make switching from one capital supplier to another costly, thus implying that the match between rms and suppliers generates surplus. Ex-post bargaining implies that suppliers can appropriate part of this surplus. Firms react strategically to the resulting holdup problem by overinvesting so as to reduce their marginal productivity and thus the negotiated price of capital. In a multifactor setting, the holdup problem in capital markets interacts with holdup problems in labor markets that typically lead to underinvestment and overemployment. This presents rms with a trade-o¤ that has non-trivial equilibrium e¤ects and that depending on the substitutability of capital and labor and the rms bargaining power in each market  can neutralize or exacerbate the distortionary e¤ects each of the holdup problems has on its own.	algorithmic trading;design rationale;marginal model;organizational behavior	André Kurmann	2014	J. Economic Theory	10.1016/j.jet.2014.02.004	physical capital;economics;financial capital;investment;capital intensity;finance;macroeconomics;microeconomics;market economy;labour economics	ECom	-2.6708309183646755	-5.727260196770095	40498
eda6d83991f24bead93151610c1574f7f77addea	application of anova to a cooperative-coevolutionary optimization of rbfns	multiobjective programming;programmation multiobjectif;optimisation;analisis estadistico;optimizacion;approximation algorithm;fonction base radiale;variance analysis;intelligence artificielle;coevolution;analisis programa;radial basis function;statistical analysis;function approximation;radial basis function network;biomimetique;analisis variancia;robustesse;analyse statistique;algoritmo aproximacion;artificial intelligence;robustness;optimization;program analysis;inteligencia artificial;analyse programme;algorithme approximation;funcion radial base;coevolucion;cooperative coevolution;hybrid algorithm;analyse variance;biomimetics;robustez;programacion multiobjetivo	In this paper the behaviour of a multiobjective cooperativecoevolutive hybrid algorithm for the optimization of the parameters defining a Radial Basis Function Network developed by our group, is analyzed. In order to demonstrate the robustness of the behaviour of the presented methodology when the parameters of the algorithm are modified, a statistical analysis has been carried out. In the present contribution, the relevance and relative importance of the parameters involved in the design of the multiobjective cooperative-coevolutive hybrid algorithm presented are investigated by using a powerful statistical tool, the ANalysis Of the VAriance (ANOVA). To demonstrate the robustness of our algorithm, a functional approximation problem is investigated.	approximation;cooperative mimo;hybrid algorithm;hybrid functional;mathematical optimization;neuron;radial (radio);radial basis function network;relevance;statistical model	Antonio J. Rivera;Ignacio Rojas;Julio Ortega	2005		10.1007/11494669_37	program analysis;biomimetics;mathematical optimization;radial basis function;analysis of variance;hybrid algorithm;function approximation;coevolution;computer science;artificial intelligence;machine learning;mathematics;radial basis function network;approximation algorithm;robustness	Vision	23.53146329425781	-12.386114064449998	40503
00aef4db14dff293c3f96fdab83205aa4a35e45d	a faster algorithm for dominating set analyzed by the potential method	backtracking algorithm;potential method;polynomial space;optimization problem;large number;space algorithm;minimum dominating set problem;current fastest algorithm;traditional measure;faster algorithm;new analyzing technique	"""Measure and Conquer is a recently developed technique to analyze worst-case complexity of backtracking algorithms. The traditional measure and conquer analysis concentrates on one branching at once by using only small number of variables. In this paper, we extend the measure and conquer analysis and introduce a new analyzing technique named """"potential method"""" to deal with consecutive branchings together. In potential method, the optimization problem becomes sparse; therefore, we can use large number of variables. We applied this technique to the minimum dominating set problem and obtained the current fastest algorithm that runs in O(1.4864n) time and polynomial space. We also combined this algorithm with a precalculation by dynamic programming and obtained O(1.4689n) time and space algorithm. These results show the power of the potential method and possibilities of future applications to other problems."""	algorithm;dominating set;potential method	Yoichi Iwata	2011		10.1007/978-3-642-28050-4_4	mathematical optimization;combinatorics;mathematics;algorithm	EDA	24.44239430154189	3.8781604270625514	40569
20ac03af40964aa301cd3b0b4e03aa42e83b7485	predicting failures in hard drives with lstm networks		Several research has been done to propose early failure detection techniques for hard disk drives in order to improve storage systems availability and avoid data loss. Failure prediction in such circumstances would allow for the reduction of downtime costs through anticipated disk replacements. Many of the techniques proposed so far mainly perform incipient failure detection thus not allowing for proper planning of such maintenance tasks. Others perform well only under a limited prediction horizon. In this work, we present a remaining useful life estimation approach for hard disk drives based on SMART parameters that is capable of predicting failures in both long and short term intervals by leveraging the capabilities of LSTM networks.	artificial neural network;downtime;experiment;hard disk drive;long short-term memory;random forest;smart	Fernando Dione dos Santos Lima;Gabriel Maia Rocha Amaral;Lucas Goncalves de Moura Leite;João Paulo Pordeus Gomes;Javam C. Machado	2017	2017 Brazilian Conference on Intelligent Systems (BRACIS)	10.1109/BRACIS.2017.72	real-time computing;recurrent neural network;data loss;downtime;computer science	Robotics	13.985872085868822	-14.867265466546097	40574
10b78d70d569aa3c6bdbd420ab4a9513b89dc8ff	surviving in a competitive market of information providers	heuristic algorithms games nash equilibrium aggregates vectors linear programming conferences;linear programming computational complexity decision making electronic data interchange;game theory;information retrieval;response dynamic algorithms competitive market information providers transport capacity information and communication technology infrastructure ict infrastructure information exchange process information producers information consumers information access investment cost decision model aggregate utility maximization np complete problem linear programming rounding heuristic algorithm game anarchy price;investment;transport capacity competitive market information providers ict information and communication technologies infrastructure information exchange process investment cost decision model np complete linear programming rounding heuristic algorithm heuristic algorithm best response dynamic algorithms;information industry;marketing;computational complexity;linear programming;competitive intelligence;electronic data interchange;marketing competitive intelligence decision making game theory information industry information retrieval investment linear programming	As the processing and transport capacity of the information and communication technologies (ICT) infrastructure increased vastly the last few years, the bottleneck of the information exchange process moved to the end points of the process, i.e. the consumers and the producers of information. On one hand there is the limited time that a consumer has to access the information and on the other hand there is the minimum utility level that a provider needs to provide to the society of consumers to cover it's investment cost. In this paper we present a novel decision model for a set of competing providers that wish to enter a market. It may happen that due to the competition, some competitors will not be able to cover their investment cost and therefore will disappear. We analyze the optimum way of forming the market, in order to maximize the aggregate utility of it. We show that this problem is NP-complete and present a linear programming rounding heuristic algorithm to solve it. Besides, we study a game where every player (provider) is to choose whether to join the market or not. We compute the price of anarchy of the game and present a heuristic algorithm that belongs to the family of best response dynamic algorithms. Systematic experiments on a real world data set have demonstrated the effectiveness of our proposed approach.	aggregate data;aggregate function;algorithm;anarchy;dynamic problem (algorithms);emoticon;entropy maximization;experiment;heuristic (computer science);information exchange;karp's 21 np-complete problems;linear programming;rounding;tracing (software)	Konstantinos Poularakis;Leandros Tassiulas	2013	2013 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)	10.1109/INFCOM.2013.6567150	game theory;competitive intelligence;investment;computer science;linear programming;electronic data interchange;information industry;management science	DB	-1.1530836049274837	-0.1704211945165901	40591
2fe07d7cc6679b9851474d55c588719e5603b7b2	ann approach to wecs power forecast	wind power plants load forecasting neural nets power grids power system analysis computing;neural networks;neural nets;wind power plants;learning method ann approach wecs power forecast wind generator portuguese electric grid artificial neural network;wind energy generation power generation artificial neural networks wind speed wind power generation wind forecasting testing distributed power generation photovoltaic systems uncertainty;wind generator;learning method;load forecasting;wecs power forecast;portuguese electric grid;learning methods;ann approach;wind speed;power system analysis computing;power grids;wind power generation;work in progress;artificial neural network	In this work-in-progress the problem with the future integration of large quantity of wind generators in the Portuguese electric grid is presented. A method based in artificial neural networks (ANN) is used to predict the average hourly wind speed. The work starts by choosing the patterns set length, the ANN structure and the learning method. As well as the dimensions of the data sets, training, validation and test. The ANN is tested with several structures until it archives an acceptable ANN based model. The obtained model is used to predict the wind speed and to forecast the power produced. The results archived are discussed. The future work perspectives are present	archive;artificial neural network	Pedro M. Fonte;J. C. Quadrado	2005	2005 IEEE Conference on Emerging Technologies and Factory Automation	10.1109/ETFA.2005.1612645	wind speed;simulation;computer science;engineering;electrical engineering;artificial intelligence;machine learning;work in process;artificial neural network	Robotics	9.808675023161493	-17.949341103374767	40727
4ddd3d0e77bdda63ee71e343e65285a418a38243	on the allocation of excesses of resources in linear production problems		In this paper we consider non-centralized linear production situations. In one of those situations, each producer iof a setNhas an optimal production plan x 0 i for a linear production problem given by max {c i x i : A i x i ≤b i,x i≥0}. In of resource (b 1−A 1 x 0 1 ,...,b n−A n x 0 n ). We study the games which describe this situation when players cooperate and side payments are possible, when players do not cooperate and when players cooperate and side payments are not possible.		Francisco R. Fernández;M. Gloria Fiestras-Janeiro;Ignacio García-Jurado;Justo Puerto	2002		10.1007/978-3-642-55537-4_75	mathematics;mathematical optimization;market game;strategy;nash equilibrium	NLP	-3.057226949619512	-2.9285524813937083	40753
6ad704cffc816fb1d88d617c76bfd6c99e07a759	generation, combination and extension of random set approximations to coherent lower and upper probabilities	inverse iteration;iterative rescaling method;mobius inversion;discrete approximation;p box;probability distribution;dempster shafer theory;cumulant;random set theory;random set;coherent lower and upper probabilities	Random set theory provides a convenient mechanism for representing uncertain knowledge including probabilistic and set-based information, and extending it through a function. This paper focuses upon the situation when the available information is in terms of coherent lower and upper probabilities, which are encountered, for example, when a probability distribution is specified by interval parameters. We propose an Iterative Rescaling Method (IRM) for constructing a random set with corresponding belief and plausibility measures that are a close outer approximation to the lower and upper probabilities. The approach is compared with the discrete approximation method of Williamson and Downs (sometimes referred to as the p-box), which generates a closer approximation to lower and upper cumulative probability distributions but in most cases a less accurate approximation to the lower and upper probabilities on the remainder of the power set. Four combination methods are compared by application to example random sets generated using the IRM. q 2004 Elsevier Ltd. All rights reserved.	approximation;coherence (physics);information rights management;iterative method;plausibility structure;set theory	Jim W. Hall;Jonathan Lawry	2004	Rel. Eng. & Sys. Safety	10.1016/j.ress.2004.03.005	probability distribution;combinatorics;discrete mathematics;dempster–shafer theory;chain rule;inverse iteration;mathematics;law of total probability;statistics;cumulant	AI	-0.2284684637887711	-20.1466903083863	40758
4bc9ae502252e5106d58fbf819cd3a6dbe95be4b	an economic order quantity model with partial backordering and incremental discount	partial backordering;incremental discounts;full backordering;eoq	Determining an order quantity when quantity discounts are available is a major interest of material managers. A supplier offering quantity discounts is a common strategy to entice the buyers to purchase more. In this paper, EOQ models with incremental discounts and either full or partial backordering are developed for the first time. Numerical examples illustrate the proposed models and solution methods. 2015 Elsevier Ltd. All rights reserved.	economic order quantity;numerical linear algebra;scrum (software development)	Ata Allah Taleizadeh;Irena Stojkovska;David W. Pentico	2015	Computers & Industrial Engineering	10.1016/j.cie.2015.01.005	economic order quantity;economics;marketing;operations management;microeconomics;commerce	AI	2.1735195700124414	-5.152362596530821	40771
1d5d10d5063fa00c6b00f17034a2c8897c54b133	evaluation based on pessimistic efficiency in interval dea	pessimistic;linear order;decision maker;input output;efficiency interval;arrangement;decision making unit;efficiency measurement;data envelope analysis;interval dea;partial order	In  Interval DEA  (Data Envelopment Analysis),  efficiency interval  has been proposed and its bounds are obtained from the optimistic and  pessimistic  viewpoints, respectively. Intervals are suitable to represent uncertainty of the given input-output data and decision makers' intuitive evaluations. Although the intervals give elements a partial order relation, it is sometimes complex, especially in case of many elements. The efficiency measurement combining optimistic and pessimistic efficiencies in Interval DEA is proposed. They are compared from the view that both of them represent the difference of the analyzed DMU (Decision Making Unit) from the most efficient one. The proposed efficiency measurement is mainly determined by the pessimistic efficiency. The optimistic one is considered if it is inadequate comparing to the pessimistic one. Such a pessimistic efficiency based evaluation is more similar to our natural evaluation and DMUs are  arranged  as a linear order.	interval arithmetic	Tomoe Entani	2009		10.1007/978-3-642-04820-3_21	partially ordered set;input/output;econometrics;decision-making;computer science;pessimism;data envelopment analysis;mathematics;total order	Logic	-2.9417612119159458	-19.12185680838012	40844
d901928c31d8ccdb12a5a08600be3c2ed10c97c8	seriesnet:a generative time series forecasting model		Time series forecasting is emerging as one of the most important branches of big data analysis. However, traditional time series forecasting models can not effectively extract good enough sequence data features and often result in poor forecasting accuracy. In this paper, a novel time series forecasting model, named SeriesNet, which can fully learn features of time series data in different interval lengths. The SeriesNet consists of two networks. The LSTM network aims to learn holistic features and to reduce dimensionality of multi-conditional data, and the dilated causal convolution network aims to learn different time interval. This model can learn multi-range and multi-level features from time series data, and has higher predictive accuracy compared those models using fixed time intervals. Moreover, this model adopts residual learning and batch normalization to improve generalization. Experimental results show our model has higher forecasting accuracy and has greater stableness on several typical time series data sets.	big data;causal filter;convolution;holism;long short-term memory;principle of good enough;time series	Zhipeng Shen;Yuanming Zhang;JiaWei Lu;Jun Xu;Gang Xiao	2018	2018 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2018.8489522	residual;time series;normalization (statistics);big data;feature extraction;machine learning;pattern recognition;convolution;artificial intelligence;computer science;data modeling;curse of dimensionality	ML	8.60784841893542	-21.759518584529918	40845
e2c07be49069c7dde97697d707f012c3946cb902	playing congestion games with bandit feedbacks	convergence;mirror descent algorithm;no regret dynamics	Almost all convergence results from each player adopting specific “no-regret” learning algorithms such as multiplicative updates or the more general mirror-descent algorithms in repeated games are only known in the more generous information model, in which each player is assumed to have access to the costs of all possible choices, even the unchosen ones, at each time step. This assumption in general may seem too strong, while a more realistic one is captured by the bandit model, in which each player at each time step is restricted to know only the cost of her currently chosen path, but not any of the unchosen ones. Can convergence still be achieved in such a more challenging bandit model? We answer this question positively. While existing bandit algorithms do not seem to work here, we develop a new family of bandit algorithms based on the mirror-descent algorithm with such a guarantee in atomic congestion games.	algorithm;information model;machine learning;multi-armed bandit;network congestion;regret (decision theory)	Po-An Chen;Chi-Jen Lu	2015			mathematical optimization;simulation;convergence;computer science;machine learning	Theory	-2.9937750373797107	1.2320944641134772	40907
852086505e252cb20f888a9f4d74a6e9db25028e	financial contracting with enforcement externalities	enforcement;costly state verification;state capacity;credit crunch;global games;credit rationing;heterogeneity;financial accelerator	Financial markets crucially rely on the development of an infrastructure dedicated to the enforcement of contracts. Here we study the effects of limited enforcement capacity on financial contracting by proposing a new theory of costly state verification. In our model the principal contracts with a population of entrepreneurs, who borrow to finance risky projects under limited liability. To sustain incentives to repay debt, the principal must build enforcement capacity ex ante, which determines state verification efforts ex post. Our theory sheds new light on such phenomena as credit crunches and the link between enforcement infrastructure accumulation, economic growth and political economy frictions.	tree accumulation	Lukasz A. Drozd;Ricardo Serrano-Padial	2018	J. Economic Theory	10.1016/j.jet.2018.09.002	credit crunch;credit reference;finance;microeconomics;business;credit enhancement	ECom	-0.9263756651787153	-8.292123521937835	40956
ca475fc0ee6a8f798047aca58db6bec4d7d4b7a7	confidence domains in the analysis of noise-induced transition to chaos for goodwin model of business cycles	goodwin model of business cycles;random disturbances;chaos;noise induced transitions	Stochastically forced Goodwin model of business cycles is considered. In a multistable zone, we study the phenomena of noise-induced transitions and chaotization. To clarify a probabilistic mechanism of these phenomena found numerically, we apply a semi-analytical method based on the stochastic sensitivity function technique and confidence domains. Using elaborated method, we estimate a critical value of the noise intensity corresponding to the onset of chaos.		Irina A. Bashkirtseva;Tatyana Ryazanova;Lev B. Ryashko	2014	I. J. Bifurcation and Chaos	10.1142/S0218127414400203	mathematical economics;operations research	ECom	0.7076737773319275	-10.087637209136703	40970
e876df3bed4795eddf39a479fbef919adec8fd8d	construction of fuzzy ¯ x - s control charts with an unbiased estimation of standard deviation for a triangular fuzzy random variable		Statistical process control and Shewhart control charts are used by organizations to aid in process understanding, assessing process stability, and identifying changes to improve the quality of the product. Shewhart control charts only considered uncertainty caused by randomness while in practice, uncertainty caused by vagueness, ambiguity, and/or incomplete information are also observed. In this article, fuzzy X̄ − S control charts which handle both kinds of uncertainty simultaneously are developed using fuzzy random variables. For this purpose, the unbiased estimation of standard deviation for a triangular fuzzy random variable is introduced and utilized to construct the fuzzy X̄ − S control charts. Then, a detailed average run length study is performed to evaluate the decisions regarding sample size and accepted out-of-control level ( ). A comparison study is performed to verify the proposed technique by comparing its performance based on average run length with previous technique in the literature. The result shows that the proposed technique could improve the detection of abnormal shift in process mean 0.1% to 30% depending on sample size and shift. Finally, the proposed fuzzy control charts are validated through a case study of noodle production in	chart;fuzzy control system;fuzzy number;numerical analysis;optimal control;process state;randomness;run-length encoding;simulation;vagueness	S. Mojtaba Zabihinpour;Mohd Khairol Anuar bin Mohd Ariffin;Say Hong Tang;A. S. Azfanizam	2015	Journal of Intelligent and Fuzzy Systems	10.3233/IFS-151551	econometrics;fuzzy number;mathematics;statistics	SE	14.343514735215404	-13.839283999636116	40973
82205dde3340f0c9e2fcd3def48b40641a586121	particle swarm optimization for economic dispatch problems with valve-point effects	continuous optimization problem economic dispatch problems valve point effects laplace crossover particle swarm optimization power system operations;laplace crossover particle swarm optimization economic dispatch;economics optimization;particle swarm optimizer;continuous optimization;power system economics;valves particle swarm optimisation power system economics;cost effectiveness;valves;economic dispatch;particle swarm optimisation;power system operation	Laplace Crossover Particle Swarm Optimization (LXPSO) is a variant of Particle Swarm Optimization (PSO) which employs Laplace Crossover to PSO. LXPSO is already proven to be cost effective and reliable for the test problems of continuous optimization. Economic dispatch (ED) problem is one of the fundamental issues in power system operations. The problem of economic dispatch turns out to be a continuous optimization problem which is solved using original PSO and its variant LXPSO in expectation of better results. Results are also compared with the earlier published results.	continuous optimization;dynamic dispatch;mathematical optimization;microwave;optimization problem;particle swarm optimization;phase-shift oscillator;program optimization	Kusum Deep;Jagdish Chand Bansal	2010	2010 IEEE Fifth International Conference on Bio-Inspired Computing: Theories and Applications (BIC-TA)	10.1109/BICTA.2010.5645606	mathematical optimization;multi-swarm optimization;simulation;engineering;continuous optimization;mathematical economics;particle swarm optimization;metaheuristic	EDA	18.73022741909555	-3.9916615735659704	40984
e83e26bdf1017259e19ec7619552da2d80fd5030	a neural network approach to multi-step-ahead, short-term wind speed forecasting	eastern canada multistep ahead short term wind speed forecasting feedforward neural network based approach explanatory variables wind direction wind temperature day time horizon forecasting telemetric measurements weather variables wind farms;neural networks;wind power plants;wind speed forecasting wind forecasting predictive models training accuracy wind power generation;wind power wind speed forecasting short term forecasting neural networks;wind power;weather forecasting;geophysics computing;feedforward neural nets;short term forecasting;wind speed forecasting;wind power plants feedforward neural nets geophysics computing weather forecasting wind power	This paper presents a novel neural network-based approach to short-term, multi-step-ahead wind speed forecasting. The methodology combines predictions from a set of feed forward neural networks whose inputs comprehend a set of 11 explanatory variables related to past averages of wind speed, direction, temperature and time of the day, and their outputs represent estimates of specific wind speed averages. Forecast horizons range from 30 minutes up to 6:30 hours ahead with 30 minutes time steps. Final forecasts at specific horizons are combinations of corresponding neural network predictions. Data used in the experiments are telemetric measurements of weather variables from five wind farms in eastern Canada, covering the period from November 2011 to April 2013. Results show that the methodology is effective and outperforms established reference models particularly at longer horizons. The method performed consistently across sites leading up to more than 60% improvement over persistence and 50 % over a more realistic MA-based reference.	artificial neural network;branch misprediction;experiment;persistence (computer science);ramp simulation software for modelling reliability, availability and maintainability;reference model;time series	Julian L. Cardenas-Barrera;Julian Meng;Eduardo Castillo Guerra;Liuchen Chang	2013	2013 12th International Conference on Machine Learning and Applications	10.1109/ICMLA.2013.130	wind power;probabilistic forecasting;simulation;weather forecasting;computer science;machine learning;artificial neural network	ML	9.521884573083023	-18.420725101963498	40986
735b71996ccae609811edca2468c94d77ebfe663	an integrated ga-dea algorithm for determining the most effective maintenance policy for a k -out-of- n problem	maintenance policy;maintenance activities;data envelopment analysis;genetic algorithm;pareto optimal solution	This paper presents a novel hybrid GA-DEA algorithm in order to solve multi-objective $$k$$ -out-of- $$n$$ problem and determine preferred policy. The proposed algorithm maximizes overall system reliability and availability, while minimizing system cost and queue length, simultaneously. To meet these objectives, an adaptive hybrid GA-DEA algorithm is developed to identify the optimal solutions and improve computation efficiency. In order to improve computation efficiency genetic algorithm (GA) is used to simulate a series production line and find the Pareto-optimal solutions which are different values of $$k$$ and $$n$$ of $$k$$ -out-of- $$n$$ problem. Data envelopment analysis is used to find the best $$k$$ and $$n$$ from Genetic Algorithm's Pareto solutions. An illustrative example is applied to show the flexibility and effectiveness of the proposed algorithm. The proposed algorithm of this study would help managers to identify the preferred policy considering and investigating various parameters and scenarios in logical time. Also considering different objectives result in Pareto-optimal solutions that would help decision makers to select the preferred solution based on their situation and preference.	algorithm;software release life cycle	Mohammad Sheikhalishahi;V. Ebrahimipour;Mehdi Hosseinabadi Farahani	2014	J. Intelligent Manufacturing	10.1007/s10845-013-0752-z	mathematical optimization;genetic algorithm;computer science;engineering;data envelopment analysis;management science	Robotics	14.610375516218863	-3.0579889908958457	41005
ff0ce5d982007c9d84472c3e1f67a8acb67cc0e9	maintenance policy selection for cellular manufacturing: a simulation study	failure;cell performance;simulation;maintenance policy;simulation study;manufacturing cells;cellular manufacturing	This paper describes the development of a simulation study to reduce the effect of failure in cellular manufacturing with the use of a maintenance policy. It analyses the effects of corrective, preventive and opportunistic maintenance policies on the performance of a manufacturing cell. We consider the productivity of the cell as performance criteria, and we study the cell performance under different times between failure distributions and different operational conditions. A simulation model was established in Arena simulation software. The results are compared to determine the best policy for a given system.	simulation	Mounir Elleuch;Habib Ben Bacha;Faouzi Masmoudi;Aref Y. Maalej	2007	IJMR	10.1504/IJMR.2007.014728	reliability engineering;simulation;computer science;systems engineering;engineering;operations management	ECom	8.927847792816385	1.2853811041316352	41018
4f9cc4008949391b7e111881ce2239c37c1e95c7	evaluating time-of-use design options		Information systems in future smart grids will expand the capabilities of the power system through new services and control options. In this context, dynamically updated time-of-use (TOU) rates can be a building block for creating effective and robust pricing schemes in future retail electricity markets: On the one hand, they are better suited to match market dynamics and uncertainties than static, linear tariffs; on the other hand they mitigate the complexity arising from hourly real-time prices. Hence, the proper design of these dynamic rates requires managing the trade-off between complexity and efficiency. To this end, careful tuning of the rate complexity with respect to variability (number of time zones) and dynamics (frequency of rate adjustments) is necessary. This challenges calls for efficient decision support that allows energy retailers to identify and implement promising rate designs. A framework to determine, analyse and compare a set of rate designs featuring different structural and dynamic design options is presented in this paper. This approach is illustrated using an exemplary scenario based on empirical electricity price data.	decision support system;heart rate variability;information system;real-time clock;terms of service	Christoph Flath	2014			electric power system;marketing;computer science;management science;information system;smart grid;electricity retailing;decision support system	EDA	1.9068047431400892	-6.886537387830219	41050
1190fb4e7f63cefe7d6b5c93a67997bf3ef9d0b6	coactive learning for locally optimal problem solving	locally optimal planning;coactive learning	Coactive learning is an online problem solving setting where the solutions provided by a solver are interactively improved by a domain expert, which in turn drives learning. In this paper we extend the study of coactive learning to problems where obtaining a globally optimal or near-optimal solution may be intractable or where an expert can only be expected to make small, local improvements to a candidate solution. The goal of learning in this new setting is to minimize the cost as measured by the expert effort over time. We first establish theoretical bounds on the average cost of the existing coactive Perceptron algorithm. In addition, we consider new online algorithms that use cost-sensitive and Passive-Aggressive (PA) updates, showing similar or improved theoretical bounds. We provide an empirical evaluation of the learners in various domains, which show that the Perceptron based algorithms are quite effective and that unlike the case for online classification, the PA algorithms do not yield significant performance gains.	interactivity;maxima and minima;online algorithm;perceptron;problem solving;reinforcement learning;solver;subject-matter expert	Robby Goetschalckx;Alan Fern;Prasad Tadepalli	2014			mathematical optimization;computer science;artificial intelligence;machine learning	AI	21.618936639498227	-20.847526493685148	41055
b89b64524dca2ae77097d474f49be0f7bf6de7f4	optimal contracts for central bankers: calls on inflation	rational expectation;inflation linked bonds;monetary policy	We consider a framework featuring a central bank, private and financial agents as well as a financial market. The central bank’s objective is to maximize a functional, which measures the classical trade-off between output and inflation plus income from the sales of inflation linked calls minus payments for the liabilities that the inflation linked calls produce at maturity. Private agents have rational expectations and financial agents are averse against inflation risk. Following this route, we explain demand for inflation linked calls on the financial market from a no-arbitrage assumption and derive pricing formulas for inflation linked calls, which lead to a supply-demand equilibrium. We then study the consequences that the sales of inflation linked calls have on the observed inflation rate and price level. Similar as in Walsh (1995) we find that the inflationary bias is significantly reduced, and hence that markets for inflation linked calls provide provide a mechanism to implement inflation contracts as discussed in the classical literature.		Christian-Oliver Ewald;Johannes Geißler	2017	Applied Mathematics and Computation	10.1016/j.amc.2016.07.011	monetary policy;relative price;inflation targeting;real interest rate;indexation;rational expectations;economic stability	NLP	-1.6361284598116737	-6.627783192403554	41062
7d91c35121b38796fe624ff5753b7c018371285c	modified interactive chebyshev algorithm (mica) for non-convex multiobjective programming		In this paper, I carry out an extension of the MICA method (modified interactive chebyshev algorithm) for non-convex multiobjective programming. This method is based on the Tchebychev method and in the reference point approach. At each iteration, the decision maker (DM) can provide aspiration levels (desirable values for the objective functions) and also, if the DM wishes, reservation levels (level under which the objective function is not considered acceptable). On the basis of this preferential information, a region of the nondominated objective set is defined. In the convex case, considering the aspiration vector as a reference point in an achievement scalarizing function and taking a set of weight vectors, the efficient solutions generated satisfy the reservation levels. In this work, I analyze the non-convex case. The main result of MICA is verified and demonstrated for the non-convex bi-objective case. The MICA method is not verified in general for multiobjective problems with three or more objective functions, which is demonstrated with a counterexample.	algorithm;multi-objective optimization	Mariano Luque	2015	Optimization Letters	10.1007/s11590-014-0743-9	mathematical optimization;artificial intelligence;mathematics;algorithm	ML	-1.4326632749251142	-17.525672614459726	41102
c1a1f3584b8c3fd1925120117dc32b4c965a331b	financial optimization models in data networks	net present value;continuous variable;combinatorial optimization problem;point of presence;data network;rollout order;internal rate of return;optimization problem;continuous optimization;growth rate;global optimization;profitability;combinatorial optimization;local minima;optimization model	In a competitive market investors in a data network need to give utmost considerations on profitability. They must have clear picture of the size, growth rate and demand for different services. However, the investors' budget may be limited, and therefore the speed at which the network is rolled out, must be carefully planned to ensure that they can meet profitability targets. We model first the roll out order as combinatorial optimization problems and then extend them as continuous optimization problems. We then implement these models in a practical problem. Numerical studies suggested that the optimization problems have multiple local minima. Therefore, a global optimization technique is used to obtain the global minimum for the continuous variable problem and a combinatorial optimization technique is used to solve the discrete variable problem. Optimal financial indicators are obtained to assess the commercial viability of the network. Finally, we demonstrate that the solution of these optimization problems can provide an investment policy to the investors in data networks.		M. M. Ali	2006	J. Global Optimization	10.1007/s10898-005-1656-z	stochastic programming;probabilistic-based design optimization;optimization problem;mathematical optimization;net present value;multi-swarm optimization;cross-entropy method;combinatorial optimization;stochastic optimization;multi-objective optimization;maxima and minima;point of presence;mathematics;internal rate of return;continuous optimization;mathematical economics;vector optimization;bilevel optimization;random optimization;metaheuristic;profitability index;global optimization;quadratic assignment problem	ML	15.862034338837908	-1.6041310078504643	41180
897f07d63ea81e9bbd7a30e62985a0cd1b6f30f3	forecasting for intermittent demand: the estimation of an unbiased average	forecasting;poisson process;prevision demande;reliability;project management;information systems;intermittent demand;maintenance;proceso llegada;soft or;information technology;packing;error sistematico;operations research;location;investment;journal;arrival process;journal of the operational research society;inventory;administracion deposito;processus arrivee;estimacion insesgada;purchasing;history of or;intermitencia;logistics;bias;marketing;scheduling;gestion stock;prevision demanda;intermittency;production;communications technology;proceso poisson;demande intermittente;computer science;operational research;unbiased estimation;intermittence;inventory control;estimation sans biais;applications of operational research;or society;jors;management science;processus poisson;infrastructure;erreur systematique;demand forecasting	The majority of the range of items held by many stockists exhibit intermittent demand. Accurate forecasting of the issue rate for such items is important and several methods have been developed, but all produce biased forecasts to a greater or lesser degree. This paper derives the bias expected when the order arrivals follows a Poisson process, which leads to a correction factor for application in practice. Extensions to some other arrival processes are briefly considered. Journal of the Operational Research Society (2006) 57, 588–592. doi:10.1057/palgrave.jors.2602031 Published online 13 July 2005	mike lesser	Estelle A. Shale;John E. Boylan;F. R. Johnston	2006	JORS	10.1057/palgrave.jors.2602031	inventory control;project management;logistics;econometrics;poisson process;inventory;economics;demand forecasting;forecasting;investment;marketing;operations management;bias;reliability;location;operations research;information technology;scheduling;statistics	Metrics	5.782244849065646	-3.8011314509099194	41260
e76f91285f4db26bece9086fcd63cb159f1b386c	modeling of tolerable repair time without affecting system reliability	fault tree;maintenance engineering belts conveyors decision making fault trees;reliability;tolerable repair time coal mine fault tree reliability;fta tolerable repair time modelling system failure system reliability curve maintenance policy management decision grace period belt conveyor system idler failure distribution function fault tree analysis;coal mine;tolerable repair time;maintenance engineering belts fault trees market research reliability engineering data models	Ideally, it is assumed that a system fails as soon as one of its components connected in series has failed. However, in many real-world system configurations do not allow the system failure immediately when a fault or component failure occurs, rather its reliability falls down at a faster rate, and soon the system fails prematurely. If the fault is rectified or the failed item is repaired within a specified time limit, system reliability curve is restored to its normal decreasing trend. The time gap, between the failure of a component or occurrence of a fault and the repair of the component or rectification of a fault so that no system failure, is observed and it is very important that the system reliability decreases at its normal rate. This allowable time gap can serve a guiding tool to the maintenance policy and management decisions. This paper proposes a reliability and maintainability based approach for calculating the grace period, commonly known as the tolerable repair time of a system component. The proposed model has been demonstrated for a belt conveyor system, used for transportation of mineral in the mines. The allowable repair time of the idler of the belt conveyor system was calculated using the cumulative failure distribution function.	belt machine;failure cause;mean time to repair;rectifier (neural networks);series and parallel circuits;world-system	Aishwarya Mishra;Pranab Murari;Sanjay Kumar Palei;Suprakash Gupta	2014	2014 IEEE International Conference on Industrial Engineering and Engineering Management	10.1109/IEEM.2014.7058842	structural engineering;reliability engineering;availability;fault tree analysis;engineering;reliability;coal mining;forensic engineering;statistics	SE	12.083685334995941	-12.748017889410479	41404
3c90e02ab36a5d471bd768654eda75412deec1cc	index tracking with fixed and variable transaction costs		Index tracking is a form of passive portfolio (fund) management that attempts to mirror the performance of a specific index and generate returns that are equal to those of the index, but without purchasing all of the stocks that make up the index. We present two mixed-integer linear programming formulations of this problem. In particular we explicitly consider both fixed and variable transaction costs. Computational results are presented for data sets drawn from major world markets.		H. Mezali;John E. Beasley	2014	Optimization Letters	10.1007/s11590-012-0534-0	tracking error	AI	1.179344269231571	-3.9018114842886455	41436
f46a75965009f20ad72f3537d7d5462769dd66f9	traveling-wave fault location techniques in power system based on wavelet analysis and neural network using gps timing	traveling wave;wavelet transform;power system;gps timing;artificial neural network;fault location	In this paper four fault location algorithms based on discrete wavelet transform using global positioning system are described and compared. In two approaches, the location of fault is determined according to arrival instances of traveling waves and in two other approaches, the non-linear relations are simulated by artificial neural network to improve the responses. All the possible fault types are generated using the ATP---EMTP and results using the four methods are discussed. Extensive simulation studies indicate that proposed networks decrease errors percentages of two wavelet-based approaches from 0.35 to 0.22 and 0.21 to less than 0.15 %, respectively, though exploiting small size data base for training.	artificial neural network;global positioning system;radio clock;wavelet	Mohammad Reza Mosavi;Amir Tabatabaei	2016	Wireless Personal Communications	10.1007/s11277-015-2958-1	real-time computing;computer science;wave;machine learning;electric power system;artificial neural network;wavelet transform	Mobile	10.886478428240528	-16.836347035795054	41549
60ee4639e7ac6d06ec90c0b495e71d760dd5e2db	wastewater do concentration control through nh4 prediction based on evolutionary radial basis function neural network	kernel;evolutionary computation;dissolved oxygen;nh4 prediction;oxygen;prediction oxygen control model evolutionary algorithms rbf neural network;evolutionary neural network;dissolved oxygen model;radial basis function networks;artificial neural networks;set point control;wastewater treatment evolutionary computation industrial control radial basis function networks;radial basis function;rbf neural network;radial basis function neural network;industries control problem;industrial control;mathematical model;evolutionary algorithms;model;wastewater do concentration control;predictive models;control;online measurement wastewater do concentration control nh4 prediction evolutionary radial basis function neural network evolutionary neural network industries control problem dissolved oxygen model set point control ammonium concentration prediction;evolutionary algorithm;radial basis function networks neural networks oxygen microorganisms automatic control sludge treatment evolutionary computation effluents wastewater treatment computer networks;prediction;evolutionary radial basis function neural network;wastewater treatment;ammonium concentration prediction;model simulation;online measurement	Evolutionary Neural network has been used in many industries control problems. This paper analyzes Dissolved Oxygen (DO) model and set-point control, then using Evolutionary Radial Basis Function (RBF) Neural Network to present a new idea and model for DO concentration control. The idea is to control DO set-point through ammonium concentration prediction based on Evolutionary RBF Neural Network. Compared to the idea of DO set-point control from on-line measurements of the ammonium concentration, new idea is better in response to actual situation. According to analyzing and Evolutionary RBF Neural Network theory, an Evolutionary RBF Neural Network is designed. Real wastewater plant data is used to the model simulation. Simulation shows that the idea and model is a good way to the DO concentration control.	artificial neural network;radial (radio);radial basis function	Jin Rui Liang;Fei Luo;Yuge Xu	2009		10.1109/ICNC.2009.259	control engineering;probabilistic neural network;engineering;artificial intelligence;machine learning	ML	12.41110286929684	-22.35002773110811	41691
1bc2e6e30e5f56b9f3d21d6d9aa657accde9d2f4	simulation-based analysis of integrated production and transport logistics		The purpose of this paper is to present an approach for the integration of production and transport logistics in one original equipment manufacturer (OEM) and to compare its performance with a sequential procedure. Herein, delays in fulfilling delivery due dates in both non-disturbed and disturbed (with oscillating transport time) scenarios will be analysed through a simulation model. The major findings demonstrate that the proposed integrative approach outperforms the sequential one in the absorption of perturbations that can delay the production and transport of orders in production facilities along a supply chain.		Enzo Morosini Frazzon;Joarez Pintarelli;Thomas Makuschewitz;Bernd Scholz-Reiter	2012		10.1007/978-3-642-35966-8_42	production schedule;control engineering;original equipment manufacturer;business;integrated production;supply chain;transportation planning	Robotics	11.507187080193509	-2.986175495959975	41700
45faa68c39f4c06fe9fe7fa28d67e77bba4a6152	an axiomatic model for measuring contradiction and n-contradiction between two aifss	intuitionistic fuzzy set;n contradiction measures;n;continuity from below and from above;atanassov s intuitionistic fuzzy sets;satisfiability;mathematical model	The importance of dealing with contradictory information or of deriving contradictory consequences in inference processes justifies undertaking a theoretical study on the subject of contradiction. In [S. Cubillo, E. Castiñeira, Contradiction in intuitionistic fuzzy sets, in: Proceedings of the Conference IPMU’2004, Perugia, Italy, 2004, pp. 2180–2186] we defined contradictory and N-contradictory Atanassov intuitionistic sets, where we established that two sets A and B are N-contradictory, with respect to a given intuitionistic negation N, if A implies NðBÞ, and are contradictory if they are N-contradictory for some negation N. The purpose of this article is to thoroughly examine the model for measuring contradiction between two Atanassov intuitionistic fuzzy sets irrespective of a fixed negation, proposed in [C. Torres-Blanc, E.E. Castiñeira, S. Cubillo, Measuring contradiction between two AIFS, in: Proceedings of the Eighth International FLINS Conference, Madrid, Spain, 2008, pp. 253–258], and also to introduce a mathematical model to measure N-contradiction between sets, where N is an intuitionistic negation. First, we justify and determine the minimum axioms that a function must satisfy to be able to be used as a measure of contradiction or a measure of N-contradiction. Also, we introduce some early examples of valid functions that conform to the model. Then, we establish the conditions for these measures to be continuous from below or continuous from above. Finally, we build families of contradiction and N-contradiction measures, establishing how they are relate to each other, and we look at how they behave with respect to continuity. 2009 Elsevier Inc. All rights reserved.	fuzzy set;intuitionistic logic;mathematical model;scott continuity;semi-continuity	Carmen Torres-Blanc;Susana Cubillo;Elena Castiñeira	2010	Inf. Sci.	10.1016/j.ins.2009.06.022	discrete mathematics;haplogroup n;mathematical model;mathematics;algorithm;statistics;satisfiability	SE	0.7835090434806011	-20.82923481094888	41705
068eca516fd596fc97d0127597df553e5ef168f2	a simple insight into iterative belief propagation's success	be lief propagation;arc consistency;posterior probability;belief propagation;belief network	In non-ergodic belief networks the posterior belief of many queries given evidence may become zero. The paper shows that when belief propagation is applied iteratively over arbitrary networks (the so called, iterative or loopy belief propagation (IBP)) it is identical to an arc-consistency algorithm relative to zero-belief queries (namely assessing zero posterior probabilities). This implies that zero-belief conclusions derived by belief propagation converge and are sound. More importantly, it suggests that the inference power of IBP is as strong and as weak as that of arcconsistency. This allows the synthesis of belief networks for which belief propagation is useless on one hand, and focuses the investigation on classes of belief networks for which belief propagation may be zero-complete. Finally, we show empirically that IBP’s accuracy is correlated with extreme probabilities, therefore explaining its success over coding applications.	algorithm;bayesian network;belief propagation;casio loopy;converge;ergodicity;iterative method;local consistency;software propagation	Rina Dechter;Robert Mateescu	2003			computer science;belief structure;artificial intelligence;machine learning;pattern recognition;bayesian network;mathematics;posterior probability;local consistency;belief propagation	AI	23.858805112850074	-19.738685820568126	41739
ec1185f5418855debe8b3726475f11fabc8cf8e4	a novel fault diagnostic technique for photovoltaic systems based on cascaded forest		A variety of faults often occur during the operation of PV arrays, which may seriously affect the normal operation of the system, the machine diagnosis of the types of fault has become a hotspot in the field of photovoltaic power generation. This paper proposes a novel fault diagnostic technique for photovoltaic systems based on Cascaded Forest. Through the in-depth analysis of the output of PV arrays from a data platform of Shijiazhuang Kelin Electric Co, the input variables of the diagnosis model are obtained. Compared with other fault diagnosis methods for the PV array, the proposed method can work under a small number of tagged data and the system can be run online and real-time. Finally, the experimental results show that the fault diagnosis method for the PV array based on the cascading forests can effectively detect four types of fault for PV array such as short-circuit, open-circuit, abnormal degradation and partial shading. This method has a good value for the intelligent fault diagnosis of PV.	artificial neural network;elegant degradation;online and offline;page view;pin grid array;random forest;real-time clock;real-time data;shading;on-line system	Liangqing Hu;Jin Ye;Shengqiang Chang;Hongtao Li;Hongyu Chen	2017		10.1145/3132479.3132482	electricity generation;computer science;shading;real-time computing;hotspot (wi-fi);small number;photovoltaic system	AI	11.664361273342683	-15.972368821902315	41742
c54b8f18092d082dd856d913cdabee0f83eb10a2	anfis model for vibration signals based on aging process in electric motors	anfis;electric motor;aging;moving average;spectral analysis;correlation coefficient	In this study, the aging process of an electric motor is accomplished by adaptive neuro-fuzzy inference system (ANFIS) using vibration signals. Different ANFIS models are compared for representing the aging process in the best possible way. An artificial aging experiment is performed and vibration data taken from the initial (healthy) and final (faulty) cases are used to identify the aging process. Four different ANFIS models are presented. Moving average (MA) filters are applied to the input and output pairs for different lagging factors to change the smoothness degree of the data and thus the performance of system identification. The success of the models is evaluated on three conditions; the performance of the ANFIS and the linear correlation between expected output (faulty case data) and aging model output, in time and frequency domains.The study also evaluates the influence of preprocessing using MA filtering on the ANFIS performance for vibration data which have stochastic characteristics.	adaptive neuro fuzzy inference system	Duygu Bayram;Serhat Seker	2015	Soft Comput.	10.1007/s00500-014-1326-5	electric motor;adaptive neuro fuzzy inference system;computer science;artificial intelligence;machine learning;control theory;moving average	ECom	13.605584471774074	-17.576899162622187	41817
603846c8181d9bd088cb37aa0ff4b8c11034e174	algorithm 614: a fortran subroutine for numerical integration in h(sub)p	numerical integration;fortran	K. Sikorski and F. Stenger were supported by U.S. Army Research Contract DAAG-29-77-G-0139. Authors' addresses: K. Sikorski, Department of Computer Science, Columbia University, New York, NY 10027; F. Stenger, Department of Mathematics, University of Utah, Salt Lake City, UT 84112; J. Schwing, Department of Computer Science, Old Dominion University, Norfolk, VA 23508. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. © 1984 ACM 0098-3500/84/0600-0152 $00.75	algorithm;columbia (supercomputer);computer science;fortran;numerical analysis;numerical integration;subroutine	Kris Sikorski;Frank Stenger;J. Schwing	1984	ACM Trans. Math. Softw.	10.1145/399.449	computational science;numerical integration;mathematics;programming language	Theory	15.569792792703245	-9.342501693067247	41850
bf6c3870a62e879e6ec4d0d0c87dc7e318dda47e	a new approach for the fuzzy shortest path problem	decision maker;shortest path;shortest path problem;fuzzy set theory	 Many researchers have paid much attention to the fuzzy shortest path problem since it is central to many applications. In this problem, the fuzzy shortest length and the corresponding shortest path are useful information for decision makers. In this paper, we propose a new approach that can obtain the important information. First, we propose a heuristic procedure to find the fuzzy shortest length among all possible paths in a network. It is based on the idea that a crisp number is a minimum number if and only if any other number is larger than or equal to it. It owns a firm theoretic base in fuzzy sets theory and can be implemented effectively. Secondly, we propose a way to measure the similarity degree between the fuzzy shortest length and each fuzzy path length. The path with the highest similarity degree is the shortest path. An illustrative example is given to demonstrate our proposed approach.	shortest path problem	Tzung-Nan Chuang;Jung-Yuan Kung	2002		10.1007/10966518_7	euclidean shortest path;canadian traveller problem;longest path problem;computer science;shortest path faster algorithm;yen's algorithm;k shortest path routing;shortest path problem;mathematical optimization;constrained shortest path first	Theory	-3.791694615385897	-21.283433472061947	41917
9f897d20441f519c6f3daa92c60a5f1b0df4ea48	beating the multiplicative weights update algorithm		Multiplicative weights update algorithms have been used extensively in designing iterative algorithms for many computational tasks. The core idea is to maintain a distribution over a set of experts and update this distribution in an online fashion based on the parameters of the underlying optimization problem. In this report, we study the behavior of a special MWU algorithm used for generating a global coin flip in the presence of an adversary that tampers the experts’ advice. Specifically, we focus our attention on two adversarial strategies: (1) non-adaptive, in which the adversary chooses a fixed set of experts a priori and corrupts their advice in each round; and (2) adaptive, in which this set is chosen as the rounds of the algorithm progress. We formulate these adversarial strategies as being greedy in terms of trying to maximize the share of the corrupted experts in the final weighted advice the MWU computes and provide the underlying optimization problem that needs to be solved to achieve this goal. We provide empirical results to show that in the presence of either of the above adversaries, the MWU algorithm takes O(n) rounds in expectation to produce the desired output. This result compares well with the current state of the art of O(n) for the general Byzantine consensus problem. Finally, we briefly discuss the extension of these adversarial strategies for a general MWU algorithm and provide an outline for the framework in that setting.	adversary (cryptography);computation;consensus (computer science);greedy algorithm;iterative method;mathematical optimization;online algorithm;optimization problem	Abhinav Aggarwal;José Abel Castellanos;Diksha Gupta	2017	CoRR		adversary;multiplicative function;computer science;a priori and a posteriori;algorithm;adversarial system;consensus;optimization problem;coin flipping	ML	22.907711847923668	-21.193659256691106	41934
04b3ca6e9f72acd43af41f38147ca7da4fb4c6b7	when is multidimensional screening a convex program?	optimal transportation;social welfare;incentive compatibility;convex programming;principal agent asymmetric information monopoly nonlinear pricing price discrimination multidimensional signalling screening social welfare maximization under budget constraint optimal taxation incentive compatibility mechanism design exclusion bunching robustness private imperfect incomplete information optimal transportation ma trudinger wang cross curvature;optimal taxation;mathematical analysis;asymmetric information;nonlinear pricing;incomplete information;convex function;necessary and sufficient condition;profitability;mechanism design;price discrimination;budget constraint;profit maximization	A principal wishes to transact business with a multidimensional distribution of agents whose preferences are known only in the aggregate. Assuming a twist (= generalized Spence-Mirrlees single-crossing) hypothesis, quasi-linear utilities, and that agents can choose only pure strategies, we identify a structural condition on the value b(x, y) of product type y to agent type x — and on the principal’s costs c(y) — which is necessary and sufficient for reducing the profit maximization problem faced by the principal to a convex program. This is a key step toward making the principal’s problem theoretically and computationally tractable; in particular, it allows us to derive uniqueness and stability of the principal’s optimum strategy — and similarly of the strategy maximizing the expected welfare of the agents when the principal’s profitability is constrained. We call this condition non-negative cross-curvature: it is also (i) necessary and sufficient to guarantee convexity of the set of b-convex functions, (ii) invariant under reparametrization of agent and/or product types by diffeomorphisms, and (iii) a strengthening of Ma, Trudinger and Wang’s necessary and sufficient condition (A3w) for continuity of the correspondence between an exogenously prescribed distribution of agents and of products. We derive the persistence of economic effects such as the desirability for a monopoly to establish prices so high they effectively exclude a positive fraction of its potential customers, in nearly the full range of non-negatively cross-curved models.	aggregate data;cobham's thesis;convex function;convex optimization;entropy maximization;monopoly;persistence (computer science);product type;scott continuity	Alessio Figalli;Young-Heon Kim;Robert J. McCann	2011	J. Economic Theory	10.1016/j.jet.2010.11.006	information asymmetry;convex function;mechanism design;budget constraint;convex optimization;economics;incentive compatibility;public economics;macroeconomics;social welfare;mathematics;microeconomics;mathematical economics;welfare economics;price discrimination;complete information;statistics;profitability index	ECom	-2.8697685532078134	-3.419658732232825	41958
c940e13cc8d4faa6a79993a53c6960e6b64a0d67	a multi-stage production-inventory model with learning and forgetting effects, rework and scrap	learning;multistage production;forgetting;lot splitting;performance measures;imperfect quality	This paper studies a serial production line where a proportion of defective items is produced at each stage. Defective units enter a rework process, which is imperfect as well. Twice defective items are scrapped. This paper also considers learning and forgetting in production and rework processes and studies how the number of shipments of a lot from a production stage to the next influences the overall performance of the system. A model for a multi-stage production-inventory system is developed and optimized against an aggregate performance measure of four partial measures that are based on production time, process yield, in-process inventory and shipment frequency. Each of these partial performance measures is weighed by the system's decision maker in accordance to importance. The numerical results show how the values of learning rates, weights assigned to the partial performance measures and the number of production stages influence the overall performance of the system.	inventory theory;rework (electronics)	Christoph H. Glock;Mohamad Y. Jaber	2013	Computers & Industrial Engineering	10.1016/j.cie.2012.08.018	engineering;operations management;operations research;forgetting;engineering drawing	ML	8.541996881509316	0.7968992409329481	41960
3d3b09f3f393a4d81d0173431b1397fb38689b60	a combined model for short-term load forecasting based on bird swarm algorithm		Short-term load forecasting (STLF) plays a very important role in the power system scheduling of smart grid. In this paper, a variable weight combined load forecasting model is proposed, effectively improves the accuracy of short-term load forecasting. A prediction model is presented by combining there single prediction models, i.e. random forest, extreme learning machine and Elman neural network. Then a bird swarm-based intelligent algorithm is utilized to solve the weighting problem among them. Experimental results demonstrate that the new constructed prediction model has higher prediction accuracy than any single load forecasting model.		Zhengcai Cao;Lu Liu;Meng Zhou	2018	2018 IEEE 14th International Conference on Automation Science and Engineering (CASE)	10.1109/COASE.2018.8560515	predictive modelling;swarm behaviour;electric power system;smart grid;artificial neural network;scheduling (computing);extreme learning machine;algorithm;computer science;random forest	Robotics	9.170018383707491	-19.059112886440236	42005
6fb5ba869d65984e44026be84198a9c06c88a62c	adaptive spatio-temporal organization in groups of robots	learning rate;robots transportation orbital robotics learning multirobot systems computer science computational modeling computer simulation convergence computer hacking;cooperative robotics;reinforcement learning;adaptive systems multi robot systems learning artificial intelligence;adaptive systems;multi robot systems;learning artificial intelligence;input states robots straightforward transportation reinforcement learning abstract behaviors spatio temporal organization adaptive capabilities groups of robots	This paper presents experiments, in simulation, with a group of robots that improve their performance on a straightforward transportation task by using reinforcement learning to associate input states with a set of abstract behaviors. We show that the improvement in performance is a result of the group adapting its spatio-temporal organization to the given environment. Spatio-temporal adaptation is a general form of adaptation in that it can improve performance over a range of different tasks and environments. Hence it increases the general applicability and autonomy of robotic systems. Lastly, we present two communication strategies that improve this ability to adapt by generally improving learning rates for cooperative robots in highly dynamic domains.	autonomous robot;autonomy;cooperative mimo;experiment;os-tan;reinforcement learning;robot;simulation	Torbjørn S. Dahl;Maja J. Mataric;Gaurav S. Sukhatme	2002		10.1109/IRDS.2002.1041529	robot learning;error-driven learning;simulation;computer science;artificial intelligence;machine learning;reinforcement learning	Robotics	17.73390849429663	-20.033649392750075	42030
052c05a83b9030bdb0335ef32adbc990fbf7e074	deep learning for reward design to improve monte carlo tree search in atari games		Monte Carlo Tree Search (MCTS) methods have proven powerful in planning for sequential decision-making problems such as Go and video games, but their performance can be poor when the planning depth and sampling trajectories are limited or when the rewards are sparse. We present an adaptation of PGRD (policy-gradient for rewarddesign) for learning a reward-bonus function to improve UCT (a MCTS algorithm). Unlike previous applications of PGRD in which the space of reward-bonus functions was limited to linear functions of hand-coded state-action-features, we use PGRD with a multi-layer convolutional neural network to automatically learn features from raw perception as well as to adapt the non-linear reward-bonus function parameters. We also adopt a variance-reducing gradient method to improve PGRD’s performance. The new method improves UCT’s performance on multiple ATARI games compared to UCT without the reward bonus. Combining PGRD and Deep Learning in this way should make adapting rewards for MCTS algorithms far more widely and practically applicable than before.	algorithm;artificial neural network;atari;deep learning;gradient method;layer (electronics);linear function;long short-term memory;monte carlo method;monte carlo tree search;nonlinear system;rl (complexity);sampling (signal processing);sparse matrix	Xiaoxiao Guo;Satinder P. Singh;Richard L. Lewis;Honglak Lee	2016			simulation;computer science;artificial intelligence;machine learning	AI	20.03304924856353	-20.5463969252027	42097
764f50b88745116abea78857a1313d973ff84552	a decomposition method to analyze the performance of frame bridge based automated container terminal	automated container terminals;journal;performance analysis;port operation	This paper studies a new automated container terminal (ACT) system which utilizes multi-storey frame bridges and rail-mounted trolleys to transport containers between the quay and the yard. Different from widely used AGV-ACT systems, the ACT system studied in this paper uses three types of handling machines, which collaborate to transport containers. This study decomposes the container flow in the new ACT system into three queuing sub-networks. Then an iterative method is developed to analyze the operational efficiency of the ACT system. We analyze its transport efficiency by comparing with the widely used AGV-based systems. This study tries to help port operators better understand the relative merits of this new design and decide whether it is applicable in their terminals.		Hongtao Hu;Youfang Huang;Lu Zhen;Byung Kwon Lee;Loo Hay Lee;Ek Peng Chew	2014	Expert Syst. Appl.	10.1016/j.eswa.2013.07.050	simulation	Robotics	11.723985709224921	1.7771739922224643	42116
f019a083bc62738195674a043a6f30e8e16cb5a9	mission cost and reliability of 1-out-of- $n$  warm standby systems with imperfect switching mechanisms	期刊论文	In this paper, mission cost and reliability of 1-out-of-N: G nonrepairable warm standby systems with imperfect switching are modeled and analyzed using an iterative method. A general switching structure is considered, which consists of an overall fault detection mechanism (FDM) and a set of individual switches (one for each standby element). The failure of the FDM prevents the replacement of the failed online element by any standby element while the failure of an individual switch only makes the corresponding standby element unavailable. The entire mission fails either when the FDM fails before the failure of the online element during the mission, or when all the system elements have failed or become unavailable before the mission completion. Based on the proposed algorithm for the mission cost and reliability analysis, the optimal element sequencing problem is further formulated and solved for 1-out-of-N: G nonrepairable warm standby systems with nonidentical elements and imperfect switching mechanisms. The objective of the problem is to find the optimal initiation sequence of system elements that can minimize the expected mission cost while providing a certain level of system reliability. Examples are given to illustrate the considered problem and the proposed solution methodology.	algorithm;fault detection and isolation;finite difference method;iterative method;mathematical optimization;network switch;numerical analysis;numerical method;optimization problem;systems design	Gregory Levitin;Liudong Xing;Yuan-Shun Dai	2014	IEEE Transactions on Systems, Man, and Cybernetics: Systems	10.1109/TSMC.2013.2294328	actuarial science;computer science	Embedded	7.376919599570632	-0.8552376803893622	42120
67ffbe07a5ac0ec608e65408deba7f6259c00498	combining aspiration level methods in multi-objective programming and sequential approximate optimization using computational intelligence	reliability engineering;multiobjective programming;goal programming aspiration level methods multiobjective programming sequential approximate optimization computational intelligence pareto optimal solutions decision making engineering design problems objective function multiobjective engineering problems mu nu svm;sequential approximate optimization;pareto optimisation;aspiration level;engineering design;support vector machines;design engineering;pareto optimal solutions;computational intelligence;engineering computing;multi objective optimization;aspiration level methods;operations research;decision maker;delta modulation;pareto optimization;mu nu svm;systems engineering and theory;multiobjective engineering problems;objective function;support vector machines decision making design engineering computing learning artificial intelligence mathematical programming operations research pareto optimisation;multiple objectives;mathematical programming;engineering design problems;goal programming;design;simulation analysis;numerical experiment;learning artificial intelligence;pareto optimal solution;optimization methods computational intelligence delta modulation pareto optimization design engineering pareto analysis reliability engineering systems engineering and theory support vector machines decision making;pareto analysis;optimization methods	Since Pareto optimal solutions in multi-objective optimization are not unique but makes a set, decision maker (DM) needs to select one of them as a final decision. In this event, DM tries to find a solution making a well balance among multiple objectives. Aspiration level methods support DM to do this in an interactive way, and are very simple, easy and intuitive for DMs. Their effectiveness has been observed through various fields of practical problems. One of authors proposed the satisficing trade-off method early in '80s, and applied it to several kinds of practical problems. On the other hand, in many engineering design problems, the explicit form of objective function can not be given in terms of design variables. Given the value of design variables, under this circumstance, the value of objective function is obtained by some simulation analysis or experiments. Usually, these analyses are computationary expensive. In order to make the number of analyses as few as possible, several methods for sequential approximate optimization which make optimization in parallel with model prediction has been proposed. In this paper, we form a coalition between aspiration level methods and sequential approximate optimization methods in order to get a final solution for multi-objective engineering problems in a reasonable number of analyses. In particular, we apply mu-nu-SVM which was developed by the authors on the basis of goal programming. The effectiveness of the proposed method was shown through some numerical experiments.	approximation algorithm;computation;computational intelligence;decision theory;digital multiplex system;engineering design process;experiment;goal programming;loss function;mathematical optimization;multi-objective optimization;numerical analysis;optimization problem;pareto efficiency;simulation;software design	Hirotaka Nakayama;Yeboon Yun	2007	2007 IEEE Symposium on Computational Intelligence in Multi-Criteria Decision-Making	10.1109/MCDM.2007.369108	pareto analysis;design;mathematical optimization;delta modulation;computer science;multi-objective optimization;machine learning;computational intelligence;goal programming;management science	AI	14.751632136986007	-4.768453812653336	42129
ca5901233ca16cd01d7e08334886717c08aba5dd	synthesis of individual best local priority vectors in ahp-group decision making	ahp;consistency measures;best priority vectors;group decision making	An assessment of the individual judgments and AHP-produced priority vectors for involved decisionmakers indicates that the individual consistencies of decision makers may vary significantly, thus making the final group decision less reliable. In this paper, an approach is proposed as to how to combine decision makers’ local priority vectors in AHP synthesis and reduce so-called group inconsistency. Instead of aggregating individual judgments (AIJ), or aggregating individually derived final priorities (AIP), we propose to perform an AHP synthesis of the best local priority vectors taken from the most consistent decision makers. The approach and related algorithm we label as MGPS after the key terms ‘multicriteria group prioritization synthesis.’ The concept is analogous to the one proposed by Srdjevic [1] for individual AHP applications where the best local priority vectors are selected based on the consistency performance onsistency measures of several of the most popular prioritization methods. Here, decision makers are combined instead of prioritization methods, and group context is fully implemented. After completing an evaluation of the decision makers inconsistencies in each node of the hierarchy, the selected best local priority vectors are synthesized in a standard manner, and the final solution is declared to be an AHP-group decision. Two numerical examples indicate that the developed approach and algorithm generate the final priorities of alternatives with the lowest overall inconsistency (in the multicriteria sense).	adaptive internet protocol;algorithm;artificial intelligence;numerical analysis	Bojan Srdjevic;Zorica Srdjevic	2013	Appl. Soft Comput.	10.1016/j.asoc.2012.11.010	analytic hierarchy process;group decision-making;computer science;data mining;management science	ML	-3.8134509534771697	-19.153079146555196	42180
a2a642e78212e0c9ab8dc7ef27e98269f03cffab	a differential evolution algorithm for the capacitated vrp with flexibility of mixing pickup and delivery services and the maximum duration of a route in poultry industry	poultry houses;hen egg production;differential evolution;self-organizing map;total cost	In this paper, we propose two heuristics to solve the General Q-Delivery Vehicle Routing Problem with consideration of flexibility of mixing pickup, delivery services and a maximum duration of a route constraint which is the extending version of the well-known VRP with pickup and delivery problem. Firstly, the heuristic called DE_G-QDVRP-FD is presented to determine the routing of transferring pullets from pullet houses to hen houses. Since the problem considered is very complicated, the DE_GQ-DVRP-FD is extended to the two-phase heuristic called MESOMDE_G-Q-DVRP-FD. The difference between two heuristics is that in the MESOMDE_G-Q-DVRP-FD algorithm, the customer vertices (pullet houses) will be clustered before determining routes. The clustering of customer vertices method called the Multifactor Based Evolving SelfOrganizing Map is proposed in the first phase in order to completely utilize the vehicle. Finally, in the second phase, the DE_G-Q-DVRP-FD is used to execute the routing. To demonstrate the algorithm efficiency, flock allocation from D. Dechampai · K. Sethanan Research Unit on System Modeling for Industry, Department of Industrial Engineering, Faculty of Engineering, Khon Kaen University, Khon Kaen 40002, Thailand e-mail: darat_dech@hotmail.com K. Sethanan e-mail: ksethanan@gmail.com L. Tanwanichkul (B) Department of Civil Engineering, Faculty of Engineering, Khon Kaen University, Khon Kaen 40002, Thailand e-mail: ladda.tan@hotmail.com R. Pitakaso Metaheuristics for Logistic Optimization Laboratory, Department of Industrial Engineering, Ubon Ratchathani University, Ubon Ratchathani, Thailand e-mail: enrapepi@ubu.ac.th pullet houses to hen houses in the egg industry is used as the case study. The results obtained from this study show that the MESOMDE_G-Q-DVRP-FD algorithm provides lower total cost values than that of the firm’s current practice by 7.59– 31.28 and 0.84–13.15 % better than the DE_G-Q-DVRP-FD algorithm. Additionally, the MESOMDE_G-Q-DVRP-FD is adjusted to solve the benchmark problem found in the literature. The experimental results show that the MESOMDE_GQ-DVRP-FD algorithm yields better total cost values by 5.72–61.60 % (with an average of 31.46 %).	algorithm;algorithmic efficiency;benchmark (computing);chicken;cluster analysis;differential evolution;distance-vector routing protocol;email;flock;heuristic (computer science);industrial engineering;metaheuristic;neural gas;production system (computer science);rs-232;two-phase commit protocol;vehicle routing problem;whole earth 'lectronic link	Darat Dechampai;Ladda Tanwanichkul;Kanchana Sethanan;Rapeepan Pitakaso	2017	J. Intelligent Manufacturing	10.1007/s10845-015-1055-3	mathematical optimization;simulation;engineering	AI	17.855970813040628	0.6278228795897032	42187
e2a2e772bb9bf3ea396ac264fddd8ef3ec23e72f	a biobjective approach to recoverable robustness based on location planning		Finding robust solutions of an optimization problem is an important issue in practice, and various concepts on how to define the robustness of a solution have been suggested. The idea of recoverable robustness requires that a solution can be recovered to a feasible one as soon as the realized scenario becomes known. The usual approach in the literature is to minimize the objective function value of the recovered solution in the nominal or in the worst case. As the recovery itself is also costly, there is a trade-off between the recovery costs and the solution value obtained; we study both, the recovery costs and the solution value in the worst case in a biobjective setting. To this end, we assume that the recovery costs can be described by a metric. We show that in this case the recovery robust problem can be reduced to a location problem. We show how weakly Pareto efficient solutions to this biobjective problem can be computed by minimizing the recovery costs for a fixed worst-case objective function value and present approaches for the case of linear and quasiconvex problems for finite uncertainty sets. We furthermore derive cases in which the size of the uncertainty set can be reduced without changing the set of Pareto efficient solutions.	schedule (computer science)	Emilio Carrizosa;Marc Goerigk;Anita Schöbel	2017	European Journal of Operational Research	10.1016/j.ejor.2017.02.014	mathematical optimization;operations management;mathematics;welfare economics	Robotics	9.684553270286193	0.06192553674760288	42218
8e4fae2f2569990d02025afe777c4e486699c165	simulation-based modeling and analysis of schedule instability in automotive supply networks		Within automotive supply chains, instability of order schedules of original equipment manufacturers (OEMs) creates inefficiencies in suppliers’ production processes. Due to the market power of the OEM, first tier suppliers are not always able to influence the scheduling behavior of their customers. However, addressing the root causes of schedule instability, in particular the unreliability of suppliers’ production processes, can help to curtail short-term demand variations and increase the overall supply chain efficiency. To this end, we introduce a stylised assembly supply chain model with two suppliers and a single OEM. This supply chain can be disrupted by a shortage occurring at one of the two suppliers due to random machine breakdowns, what consequently creates dependent requirements variations affecting both the buyer and the other supplier. Therefore the paper at hand contains two main sections. At first, a simulation model is developed containing the said mechanism causing schedule instability. Secondly, a simulation study is carried out to derive managerial and theoretical implications accordingly.	instability;simulation	Tim Gruchmann;Thomas Gollmann	2015		10.1007/978-3-319-42902-1_61	simulation	Robotics	7.472144502051797	-1.9283743430411162	42220
ca2b99296e8bff8d7c59780c0e235f9c1ea91e71	evaluation of operational factors affecting the rfid performance in cargo sorting operations			sorting	Ozgur Kabadurmus;Mehmet Serdar Kilinç;Alp Üstündag	2012	I. J. RF Technol.: Res. and Appl.	10.3233/RFT-2012-023	operations management;automotive engineering;transport engineering	Networks	9.617931995676066	-3.7614618991837974	42244
c11e17675c1d3097dd66ec85bc260eb5afbc7f63	improving linear search algorithms with model-based approaches for maxsat solving	incrementality in maxsat;maximum satisfiability;linear search algorithms;model based approaches	Improving linear search algorithms with modelbased approaches for MaxSAT solving Ruben Martins, Vasco Manquinho & Inês Lynce To cite this article: Ruben Martins, Vasco Manquinho & Inês Lynce (2015) Improving linear search algorithms with model-based approaches for MaxSAT solving, Journal of Experimental & Theoretical Artificial Intelligence, 27:5, 673-701, DOI: 10.1080/0952813X.2014.993508 To link to this article: http://dx.doi.org/10.1080/0952813X.2014.993508	journal of experimental and theoretical artificial intelligence;joão pavão martins;linear search;maximum satisfiability problem;search algorithm	Ruben Martins;Vasco M. Manquinho;Inês Lynce	2015	J. Exp. Theor. Artif. Intell.	10.1080/0952813X.2014.993508	mathematical optimization;machine learning;algorithm	AI	23.883022062083892	2.310348923336533	42258
1a92df14d2ede0fa8a15bf032ed8eb4f52806306	ant colony optimization	ant colony optimization	Ant colony optimization (ACO) is a metaheuristic that was originally introduced for solving combinatorial optimization problems.  In this chapter we present the general description of ACO, as well as its adaptation for the application to continuous optimization  problems. We apply this adaptation of ACO to optimize the weights of feed-forward neural networks for the purpose of pattern  classification. As test problems we choose three data sets from the well-known PROBEN1 medical database. The experimental  results show that our algorithm is comparable to specialized algorithms for feed-forward neural network training. Furthermore,  the results compare favourably to the results of other general-purpose methods such as genetic algorithms.  	ant colony optimization algorithms;program optimization	Katya Rodríguez-Vázquez	2005	Genetic Programming and Evolvable Machines	10.1007/s10710-005-2991-z	ant colony optimization algorithms;computer science	Theory	23.291969786001502	-2.6008388697894484	42282
baafaa10eaa3f3d752b767df635da3dec4ef5ae9	discrete fuzzy grasp affordance for robotic manipulators		Grasp affordance determines the object-hand relative configurations which lead to successful grasps. Generation and representation of grasp affordances can increase achieved grasp quality and be integrated in path planning algorithms facilitating increased efficiency. Grasp quality is determined by various measures and may have a major impact on task success. Fuzzy grasp affordance can be defined based on a fuzzy grasp quality grade and enhance the previously Boolean notion of grasp affordance. Fuzzy grasp affordances can be represented using a discrete manifold. This facilitates integration of data from various sources and representation optimization using evolutionary algorithms. A method for construction of a discrete fuzzy grasp affordance manifold is presented and demonstrated for apple selective harvesting. The affordance constructed is based on learning from human demonstration. It includes quality grade determination, manifold structure determination, cell quantization, and smoothing. An algorithm for adaptation of the computed manifold to different manipulators and grippers is developed and implemented for two different end effectors. Additionally a method for online integration of the developed affordance is presented.	cell (microprocessor);cloud computing;evolutionary algorithm;grasp;job shop scheduling;manifold regularization;mathematical optimization;motion planning;smoothing;social affordance	Danny Eizicovits;Maayan Yaacobovitch;Sigal Berman	2012		10.3182/20120905-3-HR-2030.00091	computer vision;computer science;artificial intelligence;machine learning	Robotics	18.265287652578646	-7.8618495583904515	42297
911bfc3892900bad8e23e0f61f17b84232e7342e	a multi-objective stochastic approach to hydroelectric power generation scheduling	dynamic programming;unit commitment dynamic programming hydropower scheduling multi objective optimization stochastic optimization;zinātniskās publikācijas;rīgas tehniskā universitāte;optimization stochastic processes uncertainty reservoirs power generation computational modeling;multi objective optimization;stochastic processes hydroelectric power stations pareto optimisation power generation dispatch power generation scheduling power markets;izdevums rtu zinātniskie raksti;stochastic optimization;pareto optimal hourly dispatch schedule multiobjective stochastic approach multiobjective optimization hydroelectric power generation short term scheduling artificial neural network based algorithm market prices forecasting water inflow uncertainty modeling profit based unit commitment hydropower plant day ahead bidding strategy;rtu;hydropower scheduling;unit commitment	In this paper, we propose a novel stochastic approach to multi-objective optimization of hydroelectric power generation short-term scheduling. Maximization of profit is chosen as the main objective with additional sub-objective-to reduce the number of startups and shutdowns of generating units. The random nature of future electricity prices and river water inflow is taken into account. We use an artificial neural network-based algorithm to forecast market prices and water inflow. Uncertainty modeling is introduced to represent the stochastic nature of parameters and to solve the short-term optimization problem of profit-based unit commitment. A case study is conducted on a real-world hydropower plant to demonstrate the feasibility of the proposed algorithm by providing the power generation company with the day-ahead bidding strategy under market conditions and a Pareto optimal hourly dispatch schedule of the generating units.	artificial neural network;dynamic dispatch;expectation–maximization algorithm;mathematical optimization;multi-objective optimization;nonlinear programming;nonlinear system;optimization problem;pareto efficiency;scheduling (computing);stochastic process;the daily wtf	Antans Sauhats;Roman Petrichenko;Karlis Baltputnis;Zane Broka;Renata Varfolomejeva	2016	2016 Power Systems Computation Conference (PSCC)	10.1109/PSCC.2016.7540821	mathematical optimization;simulation;economics;operations management	AI	15.566710900421535	-3.2786141425120805	42299
d99d168ad6c59adb6dadc6517f6c0e0e858bc385	a hybrid ga/pso for the concurrent design of cellular manufacturing system	cellular manufacturing biological cells workstations group technology algorithm design and analysis genetic algorithms cells biology particle swarm optimization load management cost benefit analysis;performance measure;cellular manufacturing system;group layout problem hybrid genetic algorithm particle swarm optimization concurrent design cellular manufacturing system cell formation problem;particle swarm optimisation cellular manufacturing concurrent engineering genetic algorithms;search algorithm;layout;concurrent design;biological cells;particle swarm optimization cellular manufacturing system hybrid algorithm genetic algorithm;particle swarm optimizer;group layout problem;cell formation;particle swarm optimization;workstations;load management;cell formation problem;genetic algorithm;genetic algorithms;global optimization;load modeling;particle swarm optimisation;hybrid algorithm;concurrent engineering;cellular manufacturing;gallium;hybrid genetic algorithm	In this paper, a hybrid search algorithm using genetic algorithm (GA) and particle swarm optimization (PSO) is implemented for the concurrent design of cellular manufacturing system. Traditionally, cell formation (CF) and group layout (GL) problems were considered sequentially therefore the results may be optimal in one phase but during implementation of the whole cellular manufacturing, it may not be globally optimal. Based on the studies by earlier researchers concurrent approach does indeed lead to better solution quality than the sequential approach by a magnitude of 2% to 20%. Three performance measures are considered to evaluate the proposed method. They are to minimize total inter-cell and intra-cell moves, total cell load variation and total inter cell moves of part families. The performance of the proposed hybrid GA/PSO is evaluated with the test problems available in the literature. The results obtained clearly indicate the better performance of the proposed heuristic.	cell signaling;genetic algorithm;heuristic;mathematical optimization;maxima and minima;particle swarm optimization;search algorithm;software release life cycle;turing test	Lim Chee Ming;S. G. Ponnambalam	2008	2008 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2008.4811559	mathematical optimization;simulation;genetic algorithm;computer science;artificial intelligence;global optimization;concurrent engineering	Robotics	20.710579357371103	-1.6155721646458767	42300
fdc4ae84306fecd0955c317572eaf38aa964c017	optimal operating policies for m/g/1 queuing systems	queuing system	We consider the economic behavior of a M/G/1 queuing system operating with the following cost structure: a server start-up cost, a server shut-down cost, a cost per unit time when the server is turned on, and a holding cost per unit time spent in the system for each customer. We prove that for the single server queue there is a stationary optimal policy of the form: Turn the server on when n customers are present, and turn it off when the system is empty. For the undiscounted, infinite horizon problem, an exact expression for the cost rate as a function of n and a closed form expression for the optimal value of n are derived. When future costs are discounted, we obtain an equation for the expected discounted cost as a function of n and the interest rate, and prove that for small interest rates the optimal discounted policy is approximately the optimal undiscounted policy. We conclude by establishing the recursion relation to find the optimal (nonstationary) policy for finite horizon problems.	queueing theory	Daniel P. Heyman	1968	Operations Research	10.1287/opre.16.2.362	mathematical optimization;simulation;economics;operations management;mathematics;queue management system	OS	3.7535207780468918	-1.3961369361254583	42349
970e478b0e5248f3761a60cb93ada63acc6bf2aa	on regular filters and well filters of pseudo-bci algebras		Pseudo-BCI algebra is a kind of non-classical logic algebra; it is a generalization of pseudo-BCK algebra which is closely related to non-commutative fuzzy logic algebras. In this paper, a new notion of regular filter of pseudo-BCI algebra is proposed, and a characteristic property is given. Moreover, the relationships among regular filters and other filters of pseudo-BCI algebras are presented, and the following important propositions are proved: (1) the notions of well filter and normal filter in pseudo-BCI algebras are coincide; (2) a filter of pseudo-BCI algebra is p-filter iff (that is, if and only if) it is well anti-grouped filter (or normal anti-grouped filter); (3) a filter of pseudo-BCI algebra is regular iff it is closed anti-grouped filter; (4) a filter of pseudo-BCI algebra is an associative filer (or pseudo-a filter) if and only if it is regular T-type filter (or regular pseudo q-filter). Finally, a counterexample is given to show that a regular p-filter (or well regular filter) of pseudo-BCI algebra may be not associative filter (or pseudo-a filter).	brain–computer interface;fuzzy logic;netapp filer	Xiaohong Zhang;Choonkil Park	2017	2017 13th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)	10.1109/FSKD.2017.8392926	mathematical optimization;counterexample;discrete mathematics;fuzzy logic;if and only if;computer science;algebraic logic;associative property;fuzzy control system	DB	0.3875103313801367	-23.147469493801776	42422
b1e801f5b79cf9967b2d68ff7409c73218b4cfa0	a stochastic fleet composition problem	dynamic programming;convex optimization;journal article;golden section method;fleet composition	In this paper, we consider the problem of forming a new vehicle fleet, consisting of multiple vehicle types, to cater for uncertain future requirements. The problem is to choose the number of vehicles of each type to purchase so that the total expected cost of operating the fleet is minimized. The total expected cost includes fixed and variable costs associated with the fleet, as well as hiring costs that are incurred whenever vehicle requirements exceed fleet capacity. We develop a novel algorithm, which combines dynamic programming and the golden section method, for determining the optimal fleet composition. Numerical results show that this algorithm is highly effective, and takes just seconds to solve large-scale problems involving hundreds of different vehicle types. & 2012 Elsevier Ltd. All rights reserved.	algorithm;dynamic programming;numerical method;requirement	Ryan C. Loxton;Qun Lin;Kok Lay Teo	2012	Computers & OR	10.1016/j.cor.2012.04.004	mathematical optimization;convex optimization;simulation;golden section search;dynamic programming;mathematics	AI	13.951654423971853	2.509877874378846	42443
b59dd90a8fc0465b15a600dea256a3c43d0c6174	load-frequency control service in a deregulated environment	factor;eficacia sistema;frequency regulation;puissance electrique;deregulacion;modele mathematique;rate of change;control frecuencia potencia;performance systeme;commande frequence puissance;modelo matematico;regulacion frecuencia;system performance;area control error;erreur controle;facteur systeme agc;independent power producer;system agc factor;interconnected system;system agc;mathematical model;dereglementation;potencia electrica;principio potencia virtual;deregulation;virtual power principle;electric power;regulation frequence;economic dispatch;principe puissance virtuelle;load frequency control	may the ethod by the dent the ll the ation the same Abstract: In a deregulated environment independent generators and utility generators may or not participate in the load-frequency control of the system. For the purpose of evaluating performance of such a system, a flexible method has been developed and implemented. The m assumes that load frequency control is performed by an ISO based on parameters defined participating generating units. The participating units comprise utility generators and indepen power producers. The utilities define the units which will be under load-frequency control, while independent power producers may or may not participate in the load frequency control. For a units which participate in the load-frequency control, the generator owner defines (a) gener limits, (b) rate of change and (c) economic participation factor. This information is transmitted to ISO. This scheme allows the utilities to economically dispatch their own system, while at the time permit the ISO to control the interconnected system operation.	dynamic dispatch;requirement	A. P. Sakis Meliopoulos;George J. Cokkinides;A. G. Bakirtzis	1999	Decision Support Systems	10.1016/S0167-9236(98)00078-5	deregulation;electric power;telecommunications;computer science;operations management;mathematical model;computer performance;economic dispatch	Embedded	1.7592895728966291	3.5179849585675926	42492
8421d3a4dddde39957844eae7ab7c3fe39d8e046	a strategy for time series prediction using segment growing neural gas		Segment Growing Neural Gas (Segment-GNG) has been recently proposed as a new spatiotemporal quantization method for time series. Unlike traditional quantization algorithms that are prototype-based, Segment-GNG uses segments as basic units of quantization. In this paper we extend the Segment-GNG model in order to deal with time series prediction. First Segment-GNG makes a quantization of the trajectories in the state-space representation of the time series. Then a local prediction model is associated with each segment, which allows us to make predictions. The proposed model is tested with the Mackey-Glass and Lorenz chaotic time series in one-step ahead prediction tasks. The results obtained are competitive with the best results published in the literature.	algorithm;neural gas;prototype;quantization (signal processing);state-space representation;time series	Jorge R. Vergara;Pablo A. Estévez	2017	2017 12th International Workshop on Self-Organizing Maps and Learning Vector Quantization, Clustering and Data Visualization (WSOM)	10.1109/WSOM.2017.8020033	time series;neural gas;quantization (signal processing);trajectory;machine learning;computer science;artificial intelligence	ML	9.459963114066126	-22.44409734429681	42511
cf79b371bb05dacdcc13ee168e4a1ba4e21d9377	parameter control in practice	journal article	In this chapter we summarize our experience of tuning and/or controlling various parameters of evolutionary algorithms from working on a variety of complex real word problems. We illustrate some issues on one particular case study (car distribution system). This chapter contains also general discussion on the prediction and optimization issues present in dynamic environments, and explains the ideas behind Adaptive Business Intelligence.	evolutionary algorithm;mathematical optimization;sensor;software system	Zbigniew Michalewicz;Martin Schmidt	2007		10.1007/978-3-540-69432-8_14	cylinder;acoustics;cartridge;magnetic tape;drum;geology	AI	17.040820904630767	-7.482872119154805	42514
d42a5c67fd84e6b8b29717392086461571943eca	improving approximate value iteration with complex returns by bounding	complex returns;reinforcement learning;off policy;approximate value iteration	Approximate value iteration (AVI) is a widely used technique in reinforcement learning. Most AVI methods do not take full advantage of the sequential relationship between samples within a trajectory in deriving value estimates, due to the challenges in dealing with the inherent bias and variance in the n-step returns. We propose a bounding method which uses a negatively biased but relatively low variance estimator generated from a complex return to provide a lower bound on the observed value of a traditional one-step return estimator. In addition, we develop a new Bounded FQI algorithm, which efficiently incorporates the bounding method into an AVI framework. Experiments show that our method produces more accurate value estimates than existing approaches, resulting in improved policies.	algorithm;converge;experiment;ibm notes;iteration;keneth alden simons;markov decision process;realization (probability);reinforcement learning;singular value decomposition	Robert William Wright;Xingye Qiao;Steven Loscalzo;Lei Yu	2015			mathematical optimization;computer science;artificial intelligence;machine learning;reinforcement learning;statistics	AI	23.175668819049275	-19.33560131523661	42550
035dc8e203920adbac854747871667d4a698dd80	demand growth in services: a discrete choice analysis of customer preferences and online selling	survey research and regression methods;administracion de empresas;期刊论文;economia y empresa	The selling of perishable services (e.g., hotel rooms, airline seats, and rental cars) online is increasingly popular with both retailers and consumers. Among the innovative approaches to online sales is opaque selling. First popularized by Priceline.com's name-your-own-price model, opaque selling hides some attributes of the service (notably, brand and specific location) until after the purchase decision, in exchange for a discounted price. This means that a branded “product” is being sold as somewhat of a commodity, but the brand “name” is protected by the opaque model. The attraction of this model for retailers is that they are presumably able to increase their revenue stream, albeit at a lower rate, by selling rooms that otherwise would remain in inventory. In this article, we outline the development and analysis of an online choice survey to understand consumer preferences among three types of online distribution channels: regular full information sales channels, and opaque sales channels with or without consumer bidding. A Multinomial Logit model is employed to analyze the data and measure the consumer trade-offs between price and other attributes of the product. We use the estimated model to calculate the incremental demand and revenue created by using an opaque channel simultaneously with regular full information channels. On balance, we find that correctly priced opaque channels can add to hotels revenue streams without undue cannibalization of regular room sales.	discrete choice;online shopping	Xiaoqing Xie;Rohit Verma;Chris K. Anderson	2016	Decision Sciences	10.1111/deci.12177	economics;marketing;advertising;management;commerce	ML	-2.2204562204645044	-9.03702139121608	42557
e394e5e6bf8b1a8520a2c6c7879b06959d997b58	consequences of order crossover under order-up-to inventory policies	order crossover;iterative algorithm;lead time;inventory policies;stochastic lead time	Order crossover occurs whenever replenishment orders do not arrive in the sequence in which they were placed. This paper argues that order crossover is becoming more prevalent and analyzes the dangers of ignoring it. We present an exact iterative algorithm for computing the distribution of the number of orders outstanding, and formulae for the inventory shortfall distribution (the quantity of inventory in replenishment at the start of a period) and the more common lead-time demand distribution, which are different when order crossover is possible. The lead-time demand distribution can have much higher variability than the shortfall distribution. We show that basing inventory policies on the lead-time demand distribution--rather than the shortfall distribution--can lead to significantly higher inventory cost, even if the probability of order crossover is small. We give an alternative proof to that of Zalkind (1976), which shows that the variance of shortfall is less than the variance of the standard lead-time demand.		Lawrence W. Robinson;James R. Bradley;L. Joseph Thomas	2001	Manufacturing & Service Operations Management	10.1287/msom.3.3.175.9887	financial economics;economics;operations management;iterative method;mathematical economics	Robotics	3.0267716923596835	-4.148262349614169	42567
fca52453f3c59d37b4b0ee36ff261fdd125d2e01	a demand estimation procedure for retail assortment optimization with results from implementations	assortment planning;grupo de excelencia;estimation;administracion de empresas;statistics;economia y empresa;grupo a;retailer operations	We consider the problem of choosing, from a set of N potential SKUs in a retail category, K SKUs to be carried at each store so as to maximize revenue or profit. Assortments can vary by store, subject to a maximum number of different assortments. We introduce a model of substitution behavior, in case a customer’s first choice is unavailable and consider the impact of substitution in choosing assortments for the retail chain. We view a SKU as a set of attribute levels, apply maximum likelihood estimation to sales history of the SKUs currently carried by the retailer to estimate the demand for attribute levels and substitution probabilities, and from this, the demand for any potential SKU, including those not currently carried by the retailer. We specify several alternative heuristics for choosing SKUs to be carried in an assortment. We apply this approach to optimize assortments for three real examples: snack cakes, tires and automotive appearance chemicals. A portion of our recommendations for tires and appearance chemicals were implemented and produced sales increases of 5.8% and 3.6% respectively, which are significant improvements relative to typical retailer annual comparable store revenue increases. We also forecast sales shares of 1, 11 and 25 new SKUs, for the snack cakes, tires and automotive appearance chemicals applications, respectively, with MAPEs of 16.2%, 19.1% and 28.7%, which compares favorably to the 30.7% MAPE for chain sales of two new SKUs reported by Fader and Hardie (1996).	coefficient;estimation theory;heuristic (computer science);internationalization and localization;latent class model;program optimization;substitution (logic)	Marshall Fisher;Ramnath Vaidyanathan	2014	Management Science	10.1287/mnsc.2014.1904	estimation;economics;marketing;operations management;mathematics;advertising;statistics	Metrics	1.3877358995807265	-8.79920369344609	42580
079d9552982db71d5808fec260316d359bb81ed4	learning a value analysis tool for agent evaluation	value analysis;value function;variance estimation;monte carlo	Evaluating an agent’s performance in a stochastic setting is necessary for agent development, scientific evaluation, and competitions. Traditionally, evaluation is done using Monte Carlo estimation; the magnitude of the stochasticity in the domain or the high cost of sampling, however, can often prevent the approach from resulting in statistically significant conclusions. Recently, an advantage sum technique has been proposed for constructing unbiased, low variance estimates of agent performance. The technique requires an expert to define a value function over states of the system, essentially a guess of the state’s unknown value. In this work, we propose learning this value function from past interactions between agents in some target population. Our learned value functions have two key advantages: they can be applied in domains where no expert value function is available and they can result in tuned evaluation for a specific population of agents (e.g., novice versus advanced agents). We demonstrate these two advantages in the domain of poker. We show that we can reduce variance over state-of-the-art estimators for a specific population of limit poker players as well as construct the first variance reducing estimators for no-limit poker and multi-player limit poker.	bellman equation;general game playing;interaction;loss function;mathematical optimization;mean squared error;monte carlo method;nonlinear system;random number generation;sampling (signal processing)	Martha White;Michael H. Bowling	2009			econometrics;simulation;artificial intelligence;machine learning;bellman equation;statistics;monte carlo method	AI	23.641696504796915	-18.784843150589058	42606
3dc46e50ba6008ed4cf6a71790319ef1e2ffdd95	concurrent design of machined products: a multivariate decision approach	machining;genetic algorithms concurrent engineering machining product development design for manufacture decision support systems operations research;design for assembly;life cycle;design criteria;product life cycle;design for manufacture;implementation methodology concurrent design machined products multivariate decision approach product design sequential approach design steps manufacturing assembly downstream activities product life cycle design decisions concurrent engineering approach design flaws design requirements product development life cycle design stage methodological framework product life cycle design concurrent engineering high level design features design for manufacture design for assembly design for productivity two level design design candidates multiple design criteria utility theory design information user preference attribute values genetic algorithm parametric optimization procedure milling fixture;user preferences;operations research;decision support systems;genetic algorithm;genetic algorithms;parametric optimization;product design;concurrent engineering;utility theory;product design manufacturing processes concurrent engineering algorithm design and analysis assembly systems process design product development design for manufacture doped fiber amplifiers productivity;product development	In the traditional process of product design, a sequential approach treats each of the design steps individually without considering requirements of manufacturing, assembly and other downstream activities in the product life cycle. The lack of systematic and simultaneous consideration on the impact of design decisions on manufacturing and assembly leads to repeated and excessive changes in design and processes. To resolve this problem, the concurrent engineering approach to product design foresees and avoids potential design flaws by incorporating design requirements from downstream activities of the product development life cycle early in the design stage. This research develops a methodological framework for product life cycle design in concurrent engineering. Through a cohesive organization of semantics of high-level design features and relationships, this representation provides a means to evaluate the impact of design on subsequent activities in the product life cycle, including design for manufacture (DFM), design for assembly (DFA), and design for productivity (DFP). In the first stage of a two-level design, selection of design candidates is made based on multiple design criteria using utility theory, taking into account imprecision of design information and user preference. The second stage of design further fine-tunes attribute values of the selected design by a genetic algorithm based parametric optimization procedure. An illustrative example of two alternative designs of a milling fixture demonstrates the effectiveness of the framework and its implementation methodology.	design for manufacturability;downstream (software development);genetic algorithm;high- and low-level;level design;mathematical optimization;new product development;requirement;software development process;test fixture;utility	S. Wesley Changchien;Li Lin	2000	IEEE Trans. Systems, Man, and Cybernetics, Part C	10.1109/5326.868446	iterative design;genetic algorithm;probabilistic design;axiomatic design;computer science;computer-automated design;product design specification;design review;product design;design technology;high-level design;generative design;product engineering	EDA	12.927158852686935	-3.8498473596916	42616
52d11d312e86955513c5f1a4ae757d95327cbe69	integrating learning from examples into the search for diagnostic policies	ao;search space;search algorithm;diagnostic decision making;search method;diagnostic policy;artificial intelligent;learning from examples;greedy algorithm;technical report;markov decision process	This paper studies the problem of learning diagnostic policies from training examples. A diagnostic policy is a complete description of the decision-making actions of a diagnostician (i.e., tests followed by a diagnostic decision) for all possible combinations of test results. An optimal diagnostic policy is one that minimizes the expected total cost, which is the sum of measurement costs and misdiagnosis costs. In most diagnostic settings, there is a tradeo between these two kinds of costs. This paper formalizes diagnostic decision making as a Markov Decision Process (MDP). The paper introduces a new family of systematic search algorithms based on the AO algorithm to solve this MDP. To make AO e cient, the paper describes an admissible heuristic that enables AO to prune large parts of the search space. The paper also introduces several greedy algorithms including some improvements over previously-published methods. The paper then addresses the question of learning diagnostic policies from examples. When the probabilities of diseases and test results are computed from training data, there is a great danger of over tting. To reduce over tting, regularizers are integrated into the search algorithms. Finally, the paper compares the proposed methods on ve benchmark diagnostic data sets. The studies show that in most cases the systematic search methods produce better diagnostic policies than the greedy methods. In addition, the studies show that for training sets of realistic size, the systematic search algorithms are practical on today's desktop computers.	admissible heuristic;ambient occlusion;benchmark (computing);desktop computer;early stopping;greedy algorithm;markov chain;markov decision process;post's theorem;search algorithm	Valentina Bayer Zubek;Thomas G. Dietterich	2005	J. Artif. Intell. Res.	10.1613/jair.1512	markov decision process;mathematical optimization;greedy algorithm;computer science;artificial intelligence;technical report;machine learning;data mining;mathematics;search algorithm	AI	21.699017732347983	-15.165383432427147	42641
631540ff77acd5bc05c302b84abe77cbb3825138	intelligent system for freeway ramp metering control	optimisation;traffic control vehicles learning testing intelligent systems;entrance freeway ramps intelligent system freeway ramp metering control transportation system signal control traffic operations optimization artificial intelligence technique reinforcement learning intelligent agents optimal performance freeway corridor q learning algorithm optimal control;learning;software agents automated highways learning artificial intelligence optimal control optimisation road traffic control;reinforcement learning;intelligent control artificial intelligence reinforcement learning;automated highways;traffic control;testing;intelligent control;software agents;optimal control;road traffic control;intelligent systems;artificial intelligence;vehicles;learning artificial intelligence	The number of techniques implemented in subsystems of the Intelligent System of infrastructure in transportation system in terms of agents for signal control, ramp metering, detecting incidents is numerous. Challenges, however, are still there for the researchers to optimize traffic operations. The aim of this paper is to prove the ability of artificial intelligence technique known as reinforcement learning implemented in intelligent system for freeway control. Intelligent agents are implemented as controllers in order to provide optimal performance on the freeway corridor via ramp metering control on a corridor. The algorithm used in the research was Q learning algorithm. The results are promising proving that the technique is capable for optimal control of entrance freeway ramps and suitable for building the intelligent system of the freeway.	algorithm;artificial intelligence;freeway;intelligent agent;optimal control;q-learning;ramp simulation software for modelling reliability, availability and maintainability;reinforcement learning;sensor	Kostandina Veljanovska;Zoran Gacovski;Stojce Deskovski	2012	2012 6th IEEE International Conference Intelligent Systems	10.1109/IS.2012.6335230	control engineering;simulation;engineering;transport engineering	Robotics	15.819457235729772	-16.890658275729184	42662
e9d55806239097c1b5a298c55cc1bcda71986a78	a two-stage intelligent search algorithm for the two-dimensional strip packing problem	stock cutting problem;simulated annealing;packing problem;heuristic search;packing problem heuristic search simulated annealing;heuristic algorithms;genetic algorithm;article;rectangle packing;orthogonal packing	This paper presents a two-stage intelligent search algorithm for a two-dimensional strip packing problem without guillotine constraint. In the first stage, a heuristic algorithm is proposed, which is based on a simple scoring rule that selects one rectangle from all rectangles to be packed, for a given space. In the second stage, a local search and a simulated annealing algorithm are combined to improve solutions of the problem. In particular, a multi-start strategy is designed to enhance the search capability of the simulated annealing algorithm. Extensive computational experiments on a wide range of benchmark problems from zero-waste to non-zero-waste instances are implemented. Computational results obtained in less than 60 seconds of computation time show that the proposed algorithm outperforms the supposedly excellent algorithms reported recently, on average. It performs particularly better for large instances. 2011 Elsevier B.V. All rights reserved.	benchmark (computing);computation;data structure;experiment;heuristic (computer science);industry standard architecture;local search (constraint satisfaction);local search (optimization);search algorithm;set packing;simulated annealing;time complexity	Stephen C. H. Leung;Defu Zhang;Kwang Mong Sim	2011	European Journal of Operational Research	10.1016/j.ejor.2011.06.002	beam search;mathematical optimization;packing problems;combinatorics;bin packing problem;set packing;genetic algorithm;heuristic;simulated annealing;computer science;hill climbing;machine learning;mathematics;incremental heuristic search;best-first search;adaptive simulated annealing;search algorithm	AI	23.03881121963751	0.5732937531396943	42699
e9f88bc2fd471c793a8b8dec219eb295180f1046	analysis of automotive body assembly system configurations for quality and productivity	control dimensional;concepcion ingenieria;engineering design;dimensional control;productivite;automovil;system configuration;linea montaje;conception ingenierie;performance;configuracion paralela;assembly error;automotive body assembly;productividad;assembly configuration;network analysis;hybrid configuration;configuration hybride;controle dimensionnel;assembly;assembly errors;automobile;quality;motor car;parallel configuration;assembly line;assembly systems;montage;configuracion hibrida;variation;productivity;car body;montaje;carroceria;configuration;analyse circuit;configuration parallele;automobile industry;analisis circuito;chaine montage;carrosserie	Traditional assembly systems for automotive bodies have been designed using serial configurations. Advancements in controls and other technologies allow implementation of alternative system configurations, such as parallel and hybrid configurations. These configurations offer improvements in productivity, but their performance with regard to quality, particularly dimensional variation, is not well understood. We show that trade-offs exist between quality and productivity for various configurations. The traditional serial assembly line offers the worst performance for both productivity and quality. Important factors that impact these measures are identified. Guidelines are developed to aid the selection of a system configuration that is appropriate for total system performance.		S. Jack Hu;Kathryn E. Stecke	2009	IJMR	10.1504/IJMR.2009.026575	productivity;simulation;network analysis;performance;computer science;engineering;automotive industry;artificial intelligence;operations management;assembly;configuration;engineering drawing;engineering design process;mechanical engineering	Robotics	8.74201724039434	2.8187594339398676	42748
90eee1a6ffe440726502699e6a26177a2c812a9d	a study on optimal scheduling for software projects	stochastic process models;rework modeling;optimal scheduling;optimal schedules;project scheduling;process simulation	Software projects often suffer from unexpected rework and delays. Therefore, project scheduling remains a difficult task for the managers. In this article, we compute optimal scheduling strategies for a set of sample software projects and simulate their behavior. The computations are based on a stochastic Markov decision model for software projects, which focuses on capturing the feedback between concurrent development activities. Since the underlying process model is stochastic, the strategies are stochastically optimal, that is, they minimize the expected project duration. The ultimate goal of this research is to develop guidelines for managers to schedule their software projects under uncertainty in the best possible way. The sample projects are similar, but differ in certain characteristics of the project or product, such as the strength of the coupling between the components or the degree of specialization of the teams on the tasks. By using a set of related projects, we can study how the project characteristics influence the optimal scheduling decisions in a project. After computing the optimal scheduling policies for the sample projects, we use extensive discrete-event simulations to study the behavior of the optimal policy for each given setting and compare the performance of the optimal policy against the possible list policies. List policies are a simple and commonly used class of scheduling policies. For our sample projects, the simulations show that the best list policy in general is not optimal. The higher the degree of specialization of the teams, the larger the performance gap is. On the other hand, the stronger the coupling between the components, the smaller is the improvement that the optimal policy achieves over the best list policy. Copyright  2006 John Wiley & Sons, Ltd.	computation;coupling (computer programming);john d. wiley;markov chain;partial template specialization;process modeling;rework (electronics);schedule (project management);scheduling (computing);simulation	Frank Padberg	2006	Software Process: Improvement and Practice	10.1002/spip.254	fair-share scheduling;simulation;process simulation;dynamic priority scheduling;computer science;engineering;operations management;management science;management;schedule	SE	8.000413786315574	0.7923058987106387	42776
685fc17e76d457db829d55db897e504e8d16a7de	a comprehensive survey: artificial bee colony (abc) algorithm and applications	swarm intelligence;bee swarm intelligence;artificial bee colony algorithm	Swarm intelligence (SI) is briefly defined as the collective behaviour of decentralized and self-organized swarms. The well known examples for these swarms are bird flocks, fish schools and the colony of social insects such as termites, ants and bees. In 1990s, especially two approaches based on ant colony and on fish schooling/bird flocking introduced have highly attracted the interest of researchers. Although the self-organization features are required by SI are strongly and clearly seen in honey bee colonies, unfortunately the researchers have recently started to be interested in the behaviour of these swarm systems to describe new intelligent approaches, especially from the beginning of 2000s. During a decade, several algorithms have been developed depending on different intelligent behaviours of honey bee swarms. Among those, artificial bee colony (ABC) is the one which has been most widely studied on and applied to solve the real world problems, so far. Day by day the number of researchers being interested in ABC algorithm increases rapidly. This work presents a comprehensive survey of the advances with ABC and its applications. It is hoped that this survey would be very beneficial for the researchers studying on SI, particularly ABC algorithm.	ant colony;artificial bee colony algorithm;eusociality;flocking (behavior);genetic algorithm;self-organization;swarm intelligence;whole earth 'lectronic link	Dervis Karaboga;Beyza Görkemli;Celal Ozturk;Nurhan Karaboga	2012	Artificial Intelligence Review	10.1007/s10462-012-9328-0	swarm intelligence;computer science;artificial intelligence;artificial bee colony algorithm	AI	22.206926363564758	-5.764871840429485	42797
3e3489cbc552e24fc1e5f685b38f7e79f369d07f	co-evolution of antagonistic intelligent agents using genetic algorithms	genetic algorithm;co evolution;games	Abstract   The aim of this paper is to attest the improvement on strategies of intelligent adaptive agents created using genetic algorithms in electronic games. We present an experiment on the use of genetic algorithms to create intelligent adaptive agents which iterates upon the opponent strategy. A predatory food chain was simulated, containing carnivores, herbivores and plants. This simulation uses the approach of a co-evolved asymmetric antagonistic agent population. Because they use each other as part of their environment, they are also able to learn from exhibited behavior after their evolution. Agents are expected to show a satisfactory evolution, analogous to the learning process of an intelligent being.	genetic algorithm;intelligent agent	Jhonatan da Rosa;Murillo T. de Souza;Luciana Rech;Leandro Quibem Magnabosco;Lau Cheuk Lung	2013		10.1016/j.procs.2013.05.233	simulation;artificial intelligence	AI	24.123660412424734	-12.067668339125062	42816
ac58c5c37d8a9f574471847efe898d5829d5b8f4	minimum lifecycle cost design under multiple hazards	reliability;stochastic process;discount rate;load combination;codes and standards;office building;parametric study;hazards;random process;random variable;optimal design;optimization;life span;los angeles	Design of structures against multiple hazards has become an important consideration for many important engineering facilities such as major structures and bridges. A central issue is proper consideration of the uncertainty in the demand and capacity and the balance of reliability against costs. In this study this problem is investigated based on minimization of expected lifecycle cost. The uncertainties in the loads and resistance are modeled by random processes and random variables. Costs of construction, consequences of structural limit states including damage, revenue loss, death and injury as well as discounting cost over time are considered. The importance of various design parameters is first examined by a parametric study. The method is then applied to design of a multistory office building against winds and earthquakes in Los Angeles, Seattle, and Charleston. The sensitivity of optimal design to important but uncertain design parameters such as structural life, discount rate and death and injury cost is investigated. The question of uniform reliability against different hazard is also examined. It is found that an optimization-based design is a viable approach to design against multiple hazards. The design is highly dependent on failure consequence and moderately sensitive to assumption of structural life span and discount rate. It may or may not be sensitive to death and injury cost assumption dependent on location and hazard risk characteristics. Uniform reliability against different hazards is not required. The design is often dominated, but not controlled, by the hazard that has large uncertainty and causes large consequences.		Y. K. Wen	2001	Rel. Eng. & Sys. Safety	10.1016/S0951-8320(01)00047-3	reliability engineering;random variable;stochastic process;life expectancy;hazard;engineering;optimal design;reliability;mathematics;forensic engineering;statistics	EDA	7.844905320017278	-5.822244992884257	42849
455652342384754f98e947180700220b38fcc7ca	an economic perspective of disk vs. flash media in archival storage	data retention economic perspective flash media disk media archival storage exponential drop kryders law industry projections archival flash;engineering;market research;ash media planning economic indicators data models computational modeling market research;media;computational modeling;long term preservation;disk density growth;ash;records management disc storage flash memories information retrieval systems;planning;kryder s law;disk density growth archival storage long term preservation kryder s law;modeling;cost;archival storage;data models;economic indicators	For three decades, Kryder's law correctly predicted an exponential increase in bit density on disk platters, leading to an exponential drop in cost per gigabyte, and thus to an entrenched expectation that if data could be stored for a few years the incremental cost of storing it forever would be minimal. However, disk now is over 7 times as expensive as Kryder's law would have predicted, and industry projections suggest that in 2020 the gap will reach 200 times, disrupting this expectation. Our model shows that archives based upon alternative media are surprisingly cost competitive with archives based upon traditional disk media over the long-term. We propose using Archival Flash for long-term data preservation, with the trade off between longer data retention period and lower write cycles.	archive;areal density (computer storage);case preservation;continuation;flash memory;gigabyte;hard disk drive;mark kryder;moore's law;relevance;time complexity	Preeti Gupta;Avani Wildani;Ethan L. Miller;Daniel C. Rosenthal;Ian F. Adams;Christina E. Strong;Andy Hospodor	2014	2014 IEEE 22nd International Symposium on Modelling, Analysis & Simulation of Computer and Telecommunication Systems	10.1109/MASCOTS.2014.39	market research;planning;data modeling;simulation;systems modeling;media;computer science;economic indicator;computational model	Arch	-0.41723196677159496	-9.505475145958249	42855
0e4720cb84b80276d27b160ccf0bdf59acdf66ea	estimating information amount under uncertainty: algorithmic solvability and computational complexity	cumulative distribution function;uncertainty;probabilistic uncertainty;partial information;computational complexity;amount of information;entropy;estimation error;similarity measure;interval uncertainty	Sometimes, we know the probability of different values of the estimation error ∆x def = e x− x, sometimes, we only know the interval of possible values of ∆x, sometimes, we have interval bounds on the cdf of ∆x. To compare different measuring instruments, it is desirable to know which of them brings more information – i.e., it is desirable to gauge the amount of information. For probabilistic uncertainty, this amount of information is described by Shannon’s entropy; similar measures can be developed for interval and other types of uncertainty. In this paper, we analyze the computational complexity of the problem of estimating information amount under different types of uncertainty.	computational complexity theory;entropy (information theory);shannon (unit)	Vladik Kreinovich;Gang Xiang	2010	Int. J. General Systems	10.1080/03081071003696025	econometrics;entropy;mathematical optimization;uncertainty analysis;uncertainty;cumulative distribution function;propagation of uncertainty;mathematics;computational complexity theory;sensitivity analysis;statistics;measurement uncertainty	ML	1.4984946051343555	-18.936484735206417	42869
80703c31a42198966e33b5c75d15799d3f27ab53	the computational complexity of nash equilibria in concisely represented games	circuit games;boolean circuits;nash equilibrium;concise games;nash equilibria;computational game theory;complexity class;computational complexity;graph games	Games may be represented in many different ways, and different representations of games affect the complexity of problems associated with games, such as finding a Nash equilibrium. The traditional method of representing a game is to explicitly list all the payoffs, but this incurs an exponential blowup as the number of agents grows. We study two models of concisely represented games:  circuit games , where the payoffs are computed by a given boolean circuit, and  graph games , where each agent’s payoff is a function of only the strategies played by its neighbors in a given graph. For these two models, we study the complexity of four questions: determining if a given strategy is a Nash equilibrium, finding a Nash equilibrium, determining if there exists a pure Nash equilibrium, and determining if there exists a Nash equilibrium in which the payoffs to a player meet some given guarantees. In many cases, we obtain tight results, showing that the problems are complete for various complexity classes.	computational complexity theory;nash equilibrium	Grant Schoenebeck;Salil P. Vadhan	2012	TOCT	10.1145/2189778.2189779	price of stability;markov perfect equilibrium;game theory;epsilon-equilibrium;mathematical optimization;combinatorics;traveler's dilemma;best response;sequential equilibrium;trembling hand perfect equilibrium;coordination game;computer science;folk theorem;repeated game;mathematics;correlated equilibrium;risk dominance;normal-form game;mathematical economics;subgame perfect equilibrium;equilibrium selection;symmetric game;solution concept;algorithm;nash equilibrium;symmetric equilibrium	ECom	-4.198875129925063	1.0806871802595894	42874
8dfcdde8d382e4171f9d67b74b8369173b976a77	usage of cholesky decomposition in order to decrease the nonlinear complexities of some nonlinear and diversification models and present a model in framework of mean-semivariance for portfolio performance evaluation		In order to get efficiency frontier and performance evaluation of portfolio, nonlinear models and DEA nonlinear (diversification) models are mostly used. One of the most fundamental problems of usage of nonlinear and diversification models is their computational complexity.Therefore, in this paper, a method is presented in order to decrease nonlinear complexities and simplify calculations of nonlinear and diversification models used from variance and covariance matrix. For this purpose, we use a linear transformation which is obtained from the Cholesky decomposition of covariance matrix and eliminate linear correlation among financial assets. In the following, variance is an appropriate criterion for the risk when distribution of stock returns is to be normal and symmetric as such a thing does not occur in reality. On the other hand, investors of the financial markets do not have an equal reaction to positive and negative exchanges of the stocks and show more desirability towards the positive exchanges and higher sensitivity to the negative exchanges. Therefore, we present a diversification model in the mean-semivariance framework which is based on the desirability or sensitivity of investor to positive and negative exchanges, and rate of this desirability or sensitivity can be controlled by use of a coefficient.		Hamid Siaby-Serajehlo;Mohsen Rostamy-Malkhalifeh;F. Hosseinzadeh Lotfi;Mohammad Hassan Behzadi	2016	Adv. Operations Research	10.1155/2016/7828071	financial economics;econometrics;mathematical optimization;economics;operations management	Vision	2.6187446518083464	-10.593161159495613	42885
5e87f23a4d65fcaa7c0e0aa1b56f2545c65fdeaa	evaluating flexibility on order quantity and delivery lead time for a supply chain system	modelizacion;tiempo iniciacion;usine fabrication;volume flexibility;ordered set;logistique;factory;flexibilidad;production system;systeme production;date echeance;ensemble ordonne;temps mise en route;sistema produccion;lead time;delai livraison;detaillant;modelisation;economic order quantity;setup time;logistics;flexible delivery;almacenamiento;delivery lead time flexibility;entreposage;warehousing;due date;delai d execution;plazo entrega;fabrica;fecha vencimiento;plazo ejecucion;supply chain;flexibilite;quantite economique a commander;cantidad economica pedida;supply chain flexibility;modeling;retailers;manufacturing system;simulation model;delivery lead time;time allowed;flexibility;conjunto ordenado;logistica	Most of the previous literature on production flexibility is centred on the flexibility of manufacturing systems. However, the manufacturing system is just one of several key components of a supply chain. A supply chain is a network involving all of the activities within individual organisations that link material suppliers, manufacturing factories, distributors, warehouses, retailers and customers. Research into the flexibility of a supply chain therefore extends from the intra-organisational flexibilities to the inter-organisational flexibilities. This article provides a study of examining two aspects of supply chain flexibility: order quantity flexibility and lead time flexibility, which have been clarified as the two most common changes which occur in supply chains. Order quantity flexibility refers to the ability to provide proper order quantity for customer needs. Lead time flexibility allows customers to set the order due date depending on their needs. A simulation model is built to evaluate the performance on different flexibility levels of a supply chain. The experimental results provide interesting insights and can be applied in selecting suppliers with order quantity flexibility and delivery lead time flexibility.	markov chain	Yi-Chi Wang	2008	Int. J. Systems Science	10.1080/00207720802298939	logistics;simulation;systems modeling;economic order quantity;engineering;factory;industrial engineering;simulation modeling;production system;supply chain;warehouse	Robotics	8.46233150776203	2.596609282524013	42935
0e49257a15ed2cdb46b57750cdcc69ae7b48513a	solving the single machine total weighted tardiness problem using bat-inspired algorithm	job shop scheduling;single machine scheduling optimisation search problems;neighborhood search bat algorithm single machine guided population;sociology statistics algorithm design and analysis search problems optimization job shop scheduling;statistics;optimization;search problems;guided population scheme bat inspired algorithm metaheuristic approach single machine with total weighted tardiness scheduling problem smtwt scheduling problem two swap local search or library;algorithm design and analysis;sociology	This paper aims to present an application of a recently proposed metaheuristic approach, namely bat algorithm (BA), for solving the single machine with total weighted tardiness (SMTWT) scheduling problem. In this paper, a guided population and two-swap local search are introduced to integrate with the BA. Four variants of BA methodology, including classical BA, BA with two-swap, BA with guided population, and BA with guided population and two-swap, have been developed. All BA variants are executed on the selected benchmark instances for the SMTWT problem taken from OR-library and compared search performances on deviation of solution obtained from best-known solution and computation time. The results show that two proposed BAs with a guided population scheme achieve a good performance. The BA with guided population and two-swap techniques outperforms among four proposed designs on solution quality, and it can achieve a trade-off between solution quality and execution time.	bat algorithm;benchmark (computing);business architecture;computation;local search (optimization);metaheuristic;paging;performance;run time (program lifecycle phase);scheduling (computing);time complexity	Wanatchapong Kongkaew	2015	2015 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)	10.1109/IEEM.2015.7385649	job shop scheduling;algorithm design;mathematical optimization;computer science;operations management;machine learning;mathematics;guided local search	Robotics	22.444250171499476	-0.41360791081198006	43021
11391b1dfb7d757fae55b30af09b288a965937aa	a game-theoretic analysis of a competitive diffusion process over social networks	2-player game;social network;eventual number;competitive diffusion process;nash equilibrium;game-theoretic analysis;linear threshold model;social inefficiency;general framework;diffusion process;game-theoretic model	We study a game-theoretic model for the diffusion of competing products in social networks. Particularly, we consider a simultaneous non-cooperative game between competing firms that try to target customers in a social network. This triggers a competitive diffusion process, and the goal of each firm is to maximize the eventual number of adoptions of its own product. We study issues of existence, computation and performance (social inefficiency) of pure strategy Nash equilibria in these games. We mainly focus on 2-player games, and we model the diffusion process using the known linear threshold model. Nonetheless, many of our results continue to hold under a more general framework for this process. In more detail, we first exhibit that these games do not always possess pure strategy Nash equilibria, and we prove that deciding if an equilibrium exists is co-NP-hard. We then move on to investigate conditions for the existence of equilibria. We first illustrate why we cannot hope that games over networks with special in and out-degree distributions — e.g. power law — are more stable than others, concerning for example, the form of the improvement paths, or cycles that they induce. We then study necessary and sufficient conditions for the existence of pure Nash equilibria, both for the general case but for some special cases as well. Our conditions go through the existence of generalized ordinal potential functions. We also study the existence of -generalized ordinal potentials (which yield -approximate Nash equilibria) and provide tight upper bounds on the existence of such approximations. Finally, we study the Price of Anarchy and Stability for games with an arbitrary number of players. We conclude with a discussion of the effects on the payoff of a single player (or a coalition of players) as the number of players increases. ∗National Technical University of Athens (NTUA), School of Electrical and Computer Engineering. Email: vdtzoumas@gmail.com. †Athens University of Economics and Business (AUEB), Department of Informatics. Emails: {amanatidis.c, markakis}@gmail.com.	anarchy;approximation;co-np;computation;computer engineering;directed graph;email;game theory;informatics;np-hardness;nash equilibrium;ordinal data;social network;threshold model	Vasileios Tzoumas;Christos Amanatidis;Evangelos Markakis	2012		10.1007/978-3-642-35311-6_1	industrial organization;economics;microeconomics;welfare economics	ECom	-4.400425934703637	0.16547343047699256	43035
e34575af6acc9d53a90c026575b5f3b317ca4625	a comparison of monte carlo tree search and rolling horizon optimization for large-scale dynamic resource allocation problems		Dynamic resource allocation (DRA) problems constitute an important class of dynamic stochastic optimization problems that arise in a variety of important real-world applications. DRA problems are notoriously difficult to solve to optimality since they frequently combine stochastic elements with intractably large state and action spaces. Although the artificial intelligence and operations research communities have independently proposed two successful frameworks for solving dynamic stochastic optimization problems—Monte Carlo tree search (MCTS) and rolling horizon optimization (RHO), respectively—the relative merits of these two approaches are not well understood. In this paper, we adapt both MCTS and RHO to two problems – a problem inspired by tactical wildfire management and a classical problem involving the control of queueing networks – and undertake an extensive computational study comparing the two methods on large scale instances of both problems in terms of both the state and the action spaces. We show that both methods are able to greatly improve on a baseline, problem-specific heuristic. On smaller instances, the MCTS and RHO approaches perform comparably, but the RHO approach outperforms MCTS as the size of the problem increases for a fixed computational budget.	artificial intelligence;baseline (configuration management);branching factor;combinatorial optimization;computation;computational complexity theory;dynamic resolution adaptation;floyd–warshall algorithm;heuristic;interdependence;job stream;mathematical optimization;monte carlo method;monte carlo tree search;numerical analysis;operations research;progressive meshes;progressive scan;queueing theory;regular expression;state space;stochastic optimization	Dimitris Bertsimas;J. Daniel Griffith;Vishal Gupta;Mykel J. Kochenderfer;Velibor V. Misic	2017	European Journal of Operational Research	10.1016/j.ejor.2017.05.032	mathematical optimization;simulation;operations management;monte carlo method	AI	20.241252871755975	3.4231747049869377	43115
46cb072ad5456ce1d6f33ec4a3f892de7a065ef8	loyalty programs in the sharing economy: optimality and competition		Loyalty programs are important tools for sharing platforms seeking to grow supply. Online sharing platforms use loyalty programs to heavily subsidize resource providers, encouraging participation and boosting supply. As the sharing economy has evolved and competition has increased, the design of loyalty programs has begun to play a crucial role in the pursuit of maximal revenue. In this paper, we first characterize the optimal loyalty program for a platform with homogeneous users. We then show that optimal revenue in a heterogeneous market can be achieved by a class of multi-threshold loyalty program (MTLP) which admits a simple implementation-friendly structure. We also study the performance of loyalty programs in a setting with two competing sharing platforms, showing that the degree of heterogeneity is a crucial factor for both loyalty programs and pricing strategies. Our results show that sophisticated loyalty programs that reward suppliers via stepwise linear functions outperform simple sign-up bonuses, which give them a one time reward for participating.	linear function;maximal set;sharing economy;stepwise regression	Zhixuan Fang;Longbo Huang;Adam Wierman	2018		10.1145/3209582.3209596	homogeneous;loyalty program;marketing;subsidy;distributed computing;computer science;pricing strategies;loyalty;revenue;social network;sharing economy	ECom	-2.369825168663736	-3.632495205967394	43196
ed9e50750f1a18cdfa53c37e731dfc5fed28cc11	group search optimizer with intraspecific competition and lévy walk	intraspecific competition;group search optimizer;evolutionary algorithm;standard benchmark functions;levy walk	This paper presents the mean–variance (MV) model to solve power system reactive power dispatch problems with wind power integrated. The MV model considers the profit and risk simultaneously under the uncertain wind power (speed) environment. To describe this uncertain environment, the Latin hypercube sampling with Cholesky decomposition simulation method is used to sample uncertain wind speeds. An improved optimization algorithm, group search optimizer with intraspecific competition and lévy walk, is then used to optimize the MV model by introducing the risk tolerance parameter. The simulation is conducted based on the IEEE 30-bus power system, and the results demonstrate the effectiveness and validity of the proposed model and the optimization algorithm.	algorithm;cholesky decomposition;data general eclipse mv/8000;dynamic dispatch;lévy flight;mathematical optimization;particle swarm optimization;risk aversion;sampling (signal processing);simulation;star catalogue	Y. Z. Li;Q. Henry Wu;Mengshi Li	2015	Knowl.-Based Syst.	10.1016/j.knosys.2014.09.005	mathematical optimization;simulation;lévy flight;computer science;evolutionary algorithm;intraspecific competition	DB	23.984986025382014	-5.1010573499525345	43219
d1f0e7787b93bc3095a44390c01fb3d1054e32b5	a study of maintenance policies for second-hand products	preventive maintenance;second hand product;maintenance cost;weibull;environmental protection;minimal repair;failure rate	This paper proposes two maintenance schemes for reused products. In order to reduce the purchasing cost or conform to the concept of environmental protection, reuse is considered as one of the most efficient strategies. However, the initial quality of reused product is often inferior to the new one and then product failures will occur frequently during usage period. Therefore, two periodical preventive maintenance (PM) policies in this paper are developed to decrease the high failure rate of the second-hand products. When a second-hand product with Weibull life time distribution of known age is intended to be used for a pre-specified period of time, the optimal number of PM actions and the corresponding maintenance degree are derived such that the expected total maintenance cost is minimized. The impact of providing preventive maintenance is illustrated through numerical examples.		Ruey Huei Yeh;Hui-Chiung Lo;Rouh-Yun Yu	2011	Computers & Industrial Engineering	10.1016/j.cie.2010.07.033	reliability engineering;planned maintenance;weibull distribution;preventive maintenance;engineering;operations management;failure rate;forensic engineering	DB	6.880565612500921	-1.5819424732880474	43224
7b12ef71a66930eb032417a9456d9290be593edb	the middleman as a panacea for supply chain coordination problems	simple contracts;hold up;multilateral investments;middlemen	The prevalence of intermediaries (middlemen) in supply chains is often seen as a dying remnant of less efficient times. Despite predictions that supply chains will rapidly “cut out the middleman” as technological advances have eased logistics, middlemen have continued to thrive. In this paper, we demonstrate a transaction role of middlemen that may help clarify their staying power. In a model with self-interested decision-making by both a manufacturer and a retailer, wherein incentive misalignment creates investment and production inefficiencies, we show that the integrated (first-best) outcome can be achieved with simple cost-based contracts if and only if a middleman is present. We further show that the approach of utilizing a middleman to fully coordinate the supply chain is robust in that it can be applied to a variety of circumstances discussed in the literature, including multilateral investment/effort choices, multiple product providers, and logistical investments made by the middleman.		Anil Arya;Clemens Löffler;Brian Mittendorf;Thomas Pfeiffer	2015	European Journal of Operational Research	10.1016/j.ejor.2014.07.007	economics;marketing;operations management;commerce;labour economics	Theory	-1.9612455678812948	-6.356028625604317	43340
21145cf009e9ddc03d510b176187bdf326ba9994	a framework of oligopolistic market simulation with coevolutionary computation	modelo dinamico;economie;modelizacion;economia;continuous function;multiagent system;markets;mercado;economic sciences;dynamic model;piece wise affine;dynamic system;fonction continue;intelligence artificielle;agent logiciel;coevolution;calcul analogique;equilibrium problem;dynamical system;software agents;modelisation;systeme dynamique;ciencias economicas;funcion continua;agent based computational economic;modele dynamique;marche;artificial intelligence;sciences economiques;economy;inteligencia artificial;sistema dinamico;electricity industry;sistema multiagente;modeling;supply function equilibrium;coevolucion;systeme multiagent;analog calculus;calculo analogico	The paper presents a new framework of oligopolistic market simulation based on coevolutionary computation. The coevolutionary compu-tation architecture can be regarded as a special model of the agent-based computational economics (ACE), which is a computational study of economies modeled as dynamic systems of interacting agents. The supply function equilibrium (SFE) model of an oligopolistic market is used in simulation. The piece-wise affine and continuous supply functions which have a large number of pieces are used to numerically estimate the equilibrium supply functions of any shapes. An example based on the cost data from the real-world electricity industry is used to validate the approach presented in this paper. Simulation results show that the coevolutionary approach robustly converges to SFE in different cases. The approach is robust and flexible and has the potential to be used to solve the complicated equilibrium problems in real-world oligopolistic markets.		Haoyong Chen;Xifan Wang;Kit Po Wong;Chi-yung Chung	2006		10.1007/11881070_114	simulation;computer science;artificial intelligence;dynamical system	ECom	5.280782451851832	-9.38258159207067	43369
39eea4043105446144699863287a61d9fb7c46d8	long-term forest ecosystem planning at pacific lumber	industries;multiple criteria;industries lumber wood;forest ecosystem;programming linear;linear;programming;lumber wood	In 1995, the Pacific Lumber Company contracted with VESTRA Resources to develop a 120-year, 12-period forest-eco system management plan for its properties to meet new California Board of Forestry wildlife, fisheries, and timber resource requirements and to optimize its timberland operations and profitability. VESTRA Resources developed the ecosystem planning express model, Ep(x), which seamlessly integrates geographic information systems with a database resource capability model and a policy-alternative model, providing the inputs to a linear program. The Ep(x) model uses an underlying adaptive management approach that also maximizes the knowledge base of the ecosystem through detailed sensitivity analyses. The model increased present net worth by over $398 million and generated a more optimal mix of wildlife habitat acres, including spotted-owl-nesting habitat.		L. Russell Fletcher;Henry Alden;Scott P. Holmen;Dean P. Angelides;Matthew J. Etzenhouser	1999	Interfaces	10.1287/inte.29.1.90	programming;economics;engineering;pulp and paper industry;operations management;linearity;management;forest ecology	HCI	9.219344743005621	-4.6313168203651385	43483
