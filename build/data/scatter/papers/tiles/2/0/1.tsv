id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
417dc2458db55058965fb3c10e96e38334e336ba	scalable streaming of jpeg 2000 live video using rtp over udp	video streaming;out of order;video server;transport protocol;user datagram protocol	This paper describes a scalable architecture for streaming JPEG2000 live video streams using Real Transport Protocol (RTP) over User Datagram Protocol (UDP). UDP makes the transmission faster and more efficient, due that it avoids the overhead of checking whether one packet has arrived or not. Moreover UDP is compatible with packet broadcast (sending to all on local network) and multicasting (send to all subscribers). However, with UDP the data- grams may arrive out of order, appear duplicated, or go missing without notice. With the aim of solve this problem we propose the use of RTP with a new definition of one RTP payload header (RTP Payload Format for JPEG 2000 Video Streams) that it is being defined by the IETF. This payload and its use with RTP will become a new RFC standard. Our solution, thanks in part to JPEG2000 features, is scalable. The JPEG2000 live video server developed supports multiple clients and each one can display live video at a variety of resolutions and quality levels. Thus client devices with different capabilities, variety of screen resolutions, can all achieve a scalable viewing of the same content.		A. Luis;Miguel A. Patricio	2008		10.1007/978-3-540-85863-8_68	user datagram protocol;rtp control protocol;computer science;out-of-order execution;operating system;internet privacy;internet control message protocol;world wide web;udp flood attack;transport layer;computer network	Vision	-9.478294573460595	96.84136025936522	1010
08d52b6fce562fed9a6c45ca3e11cc10585937bb	the implementation of a multibackend database system (mdbs): part ii - the design of a prototype mdbs	database system		prototype	Xin-Gui He;Masanobu Higashida;Douglas S. Kerr;Ali Orooji;Zhongzhi Shi;Paula Strawser;David K. Hsiao	1983			database;computer science	DB	-31.97671098093809	9.981847729293344	1013
f1fccc07ef98c6021402f25de1a33beaf49293c7	aligning technology and business: applying patterns for legacy transformation	application development;software requirements;project work;software life cycle;legacy system	H. M. Hess Two key goals for aligning technology and business are to increase an organization’s ability to change rapidly and to reduce the costs of technology. While many efforts are underway to improve application development, less emphasis has been placed on addressing key challenges posed by existing applications that resist rapid change. In this paper, we discuss techniques for accelerating change to legacy systems and for streamlining an application portfolio. Our approach takes business-driven application requirements, links them to analysis of an application portfolio, and identifies potential sequences of transformations to realize the targeted improvements. This paper describes our approach for mapping business requirements to application software, for using patterns to help translate business requirements to software requirements, and for using patterns to translate software requirements into potential solution designs. The paper describes how these techniques are applied to two stages of the software life cycle—initial analysis and detailed analysis—and summarizes experience gained from projects working with IBM clients.	adobe streamline;align (company);business process;business requirements;code refactoring;executable;flowchart;interoperability;legacy system;norm (social);requirement;responsiveness;scalability;software modernization;software release life cycle;software requirements;strategic management	Howard M. Hess	2005	IBM Systems Journal	10.1147/sj.441.0025	requirements analysis;software requirements specification;requirement prioritization;system requirements;business requirements;computer science;systems engineering;engineering;software design;software development;requirement;software engineering;software construction;database;rapid application development;management;legacy system;software development process;software requirements	SE	-62.085945382401555	22.758596510870603	1015
7a5e50a954976c7d7f689322f0b5fe26c42ff357	adaptive task replication strategy for human computation	inter rater agreement;human computer interaction distributed processing;task redundancy;task replication;image resolution;image resolution redundancy computational modeling internet adaptive systems program processors;task difficulty;computational modeling;internet;redundancy;adaptive systems;human computation;inter rater agreement human computation task redundancy task replication task difficulty;quality level adaptive task replication strategy human computation systems distributed systems task replication level human computation applications worker credibility;program processors	Human computation systems are distributed systems in which the processors are human beings, called workers. In such systems, task replication has been used as a way to obtain results redundancy and quality. The level of replication is usually defined before the tasks start executing. This approach, however, generates the problem of defining the suitable task replication level. If the level of replication is overestimated, it is used an excessive amount of workers and, therefore, there is an increase in the cost of executing all tasks. On the other hand, if the level of replication is underestimated, a desired level of quality cannot be achieved. This work proposes an adaptive replication strategy that defines the level of replication for each task during execution time. The strategy is based on estimations of the degree of difficulty of tasks and the degree of credibility of workers. Results from simulations using data from two real human computation applications show that, compared to non-adaptive task replication, the proposed strategy reduces the number of replicas substantially, without compromising the accuracy of the obtained answers.	central processing unit;distributed computing;human-based computation;replication (computing);run time (program lifecycle phase);simulation	Lesandro Ponciano;Francisco Vilar Brasileiro;Guilherme Gadelha;Adabriand Furtado	2014	2014 Brazilian Symposium on Computer Networks and Distributed Systems	10.1109/SBRC.2014.30	real-time computing;computer science;theoretical computer science;distributed computing	OS	-15.611660394078354	61.06328848777018	1017
f68f44ff4522baea507657c4cb260d030af3fcf4	independent update reflections on interdependent database views	datorsystem;computer systems	The problem of identifying suitable view-update strategies is typically addressed in the context of a single view. However, it is often the case that several views must co-exist; the challenge is then to find strategies which allow one view to be updated without affecting the other. The classical constant-complement strategy can provide a solution to this problem; however, both the context and the admissible updates are quite limited. In this work, the updates which are possible within this classical approach are extended substantially via a technique which considers only the states which are reachable from a given initial configuration. The results furthermore do not depend upon complementation, and thus are readily extensible to settings involving more than two views.	interdependence;reflection (computer graphics);view (sql)	Stephen J. Hegner	2012		10.1007/978-3-642-28279-9_9	computer science;theoretical computer science;data mining;database	AI	-27.12889952018664	34.959325566110756	1025
6a51f2a35e28a49f18b4f9b335e7b2fc0e50be76	static program analysis of large embedded code base: an experience	embedded coding;causal analysis;software quality assurance;large code base;software systems;embedded system;lines of code;precision;embedded application;indexation;static analysis tools;static analysis;static program analysis;domain specificity;defect detection	Static program analysis is widely used in property checking of software systems, especially safety and mission critical embedded systems. Most of these efforts check for violation of only standard properties such as array index out of bound, overflow/underflow and so on. However, our studies have shown that checking for these standard properties only captures less than 10% of all the defects detectable through static analysis. The remaining defects can be detected by checking for domain specific (custom) properties. We have applied two static analysis tools (TCS Embedded Code Analyzer and Saturn), varying in their analysis techniques, over a large embedded code base to check for a particular custom property. The code base consisted of 10 million lines of code (LOC) and belonged to the automotive domain. The custom property (semaphore consistency) to be verified was chosen after a detailed causal analysis of the history of various defects encountered in the code base. Here, we present our experience with this effort -- key problems encountered, solutions provided and results obtained. Our experience shows that static analysis of very large code bases is practically feasible and is a value-add in software quality assurance.		Shubhangi Khare;Sandeep Saraswat;Shrawan Kumar	2011		10.1145/1953355.1953368	kpi-driven code analysis;dead code;embedded system;real-time computing;computer science;theoretical computer science;operating system;software engineering;programming language;computer security;algorithm;static program analysis;unreachable code	SE	-58.216265695385594	38.86624356984172	1027
beb64bc7267d4cc3a8bc7731b6a9a77c17b5ccd5	an improved real-quadratic-field-based key exchange procedure	protocolo acceso;llave cambio;real quadratic field;securite;nucomp;cryptographic key exchange;echange cle;access protocol;function fields;key exchange;criptografia;cryptography;safety;cryptographie;reduced principal ideal;protocole acces;seguridad	To date, the only non-group structure that has been suitably employed as the key space for Diffie-Hellman-type cryptographic key exchange is the infrastructure of a real quadratic (number or function) field. We present an implementation of a Diffie-Hellman-type protocol based on real quadratic number field arithmetic that provides a significant improvement in performance over previous versions of this scheme. This dramatic speed-up is achieved by replacing the ordinary multiplication and reduction procedures for reduced ideals by a new version of the NUCOMP algorithm due to Shanks.	algorithm;cryptography;diffie–hellman key exchange;key (cryptography);key space (cryptography)	Michael J. Jacobson;Renate Scheidler;Hugh C. Williams	2005	Journal of Cryptology	10.1007/s00145-005-0357-6	key exchange;telecommunications;computer science;cryptography;mathematics;computer security;algorithm;statistics	Crypto	-40.343737275186385	79.2272301686731	1031
d146de1c3906312a58cb82b6a98b4a2622af63da	boosting degraded reads in heterogeneous erasure-coded storage systems	hadoop cluster degraded read boosting heterogeneous erasure coded storage systems distributed storage systems large scale data storage services frequent node failures data redundancy data replication erasure coding large scale storage systems fastdr data transfer;i o parallelism;decoding;encoding digital storage electronic data interchange;node heterogeneity;i o parallelism erasure coded storage system degraded reads node heterogeneity;redundancy;degraded reads;encoding decoding parallel processing optimization equations redundancy bandwidth;erasure coded storage system;bandwidth;optimization;encoding;parallel processing	Distributed storage systems provide large-scale data storage services, yet they are confronted with frequent node failures. To ensure data availability, a storage system often introduces data redundancy via replication or erasure coding. As erasure coding incurs significantly less redundancy overhead than replication under the same fault tolerance, it has been increasingly adopted in large-scale storage systems. In erasure-coded storage systems, degraded reads to temporarily unavailable data are very common, and hence boosting the performance of degraded reads becomes important. One challenge is that storage nodes tend to be heterogeneous with different storage capacities and I/O bandwidths. To this end, we propose FastDR, a system that addresses node heterogeneity and exploits I/O parallelism, so as to boost the performance of degraded reads to temporarily unavailable data. FastDR incorporates a greedy algorithm that seeks to reduce the data transfer cost of reading surviving data for degraded reads, while allowing the search of the efficient degraded read solution to be completed in a timely manner. We implement a FastDR prototype, and conduct extensive evaluation through simulation studies as well as testbed experiments on a Hadoop cluster with 10 storage nodes. We demonstrate that our FastDR achieves efficient degraded reads compared to existing approaches.	apache hadoop;computer data storage;data redundancy;erasure code;experiment;fault tolerance;greedy algorithm;input/output;overhead (computing);parallel computing;prototype;replication (computing);simulation;testbed	Yunfeng Zhu;Jian Lin;Patrick P. C. Lee;Yinlong Xu	2015	IEEE Transactions on Computers	10.1109/TC.2014.2360543	parallel processing;parallel computing;real-time computing;computer science;database;redundancy;bandwidth;encoding;statistics	OS	-15.69662589319314	53.861961435678054	1039
dad07a99c8ffcbdfac352b52dedbe7bd284e38a6	lost in translation: leistungsfähigkeit, einsatz und grenzen von emulatoren bei der langzeitbewahrung digitaler multimedialer objekte am beispiel von computerspielen				Jens-Martin Loebel	2013				SE	-101.38779884501356	27.024251715540426	1041
f6a7bcb88b06216eb21692e5442df0d95d751142	empirical software engineering research roadmap introduction	empirical software engineering	The gathering of leading Empirical Software Engineering researchers at Dagstuhl provides a unique opportunity to capture the current challenges facing the field. Our gathering enables deep discussions that identify critical issues, discuss promising opportunities, and outline future directions. A typical framework for organizing ideas and plans from thought leaders is the definition of a roadmap for a field, and the researchers gathered at Dagstuhl have agreed to define a roadmap for Empirical Software Engineering research. The following sections describe what we mean by a roadmap and elaborate on the roadmap process, uses, benefits, types, and structure. We then introduce an example skeletal roadmap for Empirical Software Engineering that serves as a starting point for Dagstuhl working group discussions. In the subsequent sections, we document the working group’s outcomes and describe the resulting overall roadmap in a final summary.	experimental software engineering;organizing (structure)	Richard W. Selby	2006		10.1007/978-3-540-71301-2_49	personal software process;verification and validation;systems engineering;social software engineering;software development;feature-oriented domain analysis;software engineering;software construction;empirical process;software analytics;software requirements;computer engineering;software peer review	SE	-66.02758366078773	23.880715276025395	1043
62368bbc4b0574a2544688424721d7b14a32aed7	on propositional form		For Aristotle the proper domain of logic was not just sentences in general, but propositions. Propositions are sentences have or falsity in them. Aristotle believed that all propositions could be viewed as having, logically, one of the four basic categorical forms. In other words, all propositions are logically subject-predicate in form. If he thought anything about other forms of propositions (viz., those more complex forms built up from combinations of categoricals) we have little evidence of it from his extant writings. Contemporary mathematical logicians recognize both simple subject-predicate propositions and propositional truth functions. A theory of the logical form of simple subject-predicate propositions can be gleaned from mathematical as well as Aristotelian logic. I think the Aristotelian view is closer to the truth. This essay has four parts.* In the first part we examine Aristotleu0027s theory of the logical form of a categorical. We then look at the notion of the form of a simple subject-predicate proposition as found in the contemporary logicianu0027s calculus of (one-place) predicates. In Section 3 a		George Englebretsen	1980	Notre Dame Journal of Formal Logic	10.1305/ndjfl/1093882942	absorption;boolean function	Logic	-11.680538516199467	5.88920381063324	1045
47ca60df3b9f08cff936a83fbe0675b6f46fb532	locating and correcting software faults in executable code slices via evolutionary mutation testing		Software testing is an important phase of software development that helps eliminating the possibility of project failure. As software systems get more complicated and larger in size, testing needs to constantly evolve and provide more ''sophisticated'' techniques, like automatic, self-adaptive muta- tion testing, targeting at improving the efficiency and effectiveness of the testing phase by handling the increased complexity that leads to increased demands in time and effort. Mutation testing is the procedure of applying a series of operators on correctly functioning programs so as to induce ''faults'' that correspond to real, common programming errors and then assess the ability of a set of test cases to reveal those errors. We introduce a novel approach for identifying and correcting faults in Java source code with the use of code slicing, mutation testing and Genetic Algorithms. Three different categories of experiments are used to assess the effectiveness of the proposed solution, demonstrating its applicability on a variety of programs and type of errors. The results are quite encouraging suggesting that the approach is able to dynami- cally detect faults and propose the appropriate corrections.	mutation testing	Pantelis Stylianos Yiasemis;Andreas S. Andreou	2012		10.1007/978-3-642-40654-6_13	development testing;regression testing;real-time computing;orthogonal array testing;white-box testing;computer science;operating system;machine learning;data mining;database;software testing;algorithm	SE	-59.69668350421353	35.98758919166927	1048
7eedafb847764bf69365f65e4464ffbc850ee415	hard real-time garbage collection im modern object oriented programming languages			garbage collection (computer science);programming language;real-time cmix	Fridtjof Siebert	2002				PL	-23.43168309656639	33.848801445235075	1054
3b2ba232d45c563d93a47942f6c19a45877a3bc2	limits with signed digit streams		We work with the signed digit representation of abstract real numbers, which roughly is the binary representation enriched by the additional digit -1. The main objective of this paper is an algorithm which takes a sequence of signed digit representations of reals and returns the signed digit representation of their limit, if the sequence converges. As a first application we use this algorithm together with Heron’s method to build up an algorithm which converts the signed digit representation of a non-negative real number into the signed digit representation of its square root. Instead of writing the algorithms first and proving their correctness afterwards, we work the other way round, in the tradition of program extraction from proofs. In fact we first give constructive proofs, and from these proofs we then compute the extracted terms, which is the desired algorithm. The correctness of the extracted term follows directly by the Soundness Theorem of program extraction. In order to get the extracted term from some proofs which are often quite long, we use the proof assistant Minlog. However, to apply the extracted terms, the programming language Haskell is useful. Therefore after each proof we show a notation of the extracted term, which can be easily rewritten as a definition in Haskell.	algorithm;binary number;correctness (computer science);haskell;heron;programming language;proof assistant	Franziskus Wiesnet	2018	CoRR		discrete mathematics;streams;algorithm;mathematics;signed-digit representation;haskell;correctness;mathematical proof;binary number;numerical digit;proof assistant	PL	-14.337219645999246	20.636156938940243	1055
7c0c2200aca5d64c02032c343b555517ea89ee7f	gamer als designer		Im Bereich der Computerspiele und virtuellen Welten wurden in den vergangenen Jahren komplexe Strategien und Werkzeuge entwickelt, welche in beispiellosem Umfang die Integration der Nutzer in den Kreativprozess und die Wertschöpfungskette ermöglichen. Durch Zusammenarbeit mit der Gemeinschaft der ambitionierten Amateure lassen sich die Kosten und Risiken innovativer Produktentwicklung reduzieren und zudem eine besonders starke Identifikation der Nutzer mit dem gemeinsam gestalteten, sich stets dynamisch verändernden Produkt erzeugen. Ausschlaggebend für die Ausschöpfung des innovativen Potenzials ist der Grad der Freiheit, den die Game-Editoren gewähren. Dieser variiert von Spiel zu Spiel erheblich und reicht von der rein oberflächlichen Dekoration der Spielwelt bis hin zur weitreichenden Umgestaltung der Spiele. Proportional zur gestalterischen Freiheit wächst allerdings die Gefahr von Zielkonflikten zwischen den verschiedenen Parteien. Die Art und Weise, wie die Entwickler diesen Zielkonflikten begegnen und die Produktion von User-generated Content koordinieren, ist für Bereiche der Produktentwicklung von Interesse, die weit über die Sphäre des Computerspiels hinausweisen.	eine and zwei;internet explorer;the grid analysis and display system (grads);user-generated content	Florian Alexander Schmidt;Danny Pannicke	2010	Informatik-Spektrum	10.1007/s00287-010-0477-8	world wide web;computer science;performance art	OS	-106.71829959803937	33.94992993614296	1056
77ca16483818bc683d4f45b6d0127520eb751e3a	anomaly detection and mitigation for disaster area networks	computer engineering;and forward;technology;anomaly detection;wireless network;computer and information science;statistical method;teknikvetenskap;natural sciences;common operating picture;intermittent connectivity;random walk;disaster area networks;mobile ad hoc network;datorteknik;security	One of the most challenging applications of wireless networking are in disaster area networks where lack of infrastructure, limited energy resources, need for common operational picture and thereby reliable dissemination are prevalent. In this paper we address anomaly detection in intermittently connected mobile ad hoc networks in which there is little or no knowledge about the actors on the scene, and opportunistic contacts together with a store-and-forward mechanism are used to overcome temporary partitions. The approach uses a statistical method for detecting anomalies when running a manycast protocol for dissemination of important messages to k receivers. Simulation of the random walk gossip (RWG) protocol combined with detection and mitigation mechanisms is used to illustrate that resilience can be built into a network in a fully distributed and attack-agnostic manner, at a modest cost in terms of drop in delivery ratio and additional transmissions. The approach is evaluated with attacks by adversaries that behave in a similar manner to fair nodes when invoking protocol actions.	adversary (cryptography);algorithm;anomaly detection;antivirus software;central processing unit;computer science;constraint (mathematics);hoc (programming language);holism;network performance;recursion;sensor;simulation;store and forward;threat model	Jordi Cucurull-Juan;Mikael Asplund;Simin Nadjm-Tehrani	2010		10.1007/978-3-642-15512-3_18	anomaly detection;mobile ad hoc network;telecommunications;computer science;wireless network;computer security;random walk;computer network;technology	Mobile	-55.50674450660149	75.0660937205554	1058
ec4e6eac367fd0ee5e360dc3895c34ae489a4354	bug localization in test-driven development	test-driven development practice;software development team;tdd cycle;incremental work cycle;iterative development step;tdd benefit;agile methodology;bugs activation;java program;bug localization technique	Software development teams that use agile methodologies are increasingly adopting the test-driven development practice (TDD). TDD allows to produce software by iterative and incremental work cycle, and with a strict control over the process, favouring an early detection of bugs. However, when applied to large and complex systems, TDD benefits are not so obvious: manually locating and fixing bugs introduced during the iterative development steps is a non-trivial task. In such systems, the propagation chains following the bugs activation can be unacceptably long and intricate, and the size of the code to be analyzed is often too large. In this paper, a bug localization technique specifically tailored to TDD is presented. The technique is embedded in the TDD cycle and it aims to improve developers’ ability to locate bugs as soon as possible. It is implemented in a tool, and experimentally evaluated on newly developed Java programs.	agile software development;complex systems;embedded system;experiment;iteration;iterative and incremental development;iterative method;java;markov chain;software bug;software propagation;test-driven development	Massimo Ficco;Roberto Pietrantuono;Stefano Russo	2011	Adv. Software Engineering	10.1155/2011/492757	real-time computing;simulation;engineering;distributed computing	SE	-61.996775105894514	37.0670747736763	1066
48dc34f27fa5adbdc71993dec67d2a9c1f01b3cf	creation and evaluation of formal specifications for system-of-systems development	formal specification;system of systems;run time execution monitoring formal specification temporal assertion prototyping;missiles;lightweight formal methods;formal verification;model checking;systems analysis;system design;safety critical software;armor plating formal specifications system of systems development formal methods requirements specification safety critical system mission essential system requirements runtime model checking system designs;requirement specification;article;formal specifications runtime monitoring prototypes computer science object oriented modeling automatic testing timing unified modeling language natural languages;missiles formal specification safety critical software formal verification systems analysis	Studies have suggested that formal specifications and lightweight formal methods help improve the clarity and precision of the requirements specification. This paper describes a process to augment the current informal approaches to system-of-systems development by introducing temporal assertions to capture the safety-critical and mission-essential system requirements and runtime model checking to evaluate the system designs and implementation. The process allows users to develop and validate temporal assertions iteratively via simulation with multiple scenarios, and to use the assertions to automate the testing of the system-of-systems under development as well as armor-plating the target system against any unexpected behaviors at runtime.	formal methods;formal specification;model checking;plating;requirement;run time (program lifecycle phase);simulation;software development process;software requirements specification;system of systems;system requirements	Doron Drusinsky;Man-tak Shing	2005	2005 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2005.1571418	model checking;systems analysis;formal methods;specification language;system of systems;formal verification;computer science;system requirements specification;formal specification;formal equivalence checking;refinement;runtime verification;programming language;language of temporal ordering specification;systems design	SE	-45.00488567437327	32.200624764433115	1071
b411d45c97e3f50d74c516a500f05a685127f3ef	pase: a prototype for ad-hoc process-aware information system declaratively constructing environment	control systems;workflow management;logic calculus;information systems;prototypes;logic;information systems business cognition process control space exploration xml prototypes;space exploration;process aware information systems;management information systems ad hoc networks;information systems ad hoc process aware information system workflow management technology knowledge intensive domain user interactions logic reasoning logic calculus graphical tool;ad hoc process aware information system;logic reasoning;user interactions;information management;business;cognition;process control;workflow management technology;xml;knowledge intensive domain;management information systems;ad hoc networks;space technology;graphical tool;information system;user interaction;real time application;business process;medical diagnostic imaging;medical control systems	Traditional workflow management technology can deal with pre-defined business processes well. However, a practical ad-hoc process case in knowledge-intensive domain is often driven by not only process control, but also the real-time application data or user interactions. So the structure of those processes are hard to be completely fixed ahead. In addition, as the only basic element in traditional process, task, is often too coarse or too fine in real applications. Even though the process structure could be pre-defined completely, there is only a non-declarative way to build the process by traditional methods. To solve the above problems, we developed a declaratively constructing environment, named PASE. We propose a multi-layer model to percept ad-hoc process. And we adopt a transformation strategy to translate the model to process specifications based on logic reasoning and calculus. We also provide a graphical tool to help user construct information systems declaratively.	business process;declarative programming;hoc (programming language);information system;interaction;layer (electronics);prototype;real-time clock;real-time computing	Bin Zhou;Hongyan Li;Lei Wang;Huaqiang Zhang;Meimei Li	2008	2008 The Ninth International Conference on Web-Age Information Management	10.1109/WAIM.2008.40	computer science;knowledge management;artificial intelligence;operating system;machine learning;process control;data mining;database;information management;world wide web;information system	DB	-38.256538247836936	9.604648455221119	1083
acb49cccd9ad502d1f08cdd877bd9709f9804e82	improving performance and reliability of an ims network by co-locating ims servers		The IP Multimedia Subsystem (IMS) is an architecture for supporting multimedia services via a Session Initiation Protocol (SIP) infrastructure. IMS, specified by the 3rd Generation Partnership Project (3GPP), is increasingly adopted as the reference multimedia network architecture by providers of wireline and wireless infrastructure and associated multimedia application services. One of the characteristics of the IMS architecture is that many different and mandatory SIP proxies are used for each end-to-end SIP session. This may adversely impact the overall IMS call processing performance, which is already affected by the universal usage of SIP. This also makes each end-to-end SIP session dependent on the good functioning of many hosts, which decreases reliability while increasing significantly the end-to-end message latency. This paper analyzes the performance and quantifies the benefits of having the SIP servers running on the same host. It then explains which types of SIP calls in the IMS network can benefit from the co-location of IMS servers and shows how to design the IMS network to maximize IMS server co-location. Finally, it introduces Lucent Technologies' SIPia BUS software architecture, which implements the co-location capability by introducing a new SIP transport in a way that is both natural and non-disruptive for the SIP stack and the SIP user agent.	application server;central processing unit;end-to-end encryption;end-to-end principle;ip multimedia subsystem;network architecture;server (computing);software architecture;user agent	Thierry Bessis	2006	Bell Labs Technical Journal	10.1002/bltj.20132	embedded system;sip trunking;computer science;operating system;world wide web;computer network	Networks	-14.735255051612306	92.95536648421863	1089
1feb5515257c8c9184e72a498e0a065be9ca2a30	architectures and languages for model building and reuse: organization and selection of reconfigurable models	object oriented programming;model building;simulation model	This paper introduces the concept of reconfigurable simulation models and describes how these models can be used to support simulation-based design. As in object-oriented programming, a reconfigurable model consists of a separate interface and multiple implementations. An AND-OR tree represents which implementations can be bound to each interface. From the resulting model space, a designer can quickly select the simulation model that is most appropriate for the current design stage. We conclude the paper with an example that illustrates the XML-based implementation of reconfigurable models.	and–or tree;reconfigurable computing;simulation;xml	Antonio Diaz-Calderon;Christiaan J. J. Paredis;Pradeep K. Khosla	2000			computer architecture;real-time computing;simulation;model building;computer science;theoretical computer science;simulation modeling;object-oriented programming	HCI	-35.17513998334724	27.08387323846643	1095
9ff020b0a7dcb819e9ebfcbab295c889bcf4962b	behavior-based worm detection and signature generation	protocols;lcseq hsg algorithm;invasive software digital signatures;signature generation;detection algorithms;sensors;behavior based detection;distributed detection;real time;digital signatures;behavior based worm detection;worm detection;network traffic behavior based worm detection signature generation;accuracy;network traffic;grippers;detection algorithm;detection rate;lcseq hsg algorithm worm detection behavior based detection signature generation;invasive software;payloads;ip networks;high efficiency;early detection;communication pattern;computer worms intrusion detection computer networks telecommunication traffic payloads character generation information science detection algorithms traffic control protocols	High efficient and real-time characteristic of the signature-based approach guarantee the early detection of most known worms; while behavior-based approach searches for communication pattern of worms in accordance with their behavioral characteristics that are different from normal network traffic. To improve the detection rate and accuracy, two detection algorithms for diffuse type communication pattern and chain communication pattern and distributed detection architecture are proposed. Through analysis on detection result, the detection approach presented here can realize detection of both known and unknown worms with a high detection rate and accuracy.	algorithm;network traffic control;real-time clock	Yu Yao;Junwei Lv;Fuxiang Gao;Yanfang Zhang;Ge Yu	2008	2008 International Multi-symposiums on Computer and Computational Sciences	10.1109/IMSCCS.2008.29	real-time computing;computer science;computer security;computer network	Arch	-60.33018413812121	66.69696932526286	1101
9dea61bacafa4710f15b64da1d14c3b12a3fa4b8	so what's innovative and exotic about star-p for matlab and other clients?	ease of use;client server;parallel computer;scientific applications;overlapping communication and computation;performance modeling	"""Star-P is a unique technology offered by Interactive Supercomputing after nurturing at MIT. Star-P through its clever abstractions is solving the ease of use problem that has plagued supercomputing. Given that there have been around 30 parallel MATLABs including three other major offerings, Star-P must be way ahead to compete in the marketplace.Some of the innovative features of Star-P are the ability to program in MATLAB, hook in task parallel codes written using a processor free abstraction, hook in existing parallel codes, and obtain the performance that represents the HPC promise. All this is through a client/server interface. Other clients such as Python or R could be possible. The MATLAB, Python, or R becomes the """"browser."""" If we make it look easy, it is because decades of parallel computing experience has taught us that it is not.This talk demonstrates the abstractions and innovations that make this possible."""		Alan Edelman	2006		10.1145/1188455.1188746	parallel computing;usability;computer science;theoretical computer science;operating system;distributed computing;programming language;client–server model	HPC	-11.994248923279422	40.411207858178095	1106
2266417d1b0bc65eb869a1cd4f02877dca87a01f	f-soft: software verification platform	software verification;program verification;verificacion programa;verification programme	In this paper, we describe our verification tool F-Soft which is developed for the analysis of C programs. Its novelty lies in the combination of several recent advances in formal verification research including SAT-based verification, static analyses and predicate abstraction. As shown in the tool overview in Figure 1, we translate a program into a Boolean model to be analyzed by our verification engine DiVer [4], which includes BDD-based and SAT-based model checking techniques. We include various static analyses, such as computing the control flow graph of the program, program slicing with respect to the property, and performing range analysis as described in Section 2.2. We model the software using a Boolean representation, and use customized heuristics for the SAT-based analysis as described in Section 2.1. We can also perform a localized predicate abstraction with register sharing as described in Section 2.3, if the user so chooses. The actual analysis of the resulting Boolean model is performed using DiVer. If a counter-example is discovered, we use a testbench generator that automatically generates an executable program for the user to examine the bug in his/her favorite debugger. The F-Soft tool has been applied on numerous case studies and publicly available benchmarks for sequential C programs. We are currently working on extending it to handle concurrent programs.	benchmark (computing);boolean algebra;control flow graph;debugger;executable;formal verification;heuristic (computer science);model checking;predicate abstraction;program slicing;software verification;standard boolean model;static program analysis;test bench	Franjo Ivancic;Zijiang Yang;Malay K. Ganai;Aarti Gupta;Ilya Shlyakhter;Pranav Ashar	2005		10.1007/11513988_31	real-time computing;verification;software verification;computer science;theoretical computer science;operating system;machine learning;database;distributed computing;high-level verification;runtime verification;programming language;computer security;intelligent verification;algorithm;functional verification	Logic	-18.553862389077374	28.537438138114126	1114
b4db8ff06725d7cfdf703c03ec1bdc5f39794eb3	analyzing the possibility of applying asymmetric transport protocols in terms of software defined networks		Network load balancing (NLB) is an important element of construction and management of fault tolerance in communication networks. At present, there are a lot of balancing algorithms both for standard approaches to networking and for software defined networks (SDN). An asymmetric transport protocol having the property to use the method anycast for parallel coupling with several servers was developed. The general description of SDN, TCP, and the first asymmetric transport protocol Trickles, as well as one conducted experiment in a network simulator in comparison with Trickles with some implementations of TCP, is presented in the article. A new algorithm for operation of the asymmetric transport protocol based on the experimental results is suggested. Several variants of using the asymmetric transport protocols in terms of SDN are discussed.	algorithm;antivirus software;anycast;automatic control;computer;control unit;fault tolerance;firewall (computing);gateway (telecommunications);interconnection;load balancing (computing);network load balancing;network congestion;network packet;networking hardware;proxy server;server (computing);simulation;software-defined networking;tcp congestion control;telecommunications network	Mikhail A. Nikitinskiy;Igor V. Alekseev	2015	Automatic Control and Computer Sciences	10.3103/S0146411615020042	computer science;theoretical computer science;distributed computing;computer network	Networks	-13.323025038222841	83.05051225806325	1118
e00f66bb539e4816e35c004d33df1a53bf90be14	a secure public key encryption from computational linear diffe-hellman problem	public key cryptography;standards;encryption;gap k computational linear assumption public key encryption security computational linear diffe hellman problem twin gap computational linear diffie hellman assumption based chosen ciphertext secure scheme twin gap computational linear diffie hellman assumption based cca secure scheme encryption decryption;public key encryption;polynomials;public key;dldh assumption;games;public key encryption cca secure dldh assumption;games encryption public key polynomials standards;cca secure	This paper proposes a practical public key encryption scheme which is provable chosen ciphertext(CCA) secure based on the gap computational linear Diffie-Hellman assumption in the standard model. This is the first CCA secure scheme based on the gap computational linear Diffie-Hellman assumption. This scheme is efficient and the proof of the security is tight. We also reduce the size of the public key from n to 2√n based on the twin gap computational linear Diffie-Hellman assumption. And the time for encryption and decryption is significantly reduced. And we point out that a generalization of the scheme can be constructed similarly based on the gap k-computational linear assumption.	ciphertext;computation;computational diffie–hellman assumption;decisional diffie–hellman assumption;diffie–hellman key exchange;encapsulation (networking);encryption;key encapsulation;provable security;public-key cryptography	Fengqing Tian;Haili Xue;Xue Haiyang	2012	2012 Eighth International Conference on Computational Intelligence and Security	10.1109/CIS.2012.110	multiple encryption;40-bit encryption;computer science;theoretical computer science;mathematics;internet privacy;public-key cryptography;deterministic encryption;key distribution;computer security;encryption;probabilistic encryption;56-bit encryption;attribute-based encryption	Crypto	-39.944147089575196	76.12845720520309	1125
db79e21db51bc3d698754933a481bc1f043cf615	challenges in automated model-based hmi testing		We describe our approach to automated model-based HMI testing. The paper is divided into two parts. In the first part, we summarize the current status of our work. In the second part, we describe a number of research areas that need to be worked on in order to achieve true model-based HMI test automation. 1 Test Automation in the HMI Domain The task of test automation involves two subtasks: (1) automated test case selection, and (2) automated test case execution. The long-term goal of test automation is to test against a complete formal specification of the unit under test. To be useful for test automation, a specification must comprise the static states and the dynamic behavior of the unit under test. The specification must be formal and sufficiently detailed in order to allow for automatic processing. We call such a comprehensive and formal specification a “model.” At present, such models are, in general, not available in the HMI domain. The design of the syntax and semantics of an appropriate specification language (i.e., modeling language) needs to take into account the requirements of automated testing. In order to be able to state these requirements, we need to start gaining experience with automated model-based testing, which in turn depends on the availability of models in the first place. In order to escape this chicken-or-egg situation, we have taken an intermediate step, in which the tests are performed not against the formal model of the HMI but against a prototype implementation of the HMI. This intermediate step is the topic of the next subsection. 1.1 Intermediate step toward test automation The key to prototype-based test automation is that the prototype implementation is used as a stand-in for the formal HMI model, which is not yet available. Instead of testing the HMI embedded control unit (the unit under test, henceforth called the HMI ECU) against the formal model, we directly compare the states and the behavior of the HMI ECU against the states and the behavior of the prototype implementation. In the following paragraphs, we briefly describe our rapid HMI prototyping framework FLUID (“Flexible User Interface Development”) and its role in the intermediate step toward model-based test automation. For a more-detailed description of FLUID, see [GES04].	device under test;embedded system;engine control unit;fltk;formal language;formal specification;model-based testing;modeling language;prototype;requirement;specification language;test automation;test case;user interface	Reinhard Stolle;Thomas Benedek;Christian Knuechel;Harald Heinecke	2005			specification language;modeling language;automation;formal specification;semantics;control unit;computer architecture;user interface;device under test;computer science	SE	-46.70340268583956	30.976992046793363	1131
18ab8d92e35af52460561614999b358b55a3f053	canonical proof nets for classical logic	000 computer science knowledge systems;510 mathematics	Proof nets provide abstract counterparts to sequent proofs modulo rule permutations; the idea being that if two proofs have the same underlying proof-net, they are in essence the same proof. Providing a convincing proof-net counterpart to proofs in the classical sequent calculus is thus an important step in understanding classical sequent calculus proofs. By convincing, we mean that (a) there should be a canonical function from sequent proofs to proof nets, (b) it should be possible to check the correctness of a net in polynomial time, (c) every correct net should be obtainable from a sequent calculus proof, and (d) there should be a cut-elimination procedure which preserves correctness. Previous attempts to give proof-net-like objects for propositional classical logic have failed at least one of the above conditions. In [23], the author presented a calculus of proof nets (expansion nets) satisfying (a) and (b); the paper defined a sequent calculus corresponding to expansion nets but gave no explicit demonstration of (c). That sequent calculus, called LK∗ in this paper, is a novel one-sided sequent calculus with both additively and multiplicatively formulated disjunction rules. In this paper (a selfcontained extended version of [23]) , we give a full proof of (c) for expansion nets with respect to LK∗, and in addition give a cut-elimination procedure internal to expansion nets – this makes expansion nets the first notion of proof-net for classical logic satisfying all four criteria.	correctness (computer science);eisenstein's criterion;emoticon;first-order predicate;logical connective;modulo operation;petri net;polynomial;prenex normal form;propositional calculus;prototype;quantifier (logic);sequent calculus;subnetwork;time complexity;utility functions on indivisible goods	Richard McKinley	2013	Ann. Pure Appl. Logic	10.1016/j.apal.2012.05.007	mathematical analysis;discrete mathematics;cut-elimination theorem;geometry of interaction;mathematics;proof calculus;noncommutative logic;curry–howard correspondence;sequent;natural deduction;structural proof theory;proof complexity;algorithm;algebra	Logic	-11.625438807307887	15.317455478352786	1133
5535d452e6d70342096c8c3768d7071d3a540443	bug localization using latent dirichlet allocation	computacion informatica;bug localization;information retrieval;program comprehension;grupo de excelencia;statistical model;latent dirichlet allocation;ciencias basicas y experimentales;latent semantic indexing	0950-5849/$ see front matter 2010 Elsevier B.V. A doi:10.1016/j.infsof.2010.04.002 * Corresponding author. Tel.: +1 256 824 6088, +1 2 6039. E-mail addresses: slukins@cs.uah.edu (S.K. Lukins), etzkorl@uah.edu (L.H. Etzkorn). 1 Tel.: +1 256 348 4740; fax: +1 256 348 0219. 2 Tel.: +1 256 824 6291; fax: +1 256 824 6039. Context: Some recent static techniques for automatic bug localization have been built around modern information retrieval (IR) models such as latent semantic indexing (LSI). Latent Dirichlet allocation (LDA) is a generative statistical model that has significant advantages, in modularity and extensibility, over both LSI and probabilistic LSI (pLSI). Moreover, LDA has been shown effective in topic model based information retrieval. In this paper, we present a static LDA-based technique for automatic bug localization and evaluate its effectiveness. Objective: We evaluate the accuracy and scalability of the LDA-based technique and investigate whether it is suitable for use with open-source software systems of varying size, including those developed using agile methods. Method: We present five case studies designed to determine the accuracy and scalability of the LDAbased technique, as well as its relationships to software system size and to source code stability. The studies examine over 300 bugs across more than 25 iterations of three software systems. Results: The results of the studies show that the LDA-based technique maintains sufficient accuracy across all bugs in a single iteration of a software system and is scalable to a large number of bugs across multiple revisions of two software systems. The results of the studies also indicate that the accuracy of the LDA-based technique is not affected by the size of the subject software system or by the stability of its source code base. Conclusion: We conclude that an effective static technique for automatic bug localization can be built around LDA. We also conclude that there is no significant relationship between the accuracy of the LDA-based technique and the size of the subject software system or the stability of its source code base. Thus, the LDA-based technique is widely applicable. 2010 Elsevier B.V. All rights reserved.	agile software development;automatic bug fixing;dynamic logic (digital electronics);extensibility;fax;generative model;information retrieval;iteration;latent dirichlet allocation;latent semantic analysis;like button;open-source software;scalability;software bug;software system;statistical model;topic model	Stacy K. Lukins;Nicholas A. Kraft;Letha H. Etzkorn	2010	Information & Software Technology	10.1016/j.infsof.2010.04.002	latent dirichlet allocation;statistical model;latent semantic indexing;computer science;theoretical computer science;machine learning;data mining;database	SE	-57.664264550325825	34.861258766854014	1142
41575b7cfb7dcdbb73455ba8666587b4ef5ac13c	two applications of analytic functors	graph theory;morphisme;morfismo;teoria grafo;foncteur;proof theory;fonction generatrice;relacion orden;forma normal;ordering;theorie graphe;term rewrite system;relation ordre;category theory;theorie categorie;functor;funcion generatriz;formule inversion;normal form;foncteur analytique;lagrange good inversion formula;generating function;forme normale;morphism;recursive path ordering;inversion formula;analytic functor;teoria categoria	We apply the theory of analytic functors to two topics related to theoretical computer science. One is a mathematical foundation of certain syntactic well-quasi-orders and well-orders appearing in graph theory, the theory of term rewriting systems, and proof theory. The other is a new veri*cation of the Lagrange–Good inversion formula using several ideas appearing in semantics of lambda calculi, especially the relation between categorical traces and *xpoint operators. c © 2002 Published by Elsevier Science B.V.	3d xpoint;graph theory;lambda calculus;rewriting;theoretical computer science;tracing (software)	Ryu Hasegawa	2002	Theor. Comput. Sci.	10.1016/S0304-3975(00)00349-2	generating function;combinatorics;discrete mathematics;order theory;graph theory;proof theory;mathematics;algorithm;functor;morphism;category theory;algebra	Logic	-6.659927944140366	17.227558457693437	1143
2868f47ea072e800a8f95a8870b5a6cb52cf0ec3	toward green computing: striking the trade-off between memory usage and energy consumption of sequential pattern mining on gpu		The energy consumption of the data centers grows rapidly in the recent years, especially due to the increasing usage of the energy consuming GPU. Note that reducing the energy consumption can not only lower the operating cost but also increase the capacity of the data center. Therefore, it is important to explore the energy efficiency of an algorithm. In this work, we analyze a trade-off between the mining efficiency and the energy consumption of parallel sequential pattern mining on GPU. Sequential pattern mining is an important topic in the field of knowledge discovery since accelerating sequential pattern mining enables the possibility of various real-time recommendation systems and many other applications. The knowledge of the trade-off helps to decide the best hardware configuration for data centers as well as develop a task scheduling algorithm in a heterogeneous environment with better energy efficiency. We first study how the memory usage affects the execution time with a program profiler. Then, we apply GPU power models to find the relationship between the memory usage and the energy consumption. Finally, we conduct extensive experiments on both synthetic and real datasets to validate our results.	algorithm;data center;data mining;experiment;graphics processing unit;real-time clock;recommender system;run time (program lifecycle phase);scheduling (computing);sequential pattern mining;synthetic intelligence	Yu-Heng Hsieh;Ming-Syan Chen	2018	2018 IEEE First International Conference on Artificial Intelligence and Knowledge Engineering (AIKE)	10.1109/AIKE.2018.00033	green computing;knowledge extraction;energy consumption;real-time computing;efficient energy use;scheduling (computing);operating cost;data center;general-purpose computing on graphics processing units;computer science	HPC	-18.962977845097058	61.710121515289565	1146
19d2853e819c746382a714fea47a94a9f657c1d7	agra: ai-augmented geographic routing approach for iot-based incident-supporting applications		Abstract Applications that cater to the needs of disaster incident response generate large amount of data and demand large computational resource access. Such datasets are usually collected in real-time at the incident scenes using different Internet of Things (IoT) devices. Hierarchical clouds, i.e. , core and edge clouds, can help these applications’ real-time data orchestration challenges as well as with their IoT operations scalability, reliability and stability by overcoming infrastructure limitations at the ad-hoc wireless network edge. Routing is a crucial infrastructure management orchestration mechanism for such systems. Current geographic routing or greedy forwarding approaches designed for early wireless ad-hoc networks lack efficient solutions for disaster incident-supporting applications, given the high-speed and low-latency data delivery that edge cloud gateways impose. In this paper, we present a novel Artificial Intelligent (AI)-augmented geographic routing approach, that relies on an area knowledge obtained from the satellite imagery (available at the edge cloud) by applying deep learning. In particular, we propose a stateless greedy forwarding that uses such an environment learning to proactively avoid the local minimum problem by diverting traffic with an algorithm that emulates electrostatic repulsive forces. In our theoretical analysis, we show that our Greedy Forwarding achieves in the worst case a 3 . 291 path stretch approximation bound with respect to the shortest path, without assuming presence of symmetrical links or unit disk graphs. We evaluate our approach with both numerical and event-driven simulations, and we establish the practicality of our approach in a real incident-supporting hierarchical cloud deployment to demonstrate improvement of application level throughput due to a reduced path stretch under severe node failures and high mobility challenges of disaster response scenarios.	anatomic node;approximation;approximation algorithm;best, worst and average case;computation;computation (action);computational resource;computer security incident management;deep learning;deploy;edge computing;emulator;event-driven architecture;geographic routing;goodput;gradient descent;greedy perimeter stateless routing in wireless networks;guided imagery;hl7publishingsubsection <operations>;hoc (programming language);internet of things;management information systems;maxima and minima;mesh networking;network packet;node - plant part;numerical analysis;real-time computing;real-time data;real-time locating system;satellite imagery;scalability;short;shortest path problem;simulation;solutions;state (computer science);stateless protocol;structure of atrioventricular node;throughput	Dmitrii Chemodanov;Flavio Esposito;Andrei M. Sukhov;Prasad Calyam;Huy Trinh;Zakariya A. Oraibi	2019	Future Generation Comp. Syst.	10.1016/j.future.2017.08.009	throughput;orchestration (computing);wireless network;distributed computing;cloud computing;scalability;computational resource;geographic routing;computer network;computer science;shortest path problem	Mobile	-18.444801402181874	80.06543342925713	1151
5f840b5dae53972ad8d8d40b55442e50d6addff5	user-centric smart buildings for energy sustainable smart cities		Over six billion people are expected to live in cities and surrounding regions by 2050. Consequently, in the near future, the autonomic and smart operation of cities may be a critical requirement to improve the economic, social, and environmental well-being of citizens. Smart urban technologies represent an important contribution to the sustainable development of cities, making smart cities a reality. In this sense, the energy sustainability of cities has become a global concern, bringing with it a wide range of research and technological challenges that affect many aspects of people’s lives. Because most of the human lifetime is spent indoors, buildings, which make up a city subsystem, require special attention. Indeed, buildings are the cornerstone in terms of power consumption and CO2 emissions on a global scale. In this paper, we analyze the role that buildings play in terms of their energy performance at city level and present an energy-efficient management system integrated in a building automation platform based on an Internet of Things approach. Occupants play a crucial role in the system’s operation to achieve energy efficient building performance, and any impact on self-sustainable smart cities will be a consequence of efficient user-centric smart building designs. Our proposal represents a user-centric smart solution as a contribution to the energy sustainability of modern cities. The building management platform has been deployed in a real (smart) building, in which a set of tests were carried out to assess different concerns involved in the building’s infrastructure management. The first stages of this experiment have already resulted in an energy saving in heating of about 20% at building level, which could translate into a reduction of 8% in the energy consumption of buildings at a European city level. Copyright © 2013 John Wiley & Sons, Ltd.	autonomic networking;database;experiment;fixed-point iteration;internet of things;john d. wiley;location awareness;real-time data;reduction (complexity);requirement;sensor;smart city;user (computing);user profile;virtual appliance	María Victoria Moreno Cano;Miguel A. Zamora;Antonio F. Gómez-Skarmeta	2014	Trans. Emerging Telecommunications Technologies	10.1002/ett.2771	simulation;engineering;civil engineering	AI	-43.97912883742846	49.973702726107874	1159
54f57aa6bc43d07146d0858e73dd600dfdec52c8	on the capacity of capacitated automata		Capacitated automata (CAs) have been recently introduced in [8] as a variant of finite-state automata in which each transition is associated with a (possibly infinite) capacity. The capacity bounds the number of times the transition may be traversed in a single run. The study in [8] includes preliminary results about the expressive power of CAs, their succinctness, and the complexity of basic decision problems for them. We continue the investigation of the theoretical properties of CAs and solve problems that have been left open in [8]. In particular, we show that union and intersection of CAs involve an exponential blow-up and that their determinization involves a doubly-exponential blow up. This blow-up is carried over to complementation and to the complexity of the universality and containment problems, which we show to be EXPSPACE-complete. On the positive side, capacities do not increase the complexity when used in the deterministic setting. Also, the containment problem for nondeterministic CAs is PSPACE-complete when capacities are used only in the left-hand side automaton. Our results suggest that while the succinctness of CAs leads to a corresponding increase in the complexity of some of their decision problems, there are also settings in which succinctness	automaton;cardinality (data modeling);computer aided verification;decision problem;expspace;expressive power (computer science);finite-state machine;flow network;ford–fulkerson algorithm;formal system;handbook;icalp;information and computation;lecture notes in computer science;pspace-complete;powerset construction;presburger arithmetic;probabilistic automaton;springer (tank);temporal logic;time complexity;universal turing machine	Orna Kupferman;Sarai Sheinvald	2016		10.1007/978-3-319-30000-9_24	expressive power;regular language;combinatorics;discrete mathematics;deterministic finite automaton;computer science	Theory	-6.517088575245851	23.42259917025244	1165
d73f4857a7c21f07710e35899f5b99c0c6d2b391	100vg-anylan: network operation and real-time capabilities	multimedia guarantees;data transmission;twisted pair cables;throughput capacities;protocols;single hub system;ieee standards;image databases;multimedia applications;application software;physical layer;real time;access protocols local area networks data communication ieee standards telecommunication signalling twisted pair cables multimedia communication;database;software applications;multimedia application;imaging techniques;access protocol;data communication;100 mbit s 100vg anylan network operation data transmission software applications graphical user interfaces visualization imaging techniques database local area networks multimedia applications local area network protocols throughput capacities demand priority protocol ieee 802 12 networking technologies cascaded hub systems single hub system physical layer signalling voice grade twisted pair cabling real time guarantees multimedia guarantees access protocol;demand priority protocol;visualization;graphical user interfaces;real time guarantees;multimedia communication;data visualization;100vg anylan;access protocols;graphic user interface;token networks;physical layer signalling;telecommunication signalling;networking technologies;network operation;ieee 802 12;voice grade twisted pair cabling;ethernet networks;application software local area networks protocols data communication graphical user interfaces data visualization image databases visual databases ethernet networks token networks;cascaded hub systems;local area networks;local area network;100 mbit s;local area network protocols;visual databases	The volume of data transmission required by current software applications utilizing graphical user interfaces, visualization and imaging techniques, ana‘ large databases, is taxing, and in some cases, overloading the capabilities of classic local area networks such as Ethernets and token rings. Multimedia applications that will be appearing and maturing in the near future will further exceed the capabilities of rhese networks. These effects have engendered the development of a number of new local area network protocols that attempt to bring higher throughput capacities to the mtwork user. The behavior and operation of one of these new nerworks, IOOVG-AnyLAN, the IO0 megabit per second demand priority protocol proposed by IEEE 802.I2. is the subject of this paper. We begin wkh a short discussion of common existing networks to place IOOVG-AnyLAN in conies with established networking technologies. The operation of single hub and cascaded hub 100VG-AnyLAN system is discussed, along with the principle features of physical layer signalling on voice grade twisted pair cabling. Finally, a summary of the types of real-time and multimedia guarantees that can be provided by the IOOVG-AnyLAN system is presented.	ana (programming language);communications protocol;data rate units;database;function overloading;graphical user interface;megabit;real-time clock;real-time transcription;throughput;token ring;twisted pair;usb hub	Stephen Barilovits;Jayant Kadambi	1994		10.1109/LCN.1994.386608	local area network;real-time computing;telecommunications;computer science;operating system;graphical user interface;distributed computing;data visualization;computer network	Visualization	-21.380960628832643	91.43961508937247	1171
e53562174fc7f6ab599357cea74ebaa5888c5330	media inter-cloud architecture and storage efficiency challenge	storage management cloud computing quality of experience quality of service;storage space efficiency digital media inter cloud architecture cloud computing internet cloud federation dropbox amazon clouddrive googledrive microsoft skydrive onedrive sugarsync quality of experience qoe quality of service qos;media cloud;media;computer architecture;synchronization;inter cloud computing;cloud storage media cloud inter cloud computing cloud federation;media cloud computing computer architecture synchronization delays quality of service;quality of service;cloud federation;cloud storage;delays;cloud computing	Digital media has been increasing very rapidly, resulting in cloud computing popularity gain. Cloud computing provides ease of management of large amount of data and resources. With a lot of devices communicating over the Internet and with the rapidly increasing user demands, solitary clouds have to communicate to other clouds to fulfill the demands and discover services elsewhere. This scenario is called inter-cloud computing or cloud federation. In this paper, we discuss the importance of inter-cloud computing and present its architecture. Inter-cloud computing also involves some issues, one of which is storage efficiency. We have evaluated some of the most noteworthy cloud storage services, namely: Drop box, Amazon Cloud Drive, Google Drive, Microsoft Sky Drive/One Drive, Box, and Sugar Sync in terms of Quality of Experience (QoE), Quality of Service (QoS), and storage space efficiency. Discussion on the results shows the acceptability of these storage services and their shortcomings.	amazon elastic compute cloud (ec2);cloud computing;cloud storage;digital media;google drive;intercloud;point cloud;quality of service;storage efficiency	Mohammad Aazam;Eui-nam Huh	2014	2014 International Conference on Cloud and Autonomic Computing	10.1109/ICCAC.2014.30	cloud computing security;cloud computing;computer science;cloud testing;utility computing;internet privacy;world wide web;computer network	HPC	-30.940459886218008	58.11830859305514	1172
370dc36b43b02806e0d6ba37d878ec95e88444ec	mobile xmpp and cloud service collaboration: an alliance for flexible disaster management	critical infrastructure;system design;ad hoc networks;collaboration;disaster management;disasters;cloud computing;cloud services;protocols	Recent crises like the Fukushima incident in Japan show that there is a demand for flexible and easy-to-use communication and sensor systems to support post-disaster management (i.e. the organization of actions in the follow-up of disasters), especially when critical infrastructure is affected. This paper introduces a system design that combines mobile XMPP-based and sensor-equipped devices with the flexibility of cloud services. This combination provides the communication between the different involved parties (e.g. rescue forces) and enables a global view on sensed data through the use of cloud-based storage and analysis services. Along with a discussion about requirements and a description of appropriate solutions and initial evaluations, we present new insights on the practical appliance of XMPP and potential enhancements for XMPP-based real life collaboration applications in hybrid (ad hoc and infrastructure) network scenarios.	cloud computing;cloud storage;hoc (programming language);real life;requirement;systems design	Ronny Klauck;Jan Gäbler;Michael Kirsche;Sebastian Schoepke	2011	7th International Conference on Collaborative Computing: Networking, Applications and Worksharing (CollaborateCom)		cloud computing;computer science;operating system;world wide web;computer security;computer network	Mobile	-44.06270063619326	52.82392248635709	1177
a385ea2b42db1cd91ed4cbc33e0cc8f4545cc530	decentralized dynamic broadcast encryption	black-box use;public-key broadcast encryption;subset-cover framework;ddh assumption;group manager;specific policy;broadcast encryption system;decentralized dynamic broadcast encryption;subset-cover construction;central authority;encrypted data	A broadcast encryption system generally involves three kinds of entities: the group manager that deals with the membership, the encryptor that encrypts the data to the registered users according to a specific policy (the target set), and the users that decrypt the data if they are authorized by the policy. Public-key broadcast encryption can be seen as removing this special role of encryptor, by allowing anybody to send encrypted data. In this paper, we go a step further in the decentralization process, by removing the group manager: the initial setup of the group, as well as the addition of further members to the system, do not require any central authority. Our construction makes black-box use of well-known primitives and can be considered as an extension to the subset-cover framework. It allows for efficient concrete instantiations, with parameter sizes that match those of the subset-cover constructions, while at the same time achieving the highest security level in the standard model under the DDH assumption.	authorization;black box;broadcast encryption;decisional diffie–hellman assumption;entity;public key infrastructure	Duong Hieu Phan;David Pointcheval;Mario Strefler	2012		10.1007/978-3-642-32928-9_10	computer science;distributed computing;on-the-fly encryption;internet privacy;computer security;encryption	Security	-41.415237656215474	72.74937735633755	1192
600d41186a562dc709dc69087115569c4ef01e87	reliability evaluation according to a routing scheme for multi-state computer networks under assured accuracy rate	system reliability;assured accuracy rate;routing scheme;transmission time;quickest path (qp);multi-state computer network (mscn)	In many real-time networks such as computer networks, each arc has stochastic capacity, lead time, and accuracy rate. Such a network is named a multi-state computer network (MSCN). Under the strict assumption that the capacity of each arc is deterministic, the quickest path (QP) problem is to find a path that sends a specific amount of data with minimum transmission time. From the viewpoint of internet quality, the transmission accuracy rate is one of critical performance indicators to assess internet network for system administrators and customers. Under both assured accuracy rate and time constraint, this paper extends the QP problem to discuss the flow assignment for a MSCN. An efficient algorithm is proposed to find the minimal capacity vector meeting such requirements. The system reliability, the probability to send d units of data through multiple minimal paths under both assured accuracy rate and time constraint, can subsequently be computed. Furthermore, two routing schemes with spare minimal paths are adopted to reinforce the system reliability. The enhanced system reliability according to the routing scheme is calculated as well. The computational complexity in both the worst case and average case are analyzed.	algorithm;best, worst and average case;computational complexity theory;google+;goto;maxima and minima;maximal set;real computation;real-time clock;requirement;routing;system administrator	Yi-Kuei Lin;Cheng-Fu Huang	2016	Annals OR	10.1007/s10479-014-1673-8	mathematical optimization;real-time computing;computer science;operations management;distributed computing	Mobile	-6.26382283053276	77.96582609072733	1207
d678a536560f91a2ded41e6664ea349bdbab02ed	the model of the stochastic generator of the test programs and a method of its analyse				Sergey Filimonov	1993	The Computer Science Journal of Moldova		theoretical computer science;mathematics	Logic	-5.62659479589433	25.259208731922403	1215
0748ca3823dc30f61b2a3b617ba8926fb911bee2	taking advantage of heterogeneity in disk arrays	analisis sensibilidad;largeur bande;adaptraid;storage system;raid;cluster of workstations;homogeneidad;capacite stockage;block distribution;heterogeneidad;sensitivity analysis;capacidad almacenaje;storage capacity;systeme memoire;anchura banda;analyse sensibilite;bandwidth;homogeneite;sistema memoria;distributed algorithm;heterogeneity;heterogeneite;disk array;homogeneity	Disk arrays, or RAIDs, have become the solution to increase the capacity and bandwidth of most storage system, but their usage has some limitations because all the disks in the array have to be equal. Nowadays, assuming a homogeneous set of disks to build an array is becoming a not very realistic assumption in many environments, especially in low-cost clusters of workstations. It is difficult to find a disk with the same characteristics as the ones in the array and replacing or adding new disks breaks the homogeneity. In this paper, we propose two block-distribution algorithms (one for RAID0 and an extension for RAID5) that can be used to build disk arrays from a heterogeneous set of disks. We also show that arrays using this algorithm are able to serve many more disk requests per second than when blocks are distributed assuming that all disks have the lowest common speed, which is the solution currently being used. r 2003 Elsevier Science (USA). All rights reserved.	algorithm;array data structure;computer data storage;disk array;standard raid levels;web server;workstation	Toni Cortes;Jesús Labarta	2003	J. Parallel Distrib. Comput.	10.1016/S0743-7315(03)00038-8	embedded system;distributed algorithm;homogeneity;disk array;telecommunications;computer science;heterogeneity;operating system;sensitivity analysis;bandwidth;algorithm;raid	DB	-19.39712022448472	45.35919725371835	1222
2679e0037d455f971caf6af6d7190a2834ea547b	direct end-user access to remote information	query language;communication protocol	Many large, widely distributed organizations struggle with the enormous task of providing the right information to the right people at the right time. Organizations facing this task often develop groups of analysts who specialize in supplying information transport and access capabilities to end-users. However, this approach has several drawbacks. Our aim is to address these problems at their source — not by replacing analysts in the information access problem, but by automating the roles assumed by analysts. Toward this end, this paper describes a strategy that combines four techniques to solve such problems: (1) an architecture for coarse-grained agents (CGAS}, (2) a communication protocol that enables CGAS to interact, (3,) an intermediate query language (IQL), designed around user-level concepts, and (4) a query translation mechanism that transforms IQL requests into database-specific queries. A prototype implementation, known as oMIE, is described.	communications protocol;information access;prototype;query language;user space	Steve Laufmann;Richard Blumenthal;Laural M. Thompson;Beth Bowen	1991		10.1145/122831.122833	communications protocol;query optimization;query expansion;web query classification;computer science;query by example;theoretical computer science;database;rdf query language;web search query;information retrieval;query language	Mobile	-36.05493553105952	8.992438767224636	1225
84b8e0146bbf46bfefd3b7021fe16785dc63c4d7	dogfish: decentralized optimistic game-theoretic file sharing		Peer-to-peer (p2p) file sharing accounts for the most uplink bandwidth use in the Internet. Therefore, in the past few decades, many solutions tried to come up with better proposals to increase the social welfare of the participants. Social welfare in such systems are categorized generally as average download time or uplink bandwidth utilization. One of the most influential proposals was the BitTorrent. Yet, soonafter studies showed that BitTorrent has several problems that incentivize selfish users to game the system and hence decrease social welfare.	file sharing;theory	Seny Kamara;Alptekin Küpçü	2018		10.1007/978-3-319-93387-0_36	game theory;the internet;computer security;cryptographic protocol;telecommunications link;computer network;computer science;file sharing;bittorrent;social welfare;download	AI	-25.787526881152292	73.75830563043907	1229
876edda8840f4c12649a9aeab3f91a7a745b26fc	syntactic and semantic modularisation of modelling languages	uml;asm;sprachprofile;modularisierung;language profiles;sprachdefinition;modularisation;language definition;sdl	Modelling languages are important in the process of software development. The suitability of a modelling language for a project depends on its applicability to the target domain. Here, domain-speci c languages have an advantage over more general modelling languages. On the other hand, modelling languages like the Uni ed Modeling Language can be used in a wide range of domains, which supports the reuse of development knowledge between projects. This thesis treats the syntactical and semantical harmonisation of modelling languages and their combined use, and the handling of complexity of modelling languages by providing language subsets called language pro les with tailor-made formal semantics de nitions, generated by a pro le tool. We focus on the widely-used modelling languages SDL [35] and UML [52], and formal semantics de nitions speci ed using Abstract State Machines [27].		Rüdiger Grammes	2007			natural language processing;fourth-generation programming language;formal language;computer science;domain-specific language;third-generation programming language;linguistics;ontology language;fifth-generation programming language;programming language;second-generation programming language;comparison of multi-paradigm programming languages	SE	-26.142801063796302	21.082419954800695	1230
c2d45599d587c8488a2d996ad562625df851e3b6	internet implications of telephone access	access network;internet access;central office;service provider;fault tolerant;e commerce;circuit switched;internet telephony;home e commerce internet implications telephone access telecommunications network subscribers public switched telephone network voice service users internet access circuit switched access isps pstn access availability voice services internet user connection central office switches network access nodes redundancy fault tolerance total service outages end to end carriers integrated voice data services carrier transparency survivable network access infrastructure access networks connection speed end to end latency problems service providers dependable access;fault tolerant computing;low latency;redundancy;long distance;internet telephony switches delay telecommunication switching circuits personal communication networks availability web and internet services central office redundancy;public switched telephone network;quality of service;quality of service internet telephony fault tolerant computing redundancy;telecommunication networks	What good is a network if it can't be accessed? In fact, access is often a single point of failure for data or telecommunications network subscribers. In the US, the Public Switched Telephone Network is the primary provider for the vast majority of voice service users. The PSTN is also the principal source of circuit-switched access to ISPs for an increasing number of PCs. Therefore, PSTN access availability is a benchmark for those who are considering providing or acquiring Internet access or voice services. The article presents a typical Internet user connection through the local PSTN to an ISP. In the PSTN, local or central office switches serve as network access nodes. Although considerable redundancy and fault tolerance are engineered into both the network itself and these switches, total service outages do occur. In the future, terms such as local carrier and long-distance carrier are likely to disappear as more end-to-end carriers emerge, providing integrated voice and data services. User demands for carrier transparency will dictate having a highly reliable, available, and survivable network access infrastructure. To date, access networks have emphasized connection speed to alleviate end-to-end latency problems. However, as subscribers experience low latency, service providers who offer dependable access will be the survivors. Fast access that isn't reliable won't be acceptable for home e-commerce.		Andrew P. Snow	1999	IEEE Computer	10.1109/2.789754	e-commerce;service provider;fault tolerance;network admission control;core network;quality of service;internet access;public switched data network;telecommunications;computer science;dial-up internet access;integrated access device;redundancy;computer security;circuit switching;computer network;network access point;access network;network access device;low latency	Visualization	-9.885617516965244	86.21808300154257	1239
54c9c1a619787bbfdb8322f8f41fbb07b7a61256	jsxm: a tool for automated test generation	tool;implementation;automated test generation;functional conformance;incremental testing;model based testing;stream x machines	The Stream X-machine (SXM) is an intuitive and powerful modelling formalism that extends finite state machines with a memory (data) structure and function-labelled transitions. One of the main strengths of the SXM is its associated testing strategy: this guarantees that, under well defined conditions, all functional inconsistencies between the system under test and the model are revealed. Unfortunately, despite the evident strength of SXM based testing, no tool that convincingly implements this strategy exists. This paper presents such a tool, called JSXM. The JSXM tool supports the animation of SXM models for the purpose of model validation, the automatic generation of abstract test cases from SXM specifications and the transformation of abstract test cases into concrete test cases in the implementation language of the system under test. A special characteristic of the modelling language and of the tool is that it supports the specifications of flat SXM models as well as the integration of interacting SXM models.	conformance testing;data structure;finite-state machine;image sxm;interaction;modeling language;object language;semantics (computer science);stream x-machine;system under test;test automation;test case;web service	Dimitris Dranidis;Konstantinos Bratanis;Florentin Ipate	2012		10.1007/978-3-642-33826-7_25	model-based testing;simulation;computer science;software engineering;implementation;engineering drawing;algorithm	SE	-46.57811931835195	29.980622284751565	1241
e5ceb27e2ec34e8edcf5235ef56929f96bd0f08b	model counting with boolean algebra and extension rule	artificial intelligence;boolean algebra;extension rule;hitting set;model counting;satisfiability problem	Model counting is an important problem in artificial intelligence and is applied in several areas of information science. Extension rule is a method which could be used to count models. But it’s not appropriate when clause length is short or clause number is huge. After studying extension rule, we found that the satisfiability problem could be solved by hitting set algorithms. And the models could be counted with extension rule after calculating hitting sets of a clause set. Therefore, we proposed an algorithm MCBE in this paper. With Boolean algebra, MCBE could easily calculate hitting sets of a clause set. Then, it gives the number of models with extension rule. The test results show that when clause length is short and clause number is big enough, the algorithm is more efficiency than the algorithm CDP and CER.	algorithm;artificial intelligence;boolean algebra;boolean satisfiability problem;information science;set cover problem	Youjun Xu;Dantong Ouyang;Yuxin Ye	2010	JCIT		filtered algebra;boolean algebra;boolean circuit;circuit minimization for boolean functions;boolean algebra;boolean domain;boolean expression;product term;computer science;maximum satisfiability problem;consensus theorem;stone's representation theorem for boolean algebras;boolean algebras canonically defined;relation algebra;cellular algebra;complete boolean algebra;allen's interval algebra;two-element boolean algebra;free boolean algebra;parity function	AI	-7.245597674969721	16.67006024161355	1242
6c1b1fedbe92e500cb82c1a63fcff518e130f5d1	effects of inter-server communication in an hla-based distributed virtual environment	load balancing interserver communication distributed virtual environment geographically distributed users shared virtual world multiserver architecture communication architecture load distribution server workload high level architecture hla declaration management services;distributed system;load balancing distributed virtual environments high level architecture inter server communication;file servers;inter server communication;resource allocation;distributed processing;virtual reality;distributed virtual environments;system performance;software architecture;distributed virtual environment;load distribution;file servers virtual reality distributed processing resource allocation software architecture;loadbalancing;virtual environment computer architecture delay computer graphics avatars internet multimedia databases concurrent computing distributed computing scalability;high level architecture;geographic distribution;virtual worlds	Distributed virtual environments (DVEs) are distributed systems that allow multiple geographically distributed users to interact concurrently in a shared virtual world. To achieve good scalability, a multi-server architecture is usually employed as the communication architecture for DVEs. However, there are still several issues in multi-server DVE systems that need to be addressed. One of the key issues is the load distribution mechanism, which aims to maximize system performance by balancing server workload and minimizing inter-server communication. However, recent research on load distribution for DVEs did not quantitatively study how inter-server communication affects the system response time. In this paper, we describe a communication architecture for DVEs based on a combination of the widely used multi-server architecture and the high level architecture (HLA), and study the effect of inter-server communication on system response time with a prototype implementation, using HLA declaration management services. Preliminary experiments and results are also presented.	declaration (computer programming);digital video effect;distributed computing;experiment;high-level architecture;inter-server;load balancing (computing);prototype;response time (technology);scalability;server (computing);virtual reality;virtual world	Ta Nguyen Binh Duong;Suiping Zhou	2005	First International Conference on Distributed Frameworks for Multimedia Applications	10.1109/DFMA.2005.26	file server;software architecture;real-time computing;resource allocation;computer science;weight distribution;operating system;distributed computing;virtual reality	HPC	-27.695232432611668	49.57126795171424	1243
56f8881bdb64293a507ced2f28cbe97570e76482	sistemas de ayuda inteligente para entornos informaticos complejos		Los sistemas de ayuda inteligente (SAI) o asistentes inteligentes son una propuesta para mejorar el aprendizaje y el rendimiento en la utilización de entornos informáticos complejos. Habitualmente la ayuda tiene como único fin la mejora del rendimiento del usuario, pero nosotros creemos que dicha asistencia debe tener también un propósito educativo. Si el usuario mejora su conocimiento sobre la aplicación su necesidad de ayuda decrecerá con el tiempo. Un punto clave de los SAI es el énfasis que se hace en la adaptación de la ayuda proporcionada a las necesidades y conocimientos específicos de cada usuario para lo que se utiliza un modelo explícito de usuario. En este trabajo se describe el SAI Aran donde se ha ejemplificado este enfoque. Palabras clave: Sistemas de ayuda inteligente, Modelado de usuario, Informática educativa, Lenguajes de marcado	baldur's gate ii: shadows of amn;linear algebra;naruto shippuden: clash of ninja revolution 3;unique name assumption	Baltasar Fernández-Manjón	2001	Inteligencia Artificial, Revista Iberoamericana de Inteligencia Artificial		data mining;data science;computer science	Security	-107.1081746916059	17.406541666628403	1252
2bbf69f90dcf096ea4590cf7ecdd0036bfc4b5d9	bounds on the propagation of selection into logic programs	monadic second order;context free grammar;logic programs;point of view	We consider the problem of propagating selections (i.e., bindings of variables) into logic programs. In particular, we study the class of binary chain programs and define selection propagation as the task of finding an equivalent program containing only unary derived predicates. We associate a context free grammar <italic>L(H)</italic> with every binary chain program <italic>H</italic>. We show that, given <italic>H</italic> propagating a selection involving some constant is possible iff <italic>L(H)</italic> is regular, and therefore undecidable. We also show that propagating a selection of the form <italic>p(X,X)</italic> is possible iff <italic>L(H)</italic> is finite, and therefore decidable. We demonstrate the connection of these two cases, respectively, with the weak monadic second order theory of one successor and with monadic generalized spectra. We further clarify the analogy between chain programs and languages from the point of view of program equivalence and selection propagation heuristics.	algorithm;context-free grammar;heuristic (computer science);language binding;logic programming;monad (functional programming);name binding;query (complexity);recursion (computer science);selection (user interface);software propagation;turing completeness;unary operation;undecidable problem	Catriel Beeri;Paris C. Kanellakis;François Bancilhon;Raghu Ramakrishnan	1987		10.1145/28659.28683	combinatorics;discrete mathematics;computer science;mathematics;monadic predicate calculus;context-free grammar;programming language;algorithm	Logic	-10.313147214803758	18.384738686481665	1261
db6b4c09d54221249203624a9719153e76e4b328	leakage resilient strong key-insulated signatures in public channel	continual key leakage;signatures;public channel;key insulation	Key-insulation aims at minimizing (i.e., compartmentalizing) the damage of users from key exposures, and traditionally requires a private channel of communication between a user and a semi-trusted party called a helper to refresh the private keys. The configuration is highly suitable to architectures where the signer is a user application and the helper resides in the safer “trusted module,” yet the user wants to remain in control of the sensitive crypto operation. The private channel employed in the model, while acceptable in some settings, certainly limits the usage of key insulation schemes (in case the user sits across the network from the trusted environment). In 2009, Bellare, Duan, and Palacio (CT-RSA 2009) refined the model of key-insulation by considering public channels (namely, ones controlled by the adversary), and showed how to convert a key-insulated signature scheme from the private channel into the public one, using extra primitives such as key exchange protocols and symmetric encryption. In this paper, we show that the primitives may be redundant in specific cases. In particular, we revisit the original key-insulated signature scheme in the private channel given by Dodis, Katz, Xu, and Yung (PKC 2003), and show that, with a tweak, the scheme can be naturally proved secure in the public channel without any additional primitives. Next we consider the area of leakage resilient cryptographic schemes which has gained much interest recently. In particular, we consider the continual key leakage scenario of our design (which is more general than the model of key exposure), and argue that our proposal, while requiring an added helper component, nevertheless enjoys several advantages over the recent signature scheme of Faust et al. (TCC 2010) with the same purpose of allowing continual leakage. Our design demonstrates how when given a more complex architecture with some parts that are safer than others, a trade-off can be applied, exploiting the safer modules but keeping users in control; further we show how to do it while mitigating the effect of exposures and leakages.	signature;spectral leakage;strong key	Le Trieu Phong;Shin'ichiro Matsuo;Moti Yung	2010		10.1007/978-3-642-25283-9_11	telecommunications;engineering;internet privacy;computer security	Crypto	-39.32122928842623	74.78972174446858	1264
9c75ba03897cb133ddd0c016986c8f0cadc46d3a	protocol verification with reactive promela/rspin.		Reactive Promela/RSPIN is an extension to the protocol validator Promela/SPIN. It enhances the simulation and veri cation capabilities of SPIN by allowing modular speci cations to be analysed while alleviating the state-space explosion problem. Reactive Promela is a simple reactive language. The tool RSPIN is a preprocessor for SPIN which translates a Reactive Promela speci cation into a corresponding Promela speci cation. The main function performed by RSPIN is to combine con gurations of Reactive Promela automata into Promela proctypes. The translated speci cation can then be simulated and veri ed using SPIN. We demonstrate the language and tool by the speci cation, translation, simulation and verication of the LAP{B data link protocol. This protocol is quite complex, and bene ts from decomposition.	automata theory;decomposition (computer science);entry point;naruto shippuden: clash of ninja revolution 3;preprocessor;promela;spin;simulation;state space;validator	Elie Najm;Frank Olsen	1996			programming language;computer science;promela	Logic	-17.339840754251508	27.222570093669226	1274
418c74da22fbf9a5ba6e862a6f4723c76a19af0c	learning asynchronous typestates for android classes		In event-driven programming frameworks, such as Android, the client and the framework interact using callins (framework methods that the client invokes) and callbacks (client methods that the framework invokes). The protocols for interacting with these frameworks can often be described by finite-state machines we dub asynchronous typestates. Asynchronous typestates are akin to classical typestates, with the key difference that their outputs (callbacks) are produced asynchronously. We present an algorithm to infer asynchronous typestates for Android framework classes. It is based on the L∗ algorithm that uses membership and equivalence queries. We show how to implement these queries for Android classes. Membership queries are implemented using testing. Under realistic assumptions, equivalence queries can be implemented using membership queries. We provide an improved algorithm for equivalence queries that is better suited for our application than the algorithms from literature. Instead of using a bound on the size of the typestate to be learned, our algorithm uses a distinguisher bound. The distinguisher bound quantifies how two states in the typestate are locally different. We implement our approach and evaluate it empirically. We use our tool, Starling, to learn asynchronous typestates for Android classes both for cases where one is already provided by the documentation, and for cases where the documentation is unclear. The results show that Starling learns asynchronous typestates accurately and efficiently. Additionally, in several cases, the synthesized asynchronous typestates uncovered surprising and undocumented behaviors.	algorithm;android;callback (computer programming);documentation;event-driven programming;finite-state machine;interaction;jamie wilkinson;memory bound function;route distinguisher;starling;turing completeness;typestate analysis;undocumented feature	Arjun Radhakrishna;Nicholas V. Lewchenko;Shawn Meier;Sergio Mover;Krishna Chaitanya Sripada;Damien Zufferey;Bor-Yuh Evan Chang;Pavol Cerný	2017	CoRR		real-time computing;computer science;artificial intelligence;theoretical computer science;distributed computing;programming language;algorithm	Security	-20.657733082858634	29.119139538140583	1291
2d3ba61cb7b32cd3d7db3898617dc7a371e85782	games of timing for security in dynamic environments		Increasing concern about insider threats, cyber-espionage, and other types of attacks which involve a high degree of stealthiness has renewed the desire to better understand the timing of actions to audit, clean, or otherwise mitigate such attacks. However, to the best of our knowledge, the modern literature on games shares a common limitation: the assumption that the cost and effectiveness of the players’ actions are time-independent. In practice, however, the cost and success probability of attacks typically vary with time, and adversaries may only attack when an opportunity is present (e.g., when a vulnerability has been discovered). In this paper, we propose and study a model which captures dynamic environments. More specifically, we study the problem faced by a defender who has deployed a new service or resource, which must be protected against cyber-attacks. We assume that adversaries discover vulnerabilities according to a given vulnerability-discovery process which is modeled as an arbitrary function of time. Attackers and defenders know that each found vulnerability has a basic lifetime, i.e., the likelihood that a vulnerability is still exploitable at a later date is subject to the efforts by ethical hackers who may rediscover the vulnerability and render it useless for attackers. At the same time, the defender may invest in mitigation efforts to lower the impact of an exploited vulnerability. Attackers therefore face the dilemma to either exploit a vulnerability immediately, or wait for the defender to let its guard down. The latter choice leaves the risk to come away empty-handed. We develop two versions of our model, i.e., a continuous-time and a discrete-time model, and conduct an analytic and numeric analysis to take first steps towards actionable guidelines for sound security investments in dynamic contested environments.	cyber spying;denial-of-service attack;insider threat;nash equilibrium;numerical analysis;vulnerability (computing);waits	Benjamin Johnson;Aron Laszka;Jens Grossklags	2015		10.1007/978-3-319-25594-1_4	simulation;engineering;social psychology;computer security	Security	-63.25158850491476	56.826167051499326	1314
66295f16de1b92576a4cf463f637e5f78cc4cb9b	of starships and klingons: bayesian logic for the 23rd century	expressive power;graphical model;first order logic;probability distribution;first order;domain theory;bayesian network;probabilistic reasoning;decision support;knowledge representation	Intelligent systems in an open world must reason about many interacting entities related to each other in diverse ways and having uncertain features and relationships. Traditional probabilistic languages lack the expressive power to handle relational domains. Classical first-order logic is sufficiently expressive, but lacks a coherent plausible reasoning capability. Recent years have seen the emergence of a variety of approaches to integrating first-order logic, probability, and machine learning. This paper presents Multi-entity Bayesian networks (MEBN), a formal system that integrates First Order Logic (FOL) with Bayesian probability theory. MEBN extends ordinary Bayesian networks to allow representation of graphical models with repeated sub-structures, and can express a probability distribution over models of any consistent, finitely axiomatizable first-order theory. We present the logic using an example inspired by the Paramount Series Star Trek.	bayesian network;coherence (physics);emergence;entity;expressive power (computer science);first-order logic;first-order predicate;formal system;graphical model;interaction;machine learning;open world;star trek	Kathryn B. Laskey;Paulo Cesar G. da Costa	2005				AI	-18.689422065530454	6.531001217397382	1315
2bbf2f27813631234b8d28f9fd0727aac5c8cded	theoretical foundations of value withdrawal explanations for domain reduction	monotone operator;theoretical foundation	Solvers on finite domains use local consistency notions to remove values from the domains. This paper defines value withdrawal explanations. Domain reduction is formalized with chaotic iterations of monotonic operators. With each operator is associated its dual which will be described by a set of rules. For classical consistency notions, there exists such a natural system of rules. The rules express value removals as consequences of other value removals. The linking of these rules inductively defines proof trees. Such a proof tree clearly explains the removal of a value (which is the root of the tree). Explanations can be considered as the essence of domain reduction.	chaos theory;internationalized domain name;iteration;local consistency	Gérard Ferrand;Willy Lesaint;Alexandre Tessier	2002	Electr. Notes Theor. Comput. Sci.	10.1016/S1571-0661(04)80788-6	discrete mathematics;monotonic function;computer science;mathematics;algorithm	AI	-10.373997395932594	13.791038779428424	1319
4413e2a642ec739f08adfab3bc3ad5d2b9d0055e	analysis of quorum-based protocols for distributed (k+1)-exclusion	high availability;generic algorithm;indexing terms;mutual exclusion;fault tolerant distributed systems;replicated data;indexation;communication cost	A generalization of the majority quorum for the solution of the distributed (k + 1)-exclusion problem is proposed. This scheme produces a family of quorums of varying sizes and availabilities indexed by integral divisors r of k. The cases r = 1 and r = k correspond to known majority based quorum generation algorithms MAJ and DIV, whereas intermediate values of r interpolate between these two extremes. A cost and availability analysis of the proposed methods is also presented. An interesting implication of this analysis is that in a reasonably reliable environment with a large number of sites, even protocols with low communication costs attain high availability. Index Terms — Mutual exclusion, fault-tolerance, distributed systems, replicated data. —————————— ✦ ——————————	algorithm;distributed computing;fault tolerance;high availability;interpolation;mutual exclusion	Divyakant Agrawal;Ömer Egecioglu;Amr El Abbadi	1995		10.1007/BFb0030830	distributed algorithm;genetic algorithm;index term;mutual exclusion;computer science;theoretical computer science;database;distributed computing;high availability	DB	-21.12626691808475	47.00176223706609	1320
c3a461722fe34f662ff85456929b6137a0b77794	evaluation of development optimization of lte base station oss by product test automation	software;optimisation;regression testing lte operation support system oss web application;regression testing;base stations;lte;testing productivity automation software monitoring base stations maintenance engineering;long term evolution;maintenance engineering;testing;web services decision support systems long term evolution mobile computing online front ends optimisation regression analysis;online front ends;monitoring;web application;decision support systems;web services;operation support system optimization lte base station oss development product test automation telecom operators web front end regression testing web browser;regression analysis;productivity;mobile computing;oss;operation support system;automation	Containment of development cost associated with new services is important issue for telecom operators. Our OSS products have web front end. We require the regression testing for existing functions with new function addition development of OSS. Generally, the test items for regression testing do not change significantly at every development. In this paper, we propose the automated program test including the regression testing using web browser, and we show the cost efficiency of developing OSS.	compaq lte;cost efficiency;mathematical optimization;open sound system;regression testing;system testing;test automation	Saki Iwami;Yuki Kishikawa;Kentaro Fujii;Satoshi Namie;Kazuhide Takahashi;Shinsaku Akiyama	2011	2011 13th Asia-Pacific Network Operations and Management Symposium	10.1109/APNOMS.2011.6076960	operations support system;maintenance engineering;web service;productivity;regression testing;web application;simulation;computer science;base station;lte advanced;operating system;automation;software testing;mobile computing;regression analysis	SE	-70.20918848787795	29.9399246974293	1322
cbbfb278901a7c08aaf117d01e19f4defbf60961	interpolation von funktionen mehrerer veränderlicher durch normalpolynome			interpolation	Horst Mittendorf	1968	Elektronische Rechenanlagen	10.1524/itit.1968.10.16.166	interpolation	Crypto	-98.67987512964565	21.93509699271983	1324
9c061fde463fccee5ae43620b9adf20351930d6b	applications of a numerical geometry system in engineering	economic evaluation;command language;engineering design;time sharing;system design	In December of 1972, a study effort was undertaken at TRA to determine if it was feasible to create a computer based system that would significantly improve the generation, usage and dissemination of geometric information in an engineering, design and manufacturing environment. The effort was divided into several phases: study, system design and economic evaluation and, finally, implementation.  The system which has been implemented is a highly interactive keystroke driven system which executes under the TSO (Time Sharing Option) provided on IBM computers. The specific functional capabilities provided by the system appear to the end user as an extension of the time sharing system command language.	command (computing);command language;computational geometry;computer;event (computing);numerical analysis;risk assessment;systems design;time sharing option;time-sharing	James Mayfield;Richard M. Burkley	1976		10.1145/800146.804791	embedded system;electronic engineering;simulation;computer science;systems engineering;engineering;electrical engineering;operating system;programming language;time-sharing;algorithm;engineering design process;computer engineering;systems design	HCI	-56.34873102591295	6.905824931820239	1348
099e67b0b8b1612b9cf921ff505fa7f1a9e70c9b	analysis of a warehouse system with the use of computer simulation	computer simulation		computer simulation	B. Aissen;K. Terzidis	1993			computer simulation;simulation software	Theory	-54.9663133279737	7.803302609071976	1350
504960aa7523ed838ef78437b123173114e3e60d	reports from the field: assessing the art and science of participatory environmental modeling	system dynamics;ecosystem management;ecosystem science;collaborative modeling;model building;simulation model;environmental problem;participatory environmental modeling;environmental modeling;participatory modeling	Since the early work of Tansley (1935) and others we have embraced the concept that an ecosystem is a synergy of its parts and the relationship between those parts. Many science-centric approaches have been developed to address ecosystem management while at the same time taking into account the needs of the public. Participatory environmental modeling that uses system dynamics is an effective process for facilitating the integration of ecosystem science and social concerns. Using the art of facilitation and the science of model building the methodology creates a common language that integrates various types of information into simulation models. This paper describes a diversity of case studies, modeling and facilitation technique, and the inventiveness of practitioners who adjust their efforts to the needs of the stakeholders and the environmental problems they are facing. Participatory modelers who use system dynamics create customized platforms through which stakeholders can simultaneously explore their system, stressors to that system, potential tipping points, whether it is fragile or resilient, and any variety of potential policies that address the environment, social concerns, and long-term sustainability.	causal filter;causality;decision support system;diagram;ecosystem;environmental resource management;holism;iteration;participatory modeling;participatory monitoring;problem solving;programming paradigm;self-replication;simulation;synergy;system dynamics;unfolding (dsp implementation)	Allyson M. Beall;Andrew Ford	2010	IJISSC	10.4018/jissc.2010040105	ecosystem management;model building;knowledge management;environmental resource management;simulation modeling;management science;system dynamics;ecology	HCI	-66.83375809416319	4.336469001285178	1353
393b95911c03f3b5d169667c60d521934941e3e0	optimal admission control strategies for next-gen wireless heterogeneous networks.	heterogeneous network;admission control			Jue Wang;Mansoor Alam;Badlishah bin Ahmad	2010			heterogeneous network;computer science;computer network	Robotics	-10.379609256875318	92.76597314098677	1355
c66c48611048c5824cff42c10275aaba34ba8838	an interactive source commenter for prolog programs	source code	Prolog meta-circular interpreters, i.e., interpreters for Prolog written in Prolog, perform at least two operations on an object program - they parse it and execute its instructions. There is a useful variant of the meta-circular interpreter, the meta-circular parser, which as its name suggests, parses an object program without executing its instructions. The value of such a parser is that it provides an elegant means to modify Prolog source code. As the object program is parsed, new information in the form of additional instructions, comments, etc., can be selectively inserted. The Prolog source code commenter we describe is a meta-circular parser with facilities added to allow a user to interactively enter comments. As a Prolog program is parsed into its basic components, the user is allowed to view that component and enter an appropriate comment. The result is a new fully commented (and formatted) source program.	executable;interactivity;parsing;prolog	David Roach;Hal Berghel;John R. Talburt	1990		10.1145/97426.98002	computer science;theoretical computer science;database;programming language;source code	PL	-27.884690549213598	26.160490136056904	1373
a82efdd0f33d9a9405e5897af198e3c207c974c1	a domain ontology approach in the etl process of data warehousing	databases;etl process;owl;data integrity;data warehouse metadata domain ontology etl process extract transform loading tool data structure semantic heterogeneity enterprise information system web ontology language owl;semantics;extract transform loading tool;data mining;ontologies artificial intelligence;etl;knowledge representation languages;ontologies data warehouses data models databases data mining semantics business;semantic heterogeneity;hospital data warehouse;business data processing;web ontology language;enterprise information system;business;data transformation;data warehousing;ontologies;hospital data warehouse domain ontology etl;tool integration;data warehouses;data warehouse;ontologies artificial intelligence business data processing data warehouses knowledge representation languages;domain ontology;data structure;data warehouse metadata;extract transform load;data models	Extract-Transform-Loading (ETL) tools integrate data from source side to target in building data warehouse. However data structure and semantic heterogeneity exits widely in the enterprise information systems. On the purpose of eliminate data heterogeneity so as to construct data warehouse, this paper introduces domain ontology into ETL process of finding the data sources, defining the rules of data transformation, and eliminating the heterogeneity. In this method, the domain ontology is embedded in the metadata of the data warehouse. Hence, the data record could be mapped from data bases to ontology classes of Web Ontology Language (OWL). As result, the accessing of information resources could be done more efficiently. The method is testing in a hospital data warehouse project, and the result shows that ontology method plays an important role in the process of data integration by providing common descriptions of the concepts and relationships of data items, and medical domain ontology in the ETL process is of practical feasibility.	artificial intelligence;data structure;data-intensive computing;database;database schema;documentation;embedded system;enterprise information system;entity–relationship model;ontology (information science);row (database);semantic heterogeneity;text mining;web ontology language	Lihong Jiang;Hongming Cai;Boyi Xu	2010	2010 IEEE 7th International Conference on E-Business Engineering	10.1109/ICEBE.2010.36	upper ontology;bibliographic ontology;data transformation;computer science;ontology;data integration;data warehouse;data mining;database;ontology-based data integration;web ontology language;owl-s;information retrieval;process ontology;data mapping;suggested upper merged ontology	DB	-34.932024976392896	10.159034506434583	1375
5a075e4dea12deabd0b769dc2503e6837c30dbc6	implementing (nondeterministic) parallel assignments	optimisation;compilateur;multiple assignment;compilers;algorithme parallele;compiler optimization;parallel;nondeterministic;optimization;program compilers;algoritmo optimo;algorithme optimal;optimal algorithm;parallel algorithms	We present an algorithm that implements a parallel assignment as an optimal sequence of single assignments on a serial computer. The algorithm is optimal in the sense that it generates the minimum number of single assignments. Further, if the CPU is capable of executing multiple instruction threads, the algorithm can generate the minimal sequence that takes advantage of them.	assignment (computer science);nondeterministic finite automaton	Piercarlo Grandi	1996	Inf. Process. Lett.	10.1016/0020-0190(96)00053-1	compiler;parallel computing;computer science;theoretical computer science;parallel;optimizing compiler;parallel algorithm;programming language;nondeterministic algorithm;algorithm	DB	-14.357513186155222	42.29446600261909	1383
510b84775ed39ac63c7bb821b66795fa916d136b	ranking-based optimal resource allocation in peer-to-peer networks	p2p system;communications society;protocols;optimal resource allocation;theoretical framework;information systems;history;peer to peer network;resource allocation;resource management;utility function;differentiated service;usa councils;resource allocation peer to peer computing;harsanyi type social welfare functions;free riders;ip networks;computer science;social welfare function;max min fairness;peer to peer computing;ranking based optimal resource allocation;private information;resource management peer to peer computing history admission control communications society computer science information systems usa councils ip networks protocols;peer to peer networks;harsanyi type social welfare functions ranking based optimal resource allocation peer to peer networks admission control free riders suspicious attackers;suspicious attackers;admission control	This paper presents a theoretic framework of optimal resource allocation and admission control for peer-to-peer networks. Peer's behavioral rankings are incorporated into the resource allocation and admission control to provide differentiated services and even to block peers with bad rankings. These peers may be free-riders or suspicious attackers. A peer improves her ranking by contributing resources to the P2P system or deteriorates her ranking by consuming services. Therefore, the ranking-based resource allocation provides necessary incentives for peers to contribute their resources to the P2P systems. We define a utility function which captures the best wish for the source peer to serve competing peers, who request services from the source peer. Although the utility function is convex, Harsanyi-type social welfare functions are devised to obtain a unique optimal resource allocation that achieves max-min fairness. The parameters used in our model can be derived from the nature of the services or chosen by the source peer. No private information is required to reveal from individual peers. This prevents selfish peers to play the system strategically and cheat the resource allocation mechanism for their own benefits. The resource allocation and admission control are fully distributed and linearly scalable.	circa;differentiated services;digraphs and trigraphs;fairness measure;max-min fairness;maxima and minima;multistage interconnection networks;peer-to-peer;personally identifiable information;quality of service;scalability;scheduling (computing);stationary process;theory;utility	Yonghe Yan;Adel El-Atawy;Ehab Al-Shaer	2007	IEEE INFOCOM 2007 - 26th IEEE International Conference on Computer Communications	10.1109/INFCOM.2007.132	free rider problem;communications protocol;private information retrieval;max-min fairness;differentiated service;resource allocation;computer science;resource management;distributed computing;computer security;information system;computer network	Metrics	-25.43758105314824	73.57672274696294	1386
f7cd6eb95b29f296d2f9df1734abd3dea4e9fdda	electrical steering of vehicles - fault-tolerant analysis and design	tolerancia falta;sistema activo;analisis sistema;fault tolerant;controle actif;reconfigurable architectures;analyse fonctionnelle;analysis and design;systeme actif;active system;fault tolerant control;fault tolerant system;detection defaut;functional analysis;control architecture;fault tolerance;defaillance;steering by wire;sistema tolerando faltas;system analysis;systeme tolerant les pannes;analyse systeme;failures;deteccion imperfeccion;fallo;architecture reconfigurable;tolerance faute;control activo;defect detection;active control;analisis funcional	The topic of this paper is systems that need be designed such that no single fault can cause failure at the overall level. A methodology is presented for analysis and design of fault-tolerant architectures, where diagnosis and autonomous reconfiguration can replace high cost triple redundancy solutions and still meet strict requirements to functional safety. The paper applies graph-based analysis of functional system structure to find a novel fault-tolerant architecture for an electrical steering where a dedicated AC-motor design and cheap voltage measurements ensure ability to detect all relevant faults. The paper shows how active control reconfiguration can accommodate all critical faults and the fault-tolerant abilities are demonstrated on a warehouse truck hardware.	fault tolerance	Mogens Blanke;Jesper Sandberg Thomsen	2006	Microelectronics Reliability	10.1016/j.microrel.2006.07.005	functional analysis;control engineering;reliability engineering;embedded system;fault tolerance;electronic engineering;engineering;control reconfiguration;mathematics	EDA	-35.35448771499289	36.37010167040645	1395
24da7d932a78e1dabacfb606ab0d1bf9a148d25c	qos-aware deployment of network of virtual appliances across multiple clouds	optimal solution;virtual machine;data communication cost qos aware deployment virtual appliance multiple clouds cloud computing ondemand access internet forward checking based backtracking fcbb genetic based approach quality of service reliability;availability;virtual appliance;home appliances;forward checking;home appliances quality of service servers monitoring cloud computing availability;data communication;service level agreements sla;service level agreements sla cloud computing virtual appliance quality of service;servers;low latency;monitoring;virtual machines;virtual machines cloud computing genetic algorithms quality of service;genetic algorithms;service level agreement;quality of service;cloud computing	Cloud computing paradigm allows on-demand access to computing and storages services over the Internet. To solve the complexity of application deployment in Cloud infrastructure, virtual appliances, pre-configured, ready-to-run applications are emerging as a breakthrough technology. However, an automated approach for deploying network of appliances is required to guarantee minimum deployment cost, low latency, and high reliability. In this paper, we propose and compare two different deployment approaches: Forward-checking-based backtracking (FCBB) and genetic-based. They take into account Quality of Service (QoS) criteria such as reliability, data communication cost, and latency between multiple Clouds to choose the most appropriate combination of virtual machines and appliances. We evaluate our approach using a real case study and different request types. Experimental results show both algorithms reach near optimal solution. Further, we investigate effects of factors such as latency requirements, and data communication between appliances on the performance of the algorithms and placement of appliances across multiple Clouds.	backtracking;cloud computing;experiment;internet;look-ahead (backtracking);loss function;optimization problem;programming paradigm;quality of service;requirement;run time (program lifecycle phase);selection algorithm;service-level agreement;software deployment;virtual appliance;virtual machine	Amir Vahid Dastjerdi;Saurabh Kumar Garg;Rajkumar Buyya	2011	2011 IEEE Third International Conference on Cloud Computing Technology and Science	10.1109/CloudCom.2011.62	embedded system;real-time computing;computer science;virtual machine;operating system;computer network	HPC	-20.61035541618796	64.21708582041751	1398
a7f3c95a3aa8264be2bdb7451a842c6cf2329d31	model-driven application development enabling information integration		Interoperability is the capability of different software systems to exchange data via a common set of exchange formats. Interoperability between two products is often developed post-facto, due to insufficient adherence to standardization during the software design process. In this paper we present a mechanism that enables the dynamic communication of different software systems at database level, based on the principles of the Enterprise Information Integration architectural framework. The mechanism is built on the top of a database schema (meta-model) and extends the framework we elaborate on for the dynamic development and deployment of web-based applications.	database schema;enterprise architecture framework;interoperability;metamodeling;model-driven integration;software deployment;software design;software system;web application	Georgios Voulalas;Georgios Evangelidis	2011			system integration	SE	-55.56560194274621	15.53162417190828	1400
438bc62e1bd3dc01a6a2caee1fd0033bc71b21f8	1-minimal resolution principle based on lattice-valued propositional logic lp(x)	lattices abstracts;process algebra inference mechanisms;minimal resolution group automated reasoning lattice valued propositional logic lp x minimal resolution principle;α minimal resolution principle α n t ary resolution automated reasoning dynamic automated reasoning system lattice algebraic structure lia lattice implication algebras α resolution based automated reasoning schemes resolution based automated reasoning lattice valued propositional logic	Based on the academic ideas of resolution-based automated reasoning and the previously established research work on binary a-resolution based automated reasoning schemes in the framework of lattice-valued logic with truth-values in a lattice algebraic structure - lattice implication algebras (LIA), this paper is focused on investigating α-n(t)-ary resolution based dynamic automated reasoning system based on lattice-valued propositional logic LP(X) based in LIA. One of key issues for α-n(t) ary resolution automated reasoning for LP(X) is how to choose generalized literals. In this paper, the definition of α-minimal resolution principle which determines how to choose generalized literals in LP(X) is introduced. Then, its soundness and completeness are proved. These results lay the foundation for research of α-n(t) ary resolution automated reasoning.	automated reasoning;iso/iec 10967;linear algebra;propositional calculus;reasoning system;resolution (logic)	Hairui Jia;Yang Xu;Yi Liu;Huicong He	2013	2013 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2013.6890877	discrete mathematics;resolution;pure mathematics;mathematics;propositional variable;algorithm	AI	-12.042128699195642	13.803992312188392	1411
ebe89438a878cee1f579bdcc9fae8f83c6cc141b	performance modelling and resource allocation of the emerging network architectures for future internet			future internet	Wang Miao	2017			the internet;network architecture;management science;computer science;resource allocation	Metrics	-16.721111290259035	87.38439218403165	1413
881a9a0f9ec8099b266fff23a72d9a268db90c4c	communicating clauses: towards synchronous communication in contextual logic programming	operational semantics;object oriented;synchronous communication	Communicating clauses are proposed as an extension to contextual logic programming aiming at specifying the synchronous communication between agents, described here as units. The expressiveness of the extended framework is argued through the coding of producer/consumer schemes and several applications combining the logic and object-oriented styles of programming. Operational and declarative semantics are designed for the new framework. The operational semantics rests on a derivation relation stating how agents can be evaluated under contextual and synchronization constraints. The declarative semantics extend the classical model and xed point theory to take these constraints into account. As suggested, an eeort has been made to keep these semantics in the main streams of logic programming semantics. However, the contextual and synchronization constraints raise new problems for which fresh solutions are proposed.	communicating sequential processes;formal grammar;logic programming;operational semantics	Jean-Marie Jacquet;Luís Monteiro	1992			multimodal logic;theoretical computer science;horn clause;asynchronous communication;programming language;logic programming;object-oriented programming;operational semantics;computer science;prolog	AI	-24.59092578805134	20.136450986368747	1415
39f1e87984eb977a293adcc04b232ef1f294f9bd	the cost of errors in software development: evidence from industry	error detection and correction;empirical study;data collection;information technology;software development;software errors and reliability;economics of information technology	The search for and correction of errors in software are often time consuming and expensive components of the total cost of software development. The current research investigates to what extent these costs of software error detection and correction contribute to the total cost of software. We initiated the research reported here with the collection of a sample of transactions recording progress on one phase of development of a set of software programs. Each of these projects represented the completion of an identical phase of development (i.e., country localisation) for a different country. This enabled each project to be compared with the other, and provided an unusually high degree of control over the data collection and analysis in real-world empirical study. The research findings relied on programmers’ self-assessment of the severity of errors discovered. It found that serious errors have less influence on total cost than errors that were classified as less serious but which occurred with more frequency once these less serious errors are actually resolved and corrected. The research suggests one explanation – that programmers have greater discretion in how and when to resolve these less severe errors. The data supports the hypothesis that errors generate significant software development costs if their resolution requires system redesign. Within the context of the research, it was concluded that uncorrected errors become exponentially more costly with each phase in which they are unresolved, which is consistent with earlier findings in the literature. The research also found that the number of days that a project is open is a log-linear predictor of the number of software errors that will be discovered, implying a bias in error discovery over time. This implies that testing results need to be interpreted in light of the length of testing, and that in practice, tests should take place both before and after systems release. 2001 Elsevier Science Inc. All	error detection and correction;kerrison predictor;language localisation;log-linear model;programmer;software bug;software development	J. Christopher Westland	2002	Journal of Systems and Software	10.1016/S0164-1212(01)00130-3	simulation;error detection and correction;computer science;engineering;software development;data mining;empirical research;management;information technology;software metric;statistics;software quality analyst;data collection	SE	-66.50423977203837	32.35747931382218	1416
fd973d755fe1e76c12a3e815c27558bd8c4c9454	broadband satellite communications for internet access	satellite communication;internet access	Broadband Satellite Communications for Internet Access is a systems engineering methodology for satellite communication networks. It discusses the implementation of Internet applications that involve network design issues usually addressed in standard organizations. Various protocols for IP- and ATM-based networks are examined and a comparative performance evaluation of different alternatives is described. This methodology can be applied to similar evaluations over any other transport medium.	communications satellite;internet access	Sastri L. Kota;Kaveh Pahlavan;Pentti A. Leppänen	2004		10.1007/978-1-4419-8895-9	internet backbone;the internet;internet access;telecommunications;internet transit;engineering;dial-up internet access;satellite internet access;internet connection sharing;computer security;internet traffic engineering;computer network	ECom	-18.25696432055722	91.50156115019196	1418
46ae6d8152605113bd5b23232e2dbe412d2df989	towards a cost-optimized cloud application placement tool	analytical models;measurement;pricing;biological system modeling;computational modeling;cloud computing;data models	The cloud computing ecosystem comprises hundreds of providers, offering diverse computing services, incompatible APIs, and significantly different pricing models. Cloud application management platforms hide the heterogeneity of the services and APIs, allowing, to varying degrees, portability between providers. These tools remove technical barriers to switching providers, but they do not provide a mechanism for evaluating the cost effectiveness of switching. This paper presents a decision support system, working within cloud application management platforms, that evaluates the costs of a customer's applications using resources from different cloud service providers. The system (1) generalizes and normalizes multiple cloud pricing models and (2) gathers pricing data from cloud providers. These, in conjunction with the application resource consumption model, allow the cloud pricing module to estimate a price for the application for each cloud provider. To demonstrate this, our cloud pricing module has been inte-grated with the SlipStream multi-cloud application management platform, allowing its users to optimize their choice of provider(s).	application lifecycle management;cloud computing;decision support system;ecosystem;software as a service	Olivier Belli;Charles Loomis;Nabil Abdennadher	2016	2016 IEEE International Conference on Cloud Computing Technology and Science (CloudCom)	10.1109/CloudCom.2016.0022	pricing;cloud computing security;data modeling;simulation;cloud computing;computer science;operating system;cloud testing;computational model;measurement	HPC	-25.58038465982453	59.89200827377481	1420
9f2aa9d65f59a1572f95a4fa744b0ee86b6dbb44	nat-muitilinguai: tools for muitilinguai interfaces in databases			database;network address translation	José Coch di Yacovo;Gustavo Crispino;Diana Cukierman;Geneviève Morize;Dina Wonsever	1991			database;nat;computer science	DB	-96.49346805598489	31.231556372152482	1421
6f3e674810c847602fcea1299b21be2193e55e63	[demo abstract] sound and soundness: practical total functional data-flow programming	stream programming;sound synthesis;real time;data flow	The field of declarative data-stream programming (discrete time, clocked synchronous, compositional, data-centric) is divided between the visual data-flow graph paradigm favored by domain experts, the functional reactive paradigm favored by academics, and the synchronous paradigm favored by developers of low-level systems. Each approach has its particular theoretical and practical merits and target audience. The programming language Sig has been designed to unify the underlying paradigms in a novel way. The natural expressivity of visual approaches is combined with the support for concise pattern-based symbolic computation of functional programming, and the rigorous, elementary semantical foundation of synchronous approaches. Here we demonstrate the current state of implementation of the Sig system by means of example programs that realize typical components of digital sound synthesis.	clock rate;compositional data;data-flow analysis;dataflow;declarative programming;functional programming;high- and low-level;programming language;programming paradigm;signature block;stream processing;symbolic computation	Baltasar Trancón y Widemann;Markus Lepper	2014		10.1145/2633638.2633644	declarative programming;programming domain;reactive programming;functional reactive programming;computer science;theoretical computer science;functional logic programming;programming paradigm;procedural programming;symbolic programming;inductive programming;algorithm	PL	-22.759898752590097	23.082478434932803	1427
95fff74fd944717daf2570d0da114f4bd112c180	long-range monitoring system of irrigated area based on multi-agent and gsm	real time;collective memory;network control;monitoring system;long range	In order to improve irrigated automatic degree of irrigated area, and make the control center and telemeter station to intelligent complete the task, structures the control center and telemeter station Agent union by instruct Multi-Agent theory, and make use of GSM network to finish the communication between the control center and telemeter station, at last make up the long-range monitoring system of irrigated area based on Multi-Agent and GSM. In this system, telemeter station finishes its functions, such as collection, memory, dealing with of the data, and while to carry control out according to the treated data, send the data that the control center needed to the control center Agent union through GSM network; control center Agent union store and deal with the accepted data information from telemeter station at first, then send the treated data to telemeter station Agent union to carry out new regulation and running. The setting-up of this system, not only make the work of the control center and telemeter station have higher intelligent, but also make the communication amount reduce greatly, communication is swifter and highefficient, which make the real-time character of long-range controls improve	real-time transcription	Tinghong Zhao;Zibin Man;Xueyi Qi	2007		10.1007/978-0-387-77251-6_56	embedded system;real-time computing;telecommunications	AI	-29.338123335071725	18.324374060322555	1430
2b0519cb11581c3513564b7d9fe172e6ed8b7159	predicting high-risk program modules by selecting the right software measurements	imbalanced data;wrapper based feature ranking;feature selection;performance metrics;data sampling;software quality classification	A timely detection of high-risk program modules in high-assurance software is critical for avoiding the high consequences of operational failures. While software risk can initiate from external sources, such as management or outsourcing, software quality is adversely affected when internal software risks are realized, such as improper practice of standard software processes or lack of a defined software quality infrastructure. Practitioners employ various techniques to identify and rectify high-risk or low-quality program modules. Effectiveness of detecting such modules is affected by the software measurements used, making feature selection an important step during software quality prediction. We use a wrapper-based feature ranking technique to select the optimal set of software metrics to build defect prediction models. We also address the adverse effects of class imbalance (very few low-quality modules compared to high-quality modules), a practical problem observed in high-assurance systems. Applying a data sampling technique followed by feature selection is a relatively unique contribution of our work. We present a comprehensive investigation on the impact of data sampling followed by attribute selection on the defect predictors built with imbalanced data. The case study data are obtained from several real-world high-assurance software projects. The key results are that attribute selection is more efficient when applied after data sampling, and defect prediction performance generally improves after applying data sampling and feature selection.	feature selection;outsourcing;rectifier;sampling (signal processing);sensor;software bug;software metric;software quality	Kehan Gao;Taghi M. Khoshgoftaar;Naeem Seliya	2011	Software Quality Journal	10.1007/s11219-011-9132-0	reliability engineering;software sizing;computer science;software reliability testing;machine learning;data mining;feature selection;software regression;software metric;software quality analyst	SE	-64.8305558627786	32.617330563614885	1432
fc472ed4bd03db42759f6f84db814502223aefb1	reflections on lms: exploring front-end alternatives	multi stage programming;domain specific languages;intermediate representation	Metaprogramming techniques to generate code at runtime in a general-purpose meta-language have seen a surge of interest in recent years, driven by the widening performance gap between high-level languages and emerging hardware platforms. In the context of Scala, the LMS (Lightweight Modular Staging) framework has contributed to ``abstraction without regret''--high-level programming without performance penalty--in a number of challenging domains, through runtime code generation and embedded compiler pipelines based on stacks of DSLs. Based on this experience, this paper crystallizes some of the design decisions of LMS and discusses potential alternatives, which maintain the underlying spirit but differ in implementation choices: specifically, strategies for realizing more flexible front-end embeddings using type classes instead of higher-kinded types, and strategies for type-safe metaprogramming with untyped intermediate representations.	code generation (compiler);compiler;disk staging;embedded system;general-purpose markup language;high- and low-level;kind (type theory);metaprogramming;pipeline (computing);reflection (computer graphics);run time (program lifecycle phase);scala;self-modifying code;type class;type safety	Tiark Rompf	2016		10.1145/2998392.2998399	real-time computing;computer science;theoretical computer science;algorithm	PL	-28.219526358967194	29.01190192396246	1440
44ff5d97cff7ccc2b687dc219c28cc4aea8c92b5	ontology knowledge spaces for semantic collaboration in networked enterprises	busqueda informacion;search and retrieval;ontologie;empresa numerica;red semantica;availability;disponibilidad;information retrieval;semantic network;service web;semantics;web service;semantica;semantique;conceptual framework;reseau semantique;firm cooperation;digital enterprise;recherche information;ontologia;coordinacion;cooperation entreprise;entreprise numerique;semantic relations;disponibilite;ontology;cooperacion empresa;servicio web;coordination	In this paper, we define a reference conceptual framework to organize ontology knowledge spaces and related semantic collaboration schemes for coordinated and virtual access to heterogeneous and distributed information resources inside and outside the enterprise, at both intraand inter-enterprise level under different collaboration requirements and goals. The framework exploits ontology knowledge spaces and enabling services for searching and retrieving the relevant information resources, namely those semantically related to a target request, both in a stable and emergent collaboration scenarios.	emergence;knowledge space;requirement;spaces	Silvana Castano;Alfio Ferrara;Stefano Montanelli	2006		10.1007/11837862_32	upper ontology;web service;availability;computer science;knowledge management;ontology;ontology;data mining;conceptual framework;database;semantics;semantic network;world wide web	Web+IR	-38.968232587342975	13.265507739409733	1441
c23a44b32cb2f5ceb2cca1e7209efca320e82828	a system architecture for cloud of sensors		The Cloud of Sensors (CoS) paradigm has emerged from the broader concept of Cloud of Things. CoS infrastructures have the potential to leverage the benefits of both Clouds and wireless sensor and actuator networks (WSAN). As traditional Cloud platforms, CoS are built on the virtualization principles. A major challenge in the development of CoS virtualization models is energy consumption, as sensors are well-known to be energy constrained. We claim that the design decisions for the specification of software artifacts composing a CoS must consider the energy efficiency as a primary requirement. Edge computing has recently raised as a paradigm to leverage computing and networking capabilities at the network edge thus, allowing a more decentralized design. In this paper, we investigate a new model that integrates cloud, edge, and WSANs in a CoS infrastructure and propose a novel three-tier system architecture. We describe an archetype that supports implementing a three-tier CoS system, fostering energy efficiency and sustainability. We also discuss issues on implementing the CoS as a real software system, running on top of FIWARE.	edge computing;multitier architecture;programming paradigm;sensor;software system;systems architecture;x86 virtualization;xojo	Igor Leão dos Santos;Marcelo Pitanga Alves;Flávia Coimbra Delicato;Paulo F. Pires;Luci Pirmez;Wei Li;Albert Y. Zomaya;Samee Ullah Khan	2018	2018 IEEE 16th Intl Conf on Dependable, Autonomic and Secure Computing, 16th Intl Conf on Pervasive Intelligence and Computing, 4th Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)	10.1109/DASC/PiCom/DataCom/CyberSciTec.2018.00118	edge device;software system;virtualization;wireless sensor network;systems architecture;software;cloud computing;edge computing;distributed computing;computer science	Embedded	-43.74315137295488	46.90625269090282	1443
6bd9a9b6ac3e7e0a5a4293ded531ebe379fd6a03	cut elimination for classical bilinear logic	cut elimination	In this paper a cut elimination theorem is proved for classical non-commutative linear logic without exponentials, presented as a dual Schutte style deductive system. The notion of equality between deductions is sketched and they are interpreted as relations, in the spirit of the formulas as types paradigm.#R##N##R##N#(The author acknowledges support from the Social Sciences and Humanities Research Council of Canada.)	bilinear transform	Joachim Lambek	1995	Fundam. Inform.	10.3233/FI-1995-22123	discrete mathematics;computer science;artificial intelligence;mathematics;algorithm;algebra	AI	-9.709402108357349	11.589380043693147	1453
e8576752edc9594177280a2314ea2ce169dcbc3f	a logical approach to data structures	version and configuration control;programming environment;group awareness;incremental merge;software development;teamware;cscw;data structure	The Galois project at the University of Texas is building a programming environment that supports the formal development and verification of data structure programs. This programming environment supports features such as pointer manipulation and destructive update that often make formal treatment difficult.	formal methods;integrated development environment;persistent data structure;pointer (computer programming)	Russell Turpin	1993		10.1145/256428.167073	data structure;reactive programming;computer science;theoretical computer science;software development;software engineering;computer-supported cooperative work;database;inductive programming;programming language	SE	-23.26442714640917	21.173811965328078	1455
6d00234b789016bffac0f8b75f7a9235b10e131e	opensesame: unlocking smart phone through handshaking biometrics	human computer interaction;smart phones accelerometers intelligent sensors magnetic sensors shape sensor phenomena and characterization;support vector machines;biometrics access control;authorisation;smart phones;accelerameter smart phone security privacy authentication;user interfaces authorisation biometrics access control human computer interaction mobile computing pattern classification smart phones support vector machines;mobile phones opensesame smart phone unlocking handshaking biometrics screen locking screen unlocking unintentional operation avoidance personal stuff security security deficiency high cost poor usability handshaking actions shaking patterns support vector machine svm authentication accelerometer;pattern classification;mobile computing;user interfaces	Screen locking/unlocking is important for modern smart phones to avoid the unintentional operations and secure the personal stuff. Once the phone is locked, the user should take a specific action or provide some secret information to unlock the phone. Existing approaches do not support smart phones well due to the deficiency of security, high cost, and poor usability. We collect 200 users' handshaking actions with their smart phones and discover an appealing observation: the shaking pattern of a person is kind of unique, stable and distinguishable. In this paper, we propose OpenSesame, which employs the users' shaking patterns for locking/unlocking. The key feature of our system lies in using four fine-grained and statistic features of handshaking to verify users. Moreover, we utilize support vector machine (SVM) for accurate classification. Results from comprehensive experiments show that our technique is robust compatible across different brands of smart phones, without the need of any specialized hardware.	angular defect;authentication;authorization;biometrics;experiment;handshaking;high-level programming language;lock (computer science);sim lock;smartphone;support vector machine;usability;vii	Yi Guo;Lei Yang;Xuan Ding;Jinsong Han;Yunhao Liu	2013	2013 Proceedings IEEE INFOCOM	10.1109/INFCOM.2013.6566796	embedded system;support vector machine;computer science;operating system;authorization;internet privacy;user interface;mobile computing;computer security	Security	-50.79972961997008	63.89495665147135	1456
168a2a2321463d4130de12c0d9d9ca0062e5fc4e	tinysoa: a service-oriented architecture for wireless sensor networks	service orientation;wsn;soa;web service;sensor network;wireless sensor network;web services;internet application;precision agriculture;service oriented architecture	Wireless sensor networks provide the means for gathering vast amounts of data from physical phenomena, and as such they are being used for applications such as precision agriculture, habitat monitoring, and others. However, there is a need to provide higher level abstractions for the development of applications, since accessing the data from wireless sensor networks currently implies dealing with very low-level constructs. We propose TinySOA, a service- oriented architecture that allows programmers to access wireless sensor networks from their applications by using a simple service-oriented API via the language of their choice. We show an implementation of TinySOA and the results of an experiment where programmers developed an application that exemplifies how easy Internet applications can integrate sensor networks.	application programming interface;exemplification;habitat;high- and low-level;internet;middleware;programmer;programming language;real programmers don't use pascal;sensor web;service-orientation;service-oriented architecture;service-oriented device architecture	Edgardo Avilés-López;J. Antonio García-Macías	2009	Service Oriented Computing and Applications	10.1007/s11761-009-0043-x	web service;sensor web;embedded system;wireless wan;wireless sensor network;wireless site survey;computer science;wireless network;service-oriented architecture;distributed computing;key distribution in wireless sensor networks;municipal wireless network;wi-fi array;mobile wireless sensor network;law;computer network;visual sensor network	Mobile	-39.20801261467549	46.691257171146134	1466
36657a936c592aeb55f4a71987517626e16c1aac	propriétés de correction séquentielle dans un langage parallèle à mémoire partagée	en francais	Nous etudions une propriete de correction de programmes ecrits dans un langage parallele. Cette propriete est une equivalence semantique entre le programme parallele et sa version sequentielle, que nous definissons. Le langage que nous considerons, outre des structures sequentielles usuelles (boucles, branchements conditionnels), comporte des boucles paralleles et des synchronisations par evenements. L'objet principal de cette these est de demontrer un theoreme qui assure cette propriete de correction, sous un certain nombre d'hypotheses, principalement une condition de preservation des dependances de donnees. Ces hypotheses portent seulement sur la semantique de la version sequentielle : autrement dit, en vertu de notre resultat, verifier la correction d'un certain programme parallele se ramene a verifier un certain nombre de proprietes de sa seule version sequentielle. Par ailleurs, nous esquissons une extension de ce resultat, par l'introduction de sections critiques, envisageant alors une version affaiblie (c'est-a-dire generalisee) de notre propriete de correction. (Resume de l'auteur).	bibliothèque de l'école des chartes	Gilbert Caplain	1998			epistemology;philosophy	NLP	-105.61354433279686	13.325671156341024	1468
73b3cc74fc2ef0d286a5c08dde11776789b50c7c	a new privacy homomorphism and applications	safety security in digital systems;confidencialidad;statistical confidentiality;analisis datos;securite;confidentiality;homomorphism;confidentialite;data analysis;privacy homomorphisms;statistical computing;digital systems;safety;homomorphisme;arithmetique numerique;analyse donnee;digital arithmetic;homomorfismo;seguridad	An additive and multiplicative privacy homomorphism is an encryption function mapping addition and multiplication of cleartext data into two operations on encrypted data. One such privacy homomorphism is introduced which has the novel property of seeming secure against a known-cleartext attack. An application to multilevel statistical computation is presented, namely classified retrieval of exact statistics from unclassified computation on disclosure-protected (perturbed) data.	computation;homomorphic encryption;known-key distinguishing attack;plaintext;utility functions on indivisible goods	Josep Domingo-Ferrer	1996	Inf. Process. Lett.	10.1016/S0020-0190(96)00170-6	homomorphism;combinatorics;discrete mathematics;confidentiality;computer science;theoretical computer science;mathematics;data analysis;computer security;computational statistics;algorithm	Security	-40.70369766620394	80.57278023759496	1469
9111ede2f6876148555055688168aa4b16eb501e	international expansion of wireless telecommunications service providers: a comparative analysis	geographic expansion strategy;vodafone;comparative analysis;service provider;america movil;wireless telecommunications service providers;business communication;singapore telecommunications;deutsche telekom;telecommunication services business communication international trade;wireless telecommunications business portfolios;telecommunication services;telefonica;profitability;telecommunication services companies portfolios profitability risk analysis performance analysis computational intelligence society privatization globalization economies of scale;wireless telecommunications business portfolios wireless telecommunications service providers geographic expansion strategy telefonica deutsche telekom singapore telecommunications vodafone america movil;international trade	Facing increasing levels of competition and regulation in their maturing domestic markets some prominent telecommunications service providers (TSPs) have begun to expand their wireless operations internationally. This paper investigates the geographic expansion strategies of five major TSPs: Telefonica, Deutsche Telekom, Singapore Telecommunications, Vodafone, and America Movil. All of these companies have invested heavily in operations abroad, in some cases in the same countries. Using a proportionate customer weighting scheme the paper analyzes the impact of the TSPs' expansion strategies on the companies' performance from 2002 to 2005 in terms of the growth, profitability, and risk characteristics of their wireless telecommunications business portfolios.	qualitative comparative analysis	Steven R. Powell	2007	2007 Wireless Telecommunications Symposium	10.1109/WTS.2007.4563339	marketing;business;commerce	Mobile	-86.77450095494426	7.870801853543956	1473
4b2bd38b7b15bf55fce9756b442706f905ea1f04	particle simulation on heterogeneous distributed supercomputers	engineering;computers;optical network;general and miscellaneous mathematics computing and information science;high speed networks;fluid flow;fluid mechanics;hypersonic flow;mathematical logic;three dimensional;computer networks;computer network;mathematics computers;simulation 420400 engineering heat transfer fluid flow;mechanics;computerized simulation;algorithms;ames research center;high performance;programming;parallel processing;cray computers	We describe the implementation and performance of a three dimensional particle simulation distributed between a Thinking Machines CM-2 and a Cray Y-MP. These are connected by a combination of two high-speed networks: a high-performance parallel interface (HIPPI) and an optical network (UltraNet). This is the first application to use this configuration at NASA Ames Research Center. We describe our experience implementing and using the application and report the results of several timing measurements. We show that the distribution of applications across disparate supercomputing platforms is feasible and has reasonable performance. In addition, several practical aspects of the computing environment are discussed.	simulation;supercomputer	Jeffrey C. Becker;Leonardo Dagum	1993	Concurrency - Practice and Experience	10.1002/cpe.4330050411	three-dimensional space;computational science;parallel processing;programming;mathematical logic;parallel computing;computer science;theoretical computer science;operating system;distributed computing;fluid mechanics	HPC	-7.645658831451178	38.4860418562536	1475
2ce16bc1791a76b7bee325e85482e68adfaa06c4	eskimo 2-way handshake	eskimo 2 way handshake eskimo esmac ieee 4 way handshake;scheduling time eskimo 2 way handshake security issues ieee 4 way handshake key recovery attack hackers secret key retrieval network access serious security vulnerability mitigation encryption system with keyed integrity and managed oracle strong cryptographic message authentication code algorithm strong cryptographic mac algorithm esmac eskimo mac;cryptography;wireless lan cryptography;wireless lan;authentication ieee 802 11 standards encryption microwave integrated circuits wireless lan	Recent research has shown that there are many security issues associated with the original IEEE 4-Way Handshake. The most serious of these security issues is the key recovery attack, during which hackers are able to retrieve the secret key and use it to access a targeted network. In order to mitigate this serious security vulnerability, this research analysed the IEEE 4-Way Handshake and identified the source of the problem. Similarly, the research reviewed some proposed ideas on how to fix this very problem, but concluded that none of the proposed approaches is secure and efficient. Finally, this research proposes the new ESKIMO (Encryption System with Keyed Integrity and Managed Oracle) 2-Way Handshake, which incorporates a strong cryptographic Message Authentication Code (MAC) algorithm called ESMAC (ESKIMO MAC). This research argues that the ESKIMO 2-Way Handshake is secure and lightweight. It provides better security and efficiency, and also cuts down the original IEEE 4-Way Handshake scheduling time by at least fifty percent.	algorithm;cryptography;encryption;key (cryptography);key escrow;key-recovery attack;message authentication code;scheduling (computing);vulnerability (computing)	Idris Ahmed;Anne James	2013	2013 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2013.6637211	telecommunications;computer science;cryptography;mathematics;computer security;statistics;computer network	Security	-45.96130817584033	75.40630823466647	1476
8e5894688094a8796ae3586f5d68b829b0147d7c	amos: schnelle manipulator-bewegungsplanung durch integration potentialfeldbasierter lokaler und probabilistischer gloabaler algorithmen		Es wird ein praktikables Verfahren zur Generierung kollisionsfreier Manipulator-Bahnen fur den echtzeitnahen Einsatz unter Berucksichtigung aller Freiheitsgrade und beliebiger Nutzlasten in realistisch komplexen Umweltszenarien beschrieben. Dabei wachst die mittlere Laufzeit des Verfahrens zum einen nur sublinear (logarithmisch) mit der Komplexitat des Umweltmodells und hangt zum anderen nicht kritisch (insbesondere nicht exponentiell) von der Anzahl der Freiheitsgrade ab. Das Verfahren stutzt sich im wesentlichen auf die Kombination der Effizienz eines lokalen, potentialfeldbasierten Planers mit der Vollstandigkeit eines globalen, probabilistischen Roadmap-Verfahrens sowie auf eine hierarchische geometrische Umweltreprasentation zur schnellen Berechnung euklidischer Distanzvektoren in realistisch komplexen, nicht-konvexen Arbeitsraumen. Wie Simulationsexperimente bestatigen, werden kollisionsfreie Bahnen fur sechsachsige Manipulatoren in komplexen Umweltmodellen auf Standardworkstations innerhalb weniger Sekunden generiert.	amos	Bernhard Braun;Ralf Corsépius	1996		10.1007/978-3-642-80324-6_14	manipulator;performance art;philosophy	Vision	-104.80092886763136	31.363644014728823	1478
eee096fbc6566d108018a270a22bbe0841e8e297	security and cloud computing: intercloud identity management infrastructure	intercloud identity management infrastructure;federated identity management;collaborative work;application software;intercloud identity management infrastructure cloud computing computational paradigm data security heterogeneous scenario federated scenario reference architecture authentication managemant;heterogeneous scenario;authentication;virtual machining;distributed computing;computational paradigm;identity management;intercloud;security cloud computing identity management systems authentication distributed computing privacy virtual machining application software service oriented architecture collaborative work;internet;data privacy;federation;open systems data privacy internet message authentication;federated scenario;identity management systems;saml cloud computing intercloud federation identity management security;message authentication;service oriented architecture;open systems;security;reference architecture;privacy;saml;authentication managemant;cloud computing;data security	Cloud Computing is becoming one of the most important topics in the IT world. Several challenges are being raised from the adoption of this computational paradigm including security, privacy, and federation. This paper aims to introduce new concepts in cloud computing and security, focusing on heterogeneous and federated scenarios. We present a reference architecture able to address the Identity Management (IdM) problem in the InterCloud context and show how it can be successfully applied to manage the authentication needed among clouds for the federation establishment.	authentication;cloud computing;federation (information technology);identity management;intercloud;privacy;programming paradigm;reference architecture	Antonio Celesti;Francesco Tusa;Massimo Villari;Antonio Puliafito	2010	2010 19th IEEE International Workshops on Enabling Technologies: Infrastructures for Collaborative Enterprises	10.1109/WETICE.2010.49	message authentication code;cloud computing security;reference architecture;application software;the internet;cloud computing;information privacy;computer science;information security;service-oriented architecture;authentication;data security;internet privacy;open system;privacy;world wide web;computer security;identity management	DB	-45.88887936800752	55.388865917192774	1489
f6e806f158aa3f5d42220d4079ab2b6c4056dcff	interoperability with distributed objects through java wrapper	application development;distributed system;prototyping system description language distributed objects java wrapper distributed systems development interoperability techniques interface wrapper model local objects specification language;software tools distributed programming open systems java distributed object management application program interfaces specification languages;specification language;java middleware data structures prototypes computer science software prototyping software systems relays standardization protocols;distributed objects;specification languages;distributed programming;application program interfaces;distributed object management;software tools;open systems;java	The mujur hurdle in developing distributed systems is the implementing the interoperability between the systems. Currently, nrost of the interoperability techniques require thut the dutu or services to be tightly coupled tu U particulur server. Furthermore. us nwst progranimers are trained in designing .stund-ulutie upplicution, developing distributed system proves tu be time-consuming und dijicult. This puper trddress the issues by creuting un interjuce wrupper model thut ullows develupers the features qf' treating distributed objects us local objects. A tool wus developed to generute Juvu interfuce wrupper from a spec(ficution lunguuge culled the Prototyping Systerri Description k n g u a g e .	distributed computing;distributed object;interoperability;java;server (computing)	Ngom Cheng;Valdis Berzins;Luqi;Swapan Bhattacharya	2000		10.1109/CMPSAC.2000.884770	interface description language;middleware;real-time computing;jsr 94;java concurrency;specification language;computer science;java modeling language;database;real time java;distributed object;distributed design patterns;open system;programming language;java;rapid application development;generics in java	DB	-34.014514845941264	42.820551711617135	1490
e3636c06bf8eb9f9cbb9fddcca827d0b60c0db8d	musikhören und arbeiten im einklang	research article			Wei-Chi Chien;Marc Hassenzahl;Kurt Mehnert	2011	i-com	10.1524/icom.2011.0037	psychology	EDA	-97.58776530836076	26.664252719504766	1491
2a90742ec5068c9176bbdb77ae4681d1c8d63c34	a cross-layer adaptive approach for performance and power optimization in stt-mram		Spin Transfer Torque Magnetic Random Access Memory (STT-MRAM) is a promising candidate as a universal on-chip memory technology due to non-volatility, high density and scalability. However, high write energy and latency are major challenges in this memory technology due to the asymmetry and stochastic nature of the write operation. Typically, the write current is set for the minimum energy point, which can further impact the write latency. To mitigate these issues, we propose an adaptive write current scaling technique that adjusts the write current, and hence the write latency and energy based on the performance needs at run-time. Using this technique, optimal energy and performance points for write current are obtained using detailed device and system level analysis. Furthermore, we use runtime adaptation of write current by predicting the write access rate for the next execution phase. We evaluate the efficiency of the proposed approach on SPEC2000 applications for STT-MRAM-based L1 and L2-cache levels. The results show that the effective write latency of L1 and L2 is reduced by 52.4% and 55.7% with 7.6% and 1.4% area overheads, respectively, corresponding to the overall system performance optimization of 15.5% while the total memory energy consumption is increasing by only 3.2%.	cpu cache;file system permissions;image scaling;magnetoresistive random-access memory;mathematical optimization;non-volatile memory;power optimization (eda);random access;scalability;stochastic process;volatility	Nour Sayed;Rajendra Bishnoi;Fabian Oboril;Mehdi Baradaran Tahoori	2018	2018 Design, Automation & Test in Europe Conference & Exhibition (DATE)	10.23919/DATE.2018.8342114	real-time computing;parallel computing;computer science;energy consumption;latency (engineering);overhead (business);scalability;magnetoresistive random-access memory;spin-transfer torque;random access;power optimization	EDA	-5.335913880977785	55.8173247485362	1493
57a2ea6ca8317471a6967fffaa2b94d463afd902	discovering re-usable design solutions in web conceptual schemas: metrics and methodology	developpement logiciel;modelizacion;patron conception;concepcion asistida;data intensive application;computer aided design;haute performance;hipertexto;red www;maintenance;validacion;reutilizacion;sintesis mecanismo;conceptual analysis;reseau web;distributed computing;patron concepcion;metric;synthese mecanisme;data mining;analisis conceptual;modeling language;reuse;recurrence;modelisation;conceptual schema;metamodel;internet;metamodele;fouille donnee;metamodelo;desarrollo logicial;recurrencia;design pattern;software development;conception assistee;alto rendimiento;mantenimiento;calculo repartido;architecture basee modele;world wide web;metrico;validation;information system;mechanism synthesis;analyse conceptuelle;modeling;high performance;busca dato;calcul reparti;hypertexte;model driven architecture;systeme information;hypertext;metrique;reutilisation;arquitectura basada modelo;sistema informacion	In the Internet era, the development of Web applications has impressively evolved and is characterized by a large degree of complexity. To this end, software community has proposed a variety of modeling methods and techniques. In this work, we provide a methodology and metrics for mining the conceptual schema of applications, to discover recurrent design solutions in an automatic manner. The mechanism is designed for models based on WebML, a modeling language for designing data-intensive applications. This approach, when applied in an applications conceptual schema, results in effective design solutions, as it facilitates reuse and consistency in the development and maintenance process. Furthermore, when applied to a large number of applications, it enables hypertext architects to identify templates for Web application frameworks for specific domains and to discover new design patterns extending the predefined set of patterns supported by WebML. Finally, we illustrate a validation scenario.	conceptual schema;data-intensive computing;design pattern;hypertext;modeling language;web application;webml	Yannis Panagis;Evangelos Sakkopoulos;Spiros Sirmakessis;Athanasios K. Tsakalidis;Giannis Tzimas	2005		10.1007/11531371_69	metamodeling;the internet;simulation;systems modeling;hypertext;metric;computer science;conceptual schema;artificial intelligence;software development;computer aided design;reuse;database;design pattern;modeling language;world wide web;information system	SE	-37.97197460198141	14.23939899639179	1506
ae0cf3b41af131ffbfb51d2cfc8073aec44c85be	a method for proving programming languages non context-free	programming language;context free			Stefan Sokolowski	1978	Inf. Process. Lett.	10.1016/0020-0190(78)90080-7	natural language processing;fourth-generation programming language;first-generation programming language;declarative programming;very high-level programming language;language primitive;programming domain;reactive programming;computer science;extensible programming;third-generation programming language;functional logic programming;programming paradigm;procedural programming;symbolic programming;inductive programming;fifth-generation programming language;programming language theory;programming language;second-generation programming language;high-level programming language;comparison of multi-paradigm programming languages;algorithm	PL	-24.349204183019705	22.127432486454197	1508
8b0974e215b7e8ac49225d63f870a56b764e7962	dna microarray manufacturing factory automation	dna;automated design;systematic investment dna microarray deposition factory automation automated fab gene chip micro fabrication technology fault detection error recovery industrial automation;chip;manufacturing automation dna manufacturing processes data mining fabrication pharmaceutical technology costs automatic control production facilities investments;factory automation;industrial automation;dna microarray;fault diagnosis factory automation biomolecular electronics dna;biomolecular electronics;fault diagnosis	This paper first reviews DNA microarray deposition technology. Different aspects of factory automation for DNA microarray manufacturing are then discussed. Problems, challenges and suggestions for investing factory automation, and design, implementing as well as managing automated fab for DNA microarray manufacturing are presented from an industrial automation viewpoint. The purpose is to propose an idea of systematic investment, development and management of factory automation for DNA microarray manufacturing.	automation;chemical vapor deposition;dna microarray;semiconductor fabrication plant	Mingjun Zhang;Karen Griswold;William Fisher;Tzyh Jong Tarn	2003		10.1109/ROBOT.2003.1242032	computer science;engineering;process automation system;automation;process engineering;totally integrated automation;manufacturing engineering	Robotics	-58.68178507514441	7.84131592196518	1526
e2a760cc2a10f14b641dcd279003bbb5999709ec	mosto: generating sparql executable mappings between ontologies	sparql executable mappings;data translation;information integration;semantic web;ontologies	Data translation is an integration task that aims at populating a target model with data of a source model, which is usually performed by means of mappings. To reduce costs, there are some techniques to automatically generate executable mappings in a given query language, which are executed using a query engine to perform the data translation task. Unfortunately, current approaches to automatically generate executable mappings are based on nested relational models, which cannot be straightforwardly applied to semantic-web ontologies due to some differences between both models. In this paper, we present Mosto, a tool to perform the data translation using automatically generated SPARQL executable mappings. In this demo, ER attendees will have an opportunity to test this automatic generation when performing the data translation task between two different versions of the DBpedia ontology.	dbpedia;executable;ontology (information science);open-source software;population;query language;sparql;semantic web	Carlos R. Rivero;Inma Hernández;David Ruiz;Rafael Corchuelo	2011		10.1007/978-3-642-24574-9_47	computer science;ontology;artificial intelligence;information integration;semantic web;data mining;database;programming language	DB	-35.69944901620801	5.105348416531339	1532
9f8625a550d75439f11d2cf3ffd379b537e077f1	evaluating electronic channels of distribution in the hotel sector: a delphi study.	information systems;hospitality industry;marketing;hotels;evaluation;delphi method	This article is based on the findings of the initial rounds of a Delphi study that focused on identifying a potential range of methods to help hotels both select and evaluate electronic channels of distribution. A review of the background to both electronic distribution in the industry and hotel distribution in particular is provided, highlighting important issues for hoteliers. The work demonstrates the need for and potential utility of a channel evaluation methodology. Construction of the Delphi and the selection process for participants is described along with key findings and interim conclusions.		Peter O'Connor;Andrew J. Frew	2000	J. of IT & Tourism		economics;hospitality industry;delphi method;marketing;operations management;evaluation;management science;information system	ECom	-85.30900200497767	4.358792337230218	1537
573a0f16a94c8c8ca323daab3ac1cded525eb052	markov analysis of multiple-disk prefetching for external mergesort			link prefetching;markov chain;merge sort	Vinay S. Pai;Alejandro A. Schäffer;Peter J. Varman	1992			parallel computing;distributed computing;merge sort;computer science;markov chain	Crypto	-10.485834734245193	43.55253408857205	1538
e69a59936992be32d4ef6dfe4de1df39c7357e89	knowledge representation with ontology and relational database to rdf converter	knowledge representation;relational database	In a process and apparatus for obtaining sterilized and bacteria-free water suitable for medicinal injections and other purposes, deionized water is supplied to a reverse osmosis filter, which is impervious to attack by bacteria and which is arranged such that its entire surface may be sterilized, to obtain a sterile permeate and a portion of the deionized water supplied to the filter is recycled. The recycled portion of deionized water may be recycled through the deionization plant from which it is derived or through a closed circuit and may be recycled in combination with the sterile permeate.	knowledge representation and reasoning;relational database	Daya Gupta;Kartar Jat	2008			database;data mining;rdf;relational database;computer science;reverse osmosis	AI	-32.6436662448059	9.121518963483862	1539
10a28405343cb32e6df4b7c51f9de25cf56beb88	toward rapid composition with confidence in robotics software		Robotics software is booming thanks in part to a rich and productive ecosystem around the Robot Operating System. We introduce a military effort to leverage the ROS ecosystem and reduce the challenges in building military robots, called ROS-M. We outline some of the work we have done on the ROS-M initiative, and explain our future directions in analyzing ROS code to balance between rapid adoption and confidence in the component.	ecosystem;military robot;open architecture;robot operating system;robotics	Neil A. Ernst;Rick Kazman;Philip Bianco	2018	2018 IEEE/ACM 1st International Workshop on Robotics Software Engineering (RoSE)	10.1145/3196558.3196567	robot;leverage (finance);systems engineering;software;simultaneous localization and mapping;computer science;government;composition (visual arts);robotics;artificial intelligence	Robotics	-54.64173413588465	10.265059374012019	1540
fc9dd664a43dd65ba20b9d65ef23fcf7ff180620	a practical model for conceptual comparison using a wiki	context awareness;community maintained knowledge base;wikipedia;wikipedia telephony context awareness taxonomy information filtering context modeling clouds vocabulary probes encyclopedias;encyclopaedias;conceptual context;conceptual comparison;semantic relevance;vocabulary;information filtering;conceptual identification community maintained knowledge base wikipedia;telephony;semantic relevance conceptual comparison conceptual context contextual relations;probes;contextual relations;clouds;web sites;taxonomy;conceptual identification;encyclopedias;context modeling;web sites encyclopaedias;knowledge base	One of the key concerns in the conceptualisation of a single object is understanding the context under which that object exists (or can exist). This contextual understanding should provide us with clear conceptual identification of an object including implicit situational information and detail of surrounding objects. For example in learning terms, a learner should be aware of concepts related to the context of their field of study and a surrounding cloud of contextually related concepts. This paper explores the use of an evolving community maintained knowledge-base (that of wikipedia) in order to prioritise concepts that are semantically relevant to the user's interest space.	wiki;wikipedia	David Webster;Jie Xu;Darren Mundy;Paul Warren	2009	2009 Ninth IEEE International Conference on Advanced Learning Technologies	10.1109/ICALT.2009.209	natural language processing;knowledge base;computer science;knowledge management;artificial intelligence;data mining;brand;context model;telephony;encyclopedia;taxonomy	Robotics	-43.51652047122483	10.788614069872313	1541
117f00b661073789cad7b86c8820b090e5106bc7	mpm4cps: multi-paradigm modelling for cyber-physical systems		The last decades have seen the emergence of truly complex, designed systems, known as Cyber-Physical Systems (CPS). Engineering such systems requires integrating physical, software, and network aspects. To date, neither a unifying theory nor systematic design methods, techniques and tools exist to meet this challenge. Individual engineering disciplines, such as mechanical, electrical, network and software engineering offer only partial solutions. Multi-Paradigm Modelling (MPM) proposes to model every part and aspect of a system, including development processes, explicitly, at the most appropriate level(s) of abstraction, using the most appropriate modelling formalism(s). Modelling language engineering, including model transformation, and the study of their semantics, are used to realize MPM. MPM is seen as an effective answer to the challenges of designing Cyber-Physical Systems. Research on modelling CPS is typically based on national activities with loose international interaction. To establish an interdisciplinary and inter-institutional platform for scientific information exchange, consensus building, and collaboration, the COST Action MPM4CPS, funded by the EU Framework Programme for Research and Innovation, has been initiated. MPM4CPS aims to develop and share foundations, techniques, and tools related to Multi-Paradigm Modelling for Cyber-Physical Systems (MPM4CPS) and to provide educational resources. In this paper we describe the overall MPM4CPS approach and its current status.	cyber-physical system;emergence;information exchange;material point method;model transformation;programming paradigm;software engineering	Hans Vangheluwe;Vasco Amaral;Holger Giese;Jan F. Broenink;Bernhard Schätz;Alex Norta;Paulo Carreira;Ivan Lukovic;Tanja Mayerhofer;Manuel Wimmer;Antonio Vallecillo	2016				SE	-61.22728656193222	16.344783142534787	1549
2074d749e61c4b1ccbf957fe18efe00fa590a124	toward a science of cyber–physical system integration	analytical models;cyberspace;system analysis and design control engineering computing embedded software;complexity theory;system analysis and design;mobile robots;software engineering;control engineering computing cyber physical system integration large scale cyber physical system design large scale cps design cross domain interaction modeling heterogeneous abstraction layers design flow heterogeneous systems passivity based design approach stability timing uncertainties model based fully automated software synthesis high fidelity performance analysis networked unmanned air vehicles high confidence embedded control software design quadrotor uav;stability;network topology;cyberspace computational modeling stability analysis network topology large scale systems analytical models complexity theory control systems system analysis and design;embedded systems;computational modeling;systems analysis;stability analysis;control engineering computing;systems analysis autonomous aerial vehicles control engineering computing embedded systems mobile robots software engineering stability;autonomous aerial vehicles;large scale systems;embedded software	System integration is the elephant in the china store of large-scale cyber-physical system (CPS) design. It would be hard to find any other technology that is more undervalued scientifically and at the same time has bigger impact on the presence and future of engineered systems. The unique challenges in CPS integration emerge from the heterogeneity of components and interactions. This heterogeneity drives the need for modeling and analyzing cross-domain interactions among physical and computational/networking domains and demands deep understanding of the effects of heterogeneous abstraction layers in the design flow. To address the challenges of CPS integration, significant progress needs to be made toward a new science and technology foundation that is model based, precise, and predictable. This paper presents a theory of composition for heterogeneous systems focusing on stability. Specifically, the paper presents a passivity-based design approach that decouples stability from timing uncertainties caused by networking and computation. In addition, the paper describes cross-domain abstractions that provide effective solution for model-based fully automated software synthesis and high-fidelity performance analysis. The design objectives demonstrated using the techniques presented in the paper are group coordination for networked unmanned air vehicles (UAVs) and high-confidence embedded control software design for a quadrotor UAV. Open problems in the area are also discussed, including the extension of the theory of compositional design to guarantee properties beyond stability, such as safety and performance.	abstraction layer;computation;computer performance;control system;coupling (computer programming);cyber-physical system;dynamical system;embedded system;emergence;experiment;interaction;mit engineering systems division;model of computation;platform-specific model;proceedings of the ieee;process (computing);profiling (computer programming);real-time clock;real-time computing;requirement;software design;system integration;systems design;theory;unmanned aerial vehicle	Janos Sztipanovits;Xenofon D. Koutsoukos;Gabor Karsai;Nicholas Kottenstette;Panos J. Antsaklis;Vijay Gupta;Bill Goodwine;John S. Baras;Shige Wang	2012	Proceedings of the IEEE	10.1109/JPROC.2011.2161529	control engineering;mobile robot;systems analysis;von neumann stability analysis;real-time computing;simulation;stability;embedded software;computer science;engineering;electrical engineering;cyber-physical system;computational model;structured systems analysis and design method;network topology	Embedded	-45.14480991709381	37.29315815272236	1551
41b0b0cda5770a0c2c984fab402d8c91be9503a7	dealing with tentative data values in disconnected work groups	databases;computers;working group;database system;internal conflict detection tentative data values disconnected work groups weakly consistent replicated data systems tentative data updates human intervention;conflict detection;databases file systems humans calendars computers data systems schedules;humans file systems computer science laboratories data systems database systems mobile computing;calendars;data structures;replicated data;concurrency control;distributed databases;schedules;data systems;data structures replicated databases concurrency control distributed databases;humans;independent component;replicated databases;file systems	This paper describes a problem of weakly-consistent replicated data systems used in support of disconnected groups of people. The problem concerns actions and updates derived from tentative data updates that are ultimately determined to be in conflict. While some such actions and updates can be automatically resolved, many require human intervention. FurthemOre, although some file and database systems support internal conflict detection and resolution. derived actions may be external to those systems, implying that human users must ensure that proper consistency is maintained between independent components of the system. The entire problem becomes exascerbated when disconnected work groups are taken into account, where tentative data values may be seen and acted upon by multiple people.	data system;database;file synchronization	Marvin Theimer;Alan J. Demers;Karin Petersen;Mike Spreitzer;Douglas B. Terry;Brent B. Welch	1994	1994 First Workshop on Mobile Computing Systems and Applications	10.1109/WMCSA.1994.18	computer science;data mining;database;distributed computing	DB	-24.411186701243572	47.615547097460635	1553
e2204f7eff104ac9f1816127ce1c0603e530090e	the open source approach opportunities and limitations with respect to security and privacy	trustworthiness;security and privacy;open source software development;software development;security;privacy;open source	TodayÕs software often does not even fulfil basic security or privacy requirements. Some people regard the open source paradigm as the solution to this problem. First, we carefully explain the security and privacy aspects of open source, which in particular offer the possibility for a dramatic increase in trustworthiness for and autonomy of the user. We show which expectations for an improvement of the software trustworthiness dilemma are realistic. Finally, we describe measures necessary for developing secure and trustworthy open source systems.	autonomy;open-source software;privacy;programming paradigm;requirement;software trustworthiness;trust (emotion)	Marit Hansen;Kristian Köhntopp;Andreas Pfitzmann	2002	Computers & Security	10.1016/S0167-4048(02)00516-3	software security assurance;trustworthiness;privacy software;information privacy;privacy by design;computer science;information security;software development;internet privacy;privacy;world wide web;computer security	Security	-49.175961468356014	56.41266557883347	1555
8f248e06e5b1f39cafd180595eb6a30a28d7ff4e	portability analysis for axiomatic memory models. porthos: one tool for all models		We present porthos, the first tool that discovers porting bugs in performance-critical code. porthos takes as input a program and the memory models of the source architecture for which the program has been developed and the target model to which it is ported. If the code is not portable, porthos finds a bug in the form of an unexpected execution — an execution that is consistent with the target but inconsistent with the source memory model. Technically, porthos implements a bounded model checking method that reduces the portability analysis problem to satisfiability modulo theories (SMT). There are two main problems in the reduction that we present novel and efficient solutions for. First, the formulation of the portability problem contains a quantifier alternation (consistent + inconsistent). We introduce a formula that encodes both in a single existential query. Second, the supported memory models (e.g., Power) contain recursive definitions. We compute the required least fixed point semantics for recursion (a problem that was left open in [47]) efficiently in SMT. Finally we present the first experimental analysis of portability from TSO to Power.	fixed point (mathematics);least fixed point;memory model (programming);model checking;modulo operation;quantifier (logic);recursion;satisfiability modulo theories;software bug;software portability	Hernán Ponce de León;Florian Furbach;Keijo Heljanko;Roland Meyer	2017	CoRR		computer science;theoretical computer science;programming language;algorithm	Logic	-17.938671749195635	22.979459772935193	1556
f3f41511ef8de961a1c500f056aea2aece26f1ad	alignment within the software development unit: assessing structural and relational dimensions between developers and testers	survey research;structural and relational dimensions;software development;it subunits;alignment within the it unit;developers and testers	Just as business-IT alignment has received significant focus as a strategic concern in the IS literature, it is also important to consider internal alignment between the diverse subunits within the IT organization. This study investigates alignment between developers and testers in software development to understand alignment within the IT unit. Prior evidence of tension between these sub-groups (and others as well) suggests that all is not necessarily well within the IT organization. Misalignment within the IT unit can certainly make it difficult for the IT unit to add strategic value to the organization. This study is an important initial step in investigating IT subunit alignment which can inform future research focusing on the alignment of other IT subunits such as architecture, operations, and customer-support. Using theoretical concepts from strategic business-IT alignment, we test a research model through a survey of professional software developers and testers. Results suggest that relational but not structural dimensions influence IT subunit alignment.	software development	Jasbir Singh Dhaliwal;Colin Gabriel Onita;Robin S. Poston;Xihui Zhang	2011	J. Strategic Inf. Sys.	10.1016/j.jsis.2011.03.001	survey research;systems engineering;engineering;operations management;software development;sociology;management;engineering drawing;strategic alignment	SE	-79.65911496712086	6.071944473789783	1561
67c0d1b0698e2a0121f912b208fb17cea43dfa14	insights into software testing	software testing;specification based testing;testing techniques;design based testing;test management	93 Volume 2, Issue 3 Copyright © 2001 John Wiley & Sons, Ltd. focus software rather than one in which no faults are found. It is certainly apparent from experience that testing can expose faults and once these are corrected, developers are provided with a practical way of obtaining increased confidence in software, although if users become aware of faults their confidence in the product may be reduced. Since, in general as mentioned above, complete, exhaustive testing is not possible, test data has to be constructed as a compromise that satisfies the need for increased confidence in the software and the equally probable need to economise on time and costs.	focus;john d. wiley;software testing;test data	Martin R. Woodward	2001	Software Focus	10.1002/swf.37	non-regression testing;test strategy;black-box testing;software performance testing;white-box testing;manual testing;system integration testing;computer science;acceptance testing;software reliability testing;software engineering;functional testing;software construction;risk-based testing;software testing;system testing;test management approach	SE	-62.813600596171625	31.631652469358826	1565
7fd2d8492a016ae64b5a13d6bd01e7f7c985cda9	a logic description on different negation relation in knowledge	semantic model	In this paper, we propose that negation relation in knowledge ought to differentiate contradictory negation and opposite negation. Based on this cognition, we (1) discovered a character of knowledge: if pair of knowledge with opposite negation relation are fuzzy knowledge, then must exist fuzzy medium object (new knowledge) between them; contrarily, if there is a fuzzy medium object between the two knowledge with opposite negation relation, then them must be fuzzy knowledge; (2) five kinds of contradictory negation and opposite relations (CDC, CFC, ODC, OFC and ROM) in the distinct knowledge and fuzzy knowledge, and formalization definition were confirmed; (3) studied these different negation relations using the medium predicate logic MF and the infinite valued semantic model i¾? of MF, and obtained the conditions of processing these different negation relations.		Zhenghua Pan	2008		10.1007/978-3-540-85984-0_98	semantic data model;natural language processing;computer science;negation as failure;artificial intelligence;mathematics;negation introduction;negation normal form;algorithm	NLP	-13.931259860299175	8.650992697987547	1568
e49de2c11c695279b111e45c8e2ce3757211417f	developing an open architecture for performance data mining	eficacia sistema;architecture systeme;procesamiento informacion;sistema informatico;performance systeme;computer system;data mining;system performance;embedded system;open architecture;information processing;performance analysis;arquitectura sistema;systeme informatique;information system;system architecture;traitement information;high performance;systeme information;sistema informacion	Performance analysis of high performance systems is a difficult task. Current tools have proven successful in analysis tasks but their implementation is limited in several respects. Closed architectures, predefined analysis and views, and specific platforms account for these limitations. Embedded systems are particularly affected by these concerns. This paper presents an open architecture for performance data mining that addresses these limitations. Comparisons of the architecture with current tools show its capabilities address a wider range of system phases and environments.	algorithm;data mining;embedded system;open architecture;operating system;software portability	David B. Pierce;Diane T. Rover	2000		10.1007/3-540-45591-4_113	embedded system;simulation;information processing;open architecture;computer science;information system	HPC	-26.591691104633846	41.48595988051188	1573
fe153bac967bf1970513d0e646b12cda8dfa0b4c	analysis and design of secure and privacy preserving systems				Muhammad Ikram	2018				EDA	-48.09203235718588	62.16256433185916	1574
cb3c696182772f4126557883dc66bc662d697b84	two adaptive logics of norm-propositions	science general;normative conflicts;deontic logic;normative gaps;dilemmas;adaptive logic;input output logics;defeasible reasoning;moral conflicts	We present two defeasible logics of norm-propositions (statements about norms) that (i) consistently allow for the possibility of normative gaps and normative conflicts, and (ii) map each premise set to a sufficiently rich consequence set. In order to meet (i), we define the logic LNP, a conflictand gap-tolerant logic of norm-propositions capable of formalizing both normative conflicts and normative gaps within the object language. Next, we strengthen LNP within the adaptive logic framework for non-monotonic reasoning in order to meet (ii). This results in the adaptive logics LNP and LNP, which interpret a given set of premises in such a way that normative conflicts and normative gaps are avoided ‘whenever possible’. LNP and LNP are equipped with a preferential semantics and a dynamic proof theory.	defeasible reasoning;linear-nonlinear-poisson cascade model;non-monotonic logic;object language;temporal logic	Mathieu Beirlaen;Christian Straßer	2013	J. Applied Logic	10.1016/j.jal.2013.02.001	epistemology;deontic logic;mathematics;defeasible reasoning;algorithm	PL	-16.38036523915745	5.89049258715023	1576
1da8788574eea0020cfc33b49e433285739d9396	making linux protection mechanisms egalitarian with userfs	lines of code;file system	UserFS provides egalitarian OS protection mechanisms in Linux. UserFS allows any user-not just the system administrator-to allocate Unix user IDs, to use chroot, and to set up firewall rules in order to confine untrusted code. One key idea in UserFS is representing user IDs as files in a /proc-like file system, thus allowing applications to manage user IDs like any other files, by setting permissions and passing file descriptors over Unix domain sockets. UserFS addresses several challenges in making user IDs egalitarian, including accountability, resource allocation, persistence, and UID reuse. We have ported several applications to take advantage of UserFS; by changing just tens to hundreds of lines of code, we prevented attackers from exploiting application-level vulnerabilities, such as code injection or missing ACL checks in a PHP-based wiki application. Implementing UserFS requires minimal changes to the Linux kernel-a single 3,000-line kernel module-and incurs no performance overhead for most operations, making it practical to deploy on real systems. Thesis Supervisor: Nickolai Zeldovich Title: Assistant Professor of Computer Science and Engineering	chroot;code injection;computer engineering;firewall (computing);item unique identification;linux;loadable kernel module;operating system;overhead (computing);php;persistence (computer science);protection mechanism;source lines of code;system administrator;unix domain socket;wiki	Taesoo Kim;Nickolai Zeldovich	2010			real-time computing;computer science;operating system;fstab;unix file types;programming language;source lines of code;computer security	Security	-54.19889716612006	58.0275735293228	1580
1e9eb1c14dca613e11bd2b7bee585154b806e061	state trees as structured finite state machines for user interfaces	object oriented language;user interface;interactive application;interactive system;tree structure;finite state machine;entry and exit	State trees are a technique for specifying the control of an interactive system by organizing states into trees to define shared structure and behavior. The tree structure permits inheritance of state information, event traps, and entry and exit actions from states to substates, thereby sharing information and reducing the amount of code to be written. An interface can be restructured by moving entire subtrees as modules. State trees separate the recognition of commands from their implementation, allowing either to be modified independently. State trees structure flow of control within an implementation without imposing an identical structure on the interface itself, which need not be hierarchical. State trees have been implemented in an object-oriented language and used to write interactive applications.	control flow;finite-state machine;interactivity;organizing (structure);tree (data structure);tree structure;uml state machine	James E. Rumbaugh	1988		10.1145/62402.62404	real-time computing;computer science;theoretical computer science;operating system;distributed computing;tree structure;finite-state machine;programming language;object-oriented programming;user interface	HCI	-30.14678865142997	26.551532756751367	1581
404cda20517d8169a8c31e353841a2468d3ff5cf	digitale transformation des speditionsgeschäfts umfasst mehr als spedition 4.0				Alf Dietrich;Felix Fiege	2017	Wirtschaftsinformatik & Management	10.1007/s35764-017-0058-6		Crypto	-101.45529596325612	25.17703348195415	1583
26bfba24e9b6f0cb1a17af85a4514a9ffffddc77	early prediction of mpp performance: th sp2, t3d, and paragon experiences	performance measure;parallel algorithm;sdsc;massively parallel processors;southern california;software development;high performance computer;stap benchmark;performance prediction;cray t3d;ibm sp2;intel paragon;program development;massively parallel processor	Abstract   The performance of  Massively Parallel Processors  (MPPs) is attributed to a large number of machine and program factors. Software development for MPP applications is often very costly. The high cost is partially caused by a lack of early prediction of MPP performance. The program development cycle may iterate many times before achieving the desired performance level. In this paper, we present an early prediction scheme we have developed at the University of Southern California for reducing the cost of application software development. Using workload analysis and overhead estimation, our scheme optimizes the design of parallel algorithm before entering the tedious coding, debugging, and testing cycle of the applications. The scheme is generally applied at user/programmer level, not tied to any particular machine platform or any specific software environment. We have tested the effectiveness of this early performance prediction scheme by running the MIT/STAP benchmark programs on a 400-node IBM SP2 system at the Maui High-Performance Computing Center (MHPCC), on a 400-node Intel Paragon system at the San Diego Supercomputing Center (SDSC), and on a 128-node Cray T3D at the Cray Research Eagan Center in Wisconsin. Our prediction shows to be rather accurate compared with the actual performance measured on these machines. We use the SP2 data to illustrate the early prediction scheme. The main contribution of this work lies in providing a systematic procedure to estimate the computational work-load, to determine the application attributes, and to reveal the communication overhead in using these MPPs. These results can be applied to develop any MPP applications other than the STAP benchmarks by which this prediction scheme was developed.	goodyear mpp	Zhiwei Xu;Kai Hwang	1996	Parallel Computing	10.1016/0167-8191(96)00034-8	parallel computing;real-time computing;computer science;software development;operating system;parallel algorithm;programming language	HPC	-7.130575228880263	39.789843660555924	1588
acaa773fa6dd11072632f1de4a3e123bc451cbc0	computer software review: reaxys	ny		reaxys;software review	Jonathan Goodman	2009	Journal of Chemical Information and Modeling	10.1021/ci900437n	chemistry	SE	-92.7006253597735	25.33578472799361	1591
6ba7289a6625cf18e316a48083f5f01d98822605	tam2: automated threat analysis	modeling technique;security analysis;empirical analysis;threat analysis;large scale;software architecture;quality;software development;software development life cycle;process;security;architecture	Identifying and resolving security problems as early as possible in the software development life cycle should by now be conventional wisdom. However, we observe that there is no threat modeling approach suitable for analysing initial software architecture. Our approach aims to fill this gap by adopting a threat modeling technique (STRIDE) that can be equally applied to software architecture diagrams. Accordingly, we claim and seek to validate that even little additional information on architecture diagrams can yield significant value in a lightweight automated security analysis. We implement and verify our approach by building a tool for automated threat analysis of software architecture diagrams. This is validated in the context of a large-scale industrial software development context providing some initial empirical analysis.	automated threat;diagram;software architecture;software development process;threat model	Andreas Schaad;Mike Borozdin	2012		10.1145/2245276.2231950	software security assurance;reference architecture;software architecture;verification and validation;software sizing;computer science;information security;package development process;backporting;software development;architecture;software design description;operating system;service-oriented modeling;software engineering;software construction;database;software architecture description;systems development life cycle;security analysis;resource-oriented architecture;software deployment;world wide web;computer security;goal-driven software development process;software development process;software quality;process	SE	-60.50209567985999	29.04658001752744	1592
20a274f08fa5296ef7b468d104badd97b1b32d63	algorithmustransformationen beim entwurf anwendungsspezifischer integrierter schaltungen				Matthias Sauer	1995				Crypto	-100.09193337981267	26.386442097847915	1596
34f7aeb91e81e74e9bc1c9bd57b7f8f637106ded	authentication mechanism over the integrated umts network and wlan platform using the cross-layer bootstrap	norme telecommunication;data transmission;protocolo acceso;protocols;wireless local area network;abonado;mobile radiocommunication;reseau transmission donnee;umts;protocole transmission;signalling;systeme umts;telecommunication sans fil;debit information;information transmission;authentication;abonne;wlan;authentication mechanism;punto caliente;ietf protocol;systeme integre;sistema integrado;signalisation;indice informacion;access protocol;radiocommunication service mobile;radiocommunication service mobile 3eme generation;wireless lan 3g mobile communication data privacy message authentication protocols telecommunication security;hot spot;authentification;vida privada;3g mobile data network;protocolo transmision;data transmission network;autenticacion;3g mobile communication;user identity privacy protection;taux transmission;private life;data privacy;user identity privacy protection authentication mechanism umts wlan cross layer bootstrap 3g mobile data network wireless local area network loose coupled integration mechanism universal mobile telecommunication system network protocol;subscriber;relacion transmision;telecomunicacion sin hilo;telecommunication standards;transmission donnee;red transmision datos;telecommunication security;autogeneration mutuelle;point chaud;information rate;vie privee;transmission rate;bootstrapping;universal mobile telecommunication system network;sistema umts;wireless lan;message authentication;transmision informacion;cross layer;transmission information;protocole acces;radiocomunicacion servicio movil;integrated system;protocolo ietf;reseau local sans fil;protocole ietf;umts system;transmision datos;senalizacion;loose coupled integration mechanism;cross layer bootstrap;protocol;wireless telecommunication;transmission protocol	The 3G mobile data network provides always-on and ubiquitous connectivity for subscribers. Although the service coverage area in wireless local area network (WLAN) is much smaller than that in a 3G mobile data network, the data transmission rate in WLAN can be from 2 to 54 Mbps, which is much faster than 3G mobile network. Obviously, the relationship between the 3G mobile data network and WLAN is complementary in terms of service coverage and data transmission rate. Therefore integration of 3G mobile network and WLAN can offer subscribers higher speed wireless service in hot spots and ubiquitous connectivity in 3G mobile data network. An authentication mechanism over the loose coupled integration mechanism using a cross-layer bootstrap is proposed. The benefits of the proposed mechanism are (a) integrating Universal Mobile Telecommunication System network and WLAN using the existing protocols defined in 3GPP, IETF and IEEE 802.11i, (b) the use of the Extension Authentication Protocol authentication method is flexible, (c) reduction of the authentication signalling when a subscriber roams from one access point (AP) to another AP and (d) user identity privacy protection.	authentication;transport layer security	Chung-Ming Huang;Jian-Wei Li	2007	IET Communications	10.1049/iet-com:20060236	wi-fi;wireless wan;public land mobile network;information privacy;telecommunications;computer science;authentication;network access control;computer security;computer network	Mobile	-14.473659212251412	95.79240938917154	1598
6e3e1dfc2b40257e400734562e1052d145865018	state isomorphism in model programs with abstract data structures	graph isomorphism;software requirements;software development;polynomial time;model based testing;data structure	Modeling software features with model programs in C# is a way of formalizing software requirements that lends itself to automated analysis such as model-based testing. Unordered structures like sets and maps provide a useful abstract view of system state within a model program and greatly reduce the number of states that must be considered during analysis. Similarly, a technique called linearization reduces the number of states that must be considered by identifying isomorphic states, or states that are identical except for reserve element choice (such as the choice of object IDs for instances of classes). Unfortunately, linearization does not work on unordered structures such as sets. The problem turns into graph isomorphism, for which no polynomial time solution is known. In this paper we discuss the issue of state isomorphism in the presence of unordered structures and give a practical approach that overcomes some of the algorithmic limitations.	abstract data type;algorithm;graph (discrete mathematics);graph isomorphism;graph labeling;koutetsu no kishi;map;model-based testing;polynomial;requirement;software requirements;spec explorer;spec#;time complexity	Margus Veanes;Juhan P. Ernits;Colin Campbell	2007		10.1007/978-3-540-73196-2_8	time complexity;model-based testing;data structure;computer science;software development;software engineering;distributed computing;graph isomorphism;software requirements;algorithm	SE	-17.672405281369027	28.573272578123337	1603
7533d958b7277e4c224811e3e8eedb4e92626550	a new framework for intrusion detection based on rough set theory	intruder detector;rendimiento elevado;networks;sistema experto;mise a jour;generic algorithm;securite;rough set theory;base connaissance;intrusion detection;packet switching;conmutacion por paquete;data mining;reduction donnee;attaque;actualizacion;ataque;computer programming;fouille donnee;theorie ensemble approximatif;table decision;knowledge acquisition;decouverte connaissance;safety;tabla decision;intrusion detection systems;rendement eleve;descubrimiento conocimiento;algorithms;base conocimiento;reduccion datos;expert knowledge;data reduction;systeme expert;detecteur intrus;rough set;seguridad;detector intruso;high efficiency;busca dato;commutation paquet;decision table;systeme detection intrusion;updating;attacking;critical infrastructure protection;knowledge base;knowledge discovery;expert system	Intrusion detection is an essential component of critical infrastructure protection mechanism. Since many current IDSs are constructed by manual encoding of expert knowledge, it is time-consuming to update their knowledge. In order to solve this problem, an effective method for misuse intrusion detection with low cost and high efficiency is presented. This paper gives an overview of our research in building a detection model for identifying known intrusions, their variations and novel attacks with unknown natures. The method is based on rough set theory and capable of extracting a set of detection rules from network packet features. After getting a decision table through preprocessing raw packet data, rough-set-based reduction and rule generation algorithms are applied, and useful rules for intrusion detection are obtained. In addition, a rough set and rule-tree-based incremental knowledge acquisition algorithm is presented in order to solve problems of updating rule set when new attacks appear. Compared with other methods, our method requires a smaller size of training data set and less effort to collect training data. Experimental results demonstrate that our system is effective and more suitable for online intrusion detection.	intrusion detection system;rough set;set theory	Zhijun Li;Yu Wu;Guoyin Wang;Yongjun Hai;Yunpeng He	2004		10.1117/12.540951	anomaly-based intrusion detection system;computer science;artificial intelligence;data mining;algorithm	ML	-63.75103289296892	69.8222819174418	1612
449f7fe07ad006e870a8a3ff69d5646db0dd3b91	le commerce électronique: un état de i'art	commerce electronique;confidencialidad;comercio electronico;articulo sintesis;protocole jepi;securite;article synthese;electronic fund transfer;authentication;protocole globeid;cle publique;jepi protocol;access protocol;confidentiality;authentification;transferencia computarizada de fondos;confidentialite;globeid protocol;autenticacion;public key;internet;criptografia;cryptography;safety;llave publica;cryptographie;echange donnee informatise;protocole acces;review;seguridad;acceso protocolo;monetique;electronic data interchange;electronic trade;cambio dato electronico	Dans cet article de synthese, sont presentees les solutions les plus representatives en matiere de commerce electronique sur l’internet. Une classification largement acceptee1 est reprise: sont donc successivement presentes l’argent electronique (Digicash, Mondex), les transactions directes client-marchand (set) et l’intermediation (GlobeID, CyberCash, First Virtual). Un projet federateur (Jepi), tentant de resoudre les incompatibilites entre les differents systemes, est ensuite presente.		Ludovic Mé;Renaud Chaillat	1998	Annales des Télécommunications	10.1007/BF02998502	telecommunications;computer science;authentication;computer security	Crypto	-35.82055697396784	62.62340818172959	1615
d0e9e7df154c9cea1053967a225502a87877e0c3	a tutorial on synthesis of logic programs from specifications	program synthesis;formal method;logic programs	1 Introduction	logic programming	Kung-Kiu Lau;Geraint A. Wiggins	1994			computer architecture;logic synthesis;formal methods;computer science;programming language;axiomatic semantics;algorithm	AI	-21.531494924851405	20.42745353916738	1619
5d4b271d1da77c2fa99b830b379383da3dadb749	improving delivery time guarantees for wireless data services	scheduling algorithm delay effects bandwidth quality of service real time systems land mobile radio cellular systems traffic control algorithm design and analysis wireless sensor networks;service provider;cellular radio;radio data systems;transport protocols;hypermedia;telecommunication traffic;scheduling algorithm;3g mobile communication;internet;web sites;cellular network;service level agreement;web browsing;quality of service;http traffic delivery time guarantee wireless data service 3g cellular network voice service web browsing file download nonreal time service service level agreement quality of service user satisfaction sigmoid function user sensitivity scheduling algorithm channel condition soft reservation scheme;wireless data;quality of data;user satisfaction;telecommunication traffic 3g mobile communication cellular radio radio data systems quality of service transport protocols web sites internet hypermedia	The deployment of 3G cellular networks is ushering in a variety of data services in conjunction with the existing voice services. A major proportion of data services will include Web browsing and file downloads categorized as nonreal time services, for which there exists no service level agreement between the users and the providers on the quality of data services. In this paper, we address how to deal with the subjectiveness of user satisfaction or expectation from service providers by defining user irritation factors with the help of Sigmoid functions. These factors reflect users' sensitivity and tolerance to delay. We propose a scheduling algorithm to provide a bound on the delivery time of the entire data content. This algorithm takes into consideration the dynamic nature of the irritation factors, channel conditions, user classes, and fairness within individual classes. Soft-reservation schemes are also proposed to cater to the higher paying users. Exhaustive simulations are conducted with synthetically generated HTTP traffic and three classes of users. Experimental results demonstrate that the proposed scheme is able to provide bounded delay even when the system utilization is as high as 80%.	algorithm;categorization;fairness measure;hypertext transfer protocol;internet;scheduling (computing);sensitivity and specificity;service-level agreement;sigmoid function;simulation;software deployment	Sourav Pal;Mainak Chatterjee;Sajal K. Das	2004	2004 IEEE Wireless Communications and Networking Conference (IEEE Cat. No.04TH8733)	10.1109/WCNC.2004.1311488	service provider;cellular network;the internet;quality of service;telecommunications;computer science;operating system;scheduling;world wide web;computer security;transport layer;computer network	Mobile	-10.853473259375978	99.90529792565118	1627
d7b7b9fe5d9475f7a497dd3b6d72f1b4502c5e9f	performance-based comparative assessment of open source web vulnerability scanners		The widespread adoption of web vulnerability scanners and the differences in the functionality provided by these tool-based vulnerability detection approaches increase the demand for testing their detection effectiveness. Despite the advantages of dynamic testing approaches, the literature lacks studies that systematically evaluate the performance of open source web vulnerability scanners. The main objectives of this study are to assess the performance of open source scanners from multiple perspectives and to examine their detection capability. This paper presents the results of a comparative evaluation of the security features as well as the performance of four web vulnerability detection tools. We followed this comparative assessment with a case study in which we evaluate the level of agreement between the results reported by two open source web vulnerability scanners. Given that the results of our comparative evaluation did not show significant performance differences among the scanners while the results of the conducted case study revealed high level of disagreement between the reports generated by different scanners, we conclude that the inconsistencies between the reports generated by different scanners might not necessarily correlate with their performance properties. We also present some recommendations for helping developers of web vulnerabilities scanners to improve their tools’ capabilities.		Mansour Alsaleh;Noura Alomar;Monirah Alshreef;Abdulrahman Alarifi;AbdulMalik S. Al-Salman	2017	Security and Communication Networks	10.1155/2017/6158107	data mining;world wide web;computer security	SE	-61.15810128058435	56.33823253360753	1644
ebe71474c73005cef4cbd2f59a3c90c3b3b39058	a pattern for secure graphical user interface systems	keyboards;secure windowing system security pattern secure gui;data integrity;secure windowing system;user interface;data confidentiality secure graphical user interface system secure operating system security pattern data integrity;graphical user interfaces user interfaces data security operating systems application software keyboards digital signatures databases expert systems pattern analysis;security pattern;security of data data integrity graphical user interfaces operating systems computers;system security;graphical user interfaces;security requirements;secure operating system;secure graphical user interface system;secure gui;graphic user interface;data confidentiality;security;security of data;operating systems computers;graphics;labeling;operating systems	Several aspects of secure operating systems have been analyzed and described as security patterns. However, existing patterns do not cover explicitly the secure interaction of users with the user interface of applications. Especially graphical user interfaces tend to get complex and vulnerable to spoofing and eavesdropping, e.g., due to key loggers or fake dialog windows. A secure user interface system has to provide a trusted path between the user and the application the user intends to use. The trusted path must be able to ensure integrity and confidentiality of the transmitted data, and must allow for the verification of the authenticity of the end points. We present a pattern for secure graphical user interface systems and evaluate its use in different implementations. This pattern shows how to fulfill the security requirements of a trusted path while preserving, in a policy-driven way, the flexibility that graphical user interfaces generally demand.	computer keyboard;confidentiality;download;graphical user interface;input/output;keystroke logging;microsoft windows;operating system;output device;requirement;trusted path;dialog	Thomas Fischer;Ahmad-Reza Sadeghi;Marcel Winandy	2009	2009 20th International Workshop on Database and Expert Systems Application	10.1109/DEXA.2009.76	user interface design;look and feel;secure by design;user;user modeling;interface metaphor;computer science;information security;operating system;graphical user interface;internet privacy;natural user interface;user interface;world wide web;computer security;multiple document interface;secure by default	Security	-52.94347621891223	53.909795710815516	1645
2c00197ca190dc8bec67def79a14bf1c424e7824	an approach to intrusion detection by means of idiotypic networks paradigm	network traffic idiotypic networks paradigm intrusion detection architecture idiotypic network theory large scale network attacks denial of service dynamic clustering adaptive clustering;telecommunication security computer network management pattern clustering security of data;pattern clustering;metaheuristics;system configuration;high speed networks;intrusion detection;immune system floods humans monitoring computer architecture servers security;large scale network attacks;idiotypic network theory;denial of service attack;dynamic clustering;computer architecture;large scale;servers;monitoring;network traffic;adaptive clustering;intrusion detection architecture;computer network management;denial of service;telecommunication security;immune system;network theory;humans;floods;point of view;security;artificial immune systems;security of data;intrusion detection system;idiotypic networks paradigm	In this paper we present a novel intrusion detection architecture based on Idiotypic Network Theory (INIDS), that aims at dealing with large scale network attacks featuring variable properties, like Denial of Service (DoS). The proposed architecture performs dynamic and adaptive clustering of the network traffic for taking fast and effective countermeasures against such high-volume attacks. INIDS is evaluated on the MITpsila99 dataset and outperforms previous approaches for DoS detection applied to this set.	cluster analysis;computer cluster;denial-of-service attack;experiment;intrusion detection system;network packet;network theory;network traffic control;programming paradigm;test data;threat (computer)	Marek Ostaszewski;Pascal Bouvry;Franciszek Seredynski	2008	2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational Intelligence)	10.1109/CEC.2008.4631077	intrusion detection system;computer science;information security;distributed computing;computer security;denial-of-service attack;metaheuristic;computer network	Vision	-62.36927373820593	64.76060014372223	1648
484d5bf191bdf61a11d365ee5bf0338c85098746	automatic inference of bounds on resource consumption		In this tutorial paper, we overview the techniques that underlie the automatic inference of bounds on resource consumption. We first explain the basic techniques on a Java-like sequential language. Then, we describe the extensions that are required to apply our method on concurrent ABS programs. Finally, we discuss some advanced issues in resource analysis, including the inference of non-cumulative resources and the treatment of shared mutable data.	aliasing;amortized analysis;circular dependency;context switch;data structure;deadlock;expectation–maximization algorithm;high- and low-level;high-level programming language;immutable object;interaction;invariant (computer science);iteration;java;linear algebra;linked list;local variable;locality of reference;loop invariant;network switch;nonlinear system;open system (computing);pointer analysis;precondition;reachability;shared variables;traverse;terminate (software);termination analysis	Elvira Albert;Diego Esteban Alonso-Blas;Puri Arenas;Jesús Correas Fernández;Antonio Flores-Montoya;Samir Genaim;Miguel Gómez-Zamalloa;Abu Naser Masud;Germán Puebla;José Miguel Rojas;Guillermo Román-Díez;Damiano Zanardini	2012		10.1007/978-3-642-40615-7_4	computer science;theoretical computer science;data mining;algorithm	PL	-19.985078816782426	30.598316891551235	1655
36564935e81e5740e5d41ac67807e195fb7739fb	ein bezugsrahmen für die implementierung von it-outsourcing-governance	it outsourcing	Die Steuerung – oder Governance – der Geschäftsbeziehung ist ein wichtiger Erfolgsfaktor im IT-Outsourcing. Allerdings sieht eine effektive Governance nicht in jedem Fall gleich aus. Vielmehr müssen verschiedene Mechanismen wie Verträge, Prozesse, Strukturen und Beziehungsprotokolle auf die jeweilige Situation zugeschnitten werden. In Abhängigkeit von Komplexität und Ungewissheit des Outsourcing-Kontextes lassen sich vier idealtypische Governance-Szenarien charakterisieren. Diese können Ansatzpunkte für die Diskussion einer Neugestaltung oder Anpassung der Governance zwischen Kunde und Dienstleister liefern.	eine and zwei;internet explorer;outsourcing	Stefan Behrens;Christopher Schmitz	2005	HMD - Praxis Wirtschaftsinform.		knowledge management;engineering;business administration;outsourcing;corporate governance	OS	-101.3038660480875	34.341967242854885	1660
a0511c43fca4240e356f033c4bd663d00299cd6b	a scalable partial-order data structure for distributed-system observation	graph partitioning;gis;doctoral thesis;disk based algorithm;dijkstra s shortest path algorithms;computer science;divide and conquer	Distributed-system observation is foundational to understanding and controlling distributed computations. Existing tools for distributed-system observation are constrained in the size of computation that they can observe by three fundamental problems. They lack scalable information collection, scalable data-structures for storing and querying the information collected, and scalable information-abstraction schemes. This dissertation addresses the second of these problems. Two core problems were identified in providing a scalable data structure. First, in spite of the existence of several distributed-system-observation tools, the requirements of such a structure were not well-defined. Rather, current tools appear to be built on the basis of events as the core data structure. Events were assigned logical timestamps, typically Fidge/Mattern, as needed to capture causality. Algorithms then took advantage of additional properties of these timestamps that are not explicit in the formal semantics. This dissertation defines the data-structure interface precisely, and goes some way toward reworking algorithms in terms of that interface. The second problem is providing an efficient, scalable implementation for the defined data structure. The key issue in solving this is to provide a scalable precedence-test operation. Current tools use the Fidge/Mattern timestamp for this. While this provides a constant-time test, it requires space per event equal to the number of processes. As the number of processes increases, the space consumption becomes sufficient to affect the precedence-test time because of caching effects. It also becomes problematic when the timestamps need to be copied between processes or written to a file. Worse, existing theory suggested that the space-consumption requirement of Fidge/Mattern timestamps was optimal. In this dissertation we present two alternate timestamp algorithms that require substantially less space than does the Fidge/Mattern algorithm.	algorithm;causality;computation;core data;data structure;distributed computing;requirement;scalability;semantics (computer science)	Paul A. S. Ward	2001			computer science;theoretical computer science;distributed computing;algorithm	PL	-21.22590149523007	49.30018902648028	1666
9d198ed7d54abe24dca4801e9a572a3f1d5281b6	casebook: a cloud-based system of engagement for case management	context awareness;groupware;collaborative work;social sciences computing case based reasoning cloud computing cooperative systems groupware information management;flexible processes;internet;cooperative systems;social sciences computing;information management;collaborative work internet context awareness organizations artificial intelligence adaptation models cloud computing;artificial intelligence;case management;organizations;knowledge management and reuse;collaborative computing case management flexible processes knowledge management and reuse;case based reasoning;adaptation models;casebook collaborative roadmapping social collaboration intelligent agents information loss knowledge workers case management cloud based system;collaborative computing;cloud computing	Casebook embraces social and collaboration technology, analytics, and intelligence to advance the state of the art in case management from systems of record to a system of engagement for knowledge workers. It addresses complex, inefficient work practices, information loss during hand offs between teams, and failure to learn from previous case experience. Intelligent agents help people adapt to changing work practices by tracking process evolution and providing updates and recommendations. Social collaboration surrounding cases integrates communication with information and supports collaborative roadmapping to enable people to work as they collaborate, thus accelerating how quickly and accurately they handle cases.	casebook;intelligent agent;social collaboration	Hamid R. Motahari Nezhad;Susan Spence;Claudio Bartolini;Sven Graupner;Charles Bess;Marianne Hickey;Parag Mulendra Joshi;Roberto Mirizzi;Kivanc M. Ozonat;Maher Rahmouni	2013	IEEE Internet Computing	10.1109/MIC.2013.58	case-based reasoning;the internet;cloud computing;computer science;organization;knowledge management;artificial intelligence;database;management science;information management;world wide web;computer security	HCI	-52.860132736227555	12.462283760625757	1669
22913b66369570c61aa7d4c42ef9b153aa85a010	orgodex: authorization as a service (aaas)		Role-Based Access Control (RBAC) has been a popular solution for securing information systems for more than two decades. The cloud computing model offers a cost effective alternative for large distributed organizations while presenting unique security challenges. This is an important concern for organizations who have significant investments in large RBAC implementations where hundreds of roles have been engineered to realize security policy and safeguard information assets. In this work, we demonstrate the general applicability of ORGODEX, validating this new alternative for engineering scalable access control solutions in the context of cloud computing. Assuming the duties of a Project Manager and Security Architect, we use the ORGODEX model and methodology to collaboratively analyze roles, information, responsibilities and constraints within a multi-disciplinary project team. Next, we realize the deployment of a new shared software solution for two geographically distributed partner institutions. Finally, we publicize results in a comprehensible role and responsibility matrix, thereby facilitating the ongoing maintenance of both the software and authorization service models.	authorization;cloud computing;computer security;information security;information system;role-based access control;scalability;software deployment;xacml	Aaron Elliott;Scott Knight	2018	2018 Annual IEEE International Systems Conference (SysCon)	10.1109/SYSCON.2018.8369532	software deployment;management science;access control;information system;role-based access control;asset (computer security);cloud computing;project manager;business;security policy	Security	-48.72920758438765	55.36825428208089	1673
f43f35f6eb29ff52be6fba8afb0d2f8a3ee820ed	factors affecting information system volatility	1503 business and management;700000 information and communication services;information system;0806 information systems	The objective of this research is to investigate the effect that various factors have on an information system’s life span by understanding how the factors affect an information system’s stability. The research builds on a previously developed two-stage model of information system change whereby an information system is either in a stable state of evolution in which the information system’s functionality is evolving, or in a state of revolution, in which the information system is being replaced because it is not providing the functionality expected by its users. A case study surveyed a number of systems within one organization. The aim was to test whether a relationship existed between the base value of the volatility index and certain system characteristics. Data relating to some 3,000 user change requests covering 40 systems over a 10-year period were obtained. The following factors were hypothesized to have significant associations with the base value of the volatility index: semantic relativism (generation of language of construction), system size, system age, and the timing of changes applied to a system. Significant associations were found in the hypothesized directions except the timing of user changes was not associated with any change in the value of the volatility index.	index (publishing);information system;significant figures;volatility	Jon Heales	2000			executive information system;information technology management;information governance;marketing;management information systems;information industry;risk management information systems;information management;strategic information system;information system;information security management;commerce	SE	-81.33312774300724	7.295848636652467	1674
e8f38d61b0955cad32adb21080f8dbeb4eb86274	discovering multi-perspective process models	process model	Process Mining techniques exploit the information stored in the executions log of a process in order to extract some high-level process model, which can be used for both analysis and design tasks. Most of these techniques focus on “structural” (control-flow oriented) aspects of the process, in that they only consider what elementary activities were executed and in which ordering. In this way, any other “non-structural” information, usually kept in real log systems (e.g., activity executors, parameter values, and time-stamps), is completely disregarded, yet being a potential source of knowledge. In this paper, we overcome this limitation by proposing a novel approach for discovering process models, where the behavior of a process is characterized from both structural and non-structural viewpoints. In a nutshell, different variants of the process (classes) are recognized through a structural clustering approach, and represented with a collection of specific workflow models. Relevant correlations between these classes and non-structural properties are made explicit through a rule-based classification model, which can be exploited for both explanation and prediction purposes. Results on reallife application scenario evidence that the discovered models are often very accurate and capture important knowledge on the process behavior.	anomaly detection;cluster analysis;control flow;experiment;high- and low-level;logic programming;plug-in (computing);process modeling;programmable read-only memory;simulation;test case	Francesco Folino;Gianluigi Greco;Antonella Guzzo;Luigi Pontieri	2008			computer science;software engineering;process modeling	SE	-53.22127093002753	18.169576355399382	1676
7240323397f73eb1987f70875e5fae0e4d59fb2b	cost optimization approaches for scientific workflow scheduling in cloud and grid computing: a review, classifications, and open issues		Workflow scheduling in scientific computing systems is one of the most challenging problems that focuses on satisfying user-defined quality of service requirements while minimizing the workflow execution cost. Several cost optimization approaches have been proposed to improve the economic aspect of Scientific Workflow Scheduling (SWFS) in cloud and grid computing. To date, the literature has not yet seen a comprehensive review that focuses on approaches for supporting cost optimization in the context of SWFS in cloud and grid computing. Furthermore, providing valuable guidelines and analysis to understand the cost optimization of SWFS approaches is not well-explored in the current literature. This paper aims to analyze the problem of cost optimization in SWFS by extensively surveying existing SWFS approaches in cloud and grid computing and provide a classification of cost optimization aspects and parameters of SWFS. Moreover, it provides a classification of cost based metrics that are categorized into monetary and temporal cost parameters based on various scheduling stages. We believe that our findings would help researchers and practitioners in selecting the most appropriate cost optimization approach considering identified aspects and parameters. In addition, we highlight potential future research directions in this on-going area of research.	categorization;cloud computing;computational science;grid computing;mathematical optimization;non-functional requirement;scheduling (computing)	Ehab Nabiel Alkhanak;Sai Peck Lee;Reza Rezaei;Reza Meimandi Parizi	2016	Journal of Systems and Software	10.1016/j.jss.2015.11.023	computer science;knowledge management;data mining;management science	HPC	-20.687031098412557	62.03039099739413	1678
afd4d1e4e67a310eb0767d95a93b210cb01dbb97	cardinality-based variability modeling with automationml		Variability modeling is an emerging topic in the general field of systems engineering and, with current trends such as Industrie 4.0, it gains more and more interest in the domain of production systems. Therefore, it is not sufficient to describe systems in several specific cases, but instead families of systems have to be used. In this paper we introduce a role class library for AutomationML to explicitly represent variability. This allows to exchange not only system descriptions but also system family descriptions. We argue for a light-weight extension of AutomationML. The variability-based modeling approach is based on cardinalities, which is a well-known concept from conceptual modeling and feature modeling. Furthermore, we also show how instantiations of variability models can be validated by our EMF-based AutomationML workbench.	eclipse modeling framework;feature model;heart rate variability;industry 4.0;library (computing);software engineering;spatial variability;systems engineering;workbench	Manuel Wimmer;Petr Novák;Radek Sindelár;Luca Berardinelli;Tanja Mayerhofer;Alexandra Mazak	2017	2017 22nd IEEE International Conference on Emerging Technologies and Factory Automation (ETFA)	10.1109/ETFA.2017.8247711	conceptual model;cardinality;control engineering;engineering;unified modeling language;systems engineering;workbench	SE	-53.604389019774175	25.793080071414906	1679
f3fe29218a220f9f76eb36d2234605acb01b3dae	modeling organizational dynamics	system engineering;software tool;modeling and simulation;software maintenance;behavior modeling;system analysis and design;simulation;dynamic model;system of systems;traffic control;complex adaptive system;small world;telephony;simulation software tool organization dynamics modeling legacy tool complex adaptive system;systems engineering and theory;software tools modeling systems engineering and theory humans adaptive systems couplings telephony telecommunication traffic traffic control sliding mode control;telecommunication traffic;organization dynamics modeling;adaptive systems;complex system;systems analysis;business data processing;legacy tool;systems analysis business data processing organisational aspects software maintenance software tools;dynamic simulation;software tools;humans;couplings;modeling;environmental scanning;sliding mode control;organisational aspects	The potential value of software tools for the modeling and simulation of organization dynamics incorporating individual behaviors in the development, deployment and sustainment of complex systems are considered. Such tools need to provide more value compared to traditional systems engineering and systems of systems tools. Value is the capability to model and assess the effects of small-world behavior in organizations exhibiting complexity. An environmental scan of legacy tools to assess how well they meet these needs is provided. From the assessment of dynamic modeling tools and software, recommendations are derived for software tools. These needs become the requirement set for adequate modeling of organizational dynamics susceptible to individual behaviors	complex systems;complexity;computer performance;emergence;organizational behavior;requirement;simulation;software deployment;system of systems;systems engineering	William David Miller;Gay McCarter;Craig O. Hayenga	2006	2006 IEEE/SMC International Conference on System of Systems Engineering	10.1109/SYSOSE.2006.1652280	behavioral modeling;complex adaptive system;systems analysis;dynamic simulation;complex systems;simulation;systems modeling;sliding mode control;system of systems;environmental scanning;computer science;systems engineering;engineering;artificial intelligence;software engineering;modeling and simulation;coupling;telephony;software maintenance;structured systems analysis and design method;computer engineering	SE	-59.458236117906615	19.58121157467019	1692
6f6e3c70a550ea64b20e4484fbbb7490345b9b14	binding, migration, and scalability in corba	tecnologia electronica telecomunicaciones;computacion informatica;fault tolerant;grupo de excelencia;distributed objects;ciencias basicas y experimentales;tecnologias	This article explains how CORBA binds requests to object implementations with the help of an implementation repository. The design of the implementation repository has profound influence on the flexibility, performance, scalability, and fault tolerance of an ORB, and we illustrate some of the trade-offs involved in various repository designs. Implementation repositories play an important role in building scalable object systems; we point out some issues that CORBA will need to address in the future to continue to scale and remain at the forefront of distributed object technology. One Repository to register them all, One Repository to find them, One Repository to start them all, and with IIOP bind them In the land of CORBA where the Objects lie.	common object request broker architecture;distributed object;fault tolerance;garbage collection (computer science);general inter-orb protocol;microsoft forefront;point of interest;referential integrity;scalability	Michi Henning	1998	Commun. ACM	10.1145/286238.286249	embedded system;fault tolerance;real-time computing;computer science;operating system;distributed object	Networks	-33.00394021684844	44.07999558073822	1698
bc778779c0721c72bc818cb51fadb72091617b81	using the diaspec design language and compiler to develop robotics systems	software engineering;assisted living;computer control;physical environment;architectural pattern;autonomic computing;domain specificity	A Sense/Compute/Control (SCC) application is one that interacts with the physical environment. Such applications are pervasive in domains such as building automation, assisted living, and autonomic computing. Developing an SCC application is complex because: (1) the implementation must address both the interaction with the environment and the application logic; (2) any evolution in the environment must be reflected in the implementation of the application; (3) correctness is essential, as effects on the physical environment can have irreversible consequences. The SCC architectural pattern and the DiaSpec domainspecific design language propose a framework to guide the design of such applications. From a design description in DiaSpec, the DiaSpec compiler is capable of generating a programming framework that guides the developer in implementing the design and that provides runtime support. In this paper, we report on an experiment using DiaSpec (both the design language and compiler) to develop a standard robotics application. We discuss the benefits and problems of using DiaSpec in a robotics setting and present some changes that would make DiaSpec a better framework in this setting.	architectural pattern;autonomic computing;business logic;compiler;complex adaptive system;component-based software engineering;correctness (computer science);hoc (programming language);interaction;pervasive informatics;robot control;robotics;spatial variability;vii	Damien Cassou;Serge Stinckwich;Pierrick Koch	2011	CoRR		architectural pattern;compiler correctness;computer science;systems engineering;engineering;artificial intelligence;software engineering;computer engineering;autonomic computing	HCI	-42.66861297156634	37.870005735477605	1700
e4d8d9ca4cc2e940e5daa04d1b454574d2554257	fbcm: strategy modeling method for the validation of software requirements	modeling technique;collaboration;balanced scorecard;model validation;management strategy;software requirements;requirement engineering;validation;goal modeling;modeling methodology;business process	The Balanced Scorecard has attracted great attention as a modeling technique for enterprise management strategy. However, the strategy model is inadequate for examining the validity of software requirements, because methods to evaluate strategy models have yet to be developed. Therefore, this paper proposes a fact based collaboration modeling methodology (FBCM). Based on the results of field observations and data of business processes, it is possible to develop enterprise strategy models from the viewpoints of collaboration between organizations. This paper describes the basic concepts and procedures of the methodology. The methodology has been evaluated via a case study of developing an SCM strategy of a Japanese automobile enterprise. The research project was conducted for seven months to develop a strategy for a complete car logistic process in which five different departments of the company are involved. The results show the effectiveness of the proposed methodology.	requirement;software requirements	Atsushi Kokune;Masuhiro Mizuno;Kyoichi Kadoya;Shuichiro Yamamoto	2007	Journal of Systems and Software	10.1016/j.jss.2006.04.035	goal modeling;systems engineering;engineering;knowledge management;process modeling;management science;regression model validation;requirements engineering;balanced scorecard;business process;management;software requirements;collaboration	SE	-69.42410853417675	18.626455013073148	1705
99feb84ac64be98145f4da11a73af40cfaa15260	modeling of domain-specific eca policies		Policy-based management is a flexible approach for the management of complex systems as policies make context-sensitive and automated decisions. For the effective development of policies it is desired to specify policies at a high level of abstraction initially, and to refine them until they are represented in a machine-executable way. We present an approach that uses models to specify event-condition-action (ECA) policies at different levels of abstraction. A relational algebra is used to specify and validate the models in a formal way. Finally, executable policy code is generated from the lowlevel models. The approach is generic as it can be applied to any domain and supports a flexible number of abstraction layers. It is applied to the network management domain and demonstrated with the modeling and refinement of policies for coverage optimization in a mobile network. Keywords-domain; policy; ECA; modeling; refinement; MDE; network management	abstraction layer;code generation (compiler);complex systems;context-sensitive grammar;domain model;event condition action;executable;graphical user interface;high- and low-level;high-level programming language;mathematical optimization;metamodeling;model-driven engineering;principle of abstraction;prototype;refinement (computing);relational algebra	Raphael Romeikat;Bernhard Bauer;Henning Sanneck	2011			systems engineering;computer science;relational algebra;complex system;abstraction;executable;network management;cellular network	AI	-45.40197684214614	23.56231502586807	1708
4aab30f0770a7ffe04d42d39979deb523dcc1257	graphical data management in a time-shared environment	data management;time sharing;large scale;oil industry;system development;data management system	At System Development Corporation there is a conviction that one of the most plausible ways to make the cost of software decline is to build general-purpose software that is capable of solving a variety of problems. SDC's most successful effort in this field has been in the area of general-purpose data management. Our initial large-scale, time-shared data management system, TSS-LUCID, enabled the nonprogrammer to describe, load, query, and maintain a data base. In use for over two years, this system provided enough generality to solve such diverse problems as comparison of salary data in different segments of the aerospace industry, analysis of statistical data for a customer in the oil industry, and monitoring of public-supported cancer research projects. Currently being implemented on the IBM 360 family of computers is an improved, more powerful version called TDMS (Time-Shared Data Management System).	computer;data hub;database;general-purpose markup language;general-purpose modeling;graphical user interface;ibm system/360;management system;secure digital container	Sally Bowman;Richard A. Lickhalter	1968		10.1145/1468075.1468128	simulation;data management;systems engineering;engineering;operations management	DB	-58.271439113365595	10.357188292239261	1710
be25e8ba64ec2c98d37f6c1dd26b7bd529425ab0	an empirical study of quality and cost based security engineering	corea;distributed system;outil logiciel;security engineering;empirical study;software tool;fiabilidad;reliability;systeme reparti;confidencialidad;methode empirique;information security;metodo empirico;securite informatique;systems engineering;empirical method;coree;production process;confidentiality;computer security;confidentialite;sistema repartido;internet;asie;fiabilite;seguridad informatica;processus fabrication;ingenierie systeme;korea;information system;herramienta software;systeme information;proceso fabricacion;asia;sistema informacion	For reliability and confidentiality of information security systems, the security engineering methodologies are accepted in many organizations. A security institution in Korea faced the effectiveness of security engineering. To solve the problems of security engineering, the institution creates a security methodology called ISEM, and a tool called SENT. This paper presents ISEM methodology considering both product assurance and production processes take advantages in terms of quality and cost. ISEM methodology can make up for the current security engineering methodology. For support ISEM methodology, SENT tool, which is operated in Internet, support the production processes and the product assurances which ISEM demands automatically.		Seok Lee;Tai-Myung Chung;Myeonggil Choi	2006		10.1007/11689522_35	computer security model;cloud computing security;security through obscurity;security engineering;computer science;information security;security service;empirical research;operations research;computer security	DB	-67.75454557778217	5.785511813040826	1712
a6c08fbdedd38b1c29c9baa034afcd8ba5360b33	understanding and supporting live and on-demand streaming service	live streaming service;live broadcasting;on demand streaming service;client server systems;system performance;p2p networks live streaming service on demand streaming service internet peer to peer networks live broadcasting client server systems cache and relay scheme;servers;client server;internet;peer to peer computing broadcasting client server systems media streaming;streaming media;system design;multimedia communication;next generation;media streaming;broadcasting ip networks next generation networking peer to peer computing availability scalability network servers anatomy system performance chromium;user behavior;p2p networks;broadcasting;communities;peer to peer computing;on demand;peer to peer;peer to peer networks;live streaming;music;cache and relay scheme;on demand peer to peer live streaming	Despite the popularity of streaming applications over Internet, little is known about its practical properties which could be potentially exploited to guide the system design of next generation peer-to-peer (P2P) networks. In this paper, we hence investigate the problem of understanding and supporting the two types of dominant streaming services - live broadcasting and on-demand streaming - with high service availability and system scalability. We make a measurement study to traditional but popular client/server (C/S) systems to reveal many fundamental and interesting observations with the benefit of totally more than 30,000,000 real workload traces. The anatomy particularly focuses on request and popularity issues which play a vital role to the system performance and also characterize evolutional user behaviors in the service community. The inherent request characteristics in live broadcasting service and drawbacks of classical cache-and-relay (CR) scheme in on-demand streaming further motivate us to discuss essential challenges and open questions in the context of supporting the two services over P2P networks.	client–server model;next-generation network;peer-to-peer;relay;scalability;server (computing);streaming media;systems design;tracing (software)	Yun Tang;Qian Meng;Kaiyun Zhang;Shiqiang Yang	2009	2009 WRI World Congress on Computer Science and Information Engineering	10.1109/CSIE.2009.354	computer science;music;computer performance;internet privacy;world wide web;broadcasting;computer network	Web+IR	-16.777516803092034	75.92486749876372	1720
c99091a9a24e1a639a9e87e16a9553b880dd402f	dhcp-based authentication for mobile users/terminals in a wireless access network	wireless access;controle acces;filtering;filtrage;routeur;access network;telecommunication sans fil;reseau acces;authentication;filtrado;wireless access network;authentification;service utilisateur;red acceso;autenticacion;internet;telecomunicacion sin hilo;router;access control;servicio usuario;user service;mobile user;wireless telecommunication	In this paper we present a simple solution to control the wireless users/terminals access to the services offered by a wireless access provider and ultimately to the global Internet. We enhance the DHCP configuration procedure with authentication capability, and add further filtering control at the access router interfacing with the global Internet. This required adding some functionality in the DHCP server; while the DHCP client is unchanged. This feature allows personalizing the offered service based on user's credentials and profile.	access network;authentication	Luca Veltri;Antonella Molinaro;O. Marullo	2004		10.1007/978-3-540-27824-5_165	ipconfig;network admission control;telecommunications;computer science;authentication;computer security;computer network	Mobile	-14.674056541053899	95.61816415112564	1721
7a45cfad9e1e3a4f015a07cf648c475177b91023	an extension to uml components to consider distribution issues in early phases of application development	application development;unified modeling language uml component distributed component based information system software engineering component based development technique distribution requirement functional requirement system architecture distributed service;component technology;formal specification;software architecture distributed processing object oriented programming unified modeling language formal specification;unified modeling language programming informatics information systems software engineering component architectures concurrent computing computer architecture service oriented architecture business communication;integrable system;component based development;distributed processing;object oriented programming;software engineering;software architecture;unified modeling language;secure system;interaction model;information system;functional requirement;component architecture;system architecture	Distributed component-based information systems are becoming one of the major trends in software engineering. Whereas distributed component technologies enable the development of reliable, scalable and secure systems, existing component-based development (CBD) techniques and methods do not explicitly address distribution. The earlier distribution requirements are considered and integrated with functional requirements, the least is the risk that the developed component architecture does not reflect these requirements. UML components is a widely known CBD method composed of a number of phases targeted at the identification of domain components (system and business), their respective interfaces, and their interaction model to compose the system architecture. In this work, we propose to extend the UML components method to encompass distribution issues in early phases of application development, namely requirements and specification. The basic idea is to identify distribution requirements that can be mapped into common distributed services (e.g. naming, concurrency). The extended method includes new or refined activities that help identifying distribution services, their provision through distributed components, and the resulting architecture that integrates system, business, and distribution components.	code generation (compiler);common object request broker architecture;component-based software engineering;concurrency (computer science);diagram;distributed computing;distributed element model;edward f. moore;entity;functional requirement;hoc (programming language);information system;scalability;systems architecture;unified modeling language	Anete Persch Espindola;Karin Becker;Avelino Francisco Zorzo	2004	37th Annual Hawaii International Conference on System Sciences, 2004. Proceedings of the	10.1109/HICSS.2004.1265649	unified modeling language;integrable system;software architecture;real-time computing;computer science;component-based software engineering;software engineering;applications of uml;formal specification;object-oriented programming;rapid application development;functional requirement;information system;systems architecture;systems design	SE	-38.28347526623576	40.02914641432441	1726
1beeb2fd89af3fbf980fc468e2f511cbed4bebb4	joza: hybrid taint inference for defeating web application sql injection attacks	databases;web services inference mechanisms security of data sql;approximation algorithms;payloads security encoding databases optimization inference algorithms approximation algorithms;hybrid taint inference approach joza web application sql injection attacks taint tracking techniques sql injection attack detection intrusive instrumentation data flow tracking program execution negative taint inference positive taint inference;web application security taint inference taint tracking sql injection;web application security;taint tracking;taint inference;sql injection;payloads;inference algorithms;optimization;security;encoding	Despite years of research on taint-tracking techniques to detect SQL injection attacks, taint tracking is rarely used in practice because it suffers from high performance overhead, intrusive instrumentation, and other deployment issues. Taint inference techniques address these shortcomings by obviating the need to track the flow of data during program execution by inferring markings based on either the program's input (negative taint inference), or the program itself (positive taint inference). We show that existing taint inference techniques are insecure by developing new attacks that exploit inherent weaknesses of the inferencing process. To address these exposed weaknesses, we developed Joza, a novel hybrid taint inference approach that exploits the complementary nature of negative and positive taint inference to mitigate their respective weaknesses. Our evaluation shows that Joza prevents real-world SQL injection attacks, exhibits no false positives, incurs low performance overhead (4%), and is easy to deploy.	dataflow;overhead (computing);php;prototype;sql injection;software deployment;synergy;taint checking;testbed;web application;wordpress	Abbas Naderi-Afooshteh;Anh Nguyen-Tuong;Mandana Bagheri-Marzijarani;Jason Hiser;Jack W. Davidson	2015	2015 45th Annual IEEE/IFIP International Conference on Dependable Systems and Networks	10.1109/DSN.2015.13	taint checking;web application security;payload;sql injection;computer science;information security;database;internet privacy;computer security;approximation algorithm;encoding	Security	-56.94989308240907	57.75534074953862	1730
42a701e9b7a15969a0a2319cd992fc618c736dee	scheduling of manufacturing systems under hard real-time constraints	mashrec architecture distributed scheduling algorithm manufacturing system real time systems transportation overheads;job shop scheduling manufacturing systems real time systems processor scheduling scheduling algorithm timing production systems time factors operations research educational institutions;real time systems production control computer aided production planning;distributed scheduling;production control;computer aided production planning;manufacturing system;hard real time;real time systems	This paper presents a distributed scheduling algorithm for the manufacturing system under hard real-time constraints. It uses a scheduling component local to every node and a distributed scheduling scheme that is specifically suited to hard real-time constraints and other timing considerations. Pre-planned parts, sporadic parts, scheduling overheads, transportation overheads due to scheduling, changeover overheads and pre-emption are all accounted for in the algorithm.	real-time clock;real-time computing;scheduling (computing)	Dania A. El-Kebbe Salaheddine	2001		10.1109/ICSMC.2001.973582	fair-share scheduling;nurse scheduling problem;fixed-priority pre-emptive scheduling;real-time computing;earliest deadline first scheduling;gang scheduling;flow shop scheduling;dynamic priority scheduling;computer science;rate-monotonic scheduling;genetic algorithm scheduling;two-level scheduling;deadline-monotonic scheduling;stride scheduling;distributed computing;scheduling;least slack time scheduling;lottery scheduling;round-robin scheduling;multiprocessor scheduling	Robotics	-10.594689978234383	61.17248373066045	1737
f0e58f7e71451d60a1e0bd1f932296c77b673817	cloud-enabled data sharing model	databases;encryption;database management systems;databases servers encryption data privacy organizations;conference paper;servers;data privacy;drntu engineering computer science and engineering information systems database management;security assurance cloud enabled data sharing model database as a service maintenance costs initial investment costs data privacy data security outsourced database service shared database;organizations;database management systems cloud computing data privacy;cloud computing	Database-as-a-Service is becoming more and more popular for many organizations. Storing data on the cloud can significantly reduce costs in terms of maintenance costs and initial investment costs. But due to data privacy and security concerns, many companies are still reluctant to use outsourced database service. In this paper, we present a solution to protect data privacy on the cloud, at the same time, allowing multiple users accessing the shared database with security assurance.	algorithmic efficiency;cloud computing;cloud database;computation;database server;encryption;information privacy;key (cryptography);multi-user;proxy server;public-key cryptography;server (computing)	Nguyen Thanh Hung;Do Hoang Giang;Wee Keong Ng;Huafei Zhu	2012	2012 IEEE International Conference on Intelligence and Security Informatics	10.1109/ISI.2012.6281922	cloud computing security;privacy software;cloud computing;information privacy;privacy by design;computer science;organization;data administration;database;data security;internet privacy;world wide web;computer security;encryption;database testing;server	DB	-41.76390389306407	66.28787098218513	1741
dc7a403edeb9ed5bb36e1578793adce0ac1a8d36	ebxml bp modeling toolkit	unified modeling language collaborative work standards organizations internet libraries protocols resource description framework moon collaboration business;united nations;formal specification;xmi ebxml bp modeling toolkit business process specification xml based b2b standard framework internet un cefact modeling methodology umm uml unified modeling language bp modeler bp editor built in bp registry client xml metadata interchange;business process model;business model;internet;specification languages business data processing xml internet software tools formal specification;specification languages;business data processing;unified modeling language;xml;software tools;modeling methodology;business process;modeling tool	Collaboration in a business system requires a business process specification defining the procedure of the business scenario. The business process specification is generated from a business process model. ebXML, which is the XML-based B2B standard framework for organizations of any size using the Internet, recommends process analysts and modelers to use the UN/CEFACT Modeling Methodology (UMM). The artifacts of the modeling are UML diagrams and worksheets. They can be transformed into an ebXML Business Process (BP) specification and other business models. The artifacts and transformed results are registered in the business library for being shared with ebXML systems and being re-used in other modeling tools. This paper reports on our implementation of an ebXML BP modeling toolkit that accepts the architecture suggested in the ebXML and considers the required functions of modeling. These functions include modeling business processes based on UMM, generating a BP specification, transforming the processes using a metaframework, and registering them in the ebXML registry. The business process modeling toolkit is made up of the business process modeler, business process editor, and built-in registry client. The business process modeler not only models the business process with UML diagrams but also generates the business process specification, reverses it, and exports XMI of the business process. The business process editor is used only for editing the business process specification. The built-in registry client stores the business process model, the business process specification, or the XMI document, and searches and loads them.	business process;canonical account;diagram;ebxml;process modeling;process specification;un/cefact;unified modeling language;xml metadata interchange	Jinyoung Moon;Daeha Lee;Chankyu Park;Hyunkyu Cho	2003		10.1109/EDOC.2003.1233858	business model;unified modeling language;the internet;xml;semantics of business vocabulary and business rules;business domain;business requirements;computer science;systems engineering;artifact-centric business process model;business process management;process modeling;data mining;formal specification;database;business process model and notation;process management;business system planning;business process;event-driven process chain;process mining;programming language;ebxml;business process discovery;management;business rule;world wide web;business process modeling;business activity monitoring;business architecture	Web+IR	-53.738167710492334	19.046135477189186	1742
48e1b0ae77e61d3211b2727b920060322b38e05d	an algorithm for learning with probabilistic description logics	description logic	Probabilistic Description Logics are the basis of ontologies in the Semantic Web. Knowledge representation and reasoning for these logics have been extensively explored in the last years; less attention has been paid to techniques that learn ontologies from data. In this paper we report on algorithms that learn probabilistic concepts and roles. We present an initial effort towards semi-automated learning using probabilistic methods. We combine ILP (Inductive Logic Programming) methods and a probabilistic classifier algorithm (search for candidate hypotheses is conducted by a Noisy-OR classifier). Preliminary results on a real world dataset are presented.	algorithm;description logic;expectation–maximization algorithm;experiment;inductive logic programming;inductive reasoning;knowledge representation and reasoning;ontology (information science);scalability;semantic web;semiconductor industry	José Eduardo Ochoa Luna;Fábio Gagliardi Cozman	2009			natural language processing;probabilistic analysis of algorithms;probabilistic ctl;probabilistic relevance model;computer science;machine learning;data mining;probabilistic logic;probabilistic argumentation	AI	-21.565027424537256	7.09546811521395	1748
59e4cc7fa90690f63ce94f0e73f91e6047d47a9a	evaluation of the ability of petri net centralized implementation techniques	force;firing;petri nets data structures;synchronization;data structures;heuristic algorithms;data structure petri net centralized implementation technique petri net execution algorithm;petri nets;petri net;data structure;algorithm design;algorithm design and analysis;heuristic algorithm;heuristic algorithms algorithm design and analysis firing petri nets synchronization data structures force	In this work we present an evaluation of the ability of interpreted and centralized implementation techniques of Petri nets. The main purpose of this work is the analysis of the performance of the Petri Net execution algorithms, comparing the performance of algorithms through a parameter that does not depend on the execution platform. This parameter is the ability of the implementation that is independent of the execution platform. Firstly there is a study of the size of data structures of the Petri Net execution algorithms. There is also a review of the term ability, defining the ability of the Representing Places technique, therefore allowing a comparative study of the ability of the different techniques.	centralized computing;petri net	Ramon Piedrafita Moreno;José Luis Villarroel Salcedo	2011		10.1109/CDC.2011.6161119	algorithm design;real-time computing;stochastic petri net;data structure;computer science;theoretical computer science;distributed computing;process architecture;petri net	EDA	-24.802752454226063	49.03537724657275	1749
f23340fddfc147488b7b327facd6f09877c1ab59	notice of retractionnew product development, strategy crafting, and the performance of entrepreneurial firms	entrepreneurial firms;entrepreneurial firm;sample selection;technological innovation;firm innovation product development strategy crafting entrepreneurial firm firm performance;government;product development innovation management organisational aspects;firm innovation;product development technological innovation government innovation management book reviews marketing and sales;entrepreneurial firms new product development strategy crafting firm performance firm innovation;innovation management;book reviews;strategy crafting;new product development;firm performance;marketing and sales;organisational aspects;product development	New product development is playing a key role in the performance of entrepreneurial firms. New product development strategy crafting is becoming more and more important. However, the research on the relationship between new product development strategy and the performance of entrepreneurial firms is still questionable yet. The problem is that the previous researches are conducted independently and sample selections are not continuous. Based on the comments of previous research, some implications are valuable for further research.	new product development	Fengzeng Xu;Chenguang Wang	2010	2010 International Conference on E-Business and E-Government	10.1109/ICEE.2010.305	economics;innovation management;marketing;management;new product development	EDA	-83.21091617051987	5.564289055646334	1753
caca3d66205bd34d5a0ced08daf73cba841e360b	sustainability in manufacturing operations scheduling: stakes, approaches and trends		In this paper it is explained how manufacturing operations scheduling can contribute to sustainability. For that purpose, the relevant stakes are first presented. Sustainable manufacturing operations are then characterized. Different forms of sustainability in manufacturing operations scheduling are pointed out and some illustrative contributions are positioned.	scheduling (computing)	Damien Trentesaux;Vittaldas Prabhu	2014		10.1007/978-3-662-44736-9_13	engineering;operations management;industrial engineering;management science	Robotics	-62.195894999127404	8.558283645733688	1759
2275fd03b4e72867a2067c63b2195614c16533c9	an effective user behavior modeling approach for data services in the field of materials engineering	materials engineering;data services;user behavior modeling;cloud computing	New challenges about data services have arisen, especially consider the impact of user behavior. For dealing with the problem of distributed heterogeneous data sharing and satisfying the demands of data service, the complex association and dynamic changes should be tracked timely. Thereby it has become increasingly important to build a data services framework based on the user behavior analysis. In this paper, we propose an effective user behavior modeling approach based on the Open Cloud Service Architecture (OCSA) to manage the domain scientific data. Firstly, we build the data services framework in the cloud environment, and describe the related theories and conceptual model about this framework. Based on this, we elaborate the definition and classification of user behavior. Secondly, we construct the User Behavior Model (UBM) by tracking the user operation behavior respectively from the time dimension and the space dimension. Finally, we designed and realized a behavior-oriented Materials Scientific Data Sharing Service Platform. Through the description and comparison of application case, the validity of user behavior modeling	algorithm;behavior model;cloud computing;knowledge engineering;personalization;plan 9 from bell labs;theory	Xin Cheng;Changjun Hu;Yang Li;Wei Lin	2013	JCP	10.4304/jcp.8.11.2972-2979	user interface design;user modeling;computer user satisfaction;cloud computing;computer science;operating system;data mining;database;data as a service;world wide web;computer security	HPC	-44.4111195199428	16.521337196727977	1762
52ea038966b7d356088ffd8c720c7663a21fbfaa	making sense of ratings: a common quantitative feedback ontology	ratings;user feedback;open world assumption;semantic interoperability;semantic web;of the web ontology language;domain specificity;meta model	This paper proposes a common ontology for ratings, i.e. for quantitative user feedback data. Such a framework allows for semantic interoperability of data that adheres to it, which in turn enables the re-use, by making it independent from the original system.  In contrast to prior attempts to establish an unambiguous vocabulary, this approach introduces two components that are in our view necessary to formally understand what a user's rating actually means. The first is the aspect or facet, i.e. the viewing angle that was chosen to look at the rated thing. The second is the meta-model of scales following the scales of measurement that are widely used in descriptive statistics. So in plain words, we allow to formally specify how many out of how many score points something gets and with regards to what.  We follow the open world assumption of the Web Ontology Language (OWL) and design a vocabulary that is not specific to any domain. In turn, we rely on the premise that all domain specific concepts are available as semantic web resources with appropriate URIs.	level of measurement;metamodeling;ontology (information science);open world;open-world assumption;semantic web;semantic interoperability;viewing angle;vocabulary;web ontology language;web resource;world wide web	Florian Marienfeld;Edzard Höfig;Andrea Horch;Maximilien Kintz;Jan Finzen	2011		10.1145/2063518.2063526	upper ontology;bibliographic ontology;computer science;ontology;data mining;world wide web;owl-s;information retrieval	Web+IR	-45.30813450178958	7.287580383990868	1764
34eed2b0c02718f0161b0b518bf5c145da300dc1	language completeness of the lambek calculus	calculus logic mathematics artificial intelligence;formal languages;group theory;group theory formal languages formal logic;formal logic;free semigroup models language completeness lambek calculus noncommutative linear logic l models;linear logic	W e prove that the Lambek calculus (essentially a subsystem of non-commutative linear logic) is complete w.r.t. L-models, i.e., free semigroup models. Introduction In 1958 J. Lambek [8 introduced a calculus for degroup models (also called language models or L-models) for this calculus were considered in [2], [3], and [4]. The more eneral class of groupoid models has been studied in f5], [6], and [7]. In [3] W. Buszkowski established that the product-free fragment of the Lambek calculus is L-complete (i.e., complete w.r.t. free semigroup models), using the canonical model. The question of L-completeness of the full Lambek calculus remained open (cf. [l]). This problem has attracted the attention of the group of mathematical logicians attending S. Artemov's workshops at Moscow University. In 1991-1992 N. Pankratiev presented an exposition of the latest results in the field and noticed that canonical models can not be used for proving L-completeness of the full calculus. In December 1992, after one of the workshops at Moscow University M. Kanovich announced about a positive solution of the language completeness problem for the full Lambek calculus. However, he has not presented the proof yet. A fortnight later, at the end of December 1992, the author of this paper came up with his own proof. It has been improved after several talks in Amsterdam and Moscow. In this paper we deliver the modified proof. 1 Preliminaries 1.1 Lambek calculus We consider the syntactic calculus introduced in [8]. The types of the Lambek calculus are built of primitive types PI,-, . . ., and three binary connectives 0 , \, /. We shall denote the set of all types by Tp. The set of finite sequences of types (resp. finite non-empty sequences of types) is denoted by Tp* (resp. Tp'). The symbol A will stand for the empty sequence of types. Capital letters A,B, . . . range over types. Capital Greek letters range over finite (possibly empty) sequences of types. riving reduction laws o I syntactic types. Free semiSequents of the Lambek calculus are of the form r + A , where I' is a non-empty sequence of types. Axioms: A+A Rules: (here IT E Tp') An+B @+A I'BA+C (\+) n+A\B r@(A\B)A+C llA+B @+A l?BA-+C (/+I l l + B / A r ( B / A ) @ A + C I'+A A+B FABA+C (.+I rA+A.B r(A.B)A+C The cut-elimination theorem for this calculus is proved in [8]. We write L Ir-+A if the sequent r + A is derivable in the Lambek calculus. Definition. Let \lr(( denote the total number of primitive type occurreces in r. Let Var(A) denote the set of all primitive types occurring in A. Definition. For any integer m, we write Tp(m) for the finite set of types { A E Tp I Var(A) C note the set of all non-empty finite sequences of types from Tp(m). Definition. For any two integers m and n, we write LST,,, for the set LST,,, + { A l . . . A[ 1 1 5 I 5 1.2 Partial semigroup models Definition. We say that (W,o) is a partial semigroup iff o is a partial function from W x W into W such that, whenever ao(boc) or (aob)oc is defined, the other expression is also defined and ao(boc) = (aob)oc. (We do not exclude the case W = 0.) {Pl,P2,. . ,Pm) and IlAll I m). BY Tp(m)+ we den, AI E Tp(m), . . . , Al E Tp(m)). Example 1 Here are some examples of partial semigroups.	canonical model;categorial grammar;free monoid;l (complexity);language model;linear logic;logical connective;sequent calculus;strong np-completeness	Mati Pentus	1994		10.1109/LICS.1994.316042	predicate logic;zeroth-order logic;linear logic;formal language;discrete mathematics;classical logic;higher-order logic;formal semantics;many-valued logic;computer science;pure mathematics;first-order logic;mathematics;proof calculus;signature;noncommutative logic;situation calculus;curry–howard correspondence;programming language;group theory;natural deduction;logic;algorithm;philosophy of logic;algebra	Logic	-8.082740041025982	14.77374838119689	1765
26074d6ff38d7e7b005f87d0d230d2c553052036	implementierung einer agroxml-schnittstelle für landwirtschaftliche prozessdaten		The aim of agroXML as a data exchange format is to network process and business information along the food and value chain. In this paper the integration of an agroXML interface in the Agricultural Process Data Service (APDS) is presented. After evaluating different implementation methods the implementation itself resulted in recommendations for further development of agroXML. 1 Einleitung und Problemstellung Die Vernetzung von Prozessund Geschäftsinformation entlang der Lebensund Futtermittelkette von der Urproduktion bis zum verarbeiteten Produkt führt zu einer Reihe unterschiedlicher Anforderungen an Inhalt und Struktur der transportierten Daten. Weitere Anforderungen ergeben sich durch notwendige Informationsflüsse von und zu dieser Produktionskette, wie bei der Zulieferung von Betriebsmitteln, Einbindung von Beratern und Lohnunternehmern und durch die Informationspflicht gegenüber staatlichen oder beauftragten Kontrollinstanzen. Schließlich sollen innerhalb des landwirtschaftlichen Produktionsprozesses erfasste Daten vor allem dem Betriebsmanagement selbst für unterschiedlichste Anwendungen wie Buchführung, Abrechung, Schlagkartei oder die teilflächenspezifische Bewirtschaftung zur Verfügung stehen [RA04]. Während innerhalb des landwirtschaftlichen Betriebes bei Nutzung elektronischer Steuerungsund Dokumentationssysteme vor allem große Mengen von georeferenzierten MaschinenProzessdaten transportiert und verarbeitet werden müssen, spielen für den Austausch über die Betriebsgrenzen hinweg eher Informationen über geschäftliche Prozesse, jedoch zunehmend unter Einbeziehung von Geodaten und aus Prozessdaten ermittelten Kenngrößen, eine Rolle.	eine and zwei;internet explorer;vhf omnidirectional range;zentralblatt math	Georg Steinberger;Christine Spietz;Matthias Rothmund	2007			adhesion;mold;materials science;molding (process);granular material;composite material	DB	-101.54076597716181	34.297669065835144	1767
fe3e8b25b5684620a8c4531c830939a5562b6f52	an implementation of vulnerability evaluation system for network security on cc	network security		network security	Jeom Goo Kim;Young-Cheol Lee;Jae-Kwang Lee	2000			computer science;computer security model;threat;vulnerability management;vulnerability (computing);network access control;cloud computing security;computer security;security service;security information and event management	Security	-49.0735014223983	61.52243541596473	1779
d4b67655af11d98537c43d903b34756e4969eb77	scalable peer-to-peer networked virtual environment	fault tolerant;peer to peer network;interest management;p2p;peer to peer p2p;scalability;massively multiplayer;virtual environment;networked virtual environment nve;peer to peer;networked virtual environment;massively multiplayer mmp;voronoi diagram	We propose a fully-distributed peer-to-peer architecture to solve the scalability problem of Networked Virtual Environment in a simple and efficient manner. Our method exploits locality of user interest inherent to such systems and is based on the mathematical construct Voronoi diagram. Scalable, responsive, fault-tolerant NVE can thus be constructed and deployed in an affordable way.	fault tolerance;locality of reference;peer-to-peer;scalability;virtual reality;voronoi diagram	Shun-Yun Hu;Guan-Ming Liao	2004		10.1145/1016540.1016552	fault tolerance;real-time computing;scalability;simulation;voronoi diagram;computer science;virtual machine;peer-to-peer;database;distributed computing	HPC	-27.402785367627075	49.57163982386243	1789
0bbe902544ef5119336082d23b2f93536a04e79c	asp for minimal entailment in a rational extension of sroel		In this paper we exploit Answer Set Programming (ASP) for rea soning in a rational extension SROEL(⊓, ×)R T of the low complexity description logic SROEL(⊓,×), which underlies the OWL EL ontology language. In the extended language, a typicality operator T is allowed to define concepts T(C) (typical C’s) under a rational semantics. It has been proven that instance checking under rational entailment has a polynomial complexity. To strengthen rational entailment, in t his paper we consider a minimal model semantics. We show that, for arbitrary SROEL(⊓,×)RT knowledge bases, instance checking under minimal entailme nt is Π2 -complete. Relying on a Small Model result, where models cor respond to answer sets of a suitable ASP encoding, we exploit Answer Set Preferences (and, in par ticular, theasprin framework) for reasoning under minimal entailment. The paper is under consideration for acceptance in Theory and Practice of Logic Programming.	answer set programming;association for logic programming;description logic;knowledge base;ontology (information science);polynomial;resources, events, agents (accounting model);stable model semantics;time complexity	Laura Giordano;Daniele Theseider Dupré	2016	TPLP	10.1017/S1471068416000399	natural language processing;textual entailment;preferential entailment;algorithm	AI	-19.965223899902323	9.58795492636132	1791
8235ebe5c72cb2f5c802fca1a08e965ea1a66512	modeling and investigation of a primitive file transfer operation	markov processes;computer communications software;distributed processing;file organisation;markov chain analysis;unix united system;distributed systems;file migration;file transfer;high-speed networks;load balancing;measurement data;modelling;performance model;prefetching;primitive file transfer operation;process migration;read phase;write phase	In recent years, the availability o f high-speed networks and the reduced cost of powerful workstations have led to wide-spread use o f distributed systems. File transfer is an important operation in a distributed system. It can considered as a primitive operation in high-level applications such as IJe migration, pr6 cess migration, and load balancing. In this paper, we develop a performame model for jik transfer which consists o f three submodels, one for each phase o f #le t r d e r . The read phase incorporates prefetching (read-ahead) Qjik blocks. the write phase incorporates pastwriting (write-behind), ami the control phase forces the jib transfer to be sequential. The throughput of jile transfer is ohained analytically using a a Markov chain analysis. The validity 4 the model is assessed using measurement data from a Unk United system. Additional results on the performance offle transfer are also presented.	cpu cache;cache (computing);disk buffer;distributed computing;file transfer;high- and low-level;load balancing (computing);markov chain;reduced cost;throughput;workstation	Richard T. Hurley;James P. Black;Johnny W. Wong	1992			self-certifying file system;parallel computing;real-time computing;device file;computer science;distributed computing;file descriptor;distributed file system;file system fragmentation	Arch	-14.69776015089514	51.25667767876494	1794
2fa80b606107fa236912375bdb3e6a74ed3d8e81	bayesualize: visualisierung bedingter wahrscheinlichkeiten		Zusammenfassung: Bayesualize ermöglicht das komfortable Berechnen von Übergangsund Randverteilungen und die Darstellung in Baumdiagrammen, im Einheitsquadrat sowie als Paling-Palette. Die Darstellung erfolgt wahlweise mit Wahrscheinlichkeiten, mit Prozentangaben oder mit natürlichen Häufigkeiten. Statistisches Material, das zum Beispiel aus Studien gewonnen wurde, kann mit Bayesualize aufbereitet und visualisiert werden. Das Programm ist für tutorielle Zwecke und zur Veranschaulichung von Risiken konzipiert.	the daily telegraph	Barbara Messing;Michael Seubert	2010				Theory	-103.99036744450032	29.433661243981415	1806
50404eba5fff055145a0ca147d640c5cb5a8693a	automated methods for identity resolution across heterogeneous social platforms	social networks;identity resolution;username history;profile linking;profile search	Users create identities on multiple social platforms for various purposes but often do not link them. Unlinked identities raise concerns for enterprises and security practitioners. To address the concerns, we propose novel methods to search and link user identities scattered across heterogenous social networks. Our methods are automated and access only public current and historic data of a user. Evaluation on fairly large datasets from multiple platforms prove our methods' efficiency in identity resolution of an online user.	social network	Paridhi Jain	2015		10.1145/2700171.2804448	internet privacy;world wide web;computer security;social network	ML	-55.78447961763241	62.90581883108389	1810
b0304fc4ff1cf9f2c363675cfe23601aba82ce08	a multi-carrier protocol for in-band control of tactical data network systems		Network management requires both awareness of the status of the network and the ability to manage key systems to maintain full network efficiency. OrderNet is an application layer protocol which rides within existing message sets to manage heterogeneous networks. This paper summarizes the analysis of alternatives performed that selected OrderNet as a promising technology for the monitoring, management, and control of tactical data networks. This paper also describes selection of the OrderNet message carriers, the OrderNet protocol design and preliminary performance analysis.	communications protocol;genetic heterogeneity;numerous	Robert L. Riley;Maryanne L. Domm	2017	2017 IEEE Conference on Control Technology and Applications (CCTA)	10.1109/CCTA.2017.8062436	application layer;network architecture;heterogeneous network;network management;internet protocol suite;network management station;tunneling protocol;network management application;real-time computing;computer network;computer science	Embedded	-18.817499405580072	86.38039225176298	1819
0e6b1f2b3f57b226ca71356173b37b022f4b57dd	la non-commutativité comme argument linguistique : modéliser la notion de phase dans un cadre logique	grammaires minimalistes;mots cles modelisation linguistique;syntax;minimalist categorial grammars;logic;categorial grammars;syntaxe;grammaires cate gorielles;mini malist grammars;generative theory;theorie generative;logique;grammaires minimalistes categorielles keywords linguistic modeling	RESUME. L'une des questions du traitement automatique des langues est de discuter de la realite de la capacite langagiere des formalismes. Au dela de la modelisation linguistique, la theorie generative de Chomsky et le minimalisme s'interessent a apprehender le langage humain en tant que processus cognitif, ce qui conduit a introduire le principe de derivation par phases. Une premiere formalisation du minimalisme a ete proposee dans (Stabler, 1997) afin, notamment, d'en etudier les proprietes computationnelles. L'extension formelle proposee ici, basee sur les Grammaires Minimalistes Categorielles, (Amblard, 2011), s'attache a integrer la notion de phase dans un cadre logique qui permet aussi de definir un calcul semantique. Les enjeux de cette modelisation nous amenent a discuter de la commutativite et de la non-commutativite dans le formalisme. ABSTRACT. One of the recurring questions in natural language processing is the models's ability to account for the reality of language ability. Chomsky's Generative Theory and Minimalism are interested in understanding human language as a cognitive process, which is especially highlighted in the latest proposals by the principle of derivation by phases. A first formaliza-tion of Minimalism was introduced in (Stabler, 1997) to study the computational properties. The extension proposed here attempts to account for the idea of phase in a logical framework that allows to easily define a semantic calculus from parsing. This approach raises the problem of using the commutativity and non-commutativity in the Minimalist Categorial Grammars , (Amblard, 2011).	linear algebra	Maxime Amblard	2015	TAL		artificial intelligence;mathematics;linguistics;algorithm	Crypto	-12.552679320615676	7.280210444544376	1825
ace40f2f471759052d761ad471512c910d34a09e	the ietf administrative oversight committee (iaoc) member selection guidelines and process		This memo outlines the guidelines for selection of members of the IETF#N#Administrative Oversight Committee, and describes the selection#N#process used by the IAB and the IESG. This document specifies an#N#Internet Best Current Practices for the Internet Community, and#N#requests discussion and suggestions for improvements.		Geoff Huston;Bert Wijnen	2005	RFC	10.17487/RFC4333	accounting;public relations;political science;data mining	HCI	-26.74517952387503	88.98680702586113	1829
4e12b8b39dcb0b746e17f7396356419e5f266910	user-preference-based service selection using fuzzy logic	component services;service composition;formal specification;service selection;interaction interface;user preferences;web service;non quantifiable factor;fuzzy logic;network based services;vague opinions;time factors;automatic selection service selection vague opinions policy management non quantifiable factor fuzzy logic;internet;engines;user preference based selection engine;web services;user requirements;internet services;web services engines quality of service fuzzy logic decision making humans time factors;user centred design;humans;user preference based service selection;user requirements user preference based service selection fuzzy logic web services interaction interface network based services internet service composition component services user preference based selection engine;quality of service;web services formal specification fuzzy logic user centred design;automatic selection;policy management	Web services provide a standard interaction interface for network-based services. With so many diverse web services available over the Internet, services can be composed together and delivered to users as a package. The way to combine services is still an open problem, especially considering users' preferences. Users always have vague opinions on these preferences when they choose component services. In this paper, we propose a user-preference-based selection engine to compose services, which allows users to define non-quantifiable factors and policies to represent their preferences. Then, the engine automatically composes web services into a package following these policies by use fuzzy logic. The engine ensures the whole package has the most satisfaction.	fuzzy logic;internet;subject (philosophy);vagueness;vii;web service	Zhengping Wu;Mu Yuan	2010	2010 International Conference on Network and Service Management	10.1109/CNSM.2010.5691228	web service;computer science;ws-policy;data mining;database;services computing;law;world wide web	DB	-45.06610990209134	14.51390655960713	1840
5eb7f6edd1b5b4692c4b291996484dde08c8e743	selective early request termination for busy internet services	request termination;load shedding;network server;design and implementation;internet services;network services	Internet traffic is bursty and network servers are often overloaded with surprising events or abnormal client request patterns. This paper studies a load shedding mechanism called selective early request termination (SERT) for network services that use threads to handle multiple incoming requests continuously and concurrently. Our investigation with applications from Ask.com shows that during overloaded situations, a relatively small percentage of long requests that require excessive computing resource can dramatically affect other short requests and reduce the overall system throughput. By actively detecting and aborting overdue long requests, services can perform significantly better to achieve QoS objectives compared to a purely admission based approach. We have proposed a termination scheme that monitors running time of requests, accounts for their resource usage, adaptively adjusts the selection threshold, and performs a safe termination for a class of requests. This paper presents the design and implementation of this scheme and describes experimental results to validate the proposed approach.	function overloading;internet protocol suite;load shedding;quality of service;sensor;server (computing);server efficiency rating tool;throughput;time complexity	Jingyu Zhou;Tao Yang	2006		10.1145/1135777.1135866	real-time computing;computer science;database;world wide web;computer security;server	Web+IR	-17.149735719435053	70.30449355658675	1841
1d4523773f3158edfbce0c4f85118593096ce8cd	optimized deployment of network function for resource pooling switch		The disadvantages of the combination of traditional switches and middleboxes have being exposed under the condition of increasingly various network function demands,such as function flexibility, performance scalability and resource utilization. To solve this problem, we design Resource Pooling Switch Architecture (RPSA), which separates some non-essential functions from line card and allocate them in Network Function Pool (NFP) to provide flexible services for data plane in the form of Service Function Chains (SFC). As the performance of the whole system could be decided by whether function deployment is reasonable or not, we purpose heuristic algorithm called Modified Fiduccia-Mattheyses based Two Phase Algorithm (MFMTP) to optimize the deployment of functions. The simulation results show that this algorithm performs well in throughput and convergence. Keywords—Switch Architecture, Function Deployment, Network Function Virtualization, Service Function Chain	algorithm;forwarding plane;heuristic (computer science);middlebox;network function virtualization;network switch;scalability;simulation;software deployment;space-filling curve;throughput;transfer function;vergence	Fei Hu;Jiong Du;Du Xu	2018	CoRR		software deployment;throughput;scalability;pooling;architecture;computer science;distributed computing;heuristic (computer science);forwarding plane;convergence (routing)	Networks	-14.258973845106034	83.09087977527368	1844
c4e2d8e383fed0c1a287763d0379ec31fb4bdefe	software safety assessment and probabilities	software reliability probability safety critical software software metrics;standards;systematics;safety standards probabilistic logic systematics software safety context;software safety;ssil software probabilistic safety assessment rationale of safety standards dal sil asil;safety;probabilistic logic;context;probabilistic safety assessment software safety assessment software complexity software size global system safety assessment	In a context where software is more and more pervasive in all systems, and where it is sometimes advocated that software complexity and size seem to provide some relevance to a probabilistic view of software behaviour, several initiatives suggest to change the way to address software in the global system safety assessment. The authors argue that whereas there are many links between safety assessment and probabilities, both at global level and for what concern random causes of failures, there are many reasons why this is not easily applicable to systematic causes in general and software in particular.	data assimilation;failure cause;failure rate;pervasive informatics;programming complexity;relevance;system safety;turing completeness	Jean-Paul Blanquart;Philippe Baufreton;Jean-Louis Boulanger;Jean-Louis Camus;Cyrille Comar;Hervé Delseny;Jean Gassino;Emmanuel Ledinot;Philippe Quéré;Bertrand Ricque	2016	2016 46th Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshop (DSN-W)	10.1109/DSN-W.2016.19	reliability engineering;long-term support;verification and validation;software verification;computer science;software reliability testing;software development;software design description;software construction;data mining;systematics;probabilistic logic;software walkthrough;software quality;software metric;avionics software;software peer review	SE	-60.748598561380646	31.999288081086814	1848
568189f7041f69149fc12aafadd8c74348af92ac	the qbfeval web portal	portail web;ing inf 05 sistemi di elaborazione delle informazioni;intelligence artificielle;logical programming;portal web;programmation logique;web portal;artificial intelligence;inteligencia artificial;programacion logica	The QBFEVAL web portal is an on-line resource supporting the participants and the organizers of the yearly evaluation of QBF solvers and instances. In this paper we describe the main features of the current release of QBFEVAL.	online and offline;true quantified boolean formula	Massimo Narizzano;Luca Pulina;Armando Tacchella	2006		10.1007/11853886_45	computer science;artificial intelligence;multimedia;world wide web	NLP	-20.535988117351774	11.90152637851586	1851
f9baafb405670d700b3bfdc888422c4f3702018a	critical infrastructure protection (ceip) with a focus on energy security			critical infrastructure protection	Alessandro Niglia	2015		10.3233/978-1-61499-572-2-5	critical infrastructure;critical infrastructure protection;business;energy security;environmental resource management;computer security	Crypto	-57.03058836020734	50.00686798009166	1852
b7cde871b2ce6272c17f5bbf3cdeb829eaf7fac7	discovery of two latent loops of designer-user interaction in the design process				Jaehyun Park;Dick Boland	2017			knowledge management;systems engineering;engineering design process;computer science	HCI	-60.388836006604315	11.326095394712727	1853
f395a60d4128c15d3e62ced60b847e17d9019a67	parallel computing of catchment basins in large digital elevation model	image segmentation;digital elevation model;catchment basin;parallel computer;local computation;parallel implementation	This paper presents a fast and flexible parallel implementation to compute catchment basins in the large digital elevation models (DEM for short). This algorithm aims at using all the specific properties of the problem to optimize local computations and to avoid useless communications or synchronizations. The algorithm has been implemented in MPI and the first benchmarks show the scalability of the method.	algorithm;approximation;automatic parallelization;computation;data structure;digital elevation model;image resolution;image segmentation;mathematical morphology;message passing interface;parallel algorithm;parallel computing;relevance;scalability;watershed (image processing)	Hiep-Thuan Do;Sébastien Limet;Emmanuel Melin	2009		10.1007/978-3-642-11842-5_17	parallel computing;digital elevation model;computer science;theoretical computer science;distributed computing;image segmentation	HPC	-7.0038685180959375	40.71546047692175	1858
94563c0bb1f9cdf0cfee58522977938e88c43276	the gridsite web/grid security system	gridsite;apache;production system;gsi;virtual organization;secure system;group membership;access control;gacl;grid security;x 509	This paper describes the architecture of the GridSite system, which adds support for several Grid security protocols to the Apache Web server platform. These include the Globus GSI authentication system, Grid Access Control Language (GACL) access policy files, and Distinguished Name (DN) List and Virtual Organization Membership Service (VOMS) group memberships. Particular emphasis is placed on how the architecture of GridSite has evolved during the past 3 years, how this has been influenced by operational experience with production systems, and how the project has led to new developments, such as GACL. Finally, a description is given of how GridSite has been made to interoperate with other deployed security systems, both as producers and consumers of GridSite’s authorization information. Copyright c © 2005 John Wiley & Sons, Ltd.	access control;authentication;authorization;interoperability;john d. wiley;lightweight directory access protocol;server (computing);voms;virtual organization;web server	Andrew McNab	2005	Softw., Pract. Exper.	10.1002/spe.690	computer science;access control;operating system;x.509;database;production system;world wide web;computer security	Security	-32.8977082171158	52.85743147561452	1862
75dc65c26d92e646887adc95c51856f103631849	component agent framework for domain-experts (cafne) toolkit	agent platform;tool support;prometheus design tool;software engineering;automatic generation;agents;agent software engineering;methodologies;domain experts;agent systems	The Component Agent Framework for domain-Experts (CAFnE) toolkit is an extension to the Prometheus Design Tool (PDT). It uses the detailed design produced by PDT with further annotations by domain experts to automatically generate executable code into a desired agent platform. The key feature of CAFnE is that it allows domain experts with limited programming skills to easily build and modify agent systems.	design tool;executable;php development tools;prometheus	Gaya Buddhinath Jayatilleke;John Thangarajah;Lin Padgham;Michael Winikoff	2006		10.1145/1160633.1160917	simulation;computer science;artificial intelligence;software agent;methodology;data mining	ML	-43.192265130440326	22.802074543508706	1865
6468be894eddb9cbd6a97dd2c544e2025f037ffa	business rules design and refinement using the xtt approach	business rules;rule based system	This paper discusses practical development of business rules (BR) systems. The main conceptual foundations of BR are discussed, and selected tools for BR design and implementation are described. An application of a rule-based systems design tool (Mirella Designer) for BR design is proposed. Business rules visual representation using XTT in Mirella Designer is both transparent and compact. The underlying logical formulation of XTT opens up possibility of formal analysis and refinement performed on-line, during the design.	design tool;online and offline;refinement (computing);rule-based system;systems design	Grzegorz J. Nalepa	2007			specification pattern;natural language processing;rule-based system;design tool;artificial intelligence;semantics of business vocabulary and business rules;data mining;systems design;business rule management system;production rule representation;computer science;business rule	EDA	-38.88499216724771	9.296499676677508	1872
54d287ed0a26c8040954d1d324f00c221e8333c6	using microprocessors without rom		Abstract   The use of ROM (ready only memory) with Motorola M6800 microprocessor systems is made unnecessary. The contents of RWM (read write memory) are loaded from a central computer, and the microprocessors are then directed to start, stop or change programs from the central computer.  Macquarie University has a Hewlett-Packard 2100S computer running a number of terminals used to control experiments, with data transfer in either direction at speeds of up to 300 k words per second. Microprocessors can be attached to these terminals and loaded with M6800 machine language programs which are stored on the disc memory of the central computer. Commands can then be issued from the central computer to control the microprocessors.  The M6800 machine language is produced by a crossassembler which runs on the HP 2100S. This program accepts M6800 assembler language and generates M6800 machine language which is stored on disc. Programs can then be selected from disc and sent to RWM of the microprocessors, and execution commenced.	microprocessor;read-only memory	A. J. Martin-Jones;D. A. Payne	1977	Microprocessors	10.1016/0308-5953(77)90191-X	embedded system;real-time computing;computer hardware;computer science;electrical engineering;operating system;database;programming language	EDA	-14.259843676540363	33.22104083383372	1877
dfef3297a96ade6ca92e70584dd266e823afc6f5	access network design for optical flow switching	optical burst switching;optical network;network design;access network;generalized multi protocol label switching;optical design optical fiber networks image motion analysis optical packet switching optical network units optical sensors bandwidth optical wavelength conversion wide area networks optical devices;building block;optical amplifier access network design optical flow switching tree based network;physical layer;optical amplifier;trees mathematics optical burst switching;optical flow switching;packet switched;trees mathematics;cost efficiency;present day;network architecture;economic integration;metropolitan area network;local area network	In this work, we consider access network design for the Optical Flow Switching (OFS) transport architecture [1]-[3]. Our work addresses the all-optical physical layer of the data plane in the context of tree-based networks. We consider the advantages and disadvantages of passive and active components - including optical amplifiers - and ultimately integrate these building blocks in the most economically attractive fashion for the required number of users.	access network;forwarding plane;network planning and design;optical amplifier;optical flow	Guy Weichenberg;Vincent W. S. Chan;Muriel Médard	2007	IEEE GLOBECOM 2007 - IEEE Global Telecommunications Conference	10.1109/GLOCOM.2007.455	local area network;economic integration;network planning and design;network architecture;optical burst switching;metropolitan area network;telecommunications;computer science;optical amplifier;physical layer;computer network;cost efficiency;access network	EDA	-10.37725676721707	86.01344118100275	1880
a497b3e38da0a07d74c89dfc568481a65880a75b	model-driven distributed systems	program behavior;developpement logiciel;modelizacion;lenguaje programacion;distributed system;interfase usuario;industrial case study;syntax;architecture systeme;systeme reparti;programming language;programming environment;red petri;user interface;distributed processing petri nets systems analysis;distributed processing;distributed system development distributed systems component behavior system design petri net based executable modeling language direct mapping modeling constructs electronics manufacturing shop floor business process change;comportamiento programa;ingenieria logiciel;syntaxe;software engineering;modeling language;analisis programa;modelisation;medio ambiente programacion;sistema repartido;business process change;design and implementation;systems analysis;drives unified modeling language manufacturing systems control systems business product design embedded software software tools control system synthesis centralized control;system design;desarrollo logicial;software development;genie logiciel;langage programmation;comportement programme;arquitectura sistema;interface utilisateur;program analysis;petri nets;analyse programme;sintaxis;system architecture;petri net;modeling;reseau petri;environnement programmation	"""The work described in this article supports the view that distributed-system component behavior can be captured during system design and directly used to drive the system at runtime. The authors present methods for achieving this """"one step"""" design and implementation of system behavior through the use of a Petri-net-based executable modeling language. This language provides a direct mapping between modeling constructs and executable code. The work is set in an industrial context through a real problem in electronics-manufacturing shop-floor system design. Through a description of the proposed facilities and their application in an industrial case study, the authors conclude that the decomposition of function, behavior, and system interaction better supports rapid distributed-system development and the required modification of systems to support business-process change."""	distributed computing;model-driven integration	Ian A. Coutts;John M. Edwards	1997	IEEE Concurrency	10.1109/4434.605919	real-time computing;simulation;computer science;operating system;programming language;petri net;systems architecture	Embedded	-39.40765418714929	25.735025362633984	1887
e8d070139957cd63d8d709b0f64434ef96c89d9b	sequential modeling for obfuscated network attack action sequences	unintentional noise sequential modeling network attack action sequences obfuscation technique intentional obfuscation;computer network security;noise analytical models correlation monitoring robustness pattern matching	This work presents a preliminary study of modeling obfuscation techniques for network attack action sequences. The aim is to develop a systematic analytical methodology that shows how intentional obfuscation or unintentional noise may affect the modeling and classification of attack sequences. Discussions on different types of obfuscation techniques will be given, followed by a formal formulation to represent the effect of obfuscation within attack sequences. Preliminary simulation results are presented to demonstrate the impact of attack sequences with and without obfuscated attack actions.	obfuscation (software);simulation	Haitao Du;Shanchieh Jay Yang	2013	2013 IEEE Conference on Communications and Network Security (CNS)	10.1109/CNS.2013.6682742	computer science;network security;data mining;internet privacy;computer security	Vision	-61.94921630400403	60.89014514940791	1893
4ee94360be7639024a0be01a5d05c1bdc3e6cd46	hardware-assisted secure resource accounting under a vulnerable hypervisor	resource accounting;virtualization;conference;cloud	With the proliferation of cloud computing to outsource computation in remote servers, the accountability of computational resources has emerged as an important new challenge for both cloud users and providers. Among the cloud resources, CPU and memory are difficult to verify their actual allocation, since the current virtualization techniques attempt to hide the discrepancy between physical and virtual allocations for the two resources. This paper proposes an online verifiable resource accounting technique for CPU and memory allocation for cloud computing. Unlike prior approaches for cloud resource accounting, the proposed accounting mechanism, called Hardware-assisted Resource Accounting (HRA), uses the hardware support for system management mode (SMM) and virtualization to provide secure resource accounting, even if the hypervisor is compromised. Using a secure isolated execution support of SMM, this study investigates two aspects of verifiable resource accounting for cloud systems. First, this paper presents how the hardware-assisted SMM and virtualization techniques can be used to implement the secure resource accounting mechanism even under a compromised hypervisor. Second, the paper investigates a sample-based resource accounting technique to minimize performance overheads. Using a statistical random sampling method, the technique estimates the overall CPU and memory allocation status with 99%~100% accuracies and performance degradations of 0.1%~0.5%.	central processing unit;cloud computing;computation;computational resource;data center;discrepancy function;formal verification;hardware virtualization;heidelberg research architecture;hypervisor;outsourcing;overhead (computing);sampling (signal processing);software deployment;systems management	Seongwook Jin;Jinho Seol;Jaehyuk Huh;Seung Ryoul Maeng	2015		10.1145/2731186.2731203	real-time computing;virtualization;throughput accounting;cloud computing;computer science;operating system;distributed computing;computer security	OS	-51.6094535206882	57.45696248134224	1896
ba9197709a85f5215209cba45260b58c7f2aa883	human-to-human authorization for resource sharing in shad: roles and protocols	single sign on;pervasive computing;authentication;resource sharing;authorization;access control	One aim of pervasive computing is to allow users to share their resources so that they seem to be part of a single pervasive computer. This is just an illusion, the result of the synergy between different systems and resources. SHAD, introduced in PerCom 2007, is the first architecture that offers actual Single Sign-On to avoid authentication obtrusiveness and maintain the illusion of a single, pervasive computer. This paper describes how SHAD allows users to securely share their resources in a easy, natural, and intuitive way. It also describes its role-based Human-to-Human architecture, the threat model, and the protocols involved. Last but not the least, it presents results of further evaluation for our working implementation. c © 2007 Elsevier B.V. All rights reserved.	algorithm;authentication;authorization;cryptography;experiment;gnu;inferno;limbo;linux;mobile device;operating system;peer-to-peer;pervasive informatics;programming language;prototype;role-based access control;single sign-on;synergy;tablet computer;threat model;ubiquitous computing	Enrique Roberto Soriano;Francisco J. Ballesteros;Gorka Guardiola	2007	Pervasive and Mobile Computing	10.1016/j.pmcj.2007.07.001	shared resource;context-aware pervasive systems;computer science;access control;operating system;authentication;authorization;internet privacy;world wide web;computer security;ubiquitous computing;computer network	Security	-47.13117956310612	56.965073681346524	1900
c2e66ea172a061095f7380cd2a138994834bd0ed	multi-round scheduling for divisible loads on the heterogeneous cluster systems of multi-core computers	shared l2 cache;heterogeneous cluster systems;heterogeneous cluster;divisible loads;limited memory;large scale;multilevel cache;task scheduling;multi core computers	By allowing the overlap execution of computation and communication, a multi-round algorithm with inter-node scheduling and intra-node scheduling for divisible loads is presented on the heterogeneous cluster systems of multi-core computers, which have different number of processing cores and distinct computation, communication and memory abilities. The presented inter-node scheduling can accommodate dynamicaly the surplus of the remaining available memory of nodes in each round scheduling, and the intra-node scheduling can efficiently make use of the characterictics of multi-core machines and the shared L2 cache to reduce the required time to access memory. The experiment results with different tests and several parameters on a heterogeneous cluster system with multi-core computers show that the presented algorithm can obtain better scheduling performance and reduce significantly the total scheduling length, and it can schedule more large-scale applications than the existing scheduling scheme.	algorithm;cpu cache;clustered file system;computation;computer cluster;multi-core processor;scheduling (computing)	Li Xia;Zhong Cheng;Qu Zeng-Yan	2009		10.1145/1655925.1656079	fair-share scheduling;fixed-priority pre-emptive scheduling;parallel computing;real-time computing;earliest deadline first scheduling;gang scheduling;flow shop scheduling;dynamic priority scheduling;computer science;rate-monotonic scheduling;genetic algorithm scheduling;two-level scheduling;distributed computing;least slack time scheduling;lottery scheduling;round-robin scheduling;multiprocessor scheduling;i/o scheduling	HPC	-14.701088524150705	60.44782611820871	1906
7dcb4b566584b2c679f0a97beb69058249af8253	starting model development in distributed teams with incremental model splitting		The rising impact of software development in globally distributed teams strengthens the need for strategies that establish a clear separation of concerns in software models. Large, weakly modularized models are hard to comprehend and to analyse. A further maintainance obstacle is introduced by conflicting changes of the model. In our recent work, we propose a structured process for distributed modeling based on a variety of distributed modeling activities. To allow modularity and to facilitate developer independence, we focus on the problem of splitting a large monolithic model into sub-models. The modeler is assisted in incrementally discovering the set of desired sub-models. Our approach is supported by an automated tool that performs model splitting using information retrieval and model crawling techniques. We demonstrate the effectiveness of our approach on a set of real-life case studies, involving UML class models and meta-models being based on the Eclipse Modeling Framework.	eclipse modeling framework;information retrieval;real life;separation of concerns;software development;unified modeling language;velocity obstacle	Daniel Strüber;Gabriele Taentzer	2015			systems engineering;software;obstacle;incremental build model;simulation;separation of concerns;software development;unified modeling language;modularity;engineering	SE	-53.70578714178282	23.64878804178322	1914
779a2c854711bc620b1d14c24708649067740f71	on scheduling of peer-to-peer video services	p2p system;shaking technique service scheduling peer to peer video services video sharing;p2p;client server systems;indexing terms;system performance;peer to peer computing video sharing ieee members system performance network servers collaborative work file servers computer science processor scheduling delay;scheduling;video servers client server systems peer to peer computing scheduling;video servers;difference set;peer to peer computing;peer to peer	Peer-to-peer (P2P) video systems provide a cost-effective way for a large number of hosts to collaborate for video sharing. Two features characterize such a system: 1) a video is usually available on many participating hosts, and 2) different hosts typically have different sets of videos, though some may partially overlap. From a client's perspective, it can be served by any host having the video it requests. From a server's perspective, it be used to serve any client requesting the videos it has. Thus, an important question is, which servers should be used to serve which clients in the system? In this paper, we refer to this problem as service scheduling and show that different matches between clients and servers can result in significantly different system performance. Finding a right server for each client is challenging not only because a client can choose only the servers that are within its limited search scope, but also because clients arrive at different times, which are not known a priori. In this paper, we address these challenges with a novel technique called Shaking. While the proposed technique makes it possible for a client to be served by a server that is beyond the client's own search scope, it is able to dynamically adjust the match between the servers and their pending requests as new requests arrive. Our performance study shows that our new technique can dynamically balance the system workload and significantly improve the overall system performance	host (network);peer-to-peer;scheduling (computing);server (computing);video	Ying Cai;Ashwin Natarajan;Johnny S. Wong	2007	IEEE Journal on Selected Areas in Communications	10.1109/JSAC.2007.070114	client;real-time computing;index term;computer science;operating system;peer-to-peer;distributed computing;computer performance;scheduling;world wide web;difference set;client–server model;server;computer network;inter-process communication	Metrics	-16.25568365486276	71.08884659390831	1915
6eca4dfa7d1cfbbdce8188ac32a31dfb8497960f	how flexible is dynamic sdn control plane?		In Software Defined Networking (SDN), for performance reasons the control plane consists of multiple SDN controllers that provide a logically centralized network control. To react to traffic changes, the control plane may adapt to improve the performance of the whole network. This adaptation includes the migration of controllers, i.e. the change of their placement, as well as the change of switch to controller assignments. We call such adaptive network control a dynamic SDN control plane. Whereas most state of the art focuses on an optimal placement of controllers considering static traffic demands, we draw the attention to the need for a dynamic control plane and the performance of the adaption process itself. The involved controller migration and switch re-assignments significantly affect the control plane performance. Moreover, in this work we evaluate the flexibility of a dynamic SDN control plane with respect to the number of controllers. In particular, we refer to a flexibility metric that takes the time into account that is needed for successful migrations to handle traffic changes. Our evaluations are based on detailed models of controller placement and migration time. The results confirm that the migration time is a critical parameter and reveal that in case only a short migration time is allowed, the number of controllers does not bring more flexibility. For relaxed migration times, a larger number of controllers leads to more flexibility as expected. The results also show that a dynamic adaptation to traffic changes, can provide an improvement of up to 60% over a non-dynamic SDN control plane.	centralized computing;control plane;game controller;network planning and design;software-defined networking	Mu He;Arsany Basta;Andreas Blenk;Wolfgang Kellerer	2017	2017 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)	10.1109/INFCOMW.2017.8116460		Embedded	-11.75434034906844	83.24975330654057	1916
ddda84420c2a5391c95de096fd7504a0aec5edf7	faster secure multi-party computation of aes and des using lookup tables		We present an actively secure protocol for secure multi-party computation based on lookup tables, by extending the recent, two-party ‘TinyTable’ protocol of Damg̊ard et al. (ePrint 2016). Like TinyTable, an attractive feature of our protocol is a very fast and simple online evaluation phase. We also give a new method for efficiently implementing the preprocessing material required for the online phase using arithmetic circuits over characteristic two fields. This improves over the suggested method from TinyTable by at least a factor of 50. As an application of our protocol, we consider secure computation of the Triple DES and the AES block ciphers, computing the S-boxes via lookup tables. Additionally, we adapt a technique for evaluating (Triple) DES based on a polynomial representation of its S-boxes that was recently proposed in the side-channel countermeasures community. We compare the above two approaches with an implementation. The table lookup method leads to a very fast online time of over 230,000 blocks per second for AES and 45,000 for Triple DES. The preprocessing cost is not much more than previous methods that have a much slower online time.	block cipher;integrated circuit;lookup table;polynomial;preprocessor;s-box;secure multi-party computation;side-channel attack;triple des	Marcel Keller;Emmanuela Orsini;Dragos Rotaru;Peter Scholl;Eduardo Soria-Vazquez;Srinivas Vivek	2017		10.1007/978-3-319-61204-1_12	theoretical computer science;computer security;computer science;eprint;cbc-mac;block cipher;computation;preprocessor;lookup table;distributed computing;secure multi-party computation	Crypto	-40.63242399719792	79.17905976518486	1920
5ab4329cd5568252254e6b0f7f48b1128891b885	automated refactoring to the null object design pattern	optional fields;refactoring;null checks;null object;design patterns	Context: Null-checking conditionals are a straightforward solution against null dereferences. However, their frequent repetition is considered a sign of poor program design, since they introduce source code duplication and complexity that impacts code comprehension and maintenance. The Null Object design pattern enables the replacement of null-checking conditionals with polymorphic method invocations that are bound, at runtime, to either a real object or a Null Object. Objective: This work proposes a novel method for automated refactoring to Null Object that eliminates null-checking conditionals associated with optional class fields, i.e., fields that are not initialized in all class instantiations and, thus, their usage needs to be guarded in order to avoid null dereferences. Method: We introduce an algorithm for automated discovery of refactoring opportunities to Null Object. Moreover, we specify the source code transformation procedure and an extensive set of refactoring preconditions for safely refactoring an optional field and its associated null-checking conditionals to the Null Object design pattern. The method is implemented as an Eclipse plug-in and is evaluated on a set of open source Java projects. Results: Several refactoring candidates are discovered in the projects used ∗Corresponding author. Address: Patission 76, 104 34, Athens, Greece. Tel: +302108203183, Fax: +302108203275 Email addresses: mariannag17@gmail.com (Maria Anna G. Gaitani), bzafiris@aueb.gr (Vassilis E. Zafeiris), nad@aueb.gr (N. A. Diamantidis), mgia@aueb.gr (E. A. Giakoumakis) Preprint submitted to Information and Software Technology October 27, 2014 in the evaluation and their refactoring lead to improvement of the cyclomatic complexity of the affected classes. The successful execution of the projects’ test suites, on their refactored versions, provides empirical evidence on the soundness of the proposed source code transformation. Runtime performance results highlight the potential for applying our method to a wide range of project sizes. Conclusion: Our method automates the elimination of null-checking conditionals through refactoring to the Null Object design pattern. It contributes to improvement of the cyclomatic complexity of classes with optional fields. The runtime processing overhead of applying our method is limited and allows its integration to the programmer’s routine code analysis activities.	algorithm;code refactoring;cyclomatic complexity;design pattern;duplicate code;eclipse;email;fax;java;null object pattern;open-source software;overhead (computing);plug-in (computing);pointer (computer programming);precondition;program transformation;programmer;run time (program lifecycle phase);static program analysis;test suite;uniform resource identifier	Maria Anna G. Gaitani;Vassilis Zafeiris;N. A. Diamantidis;Emmanouel A. Giakoumakis	2015	Information & Software Technology	10.1016/j.infsof.2014.10.010	software design pattern;real-time computing;computer science;null object pattern;programming language;code refactoring;algorithm	SE	-57.76094598101227	37.860771673705514	1921
80687096c17912368fee43ee4c67238cc655150f	power analysis attack: an approach based on machine learning	side channel attack;cryptanalysis;machine learning;template attack	In cryptography, a side-channel attack is any attack based on the analysis of measurements related to the physical implementation of a cryptosystem. Nowadays, the possibility of collecting a large amount of observations paves the way to the adoption of machine learning techniques, i.e., techniques able to extract information and patterns from large datasets. The use of statistical techniques for side-channel attacks is not new. Techniques like the template attack have shown their effectiveness in recent years. However, these techniques rely on parametric assumptions and are often limited to small dimensionality settings, which limit their range of application. This paper explores the use of machine learning techniques to relax such assumption and to deal with high dimensional feature vectors.	cryptography;cryptosystem;feature vector;machine learning;side-channel attack	Liran Lerman;Gianluca Bontempi;Olivier Markowitch	2014	IJACT	10.1504/IJACT.2014.062722	cryptanalysis;timing attack;pre-play attack;computer science;side channel attack;ciphertext-only attack;data mining;world wide web;brute-force attack;computer security;algorithm	Security	-37.28313091784296	69.2183010391591	1927
7aa81b64a7a892875fe97e0cbc41f4621b74256b	innovative thinking: programming - a job for nonprogrammers?		ions Built on Top of Abstractions. Files do a huge number of things for us. To support this broad spectrum of capabilities, there are two layers of abstraction involved: the OS and Python. Unfortunately, both layers use the same words, so we have to be careful about casually misusing the word “file”. The operating system has devices of various kinds. All of the various devices are unified using a common abstraction that we call the file system. All of a computer’s devices appear as OS files of one kind or another. Some things which aren’t physical devices also appear as files. Files are the plumbing that move data around our information infrastructure. Additionally, Python defines file objects. These file objects are the fixtures that give our Python program access to OS files. The following figure shows this technology stack. Your program makes use of Python File objects. Python, in turn, makes use of OS file objects. Yes, it can be confusing that “file” is used for both things. However, you only have to focus on the Python file; the rest is just infrastructure to support your needs. Figure 12.1: Python File and OS File How Files Work. When your program evaluates a method function of a Python file object, Python transforms this into an operation on the underlying OS file. An OS file operation becomes an operation on one of the various kinds of devices attached to our computer. Or, a OS file operation can become a network operation that reaches through the Internet to access data from remote computers. The two layers of abstraction mean that one Python program can do a wide variety of things on a wide variety of devices. 338 Chapter 12. Working with Files Programming for Non-Programmers, Release 2.5.1 Our purpose is to expose the basic method functions of Python file objects. To do this, we’ll focus on operations on our local computer’s files. In order to talk about our local computer’s files, we’ll need to dig into the OS file system in some depth. We’ll cover the following topics in this section. • While we can easily see the folder and file icons in our computer’s “Finder” or “Explorer” or “Nautilus”, we need to look under the hood at Directories and Files from the Operating System perspective. • One important thing we need to do is move past pointing and clicking of icons. To do this, we have to know How File and Directory Names Work. • The point is to be able to talk about Path Names and File Names. • Additionally, we need to be crystal clear on The Current Working Directory. All of this is necessary background for the actual Python file object and the OS file system. Directories and Files. Our disk drives (and related storage devices) are organized as directories of files. Most of our operating system tools show the directories as cute little folder icons and the files within the directories as charming document icons. At the OS level, a directory is a file that lists other files. Since a directory is a file, a directory can contain directory files. That’s why directories (and folders) can be nested inside each other to an arbitrary depth. But wait! What about the icons piled on our desktop? They aren’t in a folder! Actually, they are. In Windows and the MacOS, the desktop is a folder; it is also shown as the background that fills our display. In Windows, I find the desktop folder in C:\Documents and Settings\SLott\Desktop. On a MacOS computer, it is /Users/slott/Desktop. Every file and directory (except one) is in a directory. There is a single top-most directory, called the root directory. The root directory has sub-directories, but is not the sub-directory of any other directory on the file system. In GNU/Linux, this directory has the very short name of /. Note: Windows doesn’t have a single root directory. It has a root directory on each device. C:\ is the root directory of device C:. Some directories have no sub-directories, only files; these directories are called leaf directories. This terminology comes from using the branches of a tree as a metaphor. That’s why the parent of all directories is the “root”. While this metaphor is common in computer science, we often draw pictures of our directory folder tree with the root at the top of the tree and leaves at the bottom. A parent-child metaphor is often more clear. The root directory is the parent of all directories, and contains child directories. All other directories, except the root, have parents. Some directories have children, making them parents as well as children. How Directory and File Names Work. In the visual desktop tools, we identify files and directories by clicking their icons. Under the hood, the operating system identifies files and directories by a full name that unambiguously identifies the directory and file. When we are pointing and clicking on file icons, we can see the file names and the containing folder names. Each folder icon is shown inside a window that – visually – is the containing (or parent) folder. Imagine starting from the root directory icon and clicking on folders to open windows. We may go through several steps of opening folders, opening sub-folders, opening sub-sub-folders, until we finally get to our file icon. This sequence of folder names is a path through the file system from the root directory to the target file. Each time we click on that file icon, our GUI tools are resolving this path that navigates from the root directory through a sequence of sub-directories down to the the specific file. Path Names and File Names. From the OS perspective, all files are uniquely identified by an absolute path. This path is shows how to get from the root directory down to the file’s directory and the file. Because of this, we cannot have two files in a directory with the same name. We can, however, have files with the same name in different directories. The absolute path will distinguish between the two. 12.1. External Data and Files 339 Programming for Non-Programmers, Release 2.5.1 These absolute paths can be long-winded. Also, we tend to work in a single directory (or closeyly related group of directories). Because of this, we’d like to avoid having to repeat some parts of the path for every file we work on. Here’s an example file. One of my files of notes is named die.py. This file has the following absolute path name: /Volumes/Slott02/Writing/Tech/PFNP/notes/die.py We often separate an absolute path name into two parts. 1. The Directory Path. The route through the directories from the root of the file system, through all of the directory branches to the directory that contains the file. This is written as a string of directory names, punctuated by / (or \, on Windows) characters. For our example file, the directory portion of the path is /Volumes/Slott02/Writing/Tech/PFNP/notes 2. The Filename. The name of the file at the end of the path, sometimes called the basename. A filename can have a “root” name and an extension, separated by a .. The file extension usually encodes the format or organization of the file. We assure that the extension of our Python files is always .py. The extension names are widely used, but not mandatory. Windows and the MacOS have handy control panels for modifying the association between the extensions and the applications that process the data. For our example file, the basename or filename is die.py. The Current Working Directory. Rather than be saddled with these long, absolute path names, our OS gives us a handy short-hand. File names can be understood based on a relative path name coupled with your current working directory, sometimes called the current directory. This current working directory short-hand gives us two kinds of file paths. • An absolute path. A path that begins with / (or device:\ on Windows), the path begins at the root of the file system, and names a single file. Changing the current working directory doesn’t change the meaning of the path. • A relative path. When a GNU/Linux path begins with any character other than / (or device:\ on Windows), the path is relative to the current working directory. The relative path is combined with the current working directory to create an absolute path. Changing the current working directory changes the meaning of the relative path. This encourages us to have directories with parallel structures. In Windows, the cd command shows you the current directory. You can also use this command to change the current directory. In GNU/Linux, we use the pwd command to print the working directory. We use the command cd to change the current working directory. Some current directory examples. When I’m working on examples, I often cd to my notes directory. I use a command like the following in GNU/Linux or MacOS. I don’t actually type the entire thing; the shell helps me by filling in names after I type the first few letters and hit the tab key. cd /Volumes/Slott02/Writing/Tech/PFNP/notes Once I cd to my notes directory, I can simply name each file, and the shell will use the current directory information to create the expected absolute path name from just the file name. When I want to see some sample output from die.py, the whole interaction looks like this. 340 Chapter 12. Working with Files Programming for Non-Programmers, Release 2.5.1 DVD-iMac2:~$ cd /Volumes/SLOTT02/Writing/Tech/PFNP/notes/ DVD-iMac2:/Volumes/SLOTT02/Writing/Tech/PFNP/notes$ python die.py (6, 3) (2, 2) ... 1. The first prompt shows my current directory as ~, which means my starting, home directory. I executed a cd to change my working directory. 2. The second prompt shows my current directory as /Volumes/SLOTT02/Writing/Tech/PFNP/notes. When I enter python die.py, the OS assembles a full path name from the current working directory and the name I entered (die.py). Navigation. We have handy navigation techniques for specifying relative paths that are “above” (closer to the root) as well as “below” (further from the root). The ordinary navigation is to extend the current 	abstraction layer;computer file;computer science;desktop computer;directory (computing);gnu;graphical user interface;hood method;handy board;internet;linux;microsoft windows;operating system;programmer;python;solution stack;superuser;tree (data structure);working directory	Zili Hu;Bill Cupp;Connie Lightfoot	2005			simulation	OS	-30.827718642019647	26.580054078872553	1929
9577708faef1e49bbf2208a8ac350cbc454d601a	integração entre um modelo de simulação hidrológica e sistema de informação geográfica na delimitação de zonas tampão ripárias	sistemas de informacao geografica metodos de simulacao;solos uso;soil management;tese de doutorado;geographic information systems		numerical aperture;unified model	Ligia Barrozo Simões	2001			geography;regional science	Crypto	-105.51279830752397	17.726445536076483	1930
7b0159815876abc697b13496e79f3fe0fe4c07e8	jcatascopia: monitoring elastically adaptive applications in the cloud	cloud resource provisioning system jcatascopia cloud monitoring system cloud computing automatic resource provisioning next generation cloud service elastically adaptive cloud applications;elasticity cloud computing cloud monitoring application monitoring;resource allocation cloud computing monitoring;monitoring measurement servers probes cloud computing elasticity subscriptions	Over the past decade, Cloud Computing has rapidly become a widely accepted paradigm with core concepts such as elasticity, scalability and on demand automatic resource provisioning emerging as next generation Cloud service-must have-properties. Automatic resource provisioning for Cloud applications is not a trivial task, requiring for both the applications and platform, to be constantly monitored, capturing information at various levels and time granularity. In this paper we describe the challenges that occur when monitoring elastically adaptive Cloud applications and to address these issues we present JCatascopia, a fully automated, multi-layer, interoperable Cloud Monitoring System. Experiments on different production Cloud platforms show that JCatascopia is a Monitoring System capable of supporting a fully automated Cloud resource provisioning system with proven interoperability, scalability and low runtime footprint. Most importantly, JCatascopia is able to adapt in a fully automatic manner when elasticity actions are enforced to an application deployment.	cloud computing;elasticity (cloud computing);interoperability;layer (electronics);next-generation network;programming paradigm;provisioning;scalability;software deployment	Demetris Trihinas;George Pallis;Marios D. Dikaiakos	2014	2014 14th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing	10.1109/CCGrid.2014.41	real-time computing;cloud computing;cloud testing;world wide web;computer network	HPC	-27.65208871528633	60.685512977420075	1932
4cc8fafa1b7a37a6e9ddd104f095b7ba8958158d	krisys - a kbms supporting the development and processing of advanced engineering applications		In order to support non-standard database applications and, in particular, advanced engineering applications, enhanced DBMSs have to supply not only semantically enriched data and knowledge modeling concepts, but also means for constructing an application model in a stepwise, incremental way. The Knowledge Base Management System KRISYS, which is presented in this paper, has been developed along these lines. We give an overview of the system architecture and the individual components, illustrate the application design methodology supported by the system, and demonstrate the applicability of KRISYS in an advanced CAD framework. Additionally, we emphasize the need of refined concepts for efficient application processing in workstation/server architectures and sketch a processing model for such an environment.	computer-aided design;knowledge base;knowledge modeling;management system;server (computing);stepwise regression;systems architecture;workstation	Stefan Deßloch;Theo Härder;Franz-Josef Leick;Nelson Mendonça Mattos	1992				DB	-38.77436566605625	9.322003790980961	1936
0846a100fcdf2406ec38df867b9ec835e1ebc1e0	grammar inference technology applications in software engineering	software systems;software engineering;metamodel;context free grammar;domain specific language;speech recognition;grammar inference;domain specific languages	While Grammar Inference (GI) has been successfully applied to many diverse domains such as speech recognition and robotics, its application to software engineering has been limited, despite wide use of context-free grammars in software systems. This paper reports current developments and future directions in the applicability of GI to software engineering, where GI is seen to offer innovative solutions to the problems of inference of domain-specific language (DSL) specifications from example DSL programs and recovery of metamodels from instance models.	context-free grammar;context-free language;digital subscriber line;domain-specific language;grammar induction;metamodeling;robotics;software engineering;software system;speech recognition	Barrett R. Bryant;Marjan Mernik;Dejan Hrncic;Faizan Javed;Qichao Liu;Alan P. Sprague	2010		10.1007/978-3-642-15488-1_25	domain analysis;natural language processing;speech recognition;search-based software engineering;computer science;domain-specific language;software development;feature-oriented domain analysis;domain engineering;software construction;programming language	SE	-51.542925901823274	26.09424881376543	1940
d8005326b45df1b25d9f1e2078052e699dcccebf	an analysis of totally clairvoyant scheduling	software verification;real time;flow shop scheduling;dynamic program;performance metric;polynomial time algorithm;system design;total clairvoyance;polynomial time;precedence constraint;performance metrics;mutable dynamic programming;real time systems;time constraint	Traditional scheduling models assume that the execution time of a job in a periodic job-set is constant in every instance of its execution. This assumption does not hold in real-time systems wherein job execution time is known to vary. A second feature of traditional models is their lack of expressiveness, in that constraints more complex than precedence constraints (for instance, relative timing constraints) cannot be modeled. Thirdly, the schedulability of a real-time system depends upon the degree of clairvoyance afforded to the dispatcher. In this paper, we shall discuss Totally Clairvoyant Scheduling, as modeled within the E-T-C scheduling framework (Subramani, 2002). We show that this instantiation of the scheduling framework captures the central issues in a real-time flow-shop scheduling problem and devise a polynomial time sequential algorithm for the same. The design of the polynomial time algorithm involves the development of a new technique, which we term Mutable Dynamic Programming. We expect that this technique will find applications in other areas of system design, such as Validation and Software Verification. We also introduce an error-minimizing performance metric called Violation Degree and establish that optimizing this metric in a Totally Clairvoyant Scheduling System is NP-Hard.	scheduling (computing)	K. Subramani	2005	J. Scheduling	10.1007/s10951-005-6367-2	fair-share scheduling;time complexity;fixed-priority pre-emptive scheduling;mathematical optimization;real-time computing;earliest deadline first scheduling;flow shop scheduling;dynamic priority scheduling;software verification;computer science;rate-monotonic scheduling;theoretical computer science;operating system;foreground-background;two-level scheduling;distributed computing;least slack time scheduling;round-robin scheduling;algorithm;systems design	Theory	-9.672257861134062	60.798020737182036	1945
2e01f29121b23fa9ee6dc8fe933aa427fa9b9598	rolling out a mission critical system in an agilish way. reflections on building a large-scale dependable information system for public sector		Despite the increasing pace of development and deployment of new software systems, the expectations regarding critical information systems have not changed. Consequently, to ensure high quality in spite of rapid updates, the fashion these facilities are taken to use need careful considerations. This paper presents and analyses real-life experiences gathered during the many years of planning, design, implementation, testing and finally deploying as a service a large, multi-million euro, extremely mission critical information system for emergency services. The project was to be carried out in an agile way, although the scope, the price and the duration were fixed by signed contracts. Fortunately, the customer was willing and able to learn how to do system development iteratively and incrementally, as well as to provide personnel to constantly collaborate with the developers. Along the way, we learned that to truly succeed in such an endeavor as this one, it is not enough to do the things almost right or by the book. Instead, everyone involved must keep raising the bar every day, in a continuous, disciplined, and controlled way.	agile software development;amiga reflections;critical system;display resolution;experience;feedback;information system;load testing;make;mission critical;proxy server;real life;requirement;soak testing;software deployment;software quality assurance;software system;software testing;stress testing;test automation;unit testing	Aapo Koski;Tommi Mikkonen	2015	2015 IEEE/ACM 2nd International Workshop on Rapid Continuous Software Engineering			SE	-69.14400004654048	25.314688256276423	1948
64c47e7723633823270a5c90a08910e93e6ab838	robotic platform: a xml-based extensible robot intelligence architecture (xria)	layered architecture;software platform;pluggable architecture;robot intelligence;finite state machine;robotic software platform	This paper concerns the development of a lightweight robotic platform which targets at flexibility, scalability, extensibility and environment for intelligence embedment. The platform employs layered architecture and event driven communication paradigm which allows for efficient integration of additional sensory input and actuator control. New intelligent algorithm can be encapsulated into plug-ins and lodged into the platform to leverage the robot intelligence and learning capability. A multi-purpose XML engine is introduced for the control of system communication, business logic, finite state machine execution and decision making. The proposed robotic platform was deployed on a number of robots in our laboratory such as coffee serving robot and receptionist robot. The results show that XRIA is flexible, manageable and works well for different types of robots with required operating performance.	cognitive robotics;xml	Ridong Jiang;Yeow Kee Tan;Dilip Kumar Limbu;Tran Ang Dung	2010		10.1007/978-3-642-17248-9_5	embedded system;real-time computing;simulation;engineering;open platform	Robotics	-38.67099823650144	43.26508818507339	1949
35879ba39ea70fa7d82df4897047f4303e7ae9c9	passive taxonomy of wifi clients using mlme frame contents		Denton Gentry dgentry@google.com denny@geekhold.com Avery Pennarun apenwarr@google.com apenwarr@gmail.com      In supporting Wifi networks it is useful to identify the type of client device connecting to an AP. Knowing the type of client can guide troubleshooting steps, allow searches for known issues, or allow specific workarounds to be implemented in the AP. For support purposes a passive method which analyzes normal traffic is preferable to active methods, which often send obscure combinations of packet options which might trigger client bugs.  We have developed a method of passive client identification which observes the contents of Wifi management frames including Probes and Association requests. We show that the management frames populated by modern Wifi chipsets and device drivers are quite distinguishable, making it possible in many cases to identify the model of the device. Supplementing information from the Wifi management frames with additional information from DHCP further extends the set of clients which can be distinguished.	chipset;device driver;network packet;population;software bug;workaround	Denton Gentry;Avery Pennarun	2016	CoRR		real-time computing;telecommunications;operating system;world wide web;computer security;computer network	Networks	-56.792789646235214	67.81442661887118	1963
2e56d7b839528053ce040b6b544279a14b6ba3aa	a compiler infrastructure for embedded heterogeneous mpsocs	mpsoc programming;compiler infrastructure	Programming heterogeneous MPSoCs (Multi-Processor Systems on Chip) is a grand challenge for embedded SoC providers and users today. In this paper, we argue for the need and significance of positioning the language and tool design from the perspective of practicality to address this challenge. We motivate, describe and justify such a practical design of a compilation framework for heterogeneous MPSoCs targeting the domain of streaming applications, named MAPS (MPSoC Application Programming Studio). MAPS defines a clean, light-weight C language extension to capture streaming programming models. A retargetable source-to-source compiler is developed to provide key capabilities to construct practical compilation frameworks for real-world, complex MPSoC platforms. Our results have shown that MAPS is a promising compiler infrastructure that enables programming of heterogeneous MPSoCs and increases productivity of MPSoC software developers.	embedded system;grand challenges;mpsoc;multiprocessing;software developer;source-to-source compiler;system on a chip	Weihua Sheng;Stefan Schürmans;Maximilian Odendahl;Mark Bertsch;Vitaliy Volevach;Rainer Leupers;Gerd Ascheid	2013		10.1145/2442992.2442993	computer architecture;parallel computing;real-time computing;dynamic compilation;computer science	PL	-23.343721687463656	38.597279921107315	1964
42b0bc4c5035d7c7f38b85c461319198fdd48213	rapidly building visual management systems for context-aware services	sensibilidad contexto;context aware computing;interfaz grafica;museo;tecnologia electronica telecomunicaciones;context aware;informatique mobile;musee;management system;multimedia;graphical interface;man machine dialogue;implementation;gestion sistema;localization;reseau ordinateur;mobile computer;graphical user interface;localizacion;systeme adaptatif;computer network;informatique omnipresente;localisation;context aware service;monitoring;component framework;multimedia communication;adaptive system;graphic user interface;red informatica;sistema adaptativo;dialogo hombre maquina;ubiquitous computing;museum;location awareness;monitorage;sensibilite contexte;tecnologias;implementacion;grupo a;communication multimedia;mobile computing;monitoreo;visual interfaces;system management;interface graphique;gestion systeme;dialogue homme machine;ubiquitous computing environment	A component framework for building and operating visual interfaces for context-aware services in ubiquitous computing environments is presented. By using a compound-document technology, it provides physical entities, places, stationary or mobile computing devices, and services with visual components as multimedia representations to enable them to be annotated and controlled them. It can automatically assemble visual components into a visual interface for monitoring and managing context-aware services according to the spatial-containment relationships between their targets in the physical world by using underlying location-sensing systems. End-users can manually deploy and customize context-aware services through user-friendly GUI-based manipulations for editing documents. This paper presents the design for this framework and describes its implementation and practical applications in user/location-aware assistant systems in two museums.	context-aware network;management system	Ichiro Satoh	2008	IEICE Transactions	10.1093/ietisy/e91-d.9.2251	embedded system;simulation;human–computer interaction;computer science;operating system;graphical user interface;services computing;mobile computing	Visualization	-30.29453921332988	43.302357720402526	1968
42fb0aec37d79b371e32a7629719ebfc581231c0	chronique : consommation statique. modèles, évolutions et perspectives	consumption;diseno circuito;circuit design;modele statique;consumo;energy consumption;consommation;modelo estatico;consommation energie;static model;conception circuit;consumo energia			Christian Piguet	2007	Technique et Science Informatiques	10.3166/tsi.26.623-638	consumption;circuit design	Logic	-108.71208419006867	17.659640963162353	1972
2a4c5a2016eb0c0980a1ed35823450638417f6e9	managing variation in explanation-oriented languages	software variation management;software;variation management;visual languages software engineering;game theory;explanation oriented language design;program explanation explanation oriented language design software variation management;software engineering;explanations;visualization;visual languages;program explanation;calculus software games visualization cognition game theory software engineering;calculus;games;cognition;variation management language design explanations;language design	This work considers the intersection of explanation-oriented language design and software variation management, two seemingly disparate research areas that turn out to be quite complementary. Successful explanations are often highly variable to adapt to the needs of a particular user, but this also makes them complex and unwieldy unless supported by a sound theory of variation.		Eric Walkingshaw	2010	2010 IEEE Symposium on Visual Languages and Human-Centric Computing	10.1109/VLHCC.2010.42	computer science;theoretical computer science;programming language	Logic	-78.40011331857035	22.75544123162925	1992
1a2f3e60643c43aabf43fc178a168903eb73ea35	"""""""preagro-n"""", ein gis-gestütztes expertensystem zur planung der teilflächenspezifischen n-einsatzmengen im off-line betrieb"""		Die teilflächenspezifische NEinsatzsteuerung im Rahmen von Precision Farming kann sowohl durch on-line oder off-line Steuerungsverfahren erfolgen. Für das letztgenannte Verfahren wurde im Rahmen des Forschungsprojektes pre agro I ein GIS-gestütztes Expertensystem entwickelt und sowohl in Feldversuchen als auch Praxisexperimenten praktisch erprobt. Vorgestellt und diskutiert werden die Systemkonzeption, die naturwissenschaftlichen Grundlagen und die EDVtechnische Lösung sowie Ergebnisse aus experimentellen Überprüfungen in Praxisexperimenten.	geographic information system;institut für dokumentologie und editorik;online and offline	Karl-Otto Wenkel;Sybille Brozio;Reinhart Schwaiberger	2006				PL	-103.87575265400821	29.556466232794044	1997
54cdb2e1fc88d6e4442b4f0a3f19c84a53bc363c	certified exact transcendental real number computation in coq	proof assistant;complete metric space;theorem proving;numerical analysis;logic in computer science;mathematical software;proof of correctness;coq proof assistant;article in monograph or in proceedings	Reasoning about real number expressions in a proof assistant is challenging. Several problems in theorem proving can be solved by using exact real number computation. I have implemented a library for reasoning and computing with complete metric spaces in the Coq proof assistant and used this library to build a constructive real number implementation including elementary real number functions and proofs of correctness. Using this library, I have created a tactic that automatically proves strict inequalities over closed elementary real number expressions by computation.	approximation algorithm;automated theorem proving;computation;coq (software);correctness (computer science);elementary function;failure;html;haskell;monad (functional programming);newton–cotes formulas;programming language;proof assistant;prototype;web resource;eric	Russell O'Connor	2008		10.1007/978-3-540-71067-7_21	discrete mathematics;computer-assisted proof;numerical analysis;computer science;artificial intelligence;mathematics;automated theorem proving;proof assistant;programming language;complete metric space;proof complexity;algorithm	PL	-18.903391472031608	18.769914724837278	1998
88725b6f86632a0459ab84550acdce8e199a9240	how to have your cake and eat it too: dynamic software updating with just-in-time overhead	program interpreters;software engineering;dynamic software update;execution progress;just-in-time overhead;normal execution;program execution;shifting gear approach	We consider the overhead incurred by programs that can be updated dynamically and argue that, in general, and regardless of the mechanism used, the program must incur an overhead during normal execution. We argue that the overhead during normal execution of the updateable program need not be as high as the overhead for the updated program. In light of the fundamental limitations and the differences in the overhead that must be incurred by the updateable and updated programs, we propose a new mechanism for dynamic software update based on a new shifting gears approach. The mechanism attempts to incur just the required overhead depending on the stage of update the application is in. Before an update the execution incurs low overhead and when an update occurs the execution incurs higher overhead which reverts to low overhead as the execution progresses. We evaluate the mechanism by modifying an application by hand. Preliminary performance numbers show that the mechanism performs better than existing mechanisms for dynamic software update.	overhead (computing);patch (computing)	Rida A. Bazzi;Bryan Topp;Iulian Neamtiu	2012	2012 4th International Workshop on Hot Topics in Software Upgrades (HotSWUp)		embedded system;parallel computing;real-time computing;computer science;overhead	Arch	-21.263064552820488	35.518168849709035	2003
ebebc3d951e83a1bfe113befe7800aca44bec4c7	codeflaws: a programming competition benchmark for evaluating automated program repair tools		Several automated program repair techniques have been proposed to reduce the time and effort spent in bug-fixing. While these repair tools are designed to be generic such that they could address many software faults, different repair tools may fix certain types of faults more effectively than other tools. Therefore, it is important to compare more objectively the effectiveness of different repair tools on various fault types. However, existing benchmarks on automated program repairs do not allow thorough investigation of the relationship between fault types and the effectiveness of repair tools. We present Codeflaws, a set of 3902 defects from 7436 programs automatically classified across 39 defect classes (we refer to different types of fault as defect classes derived from the syntactic differences between a buggy program and a patched program).	automatic bug fixing;benchmark (computing);fault tolerance;software bug	Shin Hwei Tan;Jooyong Yi;Yulis;Sergey Mechtaev;Abhik Roychoudhury	2017	2017 IEEE/ACM 39th International Conference on Software Engineering Companion (ICSE-C)		systems engineering;computer science;maintenance engineering;software;benchmark (computing);reliability engineering	SE	-60.32044618643868	35.91541529931582	2005
e2a6e3ce9196a66e5885d0252ea3df01258a8ee3	multi-agent platform and toolbox for fault tolerant networked control systems	agent platform;multi agent system;fault tolerant;networked control systems;real time control;distributed networks;critical level;distributed computing;data exchange;indexing terms;network control;large scale;multi agent systems;control system;fault tolerant system;graph partitioning;network connectivity;spatial distribution;community networks;sensors and actuators;networked control system;decentralized design;fault tolerant systems design	Industrial distributed networked control systems use different communication networks to exchange different critical levels of information. Real-time control, fault diagnosis (FDI) and Fault Tolerant Networked Control (FTNC) systems demand one of the more stringent data exchange in the communication networks of these networked control systems (NCS). When dealing with large-scale complex NCS, designing FTNC systems is a very difficult task due to the large number of sensors and actuators spatially distributed and network connected. To solve this issue, a FTNC platform and toolbox are presented in this paper using simple and verifiable principles coming mainly from a decentralized design based on causal modelling partitioning of the NCS and distributed computing using multi-agent systems paradigm, allowing the use of agents with well established FTC methodologies or new ones developed taking into account the NCS specificities. The multi-agent platform and toolbox for FTNC systems have been built in Matlab/Simulink environment, which is in our days the scientific benchmark for this kind of research. Although the tests have been performed with a simple case, the results are promising and this approach is expected to succeed with more complex processes.	as-interface;agent architecture;benchmark (computing);causal filter;control system;distributed computing;fault detection and isolation;fault tolerance;formal verification;internet;matlab;multi-agent system;network computing system;programming paradigm;real-time transcription;requirement;simulink;software engineering;telecommunications network	Mário J. G. C. Mendes;Bruno M. S. Santos;José M. G. Sá da Costa	2009	JCP	10.4304/jcp.4.4.303-310	fault tolerance;real-time computing;computer science;networked control system;artificial intelligence;multi-agent system;distributed computing	Robotics	-36.06272055573134	36.94021542083938	2007
7a68ade8bb6d286664340e9fb6c71f1779df6d50	automating resource allocation for multiprocessors	modelizacion;optimisation;optimizacion;multiprocessor;programacion paralela;resource allocation;sistema informatico;heuristic method;parallel programming;metodo heuristico;computer system;modelisation;optimization;systeme informatique;asignacion recurso;methode heuristique;multiprocesador;allocation ressource;modeling;programmation parallele;multiprocesseur	Abstract   The allocation of resources of multiprocessor hardware to the separable components of parallel software has proved difficult both in theory and in practice. In this paper, a model of parallel hardware and application programs is formulated that permits the joint solution of two interdependent allocation problems: the placement of a program's code and data into shared or distributed memory and the assignment of a program's processes to multiple processors. In addition, we describe a fast allocation heuristic that employs user-supplied allocation hints to sequentially perform placements and assignments for single, large, parallel programs during program initialization. This heuristic is evaluated experimentally on a sample multiprocessor, which is the Cm ∗  multiprocessor. For this evaluation, experimental data are derived from several parallel applications programs executed on Cm ∗ . The model and the heuristic are implemented within a practical programming environment for Cm ∗ , the TASK tool system, thereby making the Cm ∗  hardware partly transparent to the application programmers.		Karsten Schwan;Cheryl Gaimon	1989	Journal of Systems and Software	10.1016/0164-1212(89)90007-1	parallel computing;real-time computing;multiprocessing;systems modeling;resource allocation;computer science;operating system;distributed computing;programming language;management	OS	-15.511863782201859	43.24944193747899	2018
085841105b130614310a663ff3817f24b66dcd65	efficient and secure self-organized public key management for mobile ad hoc networks	key management;gestion de claves;evaluation performance;self organized;tecnologia electronica telecomunicaciones;mobile radiocommunication;performance evaluation;securite;telecommunication sans fil;network security;evaluacion prestacion;canal transmision;selforganized;simulation;reseau ad hoc mobile;cle publique;simulacion;ad hoc network;red ad hoc;radiocommunication service mobile;red movil ad hoc;public key;reseau ad hoc;parameter selection;canal transmission;transmission channel;telecomunicacion sin hilo;safety;llave publica;autoorganizacion;ad hoc networks;mobile ad hoc network;self organization;tecnologias;grupo a;radiocomunicacion servicio movil;seguridad;gestion cle;autoorganisation;wireless telecommunication	This paper presents a fully self-organized key management scheme for mobile ad hoc networks. Unlike most previous schemes, there is no priori shared secret or no priori trust relationship in the proposed scheme; every node plays the same role and carries out the same function of key management. The proposed scheme consists of (1) Handshaking (HS) and (2) Certificate request/reply (CRR) procedures. In HS, a node acquires the public key of the approaching node via a secure side channel. In CRR, a node requests certificates of a remote node via a radio channel to the nodes that it has HSed. If the number of received valid certificates that contain the same public key exceeds a given threshold, the node accepts the remote node's public key as valid. Security is rigorously analyzed against various known attacks and network costs are intensively analyzed mathematically. Using this analysis, we provide parameter selection guideline to optimize performance and to maintain security for diverse cases. Simulation results show that every node acquires the public keys of all other nodes at least 5 times faster than in a previous scheme.	hoc (programming language);key management	Daeseon Choi;Younho Lee;Yongsu Park;Seunghun Jin;Hyunsoo Yoon	2008	IEICE Transactions	10.1093/ietcom/e91-b.11.3574	wireless ad hoc network;telecommunications;computer science;network security;node;computer security;computer network	Mobile	-46.68868338239291	78.27862159071069	2024
84d7156dfffb27733165da4928c3c811b435912a	bounded model generation for isabelle/hol	model generation;satisfiability;automatic generation;theorem prover;propositional logic;sat solver;higher order logic;finite model generation;interactive theorem proving	A translation from higher-order logic (on top of the simply typed λ-calculus) to propositional logic is presented, such that the resulting propositional formula is satisfiable iff the HOL formula has a model of a given finite size. A standard SAT solver can then be used to search for a satisfying assignment, and such an assignment can be transformed back into a model for the HOL formula. The algorithm has been implemented in the interactive theorem prover Isabelle/HOL, where it is used to automatically generate countermodels for non-theorems.		Tjark Weber	2005	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2004.10.027	zeroth-order logic;discrete mathematics;resolution;higher-order logic;hol;computer science;mathematics;automated theorem proving;propositional variable;propositional calculus;boolean satisfiability problem;atomic formula;proof assistant;programming language;algorithm;satisfiability	Logic	-17.307375016129157	17.79151551178397	2027
d4a1380a431c14d3937dc8bdd7d5ef26d93fde94	evaluating impact of human errors on the availability of data storage systems		In this paper, we investigate the effect of incorrect disk replacement service on the availability of data storage systems. To this end, we first conduct Monte Carlo simulations to evaluate the availability of disk subsystem by considering disk failures and incorrect disk replacement service. We also propose a Markov model that corroborates the Monte Carlo simulation results. We further extend the proposed model to consider the effect of automatic disk fail-over policy. The results obtained by the proposed model show that overlooking the impact of incorrect disk replacement can result up to three orders of magnitude unavailability underestimation. Moreover, this study suggests that by considering the effect of human errors, the conventional believes about the dependability of different RAID mechanisms should be revised. The results show that in the presence of human errors, RAID1 can result in lower availability compared to RAID5.	computer data storage;dependability;failover;human error;markov chain;markov model;monte carlo method;simulation;standard raid levels;system administrator;unavailability	Mostafa Kishani;Reza Eftekhari;Hossein Asadi	2017	Design, Automation & Test in Europe Conference & Exhibition (DATE), 2017		maintenance engineering;embedded system;parallel processing;parallel computing;real-time computing;simulation;computer science;operating system;markov process;statistics;monte carlo method	HPC	-18.70741093368485	49.847777828657975	2029
354be8d048dbe7a4da7d19910f712bed5b915a60	extended use of null productions in lr(1) parser applications	forme nour backus;grammaire cf;analyse syntaxique;production null;context free grammar;syntactic analysis;analyseur lr 1;parser;analyseur syntaxique	Applications programmed using LR(1) parsers should be designed so that as many functions as possible are controlled by the driving parsing machine through conveniently staged reductions and associated primitive actions; in this paper this is achieved by making extensive use of null productions and nullable nonterminal symbols.	canonical lr parser;parsing;terminal and nonterminal symbols	Gerard D. Finn	1985	Commun. ACM	10.1145/4284.4289	speech recognition;canonical lr parser;computer science;parsing;programming language;algorithm;lr parser;simple lr parser	OS	-24.878723890416207	24.082914037117668	2039
676b9dbea047b471da5d2799943e76be04c4a345	a multivariate adaptive method for detecting arp anomaly in local area networks	communication system traffic control;protocols;degradation;local area networks switches broadcasting telecommunication traffic computer worms protocols backplanes packet switching degradation communication system traffic control;computer worms;backplanes;packet switching;telecommunication traffic;adaptive method;true positive;traffic analysis;broadcasting;switches;local area networks;local area network	Worms use different methods to propagate in networks. One of these methods is by means of broadcasting packets. Broadcasted packets occupy high percentage of network bandwidth, and abnormal broadcast traffic analysis could be a useful method for detecting network problems and infected hosts. In this paper a new method for detecting ARP abnormal traffic in a broadcast domain is introduced. A combination of four different ARP traffic criteria are used to determine network anomaly. Four parameters: Rate, Burstiness, Dark space and Sequential scan were considered. Our method focuses on rate anomaly caused by worms, scans and poorly-configured services. We applied our method to a real network to evaluate system accuracy and noticed that during one month, 92.9 percent of alarms were true positive alarms. This technique not only traces ARP anomaly the same way as scanning worms, but also it detects any host that disturbs the traffic rate in different LAN.	anomaly detection;broadcast domain;broadcasting (networking);information theory;network packet;sensor;tracing (software);traffic analysis	M. Farahmand;A. Azarfar;A. Jafari;V. Zargari	2006	2006 International Conference on Systems and Networks Communications (ICSNC'06)	10.1109/ICSNC.2006.5	traffic generation model;local area network;embedded system;network traffic control;telecommunications;computer science;computer security;proxy arp;computer network	Networks	-5.1489469434409765	90.81503600601022	2040
586e05136fcc2430c94239b11be7adea37b88b55	acmhs: efficient access control for mobile health care system		SummaryrnTo provide privacy protection in mobile health care system, medical users typically encrypt their personal health information before publishing it to the health care center. And other medical users download, decrypt, and process it using an access control protocol. However, current access control protocols are mainly focusing the privacy of personal health information and lacking the protection for medical usersu0027 real identities, leading to a variety of security issues. At the same time, current access control protocols for mobile health care systems are usually based on bilinear map, resulting in high computation costs. Observing the above issues, we introduce a novel access control protocol with privacy-preserving called Access Control for Mobile Healthcare System (ACMHS). Similar to protocols of this field, ACMHS can provide privacy protection for mobile health care systems. However, different from other well-known approaches, ACMHS uses pseudonyms instead of real identities, which can provide privacy protection for medical usersu0027 real identities. Moreover, to reduce the computation cost of bilinear map, we introduce the algebraic signature to ACMHS and design a novel access control protocol, which is much more efficient than current bilinear-map-based protocols. By doing so, ACMHS can achieve high efficiency while still enjoying required security requirements. Experimental results show that ACMHS is feasible for real-world applications.	access control;mhealth	Changsheng Wan;Juan Zhang;Daoli Huang	2018	Int. J. Communication Systems	10.1002/dac.3364	computer network;access control;encryption;health care;bilinear map;computation;signcryption;computer science;publishing;computer security;download	HCI	-43.77280408763799	64.64891331402181	2063
ba00befb18686bfa8f97a315572f4e7504de7e8d	designing viable multi-sided data platforms: the case of context-aware mobile travel applications		Advances in data semantification and natural language querying are enabling new generations of context-aware mobile applications. Such applications would rely on platforms that integrate heterogeneous sets of user data from a range of applications and systems. Designing these platforms is challenging as they should serve multiple user groups at the same time. In this paper, we analyze who should subsidize multi-sided data platforms that enable mobile context-aware travel applications. After analyzing the different user groups and revenue models, we assess end-user acceptance of these revenue models through a survey among 197 potential users. Results show that users willing to share data with app developers are more inclined to use data-driven mobile travel apps but are less inclined to pay for them. This paradoxical result explains why premium-pricing as well as data-monetization strategies can both be viable.		Mark de Reuver;Timber Haaker;Fatemeh Nikayin;Ruud Kosman	2015		10.1007/978-3-319-25013-7_28	embedded system;simulation;engineering;transport engineering	Mobile	-41.503136607298465	52.006555590052756	2069
7535288a3a8754df21327fd17f2b7bda66667663	anomaly? application change? or workload change? towards automated detection of application performance anomaly and change	application performance anomaly;debugging;automated detection;web pages;service provider;life cycle;enterprise wide application;application software;perforation;anomaly detection;online performance modeling;software performance evaluation;transaction processing program debugging regression analysis software performance evaluation system monitoring;system monitoring;application life cycle;customer satisfaction;regression based transaction model;html;application change analysis;accuracy;servers;servers data models monitoring web pages mathematical model html accuracy;feedback;monitoring;resource consumption model automated detection application performance anomaly application life cycle customer experience customer satisfaction enterprise wide application online performance modeling application change analysis regression based transaction model;performance analysis;mathematical model;production systems;regression analysis;resource consumption model;program debugging;productivity;face detection;transaction processing;performance loss;customer experience;compact model;data models	Automated tools for understanding application behavior and its changes during the application life-cycle are essential for many performance analysis and debugging tasks. Application performance issues have an immediate impact on customer experience and satisfaction. A sudden slowdown of enterprise-wide application can effect a large population of customers, lead to delayed projects and ultimately can result in company financial loss. We believe that online performance modeling should be a part of routine application monitoring. Early, informative warnings on significant changes in application performance should help service providers to timely identify and prevent performance problems and their negative impact on the service. We propose a novel framework for automated anomaly detection and application change analysis. It is based on integration of two complementary techniques: i) a regression-based transaction model that reflects a resource consumption model of the application, and ii) an application performance signature that provides a compact model of run-time behavior of the application. The proposed integrated framework provides a simple and powerful solution for anomaly detection and analysis of essential performance changes in application behavior. An additional benefit of the proposed approach is its simplicity: it is not intrusive and is based on monitoring data that is typically available in enterprise production environments.	algorithm;anomaly detection;application server;central processing unit;client–server model;debugging;information;memory leak;multitier architecture;online and offline;performance prediction;performance tuning;programming paradigm;server (computing);systems modeling;technical standard	Ludmila Cherkasova;Kivanc M. Ozonat;Ningfang Mi;Julie Symons;Evgenia Smirni	2008	2008 IEEE International Conference on Dependable Systems and Networks With FTCS and DCC (DSN)	10.1109/DSN.2008.4630116	service provider;reliability engineering;data modeling;biological life cycle;system monitoring;face detection;anomaly detection;productivity;application software;real-time computing;html;transaction processing;computer science;operating system;web page;mathematical model;data mining;feedback;database;accuracy and precision;production system;application lifecycle management;customer satisfaction;debugging;computer security;regression analysis;server;statistics	SE	-64.98239473234545	40.02090266985413	2078
66eec8f8d36a1839ae67afd92ff7629db87f9468	a network security risk assessment framework based on game theory	game theory;network security risk assessment framework;availability;network security;computer model;prototypes;game process;risk management;game process network security risk assessment attacker behavioral decision gtadm game theoretical attack defense model hrcm hierarchical risk computing model;computer networks;hrcm game theory network security risk assessment framework gtadm;computer network;computational modeling;risk management game theory risk analysis information security computer security computer networks data security helium computer science history;telecommunication security computer networks decision theory game theory risk management;games;decision theory;telecommunication security;network security risk assessment;risk assessment;game theoretical attack defense model;gtadm;security;attacker behavioral decision;hrcm;hierarchical risk computing model	Network security risk assessment depends on the prediction of attacker¿s behavioral decision. In computer network attack and defense area, this kind of decision is the optimal judgment for attackers and defenders themselves in consideration of the opponents¿ strategy spaces. Thus, The attack and defend behavior can be seen as a game process. In this paper, we studied how to bring game theory into the research area of network security risk assessment. First, we analyze the concept and the process of risk assessment to find the combining point where game theory can be used in network security risk assessment. Then we present a risk assessment framework based on game theory, and set up a risk assessment system using this framework. We emphatically introduce GTADM (game theoretical attack-defense model) and HRCM (hierarchical risk computing model) in the system, and provide detailed analysis and specification by a scenario.	game theory;network security;risk assessment	Wei He;Chunhe Xia;Cheng Zhang;Yi Ji;Xinyi Ma	2008	2008 Second International Conference on Future Generation Communication and Networking	10.1109/FGCN.2008.166	implementation theory;positive political theory;simulation;actuarial science;network theory in risk assessment;computer science;algorithmic game theory;computer security	ML	-64.1021656951094	61.43110461905657	2083
4f03c98d26b476fec1700ea94a3bac56c0abdddc	a proof theoretic interpretation of model theoretic hiding	institution-independent model theoretic semantics;casl support hiding;model theoretic representation;specification language;proof theoretic interpretation;proof theoretic nature;logical framework;structured specification;proof theoretic semantics;structured specification building operation;model theoretic logic;model theoretic hiding	Logical frameworks like LF are used for formal representations of logics in order to make them amenable to formal machine-assisted meta-reasoning. While the focus has originally been on logics with a proof theoretic semantics, we have recently shown how to de ne model theoretic logics in LF as well. We have used this to de ne new institutions in the Heterogeneous Tool Set in a purely declarative way. It is desirable to extend this model theoretic representation of logics to the level of structured speci cations. Here a particular challenge among structured speci cation building operations is hiding, which restricts a speci cation to some export interface. Speci cation languages like ASL and CASL support hiding, using an institution-independent model theoretic semantics abstracting from the details of the underlying logical system. Logical frameworks like LF have also been equipped with structuring languages. However, their proof theoretic nature leads them to a theorylevel semantics without support for hiding. In the present work, we show how to resolve this di culty.	common algebraic specification language;encode;eclipse;formal system;integrated development environment;integration platform;javascript library;logical framework;mpeg media transport;markup language;ne (complexity);omdoc;oracle database;proof calculus;theory;web application;xml database;xquery	Mihai Codescu;Fulya Horozal;Michael Kohlhase;Till Mossakowski;Florian Rabe	2010		10.1007/978-3-642-28412-0_9	theoretical computer science;algorithm	Logic	-22.251824717730127	18.561749468999263	2090
c6997aca7ddc059a43eb40248b571fa069651428	konzeption und prototypische umsetzung des e/a-systems für einen pearl-compiler		Dieses Dokument beschreibt Konzeption und prototypische Umsetzung des I/O-Systems einer Linux Laufzeitumgebung fur einen PEARL-Compiler. Aufbauend auf den Voruntersuchungen aus [1] entstand das OpenSource-Projekt „smallpearl“ [3]. In diesem Projekt entsteht ein PEARL-Compiler, der PEARL-Quellcode in die Zwischensprache C++ ubersetzt und mit einer eigenen Laufzeitbibliothek zusammenfuhrt, um das Laufzeitverhalten von PEARL zu ermoglichen. Das erste angedachte Laufzeitsystem ist Linux. Diese Arbeit beschreibt ein Konzept sowie eine prototypische Implementierung des gesamten I/OSystems der Laufzeitumgebung. Grundlage dafur bilden die bestehenden Artefakte der Laufzeitumgebung des „smallpearl“-Projektes [3]. Das Konzept des I/O-System umfasst Ein- und Ausgaben von Peripheriegeraten, eine offene Treiberschnittstelle und das Interruptsystem, dessen Verhalten der PEARL-Spezifikation [2] entspricht. Die prototypische Implementierung realisiert nicht das Interruptsystem.	compiler	Holger Kölle	2014		10.1007/978-3-662-45109-0_5	programming language;compiler;computer science	Crypto	-102.36372723216343	31.07363302291723	2098
9457203bdc52616687dc79f481cc0862f7db3c2d	ticket-based mobile commerce system and its implementation	service provider;short message service;mobile database;sms;mobile environment;mobile commerce;signature;information storage and retrieval;mobile application;280102 information systems management	Security is a critical issue in mobile commerce, especially in mobile database systems since mobile environments are dynamic and traditional protection mechanisms do not work very well in such environments. Mobile database access usually across multiple service domains, traditional access mechanisms rely on the concept of starting home location and cross domain authentication using roaming agreements. However, the cross domain authentications involve many complicated authentication activities when the roam path is long. This limits the future mobile applicationsThis paper presents a solution for all kinds of mobile services through short message service (SMS) systems and a ticket-based service access model that allows anonymous service usage in mobile applications. A service provider can avoid roaming to multiple service domains, only contacting the Credential Centre in the model to check a user's certification. The user can preserve anonymity and read a clear record of charges in the Credential Centre at anytime, and the identity of misbehaving users can be revealed by a Trusted Centre. Furthermore, the solution has been demonstrated by the implementation with SMS and RS232	anytime algorithm;credential;database;integrated windows authentication;mobile app;mobile commerce;protection mechanism;rs-232;roaming user profile;service pack	Hua Wang;Xiaodi Huang;Goutham Reddy Dodda	2006		10.1145/1163673.1163695	mobile identification number;mobile search;mobile qos;mobile web;telecommunications;gsm services;mobile database;computer science;operating system;mobile technology;mobile radio telephone;location-based service;sms banking;mobile business development;internet privacy;mobile station;mobile computing;world wide web;computer security;computer network;short message service;mobile payment	Mobile	-48.377091740589776	65.90520233382074	2100
051f2fdbabf33f2b0ff3b7674751584c3dfede0e	assessing the cost-effectiveness of software reuse: a model for planned reuse	metrics;economic model;reuse cost benefit;software development;cost effectiveness;productivity;reusable component;reuse repository;software reuse;software quality;information system development	Information systems development is typically acknowledged as an expensive and lengthy process, often producing code that is of uneven quality and difficult to maintain. Software reuse has been advocated as a means of revolutionizing this process. The claimed benefits from software reuse are reduction in development cost and time, improvement in software quality, increase in programmer productivity, and improvement in maintainability. Software reuse entails undeniable costs of creating, populating, and maintaining a library of reusable components. There is anecdotal evidence to suggest that some organizations benefit from reuse. However, many software developers practicing reuse claim these benefits without formal demonstration thereof. There is little research to suggest when the benefits are expected and to what extent they will be realized. For example, does a larger library of reusable components lead to increased savings? What is the impact of component size on the effectiveness of reuse? This research seeks to address some of these questions. It represents the first step in a series wherein the effects of software reuse on overall development effort and costs are modeled with a view to understanding when it is most effective. 2003 Elsevier Inc. All rights reserved.	code reuse;complexity;expanded memory;information system;modular programming;population;programmer;programming productivity;simulation;software deployment;software developer;software development process;software quality;value (ethics)	Derek L. Nazareth;Marcus A. Rothenberger	2004	Journal of Systems and Software	10.1016/S0164-1212(03)00248-6	reliability engineering;productivity;cost-effectiveness analysis;systems engineering;engineering;package development process;economic model;software framework;software development;software engineering;domain engineering;metrics;software quality;software metric	SE	-61.551270454717965	27.2633659667732	2106
1834e3e427bcf1b7191e68f02a70be261e2cdcbb	automatische ableitung und verarbeitung von linienbildern				Helmut Kiesewetter;Wolfgang Osten;Jürgen Saedler	1990	it - Informationstechnik	10.1524/itit.1990.32.6.376	operating system;embedded system;computer science	NLP	-98.27643795969773	25.787803317814543	2112
0d421656232e1813e9fd3e49507bb96fe99fda4f	a abordagem poesia para a integração de dados e serviços na web semantica	banco de dados;sistemas de recuperacao da informacao;ontologia;constitutional and consumerists proving garantees	POESIA (Processes for Open-Ended Systems for Inforrnation Analysis), the approach proposed in this work, supports the construction of complex processes that involve the integration and analysis of data from severa! sources, particularly in scientific applications. This approach is centered in two types of semantic Web mechanisms: scientific workflows, to specify and compose Web services; and domain ontologies, to enable semantic interoperability and management of data and processes. The main contributions of this thesis are: (i) a theoretícal framework to describe, discover and compose data and services on the Web, including rules to check the semantic consistency o f resource compositions; (ii) ontology-based methods to help data integration and estimate data provenance in cooperative processes on the Web; (iii) partia! implementation and validation of the proposal, in a real application for the domain of agricultura! planning, analyzing the benefits and scalability problems of the current semantic Web technology, when faced with large volumes of data.	numerical aperture;ontology (information science);scalability;semantic web;semantic interoperability;web service;world wide web	Renato Fileto	2003			world wide web;philosophy	AI	-43.45830774510989	9.51280700376813	2133
090edaac1b7f49f62245a6f9283406b8b40d876b	user experience evaluation metrics for usable accounting tools	software tool;accounting tools;user interface;evaluation method;social development;strategic alignment;poverty alleviation;business environment;evaluation metric;user experience;software development;typical development;developing country;user interface design;software design;usability;service quality;business process;developing countries;small medium micro enterprises	Small Medium and Micro Enterprises (SMMEs) have become active instruments for poverty alleviation, employment generation as well as economic and social development. However, SMMEs in developing countries operate under effervescent business environments characterised by high inflation rates, poor technology infrastructure, lack of business expertise and resources. There is a need for these organisations to strategically align their business processes for them to adapt and survive in the operating environment turbulence. There are dozens of application software designed specifically to support SMME business processes on the market. On the contrary, software developers pay little attention to the user experience and usability aspects of their products resulting in the packages falling short on overall usability. Highly usable applications improve users' productivity, satisfaction and service quality delivery. Hence SMMEs in developing countries should implement software tools with usable user interfaces. Considering the applications' usability failure it becomes vital to research the user experiences of a typical accounting tool commonly used in developing countries' SMMEs. This paper proposes evaluation metrics that are applicable to evaluate the User Experience (UX) of a chosen accounting tool commonly used in developing countries. The paper introduces a brief background on the current situation in typical developing countries' SMMEs. A discussion on the accounting tools used in SMMEs operating in developing countries follows with a definition of UX, its facets and evaluation methods. Evaluation results on the UX of Pastel accounting, Xpress 2009 is presented. The paper culminates with metrics proposed to evaluate the user experience of accounting tools and proposes future work to improve on the proposed metrics.	a/ux;align (company);business process;operating environment;software developer;turbulence;usability;user experience evaluation;user interface;xpress	Job Mashapa;Darelle van Greunen	2010		10.1145/1899503.1899522	simulation;systems engineering;engineering;knowledge management	HCI	-72.94450910857695	19.86462129315281	2135
130bbbd62800df7bf9fa5e48cd7e7781156b3f31	subformula property in many-valued modal logics	many-valued modal logic;subformula property	?0. Introduction. Fitting, in [1] and [2], investigated two families of many-valued modal logics. The first, which is somewhat familiar in the literature, is that of the logics characterized using a many-valued version of the Kripke model (binary modal model in his terminology) with a two-valued accessibility relation. On the other hand, those logics which are characterized using another many-valued version of the Kripke model (implicational modal model), with a many-valued accessibility relation, form the second family. Although he gave a sequent calculus for each of these logics, it is far from having the cut-elimination property (CEP) or the subformula property. So we will give a substitute for his system enjoying the subformula property, though it is not of ordinary sequent calculus but of the many-valued version of sequent calculus initiated by Takahashi [7] and Rousseau	modal logic;well-formed formula	Mitio Takano	1994	J. Symb. Log.		modal logic;dynamic logic;t-norm fuzzy logics;normal modal logic;modal μ-calculus;axiom s5;mathematics;kripke semantics;accessibility relation;multimodal logic	Logic	-9.986993661974614	12.611317948171145	2136
a39fce60bddc71d7254cb1722585fbee3db028db	numerical methods for scientific computations and advanced applications		Modern Scientific Computing is one of the most prominent examples of a truly interdisciplinary area involving mathematics, computer science, engineering, physics, chemistry, medicine, etc. It is also the main instrument that allows us to utilize the huge computational resources of the contemporary and future high performance computer systems. The tools of Scientific Computing are usually based on mathematical models and corresponding computer codes that are used to perform virtual experiments to obtain new data or to better understand existing experimental results. They are particularly important in situationswhen the cost of the experiments or their complexity is prohibitive. For example, inmany cases it is practically impossible to perform in-vitro experiments in living organisms, or it is extremely expensive to verify experimentally all possible scenarios for the flow, dynamic or electromagnetic response of a modern aircraft. Therefore, numerical simulations revolutionized the development in such areas in recent years. Numerical Analysis is one of the crucial elements of Scientific Computing. It develops and analyzes numerical methods for discretization of continuous models and their subsequent solution, as well as for approximation of discrete data, such as: data interpolation and extrapolation, methods for solving linear and non-linear systems of algebraic equations (direct and iterative solution methods, preconditioning, multilevel and multigrid methods, etc.), methods for solving systems of ordinary and partial differential equations (finite difference, finite element, and finite volume methods, etc.), methods for solving integral equations, and optimization problems. Another crucial element of Scientific Computing is the implementation of these numerical methods into computer codes and their customization for the numerous computing systems that are available nowadays. One very recent and important development in this direction is the implementation on various shared or distributed memory parallel computer systems and GPUs that allows for truly large scale scientific computations. The papers in this Special Issue were presented at the International Conference on ‘‘Numerical Methods for Scientific Computations and Advanced Applications’’ (NMSCAA’14), May 19–22, 2014, Bansko, Bulgaria. The conference was organized by the Institute of Information and Communication Technologies (IICT) of the Bulgarian Academy of Sciences (BAS) in cooperationwith the Society for Industrial and AppliedMathematics (SIAM) and devoted to the 60th anniversary of Svetozar Margenov, an internationally recognized Bulgarian expert in the field of Scientific Computing. Svetozar Margenov received his Ph.D. degree in 1984 at the University of Sofia, and the degree of Doctor of Science in 2002 at the Bulgarian Academy of Sciences. In 2003 he was promoted to the rank of a full professor. Dr. Margenov was elected a correspondingmember of BAS in 2014, and at present he is the director of IICT of BAS. He is also one of the creators and the first chairman of the Bulgarian section of SIAM (BGSIAM). Dr. Margenov is a prominent scientist, teacher, and mentor. He authored two monographs and more than 140 papers published in respected international journals and conference proceedings, focusedmostly but not exclusively onmultiscale and multiphysics problems, robust preconditioning, Monte Carlo methods, optimization and control, and scalable parallel algorithms. He is a member of the editorial boards of Numerical Linear Algebra with Applications, Scalable Computing: Practice and Experience (SCPE), and International Journal of Numerical Analysis and Modelling, Series B. His research interests include large-scale scientific computing and parallel algorithms, numerical methods for partial differential equations, computational linear algebra (iterative methods, preconditioning, sparse matrices), large-scale computing of	academy;algebraic equation;approximation;broadcast auxiliary service;code;computation;computational resource;computational science;computer science;computer simulation;discrete mathematics;discretization;distributed memory;dr. web;experiment;extrapolation;finite difference;finite element method;finite volume method;graphics processing unit;interpolation;iterative method;linear system;mathematical model;mathematical optimization;monte carlo method;multigrid method;multiphysics;nonlinear system;numerical analysis;numerical linear algebra;numerical method;optimization problem;parallel algorithm;parallel computing;preconditioner;scalability;sparse matrix;supercomputer	Krassimir Georgiev;Oleg P. Iliev;Peter Minev	2015	Computers & Mathematics with Applications	10.1016/j.camwa.2015.11.004	computational physics;mathematics;mathematical optimization;computation;numerical analysis	HPC	-7.245762404839373	36.5028586268226	2138
41539a17cbd8cf2a0ce11b15735fa5e11609a53d	a service model for the development of management systems for it-enabled services	it service;conceptual model;information management;services science;it service management;systems development	The shift from device and application towards service-orientated IT Management raises new questions that require concepts such as portfolio management, resource planning or mass customization for IT services. These concepts increase the complexity of IT Service Management and require additional tool support. Conceptual models are necessary in order to develop appropriate tools. The goal of our paper is to propose and validate a conceptual IT service model. We introduce the characteristics of IT services and analyze existing IT service models. A common IT service model is derived (theoretically) from the literature and validated through cases of IT service providers. These case studies from three German IT service providers also yield insights for further research.	enterprise resource planning;information management;information system;requirement;software development process	Nico Ebert;Falk Uebernickel;Axel Hochstein;Walter Brenner	2007			service provider;service level requirement;service level objective;information technology infrastructure library;financial management for it services;service product management;differentiated service;service management;systems engineering;knowledge management;service delivery framework;service design;change management;service guarantee;software as a service;management science;business;service desk;it portfolio management;service integration maturity model;incident management;service system	DB	-70.73927497342402	12.162243245898065	2142
d6131965738f8cb28f1469ba1092fcf826ff93f6	identity-based key-exposure resilient cloud storage public auditing scheme from lattices		Abstract With the rapid development of cloud auditing services, key exposure has been highlighted as a serious security issue. Using the exposed private key of a client, cloud servers can forge previous auditing proofs to cheat auditors. To date, a few pairing-based cloud storage auditing schemes addressing key exposure have been proposed. However, they are not secure from quantum attacks, and they rely on public key infrastructure (PKI), which involves complex certificate management. In this paper, we propose an efficient identity-based key-exposure resilient public auditing scheme from lattice assumptions in cloud storage. Our scheme is not only quantum-resistant, but eliminates the need to establish a PKI. We employ lattice basis delegation technique to update a client’s private key flexibly, keeping the private key size constant. Based on the hardness of lattice assumptions, we prove the forward security of storage correctness guarantee against malicious cloud servers in detail, and that the proposed scheme preserves privacy against curious auditors. Furthermore, we conduct a performance comparison to demonstrate that our scheme is much more efficient and practical for post-quantum secure cloud storage.	cloud storage	Xiaojun Zhang;Huaxiong Wang;Chunxiang Xu	2019	Inf. Sci.	10.1016/j.ins.2018.09.013	machine learning;mathematics;forward secrecy;public key infrastructure;artificial intelligence;cloud storage;public-key cryptography;audit;cloud computing;delegation;server;distributed computing	DB	-42.28379591876079	68.72236828401361	2150
c52095072359df7cc7a0a3de9662e3cac75f75c0	ldow 2013: the 8th workshop on linked data on the web	linked data;semantic web;data integration;rdf	This paper presents a brief summary of the eight workshop on Linked Data on the Web. The LDOW 2013 workshop is held in conjunction with the World Wide Web conference 2013. The focus is on data publishing, integration and consumption using RDF and other semantic representation formalisms and technologies.	linked data;resource description framework;world wide web	Sören Auer;Tim Berners-Lee;Christian Bizer;Tom Heath	2015		10.1145/2740908.2742577	web modeling;data web;web mapping;web standards;computer science;sparql;simple knowledge organization system;data integration;semantic web;rdf;social semantic web;linked data;semantic web stack;database;web intelligence;web 2.0;world wide web;information retrieval;semantic analytics	AI	-39.68334783532223	5.874377454106054	2152
9f4af88b15b5335a4ee3c08b02ca39c2b7177eab	a visual logic for the description of highway traffic scenarios		In this paper, we present the syntax and semantics of the visual logic (VL) used to specify (sequences of) traffic situations on the highway. VL was developed in the context of driver assistance system development, and is intended to bridge the (terminology) gap between system engineers and traffic psychologists developing driver assistance systems, and scientists modelling, analysing and verifying such systems. To achieve this goal, the logic is intuitive and simple, thus easy to under- stand and apply, yet it has a formal, automaton-based semantics which allows to use well-established tools and formalisms for further analysis of the system. We show how VL can be used to specify scenarios on the highway involving in- teraction of driver and assistance system, and how it can be used in the context of observer-based verification to analyse and verify the assistance system with respect to these scenarios.	dudebro: my shit is fucked up so i got to shoot/slice you ii: it's straight-up dawg time;graphical user interface;semantics (computer science);semi-continuity;stephanie forrest;verification and validation;visual logic	Stephanie Kemper;Christoph Etzien	2013		10.1007/978-3-319-02812-5_17	simulation;computer science;systems engineering;transport engineering	AI	-41.143744312991366	29.633901051782914	2156
05632d6926d75f1b16e095a3c81e00a364c2961e	essential incompleteness of arithmetic verified by coq	teoria demonstracion;high order logic;theorie type;theorie preuve;automatic proving;proof theory;demostracion automatica;program verification;arithmetique;theorem proving;logica orden superior;demonstration automatique;demonstration theoreme;recursive function;verificacion programa;aritmetica;arithmetics;logique ordre 1;type theory;peano arithmetic;funcion recursiva;fonction recursive;logic in computer science;logique ordre superieur;demostracion teorema;verification programme;coq proof assistant;article in monograph or in proceedings;first order logic;logica orden 1	A constructive proof of the Gödel-Rosser incompleteness theorem [9] has been completed using the Coq proof assistant. Some theory of classical first-order logic over an arbitrary language is formalized. A development of primitive recursive functions is given, and all primitive recursive functions are proved to be representable in a weak axiom system. Formulas and proofs are encoded as natural numbers, and functions operating on these codes are proved to be primitive recursive. The weak axiom system is proved to be essentially incomplete. In particular, Peano arithmetic is proved to be consistent in Coq’s type theory and therefore is incomplete.	axiomatic system;code;coq (software);first-order logic;first-order predicate;gödel;peano axioms;primitive recursive function;proof assistant;recursion;type theory	Russell O'Connor	2005		10.1007/11541868_16	gentzen's consistency proof;computer science;primitive recursive function;pure mathematics;proof theory;first-order logic;mathematics;automated theorem proving;programming language;μ-recursive function;peano axioms;primitive recursive arithmetic;type theory;μ operator;algorithm	Theory	-13.857149593828819	17.020334801922786	2158
c256ed723f6be2cefe0a5e5482767a7f97564685	an empirical study of the telecommunications service industries using productivity decomposition		Telecommunications service industry provides services that transmit voice, data, text, sound, video, and other signals. In the Internet age, the telecommunications service industry has experienced tremendous growth to form an indispensable infrastructure platform for today's global networked economy. In this study, we examine the output performance of telecommunications service industries in 13 Organization of Economic Cooperation and Development countries from 2000 to 2011 using the Malmquist total factor productivity index (MTFPI) as the performance metric and data envelopment analysis as the measurement approach. We further decompose MTFPI into three factors: 1) technical change; 2) pure efficiency change; and 3) scale efficiency change, that represent innovation, catch-up, and demand fluctuation, respectively. The results show that these telecommunications service industries exhibit comparatively strong productivity growth. In addition, through our decomposition analysis, it is found that telecommunications service industry is an innovator skilled at adopting technological advances that turn out to be the key driving force for the observed productivity growth. By contrast, both pure efficiency change and scale efficiency change lead to negative impacts. Based on these findings, we draw and discuss implications for telecommunications service at the country and industry levels, and provide suggestions for practice and future research.	data envelopment analysis;quantum fluctuation	Benjamin B. M. Shao;Winston T. Lin;Juliana Y. Tsai	2017	IEEE Transactions on Engineering Management	10.1109/TEM.2017.2713771	productivity;the internet;economies of scale;total factor productivity;technical change;performance metric;commerce;economics;data envelopment analysis;telecommunications service	Visualization	-86.57151747940516	7.655759296233278	2165
13d413e79d171f6673a44454d539cab90e681f99	design of a cluster-based web server with proportional connection delay guarantee	proportional connection delay guarantee;low priority;communications society;control systems;file servers;web server delay control systems dispatching fuzzy control communications society computer science internet fuzzy systems pi control;web clusters;cluster based web server;fuzzy proportional integral controller;fuzzy control;unacceptable long delay;http 1 1 cluster based web server proportional connection delay guarantee internet high client loads unacceptable long delay web clusters stand alone web server qos controller fuzzy proportional integral controller system resources;system resources;internet;http 1 1;cluster system;high client loads;qos controller;web server;computer science;pi controller;stand alone web server;pi control;fuzzy systems;pi control file servers fuzzy control internet;dispatching;proportional integral	Cluster based Web servers have been widely deployed by enterprises to accommodate the ever-increasing population of Internet users. But during the period of high client loads, Web clusters may still fail to provide timely service to all the clients, and part of the clients may suffer unacceptable long delay. In order to provide better service to premium users, QoS schemes need be implemented in the Web clusters. Controlling mechanisms for stand-alone Web server have been studied in previous years, but it is still a challenging problem to design a QoS controller for Web clusters. In this paper, we apply a fuzzy Proportional Integral (PI) controller to dynamically assign system resources for guaranteeing proportional connection delay ratio. To overcome the difficulties caused by HTTP/1.1, we further propose a preemptive scheme which disconnects those idle clients with low priority if there is short of resources. We implement the controller in a real Web cluster system based on Apache, and study the performance by extensive experiments. Our results show that (1) the proposed fuzzy PI controller achieves much better performance than classic PI controller; (2) the preemptive schemes outperform non-preemptive schemes significantly.	experiment;hypertext transfer protocol;preemption (computing);quality of service;server (computing);server farm;web server;world wide web	Ka Ho Chan;Xiaowen Chu	2008	2008 IEEE International Conference on Communications	10.1109/ICC.2008.1066	pid controller;file server;real-time computing;the internet;computer science;operating system;distributed computing;world wide web;web server;fuzzy control system;computer network	Embedded	-17.761213468524634	69.30602512148394	2184
e810df5840342898918abf891a6e6b9f46909649	building enterprise data centers using virtual and cloud	software;computer room;air conditioning;virtualization;cloud computing enterprise data center information technology information center power consumption;internet business data processing computer centres information centres;information technology;virtual machining;computer centres;computer architecture;data center;servers;internet;information center;business data processing;business;information center cloud computing virtualization computer room;information centres;power consumption;servers cloud computing software hardware computer architecture virtual machining business;cloud computing;hardware	With the rapid development of information, companies in the market in order to create greater competitiveness through new information technology trickled into the new system, but with the new Goumai the server and into the new system, information center gradually space can be used to less and less from the Information Centre as the power consumption will increase significantly, increased air conditioning usage, significant increase in monthly electricity bills, especially during the summer months even more significant.	information centre;server (computing)	Dwen-Ren Tsai;Sheng-Wei Lin	2010	2010 Sixth International Conference on Intelligent Information Hiding and Multimedia Signal Processing	10.1109/IIHMSP.2010.181	data center;the internet;virtualization;simulation;cloud computing;air conditioning;computer science;operating system;information technology;world wide web;server	Robotics	-32.61959270215658	56.40674503716158	2185
39449a4873279f3d5392b705badfa8604cbc293c	multiagent knowledge and belief change in the situation calculus	reasoning with beliefs;action change and causality;belief change	Belief change is an important research topic in AI. It becomes more perplexing in multi-agent settings, since the action of an agent may be partially observable to other agents. In this paper, we present a general approach to reasoning about actions and belief change in multi-agent settings. Our approach is based on a multiagent extension to the situation calculus, augmented by a plausibility relation over situations and another one over actions, which is used to represent agents’ different perspectives on actions. When an action is performed, we update the agents’ plausibility order on situations by giving priority to the plausibility order on actions, in line with the AGM approach of giving priority to new information. We show that our notion of belief satisfies KD45 properties. As to the special case of belief change of a single agent, we show that our framework satisfies most of the classical AGM, KM, and DP postulates. We also present properties concerning the change of common knowledge and belief of a group of agents.	agent-based model;artificial intelligence;belief revision;computation;first-order predicate;high- and low-level;multi-agent system;partially observable system;plausibility structure;situation calculus	Liangda Fang;Yongmei Liu	2013			artificial intelligence	AI	-17.368142152832366	4.775420383639608	2189
0b246ff5cf02ffe8b4f966cd997f7a5d3e48d07b	analogical inferred compensation method for heterogeneous database schema mismatch and its application of hypermedia cooperative work system	heterogeneous databases;object oriented database;information system;cooperative work	Hypermedia Cooperative Work systems (HMCW) that simply use object-oriented databases have already been developed, but almost of them are not always regard to directly access stationary databases such as CD-ROMs and other conventional databases because of the need to convert to another media or to compensate for database schema mismatch. In order to solve these problems, we propose a new architecture with 3-level schema management to handle data in the heterogeneous database, and a dynamic schema mismatch compensation method based on an analogical inference mechanism. The effectiveness of this architecture and this method is established by using them in a practical hypermedia cooperative work environment: a patent information system.		Shigeru Shimada;Toshihisa Aoshima;Tetsuzo Uehara	1993		10.1007/3-540-57301-1_18	computer science;knowledge management;data mining;database;database schema;database testing	DB	-34.800150085564596	10.981862865572943	2192
092f5ee47eca4ffd6a57f34387de0c4b39482563	computer networks (elsevier) special issue on advances in wireless and mobile networks	computer network;mobile network			Violet R. Syrotiuk;Brahim Bensaou	2010	Computer Networks	10.1016/j.comnet.2009.09.003	cellular network;intelligent computer network;wireless wan;telecommunications;computer science;artificial intelligence;mobile computing;computer network	Mobile	-16.132201222687172	88.7188573676436	2194
75cd6d250dc44ecc43ca0d51847c68f19f229cfe	a review of performance measurement: towards performance management	performance measure;systeme forme chaine;evaluation performance;systems;performance evaluation;frameworks;performance management;inter organisational performance measurement;evaluacion prestacion;charpente;sistema forma cadena;recommendations;recommandation;gestion performance;typology;recomendacion;recommendation;extended enterprise;supply chain;typologie;mesure performance interorganisationnelle;chained form system;article;armadura;framework;tipologia	Describes the evolution of performance measurement in four sections: recommendations, frameworks, systems, and inter-organisational performance measurement. Measurement begins with a recommendation, which is a piece of advice related to the measures or structure of performance measurement; frameworks can be dichotomised into a structural and procedural typology that suggests structural framework development has outstripped procedural framework development. The basic requirements for a successful PM system are two frameworks one structural, and one procedural; as well as a number of other performance management tools. Inter-organisational performance measurement may be divided into supply chain and Extended Enterprise performance measurement: the former relying solely on traditional logistics measures, while the latter incorporates the structural aspects of the supply chain system and adds nonlogistics perspectives to its measurement arena. Finally the encroachment of the performance measurement literature into the processes related to performance management is examined, and areas for future research are suggested.	biological anthropology;extended enterprise;logistics;procedural programming;requirement	Paul Folan;Jim Browne	2005	Computers in Industry	10.1016/j.compind.2005.03.001	performance management;simulation;computer science;engineering;operations management;software framework;programming language;management	Metrics	-78.21579719324903	13.36914796137574	2202
570b5f1d346948155a6d78c9291a046760ea71bf	part v. standardized test notation and execution architecture	system modelling;automatic generation;test case generation;transition systems;finite state machine	  The previous parts of this volume dealt mainly with test case generation. We have presented methods or finite-state machines  and for transition systems and reviewed tools and case studies. In this part, we give two examples for formal test notations.  While the two description methods have been designed for specifying test cases manually, it is in general possible to use  these notations also for automatically generated test cases, which gives the link to the previous parts.      Chapter 16 gives an introduction to TTCN-3, the Testing and Test Control Notation, which is a standardized language to formulate  tests and to control their execution.        In Chapter 17, the standardized UML 2.0 test profile is explained. It provides means to use UML both for system modelling  as well as test case specification.      			2004		10.1007/11498490_20	reliability engineering;computer architecture;real-time computing;computer science	SE	-46.59539301019629	31.029042545216935	2205
b3f40694c1991191aef2c9974b7b5092571c9994	guest editors' introduction: special section on system-level design of reliable architectures	inf;system software;special issues and sections;special issues and sections design methodology system software computer network management fault tolerance computer architecture;computer architecture;fault tolerance;computer network management;system level design;design methodology	IT is with great pleasure that we introduce this special section on System-Level Design of Reliable Architectures to the audience of the IEEE Transactions on Computers. Six papers have been selected covering a wide spectrum of topics ranging from architectural fault-tolerant techniques to formal methodologies for reliability analysis. These papers are authored by relevant researchers in the field and cover theoretical and experimental topics. The widespread use of electronics in our life is directing more and more attention to the reliability properties of such systems in order to preserve both user’s and environmental safety; therefore, the design of reliable architectures is today a necessity rather than an option, even in not-critical application domains. At the same time, these systems are reaching high complexity levels, thus leading the designer to both develop specific components and to use and compose existing ones to achieve the desired overall functionality. In the former case, ad hoc techniques may be devised, acting on either the hardware or the software to cope with the occurrence of faults. In this latter situation, when combining independently designed modules, the enhancement and assessment of reliability becomes particularly important; for instance, specific approaches are required to be able both to apply fault detection/tolerance techniques from the initial steps of the design flow and to evaluate the effects of faults in a component while interacting with the other ones composing the overall system. As a result, the entire design flow needs to be enhanced to support reliability: from the initial modelling of the system together with the desired properties/requirements, to the fault model, from the hardware/ software partitioning step to the subsequent design exploration phase, where the more traditional metrics covering performance, costs, and power consumption need to be modified to also weight fault detection/tolerance capabilities. Functional verification and reliability analysis constitute two other aspects of this scenario to assess the quality of the designed system in terms of correctness and its ability to deal with failures. In this scenario, new advances have been achieved in all the relevant issues pertaining the system-level design of reliable systems, to support the designers in the development of innovative architectures able to cope with the occurrence of failures. Such advances lead to the definition of both new methodologies, as well as, of new architectures. Furthermore, based on the application environment in which the system will be adopted, different classes of reliability might be necessary; in some situations it is possible to achieve an autonomous fault detection capability, whereas, in critical environments, fault effects need to be completely masked, thus providing fault tolerance properties. The six papers presented in this special section were selected to address the different aspects of the important challenges related to the system level design of reliable systems. They cover all various facets of the issue, offering interesting solutions to tackle the specific problems. The first two papers deal with reliability analysis, which has become a fundamental tool to computer engineers for the validation of the design of hardened system architectures, in particular in safety and mission critical domains, such as medicine, military and transportation. The first paper is entitled “Formal Reliability Analysis Using Theorem Proving” by Osman Hasan, Sofiène Tahar, and Naeem Abbasi. This paper addresses an important aspect of reliability analysis, attempting to introduce formal verification instead of simulation-based and probabilistic approaches to assess the fault tolerance characteristics of the designed systems. The authors propose to conduct a formal reliability analysis of systems within the framework of a higher-order-logic theorem prover. In this paper, they present the higher-orderlogic formalization of some fundamental reliability theory concepts, which can be built upon to precisely analyze the reliability of various engineering systems. The proposed formalization is then applied to analyze the repairability conditions for a reconfigurable memory array in the presence of stuck-at and coupling faults. Still within the context of reliability analysis, the second paper, entitled “Efficient Microarchitectural Vulnerabilities Prediction Using Boosted Regression Trees and Patient Rule Inductions,” by Bin Li, Lide Duan, and Lu Peng, deals with Architectural Vulnerability Factor (AVF) analysis, which reflects the possibility that a transient fault eventually causes a visible error in the program output, and it indicates a system’s susceptibility to transient faults. This metric is increasingly being adopted to evaluate microprocessor’s architectures, due to their high vulnerability to transient faults, derived from shrinking feature sizes, threshold voltage, and increasing frequency. The authors propose an innovative way to predict the architectural vulnerability factor using Boosted Regression Trees, a nonparametric tree-based predictive modeling scheme, to identify the correlation across workloads, execution phases, and processor configurations, between the estimated AVF of a key processor structure and various performance metrics. The next two papers deal with fault detection techniques for different architectural components. The first paper is entitled “Concurrent Structure-Independent Fault Detection Schemes for the Advanced Encryption Standard,” authored by Mehran Mozaffari-Kermani and Arash Reyhani-Masoleh. IEEE TRANSACTIONS ON COMPUTERS, VOL. 59, NO. 5, MAY 2010 577	application domain;automated theorem proving;autonomous robot;computer engineering;correctness (computer science);decision tree;design flow (eda);electronic system-level design and verification;encryption;fault detection and isolation;fault model;fault tolerance;formal verification;hoc (programming language);ieee transactions on computers;interaction;lu decomposition;level design;mit engineering systems division;microarchitecture;microprocessor;mission critical;predictive modelling;reliability (computer networking);reliability engineering;requirement;simulation;software bug	Cristiana Bolchini;Donatella Sciuto	2010	IEEE Trans. Computers	10.1109/TC.2010.71	fault tolerance;computer architecture;computing;design methods;computer science;electronic system-level design and verification;software system	EDA	-43.92331073194095	35.984067270291824	2206
821b65a0c8fca24697d8da261ca16cc35d9d52d7	nontrivial definability by flow-chart programs	organigramme;flowchart;structure programme;recursivite;program analysis;analyse programme;recursivity;program structure;definissabilite	The ability of flow-chart programs to define relations and functions nondefinable by open first order formulas is studied. An example of a unary, not locally finite structure T o (with equality), such that every flow-chart is equivalent in T to a loop-free flow-chart, is shown, but the same does not hold for some flow-charts equipped with counters or recursion. From this it is deduced that Deterministic Dynamic Logic of regular programs is strictly weaker than (nondeterminitic) Dynamic Logic. It is also proved that flow-charts with recursion (but without counters) are able to define nontrivial functions in any structure, provided a nontrivial computable function exists over this structure.	chart;computable function;flowchart;open road tolling;recursion;unary operation	Pawel Urzyczyn	1983	Information and Control	10.1016/S0019-9958(83)80057-6	program analysis;recursion;flowchart;computer science;calculus;mathematics;programming language;algorithm	Theory	-11.615718828385708	19.031972579866995	2207
951f71098e64eaa981f2612d0a7732cf4ae43293	bristlecone: language support for robust software applications	program diagnostics;computer languages;high level structure;benchmark applications;language support;programming language;injected failures;application software;robust software applications;robust software systems;software systems;software fault tolerance;task interactions;indexing terms;runtime;web crawler;robustness application software software systems switches runtime computer languages costs web server java software tools;high level organization specification;specification languages;game server;low level operational specification;software robustness;java version;bristlecone compiler;robustness;software tools;web server;static analysis;program compilers;switches;game server language support robust software applications programming language robust software systems high level organization specification low level operational specification runtime benchmark applications web crawler web server multiroom chat server java version injected failures bristlecone compiler static analysis task interactions high level structure;programming languages;multiroom chat server;java;specification languages java program compilers program diagnostics programming languages software fault tolerance	We present Bristlecone, a programming language for robust software systems. Bristlecone applications have two components: a high-level organization specification that describes how the application's conceptual operations interact and a low-level operational specification that describes the sequence of instructions that comprise an individual conceptual operation. Bristlecone uses the high-level organization specification to recover the software system from an error to a consistent state and to reason how to safely continue the software system's execution after the error. We have implemented a compiler and runtime for Bristlecone. We have evaluated this implementation on three benchmark applications: a Web crawler, a Web server, and a multiroom chat server. We developed both a Bristlecone version and a Java version of each benchmark application. We used injected failures to evaluate the robustness of each version of the application. We found that the Bristlecone versions of the benchmark applications more successfully survived the injected failures. The Bristlecone compiler contains a static analysis that operates on the organization specification to generate a set of diagrams that graphically present the task interactions in the application. We have used the analysis to help understand the high-level structure of three Bristlecone applications: a game server, a Web server, and a chat server.	apl;benchmark (computing);compiler;diagram;game server;high- and low-level;interaction;java;level structure;programming language;robustness (computer science);server (computing);software system;static program analysis;web crawler;web server	Brian Demsky;Sivaji Sundaramurthy	2011	IEEE Transactions on Software Engineering	10.1109/TSE.2010.27	computer science;operating system;software engineering;database;programming language;robustness	SE	-56.969981840610785	37.42337982278434	2209
26ef9ebbfb8d5759ca9134ab6142ffcfbb27b21d	task-based adaptive multiresolution for time-space multi-scale reaction-diffusion systems on multi-core architectures	stiff reaction diffusion equations;adaptive grid;multi core architectures;multiresolution;task based parallelism	A new solver featuring time-space adaptation and error control has been recently introduced to tackle the numerical solution of stiff reaction-diffusion systems. Based on operator splitting, finite volume adaptive multiresolution and high order time integrators with specific stability properties for each operator, this strategy yields high computational efficiency for large multidimensional computations on standard architectures such as powerful workstations. However, the data structure of the original implementation, based on trees of pointers, provides limited opportunities for efficiency enhancements, while posing serious challenges in terms of parallel programming and load balancing. The present contribution proposes a new implementation of the whole set of numerical methods including Radau5 and ROCK4, relying on a fully different data structure together with the use of a specific library, TBB, for shared-memory, task-based parallelism with work-stealing. The performance of our implementation is assessed in a series of test-cases of increasing difficulty in two and three dimensions on multi-core and many-core architectures, demonstrating high scalability.	computation;data structure;error detection and correction;finite volume method;list of operator splitting topics;load balancing (computing);manycore processor;multi-core processor;numerical method;numerical partial differential equations;parallel computing;scalability;shared memory;solver;threading building blocks;work stealing;workstation	Stéphane Descombes;Max Duarte;Thierry Dumont;Thomas Guillet;Violaine Louvet;Marc Massot	2017	CoRR	10.5802/smai-jcm.19	mathematical optimization;parallel computing;real-time computing;computer science;theoretical computer science;algorithm	HPC	-5.3015453853795815	39.054839531075345	2210
7d4548cfd241f8ddbcd7e9b6ff1520ae5fe3d2e1	delta20 - categoricity in boolean algebras and linear orderings	linear order;boolean algebra	Abstract   We characterize  Δ  2  0 -categoricity in Boolean algebras and linear orderings under some extra effectiveness conditions. We begin with a study of the relativized notion in these structures.		Charles F. D. McCoy	2003	Ann. Pure Appl. Logic	10.1016/S0168-0072(02)00035-0	boolean algebra;combinatorics;discrete mathematics;boolean domain;ideal;boolean expression;product term;maximum satisfiability problem;stone's representation theorem for boolean algebras;boolean algebras canonically defined;mathematics;complete boolean algebra;interior algebra;total order;two-element boolean algebra;free boolean algebra;parity function;algebra	Logic	-8.596563978109836	12.226605624092116	2212
35ba431d9b2792d6660ceebe7bb5f63d72f1f273	aspect assumptions: a retrospective study of aspectj developers' assumptions about aspect usage	aspectj;retrospective study;developer assumptions;open source	Aspect developers constantly make a range of assumptions about the context in which their aspects will be deployed; ranging from assumptions about other aspects deployed to assumptions about semantic properties of the base and the joinpoints at which an aspect is woven. Although it has been acknowledged that such assumptions need to be made explicit to validate aspects in the face of evolution (both of aspects and the base) and reuse as well as to mitigate the fragile-pointcut problem, so far no study exists that identifies the types of assumptions aspect developers make. In this paper, we present a retrospective study of three medium-sized open-source AspectJ projects and assumptions identified in these. This leads to an initial classification of assumptions that can form the basis for further research into how best to support each type of assumption.	aspectj;open-source software;pointcut	Steffen Zschaler;Awais Rashid	2011		10.1145/1960275.1960288	simulation;computer science;retrospective cohort study	SE	-59.11568904728579	22.209092792433214	2216
fa03ac29ce91a82a30520afe0af84fe01285ff5d	pulsar: parallel noise despeckling of sar images	distributed memory;image enhancement;sar image;workstation cluster;image similarity	Anneal provides a means by which users can obtain significant image enhancement over other general purpose techniques for SAR noise despeckling. Its principle disadvantage lies in its rather steep computation requirements, particularly on standard sized images (2048 by 2048 and 4096 by 4096). However, PULSAR has been able to produce a portable parallel version of Anneal that has excellent parallel performance on both workstation clusters and dedicated distributed memory parallel platforms. Speed-ups of 90 have been observed using 96 processors on the GC PowerPlus with the relatively small 1024 by 1024 image. Similar performance is expected on other platforms with a reasonable interconnect. Thus, Anneal is suitable for both providers of satellite SAR images (who likely employ MPP's to perform the initial low-level processing) as well as for interpreters and direct users of SAR data, whose source of computation comes from networked workstations.	noise reduction	Cliff Addison;Enrico Appiani;R. Cook;Marco Corvi;J. Harms;G. Howard;B. Stephens	1996		10.1007/3-540-61142-8_545	computer vision;parallel computing;distributed memory;computer science	Vision	-5.104862692066451	40.398317189020375	2227
440040a61806344424d67434abe77b7168f61f25	policy 2003: workshop on policies for distributed systems and networks	distributed system	Policy-based systems are the subject of a wide range of activities in universities, standardization bodies and within industry [1]. They have a wide spectrum of applications ranging from quality-of-service management within networks to security and enterprise modeling. Within the Internet community there is considerable interest in Policy Based Networking. A number of companies have announced tools to support the specification and deployment of policies. Much of this work is focused on policies for quality-of-service management within networks and the Internet Engineering and Distributed Management Task Forces (IETF/DMTF) are actively working on standards [2] related to this area. The Security community has focused on the specification and analysis of access control policy, which has evolved into the work on Role-Based Access Control (RBAC) [3]. There had been work over a number of years in the academic community on specification and analysis of policies for Distributed Systems [4], mostly concentrating on authorization polices. Although there are strong similarities in the concepts and techniques used by the different communities there is no commonly accepted terminology or notation for specifying policy. Several research groups are looking at high-level aspects of policy related to Enterprise Modelling. An ISO Open Distributed Processing (ODP) working group is defining Policy and Role concepts within the Enterprise Viewpoint [5]. Enterprise goals or Service Level Agreements can be considered as high-level abstract policies, which must be progressively refined into implementable policies. The work on the specification and analysis of Business Rule is also relevant.	authorization;distributed computing;enterprise modelling;high- and low-level;internet;quality of service;rm-odp;role-based access control;service-level agreement;software deployment;viewpoint	Hanan Lutfiyya;Francisco Garcia;Jonathan D. Moffett	2003	Journal of Network and Systems Management	10.1023/A:1025798425019	computer science	Security	-58.86897189835758	18.999695877901328	2228
caf6251d0ce40832a57dff2248809b8fe964b3eb	an integrated automation system for design/build organisations	information systems;construction companies;relational database;aec firms;architectural practices;data warehouse;design build;office automation	The construction of buildings is a complex process that is carried out by a large number of participants with conflicting objectives. This complexity and fragmentation makes the management of the process particularly difficult. This paper presents an integrated system called MITOS  multi-phase integrated automation system that has been designed primarily for design/build firms, i.e., firms or alliances of firms that do both design and construction. But the system can also be used by independent design firms and construction companies as long as they are cooperating on the same project. This integrated system allows designers as well as constructors to receive information about all aspects of the project (including clients, subcontractors, suppliers, human resources, time, design, materials, equipment, cost, communication, quality, and procurement) in the design and construction phases of the project and hence to manage the project effectively. The system makes use of a central component called ASDB  automation system for design/build and a set of satellite programs.	constructor (object-oriented programming);fork (software development);integrated development environment;procurement	Alaattin Kanoglu;David Arditi	2004	IJCAT	10.1504/IJCAT.2004.003831	integrated project delivery;relational database;computer science;systems engineering;engineering;knowledge management;marketing;data warehouse;management;construction management;information system	EDA	-58.587664896174445	11.950930333416787	2231
40e5718ef763aa0ce64b3e40e1c5db86c92b2f34	knowledge-based management support: an application of diagnostic reasoning to corporate financing decisions	corporate finance;decision support systems;financial management	The corporate finance application domain is studied as part of designing a diagnostic decision support system (DSS) for analyzing the financial ‘health’ of a modern corporation. A conceptual model of the financial management application domain is developed to capture the inherent interactions among financial domain objects. We view the objective of financial management to be to maintain a balance among financial performance measures, such as profitability, liquidity, dividend policy, and capital structure. We represent knowledge regarding these performance measures together with the related financial accounting data. A high-level structure to the diagnostic reasoning process is introduced via a classification scheme that identifies the relevant problem categories. The proposed approach supports the interpretation of domain relationships when formulating corrective measures. More importantly, it provides proactive management support by facilitating the analysis of domain facts for probable contingencies. The approach is illustrated with examples.  1998 John Wiley & Sons, Ltd.	algorithm;application domain;artificial intelligence;comparison and contrast of classification schemes in linguistics and metadata;conceptualization (information science);decision support system;domain-driven design;high- and low-level;interaction;john d. wiley;knowledge base;knowledge representation and reasoning;level structure;linkage (software);problem solving;software documentation;software prototyping	Meral Binbasioglu;Edward J. Zychowicz	1998	Int. Syst. in Accounting, Finance and Management	10.1002/(SICI)1099-1174(199812)7:4%3C199::AID-ISAF159%3E3.0.CO;2-Q	financial modeling;financial management;accounting management;actuarial science;economics;knowledge management;artificial intelligence;corporate finance;finance;strategic financial management;management science	AI	-75.2670649386852	12.210589158133818	2235
def71184ffe79fd20ee9182c923a3f2a497d5dc6	mirna and co: methodologically exploring the world of small rnas. (miarn et compagnie: une exploration méthodologique du monde des petits arns)			bibliothèque des ecoles françaises d'athènes et de rome	Susan Higashi	2014				NLP	-105.39060273341377	13.031228017016845	2237
ec233fb06420d505d75cc2ad75a45986356e6e54	analysis of the perfect table fuzzy rainbow tradeoff		Cryptanalytic time memory tradeoff algorithms are tools for inverting one-way functions, and they are used in practice to recover passwords that restrict access to digital documents. This work provides an accurate complexity analysis of the perfect table fuzzy rainbow tradeoff algorithm. Based on the analysis results, we show that the lesser known fuzzy rainbow tradeoff performs better than the original rainbow tradeoff, which is widely believed to be the best tradeoff algorithm.The fuzzy rainbow tradeoff can attain higher online efficiency than the rainbow tradeoff and do so at a lower precomputation cost.		Byoung-Il Kim;Jin Hong	2014	J. Applied Mathematics	10.1155/2014/765394	theoretical computer science;algorithm	Crypto	-34.89271616078959	79.32703569306103	2244
6ebe282100f13c4d4bf1ff99bcfa29876743892e	maintenance of formal software developments by stratified verification	developpement logiciel;formal specification;redundancia;computer software maintenance;ingenieria logiciel;program verification;software engineering;specification formelle;especificacion formal;maintenance logiciel;verificacion programa;formal verification;redundancy;specification and verification;desarrollo logicial;software development;genie logiciel;algorithme evolutionniste;verification formelle;algoritmo evolucionista;evolutionary algorithm;evolutionary process;verification programme;redondance	The development of industrial-size software is an evolutionary process based on structured specifications. In a formal setting, specification and verification are intertwined. Specifications are amended either to add new functionality or to fix bugs detected during the verification process. In this paper we propose a system to maintain the verification of formal developments. It exploits the structure of the specification to reveal and eliminate redundant proof obligations and therefore constitutes itself a verification system in-the-large. Proofs in this system are represented as explicit proof objects allowing the system to adjust or reuse them in case the specification is changed.	verification and validation	Serge Autexier;Dieter Hutter	2002		10.1007/3-540-36078-6_3	verification;formal methods;formal verification;software verification;computer science;software development;evolutionary algorithm;formal specification;high-level verification;runtime verification;redundancy;programming language;intelligent verification;algorithm;functional verification	Logic	-41.925064697058076	26.689519190201462	2247
44523f1ac3380d6f18f51a2c47eb5c17630bf62b	the convergence of business process management and service oriented architecture	business process management;service oriented architecture		business process;service-oriented architecture	Faouzi Kamoun	2007	Ubiquity	10.1145/1276166.1276167	enterprise architecture framework;sherwood applied business security architecture;business service provider;differentiated service;computer science;knowledge management;artifact-centric business process model;business process management;applications architecture;service-oriented architecture;process modeling;enterprise architecture management;service;solution architecture;business process model and notation;process management;enterprise architecture;business system planning;business process;event-driven process chain;business process modeling;business architecture	DB	-56.892037315314866	16.58695533802172	2249
1ea17e45f6b0b08251cb8bef8aefc957ee3526e3	sensor cloud: a cloud of virtual sensors	sensor phenomena and characterization;wireless sensor networks cloud computing;wireless sensors;sensor cloud;temperature sensors;wireless communication;virtual sensors;clouds;wireless sensor networks sensor phenomena and characterization cloud computing wireless communication temperature sensors virtual environments next generation networking mobile computing;missouri science and technology sensor cloud virtual sensor cloud internet of things sensor cloud architecture resource constrained physical wireless sensors missouri s t sensor cloud;middleware;middleware sensor cloud virtual sensors wireless sensors;wireless sensor networks;cloud computing	Newer models for interacting with wireless sensors such as Internet of Things and Sensor Cloud aim to overcome restricted resources and efficiency. The Missouri S&T (science and technology) sensor cloud enables different networks, spread in a huge geographical area, to connect together and be employed simultaneously by multiple users on demand. Virtual sensors, which are at the core of this sensor cloud architecture, assist in creating a multiuser environment on top of resource-constrained physical wireless sensors and can help in supporting multiple applications.	cloud computing;geographic coordinate system;interaction;internet of things;multi-user;sensor	Sanjay Kumar Madria;Vimal Kumar;Rashmi Dalvi	2014	IEEE Software	10.1109/MS.2013.141	embedded system;wireless sensor network;sensor node;cloud computing;computer science;engineering;operating system;middleware;key distribution in wireless sensor networks;mobile wireless sensor network;wireless;computer network;visual sensor network	Mobile	-39.90293480373384	47.80727451036949	2261
8a5897bb8d76398cabed85b032920da8a5fd365e	complementary explanations	classical logic;paraconsistent logic;rational speaker;adaptive logic;modern logic	Scientific explanations arc subject to the occurrence of inconsistencies. To rule them out in many cases demands the construction of new theories. As the examples of complementary explanations show, that may take a while. Furthermore, even if possible in principle, it is not always reasonable to eliminate inconsistencies immediately, e.g., by bringing in a more sophisticated formal language. After all, under some circumstances a provisional, not fully coherent explanation may be better than none. In any case, we need a logically controlled approach to such inconsistencies. Modern logic provides the tools which are necessary to solve this task. We will mention two alternative approaches.	coherence (physics);formal language;theory;while	Max P. Urchs	1999	Synthese	10.1023/A:1005270806908		AI	-13.17752465057758	4.69208089904465	2273
e460a1e564551b1ab5ffa897ae636566211d0cde	a data-oriented m2m messaging mechanism for industrial iot applications	zmq iot m2m collaborative automation messaging mechanism	Machine-to-machine (M2M) communication is a key enabling technology for the future industrial Internet of Things applications. It plays an important role in the connectivity and integration of computerized machines, such as sensors, actuators, controllers, and robots. The requirements in flexibility, efficiency, and cross-platform compatibility of the intermodule communication between the connected machines raise challenges for the M2M messaging mechanism toward ubiquitous data access and events notification. This investigation determines the challenges facing the M2M communication of industrial systems and presents a data-oriented M2M messaging mechanism based on ZeroMQ for the ubiquitous data access in rich sensing pervasive industrial applications. To prove the feasibility of the proposed solution, the EU funded PickNPack production line with a reference industrial network architecture is presented, and the communication between a microwave sensor device and the quality assessment and sensing module controller of the PickNPack line is illustrated as a case study. The evaluation is carried out through qualitative analysis and experimental studies, and the results demonstrate the feasibility of the proposed messaging mechanism. Due to the flexibility in dealing with hierarchical system architecture and cross-platform heterogeneity of industrial applications, this messaging mechanism deserves extensive investigations and further evaluations.	computer;data access;data acquisition;distributed computing;high- and low-level;internet of things;interoperability;machine to machine;mathematical optimization;microwave;network architecture;pervasive informatics;reference architecture;requirement;robot;sensitivity and specificity;sensor;systems architecture;zeromq	Zhaozong Meng;Zhipeng Wu;Cahyo Muvianto;John Gray	2017	IEEE Internet of Things Journal	10.1109/JIOT.2016.2646375	computer science;knowledge management;multimedia	Robotics	-20.559632967922077	81.10739134894133	2284
e9b408095276204365eb6b750964b38fd7b8025b	algorithmes et architectures de récepteurs pour les systèmes multiporteuses par paquets d'ondelettes		Lu0027objectif de cette etude est de repondre a la question de faisabilite de lu0027utilisation des ondelettes pour les modulations multiporteuses. Par le biais du formalisme des ondelettes, il est possible de construire une base de fonctions orthogonales permettant de mettre au point un systeme de communication multiporteuses qui reduirai la sensibilite aux interferences introduites par le canal de propagation. Cette modulation multiporteuses est appelee DWMT (Discrete Wavelet MultiTone). Ses avantages et ses inconvenients sont developpes en comparaison a la modulation multiporteuses mise en application actuellement : la modulation OFDM (Orthogonal Frequency Division Multiplexing). La modulation DWMT est dans un premier temps evaluee dans un contexte de transmission en presence du0027un brouilleur bande etroite. Cette modulation permet du0027obtenir des formes du0027ondes modulantes aux lobes secondaires tres attenues et donc plus robustes a ce type du0027interferences. Les resultats obtenus ont montre que la modulation DWMT est peu sensible aux interferences bande etroite. La transmission a travers un canal multi-trajets reste le scenario le plus important a traiter dans une etude du0027un systeme de communication. Dans ces conditions, cu0027est essentiellement lu0027utilisation du0027un prefixe cyclique qui donne a la modulation OFDM des resultats interessants en terme de taux du0027erreur binaire. Lorsque le prefixe cyclique nu0027est pas une solution utilisee, la bonne localisation frequentielle des formes du0027ondes DWMT permet de reduire les interferences et les performances de la modulation DWMT sont legerement superieures a celles de la modulation OFDM.		Matthieu Gautier	2006			telecommunications;philosophy	Vision	-102.61180895997185	16.667107873458395	2286
76e2dd6020813610520a9b17fe581d05d00b9f4a	building the corporate im scorecard for improving quality in information management services	information management	Executive Summary : This presentation proposes a conceptual framework for assessing the business effectiveness and quality of Information Management (IM)Services in organizations by using a Corporate Information Management scorecard. The presentation applies the ‘Balanced Scorecard’ tool used by organizations typically for management strategy and performance measurement in the area of Information Systems Management , Information Systems Delivery and Implementation. Pushpak Sarkar	itil;information management;information system;systems management	Pushpak Sarkar	2002			information management;data management;balanced scorecard;knowledge management;business	DB	-71.83880481670067	9.159400065039984	2293
fc0e86ed6557461174ac2524c9aef19dd5ea2e7c	lösungsansätze zur anbindung eines kdd-systems an ein data warehouse	data warehouse			Dirk Bergemann;Elke Hickethier;Thomas Wittmann	1998			database;data warehouse;computer science	DB	-96.65528637931624	31.007592887922904	2296
a02b224ce687eb391e0a4b19887e217a01cf20b5	smt-based bounded model checking for cooperative software with a deterministic scheduler		The cooperative software, such as OSEK/VDX multi-tasks software and SystemC multi-threaded software, has been widely applied in the embedded system field. However, due to the flexible scheduling and complex cooperations between tasks or threads, the reliability of developed software is really difficult to be ensured by testing technique. To overcome this problem, model checking technique as a potential solution has attracted great attention in software industry. Recently, many model checking based methods have already been proposed and successfully applied in the verification of cooperative software, but most of them focus on the non-deterministic scheduler based cooperative software such as SystemC. The verification of deterministic scheduler based cooperative software is still at preliminary stage. In this paper, we propose an approach to verify this type of cooperative software. In our work, in order to make the proposed approach more general, the famous OSEK/VDX multi-tasks application is chosen as our target system. Furthermore, as to make the proposed approach more scalable, the advanced SMT-based bounded model checking is applied to carry out verification. We have investigated the effectiveness of our approach based on a series of experiments. The experiment results indicate that our approach can efficiently verify the cooperative software with a deterministic scheduler.	model checking;scheduling (computing)	Haitao Zhang;Yonggang Lu	2016		10.1007/978-3-319-57708-1_11	software system;software design description;software construction;software development;software verification;real-time computing;software metric;software sizing;computer science;distributed computing;software verification and validation	Logic	-42.15499622357846	31.570667824652023	2301
78c063cca0cfa6a21137f5a36ecf69c15ddf736f	high performance computational steering of physical simulations	language constructs;physical simulations;computational steering;information extraction;and forward;high performance computing;perforation;steering actions;computational intelligence;resource management;monitoring information;parallel programming;multiprogramming;data mining;runtime;intelligent control;parallel programming digital simulation computerised monitoring intelligent control multiprogramming;physics computing;high performance computational steering;resource intensive applications;computational modeling;low latency;monitoring;magellan;high performance computer;intelligent systems;high performance computing physics computing computational modeling steering systems monitoring runtime computational intelligence resource management intelligent systems data mining;acsl statements;multithreaded asynchronous steering servers;perturbation;computerised monitoring;high performance;multithreaded asynchronous steering servers high performance computational steering physical simulations resource intensive applications perturbation monitoring information fast decision making steering actions language constructs acsl statements magellan intelligent control;digital simulation;fast decision making;physical simulation;steering systems	Computational steering allows researchers to monitor and manage long running, resource intensive applications at runtime. Limited research has addressed high performance computational steering. High performance in computational steering is necessary for three reasons. First, a computational steering system must act intelligently at runtime in order to minimize its perturbation of the target application. Second, monitoring information extracted from the target must be analyzed and forwarded to the user in a timely fashion to allow fast decision-making. Finally, steering actions must be executed with low latency to prevent undesirable feedback. This paper describes the use of language constructs, coined ACSL, within a system for computational steering. The steering system interprets ACSL statements and optimizes the requests for steering and monitoring. Specifically, the steering system, called Magellan, utilizesACSL to intelligently control multithreaded, asynchronous steering servers that cooperatively steer applications. These results compare favorably to our previous Progress steering system.	ansi/iso c specification language;advanced continuous simulation language;computation;computational steering;computer simulation;excite;graphical user interface;information explosion;prototype;run time (program lifecycle phase);thread (computing)	Jeffrey S. Vetter;Karsten Schwan	1997		10.1109/IPPS.1997.580866	embedded system;real-time computing;simulation;computer science	HPC	-20.205158030070972	53.91351257730897	2302
47856d806ab05e9491c16d089f2a6bf9819e7dbe	encrypting the protocol for carrying authentication for network access (pana) attribute-value pairs		This document specifies a mechanism for delivering the Protocol for Carrying Authentication for Network Access (PANA) Attribute-Value Pairs (AVPs) in encrypted form. Information about the current status of this document, any errata, and how to provide feedback on it may be obtained at in effect on the date of publication of this document. Please review these documents carefully, as they describe your rights and restrictions with respect to this document. Code Components extracted from this document must include Simplified BSD License text as described in Section 4.e of the Trust Legal Provisions and are provided without warranty as described in the Simplified BSD License.	attribute–value pair;bsd;document;encryption;protocol for carrying authentication for network access	Alper E. Yegin;Robert Cragie	2012	RFC	10.17487/RFC6786	authentication protocol;internet privacy;computer security;computer network	Security	-26.365803049182038	88.15865885717771	2305
31317410bc7d144cb95acbf5f8a4e3e21625787d	cut-and-choose for garbled ram		Garbled RAM, introduced by Lu and Ostrovsky (Eurocrypt 2013), provides a novel method to garble RAM (Random Access Machine) programs directly. It can be seen as a RAM analogue of Yao’s garbled circuits such that, the size of the garbled program and the time it takes to create and evaluate it, is proportional only to the running time of the RAM program, avoiding the inefficient process of first converting it into a circuit. Secure RAM computation for two parties is a key application of garbled RAM. However, this construction is secure only against semi-honest adversaries. In this paper we provide a cut-and-choose technique for garbled RAM. This gives the first constant round two-party secure computation protocol for RAM programs secure against malicious adversaries that makes only black-box use of the underlying cryptographic primitives. Our protocol allows for garbling multiple RAM programs being executed on a persistent database. Security of our construction is argued in the random oracle model. ∗Research supported in part from a DARPA/ARL SAFEWARE Award, AFOSR Award FA9550-15-1-0274, NSF CRII Award 1464397 and a research grant from the Okawa Foundation. The views expressed are those of the author and do not reflect the official policy or position of the funding agencies.	black box;cryptographic primitive;cryptography;eurocrypt;ibm notes;lu decomposition;random access;random oracle;random-access machine;random-access memory;secure multi-party computation;semiconductor industry;time complexity;yao graph	Peihan Miao	2016	IACR Cryptology ePrint Archive		real-time computing;divide and choose;computer science	Crypto	-35.1571528217658	76.65417727648051	2308
ec8a9ce0c30726285d5d788ae574d4b0156afe1b	pattern language verification in model driven design	pattern language verification;model driven engineering	This paper addresses the problem of verifying the application of a Pattern Language in a design that is built based upon the patterns of the language in a model-driven approach. We propose a process named Pattern Language Verifier (PLV) which consists of four phases, working on a design model, to (1) verify the structure of every single pattern, (2) verify the relationships between the detected patterns, (3) verify the semantic aspects of the patterns, and (4) report the problems and help the designer fix them. Building a PLV for a given Pattern Language, requires the structural, syntactic, and semantic rules of the language to be precisely defined using the presented formalism. For the case study, a group of enterprise architectural patterns is selected as the Pattern Language. The structural, syntactic, and semantic rules of the language are defined using the proposed formalism. A PLV is designed and implemented as an integration into an open source modeling tool. The tool is then utilized in designing a sample web application. The usefulness of the tool is represented by walkthrough scenarios that show finding the mistakes in the model and helping the designer repair the detected problems.	argouml;code generation (compiler);compiler;computer-aided software engineering;executable uml;msde;model-driven architecture;model-driven engineering;pl/i;pattern language;semantics (computer science);unified modeling language;verification and validation;web application	Bahman Zamani;Gregory Butler	2013	Inf. Sci.	10.1016/j.ins.2013.02.038	natural language processing;model-driven architecture;interpreter pattern;computer science;artificial intelligence;modeling language;programming language;algorithm	PL	-47.12088807021153	27.43018106459622	2313
1402231cd6279b94f18fe15eb452a32e59a05525	squads: software development and maintenance on the grid by means of mobile virtual organizations using adaptive information services	service industry;virtual organization;software development;information service	This paper addresses the area of software development and maintenance for GRID- based applications. We argue that the structure  and geographical spread of these systems poses new challenges to the ICT services industry. We believe that human intelligence  and related skills are a central element in facing these challenges and see the emerging paradigm of virtual organizations  as promising direction to realize the availability of these skills.    In this paper, we describe two streams in our research: One covers the applications of organizational concepts of VO’s in  this area (squads) and lists the issues of study. The other covers the ICT support for squads themselves. We describe the  requirements for and architecture of adaptive squad information services (AS1S).      	software development	Wico Mulder;Geleyn R. Meijer	2004		10.1007/1-4020-8139-1_40	tertiary sector of the economy;economics;systems engineering;knowledge management;software development;software engineering;software as a service;management	SE	-67.63145677386494	15.460523030569602	2319
6c92cd2e7fb436a3be0476831ffdcc7f7fd82a11	a services lifecycle to support the business processes lifecycle: from modeling to execution and beyond	services lifecycle;bpms;soaml;bpmn 2 0;qos;business process lifecycle;services;business process	The realization of business process with services has gain more importance in the last decade. Several organizations are focusing nowadays in the modeling and execution of their business processes, seizing the advantages provided by both the BPMN 2.0 notation which allows these models to be executed, and the emergence of BPMS platforms which are able to execute business processes invoking internal and external services from partners and/or the cloud when needed. Although many lifecycle proposals exist to guide the definition and management of both business process and services, there is no clear relationship defined between them, i.e. how services should be defined and managed to support business processes. This is a key element that should be taken into account when implementing services for this kind of systems, in order to systematize the work and obtain better results. In this paper we present a service lifecycle to support business processes, which helps developing services for business process systems.	business process model and notation;emergence;ibm basic programming support;process architecture;quality of service;soaml	Andrea Delgado	2016	2016 IEEE International Conference on Services Computing (SCC)	10.1109/SCC.2016.117	business rule management system;business process execution language;business process reengineering;systems engineering;knowledge management;artifact-centric business process model;business process management;process modeling;business process model and notation;process management;industrialization of services business model;business;services computing;business process;business process discovery;business rule;new business development;business process modeling;business activity monitoring;system lifecycle	DB	-56.78587522612587	17.813239230549648	2325
a0b090cf03cde23a0d8159e6db42f922c52db4f7	variables as resource in separation logic	verification;variables;research outputs;separation logic;research publications;hoare logic;separation;concurrent programs;critical section;proof	Separation logic [20,21,14] began life as an extended formalisation of Burstall’s treatment of list-mutating programs [8]. It rapidly became clear that there was more that it could say: O’Hearn’s discovery [13] of ownership transfer of buffers between threads and Boyland’s suggestion [5] of permissions to deal with variable and heap sharing pointed the way to a treatment of safe resource management in concurrent programs. That treatment has so far been incomplete because it deals only with heap cells and not with with (stack) variables as resource. Adding ‘variable contexts’ — in the simplest case, lists of owned variables — to assertions in Hoare logic allows a resource treatment of variables. It seems that a formal treatment of aliasing is possible too. It gives a complete formal treatment of critical sections (for the first time, so far as I am aware).	algorithm;aliasing;assertion (software development);banished;blimey cow;cristiano castelfranchi;critical section;first-person (video games);formal proof;hoare logic;richard bornat;semantics (computer science);separation logic;weitao yang	Richard Bornat;Cristiano Calcagno;Hongseok Yang	2006	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2005.11.059	dynamic logic;variables;verification;separation logic;computer science;theoretical computer science;bunched logic;computational logic;proof;mathematics;critical section;hoare logic;programming language;algorithm;philosophy of logic	PL	-16.213769593838016	20.3459977182018	2331
1c049bd69e99e45fa5b102f7a991e1cabb9fbf07	the bulletin of symbolic logic	symbolic logic			Richard A. Shore	1995	Bulletin of Symbolic Logic		computer science;artificial intelligence;theoretical computer science;algorithm	Logic	-20.696763574627475	17.514782754438503	2332
a97c0cfe9ac5fe2d0eb7f4bd0833f9827463c681	dynamic validation of programs using assertion checking facilities	printing;debugging;testing;process design;large scale;guidelines;programming profession;error correction;source code;early detection;program processors;computer errors;large scale systems;testing debugging programming profession computer errors large scale systems guidelines process design program processors error correction printing;program correctness	Automated dynamic validation through assertion checking has been proposed in recent years to ensure program correctness [3,7]. This method has shown to be useful for validating large scale programs. When used in parallel with debugging aids, it can be useful for early detecting and properly locating errors. In this paper, we provide a theoretical basis for assertion checking with regard to validation of program correctness. This gives some guidelines for inserting assertions within the program. An assertion language is provided for stating assertions. Salient features of this language are provided. A language preprocessor is designed to process source program inserted with assertions to generate source code acceptable to the existing compiler. Finally, future extentions to the assertion language and limitations of assertion checking to ensure program correctness are discussed.	assertion (software development)	Wen-Tsuen Chen;Jone-Ping Ho;Chia-Hsien Wen	1978		10.1109/CMPSAC.1978.810479	program analysis;process design;computer architecture;error detection and correction;computer science;software engineering;database;software testing;programming language;debugging;source code	SE	-51.87105757132367	36.72422348191816	2338
e51ef4432cd3da182afbfea492a9d1091b75e2f2	remps: a reconfigurable multiprocessor for scientific supercomputing			multiprocessing;supercomputer	Kai Hwang;Zhiwei Xu	1985			distributed computing;parallel computing;multiprocessing;computer science;supercomputer	HPC	-9.628864196037288	42.619794156140706	2345
76eca6c16f4d5d9105f6f9a8a2bde0cd02706c6e	anatomy of a software engineering project	software engineering;design and implementation;software development	This paper discusses a complete software development project carried out in a one quarter undergraduate software engineering course. The project was the design and implementation of a complete system by 25 students. They worked in smaller groups on four functionally separate subsystems that were successfully integrated into a complete system. This was accomplished by using five advanced students to manage the groups, real users to criticize each step of the process, and UNIX tools to implement the subsystems. This paper describes the project, presents the methodologies used, and discusses both the positive and negative aspects of this course. It concludes by presenting a set of recommendations based on our experience with this project.	software development;software engineering;unix	Catherine L. Bullard;Inez Caldwell;James Harrell;Cis Hinkle;A. Jefferson Offutt	1988		10.1145/52964.52996	personal software process;verification and validation;software engineering process group;software project management;computer science;package development process;software design;social software engineering;component-based software engineering;software development;software design description;operating system;software engineering;software construction;systems development life cycle;software walkthrough;computer-aided software engineering;software development process;software requirements;project planning;software system	SE	-65.81895923904479	26.067482935065478	2347
aef98eeed7a6961960d6f4c661c1276939219ddb	utilizing blockchain technology in industrial manufacturing with the help of network simulation		The fourth Industrial Revolution is finding its way into modern manufacturing sites. Assets with a certain degree of implemented communication technology will be enabled to interconnect and cooperate with practically every other entity via the Industrial Internet. An extreme amount of data and information will be exchanged all the time. Machines will be empowered to make crucial decisions autonomously influencing whole production processes. Erroneous, illegitimate or tampered data will lead to incorrect decisions and will be posing a huge threat to future strong cross-linked added value networks. An approach to tackle this issue comes from a technology called distributed ledger technology with its most known variant the “blockchain”. Certain properties of blockchain technology are showing promising enhancements for industrial networks primarily in order to guarantee digital trust. In this paper, issues like the ability to scale and the adaption onto the requirements of industrial networks are investigated. It turns out that simulation approaches must be taken into consideration, and a first step for an implementation is presented.	automation;autonomous robot;bitcoin;distributed database;industrial robot;industry 4.0;requirement;simulation;value network	Thomas Kobzan;Alexander Biendarra;Sebastian Schriegel;Thomas Herbst;Thomas Muller;Jürgen Jasperneite	2018	2018 IEEE 16th International Conference on Industrial Informatics (INDIN)	10.1109/INDIN.2018.8472011	automation;systems engineering;engineering;network simulation;interconnection;distributed ledger;blockchain;manufacturing;added value;information and communications technology	Robotics	-56.23190249819344	51.158722461540826	2354
f3aeabf3d9299dcbd1c15a286029be6590b014e4	reliable orchestration of resources using ws-agreement	distributed system;haute performance;systeme reparti;juser;resource allocation;localization;distributed computing;recommandation;localizacion;satisfiability;websearch;resource use;grid;sous traitance;sistema repartido;localisation;rejilla;alto rendimiento;grille;recomendacion;calculo repartido;recommendation;coordinacion;service level agreement;asignacion recurso;subcontratacion;allocation ressource;subcontracting;publications database;global grid forum;high performance;calcul reparti;coordination	Co-ordinated usage of resources in a Grid environment is a ch allenging task impeded by the nature of resource usage and provision: Resources reside in different geograp hic locations, are managed by different organisations, and the provision of reliable access to these resource usually h s to be negotiated and agreed upon in advance. These prerequisites have to be taken into account providing solution s f r the orchestration of Grid resources. In this document we describe the use of WS-Agreement for Service Level Agreem ents paving the way for using multiple distributed resources to satisfy a single service request. WS-Agreemen t is about to be released as a draft recommendation of the Global Grid Forum and has already been implemented in a numbe r of projects, two of which we will presented in this paper.	hic;ws-federation	Heiko Ludwig;Toshiyuki Nakata;Oliver Wäldrich;Philipp Wieder;Wolfgang Ziegler	2006		10.1007/11847366_78	simulation;internationalization and localization;resource allocation;computer science;operating system;database;distributed computing;grid;world wide web;computer security;satisfiability	HPC	-28.62490301715137	43.596265864333944	2358
7dc418e6f8f3a2d590b04aff1d9654ddc2b97cc8	analysis of end-to-end recovery algorithms with preemptive priority in gmpls networks	blocking probability;algorithme rapide;level of service;optical network;switching networks;evaluation performance;switching;multiplexage longueur onde;metodo analitico;reseau communication;reseau optique;generalized multi protocol label switching;performance evaluation;longitud onda;availability;disponibilidad;evaluacion prestacion;wavelength;probabilistic approach;division;interconnection network;reseau commutation;telecomunicacion optica;telecommunication optique;optical arrays;enfoque probabilista;approche probabiliste;analytical method;fast algorithm;conmutacion;defaillance;performance analysis;optical telecommunication;methode analytique;failures;longueur onde;red de comunicacion;disponibilite;fallo;communication network;algoritmo rapido;commutation;red interconexion;multiplaje longitud onda;analytical model;disjoint paths;wavelength division multiplexing;wavelength division multiplex;reseau interconnexion	To provide high resilience against failures, optical networks must have an ability to maintain an acceptable level of service during network failures. This paper proposes a new enhanced end-to-end recovery algorithm that guarantees fast and dynamic utilization of network resources in Wavelength Division Multiplexing (WDM) based Generalized Multi-protocol Label Switching (GMPLS) networks. The proposed algorithm guarantees the maximum availability with less burden in finding as many disjointed paths. This paper also has developed an analytical model and performed analysis for the proposed algorithm in terms of two performance factors: mean system time and blocking probability.	algorithm;generalized multi-protocol label switching;preemption (computing);semantic network	Hyuncheol Kim;Jun Kyun Choi;Seong-Jin Ahn;Jin-Wook Chung	2003		10.1007/978-3-540-45235-5_12	availability;telecommunications;computer science;division;wavelength;level of service;telecommunications network;wavelength-division multiplexing	Theory	-5.005080002072713	77.49195520970103	2362
0e9e5c9dd6482c1ba7a502fe0180ba37ddd8f2c9	security certificate revocation list distribution for vanet	network architecture and design;security and protection;computer communication networks;certificate authority;vanet;simulation;revocation;wireless communication;large scale simulation;crl;epidemic model;security;certificate revocation list	In a VANET, a certificate authority issues keys and  certificates to vehicles. Each vehicle distributes these certificates to other VANET participants and subsequently signs messages against these certificates. If the certificate authority needs to revoke a certificate (e.g. due to a breach of trust), it universally distributes a certificate revocation list. We propose a method for car-to-car epidemic distribution of certificate revocation lists which is quick and efficient. Large-scale simulations based on realistic mobility traces show that this epidemic model significantly outperforms methods that only employ road side unit distribution points.	certificate authority;simulation;tracing (software)	Kenneth P. Laberteaux;Jason J. Haas;Yih-Chun Hu	2008		10.1145/1410043.1410063	chain of trust;certificate policy;implicit certificate;self-signed certificate;computer science;information security;certificate signing request;x.509;revocation list;certificate server;authorization certificate;public key certificate;internet privacy;root certificate;online certificate status protocol;computer security;certificate authority;certificate management protocol;certification path validation algorithm;computer network	Security	-48.86678310302296	74.54503249698715	2365
5468ed497e606d5623e288ad7fde81ef16400921	precise guidance to dynamic test generation	software testing;data flow analysis	Dynamic symbolic execution has been shown an effective technique for automated test input generation. However, its scalability is limited due to the combinatorial explosion of the path space. We propose to take advantage of data flow analysis to better perform dynamic symbolic execution in the context of generating test inputs for maximum structural coverage. In particular, we utilize the chaining mechanism to (1) extract precise guidance to direct dynamic symbolic execution towards exploring uncovered code elements and (2) meanwhile significantly optimize the path exploration process. Preliminary experiments conducted to evaluate the performance of the proposed approach have shown very encouraging results.	code coverage;code segment;data-flow analysis;dataflow;experiment;scalability;software testing;symbolic execution;test automation;test case;x image extension	TheAnh Do;Alvis Cheuk M. Fong;Russel Pears	2012			real-time computing;simulation;computer science;theoretical computer science;concolic testing	SE	-58.54534626528051	36.746714162715584	2370
075f8503878bb9ceaa243e7872f9840b1388922c	supporting evolution and maintenance of android apps	mining software repositories;android;api changes;empirical studies	In recent years, the market of mobile software applications (apps) has maintained an impressive upward trajectory. As of today, the market for such devices features over 850K+ apps for Android, and 19 versions of the Android API have been released in 4 years. There is evidence that Android apps are highly dependent on the underlying APIs, and APIs instability (change proneness) and fault-proneness are a threat to the success of those apps. Therefore, the goal of this research is to create an approach that helps developers of Android apps to be better prepared for Android platform updates as well as the updates from third-party libraries that can potentially (and inadvertently) impact their apps with breaking changes and bugs. Thus, we hypothesize that the proposed approach will help developers not only deal with platform and library updates opportunely, but also keep (and increase) the user base by avoiding many of these potential API ”update” bugs	android;application programming interface;backward compatibility;instability;library (computing);software bug	Mario Linares Vásquez	2014		10.1145/2591062.2591092	computer science;engineering;internet privacy;world wide web;computer security;android	SE	-61.43126003474432	43.05721582659939	2377
0af80f599ad257e701b0810d8857dc14ed4244ed	banked multiported register files for high-frequency superscalar microprocessors	microprocessors;processor bandwidth;microarchitecture;performance evaluation;multiport networks;multiple interleaved bank;interleaved storage;logic;performance evaluation pipeline processing multiport networks microprocessor chips interleaved storage circuit complexity;circuit complexity;high performance superscalar microprocessor;control structure;limit cycle;limit cycles;registers;pipelines;bandwidth;microprocessors registers pipelines logic delay bandwidth microarchitecture laboratories computer science limit cycles;superscalar processor;register file layout;banked multiported register file;register file;computer science;design complexity;high performance;high frequency;pipeline control logic;pipeline processing;design complexity banked multiported register file high performance superscalar microprocessor multiple interleaved bank pipeline control logic register file layout processor bandwidth;microprocessor chips	Multiported register files are a critical component of high-performance superscalar microprocessors. Conventional multiported structures can consume significant power and die area. We examine the designs of banked multiported register files that employ multiple interleaved banks of fewer ported register cells to reduce power and area. Banked register files designs have been shown to provide sufficient bandwidth for a superscalar machine, but previous designs had complex control structures that would likely limit cycle time and add to design complexity. We develop a banked register file with much simpler and faster control logic while only slightly increasing the number of ports per bank. We present area, delay, and energy numbers extracted from layouts of the banked register file. For a four-issue superscalar processor, we show that we can reduce area by a factor of three, access time by 20%, and energy by 40%, while decreasing IPC by less than 5%.	access time;control flow;limit cycle;microprocessor;register file;superscalar processor	Jessica H. Tseng;Krste Asanovic	2003		10.1145/859618.859627	circuit complexity;computer architecture;parallel computing;computer hardware;microarchitecture;computer science;operating system;high frequency;limit cycle;logic;register file;bandwidth	Arch	-6.946610189421294	52.821575676398545	2380
9a696baae538cefe4ee74cc63baafbf1221000e0	dart: a decision support system for cellular networks usage analysis	cellular network		decision support system;usage analysis	Ming Tan;Hao Xu;Johnson Lee	1999			data mining;decision support system;dart;computer science;cellular network	Mobile	-16.508508085328423	88.21536505308559	2384
de03fb1ed7081fd2ac04241c1046cb25ef3a45f4	forschungs- und entwicklungsprojekte	information;knowledge;semantics;knowledge processing	Neben Information ist Wissen ein Begriff, der in der Informatik eine große Rolle spielt. Beim Menschen ist Wissen ein Teil seiner Gedächtnisleistung. Erwerben und Nutzen von Wissen ist eng verwandt mit dem Begriff Denken. Das Ziel der Informatik ist es, Leistungen des Menschen im Bereich von Gedächtnis und Denken mit maschinellen Mitteln zu unterstützen. Deshalb ist eine klare begriffliche Abgrenzung zwischen dem, was Menschen tun, und dem, was Maschinen leisten oder zu leisten vermögen, für Informatikerinnen und Informatiker sehr wichtig. Maschinen sind Menschen heute bereits überlegen, wenn es um das Speichern und Wiederfinden von Information geht, oder bei sehr speziellen Aufgaben wie dem Schachspielen. Anders ist es bei Denkprozessen wie Generalisieren, Abstrahieren und dem analogen Schließen, sowie bei der Lösung alltäglicher und wenig strukturierter Probleme. Besides information, knowledge is a key term used in computing. For humans, knowledge is part of what we call intelligence. Its acquisition and use closely relates to thinking. It is the aim of computer science to support or emulate human achievements in the realm of memory and thinking. It is therefore important that students of computing understand the difference between what humans do and what machines can achieve in this respect. Machines are superior to humans when storing and retrieving information and in the case of very specialized tasks like playing chess. It is different for mental processes like generalizing, abstracting and analog reasoning, and for the solution of broad and ill-structured problems.	citeseerx;computer science;eine and zwei;internet explorer;maschinen krieger zbv 3000;tun (product standard);triple des;unified model	Albert Endres	2004	Informatik Forschung und Entwicklung	10.1007/s00450-004-0148-6	library science;database;computer science	AI	-108.28005266204	34.16286985154626	2385
9f52a19fe33cc77185676c9ba7b489ecd27acb79	synchronous games in the situation calculus	reasoning about action;verification of agent based systems;general game playing;logics for multi agent systems	We develop a situation calculus-based account of multi-player synchronous games. These are represented as action theories called situation calculus synchronous game structures (SCSGSs) that involve a single action tick whose effects depend on the combination of moves chosen by the players. Properties of games, e.g., winning conditions, playability, weak and strong winnability, etc. can be expressed in a first-order variant of alternating-time mu-calculus. Computationally effective verification can be performed. SCSGSs can be viewed as a variant of the Game Description Language (GDL) where states are represented by first-order theories.	first-order predicate;game description language;modal μ-calculus;situation calculus;theory	Giuseppe De Giacomo;Yves Lespérance;Adrian R. Pearce	2015			combinatorial game theory;fluent calculus;simulation;artificial intelligence;game mechanics;repeated game;sequential game;algorithm	AI	-17.51751985393635	4.675946212506724	2387
59bd090b4c5578485a3a003cf2ef4c64ac84597c	monitoring an algorithm's execution		"""Many software systems for Discrete Mathematics incorporate routines that compute NP-hard functions. In spite of their worst-case exponential running time, there are often a wide range of inputs for which these routines run in interactive time. It is generally diicult for a user to distinguish, a priori, the \easy"""" inputs from those that are \hard"""". A percent-done progress indicator is a software tool that allows a user to monitor the percentage of a computation that has been completed. Such indicators have been implemented for linear algorithms (e.g., le transfer programs). This paper introduces a paradigm which we call dynamic bound evaluation for monitoring the progress of a wide class of recursive algorithms that need not be linear. Further, empirical results are presented that illustrate the eeectiveness of the technique as it has been implemented in the SetPlayer system for symbolic set manipulation."""	best, worst and average case;computation;discrete mathematics;exptime;linear programming;programming paradigm;programming tool;progress indicator;recursion;software system;super-recursive algorithm;time complexity	Dave A. Berque;Mark K. Goldberg	1992			real-time computing;computer science	PL	-12.877490406466483	32.45750944184932	2399
45662ab5bd660454fb061efa1d9f5ef60307411a	platform engineering in enterprise application development	application development;software;web application component based development software product lin software reuse enterprise application platform;software product lin;software computational modeling computer architecture programming software engineering business java;web based enterprise application development platform engineering component based software development model driven development software product line engineering software quality;component based development;object oriented programming;software engineering;platform engineering;model driven development;software product line engineering;computer architecture;computational modeling;internet;web application;lessons learned;business data processing;business;software quality business data processing internet object oriented programming;component based software development;enterprise application platform;software product line;programming;software reuse;software quality;web based enterprise application development;java	Although component-based software development, model-driven development, software product line engineering, and similar things considerably improve software productivity and software quality, modern web based enterprise application development is still facing great challenges. This paper addresses how to deal with the complexity of enterprise application development via platform engineering (PE). We also propose the lessons learned in PE, challenges we are still facing, and some open issues we need to discuss.	component-based software engineering;enterprise software;model-driven architecture;model-driven engineering;software development process;software product line;software quality	Jingang Zhou;Yong Ji;Dazhe Zhao;Jiren Liu	2010	2010 International Conference on E-Business and E-Government	10.1109/ICEE.2010.36	functional software architecture;personal software process;enterprise system;enterprise systems engineering;enterprise software;software engineering process group;crowdsourcing software development;computer science;systems engineering;package development process;social software engineering;component-based software engineering;software development;software engineering;software construction;software as a service;enterprise integration;software deployment;software development process;enterprise information system;computer engineering;enterprise life cycle	SE	-61.52456669051481	23.480407900323268	2401
6da3a9881b6d2b752be35a1c08ebf42414b64dbb	a software defined networking architecture for the internet-of-things	vehicles semantics delays cameras calculus streaming media monitoring;semantics;monitoring;streaming media;calculus;vehicles;cameras;delays	The growing interest in the Internet of Things (IoT) has resulted in a number of wide-area deployments of IoT subnetworks, where multiple heterogeneous wireless communication solutions coexist: from multiple access technologies such as cellular, WiFi, ZigBee, and Bluetooth, to multi-hop ad-hoc and MANET routing protocols, they all must be effectively integrated to create a seamless communication platform. Managing these open, geographically distributed, and heterogeneous networking infrastructures, especially in dynamic environments, is a key technical challenge. In order to take full advantage of the many opportunities they provide, techniques to concurrently provision the different classes of IoT traffic across a common set of sensors and networking resources must be designed. In this paper, we will design a software-defined approach for the IoT environment to dynamically achieve differentiated quality levels to different IoT tasks in very heterogeneous wireless networking scenarios. For this, we extend the Multinetwork INformation Architecture (MINA), a reflective (self-observing and adapting via an embodied Observe-Analyze-Adapt loop) middleware with a layered IoT SDN controller. The developed IoT SDN controller originally i) incorporates and supports commands to differentiate flow scheduling over task-level, multi-hop, and heterogeneous ad-hoc paths and ii) exploits Network Calculus and Genetic Algorithms to optimize the usage of currently available IoT network opportunities. We have applied the extended MINA SDN prototype in the challenging IoT scenario of wide-scale integration of electric vehicles, electric charging sites, smart grid infrastructures, and a wide set of pilot users, as targeted by the Artemis Internet of Energy and Arrowhead projects. Preliminary simulation performance results indicate that our approach and the extended MINA system can support efficient exploitation of the IoT multinetwork capabilities.	bluetooth;coexist (image);delay-tolerant networking;end-to-end principle;flow network;genetic algorithm;high-level programming language;hoc (programming language);information architecture;internet of things;middleware;network calculus;plug-in (computing);prototype;provisioning;routing;scheduling (computing);seamless3d;sensor;simulation;software release life cycle;software-defined networking;testbed	Zhijing Qin;Grit Denker;Carlo Giannelli;Paolo Bellavista;Nalini Venkatasubramanian	2014	2014 IEEE Network Operations and Management Symposium (NOMS)	10.1109/NOMS.2014.6838365	real-time computing;telecommunications;operating system;distributed computing;semantics;computer security;computer network	Embedded	-19.54614313707116	80.88010078642591	2402
2dbd09ec7be71ce3d7ad1f81df849a415205cc90	enhancing cache robustness for content-centric networking	cache storage;file servers;routing protocols;internet routing;electronic mail;computer network security;pollution ip networks robustness servers internet routing protocols electronic mail;telecommunication network routing cache storage computer network security file servers;cache hit ratios cache robustness content centric networking ccn router cache pollution attacks cache server cacheshield cache replacement policy trace driven simulations;servers;internet;telecommunication network routing;cache replacement;cache performance;robustness;ip networks;trace driven simulation;pollution	With the advent of content-centric networking (CCN) where contents can be cached on each CCN router, cache robustness will soon emerge as a serious concern for CCN deployment. Previous studies on cache pollution attacks only focus on a single cache server. The question of how caching will behave over a general caching network such as CCN under cache pollution attacks has never been answered. In this paper, we propose a novel scheme called CacheShield for enhancing cache robustness. CacheShield is simple, easy-to-deploy, and applicable to any popular cache replacement policy. CacheShield can effectively improve cache performance under normal circumstances, and more importantly, shield CCN routers from cache pollution attacks. Extensive simulations including trace-driven simulations demonstrate that CacheShield is effective for both CCN and today's cache servers. We also study the impact of cache pollution attacks on CCN and reveal several new observations on how different attack scenarios can affect cache hit ratios unexpectedly.	algorithm;cache (computing);cache pollution;cyclomatic complexity;hit (internet);network topology;router (computing);server (computing);simulation;software deployment;web cache	Mengjun Xie;Indra Widjaja;Haining Wang	2012	2012 Proceedings IEEE INFOCOM	10.1109/INFCOM.2012.6195632	bus sniffing;file server;parallel computing;cache coloring;the internet;cache stampede;pollution;cache;computer science;network security;cache invalidation;operating system;distributed computing;smart cache;routing protocol;cache algorithms;cache pollution;server;robustness;computer network	Metrics	-12.601036094819284	78.27507480342004	2405
20c73cfd0e95be35e6b6467db524482968ed8fea	facts, arguments, annotations and reasoning	annotated logic;automated reasoning;classical logic;knowledge representation	Reasoning aboutfacts and reasoning aboutarguments regarding facts are distinct activities, and automated reasoning systems should be able to treat them accordingly. In this work, we discuss one precise sense in which this distinction can be envisaged and suggest the use of Annotated Logics to characterise it.	description logic;reasoning system	Daniela Vasconcelos Carbogim;Flávio S. Corrêa da Silva	2000	New Generation Computing	10.1007/BF03037532	knowledge representation and reasoning;opportunistic reasoning;case-based reasoning;classical logic;analytic reasoning;abductive reasoning;qualitative reasoning;verbal reasoning;computer science;artificial intelligence;theoretical computer science;non-monotonic logic;model-based reasoning;psychology of reasoning;reasoning system;automated reasoning;deductive reasoning;algorithm	AI	-15.915938018070692	6.156861740418908	2413
d0c8f8417ad9269ba6334a17fac561ec52f6c07f	preserving consistency across abstraction mappings	first order	An abstraction mapping over clausal form theories in first-order predicate calculus is presented that involves the renaming of predicate symbols. This renaming is not 1-1, in the sense that several predicate symbols Ri,.. . , Rn from the original theory are all replaced by a single symbol R in the abstract theory. In order to preserve consistency, however, the clauses that distinguish the Rj's must be discarded in the abstract theory. This leads to a simple semantics; the union of the extensions of each of the Ri's in any model of the original theory forms the extension of R in a model of the abstract theory.	conjunctive normal form;first-order logic;first-order predicate	Josh D. Tenenberg	1987			discrete mathematics;computer science;artificial intelligence;first-order logic;mathematics;predicate;algorithm	AI	-12.019591579661151	16.010077100034017	2416
6a518fdd2ece4ea12ff68ce5f76cacb87a591cae	smashing: folding space to tile through time	partial differential equation;data locality;nearest neighbor	Partial differential equation solvers spend most of their computation time performing nearest neighbor (stencil) computations on grids that model spatial domains. Tiling is an effective performance optimization for improving the data locality and enabling course-grain parallelization for such computations. However, when the domains are periodic, tiling through time is not directly applicable due to wrap-around dependencies. It is possible to tile within the spatial domain, but tiling across time (i.e. time skewing) is not legal since no constant skewing can render all loops fully permutable. We introduce a technique called smashing that maps a periodic domain to computer memory without creating any wrap-around dependencies. For a periodic cylinder domain where time skewing improves performance, the performance of smashing is comparable to another method, circular skewing, which also handles the periodicity of a cylinder. Unlike circular skewing, smashing can remove wrap-around dependencies for an icosahedron model of earth’s atmosphere.	computation;computer memory;cylinder seal;dataspaces;discretization;enabling transformation;high- and low-level;iteration;iterative method;locality of reference;mathematical optimization;memory map;parallel computing;performance tuning;periodic boundary conditions;quasiperiodicity;stencil (numerical analysis);tiling window manager;time complexity	Nissa Osheim;Michelle Mills Strout;Dave Rostron;Sanjay V. Rajopadhye	2008		10.1007/978-3-540-89740-8_6	parallel computing;computer science;theoretical computer science;k-nearest neighbors algorithm;partial differential equation	HPC	-5.798440467748717	39.146815004896844	2421
76953f3bd6b0206e16143b7133d470695dc525e8	zur konzeption benutzerspezifischer arbeits- und retrievalumgebungen in modernen informationssystemen				Ralf Cordes	1989				NLP	-98.21119164223175	24.345695351997342	2422
c970dafdb5c744002e07b92100cdcbbfbf5881a4	product life-cycle metadata modeling and its application with rdf	product identification technology;modelizacion;red sin hilo;query language;gestion memoire;ciclo desarrollo;wireless mobile telecommunication;informatique mobile;xml xsl rdf;metadata;life cycle;data design and management;traitement flux donnee;reseau sans fil;sistema gestion dato tecnico;ingenierie connaissances;end of life;helium;product life cycle management;query languages cad cam knowledge engineering meta data product design product life cycle management;storage management;product life cycle;wireless network;rdf model;knowledge management;flux donnee;flujo datos;data engineering;resource description framework;by product;resource description framework radiofrequency identification knowledge management internet database languages data models technology management engineering management data engineering;knowledge and data engineering tools and techniques;lenguaje interrogacion;technology management;data model;data modeling;knowledge engineering product life cycle metadata modeling resource description framework beginning of life phase middle of life phase end of life phase data flow internet wireless mobile telecommunication product identification technology data modeling rdf query language rdf model cad cam;query languages;modelisation;gestion memoria;internet;engineering information systems;sous produit;rdf query language;mobile telecommunication;engineering management;cad cam;subproducto;end of life phase;data flow processing;emerging technologies;metadonnee;specification rdf;cycle developpement;product data management pdm;meta data;modele donnee;langage interrogation;process model;metadatos;product design;data flow;systeme gestion donnee technique sgdt;product life cycle metadata modeling;mobile computing;beginning of life phase;xml xsl rdf data models metadata;modeling;database languages;radiofrequency identification;systeme information ingenierie;middle of life phase;rdf;data models;knowledge engineering	The whole product life cycle consists of three phases: beginning of life (BOL), middle of life (MOL), and end of life (EOL). Although large amounts of product life-cycle data are generated over the whole product life cycle, data flows are rather vague after BOL. Over the last decade, however, emerging Internet, wireless mobile telecommunications, and product identification technologies have created the potential of making the whole product life cycle visible. As a result, the scope of data to be managed has expanded over the whole product life cycle. Hence, it becomes important to describe product life-cycle metadata in a systematic manner. Although much attention has been paid to data modeling over several objects such as products and processes, modeling methodology for product life-cycle metadata is not well developed. To cope with this limitation, we develop a modeling method for product life-cycle metadata by using the resource description framework (RDF). We define an RDF data model and its schema for describing and managing product life-cycle metadata. In addition, we describe how the proposed RDF model can be usefully applied to track, trace, and infer product life-cycle data with an RDF query language.	data model;data modeling;end-of-life (product);internet;metadata modeling;rdf query language;resource description framework;vagueness	Hong-Bae Jun;Dimitris Kiritsis;Paul C. Xirouchakis	2007	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2007.190661	computer science;technology management;knowledge engineering;data mining;database;product design;mobile computing;world wide web;query language;metadata repository	DB	-35.328831253166065	12.162821450948664	2425
4915a7066237966e4240ac0dd88d3e96c492a70e	on the expressiveness of multiparty sessions	concurrency message passing session asynchrony expressiveness;004	This paper explores expressiveness of asynchronous multiparty sessions. We model the behaviours of endpoint implementations in several ways: (i) by the existence of different buffers and queues used to store messages exchanged asynchronously, (ii) by the ability for an endpoint to lightly reconfigure his behaviour at runtime (flexibility), (iii) by the presence of explicit parallelism or interruptions (exceptional actions) in endpoint behaviour. For a given protocol we define several denotations, based on traces of events, corresponding to the different implementations and compare them. 1998 ACM Subject Classification F.3.2 Semantics of Programming Languages	communication endpoint;explicit parallelism;parallel computing;run time (program lifecycle phase);semantics (computer science);tracing (software)	Romain Demangeon;Nobuko Yoshida	2015		10.4230/LIPIcs.FSTTCS.2015.560	real-time computing;computer science;theoretical computer science;distributed computing	PL	-28.00340450345558	33.531192379964686	2428
3a09e5f90699f8dc7592a201f50b02bde66067a9	die anwendung modellbasierten schließens bei der diagnose schiffstechnischer anlagen		Dieser Beitrag beschreibt unsere bisherigen Erfahrungen bei der Entwicklung eines Expertensystems zur Diagnose schiffstechnischer Anlagen im Rahmen des Projekts SHOPSY.1 Maritime Einrichtungen sind komplexe, sicherheitsrelevante Anlagen, die im Falle einer Storung ein hohes Risikopotential in sich bergen, von dem erhebliche Gefahren nicht nur fur Menschen und Material, sondern auch fur die gesamte Umwelt ausgehen. Aus diesem Grund ist es erforderlich, intelligente, entscheidungsunterstutzende Systeme in bestehende Leitsysteme zu integrieren, die in der Lage sind, Gefahrensituationen korrekt einzuschatzen und die Vollstandigkeit bei der Fehlersuche zu garantieren.		Sabine Kockskämper;Bernd Neumann;Alexander Josub;Herbert Müller	1993		10.1007/978-3-642-78073-8_3	performance art;history	NLP	-104.98117231416433	30.616314302931166	2431
7a1ee8334975d114c0b5519d0764d4e6faf87553	implicit dynamic frames: combining dynamic frames and separation logic	verification;separation logic;object oriented programming;upper bound;frame problem	The dynamic frames approach has proven to be a powerful formalism for specifying and verifying object-oriented programs. However, it requires writing and checking many frame annotations. In this paper, we propose a variant of the dynamic frames approach that eliminates the need to explicitly write and check frame annotations. In this paper, we improve upon the classical dynamic frames approach in two ways: (1) method contracts are more concise and (2) fewer proof obligations must be discharged by the verifier. We have proven soundness, implemented the approach in a verifier prototype and demonstrated its expressiveness by verifying several challenging examples from related work. Implicit Dynamic Frames: Combining dynamic frames and separation logic	formal specification;prototype;semantics (computer science);separation logic;verification and validation	Jan Smans;Bart Jacobs;Frank Piessens	2009		10.1007/978-3-642-03013-0_8	verification;separation logic;frame problem;computer science;theoretical computer science;upper and lower bounds;programming language;object-oriented programming;algorithm	AI	-18.55810190035011	27.637661946700813	2432
b825a0813501c1af7d035fb425f0df66ec604a1d	classification of apis by hierarchical clustering		APIs can be classified according to the programming domains (e.g., GUIs, databases, collections, or security) that they address. Such classification is vital in searching repositories (e.g., the Maven Central Repository for Java) and for understanding the technology stack used in software projects. We apply hierarchical clustering to a curated suite of Java APIs to compare the computed API clusters with preexisting API classifications. Clustering entails various parameters (e.g., the choice of IDF versus LSI versus LDA). We describe the corresponding variability in terms of a feature model. We exercise all possible configurations to determine the maximum correlation with respect to two baselines: i) a smaller suite of APIs manually classified in previous research; ii) a larger suite of APIs from the Maven Central Repository, thereby taking advantage of crowd-sourced classification while relying on a threshold-based approach for identifying important APIs and versions thereof, subject to an API dependency analysis on GitHub. We discuss the configurations found in this way and we examine the influence of particular features on the correlation between computed clusters and baselines. To this end, we also leverage interactive exploration of the parameter space and the resulting dendrograms. In this manner, we can also identify issues with the use of classifiers (e.g., missing classifiers) in the baselines and limitations of the clustering approach.	apache maven;application programming interface;baseline (configuration management);cluster analysis;computation;crowdsourcing;database;dendrogram;dependence analysis;digital curation;documentation;feature model;genetic algorithm;heart rate variability;hierarchical clustering;java;p versus np problem;recommender system;scalability;software engineering;solution stack;tf–idf	Johannes Härtel;Hakan Aksu;Ralf Lämmel	2018		10.1145/3196321.3196344	parameter space;data mining;feature model;software;dendrogram;computer science;hierarchical clustering;cluster analysis;baseline (configuration management);java	SE	-61.243928738016656	39.37757897428899	2440
4310177bca5e2dcf8bd7f4cd944699d38612b3c6	massively parallel x-ray scattering simulations	analytical models;simd;design engineering;nanofabrication;scattering;x ray scattering;data analysis;computational modeling;x ray scattering data analysis design engineering graphics processing units materials science computing multiprocessing systems nanofabrication;lattice qcd;shape;matrix decomposition;graphics processing units;materials science computing;blue gene;multiprocessing systems;massively parallel x ray scattering simulation gisaxs simulation code near linear scaling sequential cpu code cray xe6 24 core fermi gpu graphics processing unit many core gpu cluster multicore cpu cluster grazing incidence small angle x ray scattering simulation data analysis nanodevice fabrication nanodevice design energy relevant nanodevice material nanostructural property x ray scattering technique;graphics processing units scattering instruction sets shape computational modeling analytical models matrix decomposition;instruction sets	Although present X-ray scattering techniques can provide tremendous information on the nano-structural properties of materials that are valuable in the design and fabrication of energy-relevant nano-devices, a primary challenge remains in the analyses of such data. In this paper we describe a high-performance, flexible, and scalable Grazing Incidence Small Angle X-ray Scattering simulation algorithm and codes that we have developed on multi-core/CPU and many-core/GPU clusters. We discuss in detail our implementation, optimization and performance on these platforms. Our results show speedups of ~125x on a Fermi-GPU and ~20x on a Cray-XE6 24-core node, compared to a sequential CPU code, with near linear scaling on multi-node clusters. To our knowledge, this is the first GISAXS simulation code that is flexible to compute scattered light intensities in all spatial directions allowing full reconstruction of GISAXS patterns for any complex structures and with high-resolutions while reducing simulation times from months to minutes.	ab initio quantum chemistry methods;block size (cryptography);branch and bound;central processing unit;code;cray xe6;fermi (microarchitecture);gnu nano;graphical user interface;graphics processing unit;image scaling;incidence matrix;input device;manycore processor;mathematical optimization;message passing interface;multi-core processor;parallel algorithm;parallel computing;real-time clock;scalability;simulation;speedup;thread block	Abhinav Sarje;Xiaoye S. Li;Slim Chourou;Elaine R. Chan;Alexander Hexemer	2012	2012 International Conference for High Performance Computing, Networking, Storage and Analysis	10.1109/SC.2012.76	computational science;parallel computing;nanolithography;simd;shape;computer science;theoretical computer science;operating system;instruction set;scattering;data analysis;matrix decomposition;computational model;quantum mechanics;lattice qcd	HPC	-5.589095158949252	37.50376927009802	2450
af5bcf1fdd038087ec1adcacf3d6524a45689ebe	evaluating the java virtual machine as a target for languages other than java	lenguaje programacion;virtual machine;langage procedural;object oriented language;compilateur;programming language;java virtual machine;langage java;machine virtuelle;compiler;object oriented;langage programmation;oriente objet;lenguaje procedural;maquina virtual;orientado objeto;compilador;procedural language;java language;open source	The portability and runtime safety of programs which are executed on the Java Virtual Machine (JVM) makes the JVM an attractive target for compilers of languages other than Java. Unfortunately, the JVM was designed with language Java in mind, and lacks many of the primitives required for a straighforward implementation of other languages. Here, we discuss how the JVM may be used to implement other object-oriented languages. As a practical example of the possibilities, we report on a comprehensive case study. The open source Gardens Point Component Pascal compiler compiles the entire Component Pascal language, a dialect of Oberon-2, to JVM byte-codes. This compiler achieves runtime efficiencies which are comparable to native-code implementations of procedural languages.		K. John Gough;Diane Corney	2000		10.1007/10722581_22	parallel computing;java concurrency;computer science;operating system;java modeling language;strictfp;database;real time java;programming language;object-oriented programming;java;scala;java annotation	PL	-25.371457513889762	28.915505409580632	2453
ecf6e9986c1953f547bb022f01be44b3b7535e8d	a shortest path network security model	graph theory;shortest path;risk analysis;network security;cascade problem	This paper presents a new model, based on the resourceconstrained shortest path, for evaluating the security of computer networks. The power of the model is twofold. First, the use of the resource-constrained shortest path as an integral part of the model ties security evaluation metrics to graph theory, thus providing a rigorous mathematical base for the evaluation of network security. Second, the model allows both local (nodal) and overall (network) security considerations to be incorporated into the security evaluation process. The usefulness of the model is demonstrated by applying the model to examples from current literature. Applying the model to a generalization of the cascade problem not only yields an efficient algorithm (O(N’)) for the problem, but also points out the underlying security issues in interconnecting independently evaluated systems. As a consequence, the resource-constrained shortest path model leads to a broader understanding of network security risks.	algorithm;floor and ceiling functions;graph theory;network security;shortest path problem	John A. Fitch;Lance J. Hoffman	1993	Computers & Security	10.1016/0167-4048(93)90100-J	computer security model;canadian traveller problem;risk analysis;constrained shortest path first;longest path problem;average path length;computer science;graph theory;theoretical computer science;network security;machine learning;distributed computing;shortest path problem;computer security	Security	-24.54112491622812	78.61580777525748	2456
223af2131a7e205dac22be6684b225db0f190480	dynamic runtime re-scheduling allowing multiple implementations of a task for platform-based designs	switches;computational complexity;task analysis;embedded systems;national electric code;cost function;embedded system;rate monotonic scheduling;embedded software;steady state;hardware;drams;hot swapping	"""This paper introduces an extension to the RMS schedulingtechnique that we call """"Hot Swapping"""". Hot Swappingenables a system to choose between various selectedimplementations of one task on-the-fly and thus to optimizethe system's cost (e.g. power savings). The on-the-flyswapping between those implementations requires extra timeto save and/or transform states of a certain taskimplementation. Even if the two steady-state schedulesbefore and after the swapping are feasible, the transientschedule with the additional swapping computation timemay exceed the system's capacity. Our technique is anextension to Rate Monotonic Scheduling (RMS). Whilemaintaining and meeting performance requirements, ourtechnique shows an average reduction of 31% in powerconsumption compared to systems using a pure staticscheduling approach (RMS) that cannot make use of taskswapping. We have evaluated our algorithm throughsimulation of five real-world task sets and in addition by useof a large number of generated task sets."""	algorithm;computation;hot swapping;paging;rate-monotonic scheduling;requirement;schedule (project management);steady state	Tin-Man Lee;Wayne H. Wolf;Jörg Henkel	2002			embedded system;hot swapping;national electrical code;parallel computing;real-time computing;embedded software;network switch;computer science;rate-monotonic scheduling;operating system;task analysis;computational complexity theory;steady state;dram	Embedded	-7.069113432918249	59.19520725848224	2458
cd96127266b762982855b0e049e87fd8cb227efc	guidelines for cryptographic key management	computer science	The question often arises of whether a given security system requires#N#some form of automated key management, or whether manual keying is#N#sufficient. This memo provides guidelines for making such decisions.#N#When symmetric cryptographic mechanisms are used in a protocol, the#N#presumption is that automated key management is generally but not#N#always needed. If manual keying is proposed, the burden of proving#N#that automated key management is not required falls to the proposer.#N#This document specifies an Internet Best Current Practices for the#N#Internet Community, and requests discussion and suggestions for#N#improvements.	key (cryptography);key management	Steven M. Bellovin;Russ Housley	2005	RFC	10.17487/RFC4107	security association;computer science;static key;data mining;distributed computing;computer security	Security	-46.554812795749115	71.79966203895891	2459
029a3fe1bc3fcfc43b5f2a8f24bdbbacfb7c0cc4	a local deadlock detection and resolution algorithm for process networks	deadlock detection process networks;distributed algorithms;system recovery channel capacity processor scheduling parallel processing monitoring computer science software engineering software algorithms computer networks application software;artificial deadlocks;complexity theory;frequency modulation;memory management;bottleneck channel local deadlock detection resolution algorithm process networks kahn process network data streaming idealized kpn model unbounded channel capacities bounded scheduling policy artificial deadlocks message cooperation message complexity;data stream;deadlock detection;communication complexity;process network;bottleneck channel;bounded scheduling policy;process networks;message cooperation;computational modeling;system recovery;channel capacity;scheduling;system recovery communication complexity scheduling;local deadlock detection;unbounded channel capacities;resolution algorithm;kahn process network;data streaming;message complexity;idealized kpn model;data models	Kahn Process Network (KPN) is a popular model for data streaming applications. Since it is impractical to implement an idealized KPN model with unbounded channel capacities, a bounded scheduling policy has been proposed by T. M. Parks. However, this policy would lead to artificial deadlocks in PN. Several deadlock detection mechanisms have been proposed to address this problem. In this paper, we propose an efficient deadlock detection algorithm which extends M. Prietopsilas algorithm for PN using message cooperation. It achieves a message complexity of O(n) and finds the bottleneck channel to resolve the artificial deadlock.	algorithm;deadlock;kahn process networks;parallel computing;scheduling (computing);sensor;stream (computing)	Wei Huang;Deyu Qi	2008	2008 International Conference on Computer Science and Software Engineering	10.1109/CSSE.2008.1468	frequency modulation;data modeling;distributed algorithm;real-time computing;computer science;theoretical computer science;operating system;communication complexity;distributed computing;edge chasing;computational model;scheduling;deadlock prevention algorithms;channel capacity;memory management	DB	-16.100166360894697	46.69105851205179	2465
2f125824cc0b910ad550ea1d7415fa6c780b2261	coherent conditional probability as a measure of information of the relevant conditioning events	medida informacion;incertidumbre;conditionnement;probabilidad condicional;uncertainty;mesure information;logique floue;probabilite conditionnelle;duality;semantics;logica difusa;possibility measure;intelligence artificielle;probabilistic approach;semantica;semantique;conditioning;fuzzy logic;dualite;information measure;mesure probabilite;enfoque probabilista;approche probabiliste;artificial intelligence;norma triangular;dualidad;t norm;norme triangulaire;acondicionamiento;measures of information;incertitude;inteligencia artificial;point of view;conditional probability;probability measure;medida probabilidad	In previous papers, by resorting to the most effective concept of conditional probability, we have been able not only to define fuzzy subsets, but also to introduce in a very natural way the basic continuous T-norms and the relevant dual T-conorms, bound to the former by coherence. Moreover, we have given, as an interesting and fundamental by-product of our approach, a natural interpretation of possibility functions, both from a semantic and a syntactic point of view. In this paper we study the properties of a coherent conditional probability looked on as a general non-additive uncertainty measure of the conditioning events, and we prove that this measure is a capacity if and only if it is a possibility.	coherent	Giulianella Coletti;Romano Scozzafava	2003		10.1007/978-3-540-45231-7_12	fuzzy logic;duality;uncertainty;conditional probability;probability measure;artificial intelligence;regular conditional probability;conditioning;mathematics;semantics;t-norm;conditional mutual information;statistics	Vision	-14.24364095129225	6.880684059758496	2474
19aef9b10d6bbea2a8e32006c2e7b78aefb9186e	typed logical variables in haskell	publikationer;konferensbidrag;natural extension;artiklar;rapporter;functional logic programming	We describe how to embed a simple typed functional logic programming language in Haskell. The embedding is a natural extension of the Prolog embedding by Seres and Spivey [16]. To get full static typing we need to use the Haskell extensions of quantified types and the ST-monad.	functional logic programming;haskell;monad (functional programming);programming language;prolog;type system	Koen Claessen;Peter Ljunglöf	2000	Electr. Notes Theor. Comput. Sci.	10.1016/S1571-0661(05)80544-4	combinatory logic;computer science;theoretical computer science;type inference;functional logic programming;programming language;algorithm	PL	-21.84842021233493	23.136998033756417	2478
243160f6fe9eaba17b7f81e5b1499f375ab49afb	early availability requirements modeling using use case maps	software;automated validation;fault tolerant;early error detection;availability;architectural tactics;real time;functional properties;issu;ip routers distributed real time systems fault tolerance nonfunctional requirements system development life cycle real time design early error detection automated validation automated verification use case maps architectural tactics in service software upgrade feature;systems software error detection formal verification real time systems software fault tolerance;software fault tolerance;use case map;systems software;non functional requirement;automated verification;real time design;formal verification;redundancy;distributed real time system;design and implementation;synchronization;fault tolerance;unified modeling language;use case maps;issu non functional use case maps availability architectural tactics;system development life cycle;non functional;error detection;nonfunctional requirements;distributed real time systems;early detection;in service software upgrade feature;ip routers;validation and verification;availability redundancy software unified modeling language heart beat synchronization;heart beat;real time systems;time constraint	The design and implementation of distributed real-time systems is often dominated by non-functional considerations like timing, distribution and fault tolerance. As a result, it is increasingly recognized that non-functional requirements should be considered at the earliest stages of system development life cycle. The ability to model non-functional properties (such as timing constraints, availability, performance and security) at the system requirement level not only facilitates the task of moving towards real-time design, but ultimately supports the early detection of errors through automated validation and verification. This paper introduces a novel approach to describe availability features in Use Case Maps (UCM) specifications. The proposed approach relies on a mapping of availability architectural tactics to UCM components. We illustrate the applicability of our approach using the ISSU (In Service Software Upgrade) feature on IP routers.	authorization;beneath a steel sky;dependability;fault tolerance;functional requirement;non-functional requirement;rational clearcase ucm;real-time clock;real-time computing;refinement (computing);requirements analysis;software development process;store and forward;system requirements;systems development life cycle;timing closure;verification and validation	Jameleddine Hassine	2011	2011 Eighth International Conference on Information Technology: New Generations	10.1109/ITNG.2011.133	fault tolerance;real-time computing;computer science;operating system;software engineering;distributed computing;non-functional requirement	Embedded	-47.56537334999518	32.86745838171018	2487
c5eac6e23ef9af7dbf25c7d11f79b4e33543b9d2	strukturen der werbung 1986: ein auf informationstheoretischer basis entwickeltes verfahren der statistischen analyse von werbewirkung				Bettina Semmerling	1988				NLP	-100.42513273427421	24.106627447328524	2493
3cd422f896fa59a32c0e65298dfd5ed0441e464b	adaptive middleware design for satellite fault-tolerant distributed computing	computers;onboard data handling adaptive middleware design satellite fault tolerant distributed computing computing systems onboard satellites fault tolerance design amft failure detection failure isolation failure recovery distributed onboard computer system obc units launch vehicles interplanetary rovers;adaptive middleware;on board data handling;software fault tolerance;on board computer;planetary satellites;system recovery;aerospace control;fault tolerant systems;satellite;satellites;fault tolerance;artificial satellites;system recovery aerospace control artificial satellites data handling middleware planetary satellites software fault tolerance;on board data handling fault tolerant distributed computing adaptive middleware satellite on board computer;middleware;fault tolerant distributed computing;fault tolerance fault tolerant systems satellites computers middleware hardware;data handling;hardware	This paper is concerned with increasing the reliability with respect to debris impact of computing systems on board satellites using Fault Tolerant Distributed Computing. A novel adaptive middleware for fault-tolerance (AMFT) design is presented. The middleware performs failure detection, isolation and recovery and synchronizes the operation of a distributed On-Board Computer (OBC) system. If one of the members of the distributed group of OBCs fails, the failed processor tasks are reallocated dynamically to the other healthy OBC units. The approach can also be employed in other space applications such as launch vehicles, interplanetary rovers, etc. The proposed design is modular, low cost and modifiable according to the requirements of the target application.	distributed computing;fault tolerance;field-programmable gate array;internet branding;middleware;on-board data handling;requirement;scalability;triple modular redundancy	Muhammad Fayyaz;Tanya Vladimirova;Jean-Michel Caujolle	2012	2012 NASA/ESA Conference on Adaptive Hardware and Systems (AHS)	10.1109/AHS.2012.6268664	embedded system;middleware;real-time computing;computer science;middleware;distributed computing;distributed design patterns;satellite	HPC	-33.24984166012781	37.649636764023086	2499
e8930794074b7cd4fa6eb43031afcfa83f53f346	designing software product lines with uml	software systems;software engineering;books;software architecture;permission;unified modeling language;product design;software design;software design product design unified modeling language software engineering books software systems software architecture programming permission software development management;software product line;programming;software development management	Overview This book describes an evolutionary software engineering process for the development of software product lines, which uses the Unified Modeling Language (UML) notation. A software product line (or product family) consists of a family of software systems that have some common functionality and some variable functionality. The interest in software product lines emerged from the field of software reuse when developers and managers realized that they could obtain much greater reuse benefits by reusing software architectures instead of reusing individual software components. The field of software product lines is increasingly recognized in industry and government as being of great strategic importance for software development. Studies indicate that if three or more systems with a degree of common functionality are to be developed, then developing a product line is significantly more cost-effective than developing each system from scratch. The traditional mode of software development is to develop single systems—that is, to develop each system individually. For software product lines, the development approach is broadened to consider a family of software systems. This approach involves analyzing what features (functional requirements) of the software family are common, what features are optional, and what features are alternatives. After the feature analysis, the goal is to design a software architecture for the product line, which has common components (required by all members of the family), optional components (required by only some members of the family), and variant components (different versions of which are required by different members of the family). Instead of starting from square one, the developer creates applications by adapting and configuring the product line architecture. To model and design families of systems, the analysis and design concepts for single product systems need to be extended to support software product lines. This book is intended to appeal to readers who are familiar with modeling and designing single systems, but who wish to extend their knowledge to modeling and designing software product lines. It is also intended to appeal to readers who are familiar with applying UML to the modeling and design of single systems but not with developing software product lines. Several textbooks on the market describe object-oriented concepts and methods, which are intended for single systems. Very few books address software families or product lines; and of those that do, even fewer use UML. This book provides a comprehensive treatment of the application of UML-based object-oriented concepts to the analysis and …	book;code reuse;component-based software engineering;software architecture;software development process;software product line;software system	Hassan Gomaa	2005	29th Annual IEEE/NASA Software Engineering Workshop - Tutorial Notes (SEW'05)	10.1109/SEW.2005.5	unified modeling language;software architecture;programming;personal software process;verification and validation;software sizing;computer science;systems engineering;engineering;package development process;backporting;software design;social software engineering;software framework;component-based software engineering;software development;software design description;software engineering;software construction;software walkthrough;product design;programming language;resource-oriented architecture;software measurement;software deployment;software development process;software requirements;software system;software peer review	SE	-58.81608942482056	27.085035310974163	2503
aac3c9b88c3a29e463806e301e6818e0242d4a78	a grid-enabled problem solving environment for supporting collaborative aerodynamic engineering process	aerospace e science grid workflow;groupware;aerospace;groupware aerodynamics aerospace computing aerospace engineering grid computing;aerospace engineering;aerodynamics;e science;scientific workflow;collaboration;proof of concept;computational fluid dynamics;grid;computational modeling;large scale simulation;workflow system;aerospace computing;solid modeling;problem solving collaboration aerodynamics collaborative work aerospace engineering humans grid computing large scale systems computational modeling aerospace simulation;workflow;collaborative research;e airs system grid enabled problem solving environment collaborative aerodynamic engineering process workflow based problem solving environment aerospace engineering;grid computing;problem solving environment;problem solving;data models	This paper describes a workflow-based problem solving environment (PSE) for supporting collaborative aerospace engineering process on the grid. The focus of our work is on the design of a workflow system that supports computational activities that are to be executed on the grid and manual activities that are to be performed by humans. According to our investigation on the aerospace engineering research process through interviews with aerospace researchers and engineers, it is essential to incorporate the grid-enabled execution of large-scale simulation runs and the support for manual activities into the workflow system. Most existing scientific workflow systems are aware of the importance of the support for the execution of applications on the grid, but not concerned with the support for manual activities. On the other hand, most existing business workflow systems are designed to have human intervention in mind, but not concerned with the grid-enabled execution. Based on the analysis of the e-AIRS system which is about a collaborative PSE for aerospace engineering, we have identified a collaborative research scenario and requirements. As a proof of concept system, we have prototyped a grid-enabled workflow system that realizes the scenario and meets the requirements.	problem solving environment;requirement;simulation	June Hawk Lee;Dukyun Nam;Soonwook Hwang;Ok-Hwan Byeon	2008	2008 IEEE Fourth International Conference on eScience	10.1109/eScience.2008.13	computational science;workflow;simulation;computer science;systems engineering;workflow management system;workflow engine;workflow technology	SE	-50.17647547354651	18.39961301622784	2507
2d6a2f3891b6cc667dbecebd4b3e83afde690d30	improving the data quality of drug databases using conditional dependencies and ontologies	description logics;conditional dependencies;data quality	Many health care systems and services exploit drug related information stored in databases. The poor data quality of these databases, e.g. inaccuracy of drug contraindications, can lead to catastrophic consequences for the health condition of patients. Hence it is important to ensure their quality in terms of data completeness and soundness.  In the database domain, standard Functional Dependencies (FDs) and INclusion Dependencies (INDs), have been proposed to prevent the insertion of incorrect data. But they are generally not expressive enough to represent a domain-specific set of constraints. To this end, conditional dependencies, i.e. standard dependencies extended with tableau patterns containing constant values, have been introduced and several methods have been proposed for their discovery and representation. The quality of drug databases can be considerably improved by their usage.  Moreover, pharmacology information is inherently hierarchical and many standards propose graph structures to represent them, e.g. the Anatomical Therapeutic Chemical classification (ATC) or OpenGalen’s terminology. In this article, we emphasize that the technologies of the Semantic Web are adapted to represent these hierarchical structures, i.e. in RDFS and OWL. We also present a solution for representing conditional dependencies using a query language defined for these graph oriented structures, namely SPARQL. The benefits of this approach are interoperability with applications and ontologies of the Semantic Web as well as a reasoning-based query execution solution to clean underlying databases.	advanced transportation controller;conditional (computer programming);conditional entropy;constraint (mathematics);data quality;description logic;embedded system;emergence;exploit (computer security);functional dependency;informatics;interoperability;long division;method of analytic tableaux;ontology (information science);plasma cleaning;query language;rdf schema;referential integrity;relational database;sparql;semantic web;semantic reasoner;subsumption architecture;web ontology language	Olivier Curé	2012	J. Data and Information Quality	10.1145/2378016.2378019	description logic;data quality;dependency theory;computer science;data mining;database;world wide web;information retrieval	DB	-24.710703142193754	8.207581743411538	2511
4f75ce8f8a326a4eed1222d0fad1c0c4bf4cd64e	reliable event dissemination for time-sensible applications over wide-area networks				Christiancarmine Esposito	2009		10.6092/UNINA/FEDOA/3892	data mining;distributed computing;computer science	DB	-20.035686559458433	86.82730153103667	2516
bba7fac5ac8bc49f4d844cfd3ad230046048d014	automation of detection of security vulnerabilities in web services using dynamic analysis	response security web service testing attack vulnerability wsdl xml payload dynamic analysis enumeration automation soap request;testing;xml simple object access protocol automation security payloads testing;xml;xml data integrity security of data web services;payloads;simple object access protocol;security;soap response model automatic security vulnerability detection web services dynamic analysis data maintenance web communications data integrity vulnerability classification unchangeable dynamic vulnerabilities wsdl enumeration automation script soap request model xml format;automation	The usage of XML in maintaining data over the Web communications has lead to new ways of exploitation which are dangerous for the data integrity yet can be remediated on the basis of the vulnerability classification. The approach is reserved for the research scope of unchangeable dynamic vulnerabilities with the help of WSDL Enumeration and automation script for detection of the vulnerabilities on analysis of the SOAP Request and Response saved in XML Format with different payloads.	automation;data integrity;vulnerability (computing);web services description language;web service;world wide web;xml	Rahul Kumar;K Indraveni;Aakash Kumar Goel	2014	The 9th International Conference for Internet Technology and Secured Transactions (ICITST-2014)	10.1109/ICITST.2014.7038832	xml encryption;streaming xml;computer science;xml framework;soap;database;xml signature;world wide web;computer security	SE	-57.72302588623518	58.763116698696436	2517
3d492079f5659a63257ea588f3ad29a526cb52a2	renaissance: a method to migrate from legacy to immortal software systems	legacy software;research initiatives systems re engineering software maintenance user interfaces;software maintenance;user interface;software systems;implementation renaissance method systems migration legacy software systems immortal software systems reengineering software assets transformation evolvable systems organisational assessment candidate strategy adoption system renewal software maintenance user interface system structure system architecture system design system replacement;legacy;software systems electrical capacitance tomography investments software maintenance design engineering systems engineering and theory uniform resource locators technological innovation cost function process planning;research initiatives;transformation;point of view;legacy system;reengineering;user interfaces;assessment;systems re engineering	The Renaissance method was developed to support the reengineering of legacy software systems— that is, the transformation of valuable software assets which are difficult to maintain into new systems which can evolve both in the short and long term. The method is structured into two main phases. The first one, or what to do, aims to assess the organisation and the legacy system and identify both the need and urgency of reengineering and the best candidate strategy to adopt for renewing the system: continue with the current maintenance approach; reengineer the system from user-interface, structure, architecture, and/or design point of views; replace the existing system with a new developed one. The second phase, or how to do, supports the implementation of the planned transformation, and drives the overall process of migrating the legacy system to its immortal dimension.	code refactoring;legacy system;renaissance;software system;the immortal;user interface	Marco Battaglia;Giancarlo Savoia;John M. Favaro	1998		10.1109/CSMR.1998.665807	computer science;systems engineering;engineering;operating system;software engineering;user interface;legacy system;computer engineering	SE	-61.79648931990407	23.030787398983158	2526
22f3c47448bd7f082eecafb7a52f053edf6b090d	complexity of simple dependent bimodal logics	logica temporal;consistencia semantica;complexite calcul;soundness;temporal logic;logique propositionnelle;logical programming;satisfiability;upper bound;complejidad computacion;modal logic;programmation logique;mathematical programming;computational complexity;propositional logic;consistance semantique;logica proposicional;programacion logica;programmation mathematique;programacion matematica;logique temporelle	We characterize the computational complexity of simple dependent bimodal logics. We define an operator ⊕⊆ between logics that almost behaves as the standard joint operator ⊕ except that the inclusion axiom [2]p ⇒ [1]p is added. Many multimodal logics from the literature are of this form or contain such fragments. For the standard modal logics K,T ,B,S4 and S5 we study the complexity of the satisfiability problem of the joint in the sense of ⊕⊆. We mainly establish the PSPACE upper bounds by designing tableaux-based algorithms in which a particular attention is given to the formalization of termination and to the design of a uniform framework. Reductions into the packed guarded fragment with only two variables introduced by M. Marx are also used. E. Spaan proved that K⊕⊆S5 is EXPTIME-hard. We show that for 〈L1,L2〉 ∈ {K,T,B} × {S4, S5}, L1 ⊕⊆ L2 is also EXPTIME-hard.	algorithm;computational complexity theory;exptime;guarded logic;modal logic;multimodal interaction;pspace	Stéphane Demri	2000		10.1007/10722086_17	modal logic;soundness;discrete mathematics;temporal logic;computer science;artificial intelligence;mathematics;linguistics;propositional calculus;upper and lower bounds;computational complexity theory;algorithm;satisfiability	Logic	-8.071072228023315	19.208923331068586	2528
1adb6034bbc347d2cee4986d26a5527157930ad3	empirical performance models for java workloads	virtual machine;processor architecture;confidence level;design and development;theoretical computer science;computer science all;system performance;performance model;empirical model;statistical techniques;multivariate adaptive regression splines	Java is widely deployed on a variety of processor architectures. Consequently, an understanding of microarchitecture level Java performance is critical to optimize current systems and to aid design and development of future processor architectures for Java. Although this is facilitated by a rich set of processor performance counters featured on several contemporary processors, complex processor microarchitecture structures and their interactions make it difficult to relate observed events to overall performance. This, coupled with the complexities associated with running Java over a virtual machine, further aggravates the situation. This paper explores and evaluates the effectiveness of empirical modeling for Java workloads. Our models use statistical regression techniques to relate overall Java system performance to various observed microarchitecture events and their interactions. Multivariate adaptive regression splines effectively capture non-linear and non-monotonic associations between the response and predictor variables. Our models are interpretable, easy to construct and exhibit high correlation/low errors between predicted and measured performance. Furthermore, empirical models afford additional insights into the characteristics of Java performance and the use of statistical techniques throughout this study allow us to assign confidence levels to our estimates of performance.	central processing unit;interaction;java performance;kerrison predictor;microarchitecture;microsoft outlook for mac;multivariate adaptive regression splines;nonlinear system;performance evaluation;profiling (computer programming);smoothing spline;spline (mathematics);virtual machine	Pradeep Rao;Kazuaki Murakami	2009		10.1007/978-3-642-00454-4_22	embedded system;parallel computing;real-time computing;simulation;confidence interval;microarchitecture;multivariate adaptive regression splines;computer science;virtual machine;operating system;computer performance;programming language;empirical modelling	Arch	-22.984803842009462	56.61156273592904	2529
514a3b52d65dde8e4c1f5a7ad4ee453cbee60770	dually: a framework for architectural languages and tools interoperability	architecture description language;model transformation;tool interoperability;dually framework;architectural modeling;software architecture;dually framework architectural language tool interoperability architectural modeling;software tools open systems software architecture;architectural language;software tools;domain analysis;open systems;unified modeling language biological system modeling computational modeling software computer architecture weaving adaptation model	Nowadays different notations for architectural modeling have been proposed, each one focussing on a specific application domain, analysis type, or modeling environment. No effective interoperability is possible to date. DUALLY is an automated framework that aims to offer an answer to this need allowing architectural languages and tools interoperability. DUALLY has been implemented as an Eclipse plugin and it is based on model transformation techniques. This demonstration paper shows DUALLY by applying its approach to two outstanding architectural description languages.	application domain;description logic;eclipse;interoperability;model transformation;systems architecture	Ivano Malavolta;Henry Muccini;Patrizio Pelliccione	2008	2008 23rd IEEE/ACM International Conference on Automated Software Engineering	10.1109/ASE.2008.82	architecture description language;computer architecture;architectural geometry;architectural pattern;computer science;systems engineering;service-oriented modeling;software engineering;programming language	SE	-52.203777179926114	26.179629667036878	2532
61609e76c970704a4120cdbd4c61e641ce5e313a	assessment of voice over ip as a solution for voice over adsl	protocols;access network;dsl;voice over ip;telecommunication congestion control;internet telephony;qos;performance metric;digital subscriber lines;telecommunication traffic;admission control voice over ip voice over adsl vodsl voatm voice over atm voice traffic qos mechanisms end to end delay voice packets access network signaling mechanism;thesis;voatm;internet telephony dsl bandwidth protocols admission control delay copper quality of service communication system traffic control asynchronous transfer mode;telecommunication congestion control integrated voice data communication internet telephony telecommunication traffic quality of service delays digital subscriber lines asynchronous transfer mode protocols telecommunication signalling;telecommunication signalling;integrated voice data communication;quality of service;end to end delay;simulation model;voip;asynchronous transfer mode;delays;admission control	 This paper compares VoATM and VoIP in terms of their suitability for carrying voice traffic over DSL. ATM is currently the preferred protocol due to its built-in QoS mechanisms. Through simulations we show that IP QoS mechanisms can also be used to achieve comparable performance for voice traffic over DSL. Our performance metrics are the ETE delay of voice packets across the DSL access network and the bandwidth requirements of a voice call. We also propose an implicit signaling mechanism to provide admission control for individual voice calls over DSL. We implement a simulation model that uses our mechanism and perform simulations to verify its effectiveness. We conclude that by incorporating appropriate QoS mechanisms and implicit signaling, it is possible to achieve performance for VoIP comparable to that provided by ATM. In this case, the ubiquity of IP makes it a very attractive candidate for future deployments of VoDSL.	atm turbo;access network;asymmetric digital subscriber line;built-in self-test;quality of service;requirement;simulation	Abhishek Ram;Luiz A. DaSilva;Srinidhi Varadarajan	2002		10.1109/GLOCOM.2002.1189073	real-time computing;quality of service;telecommunications;computer science;voice over ip;computer network	Networks	-5.63436791004539	94.15830720592312	2539
1c08e147573658ceced62052adaced5e561fd51a	mensch-maschine interaktion für den interventionellen einsatz		In diesem Beitrag beschreiben wir die Möglichkeiten der Steuerung von Geräten mittels natürlicher Sprache am Beispiel eines  sprachgesteuerten 3D-Gefäßanalysesystems. Das System versteht ganze Sätze und erkennt selbständig, ob eine Äußerung an das  System gerichtet ist oder an eine andere Person. Die Sprachsteuerung wurde am Lehrstuhl für Mustererkennung der Universität  Erlangen-Nürnberg in Zusammenarbeit mit der Firma Sympalog Voice Solutions GmbH für ein Gerät zur Stenosenvermessung der Firma  Siemens Medical Solutions (Leonardo Workstation) entwickelt und erfolgreich einer klinischen Erprobung unterzogen.  	maschine	Marcus Prümmer;Elmar Nöth;Joachim Hornegger;Axel Horndasch	2005		10.1007/3-540-26431-0_99	art;performance art	NLP	-102.46160091982922	30.883465832499535	2541
5d8d4cc6d0d1aed0544fd1db2c0743d74d945ed2	research on embedded-based wireless authentication system		Fingerprint identification is a high technology that extensively used in recent years. It proposed a new authentication system design based on fingerprint identification and 3G technology which uses S3C2440 as its microcontroller and MBF200 as its fingerprint sensor. With this system, certificates can be identified accurately and some cornhill robbery can be avoided. Also the authentication process is rapid and convenient There is no need to worry about the false testimony or certificates be stolen or lost. As a result, it helps to reduce some crimes and contributes a lot to establishing a harmonious society. Also a testing to the system was done do and the experiment results show its security and stability. This system is more advanced than traditional ones by using the latest technic and possesses good application value of popularization.	authentication;embedded system	Wei Pan;Xiang Yang;Yuanyi Zhang;Zhe Guo	2012		10.1007/978-3-642-34038-3_65	wireless wan;authentication protocol;lightweight extensible authentication protocol;wireless lan controller;wi-fi array;ant;challenge-handshake authentication protocol	Mobile	-47.079941002533154	74.21463366740171	2544
6c8820f02b741c03a9829a6d89ab31e95c8735cf	simulation des systèmes d'information des smart grids : une approche par points de vue		We propose to simulate the Information Systems of Smart Grids to validate / criticize the modeling choices of domain experts. We propose a viewpoint based approach regarding two aspects : information and dynamics. We add an integration viewpoint to maintain consistency and we use Model Driven Engineering techniques. Finally, we experience our approach on a Smart Grid use case. MOTS-CLÉS : Simulation, Ingénierie Dirigée par les Modèles, Smart Grid, Système d’Information	information system;model-driven engineering;simulation;vue	Rachida Seghiri;Frédéric Boulanger;Claire Lecocq;Vincent Godefroy	2015			smart grid;information system;model-driven architecture;distributed computing;computer science	AI	-54.233741171352726	21.659523816501807	2548
608e9de2b0d45f9d624d614f49e4c505dda62226	auction based resource negotiation in nomad	distributed system;resource discovery;resource allocation;mobility;computational economy;market negotiation;mobile code;middleware;grid computing	The challenges faced by mobile and distributed applications include the ability to discover and negotiate for the resources required for execution. The NOMAD (Negotiated Object Mobility, Access and Deployment) system is a middleware platform that provides an infrastructure to support applications constructed of mobile code. This paper describes the NOMAD mechanisms for resource discovery and negotiation. NOMAD features a Marketplace providing a forum in which multiple resource requirements lead to contracts for the allocation of resources between applications and resource providers. NOMAD’s Marketplace is also a useful model for resource allocation in distributed systems such as grid computations. Experimental results show that resources are allocated consistent with the policies of both the application and the resource provider.	code mobility;computation;distributed computing;entity;grid computing;middleware;requirement;server (computing)	Kris Bubendorfer;John H. Hine	2005			simulation;resource allocation;knowledge management;business;world wide web	HPC	-31.154901711596303	52.054935848283726	2555
35210fd5ce098150d5425b71b3a424c51884b5ec	adaptive-secure vrfs with shorter keys from static assumptions		Verifiable random functions are pseudorandom functions producing publicly verifiable proofs for their outputs, allowing for efficient checks of the correctness of their computation. In this work, we introduce a new computational hypothesis, the (ntext {-}mathsf {Eigen}text {-}mathsf {Value} ) assumption, which can be seen as a particularization of the (mathcal {U}_{l,k})-(mathrm {MDDH}) assumption for the case (l=k+1), and prove its equivalence with the (ntext {-}mathsf {Rank} ) problem. Based on the newly introduced computational hypothesis, we build the core of a verifiable random function having an exponentially large input space and reaching adaptive security under a static assumption. The final construction achieves shorter public and secret keys compared to the existing schemes reaching the same properties.	computation;correctness (computer science);digital physics;eigen (c++ library);formal verification;key (cryptography);linear programming relaxation;pseudorandomness;turing completeness;verifiable random function	Razvan Rosie	2017		10.1007/978-3-030-00434-7_22	discrete mathematics;theoretical computer science;random function;verifiable secret sharing;equivalence (measure theory);computer science;mathematical proof;correctness;computation;pseudorandom number generator;exponential growth	Crypto	-38.21575286597955	76.17554379417781	2557
32dd0e6d4f969dd7b721de51f990540400a17ce2	unified algorithm for undirected discovery of exception rules	decouverte regle;systeme intelligent;sistema inteligente;regle exception;data mining;algorithme unifie;regle association;association rule;fouille donnee;theoretical analysis;decouverte connaissance;intelligent system;rule discovery;descubrimiento conocimiento;common sense;busca dato;knowledge discovery	This paper presents an algorithm that seeks every possible exception rule which violates a common sense rule and satisfies several assumptions of simplicity. Exception rules, which represent systematic deviation from common sense rules, are often found interesting. Discovery of pairs that consist of a common sense rule and an exception rule, resulting from undirected search for unexpected exception rules, was successful in various domains. In the past, however, an exception rule was represented by a change of conclusion caused by adding an extra condition to the premise of a common sense rule. That approach formalized only one type of exceptions, and failed to represent other types. In order to provide a systematic treatment of exceptions, we categorize exception rules into eleven categories, and we propose a unified algorithm for discovering all of them. Preliminary results on fifteen real-world data sets provide an empirical proof of effectiveness of our algorithm in discovering interesting knowledge. The empirical results also match our theoretical analysis of exceptions, showing that the eleven types can be partitioned in three classes according to the frequency with which they occur in data.	algorithm;categorization;exception handling;flickr;graph (discrete mathematics);literal (computer programming);triplet state	Einoshin Suzuki;Jan M. Zytkow	2005	Int. J. Intell. Syst.	10.1002/int.20090	association rule learning;computer science;artificial intelligence;data mining;knowledge extraction;algorithm	ML	-18.059831032490553	9.853906635145645	2560
0678f5bcb5c69cf421a07f427475cb5de15e8783	keynote 4: can parallel software catch up with parallel hardware? trends in automatic parallelization	automatic parallelization tool parallel software parallel hardware large scale scientific computing engineering computing material manufacture nuclear fusion simulation automotive design sx 8 vector supercomputer earth simulator parallel programming tools mpi high performance fortran openmp parallel language;hardware supercomputers automotive engineering earth parallel programming large scale systems design engineering power engineering and energy computer aided manufacturing power engineering computing;software tools parallel programming parallel languages parallel machines;parallel programming;large scale;high performance fortran;catching up;parallel programming tool;parallel machines;software tools;earth simulator;parallel programs;parallel languages;automatic parallelization	upercomputers have to be proved powerful for various fields including the development of advanced technologies such as large-scale scientific and engineering computing, new material manufacture, nuclear fusion simulation, and automotive design. On October 20, 2004, NEC Corporation announced the availability of their new supercomputer ‘SX-8’, the world’s most powerful vector supercomputer with a peak processing performance of 65TFLOPS. In last few years, the hardware of supercomputers has undergone rapid development – from Earth simulator to SX-8, only goes through 3 years – the peak performance of SX-8 exceeds 1.8 times over Earth Simulator. However, the parallel software – especially parallel programming tools – is still underdevelopment. We still use MPI, High Performance Fortran and OpenMP mostly for our parallel programming tasks. In fact, these languages and libraries are difficult to use for most of scientific and engineering users. In this talk, we will outlook the some of the existing parallel language and automatic parallelization tools and also, we will address the potential technologies of automatic parallelization. Finally, we summarize how parallel software to make effort to catch up with the development of supercomputer hardware. S	automatic parallelization;computer architecture simulator;earth simulator;high performance fortran;library (computing);message passing interface;microsoft outlook for mac;nec sx-8;openmp;parallel language;programming tool;simulation;supercomputer;vector processor	Minyi Guo	2005		10.1109/ICITA.2005.170	computational science;computer architecture;parallel computing;embarrassingly parallel;computer science;message passing interface;massively parallel;data-intensive computing;parallel programming model	HPC	-8.358262762955597	39.86920448711445	2561
1045894d4b74a0327c2335e4df9bfb015cc2f1e6	fast rerouting for ip multicast under single node failures	multicast networks ip multicast fast rerouting single node failures multicast protection trees instantaneous failure recovery multicast traffic protocol single node failure recovery;topology;routing;trees mathematics ip networks multicast protocols telecommunication network routing telecommunication traffic;network topology;routing unicast topology network topology multicast protocols arpanet;multicast protocols;arpanet;unicast	In this paper, we propose multicast protection trees that provide instantaneous failure recovery from single node failures. For a given node v, the multicast protection tree spans all the neighbors v and does not include v. Thus, when node v fails, its neighbors are connected through the multicast protection tree instead of node v, and the neighbors of node v forward the traffic over this tree. The multicast protection trees are constructed a priori, without the knowledge of the multicast traffic in the network. This facilitates protocol independent single node failure recovery in multicast networks. These trees are used when a new multicast tree is being formed after a node failure has occurred. We analyze the effectiveness of the proposed fast rerouting technique using three practical networks.	multicast;performance evaluation;routing table	Aditya Sundarrajan;Srinivasan Ramasubramanian	2013	2013 IEEE Global Communications Conference (GLOBECOM)	10.1109/GLOCOM.2013.6831381	routing;real-time computing;multicast;ip multicast;inter-domain;reliable multicast;protocol independent multicast;computer science;pragmatic general multicast;internet group management protocol;distributed computing;distance vector multicast routing protocol;source-specific multicast;multimedia broadcast multicast service;network topology;xcast;computer network;multicast address;unicast	Metrics	-6.9992988314576765	80.48857595163496	2562
99edf455808297c534c7628b99294db8d53d68a6	energy and service level agreement aware resource allocation heuristics for cloud data centers				K. Sutha;G. M. Kadhar Nawaz	2018	TIIS	10.3837/tiis.2018.11.011		Metrics	-21.20761781566714	62.97368449808241	2563
2410f0707b8e8a6657e05b4fce3465c09fe09397	software on the edge	construction process;formal specification;formal methods;satisfiability;formal method;software quality software reliability formal specification safety critical software;engines computer crashes control systems aircraft propulsion formal specifications laboratories vibration control us department of energy software safety application software;safety critical software;software component;systems failure;formal specification safety critical applications systems failure formal methods software reliability software component;safety critical applications;software reliability;software quality	As our society becomes more technologically complex, computers (and the software that they run) are being used in a potentially alarming number of high consequence safety-critical applications. When these systems fail, the outcome can be devastating. For example, several years ago a British Midland 737 aircraft crashed killing 47 people and seriously injuring 74 others. The reason for the crash was due to a failure in the engine control system. The control system monitors the engines of the aircraft and if the vibration of an engine falls in a particular range, the engine is deemed faulty and is shut down. Unfortunately, on this particular flight the vibration of a faulty engine exceeded the range checked for in the control system. Thus the corresponding sensor sensed a value that was not a “believable value”, and therefore the faulty engine was not shut down. To make matters worse, the vibration of the faulty engine traveled through the frame of the aircraft and was picked up by the vibration sensor of the functioning engine. This second vibration sensor concluded that the functioning engine was faulty and shut the engine down-resulting in the crash. A post mortuum analysis determined the probable cause of the crash to be “inadequate parameterization of the software-based control system [4]“. Because of failures such as the one described above, many people believe that “the software industry remains years behind, perhaps decades short of the mature engineering discipline needed to meet the demands of an information-age society[2]“. In spite of this, our society’s dependence on software has been growing dramatically. “The amount of code in most consumer products is doubling every two years...televisions may contain up to 500 kilobytes of software; an electric shaver, two kilobytes, the power trains in new General Motors cars run 30,000 lines of computer code[2]“.	algorithm;computer;control system;crash (computing);kilobyte;period-doubling bifurcation;razor;software industry;television	Victor L. Winter	1996		10.1109/HASE.1996.618565	reliability engineering;medical software;software requirements specification;verification and validation;formal methods;software sizing;software verification;computer science;systems engineering;package development process;backporting;social software engineering;software reliability testing;software development;software engineering;software construction;formal specification;software walkthrough;programming language;software deployment;software quality;software metric;software system;avionics software;software peer review	OS	-68.90839980847583	27.764173311813074	2577
5e39359b57f827840121ccdb99c1a56caa40efd7	a flexible approach for the design of a multimodal terminal simulation model.		We present a decision support tool for the analysis and redesign of railroad intermodal terminals. It is a simulation-based platform which provides indicators of service level, resource use and productivity linked to the terminal operations and infrastructures. The platform is composed of two basic elements: a simulation model implemented in a commercial simulator, Witness®, and a configuration interface implemented in MS Excel®. The combination of these components has facilitated building a very flexible and easy-to-use platform so that a user without any knowledge of the simulation software will be able to implement a simulation model of the terminal, by only filling in the required data through an MS Excel file. As the design of the platform has focused on flexibility, a wide range of terminals can be implemented, in terms of infrastructures, handling material, lay out and train and demand patterns. We also present a case study to demonstrate the use of the platform.	decision support system;function overloading;multimodal interaction;network model;simulation software	Alicia García;Isabel García	2009		10.7148/2009-0281-0287	simulation	Mobile	-35.30954008210045	26.01019195376971	2578
97cc21bb35e866c44bf237785108b7cf9c477cb7	interconnection of fddi lans	inter computer communication;protocols;network design;next generation network;network segments;protocols fddi lan interconnection ansi standards;ansi standards;traffic control;lan interconnection fddi local area networks next generation networking asynchronous transfer mode bandwidth telecommunication traffic traffic control hdtv integrated circuit interconnections;lan interconnection;scaleability;telecommunication traffic;fddi;integrated circuit interconnections;hdtv;fddi lan;next generation;100 mbit s lan interconnection fddi lan local area networks inter computer communication bandwidth architectural trade offs lan architecture network segments network architecture fddi protocols network design ansi fddi follow on standard scaleability;bandwidth;network architecture;fddi follow on standard;lan architecture;fddi protocols;ansi;next generation networking;architectural trade offs;local area networks;local area network;100 mbit s;asynchronous transfer mode	Local area networks (LANs) have emerged as the technology of choice for inter-computer communication. As computers get faster, so must the LAN environment that supports them. Today there are several successful competing LANs in the 100 Mbps arena, and we are now starting to look forward to the next generation of network to take over when the current generation cannot provide the necessary bandwidth and connectivity. This paper will discuss some of the architectural trade-offs available to designers of the next generation LAN, and proposes an architecture which provides for convenient interconnection of multiple FDDI networks (and network segments). The underlying network architecture is based on FDDI protocols, an attractive feature for those who have invested into FDDI. Scaleability appears to be a required feature for any future network design and is addressed in detail. The ultimate goal of this research is to present an alternative next generation networking solution that could be adopted by ANSI as the FDDI follow on standard.		Bernhard Albert;Anura P. Jayasumana	1994		10.1109/LCN.1994.386614	local area network;real-time computing;next-generation network;telecommunications;computer science;computer network	Arch	-12.527655718204302	92.41402889113596	2579
9e8e931ddc0b8f423c3d1544ddd73de08f54d5ab	design pattern detection based on the graph theory		Design patterns are strategies for solving commonly occurring problems within a given context in software design. In the process of re-engineering, detection of design pattern instances from source codes can play a major role in understanding large and complex software systems. However, detecting design pattern instances is not always a straightforward task. In this paper, based on the graph theory, a new design pattern detection method is presented. The proposed detection process is subdivided into two sequential phases. In the first phase, we concern both the semantics and the syntax of the structural signature of patterns. To do so, the system under study and the patterns asked to be detected, are transformed into semantic graphs. Now, the initial problem is converted into the problem of finding matches in the system graph for the pattern graph. To reduce the exploration space, based on a predetermined set of criteria, the system graph is broken into the possible subsystem graphs. After applying a semantic matching algorithm and obtaining the candidate instances, by analyzing the behavioral signature of the patterns, in the second phase, final matches will be obtained. The performance of the suggested technique is evaluated on three open source systems regarding precision and recall metrics. The results demonstrate the high efficiency and accuracy of the proposed method.	algorithm;code;graph theory;open-source software;pattern recognition;precision and recall;semantic matching;sensor;software design pattern;software system	B. Bafandeh Mayvan;Abbas Rasoolzadegan Barforoush	2017	Knowl.-Based Syst.	10.1016/j.knosys.2017.01.007	null model;computer science;artificial intelligence;theoretical computer science;machine learning;data mining;critical graph;intersection graph	SE	-56.81447109156004	35.25579731458981	2582
290378a296a20e38a55432bc453fb9907f1b61dc	cloud application monitoring: the mosaic approach	virtual machine;storage system;java programming;resource allocation;virtual machining;cloud;mosaic;mosaic cloud cloud monitoring sla;custom monitoring systems cloud provider independent cloud application monitoring cloud computing cloud resource management storage systems cloud paradigm programming models mosaic api mosaic framework mosaic monitoring components;programming model;connectors;cloud monitoring;monitoring system;sla;monitoring;application program interfaces;monitoring connectors cloud computing buildings virtual machining java programming;open systems;programming;buildings;cloud computing;resource allocation application program interfaces cloud computing open systems;java	Cloud computing delegates the management of any kind of resources, such as the computing environment or storage systems for example, to the network. The wide-spread permeation of the cloud paradigm implies the need of new programming models that are able to utilize such new features. Once the problem of enabling developers to manage cloud resources in a clear and flexible way is solved, a new problem emerges: the monitoring of the quality of the acquired resources and of the services offered to final users. As the first step, the mOSAIC API and framework aim at offering a solution for the development of interoperable, portable and cloud-provider independent cloud applications. As the second step, this paper introduces the mOSAIC monitoring components that facilitate the building of custom monitoring systems for cloud applications using the mOSAIC API.	application programming interface;cloud computing;emergence;interoperability;ncsa mosaic;programming model;programming paradigm;software as a service	Massimiliano Rak;Salvatore Venticinque;Tamás Máhr;Gorka Echevarria;Gorka Esnal	2011	2011 IEEE Third International Conference on Cloud Computing Technology and Science	10.1109/CloudCom.2011.117	real-time computing;cloud computing;computer science;operating system;world wide web	HPC	-30.835515390269215	55.59088310891429	2591
9234d1243ce4901c68ad517f08dafdb81e8725dd	an automated analysis methodology to detect inconsistencies in web services with wsdl interfaces	wsdl;web services;validation;dynamic analysis	Web Service Definition Language (WSDL) is being increasingly used to specify web service interfaces. Specifications of this type, however, are often incomplete or imprecise, which can create difficulties for client developers who rely on the WSDL files. To address this problem a semi-automated methodology that probes a web service with semi-automatically generated inputs and analyzes the resulting outputs is presented. The results of the analysis are compared to the original WSDL file and differences between the observed behavior of the service and the WSDL specifications are reported to the user. The methodology is applied in two case studies involving two popular commercial (Amazon and eBay) web services. The results show that the methodology can scale, and that it can uncover problems in the WSDL files that may impact a large number of clients.	executable;semiconductor industry;web services description language;web service	Marc Fisher;Sebastian G. Elbaum;Gregg Rothermel	2013	Softw. Test., Verif. Reliab.	10.1002/stvr.451	web service;java api for xml-based rpc;computer science;data mining;database;dynamic program analysis;programming language;ws-i basic profile;world wide web;devices profile for web services	Web+IR	-57.37722798809898	41.799776835678195	2595
7b4224967dfb94d5e6b79b6bb06e313b458996d7	internet voice playout scheduling for integrated delay and error concealment with time-scale modification	perceived quality;time scale;propagation losses;error concealment;decoding;real time;forward error correction propagation losses delay internet bandwidth decoding gold;performance improvement;gold;forward error correction;internet;error control;bandwidth;packet delay;delay jitter	Packet delay and loss are two essential problems to real-time voice transmission over the Internet. Some effort has been involved in packet-level delay jitter concealment and error control. To minimize the impact of packet delay jitter dropping at the receiver, a packet-based Synchronized OverLap-and-Add (SOLA) algorithm was proposed in [3] to play out packets adaptively. By employing the forward error correction (FEC) for network loss recovery, this paper further extends the applicability of audio time-scale modification to an integrated delay/error concealment Performance improvement of the proposed adaptive playout is provided by comparing with the silenced-based, FEC-integrated adaptive playout. Overhead of the proposed concealment is discussed in terms of delay and complexity. We also investigate the impact of stretching-ratio transition on the perceived quality.	algorithm;error concealment;error detection and correction;forward error correction;internet;network packet;packet delay variation;playout;real-time transcription;scheduling (computing)	Fang Liu;JongWon Kim;C.-C. Jay Kuo	2002	2002 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2002.5745207	gold;real-time computing;the internet;telecommunications;computer science;processing delay;end-to-end delay;mathematics;forward error correction;transmission delay;bandwidth;statistics;computer network	Embedded	-5.948373181158241	98.86882369644775	2596
b4c5df99fbfb73ca91d1dea204f84add621acce2	pragmatische aspekte der informationsverarbeitung aus der sicht der informatik				Erich J. Neuhold	1990			biophysics;computer science;nuclear magnetic resonance	OS	-101.22447805248565	22.119506165345896	2604
545c612e876c4865ebd3eb9d0121133431a2754e	complexity and expressiveness of shex for rdf	rdf schema graph topology validation complexity expressiveness;004	We study the expressiveness and complexity of Shape Expression Schema (ShEx), a novel schema formalism for RDF currently under development by W3C. A ShEx assigns types to the nodes of an RDF graph and allows to constrain the admissible neighborhoods of nodes of a given type with regular bag expressions (RBEs). We formalize and investigate two alternative semantics, multiand single-type, depending on whether or not a node may have more than one type. We study the expressive power of ShEx and study the complexity of the validation problem. We show that the single-type semantics is strictly more expressive than the multi-type semantics, single-type validation is generally intractable and multi-type validation is feasible for a small (yet practical) subclass of RBEs. To curb the high computational complexity of validation, we propose a natural notion of determinism and show that multi-type validation for the class of deterministic schemas using single-occurrence regular bag expressions (SORBEs) is tractable. 1998 ACM Subject Classification H.2.2 Schema and subschema	cobham's thesis;computational complexity theory;expressive power (computer science);regular expression;resource description framework;semantics (computer science)	Slawomir Staworko;Iovka Boneva;José Emilio Labra Gayo;Samuel Hym;Eric Prud'hommeaux;Harold R. Solbrig	2015		10.4230/LIPIcs.ICDT.2015.195	discrete mathematics;computer science;theoretical computer science;database;mathematics;algorithm	DB	-17.431834637132237	12.654785736821974	2607
ef89120709c590aa8144a9d8852479c920bdbd2b	konzeption und implementierung eines planungssystems für schnittstellentests		Bei der Erstellung von komponentenbasierten, betrieblichen Informationssystemen ist es eine Aufgabe der Qualitätssicherung sicherzustellen, dass die Schnittstellen zwischen den einzelnen Komponenten keine Fehler aufweisen. Der vorliegende Beitrag beschreibt die Konzeption und Umsetzung eines Entscheidungsunterstützungssystems (EUS), welches die Qualitätssicherungsabteilung eines Softwareherstellers bei der Aufstellung eines Zeitplans für die zentrale Durchführung von Schnittstellentests unterstützt. Wichtigstes Element des EUS ist ein metaheuristikbasiertes Planungssystem, welches dem Entscheider qualitativ gute Zeitpläne vorschlägt. Das Planungssystem berücksichtigt sowohl unterschiedliche Planungsziele, als auch relevante Problemrestriktionen durch eine geeignete Wahl der Problemkodierung, einer geschickten Erzeugung des Zeitplans aus der Problemkodierung sowie einer geeigneten Initialisierung.	eine and zwei;gesellschaft für informatik;institut für dokumentologie und editorik	Franz Rothlauf	2007			knowledge management;library science;computer science	OS	-103.43768233594182	33.37152645639364	2615
9efa269591438959b4557976595c37cd7f05548a	elementary propositions and independence		This paper is concerned with Wittgenstein’s early doctrine of the independence of elementary propositions. Using the notion of a free generator for a logical calculus—a concept we claim was anticipated by Wittgenstein— we show precisely why certain difficulties associated with his doctrine cannot be overcome. We then show that Russell’s version of logical atomism—with independent particulars instead of elementary propositions—avoids the same difficulties. We intend to discuss a basic notion of logical atomism: that of independence. Elementary propositions are clearly central to Wittgenstein’s atomism; their characterization rests on the doctrine of their independence. And a central tenet of Russell’s atomism is that the world is decomposable into independent components. Our aim is to elucidate these two uses of independence and the conceptions of elementary proposition to which they give rise. 1 Elementary propositions as free generators One of the central and most problematic concepts in the Tractatus is that of ‘elementarsatz’ or elementary proposition. By this is meant a proposition in some sense not further analyzable into simpler propositions. Of this concept it is asserted in the Tractatus: 5.3 Every proposition is the result of truth operations on elementary propositions. 1 That is, elementary propositions are to be regarded as the ultimate propositional constituents or generators of propositions: in this respect elementary propositions are propositional atoms, or atomic propositions. We also find the following further assertions concerning elementary propositions: 4.211 It is a sign of a proposition’s being elementary that there can be no elementary proposition contradicting it. 5.134 One elementary proposition cannot be deduced from another. Received August 17, 1995; revised February 19, 1996 ELEMENTARY PROPOSITIONS 113 Wemay take these as asserting that elementary propositions are (logically) independent of one another. Now on what grounds can it be claimed that atomic propositions, in the sense of generators, satisfy this independence condition? To clarify this question, let us examine some familiar logical systems. Consider first thepropositional calculus. Here propositions are built up by applying the logical operators to an initial stock of proposition letters which are in another, completely natural sense, atomic propositions, since they are of minimal length. They are also independent in the strong sense that, for any finite set {P1, . . . , Pn} of them, no conjunction of the formX1 & · · · & Xn is ever inconsistent, where eachXi is eitherPi or ¬Pi. This situation has a more general algebraic description. If, following the suggestion of 5.141, we identify (provably) equivalent propositions of the propositional calculus, we obtain aBoolean algebra—the Lindenbaum-Tarski algebra associated with the propositional calculus. A subset X of a Boolean algebraB is said to be free, and its elements independent, if for any finite subset {x1, . . . , xn} of X, we have y1 ∩ · · · ∩ yn = 0 where eachyi is eitherxi or x∗ i (= the Boolean complement of xi). X is said togenerate B if B is the least subalgebra of B containingX, in other words, if every element ofB can be expressed in the form y1 ∪ · · · ∪ yn , where eachyi is of the formz1 ∩ · · · ∩ zm with zi ∈ X or zi ∈ X.2 Finally B is said to befreely generated if it has a free set of generators. Now the Boolean algebra PROPassociated with the propositional calculus is freely generated, and the propositional letters constitute a free set of generators. Thus, the propositional calculus provides a perfectly good, if restricted, model of Wittgenstein’s scheme of elementary propositions, one that well illustrates his notion of elementarity, and the two notions of atomicity, introduced above. But Wittgenstein of course worked with a much richer language than that underlying the propositional calculus: in particular he required that universal and existential assertions be formulable. It is natural, therefore, to extend our discussion to the predicate calculus. We first consider the pure predicate calculus PC. We assume that the underlying language contains a set C of infinitely many constant symbols. Let PREDbe the Boolean algebra obtained by identifying equivalent sentences in PC. PREDhas a certain additional structure obtained by noting that quantified sentences are the suprema and infima of certain subsets, viz., ∃xφ(x) = sup{φ(c) : c ∈ C} ∀xφ(x) = inf{φ(c) : c ∈ C}. For this reason we shall call PREDaquantifier algebra.3 This is, of course, analogous to Wittgenstein’s discussion of quantification. Wittgenstein’s account differs from the one we have given in terms of the notion of a quantifier algebra only in the choice of truth-function with respect to which he defines the quantifiers 4; Wittgenstein uses a generalization of the Sheffer stroke rather than suprema and infima. Now it can be shown that the set of atomic sentences (i.e., the set of corresponding equivalence classes) is free in PRED and that it also generates it as a quantifier algebra. (For a proof of this assertion, see Rasiowa and Sikorski [12], Chapter VIII, 24.1.) Thus, in this extended sense, PREDis freely generated by the set of atomic sen114 JOHN L. BELL and WILLIAM DEMOPOULOS tences, so that we obtain a satisfactory model of Wittgenstein’s scheme, with atomic sentences again playing the role of elementary propositions (generators). It has often been noted that Wittgenstein required of his elementary propositions that they be logically independent of one another. It has also been observed that elementary propositions constitute the atomic constituents of more complicated propositions. What appears to have been missed, so far as we can determine, is the significance of the combination of these two ideas, namely, that together they constitute an anticipation of the notion of a free generator. 5 Thus, from our point of view it is not hard to see why Wittgenstein found his characterization of elementary propositions an attractive one; we can also give a complete answer to a question posed by David Pears: “It is a sign of a proposition’s being elementary that there can be no elementary proposition contradicting it.” ( Tractatus, 4.211) Why did Wittgenstein require the elementary propositions of the Tractatus to pass this difficult test? ([10], p. 74) On our account, Wittgenstein had, in effect, hit upon the idea of a free generator and had correctly noted, in the applications he made of the notion to the propositional calculus and to the pure predicate calculus, its connection with independence and with the notion of atomicity which we associate with minimal size; he also perceived its central role in these two logical systems. It is often remarked that the chief mathematical contribution of theTractatus was Wittgenstein’s presentation of the method of truth-tables. It seems to us that the articulation of the notion of a free generator together with the recognition of its centrality in the systems PROPandPRED was a contribution of no less importance. 2 Free generators and the Grundgedanke of the Tractatus Wittgenstein’s “Grundgedanke” or fundamental thought is presented at 4.0312: My fundamental thought is that the “logical constants” do not represent. That thelogic of the facts cannot be represented. Negation is the logical constant with respect to which the ‘fundamental thought’ is most fully elaborated in the Tractatus. In order to better see Wittgenstein’s point in connection with negation, Ramsey ([11], pp. 146f) suggested that we should imagine the negation of an elementary proposition to be symbolically represented by inverting the sentence expressing it. His idea appears to have been that this would remove the temptation to think that in¬p the negation sign introduces an additional representational element not already present in p, apoint made explicitly—or, at least, as explicitly as any point is ever made in the Tractatus—at 4.0621 (paragraph 3): The propositionsp and¬p have opposite sense, but there corresponds to them one and the same reality. In particular this would remove any basis for supposing that there must be negative facts in anything like the sense in which Russell appears to have supposed there to be negative facts, namely, as states of affairs existing alongside atomic facts. 6 It seems to us that Ramsey’s observation as well as other aspects of the fundamental thought are naturally captured by our account of elementary propositions as ELEMENTARY PROPOSITIONS 115 free generators. Let E be the set of elementary propositions and B the Boolean algebra freely generated by E, so that B is the set of all propositions. We then have an injectioni: E → B such thati[ E] is afree set of generators of B. Weusually identify p with i(p) for p ∈ E. But this can cause problems since the map i is not uniquely determined by E. For example, suppose given any such i, call it i0 , and any subset X of E; defineix : E → B by ix(p) = i0(p) for p ∈ X, andix(p) = i0(p)∗ for p ∈ E − X. Thenix : E → B is an injection andix[ E] isalso a free set of generators of B.7 Once elementary propositions are seen to be the atomic generators of all other propositions—once it is seen, in other words, that all propositions are truth functions of the elementary propositions—it remains to be observed only that the fundamental thought holds of negation, since the latitude allowed by i extends only to the possibility of assigningi(p)∗ to an elementary proposition p. If the injection ofE into B is given byiX , wemay think of the elementary propositions inX as being “given positively,” and those not in X, “given negatively.” But sinceX is arbitrary, so is the positive/negative distinction. That is, the members of E are “without orientation” (neither positive nor negative). They acquire their orientation only after insertion intoB. Wittgenstein’s pre-Tractarian notion that propositions are bipolar thus corresponds to th	assertion (software development);atomic sentence;atomicity (database systems);biconnected component;binary prefix;boolean algebra;centrality;elementary;first-order logic;formal system;linear algebra;logical connective;logical constant;mv-algebra;point of view (computer hardware company);propositional calculus;quantifier (logic);sequent calculus;sheffer stroke;turing completeness;universal quantification;viz: the computer game	John L. Bell;William Demopoulos	1996	Notre Dame Journal of Formal Logic	10.1305/ndjfl/1040067320	philosophy;epistemology;mathematics;logical atomism;algorithm	Theory	-9.361037279864767	8.673400209703583	2616
664c3abc53ae6ec19ace311f854e6e2dce8122fe	it infrastructures in manufacturing: insights from seven case studies	production data acquisition;architecture;manufacturing execution systems	IT solutions in manufacturing support the execution as well as the monitoring of production operations. Fast reaction to exceptions, detailed documentation of operations, and the detection of inefficiencies in the production are among the benefits of a tight IT integration of shop-floor processes. Several dedicated software solutions and standards exist for the manufacturing domain. However, each manufacturer must tailor the IT to the special requirements of its processes and infrastructure. We found that real-world installations show considerable variations. In this paper we present the results of seven case studies on IT infrastructures in manufacturing. For each case we portray the employed architecture and the main factor that influenced the design. From this analysis we derive reoccurring patterns on the structure of IT solutions in manufacturing and relate them to existing standards. Our results provide system architects with guidance for picking the right architectural choices in different manufacturing environments.	documentation;exception handling;requirement	Oliver Günther;Lenka Ivantysynova;Jochen Rode;Holger Ziekow	2009			manufacturing execution system;integrated computer-aided manufacturing;process development execution system;systems engineering;engineering;operations management;computer-integrated manufacturing;manufacturing engineering	SE	-60.60722942033069	20.509480087704773	2635
7514387662d41afb9addd4b64e7e0812970b82fa	parallelism viewpoint: a viewpoint to model parallelism in parallelism-intensive software systems	parallelism intensive electron microscope software system;parallelism intensive software systems;architecture viewpoint;software maintenance;software architecture architecture viewpoint software performance multithreading;software systems;parallel programming;software performance;parallelism intensive electron microscope software system parallelism intensive software systems legacy systems;software maintenance parallel programming;computer architecture;software architecture;electron microscopy;electron microscope;legacy systems;parallel processing instruction sets computer architecture software systems electron microscopy;legacy system;parallel processing;instruction sets;multithreading	The use of parallelism enhances the performance of a software system. Its excessive use, however, can degrade the performance. In this paper we propose a parallelism viewpoint to optimize the use of parallelism by eliminating unnecessarily used threads in legacy systems. The viewpoint describes the parallelism behaviour of the system, which can be used to analyze for overheads associated with threads. We illustrate the proposed viewpoint with the help of an industrial case, a parallelism-intensive electron microscope software system. We use the viewpoint to analyze threads suitable to be replaced with a small sized thread pool in this system. Results show that the viewpoint provides a profound insight into the thread-model of the system that is required to reduce the parallelism. In the thread pool analysis, we found that more than 50% threads are underused. They were replaceable with a pool of approximately 11% of these threads.	context switch;electron;iso/iec 42010;information harvesting;job stream;legacy system;metamodeling;multi-core processor;network switch;parallel computing;software system;thread (computing);thread pool;threaded code;tracing (software);unbalanced circuit;viewpoint	Naeem Muhammad;Nelis Boucké;Yolande Berbers	2011	2011 16th IEEE International Conference on Engineering of Complex Computer Systems	10.1109/ICECCS.2011.35	parallel processing;computer architecture;parallel computing;real-time computing;computer science;operating system;software engineering;data parallelism;instruction-level parallelism;legacy system;scalable parallelism;implicit parallelism;electron microscope;task parallelism	SE	-7.989864418788516	46.87702645567799	2640
fd1f6fb9e7d2583f6ab143bf974c94e3c87c89e5	proposição e avaliação de um modelo de transmissão de conhecimento coerente com comportamentos observados			unified model	Luciene Cristina Alves Rinaldi	2014				Crypto	-106.41547139869158	18.03788228839804	2643
3c7930ad8f3d6586ccb82a06ca7f0e811e773a7c	secure and privacy-preserving execution model for data services	privacy preservation;data services;rdf views	Data services have almost become a standard way for data publishing and sharing on top of the Web. In this paper, we present a secure and privacy-preserving execution model for data services. Our model controls the information returned during service execution based on the identity of the data consumer and the purpose of the invocation. We implemented and evaluated the proposed model in the healthcare application domain. The obtained results are promising.	application domain;autonomous robot;black box;information privacy;privacy policy;world wide web	Mahmoud Barhamgi;Djamal Benslimane;Said Oulmakhzoune;Nora Cuppens-Boulahia;Frédéric Cuppens;Michael Mrissa;Hajer Taktak	2013		10.1007/978-3-642-38709-8_3	computer science;database;services computing;internet privacy;data as a service;world wide web	Mobile	-42.88098564617278	61.48523553629302	2645
917306c0ed620f5551d0348ae21f8d9818a91573	internet telephony gateway location	telephone networks;telephone networks internet internetworking open systems transport protocols telecommunication network routing distributed databases wide area networks;internet telephony;transport protocols;internet;telecommunication network routing;distributed databases internet telephony gateway location nonreal time data traffic voice video interoperability telephone network brokered multicast advertisements public switched telephone network pstn ip host ip address protocol architectures hierarchical databases multicast advertisement routing protocols centralized databases wide area network;public switched telephone network;distributed databases;internetworking;internet telephony multicast protocols ip networks telecommunication traffic routing protocols databases application software user interfaces wide area networks costs;ip networks;routing protocol;open systems;non real time;wide area network;wide area networks	Although the Internet was designed to handle non-real time data traffic, it is being used increasingly to carry voice and video. One important class of contributors to this growth are Internet telephones. Critical to more widespread use of Internet telephony is smooth interoperability with the existing telephone network. This interoperability comes through the use of Internet Telephony Gateway’s (ITG’s) which perform protocol translation between an IP network and the Public Switched Telephone Network (PSTN). In order for an IP host to call a user on the PSTN, the IP host must know the IP address of an appropriate gateway. We consider here the problem of finding these gateways. An analysis of a number of protocol architectures is presented, including hierarchical databases, multicast advertisement, routing protocols, and centralized databases. We propose a new protocol architecture, called Brokered Multicast Advertisements (BMA) which serves as a lightweight, scalable mechanism for locating ITG’s. The BMA architecure is general, and can be applied to location of any service across a wide area network.	bernhard schölkopf;block-matching algorithm;border gateway protocol;centralized computing;database;erp;goto;internet protocol suite;interoperability;multicast;routing;scalability;television;web search engine;x.500	Jonathan D. Rosenberg;Henning Schulzrinne	1998		10.1109/INFCOM.1998.665066	internet protocol;reserved ip addresses;tier 1 network;the internet;next-generation network;inter-domain;telephone network;internet layer;telecommunications;ip address management;computer science;internet group management protocol;softswitch;voice over ip;network address translation;internet protocol suite;routing protocol;open system;ip tunnel;telephony;world wide web;internet connection sharing;distributed database;transport layer;computer network	Networks	-16.022072493020293	91.66099699197014	2646
4d14e8e195664639251de6f490b947cab05bc4df	qos optimized and energy efficient power control for deep space multimedia communications in interplanetary networks	internet;space communication links;energy consumption;multimedia communication;multimedia data transmission qos optimisation energy efficient power control deep space multimedia communications interplanetary networks ipn internet energy consumption budget transmission power multimedia data uep adaptive power control approach;quality of service;multimedia communication streaming media power control energy consumption optimization quality of service data communication;space communication links energy consumption internet multimedia communication quality of service	In this paper, we propose a QoS optimized power control approach to achieve an optimum multimedia communication quality over the InterPlaNetary (IPN) Internet with a strict energy consumption budget. The transmission power to send each frame is optimized by jointly considering the unequal distortion reduction gain of the multimedia data, energy consumption budget, and overall multimedia communication quality. Simulation results show that the proposed Unequal Error Protection (UEP) adaptive power control approach improves multimedia data transmission quality and achieves the energy utilization efficiency in energy-constrained IPNs.	distortion;interplanet;quality of service;simulation	Chunqiu Wang;Wei Wang;Kazem Sohraby;James Dudek	2013	IEEE International Conference on Wireless for Space and Extreme Environments	10.1109/WiSEE.2013.6737545	real-time computing;telecommunications;computer science;computer network	EDA	-5.740914517900279	101.52335328579156	2651
dfd80a868acdb3fe185b79888631ef595b38619a	möglichkeiten der beschleunigung einer zentraleinheit durch strukturelle maßnahmen				Joachim Swoboda	1973	Elektronische Rechenanlagen	10.1524/itit.1973.15.16.60	computer science;embedded system	Vision	-99.14101883458859	25.58024316554687	2656
b2622be75ff33ab65038e43555ee1c1a99d06644	module reuse by interface adaptation	developpement logiciel;adaptacion;software libraries;reutilizacion;ingenieria logiciel;data type;software engineering;reuse;interfase;desarrollo logicial;adaptation;software development;interface;module interconnection language;genie logiciel;software reuse;reutilisation	This paper describes a language called Nimble that allows designers to declare how the actual parameters in a procedure call are to be transformed at run time. Normally, programmers must edit an application’s source in order to adapt it for reuse in some new context where the interfaces fail to match exactly (e.g. the parameters may appear in a different order, data types may not exactly match, and some data may need to be either initialized or masked out when the reusable module is integrated within a new application.) But Nimble allows programmers to adapt the interfaces of existing software without having to operate on the source manually. As a result, existing software may be easily reused in a broader range of applications, and software libraries do not need to store many variants of a component that differ only in how the interfaces are used. Nimble has been implemented on a variety of Unix hosts, and is part of a broader reuse project at the University of Maryland. Our current system is suitable for use either in conjunction with existing module interconnection languages, or stand-alone with C, Pascal and Ada source programs.	ada;bsd;interconnection;library (computing);marvin (robot);multiprocessing;pascal;programmer;run time (program lifecycle phase);subroutine;unix;victor animatograph corporation;victor basili	James M. Purtilo;Joanne M. Atlee	1991	Softw., Pract. Exper.	10.1002/spe.4380210602	data type;computer science;engineering;software development;operating system;interface;reuse;database;programming language;adaptation	SE	-26.710547728139648	38.28677331029172	2657
89eac858def951bbbf4ae99bc4bfdd85d85aeab5	using high performance algorithms for the hybrid simulation of disease dynamics on cpu and gpu		In the current work the authors present several approaches to the high performance simulation of human diseases propagation using hybrid two-component imitational models. The models under study were created by coupling compartmental and discrete-event submodels. The former is responsible for the simulation of the demographic processes in a population while the latter deals with a disease progression for a certain individual. The number and type of components used in a model may vary depending on the research aims and data availability. The introduced high performance approaches are based on batch random number generation, distribution of simulation runs and the calculations on graphical processor units. The emphasis was made on the possibility to use the approaches for various model types without considerable code refactoring for every particular model. The speedup gained was measured on simulation programs written in C++ and MATLAB for the models of HIV and tuberculosis spread and the models of tumor screening for the prevention of colorectal cancer. The benefits and drawbacks of the described approaches along with the future directions of their development are discussed.	algorithm;c++;central processing unit;code refactoring;color gradient;computation;graphical user interface;graphics processing unit;high-level programming language;interconnection;level of detail;matlab;mathematical optimization;performance prediction;personal computer;random number generation;refinement (computing);simulation;social system;software propagation;source lines of code;speedup;stored procedure;supercomputer;universality probability	Vasiliy N. Leonenko;Nikolai V. Pertsev;Marc Artzrouni	2015		10.1016/j.procs.2015.05.214	computer architecture;parallel computing	HPC	-10.805590037469726	38.77723191138712	2667
ac05286d707484ef37987860c9155b93385d2b8c	provenance-aware query optimization	databases;instruments;semantics;algebra;pipelines;optimization;encoding	Data provenance is essential for debugging query results, auditing data in cloud environments, and explaining outputs of Big Data analytics. A well-established technique is to represent provenance as annotations on data and to instrument queries to propagate these annotations to produce results annotated with provenance. However, even sophisticated optimizers are often incapable of producing efficient execution plans for instrumented queries, because of their inherent complexity and unusual structure. Thus, while instrumentation enables provenance support for databases without requiring any modification to the DBMS, the performance of this approach is far from optimal. In this work, we develop provenancespecific optimizations to address this problem. Specifically, we introduce algebraic equivalences targeted at instrumented queries and discuss alternative, equivalent ways of instrumenting a query for provenance capture. Furthermore, we present an extensible heuristic and cost-based optimization (CBO) framework that governs the application of these optimizations and implement this framework in our GProM provenance system. Our CBO is agnostic to the plan space shape, uses a DBMS for cost estimation, and enables retrofitting of optimization choices into existing code by adding a few LOC. Our experiments confirm that these optimizations are highly effective, often improving performance by several orders of magnitude for diverse provenance tasks.	approximation algorithm;computer performance;database;debugging;experiment;heuristic;information retrieval;instrumentation (computer programming);linear algebra;mathematical optimization;pipeline (computing);query optimization;sql;source lines of code	Xing Niu;Raghav Kapoor;Boris Glavic;Dieter Gawlick;Zhen Hua Liu;Venkatesh Radhakrishnan	2017	2017 IEEE 33rd International Conference on Data Engineering (ICDE)	10.1109/ICDE.2017.104	computer science;theoretical computer science;data mining;database;semantics;pipeline transport;programming language;encoding	DB	-27.288662643943617	15.681370923818678	2676
350a6eb9e3750c1c466b6928ea84e6f8f21ab488	sancomsim: a scalable, adaptive and non-intrusive framework to optimize performance in computational science applications	informatica	Parallel processing has become the most common solution for developing and executing scientific computing applications. Actually, the best way to obtain good performance ratios is to exploit parallelism in both processing and communications. Although the study of computational performance has historically involved CPU power, currently the CPU is not the only concern in the overall performance. Due to the underlying design of parallel applications, communication networks play a very important role in the field of computational science. Despite the fact that networks used in multicore clusters are fast and have low latency, the amount of transferred data may cause a bottleneck in the communication system, as communicationintensive, parallel applications spend a significant amount of their total execution time exchanging data between processes. Moreover, in most cases, several users are executing different parallel applications at the same time in the cluster. In this paper we present SANComSim, a Scalable, Adaptive and Non-intrusive framework, based on simulation techniques, for optimizing the performance of the network system to execute complex applications. The main objective of this framework is to apply run-time compression, to reduce the data sent through the network, in order to increase the overall system performance. The main features of SANComSim are: adaptability, to dynamically adapt to the current state of the system; portability, the framework is neither focused on a specific programming language nor a platform; non-intrusive, since this framework is based on simulation techniques, which does not require exclusive access of the entire cluster system; scalability, any parallel application, independently of the number of processed and computing nodes, can use this framework to improve performance in cluster systems.		Alberto Nuñez;Rosa Filgueira;Mercedes G. Merayo	2013		10.1016/j.procs.2013.05.186	parallel computing;real-time computing;computer science;operating system;data mining;distributed computing	HPC	-8.985725726400275	45.49560985929794	2688
11c60376a71f64efb1605f457dc3f34f16b1f9c5	software smart cards via cryptographic camouflage	dictionary attack software smart cards cryptographic camouflage public key cryptography private keys password;public key cryptography;smart card;authorisation;dictionary attack;smart cards public key cryptography containers protection authentication dictionaries art argon world wide web cryptographic protocols;smart cards;authorisation public key cryptography smart cards	A sensitive point in public key cryptography is how to protect the private key. We outline a method of protecting private keys using cryptographic camouflage. Specifically, we do not encrypt the private key with a password that is too long for exhaustive attack. Instead, we encrypt it so that only one password will decrypt it correctly, but many passwords will decrypt it to produce a key that looks valid enough to fool an attacker. For certain applications, this method protects a private key against dictionary attack, as a smart card does, but entirely in software.	credential;dictionary attack;digital signature;e-commerce;encryption;enterprise resource planning;extranet;firewall (computing);information system;intranet;online banking epayments;password;public-key cryptography;smart card;tom	Douglas N. Hoover;B. N. Kausik	1999		10.1109/SECPRI.1999.766915	key;smart card;pbkdf2;power analysis;key exchange;computer science;key management;key generation;password psychology;public key fingerprint;internet privacy;public-key cryptography;key derivation function;key stretching;openpgp card;key space;world wide web;key distribution;computer security;password strength	Security	-44.909611906797544	71.01566498275197	2697
700f05e65e4c41f561b40ccc6221efa2928db436	formal security analysis of near field communication using model checking	security analysis;wireless communication;near field communication;probabilistic model checking;relay attack	Near field communication (NFC) is a short-range wireless communication technology envisioned to support a large gamut of smart-device applications, such as payment and ticketing. Although two NFC devices need to be in close proximity to communicate (up to 10źcm), adversaries can use a fast and transparent communication channel to relay data and, thus, force an NFC link between two distant victims. Since relay attacks can bypass the NFC requirement for short-range communication cheaply and easily, it is important to evaluate the security of NFC applications. In this work, we present a general framework that exploits formal analysis and especially model checking as a means of verifying the resiliency of NFC protocol against relay attacks. Toward this goal, we built a continuous-time Markov chain (CTMC) model using the PRISM model checker. Firstly, we took into account NFC protocol parameters and, then, we enhanced our model with networking parameters, which include both mobile environment and security-aware characteristics. Combining NFC specifications with an adversary's characteristics, we produced the relay attack model, which is used for extracting our security analysis results. Through these results, we can explain how a relay attack could be prevented and discuss potential countermeasures.	model checking;near field communication	Nikolaos Alexiou;Stylianos Basagiannis;Sophia G. Petridou	2016	Computers & Security	10.1016/j.cose.2016.03.002	computer science;internet privacy;near field communication;security analysis;computer security;wireless;computer network	Logic	-52.4091397570125	69.91132306976564	2708
c72519f6dc96fb67bd34b283b66395c8f48471e8	dynamic team access control for collaborative internet of things		The article presents a new access control model for IoT (Internet of Things), which is based on a dynamic approach. Our aim is to change the access control design concept from a static to a dynamic model in order to fit to characteristics and features of IoT. We do so by adapting TMAC (Team Access Control) model to IoT dynamic environment. DTMAC (Dynamic Team Access Control) allows the creation of dynamic teams that are deleted when the collaborative activities are over. In addition, it offers an easy management of the teams in a decentralized manner. We implement DTMAC as a web application using a relational database management to assess its security. The assessment of DTMAC shows that it adapts well to IoT dynamic network. Moreover, the model is user-driven, flexible and scalable. It also provides fine-grained access control, supports the well-known least privileges principle and separation of duties for the team members.		Hadjer Benhadj Djilali;Djamel Tandjaoui	2018		10.1007/978-3-030-03101-5_7	world wide web;web application;scalability;access control;relational database management system;separation of duties;dynamic network analysis;internet of things;computer science	HCI	-48.212775323530565	53.06006338090987	2716
3b379f42bc44c73c740ce6e1e10285de7b6047e3	cooling aware job migration for reducing cost in cloud environment	cost efficiency;cooling aware migration;cloud computing	With the growth in computing needs, energy cost includes a large portion of operating cost of cloud data centers. Electricity prices vary in different times and geographical places. Such diversity provides opportunity for diminishing total cost via migration of jobs to places with lower energy prices. Most of the previous studies only focus on computing cost of data centers and disregard other significant parameters such as cooling cost of data centers. These approaches prefer data centers which are located in states with cheaper computing cost. Nonetheless, inappropriate workload migration may lead to a remarkable increase in the total cost because of ignoring the cooling cost of data centers. To address this challenge, we show that minimization of the total cost must cover both the computing and cooling cost while considering delay requirements of jobs. Moreover, we propose an analytical approach which captures the interaction between migration decisions and cooling cost in cloud data centers. Features that make our approach distinct from other similar approaches are the following: first, we consider that cooling cost increases in a nonlinear way with respect to the data center utilization; second, we model cooling cost without any assumption about how the data center cooling system works. In order to achieve energy saving, we determine how much workload should be migrated to other data centers and also the number of servers allocated to each data center for executing the workload. We accomplish migration of workload between data centers by utilizing variety in electricity prices in different locations and times and achieve lower total cost compared with previous schemes. Eventually, using MapReduce traces, we validate our method and indicate that remarkable cost saving, around 37 % can be obtained by cooling-aware job migration.	cloud computing;computer cooling;data center;job stream;mapreduce;nonlinear system;requirement;simulation;spatiotemporal database;system migration;tracing (software)	Elahe Naserian;Seyed Mohammad Ghoreyshi;Hosein Shafiei;P. Mousavi;Ahmad Khonsari	2014	The Journal of Supercomputing	10.1007/s11227-014-1349-9	relevant cost;parallel computing;real-time computing;simulation;cloud computing;computer science;operating system;semi-variable cost;cost centre;computer security;cost efficiency	HPC	-20.584329797480034	62.25215427458434	2718
72adca24c1ce2178c940ca48f680c1d8d814ac90	behavioural analysis of component framework with multi-valued transition system	chi buuchiautomaton;object oriented programming;formal verification;model checking;automata state space methods software systems productivity programming application software appropriate technology space technology design engineering software design;component framework;state space;distributed object management;transition systems;behavioural analysis model checking verification validation exceptional cases normal regular behaviour state space searching execution performance multivalued transition systems ejb component framework;formal verification distributed object management java object oriented programming;verification and validation;verification method;java	Model-checking is a promising technique for the verification and validation of systems. Practical systems need complicated design descriptions because they often refer to situations concerning exceptional cases in addition to normal, regular behaviour. Since properties deal with many exceptional cases, the property formulae to be verified become complicated and hard to understand. Further, as both system and property descriptions are complicated, the state space searched in the model-checking process becomes large, which affects execution performance. This paper employs a model-checking technique based on multivalued transition systems to reduce such complexities. It discusses advantages by applying the idea to an example case of the behavioural analysis of the EJB component framework.	multi-agent system;transition system	Shin Nakajima	2002		10.1109/APSEC.2002.1182991	model checking;real-time computing;verification and validation;verification;formal verification;software verification;computer science;systems engineering;state space;theoretical computer science;runtime verification;programming language;object-oriented programming;java	AI	-49.87431788986959	33.022588204900075	2727
93089faee971f65388cf6b597e5b2efeb47c3ae5	musa: a middleware for user-driven service adaptation		One of the current challenges of Service Oriented Engineering is to provide instruments for dealing with dynamic and unpredictable environments and changing user requirements. Traditional approaches based on static workflows provide little support for adapting at runtime the flow of activities. MUSA (Middleware for User-driven Service Adaptation) is a holonic multi-agent system for the self-adaptive composition and orchestration of services in a distributed environment.	algorithm;autonomous robot;cloud computing;emergence;holarchy;holon (philosophy);mash-1;middleware;multi-agent system;requirement;risk management;run time (program lifecycle phase);service composability principle;service-oriented architecture;spontaneous order;user requirements document	Massimo Cossentino;Carmelo Lodato;Salvatore Lopes;Luca Sabatucci	2015			orchestration (computing);distributed computing environment;middleware;workflow;computer science;user requirements document;distributed computing	SE	-42.01453903114138	40.471746892101685	2736
3680a6004ceb3099c0c3a51b8cbdee7e89c2d92d	ahrc: an optimized cache associativity	limiting;arrays;indexes;bandwidth;scalability;conferences;australia	Hardware resources require efficient scaling because the future of computing technology seems to be intensive multithreaded. One of the main challenges in the scalability of computers hardware is the hierarchy of the memory. Chip-multiprocessors (CMPs) rely on large and multi-level hierarchies of caches to reduce cost of resources and improve systems performance. These multi-level hierarchies are the ones, which also help to solve the issue of limited bandwidth and minimize the latency of the main memory. Almost half of the area of the chip and a large percentage of the system energy is used by caches. One of the main problems limiting the scalability of cache hierarchies is called cache associativity. Caches consume a lot of energy to implement associative lookups. This affects the performance of the system by reducing the efficiency of caches. This paper describes a new design of cache that we called - Adaptive Hashing and Replacement Cache (AHRC). This design has the ability of maintaining high associativity with an advanced method of replacement policy. AHRC can improve associativity and maintain the number of possible locations, where each block is kept as small as possible. Several workloads were simulated on a large-scale CMP with AHRC as the last-level cache. We propose an Adaptive Reuse Interval Prediction (ARIP) scheme for AHRC, which is superior to the NRU scheme that was described by Seznec. Results demonstrate that AHRC has better energy efficiency and higher performance as compared to conventional caches. Additionally, large caches that utilize AHRC are the most suitable in many core CMPs to provide a more significant improvement and scalability than the smaller caches. However, AHRC with a higher-level replacement may lead to loss of energy for workloads that are not sensitive to the policy governing the replacement process.	cpu cache;cache (computing);computer data storage;image scaling;manycore processor;multithreading (computer architecture);scalability;thread (computing)	Malik Al-Manasia;Zenon Chaczko;Asma Ounzar	2016	2016 IEEE 18th International Conference on High Performance Computing and Communications; IEEE 14th International Conference on Smart City; IEEE 2nd International Conference on Data Science and Systems (HPCC/SmartCity/DSS)	10.1109/HPCC-SmartCity-DSS.2016.0117	database index;parallel computing;real-time computing;scalability;computer science;operating system;database;distributed computing;bandwidth;limiting	HPC	-9.771811106768947	53.81008293597743	2739
26a5cdb37dfeeb710618b2509046967196d03dea	an empirical comparison between direct and indirect test result checking approaches	software testing;conference_paper;metamorphic testing;controlled experiment;system under test;fault detection;cost efficiency;cost effectiveness;empirical evaluation;test oracle;open source	An oracle on software testing is a mechanism for checking whether the system under test has behaved correctly for any executions. In some situations, oracles are unavailable or too expensive to apply. This is known as the oracle problem. It is crucial to develop techniques to address it, and metamorphic testing (MT) was one of such proposals. This paper conducts a controlled experiment to investigate the cost effectiveness of using MT by 38 testers on three open-source programs. The fault detection capability and time cost of MT are compared with the popular assertion checking method. Our results show that MT is cost-efficient and has potentials for detecting more faults than the assertion checking method.	assertion (software development);cost efficiency;fault detection and isolation;metamorphic testing;open-source software;oracle database;oracle machine;sensor;software testing;system under test	Peifeng Hu;Zhenyu Zhang;Wing Kwong Chan;T. H. Tse	2006		10.1145/1188895.1188901	reliability engineering;real-time computing;computer science;test case;algorithm	SE	-60.59054705014714	37.22598784342145	2740
14f464a776150b5e1331d5d0d29b2d65561aea71	design, optimization, and formal verification of circuit fault-tolerance techniques. (conception, optimisation, et vérification formelle de techniques de tolérance aux fautes pour circuits)			fault tolerance;formal verification;mathematical optimization;program optimization	Dmitry Burlyaev	2015				EDA	-20.33910966161719	41.25711116059058	2743
c752238736d42eb1deb984a62c3685fbc0c7dcfb	revision of first-order bayesian classifiers	bayes estimation;bayesian network;bayesian classifier;intelligence artificielle;logical programming;classification;reseau bayes;estimacion bayes;first order;programmation logique;red bayes;logique ordre 1;bayes network;artificial intelligence;inteligencia artificial;logic programs;programacion logica;clasificacion;first order logic;probabilistic relational model;estimation bayes;logica orden 1	New representation languages that integrate first order logic with Bayesian networks have been proposed in the literature. Probabilistic Relational models (PRM) and Bayesian Logic Programs (BLP) are examples. Algorithms to learn both the qualitative and the quantitative components of these languages have been developed. Recently, we have developed an algorithm to revise a BLP. In this paper, we discuss the relationship among these approaches, extend our revision algorithm to return the highest probabilistic scoring BLP and argue that for a classification task our approach, which uses techniques of theory revision and so searches a smaller hypotheses space, can be a more adequate choice.	bayesian network;belief revision;first-order reduction;naive bayes classifier	Kate Revoredo;Gerson Zaverucha	2002		10.1007/3-540-36468-4_15	computer science;artificial intelligence;machine learning;bayesian network;first-order logic;data mining;statistics	ML	-18.53940750222719	9.85824793479756	2747
42577055e9c8abf8881c6a9a8cccef279d034a19	a market collaborative agents-based framework for information retrieval	search engines;mobile agents;indexes;servers;quality of service;security	The Information Retrieval in the World Wide Web (Web IR) plays an important role in the use of the Internet. An information retrieval process begins when a user enters a formal statements of information needs, known as query, into the system and ends when results are given. Results consist of several objects that match the query with different degrees of relevancy. The relevancy of the results is still an important concern and is often associated with the volume of the results: getting better relevancy involves bigger volume of returned information while lesser volume involves lower relevancy. This issue makes the Web IR an active domain of research and development. Since a decade, an interest was given to software agents technology and its applications. Agents have been successfully used in many fields: agents-based market systems prove to be efficient for implementing e-commerce or B2B applications on the Internet, thanks to inherent properties such as prominency of interactions, collaboration between agents, autonomy, scalability, flexibility, interoperability, etc. Although the use of agents in other application domains is not yet widespread, the integration of agents into market mechanisms bring clear and efficient solutions to Quality of Service (QoS) issues encountered in most distributed applications and notably in Web IR systems. Mobility allows defining a new interaction model, where agents act on behalf of final users or devices providing resources, while Market Places provide an organizational setting for the matching of demands and offers. This paper, a renewed form of [1], will show how market mechanisms and mobility can ease collaboration between agents to deliver a service with relevant results in Web IR.	algorithm;autonomy;computers, freedom and privacy conference;distributed computing;e-commerce;high-level programming language;information needs;information retrieval;intelligent agent;interaction;internet;interoperability;matching (graph theory);mike lesser;quality of service;relevance;scalability;software agent;web search engine;world wide web	Djamel Eddine Menacer;Nassima Guenaoui	2016	2016 International Conference on Collaboration Technologies and Systems (CTS)	10.1109/CTS.2016.0088	database index;simulation;quality of service;computer science;artificial intelligence;information security;operating system;management;world wide web;computer security;server	Web+IR	-45.941363651850295	13.605008468799205	2758
c8d3c0e5773b59a6294bea261f80b20a4d91e978	model checking of healthcare domain models	domain model;model driven design;software verification;public health surveillance systems;formal verification;model checking;use case;public health surveillance	This paper shows the application of a type of formal software verification technique known as lightweight model checking to a domain model in healthcare informatics in general and public health surveillance systems in particular. One of the most complex use cases of such a system is checked using assertions to verify one important system property. This use case is one of the major justifications for the complexity of the domain model. Alloy Analyzer verification tool is utilized for this purpose. Such verification work is very effective in either uncovering design flaws or in providing guarantees on certain desirable system properties in the earlier phases of the development lifecycle of any critical project.		Dibyendu Baksi	2009	Computer methods and programs in biomedicine	10.1016/j.cmpb.2009.06.007	use case;model checking;verification;formal verification;software verification;computer science;domain model;runtime verification;computer security;functional verification	SE	-44.633579227381574	31.598262947153206	2773
3c3d5a8e2121dca33e5b5ddac20ad0907b7879ba	to act or not to act - handling file format identification issues in practice				Matthias Töwe;Roland E. Suri;Franziska Geisser	2016			data mining;database;file format;computer science	SE	-93.19034424975167	22.762882413806807	2778
d720fb978d59c4092f270206ae98c49cf0bc7c24	a design unit based code generation technique for object-oriented software development	code generation		code generation (compiler);software development	Jaehyoun Kim;Youngchul Kim;C. Robert Carlson	2000			computer science;goal-driven software development process;kpi-driven code analysis;software design description;computer architecture;software construction;package development process;static program analysis;test-driven development;software sizing	SE	-51.90484281330835	30.33489485280591	2779
379cc62e73236a2eb9dfc2b924658e3861a45e62	taas (testing-as-a-service) design for combinatorial testing	concurrent taas saas combinatorial testing;taas;concurrent;combinatorial testing;ta taas design testing as a service combinatorial testing cloud environment distributed environment saas software as a service test algebra adaptive reasoning ar algorithm;testing software as a service scalability engines databases monitoring;program testing algebra cloud computing inference mechanisms;saas	Testing-as-a-Service (TaaS) in a cloud environment can leverage the computation power provided by the cloud. Specifically, testing can be scaled to large and dynamic workloads, executed in a distributed environment with hundreds of thousands of processors, and these processors may support concurrent and distributed test execution and analysis. TaaS may be implemented as SaaS and used to test SaaS applications. This paper proposes a TaaS design for SaaS combinatorial testing. Test Algebra (TA) and Adaptive Reasoning (AR) algorithm are used in the TaaS design.	algorithm;central processing unit;computation;concurrent computing;software as a service	Wei-Tek Tsai;Guanqiu Qi;Lian Yu;Jerry Zeyu Gao	2014	2014 Eighth International Conference on Software Security and Reliability	10.1109/SERE.2014.26	real-time computing;computer science;database;distributed computing	SE	-29.49710983767756	48.94751621829911	2782
035a982a165dc75f5d2fa627bb3c50fd45cefaab	optimising development and deployment of enterprise software applications on paas: the cast project	customisation;saas platform;platform as a service;paas;software as a service;saas	Platform as a Service (PaaS) is a concept whereby a computing platform and a software development stack are being offered as a combined service to prospective application developers. This model has been shown to carry a great number of benefits for developers and PaaS providers alike, and represents an important trend within cloud computing today. However, the design of mature platforms to support this model to its full extent remains a complex and challenging undertaking for enterprise application PaaS providers. The aim of this paper is to present the approach that is being undertaken within research project CAST to realise a platform that pushes the envelope of PaaS facilities and addresses the challenges associated with optimising application reusability, extensibility, configurability, integrability, and manageability. The ultimate aim is to create a software platform that fosters the creation of an ecosystem, thus pursuing the PaaS vision to its fullest extent possible.	cast tool;cloud computing;ecosystem;enterprise software;extensibility;platform as a service;prospective search;software deployment;software development	Dimitrios Kourtesis;Volker Kuttruff;Iraklis Paraskakis	2010		10.1007/978-3-642-22760-8_2	systems engineering;engineering;software engineering;world wide web	SE	-60.14378693958244	20.917783268116136	2783
09fc3ea83272d4e9103b5b5b09cfa53a28fe3c1b	wavi: a reverse engineering tool for web applications	reverse engineering tool scalable visualisation modules intuitive visualisation modules customized class diagrams force directed graphs extracted information visualisation dependency links web languages filter based mechanism static analysis webappviewer web ecosystem javascript wavi web applications;force directed diagrams reverse engineering web application static analysis javascript class diagram;cascading style sheets;reverse engineering information filters visualization documentation html cascading style sheets;html;visualization;information filters;program diagnostics authoring languages data visualisation directed graphs information filtering information retrieval internet java;documentation;reverse engineering	Web developers face some unique challenges when trying to understand, modify and document the structure of their web applications. The heterogeneity and complexity of the underlying technologies and languages heighten comprehension problems. In particular, JavaScript, as an essential part of the Web ecosystem, is a language that offers a flexibility that can make its code hard to grasp, when it comes to comprehension and documentation tasks. In this paper, we present the first iteration of WAVI (WebAppViewer), a reverse engineering tool that uses static analysis and a filter-based mechanism to retrieve and document the structure of a Web application. WAVI is able to extract elements coming from essential web languages and frameworks such as HTML, JavaScript, CSS and Node.js. The tool makes use of some simple, effective heuristics to accurately retrieve dependency links for files and methods. WAVI also offers the visualisation of the extracted information as force-directed graphs and customized class diagrams. The effectiveness of WAVI is evaluated with experiments that demonstrate that (i) it can resolve JavaScript calls better than a recent technique, and (ii) its visualisation modules are intuitive and scalable.	cascading style sheets;diagram;documentation;ecosystem;experiment;force-directed graph drawing;html;heuristic (computer science);iteration;javascript;list comprehension;node.js;reverse engineering;scalability;static program analysis;web application;web developer;world wide web	Jonathan Cloutier;Segla Kpodjedo;Ghizlane El-Boussaidi	2016	2016 IEEE 24th International Conference on Program Comprehension (ICPC)	10.1109/ICPC.2016.7503744	ajax;web application;web modeling;visualization;html;web design;documentation;web standards;computer science;theoretical computer science;unobtrusive javascript;operating system;dynamic web page;software engineering;web page;database;cascading style sheets;programming language;world wide web;reverse engineering	SE	-54.19128970657043	34.79986611471143	2784
3060ddf941f2658ade2bd92b41f3af2f8cc2f15f	checking the correctness of agent designs against model-based requirements	adaptive agents and intelligent robotics	Agent systems are used for a wide range of applications, and techniques to detect and avoid defects in such systems are valuable. In particular, it is desirable to detect issues as early as possible in the software development lifecycle. We describe a technique for checking the plan structures of a BDI agent design against the requirements models, specified in terms of scenarios and goals. This approach is applicable at design time, not requiring source code. A lightweight evaluation demonstrates that a range of defects can be found using this technique.	correctness (computer science);requirement;software development process	Yoosef Abushark;Michael Winikoff;Tim Miller;James Harland;John Thangarajah	2014		10.3233/978-1-61499-419-0-953	simulation;computer science	SE	-46.55691380600323	35.45623527202799	2785
d106c0f0191c79a3b1d2d5e48b51045d52d6a98a	a genetic algorithm for solving virtual topology reconfiguration problem in survivable wdm networks with reconfiguration constraint	wdm;distributed system;optimal solution;data transmission;metodo polinomial;topology;multiplexage longueur onde;solution optimale;moyenne ponderee;wdm network;systeme reparti;temps polynomial;dedicated path protection;gestion trafic;topologie;chemin virtuel;traffic management;algoritmo genetico;delai transmission;topologia;transmission time;virtual topology reconfiguration;polynomial time algorithm;fault tolerant system;sistema repartido;camino virtual;polynomial method;weighted average;propagation delay;solucion optima;transmission donnee;polynomial time;sistema tolerando faltas;algorithme genetique;gestion trafico;promedio pondero;genetic algorithm;systeme tolerant les pannes;survivability;methode polynomiale;plazo transmision;virtual path;transmision datos;multiplaje longitud onda;topology design;wavelength division multiplexing;wavelength division multiplex;tiempo polinomial	In wavelength division multiplexing (WDM) networks, the performance of the virtual topology designed for a pre-specified traffic pattern can be improved by performing virtual topology reconfiguration. Simultaneously, the provision of survivability of WDM networks is important, because the transmission of huge data should be protected when fiber fails. Thus, the combination of survivability and reconfiguration is an important issue on WDM networks. In this paper, the virtual topology reconfiguration problem (VTRP) in survivable WDM networks with reconfiguration constraint is studied. Given the physical topology, dedicated path-protection virtual topology and a new traffic demand matrix, the goal of VTRP is to reconfigure current virtual topology under the pre-specified reconfiguration constraint so that the objective cost can be minimized. The object cost of VTRP is the average weighted propagation delay (AWPD). Because designing a polynomial time algorithm to find the optimal solution of VTRP is impractical, in this paper, a genetic algorithm (GA) is proposed to solve this problem. Experiment results reveal that the objective cost can be decreased effectively by using the proposed genetic algorithm.	genetic algorithm;wavelength-division multiplexing	Der-Rong Din;Yu-Sheng Chiu	2008	Computer Communications	10.1016/j.comcom.2008.03.022	simulation;telecommunications;computer science;distributed computing;wavelength-division multiplexing;computer network	Logic	-4.8339415706762665	77.82424860151241	2790
fccd0e723331449aa4f8920391b75a0b03d5ba5f	iotune: a g-states driver for elastic performance of block storage		Imagining a disk which provides baseline performance at a relatively low price during low-load periods, but when workloads demand more resources, the disk performance is automatically promoted in situ and in real time. In a hardware era, this is hardly achievable. However, this imagined disk is becoming reality due to the technical advances of software-defined storage, which enable volume performance to be adjusted on the fly. We propose IOTune, a resource management middleware which employs software-defined storage primitives to implement G-states of virtual block devices. G-states enable virtual block devices to serve at multiple performance gears, getting rid of conflicts between immutable resource reservation and dynamic resource demands, and always achieving resource right-provisioning for workloads. Accompanying G-states, we also propose a new block storage pricing policy for cloud providers. Our case study for applying G-states to cloud block storage verifies the effectiveness of the IOTune framework. Trace-replay based evaluations demonstrate that storage volumes with G-states adapt to workload fluctuations. For tenants, G-states enable volumes to provide much better QoS with a same cost of ownership, comparing with static IOPS provisioning and the I/O credit mechanism. G-states also reduce I/O tail latencies by one to two orders of magnitude. From the standpoint of cloud providers, G-states promote storage utilization, creating values and benefiting competitiveness. G-states supported by IOTune provide a new paradigm for storage resource management and pricing in multi-tenant clouds.	baseline (configuration management);block (data storage);cloud computing;disk staging;end-to-end principle;immutable object;input/output;leaky bucket;middleware;multitenancy;on the fly;programming paradigm;provisioning;quality of service;replay attack;software-defined storage;total cost of ownership	Tao Lu;Ping Huang;Xubin He;Matthew Welch;Steven Gonzales;Ming Zhang	2017	CoRR		resource management;quality of service;cloud computing;block (data storage);provisioning;iops;distributed computing;middleware;reservation;computer science	OS	-22.661755599377013	59.7369731116269	2795
20e045bae6757232940a35a2f94e11456b52c9b5	it sourcing reflections: lessons for customers and suppliers	gestion integrada;gestion integree;outsourcing;externalisation;integrated management;sous traitance;subcontrata;information system;systeme information;sistema informacion	The size of the global information technology (IT) outsourcing market is estimated to be between $200 and $500 billion. Clearly, customers no longer question if they should outsource IT, but rather question how they can best exploit this immense market. Customers now expect many business advantages from IT outsourcing, including lower costs, better service, infusion of new technology, transformation of fixed IT budgets to variable IT budgets, improved business processes, and even increased revenues [LaWi01]. In short, customers expect IT outsourcing to transform IT functions into lean, dynamic groups that respond quickly to business needs and opportunties. But how do customers actually achieve such business advantage? Based on over a decade of research, we found that customers must become adept at managing four continual processes to successfully exploit IToutsourcing:	amiga reflections;business process;business requirements;outsourcing	Mary Lacity;Leslie P. Willcocks	2003	Wirtschaftsinformatik	10.1007/BF03250888	management;information system;outsourcing	ECom	-81.58313018649936	8.678549286382822	2796
4da9486b0f60ad59ef6716c8ce6963256bb4ec2a	operations interface standards for microprocessors-controlled transmission terminal equipment			microprocessor	J. T. French;F. T. Man	1984			embedded system;data circuit-terminating equipment;terminal equipment;computer science;transmission (mechanics)	HCI	-7.4594993844814566	73.95297288331001	2800
59a132f0f78af094f7208f250dc8e5ffbff3264d	scalable s-to-p broadcasting on message-passing mpps	degradation;message passing mpps;broadcasting scalability degradation casting contracts government parallel processing;performance;government;performance index;broadcasting algorithms;contracts;interconnection network;s to p broadcasting;casting;parallel architectures;parallel architectures message passing parallel machines;ke y words;message passing;performance s to p broadcasting message passing mpps broadcasting algorithms target broadcasting algorithm intel paragon scalability;target broadcasting algorithm;parallel machines;scalability;broadcasting;intel paragon;parallel processing	In s-to-p broadcasting, s processors in a p-processor machine contain a message to be broadcast to all the processors, 1 ≤ s ≤ p. We present a number of different broadcasting algorithms that handle all ranges of s. We show how the performance of each algorithm is influenced by the distribution of the s source processors and by the relationships between the distribution and the characteristics of the interconnection network. For the Intel Paragon we show that for each algorithm and machine dimension there exist ideal distributions and distributions on which the performance degrades. For the Cray T3D we also demonstrate dependencies between distributions and machine sizes. To reduce the dependence of the performance on the distribution of sources, we propose a repositioning approach. In this approach, the initial distribution is turned into an ideal distribution of the target broadcasting algorithm. We report experimental results for the Intel Paragon and Cray T3D and discuss scalability and performance.	algorithm;central processing unit;cray t3d;existential quantification;fibre channel point-to-point;intel paragon;interconnection;message passing interface;message passing;scalability	Susanne E. Hambrusch;Ashfaq A. Khokhar;Yi Liu	1996		10.1109/ICPP.1996.537145	parallel processing;parallel computing;message passing;real-time computing;casting;scalability;degradation;performance;computer science;operating system;distributed computing;broadcasting;government;computer network	Arch	-10.83361508123301	46.56596008813565	2801
71bbd82588c003b498dcc932aac0be9170fd6b91	using tk as remote gui frontend for 4gl-database applications	graphic output;gui frontend;windows platform;unix database application;database language;remote gui frontend;generation language;presentation server;text output;internet server;text-terminal output	This is a short report about our experience using Tk as GUI frontend. We wrote a compiler called F4GL , source compatible to theINFORMIX 4GL database language. (4GL means 4th generation language) This language was originally designed to create UNIX database applications with text-terminal output. 4GL programs compiled withF4GL generate Tcl-commands as graphic output (the text output is also still available). These Tcl-commands are sent over TCP/IP to a presentation server which will provide for conversation into graphic objects. We have presentation servers for X11 and Windows. 4GL Server is the presentation server for all Windows platforms and is implemented using Tcl/Tk. It is an Internet-Server like lpd or rshd whose services are available at a specific TCP port. We found different solutions for the network connection to the remote GUI / presentation server and had to solve various problems, which are discussed in detail in this poster. Writing a internet server in Tcl which evaluates Tcl-commands is very easy, but it introduces a security hole. The machine running4GL Server is accessible from the internet with tclcommands. That’s why we implemented a special protocol to authenticate the users of the 4GL Server . We made some extensions, some of it appears to be useful for the whole Tcl-community. Until now we maintained our own Windows-port of Tk3.6, because the actual Windows-Tk4.x releases from Sun still have limitations, we will explain the reasons and show some performance bottlenecks ( http://www.bj-ig.de/speed.html). In the near future we want to switch to official releases because the Tk4.xx and Tk8.xx versions offer many advantages, especially safe interpreters and the Tcl-Plugin. A good overview about the F4GL can be found athttp://www.4js.com/f4gldoc.html, this is the webpage of 4JS, our trade-partner. 1 History Two years ago we searched for a solution to migrate existing 4GL-database-programs to GUI’s. First, we discussed a cross-compiler to translate 4GL into another database-language with builtin GUI (New Era, Powerbuilder, Gupta a.s.o) but this would exclude expierenced 4GL-programmers from the development. Therefore we decided to write a source compatible compiler.That produces programs with a configurable, flexible GUIfrontend. We choose Tk for the remote GUI, because we had some experience with it (usaging it for visualization in measurement tasks under Linux/X11), and it was free. The only problem was a good Windows port for the actual Tk3.6 version, because the majority of customers were expected using Windows (especially in Germany). We took one 16bit-port fromSoftware Research Associates Inc. and started development at the Win32s platform to support native controls, sockets, optimize speed, and eliminate bugs. We call this Tk derivative WTK and it’s of course freely available with all sources. See more information at http://www.bj-ig.de/wtk.html . Of course we want to switch now to the main distribution, to use all the very nice features of Tcl/Tk8.0, but there is one reason , which doesn’t allow us to use it: performance.	authentication;computer terminal;cross compiler;fourth-generation programming language;graphical user interface;internet protocol suite;line printer daemon protocol;linux;microsoft windows;powerbuilder;programmer;query language;server (computing);software bug;sun java wireless toolkit;tcl;unix;vulnerability (computing);web page;x window system	Volker Schubert	1997			computer science;operating system;internet authentication service;appleshare;database;world wide web;windows server	OS	-32.58036240106311	42.00786886092191	2809
aef760509a43ee87862ad581d4fff5230072e188	a novel dynamic fuzzy threshold preemption scheduling algorithm for soft real-time systems	soft real time system;scheduling fuzzy set theory real time systems;fuzzy systems scheduling algorithm real time systems fuzzy control dynamic scheduling control systems usa councils linearity degradation tin;fuzzy set theory;scheduling algorithm;scheduling;switching problem dynamic fuzzy threshold preemption scheduling algorithm soft real time system dynamic fuzzy threshold least slack first scheduling;real time systems	In the context of least slack first scheduling, switching may frequently be caused. The extra overheads of preemptions among tasks debase the performance of soft real-time systems significantly. In this paper, we present a novel scheduling algorithm, named dynamic fuzzy threshold least slack first (DFTLSF) scheduling, which solved the switching problem when use least slack first scheduling algorithm in tasks. The notion of dynamic fuzzy threshold coefficient was defined to fuzzy the threshold dynamically. The slack time of the running task is reduced to its fuzzy threshold to avoid thrashing. Comparing to the traditional least slack first scheduling algorithm, the simulation results show that, the dynamic fuzzy preemption make the switching number of the novel algorithm smaller and the missed deadline percentage decreased.	algorithm;coefficient;preemption (computing);real-time clock;real-time computing;scheduling (computing);simulation;slack variable;thrashing (computer science)	Wei Ba;Dabo Zhang	2007	2007 46th IEEE Conference on Decision and Control	10.1109/CDC.2007.4434005	fair-share scheduling;fixed-priority pre-emptive scheduling;real-time computing;earliest deadline first scheduling;flow shop scheduling;dynamic priority scheduling;computer science;rate-monotonic scheduling;two-level scheduling;deadline-monotonic scheduling;distributed computing;least slack time scheduling;lottery scheduling;round-robin scheduling;scheduling;i/o scheduling	Embedded	-11.228698773042515	60.71017232279603	2810
49fb7f7c8e569760a0c8721a6c57cfe0bd452d0f	“no shit” or “oh, shit!”: responses to observations on the use of uml in professional practice	uml;software development;empirical studies;software design;notation	This paper follows a paper, “UML in Practice” presented at ICSE 2013. It summarizes and reflects on the discussion and additional investigation that arose from “UML in Practice.” The paper provides a condensed recap of “UML in Practice” findings, explains what data were collected from which sources to inform this paper, and describes how the data were analyzed. It reports on the discussion that has arisen, summarizing responses from industry practitioners, academics teaching software engineering, and the UML community, and considers how those responses reflect on the original observations. The responses to “UML in Practice” divide (crudely) between two perspectives: (1) the observations made are familiar and unsurprizing, and match personal experience (“No shit”); or (2) the observations threaten long-held beliefs about UML use, and in particular about the status of UML as the de facto standard of software engineering, implying a need to change personal practice (“Oh, shit!”).	icse;software engineering;unified modeling language	Marian Petre	2014	Software & Systems Modeling	10.1007/s10270-014-0430-4	unified modeling language;computer science;systems engineering;engineering;knowledge management;software design;software development;software engineering;applications of uml;notation;empirical research;management	SE	-71.5245117811207	23.726982152671553	2826
3dee57efba357b9d919db961cb2cfe8bb889f039	un modèle mathématique de processus d'interrogation‎ : les pseudoquestionnaires		HAL is a multi-disciplinary open access archive for the deposit and dissemination of scientific research documents, whether they are published or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers. L’archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la diffusion de documents scientifiques de niveau recherche, publiés ou non, émanant des établissements d’enseignement et de recherche français ou étrangers, des laboratoires publics ou privés. Un modèle mathématique de processus d’interrogation : les pseudoquestionnaires Michel Terrenoire	archive;comefrom;hal;linear algebra;michel hénon	Michel Terrenoire	1970				ML	-107.69033498529164	11.428278464710854	2829
6e38bf53ce8c593050dab0d21a7c3d1d64ddc61e	third-party private dfa evaluation on encrypted files in the cloud		Motivated by the need to outsource file storage to untrusted clouds while still permitting limited use of that data by third parties, we present practical protocols by which a client (the third-party) can evaluate a deterministic finite automaton (DFA) on an encrypted file stored at a server (the cloud), once authorized to do so by the file owner. Our protocols provably protect the privacy of the DFA and the file contents from a malicious server and the privacy of the file contents (except for the result of the evaluation) from an honest-but-curious client (and, heuristically, from a malicious client). We further present simple techniques to detect client or server misbehavior.	authorization;cloud computing;deterministic finite automaton;encryption;heuristic;outsourcing;privacy;server (computing)	Lei Wei;Michael K. Reiter	2012		10.1007/978-3-642-33167-1_30	self-certifying file system;computer science;stub file;unix file types;filesystem-level encryption;internet privacy;data file;file system fragmentation;world wide web;computer security	Security	-41.69665892891675	68.58582549672998	2836
7c0a48c3207cd9f38696943c0909565eaa5b1498	a disruption-tolerant restful support for the web of things	delay tolerant networking;rest services web of things delay tolerant networking;web services broadcast communication internet of things mobile radio transport protocols;robots avatars protocols internet of things sensors cloud computing;coap disruption tolerant restful support web of things wot internet of things iot web based languages web based protocols physical objects wireless interfaces web clients uri stateless services service requests service responses store carry and forward principle service invocation model unicast service invocations anycast service invocations multicast service invocations broadcast service invocations http;rest services;web of things	The Web of Things (WoT) extends the Internet of Things (IoT) considering that each physical object can be accessed and controlled using Web-based languages and protocols. However, due to the mobility of physical objects and to the short radio range of the wireless interfaces they are equipped with, frequent and unpredictable connectivity disruptions may occur between the physical objects and the Web clients used to control and access these objects. This paper presents a disruption-tolerant RESTful support for the WoT, in which resources offered by physical objects are identified by URIs and accessed through stateless services. Service requests and responses are forwarded using the store-carry-and-forward principle, and can be cached by intermediate nodes. A complete service invocation model is provided, allowing to perform unicast, anycast, multicast and broadcast service invocations either using HTTP or CoAP, which makes it particularly suited for the WoT. This disruption-tolerant support is illustrated by a scenario in the context of agricultural robotics.	agricultural robot;anycast;artifact (software development);cloud computing;constrained application protocol;delay-tolerant networking;denial-of-service attack;html;hypertext transfer protocol;image-based modeling and rendering;internet of things;middleware;multicast;representational state transfer;robotics;simulation;stateless protocol;testbed;unicast;web of things;world wide web	Nicolas Le Sommer;Lionel Touseau;Yves Mahéo;Maël Auzias;Frédéric Raimbault	2016	2016 IEEE 4th International Conference on Future Internet of Things and Cloud (FiCloud)	10.1109/FiCloud.2016.11	web service;web of things;computer science;delay-tolerant networking;internet privacy;world wide web;computer security;statistics;computer network	Mobile	-24.562061527839447	81.66293642785743	2837
489e0ea06eb4a96662a9ad7539fd75e1b4fc9b08	design alternatives for wireless local area networks	ieee 802 11;wireless local area network;wireless atm;mac protocol;wireless lans;physical layer;watm;personal area network;bluetooth;wireless lan;wireless technology;mac layer;hiperlan	"""In this paper an overview of the wireless local area network (LAN) area is provided. The two types of wireless LAN topologies used today, infrastructure and ad hoc, are presented. The requirements that a wireless LAN is expected to meet are discussed. These requirements impact on the implementation of both the Physical and MAC layer of a wireless LAN. The unique characteristics of wireless physical layers are discussed and the """"ve technology alternatives used today are presented. MAC layer issues are discussed and the two existing standards, IEEE 802.11 and HIPERLAN 1, are examined. Pollingbased MAC protocols (RAP, GRAP) are also reviewed. Finally, an introduction is made to wireless technologies that interact with WLANs, such as personal area networking (PAN) and wireless ATM and an overview of HIPERLAN 2, a WLAN using ATM technology, is provided. Copyright ( 2001 John Wiley & Sons, Ltd."""	atm turbo;access network;algorithm;authorization;bandwidth (signal processing);bluetooth;data rate units;elegant degradation;frequency allocation;frequency divider;frequency-hopping spread spectrum;hoc (programming language);integrated services digital network;interference (communication);john d. wiley;mac address;mobile computing;modulation;multiplexing;network topology;perceived performance;protocol stack;quality of service;randomness;rapid refresh;requirement;software deployment;software incompatibility;software propagation;telecommunications network;wireless access point;x.690	Petros Nicopolitidis;Georgios I. Papadimitriou;Andreas S. Pomportsis	2001	Int. J. Communication Systems	10.1002/dac.481	wireless transport layer security;embedded system;service set;wi-fi;network allocation vector;ieee 802.11;wireless wan;heterogeneous network;wireless site survey;telecommunications;inter-access point protocol;computer science;operating system;wireless network;personal area network;wireless distribution system;wireless lan controller;key distribution in wireless sensor networks;base transceiver station;capwap;municipal wireless network;wi-fi array;bluetooth;fixed wireless;wireless intrusion prevention system;physical layer;computer network;ieee 802.11e-2005	Mobile	-16.2978772622675	88.97106531095457	2838
8cfcafccae37e2c4d85879f372a9790c9ce7e9ae	"""""""human centered"""" cim-strukturen-wunsch und wirklichkeit eines esprit-projektes"""	human centered		computer-integrated manufacturing	Holm Gottschalch;Alexander Wittkowsky	1989		10.1007/978-3-642-83990-0_27		Vision	-96.39639485275656	25.01796534840611	2847
9324ca486054c53c8ecfacdebe6409137e67774f	sharing independence & relabeling: efficient formal verification of higher-order masking		The efficient verification of the security of masked hardware implementations is an important issue that hinders the development and deployment of randomness-efficient masking techniques. At EUROCRYPT 2018, Bloem et al. [6] introduced the first practical formal tool to prove the side-channel resilience of masked circuits in the probing model with glitches. Most recently Barthe et al. [2] introduced a more efficient formal tool that builds upon the findings of Bloem et al. for modeling the effects of glitches. While Barthe et al.’s approach greatly improves the first-order verification performance, it shows that higherorder verification in the probing model with glitches is still enormously time-consuming for larger circuits like a second-order AES S-box, for instance. Furthermore, the results of Barthe et al. underline the discrepancy between state-of-the-art formal security notions that allow for faster verification of circuits. Namely the strong non-interference (SNI) notion, and existing masked hardware implementations that are secure in the probing model with glitches. In this work, we extend and improve the formal approaches of Bloem et al. and Barthe et al. on manifold levels. We first introduce a so-called sharing independence notion which helps to reason about the independence of shared variables. We then show how to use this notion to test for the independence of input and output sharings of a module which allows speeding up the formal verification of circuits that do not fulfill the SNI notion. With this extension, we are for the time able to verify the security of a second-order masked DOM AES S-box which takes about 3 seconds, and up to a fifth-order AES S-box which requires about 47 days for verification. Furthermore, we discuss in which case the independence of input and output sharings lead to composability.	composability;discrepancy function;eurocrypt;first-order predicate;formal verification;glitch;input/output;interference (communication);non-interference (security);randomness;s-box;server name indication;shared variables;side-channel attack;software deployment	Roderick Bloem;Hannes Groß;Rinat Iusupov;Martin Krenn;Stefan Mangard	2018	IACR Cryptology ePrint Archive			Crypto	-35.93565672561185	75.33022444380357	2848
01520ce91db02a91f715f69b04c6fe7526649d94	improving timeliness in real-time secure database systems	phase locking;database system;real time;data processing;data bases;satisfiability;timeliness;computer applications;adaptive method;electronic security;transaction processing;data consistency;real time application;multilevel security;time constraint	Database systems for real-time applications must satisfy timing constraints associated with transactions, while maintaining data consistency. In addition to real-time requirements, security is usually required in many applications. Multilevel security requirements introduce a new dimension to transaction processing in real-time database systems. In this paper, we argue that because of the complexities involved, trade-offs need to be made between security and timeliness. We briefly present the secure two-phase locking protocol and discuss an adaptive method to support trading off security for timeliness, depending on the current state of the system. The performance of the adaptive secure two-phase locking protocol shows improved timeliness. We also discuss future research direction to improve timeliness of secure database systems.	database;lock (computer science);multilevel security;real-time clock;real-time transcription;requirement;transaction processing;two-phase locking	Sang Hyuk Son;Rasikan David;Bhavani M. Thuraisingham	1996	SIGMOD Record	10.1145/381854.381879	real-time computing;data processing;transaction processing;computer science;database;distributed computing;computer applications;data consistency;satisfiability	Security	-23.965364320880457	47.37791224334429	2850
5eb221e4105d3288f5499c691564142068329b81	caspaxos: replicated state machines without logs		CASPaxos is a wait-free, linearizable, multi-writer multi-reader register in unreliable, asynchronous networks supporting arbitrary update operations including compare-and-set (CAS). The register acts as a replicated state machine providing an interface for changing its value by applying an arbitrary user-provided function (a command). Unlike Multi-Paxos and Raft which replicate the log of commands, CASPaxos replicates state, thus avoiding associated complexity, reducing write amplification, increasing concurrency of disk operations and hardware utilization. The paper describes CASPaxos, proves its safety properties and evaluates the characteristics of a CASPaxos-based prototype of key-value storage. 2012 ACM Subject Classification Information systems → Distributed storage		Denis Rystsov	2018	CoRR		distributed computing;real-time computing;state machine replication;finite-state machine;fault tolerance;unavailability;leader election;commit;computer science;raft;crash	OS	-22.861120247505205	49.30013803528643	2851
3b227f7d1cb4c171ef731219d7fed3b9659dfafe	why do automated builds break? an empirical study	software;software quality error detection program testing;collaboration;empirical software engineering;data mining;companies;statistical analysis;automated builds;interviews;software quality empirical software engineering automated builds data mining;automated build systems integration error detection software company build breakage build failures software quality;context;software quality;software interviews companies collaboration context statistical analysis	To detect integration errors as quickly as possible, organizations use automated build systems. Such systems ensure that (1) the developers are able to integrate their parts into an executable whole, (2) the testers are able to test the built system, (3) and the release engineers are able to leverage the generated build to produce the upcoming release. The flipside of automated builds is that any incorrect change can break the build, and hence testing and releasing, and (even worse) block other developers from continuing their work, delaying the project even further. To measure the impact of such build breakage, this empirical study analyzes 3,214 builds produced in a large software company over a period of 6 months. We found a high ratio of build breakage (17.9%), and also quantified the cost of such build breakage as more than 336.18 man-hours. Interviews with 28 software engineers from the company helped to understand the circumstances under which builds are broken and the effects of build breakages on the collaboration and coordination of teams. We quantitatively investigated the main factors impacting build breakage and found that build failures correlate with the number of simultaneous contributors on branches, the type of work items performed on a branch, and the roles played by the stakeholders of the builds (for example developers vs. Integrators).	build automation;continuous integration;executable;interviews;software engineer	Noureddine Kerzazi;Foutse Khomh;Bram Adams	2014	2014 IEEE International Conference on Software Maintenance and Evolution	10.1109/ICSME.2014.26	software build;verification and validation;interview;computer science;systems engineering;engineering;software engineering;build verification test;software construction;data mining;software quality;collaboration	SE	-65.4404662285683	32.91493719079233	2856
9e69159ecac28d67f777f751e585ad2b5bb213b1	performance and energy consumption analysis of a delay-tolerant network for censorship-resistant communication	opportunistic networks;delay tolerant networks;energy overhead;microblogging;censorship resistance	Delay Tolerant Networks (DTNs) composed of commodity mobile devices have the potential to support communication applications resistant to blocking and censorship, as well as certain types of surveillance. We analyze the performance and energy consumption of such a network, and consider the impact of random and targeted denial-of-service and censorship attacks. To gather wireless connectivity traces for a DTN composed of human-carried commodity smartphones, we implemented and deployed a prototype DTN-based micro-blogging application, called 1am, in a college town. We analyzed the system during a time period with 111 users. Although the study provided detailed enough connectivity traces to enable analysis, message posting was too infrequent to draw strong conclusions based on user-initiated messages, alone. We therefore simulated more frequent message initiations and used measured connectivity traces to analyze message propagation. Using a flooding protocol, we found that with an adoption rate of 0.2% of a college town's student and faculty population, the median one-week delivery rate is 85% and the median delivery delay is 13 hours. We also found that the network delivery rate and delay are robust to denial-of service and censorship attacks eliminating more than half of the participants. Using a measurement-based energy model, we also found that the DTN system would use less than 10.0% of a typical smartphone's battery energy per day in a network of 2,500 users.	blocking (computing);blog;delay-tolerant networking;denial-of-service attack;mobile device;prototype;smartphone;software propagation;tracing (software)	Yue Liu;David R. Bild;David Adrian;Gulshan Singh;Robert P. Dick;Dan S. Wallach;Zhuoqing Morley Mao	2015		10.1145/2746285.2746320	embedded system;telecommunications;computer science;microblogging;operating system;internet privacy;computer security;computer network	Mobile	-27.85075348580783	77.80252824494096	2864
665c4493e0e422376126f92ab57ca5adcc2d78a9	strings and the sequence abstraction in pascal	sequence abstraction;information and communication services;280300 computer software;pascal strings;information and communication services not elsewhere classified;computer software;information and computing sciences;other information and communication services;programming languages	This paper examines the sequence abstraction known in Pascal as the 'file', and shows how sequences of charcters ('strings' on the SNOBOL sense) may be cleanly fitted into Pascal-like languages. The specific problems of providing the suggested facilities as an experimental extension to Pascal are examined.	string (computer science)	Arthur H. J. Sale	1979	Softw., Pract. Exper.	10.1002/spe.4380090807	computer science;theoretical computer science;programming language	NLP	-25.794652904239666	26.062123704106227	2873
34db95414290d21eafb4499826c2a4913d78c5a8	improving process discovery results by filtering outliers using conditional behavioural probabilities		Process discovery, one of the key challenges in process mining, aims at discovering process models from process execution data stored in event logs. Most discovery algorithms assume that all data in an event log conform to correct execution of the process, and hence, incorporate all behaviour in their resulting process model. However, in real event logs, noise and irrelevant infrequent behaviour are often present. Incorporating such behaviour results in complex, incomprehensible process models concealing the correct and/or relevant behaviour of the underlying process. In this paper, we propose a novel general purpose filtering method that exploits observed conditional probabilities between sequences of activities. The method has been implemented in both the ProM toolkit and the RapidProM framework. We evaluate our approach using real and synthetic event data. The results show that the proposed method accurately removes irrelevant behaviour and, indeed, improves process discovery results.		Mohammadreza Fani Sani;Sebastiaan J. van Zelst;Wil M. P. van der Aalst	2017		10.1007/978-3-319-74030-0_16	anomaly detection;systems engineering;business process discovery;filter (signal processing);outlier;conditional probability;work in process;process modeling;process mining;computer science;pattern recognition;artificial intelligence	Vision	-53.370703297961256	18.038880849136785	2874
d7d0bae303ea824e3f4b82b1add0823892e4fdfd	edgex: edge replication for web applications	databases;master thesis;web applications;data partitioning;relational databases client server systems computer centres;image edge detection cascading style sheets relational databases content distribution networks prototypes;data partitioning edge networks;edge networks;dynamic content edgex edge replication global web applications network latency data centers edge networks image caching css javascript static content application request latencies remote data center edge aware dynamic data replication architecture relational database systems	Global Web applications face the problem of high network latency due to their need to communicate with distant data centers. Many applications use edge networks for caching images, CSS, java script, and other static content in order to avoid some of this network latency. However, for updates and for anything other than static content, communication with the data center is still required, and can dominate application request latencies. One way to address this problem is to push more of the web application, as well the database on which it depends, from the remote data center towards the edge of the network. In this paper, we present preliminary work in this direction. Specifically, we present an edge-aware dynamic data replication architecture for relational database systems supporting Web applications. Our objective is to allow dynamic content to be served from the edge of the network, with low latency.	cascading style sheets;data center;dynamic data;dynamic web page;java;javascript;relational database;replication (computing);web application	Hemant Saxena;Kenneth Salem	2015	2015 IEEE 8th International Conference on Cloud Computing	10.1109/CLOUD.2015.147	web application;computer science;operating system;database;distributed computing;world wide web	DB	-18.342222620197653	71.43488866744188	2877
cd4946715ac6744170a34f0e5baf50ce42f3ebe2	entwicklung und evaluation einer software zur analyse der herzfrequenzvariabilität auf basis des routine-ekg's				Hannah Oppel	2013				PL	-100.15188130010547	24.103449270358716	2883
5006f99754a0a689d9086f79122ccc53f4a4b7cf	results concerning the bandwidth of subliminal channels	protocols;game theory;information hiding;cryptographic protocols;protocol design;subliminal channels;transmitters protocols telecommunication channels cryptography game theory digital communication receivers;indexing terms;secure communication;receivers;computer security;covert channel;steganography;digital communication;coding theory;codes;transmitter;cryptography;subliminal free communication channel;transmitters;information rate;bandwidth;bandwidth transmitters cryptography steganography communication channels communication system control cryptographic protocols data security computer security codes;subliminal receiver;telecommunication channels;communication system control;communication channels;subliminal receiver subliminal channels bandwidth information hiding secure communications protocol design subliminal free communication channel transmitter;secure communications protocol design;communication system security;data security	In conjunction with a six-month research program on Computer Security, Cryptology and Coding Theory hosted by the Isaac Newton Institute of Mathematical Sciences, University of Cambridge, Cambridge, U.K., a Workshop on Information Hiding was held from May 30 through June 1, 1996. This workshop was devoted to all aspects of information hiding—other than the usual cryptographic concealment of content, including steganography, subliminal channels, fingerprinting, covert channels, etc. Two surprising results pertaining to subliminal channels were presented or grew out of presentations made at this workshop. One is of interest to the secure communications protocol designer concerned with denying the use of subliminal channels, while the other is important to the designer, or user, of subliminal channels. The first raises the question of whether the notion of a “subliminal-free” communication channel is an oxymoron, i.e., is it possible to force the bandwidth of the subliminal channel to be truly zero? The second forces a more precise formulation of a conjecture the author had made that the bandwidth of a subliminal channel is logarithmically limited if the transmitter is unwilling to trust the subliminal receiver unconditionally. Motivated by these results, this paper reexamines the fundamental questions of the bandwidth available for subliminal communication as a function of the trust the transmitter has in the subliminal receiver and of a logically sound interpretation of the term “subliminal-free.”	channel (communications);coding theory;communications protocol;computation;computer security;covert channel;cryptography;fingerprint (computing);newton;public-key cryptography;secure communication;self-information;steganography;subliminal channel;transmitter	Gustavus J. Simmons	1998	IEEE Journal on Selected Areas in Communications	10.1109/49.668970	game theory;transmitter;telecommunications;computer science;internet privacy;computer security;statistics	Security	-44.24831995751556	83.47395451914649	2886
72caa63553de0d82dbb0c3a64c1a57980456523a	experience with the language sr in an undergraduate operating systems course	interprocess communication;remote procedure call;mutual exclusion;operating system;data races;message passing;concurrent programs;student learning;critical section;concurrent process	In undergraduate operating systems classes, students learn about concurrent process synchronization, including such things as shared data, race conditions, critical sections, mutual exclusion, semaphores, monitors, and the test-and-set hardware instruction. They also study interprocess communication, message passing, rendezvous, and remote procedure calls. Solutions to classical problems, such as the dining philosophers, producers and consumers, bounded buffers, and readers and writers, are presented using the above concepts. However, students need to write programs in a language that provides facilities for concurrent programming in order to appreciate fully the above concepts. This paper describes the SR language and discusses its successful use as an environment for concurrent programming in an undergraduate operating systems class.	concurrent computing;critical section;dining philosophers problem;inter-process communication;message passing;monitor (synchronization);mutual exclusion;operating system;parallel computing;race condition;remote procedure call;sr (programming language);semaphore (programming);synchronization (computer science);test-and-set	Stephen J. Hartley	1992		10.1145/134510.134546	parallel computing;message passing;real-time computing;mutual exclusion;computer science;operating system;software engineering;distributed computing;critical section;programming language;remote procedure call;inter-process communication	PL	-25.182486350300454	38.7788264865251	2888
38ca8bf2cce49b92a6cc7942c6cd51a4aa42e925	slickdeque: high throughput and low latency incremental sliding-window aggregation		Online analytics, in most advanced scientific and business applications, rely heavily on the efficient execution of large numbers of Aggregate Continuous Queries (ACQs). Incremental slidingwindow computation is used in the state-of-the-art ACQ processing algorithms (FlatFIT, TwoStacks, and DABA) to avoid the reevaluation of the aggregate value of the window from scratch on every update. FlatFIT and TwoStacks aim to increase throughput, and DABA to minimize latency, while all process invertible and non-invertible aggregates uniformly. In this paper, we propose a novel algorithm, SlickDeque, that distinguishes the execution between invertible and non-invertible aggregates and offers better throughput and latency for both types. In addition, our method requires less memory and efficiently supports multi-ACQ processing. We theoretically show the time and space complexity advantages of SlickDeque and experimentally validate them using a real workload. Specifically, our approach maintains 283% lower latency spikes on average while achieving up to 19% throughput improvement in a single query environment and up to 345% improvement in amulti-query environment over the state-of-the-art approaches along with requiring up to 5 times less memory.	aggregate data;aggregate function;algorithm;computation;dspace;experiment;interrupt latency;throughput	Anatoli U. Shein;Panos K. Chrysanthis;Alexandros Labrinidis	2018		10.5441/002/edbt.2018.35	sliding window protocol;database;throughput;real-time computing;computer science;latency (engineering)	DB	-18.542494930474735	54.97022178566485	2890
3df4f0c8e2032d342effc5d92bfcf97273f64fb2	an architecture for generating semantic aware signatures		Identifying new intrusion exploits and developing effecti ve detection signatures for them is essential for protecting computer networks. We present Nemean, a system for automatic generation of intrusion signatures from honeynet packet traces. Our architecture i s distinguished by its emphasis on a modular design frameworkthat encourages independent development and modification o f system components and protocol semantic awareness which allows construction of signatures that greatly reduc e false alarms. The building blocks of our architecture include transport and s ervice normalization, intrusion profile clustering and automata learning that generates connection and sessio n aware signatures. We evaluate our architecture through a prototype implementation that demonstra tes the potential of semantic-aware, resilient signatures. For example, signatures generated by Nemean fo r NetBIOS exploits had a 0% false-positive rate and a 0.04% false-negative rate.	antivirus software;automata theory;cluster analysis;fo (complexity);honeypot (computing);modular design;netbios;network packet;prototype;tracing (software);type signature	Vinod Yegneswaran;Jonathon T. Giffin;Paul Barford;Somesh Jha	2005				Security	-60.60049727208215	65.01449356001503	2895
9cf44d55bf5fc6ac72c526f7575aedfe2c19d84d	invormationsverarbeitung zwischen modalitätsspezifität und propositionalem einheitssystem	invormationsverarbeitung zwischen modalit	Wenn man in der Psychologie von modalitatsspezifischen Systemen spricht, denkt man meist an die Kontroverse uber die duale Kodierung (Paivio, 1971) oder die analoge versus propositionale visuelle Reprasentation (z.B. Pylyshyn, 1981; Kosslyn, 1981; u.a.). Dies ist eine Kontroverse, die Von beiden Seiten sehr engagiert gefuhrt wurde und noch gefuhrt wird. Wahrend es vorubergehend so schien, als ob die Diskussion zu gunsten der propositionalen Modelle entschieden ware, haben analoge Ansatze in jungerer Zeit durch das Konstrukt der ‘mentalen Modelle’ (z.B. Johnson-Laird, 1983) neuen Auftrieb erhalten. Dies durfte zu einem Wiederaufleben der Diskussion fuhren. Diese Diskussion, ihre Irrwege, aber auch dabei erzielte Erkenntnisfortschritte wollen wir ein Stuck weit aufzeigen.		Hubert D. Zimmer;Johannes Engelkamp	1986		10.1007/978-3-642-73533-2_10	philosophy;performance art	NLP	-106.08103610738627	33.03668845299663	2897
2b8f34bb53d61a6df1e074824b5aea3cba99f91e	approximation queries for building energy-aware data warehouses on mobile ad hoc networks	ipf;energy aware server;manet;iterative proportional fitting;mobile networks;manets;online analytical processing;olap;approximate queries;decision support system;dss;energy consumption;mobile ad hoc networks;decision support systems;data warehousing;mobile ad hoc network;energy aware servers;data warehouse;approximate query	This study proposes the use of approximate queries in order to improve the effectiveness and efficiency of data warehousing systems on mobile ad hoc networks (MANETs). In particular, the approach proposed shifts some computational cost, from the server to the client on the mobile network, thus increasing the power efficiency of the server. The study presents the concept of aggregate query approximation, goodness-of-fit of such approximations, and an evaluation of energy consumption.	approximation;hoc (programming language)	Hsun-Ming Lee;Francis A. Mendez Mediavilla;Enrique P. Becerra;James R. Cook	2012	IJIDS	10.1504/IJIDS.2012.045125	optimized link state routing protocol;mobile ad hoc network;decision support system;computer science;ad hoc wireless distribution service;data mining;database;computer network	Mobile	-14.389283550054	68.43395062227751	2900
e0d52893f8a3c30534f5b577711a862eb9bed175	multi-agent architecture for knowledge discovery	agent based;knowledge discovery from databases;data mining;model evaluation;multi agent systems data mining information filters;multi agent systems;recommender system;multi agent architecture;multiagent based intelligent recommendation system multiagent architecture knowledge discovery from databases business understanding data understanding data preparation agentdiscover system;information filters;multiagent systems data mining computer architecture intelligent agent databases scalability problem solving ontologies engines computer science;knowledge discovery	Knowledge discovery from databases (KDD) is a complex process composed of several phases: business understanding, data understanding, data preparation, modeling, evaluation and deployment. For each of the phases, there are many algorithms and methods available, the end-user having to select one of them. The AgentDiscover is a multi-agent based intelligent recommendation system for selection of the most appropriate solving method for each phase. This brings added value for both novice and experienced users	agent architecture;agent-based model;algorithm;compiler;data mining;database;information system;jade;java virtual machine;knowledge representation and reasoning;list of system quality attributes;multi-agent system;recommender system;software deployment;transducer;wrapper function	Daniel Pop;Viorel Negru;Calin Sandru	2006	2006 Eighth International Symposium on Symbolic and Numeric Algorithms for Scientific Computing	10.1109/SYNASC.2006.55	computer science;knowledge management;artificial intelligence;data science;multi-agent system;data mining;knowledge extraction;recommender system;data architecture	ML	-49.59341711283227	8.907488796772627	2904
9e8712fa1abc1fd0549b29dc4b123662ce16261b	einfluß der sicherheitskern-architektur auf die strukturierung von betriebssystemen		13290B) für ca. DM 20000 und ein DIN-A-3-Vierfarbenplotter (Hewlett Packard 9872A) für ca. DM 15000. Dabei wird allerdings davon ausgegangen, daß ohnehin eine größere Rechenanlage vorhanden ist, an die dieses System angeschlossen werden kann. An der Universität Kaiserslautern beispielsweise läuft ein Großteil der Software zur Erzeugung der Layoutmuster auf der Siemens 7738, die hierdurch nur mit wenigen Prozent des Durchsatzes belastet wird.	circa;die (integrated circuit);eine and zwei	Hans vor der Brück	1980	Elektronische Rechenanlagen	10.1524/itit.1980.22.16.173		OS	-102.67834974294209	31.711660243641628	2905
efc5a7d6bd7f884134e6bbf8b83c258744b2b406	optimized resource allocation in fog-cloud environment using insert select		Energy management in modern way is done using cloud computing services to fulfill the energy demands of the users. These amenities are used in smart buildings to manage the energy demands. Entertaining maximum requests in minimum time is the main goal of our proposed system. To achieve this goal, in this paper, a scheme for resource distribution is proposed for cloud-fog based system. When the request is made by the user, the allocation of Virtual Machines (VMs) to the Data Centers (DCs) is required to be done timely for DSM. This model helps the DCs in managing the VMs in such a way that the request entertainment take minimum Response Time (RT). The proposed Insert Select Technique (IST) tackle this problem very effectively. Simulation results depicts the cost effectiveness and effective response time (RT) achievement.		Muhammad Usman Sharif;Nadeem Javaid;Muhammad Junaid Ali;Wajahat Ali Gilani;Abdullah Sadam;Muhammad Hassaan Ashraf	2018		10.1007/978-3-319-98530-5_53	building automation;smart grid;response time;cloud computing;distributed computing;virtual machine;resource distribution;energy management;computer science;resource allocation	Robotics	-19.5514530599597	63.20781098155167	2906
4dcf41fd11888f8ad50cedd8199d6bd14ebf13d0	model checking and theorem proving. (le model checking et la émonstration de théorèmes)			linear algebra;model checking	Kailiang Ji	2015				Logic	-19.81166453438769	19.49774343587282	2911
15763d1fcdc617f5f615c19a1cb055a212028d48	architectural support for fair reader-writer locking	fine grain software transactional memory system;splash parallel benchmark suites;architectural support;multi threading;fairness;hardware lock;memory management;parsec benchmark suites;microbenchmarks;shared memory parallel systems;architectural support shared memory parallel systems fair reader writer locking lock based synchronization mechanism memory access lock control unit acceleration mechanism unique thread id hardware lock requestor core lcu logic autonomous core to core transfers microbenchmarks fine grain software transactional memory system splash parallel benchmark suites parsec benchmark suites lock transfer time;unique thread id;memory access;synchronisation;shared memory systems;instruction sets hardware proposals synchronization memory management coherence;requestor core;synchronization;core to core transfers;reader writer locks;lcu logic autonomous;concurrency control;lock control unit reader writer locks fairness;lock transfer time;lock control unit;acceleration mechanism;coherence;lock based synchronization mechanism;fair reader writer locking;proposals;synchronisation concurrency control multi threading shared memory systems;instruction sets;hardware	Many shared-memory parallel systems use lock-based synchronization mechanisms to provide mutual exclusion or reader-writer access to memory locations. Software locks are inefficient either in memory usage, lock transfer time, or both. Proposed hardware locking mechanisms are either too specific (for example, requiring static assignment of threads to cores and vice-versa), support a limited number of concurrent locks, require tag values to be associated with every memory location, rely on the low latencies of single-chip multicore designs or are slow in adversarial cases such as suspended threads in a lock queue. Additionally, few proposals cover reader-writer locks and their associated fairness issues. In this paper we introduce the Lock Control Unit (LCU) which is an acceleration mechanism collocated with each core to explicitly handle fast reader-writer locking. By associating a unique thread-id to each lock request we decouple the hardware lock from the requestor core. This provides correct and efficient execution in the presence of thread migration. By making the LCU logic autonomous from the core, it seamlessly handles thread preemption. Our design offers richer semantics than previous proposals, such as try lock support while providing direct core-to-core transfers. We evaluate our proposal with micro benchmarks, a fine-grain Software Transactional Memory system and programs from the Parsec and Splash parallel benchmark suites. The lock transfer time decreases in up to 30\% when compared to previous hardware proposals. Transactional Memory systems limited by reader-locking congestion boost up to 3x while still preserving graceful fairness and starvation freedom properties. Finally, commonly used applications achieve speedups up to a 7% when compared to software models.	autonomous robot;benchmark (computing);biasing;control unit;fairness measure;kerrison predictor;locality of reference;lock (computer science);lookahead carry unit;memory address;multi-core processor;mutual exclusion;network congestion;preemption (computing);process migration;read-write memory;real-time computing;real-time transcription;splash (conference);shared memory;simultaneous multithreading;software protection dongle;software transactional memory	Enrique Vallejo;Ramón Beivide;Adrián Cristal;Timothy L. Harris;Fernando Vallejo;Osman S. Unsal;Mateo Valero	2010	2010 43rd Annual IEEE/ACM International Symposium on Microarchitecture	10.1109/MICRO.2010.12	double-checked locking;giant lock;lock;lock;synchronization;parallel computing;real-time computing;spinlock;ticket lock;computer hardware;telecommunications;computer science;operating system;readers–writer lock;programming language;lock convoy	Arch	-8.216525973755488	50.97202584538473	2916
3581b0b9807e7351393542994ee62b9892f6e955	a graph-model-based testing method compared with the classification tree method for test case generation	automatic control;electronic control units;automotive engineering;formal specification;automotive industry;mechanic control units;automatic testing;industries;testing;industrial electronics;electrical equipment industry;data mining;classification tree analysis tree graphs automotive engineering electrical equipment industry automatic control industrial electronics costs automatic testing electronics industry industrial control;adaptive cruise control unit graph model testing method classification tree method test case generation automotive industry mechanic control units electronic control units industrial validation methods event sequence graphs formal specification;tree graphs;graphical user interfaces;test case generation;adaptive cruise control;indexing;electronics industry;safety critical software;event sequence graphs;testing method;industrial control;test methods;classification tree method;classification tree method event sequence graphs electronic control units;model based testing;classification tree analysis;graph model;electronic control unit;automobile industry;industrial validation methods;adaptive cruise control unit;safety critical software automobile industry formal specification	In automotive industry, mechanic control units are more and more replaced by electronic devices that are often aggregated in electronic control units (ECU). Systematic testing is one of the preferred industrial validation methods to ensure functionality of those ECUs. It is imperative to reduce the costs and improve the effectiveness of testing by automating the testing process. This paper introduces a model-based testing method using event sequence graphs and compares this approach with the classification tree method which is popular in automotive industry. A case study applies both methods to the formal specification of an adaptive cruise control unit for generation and selection of test cases. To enable a meaningful comparison, test costs and number of faults revealed by both methods will be compared.	classification tree method;decision tree;engine control unit;formal specification;imperative programming;model-based testing;test case	Fevzi Belli;Axel Hollmann;Markus Kleinselbeck	2009	2009 Third IEEE International Conference on Secure Software Integration and Reliability Improvement	10.1109/SSIRI.2009.40	non-regression testing;keyword-driven testing;reliability engineering;black-box testing;electronics;simulation;white-box testing;manual testing;classification tree method;integration testing;computer science;engineering;automotive industry;operating system;software engineering;automatic control;computer security;test management approach	SE	-57.33908731586705	32.13223010602166	2917
69112feb62e448d6499628c8fbfadaf9773ce595	the right process for each context: objective evidence needed	agile;development process;agile development;body of knowledge;software process models;empirical evidence;incremental commitment model;software process model;software development;software industry;rich;empirical evaluation;software process	"""The growing importance of software in ever more technical systems has led to new demands with respect to developing software. The demand for more functionality, higher quality, and faster delivery hence poses major challenges to the software industry. The software process community has responded with a variety of different development processes such as the waterfall model or the incremental commitment model, however, the number of late or failed projects has not decreased as much as it was desired. In the new millennium, agile development approaches promised a new way out of this dilemma. After several years of heated discussions, it is now time to evaluate applicability, advantages, and challenges of different software development approaches based on sound, empirical evidence instead of anecdotes and hearsay. This paper briefly investigates the major differences between agile and traditional approaches, illustrates the difficulties in selecting the """"right"""" approach for a given project, and proposes hypotheses for empirical evaluation, in order to build a solid body of knowledge that can be used for said selection."""	agile software development;failure;software development process;software industry;waterfall model	Ove Armbrust;H. Dieter Rombach	2011		10.1145/1987875.1987920	personal software process;verification and validation;team software process;software engineering process group;systems engineering;engineering;package development process;social software engineering;operations management;software development;iterative and incremental development;software construction;agile software development;management science;systems development life cycle;software walkthrough;empirical process;lean software development;software deployment;goal-driven software development process;software development process;software metric;software peer review	SE	-68.40512747247708	22.902622041193396	2919
ad29b4e6df0d6e15245ee0afc2418d1b685116d7	an intervalal gorithm for solving systems of linear equations to prespecified accuracy	analyse erreur;systeme equation;algorithme;algorithm;algorritmo;equation system;fixed point arithmetic;linear equations;interval arithmetic;linear equation;arithmetique intervalle;equation lineaire	We describe an interval arithmetic algorithm for solving a special class of simultaneous linear equations. This class includes but is not limited to systemsAx=b whereA andb have integer entries. The algorithm uses fixed point arithmetic, and has two properties which distinguish it from earlier algorithms: given the absolute accuracy ε desired, the algorithm uses only as much precision as needed to achieve it, and the algorithm can adjust its own parameters to minimize computation time. Wir beschreiben einen Intervallalgorithmus, der eine gewisse Klasse von linearen Gleichungssystemen löst. Diese Klasse enthält u. a. SystemeAx=b, bei denenA undb ganzzahlige Komponenten haben. Dieser Algorithmus verwendet Festpunktarithmetik und unterscheidet sich von früheren Algorithmen wie folgt. Erstens: Bei Vorgabe der gewünschten absoluten Genauigkeit ε des Ergebnisses benötigt der Algorithmus nur so viel Zwischengenauigkeit wie notwendig, um die Fehlerschranke ε zu erreichen. Zweitens kann der Algorithmus selbststeuernd seine eigenen Parameter dynamisch ändern, um die Rechenzeit zu minimieren.	algorithm;computation;eine and zwei;fixed point (mathematics);fixed-point arithmetic;internet explorer;interval arithmetic;linear equation;system of linear equations;time complexity;unified model	James Demmel;Fritz Krückeberg	1985	Computing	10.1007/BF02259840	independent equation;mathematical optimization;mathematical analysis;discrete mathematics;binary gcd algorithm;mathematics;linear equation;algorithm;algebra	Theory	-96.82305373305564	35.99142079347346	2920
86ebd01c1f4fdebd372ca9eeda6129ef0ca0177f	reversible object-oriented programming with region-based memory management - work-in-progress report		This paper presents the design and implementation Joule(^R), a reversible object-oriented language in the style of Janus, that supports common object-oriented programming patterns such as factories, iterators, and the definition of abstract datatypes. Memory management is performed using a simple notion of regions, enabling the full power of object-oriented programming to be used, while remaining completely garbage-free. The language is implemented by translation to Janus.	region-based memory management	Ulrik Pagh Schultz	2018		10.1007/978-3-319-99498-7_22	memory management;theoretical computer science;joule;object-oriented programming;janus;region-based memory management;computer science	PL	-24.682691763222937	27.107259183176645	2921
8477689919d3397d82f3df1cdb908317a7286355	a multi-way constraint computational model with purple constraint solver for direct execution of the specification	declarative programming;programming language;user interface;computer model;data processing;social system;spectrum;web service;software engineering;design method;constraint programming			Toshio Fukui	2011	CoRR		constraint logic programming;concurrent constraint logic programming;first-generation programming language;constraint programming;declarative programming;constraint satisfaction;programming domain;reactive programming;functional reactive programming;computer science;theoretical computer science;functional logic programming;database;programming paradigm;event-driven programming;procedural programming;symbolic programming;inductive programming;fifth-generation programming language;programming language;prolog;logic programming;system programming	SE	-24.253769459949886	21.097086506691333	2922
561484619fbe585a61625b38e51f5da098c00b29	securing portable document format file using extended visual cryptography to protect cloud data storage.		With the vast development in cloud computing model, various organizations and individuals often deploy the cloud without reviewing the security policies and procedures which can cause great risk in their business. Securing data in cloud storage becomes a challenging task not only for the cloud user but also to the Cloud Service Provider (CSP). Storing secret data in unencrypted form is susceptible to easy access to both the unauthorized people and the CSP. Standard encryption algorithms require more computational primitives, storage space and cost. Therefore protecting cloud data with minimal computation and storage space is of paramount significance. The Securing Portable Document Format file Using Extended Visual Cryptography (SPDFUEVC) technique proposes efficient storage to achieve data confidentiality and integrity verification with minimal computation, time complexity and storage space.	accessibility;algorithm;authorization;cloud computing;cloud storage;computation;confidentiality;data integrity;encryption;image file formats;portable document format;time complexity;visual cryptography	K. Brindha;N. Jeyanthi	2017	I. J. Network Security		computer science;database;internet privacy;world wide web	Security	-41.77283183289318	66.69021839552506	2934
4a9ed607c242d502d527cd2f7d1d75c9de3d5f05	virtual reality - anwendungen in wissenschaft, technik und medizin		"""In dieser Übersichtsarbeit werden die zukünftigen Möglichkeiten der Virtual Reality-Technik im Bereich der Visualisierung von Daten aus Supercomputer-Simulationen und Messungen diskutiert. Die Aufgabe der Visualisierungsforschung besteht in der Bereitstellung von Methoden und Programmen zur Darstellung großer 3und (3+1) -dimensionaler Datenfelder, um die Auswertung und Vermittlung der Ergebnisse zu unterstützen. Typische Anwendungsbeispiele sind Simulationsergebnisse aus der Strömungsdynamik, Materialforschung, Meteorolgie und Molekulardynamik, aber vor allem auch die großen Datenfelder aus bildgebenden Verfahren in der Medizin. Die neue Virtual Reality-Technik mit ihrem Versprechen, künstliche Räume dreidimensional durchwandern und manipulieren zu können, erscheint in diesem Zusammenhang sehr verheißungsvoll. Anhand von Erfahrungen in Anwendungen in der Operationsplanung in der Medizin und im Molekulardesign werden die gegenwärtigen Möglichkeiten des Einsatzes von VR-Systemen diskutiert. Dabei werden zwei technische Varianten, das „klassische"""" VR-System mit Stereo-Videobrille und Datenhandschuh und das neuere BOOM-System mit hochauflösenden Stereo-Bildschirmen, verglichen. Abschließend werden FuEProjekte und notwendige technische Entwicklungen vorgestellt."""	eine and zwei;like button;supercomputer;unified model;vhf omnidirectional range;virtual reality	Wolfgang Krüger	1993	it+ti - Informationstechnik und Technische Informatik	10.1524/itit.1993.35.3.31	embedded system;virtual reality;computer science;human–computer interaction	OS	-107.27385867727548	31.554795435824005	2945
ed3707528b017796f5a11c54f91fa77fbccce7f6	towards context-aware transaction services	sensibilidad contexto;distributed system;metodo adaptativo;adaptability;adaptabilite;tratamiento transaccion;systeme reparti;context aware;informatique mobile;componente logicial;interoperabilite;interoperabilidad;par a par;semantics;composant logiciel;methode adaptative;semantica;semantique;adaptabilidad;sistema repartido;poste a poste;adaptive method;software component;next generation;interoperability;sensibilite contexte;transaction processing;semantic relations;mobile computing;peer to peer;traitement transaction	For years, transactional protocols have been defined for particular application needs. Traditionally, when implementing a transaction service, a protocol is chosen and remains the same during the system execution. Nevertheless, the dynamic nature of nowadays application contexts (e.g., mobile, ad-hoc, peer-to-peer) and context variations (semantics-related aspects) motivates the need for transaction service adaptation. Next generation of transaction services should be adaptive or even better self-adaptive. This paper proposes CATE: (1) a componentbased architecture of standard 2PC-based protocols and (2) a ContextAware Transaction sErvice. Self-adaptation of CATE is obtained by context awareness and component-based reconfiguration. This allows CATE to select the most appropriate protocol with respect to the execution context. We show that using CATE performs better than using only one commit protocol in a variable system and that the reconfiguration cost is negligible.	component-based software engineering;context awareness;high-level programming language;hoc (programming language);model-driven architecture;next-generation network;peer-to-peer;performance;sequence diagram;three-phase commit protocol;two-phase commit protocol;unified modeling language	Romain Rouvoy;Patricia Serrano-Alvarado;Philippe Merle	2006		10.1007/11773887_21	interoperability;real-time computing;adaptability;two-phase commit protocol;transaction processing;telecommunications;distributed transaction;computer science;component-based software engineering;operating system;x/open xa;database;distributed computing;semantics;programming language;mobile computing	DB	-39.71798201844683	39.80487597246455	2946
6d0af7e1b6957027d4ae92f2c9c414af7c710db3	implementing reengineering using information technology	information technology;time management;information flow;bpr;business process change;internet;organizational change	Reengineering is becoming an increasingly popular option for corporations seeking radical process change. Central to the success of reengineering is the coordination of information technology (IT) throughout the organization. Essentially, IT represents the core mechanism of information flow. When companies improve core IT processes, such as gathering data only once, integrating cross‐functional systems or increasing information speed to customers, radical business process change is possible. However, for IT to be an enabler of reengineering or organizational change, it is imperative that managers are conversant with the various methods by which IT can help advance process change. At the same time, management needs to be aware of the numerous pitfalls that may doom any change effort using IT.	code refactoring	Peng S. Chan;Carl Land	1999	Business Proc. Manag. Journal	10.1108/14637159910297367	change control;the internet;information flow;time management;economics;business process reengineering;systems engineering;knowledge management;change management;process management;management;law;information technology	HCI	-73.93784208586514	8.140297938080415	2950
28043e780c15ccd7a1a97e2c43be5b27dc9688ba	on the concepts of problem and problem-solving method	problem solving method	Abstract   The concepts of problem, problem-solving method, and the application of a problem-solving method to a problem are given precise formulations, based on abstract data types. These formulations are argued to agree with the intuitive understanding of these ideas, thereby formalizing them. This formalization is based on few basic concepts: abstract data type and an extension mechanism (here, a general cluster-like module). Moreover, by embodying ideas related to stepwise refinement they are applicable both to problem solving in general and to the process of program development. Examples are provided to illustrate the main ideas and their application.	problem solving	Paulo A. S. Veloso	1987	Decision Support Systems	10.1016/0167-9236(87)90072-8	computer science;algorithm	ECom	-19.6543740507619	4.741052399601332	2954
7923fff2984b66720060ce04ee516de067d54311	the heart of eclipse	web;soa;service;development environment;business;process;oracle	"""A look inside and extensible plug-in architectureECLIPSE is both an open, extensible development environment forbuilding software and an open, extensible application frameworkupon which software can be built. Considered the most popular JavaIDE, it provides a common UI model for working with tools andpromotes rapid development of modular features based on a plug-incomponent model. The EclipseFoundation designed the platform to run natively on multipleoperating systems, including Macintosh, Windows, and Linux,providing robust integration with each and providing rich clientsthat support the GUI interactions everyone is familiar with: dragand drop, cut and paste (clipboard), navigation, and customization.You can think of Eclipse as a """"design center"""" supported by adevelopment team of 300 or more developers whom you can leveragewhen developing your own software."""	clipboard (computing);cut, copy, and paste;eclipse;graphical user interface;interaction;linux;microsoft windows;plug-in (computing)	Dan Rubel	2006	ACM Queue	10.1145/1165754.1165767	oracle;embedded system;service;computer science;operating system;software engineering;service-oriented architecture;database;development environment;programming language;world wide web;process	SE	-33.15297049386846	41.986855820918365	2956
75073f2ff49959fa5dd51844f0190b10c08d92be	parallelizing divide-and-conquer algorithms - microtasking versus autotasking	parallel algorithm;fortran;parallel implementation;convex hull;divide and conquer	Algorithms based on a divide-and-conquer strategy are well qualified for being implemented in a multitasking environment. The idea of the divide-and-conquer paradigm is to fragment a problem into subproblems of the same kind, to solve the subproblems recursively, and, finally, to combine the solutions of the subproblems into a solution of the original problem. The subdivision in smaller problems which can be solved independently provides the possibility for parallel execution on multiple processors. In this paper the parallel implementation of a divide-and-conquer algorithm to compute the convex hull in the plane is discussed. The algorithm is implemented in FORTRAN on a CRAY Y-MP8/832 using the CRAY multitasking strategies. The concepts of microtasking and autotasking are compared with respect to their qualification for the parallelization of REPEAT loop constructs which constitute the main part of the described divide-and-conquer algorithm.	algorithm;microwork;parallel computing	Renate Knecht	1990		10.1007/3-540-53065-7_132	computer architecture;parallel computing;computer science;theoretical computer science	Theory	-11.476849349710639	37.259442352138784	2962
d65010ffa6b52233b1864b44b5de2db4544f35af	automatic workarounds for web applications	automatic workarounds;rewrite rule;google map;web api;web applications;transaction data;settore ing inf 05 sistemi di elaborazione delle informazioni;failure detector	We present a technique that finds and executes workarounds for faulty Web applications automatically and at runtime. Automatic workarounds exploit the inherent redundancy of Web applications, whereby a functionality of the application can be obtained through different sequences of invocations of Web APIs. In general, runtime workarounds are applied in response to a failure, and require that the application remain in a consistent state before and after the execution of a workaround. Therefore, they are ideally suited for interactive Web applications, since those allow the user to act as a failure detector with minimal effort, and also either use read-only state or manage their state through a transactional data store. In this paper we focus on faults found in the access libraries of widely used Web applications such as Google Maps. We start by classifying a number of reported faults of the Google Maps and YouTube APIs that have known workarounds. From those we derive a number of general and API-specific program-rewriting rules, which we then apply to other faults for which no workaround is known. Our experiments show that workarounds can be readily deployed within Web applications, through a simple client-side plug-in, and that program-rewriting rules derived from elementary properties of a common library can be effective in finding valid and previously unknown workarounds.	application programming interface;client-side;data store;dynamic data;experiment;failure detector;library (computing);plug-in (computing);read-only memory;rewriting;run time (program lifecycle phase);web application;workaround	Antonio Carzaniga;Alessandra Gorla;Nicolò Perino;Mauro Pezzè	2010		10.1145/1882291.1882327	web application;web modeling;computer science;engineering;web api;software engineering;transaction data;data mining;database;programming language;world wide web;failure detector	SE	-57.365759423488484	41.446871187041594	2968
0411689f0060fc44f265789a3e98c16565ef1168	heaven: eine hierarchische speicher- und archivierungsumgebung für multidimensionale array- datenbankmanagement-systeme			eine and zwei	Bernd Reiner	2005			heaven;art;performance art	Arch	-98.94109146483612	25.15210251902321	2973
0ee3d65d411a629a8000b4da887072d48b5eb183	a method to evaluate cfg comparison algorithms	software plagiarism detection cfg comparison algorithm control flow graph cfg similarity algorithm malware detection;program debugging flow graphs invasive software;algorithm design and analysis approximation algorithms software algorithms malware accuracy software benchmark testing	Control-Flow Graph (CFG) similarity is a core technique in many areas, including malware detection and software plagiarism detection. While many algorithms have been proposed in the literature, their relative strengths and weaknesses have not been previously studied. Moreover, it is not even clear how to perform such an evaluation. In this paper we therefore propose the first methodology for evaluating CFG similarity algorithms with respect to accuracy and efficiency. At the heart of our methodology is a technique to automatically generate benchmark graphs, CFGs of known edit distances. We show the result of applying our methodology to four popular algorithms. Our results show that an algorithm proposed by Hu et al. is most efficient both in terms of running time and accuracy.	algorithm;benchmark (computing);context-free grammar;holographic principle;malware;preprocessor;time complexity	Patrick P. F. Chan;Christian S. Collberg	2014	2014 14th International Conference on Quality Software	10.1109/QSIC.2014.28	computer science;theoretical computer science;data mining;programming language;algorithm	SE	-58.798335500743	55.100833004004336	2975
7c42e755366aa1896c80b7fd43e36b72f5a7329d	quality assurance in model-based software development: challenges and opportunities		Modeling is a common practice in modern day software engineering. Since the mid 1990‟s the Unified Modeling Language (UML) has become the de facto standard for modeling software systems. The UML is used in all phases of software development – ranging from the requirements phase to the maintenance phase. However, the manner in which the UML is used differs widely from project to project and from developer to developer. This illustrates an apparent lack of quality awareness in the use of UML. In this paper I will discuss the challenges and opportunities there are for using quality assurance for software modeling for improving the quality and productivity of software development. 1 The State of the Practice and the need for Quality Assurance in Model Based Software Development Over the past 15 years I have visited many software development projects that use some form of modeling. I have collected both quantitative as well as qualitative data about the way in which those projects performed software modeling and about the impact of their modeling practice on overall project productivity and quality. Some of the key finding from these studies are the following (these are elaborated in [1]): There is a large variation in styles of software modeling between different developers and across different projects – also within individual organizations. In virtually all projects that we visited: o the UML models contained a large amount of incompleteness and inconsistency o no processes or techniques were applied for quality assurance of UML models This state of the practice suggests that there are significant opportunities for achieving improvements for quality assurance for the modeling activities in software projects. In the remainder of this paper, I will discuss: 2 Michel R.V. Chaudron ─ Economics of modeling: Does modeling actually help in creating better software? How much modeling is enough? ─ What are practical ways of starting with quality assurance in model-based software development? ─ Promises from the state of the art in software model quality assurance techniques and challenges for the future 2 Does Investing in Modeling and Quality of Modeling make Business Sense? In his seminal book Software Engineering Economics Barry Boehm reports on the relation between the stage of a project (starting, middle, finishing) and the cost of repairing defects. The relation is such that the cost of repairing increases exponentially with the progress of a project. More precisely this principle is nowadays formulated as that the cost of repairing a defect grows exponentially with the life-time of a defect. When we look at the activities of a software development project that have the largest impact on the final quality on the system, then these are the requirements and architecting/design activities. Hence, from first principles it makes economic sense to ensure quality of the early products of software projects. Next, one can ask: does it help to model our design in a systematic manner using UML? I will draw from some empirical studies to suggest that using models indeed makes business sense: In a controlled experiment subjects were shown UML models that either lacked some information or contained some inconsistencies versus models that were complete. It was found that for the incomplete models, subjects had much larger variation in the interpretation of the meaning of the model [2]. Clearly, this variation in interpretation increases the chances of miscommunication and misinterpretation. Subsequently we tried to find evidence in actual industrial software development projects. To this end, we studied the defects that we found in the project repository of a medium size industrial project [3]. We counted the number of defects per class (in the implementation), and subsequently we looked at the manner in which each of these classes was modeled in the UML design. This showed that classes for which more detail was present in the UML model (to be precise in the sequence diagrams) stage of a project cost of repair Figure 1: Relation between stage of a project and cost of repair Quality Assurance in Model-based Software Development Challenges and Opportunities 3 contained fewer defects than classes for which less detail was present in the UML model. At the same time, this study also showed that there is an effect of diminishing returns in model quality (See Fig 2): at some point, adding more detail to a model no longer correlates with a significant decrease in the expected number of defects in that class. This study shows that the quality of sequence diagrams is important when UML is used as blueprint for implementation. Figure 2: Relation between Detail in Sequence Diagram and Defect Density Complementary to the support for modeling implied by these quantitative studies, there are also more subjective pieces of evidence – obtained through surveys and interviews – in which designers and programmers state that they feel that the use of modeling aids in achieving a common understanding of a design within an team and that modeling helps as an aid in communication. These benefits ultimately lead to a positive effect on productivity and quality [4]. Additional evidence that supports the importance of good documentation comes from an interesting corner: the documentation-averse agile software development community: a recent study amongst agile development teams showed that they would prefer to have more documentation than they normally produce [5]. 3 Practical Lightweight Methods for Starting Quality Assurance for Model Based Software Development In this section I will discuss some lightweight, easy to use methods for quality assurance of UML models. 3.1 Quality of UML models The basis for any quality assessment is a quality model. In the area of software, several quality models exist. All of them define quality through a hierarchical composition which has in the leave-nodes some measurable characteristic of the software. This same approach can be applied for establishing a quality model for UML models. Rather than measuring properties in the source code, properties must be measured from the UML models. Some types of measurements (a.k.a. metrics) carry over fairly straightforwardly from code to models. However, some other metrics may 4 Michel R.V. Chaudron need to be adapted. Furthermore, UML models also offer an opportunity for defining new metrics that cannot be determined from the source code. Consider for example a measure for the complexity or criticality of a use case by counting the number of sequence diagrams that support a use case. Quality models for software used to take a “one-size-fits-all” approach. In a recent paper, we propose that the quality model for UML models should be tailored to the purpose of the task that the software-model has to support [6]. In particular, a team should determine how the models are aimed to be used or more pragmatically for which tasks they are actually most used. Figure 3 shows an example of a quality model for UML that considers maintenance and development as key activities. Figure 3: Example Quality Model for Software Models 3.2 Assessing correspondence between source code and UML model A key factor for harvesting the benefits of modeling are to ensure that the implementation actually conforms (to a large degree) to the UML design. The degree in which an implementation follows a design depends amongst other on the degree of detail and completeness in which the design was modeled. In practice we see that UML designs most often contain between 20% and 50% of the classes of the ultimate implementation. When asked how designer chose what to include in their models, designers state that they focus on complex and critical parts [7]. In line with the earlier finding that there are diminishing returns for raising the quality level of a model after some point, we should expect that there implementation may deviate somewhat from the model. This may be because the model describes the system only at a high level of abstraction and hence omits some details, or because designers in the implementation phase decide on alternative implementations (which may or may not be improvements over the design). For monitoring the compliance of the implementation to the design, we need a method that does not aim to find a perfect Quality Assurance in Model-based Software Development Challenges and Opportunities 5 match between implementation and design, but is robust against differences and tries to provide insight in the severity of potential deviations. Figure 4 shows a graph generated by Van Opzeeland [8] who provides one possible approach for a robust comparison between UML model and implementation. Figure 4 Graph showing correspondence between Design and Implementation The method works as follows: first a mapping is created between classes in the implementation to classes in the UML model (for non-OO languages, this would also work for functions/procedures or components). This step is greatly facilitated if the same naming conventions are used in design and implementation. Subsequently, a set of metrics (e.g. coupling, size (number of methods, number of attributes), depth-ofinheritance) is computed for implementation classes and corresponding UML-classes. Then, for each class one point in draws in the graph where the x-coordinate is the metric for that class in the design and the y-coordinate is the metrics for the corresponding class in the implementation. The resulting graph shows a line x=y when there is a perfect mapping between design and implementation. In practice we see a pattern that shows a linear relationship within some bandwidth. This pattern enables the easy visual identification of outliers which form the largest deviation between design and implementation and thus are the most important candidates for scrutiny. 3.3 Low Hanging Fruit for Quality Assurance for Model Based Software Development It has be	agile software development;bandwidth (signal processing);barry boehm;blueprint;coupling (computer programming);documentation;expect;fits;graph (discrete mathematics);high-level programming language;michel hénon;programmer;requirement;self-organized criticality;sequence diagram;software bug;software engineering;software industry;software quality;software system;unified modeling language	Michel R. V. Chaudron	2012		10.1007/978-3-642-27213-4_1	software quality control	SE	-65.81965980841694	30.951071085911305	2980
ee60e5f099c8ea2ed010823648ac337283dff482	admission and congestion control for 5g network slicing		Network Slicing has been widely accepted as essential feature of future 5thGeneration (5G) mobile communication networks. Accounting the potentially dense demand of network slices as a cloud service and the limited resource of mobile network operators (MNOs), an efficient inter-slice management and orchestration plays a key role in 5G networks. This calls advanced solutions for slice admission and congestion control. This paper proposes a novel approach of inter-slice control that well copes with existing pre-standardized 5G architectures.		Bin Han;Antonio De Domenico;Ghina Dandachi;Anastasios Drosou;Dimitrios Tzovaras;Roberto Querio;Fabrizio Moggio;Ömer Bulakci;Hans D. Schotten	2018	2018 IEEE Conference on Standards for Communications and Networking (CSCN)	10.1109/CSCN.2018.8581773	orchestration (computing);computer network;network architecture;network congestion;cloud computing;distributed computing;operator (computer programming);mobile telephony;slicing;computer science;cellular network	Mobile	-14.12299536510137	86.2190221918835	2982
980bfa9abae4a187b9e985d25adbfc1bdc5cdd14	biometrics based privacy-preserving authentication and mobile template protection		Smart mobile devices are playing a more and more important role in our daily life. Cancelable biometrics is a promising mechanism to provide authentication to mobile devices and protect biometric templates by applying a noninvertible transformation to raw biometric data. However, the negative effect of nonlinear distortion will usually degrade the matching performance significantly, which is a nontrivial factor when designing a cancelable template. Moreover, the attacks via record multiplicity (ARM) present a threat to the existing cancelable biometrics, which is still a challenging open issue. To address these problems, in this paper, we propose a new cancelable fingerprint template which can not only mitigate the negative effect of nonlinear distortion by combining multiple feature sets, but also defeat the ARM attack through a proposed feature decorrelation algorithm. Our work is a new contribution to the design of cancelable biometrics with a concrete method against the ARM attack. Experimental results on public databases and security analysis show the validity of the proposed cancelable template.		Wencheng Yang;Jiankun Hu;Song Wang;Qianhong Wu	2018	Wireless Communications and Mobile Computing	10.1155/2018/7107295	computer science;distributed computing;nonlinear distortion;biometrics;mobile device;information privacy;authentication;decorrelation	DB	-48.58383132039878	68.01992416120193	2989
78426f996c9e25a312b80dad5be1645c0550d4a4	scenario-based system design with colored petri nets: an application to train control systems		For the goal of model-based system software development, this paper exploits the formalism of colored Petri nets (CPNs) to design complex systems based on scenarios. The specification of UML sequence diagrams which are easily understood by customers, requirement engineers and software developers are adopted to represent scenarios as specification models. A scenario is a partial description of the system behavior, describing how users, system components and the environment interact. Thus scenarios need to be synthesized in order to obtain an overall system behavior. A large number of works (e.g., Whittle and Schumann in Proceedings of the 2000 international conference on software engineering, pp 314–323, 2000; Elkoutbi and Keller in Application and theory of Petri nets, 2000; Damas et al. in Proceedings of the 14th ACM SIGSOFT international symposium on foundations of software engineering, pp 197–207, 2000; Uchitel et al. in IEEE Trans Softw Eng 29(2):99–115, 2003) have investigated scenario synthesis providing approaches or algorithms. These synthesis approaches and algorithms result in either Petri net models (e.g., Elkoutbi and Keller 2000; Ameedeen and Bordbar in 12th international IEEE enterprise distributed object computing conference (EDOC), pp 213–221, 2008) that are mainly suitable for scenario validation or other forms of behavior models (e.g., labeled transition systems in Damas et al. 2000; Uchitel et al. 2003 and statecharts in Krüger et al. in Distributed and parallel embedded systems, pp 61–71, 1999; Whittle et al. 2000) that may be regarded as design models. Petri nets are well known for describing distributed and concurrent complex systems. Furthermore, numerous techniques, e.g., simulation, testing, state space-based techniques, structural methods and model checking, are currently available for analyzing Petri net models. Therefore, design models in the form of Petri nets, integrating all scenarios into a coherent whole and fitting for further detailed design, are promising. To this end, we present a top-down approach to establish hierarchical CPNs in accordance with specified scenarios (i.e., sequence diagrams). This approach makes use of explicitly labeling component states in the sequence diagrams to correlate scenarios. In addition, the techniques of state space analysis and ASK-CTL model checking are used to verify the correctness and consistency of the CPN model with respect to standard and system-specific properties. As the inspiration of the presented approach derives from the development of train control systems, we present an running example of designing the on-board subsystem of a satellite-based train control system to show the feasibility of our approach.	algorithm;coherence (physics);coloured petri net;complex systems;control system;correctness (computer science);embedded system;enterprise distributed object computing;formal system;model checking;on-board data handling;requirements analysis;sequence diagram;simulation;software developer;software development;software engineering;state space;systems design;top-down and bottom-up design;unified modeling language;word lists by frequency	Daohua Wu;Eckehard Schnieder	2016	Software & Systems Modeling	10.1007/s10270-016-0517-1	simulation;computer science;systems engineering;engineering;theoretical computer science;software engineering;process architecture;programming language;petri net	SE	-43.867064884628185	31.592540167566394	2990
39ef6c4015ccbe3894eece7dd7551d36fcb69c1b	an application of distributed simulation for hybrid modeling of offshore wind farms	offshore wind farm;offshore wind farms;maintenance;repair and operations;conference proceedings;maintenance repair and operations;conference paper;distributed simulation;hybrid simulation	The work in progress paper presents a case study on M&S of offshore wind farms. Two simulation models have been developed, an agent-based model and a discrete-event model, with the former modeling turbine failures using a degradation function and the latter modeling the Maintenance, Repair and Operations (MRO) strategies. The models have been implemented in NetLogo and Simul8 respectively. In this paper we present ongoing work in hybrid simulation which uses the IEEE 1516 HLA standard for distributed simulation for synchronized model execution and dynamic information exchange between the agent-based and the discrete-event models.	agent-based model;elegant degradation;event (computing);high-level architecture;information exchange;netlogo;simulation	Navonil Mustafee;M'Hammed Sahnoun;Andi Smart;Phil Godsiff	2015		10.1145/2769458.2769492	simulation;engineering;operations management;marine engineering	Robotics	-37.56255025571503	22.095261467392113	2992
642dbfed2d1971a54c7a39665077e95b02311d03	interdependent security risk analysis of hosts and flows	risk management correlation web servers monitoring iterative methods computational modeling;web servers;risk management;iterative methods;risk propagation network risk assessment flow provenance;computational modeling;monitoring;correlation;botnet initiated attacks interdependent security risk analysis high risk host detection high risk flow detection high throughput network security monitoring comprehensive risk assessment method risk propagation interdependency relationship risk scores flow provenance real world data sets distributed denial of service attacks;risk management computer network security invasive software	Detection of high risk hosts and flows continues to be a significant problem in security monitoring of high throughput networks. A comprehensive risk assessment method should consider the risk propagation among risky hosts and flows. In this paper, this is achieved by introducing two novel concepts. First, an interdependency relationship among the risk scores of a network flow and its source and destination hosts. On the one hand, the risk score of a host depends on risky flows initiated by or terminated at the host. On the other hand, the risk score of a flow depends on the risk scores of its source and destination hosts. Second, which we call flow provenance, represents risk propagation among network flows which considers the likelihood that a particular flow is caused by the other flows. Based on these two concepts, we develop an iterative algorithm for computing the risk score of hosts and network flows. We give a rigorous proof that our algorithm rapidly converges to unique risk estimates, and provide its extensive empirical evaluation using two real-world data sets. Our evaluation shows that our method is effective in detecting high risk hosts and flows and is sufficiently efficient to be deployed in the high throughput networks.	algorithm;flow network;interdependence;iterative method;risk assessment;sensor;software propagation;throughput	Mohsen Rezvani;Verica Sekulic;Aleksandar Ignjatovic;Elisa Bertino;Sanjay Jha	2015	IEEE Transactions on Information Forensics and Security	10.1109/TIFS.2015.2455414	risk management;computer science;data mining;iterative method;computational model;world wide web;computer security;correlation;web server	Security	-62.5272780518827	65.06990021412626	2993
a5b4b279964842879d8f008824b9dd26bff15fb2	performance modeling of a compressible hydrodynamics solver on multicore cpus			central processing unit;multi-core processor;solver	Raphaël Poncet;Mathieu Peybernes;Thibault Gasc;Florian De Vuyst	2015		10.3233/978-1-61499-621-7-449	parallel computing;compressibility;computer science;multi-core processor;computational science;solver	EDA	-6.188667734453397	38.26060659475969	3010
e98376f05e6ec5a112f07cc12b2f9f747688d9ed	a hierarchical p2p traffic localization method with bandwidth limitation	p2p streaming application hierarchical p2p traffic localization method bandwidth limitation peer to peer applications traffic costs internet service providers autonomous systems ases cross isp as traffic reduction network aware strategies p2p systems router aided method p2p application software;internet;peer to peer computing internet;bandwidth peer to peer computing delays proposals throughput application software;peer to peer computing	Recently, most peer-to-peer (P2P) applications ignore traffic costs at internet service providers (ISPs), and thus generate a large amount of traffic crossing ISPs or autonomous systems (ASes) on the Internet. To reduce the cross-ISP/AS traffic, the existing approaches introduce network-aware strategies in which a lot of modifications of P2P systems are required. In this paper, we propose a router-aided method for localizing P2P traffic hierarchically with multiple levels. By intentionally limiting the bandwidth of each connection path between peers based on geographical location of the peers' destinations, the traffic can be localized. Compared to the existing locality-enhancing approaches, our proposed method does not require dedicated servers, cooperation between ISPs and users, or any modification of existing P2P application software. Therefore, the proposal can be easily utilized by all types of P2P applications. The experimental results obtained with P2P streaming applications show that our proposed method not only successfully realizes traffic localization but also maintains a good performance of P2P applications.	autonomous system (internet);bandwidth (signal processing);dedicated hosting service;locality of reference;location (geography);peer-to-peer;quality of service;router (computing);traffic classification	Hiep Hoang-Van;Takumi Miyoshi;Olivier Fourmaux	2013	The 2013 RIVF International Conference on Computing & Communication Technologies - Research, Innovation, and Vision for Future (RIVF)	10.1109/RIVF.2013.6719880	computer science;distributed computing;traffic shaping;dead peer detection;computer security;computer network	HPC	-13.595189715263148	76.64135630189065	3011
3a4fac36d0fd7b43d410459a0e4e32a899f99f37	location independent routing in process network overlays	ip networks;message passing;overlay networks;resource allocation;telecommunication channels;telecommunication network routing;transport protocols;virtual machines;ip address;tcp underlays;tcp-ip;application development;application management;asynchronous message passing;asynchronous point-to-point communication channels;contextual equivalence;core erlang style;dedicated routing infrastructure;defer object overlay routing;distributed computing;distribution routing;flat identifiers;forwarding chains;interobject messages;load balancing;location independent routing;location independent routing scheme;location servers;location transparency;low-level model;message routing;mobile objects;network-aware semantics;network-oblivious behavior;network-oblivious operational semantics;networking layer;object-based language;process network overlays;processing nodes abstract network;distributed systems;object mobility;routing;computer science	In distributed computing, location transparency -- the decoupling of objects, tasks, and virtual machines from their physical location -- is desirable in that it can simplify application development and management, and enable load balancing and efficient resource allocation. Many existing systems for location transparency are built on top of TCP/IP. We argue that addressing mobile objects in terms of the host where they temporarily reside may not be the best design decision. When objects can migrate, it becomes necessary to use a dedicated routing infrastructure to deliver inter-object messages, such as location servers or forwarding chains. This incurs high costs in terms of complexity, overhead, and latency. In this paper, we defer object overlay routing to an underlying networking layer, by assuming a location independent routing scheme in place of TCP/IP. In this scheme, messages are directed to destinations determined by flat identifiers instead of IP addresses. Consequently, messages are delivered directly to a recipient object, instead of a possibly out-of-date location. We explore the scheme in the context of a small object-based language with asynchronous message passing, in the style of core Erlang. We provide a standard, network-oblivious operational semantics of this language, and a network-aware semantics which takes many aspects of distribution and message routing into account. The main result is that execution of a program on top of an abstract network of processing nodes connected by asynchronous point-to-point communication channels preserves the network-oblivious behavior in a sound and fully abstract way, in the sense of contextual equivalence. This is a novel and strong result for such a low-level model. Previous work has addressed distributed implementations only in terms of fully connected TCP underlays. But in this setting, contextual equivalence is typically too strong, due to the need for locking to resolve preemption arising from object mo- ility.	routing	Mads Dam;Karl Palmskog	2014		10.1109/PDP.2014.30	communications protocol;routing;message passing;real-time computing;overlay network;resource allocation;computer science;virtual machine;operating system;database;distributed computing;transport layer;computer network	Theory	-33.03601700138974	38.87402784482788	3016
ab1e248b0145dc86c0a1f6ca2e9cab6937f8bcea	information theoretic framework of trust modeling and evaluation for ad hoc networks	trust evaluation information theoretic framework trust propagation modeling entropy based model probability secure ad hoc routing packet forwarding;routing protocols;trust modeling and evaluation ad hoc networks security;probability;evaluation method;ad hoc network;trust model;indexing terms;satisfiability;ad hoc networks peer to peer computing sun routing public key student members information security measurement uncertainty entropy throughput;it value;telecommunication security;radiowave propagation ad hoc networks probability telecommunication security routing protocols;ad hoc networks;radiowave propagation;information theoretic;ad hoc routing	The performance of ad hoc networks depends on cooperation and trust among distributed nodes. To enhance security in ad hoc networks, it is important to evaluate trustworthiness of other nodes without centralized authorities. In this paper, we present an information theoretic framework to quantitatively measure trust and model trust propagation in ad hoc networks. In the proposed framework, trust is a measure of uncertainty with its value represented by entropy. We develop four Axioms that address the basic understanding of trust and the rules for trust propagation. Based on these axioms, we present two trust models: entropy-based model and probability-based model, which satisfy all the axioms. Techniques of trust establishment and trust update are presented to obtain trust values from observation. The proposed trust evaluation method and trust models are employed in ad hoc networks for secure ad hoc routing and malicious node detection. A distributed scheme is designed to acquire, maintain, and update trust records associated with the behaviors of nodes' forwarding packets and the behaviors of making recommendations about other nodes. Simulations show that the proposed trust evaluation system can significantly improve the network throughput as well as effectively detect malicious behaviors in ad hoc networks.	centralized computing;computer simulation;entropy (information theory);hoc (programming language);routing;software propagation;theory;throughput;trust (emotion)	Yan Lindsay Sun;Wei Yu;Zhu Han;K. J. Ray Liu	2006	IEEE Journal on Selected Areas in Communications	10.1109/JSAC.2005.861389	wireless ad hoc network;optimized link state routing protocol;computer science;distributed computing;computer security;computational trust;computer network	Mobile	-53.732012130837454	78.64946324941631	3019
52d53bdfa523f981494ef9c76cdbab87cd73c512	the performance implications of spin-waiting alternatives for shared-memory multiprocessors			shared memory	Thomas E. Anderson	1989			parallel computing;distributed computing;distributed memory;cache-only memory architecture;spin-½;computer science;shared memory	Arch	-10.136761622423725	43.14908792394941	3020
7e467281d5564be9bd787eed90417d381c3281dd	cap on mobility control for 4g lte networks		The CAP theorem [1] exposes the fundamental tradeoffs among three key properties of strong consistency, availability and partition tolerance in distributed networked systems. In this position paper, we take the CAP perspective on 4G mobility control. We view the control-plane management for mobility support as a distributed signaling system. We show that the impossibility result of the CAP theorem also holds for mobility control: It is impossible for any mobility control to guarantee sequential consistency, high service availability, and partition tolerance simultaneously. Unfortunately, the current 4G system adopts its mobility scheme with the notion of sequential consistency. Our empirical study further confirms that, the incurred data unavailability (i.e., data service suspension) time is comparable to that induced by wireless connectivity setup. We argue that the desirable mobility control for the upcoming 5G networks should take a paradigm shift. We discuss our early effort on re-examining the consistency notion for higher availability and fault tolerance.	compaq lte	Yuanjie Li;Zengwen Yuan;Chunyi Peng;Songwu Lu	2016		10.1145/2980115.2980120	real-time computing;telecommunications;engineering;computer security	Mobile	-11.754473982236249	85.58151214990596	3021
80bd5f7c95c9b2fbe949d6b2c14a58a607d98629	does quic make the web faster?	google;protocols;web pages;browsers;servers;internet;bandwidth	Increase in size and complexity of web pages has challenged the efficiency of HTTP. Recent developments to speed up the web have resulted in two promising protocols, HTTP/2 (RFC 7540) at the application layer and QUIC (multiplexed stream transport over UDP). Google servers are using HTTP/2 and QUIC whereas other major sites like Facebook and Twitter have begun using HTTP/2. In this paper, we compare the performance of HTTP/2 vs QUIC+SPDY 3.1 by studying the Web page load times. In the first set of experiments, we serve synthetic pages (only static objects) over both protocols in emulated controlled network conditions and then extend it to real network, both wired and cellular (2G/3G in India and 3G/4GLTE in US). Further, we conduct experiments on a set of web pages on the most popular sites from the Internet (Alexa Rankings) in controlled conditions. We find QUIC to perform better overall under poor network conditions (low bandwidth, high latency and high loss), for e.g. more than 90% of synthetic pages loaded faster with QUIC in 2G compared to 60% in 4GLTE. This is due to the lower connection establishment latency and improved congestion control mechanism in QUIC. However, QUIC does not offer significant advantage when a webpage consists of many small-sized objects.	blocking (computing);chemspider;computation;dhrystone;dynamic web page;embedded system;emulator;experiment;hol (proof assistant);http/2;hypertext transfer protocol;load (computing);multiplexing;network congestion;server (computing);synthetic data;uncontrolled format string;world wide web	Prasenjeet Biswal;Omprakash Gnawali	2016	2016 IEEE Global Communications Conference (GLOBECOM)	10.1109/GLOCOM.2016.7841749	quic;communications protocol;the internet;telecommunications;computer science;operating system;web page;internet privacy;world wide web;bandwidth;server;computer network	Metrics	-6.400950792143735	92.43883483290801	3027
aeb103fc19fde3247d80658b0c41a1242db4b2e6	cosmos-2d - ein system zur vollautomatischen, optischen und geometrie-invarainten vermessung von ebenen strukturoberflächen	von ebenen strukturoberfl;ein system zur vollautomatischen	Die Qualitatskontrolle gewinnt in der Industrie zunehmend an Bedeutung. Standig steigende Anforderungen an die Gute von Produkten zwingen zu umfangreichen Kontrollen wahrend der Fertigung. Die technische Komplexitat der herzustellenden Artikel bedingt immer aufwendigere und kostenintensivere Produktionsverfahren.		Bernhard Bürg;Helmut Guth	1991		10.1007/978-3-642-76501-8_31	mathematical physics;cosmos;philosophy	NLP	-101.85357719550817	27.633077392730865	3031
cf2900381fc67775e18b027d50dc4b3c2c1d1810	data conversion development: a tool-supported approach		This article provides a brief introduction to an approach toward data conversion development. The article discusses activities in the area of conversion software development, as well as a model for the life cycle of this development process. Also analyzed is a possible method of tool support for the development process.		Janis Plume	1999	Informatica, Lith. Acad. Sci.		computer science	HCI	-54.05702835209542	25.23363163930048	3032
b65e0e42863a8de694a84699fbb08e84a167bdcb	mpeg-4-szenenbeschreibung in telepräsenz-szenarien		Ein Aspekt von Teleprasenz-Szenarien ist die Ubertragung von multimodalen Datenstromen. Insbesondere fur die visuelle Modalitat entfallt ein groser Anteil dieser Daten auf eine geometrische Beschreibung der Umgebung des entfernten Teleoperators. In dieser Arbeit wird untersucht, wie durch gezielte und speziali­sierte Anwendung zweier im MPEG-4 Standard bereits definierter Kompo­nen­ten eine bessere Performanz fur das Teleprasenz-Szenario erreicht werden kann.rnDazu wird die MPEG-4 Komponente BIFS (BInary Format for Scenes) fur die schritthaltende Kompression von Geometriedaten untersucht. Mit Hilfe des MPEG-4 Object Descriptor Framework wird vorgestellt, wie selbst in kom­plexen Szenarien eine optimale Ubertragung zu allen Operatoren erreicht wer­den kann. Insbesondere wird Wert darauf gelegt, nur so wenig Bandbreite wie notig zu benutzen und gleichzeitg geringe Laufzeit­ver­zo­ge­rung­en zu erzielen.	vii	Jan Leupold	2008			art;performance art	NLP	-107.24886312516358	31.68844792815318	3033
4b780a99123d7e22f9df27b001eff88c15478662	gsm security	elliptic curve cryptology;gsm security;wireless security	Mobile wireless networks are more vulnerable to unauthorised access and eavesdropping when compared with the traditional fixed wired networks due to the mobility of users, the transmission of signals through open-air and the requirement of low power consumption by a mobile user. This paper focuses on the security techniques used within the GSM standard. First, current GSM security system vulnerabilities are given. Security evaluations of two new security protocol proposals for GSM, using private and public key techniques, are also presented. Next, a new GSM security protocols which uses the Elliptic Curve Public Key Cryptography (ECC) technique is proposed. This is followed by a security analysis of the proposed protocols.	authorization;cryptographic protocol;public-key cryptography	Basar Kasim;Levent Ertaul	2005				Security	-47.10773620767692	73.41100621233886	3037
be56fecf8c535c460dad7c20e3942e4ebb767fd0	alternative semantics for temporal logics	temporal logic	Abstract   The relationship between alternative underlying semantics for temporal logics is studied. A number of constraints on the allowable sets of computation paths can be built into a logic to try to ensure that the abstract computation path semantics of a concurrent program accurately reflects essential aspects of ‘real’ concurrent programs. Three such constraints are suffix closure (Lamport, 1980), fusion closure (Pratt, 1979) and limit closure (Abrahamson, 1980). Another common constraint is that the set of paths be  R -generable, i.e., generated by some binary relation (Manna and Pnueli, 1979). We show that each of the first three constraints is independent of the others, and their conjunction is precisely equivalent to the fourth constraint.		E. Allen Emerson	1983	Theor. Comput. Sci.	10.1016/0304-3975(83)90082-8	discrete mathematics;temporal logic;computer science;theoretical computer science;mathematics;algorithm	Logic	-13.035403062768982	21.092681381536103	3038
8c826b5208e5029cd5ca8369b85b19b601114d2f	the case for mesodata: an empirical investigation of an evolving database system	database system;computacion informatica;domain evolution;mesodata;grupo de excelencia;ciencias basicas y experimentales;article;schema evolution;database evolution	Database evolution can be considered a combination of schema evolution, in which the structure evolves with the addition and deletion of attributes and relations, together with domain evolution in which an attribute’s specification, semantics and/or range of allowable values changes. We present the results of an empirical investigation of the evolution of a commercial database system that measures and delineates between changes to the database that are (a) structural and (b) attribute domain related. We also estimate the impact that modelling using the mesodata approach would have on the evolving system. 2006 Elsevier B.V. All rights reserved.	attribute domain;backward compatibility;database;domain of discourse;emoticon;graph (discrete mathematics);schema evolution;systems design	Denise de Vries;John F. Roddick	2007	Information & Software Technology	10.1016/j.infsof.2006.11.001	semi-structured model;computer science;artificial intelligence;data mining;database;database schema;database design	DB	-32.1172277775223	11.71867250822677	3040
0077655078ab291116fdc448a033adb432b73f14	on-line integrated routing in dynamic multifiber ip/wdm networks	blocking probability;internet protocol;routing protocols;optical layer;intelligent networks wdm networks optical wavelength conversion optical fiber networks ip networks wavelength division multiplexing optical interconnections optical fibers wavelength routing optical fiber communication;wdm network;probability;layer;routing protocols optical fibre networks ip networks wavelength division multiplexing transport protocols optical wavelength conversion probability telecommunication traffic;functional equivalence;indexing terms;integrated routing;65;transport protocols;optical fibre networks;telecommunication traffic;ip;request blocking probability on line dynamic integrated routing multifiber ip wdm network wavelength division multiplexing network multifiber internet protocol one step routing two step routing extended layered graph resource assignment channel level balance link level balance traffic balance logical layer link optical layer link wavelength conversion granularity;lightpath;wavelength conversion integrated routing internet protocol ip layer internet protocol wavelength division multiplexing ip wdm lightpath multifiber network optical layer;simulation study;ip networks;multifiber network;internet protocol wavelength division multiplexing;optical wavelength conversion;ip wdm;wavelength conversion;wavelength division multiplexing;wavelength division multiplex	This paper focuses on dynamic integrated routing in multifiber Internet protocol/wavelength-division multiplexing (IP/WDM) networks, which can be implemented through either one-step routing (OSR) or two-step routing (TSR) approach. Based on an extended layered-graph, two resource assignment strategies, termed channel-level balance (CLB) and link-level balance (LLB), are proposed to balance the traffic in the network at different levels. To further improve the performance, a parameter K is introduced to make a dynamic tradeoff between the logical-layer links and the optical-layer links. Simulation studies are carried out for various topologies. The results show that LLB is better than CLB in most cases, and LLB combined with OSR has the optimal performance. Also, we find that the routing approach and the resource assignment strategy individually play different roles with different values of r/sub l/ that is introduced to indicate the resource richness of the network. As a multifiber network is functionally equivalent to a single-fiber network with limited wavelength conversion, we investigate the effects of wavelength conversion by studying the multifiber IP/WDM networks. The analysis shows that, when the granularity of each connection request is much smaller than the wavelength granularity, wavelength conversion may increase the request blocking probability in the network.	blocking (computing);erlang (unit);iso 10303;information exchange;link-state routing protocol;network topology;open-source religion;simulation;traffic sign recognition;wavelength-division multiplexing	Tong Ye;Qingji Zeng;Yikai Su;Lufeng Leng;Wei Wei;Zhizhong Zhang;Wei Guo;Yaohui Jin	2004	IEEE Journal on Selected Areas in Communications	10.1109/JSAC.2004.835752	internet protocol;static routing;telecommunications;computer science;distributed computing;computer network	Metrics	-6.106869599356933	84.75650294864775	3044
e698acf13436c19ca8b78c58afdb9bc176b74055	a flexibile contxt-aware middleware for developing context-aware applications	middleware	This invention concerns the production of 1,4-cis polybutadiene, or copolymers of butadiene with other related diolefins, through the catalytic polymerization or copolymerization of butadiene monomer by a continuous or discontinuous process carried out in the absence or substantial absence of solvents or diluents and operating in the presence of solid bodies.	middleware	Po-Cheng Huang;Yau-Hwang Kuo	2008			chemical engineering;computer science;distributed computing;middleware (distributed applications);copolymer;monomer;polymerization;polybutadiene;middleware	HCI	-37.05663923696778	42.66811349289509	3060
88a52ef934edb0f71afd26a0a54dec462b23c5b9	an approach for iterative event pattern recommendation	pattern generation;pattern recommendation;complex event pattern reuse;complex event processing;expert opinion;near real time;complex event pattern generation;pattern identification	The need for systems that act on events is growing. Such systems require an infrastructure for detecting patterns over incoming events and tools for helping domain experts by creating or changing them. The main goal of Complex Event Processing is detecting patterns of events in near real-time in order to indicate a situation of interest. Nowadays most current Complex Event Processing systems are focused rather on run-time than on design time issues. They pay little attention to the efficient pattern generation. Moreover, in many Complex Event Processing systems, complex event patterns may change over time due to the dynamic nature of the domain. Such changes may complicate even further the specification task as the domain expert must update the patterns constantly. Therefore the experts seek for additional support for the definition of required patterns beyond expert opinion.  In this paper we present an approach and its implementation that has been designed for a recommendation based pattern generation. We believe that a recommendation based pattern generation could increase the relevance and efficiency of newly generated patterns for the problem at hand by reusing knowledge coded in existing complex event patterns.	complex event processing;iterative method;real-time clock;real-time computing;relevance;run time (program lifecycle phase);sensor;subject-matter expert	Sinan Şen;Nenad Stojanovic;Ljiljana Stojanovic	2010		10.1145/1827418.1827459	simulation;computer science;data science;complex event processing;data mining;database	DB	-59.39578675291719	23.01701447607483	3066
a75ce8aa74941dc9494388aabb10462125e9b550	moses: a graphics oriented software development environment	programming language;programming environment;user interface;automated reasoning;production process;heuristic search;software development environment;program development	This paper describes a workstation-based monolingual programming environment which supports design, implementation, documentation and maintenance within the software production process. The most important features are graphic oriented program development, the design of the user interface and the adaptability to various programming languages.	documentation;graphics;integrated development environment;moses;programming language;software development;user interface;workstation	Günther Blaschek;Gustav Pomberger	1987		10.1145/322917.322927	first-generation programming language;heuristic;programming domain;reactive programming;computer science;artificial intelligence;software design;software framework;component-based software engineering;software development;extensible programming;software engineering;functional logic programming;software construction;computer programming;development environment;scheduling;programming paradigm;event-driven programming;procedural programming;symbolic programming;inductive programming;automated reasoning;fifth-generation programming language;programming language;user interface;system programming;software development process;programming in the large and programming in the small	SE	-50.719858762554786	28.890466667942	3076
c789fd30ea517bfb7c1a2a2892a1d4394601089e	on the decidability of functional uncertainty	cyclic feature description;cyclic description;feature description;functional uncertainty;feature logic	"""We show that feature logic extended by functional uncertainty is decidable, even if one admits cyclic descriptions. We present an algorithm, which solves feature descriptions containing functional uncertainty in two phases, both phases using a set of de-terministic and non-deterministic rewrite rules. We then compare our algorithm with the one of Kaplan and Maxwell, that does not cover cyclic feature descriptions. 1 Introduction Feature logic is the main device of unification grammars , the currently predominant paradigm in computational linguistics. More recently, feature descriptions have been proposed as a constraint system for logic programming (e.g. see [ll D. They provide for partial descriptions of abstract objects by means of functional attributes called features. Formalizations of feature logic have been proposed in various forms (for more details see [3] in this volume). We will follow the logical approach introduced by Smolka [9, 10], where feature descriptions are standard first order formulae interpreted in first order structures. In this formalization features are considered as functional relations. Atomic formulae (which we will call atomic constraints) are of either the form A(x) or zfy, where x, y are first order variables , A is some sort predicate and f is a feature (written in infix notation). The constraints of the form xfy can be generalized to constraints of the form xwy, where w = fl-.. fn is a finite feature path. This does not affect the computational properties. In this paper we will be concerned with an extension to feature descriptions, which has been introduced as """"functional uncertainty"""" by Kaplan and Zaenen [7] and Kaplan and Maxwell [5]. This formal device plays an important role in the framework of LFG in modelling so-called long distance dependencies and constituent coordination. For a detailed linguistic motivation see [7], [6] and [5]; a more general use of functional uncertainty can be found in [8]. Functional uncertainty consists of constraints of ITW 9002 0, from the German Bundesministerium ffir Forschung und Technologic to the DFKI project DISCO. I would like to thank Jochen Dhrre, Joachim Niehren and Ralf Treinen for reading draft version of this paper. For space limitations most of the proofs are omitted; they can be found in the complete paper [2] the form xLy, where L is a finite description of a regular language of feature paths. A constraint xLy holds if there is a path w E L such that zwy holds. Under this …"""	algorithm;atomic formula;computational linguistics;first-order logic;german research centre for artificial intelligence;kaplan–meier estimator;lexical functional grammar;logic programming;maxwell (microarchitecture);non-deterministic turing machine;programming paradigm;regular language;rewriting;unification (computer science)	Rolf Backofen	1993			computer science;algorithm	NLP	-13.977690696473582	12.29567545926274	3078
da92884f6aa2dda74c70c04f7d2eaac67e047d8a	on the security of iv dependent stream ciphers	provable security;prng;iv setup;pseudo random function;security proof;stream cipher;pseudo random number generator	Almost all the existing stream ciphers are using two inputs: a secret key and an initial value (IV). However recent attacks indicate that designing a secure IV-dependent stream cipher and especially the key and IV setup component of such a cipher remains a difficult task. In this paper we first formally establish the security of a well known generic construction for deriving an IV-dependent stream cipher, namely the composition of a key and IV setup pseudo-random function (PRF) with a keystream generation pseudo-random number generator (PRNG). We then present a tree-based construction allowing to derive a IV-dependent stream cipher from a PRNG for a moderate cost that can be viewed as a subcase of the former generic construction. Finally we show that the recently proposed stream cipher quad [3] uses this tree-based construction and that consequently the security proof for quad’s keystream generation part given in [3] can be extended to incorporate the key and IV setup.	attribute–value pair;key (cryptography);primitive recursive function;provable security;pseudorandom function family;pseudorandom number generator;pseudorandomness;random number generation;stream cipher	Côme Berbain;Henri Gilbert	2007		10.1007/978-3-540-74619-5_17	weak key;transposition cipher;two-square cipher;running key cipher;block cipher mode of operation;computer science;theoretical computer science;fluhrer, mantin and shamir attack;stream cipher attack;distributed computing;stream cipher;affine cipher;rc4;computer security	Crypto	-38.34167924701906	77.94760002985544	3079
e7d7c6576f885d9ab4385833631d6363816299b2	information management for concurrent engineering	fuzzy set;project manager;development process;lead time;information flow;research and development;information management;time to market;product development process;concurrent engineering	Abstract   The main objective of Concurrent Engineering is the reduction of development lead time. This shorter time-to-market is achieved by paralleling development and production activities. With the deliberate use of incomplete and uncertain information an enormous reduction of development lead time can be achieved in comparison to parallel activities which are only based on exact information. Thus many activities can be started even if their predecessors are not completed. This article focuses mainly on two concepts to provide an adequate information management for Concurrent Engineering. The first concept is based on a systematic integration of uncertainty into the information flow. The second concept allows to restructure the sequential and document-oriented information flow by creating small information units. The application of both concepts supports an early information flow. Thus it becomes possible to coordinate activities more efficiently because critical topics can be recognized and balanced in advance, shifting the communication requirement to the early stages of the development process. Hence, expensive iterations, especially in the late stages of the product development process, can be avoided and the length of iteration loops can be reduced.	information management	Walter Eversheim;Axel Roggatz;Hans-Jürgen Zimmermann;Thomas Derichs	1997	European Journal of Operational Research	10.1016/S0377-2217(96)00288-3	information flow;computer science;knowledge management;operations management;fuzzy set;information management;software development process;new product development;concurrent engineering	DB	-64.35638194659005	13.071535674717108	3084
d664cd50e08884821bfae5f02e6ed89d99ec418a	preference-based cloud service recommendation as a brokerage service	optimisation;cloud service broker;service ranking;mcdm	As the multitude and complexity of cloud services increases, the role of cloud brokers in the cloud service ecosystems becomes increasingly important. In particular, the lack of standard mechanisms that allow for the comparison of cloud service specifications against user requirements taking into account the implicit uncertainty and vagueness is a major hindrance during the cloud service evaluation and selection. In this paper, we discuss the Preference-based cLoud Service Recommender (PuLSaR) that uses a holistic multi-criteria decision making (MCDM) approach for offering optimisation as brokerage service. The specification and implementation details of this dedicated software are thoroughly discussed while the background method used is summarised. Both method and brokerage service allow for the multi-objective assessment of cloud services in a unified way, taking into account precise and imprecise metrics and dealing with their fuzziness.	cloud computing;ecosystem;holism;mathematical optimization;recommender system;requirement;user requirements document;vagueness	Ioannis Patiniotakis;Giannis Verginadis;Gregoris Mentzas	2014		10.1145/2676662.2676677	service level requirement;service level objective;service delivery framework;marketing;service design;data mining;business;data as a service;world wide web	Metrics	-47.04738184708459	15.29836907859966	3086
5a80a3d4484c5d8d7658002c70bce68b6dc542c8	partial interpretations of higher order algebraic types (extended abstract)	extended abstract;higher order algebraic types;partial interpretations;higher order;higher order functions	The theory of algebraic abstract types specified by conditional equations is extended to types with nonstrict operations, partial and even infinite objects based on the concept of partial interpretations. Models of such types are studied where all explicit equations have solutions. Higher order types, i.e. types comprising higher order functions are treated, too. This allows an algebraic (equational) specification of algebras including sorts with infinite objects and higher order functions (functionals).	algebraic data type	Manfred Broy	1986		10.1007/BFb0016232	higher-order logic;computer science;programming language;higher-order function	Theory	-12.606848063522825	18.790525684786648	3089
386c42be48e23479d70fcc741e2d09d53bc95da0	policy-driven middleware for heterogeneous, hybrid cloud platforms	platform as a service;hybrid cloud;dynamic and context aware adaptation	The cloud computing paradigm promises increased flexibility and scalability. However, in private cloud environments this flexibility and scalability is constrained by the limited capacity. On the other hand, organizations are reluctant to migrate to public clouds because they lose control over their applications and data. The concept of a hybrid cloud tries to combine the benefits of private and public clouds, while also decreasing vendor lock-in.  This paper presents PaaSHopper, a middleware platform for hybrid cloud applications that enables organizations to keep fine-grained control over the execution of their applications. Driven by policies, the middleware dynamically decides which requests and tasks are executed in a particular part of the hybrid cloud. We validated this work by means of a prototype on top of a hybrid cloud consisting of a local JBoss AS cluster, Google App Engine, and Red Hat OpenShift.	cloud computing;google app engine;middleware;openshift;programming paradigm;prototype;scalability;vendor lock-in;wildfly	Tom Desair;Wouter Joosen;Bert Lagaisse;Ansar Rafique;Stefan Walraven	2013		10.1145/2541583.2541585	real-time computing;simulation;engineering;distributed computing	HPC	-26.601611402460655	58.85947017089738	3093
eff5a62a378d5b9e3db2cc6ee1fa239a493274d7	entwicklung und anwendung von expertensystemen auf personal computern				Peter Mertens;Peter Spieker	1987	Robotersysteme		engineering;simulation;software engineering	NLP	-93.87754036077047	26.273804005788506	3096
f64525126600e052e65d014ffcb9a15fcf7e20e9	the art of multiprocessor programming by maurice herlihy and nir shavit	game theory;software process improvement;team dynamics;software management;social aspects of development;software engineering economics;productivity;mechanism design		maurice herlihy;multiprocessing	Jodat Vu	2011	ACM SIGSOFT Software Engineering Notes	10.1145/2020976.2021006	mechanism design;game theory;personal software process;productivity;simulation;computer science;systems engineering;engineering;software design;social software engineering;component-based software engineering;software development;software engineering;resource-oriented architecture;management;software development process;software peer review	SE	-90.7470261005943	26.104519748132578	3100
1eb9dd948ed3ef27730a3c4a0232924556c66190	a resilience mask for robust audio hashing			hash function	Jin S. Seo	2017	IEICE Transactions		fingerprint;parallel computing;real-time computing;computer science;theoretical computer science	Vision	-9.555246732251776	67.27910122915796	3104
0e302fba2fc25c0e5a150ccc3100b40140e4f89b	exploiting the parallelism available in loops	parallel architectures parallel programming program compilers scheduling;dependence analysis;multiplication operator;parallelizing compilers;parallel programming;parallel processing hardware concurrent computing system performance turning computer architecture vliw runtime pipeline processing processor scheduling;loop scheduling;parallel architectures;data dependence;scheduling;software pipelining;parallel architecture;coarse grained;program compilers;critical dependence ratio parallelism loops scheduling techniques parallel architectures synchronization overhead instruction scheduling constraints memory latencies compilation techniques;analytical model;shared memory multiprocessor	Because a loop's body often executes many times, loops provide a rich opportunity for exploiting parallelism. This article explains scheduling techniques and compares results on different architectures. Since parallel architectures differ in synchronization overhead, instruction scheduling constraints, memory latencies, and implementation details, determining the best approach for exploiting parallelism can be difficult. To indicate their performance potential, this article surveys several architectures and compilation techniques using a common notation and consistent terminology. First we develop the critical dependence ratio to determine a loop's maximum possible parallelism, given infinite hardware. Then we look at specific architectures and techniques. Loops can provide a large portion of the parallelism available in an application program, since the iterations of a loop may be executed many times. To exploit this parallelism, however, one must look beyond a single basic block or a single iteration for independent operations. The choice of technique depends on the underlying architecture of the parallel machine and the characteristics of each individual loop.<<ETX>>	basic block;compile time;compiler;exploit (computer security);instruction scheduling;iteration;overhead (computing);parallel computing;scheduling (computing)	David. J. Lilja	1994	Computer	10.1109/2.261915	software pipelining;multiplication operator;computer architecture;parallel computing;real-time computing;computer science;operating system;data parallelism;programming language;scheduling;instruction-level parallelism;scalable parallelism;implicit parallelism;task parallelism;dependence analysis	HPC	-7.944425614573154	48.80088267989183	3112
a05fbbb4d5b6bdf6a0bb2278c5d7e7d1dc87c73a	discussion of aicpa/cica systrust™ principles and criteria	qos guaranteed;interprovider qos;dmtf cim;diffservover mpls;distributed nms	In this paper, the performance of the WebService architecture for QoS guaranteed connection provisioning in inter-AS domain networks has been measured and analyzed for service publish/inquiry, collection of NMSs ASBR details, source routing by ingress NMS in constraint based routing and connection establishment. From the analysis, it has been found that, the connection between inter-AS domain networks can be established within the usual time limits of 3 seconds by the Web Service architecture. Since no standard solutions have been implemented in Interdomain QoS provisioning, this performance analysis assures Web Service architecture as a promising solution and can be easily implemented in the early stages of MPLS network employment.	common criteria	Andrew D. Bailey	2000	J. Information Systems	10.2308/jis.2000.14.s-1.9	real-time computing;computer science;distributed computing;computer network	NLP	-13.127867793507516	93.37790665027057	3116
6f245750c3ddb6626b0be4d45ae7ecf927cf5fbe	on analyzing evolutionary changes of web services	information model;web service	Web services evolve over their life time and change their beh avior. In our work, we analyze Web service related changes and investi gate interdependencies of Web service related changes. We classify changes of W eb services for an analysis regarding causes and effects of such changes and ut ilize a dedicated Web service information model to capture the changes of Web serv ices. We show how to put changes of Web services into an evolutionary context t hat allows us to develop a holistic perspective on Web services and their stake hold rs in a ecosystem of Web services.	amazon web services;ecosystem;holism;information model;interdependence;web service	Martin Treiber;Hong Linh Truong;Schahram Dustdar	2008		10.1007/978-3-642-01247-1_29	web service;web application security;web development;web modeling;data web;web analytics;web standards;information model;computer science;knowledge management;ws-policy;social semantic web;data mining;ws-addressing;web intelligence;web engineering;ws-i basic profile;law;world wide web	Web+IR	-53.92094558852822	15.16748144819505	3120
061818ba18e97de9307f68fce195dcf068194dbb	congestion pricing using a raffle-based scheme	article	We propose a raffle-based scheme for the decongestion of a shared resource. Our scheme builds on ideas from the economic literature on incentivizing contributions to a public good. We formulate a game-theoretic model for the decongestion problem in a setup with a finite number of users, as well as in a setup with an infinite number of non-atomic users. We analyze both setups, and show that the former converges toward the latter when the number of users becomes large. We compare our results to existing results for the public good provision problem. Overall, our results establish that raffle-based schemes are useful in addressing congestion problems.	game theory;network congestion;tcp congestion control	Patrick Loiseau;Galina Schwartz;John Musacchio;Saurabh Amin;S. Shankar Sastry	2011	International Conference on NETwork Games, Control and Optimization (NetGCooP 2011)		financial economics;microeconomics	Vision	-7.988890924416059	94.70656744443221	3121
2f021fcc731d83d929dfaa278c274ad1778fc46c	auf den spuren von konrad zuse - ein ortsbezogenes lern-adventure	delfi full paper research;gaming;lernspiel;mobilitat;ortsbezogenheit	Lernspiele vermitteln über eine spielerische Handlung Wissen zu bestimmten Themen oder bestimmte Fertigkeiten, indem lern-, sozialund motivationsfördernde Elemente zielgerichtet eingesetzt werden. Insbesondere für historische Sachverhalte eröffnet die Untergruppe der ortsbasierten Lernspiele Lehrenden und Ausstellern einen Gestaltungsspielraum, den klassische Lehrbücher und Museen nicht bieten können. In dem Posterbeitrag wird beispielhaft der Einsatz eines derartigen Spiels dargestellt, das den Spieler in die Welt des deutschen Computerpioniers Konrad Zuse versetzt und ihn diese aus einer neuen, persönlichen Perspektive erfahren lässt.	eine and zwei;gesellschaft für informatik	Raphael Zender;Karsten Höhne;Ulrike Lucke	2013			history;adventure;performance art		-104.99704672359006	34.723814426908575	3123
4f68a80dec7eebe30dade51c975f491ee52e7b18	an efficient public key trace and revoke scheme secure against adaptive chosen ciphertext attack	hachage;message authentication code;decisional di e hellman;securite;codigo tiempo;cle publique;collision resistant hash functions;fonction hachage;hashing;public key;criptografia;cryptography;code temps;safety;llave publica;authentification message;cryptographie;hash function;message authentication;seguridad;time code;chosen ciphertext attack	We propose a new public key trace and revoke scheme secure against adaptive chosen ciphertext attack. Our scheme is more efficient than the DF scheme suggested by Y. Dodis and N. Fazio[9]. Our scheme reduces the length of enabling block of the DF scheme by (about) half. Additionally, the computational overhead of the user is lower than that of the DF scheme; instead, the computational overhead of the server is increased. The total computational overhead of the user and the server is the same as that of the DF scheme, and therefore, our scheme is more practical, since the computing power of the user is weaker than that of the server in many applications. In addition, our scheme is secure against adaptive chosen ciphertext attack under only the decision Diffie-Hellman (DDH) assumption and the collision-resistant hash function H assumption, whereas the DF scheme also needs the one-time MAC (message authentication code) assumption.	adaptive chosen-ciphertext attack;ciphertext;collision resistance;computation;decisional diffie–hellman assumption;diffie–hellman key exchange;direction finding;hash function;message authentication code;overhead (computing);public-key cryptography;server (computing)	Chong Hee Kim;Yong Ho Hwang;Pil Joong Lee	2003		10.1007/978-3-540-40061-5_23	message authentication code;semantic security;hash function;computer science;theoretical computer science;distributed computing;computer security;algorithm	Crypto	-43.00814318583543	76.93971895865255	3136
33f8610eec8cddb7d3133549328c0c460c28921b	multi-user collusion behavior forensics: game theoretic formulation of fairness dynamics	pareto optimisation;game theory;authorisation;traitor tracing;multi user;indexing terms;game theory multimedia forensics security;multimedia forensics multiuser collusion behavior forensics game theory fairness dynamics cost effective attack digital fingerprinting noncooperative game pareto optimal set nash bargaining;forensics game theory fingerprint recognition video coding data security multimedia systems educational institutions computational modeling computer networks video sequences;nash bargaining solution;pareto optimisation authorisation fingerprint identification game theory;digital fingerprinting;cost effectiveness;profitability;non cooperative game;multimedia forensics;security;pareto optimality;fingerprint identification	Multi-user collusion is an cost-effective attack against digital fingerprinting, in which a group of attackers collectively undermine the traitor tracing capability of digital fingerprints. However, during multi-user collusion, each colluder wishes to minimize his/her own risk and maximize his/her own profit, and different colluders have different objectives. Thus, an important issue during collusion is to agree on how to distribute the risk/profit among colluders and ensure fairness of the attack. To have a better understanding of the attackers' behavior during collusion to achieve fairness, this paper models the dynamics among colluders as a non-cooperative game. We then study the Pareto-optimal set, where no colluder can further increase his/her own payoff without decreasing others', and analyze the Nash bargaining solution of this game.	digital video fingerprinting;fairness measure;fingerprint (computing);game theory;multi-user;nash equilibrium;pareto efficiency;traitor tracing	Wan-Yi Sabrina Lin;H. Vicky Zhao;K. J. Ray Liu	2007	2007 IEEE International Conference on Image Processing	10.1109/ICIP.2007.4379533	non-cooperative game;bargaining problem;game theory;fingerprint;simulation;cost-effectiveness analysis;index term;computer science;authorization;internet privacy;computer security;profitability index	DB	-59.66061884321855	74.00865456734549	3137
2d5ca4833e64f975ea063de3075b9a67c9343628	towards analyzing traceability of data leakage by malicious insiders	data distribution;data leakage;insider tracing	Data leakage committed by malicious insiders proposes a serious challenge for business secrets and intellectual property. Great efforts have been made to detect and mitigate insider threat. Due to the diversity in the motivations, previous work in this field mostly focuses on designing data holder's data distribution and insider tracing algorithms, with little consideration of malicious insiders' leakage strategies. In this paper, the traitors tracing problem is modeled as an incremental refining multi-step process. For each step, a metric is proposed to measure the efficiency of current tracing status. Theoretical and simulating analysis shows that malicious insiders can adopt sophisticated leakage strategies, which makes it difficult to distinguish them from others and leads to more innocent users involved as suspects. Thus it is important for the data holder to figure out the insiders' leakage strategies and adopt proper tracing scheme to improve the refining process. © Springer-Verlag Berlin Heidelberg 2013.	spectral leakage;traceability	Xiao Wang;Jinqiao Shi;Li Guo	2012		10.1007/978-3-642-35795-4_19	internet privacy;world wide web;computer security	Crypto	-58.54092106280861	61.9536630918491	3140
278fbcca9ad0717870c8cbae0307d95ce7e8f54e	using patterns to design rules in workflows	knowledge based systems bibliographies workflow management software object oriented programming software reusability;pattern catalog;computer society proposals skeleton process control control systems business communication;bibliographies;object oriented programming;design rules;reusable patterns;formal basis rule design dynamic environments activity flow exceptional situations process execution flexible workflow design wide project exceptional aspects main activity flow frequently occurring exceptional situations rule instantiation pattern based design pattern catalog pattern reuse;workflow design and management;rules and triggers;software reusability;workflow management software;exception handling;knowledge based systems	In order to design workflows in changing and dynamic environments, a flexible, correct, and rapid realization of models of the activity flow is required. In particular, techniques are needed to design workflows capable of adapting themselves effectively when exceptional situations occur during process execution. The authors present an approach to flexible workflow design based on rules and patterns developed in the framework of the WIDE project. Rules allow a high degree of flexibility during workflow design by modeling exceptional aspects of the workflow separately from the main activity flow. Patterns model frequently occurring exceptional situations in a generalized way by providing the designer with skeletons of rules and suggestions about their instantiation, together with indications on relationships with other rules, with the activity flow, and with related information. Pattern based design relies on a pattern catalog containing patterns to be reused and on a formal basis for specializing and instantiating available patterns.		Fabio Casati;Silvana Castano;Maria Grazia Fugini;Isabelle Mirbel;Barbara Pernici	2000	IEEE Trans. Software Eng.	10.1109/32.879813	exception handling;software design pattern;computer science;systems engineering;knowledge management;operating system;software engineering;knowledge-based systems;data mining;programming language;object-oriented programming;structural pattern;workflow management system;workflow engine;specification pattern;workflow technology	SE	-53.502521378526076	20.552953783899817	3143
8151b3a2e23e17bfc17416845a6d627572aa7632	megaphone: live state migration for distributed streaming dataflows		We design and implement Megaphone, a data migration mechanism for stateful distributed dataflow engines with latency objectives. When compared to existing migration mechanisms, Megaphone has the following differentiating characteristics: (i) migrations can be subdivided to a configurable granularity to avoid latency spikes, and (ii) migrations can be prepared ahead of time to avoid runtime coordination. Megaphone is implemented as a library on an unmodified timely dataflow implementation, and provides an operator interface compatible with its existing APIs. We evaluate Megaphone on established benchmarks with varying amounts of state and observe that compared to naı̈ve approaches Megaphone reduces service latencies during reconfiguration by orders of magnitude without significantly increasing steady-state overhead. PVLDB Reference Format: Moritz Hoffmann, Andrea Lattuada, Frank McSherry, Vasiliki Kalavri, John Liagouris, and Timothy Roscoe. Megaphone: Live state migration for distributed streaming dataflows. PVLDB, 12(xxx): xxxx-yyyy, 2019. DOI: https://doi.org/10.14778/xxxxxxx.xxxxxxx		Moritz Hoffmann;Andrea Lattuada;Frank McSherry;Vasiliki Kalavri;John Liagouris;Timothy Roscoe	2018	CoRR			DB	-19.711281154936994	54.61077481398721	3153
dc18de2270d05f762ec8cec64b61817de6e8dcd0	a hybrid state machine for component specification.	component;code generation;state machine;software component;regular expression;finite state machine;java	A wide range of software units can be classified as state machines. We extend conventional state machine notations by adding regular expressions of events and unions of source states to state machine transitions. Reusable software components are generated from these extended state machine specifications. Component specification and generation are illustrated in Java.	component-based software engineering;finite-state machine;java;regular expression	Alexander Sakharov	2000	SIGPLAN Notices	10.1145/346443.346452	extended finite-state machine;richards controller;finite state machine with datapath;real-time computing;pointer machine;x-machine;computer science;theoretical computer science;component-based software engineering;component;finite-state machine;programming language;java;regular expression;code generation;virtual finite-state machine;abstract state machines	SE	-30.671400881276806	29.348584505678687	3160
23aceb621235eb16086f2f78aa161ff90799a55e	sweets: um sistema de recomendação de especialistas aplicado a uma plataforma de gestão de conhecimento		Resumo: As organizações, com o intuito de aumentarem o seu grau de competitividade no mercado, vêm a cada instante buscando novas formas de evoluir a produtividade e a qualidade dos produtos desenvolvidos, além da diminuição de custos. Para que tais objetivos possam ser alcançados é primordial explorar ao máximo o potencial de seus colaboradores e os possíveis relacionamentos que esses colaboradores têm uns com os outros, ou seja, encontrar e partilhar conhecimento tácito. Como o conhecimento tático está na mente das pessoas, é difícil de ser formalizado e documentado, por isso, o ideal seria identificar e recomendar a pessoa que detém o conhecimento. Diante disso, o presente artigo apresenta o Sistema de Recomendação de Especialistas SWEETS e a sua implantação no ambiente a.m.i.g.o.s., uma plataforma de gestão de conhecimento baseada em conceitos voltados às redes sociais. O SWEETS foi desenvolvido em duas versões, 1.0 e 2.0. A versão 1.0, de forma pró-ativa, aproxima pessoas com especialidades em comum, ora pelos seus conhecimentos (perfil de escrita), ora pelos seus interesses (perfil de leitura). Já a versão 2.0 do SWEETS não atua de forma pró-ativa, ou seja, é necessário que haja a requisição de um usuário especialista em determinada área, e é baseada em folksonomia para extração de uma ontologia, fundamental para identificar as especialidades das pessoas de forma mais eficaz. Esta ontologia é refletida pela co-ocorrência das tags (conceitos) em relação aos itens (instâncias) e é independente de domínio, sendo a principal contribuição desse trabalho. A implantação do SWEETS no a.m.i.g.o.s. visa trazer benefícios como: minimizar o problema de comunicação na corporação, prover um incentivo ao conhecimento social e partilhar conhecimento; proporcionando, assim, à empresa, a utilização mais eficaz dos conhecimentos de seus colaboradores.	lo que tú quieras oír;numerical aperture;power-on reset;unified model;virtual instrument software architecture;visa cash	Edeilson Milhomem da Silva;Ricardo Araujo Costa;Lucas R. B. Schmitz;Silvio Romero de Lemos Meira	2011	RITA		simulation;computer science;performance art	Vision	-106.75829399451602	18.340921985643995	3172
3f77716757e5722a603648670f9999277e35cd97	a two-level patching scheme for video multicast	multicast communication;media on demand;hierarchical multicast stream merging video multicast media on demand delivery bandwidth requirement two level patching channel dynamic skyscraper algorithm client request rates;bandwidth allocation;bandwidth broadcasting streaming media merging multicast algorithms delay image communication information processing network servers heuristic algorithms;video on demand;bandwidth allocation video on demand multicast communication telecommunication channels;telecommunication channels;lower bound	Although patching has shown to be a simple and efficient technique for immediate media on-demand delivery, there is still much scope for further improvement compared with the lower bound of the server network-I/O bandwidth requirement. In this paper, a new concept of two-level patching channel is proposed for the first time, based on which patching channels are rearranged through merging and further patching and a novel channel schedule scheme is developed. Simulation results show that the proposed Two-level patching scheme outperforms the conventional patching technique by a significant margin. It even performs better than the Dynamic Skyscraper Algorithm over a wide range of client request rates. Furthermore, it is reasonably competitive with hierarchical multicast stream merging (HMSM) at low to modest client request rates. Most importantly, the implementation complexity of our algorithm is much lower than the Skyscraper and HMSM.	algorithm;input/output;multicast;regular expression;server (computing);simulation;skyscraper	Dongliang Guan;Songyu Yu	2003		10.1109/ICON.2003.1266205	multicast;telecommunications;computer science;distributed computing;upper and lower bounds;source-specific multicast;xcast;computer network;bandwidth allocation	HPC	-7.198474671665473	97.31659526263421	3176
68b475d978a6678036f1be9a688e6dc0d67f7d05	research developments in multiple inheritance with exceptions	multiple inheritance		multiple inheritance	Peter W. Eklund	1994	Knowledge Eng. Review	10.1017/S0269888900006561	multiple inheritance;computer science	AI	-22.852152137064678	20.59323864093376	3177
04ac7b5a444c098c6a93c58b2034580b53a87e5d	high performance and grid computing with quality of service control	dds integration grid computing quality of service control high performance computing systems hpc systems qos controls reliability features standard middleware libraries message passing interface mpi parallel virtual machine pvm service quality myrinet quadrics fault tolerance control data centric publish subscribe model dcps model data distribution service systems dds systems net centric system dds qos;quality of service reliability middleware computational modeling standards hardware scalability;mpi hpc qos middleware;qos;hpc;quality of service fault tolerance grid computing middleware parallel processing;middleware;mpi	Up to writing this paper, existing High Performance Computing (HPC) systems do not provide proper quality of service (QoS) controls and reliability features because of two limitations: first, standard middleware libraries such as Message Passing Interface (MPI) and Parallel Virtual Machine (PVM) do not provide means for applications to specify service quality for computation and communication. Second, modern high-speed interconnects such as Infiniband, Myrinet and Quadrics are optimized for performance rather than fault-tolerance and QoS control. The Data-Centric Publish-Subscribe (DCPS) model - the core of Data Distribution Service (DDS) systems - defines standards that enable applications running on heterogeneous platforms to control various QoS policies in a net-centric system. In this paper, we present our novel model of incorporating DDS QoS and reliability controls into HPC systems. Our results show that DDS integration into HPC adds considerable overheard in terms of performance and network utilization, when the application is mainly communication.	computation;data distribution service;electrical connection;fault tolerance;grid computing;infiniband;library (computing);message passing interface;middleware;net-centric;parallel virtual machine;quality of service	Sadiq M. Sait;Raed Al-Shaikh	2014	15th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)	10.1109/SNPD.2014.6888711	supercomputer;parallel computing;quality of service;computer science;message passing interface;operating system;middleware;distributed computing;grid computing;computer network	HPC	-11.13530875584128	45.66113105363183	3180
932e8c9b52c6374fda8d189fd0095a966defd7a4	where does all this waste come from?	software process improvement;agile processes;innovation	Agile development processes are more flexible than conventional ones. They emphasize iterative development and learning over feedback loops. Nevertheless, we experienced some pitfalls in the application of agile processes in dependable software systems. We present here the experiences we gathered in the construction of high-quality industrial software. Moreover, we will digest our experiences into a conceptual model of waste creation. This model will be refined to a case study where we take appropriate measurements in order to provide empirical evidence for it. Finally, we discuss the implications of the developed model, which helps to estimate the trade-off between agile and traditional software processes. Copyright © 2015 John Wiley & Sons, Ltd.		Wolfgang Raschke;Massimiliano Zilli;Johannes Loinig;Reinhold Weiss;Christian Steger;Christian Kreiner	2015	Journal of Software: Evolution and Process	10.1002/smr.1732	innovation;agile unified process;agile usability engineering;computer science;systems engineering;engineering;operations management;industrial engineering;software engineering;empirical process;management	Vision	-66.94064397901909	22.45284358419007	3185
7ef3ae374329ed19236f4e6146768f99e1003306	a cooperative distributed text database management method unifying search and compression based on the burrows-wheeler transformation	algorithme rapide;procesamiento informacion;systeme cooperatif;data compression;cooperative systems;fast algorithm;estructura datos;information processing;structure donnee;compresion dato;systeme gestion base donnee;information system;traitement information;sistema gestion base datos;database management system;data structure;algoritmo rapido;systeme information;compression donnee;sistema informacion	A new text database management method for distributed cooperative environments is proposed, which can collect texts in distributed sites through a network of narrow bandwidth and enables fulltext search in a uni ed e cient manner. This method is based on the two new developments in full-text search data structures and data compression. Speci cally, the Burrows-Wheeler transformation is used as a basis of constructing the su x array (or, PAT array) for full-text search and of performing the block sorting compression scheme. A cooperative environment makes it possible to employ these new methods in a uniform fashion. This framework may be also used in future for the Web text collection/search problem. The paper rst describes this method, and then provides preliminary computational results concerning I/O implementation of su x arrays and performing the su x sorting. These preliminary computational results indicate practicality of our method.	burrows–wheeler transform;computation;data compression;input/output;search data structure;search problem;sorting algorithm;world wide web	Kunihiko Sadakane;Hiroshi Imai	1998		10.1007/978-3-540-49121-7_38	data compression;data structure;information processing;computer science;data mining;database;compressed suffix array;programming language;information system;algorithm	HPC	-26.830260707510448	4.788580427307163	3192
181a24b8b316dd6ada957d79a690e67fb46347e3	reconciling requirement-driven data warehouses with data sources via multidimensional normal forms	query view transformation;conceptual modeling;qvt;conceptual model;development process;requirements;requirement analysis;multidimensional normal forms;hybrid approach;case tool;data warehousing;user requirements;normal form;information need;data warehouse	9 Successful data warehouse (DW) design needs to be based upon a requirement 10 analysis phase in order to adequately represent the information needs of DW users. 11 Moreover, since the DW integrates the information provided by data sources, it is 12 also crucial to take these sources into account throughout the development process 13 to obtain a consistent reconciliation of data sources and information needs. In this 14 paper, we start by summarizing our approach to specify user requirements for data 15 warehouses and to obtain a conceptual multidimensional model capturing these re16 quirements. Then, we make use of the multidimensional normal forms to define a 17 set of Query/View/Transformation (QVT) relations to assure that the conceptual 18 multidimensional model obtained from user requirements agrees with the available 19 data sources that will populate the DW. Thus, we propose a hybrid approach to de20 velop DWs, i.e., we firstly obtain the conceptual multidimensional model of the DW 21 from user requirements and then we verify and enforce its correctness against data 22 sources by using a set of QVT relations based on multidimensional normal forms. 23 Finally, we provide some snapshots of the CASE tool we have used to implement 24 our QVT relations. 25	computer-aided software engineering;correctness (computer science);database normalization;dreamwidth;information needs;online analytical processing;population;qvt;requirement;user requirements document	Jose-Norberto Mazón;Juan Trujillo;Jens Lechtenbörger	2007	Data Knowl. Eng.	10.1016/j.datak.2007.04.004	requirements analysis;computer science;conceptual model;data warehouse;data mining;database	DB	-34.15331333462556	11.213623955561863	3193
bd315edc5e37f21902a95980536a84b2fb97c53d	a user view of virtual terminal standardisation	standardisation		virtual terminal	Brian Gilmore	1987	Computer Networks	10.1016/0169-7552(87)90037-7	telecommunications;computer science;standardization	Visualization	-20.83580437328441	89.21563615906697	3195
4c251eecb3a7d3c8e7f08645c79e45c6ed5d6793	an empirical investigation of the miles and snow typology for small on-line businesses	e commerce;website design;business to consumer markets;strategic planning;internet;world wide web	The World Wide Web is becoming an increasingly important strategic tool for businesses around the world. The web offers businesses the ability to promote their products, conduct market research, deliver customer services, and sell their products directly to an ever-growing number of internet users worldwide. However, very little is known about how businesses should strategically leverage the capabilities of this new medium. This article presents the results of an empirical study of the Miles and Snow typology to describe the strategies of small businesses involved in web-based electronic commerce. The results demonstrate that the Miles and Snow typology can be extended to the realm of electronic commerce and that the strategies followed by online businesses are consistent with previous empirical findings. Furthermore, the results lend further support to the validity and usefulness of the typology and to the concept of co-alignment.	biological anthropology;online and offline	Pat Auger	2003	International Journal of Internet and Enterprise Management	10.1504/IJIEM.2003.003819	e-commerce;the internet;strategic planning;computer science;marketing;management;world wide web;commerce	OS	-76.30274967194566	4.265959816793924	3200
5597345018bda8b1544c1b6e4d26a341d404a928	review of	public policy issues;design;matlab;user interfaces;integrated environments;web-based services;algorithm design and analysis;general systems theory;management;distance learning;theory;data mining;parallel and vector implementations	DigiZeitschriften e.V. gewährt ein nicht exklusives, nicht übertragbares, persönliches und beschränktes Recht auf Nutzung dieses Dokuments. Dieses Dokument ist ausschließlich für den persönlichen, nicht kommerziellen Gebrauch bestimmt. Das Copyright bleibt bei den Herausgebern oder sonstigen Rechteinhabern. Als Nutzer sind Sie sind nicht dazu berechtigt, eine Lizenz zu übertragen, zu transferieren oder an Dritte weiter zu geben. Die Nutzung stellt keine Übertragung des Eigentumsrechts an diesem Dokument dar und gilt vorbehaltlich der folgenden Einschränkungen: Sie müssen auf sämtlichen Kopien dieses Dokuments alle Urheberrechtshinweise und sonstigen Hinweise auf gesetzlichen Schutz beibehalten; und Sie dürfen dieses Dokument nicht in irgend einer Weise abändern, noch dürfen Sie dieses Dokument für öffentliche oder kommerzielle Zwecke vervielfältigen, öffentlich ausstellen, aufführen, vertreiben oder anderweitig nutzen; es sei denn, es liegt Ihnen eine schriftliche Genehmigung von DigiZeitschriften e.V. und vom Herausgeber oder sonstigen Rechteinhaber vor. Mit dem Gebrauch von DigiZeitschriften e.V. und der Verwendung dieses Dokuments erkennen Sie die Nutzungsbedingungen an.	3-methyl-2-oxobutanoate dehydrogenase (ferredoxin) activity;antilymphocyte serum;aristolochia contorta;citeseerx;copyright;diethylstilbestrol;eine and zwei;endometrial intraepithelial neoplasia;epilepsy;gesellschaft für informatik;labeo caeruleus;marden walker like syndrome;medication summary:find:pt:^patient:doc;modified rano van den bent glioma 2011 oncology response criteria;osteoarthritis, spine;sie (file format);tricyclic antidepressants tested for:prid:pt:ur:nar:screen;vhf omnidirectional range;non-rheum heart valve disease;thanatophobia	Ross Gagliano	2000	Nature biotechnology	10.1145/1159402.1159404			-105.6509494710132	36.188365607987485	3204
29f282763897ecd669d91c904e6e5033f8026e42	estimating the size of data mart projects	data mart;size estimation of software design;function point analysis;data warehousing	In order to better manage software projects, we need to estimate adequately the effort of its development, independently of their peculiarities. This paper presents an adaptation of Function Point Analysis (FPA) approach for estimating the size of Data Mart (DM) systems. We validate the proposed approach using real data from finished DM projects and we also show that our approach could offer better results compared with the original FPA.	data mart;function point	Wagner Gonçalves Ferreira;Humberto Torres Marques-Neto	2013		10.1145/2480362.2480577	computer science;data science;function point;data warehouse;data mining	SE	-68.40479622593173	20.163951032252264	3205
61a5978835a44bbbc2c8df9ea48eedf428173d30	it-support zur vereinfachten abfrage privatwirtschaftlicher daten in der tierseuchenbekämpfung		Für die Tierseuchenbekämpfung stehen Daten aus behördlichen ITSystemen sowie der epidemiologische Ermittlung vor Ort zur Verfügung. Vorhandene Datenbestände sind nicht immer aktuell und nachvollziehbar für jeden Betrieb vorhanden und zusätzliche Ermittlungen im Tierseuchenfall sind zeitintensiv und binden Personal. In diesem Artikel wird ein IT-Tool vorgestellt, welches durch sein technisches und organisatorisches Konzept die Kommunikation zwischen Privatwirtschaft und Behörden unterstützt und die behördliche Abfrage privatwirtschaftlicher Daten zur Tierseuchenbekämpfung vereinfacht.	open road tolling;vhf omnidirectional range	Stefanie Slütter;Sophia Schulze-Geisthövel;Alexander Ellebrecht;Brigitte Petersen	2013				NLP	-106.2554411735941	33.676713002039804	3206
9061984dfb6e14587e9ced9fb78ed5046970f677	affine arithmetic and applications to real-number proving		Accuracy and correctness are central issues in numerical analysis. To address these issues, several self-validated computation methods have been proposed in the last fifty years. Their common goal is to provide rigorously correct enclosures for calculated values, sacrificing a measure of precision for correctness. Perhaps the most widely adopted enclosure method is interval arithmetic. Interval arithmetic performs well in a wide range of cases, but often produces excessively large overestimations, unless the domain is reduced in size, e.g., by subdivision. Many extensions of interval arithmetic have been developed in order to cope with this problem. Among them, affine arithmetic provides tighter estimations by taking into account linear correlations between operands. This paper presents a formalization of affine arithmetic, written in the Prototype Verification System (PVS), along with a formally verified branch-andbound procedure implementing that model. This procedure and its correctness property enables the implementation of a PVS strategy for automatically computing upper and lower bounds of real-valued expressions that are provably correct up to a user-specified precision.	affine arithmetic;algorithm;automated theorem proving;branch and bound;c++;code;compiler;computation;coq (software);correctness (computer science);data structure;experiment;formal proof;formal verification;hol (proof assistant);high-level programming language;horner's method;interval arithmetic;isabelle;numerical analysis;operand;polynomial;prototype verification system;rate of convergence;subdivision surface;test case;zonohedron	Mariano M. Moscato;César A. Muñoz;Andrew P. Smith	2015		10.1007/978-3-319-22102-1_20	affine arithmetic	PL	-16.422554058265295	25.480170588697746	3216
1ef8730f6606c23bcea5c34bf55dfd57b860dec0	designing and building modern information systems; a series of decisions to be made		This paper aims at surveying several critical aspects in the process of creating modern information systems, such as: methods utilized to build the system and to select the IT platform, integration into the target enterprise and evaluation of the process.	information system;requirement	Florin G. Filip	2011	The Computer Science Journal of Moldova		information system;mathematics;mathematical optimization;management science	DB	-67.69065167002296	4.273481371144379	3223
02c9f32cb296ea163b42921820a7bdab8847dcee	sparqs: a qualitative spatial reasoning engine	graphical interface;qualitative spatial reasoning;spatial database;qualitative spatial representation;qa75 electronic computers computer science;design and implementation;spatial representation;spatial composition tables;spatial relationships	In this paper the design and implementation of a general qualitative spatial reasoning engine (SPARQS) is presented. Qualitative treatment of information in large spatial databases is used to complement the quantitative approaches to managing those systems, in particular, it is used for the automatic derivation of implicit spatial relationships and in maintaining the integrity of the database. To be of practical use, composition tables of spatial relationships between different types of objects need to be developed and integrated in those systems. The automatic derivation of such tables is considered to be a major challenge to current reasoning approaches. In this paper, this issue is addressed and a new approach to the automatic derivation of composition tables is presented. The method is founded on a sound settheoretical approach for the representation and reasoning over arbitrarily shaped objects in space. A reasoning engine tool, SPARQS, has been implemented to demonstrate the validity of the approach. The engine is composed of a basic graphical interface where composition tables between the most common types of spatial objects are built. An advanced interface is also provided, where users are able to describe shapes of arbitrary complexity and to derive the composition of chosen spatial relationships. Examples of the application of the method using different objects and different types of spatial relationships are presented and new composition tables are built using the reasoning engine. q 2004 Elsevier B.V. All rights reserved.	adjacency matrix;complexity;concave function;graphical user interface;object type (object-oriented programming);realization (linguistics);semantic reasoner;spatial database;spatial–temporal reasoning;viz: the computer game	Baher A. El-Geresy;Alia I. Abdelmoty	2004	Knowl.-Based Syst.	10.1016/j.knosys.2004.03.004	spatial relation;qualitative reasoning;computer science;theoretical computer science;data mining;graphical user interface;spatial database	AI	-21.567332038130413	4.456779227067793	3224
2e0f63241cfa7a0333c984247b03165abbfe95fc	"""self-configuration of """"home abstraction layer"""" via sensor-actuator network"""	home as smart environment;actuator;sensor;home device management;osgi	We propose a mechanism and system for the identification, self-configuration, monitoring and control of non-networked home devices through a shared backplane of networked sensors and actuators. The resulting generic home abstraction layer interfaces to all kinds of physical entities of the home through a software proxy, as if they were state-of-the-art networked devices. The matching of the entities being discovered in the home/building environment to known semi-generic models is performed by iterative approximation. The architecture and OSGi-based implementation of this system is described. Examples are provided for typical home appliances and other subsystems of the home/building that may be dealt with in a similar way.	abstraction layer	Zheng Hu;Gilles Privat;Stéphane Frénot;Bernard Tourancheau	2011		10.1007/978-3-642-25167-2_17	embedded system;home automation;real-time computing;sensor;computer security;actuator	ECom	-39.56717247133639	46.02195300314233	3228
fbab4c0f264fdf350ddfe954dd2c81f8873cde1c	open source development tools for ims research	open source software project;packet-switched architecture;practical ims;rich service;ims research;next generation network architecture;ims technology;open source development tool;ims project;ims user;open source;open internet protocol;internet protocol;testbed;web development;next generation network;ims;test bed;proof of concept	The 3GPP IMS is a next generation network architecture aimed at bringing the features and rich services of the Internet to the telephony world. Traditionally telephony products are developed by large companies with access to the proprietary solutions required for PSTN products. However, the shift to a packet-switched architecture and open Internet protocols has increased the developer base to include the huge community of web-developers.  Consequently there are currently several open source software projects that aim to provide proof-of-concept implementations and research tools for promoting the development and adoption of IMS technologies. This work investigates the tools created by four open source IMS projects and incorporates these tools into a practical IMS test-bed framework. Evaluations are performed that demonstrate the capabilities and limitations of these tools in providing rich services to IMS users.	iptv;information management system (ims);internet protocol suite;network architecture;network packet;next-generation network;open-source software;packet switching;programming tool;server (computing);software deployment;streaming media;testbed	David Waiting;Richard Good;Richard Spiers;Neco Ventura	2008		10.1145/1390576.1390627	computer science;operating system;distributed computing;world wide web;computer network;testbed	SE	-15.760422657112228	92.55710527536941	3232
729f980f2fa3e2bf27cda65465687ea9c846155a	a case study using the imse experimentation tool	hospital information system;performance modelling	This paper presents a tool for creating and running experiments within a performance modelling environment, and a practical case study through which its key features are illustrated. The case study is concerned with optimising the load arrangement of a hospital information system. The paper describes the experiments created to support the case study in a textual and a graphical format.		Jane Hillston;Andreas L. Opdahl;Rob Pooley	1991		10.1007/3-540-54059-8_90	simulation;computer science;systems engineering;management science	HCI	-58.716646054160705	12.795559797695795	3236
59c046883945f467e86b1aa59c4575aec8e99651	dynsec: on-the-fly code rewriting and repair		Security patches protect an application from discovered vulnerabilities and should be applied as fast as possible. On the other hand, patching the application reduces the availability of the service due to the necessary restart. System administrators need to balance system availability with a potential compromise of system integrity. A dynamic software update mechanism applies security updates on the fly but does not protect from unknown vulnerabilities. Software-based fault isolation on the other hand uses a sandbox to protect the integrity of a system by detecting unpatched vulnerabilities but provides no mechanism to repair any vulnerabilities. This paper presents DynSec, a mechanism for on-thefly code rewriting and repair that dynamically applies security patches for unmodified binary applications. A sandbox protects the integrity of the system while the dynamic update mechanism increases the availability of the application. A prototype implementation that needs no a-priori cooperation from the application incurs a combined overhead of 11% on the SPEC CPU2006 benchmarks for the sandbox and the dynamic update mechanism.	fault detection and isolation;on the fly;overhead (computing);patch (computing);prototype;rewriting;sandbox (computer security);sensor;system administrator;system integrity;vulnerability (computing)	Mathias Payer;Boris Bluntschli;Thomas R. Gross	2013			system integrity;software;on the fly;real-time computing;spec#;sandbox (computer security);fault detection and isolation;rewriting;computer science;compromise	OS	-55.36749830347227	56.07095393597662	3237
fda95db1105bb56c692d30a3b9f7851ca779d894	policies and patterns for high-performance, real-time object request brokers	real time systems middleware distributed computing quality of service computer networks concurrent computing embedded computing object oriented modeling protocols embedded system;protocols;concurrent computing;distributed computing;embedded system;computer networks;middleware;quality of service;high performance;object request broker;object oriented modeling;embedded computing;real time systems	Middleware is becoming increasingly important for building flexible communication systems that reduce software development cycle time and effort. Unfortunately, conventional middleware implementations of CORBA have historically lacked the efficiency, predictability, and scalability required by real-time systems. A decade of intensive R&D on design techniques and optimization principle patterns has recently converged, however, to yield high-performance and real-time middleware that can meet end-to-end Quality of Service (QoS) requirements for real-time systems. This tutorial outlines recent advances in real-time middleware, focusing on the policies and patterns in Real-time CORBA. Real-time CORBA defines a standard set of interfaces and capabilities to manage CPU, network, and memory resources predictably and efficiently end-to-end. This tutorial will illustrate via real world examples the key features and policies in the Real-time CORBA programming model. It will also describe the patterns that can be applied in ORB architectures to minimize priority inversion and non-determinism, associate client requests with servants in constant time, and implement standard and custom middleware protocols using small memory footprints. The patterns and optimizations covered in the tutorial are based on TAO, which is a widely used open-source real-time ORB that supports end-to-end QoS guarantees over a range of networks and embedded system interconnects. TAO is currently deployed at Boeing, Lockheed, Lucent, Motorola, Nokia, Nortel, Raytheon, SAIC, and Siemens, where it is used for real-time avionics, simulations, and telecommunications systems. Source code, documentation, and technical papers on TAO are available at www.cs.wustl.edu/~schmidt/TAO.html. Douglas Schmidt is an Associate Professor in the Electrical and Computer Engineering Department at the University of California, Irvine. Previously, he was Director of the Center for Distributed Object Computing and an Associate Professor of Department of Computer Science and the Department of Radiology at Washington University in St. Louis, Missouri, USA.His research focuses on design patterns, implementation, and experimental analysis of object-oriented frameworks that facilitate the development of high-performance, real-time distributed object computing systems on parallel processing platforms running over high-speed networks and embedded system interconnects. Proceedings of the Third International Symposium on Distributed-Objects and Applications (DOA’01) 0-7695-1300-X/01 $10.00 © 2001 IEEE Dr. Schmidt is an internationally recognized expert on distributed object computing and has published widely in top IEEE, ACM, IFIP, and USENIX technical conferences and journals on topics ranging from high-performance communication software systems and parallel processing for high-speed networking protocols to real-time distributed object computing with CORBA and object-oriented design patterns. Proceedings of the Third International Symposium on Distributed-Objects and Applications (DOA’01) 0-7695-1300-X/01 $10.00 © 2001 IEEE	avionics;central processing unit;common object request broker architecture;communications protocol;computer engineering;computer science;design pattern;distributed object;electrical connection;embedded system;end-to-end encryption;end-to-end principle;international federation for information processing;mathematical optimization;middleware;nondeterministic algorithm;open-source software;parallel computing;priority inversion;programming model;quality of service;radiology;real-time clock;real-time computing;real-time transcription;requirement;scalability;schmidt decomposition;simulation;software development process;software documentation;software system;speaker wire;tao;time complexity	Douglas C. Schmidt	2001		10.1109/DOA.2001.954102	general inter-orb protocol;real-time computing;computer science;object request broker;common object request broker architecture;database;distributed computing;utility computing;grid computing;autonomic computing	Embedded	-33.56045266779951	44.91262729680631	3238
84d726c89ff91c83a478d903e1b83791139fe62b	ontology proposal for quality oriented reuse	quality assurance;software development process;ontologies artificial intelligence;ontologies proposals programming productivity quality assurance history software libraries design engineering software architecture area measurement;software reusability;software development;quality criteria;software quality assurance process ontology proposal software reusability software development;ontology proposal;software quality assurance process;software reusability ontologies artificial intelligence software quality;software quality	Quality and productivity improvement are some advantages of reuse in the software development process. Although the reuse concept is not new, there is not a rigorous representation of related reuse concepts and quality. In this paper, we present a historical review of the reuse concepts, and ontology based on reuse definitions and its relation with quality assurance processes. This ontology is part of a research in progress that promotes reuse from the beginning of the software development process in a specific domain, focusing in quality criteria	software development process	Pamela Ghiotto;Maryoly Ortega;Anna Grimán;Luis Eduardo Mendoza;María A. Pérez	2006	2006 IEEE International Conference on Information Reuse & Integration	10.1109/IRI.2006.252470	software security assurance;quality assurance;reusability;personal software process;verification and validation;computer science;package development process;social software engineering;software development;software engineering;software construction;software walkthrough;resource-oriented architecture;software deployment;software quality control;goal-driven software development process;software development process;software quality;software metric;software quality analyst;software peer review	SE	-60.011459369876825	25.304736808247643	3239
6c280957dcc436a03e8a110ce2afb914f1abe569	appliance-level short-term load forecasting using deep neural networks		The recently employed demand-response (DR) model enabled by the transformation of the traditional power grid to the SmartGrid (SG) allows energy providers to have a clearer understanding of the energy utilisation of each individual household within their administrative domain. Nonetheless, the rapid growth of IoT-based domestic appliances within each household in conjunction with the varying and hard-to-predict customer-specific energy requirements is regarded as a challenge with respect to accurately profiling and forecasting the day-to-day or week-to-week appliance-level power consumption demand. Such a forecast is considered essential in order to compose a granular and accurate aggregate-level power consumption forecast for a given household, identify faulty appliances, and assess potential security and resilience issues both from an end-user as well as from an energy provider perspective. Therefore, in this paper we investigate techniques that enable this and propose the applicability of Deep Neural Networks (DNNs) for short-term appliance-level power profiling and forecasting. We demonstrate their superiority over the past heavily used Support Vector Machines (SVMs) in terms of prediction accuracy and computational performance with experiments conducted over real appliance-level dataset gathered in four residential households.	administrative domain;aggregate data;deep learning;experiment;feature selection;neural networks;requirement;suicidegirls;support vector machine	Ghulam Mohi Ud Din;Andreas Mauthe;Angelos K. Marnerides	2018	2018 International Conference on Computing, Networking and Communications (ICNC)	10.1109/ICCNC.2018.8390366	grid;support vector machine;data mining;artificial neural network;administrative domain;profiling (computer programming);smart grid;internet of things;computer science	EDA	-27.243798504508092	63.46159591082036	3244
03881c039fcef734cfb24b392df366d247cf3c00	certificate validation service using xkms for computational grid	key management;globus toolkit;computational grid;gsi;grid;security requirements;grid service;xml;xml security;grid security infrastructure;access control;xkms;certificate validation;high end computing;security;security assertion markup language;global grid forum;markup language;web services security;grid system	A computational grid is a hardware and software infrastructure capable of providing dependable, consistent, pervasive, and inexpensive access to high-end computational resource. There are many ways to access the resources of a computational grid, each with unique security requirements and implications for both the resource user and the resource provider. Current Grid security Infrastructure using PKI based on SSO. But open grid service Security Infrastructure in Global Grid Forum(GGF) will extend use of grid system or services up to business area using XML web services security technology. This paper describes a novel security approach on open grid service to validate certificate based on current globus toolkit environment using XKMS(XML Key Management Specification) and SAML(Security Assertion Markup Language), XACML(extensible Access Control Markup Language) in XML security.	access control;computational resource;computer hardware;dependability;grid security infrastructure;grid computing;key management;pervasive informatics;public key infrastructure;requirement;security assertion markup language;ws-security;web service;xkms;xml	Namje Park;Kiyoung Moon;Sungwon Sohn	2003		10.1145/968559.968577	semantic grid;computer science;database;security service;world wide web;computer security;drmaa;grid computing	HPC	-46.280751954748425	54.847908363992985	3245
fe567da06d0f9c1e4a6d847160b9527078fa2a16	seamless mobile applications across heterogeneous internet access	ip connectivity;protocols;internet access;application context transfer framework;internet transcoding wireless lan web server peer to peer computing ip networks network servers streaming media usa councils access protocols;ip level handoff;usa councils;network servers;internet;streaming media;mobile communication;access protocols;ip networks;ip connectivity seamless mobile applications heterogeneous internet access mobile internet application context transfer framework ip level handoff mobile ip;wireless lan;internet application;web server;seamless mobile applications;peer to peer computing;mobile computing;mobile internet;heterogeneous internet access;transcoding;protocols mobile communication radio access networks mobile computing internet;mobile application;mobile ip;radio access networks	With the growing popularity of the mobile Internet, end users would demand seamless continuation of their Internet applications as they hop across different access technologies and administrative domains during an ongoing session. An important technology to enable this is mobile IP and its companion seamless handoff protocols, which are currently under development in the IETF. These protocols can potentially provide the mobile end user with a seamless IP connectivity. However, additional techniques may be required to ensure seamless continuation of the user's Internet applications after an IP-level handoff. In this paper, we focus on the handoff scenario in which there is a need to relocate (or provision) certain application-specific functionality to ensure the seamless continuation of the user's application after handoff. We provide a solution framework called the application context transfer (ACT) framework to address this need and describe its application to such handoff scenarios.	internet access;mobile app;seamless3d	Dirk Trossen;Hemant H. Chaskar	2003		10.1109/ICC.2003.1204469	communications protocol;the internet;transcoding;mobile telephony;internet access;computer science;operating system;internet privacy;mobile computing;world wide web;web server;mobile ip;computer network	Metrics	-15.02427118686267	91.12365680552128	3246
02850303f1a5c8375aab0ba68a2ebe884a43c56d	a one's complement cache memory	distributed data;cadcam;microprocessors;data transmission;relational data;memory management;prefetching;cache memory;runtime;system performance;chip;memory access;computer aided manufacturing;hitting time;cache performance;parallel computer;computer science;cache memory prefetching runtime optimizing compilers program processors costs parallel processing computer aided manufacturing cadcam computer science;optimizing compilers;program processors;trace driven simulation;parallel processing;mathematics computers information science management law miscellaneous	Most of today's microprocessors have an on-chip cache to reduce average memory access latency. These on-chip caches generally have low associativity and small sizes. Cache line conflicts are the main source of cache misses which are essential to overall system performance. This paper introduces an innovative, conflict-free cache design, called one's complement cache. By means of parallel computation of cache addresses and memory addresses of data, the new design does not increase critical hit time of cache accesses. Cache misses caused by line interferences are minimized by means of evenly distributing data items referenced by program loops across all sets in a cache. Evenly distribution of data in the cache is achieved by making the number of sets in the cache a prime or an odd number thereby the chance of related data being mapped to a same set is small. Trace-driven simulations are used to evaluate the performance of the new design. Performance results on a set of programs from SPEC92 benchmarks show that the new design improves cache performance over the conventional set-associative cache by about 100% with negligibly additional hardware cost.	cpu cache;computation;control flow;microprocessor;model-driven architecture;ones' complement;parallel computing;simulation	Qing Yang;Sridhar Adina	1994	1994 International Conference on Parallel Processing Vol. 1	10.1109/ICPP.1994.39	chip;bus sniffing;least frequently used;pipeline burst cache;parallel processing;computer architecture;cache-oblivious algorithm;snoopy cache;parallel computing;real-time computing;cache coloring;page cache;cpu cache;telecommunications;cache;relational database;computer science;write-once;cache invalidation;operating system;adaptive replacement cache;smart cache;hitting time;programming language;mesi protocol;cache algorithms;cache pollution;mesif protocol;data transmission;memory management	Arch	-8.236412451478577	53.26033188520803	3247
33f5b231dd5e394dc568cd2eb1e8588d31ec200f	using code ownership to improve ir-based traceability link recovery	textual information;standards;software system;software maintenance;information retrieval;ir based traceability link recovery improvement software system java classes traceability link recovery using information retrieval and code ownership tyrion class retrieval source code component documentation code ownership information textual information noise filtering textual similarity software artifacts ir technique;vocabulary;software systems;system documentation;text analysis;code ownership;text analysis information retrieval java software maintenance system documentation;traceability link recovery;tyrion;accuracy;empirical studies traceability link recovery code ownership information retrieval;code ownership information;software artifacts;traceability link recovery using information retrieval and code ownership;noise filtering;source code component;context accuracy java vocabulary standards software systems;ir technique;empirical studies;ir based traceability link recovery improvement;class retrieval;java classes;context;textual similarity;documentation;java	Information Retrieval (IR) techniques have gained wide-spread acceptance as a method for automating traceability recovery. These techniques recover links between software artifacts based on their textual similarity, i.e., the higher the similarity, the higher the likelihood that there is a link between the two artifacts. A common problem with all IR-based techniques is filtering out noise from the list of candidate links, in order to improve the recovery accuracy. Indeed, software artifacts may be related in many ways and the textual information captures only one aspect of their relationships. In this paper we propose to leverage code ownership information to capture relationships between source code artifacts for improving the recovery of traceability links between documentation and source code. Specifically, we extract the author of each source code component and for each author we identify the “context” she worked on. Thus, for a given query from the external documentation we compute the similarity between it and the context of the authors. When retrieving classes that relate to a specific query using a standard IR-based approach we reward all the classes developed by the authors having their context most similar to the query, by boosting their similarity to the query. The proposed approach, named TYRION (TraceabilitY link Recovery using Information retrieval and code OwNership), has been instantiated for the recovery of traceability links between use cases and Java classes of two software systems. The results indicate that code ownership information can be used to improve the accuracy of an IR-based traceability link recovery technique.	artifact (software development);cognitive dimensions of notations;control system;documentation;information retrieval;information source;java;linear discriminant analysis;relevance;software system;telecommunications network;traceability;version control;vocabulary mismatch	Diana Diaz;Gabriele Bavota;Andrian Marcus;Rocco Oliveto;Silvia Takahashi;Andrea De Lucia	2013	2013 21st International Conference on Program Comprehension (ICPC)	10.1109/ICPC.2013.6613840	text mining;computer science;software engineering;reverse semantic traceability;data mining;database;world wide web;software system	SE	-57.669500969538355	33.521793613608246	3252
1eed828938753e60230e903383b2a05896b0df54	managing conflicts when using combination strategies to test software	software testing;conflict handling;automatic testing;software testing system testing informatics engineering management software engineering automatic testing;replace method software testing input parameter model conflict handling test case generation avoid method;system under test;software engineering;test case generation;program testing;engineering management;input parameter model;datavetenskap datalogi;system testing;informatics;computer science;reduction method;avoid method;replace method	Testers often represent systems under test in input parameter models. These contain parameters with associated values. Combinations of parameter values, with one value for each parameter, are potential test cases. In most models, some values of two or more parameters cannot be combined. Testers must then detect and avoid or remove these conflicts. This paper proposes two new methods for automatically handling such conflicts and compares these with two existing methods, based on the sizes of the final conflict-free test suites. A test suite reduction method, usable with three of the four investigated methods is also included in the study, resulting in seven studied conflict handling methods. In the experiment, the number and types of conflicts, as well as the size of the input parameter model and the coverage criterion used, are varied. All in all, 3854 test suites with a total of 929,158 test cases were generated. Two methods stand out as tractable and complementary. The best method (called the avoid methods) with respect to test suite size is to avoid selection of test cases with conflicts. However, this method cannot always be used. The second best method (called the replace method), removing conflicts from the final test suite, is completely general.	algorithm;cobham's thesis;experiment;parameter (computer programming);point of view (computer hardware company);test case;test suite	Mats Grindal;A. Jefferson Offutt;Jonas Mellin	2007	2007 Australian Software Engineering Conference (ASWEC'07)	10.1109/ASWEC.2007.27	reliability engineering;computer science;systems engineering;engineering;software engineering;software testing;system under test;informatics;system testing;test case	SE	-59.91810334030833	32.733513630515475	3257
00059e34651b8d46f68d5caa2cf71396d74523e1	relations over words and logic: a chronology				Christian Choffrut	2006	Bulletin of the EATCS		and gate;mathematics;chronology;algorithm	Logic	-12.538928854923752	10.766177679951797	3259
68a0348217a42f3c9d1f491247361fc8d2c8b628	partial utility-driven scheduling for flexible sla and pricing arbitration in clouds	community clouds;vm scheduling;processor scheduling;service level agreements;virtual machining;resource management;computational modeling;scheduling;resource management cloud computing degradation scheduling computational modeling virtual machining processor scheduling;vm allocation;utility driven scheduling;article;vm scheduling cloud computing community clouds service level agreements utility driven scheduling vm allocation;cloud computing	Cloud SLAs compensate customers with credits when average availability drops below certain levels. This is too inflexible because consumers lose non-measurable amounts of performance being only compensated later, in next charging cycles. We propose to schedule virtual machines (VMs), driven by range-based non-linear reductions of utility, different for classes of users and across different ranges of resource allocations: partial utility. This customer-defined metric, allows providers transferring resources between VMs in meaningful and economically efficient ways. We define a comprehensive cost model incorporating partial utility given by clients to a certain level of degradation, when VMs are allocated in overcommitted environments (Public, Private, Community Clouds). CloudSim was extended to support our scheduling model. Several simulation scenarios with synthetic and real workloads are presented, using datacenters with different dimensions regarding the number of servers and computational capacity. We show the partial utility-driven driven scheduling allows more VMs to be allocated. It brings benefits to providers, regarding revenue and resource utilization, allowing for more revenue per resource allocated and scaling well with the size of datacenters when comparing with an utility-oblivious redistribution of resources. Regarding clients, their workloads' execution time is also improved, by incorporating an SLA-based redistribution of their VM's computational power.	analysis of algorithms;cloudsim;computation;data center;elegant degradation;image scaling;like button;nonlinear system;run time (program lifecycle phase);scheduling (computing);service-level agreement;signed zero;simulation;synthetic intelligence;virtual machine	José Simão;Luís Veiga	2016	IEEE Transactions on Cloud Computing	10.1109/TCC.2014.2372753	fair-share scheduling;fixed-priority pre-emptive scheduling;real-time computing;earliest deadline first scheduling;cloud computing;dynamic priority scheduling;computer science;rate-monotonic scheduling;resource management;operating system;two-level scheduling;stride scheduling;distributed computing;scheduling;round-robin scheduling;computational model;scheduling	HPC	-21.643922707782053	61.45973242461202	3261
38b9b3857b5a38f5bbfa70f8f1e16dc70129f8c6	hardware & software: programme zur datenkompression				H. Baumgarten	1996	LOG IN		software engineering;engineering;software	Arch	-93.19392427690946	25.732265015567332	3263
aded631076e160870f24961c761e4ae9f2755dca	smartness versus embeddability: a tradeoff for the deployment of smart agvs in industry		In order to deploy AGVs in industry, it is mandatory to consider the tradeoff between smartness and embeddability. This paper aims at making the manufacturing research community more sensitive about this tradeoff and its consequences. Nowadays, AGVs are widely chosen by manufacturers to implement flexible material-handling systems which are necessary to cover the industrial requirements. However, many issues, presented in this paper, must be tackled to deploy these AGVs. A tradeoff-oriented procedure is proposed by considering these issues in flexible manufacturing system applications. Then, an approach is proposed to illustrate this procedure by providing simulation and experimental results. This approach is also used to roughly describe the smartness/embeddability tradeoff.		Guillaume Demesure;Damien Trentesaux;Michael Defoort;Abdelghani Bekrar;Hind Bril;Mohamed Djemaï;André Thomas	2017		10.1007/978-3-319-73751-5_30	software deployment;systems engineering;flexible manufacturing system;computer science	ECom	-61.732064973147175	11.26787624184949	3269
c37b6262f0db12987b2898df5d5f63a61978dfc3	design by framework completion	component based software engineering;object oriented framework;learning by example;software reuse	An object-oriented framework in essence defines an architecture for a family of applications or subsystems in a given domain. Every application in the family obeys these architectural restrictions. Such frameworks are typically delivered as collections of inter-dependent abstract classes, together with their concrete subclasses. The abstract classes and their interdependencies implicitly realize the architecture. Developing a new application reusing classes of a framework requires a thorough understanding of the framework architecture. We introduce an approach called “Design by Framework Completion”, in which an exemplar (an executable visual model for a minimal instantiation of the architecture) is used for documenting frameworks. We propose exploration of exemplars as a means for learning the architecture, following which new applications can be built by replacing selected pieces of the exemplar. For the piece to be replaced, the inheritance lattice around its class provides the space of alternatives, one of these classes may be suitably adapted (say, by sub-classing) to create the new replacement. “Design by Framework Completion” proposes a paradigm shift when designing in presence of reusable components: It enables a much simpler “top-down” approach for creating applications, as opposed to the prevalent “search for components and assemble them bottom-up” strategy. We believe that this paradigm shift is essential because components can only be fitted together if they all obey the same architectural rules that govern the framework.	bottom-up proteomics;executable;interdependence;programming paradigm;software documentation;top-down and bottom-up design;universal instantiation;visual modeling	Dipayan Gangopadhyay;Subrata Mitra	1996	Automated Software Engineering	10.1007/BF00132567	enterprise architecture framework;reference architecture;the open group architecture framework;real-time computing;computer science;systems engineering;theoretical computer science;software framework;component-based software engineering;programming language;view model	SE	-50.671316315675995	26.36247881542165	3274
cb28f287e6879f51e64d0470f903d1357b70dfc4	network anomaly detection: flow-based or packet-based approach?	network security;anomaly detection;internet architecture;system security;intrusion prevention system;network anomaly detection	One of the most critical tasks for network administrator is to ensure system uptime and availability. For the network security, anomaly detection systems, along with firewalls and intrusion prevention systems are the must-have tools. So far in the field of network anomaly detection, people are working on two different approaches. One is flow-based; usually rely on network elements to make so-called flow information available for analysis. The second approach is packet-based; which directly analyzes the data packet information for the detection of anomalies. This paper describes the main differences between the two approaches through an in-depth analysis. We try to answer the question of when and why an approach is better than the other. The answer is critical for network administrators to make their choices in deploying a defending system, securing the network and ensuring business continuity. Operators of mission critical networks employ a variety of strategies to ensure system uptime and availability. To secure the network from outside malicious activities, firewalls and intrusion prevention systems (IPSs) maybe utilized, along with performance measurement tools and network infrastructure health monitoring systems. However, to protect networks against threads such as DDoS attacks and worm outbreaks, intelligent, real-time solutions are needed. Such kinds of anomalies generate vast amounts of bogus traffic, which can overwhelm the network and any attached hosts. In addition, the traffic that is generated by anomalies may not have a signature, which is required by a typical IPS. It may also arrive on otherwise completely legitimate ports, passing the security checks of a firewall. As a result, a new category of network security systems has appeared, specifically geared to solve this problem. These systems utilize what is commonly known as Behavioral Anomaly Detection or Network Behavior Analysis. Rather than just looking at volumes of packets, these systems intelligently take into account the behavior of the network and the hosts that are attached to that network. Changes in the network behavior are used to detect DDoS attacks, worm outbreaks and otherwise misbehaving hosts or network elements with dramatically improved accuracy. As more	anomaly detection;artificial intelligence;business continuity;denial-of-service attack;experiment;firewall (computing);intrusion detection system;mission critical;network packet;network security;real-time locating system;scott continuity;uptime	Huy Anh Nguyen;Deokjai Choi	2010	CoRR		anomaly-based intrusion detection system;intrusion detection system;host-based intrusion detection system;anomaly detection;network traffic control;intelligent computer network;network management station;computer science;network security;data mining;internet privacy;network access control;computer security;intrusion prevention system	Security	-60.31596701954506	65.40798127214329	3283
884db9a5997e6dbdb0597aa9568f29d321a4e4f3	data integration between objectiver and db-main: a case study of a model-driven interoperability bridge		In building software systems, the integration of tools with the purpose of exchanging data (i.e. tool interoperability) is common practice. Such an integration is one of the application scenarios of Model-Driven Engineering (MDE), which is often called Model-Driven Interoperability (MDI). In the last few years, some MDI approaches have been presented, and they have shown how MDE techniques are useful in bridging tools in order to integrate data. However, the number of case studies is still limited and more practical experiences of building MDI bridges should be published. In this article, we present an MDI bidirectional bridge that integrates the Objectiver and DB-Main tools. DB-Main database schemas are obtained from Objectiver object models, and they are kept consistent. Through this case study, we contrast the majority of techniques that can be used to implement a MDI solution. We mainly focus on the level of automation offered by each alternative. Some lessons learned are commented on.	application programming interface;assignment (computer science);automation;bridging (networking);digital subscriber line;documentation;eclipse modeling framework;high-level programming language;imperative programming;java architecture for xml binding;logical unit number;medium-dependent interface;metamodeling;model driven interoperability;model-driven engineering;model-driven integration;multiple document interface;object constraint language;pivot table;qvt;randomness extractor;software system;windows metafile;workbench	Francisco Javier Bermudez Ruiz;Jesús Joaquín García Molina;Oscar Díaz García	2016	2016 4th International Conference on Model-Driven Engineering and Software Development (MODELSWARD)		computer science;systems engineering;automation;software system;database schema;data modeling;data integration;interoperability;bridging (networking)	SE	-49.22250850753446	25.645537412455166	3285
b7bafca2ec8c2cc2ea0cd80693b2b4fbf24ec4ef	establishing arc consistency for multiple database views	arc consistency;partial order	This paper introduces a class of problems where it is desirable to develop a number of potential search orders in advance. These problems emerge from a database environment in which the high volume of transactions means that pre-processing of the data can make a big difference to run-time performance. Furthermore, the kinds of queries that are made to this database are fairly stereotypical and are derived from a finite set of views of the database. The access path for each view may be expressed as a set of total variable orderings. Seen as a single partial ordering the question then arises as to how local consistency is to be established. Rather than enforcing consistency for each view separately the partial order is processed as a single structure. By organising the variables into groups of mutually dependent variables, this high level structure may be processed in a single DAC-like pass, while full arc-consistency is obtained for each sub-group.	local consistency;view (sql)	Steven A. Battle	1996		10.1007/BFb0034669	partially ordered set;computer science;data mining;local consistency	DB	-26.003601339609105	9.747876210728107	3294
144bdf1cd302ecb2e865d022aef6bf0d76ed5f10	information services for dynamically assembled semantic grids	e science information service dynamically assembled semantic grid web service standard uddi ws context;web service;semantic information;information management;web services;semantic web;information service;grid computing;high performance;semantic grid;scientific information systems;assembly information systems earthquakes informatics art scalability web services predictive models streaming media middleware;web services grid computing scientific information systems semantic web	Many large semantic systems can be described as semantic grids of semantic grids with large amounts of relatively static services and associated semantic information combined with multiple dynamic regions (sessions or subgrids) where the semantic information is changing rapidly. We design a hybrid Information Service supporting both the scalability of large amounts of relatively slowly varying data and a high performance rapidly updated information service for dynamic regions. We use the two Web service standards UDDI and WS-Context in our system.	scalability;ws-context;web services discovery;web service	Mehmet S. Aktas;Geoffrey C. Fox;Marlon E. Pierce	2005	2005 First International Conference on Semantics, Knowledge and Grid	10.1109/SKG.2005.83	web service;semantic interoperability;semantic computing;semantic integration;data web;semantic search;semantic grid;web standards;computer science;semantic web;social semantic web;semantic web stack;database;information management;web intelligence;semantic technology;law;world wide web;information retrieval;semantic analytics;web coverage service	HPC	-41.44532465517016	8.311109349872275	3295
24e1217da8e2c6d057575f6027f6e6446bf1f4a6	mining context-based user preferences for m-services applications	content management;conceptual architecture;publishing;large scale content management;semantic web content management publishing;open online publishing system;community oriented semantic tagging;large scale;user profiling;real world application;clustering;open access;semantic web;flexible online publishing large scale content management semantic web collaborative tagging conceptual architecture community oriented semantic tagging open online publishing system;web pages information processing problem solving web mining classification algorithms switches logic computer science grain size set theory;collaborative tagging;incremental web mining;web page classification;flexible online publishing	Granular computing is a new conceptual and computing paradigm of information processing, the idea of which is the use of granules for problem solving at different granularities. Web page classification is an important research direction for Web mining. In this paper, we propose an approach to Web page classification based on granules. Some concepts are defined firstly. Then, a Web page classification framework based on granules is built and an automatic classification algorithm is proposed. Experiment results demonstrate that the proposed approach is promising and effective.1	algorithm;granular computing;information processing;problem solving;programming paradigm;web mining;web page	Chen Wu;Elizabeth Chang	2007	IEEE/WIC/ACM International Conference on Web Intelligence (WI'07)	10.1109/WI.2007.15	content management;computer science;conceptual architecture;semantic web;data mining;database;publishing;cluster analysis;world wide web;information retrieval	Web+IR	-41.572627738201135	6.998441820047933	3300
af042db85ab2216764b04c6401b4fdd3872b08d5	combining interaction and automation in process algebra verification	theorem prover;state explosion;process algebra;user interaction	Most existing verification tools for process algebras allow the correctness of specifications to be checked in a fully automatic fashion. These systems have the obvious advantage of being easy to use, but unfortunately they also have some drawbacks. In particular, they do not always succeed in completing the verification analysis, due to the problem of state explosion, and they do not provide any insight into the meaning of the intended specifications. In this paper we consider an alternative approach in which both interactive and automatic techniques are combined in the hope that the advantages of automation are retained, and that some of its disadvantages are overcome. To achieve our goal, we use the interactive theorem prover ItOL as a framework for supporting the theory of observational congruence of ccs, and provide a set of automatic proof tools, based on the algebraic axiomatization of the language, which can be used interactively. To illustrate how interaction and automation can be intermixed, we describe two verification strategies which exhibit different degrees of user interaction. 1 I n t r o d u c t i o n In the past few years, many verification tools based on process algebras (e.g. ccs, csP, ACP, LOTOS) have been proposed for proving properties of concurrent systems. Most of them generate a state transition representation of process algebra specifications, and use these to verify equivalences of specifications, and to show that specifications satisfy some logical (modal) properties. All these systems function fully automatically, and no facility is provided to perform any incremental or interactive reasoning. Our experience is that it is not always possible to rely on a completely automatic approach to verification, even in the simple case of finite processes. Apart from belng necessary to address problems which are, in the general case, undecidable, a system which leaves some crucial decisions to the user can, for efficiency reasons, also be useful when tackling decidable problems. A totally *Research supported by, and collaboratively carried out at, Hewlett-Packard Laboratories, Pisa Science Centre, Corso Italia 115, 1-56125 Pisa, Italy.	algebra of communicating processes;automated theorem proving;automation;axiomatic system;communicating sequential processes;concurrency (computer science);congruence of squares;correctness (computer science);interactivity;modal logic;process calculus;proof assistant;state transition table;undecidable problem	Albert John Camilleri;Paola Inverardi;Monica Nesi	1991		10.1007/3540539816_72	discrete mathematics;theoretical computer science;pure mathematics;mathematics	Logic	-17.53154943082986	25.90251885539869	3302
1a6523fdf59646ce01d5b22a59766e759c53b361	métodos não lineares descontínuos submalha para a equação de convecção-difusão-reação				Natalia Cristina Braga Arruda Alves da Silva	2010				Crypto	-105.7183212908318	18.32335297514598	3326
9d892169c60c6e557f59d0fb105dc776bf9da78d	efficient secure similarity computation on encrypted trajectory data	protocols;1712 software;encryption;query processing;1710 information systems;trajectory protocols encryption query processing;polynomial time secure similarity computation encrypted trajectory data database outsourcing large scale data storage data management query processing spatiotemporal relationships privacy concerns secure trajectory similarity computation secure trajectory query processing secure trajectory query evaluation;trajectory;1711 signal processing;storage management cloud computing computational complexity cryptographic protocols data privacy outsourcing query processing	Outsourcing database to clouds is a scalable and cost-effective way for large scale data storage, management, and query processing. Trajectory data contain rich spatio-temporal relationships and reveal many forms of individual sensitive information (e.g., home address, health condition), which necessitate them to be encrypted before being outsourced for privacy concerns. However, efficient query processing over encrypted trajectory data is a very challenging task. Though some achievements have been reported very recently for simple queries (e.g., SQL queries, kNN queries) on encrypted data, there is rather limited progress on secure evaluation of trajectory queries because they are more complex and need special treatment. In this paper, we focus on secure trajectory similarity computation that is the cornerstone of secure trajectory query processing. More specifically, we propose an efficient solution to securely compute the similarity between two encrypted trajectories, which reveals nothing about the trajectories, but the final result. We theoretically prove that our solution is secure against the semi-honest adversaries model as all the intermediate information in our protocols can be simulated in polynomial time. Finally we empirically study the efficiency of the proposed method, which demonstrates the feasibility of our solution.	bluetooth;computation;computer data storage;database;encryption;information sensitivity;outsourcing;sql;scalability;semiconductor industry;time complexity	An Liu;Kai Zheng;Lu Li;Guanfeng Liu;Lei Zhao;Xiaofang Zhou	2015	2015 IEEE 31st International Conference on Data Engineering	10.1109/ICDE.2015.7113273	communications protocol;query optimization;computer science;trajectory;theoretical computer science;data mining;database;encryption	DB	-40.4793456435464	66.07033570330488	3334
e99bebd77ac82455b3ad1e6f49934094297538a0	data visibility and trust enhancement of enterprise customers in cloud computing services		This Cloud computing is a pervasive technology and has been a platform in IT for several years. Cloud service providers (CSP) have developed and offered different service platforms to accommodate different needs of enterprise subscribers. However, there still exists the situation of enterprise customers' hesitation and reluctance to deploy their core applications using cloud service platforms. Our survey results show that security is the perceived major concern of existing and prospective enterprise customers of cloud services. The research investigates the trust between enterprise customers and cloud service provider with regards to the perceived security of cloud services. Enterprise customers expected to be reassured of cloud service security in a more visible way from cloud service provider. The perception of data security in cloud computing platform can be enhanced by data visibility. The term data visibility has been widely used in the IT industry especially from ICT product and solution vendors. However, there is no common practice guideline or standard in industry. This paper defines the characteristic and elements of data visibility and proposes a model and architecture as best practice reference for the industry. We expect that the enhancement of data visibility can earn the trust of enterprise customers in adopting public cloud services.	australian partnership for advanced computing;best practice;cloud computing;comment (computer programming);cryptographic service provider;data security;feedback;pervasive informatics;prospective search;prototype;reference model;value (ethics)	W. P. Yuen;K. B. Chuah	2017	2017 12th International Conference for Internet Technology and Secured Transactions (ICITST)	10.23919/ICITST.2017.8356435	architecture;visibility;computer security;data security;best practice;computer science;cloud computing;data modeling;information technology;information and communications technology	Metrics	-79.5969931811982	12.83602295945816	3336
43ea7557eab4e27a97cf73c0fa9a424735845c9e	distributed computing environments: effects on software maintenance difficulty	distributed system;systeme reparti;life cycle;rate of change;maintenance;programming environment;software maintenance;software management;information technology;qualitative data;distributed computing;conceptual model;technologie information;customer service;ease of use;medio ambiente programacion;computer architecture;sistema repartido;distributed computing system;mantenimiento;calculo repartido;systems and applications;distributed computing environment;information system;tecnologia informacion;calcul reparti;systeme information;gestion logiciel;environnement programmation;sistema informacion	The computing world is undergoing a significant transformation from centralized computer architectures to non-centralized or distributed computer architectures. Historically, the largest single life cycle computer system cost has been for maintaining information system software. This paper examines a new area of research in software maintenance (and development in general), focusing on the issue of whether and to what extent distributed computer operating environments directly affect software maintenance. The issue depends on two diametrics of information system architectures: component simplicity and system complexity. The smaller (but more numerous) the system components, the easier they are to deal with individually (i.e., lower software maintenance costs) but the more difficult it is to deal with the overall system (i.e., higher software maintenance costs). This paper proposes a new conceptual model for information systems complexity based on component number and variety, interaction number and variety, and the overall rate of change. By applying this complexity model to distributed computing environments, this research sought empirical quantitative and qualitative data from IS system and application software analysts, designers, programmers, testers, and customer service representatives to determine whether the complexity of a distributed computer system has a greater effect on software maintenance than component simplicity and what the related explanatory factors are. Results indicate that the overall complexity of the studied distributed systems overwhelmed the ease-of-use and simplicity of their components -thus increasing the overall difficulty of software maintenance. Some distributed computing factors had a much greater effect than others. Finally, this paper offers some management guidance on lowering the cost of distributed computing environ-	centralized computing;computer architecture;distributed computing;information system;programmer;software maintenance	Scott L. Schneberger	1997	Journal of Systems and Software	10.1016/S0164-1212(96)00107-0	biological life cycle;qualitative property;simulation;software sizing;usability;computer science;systems engineering;engineering;conceptual model;component-based software engineering;software development;software design description;operating system;software engineering;middleware;distributed computing;software maintenance;information technology;software deployment;information system;software metric;distributed computing environment	SE	-68.15567104681911	31.741026085498547	3337
3176bc64201bd158d3f6d2cc7746b8ef689bfbba	using swrl and owl to capture domain knowledge for a situation awareness application applied to a supply logistics scenario	lenguaje programacion;representacion conocimientos;ontologie;logistique;programming language;web semantique;service web;web service;lenguaje marcacion;domain knowledge;logistics;abstract syntax;web semantica;inferencia;representation connaissance;situation awareness;semantic web;langage programmation;ontologia;knowledge representation;markup language;ontology;inference;servicio web;logistica;langage marquage	When developing situation awareness applications we begin by constructing an OWL ontology to capture a language of discourse for the domain of interest. Such an ontology, however, is never sufficient for fully representing the complex knowledge needed to identify what is happening in an evolving situation – this usually requires general implication afforded by a rule language such as SWRL. This paper describes the application of SWRL/OWL to the representation of knowledge intended for a supply logistics scenario. The rules are first presented in an abstract syntax based on n-ary predicates. We then describe a process to convert them into a representation that complies with the binaryonly properties of SWRL. The application of the SWRL rules is demonstrated using our situation awareness application, SAWA, which can employ either Jess or BaseVISor as its inference engine. We conclude with a summary of the issues encountered in using SWRL along with the steps taken in resolving them.	abstract syntax;inference engine;jess;logistics;semantic web rule language;syntactic predicate	Christopher J. Matheus;Kenneth Baclawski;Mieczyslaw M. Kokar;Jerzy Letkowski	2005		10.1007/11580072_11	web service;logistics;situation awareness;abstract syntax;computer science;artificial intelligence;semantic web;ontology;data mining;database;markup language;programming language;world wide web;domain knowledge	AI	-21.970502704492553	9.846688109438572	3342
984f76bbc9ce4f8b7f2770c46b60bdaac21a3c63	establishing extra organizational reuse capabilities	developpement logiciel;component based software engineering;componente logicial;reutilizacion;composant logiciel;classification;software engineering;reuse;technology management;desarrollo logicial;software development;software component;genie logiciel;ingenieria informatica;software reuse;clasificacion;reutilisation	Traceability From Strategic Intent to Capability Today any organization must be cognizant of what is happening in the world around it and recognize the drivers that will affect its future business viability and performance. If it is paying attention, it will be aware of the social, technological, economic, environmental and political changes in its business landscape and will also see threats and opportunities. Based upon these factors, its current and anticipated performance, and an understanding of the fit with its current capabilities, it will renew its strategic intent and reexamine its market and capabilities to plan modifications. Key aspects of this examination will be the nature of its products and services as well as its relationships with its customers and other stakeholders. Gaps or anticipated future gaps will have to be actioned. New Products and Services and the desire to maintain or improve the health of its relationships with its stakeholders will require a periodic examination of its capabilities since realizing the desired future state will not happen just by declaring a vision of it. The required capabilities of the organization fall into many types, ranging from very physical to highly intellectual. All of these capabilities will be potentially affected in order for the ‘Organization in Focus’ to attain its strategic intent. Programs and Projects are the typical vehicles that realign these capabilities with the purpose of the organization. Any breaks in this alignment will mean that whatever changes are being worked on will not be traceable to strategic intent and later business performance will suffer.	business process;closing (morphology);traceability	Markus Voss	2006		10.1007/11763864_28	computer science;technology management;component-based software engineering;programming language	HCI	-79.92023848144879	6.9789687516816	3343
1ecbedce04d5f8e74d22f8416d24a62f396f6011	performance extrapolation of io intensive workloads: work in progress	extrapolation;io traces;performance prediction	Performance prediction of an application before migrating from a source system and deploying on the target system is a challenging but important task. In this paper, we present a method for predicting the performance of an IO intensive multithreaded enterprise application workload on target systems connected to advanced storage devices. Our approach is an extension of well-known trace and replay method. We extract traces of IO intensive enterprise workloads representing temporal and spatial characteristics (e.g. read and write requests) on the source system where application is currently deployed. These traces are replayed on the system of interest called target system. The experimental results presented demonstrate the effectiveness and accuracy of this method.	enterprise software;extrapolation;multithreading (computer architecture);performance prediction;thread (computing);tracing (software)	Dheeraj Chahal;Rupinder Virk;Manoj K. Nambiar	2016		10.1145/2851553.2858665	parallel computing;real-time computing;computer science;operating system;extrapolation	HPC	-23.50324930176043	56.6473406621157	3344
a4cefefc8aa50b722fa8bffba6c08ac5d0b2569d	realizing the parallelism of array-based computation	simd architecture;implementation of array operations;relationship of communication and parallelism;array-based computation	Abstract   ‘Array-based’ computation is a style of programming in which most of the work is done by manipulating arrays as units with operations that are conceptually parallel. There are numerous problems that lend themselves to this style of programming. Array-based computation has held out the promise of highly parallel computation, but it is only with the advent of massively parallel architectures, that this promise can be fulfilled.  In this paper we describe algorithms for the implementation of a number of archetypical array operations on fine-grained SIMD architectures. The results of this work promise radical speedup of array-based computation. Our algorithms shed light on the relationship of communications and parallelism in both array-based computation and SIMD machines.	computation;parallel computing	Carl McCrosky	1989	Parallel Computing	10.1016/0167-8191(89)90075-6	parallel computing;computer science;theoretical computer science;distributed computing	HPC	-13.05114639006949	39.923039189799276	3352
99a253c226f6d95c89d48ba6b5f34fa9b690c81e	guest editorial ip unwired, part 2	design;standards;wireless communication;network protocols;standardization;theory;performance	"""This is the second of two special issues on """"IP Unwired"""", a sequence of articles providing an overview of evolving wireless standards and access network architectures that are driving the development of wide area support for IP mobility. These two issues also provide a snapshot of some research/pre-standards work underway on supporting mobility in future all IP-based cellular and hybrid access networks."""	access network;mobile ip;next-generation network;proxy mobile ipv6;snapshot (computer storage)	M. Scott Corson	2000	Mobile Computing and Communications Review	10.1145/380516.380532	computer science;computer network;distributed computing	Mobile	-16.198191013932174	88.88319648032721	3355
59de6eed1c3c1753663ba7d50a129ca4a04137a6	a language for multi-dimensional updates	dynamic logic;multi dimensional;logic programs	Dynamic Logic Programming (DLP) was introduced to deal with knowledge about changing worlds, by assigning semantics to sequences of generalized logic programs, each of which represents a state of the world. These states permit the representation, not only of time, but also of specificity, strength of updating instance, hierarchical position of the knowledge source, etc. Subsequently, the Language of Updates LUPS was introduced to allow for the association, with each state, of a set of transition rules. It thereby provides for an interleaving sequence of states and transition rules within an integrated declarative framework. DLP (and LUPS), because defined only for a linear sequence of states, cannot deal simultaneously with more than a single dimension (e.g. time, hierarchies,...). To overcome this limitation, Multi-dimensional Dynamic Logic Programming (MDLP) was therefore introduced, so as to make it possible to organize states into arbitrary acyclic digraphs (DAGs). In this paper we now extend LUPS, setting forth a Language for Multidimensional Updates (MLUPS). MLUPS admits the specification of flexible evolutions of such DAG organized logic programs, by allowing not just the specification of the logic programs representing each state, but to the evolution of the DAG topology itself as well.	digital light processing;directed acyclic graph;forward error correction;logic programming;production (computer science);sensitivity and specificity	João Leite;José Júlio Alferes;Luís Moniz Pereira;Halina Przymusinska;Teodor C. Przymusinski	2002		10.1016/S1571-0661(04)80586-3	dynamic logic;description logic;computer science;artificial intelligence;theoretical computer science;mathematics;programming language;logic programming;multimodal logic;algorithm	AI	-17.22485337023356	9.248651717157479	3356
1671ade453da28fd77c2acec112961076016446c	hysat: an efficient proof engine for bounded model checking of hybrid systems	verification;satisfiability;infinite state systems;bounded model checking;decision procedure;hybrid system;linear program;decision procedures;sat solver;pseudo boolean;hybrid systems	In this paper we present HySAT, a bounded model checker for linear hybrid systems, incorporating a tight integration of a DPLL–based pseudo–Boolean SAT solver and a linear programming routine as core engine. In contrast to related tools like MathSAT, ICS, or CVC, our tool exploits the various optimizations that arise naturally in the bounded model checking context, e.g. isomorphic replication of learned conflict clauses or tailored decision strategies, and extends them to the hybrid domain. We demonstrate that those optimizations are crucial to the performance of the tool.	amortized analysis;apache axis;automata theory;automated theorem proving;boolean satisfiability problem;causality;computation;computational complexity theory;conjunctive normal form;constraint satisfaction problem;dpll algorithm;design rationale;donald becker;duration calculus;erika enterprise;experiment;goto;heuristic (computer science);hybrid automaton;hybrid system;integrated circuit;intelligent platform management interface;internet information services;lazy evaluation;linear programming;mathematical optimization;model checking;nonlinear system;programming paradigm;proof assistant;reachability;solver;temporal logic	Martin Fränzle;Christian Herde	2007	Formal Methods in System Design	10.1007/s10703-006-0031-0	computer science;linear programming;theoretical computer science;programming language;algorithm;hybrid system	Logic	-15.734235670261134	14.450861739773242	3357
b7db11fc24418c16c6f187b55d6bb550611ed962	discovering (frequent) constant conditional functional dependencies	data dependencies;association rules;databases theory;data mining;cfds;data cleaning;conditional functional dependencies;database theory	Conditional functional dependencies (CFDs) have been recently introduced in the context of data cleaning. They can be seen as an unification of functional dependencies (FDs) and association rules (AR) since they allow to mix attributes and attribute/values in dependencies. In this paper, we introduce our first results on constant CFD inference. Not surprisingly, data mining techniques developed for functional dependencies and association rules can be reused for constant CFD mining. We focus on two types of techniques inherited from FD inference: the first one extends the notion of agree sets and the second one extends the notion of non-redundant sets, closure and quasi-closure. We have implemented the latter technique on which experiments have been carried out showing both the feasibility and the scalability of our proposition.	ambiguous name resolution;association rule learning;conjunctive query;data mining;directed acyclic graph;experiment;functional dependency;plasma cleaning;real life;regular expression;scalability;synthetic intelligence;unification (computer science);word lists by frequency	Thierno Diallo;Noel Novelli;Jean-Marc Petit	2012	IJDMMM	10.1504/IJDMMM.2012.048104	database theory;association rule learning;armstrong's axioms;dependency theory;computer science;contract for difference;machine learning;data mining;database;functional dependency	DB	-24.110151512275706	6.4566908742662505	3359
6c8b0542640bafaa83b0ffb8786087a94a24a548	nesting-based relational-to-xml schema translation	xml schema;relational data;xml document	 this paper, we study the problems in this conversion. Especially, we are interested in finding XML schema (e.g., DTD, XML-Schema, RELAX) that best describes the existing relational schema. Having the XML schema that precisely describes the semantics and structures of the original relational data is important to further maintain the converted XML documents in future. We first present a straightforward relational to XML translation algorithm, called Flat Translation (FT). Since FT maps the flat... 	algorithm;database normalization;relational model;xml schema	Dongwon Lee;Murali Mani;Frank Chiu;Wesley W. Chu	2001			database;streaming xml;document structure description;computer science;relax ng;semi-structured model;xml schema;xml validation;xml schema editor;efficient xml interchange	DB	-30.085654594917976	10.170387240672357	3361
fce75fbee5d6efcce0df7e595872352fc7fcece6	ws-policy for service monitoring	besoin de l utilisateur;inf;exigencia no funcional;surveillance;securite informatique;exigence usager;exigencia usuario;service web;necesidad usuario;web service;exigence non fonctionnelle;non functional requirement;qualite service;web service composition;computer security;vigilancia;col;monitoring;user need;user requirement;seguridad informatica;user requirements;monitorage;information system;monitoreo;systeme information;service quality;servicio web;calidad servicio;sistema informacion	The paper presents a monitoring framework for WS-BPEL proce sses. It proposes WS-Policy and WS-CoL (Web Service Constraint La gu ge) as domain-independent languages for specifying the user requir ments (constraints) on the execution of Web services compositions. These langua ges provide a uniform framework to accommodate both functional and non-func tio al constraints, but the paper only addresses non-functional requirements. It concentrates on security, which is one of the most challenging QoS dimensions f or this class of applications.	assertion (software development);business process execution language;experiment;functional requirement;non-functional requirement;quality of service;ruleml;ws-federation;ws-policy;web services description language;web service	Luciano Baresi;Sam Guinea;Pierluigi Plebani	2005		10.1007/11607380_7	computer science;service delivery framework;user requirements document;database;world wide web;computer security	SE	-41.29842339794012	24.373119004441115	3380
15705715d62fafa3bdf28960524358c795d46005	o efeito enquadramento nas decisões sobre a divulgação de informações pessoais: um estudo experimental no âmbito dos aplicativos móveis	doctoralthesis;aplicacoes para a web;direito a privacidade;dispositivos moveis;tomada de decisoes;seguranca da informacao;administracao de empresas		unified model	Sady Darcy da Silva Junior	2015				Logic	-106.15073378408364	18.885633811057126	3389
284670cb38a758f33cc3108c9e59877a690d8075	policy-based access control for constrained healthcare resources		In this paper, we propose an access control architecture for constrained healthcare resources in the IoT. Our policy-based approach provides fine-grained access for authorised users to services while protecting valuable resources from unauthorised access. We use a hybrid approach by employing attributes, roles and capabilities for our authorisation design. We apply attributes for role membership assignment and in permission evaluation. Membership of roles grants capabilities. The capabilities which are issued may be parameterised based on further attributes of the user and are then used to access specific services provided by IoT devices. This significantly reduces the number of policies required for specifying access control settings. The proposed scheme is XACML driven. Our approach requires very little additional overhead when compared to other proposals employing capabilities for access control in the IoT. We have implemented a proof of concept prototype and provide a performance evaluation of the implementation.	access control;authorization;overhead (computing);performance evaluation;prototype;xacml	Shantanu Pal;Michael Hitchens;Vijay Varadharajan;Tahiry M. Rabehaja	2018	"""2018 IEEE 19th International Symposium on """"A World of Wireless, Mobile and Multimedia Networks"""" (WoWMoM)"""	10.1109/WoWMoM.2018.8449813	computer network;access control;proof of concept;architecture;authorization;cryptography;xacml;internet of things;distributed computing;permission;computer science	Arch	-43.15573579138125	66.03998160094403	3391
6c61ef8f8b7248fb5846b39ea81d0af1fe9e5a26	cosmology and computers: haccing the universe	computers;uncertainty;biological system modeling;physics;computational modeling;parallel architectures;computational modeling physics biological system modeling computer science parallel architectures computers uncertainty;computer science;cosmology astronomy computing;computer science statistical uncertainties cosmological modeling hacc framework hardware hybrid accelerated cosmology code framework portable particle based simulation model computational analysis algorithmic analysis physics analysis in situ analysis resilience features	Summary form only given. Deep and wide surveys of the sky have led to a remarkable set of discoveries in cosmology. As the survey volumes become so large that statistical uncertainties almost disappear, cosmological modeling must reach unprecedented levels of scale and accuracy to properly interpret observational results. I will describe the key scientific problems and issues involved and then present the HACC (Hardware/Hybrid Accelerated Cosmology Code) framework, designed around a portable particle-based simulation model for the required, very high dynamic range applications. I will briefly cover the key features of HACC and plans for its future development, focusing on computational, algorithmic, and physics advances, in-situ analysis, and resilience features, while emphasizing the associated computer science needs.	algorithm;computer science;high dynamic range;simulation	Salman Habib	2015	2015 International Conference on Parallel Architecture and Compilation (PACT)	10.1109/PACT.2015.50	simulation;uncertainty;computer science;computational model	Robotics	-5.913150371175945	35.49764385227247	3397
e8f319f6afdf488ec4e0a3005d45006a1e46e4eb	connectivity map: prädiktion von konnektivität in zellulärer fahrzeugkommunikation				Tobias Pögel	2014				HCI	-95.37898306374603	21.52057640480032	3424
8b5476219f28d117437290937429aac8a0339cb8	clean your variable code with featureide		FeatureIDE is an open-source framework to model, develop, and analyze feature-oriented software product lines. It is mainly developed in a cooperation between TU Braunschweig, University of Magdeburg, and Metop GmbH. Nevertheless, many other institutions contributed to it in the past decade. Goal of this tutorial is to illustrate how FeatureIDE can be used to clean variable code, whereas we will focus on dependencies in feature models and on variability implemented with preprocessors. The hands-on tutorial will be highly interactive and is devoted to practitioners facing problems with variability, lecturers teaching product lines, and researchers who want to save resources in building product-line tools based on the FeatureIDE infrastructure.	hands-on computing;heart rate variability;magdeburg;open-source software;software product line;spatial variability	Thomas Thüm;Sebastian Krieter;Thomas Leich	2018		10.1145/3233027.3233053	eclipse;simulation;computer science;systems engineering;engineering;operating system;software engineering;antenna;programming language;configuration	SE	-63.60720411064484	23.616436900662325	3436
ae4cb077b5b105b479bc7132d4bc035b5256ee8f	a model-checking approach to schedulability analysis of global multiprocessor scheduling with fixed offsets	multiprocessor scheduling;schedulability analysis;model checking;real time scheduling	It is an active research topic to determine schedulability of a real-time sporadic or periodic taskset on a multicore processor with global scheduling policies such as global fixed-priority (FP) or earliest deadline first (EDF) algorithms. Analytical techniques such as utilisation bound tests and response time analysis algorithms generally suffer from excessive pessimism, and may cause low system utilisation. In this paper, we apply model-checking to address the restricted task model of periodic tasks with fixed release offsets and possible release jitter. We believe that this restricted task model is more realistic for current industry practice than the more general sporadic task model, since it can achieve higher CPU utilisation and better predicability. We present an approach to schedulability analysis of this restricted task model using the timed automata model-checker UPPAAL. This modelling framework is flexible and expressive, and can achieve reasonable scalability.	algorithm;automata theory;central processing unit;earliest deadline first scheduling;model checking;multi-core processor;multiprocessing;multiprocessor scheduling;real-time clock;response time (technology);scalability;scheduling (computing);scheduling analysis real-time systems;timed automaton;uppaal	Zonghua Gu;Zhu Wang;Haolan Chen;Haibin Cai	2014	IJES	10.1504/IJES.2014.063815	model checking;parallel computing;real-time computing;earliest deadline first scheduling;computer science;distributed computing;multiprocessor scheduling	Embedded	-8.811282254034003	60.106301312921076	3440
171ba2477b2728d1c0f381aa1471a9d1a14d1403	gargoyle: a network-based insider attack resilient framework for organizations		Anytime, Anywhereu0027 data access model has become a widespread IT policy in organizations making insider attacks even more complicated to model, predict and deter. Here, we propose Gargoyle, a network-based insider attack resilient framework against the most complex insider threats within a pervasive computing context. Compared to existing solutions, Gargoyle evaluates the trustworthiness of an access request context through a new set of contextual attributes called Network Context Attribute (NCA). NCAs are extracted from the network traffic and include information such as the useru0027s device capabilities, security-level, current and prior interactions with other devices, network connection status, and suspicious online activities. Retrieving such information from the useru0027s device and its integrated sensors are challenging in terms of device performance overheads, sensor costs, availability, reliability and trustworthiness. To address these issues, Gargoyle leverages the capabilities of Software-Defined Network (SDN) for both policy enforcement and implementation. In fact, Gargoyleu0027s SDN App can interact with the network controller to create a u0027defence-in-depthu0027 protection system. For instance, Gargoyle can automatically quarantine a suspicious data requestor in the enterprise network for further investigation or filter out an access request before engaging a data provider. Finally, instead of employing simplistic binary rules in access authorizations, Gargoyle incorporates Function-based Access Control (FBAC) and supports the customization of access policies into a set of functions (e.g., disabling copy, allowing print) depending on the perceived trustworthiness of the context. Our extensive evaluation results prove the practicality of Gargoyle with better performance metrics compared to existing solutions.	access control;anomaly detection;anytime algorithm;control system;data access;interaction;machine learning;network interface controller;network traffic control;next-generation secure computing base;programmed data processor;sensor;software deployment;software-defined networking;traffic analysis;trust (emotion);ubiquitous computing	Arash Shaghaghi;Salil S. Kanhere;Mohamed Ali Kâafar;Elisa Bertino;Sanjay Jha	2018	CoRR		access control;network interface controller;computer network;personalization;enterprise private network;gargoyle;computer science;computer security;ubiquitous computing;data access;insider	Security	-56.21368725054212	66.49717858218521	3441
eac7f5d72c4e4e2b18fec627671b9a05fe5e55b0	iot-basierte geschäftsmodelle für den schweizer mittelstand – konzepte für die digitale zukunft		Der Schweizer Mittelstand scheint für die Zukunft allgemein noch gut aufgestellt. Schaut man sich aber aktuelle Trends wie das Internet der Dinge (IoT) an, zeichnet sich ein sehr viel verhalteneres Bild der Innovationsfreude. Ein noch immer allgemein tiefer Digitalisierungsgrad, verbunden mit der Komplexität der Thematik und anderen, allumfassenden Stossrichtungen (z. B. die allseits proklamierte „Industrie 4.0“) paralysieren den Mittelstand durch Unsicherheit und Überforderung. Anders die in dieser Fallstudie vorgestellte Firma Walter Meier AG, die in Zusammenarbeit mit der Swisscom einen Weg der Co-Creation gegangen ist und ihr Geschäftsmodell durch IoT grundlegend und nachhaltig transformiert hat. Die Einsichten aus dieser Fallstudie können anderen Unternehmen einen Weg aufzeigen wie sie den Anschluss nicht verpassen und rechtzeitig wohlüberlegte IoT-Projekte mit handhabbaren Risiken durchführen können. Wichtige Voraussetzung sind dabei (1) die Kooperation mit Technologiepartnern und (2) die Verfügbarkeit einfacher, vertikal integrierbarer Plattformen, die von Anfang an feldtauglich sind, damit Unternehmen erfolgreich Ihre Innovation in Form sogenannter Minimum-Valuable-Products (MVPs) auf den Markt bringen können. Switzerland’s small and medium sized enterprises seem to be well prepared for the future on the first glance. However, looking into current trends like internet of things (IoT) a differentiated picture emerges. A generally low degree of digitalization, combined with the complexity and proclamation of all-embracing approaches, such as “Industry 4.0” (a fancy term for factories of the future, touted everywhere across Europe) aids to a paralysis of the companies instead of progressive innovation. Henceforth, this paper reports on one of the rarer cases, where an IoT innovation project conducted by an SME (Walter Meier AG) in co-creation and cooperation with a technology provider (Swisscom AG) transformed their business model successfully and sustainable. The insights from this case study can help other companies to engage in new opportunities by conducting IoT projects timely and with manageable risks. We identify (1) a strong cooperation with a technology provider in the role of a business partner and (2) the availability of vertically integrated platforms that allow companies to rapidly prototype and create minimum viable products as key prerequisites to successful IoT innovation.	acquired immunodeficiency syndrome;antigens;behavior;david h. koch institute for integrative cancer research at mit;die (integrated circuit);endometrial intraepithelial neoplasia;feeling/behaving old/senile;industry 4.0;institut für dokumentologie und editorik;internet explorer;internet of things;osteoarthritis, spine;prototype;rutstroemia firma;sie (file format);silver;switzerland;van der woude syndrome	Axel Uhl;Peter Heinrich;Ralf Günthner	2018	HMD Praxis der Wirtschaftsinformatik	10.1365/s40702-018-0395-9		OS	-94.84640500950742	27.538114592507835	3462
1e6e3d48626bf7f6bc41b203ef843e6537d1b167	automata-based confidentiality monitoring	static checking;language based security;computer and information science;confidentiality;automata;non interference;information flow;monitoring;data och informationsvetenskap;type system	Non-interference is typically used as a baseline security policy to formalize confidentiality of secret information manipulated by a program. In contrast to static checking of non-interference, this paper considers dynamic, automaton-based, monitoring of information flow for a single execution of a sequential program. The monitoring mechanism is based on a combination of dynamic and static analyses. During program execution, abstractions of program events are sent to the automaton, which uses the abstractions to track information flows and to control the execution by forbidding or editing dangerous actions. The mechanism proposed is proved to be sound, to preserve executions of well-typed programs (in the security type system of Volpano, Smith and Irvine), and to preserve some safe executions of ill-typed programs.	automaton;baseline (configuration management);confidentiality;interference (communication);non-interference (security);security type system;static program analysis	Gurvan Le Guernic;Anindya Banerjee;Thomas P. Jensen;David A. Schmidt	2006		10.1007/978-3-540-77505-8_7	real-time computing;computer science;database;computer security	PL	-54.4468579096912	53.016464963400004	3463
724aae5f9aeb6054fcbbf930cdd7aa2fdb054676	cloud robotics: a framework towards cloud-enabled multi-robotics survivability	srm;r2c;cloud;drivability;multi robotics;disconnection;survivability;r2r	"""The emergence of cloud computing has transformed the potential of robotics by enabling multi-robotic teams to fulfil complex tasks in the cloud. This paradigm is known as """"cloud robotics"""" and relieves robots from hardware and software limitations as large amounts of available resources and parallel computing capabilities are available in the cloud. A major challenge however, currently faced in cloud robotics is the inherent problem of cloud disconnection which would result in a robot not being able to fulfil certain tasks. This work serves to assist with the challenge of disconnection in cloud robotics by proposing a survivable cloud multi-robotics (SCMR) framework for heterogeneous environments. The SCMR framework leverages the combination of a virtual ad hoc network formed by the robot-to-robot communication and a physical cloud infrastructure formed by the robot-to-cloud communications. The Quality of Service (QoS) on the SCMR framework is tested and validated by determining the optimal energy utilization and Time of Response (ToR) on drivability analysis with and without cloud connection. The experimental results demonstrate that the proposed framework is feasible for current multi-robotic applications and shows the survivability aspect of the framework in instances of cloud disconnection."""	cloud communications;cloud computing;cloud robotics;emergence;hoc (programming language);parallel computing;programming paradigm;quality of service;robot	Vikash Ramharuk;Isaac Osunmakinde	2014		10.1145/2664591.2664602	simulation;cloud computing;engineering;cloud testing;distributed computing;computer security	HPC	-26.965915776218424	63.29278508563085	3467
9e2de8f6e79b9f8a99557492227ba8339ebc38b7	the datagrid workload management system: challenges and results	grid scheduling;management system;resource manager;resource reservation;distributed scheduling;design and implementation;data access;distributed resource management	The workload management task of the DataGrid project was mandated to define and implement a suitable architecture for distributed scheduling and resource management in a Grid environment. The result was the design and implementation of a Grid Workload Management System, a super-scheduler with the distinguishing property of being able to take data access requirements into account when scheduling jobs to the available Grid resources. Many novel issues in various fields were faced such as resource management, resource reservation and co-allocation, Grid accounting. In this paper, the architecture and the functionality provided by the DataGrid Workload Management System are presented.	data access;disk space;disk staging;distributed computing;egi;job stream;management system;mathematical optimization;middleware;requirement;scalability;scheduling (computing);service-oriented architecture;software system;testbed;glite	Giuseppe Avellino;Stefano Beco;Barbara Cantalupo;Alessandro Maraschini;Fabrizio Pacini;Massimo Sottilaro;Annalisa Terracina;Dave Colling;Francesco Giacomini;Elisabetta Ronchieri;Alessio Gianelle;Mirco Mazzucato;R. Peluso;Massimo Sgaravatto;Andrea Guarise;Rosario M. Piro;Albert Werbrouck	2004	Journal of Grid Computing	10.1007/s10723-005-0150-7	data access;real-time computing;resource allocation;resource management;management system;database;distributed computing;management;drmaa	HPC	-30.335336916935372	53.15332088871592	3474
9735c75e711c9c53c48c00b2c7f3480808882b0d	adjacency-based data reordering algorithm for acceleration of finite element computations	high performance computing;per-core performance;l3 cache;finite element computation;adjacency-based data reordering algorithm;mesh vertex;craypat hardware performance tool;overall cache utilization;current ajacency-based data reordering;part level mesh topological;data reordering;unstructured mesh;finite element	Effective use of the processor memory hierarchy is an important issue in high performance computing. In this work, a part level mesh topological traversal algorithm is used to define a reordering of both mesh vertices and regions that increases the spatial locality of data and improves overall cache utilization during on processor finite element calculations. Examples based on adaptively created unstructured meshes are considered to demonstrate the effectiveness of the procedure in cases where the load per processing core is varied but balanced (e.g., elements are equally distributed across cores for a given partition). In one example, the effect of the current ajacency-based data reordering is studied for different phases of an implicit analysis including element-data blocking, element-level computations, sparse-matrix filling and equation solution. These results are compared to a case where reordering is applied to mesh vertices only. The computations are performed on various supercomputers including IBM Blue Gene (BG/L and BG/P), Cray XT (XT3 and XT5) and Sun Constellation Cluster. It is observed that reordering improves the per-core performance by up to 24% on Blue Gene/L and up to 40% on Cray XT5. The CrayPat hardware performance tool is used to measure the number of cache misses across each level of the memory hierarchy. It is determined that the measured decrease in L1, L2 and L3 cache misses when data reordering is used, closely accounts for the observed decrease in the overall execution time.	block (data storage);blocking (computing);blue gene;cpu cache;computation;cooley–tukey fft algorithm;cray xt5;finite element method;ibm personal computer xt;job control (unix);locality of reference;memory hierarchy;principle of locality;run time (program lifecycle phase);sparse matrix;supercomputer;unstructured grid;vertex (geometry);vertex (graph theory)	Min Zhou;Onkar Sahni;Mark S. Shephard;Christopher D. Carothers;Kenneth E. Jansen	2010	Scientific Programming	10.3233/SPR-2010-0301	parallel computing;computer science;theoretical computer science;finite element method;distributed computing	HPC	-5.627965742788162	40.78357039916096	3479
51f61e6b5f16d62be3ac484f7f35ac983c23b5a6	strukturierte verankerte diskussion als form kooperativen lernens mit electures	workshop			Tobias Lauer;Stephan Trahasch	2005				HCI	-95.1278075756254	23.85477327381142	3487
399819e2a3ecf1ae2bfae7714a92c0b90a353c83	architecture and schemes for intelligent mobility management in future mobile telecommunication systems	terrestrial mobile network architecture;hierarchical structure;umts;telecommunication control;cascading style sheets;mobile telecommunication systems;location update;signalling load;hlr;location agent;location updating traffic;paging communication;database control;mobility management signalling traffic;computer architecture;telecommunication traffic;3g mobile communication;hierarchical cell structure;mobile telecommunication;location databases;mobility management;distributed databases;intelligent mobility management;telecommunication control microcellular radio telecommunication network management telecommunication signalling telecommunication traffic paging communication intelligent networks;microcellular radio;macro cell based approach;intelligent networks;mobile radio mobility management cascading style sheets intelligent networks 3g mobile communication gsm computer architecture mobile computing telecommunication control distributed databases radio access networks;telecommunication signalling;gsm;mobile computing;signalling transfer point;swap technique;terrestrial mobile telecommunication networks;macro micro cell based paging control;mobile network;mobile radio mobility management;location databases intelligent mobility management mobile telecommunication systems terrestrial mobile network architecture terrestrial mobile telecommunication networks umts hierarchical cell structure database control macro cell based approach macro micro cell based paging control signalling load location updating traffic location agent signalling transfer point mobility management signalling traffic hlr swap technique;telecommunication network management;radio access networks	In this paper, we present a novel terrestrial mobile network architecture that aims at the simplification of mobility management for the terrestrial mobile telecommunication networks. The work is in line with the UMTS approach. The proposed architecture uses the hierarchical cell concept and facilitates strategic placement of database control. A macro-cell based approach for location updating and a combined macro/micro cell based paging control plays a key role in our proposal. Another significant feature of the proposed scheme is that, each micro-cell has the same signalling load, regardless of the location updating traffic. A Location Agent (LAg) that is co-located with Signalling Transfer Point (STP) and hierarchically structured is also proposed to reduce mobility management signalling traffic to and from the HLR. A swap technic is applied at the location databases, which further reduces the signalling traffic. The simplifications achieved through the use of this architecture benefit both location updating and paging procedures by optimising the traffic in both cases.	cell (microprocessor);compare-and-swap;database;hidden line removal;network architecture;paging;terrestrial television;text simplification	Kuo-Hsing Chiang;Nirmala Shenoy	2000		10.1109/GLOCOM.2000.891883	gsm;embedded system;cellular network;intelligent network;telecommunications;computer science;operating system;cascading style sheets;mobile computing;hierarchical cell structure;umts frequency bands;computer network	Mobile	-12.944950235285503	89.86754403732674	3489
5fff47c921faedaf56f3a0bc2c8be7e088019d02	domain modeling for the semantic web: assessing the pragmatics of ontologies		The Semantic Web requires the ability to express information in a precise-interpretable form so that software sharing data can also gain an understanding of the meaning of the terms describing the data. Referred to as the third component of the Semantic Web, ontologies are the means by which separate web components can share a common language and communicate in order to work together efficiently. However, there is a lack of understanding of ontology quality, specifically as it relates to selecting or creating an ontology for a Semantic Web application. Quality assessment systems are needed which include a way to assess the pragmatics, or usefulness, of a domain ontology for its intended purpose. This research analyzes the pragmatics of domain ontologies with respect to consistency, coverage, and usability to derive a set of evaluation metrics, which are represented in a framework. An empirical evaluation illustrates the usefulness of the metrics.	brian;domain of discourse;olga (technology);ontology (information science);semantic web;usability;web components;web application	Melinda McDaniel;Veda C. Storey	2017			domain model;natural language processing;pragmatics;semantic web;ontology (information science);artificial intelligence;computer science	Web+IR	-45.25285518452593	7.121123056664956	3490
8142ab8ed3fdbccee8c4ab9857143929f749b597	revisiting semantics for epistemic extensions of description logics		Epistemic extensions of description logics (DLs) have been introduced several years ago in order to enhance expressivity and querying capabilities of these logics by knowledge base introspection. We argue that unintended effects occur when imposing the semantics traditionally employed on the very expressive DLs that underly the OWL 1 and OWL 2 standards. Consequently, we suggest a revised semantics that behaves more intuitively in these cases and coincides with the traditional semantics of less expressive DLs. Moreover, we introduce a way of answering epistemic queries to OWL knowledge bases by a reduction to standard OWL reasoning. We provide an implementation of our approach and present first evaluation results.	cache (computing);description logic;introspection;knowledge base;microsoft outlook for mac;preprocessor;program optimization;prototype;software deployment;unintended consequences;usability testing;web ontology language	Anees Mehdi;Sebastian Rudolph	2011			natural language processing;algorithm	AI	-22.350245792093045	8.624504591970936	3492
dfe07520b9dc0cd05995e3117fe51aa4575eec1c	an architecture for intelligent interfaces: outline of an approach to supporting operators of complex systems	intelligent interfaces;complex system	The conceptual design of a comprehensive support system for operators of complex systems is presented. Key functions within the support system architecture include information management, error monitoring, and adaptive aiding. One of the central knowledge sources underlying this functionality is an operator model that involves a combination of algorithmic and symbolic models for assessing and predicting an operator's activities, awareness, intentions, resources, and performance. Functional block diagrams are presented for the overall architecture as well as the key elements within this architecture. A variety of difficult design issues are discussed, and ongoing efforts aimed at resolving these issues are noted.	complex systems	Willian Bill Rouse;Norman D. Geddes;Renwick E. Curry	1987	Human-Computer Interaction	10.1207/s15327051hci0302_1	enterprise architecture framework;simulation;human–computer interaction;computer science;applications architecture;artificial intelligence;solution architecture;data architecture	HCI	-51.16889696636626	8.959031013159938	3497
1c9ca5a6b4035bc7b93bef19bb6f4cf20a4c361d	from software architecture analysis to service engineering: an empirical study of methodology development for enterprise soa implementation	empirical study;design process;service engineering schematic;virtual enterprises;system modeling;service orientation;system architectures;financial services;computer systems organization management of computing and information systems system architectures system integration system modeling;methodology development;decision support systems business service oriented architecture computer architecture software architecture conferences computational modeling;computer architecture;software architecture;computational modeling;business it alignment;software architecture analysis;enterprise soa implementation;system integration;management of computing and information systems;business;decision support systems;virtual enterprises software architecture;business it alignment software architecture service engineering schematic methodology development enterprise soa implementation service oriented enterprise system bitam soa framework;bitam soa framework;service oriented enterprise system;enterprise system;information system;system architecture;service oriented architecture;action research;service engineering;conferences;computer systems organization;integrated services	This paper presents an integrated service-oriented enterprise system development framework (called the BITAM-SOA Framework) as well as an instantiated design process model (called the Service Engineering Schematic) that was a result from a three-year action research case study with a Fortune 50 company in the financial services industry. The BITAM-SOA Framework and Schematic advance are both business-IT alignment and software architecture analysis techniques supporting the engineering of enterprise-wide service-oriented systems-that is, service engineering.	align (company);architecture tradeoff analysis method;business process;complexity theory and organizations;embedded system;enterprise resource planning;enterprise system;iteration;oracle soa suite;orchestration (computing);positive feedback;process modeling;programming paradigm;relevance;schematic;service-oriented architecture;service-oriented device architecture;service-oriented infrastructure;software architecture;top-down and bottom-up design	Hong-Mei Chen;Rick Kazman;Opal Perry	2010	IEEE Transactions on Services Computing	10.1109/TSC.2010.21	enterprise architecture framework;functional software architecture;software architecture;the open group architecture framework;architecture tradeoff analysis method;enterprise system;systems modeling;enterprise systems engineering;design process;decision support system;financial services;computer science;knowledge management;service-oriented architecture;action research;database;service;integrated services;enterprise integration;view model;empirical research;computational model;information system;systems architecture;system integration;enterprise life cycle	SE	-57.765297131640686	16.935524180103453	3500
fa200442270e87aca662ac9ac13475fd15c8731a	a string comparison approach to process logic differences between business process models	business process model;gap analysis;process logic;process benchmarking;string coding;string comparison	Analyzing process logic differences between company’s process and best practice process can assist project team in discovering process improvement opportunities. Process logic comparison has been mentioned in some literature, but research in this area still lacks effectively and efficiently analytical approach. This research has applied string coding and comparison to analyze the process logic differences between business process models. There are three portions in this approach. First of all, all process paths embedded in each compared process model are identified and coded into alphabetical process path strings. Then, all process path pairs necessary for further processing difference analysis are decided. Finally, process path string comparison is applied to each process path pair to identify the differences in processing sequence and processing mode.	best practice;business process;comparison of programming languages (string functions);embedded system;process modeling	Yeh-Chun Juan	2006		10.2991/jcis.2006.23	computer science;theoretical computer science;machine learning;engineering drawing	HPC	-70.25388864706704	19.311359963045224	3505
6d14b3bf35aacf4a379d88645954826c2615c8ff	randomness leakage in the kem/dem framework	public key encryption	Recently, there have been many studies on constructing cryptographic primitives that are secure even if some secret information leaks. In this paper, we consider the problem of constructing public-key encryption schemes that are resilient to leaking the randomness used in the encryption algorithm. In particular, we consider the case in which publickey encryption schemes are constructed from the KEM/DEM framework, and the leakage of randomness in the encryption algorithms of KEM and DEM occurs independently. For this purpose, we define a new security notion for KEM. Then we provide a generic construction of a public-key encryption scheme that is resilient to randomness leakage from any KEM scheme satisfying this security. Also we construct a KEM scheme that satisfies the security under the decisional Diffie-Hellman assumption.	algorithm;cryptographic primitive;decisional diffie–hellman assumption;diffie–hellman problem;digital elevation model;encryption;public-key cryptography;randomness;spectral leakage	Hitoshi Namiki;Keisuke Tanaka;Kenji Yasunaga	2011		10.1007/978-3-642-24316-5_22	theoretical computer science;mathematics;internet privacy;computer security;probabilistic encryption	Crypto	-39.68080233020874	75.9174535452218	3507
5eec38a67aaddd962c674e1e5601477d785b4629	apisense® : une plate-forme répartie pour la conception,le déploiement et l'exécution de campagnes de collecte de données sur des terminaux intelligents. (apisense®: a distributed platform for deploying, executing and managing data collection campaigns using smart devices)			linear algebra;smart device	Nicolas Haderer	2014				Crypto	-100.4788127668362	15.870615483839199	3510
c7a71082962fa5786949bb848a6e33e4e77349f1	introducing auto generated certificates to rank wireless home network security	wireless networks;certificate model wireless networks security standards wardriving ranking;telecommunication security certification home networks radio networks;ranking;security standards;standards wireless networks encryption authentication communication system security;wardriving;certificate model;bash scripting language autogenerated certification mechanism wireless home network security ranking security standards wireless network card ability monitoring mode service set identification ssid network name encryption type	Periodically research focusing on the level of security in home wireless networks, would reveal a considerable percentage of insecure networks. Labeling them as such, could come from either failing to utilise appropriate security standards or by using the obsolete ones. Relevant data specifying the security standards used was collected by performing an act of wardriving. The essential part of it would be to use the wireless network card's ability to work in a monitoring mode, and passively collect required information such as network name, encryption type and Service set identification (SSID). In this article a system is proposed that uses this data as the basis for a certification mechanism that has a purpose of alerting users by providing them with the insight of the level of their home security wireless network. Four important parts of this mechanism are presented with a flow based algorithm and implemented using Bash scripting language. Finally, a test case scenario and expected results are provided.	algorithm;encryption;failure;network interface controller;network security;scripting language;test case;wardriving	Marko Gaj;Nik Bessis;Lu Liu	2015	2015 10th International Conference on P2P, Parallel, Grid, Cloud and Internet Computing (3PGCIC)	10.1109/3PGCIC.2015.17	computer security model;cloud computing security;wi-fi;wireless wan;security through obscurity;security information and event management;computer science;wireless network;security service;data security;internet privacy;municipal wireless network;cracking of wireless networks;network access control;network security policy;computer security;computer network	Mobile	-54.67352319303506	69.48460031442514	3516
d0f04a8e3a21b0142f46067b98530ec833672e93	compressed accessibility map: efficient access control for xml	access control problem;accessibility map;cam size;data item;efficient access control;xml data;xml data item;tree-structured data;sophisticated access control specification;data representation integration;cam lookup algorithm;specific xml data item;access control	Publisher Summary#R##N#This chapter proposes a space- and time-efficient solution to the access control problem for eXtensible Markup Language (XML) data. The solution is based on a novel notion of a compressed accessibility map (CAM), which compactly identifies the XML data items to which a user has access, by exploiting structural locality of accessibility in tree-structured data. Transacting business over the Internet using XML is becoming more and more of a reality as one moves towards a world of Internet-worked data, with applications requiring access to data on the Internet. In this setting, there is a need for much more sophisticated types of access control than is permitted by simple firewalls. XML is widely regarded as a promising means for data representation integration, and exchange. As companies transact business over the Internet, the sensitive nature of the information mandates that access must be provided selectively, using sophisticated access control specifications. Using the specification directly to determine if a user has access to a specific XML, data item can hence be extremely inefficient.	access control;accessibility;xml	Ting Yu;Divesh Srivastava;Laks V. S. Lakshmanan;H. V. Jagadish	2002		10.1016/B978-155860869-6/50049-4	data exchange;xml validation;xml encryption;xml schema;computer science;xml framework;soap;data mining;xml database;xml schema;database;xml signature;ebxml;world wide web;xml schema editor;cxml;efficient xml interchange	DB	-34.94357398007699	9.381010399207781	3522
9317baf000ec2efb4ad5a26c5711a91cefc82957	software testing and analysis of object-oriented software	software testing;concurrent language;program visualization;object oriented software;object oriented approach;message passing;concurrent programs	Our work has studied new language mechanisms for accessing message invocations in message passing based concurrent programming languages. Invocation handling mechanisms in many concurrent languages have significant limitations that make it difficult or costly to solve common programming situations encountered in program visualization, debugging, and scheduling scenarios. We have defined and implemented new such mechanisms within the SR concurrent language and have gained some experience with them.This work has led us to want a cleaner, higher-level way of defining mechanisms for message invocation. We are, therefore, now taking an object-oriented approach. As a step toward that goal, we are currently applying our ideas to Java.Below, we briefly summarize these two areas.	concurrent computing;debugging;java;message passing;parallel computing;programming language;scheduling (computing);software testing	A. Jefferson Offutt	2000	ACM SIGSOFT Software Engineering Notes	10.1145/340855.340986	message passing;real-time computing;computer science;software engineering;distributed computing;software testing;programming language	PL	-25.61656635750551	35.799824019029494	3524
ffefef164fec49d4714f352c8a5b478fcb053a85	elkar - ré-ingénierie d'applications pour la mise en oeuvre de la coopération : méthodologie et architecture			linear algebra	Philippe Roose	2000				Crypto	-104.97303480273666	14.64929658482756	3526
aa8cfcb1cb9454a3314dc49662510521c0b995dd	specification and analysis of timing constraints for embedded systems	logic cad real time systems constraint handling timing flow graphs;timing embedded system performance analysis hardware valves delay algorithm design and analysis costs application software velocity control;concepcion asistida;computer aided design;constraint propagation;interaction analysis;implementation;performance estimation;specification;indexing terms;satisfiability;constraint satisfaction;flow graphs;embedded system;algorithme;algorithm;ejecucion;satisfaction contrainte;embedded system design;especificacion;system synthesis;synthese systeme;control flow;software component;sintesis sistema;conception assistee;timing analysis;constraint handling;hardware software implementation tradeoffs timing constraints embedded systems operation delay constraints execution rate constraints constraint propagation techniques constraint analysis techniques static analysis cosynthesis system vulcan;relative rate;satisfaccion restriccion;static analysis;logic cad;software implementation;real time systems;algoritmo;timing;time constraint	Embedded systems consist of interacting hardware and software components that must deliver a speci c functionality under constraints on relative timing of their actions. We describe operation delay and execution rate constraints that are useful in the context of embedded systems. A delay constraint bounds the operation delay or speci es any of the thirteen possible constraints between the intervals of execution of a pair of operations. A rate constraint bounds the rate of execution of an operation and may be speci ed relative to the control ow in the system functionality. We present constraint propagation and analysis techniques to determine satisfaction of imposed constraints by a given system implementation. In contrast to previous purely analytical approaches on restricted models or statistical performance estimation based on runtime data, we present a static analysis in presence of conditionals and loops with the help of designer assists. The constraint analysis algorithms presented here have been implemented in a co-synthesis system, Vulcan, that allows the embedded system designer to interactively evaluate the e ect of performance constraints on hardware-software implementation trade-o s for a given functionality. We present examples to demonstrate the application and utility of the proposed techniques.	algorithm;automated planning and scheduling;binary constraint;component-based software engineering;computation;context switch;embedded system;interaction;interactivity;local consistency;norsk data;real life;runtime system;scheduling (computing);software propagation;static program analysis;statistical machine translation;systems design;throughput;dbase	Rajesh K. Gupta;Giovanni De Micheli	1997	IEEE Trans. on CAD of Integrated Circuits and Systems	10.1109/43.594830	embedded system;mathematical optimization;electronic engineering;real-time computing;index term;constraint satisfaction;computer science;related rates;theoretical computer science;component-based software engineering;operating system;computer aided design;constraint;programming language;control flow;implementation;specification;static timing analysis;static analysis;algorithm;local consistency;satisfiability	EDA	-16.14029810415938	30.075259323078704	3529
5e8d189e9b9226041bcb69286b8ae66cf476672b	mathematical algorithms and computer communications technology	communication technology	The development of computer-based management-science scheduling models has been hampered by their increasing level of mathematical abstraction. Their use requires that the models be iteratively solved using a systematically developed sequence of supply (demand) policy scenarios, and that a laborious synthesis of the solutions for different scenarios be carried out and interpreted and modified by the analyst. It is therefore necessary to develop special schematic procedures and algorithms for representing models, producing solutions to facilitate their use. One viable approach is to couple technological advances in the multimedia and computer industries with algorithm and modelling techniques.	algorithm	Bernardo Nicoletti	1979	Computer Communications	10.1016/0140-3664(79)90181-6	information and communications technology;simulation;computer science;management science;computer security	Arch	-63.924151840295444	15.243934162561688	3535
95f347b678d9f68d08a2447ce21bd8f8a6972d81	a self-replication algorithm to flexibly match execution traces	context information;execution traces;self replication algorithms;stateful aspects	Stateful aspects react to the history of a computation. Stateful aspect developers define program execution patterns of interest to which aspects react. Various stateful aspect languages have been proposed, each with non-customizable semantics for matching a join point trace. For instance, most languages allow multiple matches of a sequence when the associated context information is different. Obtaining a different matching semantics requires ad hoc modifications of the aspects, if at all possible. In order to allow flexible customization of the matching semantics of a given aspect, this paper presents a self-replication algorithm called MatcherCells. Through the composition of simple reaction rules, MatcherCells makes it possible to express a wide range of matching semantics, per aspect. In addition, we present an initial implementation of our proposal.	algorithm;computation;execution pattern;hoc (programming language);join point;self-replication;state (computer science);stateful firewall;tracing (software)	Paul Leger;Éric Tanter	2012		10.1145/2162010.2162019	real-time computing;computer science;distributed computing;programming language	DB	-40.14706236375092	38.14887884811992	3542
d911116ea5a10a1e8f95fcc32a7fc7725521d520	uniform dos traceback	autonomous system;network security;traceback;dos;qa75 electronic computers computer science;packet marking	Denial of service (DoS) is a significant security challenge in the Internet. Identifying the attackers so that their attack traffic can be blocked at source is one strategy that can be used to mitigate DoS attacks. However, determining the source can be difficult due to the inherent connectionless nature of IP. Traceback using various marking schemes that overload, mostly unused, fields in the IP header are promising techniques to identify the source of the attack. This paper shows that the marking probability used in two existing techniques: probabilistic packet marking (PPM) and dynamic probabilistic packet marking (DPPM) are not optimal and derives an optimal marking scheme called uniform probabilistic packet marking (UPPM). The performance of UPPM is shown to be improved compared to PPM and DPPM by performing comparative numerical analysis. One significant advantage of UPPM over these earlier techniques is that it performs marking at the level of autonomous systems (ASs) rather than at every router. This has advantages both in terms of marking overhead and allowing the optimal formulation of marking probability by utilizing metrics readily available from BGP-4, the inter-AS routing protocol. a 2014 Elsevier Ltd. All rights reserved.	autonomous system (internet);connectionless communication;denial-of-service attack;ip traceback;internet;item unique identification;network packet;numerical analysis;overhead (computing);router (computing);routing	Mohammed Alenezi;Martin J. Reed	2014	Computers & Security	10.1016/j.cose.2014.04.008	telecommunications;computer science;autonomous system;network security;computer security;computer network	Security	-57.793463851035014	70.76399880443492	3543
56ac06a49b77f55bbc527156625bcbabe2789867	the automation of proof: a historical and sociological exploration	informatica;hardware designs proof automation sociological historical computer history mathematical proofs automatic theorem proving deduction interactive theorem proving artificial intelligence mathematical logic computer program verification;computer program;theorem;mathematics;mathematics computing;history;ordinateur;sociologia;logic;century 20;data processing;automatisation;intelligence artificielle;computer;theorem proving;artificial intelligent;lňgica;siecle 20;matematicas;automatic theorem proving;formal logic;theoreme;computador;invencion;artificial intelligence;teorema;hardware design;informatique;inteligencia artificial;artificial intelligence history humans hardware mathematics machine intelligence computational modeling logic design design automation laboratories;sociologie;automatisaciňn;sociology;siglo 20;invention;logique;interactive theorem proving;mathematiques;formal logic theorem proving mathematics computing history artificial intelligence;automation	"""""""DONALD MACKENZIE This article reviews the history of the use of computers to automate mathematical proofs. It identifies three broad strands of work: automatic theorem proving where the aim is to simulate human processes of deduction; automatic theorem proving where any resemblance to how humans deduce is considered to be irrelevant; and interactive theorem proving, where the proof is directly guided by a human being. The first strand has been underpinned by com~ ~itment to the goal of artificial intelligence; practitioners of the sec/ -~ ... ond strand have been drawn mainly from mathematical logic; and e third strand has been rooted primarily in the verification of computer programs and hardware 'designs.· ·"""	artificial intelligence;automated theorem proving;computer program;natural deduction;proof assistant;relevance;simulation;strand (programming language)	Donald MacKenzie	1995	IEEE Annals of the History of Computing	10.1109/85.397057	data processing;computer science;engineering;electrical engineering;artificial intelligence;database;programming language;logic;algorithm	Logic	-9.781070046346358	5.029912790300158	3549
ca69242716a5ab582700ba9daed411d402a6b752	openmp programming on intel xeon phi coprocessors: an early performance comparison	benchmarking;parallel computing;paper;vielkernarchitekturen;performance;single chip cloud computer;intel;intel phi;informatik;openmp;computer science;many core architectures;many integrated core architecture	The demand for more and more compute power is growing rapidly in many fields of research. Accelerators, like GPUs, are one way to fulfill these requirements, but they often require a laborious rewrite of the application using special programming paradigms like CUDA or OpenCL. The Intel R © Xeon Phi TM coprocessor is based on the Intel R © Many Integrated Core Architecture and can be programmed with standard techniques like OpenMP, POSIX threads, or MPI. It will provide high performance and low power consumption without the immediate need to rewrite an application. In this work, we focus on OpenMP*-style programming and evaluate the overhead of a selected subset of the language extensions for Intel Xeon Phi coprocessors as well as the overhead of some selected standardized OpenMP constructs. With the help of simple benchmarks and a sparse CG kernel as it is used in many PDE solvers we assess if the architecture can run standard applications efficiently. We apply the Roofline model to investigate the utilization of the architecture. Furthermore, we compare the performance of a Intel Xeon Phi coprocessor system with the performance reached on a large SMP production system.	cuda;coprocessor;graphics processing unit;intel core (microarchitecture);message passing interface;opencl api;openmp;overhead (computing);posix threads;production system (computer science);programming paradigm;requirement;rewrite (programming);sparse matrix;symmetric multiprocessing;xeon phi	Tim Cramer;Dirk Schmidl;Michael Klemm;Dieter an Mey	2012			machine check architecture;computer architecture;parallel computing;direct media interface;intel gma;intel high definition audio;intel 80286;single-chip cloud computer;computer science;x86;operating system;intel ipsc;intel 80386;rdrand;pentium;mmx;coprocessor;itanium	HPC	-5.620878147551696	43.414694310277554	3551
8bf5fee06df62cdb8bbefbf93a37cc15b2e8b326	mining and modeling decision workflows from dss user activity logs.		This paper introduces the concept of decision workflows, regarded as the sequence of actions of the decision maker in decision making process. We show how, based on a decision support system we previously created, we log the behaviour of the decision maker. The log is then imported into ProM framework and mined using existent process mining algorithms. The mined model will show us the control-flow perspective (which is the order of decision maker’s actions), the organisational perspective (which is the actual relationship among decision makers in group decisions), and the case perspective (what kind of support is required by each type of decisions). The aim of our research is to automate the creation of decision making patterns. Once obtained, the workflows can be merged into a financial enterprise model, which, properly validated, can become a financial reference model.	algorithm;control flow;decision support system;enterprise modelling;mined;programmable read-only memory;reference model	Razvan Petrusel	2009		10.5220/0001987701440149	data mining;database;world wide web	Web+IR	-53.739043292083785	17.44090644217596	3555
61c2c72b137c93a304d221abe673c1d23f8f6f76	proving acceptability properties of relaxed nondeterministic approximate programs	relational hoare logic;programming language;perforation;coq;program transformation;data type;satisfiability;hoare logic;acceptability;relaxed programs	Approximate program transformations such as skipping tasks [29, 30], loop perforation [21, 22, 35], reduction sampling [38], multiple selectable implementations [3, 4, 16, 38], dynamic knobs [16], synchronization elimination [20, 32], approximate function memoization [11],and approximate data types [34] produce programs that can execute at a variety of points in an underlying performance versus accuracy tradeoff space. These transformed programs have the ability to trade accuracy of their results for increased performance by dynamically and nondeterministically modifying variables that control their execution.  We call such transformed programs relaxed programs because they have been extended with additional nondeterminism to relax their semantics and enable greater flexibility in their execution.  We present language constructs for developing and specifying relaxed programs. We also present proof rules for reasoning about properties [28] which the program must satisfy to be acceptable. Our proof rules work with two kinds of acceptability properties: acceptability properties [28], which characterize desired relationships between the values of variables in the original and relaxed programs, and unary acceptability properties, which involve values only from a single (original or relaxed) program. The proof rules support a staged reasoning approach in which the majority of the reasoning effort works with the original program. Exploiting the common structure that the original and relaxed programs share, relational reasoning transfers reasoning effort from the original program to prove properties of the relaxed program.  We have formalized the dynamic semantics of our target programming language and the proof rules in Coq and verified that the proof rules are sound with respect to the dynamic semantics. Our Coq implementation enables developers to obtain fully machine-checked verifications of their relaxed programs.	approximation algorithm;computation;coq (software);global serializability;loop perforation;memoization;nondeterministic algorithm;program transformation;programming language;sampling (signal processing);unary operation	Michael Carbin;Deokhwan Kim;Sasa Misailovic;Martin C. Rinard	2012		10.1145/2254064.2254086	data type;computer science;theoretical computer science;hoare logic;programming language;algorithm;satisfiability	PL	-16.875668461322466	20.989675232292615	3557
2725a10bd2744bc7d06a8da8738de7747acdab86	parallelization of shape function generation for hierarchical tetrahedral elements	computer algebra system	  Research has gone into parallelization of the numerical aspects of computationally intense analysis and solutions. Recent  advances in computer algebra systems have opened up new opportunities for research: generating closed-form, symbolic solutions  more efficiently by parallelizing the symbolic manipulations.    	automatic parallelization;parallel computing	Sara E. McCaslin	2009		10.1007/978-90-481-9112-3_69	computational science;computer science;theoretical computer science	HCI	-7.2381686385859325	37.21339305168042	3560
2685b807889a2d993cdc6031db800033408bbad7	enabling technology for migrating legacy systems to client-server systems	systems re engineering business data processing client server systems object oriented methods software maintenance software tools;organization;object oriented methods;software maintenance;source code business rules client server systems data reallocation data redistribution discrete objects enabling technology legacy systems migration organization software maintenance software reengineering projects software tools;client server systems;software reengineering projects;enabling technology;client server;legacy systems migration;business data processing;data reallocation;discrete objects;software tools;source code;data redistribution;legacy system;business rules;systems re engineering	Increasingly, the goal of software reengineering projects is the migration of legacy systems to client server systems based around discreet objects. Although we are far from being able to fully automate this migration, tools and technology can assist an organization with the process. There are two major strategies for the reallocation and redistribution of data and functionality to a client server environment. The first involves the identification of functionality with its associated sub-subsystems. The second is the identification of business rules and the redevelopment of supporting source code. Business rule extraction is accomplished by identifying key business logic often hidden within the legacy applications, Intelligent tools use pattern matching or expert system rules to search for explicit or implicit information within the source code. The resulting information is added to a data base of business rules that is then accessed and queried during subsequent passes by the tool through the same application or additional applications. Which strategy is best usually depends on the quality of the legacy source code. Good quality code should be preserved and reused using slice technology. Poor quality code should be discarded after business rule extraction. Quality of code can be determined using complexity metrics or a maintenance history. 2 Strategies 3 Summary Identification of functionality with its associated subsystems can be accomplished using slice technology. Functional slicing can identify cohesive sub-subsystems which support previously chosen key elements (i.e. slice criteria) such as major data structures. Data name rationalization (DNR) tools can be employed to eliminate redundancy across data structures and between applications. DNR tools also help to enhance the attributes of the chosen key elements by identifying related data fields hidden due to the usage of synonyms or misspellings. These key elements, with their supporting sub-systems, serve as the seeds for proto-objects which can then be further refined by the addition of characteristics that complete their transformation to fully defined, discreet objects. An example of this refinement is the identification and development of class hierarchies and inheritance traits consistent with data abstraction (using object normalization techniques). Distribution of the resulting objects is normally based on usage frequency analysis performed on the original seed data structures. Some applications, sub-systems, or systems cannot be easily sliced and incrementally dismantled. This is normally due to either a high degree of coupling with other system elements or to a design that is highly complex and non-modular in nature. If such system elements still manage to maintain a functional cohesiveness, then wrapping (i.e. encapsulation although the Year 2000 community have slightly altered this definition) may be a better approach for migration to an object-oriented client server environment. By sealing off the application and carefully controlling its input and output, the associated functionality can be preserved while allowing the encapsulated program to be treated as a de facto object. This strategy has gained some popularity as a temporary solution to the Year 2000 date problem (known as “bridging”) and as a solution for interfacing between reengineered and non-reengineered components during incremental reengineering projects.	abstraction (software engineering);array slicing;bridging (networking);business logic;class hierarchy;client–server model;code refactoring;cohesion (computer science);coupling (computer programming);data structure;database;encapsulation (networking);expert system;frequency analysis;input/output;known-key distinguishing attack;legacy system;noise reduction;pattern matching;record sealing;redundancy (engineering);refinement (computing);rule induction;server (computing);wrapping (graphics);year 2000 problem	Michael R. Olsem	1997		10.1109/ICSM.1997.624262	computer science;systems engineering;organization;operating system;software engineering;database;software maintenance;business rule;legacy system;client–server model;source code	SE	-52.218414392285794	32.45760604675477	3563
a1371158824d3db95976e5ed4b9e25cb34b61068	adoção de tecnologias da informação em micro, pequenas e médias empresas: estudo a partir da adaptação do modelo techonology, organization and environment (toe) sob influência de fatores institucionais				Rodrigo Cesar Reis de Oliveira	2017				Crypto	-106.33114583855182	18.341374248808908	3572
b52434a3513997b92f06103569abaf34c99bdb49	convert: a high level translation definition language for data conversion (abstract).	file organization;retrieval;boolean functions;information retrieval;inverted file;attributes;multilist file;boolean queries	This paper describes in detail a translation definition language, CONVERT, for specifying the mapping of the instances of source items, which may be components of one or more files, into instances of target data, which may constitute multiple files.The language is designed for the class of users who are familiar with the logical aspects of their data, know what they want to be done, but do not want to be concerned with the details of how to accomplish it. It is high level and non-procedural according to current standards.The language provides very powerful and highly flexible restructuring capability. Although primarily designed for hierarchically structured data, it is applicable to other kinds of data structures as well. It is believed that the language can handle all common processes required in a data translation. Furthermore, the simple underlying concepts enable the users to visualize the translation processes, thus making data conversion a much simpler task. Examples to illustrate the languages' applications are included.		Nan C. Shu;Barron C. Housel;Vincent Y. Lum	1975		10.1145/500080.500096	inverted index;computer science;theoretical computer science;data mining;database;boolean function;programming language	NLP	-28.88367046097581	12.41565588196	3573
faba26337e36c0e4fc4efcf5fac8ef79138f0f32	optimization of a parallel ocean general circulation model	parallel computations;optimization;ocean modeling;vortices;computational modeling;deformation;climate change;atmospheric modeling;global climate model;message passing;hardware;parallel computer;three dimensional;ocean currents;atmosphere;ocean temperature;predictive models;radii;load balance;concurrent computing;propulsion;marine technology;life cycle	Global climate modeling is one of the grand challenges of computational science, and ocean modeling plays an important role in both understanding the current climatic conditions and predicting the future climate change. Three-dimensional time-dependent ocean general circulation models (OGCMs) require a large amount of memory and processing time to run realistic simulations. Recent advances in computing hardware have dramatically affected the prospect of studying the global climate. The significant computational resources of massively parallel supercomputers promise to make such studies feasible. In addition to using advanced hardware, designing and implementing a well-optimized parallel ocean code will significantly improve the computational performance and reduce the total research time to complete these studies.In our present work, we chose the most widely used OGCM code as our base code. This OGCM is based on the Parallel Ocean Program (POP) developed in FORTRAN 90 on the Los Alamos CM-2 Connection Machine by the Los Alamos ocean modeling research group. During the first half of 1994, the code was ported to the Cray T3D by Cray Research using SHMEM-based message passing. Since the code on the T3D was still time-consuming when large problems were encountered, improving the code performance was considered essential.We have developed several general strategies to optimize the ocean general circulation model on the Cray T3D. These strategies include memory optimization, effective use of arithmetic pipelines, and usage of optimized libraries. The optimized code runs 2 to 2.5 times faster than the original code, which gives significant performance improvements for modeling large scaled ocean flows. Many test runs for both of the original and the optimized code have been carried out on the Cray T3D using various numbers of processors (1-256). Comparisons are made for a variety of real-world problems. A nearly linear scaling performance line is obtained for the optimized code, while the speed up data of the optimized code also shows excellent improvement over the original code.In addition to discussing the optimization of the code, we also address the issue of portability. Given the short life cycle of the massively parallel computer, usually on the order of three to five years, we emphasize the portability of the ocean model and the associated optimization routines across several computing platforms. Currently, the ocean modeling code has been ported successfully to the Hewlett Packard (HP)/Convex SPP-2000, and is readily portable to Cray T3E.This paper reports our efforts to optimize the parallel implementations of the oceanic model. So far, the work has focused on improving the load balancing and single node performance of the code on the Cray T3D. As a result, the atmosphere and ocean model components running side-by-side can achieve a performance level of slightly more than 10 GFLOPS on 512 processors of that machine. We have also developed a user-friendly coupling interface with atmospheric and biogeochemical models, in order to make the global climate modeling more complete and more realistic.	ocean general circulation model;program optimization	Ping Wang;Daniel S. Katz;Yi Chao	1997		10.1109/SC.1997.10019	general circulation model;three-dimensional space;biological life cycle;atmospheric model;radius;sea surface temperature;parallel computing;message passing;simulation;propulsion;concurrent computing;prediction;computer science;load balancing;theoretical computer science;operating system;atmosphere;predictive modelling;marine technology;ocean current;climate change;computational model;vortex;deformation;climate model	NLP	-5.547757723933429	37.044223829518494	3574
51a781faf5d8a5c9d90c2cf461cc3b9d70af563c	logische und physische kopplung von rechnerverbundsystemen				Ronald Baudisch	1987				NLP	-95.79647767480195	21.862476020189003	3579
ca8bff13872fb1e3e0217dd4200327974cccf17e	a common parsing scheme for left- and right-branching languages		This paper presents some results of an attempt to develop a common parsing scheme that works systematically and realistically for typologically varied natural languages. The scheme is bottom-up, and the parser scans the input text from left to right. However, unlike the standard LR(k) parser or Tomita's extended LR(1) parser, the one presented in this paper is not a pushdown automaton based on shift-reduce transition that uses a parsing table. Instead, it uses integrated data bases containing information about phrase patterns and parse tree nodes, retrieval of which is triggered by features contained in individual entries of the lexicon. Using this information, the parser assembles a parse tree by attaching input words (and sometimes also partially assembled parse trees and tree fragments popped from the stack) to empty nodes of the specified tree frame, until the entire parse tree is completed. This scheme, which works effectively and realistically for both left-branching languages and right-branching languages, is deterministic in that it does not use backtracking or parallel processing. In this system, unlike in ATN or in LR(k), the grammatical sentences of a language are not determined by a set of rewriting rules, but by a set of patterns in conjunction with procedures and the meta rules that govern the system's operation.	ana (programming language);backtracking;bottom-up parsing;canonical lr parser;database;em (typography);execution unit;integrated development environment;lexicon;naruto shippuden: clash of ninja revolution 3;natural language;php;parallel computing;parse tree;parser combinator;pushdown automaton;rewriting;stack (abstract data type);top-down and bottom-up design	Paul T. Sato	1988	Computational Linguistics			NLP	-24.58309957259088	24.590830693252617	3585
7411f6947320e06e470071a403a0279f0bb02209	empirical evaluation of voip aggregation over a fixed wimax testbed	main focus;application-layer aggregation;voip aggregation;main gauge;fixed wimax testbed;fixed wimax;wimax reference network architecture;overall performance;empirical evaluation;comparable mos value;applicationand network-level aggregation;aggregation scheme;network architecture;wimax;mean opinion score;point to point;voip;network topology;voice over ip	The WiMAX Reference Network Architecture can be used in point-to-point and point-to-multipoint network topologies, and is suitable for providing last-mile, building-to-building, and residential broadband connectivity. Another major application, and the main focus of this study, is the use of fixed WiMAX as backhaul for voice and data services. We evaluate voice over IP (VoIP) performance over a fixed WiMAX testbed and quantify the benefits from employing applicationand network-level aggregation. We examine such aggregation schemes using our fixed WiMAX testbed and report the results for both uplink and downlink. If we use objective mean opinion scores (MOS) as the main gauge of overall performance, application-layer aggregation appears to be the best scheme, allowing our fixed WiMAX testbed to sustain nearly three times more flows in the downlink and over two times more flows in the uplink than when no aggregation is used, at comparable MOS values.	backhaul (telecommunications);last mile;multipoint ground;network architecture;network topology;point-to-multipoint communication;point-to-point (telecommunications);telecommunications link;testbed	Kostas Pentikousis;Esa Piri;Jarno Pinola;Frerk Fitzek;Tuomas Nissilä;Ilkka Harjula	2008		10.1145/1390576.1390599	real-time computing;telecommunications;computer science;voice over ip;computer network	Metrics	-10.007495613944537	87.28486024803546	3586
5d6dd5e5edd94b9d98881fe36ad96d126afe9075	policyglobe: a framework for integrating network and operating system security policies	empirical study;policy integration;network security;traffic control;access control policy;operating system;integral operator;se linux;access control;networked systems;security policy;firewall	"""In modern networked systems with many machines and traffic control devices (such as firewalls), it is difficult to determine the overall effect of the security policies and configurations implemented inside the operating system and network devices. This paper describes PolicyGlobe, a framework to integrate operating system and network security policies. Using the idea of accessibility sets, PolicyGlobe integrates the Security Enhanced Linux (SE-Linux) access control policies with firewall configurations and traffic control policies. Using this framework, it is possible to construct a global accessibility set for each process in the system. PolicyGlobe makes it possible to determine the global effect of the local security policies and firewall configurations and answer the basic questions """"can a subject in one machine access an object in another machine?"""" We have developed the integration algorithms, optimized the algorithms, implemented the entire framework, and conducted empirical studies on it. The studies show that in a network of 10 densely connected machines each with a large SE-Linux policy (~275,000 lines of rules), PolicyGlobe can build the global accessibility sets in about 10 minutes. In a system with a more limited connectivity, the analysis takes a much shorter amount of time."""	access control;accessibility;algorithm;firewall (computing);linux;network security;operating system;selinux	Hamed Okhravi;Ryan H. Kagin;David M. Nicol	2009		10.1145/1655062.1655074	application firewall;computer security model;firewall;real-time computing;computer science;security policy;access control;network security;information security standards;internet security;distributed computing;empirical research;network access control;network security policy;computer security	Security	-17.59047228580099	83.91665777707213	3589
762ebd29fbbbb301a5b1d2a7dabb360cb2f3c028	boardroom voting scheme with unconditionally secret ballots based on dc-net	anonymous broadcast;zero knowledge proof;cryptographic protocol;ballot secrecy;electronic voting	A novel electronic voting scheme is proposed which is quite suitable for small scale election settings. An outstanding characteristic of the design is its guarantee of unconditionally perfect ballot secrecy. It satisfies self-tallying, fairness and verifiability. Disruption of the result of an election equals to breaking the Discrete Logarithm Problem. Our scheme is built on top of the DC-net(dining cryptographers network) anonymous broadcast protocol. It needs no trusted authority to guarantee its security, but assumes a complete network of secure private channels between voters.	dc-to-dc converter	Long-Hai Li;Cheng-Qiang Huang;Shao-Feng Fu	2012		10.1007/978-3-642-34601-9_17	computer science;cryptographic protocol;internet privacy;computer security;zero-knowledge proof	Crypto	-41.01693105638785	74.04125970014086	3591
359ecd9ef0f97fb986d9eb4a29355af95446c23f	convergence of interoperability of cloud computing, service oriented architecture and enterprise architecture		The research identifies the gap that there is a convergence of interoperability of Cloud Computing (CC), Service Oriented Architecture (SOA) and Enterprise Architecture (EA). Furthermore, it outlines the existing non dynamic links between EA and SOA that are currently practiced in the industry and confirmed by scholarly articles; and provides a state of art of the link that could exist in practice between cloud computing and SOA as researched from the published scholarly material. This researched paper also refers to the planned research to test this theory first by developing a logical architectural model of such a feasibility followed by a Proof of Concept Convergence of Interoperability of Cloud Computing, Service Oriented Architecture and Enterprise Architecture	cloud computing;enterprise architecture;interoperability;service-oriented architecture	Susan Sutherland	2013	IJEEI	10.4018/jeei.2013010104	enterprise architecture framework;reference architecture;the open group architecture framework;space-based architecture;tafim;architecture domain;applications architecture;service-oriented modeling;enterprise architecture management;service;solution architecture;distributed computing;enterprise architecture;enterprise integration;view model;enterprise information security architecture;data architecture;computer network;business architecture	Web+IR	-58.83297765346212	17.272446586023214	3592
88c45b2d863e415a84b4dac6b6e94496e367e4a0	incorporating time in the modeling of hardware and software systems: concepts, paradigms, and paradoxes	system engineering;time dependent;value of time;software system;abstract state machine;systems engineering;software systems;hardware software systems real time systems timing systems engineering and theory physics temperature embedded system laboratories predictive models;distributed modelling;modeling language;embedded system;systems engineering and theory;physics;specification languages;systems engineering programming languages real time systems specification languages;high level model;model based development;predictive models;model merging;real time system;timed abstract state machine language hardware system software system system engineering real time system modeling language high level model;temperature;hardware system;timed abstract state machine language;programming languages;hardware;real time systems;timing	In this paper, we present some of the issues encountered when trying to apply model-driven approaches to the engineering of real-time systems. In real-time systems, quantitative values of time, as reflected through the duration of actions, are central to the system's correctness. We review basic time concepts and explain how time is handled in different modeling languages. We expose the inherent paradox of incorporating quantitative time-dependent behavior in high-level models. High-level models are typically built before the system is implemented, which makes quantitative time metrics difficult to predict since these metrics depend heavily on implementation details. We provide some possible answers to this paradox and explain how the Timed Abstract State Machine (TASM) language helps address some of these issues.	abstract state machines;correctness (computer science);high- and low-level;model-driven engineering;modeling language;real-time clock;real-time computing;software system	Martin Ouimet;Kristina Lundqvist	2007	International Workshop on Modeling in Software Engineering (MISE'07: ICSE Workshop 2007)	10.1109/MISE.2007.7	real-time computing;real-time operating system;computer science;engineering;theoretical computer science;operating system;software engineering;programming language;model-based design;software system	SE	-44.76678571288466	32.702441610845625	3596
038e21706028d8c1edeaea1ed1bef0b9bb27092d	deriving weak bisimulation congruences from reduction systems	reduccion sistema;modelizacion;distributed system;semantica operacional;systeme reparti;labelled transition system;observational equivalence;process calculi;observable;bisimulacion;system reduction;operational semantics;simultaneidad informatica;bisimulation;systeme transition etiquete;congruencia;modelisation;concurrency;semantique operationnelle;weak bisimilarity;sistema repartido;reduction systeme;relative push out;reactive systems;2 categories;modeling;simultaneite informatique;double categories;congruence;sistema transicion marcada;labelled transition systems	The focus of process calculi isnteraction rather thancomputation, and for this very reason: (i) their operational semantics is conveniently expressed by labelled transition systems (LTSs) whose labels model the possible interactions of a process with the environment; (ii) their abstract semantics is conveniently expressed by observational congruences. However, many current-day process calculi are more easily equipped with reduction semantics, where the notion of observable actionis missing. Recent techniques attempted to bridge this gap by synthesising LTSs whose labels are process contexts that enable reactions and for which bisimulation is a congruence. Starting from Sewell’s set-theoretic construction, category-theoretic techniques were defined and based on Leifer and Milner’s relative pushouts , later refined by Sassone and the fourth author to deal with structural congruences given as groupoidal 2-categories . Building on recent works concerning observational equivalences for tile logic, the paper demonstrates that double categories provide an elegant setting in which the aforementioned contributions can be studied. Moreover, the formalism allows for a straightforward and natural definition of weak observational congruence.	bisimulation;category theory;congruence of squares;interaction;observable;operational semantics;process calculus;rewriting;semantics (computer science);set theory	Roberto Bruni;Fabio Gadducci;Ugo Montanari;Pawel Sobocinski	2005		10.1007/11539452_24	discrete mathematics;systems modeling;concurrency;observable;reactive system;computer science;bisimulation;congruence;mathematics;programming language;operational semantics;algorithm	Logic	-9.031407833483525	21.974118006576273	3603
a1203e45709092c4a1df0f376602c255e4ae814a	s-aka: a provable and secure authentication key agreement protocol for umts networks	universal mobile telecommunication system;protocols;man in the middle attack;cellular radio;authentication;cryptographic protocols;and key agreement;authenticated key agreement;3g mobile communication;authentication 3g mobile communication bandwidth gsm protocols;secure authentication key agreement protocol s aka protocol umts networks universal mobile telecommunication system global system for mobile communications gsm systems man in the middle attack redirection attack secure aka protocol mobile subscriber authentication;mobile communication;telecommunication security;bandwidth;key agreement protocol;gsm;global system for mobile communication;universal mobile telecommunication system umts networks;article;universal mobile telecommunication system umts networks authentication protocol key agreement protocol;telecommunication security 3g mobile communication cellular radio cryptographic protocols;authentication protocol	The authentication and key agreement (AKA) protocol of Universal Mobile Telecommunication System (UMTS), which is proposed to solve the vulnerabilities found in Global System for Mobile Communications (GSM) systems, is still vulnerable to redirection and man-in-the-middle attacks. An adversary can mount these attacks to eavesdrop or mischarge the subscribers in the system. In this paper, we propose a secure AKA (S-AKA) protocol to cope with these problems. The S-AKA protocol can reduce bandwidth consumption and the number of messages required in authenticating mobile subscribers. We also give the formal proof of the S-AKA protocol to guarantee its robustness.	adversary (cryptography);authentication and key agreement (protocol);extensible authentication protocol;formal proof;key-agreement protocol;man-in-the-middle attack;provable security	Yu-Lun Huang;Chih-Ya Shen;Shiuh-Pyng Shieh	2011	IEEE Transactions on Vehicular Technology	10.1109/TVT.2011.2168247	man-in-the-middle attack;gsm;otway–rees protocol;communications protocol;universal composability;mobile telephony;telecommunications;computer science;authentication protocol;key-agreement protocol;authentication;cryptographic protocol;computer security;bandwidth;challenge-handshake authentication protocol;computer network	Security	-47.212930210285776	73.34731264845412	3604
f6bc4d15b0652878bcbebc2b30036e4e667acf91	(star-based) three-valued kripke-style semantics for pseudo- and weak-boolean logics		This article investigates Kripke-style semantics for two sorts of logics: pseudo-Boolean (pB) and weak-Boolean (wB) logics. As examples of the first, we introduce G3 and S5 pB 3 . G3 is the three-valued Dummett–Gödel logic; S5 3 is the modal logic S5 but with its orthonegation replaced by a pB (or Heyting) negation. Examples of wB logic are GwB 3 and S5 wB 3 . G wB 3 is G3 with a wB negation in place of its pB negation; S5 wB 3 is S5 with a wB negation replacing its orthonegation. For each system, we provide a three-valued Kripke-style semantics with and without star operation (which is like the star operation of the Routley–Meyer semantics of relevance logics). We prove soundness and completeness theorems in each case. Note that wB logics may be equivalent to logics with Baaz’s projection . We finally introduce the G3 and the S5 pB 3 both with and show that they are equivalent to G wB 3 and S5wB 3 , respectively.	boolean algebra;gödel logic;kleene star;modal logic;relevance;s5 (modal logic);soundness (interactive proof)	Eunsuk Yang	2012	Logic Journal of the IGPL	10.1093/jigpal/jzr030	monoidal t-norm logic;t-norm fuzzy logics;kripke semantics;operational semantics	Logic	-12.426081046791392	12.728048430434125	3611
6d0d4f599c7fc4041a8e3115736a5b5ffd38e80f	traffic scheduling in hybrid wdm–tdm pon with wavelength-reuse onus	colorless onu;hybrid wdm tdm pon;traffic scheduling;tdm pon	Hybrid WDM–TDM PON (wavelength division multiplexing–time division multiplexing passive optical network) that applies wavelength-independent or colorless ONU (optical network unit) technologies will further reduce implementation and maintenance expenses. The “wavelength-reuse” colorless ONU technology imposes a physical constraint in the hybrid WDM–TDM PON that the same wavelength is used for both upstream and downstream traffic transmission of an ONU. This physical constraint brings a new challenge to developing traffic scheduling algorithms in the network, as upstream traffic scheduling is no longer independent of downstream traffic scheduling and the existing traffic scheduling algorithms that treated the upstream and downstream traffic independently cannot be applied in this case. We propose a new traffic scheduling algorithm that takes both directions’ traffic scheduling into account at the same time. A logical PON concept is defined, and wavelength resource sharing is performed based on reconfiguring logical PONs. Simulation study on this algorithm is conducted, and results show that it achieves efficient wavelength and bandwidth resource sharing in the network.	algorithm;bandwidth (signal processing);downstream (software development);network interface device;passive optical network;scheduling (computing);simulation;toad data modeler;transmitter;wavelength-division multiplexing	Luying Zhou;Nicholas Heng-Loong Wong;Yong-Kee Yeo;Xiaofei Cheng;Xu Shao;Zhaowen Xu	2012	Photonic Network Communications	10.1007/s11107-012-0375-x	real-time computing;telecommunications;computer network	Metrics	-5.109052318978318	86.17464623372746	3612
9afefd9d665df49eb69649d3068cd2a5341fa53b	an approach for fault tolerant and secure data storage in collaborative work environments	key management;replication mechanisms;replication;collaborative work;secret sharing;fault tolerant;distributed data storage;collaborative work environments;performance;architectural frameworks;metrics;xor secret sharing;confidentiality;performance metric;tradeoffs;data storage;col;byzantine fault tolerance;computation overheads;secret sharing techniques for storing data;byzantine fault tolerant;technical report;sensitive data;secret sharing scheme;collaborative environments;secure and fault tolerant data storage services	We describe a novel approach for building a secure and fault tolerant data storage service in collaborative work environments, which uses perfect secret sharing schemes to store data. Perfect secret sharing schemes have found little use in managing generic data because of the high computation overheads incurred by such schemes. Our proposed approach uses a novel combination of XOR secret sharing and replication mechanisms, which drastically reduce the computation overheads and achieve speeds comparable to standard encryption schemes. The combination of secret sharing and replication manifests itself as an architectural framework, which has the attractive property that its dimension can be varied to exploit tradeoffs amongst different performance metrics. We evaluate the properties and performance of the proposed framework and show that the combination of perfect secret sharing and replication can be used to build efficient fault-tolerant and secure distributed data storage systems.	computation;computer data storage;encryption;enterprise architecture framework;exclusive or;fault tolerance;secret sharing	Arun Subbiah;Douglas M. Blough	2005		10.1145/1103780.1103793	computer science;theoretical computer science;distributed computing;secure multi-party computation;computer security	OS	-38.9415140978681	69.89188290551607	3613
3cc6b99065e13ba651eb9e3779d8f630dd7a0095	enterprise scheduling: hybrid and hierarchical issues	discrete event simulation;manufacturing systems;scheduling;discrete-continuous simulation model;enterprise scheduling;information theory;manufacturing enterprise system;system dynamics modeling	We build a hybrid discrete-continuous simulation model of the manufacturing enterprise system. This model consists of an overall system dynamics model of the manufacturing enterprise and connected to it are a number of discrete event simulations for selected operational and tactical functions. System dynamics modeling best fits the macroscopic nature of activities at the higher management levels while the discrete models best fit the microscopic nature of the operational and tactical levels. An advanced mechanism based on information theory is used for the integration of the different simulation modeling modalities. In addition, the impact of the decisions at the factory level in scheduling are analyzed at the management level. The different models of control are discussed.	curve fitting;enterprise system;fits;information theory;scheduling (computing);simulation;system dynamics	John Pastrana;Mario Marin;Magdy Helal;Carlos Mendizabal	2010	Proceedings of the 2010 Winter Simulation Conference		manufacturing execution system;synchronization;enterprise system;real-time computing;simulation;continuous simulation;information theory;computer science;discrete event simulation;integrated enterprise modeling;simulation modeling;system dynamics;computational model;scheduling	HPC	-54.50364527009824	9.773000111917243	3615
34deba8182745ccb1e0aefa1f13e8e28de45d6fc	performance evaluation of data integrity mechanisms for mobile agents	hash chain;mobile agents authentication protection data security application software software agents electronic commerce computer science data engineering computer security;internet security performance evaluation data integrity mechanisms mobile agents security challenge data protection e commerce applications software agents partial result authentication codes prac hash chaining set authentication code methods modified set authentication code method dads mobile agent system data integrity module key generation agent size security features;performance evaluation;data integrity;perforation;e commerce;software agent;mobile agents;telecommunication security cryptography internet data integrity mobile agents message authentication;generation time;indexing terms;computer security;mobile agent system;internet;cryptography;telecommunication security;message authentication;mobile agent;internet security;mobile agents computer software	A primary security challenge of the mobile agent paradigm is that of protecting the data carried by a mobile agent. With the growing popularity of e-commerce applications that use software agents, the protection of mobile agent data has become imperative. To that end, we evaluate the performance of four methods that protect the data integrity of mobile agents. While some techniques have been proposed in the literature, there has previously been no experimental study comparing the various alternatives. The set of integrity mechanisms that we investigate includes existing approaches known as the Partial Result Authentication Codes (PRACs), Hash Chaining, and Set Authentication Code methods, as well as a technique of our own design, which we refer to as the Modified Set Authentication Code method. The Modified Set Authentication Code method addresses several limitations of the Set Authentication Code method. The performance experiments were run using the DADS mobile agent system, for which we designed a Data Integrity Module. The experimental results showed that our Modified Set Authentication Code technique performed comparably to the Set Authentication Code method, showing some improvement in the key generation times and agent size. In this paper, we compare the trade-offs between security features and performance for all four methods and identify the niche that each technique could fill.	authentication;data integrity;e-commerce;experiment;hash table;imperative programming;key generation;mobile agent;niche blogging;performance evaluation;programming paradigm;software agent	Vandana Gunupudi;Stephen R. Tate	2004	International Conference on Information Technology: Coding and Computing, 2004. Proceedings. ITCC 2004.	10.1109/ITCC.2004.1286427	computer science;internet privacy;world wide web;computer security	SE	-45.94741360302547	70.92672049665491	3627
b9bbc77151cd6e1469597e3ee3c6eda32df2b8e8	modeling organizational architectural styles in uml: the tropos case	distributed application	Today’s software operate in a dynamic, organizational context and hence, it needs flexible architectures based in social and intentional concepts to enable software to evolve consistently with its operational environment. The Tropos requirements oriented development methodology, has defined a number of organizational architectural styles which are suitable to cooperative, dynamic and distributed applications. In this paper, we use the UML to describe these novel architectural styles. In doing so we are able to provide a detailed representation of both the structure and behaviour of the styles.	complex systems;cross-reference;distributed computing;loose coupling;real-time transcription;requirement;traceability;unified modeling language	Carla Schuenemann;Jaelson Brelaz de Castro	2002			architectural style;architectural pattern;applications of uml;software;unified modeling language;systems engineering;engineering	SE	-43.72709233333294	20.972377088056678	3630
88276ec8771510f2ae040fcce7f5d7f455a42d11	formal correctness of supply chain design	model verification;supply chain design;scor;supply chain management	Many companies use supply chain models for designing the flow of goods and services from their suppliers all the way up to the final customers. Over the past 15years, the Supply Chain Operations Reference Model (SCOR) has become a widespread modeling technique for designing such supply chains and sharing design information with supply chain stakeholders. However, neither the syntax nor the semantics of SCOR are well defined. This limitation has important consequences for its usage: Supply chain models may be ambiguous and their correctness cannot be verified. We address this problem by mapping SCOR supply chains onto graphs and formalize the semantics of SCOR. The mapping is driven by constructs from the supply chain management literature. The proposed artifact is a supply chain grammar, which we apply to a set of SCOR models taken from industry sources. We show the grammar's usefulness by verifying the correctness of these models using analytical techniques. We deduce a grammar of the SCOR modeling technique from SCM literature.We propose correctness properties for SCOR-based designs.We demonstrate the usefulness of the grammar for detecting errors in existing models.		Jörg Leukel;Vijayan Sugumaran	2013	Decision Support Systems	10.1016/j.dss.2013.06.008	real-time computing;supply chain management;computer science;marketing;operations management	ECom	-54.68570544389494	19.62366165263405	3631
51583994b86e053b4dc87bfbc42516f567e7a9de	research on resource scheduling of cloud based on improved particle swarm optimization algorithm	resource scheduling particle;swarm optimization;qos requirements;cloud computing	Resource of cloud computing has the characteristics of dynamic, distribution, complexity. How to have the effective scheduling according to the users' QoS (Quality of Service) demand and in order to maximize the benefits is the challenge encountered in cloud computing resource allocation. In this paper, according to the characteristics of the resources of cloud computing, considering the constraints of time and budget needs of users, we designed the scheduling model of resource based on particle swarm optimization algorithm, and used the IPSO (Improved Particle Swarm Optimization algorithm) for global search to obtain the multi-objective optimization solutions that satisfies the requirements. Experimental results show that: when the IPSO applied to the resource of cloud computing compares with other algorithms, it has faster response time and could take efficient use of resource to meet the users' QoS requirements in solving multi-objective problems.	algorithm;particle swarm optimization;scheduling (computing)	Yan Wang;Jinkuan Wang;Cuirong Wang;Xin Song	2013		10.1007/978-3-642-38786-9_14	mathematical optimization;multi-swarm optimization;real-time computing;computer science;distributed computing	EDA	-18.75292890216332	63.435341360852014	3633
262dde458140add1b89410293f45a8c02c8bf9e1	authentication schemes from actions on graphs, groups, or rings	one way function;graph coloring	We propose a general way of constructing zero-knowledge authentication schemes from actions of a semigroup on a set, without exploiting any specific algebraic properties of the set acted upon. Then we give several concrete realizations of this general idea, and in particular, we describe several zero-knowledge authentication schemes where forgery (a.k.a. impersonation) is NP-hard. Computationally hard problems that can be employed in these realizations include (Sub)graph Isomorphism, Graph Colorability, Diophantine Problem, and many others.	authentication;linear algebra;np-hardness	Dima Grigoriev;Vladimir Shpilrain	2010	Ann. Pure Appl. Logic	10.1016/j.apal.2010.09.004	combinatorics;mathematical analysis;discrete mathematics;topology;graph coloring;mathematics;one-way function;algorithm;algebra	Crypto	-39.69101612666394	79.93334072479192	3634
ef0e5d5bf7dc78e8e41fd0db3318e722d60f9b43	a virtual supply chain model for qos assessment		A hybrid virtual supply chain model for the assessment of Quality of Service (QoS) in supply chains is being presented. Due to the large complexity of the problem domain a combined – analytical and simulation – model has been recognised as the best solution for QoS evaluation within a supply chain. The main problem of building such a model is the level of detail, which on one hand is limited by the transparency of business processes of contributing partners and on the other by the desired relevancy of the gathered results. Our model builds on data that are usually made public or are at least subject to contract terms among partners in a supply chain and provide a “good-enough” foundation for QoS evaluation.	quality of service	Roman Gumzej;Brigita Gajsek	2012		10.1007/978-3-642-24806-1_12	business process;process management;business;supply chain management;supply chain;quality of service;level of detail;transparency (graphic);problem domain	ECom	-65.74051232912134	13.789492294591279	3641
828df2e31796e948b726f626cee87e37ff1593cf	smc4ac: a new symbolic model checker for intelligent agent communication		Social approaches have been put forward to define semantics for intelligent agent communication messages and to tackle the shortcomings of mental approaches. Formal semantics of those social approaches can be model checked as they are focused on public behaviors instead of private mental states. Social conditional commitments are essential concepts in social approaches that can effectively model agent communications. However, conditional commitments exclusively are not able to model agent communication actions, the cornerstone of the fundamental agent communication theory, namely speech act theory. These actions provide mechanisms for dynamic interactions and enable designers to track the evolution of active conditional commitments. From the perspective of model checking, we need to define a formal and computationally grounded semantics for relevant social actions that can directly be applied to active conditional commitments. This manuscript describes a new symbolic model checker, SMC4AC, developed and implemented to automate the verification of interaction among intelligent agents. SMC4AC is the result of developing a new symbolic model checking algorithm devoted to CTLC, a combination of CTL and new temporal modalities to represent and reason about conditional commitments ∗Address for correspondence: Concordia Institute for Information Systems Engineering, Concordia University, Canada. †Also afiliated as Concordia Institute for Information Systems Engineering, Concordia University, Canada. Received December 2015; revised November 2016. 224 W. El Kholy et al. / SMC4AC: A New Symbolic Model Checker for Intelligent Agent Communication and common commitment actions. The core of this paper consists of a new logical language, a detailed description of the symbolic algorithms needed for commitments and their action modalities, complexity analysis, implementation and application. The implementation of our algorithm and its graphical user interface is built on top of the MCMAS symbolic model checker tailored for checking intelligent multi-agent systems. We select business processes and multi-agent interaction protocols as application domains to test and validate the effectiveness and scalability of SMC4AC. We report extensive experimental results, which confirm the theoretical findings and make SMC4AC practical.	algorithm;analysis of algorithms;business process;graphical user interface;intelligent agent;interaction;mental state;model checking;multi-agent system;scalability;systems engineering	Warda El Kholy;Jamal Bentahar;Mohamed El-Menshawy;Hongyang Qu;Rachida Dssouli	2017	Fundam. Inform.	10.3233/FI-2017-1519	computer science;theoretical computer science;distributed computing;programming language;symbolic trajectory evaluation	AI	-42.76813970448099	23.08613457775765	3644
975219a15c52c66d2773352414c107c2edb0b694	the level of handshake required for managing a connection	formal model;incarnations;transport layer;tcp memory requirements;connection management;handshake;wide area network;time constraint	A connection between two hosts across a wide-area network may consist of many sessions over time, each called an incarnation. A connection is synchronized using a connection establishment protocol, based on a handshake mechanism, to allow reliable exchange of data. This paper identifies the precise level of handshake needed under different assumptions on the nodes and on the network, using a formal model for connection management. In particular, the following parameters are studied: the size of the memory at the nodes, the information retained between incarnations, and the existence of time constraints on the network. Among the results we obtain are: (1) If both nodes have bounded memory, no incarnation management protocol exists. (2) If the nodes have unbounded memory, then a two-way handshake incarnation management protocol exists. (3) If the nodes have unbounded memory, and the server does not retain connection-specific information between incarnations, then a three-way handshake incarnation management protocol exists. On the other hand, a two-way handshake incarnation management protocol does not exist, even if some global information is retained. (4) If a bound on maximum packet lifetime (MPL) is known, then a two-way handshake incarnation management protocol exists, in which the server does not retain connection-specific information between incarnations.	formal language;multiphoton lithography;network packet;server (computing)	Hagit Attiya;Rinat Rappoport	1997	Distributed Computing	10.1007/s004460050041	real-time computing;computer science;distributed computing;transport layer;computer network	Theory	-7.712366847816708	88.2277948478734	3652
98cf6d684373433d6b1f1af2d1f956bf15ad544c	scaling mobile network capacity aggressively with quickc		The global demand for mobile data is proliferating. In the last five years, the volume of data carried by mobile networks around the world has increased eighteen-fold, from 0.4 exabytes/month in 2011 to 7.2 exabytes/month in 2016 [1]. With mobile video and apps increasing in popularity, richer forms of content like augmented and virtual reality taking the world by storm, and mobile Internet of Things devices like smart wearables and connected cars set to pervade our daily lives, this proliferation in demand is projected to continue at least through the next decade if not beyond. How will mobile access networks scale their capacity to meet this proliferating demand.	access network;connected car;exabyte;internet of things;quickc;scalability;virtual reality;wearable computer	Rakesh Misra;Aditya Gudipati;Sachin Katti	2017	GetMobile	10.1145/3103535.3103546	wearable computer;mobile broadband;internet privacy;scaling;access network;popularity;virtual reality;cellular network;internet of things;engineering	Mobile	-20.324525073156877	77.23184922816712	3656
9cd8e7b53ab8f0fc93708d50d6c778e5a3993b8d	fractional gaussian noise: a tool of characterizing traffic for detection purpose	content management;internet protocol;modelizacion;gaussian noise;autocorrelation function;analisis estadistico;protocole transmission;flood;red www;protocolo internet;securite;erreur quadratique moyenne;protocole internet;ruido gaussiano;inundacion;reseau web;gestion contenido;analyse temporelle;time series;probabilistic approach;analisis temporal;protocole tcp;statistical model;transmission control protocol;time analysis;funcion autocorrelacion;modelisation;inondation;protocolo transmision;statistical analysis;protocolo tcp;time series analysis;fonction autocorrelation;network traffic;mean square error;enfoque probabilista;approche probabiliste;bruit gaussien;safety;denial of service;analyse statistique;distributed denial of service;serie temporelle;serie temporal;fractional gaussian noise;gestion contenu;world wide web;information system;error medio cuadratico;seguridad;modeling;systeme information;denegacion de servicio;deni service;sistema informacion;transmission protocol	Detecting signs of distributed denial-of-service (DDOS) flood attacks based on traffic time series analysis needs characterizing traffic series using a statistical model. The essential thing about this model should consistently characterize various types of traffic (such as TCP, UDP, IP, and OTHER) in the same order of magnitude of modeling accuracy. Our previous work [1] uses fractional Gaussian noise (FGN) as a tool for featuring traffic series for the purpose of reliable detection of signs of DDOS flood attacks. As a supplement of [1], this article gives experimental investigations to show that FGN can yet be used for modeling autocorrelation functions of various types network traffic (TCP, UDP, IP, OTHER) consistently in the sense that the modeling accuracy (expressed by mean square error) is in the order of magnitude of 10 -3 .		Ming Li;Chi-Hung Chi;Dongyang Long	2004		10.1007/978-3-540-30483-8_12	telecommunications;computer science;time series;computer security;denial-of-service attack;statistics	ML	-62.96726492142488	69.54142188520281	3660
623f447535f8af3c65daafbc0fd5b5d4388a961e	deep analysis of invalid handoffs in wlans based on network-client collaborative framework		As 802.11 network has become an important infrastructure for the Mobile Internet, the performance of WLAN handoff is critical to the quality of user's experience. Although it has shown that there are a large number of invalid handoffs in large-scale 802.11 networks with dense AP, the reason and seriousness still remain unclear. In this paper, we propose HandoffAnalyser, a client-network collaborative framework, to deeply measure and analyze the handoffs. Then, we give a clear explanation of invalid handoffs by defining two patterns termed Loose Ping-Pong (LPP) and Strict Ping-Pong (SPP) based on association history. The experimental results of real campus WLAN with about 2,700 APs and 220,000 mobile devices show that 22.1% and 31.3% of handoffs are SPP and LPP respectively. We even observe that several clients experience more than 280 LPP and 220 SPP per day respectively. Such a large number of ping-pong will seriously affect the performance of both client and network. We carry out in-depth analysis and find that ping-pong has relationship to some wireless metrics such as channel utilization, network configuration such as AP with Multi-SSID, and hardware defect such as poor 5 GHz chip. These findings and conclusions can help us to better optimize the WLAN.	interference (communication);load balancing (computing);mathematical optimization;mobile device;network load balancing;ping-pong scheme;self-propelled particles;software bug;software deployment;throughput;wireless router	Xiaokang Sang;Qian Wu;Hewu Li	2017	2017 IEEE International Symposium on Local and Metropolitan Area Networks (LANMAN)	10.1109/LANMAN.2017.7972162	handover;computer network;chip;wireless;the internet;computer science;mobile device;communication channel	Mobile	-7.8016443617576865	91.00340762346933	3664
a7ffc2451c2ba20e5240bf3a8c940d51f4e7cba7	a framework for automatic generation of security controller	security policies;controller operator;synthesis of controller program;partial model checking;process algebra operators	This paper concerns the study, the development and the synthesis of mechanisms for guaranteeing the security of complex systems, i.e., systems composed by several interacting components. A complex system under analysis is described as an open system, i.e., a system in which an unspecified component (a component whose behaviour is not fixed in advance) interacts with the known part of the system. Within this formal approach, we propose techniques that aim to synthesize controller programs able to guarantee that, for all possible behaviours of the unspecified component, the system should work properly, e.g., it should be able to satisfy a certain property. For performing this task, we first need to identify the set of necessary and sufficient conditions that the unspecified component has to satisfy in order to ensure that the whole system is secure. Hence, by exploiting satisfiability procedures for temporal logic, we automatically synthesize an appropriate controller program that forces the unspecified component to meet these conditions. This will ensure the security of the whole system. In particular, we contribute within the area of the enforcement of security properties by proposing a flexible and automated framework that goes beyond the definition of how a system should behave to work properly. Indeed, while the majority of related work focuses on the definition of monitoring mechanisms, we also address the synthesis problem. Moreover, we describe a tool for the synthesis of secure systems which is able to generate appropriate controller programs. This tool is also able to translate the synthesized controller programs into the ConSpec language. ConSpec programs can be actually deployed for enforcing security policies on mobile Java applications by using the run-time framework developed in the ambit of the European Project S3MS. Copyright c © 2008 John Wiley & Sons, Ltd.	ambit;algorithm;automata theory;automaton;boolean satisfiability problem;complex system;complex systems;hp 48 series;halting problem;insertion sort;interaction;java platform, micro edition;john d. wiley;lateral computing;lateral thinking;model checking;newman's lemma;open system (computing);process calculus;rewriting;simulation;temporal logic;terminate (software);truncation;zero suppression	Fabio Martinelli;Ilaria Matteucci	2012	Softw. Test., Verif. Reliab.	10.1002/stvr.441	real-time computing;simulation;computer science;engineering;security policy;software engineering;programming language	Logic	-34.76465053731967	31.477445736472287	3665
0d9af61097d585bc393e82261db4b3089457e7ff	providing grid systems' dependability and security	distributed grid systems;spectrum;telecommunication security grid computing;grid computing data security resource management distributed computing computational modeling public key authentication network servers computer networks metacomputing;dependability;telecommunication security;security grid computing distributed grid systems dependability;security;grid computing;grid security;grid system	Grid computing is concerned with sharing and coordinated use of diverse resources and data in distributed Grid systems rely upon the provision of adequate security. Therefore specific techniques have to be developed in order to attain security. A comprehensive threat model it's a must for simulating grid security model. This article discusses a spectrum of threats and also presents a threat model for grid systems' dependability and security.	dependability;grid systems corporation;grid computing;simulation;threat (computer);threat model	Razvan Bogdan	2007	2007 4th International Symposium on Applied Computational Intelligence and Informatics	10.1109/SACI.2007.375487	computer security model;cloud computing security;spectrum;security information and event management;semantic grid;computer science;information security;data grid;dependability;security service;distributed computing;computer security;drmaa;grid computing;computer network	HPC	-46.41538394980342	56.02076279371551	3677
1db750842ee15d62f3960ceae13fe59ea806dd91	betriebsorganisatorische beherrschung der rechnergestützten transportoptimierung im sozialistischen grosshandel				Friedhelm Zedler	1988				NLP	-95.54387892211723	21.53060012865735	3686
2d0bfeb9a03f16508c47d4e56d4a264c4f4a99f5	preserving topology confidentiality in inter-domain path computation using a path-key-based mechanism		"""Status of This Memo This document specifies an Internet standards track protocol for the Internet community, and requests discussion and suggestions for improvements. Please refer to the current edition of the """"Internet Official Protocol Standards"""" (STD 1) for the standardization state and status of this protocol. Distribution of this memo is unlimited. Abstract Multiprotocol Label Switching (MPLS) and Generalized MPLS (GMPLS) Traffic Engineering (TE) Label Switched Paths (LSPs) may be computed by Path Computation Elements (PCEs). Where the TE LSP crosses multiple domains, such as Autonomous Systems (ASes), the path may be computed by multiple PCEs that cooperate, with each responsible for computing a segment of the path. However, in some cases (e.g., when ASes are administered by separate Service Providers), it would break confidentiality rules for a PCE to supply a path segment to a PCE in another domain, thus disclosing AS-internal topology information. This issue may be circumvented by returning a loose hop and by invoking a new path computation from the domain boundary Label Switching Router (LSR) during TE LSP setup as the signaling message enters the second domain, but this technique has several issues including the problem of maintaining path diversity. This document defines a mechanism to hide the contents of a segment of a path, called the Confidential Path Segment (CPS). The CPS may be replaced by a path-key that can be conveyed in the PCE Communication Protocol (PCEP) and signaled within in a Resource Reservation Protocol TE (RSVP-TE) explicit route object."""	autonomous system (internet);communications protocol;computation;confidentiality;generalized multi-protocol label switching;inter-domain;internet protocol suite;multiprotocol label switching;rsvp-te;resource reservation protocol;router (computing);std bus;test engineer;uniform resource identifier	Rich Bradford;Jean-Philippe Vasseur;Adrian Farrel	2009	RFC	10.17487/RFC5520	fast path;telecommunications;computer science;distributed computing;computer network	Networks	-24.948371677627115	88.54068495306582	3691
0c37c29a6d8f0eae288dd4fbb43177754e464d8c	a model of rsdm implementation	modelizacion;systeme intelligent;procesamiento informacion;adquisicion del conocimiento;sistema inteligente;acquisition connaissance;modelisation;data base management system;knowledge acquisition;model integration;information processing;intelligent system;systeme gestion base donnee;information system;traitement information;rough set;modeling;sistema gestion base datos;database management system;ensemble approximatif;systeme information;sistema informacion	Today's Data Base Management Systems do not provide functionality to extract potencially hidden knowledge in data. This problem gave rise in the 80's to a new research area called Knowledge Discovery in Data Bases (KDD). In spite the great amount of research that has been done in the past 10 years, there is no uniform mathematical model to describe various techniques of KDD. The main goal of this paper is to describe such a model. The Model integrates in an uniform framework various Rough Sets Techniques with standard, non Rough Sets based techniques of KDD. The Model has been already partially implemented in RSDM (Rough Set Data Miner) and we plan to complete the implementation by integrating all the operations in the code of database management systems. Operations that are deened in the paper have successfully been implemented as part of RSDM.	data mining;database;mathematical model;rough set	María C. Fernández-Baizán;Ernestina Menasalvas Ruiz;Anita Wasilewska	1998		10.1007/3-540-69115-4_26	systems modeling;rough set;information processing;computer science;artificial intelligence;machine learning;data mining;operations research;information system	DB	-32.32959901547297	13.963914487238416	3692
ab1a7da469f9c9c0923bf845270df81ca08e26a7	a hybrid trust management model for mas based trading society	multi agent system;trust management;cryptographic protocol;hybrid approach	The widespread use of the Internet signals the need for a better understanding of trust as a basis for secure on-line interaction. In this paper we provide and discuss existing works on trust management models in the area of Multi Agent Systems and highlight the shortcomings. Our proposed model is not presented as the final solution to the issue. This new model will have a mechanism that allows agents to manage trust not by just one way but a few combinations of different types of trust in different situations. The proposed model is concerned with the general notion of trust, one that goes beyond cryptographic protocols. Findings from this paper can be used for future research work in the area of trust in Multi Agent Systems and to address further the importance of trust management on the Internet.	cryptographic protocol;encryption;hoc (programming language);intelligent network;internet;multi-agent system;online and offline;simulation;swarm;trust management (information system);trust management (managerial science)	Kanagaraj Krishna;Mohd. Aizani Bin Maarof	2003	Int. Arab J. Inf. Technol.		web of trust;computer science;knowledge management;artificial intelligence;multi-agent system;cryptographic protocol;computer security;computational trust	AI	-43.11348093992902	56.747117773419966	3693
668fc343df34694a7abb94dd74efb1d01e5661d9	macro requirement within a simulation interface	simulation interface;data interface;assembly cell;macro;macro classification	"""The use of simulation has traditionally required a great deal of expertise. Simulation Interfaces to General Purpose Simulation Languages made simulation easier to use. However, for successfully building simulation models, modelling expertise is still required.Macros have been proposed as a tool within Simulation Interfaces which simplify the building of models. A macro in this context should be seen as a group of SimulationLanguage statements with a data interface to the user.The questions to be answered were: """"What should a macro do and how should it be implemented?"""" The first objective of the work was to specify the macro functionality more tightly and then develop some macros.The wide range of macro applications allows categorising. An attempt was made to classify the different types of macros in a hierarchical structure. Implementation of macros inSIMAN® [1],showed difficulties such as nesting, attribute sharing, and code connectivity. To make data input for complete models less laborious, generic da..."""	simulation	M. Munir Ahmad;Frank A. Stam	1993	Simulation	10.1177/003754979306000305	real-time computing;simulation;computer hardware;computer science;macro;programming language	EDA	-34.484878954948925	26.841275563627832	3695
27bee93a5394801878c82a45fd2d6859807b36c4	evaluation of parallel execution of program tree structures	integral transformations;processing;general and miscellaneous mathematics computing and information science;transformations 990200 mathematics computers;efficiency;performance;laplace transform;data processing;program transformation;mean value analysis;multiclass queueing networks;probabilistic model;error analysis;integral transforms;mathematical models;computer calculations;array processors;tree structure;laplace transformation;comparative evaluations;approximate solutions;mathematical model;task scheduling;laplace stieltjes transform;product from solutions;programming;parallel processing	We define and evaluate two policies (NA-policy, A-policy) for parallel execution of program tree structures. Via a probabilistic model we analytically determine, for each policy, the Laplace-Stieltjes transform for the tree processing time distribution. The acceleration of the program execution time achieved when adding processors to a single processor environment, is computed and plotted for each policy.	central processing unit;plot (graphics);run time (program lifecycle phase);statistical model	Ph. Mussi;Philippe Nain	1984		10.1145/800264.809315	parallel processing;mathematical optimization;discrete mathematics;data processing;computer science;theoretical computer science;operating system;mathematical model;mathematics;laplace transform;statistics	Metrics	-11.604189147047308	41.24271603058014	3702
53244dcb035d0b979f25c33c1c08235375fa0067	embedded systems security		Not long ago, it was thought that only software applications and general purpose digital systems i.e. computers were prone to various types of attacks against their security. The underlying hardware, hardware implementations of these software applications, embedded systems, and hardware devices were considered to be secure and out of reach of these attacks. However, during the previous few years, it has been demonstrated that novel attacks against the hardware and embedded systems can also be mounted. Not only viruses, but worms and Trojan horses have been developed for them, and they have also been demonstrated to be effective. Whereas a lot of research has already been done in the area of security of general purpose computers and software applications, hardware and embedded systems security is a relatively new and emerging area of research. This chapter provides details of various types of existing attacks against hardware devices and embedded systems, analyzes existing design methodologies for their vulnerability to new types of attacks, and along the way describes solutions and countermeasures against them for the design and development of secure systems.	embedded system	Claudia Eckert	2012	Datenschutz und Datensicherheit - DuD	10.1007/s11623-012-0289-x	software security assurance;computer security model;cloud computing security;security service	Crypto	-51.836737073703894	59.925661115993144	3714
5d1ea7e26afbb9ac0939bcc4a0d6ec773953c12d	implementing constrained cyber-physical systems with iec 61499	synchronous;cyber physical systems;compilers;function blocks;iec 61499;software synthesis	Cyber-physical systems (CPS) are integrations of computation and control with sensing and actuation of the physical environment. Typically, such systems consist of embedded computers that monitor and control physical processes in a feedback loop. While modern electronic systems are increasingly characterized as CPS, their design and synthesis still rely on traditional methods, which lack systematic and automated techniques for accomplishment.  Recently, IEC 61499 has been proposed as a standard for designing industrial process-control and measurement systems. It prescribes a component-based approach for developing industrial automation software using function blocks. Executable code can then be automatically generated and simulated from these function blocks. This bodes well for designers of CPS, who are more likely to be experts in specific industrial domains, rather than in computer science. The intuitive graphical nature and automatic code synthesis of IEC 61499 programs will alleviate the programming burden of industrial engineers, while ensuring more reliable software. While software synthesis from IEC 61499 programs is not new, the generation of efficient code from them has been wanting. This has made it difficult for function blocks to be used in software development for resource-constrained embedded controllers commonly employed in CPS. To address this, we present an approach that can generate very efficient code from function block descriptions. Experimental results from a benchmark suite shows that our approach produces substantially faster and smaller code compared to existing techniques.	automation;component-based software engineering;computation;computer science;cyber-physical system;embedded system;feedback;graphical user interface;software development;system of measurement	Li Hsien Yoong;Partha S. Roop;Zoran A. Salcic	2012	ACM Trans. Embedded Comput. Syst.	10.1145/2362336.2362345	embedded system;compiler;real-time computing;a-normal form;computer science;operating system;programming language;cyber-physical system;synchronous learning	Embedded	-42.53247414918949	34.99380078310143	3717
da4e7e01d0d0f2acb7b34b70a6615e94dff16e65	dynamic retrieval of remote digital objects	digital library;hierarchical representation;network traffic;waiting time	A hierarchical representation and dynamic retrieval scheme for digital objects is presented. When the server load and network traffic are heavy in a digital library system, clients have to wait a long time to view the document because of transmission time, especially if the document is large. In order to reduce the client waiting time, we present a new representation and retrieval scheme. It is based on the fact that during a given period of time, the client only focuses on a small part of the document. We store and deliver the document in a certain order according to the client request. The client can view a part of the document as soon as it arrives, while the remaining parts are being transmitted. When the client wants to view a part that has not yet arrived, the server delivers this part immediately so that the client always encounters a short waiting time. This scheme provides the server with the ability to serve more clients without much performance degradation. Experiments show that the client waiting time is greatly reduced using our scheme when server load is heavy.	client (computing);digital library;elegant degradation;library classification;network traffic control;server (computing)	Yongcheng Li;Varna Puvvada	1995		10.1145/221270.221565	digital library;computer science;theoretical computer science;multimedia;world wide web	Web+IR	-15.92192234095896	71.89831565967938	3729
1b8888e6193119d23d482176ebe019c168f24dbe	poster: v2v communication — keeping you under non-disputable surveillance	cooperative awareness message;vehicle to vehicle communication;pseudonym concept	A deployment of V2V technology to ITS vehicle stations according to ETSI is in preparation in Europe. ETSI provides two classes of keys and certificates for ITS vehicle stations: long term authentication keys for entity authentication and pseudonymous keys and certificates to assure vehicular privacy. We show that the periodically send Cooperative Awareness Messages with extensive data together with the pseudonym concept miss the assumed privacy requirements.	authentication;pseudonymity;requirement;software deployment;vehicle-to-vehicle	Markus Ullmann;Thomas Strubbe;Christian Wieschebrink	2016	2016 IEEE Vehicular Networking Conference (VNC)	10.1109/VNC.2016.7835975	telecommunications;computer science;vehicular communication systems;internet privacy;computer security;computer network	Mobile	-49.25560576142687	73.81313580030772	3730
ef51e0e63bc7049e44221813ecaf5a9188a13d83	web applications design with a multi-process approach	developpement logiciel;proceso concepcion;design process;red www;reseau web;complex method;preparacion serie fabricacion;web application design;metamodel;internet;metamodele;metodo complex;specific activity;metamodelo;desarrollo logicial;methode complex;software development;world wide web;process planning;preparation gamme fabrication;meta model;processus conception	This paper deals with a new approach WApDM (Web Applications Design Method) for Web Applications Design. The WApDM is a multi-process approach whish covers all aspects should be considered during design of a Web application. The proposed approach is specified with the MAP formalism. The use of the MAP is three fold: (a) the MAP process meta-model is adapted to the specification of complex methods, (b) MAP introduces more flexibility in the method and (c) implicit information of the method are made explicit during the specification activity.	emoticon;experiment;map;metamodeling;multi-model database;parallel computing;principle of abstraction;semantics (computer science);web application;web design	Semia Sonia Selmi;Naoufel Kraïem;Henda Hajjami Ben Ghézala	2005		10.1007/11531371_65	metamodeling;web modeling;simulation;artificial intelligence;software engineering	SE	-46.868310273875494	22.691980049582433	3735
985abce99ea03bfdcaa680bd3c7f4bcefae72f4c	intersection types from a proof-theoretic perspective	lambda calculus;type assignment systems;structural proof theory;λ calculus;intersection types	In this work we present a proof-theoretical justification for IT by means of the logical system Intersection Synchronous Logic (ISL). ISL builds equivalence classes of deductions of the implicative and conjunctive fragment of NJ. ISL results from decomposing intuitionistic conjunction into two connectives: a synchronous conjunction, that can be used only among equivalent deductions of NJ, and an asynchronous one, that can be applied among any sets of deductions of NJ. A term decoration of ISL exists so that it matches both: the IT assignment system, when only the synchronous conjunction is used, and the simple types assignment with pairs and projections, when the asynchronous conjunction is used. Moreover, the proof of strong normalization property for ISL is a simple consequence of the same property in NJ and hence strong normalization for IT comes for free.	formal system;intuitionistic logic;logical connective;name mangling;naruto shippuden: clash of ninja revolution 3;normalization property (abstract rewriting);theory;isl	Elaine Pimentel;Simona Ronchi Della Rocca;Luca Roversi	2012	Fundam. Inform.	10.3233/FI-2012-778	discrete mathematics;computer science;artificial intelligence;lambda calculus;mathematics;programming language;structural proof theory;algorithm;algebra	Logic	-13.578510888731365	13.679261688391103	3738
d9ff539ca07a296a304685610233df0bd2324ccc	predicting timeliness of reactive systems under flexible scheduling	flexible scheduling;kernel;degradation;state space methods;state space methods scheduling petri nets real time systems;processor scheduling;loading conditions timeliness prediction reactive systems flexible scheduling modeling validation timed petri nets tpn adaptivetpns real time systems state space analysis exhaustive prediction;timed petri nets;exhaustive prediction;timeliness prediction;processor scheduling real time systems quality of service petri nets state space methods predictive models degradation timing springs kernel;adaptivetpns;loading conditions;springs;time petri net;hard real time system;flexible computation;scheduling;state space;timed behavior;reactive system;state space analysis;validation;predictive models;reactive systems;petri nets;quality of service;petri net;process scheduling;tpn;enumerative analysis;modeling;hard real time systems;real time systems;timing	A modeling and validation approach extending the formalism of timed Petri nets (TPN) for the analysis of real time systems with flexible scheduling capabilities is introduced. The new formalism is called AdaptiveTPNs. State space analysis of the model supports exhaustive prediction of the time needed to complete critical functions, and permits automatic identification of loading conditions which determine the reduction of the quality of produced results.		Giacomo Bucci;Andrea Fedeli;Enrico Vicario	2003		10.1109/ISADS.2003.1193940	embedded system;real-time computing;reactive system;computer science;operating system;distributed computing;scheduling;petri net	Embedded	-39.49391000333313	33.972380885492484	3746
0ead1d8632532987885c178e3bcf0bc0d5fa2225	software organization to facilitate dynamic processor scheduling	processor scheduling;ground support systems;ground support system dynamic processor scheduling nasa jet propulsion laboratory mission data system software architecture high level planner low level execution manager time utility functions;software architecture;aerospace computing;aerospace propulsion;mission data system;space missions;planning;time utility function;processor scheduling propulsion laboratories data systems space missions organizing system testing computer architecture dynamic scheduling systems engineering and theory;software architecture processor scheduling dynamic scheduling planning aerospace propulsion aerospace computing ground support systems;dynamic scheduling;jet propulsion laboratory	Summary form only given. The NASA Jet Propulsion Laboratory (JPL) is developing the mission data system (MDS) for potential use in future space missions, where it is expected to reduce the complexity and effort required to produce mission and ground-support software, while also enabling greater autonomy for remotely deployed systems, including future planetary rovers. The MDS software architecture includes two levels of processor scheduling: a high-level planner and a low-level execution manager. JPL and MITRE are investigating the use of time-utility functions to manage tow-level scheduling, since they promise to enable adaptive, short-term processor scheduling based on mission utility. Recent development work on MDS has focused on organizing the software so that low-level processor scheduling will be most effective. We describe the major organizing principles that have shaped this work.	data system;high- and low-level;organizing (structure);planetary scanner;scheduling (computing);software architecture;windows nt processor scheduling	Raymond K. Clark;E. Douglas Jensen;Nicolas F. Rouquette	2004	18th International Parallel and Distributed Processing Symposium, 2004. Proceedings.	10.1109/IPDPS.2004.1303083	planning;embedded system;software architecture;parallel computing;real-time computing;simulation;dynamic priority scheduling;computer science;space exploration;operating system;two-level scheduling	Arch	-45.01498734625316	36.740906770709216	3750
1fa0ca5cd37e0b3da348ff1271e0c5406e0b9d92	knowledgeable development environments using shared design models	design model;development;life cycle;user interface;collaboration;development environment;design knowledge;design;technical report;models;human computer interface;knowledge base	We describe MASTERMIND, a step toward our vision of a knowledge-based design-time and run-time environment in which human-computer interfaces development is centered around an all-encompassing design model. The MASTERMIND approach is intended to provide integration and continuity across the entire life cycle of the user interface. In addition, it facilitates higher quality work within each phase of the life cycle. MASTERMIND is an open framewodc, in which the design knowledge base allows multiple tools to come into play and makes knowledge created by each tool accessible to the others.	knowledge base;runtime system;scott continuity;user interface	Robert Neches;James D. Foley;Pedro A. Szekely;Noi Sukaviriya;Ping Luo;Srdjan Kovacevic;Scott E. Hudson	1993		10.1145/169891.169901	user interface design;design;knowledge base;user experience design;human–computer interaction;computer science;knowledge management;technical report;development environment;design education;user interface;collaboration	HCI	-49.73479249676195	22.36829349703615	3758
7dc07eab8f1870e380de124aefee7db014e77d6a	tisefe: time series evolving fuzzy engine for network traffic classification		Monitoring and analyzing network traffic are very crucial in discriminating the malicious attack. As the network traffic is becoming big, heterogeneous, and very fast, traffic analysis could be considered as big data analytic task. Recent research in big data analytic filed has produced several novel large-scale data processing systems. However, there is a need for a comprehensive data processing system to extract valuable insights from network traffic big data and learn the normal and attack network situations. This paper proposes a novel evolving fuzzy system to discriminate anomalies by inspecting the network traffic. After capturing traffic data, the system analyzes it to establish a model of normal network situation. The normal situation is a time series data of an ordered sequence of traffic information variable values at equally spaced time intervals. The performance has been analyzed by carrying out several experiments on real-world traffic dataset and under extreme difficult situation of high-speed networks. The results have proved the appropriateness of time series evolving fuzzy engine for network classification.	anomaly detection;big data;binary classification;experiment;fuzzy control system;fuzzy logic;knowledge base;network packet;network traffic control;norm (social);real-time clock;sensor;software deployment;time series;traffic analysis;traffic classification	Shubair Abdulla;Amaal Saleh Al Hashimy	2018	IJCNIS		traffic analysis;distributed computing;time series;fuzzy logic;big data;traffic classification;machine learning;computer science;fuzzy control system;data processing system;artificial intelligence	ML	-62.44770939747613	66.33763382435794	3759
6e0ef278ac0e440ef25d822588dc192860640ff8	array data flow analysis for load-store optimizations in superscalar architectures	shift operator;register allocation;collective behavior;data flow analysis;superscalar processor;data flow;flow analysis	The performance of scientiic programs on superscalar processors can be signiicantly degraded by memory references that frequently arise due to load and store operations associated with array references. Therefore, register allocation techniques have been developed for allocating registers to array elements whose values are repeatedly referenced over one or more loop iterations. To place load, store, and register-to-register shift operations without introducing fully/partially redundant and dead memory operations, a detailed value ow analysis of array references is required. We present an analysis framework to ee-ciently solve various data ow problems required by array load-store optimizations. The framework determines the collective behavior of recurrent references spread over multiple loop iterations. We also demonstrate how our algorithms can be adapted for various ne-grain architectures.	algorithm;arithmetic shift;central processing unit;data-flow analysis;dataflow architecture;iteration;ne (complexity);register allocation;service-oriented architecture;superscalar processor	Rastislav Bodík;Rajiv Gupta	1995		10.1007/BFb0014188	computer architecture;parallel computing;real-time computing;computer science;data-flow analysis;programming language	Arch	-5.029362425150684	51.759447153719414	3763
574708fd48ae589daf6686b9080849fb413de4ba	towards automated capturing and processing of user feedback for optimizing mobile apps		Abstract Mobile devices are nowadays ubiquitous and are heavily used for business and private purposes. Millions of apps exist that support users in multiple ways, e.g., for car navigation, fitness purposes, or messaging in our private lives, but also for business planning purposes and even for controlling whole business processes. Failures in mobile business apps can lead to dramatic consequences in terms of lost revenue, but also in terms of lost trust or even threats for human beings, and thus quality plays a crucial role. On the other hand, as software is nowadays one of the main drivers for innovation, fast delivery of new apps, respectively new functionality, is necessary, i.e., the time to market must often be short. However, in order to understand whether the quality is sufficient, and whether the functionality of the app serves the needs of the users, lean development approaches are emerging and propose the deployment of apps as a minimal viable product (MVP). Here, the app is provided with acceptable quality, but not with every feature, just with the main functionality. Based on such an MVP, early feedback from users is to be collected, which may be related to the quality of the app, but also include wishes and requests for new functionality. In order to analyze and draw conclusions from user feedback, we first have to understand what kind of feedback exists and how it can be interpreted, respectively how valid such feedback is. In this publication, we provide a classification of user feedback for mobile apps, derive feedback channels, and sketch how this can be used in a continuous process to improve mobile apps.	optimizing compiler	Frank Elberzhager;Konstantin Holl	2017		10.1016/j.procs.2017.06.087	mobile deep linking;business process;software deployment;minimum viable product;data mining;software;computer science;time to market;mobile device;lean software development	SE	-76.6407012279131	30.87106724010703	3767
468f8727ea389508fd1549b12b526289b4680b86	merge and forward: a self-organized inter-destination media synchronization scheme for adaptive media streaming over http		In this chapter, we present Merge and Forward, an IDMS scheme for adaptive HTTP streaming as a distributed control scheme and adopting the MPEG-DASH standard as representation format. We introduce so-called IDMS sessions and describe how an unstructured peer-to-peer overlay can be created using the session information using MPEG-DASH. We objectively assess the performance of Merge and Forward with respect to convergence time (time needed until all clients hold the same reference time stamp) and scalability. After the negotiation on a reference time stamp, the clients have to synchronize their multimedia playback to the agreed reference time stamp. In order to achieve this, we propose a new adaptive media playout approach minimizing the impact of playback synchronization on the QoE. The proposed adaptive media playout is assessed subjectively using crowd sourcing. We further propose a crowd sourcing methodology for conducting subjective quality assessments in the field of IDMS by utilizing GWAP. We validate the applicability of our methodology by investigating the lower asynchronism threshold for IDMS in scenarios like online quiz games.	hypertext transfer protocol;streaming media	Benjamin Rainer;Stefan Petscharnig;Christian Timmerer	2018		10.1007/978-3-319-65840-7_21	timestamp;distributed algorithm;scalability;merge (version control);synchronization;overlay;convergence (routing);distributed computing;computer science	HPC	-7.769781692574247	97.01570363711384	3774
3a87d874ccc1a2bbd99e15d900e3c8831a48f1ed	proposition d'une annotation sémantique floue guidée par ontologie pour l'interprétation des images de télédétection : application à la gestion des risques naturels. (fuzzy semantic annotation based on ontology for remote sensing images interpretation: application to natural risks)		HAL is a multi-disciplinary open access archive for the deposit and dissemination of scientific research documents, whether they are published or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers. L’archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la diffusion de documents scientifiques de niveau recherche, publiés ou non, émanant des établissements d’enseignement et de recherche français ou étrangers, des laboratoires publics ou privés. Proposition d’une annotation sémantique floue guidée par ontologie pour l’interprétation des images de télédétection : Application à la gestion des risques naturels Wassim Messoudi		Wassim Messoudi	2013				Crypto	-107.68103843364591	12.5735537466834	3780
6992d8dd8e5d06fba408ba726849fed63ca122cb	p2v migration with post-copy hot cloning for service downtime reduction	software;server virtualization;performance evaluation;virtual machining;hot cloning;cloning;virtual machines linux;virtual machine monitors;virtual machines;cloning virtual machine monitors software linux virtual machining throughput performance evaluation;linux;server virtualization p2v hot cloning;linux based hyper visors p2v migration post copy hot cloning service downtime reduction physical to virtual copy disk images virtual machine storage devices hot cloning mechanism physical machine source physical machine disk image linux network block device nbd kernel module;p2v;throughput	We propose a post-copy hot cloning mechanism to reduce service downtime of physical to virtual (P2V) migration. The conventional P2V migration requires to copy disk images from a physical to a virtual machine with stopping the system to ensure the consistency of the disk images, but it usually takes long time (e.g., several hours) because of slow I/O performance of storage devices. The proposed hot cloning mechanism copies disk images from a physical machine to a virtual machine in the background with running the system on the virtual machine. Since the disk image copy is the bottleneck of P2V migration, this hot cloning mechanism drastically reduces the service downtime. To ensure the consistency of disk images during the hot cloning, the hot cloning mechanism provides a block device of the source physical machine and a disk image (or block device) of the destination virtual machine as one virtualized block device that manages all I/O operations to the virtual machine. In this paper, we implement this post-copy hot cloning mechanism using the Linux Network Block Device (NBD) kernel module to support Linux-based hyper visors such as Xen and Linux KVM. We discuss the service downtime that is reduced by the proposed mechanism, and demonstrate that the implementation performs well in practical environments throughout the performance evaluations on the copy speed, hyper visor load, and system performance on a virtual machine.	disk image;downtime;elegant degradation;hypervisor;input/output;linux;loadable kernel module;network block device;operating system;quantum cloning;vehicle-to-vehicle;virtual machine	Hirochika Asai	2013	2013 International Conference on Cloud and Green Computing	10.1109/CGC.2013.9	sysfs;embedded system;temporal isolation among virtual machines;computer hardware;computer science;virtual machine;kernel virtual address space;operating system	OS	-16.94574792230351	51.980798425396	3783
7dc3e567b5d9865c4c931e79cd3bea90c91999fe	completeness of kozen's axiomatization for the modal mu-calculus: a simple proof		The modal μ-calculus, introduced by Dexter Kozen, is an extension of modal logic with fixpoint operators. Its axiomatization, Koz, was introduced at the same time and is an extension of the minimal modal logic K with the so-called Park fixpoint induction principle. It took more than a decade for the completeness of Koz to be proven, finally achieved by Igor Walukiewicz. However, his proof is fairly involved. In this article, we present an improved proof for the completeness of Koz which, although similar to the original, is simpler and easier to understand.	axiomatic system;dexter kozen;fixed point (mathematics);igor muttik;modal logic;modal μ-calculus	Kuniaki Tamura	2014	CoRR		modal μ-calculus;discrete mathematics;mathematics;algorithm	Logic	-10.826713304586233	14.965766593539344	3787
062f26f21902ea1629c356c07cd5618b742522b3	web service access management for integration with agent systems	agent interaction;web service management;agent middleware;web service interaction and integration;service management;web service;virtual organization;agent systems;middleware;monitoring and control	The agent paradigm includes the notion that agents interact with services. This paper identifies the need for controlled access to such services, from the perspective of agent systems (and not as is generally the case by web service providers). Mediating between web service requests from (virtual) organizations of agents, the web service gateway proposed regulates (i.e., monitors and controls) web service access according to the SLAs and organizational policies that are in effect. In addition to a model for web service access regulation, an implementation of a middleware component for web services access regulation based on SOAP and described in WSDL is presented.	middleware;programming paradigm;soap;web services description language;web service	Benno J. Overeinder;P. D. Verkaik;Frances M. T. Brazier	2008		10.1145/1363686.1364135	web service;web application security;middleware;web development;differentiated service;web standards;service management;computer science;knowledge management;service delivery framework;operating system;ws-policy;service-oriented architecture;service design;middleware;ws-addressing;database;ws-i basic profile;world wide web;devices profile for web services;universal description discovery and integration	Web+IR	-48.102797208976426	15.98640172753913	3788
37f3d43e24d1321a419e2f2104aff0c546f79066	linking localisation and language resources	industrial localisation;conference paper;computer science and engineering	Industrial localisation is changing from the periodic translation of large bodies of content to a long-tail of small, heterogeneous translations processed in an agile and demand-driven manner. Software localisation and crowd-source translation already practice continuous fine-grained distribution of translation work. This requires close integration and round-trip interoperability between content creation and localisation processes, while at the same time recording the provenance of translated content to maximise it reuse in future translation tasks, and, increasingly, in training Statistical Machine Translation (SMT) engines. This work adopts a Linked Data approach to integrating the content translation round-trip process with the logging of process quality assurance provenance. This integration supports a pull-based interoperability model that supports continuous synchronising of content and process meta-data between the generating organisation and any number of language service providers or translators. We present a platform architecture for sharing, searching and interlinking of Linked Localisation and Language Data (termed L3Data) on the web. This is accomplished using a semantic schema for L3Data that is compatible with existing localisation data exchange standards and can be used to support the round-trip sharing of language resources. The paper describes our approach to development of L3Data schema and data management processes, web-based tools and data sharing infrastructure that use it. An initial proof of concept prototype is presented which implements a web application that segments and machine translates content for crowd-sourced post-editing and rating. David Lewis · Alexander O’Connor · Sebastien Molines · Leroy Finn · Dominic Jones · Stephen Curran · Séamus Lawless Centre for Next Generation Localisation, Knowledge and Data Engineering Group, Trinity College Dublin, Ireland e-mail: {Dave.Lewis,Alex.OConnor,moliness,finnle,Dominic.Jones,Stephen.Curran,slawless}@cs.tcd.ie	agile software development;crowdsourcing;email;information engineering;internationalization and localization;interoperability;jones calculus;linked data;long tail;next-generation network;postediting;prototype;statistical machine translation;trinity;web application	David Lewis;Alexander O'Connor;Sebastien Molines;Leroy Finn;Dominic Jones;Stephen Curran;Séamus Lawless	2012		10.1007/978-3-642-28249-2_5	language localisation;computer science;database;multimedia;world wide web	DB	-41.57507407427474	4.901807926754056	3802
4f10ffe1e306fcd98383044975058bd48f1cbfd8	schnellere approximationsalgorithmen zur partiell-dynamischen berechnung kürzester wege				Sebastian Krinninger	2015				Crypto	-95.82422042704474	21.767130246426404	3805
0bdb523165d9e5f39088e777de2fb566ad20039f	now that we are all so well-educated about spyware, can we put the bad guys out of business?	university student;spyware;secure computation;bots;adware;malware;antispyware;greyware	"""The phenomenon known variously as spyware, adware, or malware has grown exponentially in the past few years and has been swamping our computer systems, much like email spam but significantly worse in every sense of the word.Complicating the matter, the line between viruses and spyware is rapidly blurring, largely because of the increasing sophistication of this spyware and the multiplication of """"bots."""" While it is possible that university students are even more susceptible than the general public, it is certainly true that this is a serious national problem.Learning to identify and remove these insidious files is a critical step in securing computers and networks and in enabling faculty, staff, and students to complete their work safely and efficiently."""	computer;email;malware;spamming;spyware	Karen McDowell	2006		10.1145/1181216.1181269	scareware;engineering;adware;malware;internet privacy;world wide web;computer security	Security	-55.38611826297964	63.47110877601477	3807
7e696f9ca98ec78378a22193a32f51a47baa0bda	a quantum bit commitment scheme provably unbreakable by both parties	bob;quantum computing physics computing security microwave integrated circuits polarization optical computing power measurement cryptographic protocols computational complexity cryptography;microwave integrated circuits;protocols;complete protocol;circular polarization;quantum information;cryptographic protocols;polarization;optical computing;physics computing;quantum bit commitment scheme;cryptography protocols;quantum physics;conjugate transmission reception basis;computational complexity;cryptography;polarized photons;circular polarization quantum bit commitment scheme complete protocol polarized photons conjugate transmission reception basis;quantum computing;security;commitment scheme;power measurement	Can quantum mechanics be harnessed to provide unconditionally secure bit commit ment schemes and other cryptographic primitives beyond key distribution We review the general impossibility proof of Mayers and illustrate it by showing how to break some recent attempts to bypass it In particular secure schemes would follow if we could force participants to perform measurements at speci ed points in the execution of the proto col It has been suggested to use short lived classical bit commitment schemes for this purpose Alas this strategy was doomed as measurements can always be postponed in an undetectable way until cheating becomes possible It is well known that quantum mechanics can be used to allow two people to establish con dential communication under the nose of an eavesdropper equipped with unlimited computing power Can quantum mechanics be useful for the implementation of other cryptographic tasks One of the most important primitives in classical cryptography is the bit commitment scheme which allows one party Alice to give something to another party Bob in a way that commits her to a bit b of her choice so that Bob cannot tell what b is but Alice can later prove to him what b originally was You may think of this as Alice sending to Bob a locked safe that contains the value of b written on a slip of paper later she can send him the key that opens the safe if she wants him to know to which bit she had committed A commitment scheme is binding if Alice cannot change the value of b and it is concealing if Bob cannot obtain any information about b without the help of Alice It is secure if it is simultaneously binding and concealing and it is unconditionally secure if it is secure against a cheater either Alice or Bob equipped with unlimited technology and computational power It has long been known that unconditionally secure bit commitment schemes cannot exist in Supported in part by Canada s nserc Qu ebec s fcar and the Canada Council y D epartement IRO Universit e de Montr eal C P succursale centre ville Montr eal Qu ebec Canada H C J e mail brassard iro umontreal ca z Supported in part by Canada s nserc and Qu ebec s fcar x School of Computer Science McGill University Pavillon McConnell rue University Montr eal Qu ebec Canada H A A e mail crepeau cs mcgill ca Department of Computer Science Princeton University Princeton NJ USA e mail mayers cs princeton edu k Supported in part by the Danish National Research Foundation under the Basic Research in Computer Science BRICS programme Department of Computer Science University of Aarhus Ny Munkegade building DK Arhus C Denmark e mail salvail daimi aau dk the classical world After the success of quantum key distribution it was natural to hope that quantum mechanics might provide such an unconditionally secure scheme A protocol for quantum bit commitment henceforth referred to as BCJL was proposed in and claimed to be provably secure which would also have allowed secure quantum oblivious transfer another fundamental primitive in classical cryptography Because of this the future of quan tum cryptography looked very bright indeed with new applications such as the identi cation protocol of Cr epeau and Salvail coming up regularly Trouble began in October when Mayers found a subtle aw in the BCJL protocol The proof that Bob cannot cheat BCJL and learn information on the committed bit was correct and so was the proof that it is not possible for Alice to simultaneously know how to open the commitment to show a and how to open it to show a However it was possible for Alice through the magic of quantum entanglement to learn at her choice either how to open the commitment to show a or how to open it to show a Because of the destructive nature of quantum measurements choosing either one of these alternatives destroyed her chances for learning how to open the commitment the other way a situation that has no classical analogue It is as if Alice could provide Bob with one of two possible quantum keys to open the safe whose e ect would be to make the desired bit appear surreptitiously on the slip of paper in an irreversible manner Though Mayers explained his discovery to several researchers interested in quantum information processing his result was not made public until after Lo and Chau discovered independently a similar technique This prompted several attempts at xing the protocol but every time ad hoc successful attacks soon followed This cat and mouse game went on until Mayers proved that all these attempts had been in vain unconditionally secure quantum bit commitment schemes are impossible Despite all odds various kinds of ideas continued to be proposed by some of us as well as others in the hope they would not fall prey to Mayers result Perhaps the most interesting approach consisted in using various types of short lived classical bit commitment schemes to force the cheater to perform measurements at speci ed points in the execution of the protocol which indeed would x the problem Alas this strategy was doomed as measurements can always be postponed until cheating becomes possible This is not surprising in retrospect because it has since been realized that these apparently promising ideas were also ruled out by Mayers general impossibility theorem Nevertheless these attempts contributed to enhance our understanding of what is going on with quantum bit commitment schemes and other quantum cryptographic protocols Assume for example that you are given an unknown quantum bit j i j i j i and you are expected to measure it either in the computational basis j i versus j i or the diagonal basis j i j i p versus j i j i p You could cheat the protocol if only you could postpone the choice of basis and therefore the measurement until after subsequent classical information becomes available To prevent this you are asked to use a classical perhaps multi party commitment scheme to commit to your choice of basis computational or diagonal as well as to the result of your measurement Then either you are immediately challenged to open those commitments to prove that you had measured in which case this quantum bit is not used any further or you are allowed to keep secret your choice of basis and measurement result for later use The hope was that this approach could turn a classical bit commitment scheme that is reasonably secure for a very short time into a quantum bit commitment scheme that is permanently secure Unfortunately it turns out that you can always postpone measuring the quantum bit and o er fake commitments even if you are unable to break the underlying commitment scheme in the time that elapses between the commitment and the challenge if a challenge comes you can open your commitments in a way consistent with what you would have committed to had you measured the quantum bit and if the challenge does not come you can keep the quantum bit intact for later measurement in the basis of your choice For a detailed description of these awed quantum bit commitment schemes and how to break them the reader is encouraged to consult a preliminary version of the full paper currently available on the Los Alamos National Laboratory e Print quantum physics archive	alice and bob;archive;commitment scheme;computation;computer science;cryptographic protocol;cryptography;email;hoc (programming language);information processing;naruto shippuden: clash of ninja revolution 3;oblivious transfer;physical information;prey;provable security;quantum entanglement;quantum information science;quantum key distribution;quantum mechanics;qubit;retrospect (software);steve mcconnell;turned a	Gilles Brassard;Claude Crépeau;Richard Jozsa;Denis Langlois	1993		10.1109/SFCS.1993.366851	circular polarization;quantum information;commitment scheme;polarization;computer science;cryptography;theoretical computer science;mathematics;distributed computing;optical computing	Theory	-43.726163725569016	83.32415413935807	3840
b6c3a77c9037941228a4bd309da302153d99f68c	ein meßsystem zur bewertung von multicast-protokollen im internet.		Das Internet als weltweite Kommunikationsplattform hat in den vergangenen Jahren einen enormen Aufschwung erhalten. Es ermöglicht die Kommunikation und Kooperation zwischen Partnern, die weltweit verteilt operieren. Zugleich bildet es die Basis für zahlreiche Dienste, wie beispielsweise das World Wide Web (WWW), News oder E-Mail. In zunehmendem Maße finden auch Systeme zur Durchführung von Videokonferenzen oder zur verteilten Teamarbeit das Interesse der Anwender. Banken und Zeitschriftenverlage gehen dazu über, Informationen über das Internet an eine prinzipiell beliebig große Anzahl von Empfängern zu verteilen. Das hierbei zugrundeliegende Kommunikationsprinzip läßt sich meist auf den Basisfall der sogenannten Multicast-Kommunikation abbilden. Hierbei übermittelt ein Sender die Daten an mehrere Empfänger. Dies kann durch mehrfache Nutzung der herkömmlichen Punkt-zu-Punkt Kommunikation erfolgen, indem der Sender mehrere Kopien ein und desselben Datenpaketes an die jeweiligen Empfänger adressiert. Es ist offensichtlich, daß eine solche Realisierung des Multicast-Dienstes äußerst ineffizient ist und in größeren, globalen Gruppen kaum eingesetzt werden kann. Dem Entwurf und der Entwicklung spezieller Multicast-Protokolle kommt demnach eine entscheidende Rolle zu.	eine and zwei;institut für dokumentologie und editorik;internet explorer;multicast;www;world wide web	Markus Hans Hofmann;Jörn Hartroth	1997			multicast;computer network;the internet;computer science	OS	-104.86927158702287	36.63571517286587	3847
806799fc8c91f64be71d6f21a5cb63679fc897c0	simulation and decision support models for rough mills: a multi-agent perspective	distributed system;decision support;multi agent system;distributed systems decision support systems agent based systems simulation manufacturing rough mills;agent based model;distributed systems simulation support model decision support model multiagent perspective manufacturing facility multiagent system graphical user interface rough mill floor discrete event based simulation model agent based model java agent development framework decision support systems agent based systems;multi agent systems;decision support system;graphical user interfaces;agent based system;decision support systems;production facilities;graphic user interface;agent systems;java production facilities milling decision support systems multi agent systems graphical user interfaces discrete event simulation;milling;simulation model;discrete event;milling machines multiagent systems strips discrete event simulation production facilities virtual manufacturing costs manufacturing automation fuel cells technological innovation;java;discrete event simulation	A rough mill is a manufacturing facility where loads of lumber (jags) are processed into specific size components. In this paper, we describe a multi-agent system that simulates the operations of the ripsaw, conveyor and chopsaw, and provides the user with recommended decisions for selecting jags and cut-lists (specific-size components for cutting). Using the graphical user interface, the user can run several scenarios, and test the recommended decisions through simulation. The operator can then make an informed decision on the rough mill floor. A discrete event based simulation model provides functionality for the agent-based model. The Java Agent Development Framework (JADE) was used to develop the agent system. The new system was validated by comparing results obtained using a centralized simulator.	agent-based model;centralized computing;decision support system;graphical user interface;jade;java;list scheduling;multi-agent system;rough set;scheduling (computing);simulation	Eman Elghoneimy;Özge Uncu;William A. Gruver;Dilip B. Kotak	2005	2005 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2005.1571725	simulation;decision support system;computer science;artificial intelligence;graphical user interface	Robotics	-37.6548148579726	22.0564184112041	3848
1cbbf78ef8186e0c0c05c3c04b22f6566b2ed7af	applications of reliable scientific computing	scientific computing		computational science	A. Gabaldon	1997	Reliable Computing	10.1023/A:1009957806357	mathematics	HPC	-7.666520573680005	37.71828133026955	3849
5aa63e1c48abdb4e9dc128e6d28a605b5a3dd3a5	a kernel based structure matching for web services search	ranking svm;wsdl;n spectrum kernel;spectrum;web service;web services;service discovery;web services matching	This paper describes a kernel based Web Services (abbrevi-ated as service) matching mechanism for service discoveryand integration. The matching mechanism tries to exploitthe latent semantics by the structure of services. Using textual similarity and n-spectrum kernel values as features of low-level and mid-level, we build up a model to estimate thefunctional similarity between services, whose parameters arelearned by a Ranking-SVM. The experiment results showedthat several metrics for the retrieval of services have beenimproved by our approach.	high- and low-level;kernel (operating system);ranking svm;web service	Jianjun Yu;Shengmin Guo;Hao Su;Hui Zhang;Ke Xu	2007		10.1145/1242572.1242791	web service;computer science;data mining;database;law;world wide web	Web+IR	-44.730212914981905	13.359661767876826	3850
243ab97f3a1adc2b5684767d99ff1ca9f5a41046	irods primer 2: integrated rule-oriented data system		Policy-based data management enables the creation of community-specific collections. Every collection is created for a purpose. The purpose defines the set of properties that will be associated with the collection. The properties are enforced by management policies that control the execution of procedures that are applied whenever data are ingested or accessed. The procedures generate state information that defines the outcome of enforcing the management policy. The state information can be queried to validate assessment criteria and verify that the required collection properties have been conserved. The integrated Rule-Oriented Data System implements the data management framework required to support policy-based data management. Policies are turned into computer actionable Rules. Procedures are composed from a microservice-oriented architecture. The result is a highly extensible and tunable system that can enforce management policies, automate administrative tasks, and periodically validate assessment criteria. iRODS 4.0+ represents a major effort to analyze, harden, and package iRODS for sustainability, modularization, security, and testability. This has led to a fairly significant refactorization of much of the underlying codebase. iRODS has been modularized whereby existing iRODS 3.x functionality has been replaced and provided by small, interoperable plugins. The core is designed to be as immutable as possible and serve as a bus for handling the internal logic of the business of iRODS. Seven major interfaces have been exposed by the core and allow extensibility and separation of functionality into plugins.	data system;primer	Hao Xu;Terrell G. Russell;Jason Coposky;Arcot Rajasekar;Reagan Moore;Antoine de Torcy;Michael Wan;Wayne Shroeder;Sheau-Yen Chen	2017		10.2200/S00760ED1V01Y201702ICR057		DB	-47.76958898543988	37.74559557605449	3864
06c51628bf4da0aa31350d9ef878fe85c6929131	event tree analysis for flood protection - an exploratory study in finland	extreme events;adaptation;impact modelling;event tree analysis	Decision-making for the purpose of adaptation to climate change typically involves several stakeholders, regions and sectors, as well as multiple objectives related to the use of resources and benefits. In the case of adapting to extreme events, modelling of the impact pathways and consequences need to be conducted in some way. We explore the role of event tree analysis of extreme events in the context of flood protection of critical infrastructure. Experts representing potentially affected infrastructure services are consulted on the usability of the ETA method for providing structured information on flood scenarios, system impacts and consequences, risks and counter measures. The main users of the analysis results are the asset owners and the local public decision-makers whose joint efforts are usually required to fund and prioritize such measures of adaptation.	event tree analysis	Tony Rosqvist;Riitta Molarius;Hanna Virta;Adriaan Perrels	2013	Rel. Eng. & Sys. Safety	10.1016/j.ress.2012.11.013	reliability engineering;simulation;data mining;event tree analysis;adaptation	DB	-75.54012214849911	13.29198476372563	3867
96656d6b90179fab6b8dfe4ba864b892edde8402	the linux-srt integrated multimedia operating system: bringing qos to the desktop	fairness;time sharing computer systems;quality of service unix operating systems computers multimedia systems real time systems;user interface;processor scheduling;integrable system;qos guarantee;resource management;integrated multimedia operating system;multimedia systems;operating system;multimedia systems operating systems quality of service linux processor scheduling real time systems dynamic scheduling laboratories resource management time sharing computer systems;qos management;predictable scheduling;real time systems linux srt integrated multimedia operating system fairness timesharing systems quality of service user interface predictable scheduling cpu disk bandwidth;linux;disk bandwidth;linux srt;quality of service;cpu;operating systems computers;unix;timesharing systems;dynamic scheduling;operating systems;real time systems	Current operating systems are being used to pegorm tasks for which they were not designed. Architectures conceived to provide fairness in timesharing systems are being asked to support applications needing Quality of Service (QoS) guarantees. Past research has focussed on specialised multimedia operating systems; QoS features are now ready to be incorporated in mainstream operating systems, allowing existing applications to be used without modijication. Appropriate user-intelface features are essential if QoS is to become widely used. We present the Linux-SRT [SJ system, a version of Linux enhanced with support f o r predictable scheduling and QoS management. It is binary compatible with standard Linux: existing applications can benefit from QoS without being modijied in any way. CPU and disk bandwidth are scheduled, and scheduling policies are propagated to servers. Automated control and management features simplib the use of advanced features. Linux-SRT has been proven in everyday use. It provides an integrated system approach to QoS f o r multiple devices, from low-level scheduling to high-level user tools.	central processing unit;division algorithm;fairness measure;high- and low-level;linux;operating system;quality of service;scheduling (computing);time-sharing	Stephen Childs;David Ingram	2001		10.1109/RTTAS.2001.929879	embedded system;integrable system;real-time computing;mobile qos;quality of service;dynamic priority scheduling;computer science;resource management;operating system;central processing unit;unix;user interface;linux kernel	Embedded	-10.657821328046722	64.48392513245958	3872
95e4c4b02f9844f16204459324239452883c6523	semantic technologies for the iot - an inter-iot perspective	standards;sensors;semantics;iot related semantic approaches semantic technologies inter iot project open cross layer framework heterogeneous internet of things platforms software stack semantic interoperability;medical services;semantic web internet of things;ontologies;interoperability;cloud computing;ontologies sensors semantics interoperability standards cloud computing medical services	The Inter-IoT project is aiming at the design and implementation of, and experimentation with, an open cross-layer framework and associated methodology, to provide voluntary interoperability among heterogeneous Internet of Things (IoT) platforms. The project is initially driven by uses cases from two domains: (e/m) Health and transportation and logistics in a port environment. While the Inter-IoT will provide interoperability across the software stack, here, we focus our attention on the semantic interoperability. In this context, we present a concise overview of existing IoT-related semantic approaches, which might either be directly applicable to, or serve as a source of inspiration for, the Inter-IoT applications.	internet of things;logistics;semantic interoperability	Maria Ganzha;Marcin Paprzycki;Wieslaw Pawlowski;Pawel Szmeja;Katarzyna Wasielewska	2016	2016 IEEE First International Conference on Internet-of-Things Design and Implementation (IoTDI)	10.1109/IoTDI.2015.22	semantic interoperability;interoperability;semantic computing;cloud computing;telecommunications;semantic grid;computer science;sensor;ontology;operating system;data mining;semantic web stack;database;semantics;cross-domain interoperability;world wide web	Visualization	-42.12331806857139	46.867790633155984	3873
57464fc9a7d6a38a49de09e5fd56c0d4068e7d17	secure network coding over the integers	vector space;standard model;network coding;bilinear map;signature scheme;public key;random oracle model	Network coding offers the potential to increase throughput and improve robustness without any centralized control. Unfortunately, network coding is highly susceptible to “pollution attacks” in which malicious nodes modify packets improperly so as to prevent message recovery at the recipient(s); such attacks cannot be prevented using standard endto-end cryptographic authentication because network coding mandates that intermediate nodes modify data packets in transit. Specialized “network coding signatures” addressing this problem have been developed in recent years using homomorphic hashing and homomorphic signatures. We contribute to this area in several ways: – We show the first homomorphic signature scheme based on the RSA assumption (in the random oracle model). – We give a homomorphic hashing scheme that is more efficient than existing schemes, and which leads to network coding signatures based on the hardness of factoring (in the standard model). – We describe variants of existing schemes that reduce the communication overhead for moderate-size networks, and improve computational efficiency (in some cases quite dramatically – e.g., we achieve a 20-fold speedup in signature generation at intermediate nodes). Underlying our techniques is a modified approach to random linear network coding where instead of working in a vector space over a field, we work in a module over the integers (with small coefficients).	antivirus software;authentication;centralized computing;coefficient;computation;cryptography;digital signature;homomorphic encryption;integer factorization;linear network coding;malware;network packet;overhead (computing);random oracle;speedup;throughput	Rosario Gennaro;Jonathan Katz;Hugo Krawczyk;Tal Rabin	2009		10.1007/978-3-642-13013-7_9	random oracle;standard model;linear network coding;bilinear map;vector space;computer science;theoretical computer science;mathematics;distributed computing;public-key cryptography;computer security;algorithm;algebra	Crypto	-50.966865928843255	78.27232376034888	3874
27a4d481d189e3d93da7089107492f3b233f12fb	making the world (of communications) a different place	buffer size;research agenda;tcp;congestion control;all optical routers	How might the computing and communications world be materially different in 10 to 15 years, and how might we define a research agenda that would get us to that world?		David D. Clark;Craig Partridge;Robert Braden;Bruce S. Davie;Sally Floyd;Van Jacobson;Dina Katabi;Greg Minshall;K. K. Ramakrishnan;Timothy Roscoe;Ion Stoica;John Wroclawski;Lixia Zhang	2005	Computer Communication Review	10.1145/1070873.1070887	simulation;telecommunications;computer science;transmission control protocol;network congestion;computer network	HCI	-9.212544110815967	92.8964518881263	3878
24b8fd2afefc6f348b9c7aaeef1ce76c6c67c8e0	case analysis of imitative innovation in chinese manufacturing smes: products, features, barriers and competences for transition	organizational learning;manufacturing smes;novelty;china;imitative innovation	Instead of viewing imitation and innovation as two opposite extremes, this research views firms' new product development as a continuous spectrum in which pure imitation at the one end and original innovation at the other. Firms change their position gradually by means of continuous organizational learning and systematic improvement in R&D capability during the imitative innovation process. Novelty and originality of innovations were increased gradually, and finally firms are able to carry out original innovations with good novelty. This case study investigates how the Chinese manufacturing SMEs go through this process. Drawing upon a multiple case study approach, this research in particular addresses the following questions: How do Chinese firms transit from pure imitation to original innovation through imitative innovation? What barriers may firms encounter in each stage of the transition? What competences do firms need to develop in order to make the transition successfully? Through appropriate Chinese manufacturing SMEs design they may improve their innovation capability to enable support to governmental policy making.	barrier (computer science);chinese wall	Xiubao Yu;Jie Yan;Dimitris Assimakopoulos	2015	Int J. Information Management	10.1016/j.ijinfomgt.2015.03.003	organizational learning;economics;computer science;knowledge management;marketing;operations management;management;china	AI	-78.31299057026425	4.212966418579521	3881
12751d53b1e9dda42010629b0c311749afe45401	samaritancloud: secure and scalable infrastructure for enabling location-based services	encryption servers privacy protocols additives social network services;social networking online cloud computing cryptography mobile computing;samaritancloud additive homorphic encryption geographically dispersed personal computing devices scalable infrastructure osn infrastructures online communities online social networks location based services;cryptography;social networking online;homorphic encryption location based service mobile devices privacy;mobile computing;cloud computing	With the maturation of online social networks (OSNs), people have begun to form online communities and look for assistance at a particular place and time from people they only know virtually. However, seeking for such help on existing OSN infrastructures has some disadvantages including loss of privacy (in terms of both location as well as the nature of the help sought) and high response times. In this paper we propose SamaritanCloud, a scalable infrastructure that enables a group of mobile and geographically-dispersed personal computing devices to form a cloud for the purpose of privately sharing relevant locality-specific information. From a technical standpoint our main contribution is to show that only additive homorphic encryption is sufficient to compute nearness in a cryptographically secure and scalable way in a distributed setting. This allows us to harness the benefit of linear match time while guaranteeing the locational privacy of the clients. In terms of performance our system compares favorably with simpler publish/subscribe schemes that support only equality match. We demonstrate the practical feasibility of SamaritanCloud with an experimental evaluation.	encryption;global positioning system;homomorphic encryption;hotspot (wi-fi);java hotspot virtual machine;locality of reference;location-based service;mobile app;online community;personal computer;procedural generation;publish–subscribe pattern;scalability;server (computing);smartphone;social network;utility functions on indivisible goods;ios	Abhishek Samanta;Fangfei Zhou;Ravi Sundaram	2013	2013 IFIP Networking Conference		computer science;internet privacy;world wide web;computer security	Mobile	-40.89359060165036	59.845389243176534	3894
01c3a339665ba84d88b780b17617a86d6fad7262	trade-off between automated and manual software testing		The study explores the current state of test automation in software testing organizations by focusing on the views and observations of managers, testers and developers in each organization. The case study was conducted in selected organizational units that develop and test technical software for industrial automation or telecommunication domains. The data was collected with 41 theme-based interviews in each unit. The interview data was analyzed qualitatively by using the grounded theory research method. It was found that although test automation was viewed as beneficial, it was not utilized widely in the companies. The main benefits of test automation were quality improvement, the possibility to execute more tests in less time and fluent reuse of testware. The major disadvantages were the costs associated with developing test automation especially in dynamic customized environments. Such issues as properties of tested products, attitudes of employees, resource limitations, and customers influenced the level of test automation in the case organizations.	cem kaner;cloud computing;design rationale;requirement;software business;software industry;software project management;software testing;test automation;test case;testware	Ossi Taipale;Jussi Kasurinen;Katja Karhu;Kari Smolander	2011	Int. J. Systems Assurance Engineering and Management	10.1007/s13198-011-0065-6	reliability engineering;systems engineering;engineering;operations management;software engineering;management;testware	SE	-69.12090542063353	21.363736425874315	3897
92f6d4a31d4811e6f2600495a8061fa0d3370a5d	pnml-notierte objektpetrinetze zur integration von prozess- und organisationsmodellen				Thomas Theling;Kamyar Sarshar;Peter Loos;Mirko Jerrentrup	2005				Vision	-97.59681416758094	23.37023415382639	3900
ef8584aeca2b196b0dc702b54054de2de97e5e75	mobi-coswac: an access control approach for collaborative scientific workflow in mobile environment	collaborative scientific workflow;role based access control;mobile environment;role evolution	With the development of mobile technology and popularity of pervasive applications, more and more scientific collaborative research works are carried out in the wild or on the move. How to make an optimal tradeoff between deep collaboration and strict access control in mobile environments is a challenging work. In this paper, we propose Mobi-CoSWAC, an access control approach for collaborative scientific workflow in mobile environments. In our approach, ranked access permissions will be provided to users for continuous collaboration in disconnected settings, as well as the owned access permissions can be dynamically assigned according to the role's evolution in various mobile contexts. The model and access control algorithms of Mobi-CoSWAC are elaborated and a prototype is implemented on Android mobile devices in collaborative proteomics research platform CoPExplorer to demonstrate its effectiveness.	access control	Zhaocan Chen;Tun Lu;Tiejiang Liu;Ning Gu	2012		10.1007/978-3-642-37015-1_12	mobile search;computer science;knowledge management;role-based access control;distributed computing;mobile computing;world wide web;computer security	HPC	-39.642131440847336	52.7191663886676	3906
56729a7fa3eb89cadd0d3635e0197a8a2269608f	a system dynamics approach to the modelling of complex natural systems			system dynamics	Ljiljana Nestan;Vesna Dunak	2011		10.7148/2011-0115-0121	human dynamics	Robotics	-54.62670178408369	7.675905695738766	3917
237989a99ad1360df09e7d743f24f140efe7f47d	value-mapping it platform options in global health: a multi-year case of code rot versus the event-driven outbreak economy	options value mapping;technological innovation;surveillance;training;maintenance engineering;investment;it platform assessments value mapping it platform code rot event driven outbreak economy multiyear action research case study global public health organization open source online it platform fichman framework;online collaboration community development;investment public healthcare organizations technological innovation training maintenance engineering surveillance;options value mapping online collaboration community development case study action research;organizations;action research;public healthcare;internet health care	This multi-year action research case study examines a global public health organization's efforts at innovation enabled by investments into an open source online IT platform. The case illustrates a core managerial challenge of balancing the tension between on-going maintenance and event-driven investments in an organization with few technology human resources internally. Using Fichman's (2004) framework for assessing real options of IT platform assessments, we identify the specific sources of value from such an investment in this organization, drawing out research and practice implications.	event-driven programming;link rot;open-source software;software rot	Dominic Thomas;Renee Subramanian	2016	2016 49th Hawaii International Conference on System Sciences (HICSS)	10.1109/HICSS.2016.384	maintenance engineering;investment;knowledge management;environmental resource management;marketing;operations management;software engineering;action research;management	Robotics	-79.1211724824123	5.486015144446821	3918
33b7ee84455dc9e0c9af29bc565f13ca53bf1be7	learning to classify parallel input/output access patterns	feedforward neural network;parallel file systems;learning algorithms;feedforward neural networks;learning algorithm;yarn;neural networks;hidden markov model;pattern classification hidden markov models file organisation learning artificial intelligence feedforward neural nets;input output;probabilistic model;interleaved codes;hidden markov models;adaptive systems;file system;pattern matching;probabilistic model parallel input output access patterns parallel file systems pattern classification learning algorithms feedforward neural network qualitative classifications hidden markov models;qualitative classifications;parallel input output access patterns;parallel file system;adaptive policies;pattern classification;pattern recognition;parallel i o;feedforward neural nets;file systems pattern classification hidden markov models pattern matching adaptive systems neural networks feedforward neural networks pattern recognition interleaved codes;learning artificial intelligence;access pattern classification;parallel applications;file systems;file organisation	Input/output performance on current parallel file systems is sensitive to a good match of application access patterns to file system capabilities. Automatic input/output access pattern classification can determine application access patterns at execution time, guiding adaptive file system policies. In this paper, we examine and compare two novel input/output access pattern classification methods based on learning algorithms. The first approach uses a feedforward neural network previously trained on access pattern benchmarks to generate qualitative classifications. The second approach uses hidden Markov models trained on access patterns from previous executions to create a probabilistic model of input/output accesses. In a parallel application, access patterns can be recognized at the level of each local thread or as the global interleaving of all application threads. Classification of patterns at both levels is important for parallel file system performance; we propose a method for forming global classifications from local classifications. We present results from parallel and sequential benchmarks and applications that demonstrate the viability of this approach.	input/output	Tara M. Madhyastha;Daniel A. Reed	2002	IEEE Trans. Parallel Distrib. Syst.	10.1109/TPDS.2002.1028437	feedforward neural network;computer science;theoretical computer science;machine learning;data mining;distributed computing;hidden markov model	Arch	-13.023330758168226	57.528435046166315	3920
74a6eb9068a1c3b53a52bfcbc530bebf9e89fcce	improving ultra-wideband positioning security using a pseudo-random turnaround delay protocol	pseudo random turnaround delay;location;ranging;ultra wideband uwb;security	Ultra-wideband (UWB) technology is very suitable for indoor wireless localization and ranging. IEEE 802.15.4a is the ̄rst physical layer standard speci ̄cally developed for wireless ranging and positioning. While malicious devices are not typically present, snoopers, impostors and jammers can exist. The data link and network layers in standards such as Wi-Fi, IEEE 802.15.4 and 802.11 mainly provide authentication and encryption support, but security about ranging or location is rarely considered. Ranging can be achieved using just the preamble and start of frame delimiter (SFD), so in this case malicious devices can easily obtain position information. Therefore, the security of ranging or positioning protocols is very important, which di®ers from the case with data exchange protocols. To provide secure location services, a protocol is presented which is based on a pseudo-random turnaround delay. In this protocol, devices use di®erent turnaround times so that it is di±cult for a snooper to ̄gure out the location of sensor devices in protected areas. At the same time, in the period of Hello frame transmission, together with the authentication mechanism of IEEE 802.15.4, an impostor cannot easily engages its deception attack.	authentication;delimiter;encryption;pseudorandomness;radio jamming;ultra-wideband	Xue-rong Cui;Juan Li;Hao Zhang;T. Aaron Gulliver;Chunlei Wu	2015	Journal of Circuits, Systems, and Computers	10.1142/S0218126615501492	embedded system;ranging;telecommunications;computer science;engineering;information security;location;computer security;computer network	Mobile	-52.88164150719953	70.51347470508028	3923
d2208f5fb5a5e556d2e2363325d226c90f6337f4	development of service performance indicators for operations management in airline	flight operations management;airline business;performance indicators;performance measurement	Reliable and efficient operations are essential for successful service business, and Performance Indicators (PIs) are useful tools for assessing appropriateness of service operations and providing cues to remedy flawed performance. Performance indicators should be based on objective data on operation performance, derivable by concrete and simple calculation rules, and exhaustively related to business goals. Development of such PIs is not an easy task, and this work tries to propose a framework for developing PIs using an application example of operations management in an airline. The proposed scheme of development is so general that it is applicable also to services other than airline business.		Toru Gengo;Kazuo Furuta;Taro Kanno;Katsuya Fukumoto	2010		10.1007/978-3-642-25655-4_25	simulation;operations management;transport engineering;business	OS	-78.76234027988674	10.443160830682833	3925
18a5a3241c1c8c7df84aa98aeda15866b71226db	translucent cryptography—an alternative to key escrow, and its implementation via fractional oblivious transfer	national security;protection information;communication process;communications policy;encryption;oblivious transfer;securite;translucence;implementation;translucent;cle publique;transmission message;key words key escrow;logarithme discret;discrete logarithm;message transmission;proceso comunicacion;ejecucion;processus communication;public key;cryptage el gamal;cryptage;proteccion informacion;discret logarithm;criptografia;cryptography;discrete logarithms;information protection;safety;llave publica;translucidez;cryptographie;seguridad;translucidite;securite nationale;transmision mensaje	We present an alternative to the controversial ``key-escrow'' techniques for enabling law enforcement and national security access to encrypted communications. Our proposal allows such access with probability p for each message, for a parameter p between 0 and 1 to be chosen (say, by Congress) to provide an appropriate balance between concerns for individual privacy, on the one hand, and the need for such access by law enforcement and national security, on the other. (For example, with p=0.4 , a law-enforcement agency conducting an authorized wiretap which records 100 encrypted conversations would expect to be able to decrypt (approximately) 40 of these conversations; the agency would not be able to decrypt the remaining 60 conversations at all.) Our scheme is remarkably simple to implement, as it requires no prior escrowing of keys. We implement translucent cryptography based on noninteractive oblivious transfer. Extending the schemes of Bellare and Micali [2], who showed how to transfer a message with probability ½, we provide schemes for noninteractive fractional oblivious transfer, which allow a message to be transmitted with any given probability p . Our protocol is based on the Diffie—Hellman assumption and uses just one El Gamal encryption (two exponentiations), regardless of the value of the transfer probability p . This makes the implementation of translucent cryptography competitive, in efficiency of encryption, with current suggestions for software key escrow.	authorization;cryptography;encryption;interactivity;key escrow;mihir bellare;oblivious transfer;privacy	Mihir Bellare;Ronald L. Rivest	1999	Journal of Cryptology	10.1007/PL00003819	discrete logarithm;key escrow;telecommunications;computer science;cryptography;national security;oblivious transfer;internet privacy;implementation;computer security;encryption	Crypto	-42.51417198724376	77.56495200582995	3929
9222a0edd047349d3c97a8fd67404a03b8efb305	a generalized partition refinement algorithm, instantiated to language equivalence checking for weighted automata		We present a generic algorithm, generalizing partition refinement, for deciding behavioural equivalences for various types of transition systems. In order to achieve this generality, we work with coalgebra, which offers a general framework for modelling different types of state-based systems. The underlying idea of the algorithm is to work on the so-called final chain and to factor out redundant information. If the algorithm terminates, the result of the construction is a representative of the given coalgebra that is not necessarily unique and that allows to precisely answer questions about behavioural equivalence. We instantiate the algorithm to the particularly interesting case of weighted automata over semirings in order to obtain a procedure for checking language equivalence for a large number of semirings. We use fuzzy automata with weights from an l-monoid as a case study.	algorithm;automata theory;automaton;finite-state transducer;formal equivalence checking;partition refinement;refinement (computing);turing completeness	Barbara König;Sebastian Küpper	2018	Soft Comput.	10.1007/s00500-016-2363-z	combinatorics;discrete mathematics;mathematics;algorithm	Logic	-5.478575008038172	17.573456509270287	3930
2dc7f7d5331df07b3cf6e8a6197da708b6b6502a	teaching embedded systems the berkeley way	capstone project;student experience;iron;real time operating system;cyber physical systems;embedded system;operating system;embedded system design;levels of abstraction;system design;critical thinking;laboratory experiment;model based design;embedded software	This paper describes an approach to teaching embedded systems from the perspective of cyber-physical systems. We place less emphasis on the mechanics of embedded system design and more on critical thinking about design technologies and on how the design of embedded software affects the behavior, safety, and reliability of cyber-physical systems. The course gives students experience with three distinct levels of design of embedded software, namely bare-iron programming (software that executes in the absence of an operating system), programming within a real-time operating system, and model-based design. In each case, students are taught to think critically about the technology, to probe deeply the mechanisms and abstractions that are provided, and to understand the consequences of chosen abstractions on overall system design. This paper describes a laboratory experience that first exposes students to the three levels of abstraction through a structured sequence of exercises, followed by an open-ended capstone project. Several example projects are described.	capstone (cryptography);cyber-physical system;embedded software;embedded system;nonlinear gameplay;principle of abstraction;real-time clock;real-time operating system;systems design	Edward A. Lee;Sanjit A. Seshia;Jeff C. Jensen	2012		10.1145/2530544.2530545	simulation;computer science;systems engineering;computer engineering	Embedded	-53.571354120409985	5.483397476332138	3935
3e07b55c4651b2c56f5466eaf9db0770cd809e22	linking to linguistic data categories in isocat		ISO Technical Committee 37, Terminology and other language and content resources, established an ISO 12620:2009 based Data Category Registry (DCR), called ISOcat (see http://www.isocat.org), to foster semantic interoperability of linguistic resources. However, this goal can only be met if the data categories are reused by a wide variety of linguistic resource types. A resource indicates its usage of data categories by linking to them. The small DC Reference XML vocabulary is used to embed links to data categories in XML documents. The link is established by an URI, which servers as the Persistent IDentifier (PID) of a data category. This paper discusses the efforts to mimic the same approach for RDF-based resources. It also introduces the RDF quad store based Relation Registry RELcat, which enables ontological relationships between data categories not supported by ISOcat and thus adds an extra level of linguistic knowledge.	data model;enterprise resource planning;iso 12620;linked data;linker (computing);named graph;pid;persistent identifier;resource description framework;semantic interoperability;unified modeling language;uniform resource identifier;vocabulary;xml	Menzo Windhouwer;Sue Ellen Wright	2012		10.1007/978-3-642-28249-2_10	computer science;data mining;database;world wide web	NLP	-41.09844029899814	4.8659942172338635	3948
0d4c10bc7f90413b432d2ec769ef1819369666c0	real-time emulation of intrusion victim in honeyfarm	content management;emulateur;red www;securite;real time;reseau web;gestion contenido;vulnerability;vulnerabilite;empreinte digitale;vulnerabilidad;internet;temps reel;safety;gestion contenu;fingerprint;tiempo real;world wide web;huella digital;emulador;information system;delinquency;seguridad;delinquance;systeme information;emulator;sistema informacion;delincuencia	Security becomes increasingly important. However, existing security tools, almost all defensive, have many vulnerabilities which are hard to overcome because of the lack of information about hackers techniques or powerful tools to distinguish malicious traffic from the huge volume of production traffic. Although honeypots mainly aim at collecting information about hackers’ behaviors, they are not very effective in that honeypot implementers tend to block or limit hackers’ outbound connections to avoid harming non-honeypot systems, thus making honeypots easy to be fingerprinted. Additionally, the main concern is that if hackers were allowed outbound connections, they may attack the actual servers thus the honeypot could become a facilitator of the hacking crime. In this paper we present a new method to real-time emulate intrusion victims in a honeyfarm. When hackers request outbound connections, they are redirected to the intrusion victims which emulate the real targets. This method provides hackers with a less suspicious environment and reduces the risk of harming other systems.	emulator;fingerprint;firewall (computing);honeypot (computing);ieee systems, man, and cybernetics society;internet security;link prefetching;octal;real-time clock;real-time transcription;www;web cache;web content;world wide web	Xing-Yun He;Kwok-Yan Lam;Siu Leung Chung;Chi-Hung Chi;Jia-Guang Sun	2004		10.1007/978-3-540-30483-8_18	fingerprint;juvenile delinquency;the internet;simulation;content management;vulnerability;computer science;artificial intelligence;operating system;database;distributed computing;world wide web;computer security;information system	Security	-54.77815563453794	65.0829969327449	3956
5830e78a5325020ce6aa76f0e5dc85739406a466	security analysis of a server-aided rsa key generation protocol	hand held device;distributed system;dispositivo potencia;systeme reparti;security analysis;informatique mobile;protocole transmission;securite informatique;dispositif puissance;useful information;informacion util;cryptage rsa;rsa ciphering;computer security;protocolo transmision;sistema repartido;low power;cifrado rsa;seguridad informatica;puissance faible;power device;handheld device;information system;mobile computing;systeme information;information utile;sistema informacion;potencia debil;transmission protocol	Modadugu, Boneh and Kim proposed two RSA key generation protocols (MBK Protocols) to generate the RSA keys efficiently on a low-power handheld device with the help of the untrusted servers, and the servers do not get any useful information about the keys they helped generation. The security of MBK Protocols is based on the assumption that the two servers are unable to share information with each other. To resists a ”collusion attack” ,namely the attack which the two servers collude to share information in MBK Protocols, Chen et al. proposed two improved protocols and claimed that their protocols are secure against such collusion attack. This paper shows that Chen et al.'s standard RSA key generation protocol cannot resist collusion attack and then cannot be used in practice.		Tianjie Cao;Xianping Mao;Dongdai Lin	2006		10.1007/11689522_29	embedded system;telecommunications;computer science;mobile device;security analysis;mobile computing;computer security;information system	Crypto	-44.530585052092725	77.23862315792694	3962
b3b63b5d64798c345a3e59f17dc1156dd3bc17ec	a theory of parameterized pattern matching: algorithms and applications	software maintenance;efficient algorithm;software systems;pattern matching;data structure	This paper develops a theory and algoritbrns for an application problem arising in software maintenance. The application is to track down duplication in a large software system. We want to find not only exact matches between sections of code, but parametrized matches, where a parametrized match between two sections of code means that one section can be transformed into the other by replacing the parameter names (e.g. identifiers and constants) of one section by the parameter names of the other via a one-to-one function. This paper formalizes this problem in terms of parametrized strings and parametrized pattern matching and detirtes a new data structure (parametrized sujjfi.x tree) suitable for parametrized pattern matching. It gives efficient algorithms for constructing this data structure, efficient algorithms for parametrized pattern matchmg, and an efficient algorithm for timing all maximal parametrized matches over a threshold length in a parametrized string. The algorithms for constructing parametrized suffix trees and for reporting duplication over a threshold length have been implemented. Tests on C code indicate that these algorithms should perform well in the application.	algorithm;data structure;identifier;maximal set;one-to-one (data model);pattern matching;software maintenance;software system	Brenda S. Baker	1993		10.1145/167088.167115	state pattern;data structure;computer science;3-dimensional matching;strategy pattern;theoretical computer science;pattern matching;distributed computing;programming language;software maintenance;software system	Theory	-25.686558133656987	11.91909090868851	3964
491809aa5c65a608156baaf7e4ae319fca609366	viewmap: sharing private in-vehicle dashcam videos.		Today, search for dashcam video evidences is conducted manually and its procedure does not guarantee privacy. In this paper, we motivate, design, and implement ViewMap, an automated public service system that enables sharing of private dashcam videos under anonymity. ViewMap takes a profile-based approach where each video is represented in a compact form called a view profile (VP), and the anonymized VPs are treated as entities for search, verification, and reward instead of their owners. ViewMap exploits the line-of-sight (LOS) properties of dedicated short-range communications (DSRC) such that each vehicle makes VP links with nearby ones that share the same sight while driving. ViewMap uses such LOS-based VP links to build a map of visibility around a given incident, and identifies VPs whose videos are worth reviewing. Original videos are never transmitted unless they are verified to be taken near the incident and anonymously solicited. ViewMap offers untraceable rewards for the provision of videos whose owners remain anonymous. We demonstrate the feasibility of ViewMap via field experiments on real roads using our DSRC testbeds and trace-driven simulations.	entity;experiment;line-of-sight (missile);simulation	Minho Kim;Jaemin Lim;Hyunwoo Yu;Kiyeon Kim;Younghoon Kim;Suk-Bok Lee	2017			multimedia;internet privacy;world wide web	Mobile	-45.89781045657524	60.67764481217311	3965
21e2431b4a7c01f6d30f5dbb3ea0739b548a43d7	modeling of a highly reliable real-time distributed system using the rto.k model and the monitor object	reliable real time distributed system;distributed system;rto k model;fault handling;kernel;reliability;object oriented methods;real time monitor object;dream kernel;real time systems power system modeling power system reliability kernel fault detection computerized monitoring computer science event detection moon industrial power systems;kodas;real time;reliable real time distributed system modelling;distributed processing;abstract software component;real time monitoring;system monitoring;event detection;distribution automation system;computerized monitoring;fault tolerant computing;reliability distributed processing real time systems object oriented methods system monitoring fault tolerant computing;lg industrial systems co ltd;moon;fault detection;structuring schemes;software component;dream kernel reliable real time distributed system modelling rto k model real time monitor object reliable operations abstract software component fault handling structuring schemes kodas korea distribution automation system lg industrial systems co ltd simulation model;power system reliability;computer science;power system modeling;reliable operations;modeling;simulation model;real time object;korea distribution automation system;industrial power systems;monitor;object model;real time systems	The main focus of this paper is the modeling of a highly reliable real-time distributed system using the RTO.k (Real-Time Object) model incorporating the monitor approach. The RTO.k model is used to specify the real-time distributed nature of the system, while the monitor is used to support reliable operations of the system. A monitor is an abstract software component to handle faults occurring during the system's operation. It can be modeled as an RTO.k object called a real-time monitor object (RTMO). In this paper, we present three basic structuring schemes with an RTO.k object and an RTMO. Our modeling approach utilizes these structuring schemes. To illustrate our approach, KODAS (KOrea Distribution Automation System), being developed at LG Industrial Systems Co. Ltd., is used. In order to examine the effectiveness of our approach, we have designed a simulation model for KODAS and implemented it on the DREAM kernel. The RTO.k model and the monitor approach can be easily incorporated, and can be integrated as a set of RTO.k objects so as to facilitate the modeling of a highly reliable real-time distributed system.	distributed computing;monitor (synchronization);real-time clock	Moon-hae Kim;Yong-Min Park;Seung-Min Yang;Jong-Kook Park	1997		10.1109/WORDS.1997.609925	embedded system;real-time computing;engineering;operating system	Robotics	-36.33552750901978	35.89555450868829	3967
7dafe57c1589db211fcb434ef5294f67de91deb5	an energy-efficient multisite offloading algorithm for mobile devices		Computation offloading is a popular approach for reducing energy consumption of mobile devices by offloading computation to remote servers. Most of the prior work focuses on a limited form of offloading part of computation from a mobile device to a single server. However, with the advent and development of cloud computing, it is more promising for the mobile device to reduce energy consumption by offloading part of computation to multiple remote servers/sites. This paper proposes an Energy-Efficient Multisite Offloading (EMSO) algorithm, which formulates the multiway partitioning problem as the 0-1 Integer Linear Programming (ILP) problem. Moreover, our proposed EMSO algorithm adopts the multi-way graph partitioning based algorithm to solve it. Experimental results demonstrate that our algorithm can significantly reduce more energy consumption as well as execution time and better adapt to the unreliability of wireless networks (such as the network bandwidth changes), compared with the existing algorithms.	algorithm	Ruifang Niu;Wenfang Song;Yong Liu	2013	IJDSN	10.1155/2013/518518	embedded system;real-time computing;computer science;theoretical computer science;distributed computing	EDA	-22.075604597734767	67.06543883030409	3971
ec1a3dda6b599c7817f6e06db991daad463c4521	feasibility analysis of data transmission in sdn			software-defined networking	Shu Gong;Muhammad Kamran Siddiqui;Yi Luo;Wei Gao	2017	Journal of Intelligent and Fuzzy Systems	10.3233/JIFS-169366	machine learning;artificial intelligence;mathematics;data analysis;transmission (mechanics)	Robotics	-12.86529277214086	86.59023624632079	3974
9e8aeed2846a5d7a05f1078a718e005656b703bc	defining a test coverage criterion for model-level testing of fbd programs	logical function mutation errors;test coverage criteria;data flow programs;ccc;fbd;mc dc	Context: The Programmable Logic Controller (PLC) is being integrated into the automation and control of computer systems in safety-critical domains at an increasing rate. Thoroughly testing such software to ensure safety is crucial. Function Block Diagram (FBD) is a popular data-flow programming language for PLC. Current practice often involves translating an FBD program into an equivalent C program for testing. Little research has been conducted on coverage of direct testing a data-flow program, such as an FBD program, at the model level. There are no commonly accepted structural test coverage criteria for data-flow programs. The objective of this study is to develop effective structural test coverage criterion for testing model-level FBD programs. The proposed testing scheme can be used to detect mutation errors at the logical function level. Objective: The purpose of this study is to design a new test coverage criterion that can directly test FBD programs and effectively detect logical function mutation errors. Method: A complete test set for each function and function block in an FBD program are defined. Moreover, this method augments the data-flow path concept with a sensitivity check to avoid fault masking and effectively detect logical function mutation errors. Results: Preliminary experiments show that this test coverage criterion is comprehensive and effective for error detection. Conclusion: The proposed coverage criterion is general and can be applied to real cases to improve the quality of data-flow program design.	algorithm;dataflow;error detection and correction;fault coverage;fully buffered dimm;function-level programming;graphical user interface;level structure;model of computation;numerical analysis;programming language;sensor;sequential logic;software testing;subsumption architecture;test case;test set	Yi-Chen Wu;Chin-Feng Fan	2013	Information & Software Technology	10.1016/j.infsof.2013.07.003	modified condition/decision coverage;reliability engineering;real-time computing;computer science;algorithm	SE	-59.01508467586383	33.84297595893044	3975
53ea1d57961abec3dc804a2a548d6d21a1a96593	your friends are more powerful than you: efficient task offloading through social contacts	mobile social network task allocation traffic balancing;social networking online mobile communication mobile computing random processes;mobile communication resource management mobile computing social network services load management ad hoc networks load modeling;real trace driven simulation task offloading social contacts distributed task reassignment balanced task reassignment mobile social networks random walking model ball and bin theory intolerable time delay uneven task distribution pure random assignment top k friend selection	In this work, we investigate the distributed and balanced task reassignment in mobile social networks. Previous studies have shown the power of random choice in load balancing with random walking model. Inspired by the `2-choice' paradigm in `ball and bin' theory, we evaluate this simple but effective scheme with real trace data `MobiClique'. According to the preliminary evaluation results, we find that, social relationship significantly differs from pure random walk model, and will bring challenges in task reassignment in the followings: First, friendships are relatively stable, which will lead to imbalanced task assignment. Second, some users meet quite infrequently, which will lead to intolerable time delay and uneven task distribution. In tackling with these challenges, we propose `iTop-K', leveraging the basic concept, i.e., your friends are more powerful than you, which encourages mobile users to assign tasks among intimate friends instead of pure random assignment. With the selection of `top-K' friends, we can achieve load balancing and guaranteed network performance at the same time. Experimental studies verify our scheme and show the effectiveness. In typical working scenario, where real-trace driven simulation is applied, ours outperforms the conventional random choice up to 15×, and the social relationship assignment without priority method up to 9×.	broadcast delay;load balancing (computing);network performance;programming paradigm;simulation;social network;turing degree	Qingyu Li;Panlong Yang;Yubo Yan;Yue Tao	2014	2014 IEEE International Conference on Communications (ICC)	10.1109/ICC.2014.6883300	real-time computing;simulation;computer science;distributed computing;mobile computing;computer network	DB	-13.588627889055164	66.55923985452962	3978
8cee2fd93d844ec70773243d9978c84ecc5e4e01	improved updating in relational data base systems by deuter-sphere algorithms	deuter sphere;relational data;data independency;deuter criterion;deuter disc;individual ict;partial pct;functional dependency;insertion deletion rules;rectangular rule;content thesauri human hct;analytical method;normalization;normal form;views relational;user;updating	The need to ease the user's handling of data banks concerning natural insertions and deletions, requires more data independencies. A fundamental trend to 'normalize' relations in first and second normal forms has been indicated by Codd and Kent. When it comes to the third normal form, obscurities are spreading. This touches the very moment, when adequate content relations of programming morphologies to content realities are missing. As Chamberlin, Gray and Traiger pointed out, a so called 'rectangle of a base relation' aids to clarify natural function dependencies. These indications have been further developed and advanced to a level, where each tuple and entity is subdivided into six deuter-criteria: 1. identity, 2. age, 3. association, 4. frequency, 5. significance and 6. truth. The validity of each criterion is related to all potential components within a relational data base. The metric data of each deuter criterion can be derived by analytical methods within three- and four dimensional realities in the model of a deuter-sphere. This leads to fundamental insertion- and deletion rules and an improved updating.		Peter Kümmel	1976			user;relational database;computer science;normalization;data mining;functional dependency;engineering drawing;algorithm	DB	-25.98239314312329	14.66734336039485	3982
afb254d0ee722e183e3e8b0e00fa67f87fbd5158	network in a box: facilitating problem-based learning through network emulation	network emulation;distributed applications;problem based learning	Problem-based learning (PBL) is a pedagogical method that challenges students to ‘‘learn how to learn’’ by working cooperatively in groups to develop solutions to real world problems. Network in a Box (NiB) is a unique, engineering-oriented effort to employ PBL in core computer networks curriculum. Developed as an openended exercise in PBL, NiB sets a learning environment in which student teams design and implement a network emulator using off-the-shelf hardware and open-source software. The aims of this article are twofold. The first goal of this article is to catalogue our efforts at San Diego State University in implementing PBL in a computer networks engineering course. The second goal of this article is to introduce the NiB project as a viable example of PBL in the field of computer networks. This project not only arms students with a good understanding of network impairments but also provides the knowledge of low level packet processing, packet creation, packet injection into the network, meeting real time processing requirements of packet processing and packet filtering. By arming students with an understanding of network impairments, a working knowledge of low-level packet processing, packet creation, packet injection, and their processing requirements, NiB serves as a model introduction of PBL into the field of computer networks engineering. In this article we disseminate the product specification and system design as an educational and research utility along with an assessment of this project as perceived by students over several semesters. 2009 Wiley Periodicals, Inc. Comput Appl Eng Educ 19: 433 446, 2011; View this article online at wileyonlinelibrary.com/journal/cae; DOI 10.1002/cae.20322	coat of arms;emulator;eng-tips forums;firewall (computing);high- and low-level;john d. wiley;network emulation;network packet;open-source software;real-time computing;requirement;specification language;systems design	Yusuf Öztürk	2011	Comp. Applic. in Engineering Education	10.1002/cae.20322	simulation;computer science;engineering;electrical engineering;artificial intelligence;world wide web;mechanical engineering	Networks	-53.5656047511102	5.301276747393753	3983
eea18e615e79e3f28d1d3441994c32666c7f21b6	empirical analysis of the impact of information technology on vertical integration	vertical integration;empirical analysis;information technology;transaction cost economics;information technology spending;interorganisational coordination;coordination costs	In 1981 the Energy Systems Program at the International Institute of Applied Systems Analysis (IlASA) published five scenarios of global energy demand and supply for the period 1975-2030. At least two of them, the high and low scenarios, are still quoted today. This paper analyses how accurately the IlASA scenarios of 1981 captured the actual development during the first 15 years of their time horizon. Those parts of the scenarios that refer to developments still in the future are also compared with current views of the long-term development of the global energy system as expressed in recent results collected by the International Energy Workshop (lEW). The comparisons show that the low scenario of 1981 carne closest to actual developments up to 1990. With the exception of nuclear energy, its further projections fall well within the range of today's global energy scenarios.	energy systems language	Namchul Shin	2002	IJSTM	10.1504/IJSTM.2002.001616	industrial organization;vertical integration;transaction cost;economics;marketing;operations management;management;information technology;commerce	Arch	-70.77964173447855	16.952580263876172	3984
4310b5b61ed81d162d3515fc5ee90de5201e39cf	managing change in software process improvement	mejoramiento procedimiento;developpement logiciel;software quality coordinate measuring machines quality management project management context financial management engineering management productivity investments software metrics;case report;project management;systemvetenskap informationssystem och informatik;information systems;change management;return on investment;software process improvement;software management;best practice;customer satisfaction;amelioration procede;desarrollo logicial;organizational change change management software process improvement;organizational change;software development;organizational change management software process improvement software quality software reliability customer satisfaction;management of change;gestion projet;process improvement;quality management software process improvement management of change software reliability software development management organisational aspects software quality;software reliability;software quality;software development management;gestion proyecto;quality management;organisational aspects	Software process improvement has become the primary approach to improving software quality and reliability, employee and customer satisfaction, and return on investment. Although the literature acknowledges that SPI implementation faces various problems, most published cases report success, detailing dramatic improvements. Such best-practice cases are a great benefit when learning how to effectively implement SPI. On the basis of experiences from SPI initiatives and insights into organizational-change management, we offer the following advice for successful SPI implementation: software managers must appreciate that each SPI initiative is unique and carefully negotiate the context of change. Managers must also understand the elements of change involved. SPI can't succeed without managerial commitment and a mastery of appropriate change tactics.	final fantasy tactics;software development process;software quality	Lars Mathiassen;Ojelanki K. Ngwenyama;Ivan Aaen	2005	IEEE Software	10.1109/MS.2005.159	reliability engineering;systems engineering;engineering;knowledge management;software engineering;change management;management;software quality	SE	-69.64823515454569	20.289657473468623	3990
ea983e3da428eae60f45deabe09cb23e2cb0fe5d	binary code retrofitting and hardening using sgx		Trusted Execution Environment (TEE) is designed to deliver a safe execution environment for software systems. Intel Software Guard Extensions (SGX) provides isolated memory regions (i.e., SGX enclaves) to protect code and data from adversaries in the untrusted world. While existing research has proposed techniques to execute entire executable files inside enclave instances by providing rich sets of OS facilities, one notable limitation of these techniques is the unavoidably large size of Trusted Computing Base (TCB), which can potentially break the principle of least privilege.  In this work, we describe techniques that provide practical and efficient protection of security sensitive code components in legacy binary code. Our technique dissects input binaries into multiple components which are further built into SGX enclave instances. We also leverage deliberately-designed binary editing techniques to retrofit the input binary code and preserve the original program semantics. Our tentative evaluations on hardening AES encryption and decryption procedures demonstrate the practicability and efficiency of the proposed technique.	binary code;binary file;cryptography;encryption;hardening (computing);intel developer zone;network enclave;operating system;principle of least privilege;semantics (computer science);software system;trusted computing base;trusted execution environment	Shuai Wang;Wenhao Wang;Qinkun Bao;Pei Wang;XiaoFeng Wang;Dinghao Wu	2017		10.1145/3141235.3141244	software system;binary code;real-time computing;trusted computing base;software;executable;principle of least privilege;software security assurance;hardening (computing);computer science	Security	-54.93139949436671	56.452630190813665	3996
093388219ab50f7b3395a9971384137ebc2ff1e1	self-healing bpel processes with dynamo and the jboss rule engine	context aware application;resource discovery;ambient intelligence;pervasive computing;web service;tuples;service oriented architecture;evolving tuples	"""Many emerging domains such as ambient intelligence, context-aware applications, and pervasive computing are embracing the assumption that their software applications will be deployed in an open-world. By adopting the Service Oriented Architecture paradigm, and in particular its Web service based implementation, they are capable of leveraging components that are remote and not under their jurisdication, i.e. services. However, the distributed nature of these systems, the presence of many stakeholders, and the fact that no one has a complete knowledge of the system preclude classic static verification techniques. The capability to """"self-heal"""" has become paramount.  In this paper we present our solution to self-healing BPEL compositions called Dynamo. It is an assertion-based solution, that provides special purpose languages (WSCoL and WSReL) for defining monitoring and recovery activities. These are executed using Dynamo, which consists of an AOP-extended version of the ActiveBPEL orchestration engine, and which leverages the JBoss Rule Engine to ensure self-healing capabilities. The approach is exemplified on a complex case study."""	ambient intelligence;business process execution language;business rules engine;exemplification;open world;open-world assumption;programming paradigm;separation of concerns;service-oriented architecture;software verification;ubiquitous computing;web service;wildfly	Luciano Baresi;Sam Guinea;Liliana Pasquale	2007		10.1145/1294904.1294906	real-time computing;computer science;database;world wide web	SE	-42.0852809065215	40.73362903292267	4001
aba51580c3ff8340021fc8bd9be94c235f0a9691	m-label: a naming scheme for services in future networks	internet;attribute future internet services naming scheme labels;internet scalability routing encoding registers content management educational institutions;m label naming scheme universal network content aware services management system content attribute information user friendly name revolutionary services future networks	Future networks will care more about the services and contents, and will support revolutionary services. Thus, naming scheme for services becomes a core technology for realizing future networks. This paper proposes a naming scheme for services in future networks, named by M-Label. M-Label has three features: it adopts user-friendly name; it describes the attribute information of content in the name from different dimensions; and it is unstructured, has a good scalability. Based on this naming scheme, we implement our content aware services management system and check the usability in the platform of Universal Network. The result shows that M-Label has a high scalability and can be used for a huge number of services or contents in future networks.	china internet network information center;mathematical optimization;scalability;usability	Wei Quan;Jianfeng Guan;Yuanlong Cao;Changqiao Xu;Hongke Zhang	2012	2012 IEEE 2nd International Conference on Cloud Computing and Intelligence Systems	10.1109/CCIS.2012.6664284	the internet;computer science;services computing;internet privacy;world wide web;computer security	DB	-39.64586334739922	57.42049763777783	4004
b826e54d98b696d049f4f5ca4befd90f398102db	multicriteria approach for process modelling in strategic environmental management planning	multicriteria decision making;anp;public participation;process modelling;environmental assessment	The objective of this paper is to propose a multicriteria methodological approach based on the Analytic Network Process methodology (ANP) in order to examine the scope and feasibility of a process modelling integrated with public participation for environmental assessment. In fact, environmental challenges decisions are often characterized by complexity, irreversibility and uncertainty. Much of the complexity arises from the multiple-use nature of goods and services, diffi culty in monetary valuation of ecological services and the involvement of numerous stakeholders. From this point of view multicriteria techniques and process modelling are considered as a promising framework to take into account confl ictual, multidimensional, incommensurable and uncertain effects of decisions explicitly. In particular the integration of ANP with tools for public participation and process modelling pose certain methodological challenges, but provide an innovative approach to designing the scope of the environmental assessment and defi ning and assessing alternatives.	complexity;ecosystem services;environmental resource management;process modeling;value (ethics)	Fabio De Felice;Antonella Petrillo	2013	IJSPM	10.1504/IJSPM.2013.055190	systems engineering;engineering;environmental resource management;process modeling;management science;environmental impact assessment	AI	-70.42093627580275	7.426271957828831	4030
82b5b60e105941219c22edd4451a009ab8f25eed	incremental file reorganization schemes	traveling salesman problem	For many files, reorganization is essential during their lifetime in order to maintain an adequate performance level for users. File reorganization can be defined as the process of changing the physical structure of the file. In this paper we are mainly concerned with changes in the placement of records of a file on pages in secondary storage. We model the problem of file reorganization in terms of a hypergraph and show that this problem is NP-hard. We present two heuristics which can be classified as incremental reorganization schemes. Both algorithms incorporate a heuristic for the traveling salesman problem. The objective of our approach is the minimization of the number of pages swapped in and out of the main memory buffer area during the reorganization process. Synthetic experiments have been performed to compare our heuristics with alternative strategies.	algorithm;auxiliary memory;computer data storage;data buffer;experiment;heuristic (computer science);np-hardness;travelling salesman problem	Edward Omiecinski	1985			parallel computing;computer science;theoretical computer science;database;distributed computing;travelling salesman problem	DB	-14.500113522471647	57.23408458600623	4032
07850f8b04c155a7b2c308a838df29b795b53a10	towards a data-oriented optimization of manufacturing processes - a real-time architecture for the order processing as a basis for data analytics methods		Real-time data analytics methods are key elements to overcome the currently rigid planning and improve manufacturing processes by analysing historical data, detecting patterns and deriving measures to counteract the issues. The key element to improve, assist and optimize the process flow builds a virtual representation of a product on the shop-floor called the digital twin or digital shadow. Using the collected data requires a high data quality, therefore measures to verify the correctness of the data are needed. Based on the described issues the paper presents a real-time reference architecture for the order processing. This reference architecture consists of different layers and integrates real-time data from different sources as well as measures to improve the data quality. Based on this reference architecture, deviations between plan data and feedback data can be measured in real-time and countermeasures to reschedule operations can be applied.	correctness (computer science);countermeasure (computer);data quality;data structure;digital footprint;geolocation;holism;mathematical optimization;missing data;precondition;program optimization;real-time clock;real-time computer graphics;real-time data;real-time transcription;reference architecture;sensor	Matthias Blum;Günther Schuh	2017		10.5220/0006326002570264	analytics;computer science;data science;data mining	Graphics	-65.65116219782416	12.284215602497063	4034
e3dab35426d490610101ddbfa63476aa6b757631	understanding the behavior of spark workloads from linux kernel parameters perspective	virtual memory;datacenters;parameters;spark;linux	Despite a number of innovative computer systems with high capacity memory have been built, the design principles behind an operating system kernel have remained unchanged for decades. We argue that kernel parameters is a kind of special interface of operating system and must be factored into the operation and maintenance of datacenters. To shed some light on the effectiveness of tuning Linux parameters of virtual memory subsystem when running Spark workloads, we evaluate the benchmarks in a simple standalone deploy mode. Our performance results reveal that some of the Linux memory parameters must be carefully set to efficiently support these processing workloads. We hope this work yields insights for datacenter system operators.	data center;kernel (operating system);linux;operating system;random-access memory;sysop	Li Wang;Tianni Xu;Jing Wang;Weigong Zhang;Xiufeng Sui;Yungang Bao	2016		10.1145/3007592.3007593	sysfs;embedded system;real-time computing;engineering;operating system	OS	-15.255595196141917	52.53830603716253	4035
9925e71e06ce18a122b2f44ae84264944e41b07d	optimierte varianten- und anforderungsabdeckung im test		Die in zunehmendem Maße wachsende Anzahl an Fahrzeugvarianten stellt eine große Herausforderung für einen Automobilhersteller dar. Zusätzlich spielen eingebettete E/E-Systeme eine immer wichtigere Rolle in der Realisierung der wettbewerbsfähigen Funktionsvarianten. Der hohe Vernetzungsgrad zwischen den Systemen sowie die hohe Varianz führen zu einer komplexen Kombinatorik und stellen somit höchste Ansprüche an den Entwicklungsprozess. Zur Bewältigung dieser Komplexität bedarf es methodischer Unterstützung. Insbesondere während der Phase der Absicherung ist ein systematisches Vorgehen zur Selektion von testrelevanten Varianten notwendig, um unnötig hohe Kostenund Zeitaufwände zu vermeiden. Im Beitrag wird hierfür ein Verfahren vorgeschlagen, welches basierend auf einer 100%igen Anforderungsüberdeckung eine minimale Auswahl an Varianten liefert, die abgesichert werden müssen. Zur Erfassung und Verwaltung der Variabilität wird der Ansatz der Produktlinienentwicklung genutzt, bei dem die Gemeinsamkeiten und Unterschiede eines Testobjektes innerhalb eines Merkmalmodells dargestellt und dann auf Anforderungsund Testartefakte gemappt werden. Auf Basis dieser Produktlinieninfrastruktur erfolgt dann die Selektion mittels eines Greedy-Algorithmus. Der folgende Beitrag stellt das Vorgehen detailliert vor und demonstriert das Verfahren anhand eines Fallbeispiels der Daimler AG.	die (integrated circuit);eine and zwei;gesellschaft für informatik;greedy algorithm;unified model;vhf omnidirectional range;zur farbenlehre	Anastasia Cmyrev;Ralf Reißing	2013			computer science	OS	-105.34951406921701	32.2587857630144	4037
e24cbaa238c0ea0d06f4f1454ce6d14b261915a9	new results on testing modularity of local supervisors using abstractions	scada systems discrete event systems;control structure;discrete event system local supervisors modular system testing system control structure supervisor reduction;modular system;discrete event systems;scada systems;explosions automata electrical equipment industry automation automatic testing system testing automatic control computer science terminology sufficient conditions	This paper presents a variation of the methodology, established in a previous paper, to test a modular system for nonconflict using abstractions of its supervisors. In this work, information about the model and control structure of the system are used to derive a solution where some of the conditions established before are not required. The local modular approach is used to design the supervisors and supervisor reduction techniques are also used to help defining the set of events that are kept in the abstractions.	control flow	Patrícia Nascimento Pena;José Eduardo Ribeiro Cury;Stéphane Lafortune	2006	2006 IEEE Conference on Emerging Technologies and Factory Automation	10.1109/ETFA.2006.355433	control engineering;reliability engineering;embedded system;real-time computing;computer science;engineering;programming language;control flow	Robotics	-5.933286288987132	28.551694916507767	4041
0cc51515fadc403cf45aa16e459a42401638a513	recap simulator: simulation of cloud/edge/fog computing scenarios		With the increasing trend towards edge and fog computing, the aim of the RECAP simulator is to simulate large scale scenarios in the cloud, fog and edge computing space in order to provide decision and control support for application and data center resource administration. This will be accomplished through the simulation of applications and application subsystems, simulation of infrastructure resources and resource management systems, and experimentation and validation of simulation results. The RECAP simulator and associated models will provide support for understanding and predicting impact on resources, workloads and quality of service (QoS) metrics as well as trade-offs for energy efficiency and cost within cloud, edge and fog computing scenarios, while maintaining the service level agreements (SLAs) of users.	cloud computing;data center;edge computing;fog computing;quality of service;service-level agreement;simulation	James Byrne;Sergej Svorobej;Anna Gourinovitch;Divyaa Manimaran Elango;Paul Liston;Peter J. Byrne;Theo Lynn	2017	2017 Winter Simulation Conference (WSC)	10.1109/WSC.2017.8248208	quality of service;computer science;simulation;resource management;efficient energy use;cloud computing;data center;service level;edge computing	HPC	-22.002102215704657	62.52232821706319	4046
5133475438899979c0ca8e2a375a6d2e3153060a	towards formal analysis of artifact-centric business process models	business process model;formal analysis	Business process (BP) modeling is a building block for desig n and management of business processes. Two fundamental aspects of BP mo deling are: a formal framework that well integrates both control flowanddata, and a set of tools to assist all aspects of a BP life cycle. A typical BP life cycle includes at least a design phase where the key concerns are around “correct” realization of busine ss logic in a resource constrained environment, and an operational phase where a main objective is to optimize and improve the realization during the execution (operation). This paper is an initial attempt to address both aspects of BP modeling. We view our investiga tion s a precursor to the development of a framework and tools that enable automated c onstruction of processes, along the lines of techniques developed around OWL-S and Sem antic Web Services. Over the last decade, the idea of coupling control and data in an “artifact-centric” approach emerged in the practice of business process design . The key focus is on the “moving” data as they are manipulated throughout a process. The idea works well with many applications, typical examples include customer orde r processing, insurance claim handling, etc. Based on this idea, we formulate a theoretica l model for artifact-centric business processes and develop complexity results concern i g static analysis of three problems of immediate practical concerns. The problems foc us n the ability to complete an execution, existence of an execution “deadend”, and redu ndancy. It is shown that the problems are undecidable in the general case; and under vari ous estrictions they are decidable but complete in PSPACE, co-NP, andNP; and in some cases decidable in linear time.	artifact-centric business process model;co-np;control flow;decision problem;diagram;owl-s;pspace;reachability;redundancy (engineering);semantic web service;time complexity	Kamal Bhattacharya;Cagdas Evren Gerede;Richard Hull;Rong Liu;Jianwen Su	2007		10.1007/978-3-540-75183-0_21	computer science;systems engineering;engineering;artifact-centric business process model;operations management;database;management;business process modeling;algorithm	PL	-55.009658147458126	19.99823990805325	4051
d46fd51293b197573cf3381e3f147c3c95f7ee8b	design and implementation of stetho: network sonificiation system				Masahiko Kimoto;Hiroyuki Ohno	2002			element management system;network planning and design;network architecture;open network architecture;network management station;system model;design technology	Networks	-20.54438749305346	86.93872032242069	4058
8a384e3ace208007eff9f31b131b144a623dd6fb	local scheduling in multi-agent systems: getting ready for safety-critical scenarios		Multi-Agent Systems (MAS) have been supporting the development of distributed systems performing decentralized thinking and reasoning, automated actions, and regulating component interactions in unpredictable and uncertain scenarios. Despite the scientific literature is plenty of innovative contributions about resource and tasks allocation, the agents still schedule their behaviors and tasks by employing traditional general-purpose scheduling algorithms. By doing so, MAS are unable to enforce the compliance with strict timing constraints. Thus, it is not possible to provide any guarantee about the system behavior in the worst-case scenario. Thereby, as they are, they cannot operate in safetycritical environments. This paper analyzes the agents’ local schedulers provided by the most relevant agent-based frameworks from a cyberphysical systems point of view. Moreover, it maps a set of agents’ behaviors on task models from the real-time literature. Finally, a practical case-study is provided to highlight how such “MAS reliability” can be achieved.	agent-based model;algorithm;distributed computing;entity;formal verification;general-purpose markup language;general-purpose modeling;hoc (programming language);interaction;java;map;multi-agent system;real-time clock;real-time computing;real-time locating system;real-time operating system;real-time transcription;scheduling (computing);scientific literature;worst-case complexity;worst-case scenario	Davide Calvaresi;Mauro Marinoni;Alicia Waring;Kevin Appoggetti;Paolo Sernani;Aldo Franco Dragoni;Michael Schumacher;Giorgio C. Buttazzo	2017		10.1007/978-3-030-01713-2_8	management science;scientific literature;scheduling (computing);cyber-physical system;multi-agent system;computer science	AI	-40.96234387883033	22.466408328199442	4060
900d1abbf5712cd1e488f65d5431bce146eda716	video quality management over the software defined networking	performance modelling and analysis;openflow;traffic intensity;software defined networking;quality of experience;multimedia communication;multimedia services	Dynamic Adaptive Streaming over HTTP (DASH) or MPEG-DASH is a popular technique that allows video quality adaptation for high quality streaming over the Internet. However, with bandwidth fluctuations, DASH performs poorly due to annoying frequent number of stalls. Software Defined Networking (SDN) has emerged as an attractive technology which has found its way into datacentres. Since one of the main goals of the SDN architecture is to make the network programmable and accelerate network innovation by utilizing its control plane, this paper has used the SDN control plane to propose a video quality management scheme based on the traffic intensity under DASH. Experimental results obtained by using Mininet and OpenDaylight controller have shown that the proposed scheme can significantly reduce the number of frequently annoying stalls and their duration by at least 84% and 94%, respectively. This has been achieved by switching network flows from high to low congested network paths within an SDN architecture.	control plane;display resolution;dynamic adaptive streaming over http;hypertext transfer protocol;internet;moving picture experts group;software-defined networking	Is-Haka Mkwawa;Alcardo Alex Barakabitze;Lingfen Sun	2016	2016 IEEE International Symposium on Multimedia (ISM)	10.1109/ISM.2016.0122	openflow;real-time computing;telecommunications;computer science;operating system;traffic intensity;distributed computing;software-defined networking;law;world wide web;computer network	Arch	-7.219985636904763	98.21737703868537	4064
ba98ef533a0ab864ea538c7aa3f29490938d9a28	effective and efficient visual description based on local binary patterns and gradient distribution for object recognition. (description visuelle efficace et efficace basée sur des modèles binaires locaux et une distribution en gradient pour la reconnaissance d'objets)			gradient;linear algebra;local binary patterns;outline of object recognition	Chao Zhu	2012				ML	-104.53520431507445	11.74091835256166	4065
09e09546559e7e6f9e6b4dc8277830b3c0cea513	towards multi-tenant cache management for isp networks	placement;bandwidth motion pictures servers routing topology educational institutions content distribution networks;virtualisation cache storage internet;ibcn;technology and engineering;real content request traces multitenant cache management approach isp networks virtualization technology internet service providers content delivery networks content producers cdn bandwidth utilization	The decreasing cost of storage and the advent of virtualization technology can allow Internet Service Providers (ISPs) to deploy multi-tenant caching infrastructures and lease them to content producers and Content Delivery Networks (CDNs). Serving content requests directly from the ISP network does not only reduce the delivery time, but also allows the ISP to optimize the network resources by controlling the placement and routing of content items. In this paper, we introduce a multi-tenant cache management approach that significantly reduces the bandwidth utilization of ISPs networks by pro-actively allocating caching space, leased by content producers and/or CDNs, and intelligently routing content to the end users. Using real content request traces, we show that the optimal solution to this problem can increase the cache hit ratio by 70.64% while reducing the bandwidth usage by 57.17% on average, compared to a commonly used reactive cache management scheme. These results provide a benchmark for the development of novel multi-tenant cache management strategies.	benchmark (computing);cache (computing);content delivery network;hit (internet);multitenancy;overhead (computing);place and route;routing;tracing (software);x86 virtualization	Maxim Claeys;Daphné Tuncer;Jeroen Famaey;Marinos Charalambides;Steven Latré;Filip De Turck;George Pavlou	2014	2014 European Conference on Networks and Communications (EuCNC)	10.1109/EuCNC.2014.6882692	real-time computing;cache;computer science;smart cache;cache algorithms;world wide web;computer network	HPC	-16.014634675157073	75.26876721174368	4071
6352a2a76c2b29af93a6705083ace1ff74cc92cf	formal derivation of an efficient parallel 2-d gauss-siedel method	multiprocessor interconnection networks;physical processors;parallel 2 d gauss seidel method;logical processors;iterative methods;perfect processor utilization algorithm mapping algorithm scheduling equational transformations processor mesh parallel 2 d gauss seidel method two dimensional mesh parallel architecture wave front scheduling logical processors physical processors;efficient implementation;partial differential equations;processor mesh;gaussian processes processor scheduling parallel architectures two dimensional displays scheduling algorithm filters computer science partitioning algorithms equations pipelines;wave front scheduling;equational transformations;parallel implementation;parallel architecture;perfect processor utilization;two dimensional mesh;algorithm scheduling;algorithm mapping;gauss seidel;partial differential equations finite difference methods iterative methods multiprocessor interconnection networks parallel algorithms;finite difference methods;parallel algorithms	Presents a formal derivation of a highly efficient parallel implementation of the 2-D Gauss-Seidel method for machines based on the two-dimensional mesh of processors. A methodology is illustrated which formalizes the process of mapping and scheduling a high level algorithm onto a particular target parallel architecture. It starts from a simple initial program. Equational transformations are then applied: to partition the abstract problem onto processors; to make communication among processors explicit; to pipeline the computation by wave-front scheduling; and finally to map logical processors onto physical processors for perfect processor utilization. All the derivation steps preserve equalities so that the derived programs are equivalent to the initial program. Using this methodology, the paper develops efficient implementations for other parallel architectures. >	gauss–seidel method	J. Allan Yang;Young-il Choo	1992		10.1109/IPPS.1992.223046	parallel computing;computer science;theoretical computer science;distributed computing	HPC	-10.372622270020418	39.059671475541634	4072
4a92eb824b58517ae473c9ffad6a7f0d7032f8c8	low-complexity reorder buffer architecture	power saving;real estate;issue queue;low complexity;satisfiability;chip;low complexity datapath;reorder buffer;low power;superscalar processor;low power design	In some of today's superscalar processors (e.g.the Pentium III), the result repositories are implemented as the Reorder Buffer (ROB) slots. In such designs, the ROB is a complex multi-ported structure that occupies a significant portion of the die area and dissipates a non-trivial fraction of the total chip power, as much as 27% according to some estimates. In addition, an access to such ROB typically takes more than one cycle, impacting the IPC adversely.We propose a low-complexity and low-power ROB design that exploits the fact that the bulk of the source operand values is obtained through data forwarding to the issue queue or through direct reads of the committed register values. Our ROB design uses an organization that completely eliminates the read ports needed to read out operand values for instruction issue. Any consequential performance degradation is countered by using a small number of associatively-addressed retention latches to hold the most recent set of values written into the ROB. The contents of the retention latches are used to satisfy the operand reads for issue that would otherwise have to be read from the ROB slots. Significant savings of the ROB real estate as well as power savings in the range of 20% to 30% for the ROB are achieved using the proposed technique. At the same time, the fact that results are accessible in a single cycle from the retention latches actually leads to an overall improvement in the IPC of up to 3% on the average for SPEC 2000 benchmarks.	benchmark (computing);central processing unit;data buffer;elegant degradation;low-power broadcasting;operand forwarding;re-order buffer;register renaming;superscalar processor	Gürhan Küçük;Dmitry V. Ponomarev;Kanad Ghose	2002		10.1145/514191.514202	chip;parallel computing;real-time computing;re-order buffer;computer hardware;computer science;operating system;real estate;satisfiability	HPC	-7.779790457062902	53.44916720199439	4076
4004b1fa124f516d818dd2eb2f9f5ff738bec75f	c2ms: dynamic monitoring and management of cloud infrastructures	pattern clustering;ganglia;cluster;resource allocation;software performance evaluation;ganglia monitoring management cloud computing grid cluster;grid;network servers;monitoring;software performance evaluation cloud computing network servers pattern clustering resource allocation;servers monitoring temperature measurement clouds temperature sensors educational institutions;c2ms customized monitoring metrics administrator specified actions large scale dynamic cloud infrastructures server reconfiguration open source scalable system performance monitoring tool ganglia virtual servers physical servers cloudlet control and management system large scale cloud infrastructures static configuration dynamically changing group monitoring server monitoring tools performance optimization load management server accessibility server clustering dynamic cloud infrastructure management dynamic cloud infrastructure monitoring;management;cloud computing	Server clustering is a common design principle employed by many organisations who require high availability, scalability and easier management of their infrastructure. Servers are typically clustered according to the service they provide whether it be the application(s) installed, the role of the server or server accessibility for example. In order to optimize performance, manage load and maintain availability, servers may migrate from one cluster group to another making it difficult for server monitoring tools to continuously monitor these dynamically changing groups. Server monitoring tools are usually statically configured and with any change of group membership requires manual reconfiguration, an unreasonable task to undertake on large-scale cloud infrastructures. In this paper we present the Cloudlet Control and Management System (C2MS), a system for monitoring and controlling dynamic groups of physical or virtual servers within cloud infrastructures. The C2MS extends Ganglia - an open source scalable system performance monitoring tool - by allowing system administrators to define, monitor and modify server groups without the need for server reconfiguration. In turn administrators can easily monitor group and individual server metrics on large-scale dynamic cloud infrastructures where roles of servers may change frequently. Furthermore, we complement group monitoring with a control element allowing administrator-specified actions to be performed over servers within service groups as well as introduce further customized monitoring metrics. This paper outlines the design, implementation and evaluation of the C2MS.	accessibility;amazon elastic compute cloud (ec2);central processing unit;cloud computing;cloudlet;cluster analysis;computer cluster;download;experiment;ganglia;hpc challenge benchmark;high availability;hoc (programming language);inversive congruential generator;management system;open-source software;overhead (computing);scalability;server (computing);system administrator;user interface	Gary A. McGilvary;Josep Rius Torrento;Iñigo Goiri;Francesc Solsona;Adam Barker;Malcolm P. Atkinson	2013	2013 IEEE 5th International Conference on Cloud Computing Technology and Science	10.1109/CloudCom.2013.45	real-time computing;cloud computing;resource allocation;computer science;operating system;distributed computing;grid;world wide web;client–server model;server;server farm;cluster	DB	-29.920376071338776	53.566013802726864	4086
c1b1ebec6506154b90fd9cf858584b2d2f746577	a document rules description language based on feature logic for xml document exchange	xml document	We propose a minimum Document Rules Description Language DRDL for XML validation and translation in web-based document exchange. DRDL has small syntax and simple semantics so that it can ease to reuse document rules. Furthermore, DRDL can describe mapping constraints from XML tree structure to table structure for web input-form generation. The technical feature of DRDL is to extend and reinterpret a Feature Logic, which has been used for representing linguistic knowledge, in order to allow existential and universal quantifiers over list-value to cope with XML. DRDL has already been applied to real systems such as elevator design support, Web-EDI, government-to-business and facility management.	e-governance;electronic data interchange;tree structure;universal quantification;web application;xml tree;xml validation	Makoto Imamura;Yasuhiro Takayama;Yasuhiro Okada;Norihisa Komoda	2007			well-formed document;xml catalog;natural language processing;xml validation;xml encryption;regular language description for xml;simple api for xml;xml;xml schema;streaming xml;computer science;document type definition;document definition markup language;document structure description;xml framework;document type declaration;xml schema;database;document schema definition languages;xml signature;xml schema editor;information retrieval;efficient xml interchange;design document listing	Web+IR	-37.00355129531692	7.498774809697433	4087
35f958ff655a3400655dea5eeaf38cafd2cf3da9	semantic web services, part 1	semantic web service;web services semantic web;semantic web mediation business robustness large scale systems distributed information systems international collaboration technology management costs concrete;selected works;service orientation;knowledge sharing semantic web services ontologies reasoning knowledge representation;semantic technologies;inference mechanisms;semantic technologies semantic web services technology research;web service;semantic web technology;ontologies artificial intelligence;web services inference mechanisms ontologies artificial intelligence semantic web;semantic web computer science resource description framework service oriented architecture history owl energy management web services myspace humans;composite services semantic web services semantic technologies sawsdl services service orientation service oriented architecture;sawsdl;semantic web services;composite services;technology research;web services;knowledge sharing;semantic web;world wide web;bepress;ontologies;services;reasoning;knowledge representation;service oriented architecture;service provision	Semantic Web services (SWS) has been a vigorous technology research area for about six years. A great deal of innovative work has been done, and a great deal remains. Several large research initiatives have been producing substantial bodies of technology, which are gradually maturing. SOA vendors are looking seriously at semantic technologies and have made initial commitments to supporting selected approaches. In the world of standards, numerous activities have reflected the strong interest in this work. Perhaps the most visible of these is Sawsdl (Weerawarana, 2005). Sawsdl recently achieved Recommendation status at the World Wide Web Consortium. Sawsdl's completion provides a fitting opportunity to reflect on the state of the art and practice in SWS - past, present, and future. This two-part installment of Trends & Controversies discusses what has been accomplished in SWS, what value SWS can ultimately provide, and where we can go from here to reap these technologies' benefits.	consortium;sawsdl;semantic web service;sinewave synthesis;world wide web	David L. Martin;John Domingue;Michael L. Brodie;Frank Leymann	2007	IEEE Intelligent Systems	10.1109/MIS.2007.94	web service;knowledge representation and reasoning;computer science;knowledge management;artificial intelligence;database;world wide web	HCI	-45.19565128231581	12.181448956760358	4092
03782743ed1a7293fc4095f9360f01d778926602	lan cabling design for the new bls building	telecommunication cables;spine;personal communication networks;bureau of labor statistics;pc workstations media access control level bridge lan cabling design local area network us bureau of labor statistics ethernet based central backbone segment primary distribution segments secondary distribution segments multiport repeaters;secondary distribution segments;bridges;pc workstations;primary distribution segments;telecommunication cables building wiring local area networks;workstations;statistics;central backbone segment;repeaters;multiport repeaters;us bureau of labor statistics;local area networks buildings power cables spine statistics bridges repeaters workstations ethernet networks personal communication networks;media access control level bridge;ethernet based;ethernet networks;power cables;building wiring;buildings;local area networks;local area network;lan cabling design	The third generation of the Cabling Architecture for the BLS LAN in its new building is described. For each architectural element the design is described and the major alternatives are identified, and for each alternative the major trade-offs are identified. The application of this architecture to the Bureau’s Regional Offices is discussed.		P. B. Stevens	1989		10.1109/LCN.1989.65248	local area network;telecommunications;computer science;backbone network;statistics;computer network	Arch	-20.850025856917807	91.74299152233002	4095
cbdae5aa16ef9fc3e539fb4e9a8af71620255285	über lineare und nichtlineare funktionale des weissen rauschens in kontinuierlicher zeit und deren anwendungen				Georg Budke	1988				Crypto	-100.70557416329211	26.86710181754432	4107
d7393bed5b4803a588c3ce3eb5b530e93475deb4	parallel bloom filter on xeon phi many-core processors	bloom filter;counting bloom filters;many core processor;bloom filters;xeon phi;cache coherence protocol;cache coherence protocols;memory latencies;many core processors;scalability;parallel processing;synchronization cost	Bloom filters are widely used in databases and network areas. These filters facilitate efficient membership checking with a low false positive ratio. It is a way to improve the throughput of bloom filter by parallel processing. Common many-core processors such as Xeon Phi can provide high parallelism. Thus, we build an iterative model to analyze memory access performance. This performance suggests that the bottleneck in the traditional design is mainly caused by synchronization cost and memory latency on many-core platforms. Therefore, we propose a parallel bloom filter PBF, which is a lockless method involving input data preprocessing. This method reduces synchronization overhead and improves cache locality. We also implement and evaluate PBF on a Xeon Phi processor. Results show that the memory access performance is three times better than that of the counting bloom filter. PBF provides improved scalability, and the speedup ratio can reach a maximum of 80.7x.	bloom filter;manycore processor;xeon phi	Sheng Ni;Rentong Guo;Xiaofei Liao;Hai Jin	2015		10.1007/978-3-319-27122-4_27	parallel processing;parallel computing;real-time computing;computer hardware;computer science;bloom filter;operating system;distributed computing;programming language	HPC	-12.948793262936844	50.631945034753855	4110
f0547b883a63ffbb7e2c82482922e954cb67ab68	pay-as-you-go: an adaptive approach to provide full context-aware text search over document content	xquery;context aware;information retrieval;data migration;xquery full text;semi structured data;pay as you go;tree index;keyword search;indexation;text index;xml;adaptive architecture;text indexing;structured data;xml index	RDBMS provides best performance for querying structured data that starts out with a well-defined schema. However, such a 'schema first, data later' approach does not work for unstructured data or data without much structure. Therefore, RDBMS typically stores such data without any schema in LOB columns (for example, Character Large Object (CLOB) or Binary Large Object (BLOB) columns) and provides Information-Retrieval (IR) style, keyword-based search capability over these LOB columns. Lately, XML as a native datatype (XMLType) in RDBMS has been introduced via the SQL/XML standard. Semi-structured data with or without any schema can be stored into such XMLType columns, and XQuery provides query capability over them. In particular, XQuery full text specification provides the capability of searching keywords within document context. Such full context-aware text search capability is more powerful than pure keyword search, since the user can now provide fine-grained context in which the keywords should occur. However, XML with XQuery full text searching requires that the user first convert her text data into XML and store them into XMLType column. Such massive physical data migration with possible loss of document fidelity and its potential impact on existing production environments are often expensive enough that users are reluctant to adopt the XML/XQuery approach.  In this paper, we propose a pay-as-you-go architecture to provide XML text view over LOB columns, so that user can take advantage of context-aware full-text search capability adaptively. This adaptive architecture includes a novel XML text index that can be created over the LOB column where the content is stored. The XML text index supports an XML text view over LOB data on top of which XQuery full-text search capability is feasible. Such an adaptive index/view approach provides least intrusion over existing data, as it requires no physical data migration. We describe the design and challenge of building such an adaptive XML text index. Furthermore, we advocate that the pay-as-you-go approach provides the integration bridge between the structured relational world and text oriented document world and fulfills the primary motivation of XML in the database.	adaptive architecture;binary large object;column (database);database schema;information retrieval;relational database management system;sql;sql/xml;search algorithm;semi-structured data;string searching algorithm;text corpus;xml;xquery	Zhen Hua Liu;Thomas Baby;Sukhendu Chakraborty;Junyan Ding;Anguel Novoselsky;Vikas Arora	2010		10.1145/1807167.1807281	well-formed document;xml catalog;xml validation;xml encryption;simple api for xml;semi-structured data;data migration;xml;processing instruction;relax ng;xml schema;data model;streaming xml;computer science;document structure description;xml framework;xml database;xml schema;database;xml signature;world wide web;xml schema editor;information retrieval;efficient xml interchange	DB	-34.09845750830894	6.039512235021193	4118
05426407e184054b3bdc932944f36b05a9f5916a	a topology-aware method for scientific application deployment on cloud	deployment;topology aware;scientific applications;cloud computing;communication topology	Nowadays, more and more scientific applications are moving to cloud computing. The optimal deployment of scientific applications is critical for providing good services to users. Scientific applications are usually topology-aware applications. Therefore, considering the topology of a scientific application during the development will benefit the performance of the application. However, it is challenging to automatically discover and make use of the communication pattern of a scientific application while deploying the application on cloud. To attack this challenge, in this paper, we propose a framework to discover the communication topology of a scientific application by pre-execution and multi-scale graph clustering, based on which the deployment can be optimised. In addition, we present a set of efficient collective operations for cloud based on the common interconnect topology. Comprehensive experiments are conducted by employing a well-known MPI benchmark and comparing the performance of our method with those of other methods. The experimental results show the effectiveness of our topology-aware deployment method.	software deployment	Pei Fan;Zhenbang Chen;Ji Wang;Zibin Zheng;Michael R. Lyu	2014	IJWGS	10.1504/IJWGS.2014.064937	cloud computing;computer science;theoretical computer science;operating system;data mining;distributed computing;software deployment	HPC	-25.80702681292701	57.331729558609915	4121
7f5968ac4358af0fff922247fa24112d7d4b2a29	how do software developers identify design problems?: a qualitative analysis		When a software design decision has a negative impact on one or more quality attributes, we call it a design problem. For example, the Fat Interface problem indicates that an interface exposes non-cohesive services Thus, clients and implementations of this interface may have to handle with services that they are not interested. A design problem such as this hampers the extensibility and maintainability of a software system. As illustrated by the example, a single design problem often affects several elements in the program. Despite its harmfulness, it is difficult to identify a design problem in a system. It is even more challenging to identify design problems when the source code is the only available artifact. In particular, no study has observed what strategy(ies) developers use in practice to identify design problems when the design documentation is unavailable. In order to address this gap, we conducted a qualitative analysis on how developers identify design problems in two different scenarios: when they are either familiar (Scenario 1) or unfamiliar (Scenario 2) with the analyzed systems. Developers familiar with the systems applied a diverse set of strategies during the identification of each design problem. Some strategies were frequently used to locate code elements for analysis, and other strategies were frequently used to confirm design problems in these elements. Developers unfamiliar with the systems relied only on the use of code smells along the task. Despite some differences among the subjects from both scenarios, we noticed that developers often search for multiple indicators during the identification of each design problem.	code smell;documentation;extensibility;list of system quality attributes;software design;software developer;software system	Leonardo da Silva Sousa;Roberto Felicio Oliveira;Alessandro F. Garcia;Jaejoon Lee;Tayana Conte;Willian Nalepa Oizumi;Rafael Maiani de Mello;Adriana Lopes;Natasha M. Costa Valentim;Edson Cesar Cunha de Oliveira;Carlos José Pereira de Lucena	2017		10.1145/3131151.3131168	systems engineering;software system;documentation;code smell;maintainability;software design;computer science;extensibility;source code;software	SE	-64.56070271051215	33.84886560125679	4124
2d950ffc1206c0971e7eba15ae48cdab83fd0857	a default logical semantics for defeasible argumentation		Defeasible argumentation and default reasoning are usually perceived as two similar, but distinct approaches to commonsense reasoning. In this paper, we combine these two fields by viewing  (defeasible resp. default) rules as a common crucial part in both areas. We will make use of possible worlds semantics from default reasoning to provide examples for arguments, and carry over the notion of plausibility to the argumentative framework. Moreover, we base a priority relation between arguments on the tolerance partitioning of system Z and obtain a criterion phrased in system Z terms that ensures warrancy in defeasible argumentation.	argumentation framework;defeasible reasoning	Gabriele Kern-Isberner;Guillermo Ricardo Simari	2011			natural language processing;artificial intelligence;algorithm	AI	-17.556321819811554	6.517512320551989	4125
e7d72b94fbb8b717199a0334d9444f70a824ddff	semanticlife collaboration: security requirements and solutions - security aspects of semantic knowledge management	muraille chine;modelizacion;controle acces;ontologie;keyword;ingenierie connaissances;securite informatique;knowledge management;interrogation base donnee;interrogacion base datos;semantics;hombre;palabra clave;mot cle;semantica;semantique;computer security;modelisation;security requirements;seguridad informatica;human;ontologia;access control;modeling;chinese wall;ontology;database query;muralla china;homme;knowledge engineering	SemanticLIFE is a project that stores all information an individual works with in a semantically enriched form. Ontologies are used to improve the search process and to express queries in the way humans think – e.g. “Find the draft I’ve been working on when traveling home from the conference in Chicago”. When people cooperate on projects they obviously need to share information without spending time on entering keywords and thinking about who should be able to access which data; the issue is to correctly configure access controls so that only required information is shared with the appropriate people. Using a combination of the Chinese Wall and the Bell LaPadula model we show how access controls can be configured correctly with little effort by the users.	access control;chinese wall;data integrity;database;humans;ontology (information science);requirement;role-based access control;semantic knowledge management	Edgar R. Weippl;Alexander Schatten;Muhammad Shuaib Karim;A Min Tjoa	2004		10.1007/978-3-540-30545-3_34	systems modeling;computer science;artificial intelligence;access control;knowledge engineering;ontology;data mining;database;semantics;computer security	DB	-37.36609026416204	12.325230795897998	4127
d5856410fe0d50e33203b4b6938ec7c41ce6d695	high intelligence computing: the new era of high performance computing	computers;data flow high intelligence computing high performance computing vector computing multiprocessing computer intelligence;vector computing;technological innovation;multiprocessing;high performance computing;computer intelligence;data structures;planets;high intelligence computing;high performance computer;artificial intelligence;data flow computing;humans;data structures artificial intelligence data flow computing;computers high performance computing technological innovation artificial intelligence planets hardware humans;data flow;hardware	Ralf Fischer will describe innovations in High Performance Computing, including the introduction of the fused multiply-add dataflow, and innovations in vector computing and multiprocessing. This has led to a new era in high performance that has created human intelligence in computers. Ralf will describe this computer intelligence in the computer named “Watson”. He will also describe how computer intelligence will translate into a Smarter Planet.	computational intelligence;computer;dataflow;michael j. fischer;multiply–accumulate operation;multiprocessing;smarter planet;supercomputer;vector processor	Ralf Fischer	2011	2011 IEEE 20th Symposium on Computer Arithmetic	10.1109/ARITH.2011.42	planet;data flow diagram;multiprocessing;data structure;computer science;theoretical computer science;computational intelligence	Arch	-9.384215028799053	41.67850084432229	4128
7880a23a30dd7150adef4440956cd338aed1adeb	coping with uncertainty in mobile wireless networks	wireless links;protocols;topology control;routing mobile wireless network uncertainty internet access end to end quality of service end to end qos wireless channels network topology node mobility network protocols traffic load host mobility resource availability wireless link characteristics network dynamics information theoretic learning tools information theoretic prediction tools decision making temporal information spatial information entropy rates network state information mobility tracking resource management;internet access;wireless channels;theoretical framework;mobile device;resource allocation;wireless network;resource manager;adaptive protocol;uncertainty wireless networks ip networks quality of service network topology wireless application protocol robustness telecommunication traffic entropy large scale systems;design optimization;prediction theory telecommunication network routing entropy protocols mobile computing mobile radio internet quality of service network topology telecommunication traffic resource allocation learning artificial intelligence;network topology;large scale;telecommunication traffic;internet;prediction theory;telecommunication network routing;mobile radio;entropy rate;information theoretic learning;network dynamics;mobile wireless network;resource availability;entropy;learning artificial intelligence;quality of service;mobile computing;spatial information;mobile network;design methodology	"""Extremely large-scale wireless networks of interconnected mobile devices are inevitable in the near future. Almost all these varied devices are likely to require some form of Internet access. The uncertainty associated with wireless mobile networks produces unique challenges to achieving seamless integration with the Internet while provisioning end-to-end quality of service (QoS). In particular, the uncertainty in wireless channels, and in network topology, due to node mobility, can bedevil protocols more suited to a """"classical"""" Internet structure. Therefore, new protocols have to be designed that must be: (i) robust against the uncertainty in traffic load, host mobility, resource availability and wireless link characteristics; (ii) adaptive to the network dynamics, thus learning and prediction become integral components in the design methodology; (iii) intrinsically on-line so as to make real-time decisions based on temporal and spatial information. In order to cope with the uncertainty, we propose an overarching theoretical framework to represent relevant network information in terms of underlying entropies, entropy rates and their inter-relationships. We demonstrate how to apply information theoretic learning and prediction tools for collecting and disseminating network state information that can be used for robust and adaptive protocol design. Specifically, we investigate the applicability of this novel framework in designing optimal mobility tracking and resource management, while coping with uncertainty in traffic load, topology control and routing."""	communications protocol;end-to-end encryption;information theory;internet access;mobile device;mobile phone;network topology;online and offline;provisioning;quality of service;real-time transcription;routing;seamless3d;topology control	Sajal K. Das;Christopher Rose	2004	2004 IEEE 15th International Symposium on Personal, Indoor and Mobile Radio Communications (IEEE Cat. No.04TH8754)	10.1109/PIMRC.2004.1370845	communications protocol;cellular network;entropy;the internet;multidisciplinary design optimization;quality of service;internet access;design methods;resource allocation;computer science;resource management;operating system;wireless network;network dynamics;mobile device;distributed computing;spatial analysis;mobile computing;computer security;entropy rate;network topology;computer network	Mobile	-12.214797932001739	87.481521865113	4139
821c0203026bcb97b675100399c2564711a2d243	generating a business model through the elicitation of business goals and rules within a spem approach		Business Models play a pivotal role in organizations, especially in building bridges and enabling dialogue between business and technological worlds. Complementarily, as Use Cases are one of the most popular techniques for eliciting requirements in the design of Information Systems, Business Goals and Business Rules associate with Business Process Use Cases to compose a Business Model base structure. However, methods for relating Business Processes, Goals and Rules (PGR) are scarce, dissonant or poorly grounded. In this sense, we propose the specification of a method, within a SPEM approach, covering the elicitation of Business Goals and Rules from Process-level Use Cases, and their mapping to a Business Model representation. As a result, a tailorable method for the generation of a solution Business Model, by aligning the resulting trios (PGR) with a Business Model Canvas, is presented and demonstrated in a live project.	meta-process modeling	Carlos E. Salgado;Juliana Teixeira;Ricardo J. Machado;Rita Suzana Pitangueira Maciel	2014		10.1007/978-3-319-11958-8_5	semantics of business vocabulary and business rules;artifact-centric business process model;business administration;management science;business process model and notation;process management;business rule;business process modeling	ML	-55.376131879494054	18.407989215163944	4143
b2a5f2005c1cd8158bc80d63e56054a5b2601863	sysml-sec: a model driven approach for designing safe and secure systems	model driven engineering safety security sysml embedded systems;security unified modeling language safety hardware embedded systems computer architecture;safety related function sysml sec model driven approach embedded system security mechanism security related function;computer architecture;embedded systems;embedded systems model driven engineering safety security sysml;eurecom ecole d ingenieur telecommunication centre de recherche graduate school research center communication systems;safety;unified modeling language;model driven engineering;security;sysml embedded systems formal verification security of data;sysml;hardware	Security flaws are open doors to attack embedded systems and must be carefully assessed in order to determine threats to safety and security. Subsequently securing a system, that is, integrating security mechanisms into the system's architecture can itself impact the system's safety, for instance deadlines could be missed due to an increase in computations and communications latencies. SysML-Sec addresses these issues with a model-driven approach that promotes the collaboration between system designers and security experts at all design and development stages, e.g., requirements, attacks, partitioning, design, and validation. A central point of SysML-Sec is its partitioning stage during which safety-related and security-related functions are explored jointly and iteratively with regards to requirements and attacks. Once partitioned, the system is designed in terms of system's functions and security mechanisms, and formally verified from both the safety and the security perspectives. Our paper illustrates the whole methodology with the evaluation of a security mechanism added to an existing automotive system.	cipher;computation;computer simulation;cyber-physical system;embedded system;formal verification;model-driven architecture;requirement;semiconductor industry;systems modeling language;systems engineering	Yves Roudier;Ludovic Apvrille	2015	2015 3rd International Conference on Model-Driven Engineering and Software Development (MODELSWARD)	10.5220/0005402006550664	software security assurance;computer security model;reliability engineering;unified modeling language;model-driven architecture;security information and event management;security engineering;systems modeling language;computer science;systems engineering;engineering;information security;software engineering;database;security service;distributed system security architecture;security testing;computer security	EDA	-54.63380225050881	49.72851582307032	4153
efd3bcbb10647686772a867ac305a8132fcf6cb3	generating maximal fault coverage conformance test sequences of reduced length for communication protocols	sequences;protocols;finite state machines protocols conformance testing sequences;testing;reduced length sequences;algorithm;automata;finite state machines;reduced length sequences maximal fault coverage conformance test sequences communication protocols finite state machine algorithm;conformance testing;testing protocols computer science educational institutions nasa character generation automata context fault detection;character generation;fault detection;communication protocol;fault coverage;computer science;nasa;context;finite state machine;communication protocols;maximal fault coverage conformance test sequences	This paper focuses on a, technique to reduce the lengih of maximal fault coverage test sequences for communication protocols b y removing redundant test segments. This approach conceptually begins with all the iest segments needed for the generation of maximal fault coverage test sequences as in [MPE?]], analyzes the structure of the specified Finite State Machine (FSM) for the protocol and shows that certain segments in these tests are unnecessary t o guarantee maximal fault coverage. From this analysis an algorithm i s proposed for generating the reduced length sequences that still guarantee maximal fault coverage. We describe how these tests are in some sense minimal, or near minimal, length test sequences without losing fault coverage.	algorithm;communications protocol;conformance testing;fault coverage;finite-state machine;maximal set	Raymond E. Miller;Sanjoy Paul	1993		10.1109/ICNP.1993.340916	communications protocol;real-time computing;fault coverage;computer science;automatic test pattern generation;finite-state machine;algorithm;computer network	EDA	-10.53875564055819	30.625029561874953	4158
be8d1e9379a7d729e3003675990dd11a2ba33d11	sidd: a framework for detecting sensitive data exfiltration by an insider attack	steganography;sidd system;organizational information protection strategy;steganography method;data privacy;sensitive information dissemination detection system;insider threat detection;sensitive content;pattern classification;information filtering;detecting sensitive data exfiltration;sensitive data;traffic flow classification;multilevel framework;signal processing technique;insider attack;covert communication detection;network protection;information dissemination;feature extraction;mitigating insider threat;sensitive information;network resource;network-level application identification;statistical technique;telecommunication security;telecommunication traffic;insider threat mitigation;protected network;content signature generation;sensitive data exfiltration detection framework;intranets;high-speed transparent network bridge;security of data;signal processing;traffic flow	Detecting and mitigating insider threat is a critical element in the overall information protection strategy. By successfully implementing tactics to detect this threat, organizations mitigate the loss of sensitive information and also potentially protect against future attacks. Within the broader scope of mitigating insider threat, we focus on detecting exfiltration of sensitive data through a protected network. We propose a multilevel framework called SIDD (Sensitive Information Dissemination Detection) system which is a high-speed transparent network bridge located at the edge of the protected network. SIDD consists of three main components: 1) network-level application identification, 2) content signature generation and detection, and 3) covert communication detection. Further, we introduce a model implementation of the key components, demonstrating how our system can be deployed. Our approach is based on the application of statistical and signal processing techniques on traffic flow to generate signatures and/or extract features for classification purposes. The proposed framework aims to address methods to detect, deter and prevent deliberate and unintended distribution of sensitive content outside the organization using the organization's system and network resources by a trusted insider.		Yali Liu;Cherita L. Corbett;Ken Chiang;Rennie Archibald;Biswanath Mukherjee;Dipak Ghosal	2009		10.1109/HICSS.2009.905	traffic flow;signal processing;data mining;internet privacy;computer security	Security	-60.087282219969666	64.43904732022978	4165
dc5e62a19103e1f855739f4fba87734f1b8b27fc	practical user interface design notation	user interface design	A notation for describing user interfaces has been developed which offers the benefits of being theoretically well-founded and reasonably formal while at the same time being of practical use in an industrial software design environment. The notation is based on a five-layer model of interface software derived from user interface management system and window management work, and is, essentially, a statetransitional approach to the description of interfaces and their behaviour. A short example of the use of the notation is given and its strengths and weaknesses are discussed.	software design;user interface design;user interface management systems;window manager	Peter Windsor;Graham Storrs	1993	Interacting with Computers	10.1016/0953-5438(93)90006-F	user interface design;user;human–computer interaction;computer science;software engineering;user interface	SE	-48.33660815642353	24.030982264986154	4173
617a66e73f689c3f13d04c1c63675604a946b29f	improving the detection of on-line vertical port scan in ip traffic	computer network security;bloom filter;internet measurements online vertical port scan detection algorithm ip traffic port scan attack detection destination ip addresses destination ports two dimensional bloom filter hashing functions sliding window;radiation detectors;internet measurements;attack detection bloom filter internet measurements on line algorithms;on line algorithms;attack detection;manganese;telecommunication traffic;internet;data structures;radiation detectors ip networks algorithm design and analysis manganese context internet;telecommunication traffic computer network security data structures internet ip networks;ip networks;context;algorithm design and analysis	We propose in this paper an on-line algorithm based on Bloom filters to detect port scan attacks in IP traffic. Only relevant information about destination IP addresses and destination ports are stored in two steps in a two-dimensional Bloom filter. This algorithm can be indefinitely performed on a real traffic stream thanks to a new adaptive refreshing scheme that closely follows traffic variations. It is a scalable algorithm able to deal with IP traffic at a very high bit rate thanks to the use of hashing functions over a sliding window. Moreover it does not need any a priori knowledge about traffic characteristics. When tested against real IP traffic, the proposed on-line algorithm performs well in the sense that it detects all the port scan attacks within a very short response time of only 10 seconds without any false positive.	online and offline;port scanner	Yousra Chabchoub;Christine Fricker;Philippe Robert	2012		10.1109/CRISIS.2012.6378945	data structure;telecommunications;computer science;manganese;bloom filter;network security;programming language;computer security;computer network	ECom	-60.7190024243496	67.77481094146061	4176
9bed8e212b75eb41754c2be716faf3cc52d84b4d	die bedeutung des electronic bankings für das cash management im multinationalen konzern			bibliothèque de l'école des chartes	Carl Loyal	1992				DB	-98.53285247950997	26.458544332071472	4178
06839907ee72658fd322fbec9352de17ad206abe	middleware for user controlled environments	natural language interfaces;middleware ontologies natural languages pervasive computing informatics logic devices natural language processing event detection access control security;abstract data types;ontologies artificial intelligence;dynamic environment;formal logic;ubiquitous computing;middleware;formal logic ubiquitous computing middleware natural language interfaces ontologies artificial intelligence abstract data types;description logic;qa0075 electronic computers computer science;user abstractions middleware user controlled environments description logic natural language processing ontology pervasive computing;natural language processing	In this paper, we describe the middleware that has evolved from our attempt to capture user descriptions of policies controlling devices and services from natural language. Description logic (DL) provides a formal link between the natural language processing, the ontology and the middleware. We show that the use of a formalism such as DL opens useful avenues to detecting and resolving conflicts in policies, both in formulation and when resolving them against incoming events and requests. We finish by arguing that pervasive middleware needs to move closer to the users' abstractions to provide a service for what will be a highly dynamic environment.	abox;compile time;compiler;description logic;middleware;natural language processing;pervasive informatics;semantics (computer science);sensor;subsumption architecture	Bill Keller;Tim Owen;Ian Wakeman;Julie Weeds;David J. Weir	2005	Third IEEE International Conference on Pervasive Computing and Communications Workshops	10.1109/PERCOMW.2005.57	middleware;description logic;human–computer interaction;computer science;theoretical computer science;operating system;middleware;database;ontology language;programming language;abstract data type;logic;ubiquitous computing	Robotics	-42.07710035625487	15.540696227377385	4191
5ce8f097f6ed4a0cf2ab53ce27dd179024f8a646	a contribution to laboratory performance measurements of ieee 802.11 b, g wep point-to-point links	performance measure;wep point to point links;ieee 802 11;percentage datagram loss;access point;telecommunication network reliability;point to point;measurement systems;wireless network;udp;ftp transfer rate;ieee 802 11g;wlan;enterasys networks;throughput jitter laboratories ieee 802 11g standard open systems wireless communication;tcp throughput;osi levels;microwaves;wireless lan measurement systems radio links telecommunication network reliability;wireless communication;wireless network laboratory performance wi fi wlan wep point to point links ieee 802 11b ieee 802 11g;laboratory performance measurements;wireless network laboratory performance;wireless communications;ieee 802 11b;access points;wireless lan;jitter;open systems;wi fi;ftp transfer rate laboratory performance measurements ieee 802 11 wep point to point links wi fi wireless communications microwaves access points enterasys networks osi levels udp tcp throughput percentage datagram loss;throughput;ieee 802 11g standard;radio links	Wireless communications using microwaves are increasingly important, e.g. Wi-Fi. Performance is a fundamental issue, resulting in more reliable and efficient communications. Laboratory measurements are made about several performance aspects of Wi-Fi (IEEE 802.11b, g) WEP point-to-point links using available access points from Enterasys Networks (RBTR2). Through OSI levels 4 and 7, detailed results are presented and discussed from TCP, UDP and FTP experiments, namely: TCP throughput, jitter, percentage datagram loss and FTP transfer rate. Conclusions are drawn about link performance.	datagram;experiment;microwave;osi model;point-to-point (telecommunications);throughput;wired equivalent privacy;wireless access point	José A. R. Pacheco de Carvalho;Hugo Veiga;Nuno Marques;C. F. Ribeiro Pacheco;Antonio D. Reis	2010	2010 International Conference on Wireless Information Networks and Systems (WINSYS)		embedded system;telecommunications;computer science;wireless;computer network	Mobile	-5.457789352887971	92.68484823645872	4192
30d7202db0c29a5f996754cd3fc41c31d99d826e	using quality function deployment in singulation process analysis	house of quality;value engineering;quality function deployment	This paper presents the application of quality function deployment (QFD) to process analysis. QFD has been applied in various industries since the 1960s, but traditional QFD methodologies still have many limitations. An improved QFD framework is proposed, in which, customer requirements are identified through value engineering and strategic analysis. Process mapping using the IDEF methodology is then employed to formulate the design specification of a system before the “House of Quality” is applied. A case study is conducted to demonstrate the usefulness of the proposed framework in analyzing the effectiveness and performance of two types of sigulation processes in semi-conductor industry.	idef;quality function deployment;requirement;semiconductor industry;singulation;software deployment	Hang-Wai Law;Meng Hua	2007	Engineering Letters		aperture;control theory;computer science;fastener;quality function deployment;mechanical engineering	SE	-65.35927467234552	14.029386920534511	4195
d1a90abb7f32f52ceba7815b81eb8d936f577def	behavioral specification of distributed software component interfaces	distributed application;formal specification;eiffel style postconditions;application software;software maintenance;computer model;distributed programming application program interfaces remote procedure calls formal specification object oriented programming software reusability java;eiffel style invariants;java remote method invocation interfaces;distributed computing;eiffel style preconditions;contracts;object oriented programming;biscotti;distributed applications;computer networks;distributed software component reuse;eiffel style invariants behavioral specification distributed software component interfaces networked computers distributed applications distributed software component reuse biscotti java extension java remote method invocation interfaces eiffel style preconditions eiffel style postconditions;network servers;distributed programming;remote method invocation;application program interfaces;software reusability;distributed software component interfaces;software component;interface definition language;robustness;behavioral specification;object oriented modeling java software reusability network servers distributed computing contracts computer networks application software robustness software maintenance;networked computers;remote procedure calls;object oriented modeling;network computing;java extension;java	46 Computer W ith the popularity of the Web and the Internet, networked computers are finding their way into a broader range of environments, from corporate offices to schools, homes, and shirt pockets. This new computing model fosters the development of distributed software components that communicate with one another across the underlying net-worked infrastructure. A distributed software component can be plugged into distributed applications that may not have existed when it was created. The intention is that many developers will reuse distributed software components to build new systems. For a developer to know what to expect from a distributed software component, the design must clearly document both the component's interface (operation signature and calling conventions) and the operations' behavior. An interface definition language usually is used to describe a distributed software component's interface. However, a notable limitation of current IDLs is that they generally only describe the names and type signatures of the component's attributes and operations. Current IDLs don't formally specify the behavior of the software component's operations. Inadequate specification of reusable software can result in disaster. The failed launch of the $500 million Ariane 5 in 1996 is a case in point. 1 Code originally intended to convert a number less than 2 16 from a 64-bit floating-point number to a 16-bit unsigned integer was applied to a greater number, causing the software, and the rocket, to crash. Design by contract 2 solves the problem of behavioral specification for classes in an object-oriented program. In this article, we discuss how to apply design by contract to distributed software component interfaces. Biscotti (behavioral specification of distributed software component interfaces) is an extension of Java that enhances Java remote method invocation interfaces with Eiffel-style preconditions, postconditions, and invariants. Integrating this specification information into the language complements other Java features , such as reflection, which is fundamental to JavaBeans software components. Java developers easily transition to using Biscotti because it is an extension of a familiar programming model. Ideally, a distributed software component has the following characteristics: • a well-defined, well-documented interface defining the services it provides, • software independence from its clients, • remote access, • encapsulated implementation, • robustness, and • reusability. The object-oriented models that are the basis of most common distributed software component technologies foster reuse through the abstraction and encapsulation characteristics of objects. A key strength of the object model is its ability to deal with …	16-bit;64-bit computing;calling convention;component-based software engineering;computer;design by contract;distributed computing;double-precision floating-point format;eiffel;electronic signature;encapsulation (networking);integer (computer science);interface description language;internet;invariant (computer science);java remote method invocation;postcondition;precondition;programming model;reflection (computer programming);subroutine;type signature;world wide web	Cynthia Della Torre Cicalese;Shmuel Rotenstreich	1999	IEEE Computer	10.1109/2.774918	computer simulation;interface description language;application software;real-time computing;computer science;backporting;component-based software engineering;software development;operating system;software engineering;middleware;software construction;formal specification;component;distributed computing;distributed design patterns;programming language;object-oriented programming;software maintenance;java;remote procedure call;robustness	SE	-34.310748467894925	42.710763940950805	4209
9a936d2c35ed867b35abe489f1ac500695dddac8	the integration of user perception in the heterogeneous m/m/2 queue	user perception			Robert Geist;Kishor S. Trivedi	1983			human–computer interaction;computer science;knowledge management;multimedia	HPC	-96.22751839402514	29.193204254478808	4214
65a0c806864fffdd628d324f1a31aec9c40ae6bb	an identification of system key attributes in structural reasoning for formal validation of industrial programming	system run time behavior system key attribute identification structural reasoning formal validation industrial programming nonchaotic system system correctness reasoning factory automation;chaotic system;state space methods costs manufacturing automation programmable control system testing automatic control formal languages statistical analysis service robots biomimetics;factory;clocks;probability density function;industrial programming;system monitoring;formal methods;formal validation;nonchaotic system;data mining;belts;formal method;system monitoring factory automation programming reasoning about programs;reasoning about programs;state space;cognition;unified modeling language;aerospace electronics;factory automation;system correctness reasoning;structural reasoning;system key attribute identification;programming;system run time behavior;automation;automation structural reasoning formal methods factory	Each non-chaotic system has a set of attributes or properties forming a ‘character’ of the system. Being a unique entity means to possess a unique (sub-)set of attributes. This paper shows how these attributes can be identified and applied in reasoning on system correctness. The field of application is a domain of factory automation, where the structural reasoning on system state spaces is used to assist in formal validation of the overall system behavior. The paper defines ‘structural reasoning’ and ‘key attributes’ that can serve as a base for understanding developed system.	chaos theory;correctness (computer science);data mining;model checking;state space;system analysis	Andrei Lobov;José L. Martínez Lastra	2008	2008 IEEE International Conference on Robotics and Biomimetics	10.1109/ROBIO.2009.4913178	real-time computing;simulation;formal methods;computer science;systems engineering;engineering;artificial intelligence;automation	Robotics	-45.68107376180145	32.89173111526953	4218
524a530af90eac22f4f485f5ba872d9128abdc12	providing non-hierarchical security through interface mechanisms	common security model;commercial requirement;secure code;hierarchical fashion;complete trust;multiple type;non-hierarchical policy;outer circle;interface mechanism;non-hierarchical security mechanism;security model;data security;prototypes;paradigm shift;operating systems;information security;databases;production	Common security models provide protection in an hierarchical fashion (i.e. there is a trusted core with outer circles of less secure code and data). There is only one method of providing protection. This model makes it difficult to protect code and data with multiple types of non-hierarchical policies. It implies complete trust in the core requiring thorough evaluation each time modifications are made. This paper first describes a paradigm shift to non-hierarchical security. It then describes an interface mechanism with the potential for providing an efficient, configurable and non-hierarchical security mechanism more suitable for commercial requirements.	programming paradigm;requirement	Deborah Hamilton	1994			software security assurance;information security audit;computer security model;standard of good practice;cloud computing security;paradigm shift;security through obscurity;security information and event management;security convergence;covert channel;asset;computer science;information security;logical security;security service;prototype;data security;internet privacy;security analysis;security testing;network security policy;world wide web;computer security;information security management	Security	-51.30897098744053	52.79500674867128	4219
a81ecb3afa5f2070d27f92667ea9df1c149b1d4e	exploring capturable everyday memory for autobiographical authentication	capturable everyday memory;android;smartphones;everyday memory;autobiographical authentication	We explore how well the intersection between our own everyday memories and those captured by our smartphones can be used for what we call autobiographical authentication-a challenge-response authentication system that queries users about day-to-day experiences. Through three studies-two on MTurk and one field study-we found that users are good, but make systematic errors at answering autobiographical questions. Using Bayesian modeling to account for these systematic response errors, we derived a formula for computing a confidence rating that the attempting authenticator is the user from a sequence of question-answer responses. We tested our formula against five simulated adversaries based on plausible real-life counterparts. Our simulations indicate that our model of autobiographical authentication generally performs well in assigning high confidence estimates to the user and low confidence estimates to impersonating adversaries.	amazon mechanical turk;bayesian network;challenge–response authentication;experience;field research;mobile device;real life;simulation;smartphone	Sauvik Das;Eiji Hayashi;Jason I. Hong	2013		10.1145/2493432.2493453	computer science;operating system;internet privacy;computer security;android	HCI	-52.18424555878014	66.2466230001962	4231
0984bf046271821bd3f0a24fa1e167ad4cd7b44c	modellqualität als indikator für softwarequalität: eine taxonomie	dblp	Komplexität, Anforderungsmanagement und Variantenvielfalt sind zentrale Herausforderungen bei der Entwicklung und Evolution heutiger softwaregesteuerter Systeme. Diesen wird zunehmend durch den Einsatz modellbasierter Entwicklungsmethoden begegnet. Dadurch wird das Modell zum zentralen Artefakt und die Erstellung und Nutzung von Modellen zu einer zentralen Tätigkeit in der Softwareentwicklung. Mit der Bedeutung der Modelle steigen auch die Ansprüche an ihre Qualität. Dieser Beitrag untersucht die Implikationen, die daraus entstehen, insbesondere werden sinnvolle Qualitätsmerkmale für softwarebeschreibende Modelle identifiziert und diskutiert.	eine and zwei;v-model	Florian Fieber;Michaela Huhn;Bernhard Rumpe	2008	Informatik-Spektrum	10.1007/s00287-008-0279-4	computer science	AI	-103.34004336671752	33.781610557612986	4232
fb76dd08846264f5d7d6c2f35aebb95539f1badb	expression, contrôle et exploitation de la navigation hypertextuelle dans le langage sgmlql			linear algebra	Emmanuel Bruno;Jacques Le Maitre;Elisabeth Murisasco	1999				Crypto	-104.57931438139887	14.154919645672697	4238
73eecf68f96c8439891c00a77d31d15b958456f9	towards a process factory for developing situational requirements engineering processes	feature modeling;software systems;requirements engineering;requirement engineering;situational method engineering;software development;process factory;software product line	Selecting a suitable Requirements Engineering (RE) process is usually based on personal preferences or existing company practices rather than on the characteristics of the project at hand (project situation). Feature-oriented software development is the overall process of developing software systems in terms of their features. The Software Product Line (SPL) approach is a paradigm for systematic reuse of software products, and a Software Factory is a SPL aimed at the industrialization of software development. Based on the notion that a software/RE process can be developed via an engineering process, this research aims to provide a feature-based RE process factory to develop RE processes based on project situations. In our approach, the project situation is modeled as the problem domain (resulting in a situation model). A feature model can encapsulate all the features in an SPL; therefore, the variations and commonalities among RE processes can be represented in the form of a feature model, considered as a model of the solution domain. A mapping for translating the situation model to the RE process feature model is proposed with the specific aim of promoting traceability and rationality in the selection of RE process features.	feature model;feature-oriented programming;problem domain;programming paradigm;rationality;requirements engineering;software development;software factory;software product line;software system;traceability	Omid Jafarinezhad;Raman Ramsin	2012		10.1145/2245276.2231946	domain analysis;personal software process;verification and validation;team software process;software engineering process group;software sizing;software project management;package development process;software design;social software engineering;software development;requirement;feature-oriented domain analysis;software engineering;domain engineering;software construction;requirements engineering;empirical process;goal-driven software development process;software development process;software requirements;software system	SE	-56.99557209471345	26.067252298797143	4239
df12f569dd0e415a6a74e00b555189156cc71ae2	strategic and managerial ties for the new product development	community of practice;automotive industry;data collection;knowledge sharing;extended enterprise;new product development	The extended enterprise continuously creates partnerships with external partners for innovation. The structuring of these  partners into networks and communities of practice is an efficient manner to manage their knowledge. In this paper, we show  that strategic communities can emerge from managerial networks through a case study about a large manufacturing company, in  the automotive industry, gathering its after sales services partners in a network, for the managerial collaboration, and the  most strategic ones in a community. We use various data collection methods: interviews with the managers of the network and  the strategic community, a survey dedicated to a sample from the network population, and a questionnaire to the strategic  partners. We provide the mechanisms and dynamics of the managerial and strategic ties. The firsts are known-interdependencies,  integrated IT-tools, trust and collaborative leadership. The successful collaboration contributes to the identification of  the most strategic partners, thus the emergence of strategic ties. The strategic community uses a specific IT-tool to insert  technical new knowledge that is filtered by strategic gatekeepers and benefits from a non-financial reward system to enhance  knowledge sharing.  	new product development	Angelo Corallo;Nouha Taifi;Giuseppina Passiante	2008		10.1007/978-3-540-87783-7_50	knowledge management;automotive industry;marketing;product management;new product development;statistics;data collection	NLP	-77.87317502444327	4.732023721827338	4241
c33785b0917f539a3fbbadb6b5d7766710abf590	3d adjacency: a communication-aware online scheduling algorithm for 2d partially reconfigurable devices	communication time online scheduling reconfigurable computing adjacent surface data dependency;data dependency;conferences scientific computing;reconfigurable computing;online scheduling;telecommunication computing;telecommunication computing scheduling;adjacent surface;scheduling;3d adjacency communication cost dependency tasks cost function data communication 3dmas algorithm online 3d maximum adjacency surface hardware task scheduling partial reconfigurable devices 2d partially reconfigurable devices communication aware online scheduling algorithm;communication time	One main issue of partial reconfigurable devices to be solved is the hardware task scheduling and placement. This paper presents an online 3D maximum adjacency surface (3DMAS) algorithm taking data communication between tasks into consideration. This algorithm using 3D adjacency area as cost function aims at reducing the distance between dependency tasks while scheduling. The experimental results show that 3DMAS affords a considerable decrease in communication cost and a better placement.	algorithm;loss function;scheduling (computing)	Yingying Sheng;Renfa Li;Yan Liu	2013	2013 IEEE 16th International Conference on Computational Science and Engineering	10.1109/CSE.2013.108	fair-share scheduling;fixed-priority pre-emptive scheduling;parallel computing;real-time computing;earliest deadline first scheduling;dynamic priority scheduling;reconfigurable computing;computer science;rate-monotonic scheduling;operating system;two-level scheduling;distributed computing;round-robin scheduling;scheduling	Robotics	-12.44090015810259	59.16218028641963	4242
bd45de82f1e38b788ed9cfef3717e0c9c85eb37b	combining control and data integration in the spade-1 process-centered software engineering environment	programming environments;data integrity;software engineering fuses software tools broadcasting software design process design petri nets information filtering information filters environmental management;process centered software engineering environment;computer aided software engineering software tools programming environments;computer aided software engineering;data oriented integration spade 1 process centered software engineering environment data integration regular unix tools vi cc integrated environments selective message broadcast dec fuse common structured data control oriented integration;software tools;structured data	For Process-centered Software Engineering Environments (PSEEs) it is essential to provide the set of tools that are needed t o actually produce software in p ra ct a ce. SPADE-1 is a PSEE originally designed in order to allow the developer t o use regular Unix tools (such as vi and CC) and integrated environments based on the selective message broadcast (like DEC FUSE). However, this approach did not allow tools (and users) t o share common structured data. This paper describes the mechanism that has been introduced in SPADE-1 in order to achieve both controloriented integration and data-oriented integration of tools. In this way tools can share structured d a t a in an ordered and controlled way.	integrated development environment;software engineering;unix	Sergio Bandinelli;Alfonso Fuggetta;Luigi Lavazza;Gian Pietro Picco	1994		10.1109/ISPW.1994.512774	personal software process;verification and validation;computing;software engineering process group;software sizing;search-based software engineering;computer science;package development process;software design;social software engineering;software framework;component-based software engineering;software development;software engineering;software construction;database;resource-oriented architecture;software deployment;computer-aided software engineering;software development process;software requirements;software system;computer engineering	SE	-49.4586558113745	29.33860143658121	4249
99123f3546ca4526c62eb66040bee7e22a3d0a83	design of broadcast programming primitives for distributed systems	distributed system;service replication;fail stop behaviour;group communication;dynamic groups;message dependency;state consistency;logical time;event ordering	This paper discusses causal broadcast programming primitives that allow distributed applications to specify message ordering requirements. The primitives, implemented in the communication interface by a kernel, allow event-driven programming whereby application entities specify partial ordering on delivery of messages and all entities perceive the flow of logical time upon exchanging messages. Since entities see the same order on messages, they have the same view of application state at every tick in logical time. The primitives allow ordering constraints to be carried in messages in the form of predicates, and a message to be delivered at destinations after its predicates are satisfied. The approach insulates the specification from the implementation, thereby providing flexibility and uniformity in the communication structure of applications. The programming interface supports an execution model that is derived from the ISIS 'process group' style of programming, whereby application entities are organized as members of a group, and a message exchanged in the group is delivered to all members, satisfying the ordering requirements. A realization of distributed shared memory using the primitives is also given. The paper also discusses the kernel level protocol support required to implement the interface. The primitives are useful in structuring complex applications such as distributed services and replica management.	distributed computing	S. Ramakrishna;B. Prasad;A. Thenmozhi;S. Samdarshi;K. Velaga;K. Shah;K. Ravindran	1993	Computer Communications	10.1016/0140-3664(93)90026-O	real-time computing;communication in small groups;computer science;theoretical computer science;operating system;distributed computing;computer security;computer network	Networks	-31.671783447895933	34.59157390272089	4252
00f232af15f933779a6177c4fb87d299be699a29	adaptive predictor integration for system performance prediction	k nn based supervised learning;classification algorithm;multiple predictor integration;supervised learning;pattern classification grid computing learning artificial intelligence;null;system performance;grid computing adaptive predictor integration system performance prediction multiple predictor integration classification algorithms k nearest neighbor k nn based supervised learning;system performance prediction;classification algorithms;prediction accuracy;pattern classification;adaptive predictor integration;k nearest neighbor;learning artificial intelligence;cumulant;grid computing;system performance predictive models virtual machining weather forecasting bandwidth availability accuracy classification algorithms grid computing principal component analysis;forecast accuracy;network weather service	The integration of multiple predictors promises higher prediction accuracy than the accuracy that can be obtained with a single predictor. The challenge is how to select the best predictor at any given moment. Traditionally, multiple predictors are run in parallel and the one that generates the best result is selected for prediction. In this paper, we propose a novel approach for predictor integration based on the learning of historical predictions. It uses classification algorithms such as k-Nearest Neighbor (k-NN) based supervised learning to forecast the best predictor for the workload under study. Then only the forecasted best predictor is run for prediction. Our experimental results show that it achieved 20.18% higher best predictor forecasting accuracy than the cumulative MSB based predictor selection approach used in the popular network weather service system. In addition, it outperformed the observed most accurate single predictor in the pool for 44.23% of the performance traces.	branch predictor;k-nearest neighbors algorithm;kerrison predictor;performance prediction;supervised learning;tracing (software)	Jian Zhang;Renato J. O. Figueiredo	2007	2007 IEEE International Parallel and Distributed Processing Symposium	10.1109/IPDPS.2007.370277	computer science;machine learning;pattern recognition;data mining;supervised learning;k-nearest neighbors algorithm;grid computing;cumulant	Arch	-24.38097008898196	60.84932476917978	4254
948f59ba32297b3f68b12636b0025daa8aba67d9	the inclusion of data frames illustrating data flow in control structure charts for novice students	control structure;data flow	Studies [3], [5] have demonstrated the power of graphical versus textual presentations. Our postulate slates that graphical definition must include a graphical representation of the data structures used [1]. Current research addresses the representation issue, the goal being not only to visually present graphical control flows but also the data being transformed by those control structures. The initial problem solving course at the USA is designed to teach concepts of algorithm formation as a method of solving formal analytic problems. Algorithms are defined graphically in control structure charts using textual dictionaries of variables. With this method, data is visualized graphically as it flows through the al­ gorithm. Research has been undertaken to investigate the necessary components and features required to support the environment. This research is based on Law’s work [4] defining structure charts in the SLAW (Structure Language Algorithm Writer) environment. A graphical tree represents the control flow of the algorithm. Textual dictionaries define the data structures employed. The flow of control is indicated by a hierarchy of shapes. SLAW has successfully been used as a teaching tool [2]. The primary focus of this research is the generation and investigation of a method for algorithm develop­ ment, DATA (Data to Algorithm Translation Analysis), entirely free of any programming language syntax, has been developed to graphically represent data structures and flows which yield structure charts in the SLAW method of algorithm development. The user creates a structure chart. Multiple dictionaries are now defined, each using graphical iconic symbols. Each lask/action in the chart will have associated with it a dictionary bubble which will become active and expand into a data window at the execution or reference of the lask/action. The data windows allow the user to monitor the data flows.	algorithm;control flow;data structure;dataflow;dictionary;graphical user interface;microsoft windows;problem solving;structure chart	Michael V. Doran;Herbert E. Longenecker	1990		10.1145/98949.99083	computer science;engineering drawing;algorithm	ML	-32.47941245525253	24.39792860463146	4263
0e56963dd2c16af8d13b1df7121b41a4fe6d7b56	prozeßrechner koordiniert verschiedene roboter mittels sensorinformationen				P. Drews;K. Fuchs;R. Wagner;K. Willms	1987	Robotersysteme		simulation;industrial robot;control engineering;engineering;welding	NLP	-105.05941737314146	24.873950642543047	4265
ed966ab71afdb00ff2d3f44e26e95ddbdc444a38	temporal constrained objects: application and implementation		We present a novel programming concept called temporal constrained objects for modeling of dynamic systems. It is an extension of the paradigm of constrained objects which provides a principled approach to modeling complex engineering systems based upon two main principles: a compositional specification of the structure of the system, using objects, and a declarative specification of its behavior, using constraints. A novel feature of temporal constrained objects is the series variable, the sequence of whose values is determined by constraints that include metric temporal operators. The emergent behavior of a dynamic system is determined through a process of time-based simulation and constraint satisfaction at each step. Each class definition for a temporal constrained object is translated into a Prolog rule with constraint-solving capability. In order to improve the performance for long simulations, a partial evaluation technique is adopted for optimized code generation. This paper describes the syntax of a language called TCOB along with several examples, our objective being to demonstrate the benefits of the programming paradigm. TCOB has been implemented and all examples presented in this paper were tested using this implementation. An overview of the translation as well as partial execution and its performance improvements are presented in the paper. A novel programming concept called temporal constrained objects.Series variable and metric temporal operators for specifying dynamic behavior.Computational model involves time-based simulation and constraint satisfaction.Class definitions translated into Prolog predicates, optimized by partial evaluation.Examples from engineering domain and performance results.		Jinesh M. Kannimoola;Bharat Jayaraman;Pallavi Tambay;Krishnashree Achuthan	2017	Computer Languages, Systems & Structures	10.1016/j.cl.2017.03.002	theoretical computer science;code generation;operator (computer programming);programming language;computer science;dynamical system;syntax;programming paradigm;machine learning;partial evaluation;constraint satisfaction;artificial intelligence;prolog	Logic	-28.002490655448767	20.763659020665354	4278
92d6530b29fdfd1262a44f20bf9c3c998bc0aeef	inner-product lossy trapdoor functions and applications	lattices;会议论文;inner product encryption;inner product lossy trapdoor functions	In this paper, we propose a new cryptographic primitive called inner-product lossy trapdoor function (IPLTDF). We give a formal definition, and a concrete construction from lattices. We then show this primitive is useful to obtain efficient chosen-plaintext secure inner-product encryption (IPE) schemes. The resulting IPE scheme has almost the same public key size for multi-bit encryption compared with a recent IPE scheme proposed by Agrawal, Freeman and Vaikuntanathan [] for single-bit encryption. Unfortunately, our IPE scheme only supports attribute vectors with logarithmic length. On the positive side, our basic IPE scheme can be extended to achieve chosen-ciphertext (CCA) security. As far as we are aware, this is the first CCA-secure IPE scheme based on lattices.	lossy compression;trapdoor function	Xiang Xie;Rui Xue;Rui Zhang	2012		10.1007/978-3-642-31284-7_12	theoretical computer science;lattice;mathematics;computer security;algorithm	Theory	-38.36714946292602	77.61795078406284	4279
dea3d1d59d676c9fca2ec50619a902ee92b5c306	using exact feasibility tests for allocating real-time tasks in multiprocessor systems	software metrics;system testing real time systems multiprocessing systems processor scheduling upper bound computational efficiency measurement partitioning algorithms costs;high processor utilization exact feasibility tests real time task allocation partitioning schemes multiprocessor real time systems higher processor utilization enhanced schedulability schedulability limit bin packing algorithms processor allocation metrics np hard optimal algorithm maximum achievable utilization dynamic priority policies fixed priority policies task sets;processor scheduling;resource allocation;computational complexity;software metrics multiprocessing systems real time systems resource allocation processor scheduling computational complexity;multiprocessing systems;real time systems	This paper introduces improvements in partitioning schemes for multiprocessor real-time systems which allow higher processor utilization and enhanced schedulability by using exact feasibility tests to evaluate the schedulability limit of a processor. The paper analyzes how to combine these tests with existing bin-packing algorithms for processor allocation and provides new variants which are exhaustively evaluated using two assumptions: variable and fixed number of processors. The problem of evaluating this algorithms is complex, since metrics are usually based on comparing the performance of a given algorithm to an optimal one, but determining optimals is often NP-hard on multiprocessors. This problem has been overcome by defining lower or upper bounds on the performance of an optimal algorithm and then defining metrics with respect this bounds. The evaluation has shown that the algorithms exhibit extremely good behaviors and they can be considered very close to the maximum achievable utilization. It is also shown that dynamic priority policies produces significantly better results than fixed priorities policies when task sets require high processor utilizations.	algorithm;bin packing problem;central processing unit;clustered file system;earliest deadline first scheduling;multiprocessing;np-hardness;real-time clock;real-time computing;refinement (computing);requirement;scheduling (computing);set packing	Sergio Saez;Joan Vila i Carbó;Alfons Crespo	1998		10.1109/EMWRTS.1998.685068	parallel computing;real-time computing;resource allocation;computer science;distributed computing;computational complexity theory;software metric	Embedded	-11.49575803511874	60.49430859404094	4284
0ba636efd989dcce0088c779ae617ef9104a6f10	efficient scheme of verifying integrity of application binaries in embedded operating systems	application binaries;integrity;operating system;embedded operating system;ubiquitous computing	Currently, embedded systems have been widely used for ubiquitous computing environments including digital setup boxes, mobile phones, and USN (Ubiquitous Sensor Networks). The significance of security has been growing as it must be necessarily embedded in all these systems. Up until now, many researchers have made efforts to verify the integrity of applied binaries downloaded in embedded systems. The research of problem solving is organized into hardware methods and software-like methods. In this research, the basic approach to solving problems from the software perspective was employed. From the software perspective, unlike in the existing papers (Seshadri et al., Proc. the IEEE symposium on security and privacy, 2004; Seshadri et al., Proc. the symposium on operating systems principals, 2005) based on the standardized model (TTAS.KO-11.0054. http://www.tta.or.kr 2006) publicized in Korea, there is no extra verifier and conduct for the verification function in the target system. Contrary to the previous schemes (Jung et al. http://ettrends.etri.re.kr/PDFData/23-1_001_011.pdf , 2008; Lee et al., LNCS, vol. 4808, pp. 346–355, 2007), verification results are stored in 1 validation check bit, instead of storing signature value for application binary files in the i-node structure for the purpose of reducing run-time execution overhead. Consequently, the proposed scheme is more efficient because it dramatically reduces overhead in storage space, and when it comes to computing, it performs one hash algorithm for initial execution and thereafter compares 1 validation check bit only, instead of signature and hash algorithms for every application binary. Furthermore, in cases where there are frequent changes in the i-node structure or file data depending on the scheme application, the scheme can provide far more effective verification performance compared to the previous schemes.	algorithm;binary file;canonical account;embedded operating system;embedded system;hash function;jung;lecture notes in computer science;mobile phone;ntruencrypt;overhead (computing);parity bit;problem solving;requirement;run time (program lifecycle phase);ubiquitous computing;verification and validation;xtr	Soon-Seok Kim;Deok-Gyu Lee;Jong Hyuk Park	2010	The Journal of Supercomputing	10.1007/s11227-010-0465-4	embedded system;embedded operating system;parallel computing;real-time computing;computer science;theoretical computer science;operating system;distributed computing;computer security;ubiquitous computing	Security	-46.85747894219731	74.95254166486953	4286
c36954178075cf15cda1d5bd7e8904cd828cad47	unifying network configuration and service assurance with a service modeling language	network provisioning;protocols;spine;formal models;service modeling language;quality of service parameters;service management;formal languages;backbone switch provisioning;telecommunication computing;declarative service model;packet switched;packet switching;data communication;modeling language;ip networks switches protocols quality of service spine monitoring packet switching aggregates technology management data communication;technology management;service model;transport protocols;monitoring;ip protocol;backbone switches;formal languages quality of service telecommunication computing transport protocols packet switching telecommunication network management multimedia communication;aggregates;network configuration;multimedia communication;edge devices;ip networks;formal models network configuration service assurance service modeling language declarative service model service definition network provisioning element configuration ip protocol packet switched networks backbone switches edge devices user level services quality of service parameters network level aggregate services backbone switch provisioning service management multimedia data transmission;element configuration;packet switched networks;quality of service;switches;user level services;multimedia data transmission;service provision;network level aggregate services;service definition;telecommunication network management;service assurance	A declarative service model is introduced to describe the multiple facets of service definition, network provisioning, and element configuration. This approach addresses the emergent large packet-switched networks based on the IP protocol that involve high capability backbone switches in conjunction with the edge devices supporting protocol adaptation for the end users. A modeling language is used to define the various aspects of user-level services such as rate, duration, connectivity, and quality-of-service parameters. These definitions can be processed to automatically determine the network-level aggregate services most relevant for backbone switch provisioning. On the other hand, these service definitions can be transformed to generate detailed configuration parameters for a large number of edge devices.	aggregate data;emergence;internet backbone;modeling language;network packet;network switch;packet switching;provisioning;quality of service;service assurance;user space	Rajeev Gopal	2002		10.1109/NOMS.2002.1015619	internet protocol;communications protocol;formal language;edge device;real-time computing;quality of service;spine;service assurance;network switch;differentiated service;service management;computer science;technology management;service-oriented modeling;distributed computing;modeling language;provisioning;transport layer;packet switching;computer network	Networks	-13.965355746331884	92.9869984378625	4295
616f2bb6979cc15c5f229bbba50c293f30b67159	effiziente parallelisierung von algorithmen der digitalen bildverarbeitung und mustererkennung				Hans Burkhardt;Michael Nölle;Gerald Schreiber	1995	it+ti - Informationstechnik und Technische Informatik	10.1524/itit.1995.37.2.29	computer graphics (images);embedded system;computer science	NLP	-100.60213256499702	25.02020060827598	4300
3ded3104124e25e2cc06900df8a55456ad7244b3	sevatax: service taxonomy selection & validation process for spl development	analytical models;software;xml taxonomy proposals software computer architecture analytical models semantics;application engineering service taxonomy selection and validation process spl development variability management software product line development product selection product verification sevatax process domain engineering;semantics;computer architecture;taxonomy;xml;software product lines program verification;proposals	Variability management is a key activity in Software Product Lines (SPL) development. To be more specific, the product selection and verification allow us to generate correct derivations. In this work we present a process, called SeVaTax, to guide the selection and validation activities -both in domain and application engineering- together with a supporting tool.	assembly language;data validation;datasheet;domain engineering;domain-validated certificate;heart rate variability;information needs;knowledge-based configuration;metamodeling;software product line;well-formed element	Matias Pol'la;Agustina Buccella;Maximiliano Arias;Alejandra Cechich	2015	2015 34th International Conference of the Chilean Computer Science Society (SCCC)	10.1109/SCCC.2015.7416580	domain analysis;reliability engineering;personal software process;verification and validation;software engineering process group;software verification;computer science;systems engineering;software development;domain engineering;software construction;database;software development process	SE	-56.84460371070541	29.03618495605547	4301
97da9a0315c294047fe51930b0d4882a043b7bca	web-log-driven business activity monitoring	benchmarking;site web;legislation;business activity monitoring;red www;chaine approvisionnement;e commerce;processus metier;information technology;reseau web;technologie information;bam metrics;business activity management;supply chains;business process transformation;internet supply chains supply chain management business process re engineering organisational aspects legislation;transformation processus base web;internet;monitoring supply chains magnesium compounds business process re engineering logistics costs investments manufacturing industries risk management pricing;world wide web;proceso oficio;supply chain;sitio web;regulatory compliance exceptions web log driven business activity monitoring business process transformation business optimization ibm supply chain organization logistics;business process re engineering;tecnologia informacion;supply chain management;web site;business process;organisational aspects	Business process transformation defines a new level of business optimization that manifests as a range of industry-specific initiatives that bring processes, people, and information together to optimize efficiency. This new optimization level is possible because the Web has assumed the role of a common infrastructure. To examine how BPT can optimize an organization's processes, we describe a corporate initiative that was developed within IBM's supply chain organization to transform the import compliance process that supports the company's global logistics. The initiative sought to give IBM greater awareness of regulatory compliance exceptions - information critical to the corporation and its importing partners.	business activity monitoring;business process;logistics;mathematical optimization;world wide web	Savitha Srinivasan;Vikas Krishna;Scott Holmes	2005	Computer	10.1109/MC.2005.109	supply chain management;computer science;software engineering;supply chain;management;information technology	AI	-73.75937463295361	8.02058622206924	4318
4080d4ce202a048cf2695128d485811f2d969c7d	the puzzle of japanese software	tecnologia electronica telecomunicaciones;computacion informatica;grupo de excelencia;ciencias basicas y experimentales;tecnologias	Searching for answers to the many questions of software production.		Michael A. Cusumano	2005	Commun. ACM	10.1145/1070838.1070858	programming language;software;software engineering;computer science	OS	-98.80334232039576	11.466882825349183	4321
99ee08907cc2cdaf8d8a38d34a12964e65c1d72c	development of models for computer systems of processing information and control for tasks of ergonomic improvements		The questions of search of ergonomic reserves of efficiency of computer systems of processing information and control are considered. A set of models of a computer system of processing information and control, describing it in the necessary sections, was developed. The results can be useful in design of information provision for Decision Support Systems, devoted to questions of programs ergonomic quality of automated systems.	human factors and ergonomics	Evgeniy Lavrov;Nadiia Pasko	2018		10.1007/978-3-319-99972-2_8	information processing;decision support system;human factors and ergonomics;systems engineering;computer science	DB	-66.2010242467529	6.677983229153672	4326
d9a4f4c3ed4f1e4990ee3fd8ffb6af35a472930b	optimizing model-based software product line testing with graph transformations		Software Product Lines (SPLs) are increasing in relevance and importance as various domains strive to cope with the challenges of supporting a high degree of variability in modern software systems. Especially the systematic testing of SPLs is non-trivial as a high degree of variability implies a vast number of possible products. As testing every valid product individually quickly becomes infeasible, heuristics are often used to choose a representative subset of products to be tested. MoSo-PoLiTe (Model-Based Software Product Line Testing) is a framework for SPL testing that combines and applies combinatorial (in particular pairwise) and model-based testing to SPL feature models. In this paper, we (1) present MoSo-PoLiTe as a novel case study for graph transformations in general and Story Driven Modelling (SDM) in particular, (2) show why we consider SDMs to be ideal for rapid prototyping optimization strategies in this context, and (3) evaluate our implemented optimizations and quantify the realized improvements for MoSo-PoLiTe.	feature model;graph rewriting;heart rate variability;heuristic (computer science);java;knowledge-based configuration;mathematical optimization;model-based testing;optimizing compiler;rapid prototyping;relevance;scalability;software product line;software system;solver;spl (unix)	Anthony Anjorin;Sebastian Oster;Ivan Zorcic;Andy Schürr	2012	ECEASST	10.14279/tuj.eceasst.47.724	orthogonal array testing;software performance testing;white-box testing;computer science;theoretical computer science;software construction;software testing;algorithm	SE	-56.20104087143593	30.529905204588097	4330
97d265300d8949f9baa6d8938289db7df4b104eb	b-w: öffentliche einrichtungen verzichten auf den like-button von facebook		ment“ gestartet. In zwei Arbeitsgruppen haben sich Expertinnen und Experten aus Verbraucherund Datenschutz, Wirtschaft, Forschung und Verwaltung schwerpunktmäßig mit Zahlungssicherheit und Datenschutz bei mobilen Bezahlverfahren befasst. Teilgenommen haben rund 20 Verbände, Unternehmen, Organisationen und Einrichtungen, darunter marktführende Akteure. In beiden Arbeitsgruppen ist es gelungen, einstimmig Empfehlungen zur verbrauchergerechten Angebotsgestaltung zu verabschieden, die seit dem 04. November 2013 vorliegen. Die Empfehlungen zur Zahlungssicherheit und zum Datenschutz richten sich an Anbieter von Mobile Payment-Verfahren und dienen insbesondere der Verbrauchersicherheit, den Verbraucherrechten sowie der Verbraucherinformation. Sie definieren Anforderungen an einen verbraucherfreundlichen Einsatz von Mobile Payment und enthalten grundlegende Kriterien u.a. zur Datenverarbeitung, Nutzerregistrierung und -authentifizierung, Zahlungsautorisierung, Transparenz und Kostenkontrolle sowie zu technisch-organisatorischen Sicherheitsvorkehrungen. Sie sollen im Sinne eines präventiven Verbraucherund Datenschutzes dazu beitragen, dass Mobile Bezahlverfahren von Anfang an sicher und verbrauchergerecht auf dem deutschen Markt eingeführt werden. Der Verbraucherdialog ist eine Veranstaltung des MJV in Kooperation mit der Verbraucherzentrale Rheinland-Pfalz e.V. und dem rheinland-pfälzischen Landesbeauftragten für den Datenschutz und die Informationsfreiheit. Auch Experten des Bundesamts für Sicherheit in der Informationstechnik (BSI) waren intensiv an diesem Verbraucherdialog beteiligt. Sie empfahlen unter anderem die Nutzung des neuen Personalausweises für die Kundenidentifizierung bei mobilen Bezahlverfahren als technische Lösung und formulierten zentrale Sicherheitsanforderungen an das kontaktlose mobile Bezahlen. Zum Abschluss des Verbraucherdialogs wurden die Arbeitsergebnisse in zwei Papieren zusammengefasst. Zur technischen Absicherung kontaktloser Bezahlverfahren wird demnach insbesondere empfohlen eine gegenseitige Authentisierung der Kommunikationspartner zu verwenden, Nutzinformationen, die kontaktlos ausgetauscht oder gespeichert werden, zu verschlüsseln, Vertrauensanker („Secure Elements“) zu nutzen, um potenziell unsichere Smartphones für sicheres kontaktloses Bezahlen nutzbar zu machen. Beide Ergebnispapiere sind auf der Website http://www.mjv.rlp.de/ Startseite/broker.jsp?uMen=1fb6ec57-083e-e310-caca-fc377fe9e30b verfügbar.	broadcast signal intrusion;eine and zwei;eddie (text editor);like button;mobile payment;sie (file format);smartphone;unified model	Das Bundesamt	2014	Datenschutz und Datensicherheit - DuD	10.1007/s11623-014-0019-7		Mobile	-104.01900016065822	36.86840366834448	4336
39d540af369df88cf73ca4d370321cc3924d7b3b	tardiness bounds under global edf scheduling on a multiprocessor	soft real time system;global edf scheduling;multiprocessor scheduling;processor scheduling;soft real time;multiprocessor based soft real time systems;processor scheduling real time systems scheduling algorithm computer science costs quality of service timing partitioning algorithms quantum mechanics;multiprocessor based soft real time systems global edf scheduling multiprocessor scheduling soft real time sporadic task systems;scheduling algorithm;hard real time system;soft real time sporadic task systems;hard real time;real time systems processor scheduling;real time systems	This paper considers the scheduling of soft real-time sporadic task systems under global EDF on an identical multiprocessor. Prior research on global EDF has focused mostly on hard real-time systems, where, to ensure that all deadlines are met, approximately 50% of the available processing capacity will have to be sacrificed in the worst case. This may be overkill for soft real-time systems that can tolerate bounded tardiness. In this paper, we derive tardiness bounds under preemptive and non-preemptive global EDF on multiprocessors when the total utilization of a task system is not restricted and may equal the number of processors. Our tardiness bounds depend on per-task utilizations and execution costs - the lower these values, the lower the tardiness bounds. As a final remark, we note that global EDF may be superior to partitioned EDF for multiprocessor-based soft real-time systems in that the latter does not offer any scope to improve system utilization even if bounded tardiness can be tolerated	earliest deadline first scheduling;multiprocessing;scheduling (computing)	UmaMaheswari Devi;James H. Anderson	2005		10.1109/RTSS.2005.39	fair-share scheduling;fixed-priority pre-emptive scheduling;parallel computing;real-time computing;dynamic priority scheduling;computer science;operating system;distributed computing;scheduling;multiprocessor scheduling	Embedded	-10.870139747908267	60.62978606702562	4337
aa759ac9461b8d6aa156a9080cba34e120c90355	a dash-based performance-oriented adaptive video distribution solution	video streaming;classic dash video distribution approach dash based performance oriented adaptive video distribution solution video based content limited bandwidth resource networks content adaptation particular video adaptation dynamic adaptive streaming over http standard adaptation mechanism video delivery mechanism user quality of experience user qoe video providers local network remote hosts local nodes video segment delivery delivered video quality;streaming media servers performance evaluation media bit rate peer to peer computing logic gates;video streaming quality of experience;quality of experience;quality of experience video distribution networking device orientation	Nowadays increasing amounts of video-based content is stored and distributed across networks for diverse use, including learning. These networks also carry multiple other traffic types, all of which are growing in intensity. In the context of limited bandwidth resource networks, content adaptation and in particular video adaptation is seen as a very promising solution. As the newly introduced Dynamic Adaptive Streaming over HTTP (DASH) standard supports video adaptation, but does not introduce any adaptation mechanism, this paper proposes a DASH-based performance oriented Adaptive Video distribution solution (DAV) which improves video delivery mechanism to ensure better viewing experience. DAV's goal is to increase user Quality of Experience (QoE) by considering various factors, including characteristics of the links connecting video providers and the local network, quantity of requested content available locally, and device and user profiles. This solution dynamically selects best performing sources (among remote hosts and local nodes) for the delivery of video segments. Preliminary evaluation shows that DAV increases the quality of delivered video in comparison with a classic DASH video distribution approach.	content adaptation;digital video;diversification (finance);dynamic adaptive streaming over http;hypertext transfer protocol;personalization;streaming media;user experience;user profile;webdav	Lejla Rovcanin;Gabriel-Miro Muntean	2013	2013 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)	10.1109/BMSB.2013.6621698	simulation;telecommunications;computer science;video tracking;multimedia	Arch	-17.205800037663156	74.56006683014803	4338
db79637c33d499036b3041ae084bf7c29643a98e	fragile watermarking schemes for image authentication: a survey		This paper presents a survey of fragile watermarking schemes for image authentication proposed in the past decade. The limited embedding capacity and extent of tampering are some of the important issues among other issues that drive the research in this area. Therefore we have presented in this survey the gist of the fragile watermarking schemes in just enough detail so that the reader may gain a fair idea of the issues, techniques adopted in general to address them and the comparison of results. The general frame work of the fragile watermarking system, different categories of attacks and parameters used to evaluate the schemes are presented in this survey. The comparative analysis and the quantitative comparison of basic schemes and their variations with improvements will help the researchers in quick review of the recent developments in this area.	authentication;digital watermarking	K. Sreenivas;V. Kamakshi Prasad	2018	Int. J. Machine Learning & Cybernetics	10.1007/s13042-017-0641-4	gist;digital watermarking;digital watermarking alliance;computer security;authentication;computer science	HCI	-47.171178551699676	67.46806849771853	4340
deeb8b2a9a9a56e2bda22099aa97861f7e6bd2ec	fellowship - simulation and modeling of a simultaneous multithreading processor	simultaneous multithreading		multithreading (computer architecture);simulation;simultaneous multithreading	Dean M. Tullsen	1996			computer architecture;simultaneous multithreading;parallel computing;computer science	EDA	-9.44985861423354	41.911392731022666	4345
ad7f5212b862257c7e7b73cb3e764b61a7f5ed78	mandatory access control and role-based access control revisited	mandatory access control;role-based access control;role based access control;access control	In this paper we reexamine the interaction between role-based access control and mandatory access control. We examine the question: from the perspect,ive of a given role graph in which the objects have been assigned security classifications, can its roles be assigned to subjects without violating mandai.ory access control rules? A detailed study of the structure of individual roles and edges in a role graph is undertaken. We show that the combinai,ion of the structure imposed by the role graphs and the MAC rules means that the possible structure of a role graph in which roles are assignable to subjects without violating MAC rules is greatly restricted.	mandatory access control;role-based access control	Sylvia L. Osborn	1997		10.1145/266741.266751	computer science;access control;role-based access control;computer security	Security	-50.24515323429538	50.403789610235386	4348
81005650171a59299c2dbb8220bb2f7c9b1cb418	design of a description model for location information (dmli)	space technology solid modeling dictionaries statistics artificial intelligence computer science design engineering wire wireless communication mobile radio mobility management;data type;location based application;location data type;location information description model design;location based application location information description model design location data type metadata element interoperability;positioning system;meta data;interoperability;metadata element;open systems meta data mobile computing;mobile computing;open systems	This paper proposes design of a description model for location information (DMLI) to solve problems of different standards according to different location-data type from various positioning systems. DMLI does not redefine metadata of location data expressed by different types, but is a classification model that includes metadata including all the types. DMLI is composed of facility, place, and service elements. DMLI can translate metadata element about existing location-data into its own metadata element using a dictionary that defines information between metadata elements related location. This study shows interoperability of DMLI between heterogeneous metadata, and adoptability of DMLI to various location-based applications for service such as searching, statistics, etc	dictionary;interoperability	Jae-Won Kim;O-Hoon Choi;Doo-Kwon Baik	2006	Fourth International Conference on Software Engineering Research, Management and Applications (SERA'06)	10.1109/SERA.2006.27	metadata modeling;interoperability;data type;computer science;data element definition;operating system;marker interface pattern;data mining;database;open system;programming language;mobile computing;metadata;world wide web;data element;meta data services;data mapping;metadata repository;data dictionary	SE	-42.386351501862784	11.511899125393631	4353
36fd20d71678ab5f3bfce1852ab891f4039d50a1	ein expertensystem für die automatische erfassung von technischer graphik	automatische erfassung von technischer;ein expertensystem	In dem vorliegenden Beitrag wird ein in Entwicklung befindliches Expertensystem zur automatischen Erfassung technischer Graphik vorgestellt. Es werden die Verfahren fur die Bearbeitung graphischer Daten, der Umfang des notwendigen Wissens, die Methode der Wissensreprasentation sowie die Organisation des Expertensystems beschrieben. Das System erstellt als Ergebnis eine symbolische Beschreibung der graphischen Vorlage.		Uwe Domogalla	1984		10.1007/978-3-662-02390-7_42		NLP	-105.23510522631933	31.795055724590476	4356
95d86ae0e8b2cbecad759110c28cde67298e40e4	insider threats: identifying anomalous human behaviour in heterogeneous systems using beneficial intelligent software (ben-ware)	assistive tool;dr leonardus arief;human behaviour;detection;anomalous behaviour;ethics;eprints newcastle university;insider threats;open access;artificial intelligence;dr carl gamble;professor aad van moorsel	"""In this paper, we present the concept of """"Ben-ware"""" as a beneficial software system capable of identifying anomalous human behaviour within a 'closed' organisation's IT infrastructure. We note that this behaviour may be malicious (for example, an employee is seeking to act against the best interest of the organisation by stealing confidential information) or benign (for example, an employee is applying some workaround to complete their job). To help distinguish between users who are intentionally malicious and those who are benign, we use human behaviour modelling along with Artificial Intelligence. Ben-ware has been developed as a distributed system comprising of probes for data collection, intermediate nodes for data routing and higher nodes for data analysis. This allows for real-time analysis with low impact on the overall infrastructure, which may contain legacy and low-power resources. We present an analysis of the appropriateness of the Ben-ware system for deployment within a large closed organisation, comprising of both new and legacy hardware, to protect its essential information. This analysis is performed in terms of the memory footprint, disk footprint and processing requirements of the different parts of the system."""	artificial intelligence;central processing unit;computation;computer;confidentiality;distributed computing;false precision;human factors and ergonomics;insider threat;low-power broadcasting;malware;memory footprint;multi-level cell;norm (social);prototype;real-time clock;requirement;routing;scalability;sensor;software deployment;software system;synthetic data;warez;while;workaround	A. Stephen McGough;David Wall;John Brennan;Georgios Theodoropoulos;Ed Ruck-Keene;Budi Arief;Carl Gamble;John S. Fitzgerald;Aad P. A. van Moorsel;Sujeewa Alwis	2015		10.1145/2808783.2808785	ethics;computer science;artificial intelligence;human behavior;operations research;computer security	SE	-60.230551548915756	63.310936270242976	4358
ef0abbe930eae769411c4d24aad5b8c13b5fc34a	ocra: a tool for checking the refinement of temporal contracts	temporal logic formal specification object oriented programming program verification software reusability software tools specification languages;formal specification;temporal logic;object oriented programming;program verification;specification languages;contracts cognition model checking context embedded systems unified modeling language automata;software reusability;software tools;case tools ocra tool temporal contract refinement checking component interface model contract based design stepwise refinement compositional verification component reuse formal verification othello contracts refinement analysis linear time temporal logic specification language discrete constraints metric constraints reasoning engine	Contract-based design enriches a component model with properties structured in pairs of assumptions and guarantees. These properties are expressed in term of the variables at the interface of the components, and specify how a component interacts with its environment: the assumption is a property that must be satisfied by the environment of the component, while the guarantee is a property that the component must satisfy in response. Contract-based design has been recently proposed in many methodologies for taming the complexity of embedded systems. In fact, contract-based design enables stepwise refinement, compositional verification, and reuse of components. However, only few tools exist to support the formal verification underlying these methods. OCRA (Othello Contracts Refinement Analysis) is a new tool that provides means for checking the refinement of contracts specified in a linear-time temporal logic. The specification language allows to express discrete as well as metric real-time constraints. The underlying reasoning engine allows checking if the contract refinement is correct. OCRA has been used in different projects and integrated in CASE tools.	asynchronous i/o;component-based software engineering;computer-aided software engineering;design by contract;embedded system;fault tree analysis;formal verification;linear temporal logic;real-time cmix;real-time computing;real-time transcription;refinement (computing);semantic reasoner;specification language;stepwise regression;top-down and bottom-up design	Alessandro Cimatti;Michele Dorigatti;Stefano Tonetta	2013	2013 28th IEEE/ACM International Conference on Automated Software Engineering (ASE)	10.1109/ASE.2013.6693137	real-time computing;formal methods;temporal logic;formal verification;computer science;design by contract;formal specification;database;refinement;programming language;object-oriented programming	SE	-42.69083092035008	31.11053611771522	4362
1c4026d2fe957c4d5a29f2e091b9ee6609882c00	predictive modeling in a polyhedral optimization space	polyhedral optimization;performance counters;iterative compilation;machine learning;loop transformation	Significant advances in compiler optimization have been made in recent years, enabling many transformations such as tiling, fusion, parallelization and vectorization on imperfectly nested loops. Nevertheless, the problem of finding the best combination of loop transformations remains a major challenge. Polyhedral models for compiler optimization have demonstrated strong potential for enhancing program performance, in particular for compute-intensive applications. But existing static cost models to optimize polyhedral transformations have significant limitations, and iterative compilation has become a very promising alternative to these models to find the most effective transformations. But since the number of polyhedral optimization alternatives can be enormous, it is often impractical to iterate over a significant fraction of the entire space of polyhedrally transformed variants. Recent research has focused on iterating over this search space either with manually-constructed heuristics or with automatic but very expensive search algorithms (e.g., genetic algorithms) that can eventually find good points in the polyhedral space. In this paper, we propose the use of machine learning to address the problem of selecting the best polyhedral optimizations. We show that these models can quickly find high-performance program variants in the polyhedral space, without resorting to extensive empirical search. We introduce models that take as input a characterization of a program based on its dynamic behavior, and predict the performance of aggressive high-level polyhedral transformations that includes tiling, parallelization and vectorization. We allow for a minimal empirical search on the target machine, discovering on average 83% of the search-space-optimal combinations in at most 5 runs. Our end-to-end framework is validated using numerous benchmarks on two multi-core platforms.	automatic vectorization;benchmark (computing);code;computer science;end-to-end principle;genetic algorithm;heuristic (computer science);high- and low-level;ibm notes;iteration;iterative method;kerrison predictor;locality of reference;machine learning;mathematical optimization;multi-core processor;numerical analysis;optimizing compiler;parallel computing;polyhedral;polyhedron;polytope model;predictive modelling;search algorithm;speedup;test suite;tiling window manager	Eunjung Park;Louis-Noël Pouchet;John Cavazos;Albert Cohen;P. Sadayappan	2011	International Symposium on Code Generation and Optimization (CGO 2011)	10.1007/s10766-013-0241-1	parallel computing;real-time computing;frameworks supporting the polyhedral model;computer science;theoretical computer science;operating system;programming language;algorithm	Arch	-17.489169476573977	37.118366032092666	4372
93e36be496e14952e6425726af69f2755591d47c	wireless multicasting of video signals based on distributed compressed sensing	compressed sensing;video multicast;graceful degradation;distributed video coding	Multicasting of video signals over wireless networks has recently become a very popular application. Here, one major challenge is to accommodate heterogeneous users who have different channel characteristics and therefore will receive different noise-corrupted video packets of the same video source that is multicasted over the wireless network. This paper proposes a distributed compressed sensing based multicast scheme (DCS-cast), where a block-wise compressed sensing (BCS) is applied on video frames to obtain measurement data. The measurement data are then packed in an interleaved fashion and transmitted over OFDM channels. At the decoder side, users with different channel characteristics receive a certain number of packets and then reconstruct video frames by exploiting motion-based information. Due to the fact that the CS-measuring and interleaved packing together produce equally-important packets, users with good channel conditions will receive more packets so as to recover a better quality, which guarantees our DCS-cast scheme with a very graceful degradation rather than cliff effects. As compared to the benchmark SoftCast scheme, our DCS-cast is able to provide a better performance when some packets are lost during the transmission. & 2014 Elsevier B.V. All rights reserved.	benchmark (computing);channel (communications);compressed sensing;discrete cosine transform;distortion;elegant degradation;encoder;fault tolerance;forward error correction;full rate;motion estimation;multicast;network packet;network topology;r-cast;relay;set packing	Anhong Wang;Bing Zeng;Hua Chen	2014	Sig. Proc.: Image Comm.	10.1016/j.image.2014.03.002	computer vision;fault tolerance;real-time computing;telecommunications;computer science;video tracking;compressed sensing;computer network	Mobile	-6.403658299364069	103.01661766125179	4376
3e7503f388dd3ebf3d1470d4f8787dca2a802225	abstracting the logical processing life cycle for entities using the recast method	structured systems analysis and design method;history;life cycle;recast method;entities;cobol;program slicing technique;software maintenance;translation rules;system analysis and design;cobol system;logical processing life cycle;software systems;systems analysis reverse engineering software tools cobol;entity life history;systems engineering and theory;logical representation;computer aided software engineering;life history;operating system;systems analysis;ssadm;computer aided software engineering system analysis and design software maintenance reverse engineering design methodology history software systems costs systems engineering and theory tiles;transformation rules;transformation rules translation rules logical processing life cycle entities recast method reverse engineering case source code cobol system structured systems analysis and design method ssadm program slicing technique logical representation entity life history;software tools;source code;tiles;program slicing;logical process;case;reverse engineering;design methodology	The reverse engineering into CASE technology method (RECAST) takes the source code for an existing COBOL system and derives a no-loss representation of the system documented in a structured systems analysis and design method (SSADM) format. One key element of the method is the abstraction of logical processing that affects the individual entities of the system. For each entity this processing is extracted from the physical implementation of the system using a program slicing technique and is then transformed into a logical representation (as an entity life history) using a set of translation and transformation rules. How the abstraction is achieved is described, and an example derived from an existing operational system that has been used as a case study for the method is given. u003e	entity;software maintenance	Helen M. Edwards;Malcolm Munro	1993		10.1109/ICSM.1993.366946	computer science;systems engineering;engineering;software engineering;database;programming language;structured systems analysis and design method	Robotics	-50.30695710934552	30.732057101281367	4379
7b17d0edd8bd71bfce731300ef2377212f0d0642	folgt auf die revolution am schreibtisch der große frust? über desktop publishing und schulungsmaßnahmen		Als Schulungsgegenstand last sich Desktop Publishing als die ziel- und zweckgerichtete, integrierte Weiterverarbeitung verschiedener PC-Anwendungen zur Drucksache beschreiben. Durch die Anbindung an den Personalcomputer zeichnet sich dieser Prozes heute als massenhafter Produktionsprozes ab. Entsprechend sind die Schulungszielgruppen heterogen. Insgesamt haben wir es mit der Gruppe der Drucksachen-Profis (Druckindustrie) und der -Laien (Buro u.a.) zu tun. DTP-Schulungsmasnahmen im Buro laufen parallel zu dortigen Strukturmasnamen und Technologieeinfuhrungen (z.B. Burokommunikation, Gruppenorganisation, Arbeitsintegration, ganzheitliche Arbeitsplatzgestaltung). Zielen die Schulungsmasnahmen bei professionellen Berufen aus der Druckindustrie vordringlich auf die Vermittlung von Technologiewissen, so kommt bei den Laien-Anwendern die Vermittlung grundlegenden typografischen Gestaltungswissens hinzu. Fur eine praktikable, systematische und differenzierte Gliederung des gesamten Bereichs der DTP-Schulungsziele wird ein horizontales und vertikales Gliederungsmodell vorgestellt, das es ermoglicht, den Komplex Desktop Publishing und Schulung systematisch zu erfassen und auf den Bedarf von Anwenderzielgruppen abzustimmen. Es wird hierfur zwischen der Systemkompetenz, der DTP-Software-Kompetenz und der Gestaltungskompetenz differenziert. Fur die Schulungsorganisation wird unterschieden zwischen einer externen Schulung, die technologieorientiert und in einem modularen Kursmodell organisiert ist, und einer betrieblichen Schulung, die arbeitsablauforientiert und als Begleitschulung konzipiert ist.		Günter Dunz-Wolff	1987		10.1007/978-3-662-01110-2_22	computer graphics (images);desktop publishing;art	NLP	-106.23749215250973	33.17868256792485	4382
7e69412a23b01604dc61a709b4a91e41f72966b4	changing mobile data analysis through deep learning	mobile communication;accelerometers;feature extraction;data analytics;biomedical monitoring;sensors;human factors;big data;deep learning	Situational and context awareness are becoming more and more important on the road toward intelligent machines and devices that can offer a comprehensive toolset for improving quality of life. The increased computational capacity of personal and smart devices, and their constantly increasing capabilities for sensing, allow a large amount of collected data to be stored, processed, and transmitted over mobile devices and networks. Consequently, fast processing and analysis of this mobile data is becoming a big challenge. In this article, the authors present common mobile context-aware applications and reference current mobile data analysis practices and approaches. They then propose using deep learning to analyze sensor data from mobile devices and discuss open issues related to this approach.	computation;context awareness;deep learning;mobile device;programming tool;smart device	Panagiotis Kasnesis;Charalampos Z. Patrikakis;Iakovos S. Venieris	2017	IT Professional	10.1109/MITP.2017.52	computer engineering;mobile broadband;computer security;mobile search;big data;data mining;computer science;mobile technology;mobile web;mobile computing;context awareness;mobile device	HCI	-42.8034385289005	50.08238289826645	4383
7b6fa5c309875a780660a2567e46be131cd5178d	new trends in high performance computing	high performance computing;new trend	Abstract This paper describes the automatically tuned linear algebra software (ATLAS) project, as well as the fundamental principles that underly it. ATLAS is an instantiation of a new paradigm in high performance library production and maintenance, which we term automated empirical optimization of software (AEOS); this style of library management has been created in order to allow software to keep pace with the incredible rate of hardware advancement inherent in Mooreu0027s Law. ATLAS is the application of this new paradigm to linear algebra software, with the present emphasis on the basic linear algebra subprograms (BLAS), a widely used, performance-critical, linear algebra kernel library.	supercomputer	Osman Yasar;Yining Deng;R. E. Tuzun;David Saltz	2001	Parallel Computing	10.1016/S0167-8191(00)00086-7	computational science;parallel computing;computer science;theoretical computer science;linear algebra;operating system;basic linear algebra subprograms;mathematics;numerical linear algebra;programming language;algorithm;algebra	HPC	-8.639872457840491	39.86914803419696	4385
560330c479dc4ea9271489f11240d0c3f56a4d28	the convergence of heterogeneous internet-connected clients within imash	estensibilidad;architecture systeme;convergence;mobile radiocommunication;aplicacion medical;telecommunication sans fil;application software;teaching tool;pervasive computing;reseau ordinateur;application server;radiocommunication service mobile;computer network;hardware architecture;convergence internet middleware computer architecture pervasive computing mobile computing application software pipelines medical services hardware;computer architecture;medical services;internet;computer experiment;telecomunicacion sin hilo;pipelines;red ordenador;content adaptation;middleware;arquitectura sistema;medical application;extensibilite;scalability;radiocomunicacion servicio movil;system architecture;mobile computing;mobile application;hardware;application medicale;wireless telecommunication	"""The ubiquitous use of computers for daily productivity is a goal that presently remains unfulfilled. We believe that the convergence of desktop and mobile applications into a seamless computing experience will provide a strong motivation for future """" anytime, anywhere computing. """" In this article we describe this convergence as the capability to perform the handoff of application sessions across heterogeneous platforms using the network as a conduit as part of our Interactive Mobile Application Support for Heterogeneous Clients (iMASH) project. To that end, we have designed a software and hardware architecture to support heterogeneous client communication with application servers. Additionally , our architecture features a distributed middleware service tier to increase scalability and a content adaptation pipeline used to adapt data appropriately for heterogeneous clients. To demonstrate our concepts, we have implemented the architecture in order to support a real-world medical application that serves as a teaching tool for radiology clinicians."""	anytime algorithm;application server;content adaptation;desktop computer;middleware;mobile app;multitier architecture;norm (social);radiology;scalability;seamless3d	Thomas Phan;George Zorpas;Rajive L. Bagrodia	2002	IEEE Wireless Communications	10.1109/MWC.2002.1016706	embedded system;application software;scalability;the internet;computer experiment;convergence;telecommunications;computer science;operating system;middleware;hardware architecture;distributed computing;pipeline transport;mobile computing;computer security;application server;computer network	HPC	-35.46853918741071	51.44118072269051	4399
0f25da33183b9848b9789654f12e6f24aa290929	deploying access control in distributed workflow	wfms;petri nets;security policy;orbac	Workflows are operational business processes. Workfow Management Systems (WFMS) are concerned with the control and coordination of these workflows. In recent years, there has been a trend to integrate WFMS in distributed inter-organizational systems. In this case malfunctioning of one WFMS can affect more than one organization, making the correct functioning of a WFMS a critical issue. Thus, an important function of WFMS is to enforce the security of these inter-organizational workflows. Several works have been done to integrate the security aspects in the workflow specification. Unfortunately, these research works generally adopt a centralized management approach and are based on static access control models. In this paper, we suggest a decentralized and dynamic approach to handle access control in workflows.	access control list;algorithm;authorization;business process;centralized computing;management system;organisation-based access control;organizational behavior;petri net;secure environment	Samiha Ayed;Nora Cuppens-Boulahia;Frédéric Cuppens	2008			computer science;knowledge management;security policy;database;distributed computing;petri net	Security	-49.16884652386667	52.885668413768265	4403
76254f95d258b6f09761d0315186f86979624c4e	providing flexible services for managing shared state in collaborative systems	different measure;collaboration tool;different collaboration semantics;management requirement;shared state;specific characteristic;flexible service;specific need;varying collaboration situation;collaborative system;corona server;shared state management service;service provider	To effectively collaborate in Internet environments, it is critical to efficiently manage the shared state of collaboration. However, the management of shared state is highly situational; different collaboration semantics require different measures tailored to their specific needs. Hence, providing a general set of services that meet the management requirements of varying collaboration situations is challenging. In this paper, we discuss our approach to providing such services. The services are made flexible by allowing collaborators to choose appropriate services based on the needs of their collaboration tools and specific characteristics of their shared state. We present the shared state management services provided by our Corona server that embodies our approach and report experience with its use.	collaborative software;internet;requirement;server (computing);state management	Hyong Sop Shim;Robert W. Hall;Atul Prakash;Farnam Jahanian	1997			service provider;knowledge management;services computing;law;world wide web	SE	-50.48354344204273	14.43138750742427	4405
c646fee2eb5a6e2c24275627c7d9893b490b3be7	efficient scheduling strategies for web services-based e-business transactions	distributed application;offre service;distributed system;commerce electronique;tratamiento transaccion;service composition;systeme reparti;comercio electronico;protocole transmission;service provider;temps service;interoperabilite;interoperabilidad;service orientation;formatage;information technology;distributed computing;service web;technologie information;service process;tiempo servicio;service time;web service;data format;orientado servicio;proceso servicio;application integration;formataje;data communication;qualite service;formatting;business environment;protocolo transmision;sistema repartido;internet;processus service;scheduling;service oriented computing;algorithme reparti;algoritmo repartido;oriente service;interoperability;quality of service;transaction processing;tecnologia informacion;distributed algorithm;proposals;service quality;ordonnancement;electronic trade;traitement transaction;reglamento;servicio web;service oriented;calidad servicio;transmission protocol	Web services platform, strongly backed by the information technology industry, is destined to change the software application integration, application interoperability, and distributed computing in radical ways. Web services platform is based on open data communication and data formatting protocols; therefore it has a very promising future in terms of adoptability. Future distributed applications in general and e-business applications in particular will be built rapidly by reusing web services components that are made available on Internet. These applications will talk to each other and utilize each other's functionality. This is in general called Service-Oriented Computing. For the service-oriented applications Quality of Service (QoS) will be a major concern considering the dependency over remote applications and Internet communication. In this paper we consider a service-oriented computing (e.g., e-business) environment where “similar” services are provided by many providers. Therefore, service clients can choose any of these service providers during run-time. Transactions (client programs that request services from multiple providers) are processed via “web services monitors” that handle service composition execution. Transaction monitor in this case need to efficiently schedule service requests to the best service provider in order to optimize the system throughput. We present some basic strategies for efficient scheduling of web services transactions. We show through a simulation-based evaluation that even basic strategies improve the system throughput substantially.	scheduling (computing);web service	Erdogan Dogdu;Venkata Mamidenna	2005		10.1007/11607380_10	service provider;web service;interoperability;distributed algorithm;middleware;web development;web modeling;mobile qos;the internet;quality of service;transaction processing;disk formatting;computer science;service delivery framework;operating system;ws-policy;service-oriented architecture;ws-addressing;database;distributed computing;services computing;web engineering;ws-i basic profile;information technology;scheduling;world wide web;computer security;service quality;universal description discovery and integration	Mobile	-28.91656397433224	43.70479636306459	4406
88e34f3f8f37a31672ead11e99f33a925da72852	ein computergestütztes lernsystem für komplexe technische bewegungen				Peter Weinmann	1982				EDA	-101.18477744758147	27.8588288694276	4407
a41b97ac9e126d5915a8b481d3e1fae4b7bfab32	surfing an odbms (maintaining www documents with o/sub 2/)	document integration;fine grained document retrieval;query language;file servers;document handling;information systems;data integrity;world wide web html web server java internet access control file systems image databases information systems database languages;image databases;authorisation;referential integrity;o 2 database;internet server;document repository;object oriented dbms;hypermedia;html;internet;user transparency;world wide web document maintenance;object oriented;file system;file system object oriented dbms o sub 2 database internet server fine grained document storage fine grained document retrieval world wide web document maintenance referential integrity document integration application objects access control versioning user transparency document repository;hypermedia object oriented databases internet file servers document handling data integrity authorisation configuration management;world wide web;object oriented databases;access control;fine grained document storage;web server;application objects;versioning;configuration management;database languages;file systems;java	Deals with the idea of an object-oriented DBMS (O/sub 2/) working as an Internet server. The approach introduced offers fine-grained storage and retrieval of World Wide Web (WWW) documents, providing sophisticated document maintenance, supporting referential integrity, document integration, integration with application objects, access control and versioning. In contrast to comparable systems, it acts transparently to the WWW users, i.e. without using proprietary query languages or protocols. While achieving the full benefits of a modern document repository, it keeps the ease of the traditional file system it replaces.	norm (social);www	Frank Buddrus;Marco Bellavia	1997		10.1109/ICDAR.1997.620627	computer science;database;world wide web;information retrieval;query language	AI	-34.609544433669164	10.09157716984597	4413
6116a42c068c56312dfd6bb1c1b6e08154172af1	beginning sap r/3 implementation at geneva pharmaceuticals		Faced with intense competition in the generics drugs industry, eroding margins, and continuous price pressures, Geneva Pharmaceuticals, the North American subsidiary of Novartis International AG, made a bold, multi-million dollar decision to reengineer all its demand and supply processes using the SAP R/3 system. This case describes Geneva’s journey through the first two of three phases of R/3 implementation from mid-1997 to mid-2000, and the company’s plans for Phase III (scheduled for completion by late-2000). It highlights initial mistakes during this journey, strategies that helped overcome those mistakes, and how R/3 delivered operational efficiencies and competitive advantage under difficult business circumstances. As the case illustrates, ERP implementation is much more than technology change, it also incorporates substantive process and people changes; and without appropriate change management strategies and experienced leadership, ERP projects are likely to fail.	sap r/3	Anol Bhattacherjee	2000	CAIS		engineering;operations management;data mining;operations research	EDA	-75.1247982664671	9.550384498890722	4414
5e67328bddd52542c2a32b8e553ca89de4bd76fd	dynamic profiling and trace cache generation	java;optimising compilers;program diagnostics;virtual machines;java virtual machine;sablevm;branch correlation graphs;cache stability;design criteria;direct-threaded java vm;dispatch rate;dynamic profiling;dynamic program optimization;instruction stream coverage;profiling mechanisms;runtime performance;state information;trace cache generation;trace cache mechanisms;trace detection;trace optimizability;trace strategy	Dynamic program optimization is increasingly important for achieving good runtime performance. A key issue is how to select which code to optimize. One approach is to dynamically detect traces, long sequences of instructions spanning multiple methods, which are likely to execute to completion. Traces are easy to optimize and have been shown to be a good unit for optimization.This paper reports on a new approach for dynamically detecting, creating and storing traces in a Java virtual machine. We first describe four important criteria for a successful trace strategy: good instruction stream coverage, low dispatch rate, cache stability, and optimizability of traces. We then present our approach based on branch correlation graphs. A branch correlation graph stores information about the correlation between pairs of branches, as well as additional state information.We present the complete design for an efficient implementation of the system, including a detailed discussion of the trace cache and profiling mechanisms. We have implemented an experimental framework to measure the traces generated by our approach in a direct-threaded Java VM (SableVM) and we present experimental results to show that the traces we generate meet the design criteria.	ball project;cpu cache;digital footprint;dynamic dispatch;dynamic programming;embedded system;entity–relationship model;file spanning;ieee transactions on computers;inline expansion;java bytecode;java virtual machine;just-in-time compilation;ken arnold;m.o.j.o.;mathematical optimization;microarchitecture;mock object;optimizing compiler;overhead (computing);profile-guided optimization;profiling (computer programming);program optimization;programming language design and implementation;run time (program lifecycle phase);sensor;theoretical computer science;threaded code;tracing (software)	Marc Berndl;Laurie J. Hendren	2003			software pipelining;electricity generation;in situ resource utilization;parallel computing;real-time computing;multidisciplinary design optimization;computer science;virtual machine;operating system;instruction scheduling;programming language;java	PL	-18.56049267569489	36.766678385995974	4415
48a4f38092d2dc91f8fb12f421494020f16192cf	checking consistency in multimedia synchronization constraints	directed graphs;graph theory;multimedia authoring;theoretical framework;data integrity;authoring systems;multimedia computing;synchronisation;multimedia authoring multimedia synchronization constraints constraint graph constraint based authoring systems directed graph graph theory consistency checking automatic spatio temporal layout generation user interactive authoring;spatial relation;directed graph;graph;synchronization constraint;consistency checking;authoring system;data integrity multimedia computing authoring systems synchronisation directed graphs;object oriented modeling multimedia systems authoring systems constraint theory graph theory master slave control systems computer science telecommunication computing laboratories;user interaction	Constraint-based design is often used to correctly author a multimedia scenario due to its flexibility and efficiency. However, such a system must provide a mechanism with which users can easily manipulate the underlying structures to meet the application requirements. This paper proposes a novel method for analyzing multimedia synchronization constraints based on the constraint graph and classification, which is essential in developing efficient system support tools for constraint-based authoring systems. We specify temporal and spatial relations between multimedia objects, and use a directed graph to represent the constraints among the objects in a multimedia scenario. Moreover, we develop a method for analyzing temporal and spatial synchronization constraints based on graph theory, solving the problems of completeness checking, consistency checking, constraints relaxation and automatic spatio-temporal layout generation in a unified theoretical framework. We also discuss the effects of user interactive authoring. Compared with other methods, the proposed approach is simpler, more efficient, and easier to implement.	algorithm;constraint graph;directed graph;graph theory;interactivity;linear programming relaxation;np-completeness;requirement	Huadong Ma;Kang G. Shin	2004	IEEE Transactions on Multimedia	10.1109/TMM.2004.830807	directed graph;computer science;graph theory;theoretical computer science;database;distributed computing	DB	-27.94529986643046	15.9043888720286	4425
ccf72b686c4c93a99e14f419ccc82866c18476dc	methoden zur ermittlung montagerelevanter eigenschaften in der automobilindustrie mithilfe von konnektivitätsgraphen				Philipp Hoffmann-Berling	2017				Vision	-100.3194769495976	25.3086492451692	4430
05848b25753d5d7096772737f028c739358dc571	cryptographic verification of test coverage claims	verification;developpement logiciel;outil logiciel;software tool;reliability;information systems;standards;principal definition;ingenieria logiciel;software engineering;computer programs;pdg;influencing predicates;slicing;test and evaluation;distributed objects;herramienta controlada por logicial;criptografia;desarrollo logicial;cryptography;test coverage;digital systems;software development;ddgraph;software component;data processing security;genie logiciel;vendors;cryptographie;source code;rapid deployment;automatic test data generation;electronic security;quality control	The market for software components is growing, driven on the ‘&demand side” by the need for rapid deployment of highly functional products, and on the “supply side” by distributed object standards. As components and component vendors proliferate, there is naturally a growing concern about quality, and the effectiveness of testing processes. White box testing, particularly the use of coverage criteria, is a widely used method for measuring the ‘thoroughness” of testing efforts. High levels of test coverage are used as indicators of good quality control procedures. Software vendors who can demonstrate high levels of test coverage have a credible claim to high quality. However, verifying such claims involves knowledge of the source code, test cases, build procedures etc. In applications where reliability and quality are critical, it would be desirable to verify test coverage claims without forcing vendors to give up valuable technical secrets. In this paper, we explore cryptographic techniques that can be used to verify such claims. Our techniques have some limitations; however, if such methods can be perfected and popularized, they can have an important “leveling” effect on the software market place: small, relatively unknown software vendors with limited resources can provide credible evidence of high-quality processes, and thus compete with much larger corporations.	component-based software engineering;cryptography;display resolution;distributed object;fault coverage;software deployment;test case;verification and validation;white-box testing	Premkumar T. Devanbu;Stuart G. Stubblebine	1997		10.1145/267895.267923	reliability engineering;quality control;verification;computer science;engineering;cryptography;software development;software engineering;reliability;database;distributed object;programming language;computer security;source code	SE	-61.44508528417285	29.739383201052547	4433
176f61676305c589261c6884008229aabb717f27	tool interoperability in the maude formal environment	church-rosser checker;maude termination;main feature;executable formal specification;current release;coherence checker;tool interoperability;maude sufficient completeness checker;high-level design decision;maude formal environment;maude inductive theorem prover	We present the Maude Formal Environment (MFE), an executable formal specification in Maude within which a user can seamlessly interact with the Maude Termination Tool, the Maude Sufficient Completeness Checker, the Church-Rosser Checker, the Coherence Checker, and the Maude Inductive Theorem Prover. We explain the high-level design decisions behind MFE, give a summarized account of its main features, and illustrate with an example the interoperation of the tools available in its current release.	interoperability;maude system	Francisco Durán;Camilo Rocha;José María Álvarez	2011		10.1007/978-3-642-22944-2_30	computer science;theoretical computer science;database;programming language	HPC	-40.91219288484319	28.47358459633455	4436
687daf5c00dd37d3057a72eaac2bd4da9357458c	crisis clever, a system for supporting crisis managers		Crisis management is a special type of collaborative approach in which the actors are subject to an uninterrupted stress. It is a quite significant issue because the consequences of crises can bring huge damages (human and economic loses). In order to learn from expertise and reduce consequences, we present in this paper our first results related to the definition of structure and interfaces in order to handle experience of crisis management. The project aims to define the CCS (Crisis Clever System) as a decision making environment based on the emergency experience feedback (Experience representation and use).	crisis (dynamical systems)	Mohamed Sediri;Nada Matta;Sophie Loriette;Alain Hugerot	2013			environmental resource management;crisis management;damages;business	AI	-65.3743995037092	4.463614846218846	4446
fcf2eda96a3e71cbb2efce558384dc39415251be	easycommit: a non-blocking two phase commit protocol		Large scale distributed databases are designed to support commercial and cloud based applications. The minimal expectation from such systems is that they ensure consistency and reliability in case of node failures. The distributed database guarantees reliability through the use of atomic commitment protocols. Atomic commitment protocols help in ensuring that either all the changes of a transaction are applied or none of them exist. To ensure efficient commitment process, the database community has mainly used the two-phase commit (2PC) protocol. However, the 2PC protocol is blocking under multiple failures. This necessitated the development of the non-blocking, three-phase commit (3PC) protocol. However, the database community is still reluctant to use the 3PC protocol, as it acts as a scalability bottleneck in the design of efficient transaction processing systems. In this work, we present Easy Commit which leverages the best of both the worlds (2PC and 3PC), that is, non-blocking (like 3PC) and requires two phases (like 2PC). Easy Commit achieves these goals by ensuring two key observations: (i) first transmit and then commit, and (ii) message redundancy. We present the design of the Easy Commit protocol and prove that it guarantees both safety and liveness. We also present a detailed evaluation of EC protocol, and show that it is nearly as efficient as the 2PC protocol.	blocking (computing);commit (data management);commitment scheme;communications protocol;distributed database;liveness;non-blocking algorithm;scalability;transaction processing system;two-phase commit protocol	Suyash Gupta;Mohammad Sadoghi	2018		10.5441/002/edbt.2018.15	database;computer science;distributed computing;two-phase commit protocol	DB	-22.787307376668917	52.3682033148686	4464
62d3ff0acadb32cb38c2aeef8f03526d2df40e9d	iso/ieee 11073 phd adapter board for standardization of legacy healthcare device	ieee standards;iso standards;blood pressure;nonstandard healthcare device iso ieee 11073 phd adapter board legacy healthcare device standardization iso ieee 11073 phd standard standard phd protocol message block legacy device information blood pressure weighting scale;handicapped aids;iso standards biomedical equipment blood pressure measurement handicapped aids health care ieee standards;blood pressure measurement;medical services iso standards biomedical monitoring protocols graphical user interfaces adaptation models;biomedical equipment;health care	As society is aging, many personal devices for healthcare have recently been developed. In response to the increase in the individual development of health devices, the ISO/IEEE 11073 PHD standard has been established for the gathering of health measurement data. However, many existing healthcare devices have not been standardized because of standardization cost and implementation difficulties. In this paper, we propose an implementation model of standardization for legacy healthcare devices which do not follow the 11073 PHD standard. The proposed system generates standard PHD protocol message blocks using legacy device information, and the generated protocol message blocks are working in the adapter board. For the experiments, we test the proposed system with two healthcare devices: blood pressure, weighting scale. The proposed system has an advantage that even if the user doesn't know the standard in detail, the user can easily standardize the non-standard healthcare devices.	experiment;iso/iec 42010	Chan Yong Park;Joon-Ho Lim;Soojin Park	2012	2012 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2012.6161953	embedded system;engineering;blood pressure;operating system;software engineering;biological engineering	Robotics	-51.8231893837498	7.818064807454722	4466
0219b39d5e54073f67204518ae59f898b2577de6	magpie: experiences in supporting semantic web browsing	application development;web pages;annotation;navigation;application development framework;semantic web;services;web browsing;open hypermedia systems;user interaction;semantic browsing	Magpie has been one of the first truly effective approaches to bringing semantics into the web browsing experience. The key innovation brought y Magpie was the replacement of a manual annotation process by an automatically associated ontology-based semantic layer over web resources, hich ensured added value at no cost for the user. Magpie also differs from older open hypermedia systems: its associations between entities in a eb page and semantic concepts from an ontology enable link typing and subsequent interpretation of the resource. The semantic layer in Magpie lso facilitates locating semantic services and making them available to the user, so that they can be manually activated by a user or opportunistically riggered when appropriate patterns are encountered during browsing. In this paper we track the evolution of Magpie as a technology for developing pen and flexible Semantic Web applications. Magpie emerged from our research into user-accessible Semantic Web, and we use this viewpoint o assess the role of tools like Magpie in making semantic content useful for ordinary users. We see such tools as crucial in bootstrapping the emantic Web through the automation of the knowledge generation process. 2007 Elsevier B.V. All rights reserved.	bootstrapping (compilers);entity;hypermedia;semantic web;web resource	Martin Dzbor;Enrico Motta;John Domingue	2007	J. Web Sem.	10.1016/j.websem.2007.07.001	navigation;semantic computing;service;semantic web rule language;data web;semantic search;semantic grid;web standards;computer science;semantic web;web navigation;social semantic web;web page;semantic web stack;database;multimedia;rapid application development;world wide web;owl-s;semantic analytics	Web+IR	-41.185661549397075	9.009521038947662	4479
99d382d24268a450b1b81ca547611e133345f2db	a unified schema matching framework		The proliferation of applications dealing with shared data radically increases the need to identify and discover the semantically corresponding elements. To cope with the difficulties of the necessary schema matching, we propose a unified framework. The framework tries to collect the most well-known work concerning schema matching in a generalized approach. We observe that nearly all of this work share in their phases but each phase may has its own steps depending on the nature of the matching approach. Therefore, we suggest a unified approach to give a complete and integrated view about schema matching. Throughout this paper, we take mainstream matching systems, namely COMA, Cupid, SemInt, and LSD as main examples to demonstrate the applicability of our approach. This paper aims mainly at presenting a unified approach for the schema matching problem.	algorithm;concurrent data structure;matching (graph theory);open research;unified framework	Alsayed Algergawy;Eike Schallehn;Gunter Saake	2007			document schema definition languages;star schema;database;document structure description;database schema;semi-structured model;schema matching;xml schema;conceptual schema;computer science	DB	-32.897448257198356	5.506675997388392	4487
41db30965762ba8fca3be3224e9a0ac6f77ae241	dynamic resource allocation for spot markets in cloud computing environments	virtual machine;predictive control;average request waiting time minimization dynamic resource allocation spot markets cloud computing environments computational resources public utilities resource demands market driven resource allocation public infrastructure as a service amazon ec2 virtual machines auction based market total revenue maximization revenue maximization customer satisfactions energy cost minimization discrete time optimal control problem model predictive control static allocation strategies;capacity allocation;virtual machines cloud computing customer satisfaction discrete time systems optimal control predictive control resource allocation;public infrastructure;resource allocation;model predictive control cloud computing resource management;computer model;resource manager;resource management;public utilities;discrete time systems;discrete time;model predictive control;customer satisfaction;optimal control;dynamic resource allocation;resource management cloud computing electricity predictive models dynamic scheduling optimal control computational modeling;computational modeling;virtual machines;waiting time;simulation study;electricity;predictive models;energy cost;prediction model;dynamic scheduling;optimal control problem;cloud computing	The advent of cloud computing promises to provide computational resources to customers like public utilities such as water and electricity. To deal with dynamically fluctuating resource demands, market-driven resource allocation has been proposed and recently implemented by public Infrastructure-as-a-Service (IaaS) providers like Amazon EC2. In this environment, cloud resources are offered in distinct types of virtual machines (VMs) and the cloud provider runs an auction-based market for each VM type with the goal of achieving maximum revenue over time. However, as demand for each type of VMs can fluctuate over time, it is necessary to adjust the capacity allocated to each VM type to match the demand in order to maximize total revenue while minimizing the energy cost. In this paper, we consider the case of a single cloud provider and address the question how to best match customer demand in terms of both supply and price in order to maximize the providers revenue and customer satisfactions while minimizing energy cost. In particular, we model this problem as a constrained discrete-time optimal control problem and use Model Predictive Control (MPC) to find its solution. Simulation studies using real cloud workloads indicate that under dynamic workload conditions, our proposed solution achieves higher net income than static allocation strategies and minimizes the average request waiting time.	amazon elastic compute cloud (ec2);cloud computing;computational resource;control theory;optimal control;simulation;virtual machine	Yiqing Li;Quanyan Zhu;Raouf Boutaba	2011	2011 Fourth IEEE International Conference on Utility and Cloud Computing	10.1109/UCC.2011.33	simulation;computer science;virtual machine;resource management;operating system;predictive modelling;model predictive control	Metrics	-22.82589017791766	63.64925359027401	4489
358958f026be85d61086b70e2a989524551563c1	vorhersage des cache-verhaltens für optimierende übersetzer				Holger Hopp	2001				Crypto	-96.31964305434778	22.568735897600355	4496
15c22371908e0e90ac2118a604f900f7976a4324	web 2.0: nachhaltige anwendungspotenziale in landwirtschaft und agribusiness?		Im Beitrag werden wichtige Web 2.0 Technologien kurz vorgestellt. Welche praktische Anwendung hat die Technologie im Bereich des Agribusiness, einschliesslich der Landwirtschaft? Bieten diese Anwendungen ihren Anwendern einen deutlichen ökonomischen Vorteil? Der Beitrag schließt mit einer subjektiven Prognose des Anwendungserfolgs einzelner Web 2.0 Technologien in der Landwirtschaft und im Agribusiness.	intentionally blank page;web 2.0	Karsten Borchard;Rolf A. E. Müller	2007			engineering;marketing;operations management;software engineering	Web+IR	-101.04484452695908	34.185530214562434	4499
ee8576da50adfd2210b522c32235951973d7fa2d	when do changes induce fixes?	distinct pattern;cvs archives;eclipse history;software system evolves;version archive;fix-inducing change;bug database;taxonomy;software systems	As a software system evolves, programmers make changes that sometimes cause problems. We analyze CVS archives for fix-inducing changes---changes that lead to problems, indicated by fixes. We show how to automatically locate fix-inducing changes by linking a version archive (such as CVS) to a bug database (such as BUGZILLA). In a first investigation of the MOZILLA and ECLIPSE history, it turns out that fix-inducing changes show distinct patterns with respect to their size and the day of week they were applied.		Jacek Sliwerski;Thomas Zimmermann;Andreas Zeller	2005	ACM SIGSOFT Software Engineering Notes	10.1145/1082983.1083147		SE	-64.06368818873366	35.32882024864636	4506
76a4b8b1875f91a0da90ab1ba5f7893cb4cdd7ea	snets: a first generation model engineering platform	developpement logiciel;modelizacion;ontologie;universite;generic model;red semantica;semantic network;function block;modelisation;reseau semantique;desarrollo logicial;software development;architecture basee modele;ontologia;model management;university;eclipse modeling framework;modeling;universidad;ontology;model driven architecture;arquitectura basada modelo;architectural style	As we are currently improving AMMA (the ATLAS Model Management Architecture), a second generation model engineering platform installed on top of the Eclipse Modeling Framework (EMF), we borrow inspiration from a previous work undertaken fifteen years ago at the University of Nantes. This initial model-engineering platform named sNets (for Semantic Networks) included several functional blocks like the sMachine, the sBrowser, the sQuery, the sAction system, etc. Several parts of these tools are still in use today. At a time when we are defining the main architectural style of the new platform, it may be helpful to come back on the initial learning of this project. This paper describes the sNets project and summarizes some lessons learnt in the course of the design and utilization of this first generation modeling framework.		Jean Bézivin	2005		10.1007/11663430_18	simulation;systems modeling;computer science;engineering;software development;ontology;semantic network	DB	-42.112592257923964	25.65768332927436	4509
7ce619adc8164b6ef18caf0f5ab32397ab52e622	transformación de especificación de requisitos en esquemas conceptuales usando diagramas de interacc		Resumen. En el ámbito de la Ingeniería del Software, diversos métodos han sido propuestos para la especificación y modelado de requisitos y el modelado conceptual. En este trabajo se presenta un Proceso de Análisis de Requisitos (PAR) que permite refinar una Especificación de Requisitos representada mediante Casos de Uso utilizando Diagramas de Secuencia extendidos con estereotipos de UML. Se define además, un conjunto de reglas que facilita la trazabilidad de los elementos de la Especificación de Requisitos a una representación equivalente en el Esquema Conceptual.	han unification;linear algebra;naruto shippuden: clash of ninja revolution 3;unified modeling language;unique name assumption	Emilio Insfrán;E. Tejadillos;Sofía Martí;M. Burbano	2002			programming language;philosophy	Security	-108.09844385292872	18.945921844056475	4512
18823d9bca22099689aacf249b1d7cec11559ba8	the nature of knowledge-intensive person-oriented services - challenges for leveraging service engineering potentials		Knowledge-intensive person-oriented services (KIPOS) are predominately existent in sectors such as health care, home care or education. They are of high economic relevance in terms of market size and growth. Yet they are laggards in terms of leveraging typical service engineering potentials as applying (partial) automation, process standardization or customer integration techniques, since the most value creating activities in service provision are bound to persons or personal knowledge. In this paper, we first analyze existing typologies from literature and derive a characteristic profile of KIPOS. Next to this, we present specific challenges for KIPOS engineering derived from qualitative interviews with service providers and observations. Our results can serve as an input for developing service engineering methods for KIPOS.	relevance	Philipp Menschner;Marco Hartmann;Jan Marco Leimeister	2010			system of systems engineering;systems engineering;information management;service provider;service delivery framework;standardization;health care;management science;health systems engineering;service design;knowledge management;engineering	HCI	-70.82608944104102	11.683916620304295	4513
54e0395695209907eb459a905c78577eae05652e	a history of quantification		Aristotle (384–322 BC), the founder of the discipline of logic, also founded the study of quantification. Normally, Aristotle begins a topic by reviewing the common opinions, including the opinions of his chief predecessors. In logic, however, he could not adopt the same strategy; before him, he reports, “there was nothing at all” (Sophistical Refutations 183b34–36). Aristotle’s theory dominated logical approaches to quantification until the nineteenth century. That is not to say that others did not make important contributions. Medieval logicians elaborated Aristotle’s theory, structuring it in the form familiar to us today. They also contemplated a series of problems the theory generated, devising increasingly complex theories of semantic relations to account for them. Textbook treatments of quantification in the seventeenth and nineteenth centuries made important contributions while also advancing some peculiar theories based on medieval contributions. Modern quantification theory emerged from mathematical insights in the middle and late nineteenth century, displacing Aristotelian logic as the dominant theory of quantifiers for roughly a century. It has become common to see the history of logic as little more than a prelude to what we now call classical first-order logic, the logic of Frege, Peirce, and their successors. Aristotle’s theory of quantification is nevertheless in some respects more powerful than its modern replacement. Aristotle’s theory combines a relational conception of quantifiers with a monadic conception of terms. The modern theory combines a monadic conception of quantifiers with a relational theory of terms. Only recently have logicians combined relational conceptions of quantifiers and terms to devise a theory of generalized quantifiers capable of combining the strengths of the Aristotelian and modern approaches. There is no theory-neutral way of defining quantification, or even of delineating the class of quantifiers. Some logicians treat determiners such as ‘all,’ ‘’every,’ ‘most,’ ‘no,’ ‘some,’ and the like as quantifiers; others think of them as denoting quantifiers. Still others think of quantifiers as noun phrases containing such determiners (‘all men,’ ‘every book,’ etc.). Some include other noun phrases (‘Aristotle,’ ‘Peter, Paul, and John,’ etc.). Some define quantifiers as variable-binding expressions; others lack the concept of a variable. My sketch of the history of our understanding of quantification thus traces the development of understandings of what is to be explained as much as how it is to be explained.	first-order logic;first-order predicate;frege;monadic predicate calculus;quantifier (logic);regular expression;relational theory;tracing (software)	Daniel Bonevac	2012		10.1016/B978-0-444-52937-4.50002-2	humanities;philosophy;literature;algorithm	Logic	-11.191293889135881	6.109632859244344	4515
a07055ad7360b9102b1f0319e9a8e29ea2f3de07	fixed-priority multiprocessor scheduling: critical instant, response time and utilization bound	real time systems multiprocessing systems parallel architectures processor scheduling;job shop scheduling;real time systems job shop scheduling time factors scheduling algorithms multicore processing;processor scheduling;naturvetenskap;natural sciences;time factors;scheduling algorithms;parallel architectures;multicore processing;multiprocessing systems;real time systems;parallel architectures fixed priority multiprocessor scheduling multicore processors realtime systems high performance requirement low power requirement	The rapid development of multi-core processors leads to a constantly increasing trend of deploying real-time systems on multi-core platforms, to satisfy the dramatically increasing high-performance and low-power requirements. This trend demands effective and efficient multiprocessor real-time scheduling techniques. The uniprocessor scheduling problem has been well studied during the last 40 years. However the multiprocessor scheduling problem to map tasks onto parallel architectures is a much harder challenge. In this work, we study several fundamental problems in multiprocessor scheduling, namely the critical instant, bounded responsiveness, and utilization bound.	central processing unit;low-power broadcasting;multi-core processor;multiprocessing;multiprocessor scheduling;real-time clock;real-time computing;real-time operating system;requirement;responsiveness;scheduling (computing);uniprocessor system	Nan Guan;Wang Yi	2012	2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops & PhD Forum	10.1109/IPDPSW.2012.305	multi-core processor;fair-share scheduling;fixed-priority pre-emptive scheduling;job shop scheduling;parallel computing;natural science;real-time computing;earliest deadline first scheduling;gang scheduling;flow shop scheduling;dynamic priority scheduling;computer science;rate-monotonic scheduling;genetic algorithm scheduling;operating system;two-level scheduling;stride scheduling;distributed computing;scheduling;least slack time scheduling;round-robin scheduling;scheduling;multiprocessor scheduling	Embedded	-6.404617693263738	59.444082014495145	4535
22e7683ba7836873a66ffaed1673fa0abe4b36ab	why iv setup for stream ciphers is difficult	stream cipher iv setup;004;stream cipher;initialization vector	In recent years, the initialization vector (IV) setup has proven to be the most vulnerable point when designing secure stream ciphers. In this paper, we take a look at possible reasons why this is the case, identifying numerous open research problems in cryptography.	block cipher;candidate key;cryptography;fast software encryption;gilbert cell;hash function;lively kernel;open research;primitive recursive function;realms;stream cipher;symmetric-key algorithm	Erik Zenner	2007			differential cryptanalysis;real-time computing;theoretical computer science;fluhrer, mantin and shamir attack;block size;key schedule;stream cipher attack;distributed computing;stream cipher	Crypto	-38.90176010354428	79.63638348426937	4537
23528cfe106666011ed17851e9e01939bd161100	secure smart health with privacy-aware aggregate authentication and access control in internet of things		Abstract With the rapid technological advancements in the Internet of Things (IoT), wireless communication and cloud computing, smart health is expected to enable comprehensive and qualified healthcare services. It is important to ensure security and efficiency in smart health. However, existing smart health systems still have challenging issues, such as aggregate authentication, fine-grained access control and privacy protection. In this paper, we address these issues by introducing SSH, a Secure Smart Health system with privacy-aware aggregate authentication and access control in IoT. In SSH, privacy-aware aggregate authentication is enabled by an anonymous certificateless aggregate signature scheme, in which usersu0027 identity information is protected based on symmetric encryption mechanisms. In addition, privacy-aware access control is based on anonymous attribute-based encryption technologies. Our formal security proofs indicate that SSH achieves batch authentication and non-repudiation under the Computational Diffie-Hellman assumption. Extensive experimental results and performance comparisons show that SSH is practical in terms of computation cost and communication overheads.	access control;aggregate data;authentication;internet of things	Yinghui Zhang;Robert H. Deng;Gang Han;Dong Zheng	2018	J. Network and Computer Applications	10.1016/j.jnca.2018.09.005	computer network;access control;wireless;encryption;symmetric-key algorithm;computation;cloud computing;overhead (business);computer science;authentication	Security	-43.621545223369935	66.10382214471113	4545
16fab5fa8476e7a1c402090e5394a7b5a4682a48	static analysis of variability in system software: the 90, 000 #ifdefs issue		System software can be configured at compile time to tailor it with respect to a broad range of supported hardware architectures and application domains. The Linux v3.2 kernel, for instance, provides more than 12,000 configurable features, which control the configurationdependent inclusion of 31,000 source files with 89,000 #ifdef blocks. Tools for static analyses can greatly assist with ensuring the quality of code-bases of this size. Unfortunately, static configurability limits the success of automated software testing and bug hunting. For proper type checking, the tools need to be invoked on a concrete configuration, so programmers have to manually derive many configurations to ensure that the configuration-conditional parts of their code are checked. This tedious and error-prone process leaves many easy to find bugs undetected. We propose an approach and tooling to systematically increase the configuration coverage (CC) in compile-time configurable system software. Our VAMPYR tool derives the required configurations and can be combined with existing static checkers to improve their results. With GCC as static checker, we thereby have found hundreds of issues in Linux v3.2, BUSYBOX, and L4/FIASCO, many of which went unnoticed for several years and have to be classified as serious bugs. Our resulting patches were accepted by the respective upstream developers.	application domain;build automation;busybox;cognitive dimensions of notations;compile time;compiler;experiment;feature model;heart rate variability;l4 microkernel family;linux;login;programmer;software bug;software testing;spatial variability;static program analysis;test automation;type system;upstream (software development)	Reinhard Tartler;Christian Dietrich;Julio Sincero;Wolfgang Schröder-Preikschat;Daniel Lohmann	2014				SE	-58.08390483712628	38.92516389467971	4547
987e479816d4de7d724fb668d822e7a036a5f9bf	lightweight pki for wsn upki		ireless sensor networks (WSN) grows in size and gain new applications in our lives ranging from military applications to civilian ones. However security in WSN was not carefully carried out, since only some symmetric encryption based protocols are proposed in literature, under the assumption that the nature of sensor nodes does not support public key encryption due to the limitation in battery and CPU power. However the new development of sensors technologies may allow more computational power and gives us the possibility to use public key encryption in WSN if the used algorithm is energy efficient such as ECC. Therefore in this paper we propose a lightweight implementation of Public Key Infrastructure (PKI). Our proposed protocol called μPKI uses public key encryption only for some specific tasks as session key setup between the base station and sensors giving the network an acceptable threshold of confidentiality and authentication.	authentication;central processing unit;confidentiality;ecc memory;encryption;network packet;public key infrastructure;public-key cryptography;sensor node;session key;symmetric-key algorithm	Benamar Kadri;Mohammed Feham;Abdallah M'hamed	2010	I. J. Network Security		computer network;computer security;wireless sensor network;key management;public key infrastructure;session key;public-key cryptography;computer science;cpu power dissipation;ranging;base station	Security	-50.21343685190693	74.94693413685951	4551
e9815279e6c8aeff567c6d71c48c5ff025e54b31	a performance engineering method for web applications	software;distributed system;user satisfaction modeling;autonomic computing performance software architectures decision making;performance;software performance evaluation;utility function;software fault tolerance;computer architecture time factors runtime business software data models computational modeling;runtime;web applications;performance metric;computer architecture;software architecture;computational modeling;time factors;internet;software performance for autonomic computing performance engineering method web applications informational distributed systems transactional distributed systems autonomic manager user satisfaction modeling utility function;user interfaces internet software architecture software fault tolerance software performance evaluation;business;informational distributed systems;performance model;transactional distributed systems;performance engineering method;software performance for autonomic computing;development time;user satisfaction;user interfaces;autonomic computing;autonomic manager;software architectures;data models;architectural style	Performance engineering for informational and transactional distributed systems must take into account both the development and runtime information about the target system and its environment. At development time, the architects have to choose from many architecture styles and consider all performance requirements across a multitude of workload. At runtime, an Autonomic Manager has to compensate for changing operating and environment conditions not accounted for at the design time and make decisions about changes in architecture so the performance requirements are met. This paper proposes a formal framework, SPAC, for making decisions with regard to a possible set of candidate architectures: usage scenarios are criteria according to which architectures are evaluated; actual performance metrics, such as response time or throughput, are obtained by solving performance models and then matched against the performance requirements; performance requirements are defined by modeling user satisfaction with a utility function. Criteria can be weighted to reflect their importance. The framework can be used both at design and run time.	autonomic computing;distributed computing;performance engineering;requirement;response time (technology);run time (program lifecycle phase);throughput;utility;web application	Marin Litoiu	2010	2010 12th IEEE International Symposium on Web Systems Evolution (WSE)	10.1109/WSE.2010.5623583	data modeling;software architecture;web application;real-time computing;the internet;performance engineering;performance;computer science;operating system;software engineering;database;programming language;user interface;computational model;law;software fault tolerance;autonomic computing	Arch	-23.717537014591674	61.39460781958707	4552
a70c7701278d2bf7b0bd1bfe3b60f11b0d2f385a	definitions of managed objects for parallel-printer-like hardware devices		This memo defines a portion of the Management Information Base (MIB) for use with network management protocols in TCP/IP based internets. In particular, it defines objects for the management of parallel-printer-like devices.	printer (computing)	Bob Stewart	1992	RFC	10.17487/RFC1318	computer science;database;structure of management information;computer security;computer network	Arch	-23.70913481033606	87.84993484722206	4558
2112ff65b703c1e19a6d64724cc9e298fc95ef09	wisedb: a learning-based workload management advisor for cloud databases		Workload management for cloud databases deals with the tasks of resource provisioning, query placement, and query scheduling in a manner that meets the application’s performance goals while minimizing the cost of using cloud resources. Existing solutions have approached these three challenges in isolation while aiming to optimize a single performance metric. In this paper, we introduce WiSeDB, a learning-based framework for generating holistic workload management solutions customized to application-defined performance goals and workload characteristics. Our approach relies on supervised learning to train cost-effective decision tree models for guiding query placement, scheduling, and resource provisioning decisions. Applications can use these models for both batch and online scheduling of incoming workloads. A unique feature of our system is that it can adapt its offline model to stricter/looser performance goals with minimal re-training. This allows us to present to the application alternative workload management strategies that address the typical performance vs. cost trade-off of cloud services. Experimental results show that our approach has very low training overhead while offering low cost strategies for a variety of performance metrics and workload characteristics.	artificial neural network;cloud computing;database;decision tree;experiment;external variable;heuristic (computer science);holism;loose coupling;machine learning;online and offline;overhead (computing);provisioning;reinforcement learning;scheduling (computing);supervised learning	Ryan Marcus;Olga Papaemmanouil	2016	PVLDB	10.14778/2977797.2977804	real-time computing;simulation;computer science;data mining;database;distributed computing	DB	-23.275921549564252	61.72831916677375	4562
9e44abcdbe6fa1df461065e1ba7d649ed7eb9e1c	a basic study on highly distributed production scheduling		Recent manufacturing systems are required to be flexible to cope with variable situation. This requirement has driven development of distributed methods of production simulation and scheduling. Recent advances in computer network technology is achieving highly distributed manufacturing systems (HDMSs) where each facility is computerized and manages itself autonomously by communicating with other facilities. A distributed simulation method for HDMSs was proposed. To take the full advantage of this method, scheduling problem should be also dis- cussed. Conventional distributed scheduling methods would be inappro- priate for HDMSs, since some elements perform processes for information integration and decision making and are therefore subject to heavy com- putational load when those methods are applied to HDMSs. This paper proposes a distributed scheduling method based on a dispatching rule where the processes for decision making are not performed by any ele- ments but indirectly by a communication protocol.	distributed manufacturing;scheduling (computing)	Eiji Morinaga;Eiji Arai;Hidefumi Wakamatsu	2012		10.1007/978-3-642-40361-3_81	fair-share scheduling;real-time computing;dynamic priority scheduling;computer science;operations management;two-level scheduling;distributed computing;scheduling	HCI	-53.11552626502041	13.066207835549708	4563
dc0f5f23a6fe3f8625cda84c070a71586ee84a37	work-in-progress paper: an analysis of the impact of dependencies on probabilistic timing analysis and task scheduling		Recently there has been a renewed interest for probabilistic timing analysis (PTA) and probabilistic task scheduling (PTS). Despite the number of works in both fields, the link between them is weak: works on the latter build upon a series of assumptions on the probabilistic behavior of each task – or instances (jobs) of it – that have not been shown how to be fulfilled by PTA. This paper makes a first step towards covering this gap with emphasis on providing the right meaning of pWCET estimate as understood by both PTA and PTS. We show that the main issue related to ensuring that PTS assumptions on pWCET estimates are captured by PTA relates to the dependencies among tasks, and even jobs of a given task. Both change the scope of applicability of pWCET estimates provided by PTA and hence, their use by PTS.		Enrico Mezzetti;Jaume Abella;Carles Hernández;Francisco J. Cazorla	2017	2017 IEEE Real-Time Systems Symposium (RTSS)	10.1109/RTSS.2017.00042	probability distribution;real-time computing;task analysis;probabilistic logic;scheduling (computing);work in process;job shop scheduling;static timing analysis;software;computer science	Embedded	-12.746405459676268	30.271938331517937	4567
8d0b185989ec6a8836e8c351e79e4c64e3385339	assessing trace evidence left by secure deletion programs	trace evidence;secure deletion;fat12 file system;windows xp;digital media	  Secure deletion programs purport to permanently erase files from digital media. These programs are used by businesses and  individuals to remove sensitive information from media, and by criminals to remove evidence of the tools or fruits of illegal  activities. This paper focuses on the trace evidence left by secure deletion programs. In particular, five Windows-based secure  deletion programs are tested to determine if they leave identifiable signatures after deleting a file. The results show that  the majority of the programs leave identifiable signatures. Moreover, some of the programs do not completely erase file metadata,  which enables forensic investigators to extract the name, size, creation date and deletion date of the “deleted” files.    		Paul Burke;J. Philip Craiger	2006			real-time computing;computer science;database;distributed computing	Security	-53.25299818325575	62.68419324926055	4568
29a89d30df8542a7e3360fa382033b084f97bcc1	checking dl-lite modularity with qbf solvers	description logic	We show how the reasoning tasks of checking various versions of conservativity for the description logic DL-Litebool can be reduced to satisfiability of quantified Boolean formulas and how off-the-shelf QBF solvers perform on a number of typical DL-Litebool ontologies.	adobe flash lite;description logic;ontology (information science);true quantified boolean formula	Roman Kontchakov;Vladislav Ryzhikov;Frank Wolter;Michael Zakharyaschev	2008			description logic;ontology (information science);theoretical computer science;algorithm;modularity;computer science	AI	-21.122431781523083	9.72375709508941	4575
8093633a45caf0acd5ad75445d6e474819d20823	daily iterations: approaching code freeze and half the team is not agile	iteration planning;program testing gui team code frost approach code freeze approach iteration planning;graphical user interfaces project management microprogramming meeting planning lead computer bugs hardware process planning system testing code standards;project management;gui team;code frost approach;code standards;strategic planning;team working;graphical user interfaces;program testing;lead;meeting planning;software development management program testing team working strategic planning project management;system testing;process planning;microprogramming;computer bugs;software development management;hardware;code freeze approach	Our team was split geographically. One team was working on an embedded application. The success of XP on an earlier, similar project, led management to decide to do XP with some members from the previous team. The embedded developers were convinced that XP would not work for hardware or firmware development. The GUI team used XP from the start. The firmware team chose to use a form of iterative waterfall.	agile software development;embedded system;firmware;freeze (software engineering);graphical user interface;iteration;waterfall model	Curtis R. Cooley	2003		10.1109/ADC.2003.1231467	simulation;systems engineering;engineering;software engineering	HCI	-67.64824838766029	25.948259704647988	4577
448e6c19a8b23dd398fad3fa986b6a5be4936067	facilitating exploration of unfamiliar source code by providing 21/2d visualizations of dynamic call graphs	module interaction;front end;software testing;computer languages;legacy software;function interaction;context information;call graph;software maintenance;software maintenance c language program compilers program visualisation;software systems;c c software system;packaging;dynamic call graph;loc system;source code 21 2d visualization module interaction function interaction loc system c c software system legacy software system dynamic call graph;navigation;visualization;c language;21 2d visualization;levels of abstraction;data structures;visualization navigation software systems lab on a chip software testing system testing data structures computer languages packaging;lab on a chip;system testing;system development;source code;program compilers;program visualisation;legacy software system	For modifying functionality of legacy software systems developers often need to work within millions of lines of unfamiliar code. In this paper we propose a concept that exploits dynamic call graphs for (a) identifying code parts that implement the functionality to be modified and (b) guiding developers while navigating from one source code file to another. The proposed concept is implemented within a tool for analyzing complex C/C++ software systems and has been tested on various million LOC systems. The tool provides a visualization front-end that permits developers to explore the system implementation on 3 levels of abstraction: (I) source code, (2) function interaction, and (3) module interaction. A 21/2D visualization view exploits perspective distortion for displaying both detailed and context information on Junctions and modules, by this, supporting developers during their comprehension tasks.	2.5d;c++;distortion;emoticon;exploit (computer security);legacy system;principle of abstraction;software system	Johannes Bohnet;Jürgen Döllner	2007	2007 4th IEEE International Workshop on Visualizing Software for Understanding and Analysis	10.1109/VISSOF.2007.4290701	kpi-driven code analysis;call graph;packaging and labeling;navigation;visualization;data structure;lab-on-a-chip;computer science;theoretical computer science;front and back ends;operating system;software engineering;software testing;programming language;software maintenance;system testing;legacy system;software system;source code	SE	-54.86482630361176	35.42606102718225	4580
441eaa6f1ec10fe7dc19bbafa49382419eb28f18	a time-lag analysis for improving communication among oss developers	distributed development;time lag analysis;oss	In the open source software (OSS) development environment, a communication time-lag among developers is more likely to happen due to time differences among locations of developers and differences of working hours for OSS development. A means for effective communication among OSS developers has been increasingly demanded in recent years, since an OSS product and its users requires a prompt response to issues such as defects and security vulnerabilities. In this paper, we propose an analysis method for observing the time-lag of communication among developers in an OSS project and then facilitating the communication.		Masao Ohira;Kiwako Koyama;Akinori Ihara;Shinsuke Matsumoto;Yasutaka Kamei;Ken-ichi Matsumoto	2009		10.1007/978-3-642-14888-0_13	systems engineering;engineering;software engineering;world wide web	SE	-67.03809451839152	28.955026533766006	4584
b4ecdfa4bb5f75fbe9f8621cef2ef0a60572ee0e	a wireless body sensor network for different health related applications	information feedback;health related application;dexternet;wireless sensor networks body sensor networks monitoring air pollution feedback ip networks web and internet services web server medical services risk management;risk management air pollution body sensor networks health care internet patient monitoring;body sensor networks;mobile device;risk factor wireless body sensor network health related application dexternet body sensor layer bsl integrated monitoring geographic location air pollution exposure personal network layer pnl wireless mobile device sensed data information feedback mobile device internet global network layer gnl web server clinical module healthcare management health problem personal health module health symptom community module participatory sensing health research module anonymous sensor data collection;wireless body sensor network;risk factor;web and internet services;geographic location;gnl;risk management;community module;bsl;integrated monitoring;personal network layer;personal health module;wireless communication;risk factors;feedback;medical services;internet;monitoring;sensed data;health research module;air pollution;health problems;participatory sensing;healthcare management;mobile handsets;health problem;personal network;ip networks;patient monitoring;clinical module;web server;information service;communities;wireless mobile device;body sensor layer;pnl;wireless sensor networks;global network layer;air pollution exposure;conferences;anonymous sensor data collection;health symptom;body sensor network;open source;health care	We present an application of an open source platform for wireless body sensor network called DexterNet to the problem of monitoring different subjects. The architecture of the system consists of three layers. At the body sensor layer (BSL), the integrated monitoring of a person’s activities, geographic location, and air pollution exposures occurs. At the personal network layer (PNL), a wireless mobile device worn by the person summarizes the sensed data, and provides information feedback. The mobile device communicates wirelessly over the Internet with the third global network layer (GNL), in which a web server provides the following four information services: a clinical module that supports the healthcare management of different health problems cases, a personal health module that supports individual prevention of some health symptoms, a community module that supports participatory sensing, and a health research module that supports the collection of anonymous sensor data for research into the risk factors associated with the given health issues. We illustrate the potential for the system to serve as a comprehensive strategy to manage different health cases . 2010 IEEE International Conference on Sensor Networks, Ubiquitous, and Trustworthy Computing 978-0-7695-4049-8/10 $26.00 © 2010 IEEE DOI 10.1109/SUTC.2010.76 1	8b/10b encoding;boost;geographic coordinate system;global network;inverter (logic gate);mobile device;open-source software;participatory sensing;personal network;server (computing);trustworthy computing;web server	Ruzena Bajcsy	2010	2010 IEEE International Conference on Sensor Networks, Ubiquitous, and Trustworthy Computing	10.1109/SUTC.2010.76	wireless wan;wireless sensor network;risk management;computer science;wireless network;body area network;key distribution in wireless sensor networks;internet privacy;wi-fi array;mobile wireless sensor network;computer security;risk factor;computer network	Mobile	-43.19235634830159	60.090225108077455	4586
1f35812523e7585775513de465d1c8ea4ddb98e6	multilevel security in upnp networks for pervasive environments	universal plug and play mobile computing security authentication authorization;ubiquitous computing authorisation computer network security data privacy;authentication authorization protocols privacy context;seamless security level negotiation protocol multilevel security upnp networks pervasive environments personal information malicious users upnp architecture service access control inherited heterogeneity privacy requirement concerns multilevel user authentication pervasive upnp services	Security has become a critical concern in pervasive environments, since personal information can be available to malicious users. In this context, some of the major drawbacks in UPnP architecture are the user authentication and service access control, which are not suitable for pervasive environments. Moreover, the inherited heterogeneity of pervasive environments brings different security and privacy requirement concerns depending on the environment and the services provided. This paper introduces a UPnP extension that not only allows multilevel user authentication for pervasive UPnP services, but also provides a flexible security approach that adapts to the network. What is more, it offers a seamless security level negotiation protocol.	access control;authentication;authorization;computer security;control point (mathematics);interoperability;markup language;multilevel security;personally identifiable information;pervasive informatics;prototype;seamless3d;universal plug and play;user interface;user profile;xacml	Thiago Bruno Melo de Sales;Leandro Melo de Sales;Hyggo Oliveira de Almeida;Angelo Perkusich;Marcello Alves de Sales Junior	2013	IEEE Transactions on Consumer Electronics	10.1109/TCE.2013.6490253	universal plug and play;internet privacy;network access control;computer security;computer network	Security	-46.07881014606864	54.89222648713429	4588
9690c62afa3522a5dc5c377e56d02d99dbd9adbd	which operating system access control technique will provide the greatest overall benefit to users?	graph transformation systems;specification;operating system;access control;methodology	ly, traditional mandatory access controls provide strong guarantees for the separation of information based on its confidentiality or integrity characteristics. However, these models also require that many important system functions be placed into trusted subjects that operate outside of the constraints of the policy model. Hence, the security of the entire system typically devolves to the security of the trusted subjects, and these systems frequently require many trusted subjects for normal operation. Furthermore, mechanisms for limiting these trusted subjects to least privilege are typically coarse-grained and must be provided separately from the ordinary mandatory access control mechanism. A different form of mandatory access control known as Type Enforcement [3] (TE) offers several advantages over the traditional model. Security labels are not required to form a partial order, so intransitive relationships can be defined to support protected subsystems and assured pipelines. The security policy logic is defined through a set of separate tables, so the security policy can be easily customized. Controls over program execution and changes in access rights (domains) are explicitly defined in the TE tables, so no separate mechanism is required for this purpose. No trusted subjects that can operate outside of the constraints of the TE tables are needed, since the tables can be configured to grant exactly those access rights that are required for privileged subjects. Users and individual programs can be easily limited to least privilege through the definition of domains and domain transitions. However, TE also has its limitations. Since the security policy logic is defined through tables and there are no implicit relationships among labels, it would be cumbersome to express a complex BLP or Biba lattice using TE, and it is more difficult to verify that TE tables provide the same guarantees for the separation of information. TE also does not directly address dynamic security policy requirements, which are often needed in real-world environments. Since no single model is likely to meet all user's needs, operating systems must be flexible in their support for security policies. Policy flexibility requires a mandatory access control architecture that provides clean separation of policy from enforcement and well-defined interfaces for obtaining policy decisions. In order to support dynamic security policy requirements, this architecture must provide a mechanism for supporting policy changes and in particular for revoking permissions, including permissions that are Copyright is held by the author/owner(s). SACMAT’01, May 3-4, 2001, Chantilly, Virginia, USA ACM 1-58113-350-2/01/00005.	confidentiality;mandatory access control;pipeline (computing);principle of least privilege;privilege (computing);requirement;table (database);trusted operating system;type enforcement	Stephen Smalley	2001		10.1145/373256.378409	embedded operating system;real-time computing;computer science;access control;methodology;database;distributed computing;computer security;specification	Security	-52.21153511162755	53.94134156483586	4589
5915ceaeed50e997a7f3e1a61440234394ce9888	the user profile for the virtual home environment	roaming user;information model;personal communication networks;telecommunication network management personal communication networks;application software;availability;information retrieval;personalized service;information analysis context aware services information retrieval calendars computer architecture computer industry application software subscriptions contracts availability;contracts;computer industry;calendars;modular information model;virtual home environment;computer architecture;user profile;flexible information model;service applications;vhe management;data access;subscriptions;network architecture;heterogeneous networks;information analysis;user data;heterogeneous network;telecommunication network management;context aware services;vhe management user data user profile virtual home environment heterogeneous networks service applications personalized service roaming user modular information model flexible information model network architecture	This article addresses user data issues for virtual home environment (VHE) management among heterogeneous networks and service applications. User data represent all information describing the network and personalized service environments related to a roaming user. After analyzing user data accessed by identified VHE functions and roles, a modular and flexible information model called a user profile has been designed. Various architecture scenarios are then addressed for the implementation of this user profile. Among them, the best one fits the needs and constraints due to existing network architectures, and eventually provides a global federating solution for VHE management.	user profile	François Bougant;Frédéric Delmond;Christine Pageot-Millet	2003	IEEE Communications Magazine	10.1109/MCOM.2003.1166662	user interface design;user modeling;heterogeneous network;computer science;operating system;database;world wide web;computer network	Visualization	-35.4356044643014	48.736945272813024	4597
30560d9ad6c1e9e7e2cca7b5bf63b0fe14a0ea2a	towards trusted online dissemination of consumer information	internet privacy;key management;group communications;telecommunication security information dissemination cryptography;data collection;secure communication;group communication;electronic services;group key management trusted online information dissemination consumer information electronic services secure communication trusted information distribution encryption;electrostatic precipitators computer science privacy telephony web and internet services information systems business application software permission cryptography;cryptography;information dissemination;telecommunication security;group key management;consumer consent;combinatorial optimization;privacy	Given the growing number of electronic services and their applications in many existing and evolving domains, such as e-tail and e-medicine, much recent attention has been focused on the collection, maintenance, and dissemination of personal information by electronic service providers (ESPs). Often, information is gathered without the active involvement of the consumers, who may be unaware of the nature of the data collected or the purposes for which these data will be used. Techniques such as cookies generally are implemented on the browsers without permission from the targeted consumer. In this paper, we propose a new framework for information dissemination to ESPs using secure communication of consumer-controlled information via trusted information distribution (TID) services. A TID service gathers information based on consumer consent and consumer-controlled profile then disseminates relevant information to targeted groups of ESPs. Using encryption and a group key management scheme, our communications mechanism securely and discriminately disseminates information to dynamic groups of ESPs that subscribe to the TID service.	e-commerce;encryption;group key;http cookie;key management;personally identifiable information;secure communication	Mohamed Eltoweissy;Mohammad Hossain Heydari;Linda Morales;Ivan Hal Sudborough	2004	37th Annual Hawaii International Conference on System Sciences, 2004. Proceedings of the	10.1109/HICSS.2004.1265660	secure communication;combinatorial optimization;computer science;cryptography;key management;database;internet privacy;privacy;world wide web;computer security;data collection	DB	-41.26230207630038	64.5175964455756	4598
ed8d35101e97d3b80f32a0c27452f29f2b8ceb32	protection switching for optical bursts using segmentation and deflection routing	optical burst switching;telecommunication network routing telecommunication switching telecommunication network reliability optical fibre networks;telecommunication network reliability;optical burst switched;red conmutacion por paquete;routing;defleccion;routage;telecommunication network;deflection routing;segmentation;packet switching;indexing terms;packet switched network;conmutacion por paquete;red fibra optica;reseau commutation paquet;enlace descendente;conmutacion optica;protection switching routing optical switches telecommunication traffic nist event detection optical burst switching burst switching optical losses analytical models;optical fibre networks;optical switching;telecommunication network routing;downlink;telecommunication switching;protection switching burst segmentation obs network optical burst switching deflection routing tail end segment downstream link failure data retransmission;red telecomunicacion;link failure;reseau fibre optique;defaillance;reseau telecommunication;deflection;deflexion;optical fiber network;failures;burst segmentation;canal descendant;commutation optique;optical fiber communication;fallo;commutation paquet;segmentacion;communication fibre optique;enrutamiento;protection and restoration	Burst segmentation in OBS networks can significantly reduce the amount of data that is lost due to contention events by dropping or deflecting only the portion of a burst that overlaps another contending burst. In this letter, we demonstrate how segmentation combined with deflection routing can be used to reduce the amount of data that is lost when network elements fail. By enabling an OBS switch to deflect the tail-end segments of bursts that are in transmission as soon as it becomes aware of a downstream link failure, the retransmission of lost data can be reduced.	burst error;downstream (software development);optical burst switching;retransmission (data networks);routing	David W. Griffith;Kotikalapudi Sriram;Nada Golmie	2005	IEEE Communications Letters	10.1109/LCOMM.2005.10006	telecommunications;computer science;packet switching;computer network	Mobile	-6.427112282257788	88.21381338033181	4599
c2a56a3c258c9397bca59987015853dbb37d7335	interactive model driven graphical user interface generation	generic model;user interface;semi automatic customization;development process;satisfiability;device independence;software engineering;interactive user interface generation;model driven;graphic user interface;information design;interaction model;interactive user interface;model driven architecture	The current multitude of devices with different screen resolutions or graphic toolkits requires different user interfaces (UIs) for the same application. Model Driven UI Development solves this problem by transforming one target device independent specification into several target device dependent UIs. However, the established Model Driven Architecture (MDA) transformation process is not flexible enough to fully support all requirements of UI development. The vision of this thesis is to bridge the gap between the capabilities of model driven software engineering and the requirements of UI development. This work introduces an interactive model driven UI development approach that gives the designer control over the UI during the development process. Additional interactive support enables the designer to make informed design decisions which will ultimately lead to more satisfying UIs.	device independence;display resolution;graphical user interface;list of toolkits;model-driven architecture;requirement;software engineering	David Raneburger	2010		10.1145/1822018.1822071	user interface design;ui data binding;simulation;human–computer interaction;computer science;systems engineering;user interface	HCI	-48.77211238011557	23.6860715107319	4600
4d009b3dcb6a9552d9720f343a518c122af59bf6	multi-agent systems for protecting critical infrastructures: a survey	critical infrastructures;multi agent systems;security	Multi-agent systems have emerged as a very significant platform in provisioning distributed and collaborative services to critical applications. Such applications require ubiquitous agent presence in the environment for monitoring, collecting data, communication, and subsequent data analysis, where the sensitivity of the application’s nature cannot be understated. Recent advances in the field of autonomous, ubiquitous, intelligent and distributed computing have led to corresponding developments in the use of collaborating multi-agents to protect critical infrastructures. Such systems have witnessed crucial demand for deployment in diverse application scenarios such as E-commerce, E-health, Network Intrusion Detection, Telematics and Transport Systems, Environmental Monitoring, as well as for distributed information processing in general. Critical infrastructures have longed for a distributed system in place for their uninterrupted and accurate operations. Multi-agents have provided one such approach towards addressing the issue of protecting such infrastructures through collaborative and distributed information processing. In this paper, a state-of-the-art on the use of multi-agent based systems for protecting five most common critical infrastructures, is presented. & 2012 Elsevier Ltd. All rights reserved.	agent-based model;autonomous robot;critical infrastructure protection;distributed computing;e-commerce payment system;information processing;intrusion detection system;multi-agent system;provisioning;software deployment;telematics	Zubair A. Baig	2012	J. Network and Computer Applications	10.1016/j.jnca.2012.01.006	computer science;information security;multi-agent system;distributed computing;computer security	AI	-44.176236763525836	53.020379056808	4602
0c47422a7bbf8b73510828e29f221840a5520651	an improved (k,p,l)-anonymity method for privacy preserving collaborative filtering		Collaborative Filtering (CF) is a successful technique that has been implemented in recommender systems and Privacy Preserving Collaborative Filtering (PPCF) aroused increasing concerns of the society. Current solutions mainly focus on cryptographic methods, obfuscation methods, perturbation methods and differential privacy methods. But these methods have some shortcomings, such as unnecessary computational cost, lower data quality and hard to calibrate the magnitude of noise. This paper proposes a (k,p,l) anonymity method that improves the existing $k$-anonymity method in PPCF. The method works as follows: First, it applies Latent Factor Model (LFM) to reduce matrix sparsity. Then it improves Maximum Distance to Average Vector (MDAV) microaggregation algorithm based on importance partitioning to increase homogeneity among records in each group which can retain better data quality and (p,l)-diversity model where $p$ is attacker's prior knowledge about users' ratings and $l$ is the diversity among users in each group to improve the level of privacy preserving. Theoretical and experimental analyses show that our approach ensures a higher level of privacy preserving based on lower information loss.	algorithm;algorithmic efficiency;collaborative filtering;computation;cryptography;data quality;differential privacy;horner's method;perturbation theory;recommender system;sql;sparse matrix	Ruoxuan Wei;Hong Shen;Hui Tian	2017	GLOBECOM 2017 - 2017 IEEE Global Communications Conference	10.1109/GLOCOM.2017.8255081	real-time computing;collaborative filtering;recommender system;differential privacy;machine learning;cryptography;information privacy;computer science;homogeneity (statistics);magnitude (mathematics);data quality;artificial intelligence	DB	-38.745752947465796	64.86004192842431	4604
f4d46e69edd6e86563fb2c1c094ba227738b1a12	a tutor agent for supporting distributed knowledge modelling in interactive product design	modelizacion;distributed system;optimisation;controleur logique programmable;concepcion ingenieria;multiagent system;engineering design;ciclo desarrollo;systeme reparti;base de connaissances;controlador logica programable;life cycle;software maintenance;conception ingenierie;distributed teams;product life cycle;educational software program;tutor agents;produccion multiple;developpement produit;production multiple;didacticiel;user assistance;modelisation;maintenance logiciel;saber hacer;knowledge modelling;sistema repartido;assistance utilisateur;distributed knowledge modelling;programmable logical controller;know how;team work;savoir faire;asistencia usuario;cycle developpement;multiple production;travail equipe;trabajo equipo;ta;base conocimiento;programa didactico;interactive product design;product design;sistema multiagente;plc constraints;modeling;desarrollo producto;systeme multiagent;knowledge base;product development	Currently, the process of problem analysis in product design involves multiple partners in a distributed environment. A tutoring approach to support knowledge modelling will be presented in this article, focusing on variables as a key component of knowledge. A set of Tutor Agents intends to aid distributed partners to coherently elicitate their know-how in order to use to analyse design problems. A granular Knowledge Base is proposed to standardise distributed knowledge, providing Product Life-Cycle (PLC) constraints (based on product knowledge) to construct optimisation models. Furthermore, a case will be presented in order to clarify the approach. The results will enable design teams to integrate constraints from PLC knowledge in order to optimise the evolution of design concepts. 400 R. Mejía-Gutiérrez, X. Fischer and F. Bennis	best practice;engineering design process;graph theory;knowledge base;mathematical optimization;michael j. fischer;multi-agent system;problem solving;prototype;search algorithm	Ricardo Mejia-Gutierrez;Xavier Fischer;Fouad Bennis	2008	IJISTA	10.1504/IJISTA.2008.017281	biological life cycle;knowledge base;simulation;systems modeling;teamwork;systems engineering;engineering;knowledge management;artificial intelligence;product lifecycle;product design;software maintenance;new product development	AI	-39.33511191806757	17.50192923854389	4611
08e130954bbfdfa7b89d1e45cb0252ae2baf9d2a	a dynamic clinical dental relational database	database management systems databases factual dentistry epidemiologic methods humans information storage and retrieval medical records systems computerized online systems tooth diseases user computer interface;dentistry;relational database;dynamic data structure clinical dynamic database relational database data logical organization data structure dental epidemiological data internal table structure clinical survey data;medical information systems;data structures;medical information systems relational databases data structures dentistry;survey data;relational databases;dentistry relational databases diseases pediatrics teeth pattern analysis prototypes data structures performance analysis	The traditional approach to relational database design is based on the logical organization of data into a number of related normalized tables. One assumption is that the nature and structure of the data is known at the design stage. In the case of designing a relational database to store historical dental epidemiological data from individual clinical surveys, the structure of the data is not known until the data is presented for inclusion into the database. This paper addresses the issues concerned with the theoretical design of a clinical dynamic database capable of adapting the internal table structure to accommodate clinical survey data, and presents a prototype database application capable of processing, displaying, and querying the dental data.	addresses (publication format);data table;database design;epidemiology;prototype;relational database	Doris Taylor;Raouf N. Gorgui-Naguib;S. Boulton	2004	IEEE Transactions on Information Technology in Biomedicine	10.1109/TITB.2004.832541	data modeling;data definition language;database theory;semi-structured data;data structure;data model;relational database;computer science;probabilistic database;data science;data administration;database model;data mining;database;view;database schema;database design;spatiotemporal database	DB	-28.968307817849954	7.502838180015722	4628
dbb48d568b816a47627a2234f8f4bacac6cd248c	embedding business intelligence systems within organisations		Embedding business intelligence systems within organisations requires a seamless integration of technology, business processes and routines into the fabric of the organisation. In this paper, we propose a set of five dimensions for embeddedness of business intelligence systems within organisations. We argue that these dimensions can be related to different types of problem. We then present four case studies and a number of key insights emerge from a cross case analysis.	business process;capability maturity model;seamless3d	Graeme G. Shanks;Nargiza Bekmamedova;Frédéric Adam;Mary Daly	2012		10.3233/978-1-61499-073-4-113	business intelligence;computer science;knowledge management;data science;embedding	AI	-73.12095274486039	8.514316037216442	4630
2c0f9b8d51eb6ddac9d91e641720cbbbaa563863	application of petri nets to communication networks			petri net;telecommunications network		1999		10.1007/BFb0097770	process architecture;petri net	EDA	-20.612247714778572	87.54550588727012	4634
8d87e7ec7dc9c65f12b6307607f39d4cf484de18	a domain equation for bisimulation	logique hennessy milner;programmation;semantica denotacional;logique mathematique;logica matematica;mathematical logic;programacion;concurrency;simultaneidad;informatique theorique;denotational semantics;simultaneite informatique;programming;semantique denotationnelle;computer theory;informatica teorica	Abramsky’s seminal article [3] is devoted to a detailed concurrency-theoretic application of the author’s “theory of domains in logical form” programme [4]. One of the main results in [3] is a logical characterization of the finitary bisimulation (cf. Theorem 5.8 on p. 191). (See [5] for a behavioural characterization of the finitary bisimulation.) More precisely, Abramsky shows that two processes in any transition system are related by the finitary bisimulation iff they are related by the preorder on processes induced by the finitary version of the domain logic for transition systems synthesized in [3]. This result is really the acid test for the goodness of the domain logic for transition systems, in that it shows that, unlike Hennessy–Milner logic [6, 7], its finitary version captures exactly the finitely observable distinctions made by the notion of bisimulation. Unfortunately, there is a subtle error in the proof of Theorem 5.8 on p. 191. The interpretation of the domain logic in [3] yields a logic which is sufficiently powerful to distinguish processes that are related by the finitary bisimulation. This is in contrast to the interpretation of the domain logic for transition systems offered by Abramsky in his doctoral dissertation [1]. The original interpretation of the domain logic is subtly different from the one published in the journal article [3], and is the correct one. In particular, the logical characterization theorem for the finitary bisimulation offered in [1, Theorem 5.5.8] does hold. The researchers who wish to apply Abramsky’s domain logic for transition systems in their work should therefore use the version presented in [1, Chap. 5] in lieu of that offered in the journal paper. In the remainder of this communication, we present a simple counterexample to Theorem 5.8 on p. 191 of [3] and show that the version of the finitary domain logic presented in [3] is strictly more expressive than the original version of the logic from [1]. Familiarity with [3] is assumed below, although some basic definitions are repeated briefly for the sake of clarity. Let (Proc, Act,→,↑) be a transition system in the sense of [3, Definition 2.1]. The finitary bisimulation, denoted by∼ F , is defined thus: for every p,q∈ Proc,	bisimulation;business logic;challenge-handshake authentication protocol;concurrency (computer science);descriptive complexity theory;hennessy–milner logic;observable;transition system	Samson Abramsky	1991	Inf. Comput.	10.1006/inco.1991.9999	programming;mathematical logic;discrete mathematics;concurrency;computer science;domain theory;mathematics;programming language;denotational semantics of the actor model;denotational semantics;algorithm	Logic	-15.098714163353533	17.90565043237489	4635
37336a83ee12d2e52c8052caee550cae1a966311	5g os: control and orchestration of services on multi-domain heterogeneous 5g infrastructures		A heterogeneous 5G infrastructure includes physical and virtual computation, storage, and networking resources, exploiting different technologies and spanning across several administrative domains. These resources can be combined into end-to-end slices that can host different services. Services consist of virtual or physical elements and have different requirements, e.g., in terms of resource demands, latency, performance guarantees. The underlying resources need to be efficiently allocated to the slices and their corresponding services. To abstract away the complexities of the underlying 5G infrastructure and to provide the common functionalities required for efficient and flexible service and slice management and orchestration, we propose 5G Operating System (5G OS). We describe the requirements of such a system, present the high-level architecture of 5G OS, and describe the design challenges for different interfaces within 5G OS. Our proposed architecture includes scalable hierarchies of multi-domain and domain-specific slice and service orchestrators, software-defined network controllers, and network function virtualization management and orchestration frameworks.	computation;end-to-end principle;file spanning;high-level architecture;network function virtualization;operating system;requirement;scalability;software-defined networking;transfer function	Sevil Dräxler;Holger Karl;Hadi Razzaghi Kouchaksaraei;Azahar Machwe;Crispin Dent-Young;Kostas Katsalis;Konstantinos Samdanis	2018	2018 European Conference on Networks and Communications (EuCNC)	10.1109/EuCNC.2018.8443210	orchestration (computing);latency (engineering);network functions virtualization;computation;scalability;architecture;computer science;distributed computing	HPC	-15.024411231562288	82.88060737177038	4639
04a635657e07a0ee2bac2e1e171db877825365a0	effective seamless remote mpi-i/o operations with derived data types using pvfs2	distributed memory;distributed system;sistema operativo;algoritmo paralelo;virtual machine;remote access;pvfs2;entrada salida;haute performance;systeme reparti;acceso remoto;parallel algorithm;visualizacion;base donnee tres grande;gestion archivos;memoria compartida;communicating process;acces a distance;distributed computing;mpi i o process;tipo dato;gestion fichier;derived data type;file management;data type;machine virtuelle;remote operation;parallel computation;algorithme parallele;proceso comunicante;input output;visualization;calculo paralelo;sistema repartido;stampi;message passing interface;operating system;visualisation;file system;teleaccion;envoi message;processus communicant;pc cluster;parallel computer;message passing;alto rendimiento;calculo repartido;systeme exploitation;mpi;very large databases;memoire repartie;type donnee;maquina virtual;mpi i o;high performance;calcul parallele;calcul reparti;teleoperation;entree sortie	Parallel computation outputs intermediate data periodically, and typically the outputs are accessed for visualization in remote operation. To realize this kind of operations with derived data types among computers which have different MPI libraries, a Stampi library was proposed. For effective data-intensive I/O, a PVFS2 file system has been supported in its remote MPI-I/O operations by introducing MPICH as an underlying MPI library. This mechanism has been evaluated on interconnected PC clusters, and sufficient performance has been achieved with huge amount of data. In this paper, architecture, execution mechanism, and preliminary performance results are reported and discussed.	input/output;message passing interface;parallel virtual file system;seamless3d	Yuichi Tsujita	2006		10.1007/11846802_35	embedded system;visualization;computer science;message passing interface;operating system;distributed computing	HPC	-17.551863728036665	43.02596254353544	4641
9b9fc9179e6ac3eac2cb8c7fe161d4060f207202	onelink managertm ems for the 7r/etm switch		October–December 2000 Copyright 2000. Lucent Technologies Inc. All rights reserved. Introduction The 7R/E† Packet Network Solution is Lucent Technologies’ newest communications product designed to provide classic telephony services and packet-based services for voice, video, and data. It will enable service providers to move forward into the age of voice and data convergence. A 7R/E-based network can support traditional telephone terminations and packet-based devices, such as digital subscriber line (DSL) and Internet protocol (IP) devices. It may interface to asynchronous transfer mode (ATM), IP, or circuit—that is, time division multiplexed (TDM)—transport networks. Merging the telecommunication operations, administration, maintenance, and provisioning (OAM&P) capabilities of the 7R/E system with data switching and dedicated element management systems (EMSs) enables numerous technologies and equipment from thirdparty vendors to work together within a unified standards-based interface.	atm turbo;data center;digital subscriber line;image scaling;local loop;multiplexing;network packet;provisioning	Kevin W. McKiou;Daniel Esposito	2000	Bell Labs Technical Journal	10.1002/bltj.2252	real-time computing;telecommunications;engineering;operating system;computer security;computer network	Networks	-19.632129284735832	89.44084432430563	4654
3bd811029edeaf5508a7c7b89007bbf6843fea04	maintaining the time in a distributed system	distributed system;systeme reparti;algorithme;synchronisation;algorithm;algorritmo;synchronization;horloge;sincronizacion;clock;service temps	To a client, one of the simplest services provided by a distributed system is a time service. A client simply requests the time from any set of servers, and uses any reply. The simplicity in this interaction, however, misrepresents the complexity of implementing such a service. An algorithm is needed that will keep a set of clocks synchronized, reasonably correct and accurate with rcspcct to a standard, and able to withstand errors such as communication failures and inaccurate clocks. This paper presents a partial solution to the problem by describing two algorithms which will keep clocks both correct and synchronized.	algorithm;client (computing);distributed computing	Keith Marzullo;Susan S. Owicki	1985	Operating Systems Review	10.1145/850776.850780	synchronization;real-time computing;telecommunications;computer science	Networks	-21.169079676680386	43.87737966654614	4657
3a241dc7c12a3d988608ed865654bfa0568d0a40	automated verification of e-cash protocols		Electronic cash (e-cash) permits secure e-payments by providing security and anonymity similar to real cash. Several protocols have been proposed to meet security and anonymity properties of e-cash. However, there are no general formal definitions that allow the automatic verification of e-cash protocols. In this paper, we propose a formal framework to define and verify security properties of e-cash protocols. To this end, we model e-cash protocols in the applied π-calculus, and we formally define five relevant security properties. Finally, we validate our framework by analyzing, using the automatic tool ProVerif, four e-cash protocols: the online and the offline Chaum protocols, the Digicash protocol, and the protocol by Petersen and Poupard.		Jannik Dreier;Ali Kassem;Pascal Lafourcade	2015		10.1007/978-3-319-30222-5_11	intelligent verification	Security	-36.4641043108293	72.35274079015402	4660
58a16972662f497a7b551d2ed16c48966d1aa1f2	remote controller for regression test in the robot framework	computer supported cooperative work;human-computer interaction;remote controller	Robot framework is an open source efficient test automation framework. Regression test is an expensive but necessary maintenance activity on modified software. Although robot framework supports test suite and can generate straightforward test report and log. However, deployment of robot framework on various testers' computer is expensive and time-consuming. In addition, testers may only use the framework in specific project because they could focus on other test technology on other projects. It is also hard to manage test reports and logs because test work can take place in the same or different place at the same or different time under the label Computer Support Cooperative Work. In this paper, we design and implement a tool called Remote Controller to solve problems existed in the robot framework. To demonstrate the usability of the Remote Controller for research and design, we use Remote Controller to explore the cost saving in different projects: Enterprise Architecture and Relay loader. © Springer-Verlag Berlin Heidelberg 2013.	regression testing;robot framework	Ziyuan Zhang;Haoran Guo;Heng Wang	2013		10.1007/978-3-642-39476-8_126	simulation;robot calibration	Robotics	-50.216308787719385	23.546593922664222	4663
10c63e0d52297b1f6715f49a6f93fc14ea00d84b	enforcing confinement in distributed storage and a cryptographic model for access control		This work is concerned with the security of the standard T10 OSD protocol, a capabilitybased protocol for object stores designed by the OSD SNIA working group. The Object Store security protocol is designed to provide access control enforcement in a distributed storage setting such as a Storage Area Network (SAN) environment. In this work we consider in particular the ability of the OSD protocol to enforce confinement, which is the property that even misbehaving participants can not leak secret information across predefined boundaries. We observe that being a “pure capability” protocol, the plain vanilla OSD protocol is incapable of enforcing confinement. We show, however, that given a trustworthy infrastructure for authentication and secure channels, the protocol can be used in a manner that achieves the desired property (and does not require any change in the message format). Thus we demonstrate that object stores can in principle be used in a standard fashion in applications that require protection against leakage of secret data. Having identified a problem and proposed a solution, we proceed to prove formally that the proposed protocol indeed meets all its security goals. In the process we refine common cryptographic models in order to be able to reason about confinement, and then devise a precise model for a distributed capability-based access-control mechanism. To our knowledge, this is the first time such a model for access-control is defined, and defining it highlights what can and cannot be achieved by such mechanisms. ∗IBM T. J. Watson Research Center, Hawthorne, NY, USA. shaih@alum.mit.edu, karger@watson.ibm.com. †IBM Haifa Research Laboratory, Haifa, Israel. dalit@il.ibm.com	access control list;authentication;clustered file system;cryptographic protocol;cryptography;object storage;spectral leakage;storage networking industry association;storage area network;thomas j. watson research center	Shai Halevi;Paul A. Karger;Dalit Naor	2005	IACR Cryptology ePrint Archive			Security	-43.701820754048796	69.59439420715962	4664
60e06e39ab6ce2b4b79a86f1c793674f47cb5031	automated refinement of first-order horn-clause domain theories	theory revision;knowledge refinement;inductive logic programming	Knowledge acquisition is a difficult, error-prone, and time-consuming task. The task of automatically improving an existing knowledge base using learning methods is addressed by the class of systems performingtheory refinement. This paper presents a system,Forte (First-Order Revision of Theories from Examples), which refines first-order Horn-clause theories by integrating a variety of different revision techniques into a coherent whole.Forte uses these techniques within a hill-climbing framework, guided by a global heuristic. It identifies possible errors in the theory and calls on a library of operators to develop possible revisions. The best revision is implemented, and the process repeats until no further revisions are possible. Operators are drawn from a variety of sources, including propositional theory refinement, first-order induction, and inverse resolution.Forte is demonstrated in several domains, including logic programming and qualitative modelling.	belief revision;cognitive dimensions of notations;coherence (physics);first-order predicate;forte 4gl;heuristic;hill climbing;horn clause;knowledge acquisition;knowledge base;logic programming;refinement (computing);theory	Bradley L. Richards;Raymond J. Mooney	1995	Machine Learning	10.1007/BF01007461	knowledge base;computer science;artificial intelligence;hill climbing;machine learning;domain theory;first-order logic;mathematics;algorithm	AI	-19.731480436287477	11.936525134286315	4668
c2826d817dec8a833f5cca7ef146749cf8b6320b	bioinformatics analysis and consensus ranking for biological high throughput data. (analyses bioinformatiques et classements consensus pour les données biologiques à haut débit)			bioinformatics;throughput	Bo Yang	2014				PL	-102.5471852210465	11.249189223172175	4672
816d7e944ea4f53bdd11b60a32d93ecc6318a9c3	didaktische grundlegung zur informatikbezogenen schulischen berufsausbildung für kaufmännische ausbildungsberufe			zur farbenlehre	Manfred König	1988				NLP	-100.4225574097444	24.515518761718788	4673
063d9281a9bea121f4a301260e19d72fb3a858a8	snap: a protocol for negotiating service level agreements and coordinating resource management in distributed systems	distributed application;distributed system;globus toolkit;systeme reparti;protocole transmission;negociation;resource manager;resource management;distributed computing;qualite service;grid;gestion recursos;protocolo transmision;sistema repartido;negociacion;rejilla;scheduling;bargaining;grille;gestion ressources;calculo repartido;ordonamiento;service level agreement;quality of service;calcul reparti;service quality;ordonnancement;data transfer;capabilite;calidad servicio;transmission protocol	A fundamental problem with distributed applications is how to map activities such as computation or data transfer onto a set of resources that will meet the application’s requirement for performance, cost, security, or other quality of service metrics. An application or client must engage in a multi-phase negotiation process with resource managers, as it discovers, reserves, acquires, configures, monitors, and potentially renegotiates resource access. We present a generalized resource management model in which resource interactions are mapped onto a well defined set of symmetric and resource independent service level agreements. We instantiate this model in (the Service Negotiation and Acquisition Protocol (SNAP) which provides integrated support for lifetime management and an at-most-once creation semantics for SLAs. The result is a resource management framework for distributed systems that we believe is more powerful and general than current approaches. We explain how SNAP can be deployed within the context of the Globus Toolkit.	computation;distributed computing;interaction;quality of service;service-level agreement	Karl Czajkowski;Ian T. Foster;Carl Kesselman;Volker Sander;Steven Tuecke	2002		10.1007/3-540-36180-4_9	quality of service;computer science;resource management;operating system;database;distributed computing;grid;scheduling;computer security;service quality;negotiation	HPC	-29.016936858895743	42.767432459564134	4682
cd288caec0a44c0baf6d0869a0c4cc4dee29fa25	nexusscout: an advanced location-based application on a distributed, open mediation platform	location-based service;mobile user;open mediation platform;data provider;advanced use case;spatial data;use case;spatial information;advanced location-based application;location-based information;spatial event;mobile object;middleware;location based service	This demo shows several advanced use cases of location-based services and demonstrates how these use cases are facilitated by a mediation middleware for spatial information, the Nexus Platform. The scenario shows how a mobile user can access location-based information via so called Virtual Information Towers, register spatial events, send and receive geographical messages or find her friends by displaying other mobile users. The platform facilitates these functions by transparently combining spatial data from a dynamically changing set of data providers, tracking mobile objects and observing registered spatial events.	location-based service;middleware	Daniela Nicklas;Matthias Großmann;Thomas Schwarz	2003			mobile search;computer science;spatial contextual awareness;database;spatial analysis;internet privacy;world wide web	Mobile	-35.547075845575584	14.787319876414616	4691
c70cd0666ebe2dad8812c51aded763fdc18af248	emergency response organization through enterprise engineering perspective		Despite the best preparedness of prevention, natural disaster can strike to everyone. Emergency response is a vital action for saving people from loss and damage. We select 2011 Thailand floods, one of the costliest disasters, as our case study. This paper focuses on lifesaving processes including rescue management, temporary evacuation shelter management, and relief management. In order to understand the whole sketch of the emergency response organization, it is necessary to know the essence of the overall processes. The research objective is to find out essential components of the emergency response organization to be guideline for emergency management. It is conducted through analysis and comparison with other cases. Based on literatures and face-to-face interviews with officers from related organizations, the general emergency response activities are visualized by DEMO. The findings can suggest fundamental transactions to policy-and-decision makers to put forward their plan for the upcoming disasters.	enterprise engineering;syn flood	Natt Leelawat;Junichi Iijima	2013		10.5220/0004520901480155	enterprise systems engineering;enterprise software;knowledge management;enterprise integration;enterprise planning system;enterprise life cycle	Web+IR	-75.54281325436926	9.308856478457415	4696
af3ab758a426f3f772e6f22d73cf37aed473644e	applying domain-specific languages in metaedit+ for product line development		This demonstration shows how domain-specific languages are applied with MetaEdit+ tool in various kinds of product lines, ranging from industry automation to consumer electronics. In the demonstration practical examples are illustrated and executed covering both domain engineering and application engineering. In particular evolution and versioning of domain knowledge and application knowledge is detailed and demonstrated.	domain engineering;domain-specific language;metaedit+;version control	Juha-Pekka Tolvanen	2017		10.1145/3109729.3109755	feature-oriented domain analysis;domain analysis;domain model;systems engineering;domain knowledge;domain engineering;computer science;software product line;ranging;domain (software engineering)	SE	-54.22419379308681	25.915829204609818	4699
862cc860bcc5bb59490627c21082e99f0c630641	steiner-optimal data replication in tree networks with storage costs	multiprocessor interconnection networks replicated databases computational complexity tree data structures storage management trees mathematics optimisation;multiprocessor interconnection networks;steiner optimal replica set steiner optimal data replication tree networks storage costs multiple locations distributed system interconnection network write requests read requests tree nodes replica nodes closest replica node minimum cost steiner tree storage cost minimum total cost;distributed system;optimisation;storage management;data replication;tree data structures;trees mathematics;interconnection network;computational complexity;intelligent networks cost function surface mount technology computer science web sites web pages internet system performance multiprocessor interconnection networks;tree network;steiner tree;replicated databases;reading and writing	We consider the problem of placing copies of objects at multiple locations in a distributed system, whose interconnection network is a tree, in order to minimize the cost of servicing read and write requests to the objects. We assume that the tree nodes have limited storage and the number of copies permitted may be limited. The set of nodes that have a copy of the object, called replica nodes, constitute the replica set of the object. Read requests of a node are serviced from the closest replica node. Write requests of a node are propagated to all the replicas of the object using a minimum cost Steiner tree that includes the writer and all replica nodes. The total cost associated with a replica set equals the cost of servicing all the read and write requests, plus the storage cost at all the replica nodes. We are interested in finding a replica set with minimum total cost, i.e. a Steiner-optimal replica set. Given a tree with n nodes, we provide an O(n/sup 6/p/sup 2/)-time algorithm for finding a Steiner-optimal replica set of size p, taking into consideration the read, write, and storage costs. Our algorithm can also find a Steiner-optimal replica set for a tree with n nodes in time O(n/sup 8/). We also demonstrate that the policy used to propagate write requests to all the replica nodes in the network affects the cost and configuration of the optimal replica set for the object.	replication (computing);steiner tree problem	Konstantinos Kalpakis;Koustuv Dasgupta;Ouri Wolfson	2001		10.1109/IDEAS.2001.938096	parallel computing;steiner tree problem;computer science;theoretical computer science;database;distributed computing;tree;computational complexity theory;replication	OS	-15.280567897838413	66.7173016256357	4702
18a8cba7324cb337adb4714f3f762962f79c2f6a	softwarekartographie: visualisierung von anwendungslandschaften und ihrer schnittstellen		Die verschiedenen betrieblichen Informationssysteme eines Unternehmens arbeiten nicht autark sondern sind in einem komplexen Netz – der Anwendungslandschaft – miteinander verwoben. Die einzelnen Beziehungen unterscheiden sich hinsichtlich ihrer technischen Realisierung und den fachlichen Aufgaben, so dass eine adäquate Beschreibung beide Dimensionen berücksichtigen muss. In unserem Forschungsprojekt Softwarekartographie haben wir den Ist-Zustand hinsichtlich der Beschreibungstechniken für Anwendungslandschaften in Zusammenarbeit mit namhaften deutschen Unternehmen erfasst und die Unternehmen nach ihren Anforderungen und relevanten Aspekten für Softwarekarten befragt. Dabei wurden erhebliche Defizite bei der adäquaten Beschreibung der Vernetzung der Anwendungslandschaften und ihrer Schnittstellen deutlich. Aufbauend auf dieser Ist-Analyse stellen wir ein Modell zur Beschreibung und Visualisierung von Schnittstellen in Anwendungslandschaften vor. Die technische Sicht dieses Modells beinhaltet die Kommunikationsart (synchron vs. asynchron), die verschiedenen Technologien und die unterstützenden Systeme. Auf fachlicher Ebene sind die definierten Verträge der Schnittstellen relevant, welche Geschäftsobjekte an den Schnittstellen über Services zugreifbar sind, welche Art des Zugriff (lesend vs. schreibend) erlaubt und wie die Kommunikationssteuerung (Online vs. Offline vs. Manuell) erfolgt. 1 Softwarekartographie und Softwarekarten Ziel des Forschungsprojektes Softwarekartographie ist es, einen grundlegenden Begriffsapparat zur Beschreibung von Anwendungslandschaften aufzubauen, ein Modell für Softwarekarten zu entwickeln und das Modell adäquat in graphische Repräsentationen umzusetzen. Diese Softwarekarten sollen Anwendungslandschaften und die relevanten Aspekte der betrieblichen Informationssysteme, aus denen sich die Anwendungslandschaft zusammensetzt, visualisieren. Wir haben zunächst in Zusammenarbeit mit den IT-Strategie-Abteilungen namhafter deutscher Unternehmen (u.a. BMW Group, Deutsche Post UB Brief, T-Com) den IstZustand zur Beschreibung von Anwendungslandschaften analysiert. Ein Ergebnis des Forschungsprojektes ist, dass eine Analyse von Anwendungslandschaften eine Betrachtung auf unterschiedlichen Ebenen (siehe Abbildung 1) erfordert (vgl. [MW04]): Auf der obersten Ebene muss die Analyse die unternehmerischen und strategischen Ziele eines Unternehmens berücksichtigen. Diese Zielsetzung eines Unternehmens wird auf der mittleren Ebene in den Geschäftsprozessen des Unternehmens abgebildet bzw. verändert existierende. Zusätzlich müssen gesetzliche Regelungen und Maßgaben betrachtet werden, die Zielsetzungen und damit auch die Geschäftsprozesse beeinflussen. Neben diesen operativen Belangen ergeben sich weitere administrative und dispositive Aufgaben (z.B. Buchhaltung, Personalwesen, Controlling, Dokumentenmanagement etc.), die bei der Erreichung der operativen Ziele unterstützen und ebenso für die Analyse der Anwendungslandschaft von Relevanz sind. Auf der untersten Ebene werden die Geschäftsprozesse/-objekte implementiert bzw. durch betriebliche Informationssysteme unterstützt, die durch unterschiedliche Technologien realisiert sind, verschiedene Softwarearchitekturen benutzen etc. Neben dieser statischen Analyse auf den unterschiedlichen Betrachtungsebenen ist eine dynamische Analyse, die die Evolution der Anwendungslandschaft berücksichtigt, von äquivalenter Bedeutung. Die Änderung von Zielen und Geschäftsprozessen kann eine Veränderung der Anwendungslandschaft zur Folge haben, da die Änderungen in existierenden oder neuen Informationssystemen abgebildet werden müssen. Ebenso kann das Auslaufen von Wartungsverträgen oder Produkten zu Veränderungen der Anwendungslandschaft führen. Aus diesen drei Betrachtungsebenen und der dynamische Komponente haben sich im Rahmen der Anforderungsanalyse, bei der wir die Unternehmen nach ihren Anforderungen an Softwarekarten befragt haben, die folgenden Kategorien von relevanten Aspekten für Softwarekarten herauskristallisiert: Die planerischen Aspekte erfassen die Veränderung der Anwendungslandschaft über die Zeit. Parallel laufende Programme und Projekte verändern die Anwendungslandschaft permanent und müssen aufeinander abgestimmt und priorisiert werden. Die Unterscheidung von Ist-, Sollund Plan-Anwendungslandschaften führt in Kombination mit dem Lebenszyklen von Informationssystemen (in Planung, in Entwicklung, im Test, in Produktion, in Ablösung und abgelöst) zu einer zeitlichen Analyse der Anwendungslandschaft. Die wirtschaftlichen Aspekte umfassen die verschiedenen Kostenarten, die bei der Entwicklung, dem Betrieb, der Wartung, dem Einkauf etc. von Informationssystemen entAbbildung 1 Statische Betrachtungsebenen standen sind, entstehen bzw. entstehen werden. Verschiedenen Kostenarten, ITKennzahlen [Kü03] und Balanced Scorecards [KN91] sollen visualisiert werden. Fachliche Aspekte kombinieren u.a. Organisationseinheiten, Prozesse, Geschäftsobjekte und Funktionsbereiche mit den Informationssystemen. Auch die Anzahl von Nutzern oder der quantifizierbare Nutzen von Informationssystemen (z.B. mittels Function Points [IFP01]) zählen zu den fachlichen Aspekten. Technische Aspekte erstrecken sich von der Implementierungssprache eines Informationssystems, über die Verbindungen bis hinzu Eigenschaften wie Architektur oder genutzter Middleware. Im Gegensatz zu Beschreibungssprachen wie UML [OMG03] fokussiert die Softwarekartographie auf die Zusammenhänge in der gesamten Anwendungslandschaft. Ziele wie Homogenisierung von Datenbanksystemen, Enterprise Application Integration [Ke02] oder Individualvs. Standardsoftware werden betrachtet. Operative Aspekte beziehen sich auf den unmittelbaren Betrieb von Informationssystemen und die verbundenen Ereignisse. Domino-Effekte bei Ausfällen oder der Ablauf von zeitgesteuerten Prozessen werden im Fokus der Anwendungslandschaft berücksichtigt. Ein Modell, das die relevanten Aspekte abbildet, muss hierbei zwischen den relevanten, den erfassbaren und pflegbaren Aspekten unterscheiden, da Unternehmen zwar alle Informationen besitzen, jedoch die Pflege der Informationen, der mit Aufwand und Kosten verbunden ist, einen entsprechenden Nutzen und Mehrwert besitzen muss. 2 Schnittstellen und Konnektoren In diesem Beitrag konzentrieren wir uns auf den technischen und fachlichen Aspekt der Schnittstellen für Softwarekarten, bei der unterschiedliche Geschäftsobjekte über verschiedenste Verbindungen zwischen Informationssystemen zugänglich gemacht werden. Die Vernetzung der Anwendungslandschaft wird von den existierenden Softwarekarten unser Projektpartner unterschiedlich dargestellt und dokumentiert, wobei Detailinformationen teilweise in externen Datenspeichern (Tabellen etc.) gepflegt werden. Werden die Darstellungsformen generalisiert, ergeben sich die folgenden Stufen (siehe Abbildung 2), die zu dem Modell in Abbildung 3 führen: Stufe 0 visualisiert die Informationssysteme in Zusammenhang mit Organisationseinheiten, Prozessen und Funktionsbereichen. Auf diese Stufe werden keine direkten Verbindungslinien zwischen den einzelnen Informationssystemen gezeichnet, sondern die Verbindungen ergeben sich indirekt durch die zweidimensionale Anordnung nach Funktionsbereichen, Prozessen etc. Werden die Informationssysteme mittels Linien oder Pfeilen miteinander verbunden (Stufe 1), soll dies Schnittstellen dokumentieren, die einzelne Systeme miteinander verbinden. Die Pfeilenden der Linien stellen entweder die Datenflussrichtung oder die Rolle des Client vs. des Servers dar und führt somit zu einer Überladung dieses Gestaltungsmittels. In der betrieblichen Praxis wird der Begriff Schnittstelle zur Bezeichnung einer Verbindung zwischen zwei Informationssystemen verwendet, was nicht der im Software Engineering üblichen Terminologie entspricht. So definiert UML eine Schnittstelle als „A named set of operations that characterize the behavior of an element.“ [OMG03]; gleiches gilt für Definitionen bei CORBA [OMG04] oder Szyperski [Sz02]. Daher vermeiden wir in unserem Modell den überladenen Begriff der Schnittstelle. Die existierenden Definitionen und Visualisierungen von Schnittstellen unterscheiden des Weiteren nicht zwischen einer technischen und einer fachlichen Sicht. Werden Schnittstellen im Kontext der Anwendungslandschaft analysiert, so treten die exakten Definitionen der einzelnen Methoden und deren Signaturen in den Hintergrund. Stattdessen sind die Konnektoren mit Kommunikationsart, Art des Zugriffs (lesend vs. schreibend) und die Technologie entscheidend. Wird von dem eigentlichen Vertrag, den ein Export-Konnektor schließt, abstrahiert, so sind der Service und die betroffenen Geschäftsobjekte für das Netz in der Anwendungslandschaft von größerer Bedeutung (siehe Abbildung 3). Stufe 2 führt somit zu einer Betrachtung von Export-Konnektoren, die Services für andere Informationssysteme anbieten und einer Unterscheidung zwischen einer fachlichen und einer technischen Ebene. Ein Informationssystem kann hierbei den gleichen Service, der durch einen fachlichen Export-Konnektor angeboten wird, mittels verschiedener Technologien exportieren, für einen fachlichen Export-Konnektor können somit mehrere technische Export-Konnektoren existieren (in den Abbildungen nicht visualisiert). Wird eine Verbindung mit Export-Konnektoren analysiert, so agiert das exportierende System als Server, welches Services gegenüber anderen Systemen (den Clients) anbietet. In einer Stufe 3 werden auf Seite der Clients die Import-Konnektoren ergänzt. Diese Import-Konnektoren nutzen beispielsweise nur Teile der Export-Konnektoren oder greifen nur lesend auf einen Export-Konnektor zu, der sowohl lesenden als auch schreibenden Zugriff anbietet. Die Visualisierung der Export-Konnektoren ist an das von UML [OMG03] und anderen Visualisierungssprachen bekannten Lollipop-Symbols angelehnt, die Import-Konnektoren werden durch ein ausgefülltes Rechteck auf der Seite des Clients dargestellt. A	altran praxis;common object request broker architecture;eine and zwei;es evm;eddie (text editor);enterprise application integration;function point;gesellschaft für informatik;information system;institut für dokumentologie und editorik;intentionally blank page;internet explorer;middleware;online and offline;software engineering;système universitaire de documentation;the daily telegraph;unified modeling language;v-model;vhf omnidirectional range	Florian Matthes;André Wittenburg	2004			engineering	OS	-101.97579710995916	32.94429493644238	4706
099498705e2ae965890679beff0f01fdb2ac4b85	modell-gestütztes framework für das testen von mess- und automatisierungssoftware für prüfstände der automobilindustrie		Die zunehmende Komplexität heutiger Softwaresysteme erfordert eine sorgfältige und systematische Qualitätssicherung. Dies gilt insbesondere für sicherheitskritische Anwendungen wie Messund Automatisierungssysteme in Prüfständen der Automobilindustrie. Die Folge ist eine stetig wachsende Anzahl von durchzuführenden Tests verschiedenster Ausprägung in verschiedenartigsten Testumgebungen. Der Beitrag gibt einen Überblick über ein bei FEV GmbH entwickeltes modell-basiertes Test-Framework, welches mittels einer domänen-spezifischen Spezifikationssprache sowohl die manuelle Definition von Tests erlaubt als auch die einfache Einbindung von TestfallGeneratoren ermöglicht. Dabei stützt sich diese Sprache sowohl auf die spezifizierbare Domäne des zu testenden Systems (SUT) als auch auf die besonderen Konzepte des Systemtests. Das Test-Framework erzeugt, zu einem großen Teil ohne manuelle Eingriffe, ausführbare TestSuiten bestehend aus der Ablauf-Logik, den Testdaten und Konfigurationen sowohl für den Prüfling selbst als auch für die Umgebungssimulation. Es unterscheidet zwischen Parametern mit und ohne Echtzeit-Verhalten und generiert entsprechende Test-Artefakte.	die (integrated circuit);eine and zwei;internet explorer;system under test;test case;v-model	Udo Oligschläger	2013	Softwaretechnik-Trends		computer science;software engineering;performance art	OS	-102.2961983990065	31.839120476371853	4711
48fcac10dfd2da3b1dbb480abb9ad0d7ae82278e	book reviews		"""A common misconception among mathematicians is to think of intuitionistic mathematics as """"mathematics without the law of the excluded middle"""" (the law asserting that every statement is either true or false). From this point of view, intuitionistic mathematics is a proper subset of ordinary mathematics, and doing your mathematics intuitionistically is like doing it with your hands tied behind your back. Another more realistic viewpoint is to regard intuitionistic logic, and the mathematics based on that logic, as the logic of sets with some structure, rather than of bare sets. Traditional examples are sets growing in time (as in Kripke semantics [9]), or set with some recursive structure (as in Kleene's realizability interpretation [7]), or sets continuously varying over some fixed parameter space. Universes of such sets are perfectly suitable for developing mathematics, but one is often forced to use intuitionistic logic. This is a small price to pay for the many new phenomena that can be observed in such universes: For example, in some such universes one has a """"recursive axiom of choice,"""" which states that for any sequence {An}n of nonempty subsets ^ c N there is a recursive choice function ƒ : N —• N selecting an element f(n) from"""	intuitionistic logic;kripke semantics;recursion	Geoffrey Sirc;C. T. Rupert	1985	Computers and the Humanities	10.1007/BF02259535		Theory	-11.524802604750018	4.57174266094774	4713
4d9ab81af8ceabcdf20d2e50f1b3adc619f7b275	beyond just data privacy	secret sharing;perforation;system performance;data privacy;distributed systems	We argue that designing a system that “guarantees” the privacy of its information may not be enough. One must also consider the price for providing that protection: For example, is the information preserved adequately? Does the system perform well? We illustrate this point by presenting the concept of a configuration that can capture the security, longevity and performance aspects of managing information. Configurations can be useful for describing the policies used to safeguard information, as well as in selecting the right mix of security, longevity and performance levels.	information privacy	Bobji Mungamuru;Hector Garcia-Molina	2007			personally identifiable information;information privacy;privacy by design;computer science;database;computer performance;internet privacy;secret sharing;world wide web;computer security	Security	-46.8256521903487	58.79528905971407	4715
73ad62ae6c985fe12664b161c81503fb8d7f4ae5	computer aided enterprise information systems engineering with bpsim studio	engineering;analytical models;software;simulation engineering developing case;design process;information systems;software modules generation;cad;systems engineering;simulation;dynamic model;multiagent resource conversion process computer aided enterprise information systems engineering bpsim studio process simulation software engineering process formalization business processes decision making process dynamic models development process re engineering software modules generation object oriented approach design process intelligence uml diagrams systems analysis simulation modeling;computer aided enterprise information systems engineering;multiagent resource conversion process;dynamic models development;information systems systems engineering and theory object oriented modeling analytical models computational modeling software engineering computer aided software engineering business process re engineering decision making optimization methods;software engineering;uml diagrams;simulation experiment;design process intelligence;computer aided software engineering;systems analysis;decision making process;developing;solid modeling;enterprise information system;case tool;unified modeling language;object oriented approach;system analysis;management information systems;process simulation;bpsim studio;simulation model;process re engineering;systems engineering cad management information systems software engineering;object oriented modeling;case;process formalization;business process;simulation modeling;business processes	The paper focuses on the actual problem of development of methods and tools for processes simulation and software engineering, that automate the whole process starting with the processes formalization, business processes and decision-making processes dynamic models development, carrying out simulation experiments with the models, bottlenecks analysis, process re-engineering and optimization, and finishing with software engineering, which includes database and software modules generation. During research many available CASE tools were studied, analyzed and searched for disadvantages. Particularly, most of the tools lacked structural and object-oriented approaches integration and design process intelligence, i.e. no interaction between various kinds of UML diagrams. Keeping in mind these factors and considering small performance on complicated systems analysis utilizing simulation modeling a multi-agent resource conversion processes based CASE tool was engineered.	agent-based model;business process;computer-aided software engineering;data flow diagram;enterprise information system;experiment;mathematical optimization;multi-agent system;simulation;software design;software developer;systems engineering;uml state machine	Konstantin A. Aksyonov;Irina A. Spitsina;Eugene A. Bykov;Elena F. Smoliy	2008	2008 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2008.4811839	unified modeling language;process simulation;computer science;function model;simulation modeling;business process;computer-aided software engineering;goal-driven software development process;enterprise information system	SE	-59.18249543313811	19.57448525349107	4733
98423e28f9b1766eaac180e01d9c20211c2eb29d	multi-tape computing with synchronous relations		We sketch an approach to encode relations of arbitrary arity as simple languages. Our main focus will be faithfulness of the encoding: we prove that with normal finite-state methods, it is impossible to properly encode the full class of rational (i.e. transducer recognizable) relations; however, there is a simple encoding for the synchronous rational relations. We present this encoding and show how standard finite-state methods can be used with this encoding, that is, arbitrary operations on relations can be encoded as operations on the code. Finally we sketch an implementation using an existing library (FOMA).	concatenation;encode;library (computing);regular language;transducer;type system;undecidable problem;user interface	Christian Wurm;Simon Petitjean	2017		10.18653/v1/W17-4005	computer science;theoretical computer science	Logic	-4.8122326568066915	18.623297607659264	4737
45aade981ba2ff872ebe259c725622baacc3bad3	a new efficient protocol for k-out-of-n oblivious transfer	one out of n oblivious transfer;oblivious transfer;diffie hellman protocol;oblivious key exchange;same message attack;k out of n oblivious transfer	This paper presents a new efficient protocol for k-out-of-n oblivious transfer which is a generalization of Parakh's 1-out-of-2 oblivious transfer protocol based on Diffie-Hellman key exchange. In the proposed protocol, the parties involved generate Diffie-Hellman keys obliviously and then use them for oblivious transfer of secrets.	oblivious transfer	Ashwin Jain;C. Hari	2010	Cryptologia	10.1080/01611194.2010.509284	private information retrieval;computer science;oblivious transfer;distributed computing;internet privacy;computer security	Crypto	-41.56692607735889	75.75333146142303	4747
62b0583212853ba03acdafe1d4f4943ac2d94901	an efficient object promotion algorithm for persistent object systems	persistent object stores;persistence;bulk loading;sphere;persistent object systems;promotion	Abstract#R##N##R##N#We report on a bulk object-loading algorithm for persistent object stores called Ghosted Allocation. It allocates large numbers of objects in a persistent store atomically, efficiently, and reliably. Its main strengths are that it minimizes I/O traffic, optimizes the disk access pattern, and does not impose complex requirements on applications. Our experiments demonstrate that the Ghosted Allocation algorithm is efficient and most importantly scalable, sustaining allocation rates of up to 63 000 objects s-1. Copyright © 2001 John Wiley & Sons, Ltd.	algorithm	Tony Printezis;Malcolm P. Atkinson	2001	Softw., Pract. Exper.	10.1002/spe.395	persistence;real-time computing;simulation;computer science;distributed computing;programming language;sphere;promotion	SE	-19.444466723468015	54.088390123979636	4752
930e7e3234f26a9284cc8f774b0cc046899ee03f	programming parallel embedded and consumer applications in openmp superscalar	openmp superscalar;consumer;embedded;parallel programming model;ompss	In this paper, we evaluate the performance and usability of the parallel programming model OpenMP Superscalar (OmpSs), apply it to 10 different benchmarks and compare its performance with corresponding POSIX threads implementations.	embedded system;openmp;posix threads;parallel computing;parallel programming model;superscalar processor;usability	Michael Andersch;Chi Ching Chi;Ben H. H. Juurlink	2012		10.1145/2145816.2145854	computer architecture;parallel computing;consumer;computer science;operating system;parallel programming model	HPC	-6.777148486188218	44.09642394508142	4753
6a95e997c0b4e7dee3e6d6aa7b117e604e8a7f59	virtual development environment based on systemc for embedded systems	system modeling;hardware simulation;embedded system;development environment;embedded system development;virtual development environment;remote debug interface;simulation environment;embedded software;systemc	Virtual development environment increases efficiency of embedded system development because it enables developers to develop, execute, and verify an embedded system without real hardware. We implemented a virtual development environment that is based on SystemC, a system modeling language. This environment was implemented by linking the AxD debugger with a SystemC-based hardware simulation environment through the RDI interface. We minimized modification of SystemC simulation engine so that the environment can be easily changed or extended with various SystemC models. Also, by using RDI, any debugging controller that support RDI can be used to develop an embedded software on the simulation environment. We executed example applications on the developed environment to verify operations of our implemented models and debugging functions. Our environment targets in ARM cores that are widely used in commercial business.		Sang-Young Cho;Yoojin Chung;Jung-Bae Lee	2007		10.1007/978-3-540-72590-9_143	embedded system;computer architecture;real-time computing;systems modeling;embedded software;computer science;development environment	EDA	-37.954265375786164	33.91723931022109	4757
0b2eeaec883f1bde253f7298fbaae0fcb94f4d2b	dessy: search and synchronization on the move	indexing internet search engines smart phones information technology prototypes java file systems personal digital assistants conference management;mobile device;mobile computer;metadata synchronization smartphones mobile devices desktop search information synchronization remote computers internet syxaw file synchronizer;information organization;synchronisation;search;servers;internet;energy usage synchronization search mobile computing;synchronization;storage capacity;batteries;mobile communication;energy usage;mobile handsets;meta data;wireless lan;mobile computing;synchronisation file organisation meta data mobile computing;file organisation	Current smartphones have a storage capacity of several gigabytes. More and more information is stored on mobile devices. To meet the challenge of information organization, we turn to desktop search. Users often possess multiple devices, and synchronize (subsets of) information between them. This makes file synchronization more important. This paper presents Dessy, a desktop search and synchronization framework for mobile devices. Dessy supports synchronization of search results, individual files, and directory trees. It allows finding and synchronizing files that reside on remote computers, or the Internet. The contributions of this paper include an energy usage evaluation of the system. Dessy is closely integrated with the Syxaw file synchronizer, which provides efficient file and metadata synchronization, optimizing network usage.	desktop computer;directory (computing);emulator;experiment;file synchronization;gigabyte;internet;knowledge organization;microsoft sync framework;mobile device;smartphone;synchronizer (algorithm);transmitter	Eemil Lagerspetz;Sasu Tarkoma;Tancred Lindholm	2010	2010 Eleventh International Conference on Mobile Data Management	10.1109/MDM.2010.18	embedded system;synchronization;real-time computing;computer science;operating system;database;data synchronization;file synchronization;mobile computing;world wide web;computer network	OS	-38.44333301151058	50.08357435998289	4766
b5e27db9d6a430ad517abebe90cae533da56f394	automatic test generation based on formal specifications: practical procedures for efficient state space exploration and improved representation of test cases		Test System Interface ParallelTC TTCN-3 Test System	state space;ttcn-3;test case	Michael Schmitt	2003			formal specification;theoretical computer science;test case;state space;computer science	SE	-45.89461881187193	31.236123006675363	4767
f181b26860eca527fe8176dcece72fab7f4b5c21	service robots: an industrial perspective	robot sensing systems;service robots;maintenance engineering;inspection;tanks containers cleaning electric generators industrial robots inspection maintenance engineering pipes service robots solar cell arrays solar power;rotors;service process characteristics commercialized robotic product maintenance sector repair sector industrial service robot motor inspection generator inspection solar panel inspection solar panel cleaning tank inspection pipe inspection;service robotics robot service inspection;inspection robot sensing systems service robots cleaning maintenance engineering rotors;cleaning	This paper presents the overview and the introduction of commercialized robotic products in the industrial service, maintenance and repair sectors. General facts of the industrial service are briefly described, and then we focus on four specific applications including motor /generator inspection, solar panel inspection /cleaning, tank inspection and pipe inspection. For each application, service process characteristics, operational details, technical challenges, requirements are described. Robotics solutions with commercialized products of each application area were introduced and detailed with special features and specification.	fagan inspection;requirement;robot;technical standard	Sang Choi;Gregory F. Rossano;George Zhang;Thomas A. Fuhlbrigge	2015	2015 IEEE International Conference on Technologies for Practical Robot Applications (TePRA)	10.1109/TePRA.2015.7219679	embedded system;engineering;forensic engineering;manufacturing engineering	Robotics	-58.763176876544	6.901859466573566	4768
05918acfdc05280e5dc3ad24f543f42ed2f0887b	cipher instruction search attack on the bus-encryption security microcontroller ds5002fp	protection information;software;microprocessors;microcontrollers;microprocessor;random access memory;bus encryption;industrial property cryptography firmware microcontrollers;storage access;logiciel;securite;application software;firmware;tamper resistance;decrypted memory content cipher instruction search attack bus encryption security microcontroller ds5002fp microprocessor;microcontrollers cryptography microprogramming read write memory random access memory data security application software monitoring microprocessors protection;protection;codificacion;monitoring;proteccion informacion;criptografia;cryptography;microregisseur;information protection;safety;coding;acces memoire;secure microprocessor;cipher instruction search attack;codage bus;acceso memoria;cryptographie;logicial;microprocesseur;industrial property;microcontrolador;read write memory;protection donnees;microprogramming;crypto processor;seguridad;software protection;microprocesador;decrypted memory content;bus encryption security microcontroller ds5002fp;codage;microcontroller;data security	A widely used bus-encryption microprocessor is vulnerable to a new practical attack. This type of processor decrypts onthe-fly while fetching code and data, which are stored in RAM only in encrypted form. The attack allows easy, unauthorized access to the decrypted memory content.	authorization;bus encryption;cipher;microcontroller;microprocessor;random-access memory	Markus G. Kuhn	1998	IEEE Trans. Computers	10.1109/12.729797	microcontroller;embedded system;parallel computing;computer science;operating system;stream cipher attack;computer security	Security	-35.30104299337719	66.49732270575583	4773
642684c948e8a1346d0fa34dbe564d7333ef6b30	analyzing transaction codes in manufacturing for compliance monitoring		Companies, especially manufacturers, are operating in increasing demands of variability and customizability nowadays. They are faced with a multitude of complex rules and regulations that must be complied with, as well as supply chain disruptions that threaten business. Smart manufacturers take advantage of advanced information and manufacturing technologies to enable flexibility in physical processes. However, existing information systems are far from being effective and efficient for compliance monitoring in service-oriented manufacturing. Failure to interface well with different information systems is still a general phenomenon among manufacturers, leading to problems such as delay of goods delivery or missing inventory. This research analyzes transaction codes in information systems using process modelling with simulation, validated by an intensive manufacturing case study. Design principles in IT systems for continuous monitoring and auditing in manufacturing are developed. The results contribute to enterprise systems configuration by proposing internal control measures based on IoT fault detection sensors.		Yuxin Wang;Joris Hulstijn;Yao-Hua Tan	2018			knowledge management;risk analysis (engineering);enterprise system;information system;supply chain;computer science;continuous monitoring;information technology;fault detection and isolation;database transaction;process modeling	ML	-64.48242084869152	9.394850819598947	4775
64afba4ed9da6fbdfef7d3229d433eba11dec462	multi-bit homomorphic encryption based on learning with errors over rings		Zhang Wei, Liu Shuguang, Yang Xiaoyuan Key Lab of Computer Network and Information Security under CAPF, Shaanxi 710086, China zhaangweei@yeah.net Abstract:Basing on Learning with errors over rings (RLWE) assumption, we provide a new multi-bit somewhat homomorphic encryption scheme. We introduce canonical embedding to transform a ring element into a vector, such that polynomial multiplication can be performed in ~ (nlog n) scalar operations, and ciphertext size is reduced at the same time. The CPA security of	ciphertext;cost per action;homomorphic encryption;information security;learning with errors;nl (complexity);polynomial ring;restrictions on geographic data in china;yang	Wei Zhang;Shuguang Liu;Xiaoyuan Yang	2013	IACR Cryptology ePrint Archive		discrete mathematics;theoretical computer science;mathematics;algebra	Crypto	-40.669394371657134	80.26705992131734	4780
9b7eb436c48fd8d20164cb3a9b8fb6f952e44ac0	stabilizing phase-clocks	distributed system;autostabilisation;systeme reparti;program design;horloge numerique;distributed computing;aproximacion probabilista;conception programme;probabilistic approach;approche deterministe;deterministic approach;synchronisation;self stabilization;sistema repartido;uniform processes;synchronization;approche probabiliste;reloj numerico;enfoque determinista;horloge phase;clock synchronization;sincronizacion;tree network;digital clock;concepcion programa	This note considers the problem of synchronizing a network of digital clocks: the clocks all run at the same rate, however, an initial state of the network may place the clocks in arbitrary phases. The problem is to devise a protocol to advance or retard clocks so that eventually all clocks are in phase. The solutions presented in this note are protocols in which all processes are identical and use a constant amount of space per process. One solution is a deterministic protocol for a tree network; another solution is a probabilistic protocol for a network of arbitrary topology.	communications protocol;in-phase and quadrature components;tree network	Ted Herman;Sukumar Ghosh	1995	Inf. Process. Lett.	10.1016/0020-0190(95)00050-M	synchronization;real-time computing;simulation;computer science;synchronous network;distributed computing	Networks	-21.39321462368553	43.608376907598704	4783
1cad284bc54ebc2e96376c727ffed558626f7b7d	vehicular ad hoc networks (vanets): current state, challenges, potentials and way forward	v2i;vehicular ad hoc networks vehicle collision warning traffic information dissemination its intelligent transport systems v2v communication v2i communication vehicle to infrastructure communications vehicle to vehicle communication auto mobile industry wireless communication technologies vanet;ieee 802 11p;wave;ieee 1609 vehicular communication v2v v2i its ieee 802 11p wave;v2v;ieee 1609;its;vehicles vehicular ad hoc networks wireless communication reliability communication system security standards;vehicular communication;vehicular ad hoc networks intelligent transportation systems	Recent advances in wireless communication technologies and auto-mobile industry have triggered a significant research interest in the field of VANETs over the past few years. VANET consists of vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) communications supported by wireless access technologies such as IEEE 802.11p. This innovation in wireless communication has been envisaged to improve road safety and motor traffic efficiency in near future through the development of Intelligent Transport Systems (ITS). Hence, government, auto-mobile industries and academia are heavily partnering through several ongoing research projects to establish standards for VANETs. The typical set of VANET application areas, such as vehicle collision warning and traffic information dissemination have made VANET an interested field of wireless communication. This paper provides an overview on current research state, challenges, potentials of VANETs as well the way forward to achieving the long awaited ITS.	assistive technology;data security;frequency band;hoc (programming language);mercedes-euklid;network congestion;open research;prototype;refinement (computing);requirement;typical set;unification (computer science);vehicle-to-vehicle	Elias Chinedum Eze;Sijing Zhang;Enjie Liu	2014	2014 20th International Conference on Automation and Computing	10.1109/IConAC.2014.6935482	vehicular ad hoc network;telecommunications;wave;vehicular communication systems;computer security;computer network	Mobile	-15.934614391224818	89.98802226339116	4784
170cb18135e64b327ea51a15faca470527a6825e	minicast: a multicast-anycast protocol for message delivery	busqueda informacion;multiserver queue;algoritmo paralelo;largeur bande;core based tree;file n serveurs;parallel algorithm;protocole transmission;destinateur quelconque;information retrieval;interrogation base donnee;multidestinatario;distributed computing;service web;interrogacion base datos;cache memory;web service;algorithme parallele;antememoria;protocolo transmision;antememoire;arbre partage par groupe;recherche information;anchura banda;retard;parallel computer;internet services;calculo repartido;bandwidth;fila n servidores;network delay;arbol partido en grupo;cualquier destinatario;anycast;retraso;080503 networking and communications;calcul reparti;database query;multidestinataire;multicast;servicio web;transmission protocol	Anycast and multicast are two important Internet services. Combining the two protocols can provide new and practical services. In this paper we propose a new Internet service, Minicast: in the scenario of n replicated or similar servers, deliver a message to at least m members, 1 ≤ m ≤ n. Such a service has potential applications in information retrieval, parallel computing, cache queries, etc. The service can provide the same Internet service with an optimal cost, reducing bandwidth consumption, network delay, and so on. We design a multi-core tree based architecture for the Minicast service and present the criteria for calculating the subcores among a subset of Minicast members. Simulation shows that the proposed architecture can even the Minicast traffic, and the Minicast application can save the consumptions of network resource.	anycast;multicast	Shui Yu;Wanlei Zhou;Justin T. Rough	2004		10.1007/978-3-540-30566-8_93	web service;anycast;best-effort delivery;parallel computing;multicast;cpu cache;computer science;operating system;database;distributed computing;parallel algorithm;world wide web;computer security;network delay;bandwidth;computer network	Crypto	-12.067034741004216	70.25694292509647	4798
1e804297a169912317499be6f266bd39e63d9238	an experimental evaluation of communication in an organization-based multi-agent system	protocols;complexity theory;measurement;cooperative information gathering organization based multi agent system evaluation communication;organization based multi agent system;cooperative information gathering;organizations concrete receivers complexity theory measurement syntactics protocols;receivers;multi agent systems;syntactics;evaluation;organizations;communication;experimental communication evaluation cooperative information gathering system interaction quality evaluation organization based multiagent system;concrete	This paper proposes an experimental evaluation of the communication in an Organization-based Multi-Agent System. We defined a set of criteria to evaluate the quality of interactions between agents at run time. The originalities and the advantages of our proposal are threefold: (1) the adaptation of a set of criteria and their corresponding metrics to evaluate communication in an Organization-based Multi-agent System, (2) the implementation of a Cooperative Information Gathering System on top of which we have tested our metrics, and (3) the execution of different scenarios at run time to analyze the influence of the organization size on these metrics.	interaction;multi-agent system;run time (program lifecycle phase)	Issam Bouslimi;Chihab Hanachi;Khaled Ghédira	2014	2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)	10.1109/WI-IAT.2014.152	systems engineering;engineering;knowledge management;management science	AI	-43.665215485253256	20.361221651407032	4801
02d9847bccfc59026b8a3a35b6035037eec286f4	touchstroke: smartphone user authentication based on touch-typing biometrics	transparent;keystroke;smartphone;behavioral biometrics	Smartphones are becoming pervasive and widely used for a large variety of activities from social networking to online shopping, from message exchanging to mobile gaming, to mention just a few. Many of these activities generate private information or require storing on the phone user credentials and payment details. In spite of being so security and privacy critical, smartphones are still widely protected by tradi- tional authentication mechanisms such as PINs and passwords, whose limitations and drawbacks are well known and documented in the secu- rity community. New accurate, user-friendly and effective authentication mechanisms are required. To this end, behavior-based authentication has recently attracted a significant amount of interest in both commercial and academic contexts. This paper proposes a new bi-modal biometric authentication solu- tion, Touchstroke, which makes use of the user's hand movements while holding the device, and the timing of touch-typing(Touch-typing is the act of typing input on the touchscreen of a smartphone.) when the user enters a text-independent 4-digit PIN/password. We implemented and tested the new biometrics in real smartphones. Preliminary results are encouraging, showing high accuracy. Thus, our solution is a plausible alternative to traditional authentication mechanisms.	authentication;biometrics;smartphone;touch typing	Attaullah Buriro;Bruno Crispo;Filippo Del Frari;Konrad S. Wrona	2015		10.1007/978-3-319-23222-5_4	keystroke logging;computer science;operating system;multi-factor authentication;internet privacy;world wide web;computer security	Mobile	-50.3259662151913	65.83747401150194	4809
f97eae3bf004f5bac4bd1561afba1c4fce05d1f8	tactics for hierarchical proof	construction process;interaction point;operational semantics;theorem proving;construct validity;theorem prover;hierarchical proof;hiproof;tactical theorem proving	There is something of a discontinuity at the heart of popular tactical theorem provers. Low-level, fully-checked mechanical proofs are large trees consisting of primitive logical inferences. Meanwhile, high-level human inputs are lexically structured formal texts which include tactics describing search procedures. The proof checking process maps from the high-level to low-level, but after that, explicit connections are usually lost. The lack of connection can make it difficult to understand the proof trees produced by successful tactic proofs, and difficult to debug faulty tactic proofs. We propose the use of hierarchical proofs, also known as hiproofs, to help bridge these levels. Hiproofs superimpose a labelled hierarchical nesting on an ordinary proof tree, abstracting from the underlying logic. The labels and nesting are used to describe the organisation of the proof, typically relating to its construction process. In this paper we introduce a foundational tactic language Hitac which constructs hiproofs in a generic setting. Hitac programs can be evaluated using a big-step or a small-step operational semantics. The big-step semantics captures the intended meaning, whereas the small-step semantics is closer to possible implementations and provides a unified notion of proof state. We prove that the semantics are equivalent and construct valid proofs. We also explain how to detect terms which are stuck in the small-step semantics, and how these suggest interaction points with debugging tools. Finally we show some typical examples of tactics, constructed using tactical combinators, in our language.	automated proof checking;automated theorem proving;combinatory logic;debugging;hitac;high- and low-level;map;operational semantics;reflections of signals on conducting lines	David Aspinall;Ewen Denney;Christoph Lüth	2010	Mathematics in Computer Science	10.1007/s11786-010-0025-6	formal proof;combinatorics;discrete mathematics;computer-assisted proof;computer science;automated proof checking;analytic proof;mathematics;automated theorem proving;proof assistant;programming language;operational semantics;proof complexity;algorithm;algebra	PL	-17.686579089380256	21.97678834544983	4825
2dd644ee0b3117d325c270c19ee5ca7315e4cdc8	borel determinacy of concurrent games	tree game;gale-stewart game;borel determinacy;event structure;traditional game;determinacy proof proceed;concurrent game	Just as traditional games are represented by trees, so distributed/concurrent games are represented by event structures. We show the determinacy of such concurrent games with Borel sets of configurations as winning conditions, provided the games are race-free and bounded-concurrent. Both restrictions are shown necessary. The determinacy proof proceeds via a reduction to the determinacy of tree games, and the determinacy of these in turn reduces to the determinacy of Gale-Stewart games. KeywordsConcurrent games; Non-deterministic strategies; Winning conditions; Borel Determinacy; Event structures.	traditional game	Julian Gutierrez;Glynn Winskel	2013		10.1007/978-3-642-40184-8_36	combinatorics;discrete mathematics;axiom of projective determinacy;mathematics;wadge hierarchy;determinacy	Logic	-7.057088833948278	20.580757577057014	4826
5a0ff3dfb021f3d4752173e61ddd2fc7efb549cf	code generation from uml models with semantic variation points	developpement logiciel;modelizacion;variabilidad;verificacion modelo;modele entreprise;lenguaje uml;diagramme etat;generation code;degree of freedom;generacion codigo;code generation;verification modele;semantics;langage modelisation unifie;program verification;modelo empresa;semantica;semantique;language family;familia lenguaje;modelisation;business model;software architecture;verificacion programa;metamodel;diagrama estado;model checking;metamodele;metamodelo;generation test;desarrollo logicial;unified modelling language;software development;state diagram;architecture basee modele;test generation;variability;verification programme;variabilite;modeling;simulation model;generacion prueba;famille langage;software process modelling;mdd;model driven architecture;architecture logiciel;sp metamodel;process modelling languages;arquitectura basada modelo	UML semantic variation points provide intentional degrees of freedom for the interpretation of the metamodel semantics. The interest of semantic variation points is that UML now becomes a family of languages sharing lot of commonalities and some variabilities that one can customize for a given application domain. In this paper, we propose to reify the various semantic variation points of UML 2.0 statecharts into models of their own to avoid hardcoding the semantic choices in the tools. We do the same for various implementation choices. Then, along the line of the OMG's Model Driven Architecture, these semantic and implementation models are processed along with a source UML model (that can be seen as a PIM) to provide a target UML model (a PSM) where all semantic and implementation choice are made explicit. This target model can in turn serve as a basis for a consistent use of code generation, simulation, model-checking or test generation tools.	application domain;code generation (compiler);component-based software engineering;conformity;diagram;dynamic dispatch;executable;hard coding;metamodeling;model checking;model-driven architecture;model-driven engineering;requirement;simulation;unified modeling language	Franck Chauvel;Jean-Marc Jézéquel	2005		10.1007/11557432_5	metamodeling;business model;model checking;unified modeling language;software architecture;semantic computing;state diagram;simulation;systems modeling;uml tool;computer science;artificial intelligence;software development;software engineering;applications of uml;simulation modeling;semantics;degrees of freedom;programming language;algorithm;code generation	SE	-41.745856484142664	25.57336142983961	4827
17949b5777597e5a7f5409bb07e9490c5259c0a2	dcsp-mc: dependable cloud-based storage platform for mobile computing	data sharing;reliability;virtual organisations;file querying;data integrity;data centre;data navigation;semantic based data retrieval;data storage;virtual organisation;data centres;mobile computing;flexibility;cloud computing	Recently, mobile devices have become more and more important in people’s daily lives. Due to the data inflation and natural limitations of mobile devices, such as limited storage space and computing capability, data storage and sharing have become urgent issues in mobile computing environments. In this paper, we implement a lightweight cloud-based storage framework, which provides an easy-to-use file navigation service for attribute-based file querying or semantic-based data retrieving. In addition, it incorporates an effective mechanism for users to verify their data integrity, which can relieve much burden from mobile devices. Experimental evaluations show that the proposed framework is effective to provide flexible and reliable data sharing in mobile computing environments.	cloud computing;computer data storage;data integrity;data security;mobile computing;mobile device;prototype	Peng Xiao;Guofeng Yan	2013	IJNVO	10.1504/IJNVO.2013.053745	data center;mobile search;cloud computing;mobile database;computer science;operating system;mobile technology;computer data storage;data integrity;reliability;database;data efficiency;internet privacy;mobile computing;world wide web;computer security;computer network	Mobile	-42.152903368970215	59.58574452684759	4828
4046852a360c9bf93e69774ca04257538fa6996a	fair multi-agent task allocation for large data sets analysis	multi agent system;big data;mapreduce;negotiation	Many companies are using MapReduce applications to process very large amounts of data. Static optimization of such applications is complex because they are based on user-defined operations, called map and reduce, which prevents some algebraic optimization. In order to optimize the task allocation, several systems collect data from previous runs and predict the performance doing job profiling. However they are not effective during the learning phase, or when a new type of job or data set appears. In this paper, we present an adaptive multi-agent system for large data sets analysis with MapReduce. We do not preprocess data and we adopt a dynamic approach, where the reducer agents interact during the job. In order to decrease the workload of the most loaded reducer-and so the execution time-we propose a task reallocation based on negotiation.		Quentin Baert;Anne-Cécile Caron;Maxime Morge;Jean-Christophe Routier	2016		10.1007/978-3-319-39324-7_3	real-time computing;computer science;database;distributed computing	ML	-18.558547908970105	56.47861526413742	4831
03e2fe4ea66c328f3bc76f253657789269315853	a compositional verification method for lotos	distributed algorithms;specification;unity;compositional verification;fault tolerance;validation	presents a compositional verification for the LOTOS language [11. The method has been widely inspired by !&li;g’; work on the C CS language [2]. Xt shows how to decompose a general formula in smaller parts in order to be verified by parts of the program which describes a system. Then, for each operator, a rule has been defined.	cs-blast;language of temporal ordering specification	Hacène Fouchal;Ana R. Cavalli	1994		10.1145/197917.198169	distributed algorithm;fault tolerance;real-time computing;computer science;database;distributed computing;programming language;specification	Logic	-29.2580695659311	31.471595092178006	4833
6ff653d533d605d6db30b15a30f1bdc0e999a8c8	semi-automatic techniques for deriving interscheme properties from database schemes	esquema;semiautomatique;semiautomatico;design tool;base donnee;database;cooperative information system;base dato;semantics;conception;semantica;semantique;schema;algorithme;algorithm;database schemes;cooperative information systems;diseno;semiautomatic;e r schemes;design;information system;design tools;interscheme properties;scheme;systeme information;interscheme;structural properties;algoritmo;sistema informacion	Many organizations nowadays own several information systems storing precious data whose effective exploitation is a key issue in many contexts. A major problem to be faced is the semantic normalization of scheme objects used in heterogeneous, independent and pre-existing databases as to single out differences and similitudes among data whereby a consistent, integrated view of available information can be obtained. To solve large instances of this problem, the development of automatic support tools appears to be mandatory. This paper gives a contribution in this framework by presenting algorithms for extracting useful properties holding among objects belonging to sets of database schemes. The algorithms are capable of deriving both nominal and structural properties of scheme objects starting from database scheme descriptions. Some application examples are presented to demonstrate the effectiveness of our approach.	semiconductor industry	Luigi Palopoli;Domenico Saccà;Domenico Ursino	1999	Data Knowl. Eng.	10.1016/S0169-023X(99)00013-0	design;scheme;computer science;artificial intelligence;theoretical computer science;data mining;schema;database;semantics;programming language;information system	DB	-36.18499427959426	11.896917133553211	4844
44fcefc228d4f770c7fe73d18ed19bca85d4a0f9	universal recursion theoretic properties of r.e. preordered structures		When dealing with axiomatic theories from a recursion-theoretic point of view, the notion of r.e. preordering naturally arises. We agree that an r.e. preorder is a pair = 〈 P , ≤ P 〉 such that P is an r.e. subset of the set of natural numbers (denoted by ω ), ≤ P is a preordering on P and the set {〈; x, y 〉: x ≤ P y } is r.e.. Indeed, if is an axiomatic theory, the provable implication of yields a preordering on the class of (Godel numbers of) formulas of . Of course, if ≤ P is a preordering on P , then it yields an equivalence relation ~ P on P , by simply letting x ~ P y iff x ≤ P y and y ≤ P x . Hence, in the case of P = ω , any preordering yields an equivalence relation on ω and consequently a numeration in the sense of [4]. It is also clear that any equivalence relation on ω (hence any numeration) can be regarded as a preordering on ω . In view of this connection, we sometimes apply to the theory of preorders some of the concepts from the theory of numerations (see also Ersov [6]). Our main concern will be in applications of these concepts to logic, in particular as regards sufficiently strong axiomatic theories (essentially the ones in which recursive functions are representable). From this point of view it seems to be of some interest to study some remarkable prelattices and Boolean prealgebras which arise from such theories. It turns out that these structures enjoy some rather surprising lattice-theoretic and universal recursion-theoretic properties. After making our main definitions in §1, we examine universal recursion-theoretic properties of some r.e. prelattices in §2.	recursion;theory	Franco Montagna;Andrea Sorbi	1985	J. Symb. Log.		combinatorics;discrete mathematics;mathematics	Theory	-6.644711073500719	12.557965204155803	4848
8d77492fb2178da2184a3b215c0e675bbab35610	a new industrial cooperative tele-maintenance platform	tecnologia industrial tecnologia mecanica;computacion informatica;collaborative work;neural networks;industrial tele maintenance;ciencias basicas y experimentales;distributed interface;tecnologias;grupo a;cooperative work;neural network	With the maturation of new telecommunication technologies, the industrial enterprises wants to integrate the advances to activate on site interventions of experts and then reduce the machines inactivity time. We propose a hardware and software solution allowing us to make a cooperative tele-maintenance: the maintenance staff can do the work remotely and also in collaboration with other experts (Cooperative Work). We emphasize network aspects (heterogeneity, dynamism) and cooperating members mobility for maintenance action achievement. The system we propose allows us to increase availability, mobility, yield and quality and to reduce the costs of equipment failures. In this paper, we show how computer innovative technologies and industrial maintenance techniques combination can offer new uses.	television	Éric Garcia;Hervé Guyennet;Jean-Christophe Lapayre;Noureddine Zerhouni	2004	Computers & Industrial Engineering	10.1016/j.cie.2004.05.019	simulation;computer science;engineering;operations management;machine learning;artificial neural network	SE	-65.53619810436255	10.0681428275661	4849
6b73301c14dc141eefc13bff2e435e344db87960	an interactive graph-based automation assistant: a case study to manage the gipsy's distributed multi-tier run-time system	visualization;gipsy network;gui;demand driven distributed computation;graph based management	The GIPSY system provides a framework for a distributed multi-tier demand-driven evaluation of heterogeneous programs, in which certain tiers can generate demands, while others can respond to demands to work on them. They are connected through a virtual network that can be flexibly reconfigured at run-time. Although the demand generator components were originally designed specifically for the eductive (demand-driven) evaluation of Lucid intensional programs, the GIPSY's run-time's flexible framework design enables it to perform the execution of various kinds of programs that can be evaluated using the demand-driven computational model. Management of the GISPY networks has become a tedious (although scripted) task that required a manual command-line console, which does not scale for large experiments. Therefore a new component has been designed and developed to allow users to represent, visualize, and interactively create, configure and seamlessly manage such a network as a graph. Consequently, this work presents a Graphical Manager, an interactive graph-based assistant component for the GIPSY network creation and configuration management. Besides allowing the management of the nodes and tiers (mapped to hosts where store, workers, and generators reside), it lets the user to visually control the network parameters and the interconnection between computational nodes at run-time. In this paper we motivate and present the key features of this newly implemented graph-based component. We give the graph representation details, mapping of the graph nodes to tiers, tier groups, and specific commands. We provide the requirements and design specification of the tool and its implementation. Then we detail and discuss some experimental results.	command-line interface;computation;computational model;configuration management;experiment;graph (abstract data type);graph (discrete mathematics);graphical user interface;intensional logic;interactivity;interconnection;lucid;multitier architecture;requirement;runtime system;scalability	Sleiman Rabah;Serguei A. Mokhov;Joey Paquet	2013		10.1145/2513228.2513286	parallel computing;simulation;visualization;computer science;theoretical computer science;operating system;graphical user interface;distributed computing;programming language;algorithm	PL	-33.79093143192886	52.941130610740565	4851
9fbb7c3dea7437dd19b1af5fb385b228972c37ce	etre ou ne pas être usager d'internet telle est la question ?		"""On ne peut plus considerer aujourd'hui la dite fracture numerique par une double approche#N#en termes d'acces a l'ordinateur et a Internet - qui nierait la question des usages et des competences#N#- ou en termes de posseder ou de ne pas posseder la technologie adequate. L'objectif#N#de notre travail est donc de participer a la comprehension de cette partie de la population qui#N#declare ne pas utiliser Internet ou est classee par les enquetes comme 'non-internaute'. A ce#N#titre, les recents, mais encore rares travaux, menes sur la question du non-usage mettent en#N#lumiere la diversite des situations et montrent que la description des situations de non-usages#N#ne peut etre basee sur une simple dichotomie usagers/non-usagers.#N#La question qui emerge immediatement est celle de la maniere de definir le non-usage.#N#Ainsi, la determination des indicateurs propres a caracteriser les usagers varie d'un institut a#N#un autre.#N#Le CREDOC (Centre de Recherche pour l'EtuDe et l'Observation des Conditions de vie)#N#definit les internautes selon """"tous modes de connexion confondus : a domicile, a l'ecole ou#N#sur le lieu de travail, dans les lieux publics, en Wi-Fi et a l'aide de son telephone portable"""".#N#Il n'y a pas de notion de frequence d'usage : est internaute la personne qui a repondu utiliser#N#""""tous les jours"""" ou """"1 a 2 fois par semaine"""" ou """"plus rarement"""" (Bigot et Croutte, 2007). Selon#N#Mediametrie, les internautes sont tous """"les individus [de 11 ans et plus] s'etant connectes a#N#Internet au cours des 30 derniers jours quel que soit leur lieu de connexion : domicile, travail,#N#autres lieux"""" (Mediametrie, 2008).#N#Partant d'une base de donnees existante sur les usages et non usages d'internet (enquete#N#M@rsouin de 2009, qui portait sur les usages et equipements de 2008 particuliers et menages#N#en Bretagne, http://www.marsouin.org/), nous avons essaye de repondre a des questions,#N#posees par des sociologues, pour lesquelles cette base n'etait pas prealablement concue.#N#La necessite d'obtenir des resultats simples et lisibles nous a encourages a utiliser, des methodes#N#descriptives et les methodes a bases de regles (C4.5 (Quinlan, 1993), Apriori (Agrawal#N#et Strikant, 1994) et Farthestfirst (Hochbaum et Shymoys, 1985)). Nous presentons quelques#N#resultats, notamment, permettant de verifier certaines hypotheses autour du non-usage : par#N#exemple, il existe des groupes d'usagers qui fondent en meme temps leur pratique et leur#N#non-pratique d'internet ; nous avons pu egalement identifier qu'il existe des groupes de faux#N#non-usagers et de faux vrais usagers. Enfin, nous avons souleve la pertinence de certaines idees#N#a propos de l'isolement social. En effet, il ressort de nos analyses qu'il n'est pas une cause principale#N#du non-usage et que le non-usage peut exister dans un environnement social favorable a#N#l'usage.#N#Par ailleurs, l'etude que nous avons menee a permis de deceler que la difficulte de l'analyse#N#du non-usage dans les enquetes a large diffusion reside aussi et surtout dans la prise en charge#N#des situations de non-usage. Elle met en evidence que certains choix, comme la selection des#N#questions, la precision des questions, peuvent devenir determinants pour les analyses et les#N#questions relatives aux usages et non usages que les sociologues devront affronter."""	linear algebra	Abdoulaye Sarr;Philippe Lenca;Annabelle Boutet;Jocelyne Tremenbert	2011			publics;humanities;philosophy;population	Theory	-106.98450755993662	14.399211814415114	4853
93885829131070461670806c52d1c83bf3b63d46	dependability: the challenge for the future of computing and communication technologies	communication technology	Without Abstract	dependability	Jean-Claude Laprie	1994		10.1007/3-540-58426-9_144	information and communications technology;computer science;systems engineering;dependability;management science;computer engineering	HPC	-63.13052897781441	7.509413545500841	4856
1f9c5996d58772de117e965fc983c981de162e7d	knowledge-based software engineering	knowledge based software engineering	Knowledge-based software engineering emphasizes the fact that creating software is a knowledge-intensive activity, and proposes that making more knowledge available will facilitate the timely production of high-quality software. The author gives four reasons for software engineering being an interesting area for AI research. He also stipulates that KBSE researchers must answer several crucial questions: what part of the software process is targeted; what knowledge is applicable and how can it be represented, acquired and maintained; and how can one present the knowledge to developers to improve the quality and cost of software development?.<>	knowledge based software assistant;software engineering	W. Lewis Johnson	1992	Knowledge Eng. Review	10.1017/S0269888900006482	personal software process;verification and validation;software engineering process group;software verification;search-based software engineering;computer science;social software engineering;component-based software engineering;software development;feature-oriented domain analysis;civil engineering software;software engineering;software construction;software walkthrough;software measurement;software deployment;software requirements;software system;computer engineering	SE	-59.98601043555758	24.69319235973867	4857
2f69d49505544290c44047bef6de22d5d9599d90	viewpoint quality model : a software quality model for the application of software quality metrics			software metric;software quality;viewpoint	Kenneth Sears	1992				SE	-63.03834775742847	27.155923958569375	4858
4fd6854a9e4874bd8be85b3b2aa95addff83f827	application management als outsourcing-strategie	application management	Abstrakt: In vielen Unternehmen herrscht eine stark heterogene, gewachsene ITLandschaft mit einer Vielzahl unterschiedlicher individueller Anwendungen, die mehr oder weniger eng gekoppelt sind. Die Anwendungssoftware soll die Kerngeschäfte des Unternehmen lediglich unterstützen und vereinfachen, bindet jedoch enorme Kapazitäten. Wie kann ein Application Management als Managed Service definiert und erfolgreich ausgegliedert werden? Welche Maßnahmen zur Unterstützung eines solchen externen Managed Service greifen, um gesetzte Qualitätsstandards zu halten oder sogar zu verbessern. Im Beitrag werden langjährige Erfahrungen als externer Dienstleister für Application Management diskutiert.	application lifecycle management;eine and zwei;internet explorer;outsourcing;triple des;unified model	Karin Vosseberg;Tobias Holzem	2004			process management;outsourcing;engineering;application lifecycle management	OS	-101.47172763013236	34.40058150410125	4859
c3447a0f7b69aee5e1a519d9117781cf60b039fc	one wide-sense circuit tree per traffic class based inter-domain multicast	routing protocols;ip multicast;wide sense circuit tree ip multicast scalability reconfigurable;trees mathematics;receivers;wide sense circuit tree;telecommunication traffic;internet;multicast protocols;switches receivers ip networks quality of service routing protocols internet;wide sense circuit tree quality of service qos multicast packets multicast traffic multicast receiver multicast sources multicast protocol traffic class based inter domain multicast;ip networks;trees mathematics multicast protocols quality of service telecommunication traffic;scalability;quality of service;switches;reconfigurable	Traditional multicast protocol forms multicast trees rooted at different sources to forward packets. If the multicast sources and receivers are in different domains, these trees will produce a great number of multicast states in the backbone, resulting in poor scalability. Therefore, we propose a one Wide-Sense Circuit Tree per Traffic Class based inter-domain multicast (WSCT-TC), in which a Wide-Sense Circuit Tree (WSCT) is established for a class of multicast traffic. The WSCT is established in the backbone, along which multicast packets are forwarded by label switching. The spec of WSCT can be reconfigured according to the QoS (Quality of Service) requirement of multicast applications, to provide preferable QoS. Simulating experiment shows that WSCT-TC behaves better scalability.	inter-domain;internet backbone;multicast;multiprotocol label switching;quality of service;router (computing);scalability;spec#	Yue Chen;Chaoling Li;Kaixiang Huang;Xiaobo Zhang	2013	2013 IEEE Eighth International Conference on Networking, Architecture and Storage	10.1109/NAS.2013.46	real-time computing;multicast;scalability;the internet;ip multicast;inter-domain;quality of service;reliable multicast;network switch;protocol independent multicast;computer science;pragmatic general multicast;internet group management protocol;distributed computing;routing protocol;distance vector multicast routing protocol;source-specific multicast;multimedia broadcast multicast service;xcast;computer network;multicast address	HPC	-7.149517297190862	85.63033510470734	4867
0d096da1f8f97ace914ae262fa1b71a09869d03b	grundlagenuntersuchungen zum einsatz der mikrorechentechnik für die arterielle und venöse nichtinvasive funktionsprüfung in der angiologie				Eva Pfeiffer	1985				Robotics	-101.15581264199965	26.662364415688735	4873
5ee60ef42bcd5ce9238a302ca20470818b2eae72	functional-level energy characterization of μc/os-ii and cache locking for energy saving	arm7tdmi-based embedded system;reassigning cache location;energy-consuming kernel routine;total energy consumption;energy consumption;os-ii real time kernel;functional-level energy characterization;kernel function;individual operating system;cache contention;energy saving;os-ii kernel;cache locking	We show how to characterize the energy consumption of individual operating system (OS) functions in the μC/OS-II real time kernel running on an ARM7TDMI-based embedded system. We then derive a strategy for saving energy based on locking more energy-consuming kernel routines of μC/OS-II into the cache and reassigning cache locations to reduce cache contention between frequently invoked kernel functions. The proposed method saves about 37 percent of the energy otherwise consumed by the μC/OS-II kernel, leading to reductions of up to 5.9 percent in the total energy consumption, which includes the energy consumed by the application. © 2012 Alcatel-Lucent.	arm7;cpu cache;embedded system;firefox os;kernel (operating system);lock (computer science);operating system	Kyungtae Kang;Kyung-Joon Park;Hongseok Kim	2012	Bell Labs Technical Journal	10.1002/bltj.21532	parallel computing;real-time computing;computer hardware;computer science	Embedded	-6.1324327663638725	54.58056337704081	4876
b8e121b2f4d13c5f6b4f57717ecada3fb56e6d2a	a context-driven model for the flat roofs construction process through sensing systems, internet-of-things and last planner system	building construction process;distributed sensors;internet-of-things;last planner system;service oriented architecture;smart sensor networks	The main causes of building defects are errors in the design and the construction phases. These causes related to construction are mainly due to the general lack of control of construction work and represent approximately 75% of the anomalies. In particular, one of the main causes of such anomalies, which end in building defects, is the lack of control over the physical variables of the work environment during the execution of tasks. Therefore, the high percentage of defects detected in buildings that have the root cause in the construction phase could be avoidable with a more accurate and efficient control of the process. The present work proposes a novel integration model based on information and communications technologies for the automation of both construction work and its management at the execution phase, specifically focused on the flat roof construction process. Roofs represent the second area where more defects are claimed. The proposed model is based on a Web system, supported by a service oriented architecture, for the integral management of tasks through the Last Planner System methodology, but incorporating the management of task restrictions from the physical environment variables by designing specific sensing systems. Likewise, all workers are integrated into the management process by Internet-of-Things solutions that guide them throughout the execution process in a non-intrusive and transparent way.	autonomous system (internet);business process;conflict (psychology);context switch;control system;environment variable;experiment;flatfoot;internet backbone;lightweight portable security;mobile device;process modeling;programming paradigm;prototype;service-oriented architecture;solutions;traceability;vertebral column;benefit;sensor (device)	María Dolores Andújar-Montoya;Diego Marcos-Jorquera;Francisco Manuel García-Botella;Virgilio Gilart-Iglesias	2017		10.3390/s17071691	systems engineering;automation;electronic engineering;planner;engineering;root cause;flat roof;simulation;service-oriented architecture;management process;internet of things	SE	-43.04539882687665	45.727956397071466	4882
d8bcd735ed5847f16f1954cfcc9ad5f9633ac626	replicating rare software failures with exploratory visual gui testing		Saab AB developed software that had a defect that manifested itself only after months of continuous system use. After years of customer failure reports, the defect still persisted, until Saab developed failure replication based on visual GUI testing.	datasaab;exploratory testing;graphical user interface testing;software bug	Emil Alégroth;Johan Gustafsson;Henrik Ivarsson;Robert Feldt	2017	IEEE Software	10.1109/MS.2017.3571568	computer science;systems engineering;software development;visualization;software engineering;graphical user interface testing;memory management;software;graphical user interface	SE	-63.59334555772851	30.024717845607373	4888
466df29203175f3b2bacb9a917b40df3c9ca529e	an undergraduate embedded systems project	distributed system;microcontrollers;hardware microcontrollers embedded systems program processors protocols;protocols;undergraduate education;project management;safety interlocks undergraduate embedded systems project embedded systems courses rapid transit system distributed system microcontrollers ethernet control software udp packets web page c language open source operating system trams;web pages;project management educational courses embedded systems local area networks microcontrollers operating systems computers;embedded system;input output;undergraduate education embedded systems;embedded systems;operating system;system design;waiting time;educational courses;program processors;operating systems computers;local area networks;hardware;open source	Courses in embedded systems can approach the subject from a variety of perspectives, ranging from emphasis on hardware, emphasis on software, or emphasis on system design. This paper describes a course in embedded systems that requires students to develop software to control a system that is physically available in the laboratory. The hardware models a rapid transit system that is controlled using a distributed system consisting of five independent microcontrollers, communicating with each other via Ethernet. Designing the control software includes communicating with the other microcontrollers via UDP packets, communicating with the system hardware, and providing a web page to report status and accept system parameters such as tram speed and station waiting time. Students use an open-source operating system for their software, which they write in the C language; the operating system uses cooperative threads and provides library functions for serial and Ethernet input/output. The semester goal is to properly control as many trams as possible, with safety interlocks implemented. The physical system allows students to see the results of their software.	common gateway interface;distributed computing;distributed control system;embedded system;hypertext transfer protocol;input/output;internet protocol suite;library (computing);microcontroller;open-source software;operating system;profiling (computer programming);real-time transcription;systems design;thread (computing);usb;web page	John F. Greco;John A. Nestor	2011	2011 IEEE International Conference on Microelectronic Systems Education	10.1109/MSE.2011.5937089	local area network;project management;microcontroller;input/output;embedded system;communications protocol;embedded operating system;real-time computing;computer science;electrical engineering;operating system;web page;real-time control system software;software system;computer engineering;systems design	Embedded	-28.58779272586091	38.89645902512441	4895
03ac2ab9329b2272f0b7b51f421320d8d48bbf03	condor: an architecture for controlling the utah-mit dexterous hand	software;systeme commande;sistema control;architecture systeme;robots computer architecture computerised control;systeme condor;logiciel;computerised control;real time;manipulateur;robotics;computer architecture;control system;manipulador;flexible symbolic debugger robots utah mit dexterous hand computational architecture condor real time control processors file server;temps reel;robots;computer architecture robot sensing systems robot control control systems actuators servomechanisms process control real time systems laboratories tendons;robotica;tiempo real;logicial;arquitectura sistema;robotique;system architecture;computational efficiency;manipulator;materiel informatique;program development;material informatica;hardware	This paper describes a fully implemented computational architecture that controls the Utah-MIT dexterous hand and other complex robots. Robots like the Utah-MIT hand are characterized by large numbers of actuators and sensors, and require high servo rates. Consequently, powerful and flexible computer architectures are needed to control them. The architecture described in this paper derives its power from the highly efficient real-time environment provided for its control processors, coupled with a development host that enables flexible program development. By mapping the memory of a dedicated group of processors into the address space of a host computer, efficient sharing of system resources between them is possible. The software is characterized by a few simple design concepts but provides the facilities out of which more powerful utilities like a multi-processor pseudo-terminal emulator, a transparent and fast file server, and a flexible symbolic debugger could	address space;central processing unit;computer architecture;debugger;file server;host (network);multiprocessing;real-time clock;robot;sensor;server (computing);servo;terminal emulator	Sundar Narasimhan;David M. Siegel;John M. Hollerbach	1989	IEEE Trans. Robotics and Automation	10.1109/70.88080	robot;embedded system;real-time computing;simulation;computer science;engineering;control system;artificial intelligence;manipulator;robotics	Robotics	-32.243131532183135	37.92669147446187	4897
84241fab6a932f498ad890c692135a945f8e1bca	reliable ale middleware for rfid network applications	middleware	Radio frequency identification (RFID), an identified object technology, utilizes air interface to deliver information required for object identification. The applicable technical information of RFID is convenient to manage and operate, and is widely promoted by the large-scale chain industry. Additionally, electronic product code (EPC) network technology enables immediate, automatic identification and sharing of information on items in the supply chain. This work presents an EPC Network backbone and an RFID service middleware with a high-reliable and highefficiency ALE-based (Application Level Event) prototype mechanism according to the EPCglobal. An SMR (Student Muster Roll) application testbed based on RFID system is implemented by the proposed ALEbased scheme. The SMR system is employed to manage the absent records of students for taking a subject, and can manage and control the operation multi-Reader devices simultaneously. The proposed system can filter attendance accurately, eliminating the possibility of reduplication in student records.	automatic identification and data capture;electronic product code;internet backbone;middleware;prototype;radio frequency;radio-frequency identification;shingled magnetic recording;testbed	Nong-Kun Chen;Teng-Hsun Chang;Jiann-Liang Chen;Cheng-Yen Wu;Huan-Wen Tzeng	2007	Int. Journal of Network Management	10.1002/nem.698	embedded system;real-time computing;computer science;middleware;computer security;computer network	Mobile	-21.781030611658657	82.53755241535637	4899
4a972d50e6073cefb400945219cafce055c501bc	correction note on l = lambda w		"""The purpose of this note is to point out and correct a careless error in the proofs of theorems 2.1 and 2.2 in Whit t [6], which also appeared in Glynn and Whit t [1-3]. In particular, as pointed out by S. St idham, Jr., the first inequalitY in (211) in [6] is not correct unless the service discipline is first-in first-out (FIFO). Of course, the relation L = AW does not actually depend on the F IFO condition. To obtain what was in tended, simply replace D(t) in both (2.1) and the s ta tement of t heo rem 2.1 of [6] by D'(t) , where D'( t ) counts the number of k such that D~ < t with D~ --max{Dj"""" 1 < j < k} as in (2.6) of [6]. Note that the complicat ion of non -F IFO disciplines is accounted for in (2.6); this modificat ion does the same for (2.1). Indeed, a variant of this modif icat ion is used in the more general setting in Glynn and Whit t [4]; see remark 5 and l emma 3 on p. 640 there. Moreover, p roper modifications of (2.1) routinely appear elsewhere, such as in S t idham [5] and Wolff [7]. The use of D~ and D'( t ) is a conceptually simple approach. The indicated modif icat ion of (2.1) in [6] is also needed in the proof of t heo rem 2.2 in [6] (which shows that the condi t ion on D'( t ) in the new s ta tement of t heo rem 2.1 in [6] is actually not needed) . This modificat ion works because, first, A k < D k < D'~ and D'( t ) < D( t ) < A ( t ) and, second, t l D ' ( t ) ~ h as t ~ if and only if -1 , -1 k D k ~ A as k ~ oo. (We use the fact that D k is nondecreas ing in k here. In contrast, as no ted in t heo rem 2(d) of [1], in general (without FIFO) the limit k 1 D k ~ A1 as k ~ oo implies, but is not implied by, the limit t -~D( t ) ~ A as t ~ ~. The failure of the limit t l D ( t ) ~ ,t as t --* oo to imply the limit k 1 D k ~ A1 as k ~ oo is the key reason for the asymmetry in t heo rem 2.2 of [6]; the second sentence of remark (2.2) in [6] confuses the issue.) Unfor tunately , even though this oversight concerning (2.1) in [6] does not appear in [4], it does appear in previous papers. This same error appears in the first inequality in t heo rem la, p. 196, of Glynn and Whit t [1], but not in the remainder term R( t ) there; the p roof of t heo rem 2(f) on p. 686 of Glynn and Whit t [2]; and (4.2) in the p roof of (1.16) in t heo rem 3 on p. 704 of Glynn and"""	adjusted winner procedure;comment (computer programming);emma;fifo (computing and electronics);linear algebra;linux;sensitivity index;social inequality	Ward Whitt	1992	Queueing Syst.	10.1007/BF01158812		ML	-11.447393548134114	4.9639650167530585	4901
02148cd37551505200e7f87837b7e9afcda01ab5	verifying controllability of time-aware business processes		We present an operational semantics for time-aware business processes, that is, processes modeling the execution of business activities, whose durations are subject to linear constraints over the integers. We assume that some of the durations are controllable, that is, they can be determined by the organization that enacts the process, while others are uncontrollable, that is, they are determined by the external world. Then, we consider controllability properties, which guarantee the completion of the enactment of the process, satisfying the given duration constraints, independently of the values of the uncontrollable durations. Controllability properties are encoded by quantified reachability formulas, where the reachability predicate is recursively defined by a set of Constrained Horn Clauses (CHCs). These clauses are automatically derived from the operational semantics of the process. Finally, we present two algorithms for solving the so called weak and strong controllability problems. Our algorithms reduce these problems to the verification of a set of quantified integer constraints, which are simpler than the original quantified reachability formulas, and can effectively be handled by state-of-the-art CHC solvers.	algorithm;business process;horn clause;integer programming;operational semantics;process (computing);reachability;recursion;recursive definition	Emanuele De Angelis;Fabio Fioravanti;Maria Chiara Meo;Alberto Pettorossi;Maurizio Proietti	2017		10.1007/978-3-319-61252-2_8	residual time;controllability;business process;database;real-time computing;business process modeling;operational semantics;computer science;business process management;integer	Logic	-13.333970192543346	24.01124648338715	4908
a41719531752004f46e0af5fedfa331cb6e4b356	méthodologie de définition de e-services pour la gestion des connaissances à partir d'un plateau de créativité : application au e-learning instrumental		En s'appuyant sur la théorie de l'activité, nous avons mis au point une méthodologie de gestion des connaissances à base de e-services sur un plateau de créativité visant à faire piloter le processus de fabrication métier par celui des usages. Nous l'avons testé avec la réalisation d'un e-service d'apprentissage instrumental de pièces de musique à la guitare (E-guitare). Dans le contexte mondial de compétitivité, de recherche et d'innovation, la conception de e-services, c'est-à-dire de produits/services accessibles au travers de l'Internet, nécessite un développement qui débouche sur l'usage (Musso et al. (2005). En effet, dans le monde mou-vant des Technologies de l'Information et de la Communication (TIC), et notamment avec l'émergence du Web 2.0, le progrès ne réside plus aujourd'hui seulement dans l'objet tech-nologique en lui-même, mais bien dans sa capacité à intégrer des contenus variés que l'on cherche à produire en co-construction avec les utilisateurs pour garantir son utilité. La gestion des connaissances se trouve donc plongée au coeur de ce processus créatif d'in-novation par l'usage (Poulain, 2002), à la fois démocratique (von Hippel (2005)) et participatif (Tapscott et Williams (2006)). Ce type de gestion des connaissances dont le but est de trouver des solutions centrées utilisateurs ne procède pas seulement à partir de données textuelles du domaine à extraire et à traiter. Par exemple dans le domaine du e-learning instrumental, les connaissances musicales ne sont pas formalisables uniquement par du texte codifié via XML (partition de musique en musicXML, mais aussi et surtout par d'autres média (son, image) qui rendent mieux compte du sens et de l'intention désirée par l'interprète sur les objets de la partition (notes, accords, doigtés, silences, phrasé, ...). Ce sont ces objets de l'interprétation hu-maine que nous cherchons à extraire et gérer car ils coïncident avec une vraie demande (Pachet (2004)) pour la diffusion des connaissances. La méthodologie que nous proposons s'appuie sur l'identification, l'extraction et la conversion de ces objets multimédia sur un plateau de créativité. Ils proviennent des savoir-faire im-plicites des interprètes experts. Nous souhaitons les expliciter (Nonaka et Takeuchi (1995)) avec des outils appropriés en tenant compte des pratiques du domaine. Pour cela on met en oeuvre des processus itératifs tant du côté de l'offre (métier) que de la demande (usage) qui s'appuient sur la démarche expérimentale. On combine ainsi des phases de généralisation inductive à partir d'exemples (conception et élaboration du e-service), de vérification des hypothèses (expérimentation du e-service) à l'aide …	bibliothèque de l'école des chartes;bibliothèque des ecoles françaises d'athènes et de rome;e-services;geforce 6 series;holographic principle;john c.s. lui;les houches accords;linear algebra;maker faire;meme;triple des;web 2.0;xml	Noël Conruyt;David Grosser;Olivier Sébastien	2008			firmware;operand;operating system;e-services;data processing system;control store;function code;computer science	Crypto	-102.71213047390238	21.584445100211262	4912
3f3ea07fd6acfa020ef0d844d197e86ffd4a8dba	scalable video streaming over p2p networks: a matter of harmony?	data transmission;optimisation;reliability;scalable video;video streaming;layered video;ucl;mesh based p2p;peer to peer network;harmony search p2p video streaming h 264 svc mesh based p2p pull delivery optimization;optimization technique;resource allocation;p2p video streaming;discovery;p2p;theses;conference proceedings;pull delivery;indexes;video streaming optimisation peer to peer computing resource allocation;digital web resources;peer to peer streaming;streaming media;ucl discovery;open access;indexation;static var compensators;ucl library;optimization;load balance;search problems;streaming media reliability indexes static var compensators peer to peer computing scalability search problems;scalability;book chapters;p2p networks;open access repository;peer to peer computing;harmony search;static var compensator;load balancing scalable video streaming p2p networks peer to peer networks receiver driven streaming mechanism video requesting policy np hard problem optimization harmony search;h 264 svc;ucl research	In this paper we address the problem of efficient layered video streaming over peer-to-peer networks and we propose a new receiver-driven streaming mechanism. The main design goal of our new layered video requesting policy is to optimize the overall distribution of video streams in terms of reliability and overhead. Since the layered peer-to-peer streaming problem is NP-Hard, we show that the classic approaches widely used in layered P2P streaming systems have some limitations and we propose an optimization technique based on harmony search which aims at increasing the rate of successful data transmissions for the most important video layers, while reducing the protocol overhead and ensuring load balancing among the participating peers. Analytical results have demonstrated that our new requesting policy enhances the streaming of layered video over mesh-based peer-to-peer networks and outperforms classic approaches.	harmony search;load balancing (computing);mathematical optimization;np-hardness;overhead (computing);peer-to-peer;streaming media	Samir Medjiah;Toufik Ahmed;Eleni Mykoniati;David Griffin	2011	2011 IEEE 16th International Workshop on Computer Aided Modeling and Design of Communication Links and Networks (CAMAD)	10.1109/CAMAD.2011.5941101	database index;scalability;harmony search;resource allocation;computer science;load balancing;peer-to-peer;reliability;static var compensator;multimedia;world wide web;computer network;data transmission	Metrics	-14.745367312959385	74.31122178463302	4916
81365c09f18e74e5d34dc369a60f524fd700d050	modeling and optimization of manufacturing process performance using modelica graphical representation and process analytics formalism	optimization;manufacturing process;process analytics;graphical user interface	This paper concerns the development of a design methodology and its demonstration through a prototype system for performance modeling and optimization of manufacturing processes. The design methodology uses a Modelica simulation tool serving as the graphical user interface for manufacturing domain users such as process engineers to formulate their problems. The Process Analytics Formalism, developed at the National Institute of Standards and Technology, serves as a bridge between the Modelica classes and a commercial optimization solver. The prototype system includes (1) manufacturing model components’ libraries created by using Modelica and the Process Analytics Formalism, and (2) a translator of the Modelica classes to Process Analytics Formalism, which are then compiled to mathematical programming models and solved using an optimization solver. This paper provides an experiment toward the goal of enabling manufacturing users to intuitively formulate process performance models, solve problems using optimization-based methods, and automatically get actionable recommendations.	mathematical optimization;semantics (computer science)	Guodong Shao;Alexander Brodsky;R. Miller	2018	J. Intelligent Manufacturing	10.1007/s10845-015-1178-6	artificial intelligence;machine learning;theoretical computer science;systems engineering;design methods;formalism (philosophy);modelica;solver;computer science;graphical user interface;analytics	Robotics	-58.778294209699055	12.775384439390493	4919
a7eb5e68792fd97dfac8c505bdf9a3c13b8a2eea	sampling simulation model profile data for analysis		The capture of data about the events executed by a discrete event simulation can easily lead to very large trace data files. While disk space is relatively inexpensive and mostly capable of storing these large trace files, the manipulation and analysis of these large trace files can prove difficult. Furthermore, some types of analysis must be performed in-core and they cannot be performed with the trace data exceeds the size of the physical RAM where the analysis is performed. Because of these limits, it is often necessary to strictly limit the simulation run time to satisfy the analysis time memory limits. Experience with the DESMetrics tool suite (a collection of tools to analyze event trace files), demonstrates that our in-memory analysis tools are limited to trace files on the order of 10GB (on a machine with 24GB of RAM). Furthermore, even when it is possible to analyze large trace files, the run time costs of performing this analysis can take several days to complete. While high performance analysis of traces data is not strictly necessary, the results should be available within some reasonably bounded time frame. This paper explores techniques to overcome the limits of analyzing very large event trace files. While explorations for out-out-core analysis have been examined as part of this work, the run time costs for out-of-core processing can increase processing time 10-fold. As a result, the work reported here will focus on an approach to capture and analyze small samples from the event trace file. The work reported in this paper will examine how closely the analysis from sampling matches the analysis from a full trace file. Two techniques for comparison are presented. First a visual comparison of analysis results between the full trace and a trace sample are presented. Second, numerical quantification of the different analysis results (between the full trace and trace sample) will be reported using the Wasserstein, Directed Hausdorff, and Kolmogorov-Smirnov distance metrics. Finally, the ability to process trace samples from a very large trace file of 80GB is demonstrated.	disk space;hausdorff dimension;in-memory database;numerical analysis;out-of-core algorithm;random-access memory;run time (program lifecycle phase);sampling (signal processing);simulation;tracing (software);visual comparison	Patrick Crawford;Peter D. Barnes;Stephan Eidenbenz;Philip A. Wilsey	2018		10.1145/3200921.3200944	discrete event simulation;real-time computing;sampling (statistics);data file;computer science;bounded function;hausdorff space	SE	-12.894893078978319	32.66891097113399	4924
0fef8b49c836955303fd7d97a80ebeda1318cd41	it impacts on operation-level agility in service industries	st gallen media reference model;enterprise mashups;electronic markets;design science	This study examines the relationship between operation-level agility and firm performance in service industries. The study is augmented by investigating the role of IT resources and competence to achieve this specific type of agility. As of to date, most of the published literature in this stream of research has focused on manufacturing industry. This research is an early attempt to examine the strategic value of IT-enabled operational capabilities in service industry. We propose a theory-based model of the positive relationships among IT service competence, operation-level agility, and firm performance. Survey data of medium to large-size enterprises in service industries in the United States were used to validate the proposed model. The results indicate that operation-level agility is a significant driving force of firm performance in the service industries and that IT service competence significantly determines the operation-level agility. The results emphasize that IT-supported operation-level agility significantly leads to a better performance.	theory	One-Ki Daniel Lee;Peng Xu;Jean-Pierre Kuilboer;Noushin Ashrafi	2009			systems engineering;engineering;marketing;software engineering	HPC	-81.90665448248322	4.768954640005194	4925
44840e1a131a699bd93df8176e9bb1786dd66282	business process co-design for energy-aware adaptation	green it and energy aware applications adaptive and context aware processes service oriented architectures for bpm resource management in business process execution;context aware;service oriented architecture business data processing energy consumption power aware computing resource allocation;energy efficient;resource allocation;conceptual model;energy consumption quality of service middleware business virtual environments time factors power demand;virtual environments;service oriented architectures for bpm;service oriented architecture business process codesign energy aware adaptation green it physical resource virtualization energy efficient hardware infrastructures energy consumption energy aware business process business process conceptual model energy leakage;green it and energy aware applications;power aware computing;time factors;energy consumption;business data processing;business;adaptive and context aware processes;time factor;middleware;virtual environment;quality of service;service oriented architecture;power demand;business process;resource management in business process execution	Green IT mainly focuses on techniques to extend the products longevity or to virtualise physical resources as well as the provision of energy efficient hardware infrastructures. Less attention has been paid on the applications that run on the machines and their impact on energy consumption. This paper proposes an approach for enabling an efficient use of energy driven by the design of energy-aware business processes. Energy-awareness is given by an enrichment of a typical Business Process conceptual model with annotations able to support the assessment of the energy consumption of the involved business tasks. This information is the basis for the energy-aware adaptation to enact specific strategies to adapt process execution in case energy consumption needs to be lowered or energy leakages have been identified.	business process;data center;data dependency;gene ontology term enrichment;operating system;quality of service;relevance;testbed	Cinzia Cappiello;Maria Grazia Fugini;Alexandre Mello Ferreira;Pierluigi Plebani;Monica Vitali	2011	2011 IEEE 7th International Conference on Intelligent Computer Communication and Processing	10.1109/ICCP.2011.6047917	real-time computing;simulation;knowledge management;artifact-centric business process model;business;business process discovery	Embedded	-42.56272807235547	41.990968076791475	4928
3bba293730c79fca1e82199e066cc7db40d3ed61	new constructions of convertible undeniable signature schemes without random oracles		In Undeniable Signature, a signature’s validity can only be confirmed or disavowed with the help of an alleged signer via a confirmation or disavowal protocol. A Convertible undeniable signature further allows the signer to release some additional information which can make an undeniable signature become publicly verifiable. In this work we introduce a new kind of attacks, called claimability attacks, in which a dishonest/malicious signer both disavows a signature via the disavowal protocol and confirms it via selective conversion. Conventional security requirement does not capture the claimability attacks. We show that some convertible undeniable signature schemes are vulnerable to this kind of attacks. We then propose a new efficient construction of fully functional convertible undeniable signature, which supports both selective conversion and universal conversion, and is immune to the claimability attacks. To the best of our knowledge, it is the most efficient convertible undeniable signature scheme with provable security in the standard model. A signature is comprised of three elements of a bilinear group. Both the selective converter of a signature and the universal converter consist of one group element only. Besides, the confirmation and disavowal protocols are also very simple and efficient. Furthermore, the scheme can be extended to support additional features which include the delegation of conversion and confirmation/disavowal, threshold conversion and etc. We also propose an alternative generic construction of convertible undeniable signature schemes. Unlike the conventional sign-then-encrypt paradigm, the signer encrypts its (standard) signature with an identity-based encryption instead of a public key encryption. It enjoys the advantage of short selective converter, which is simply an identity-based user private key, and security against claimability attacks.	bilinear filtering;dictionary attack;digital signature;formal verification;id-based encryption;malware;programming paradigm;provable security;public-key cryptography;undeniable signature	Qiong Huang;Duncan S. Wong	2009	IACR Cryptology ePrint Archive		public-key cryptography;verifiable secret sharing;delegation;encryption;undeniable signature;provable security;distributed computing;computer science	Crypto	-40.634060175868164	75.727591984325	4931
eb407d0b92796f73fa911d77817ebbe3076e5ef3	attacks on a lightweight cipher based on a multiple recursive generator	resource limitation;distinguishing attack;low complexity;rfid tag;known plaintext attack;cryptanalysis;cryptography;rfid;sensor nodes;wireless sensor networks	At IEEE GLOBECOM 2008, a lightweight cipher based on a Multiple Recursive Generator (MRG) was proposed for use in resource limited environment such as sensor nodes and RFID tags. This paper proposes two efficient attacks on this MRG cipher. A distinguishing attack is firstly introduced to identify the use of an MRG cipher that has a modulus suggested by its designers. It requires 218 words of ciphertext and the most significant bit of each corresponding plaintext word. Then an efficient known plaintext attack is proposed to construct the cipher’s current state and generate subkeys used for all subsequent encryption. The known plaintext attack, when targeted at the MRG ciphers optimized for efficiency, only requires 2k words of known plaintext and trivial computation where k is the MRG order. Even the ciphers based on complicated and inefficient MRGs can be attacked with low complexity, e.g., in the magnitude of 212 words of known plaintext for all MRG ciphers with order 47, regardless of which MRG modulus is used. These two attacks indicate that the examined MRG cipher structure is seriously flawed.	cipher;ciphertext;computation;cryptography;distinguishing attack;encryption;global communications conference;known-plaintext attack;modulus robot;most significant bit;plaintext;radio-frequency identification;recursion;time complexity	Lu Xiao;Gregory G. Rose	2009	Security and Communication Networks	10.1002/sec.189	radio-frequency identification;transposition cipher;differential cryptanalysis;two-square cipher;null cipher;key clustering;computer science;unicity distance;theoretical computer science;ciphertext-only attack;plaintext;stream cipher attack;stream cipher;internet privacy;malleability;slide attack;computer security;ciphertext	Security	-43.20429338180673	80.07684877222262	4935
c7e7c5f125afa5b41461a4ada6c261c4850170ce	service composition for rest	service composition;standards;escience domain service oriented architectures service reuse rest architectural style rest service composition rest constraints bpel composition language requirements analysis meta model bpel extension;context servers standards service oriented architecture unified modeling language computer architecture;rest;computer architecture;simulation workflow;servers;simulation workflow rest service composition bpel escience;unified modeling language;bpel;service oriented architecture;escience;context;web services business process execution language formal specification formal verification natural sciences computing service oriented architecture software reusability	One of the key strengths of service oriented architectures, the concept of service composition to reuse and combine existing services in order to achieve new and superior functionality, promises similar advantages when applied to resources oriented architectures. The challenge in this context is how to realize service composition in compliance with the constraints defined by the REST architectural style and how to realize it in a way that it can be integrated to and benefit from existing service composition solutions. Existing approaches to REST service composition are mostly bound to the HTTP protocol and often lack a systematic methodology and a mature and standards based realization approach. In our work, we follow a comprehensible methodology by deriving the key requirements for REST service composition directly from the REST constraints and then mapping these requirements to a standard compliant extension of the BPEL composition language. We performed a general requirements analysis for REST service composition, defined a meta model for a corresponding BPEL extension, realized this extension prototypically and validated it based on a real world use case from the eScience domain. Our work provides a general methodology to enable REST service composition as well as a realization approach that enables the combined composition of WSDL and REST services in a mature and robust way.	asynchronous i/o;business process execution language;callback (computer programming);e-science;hypertext transfer protocol;metamodeling;polling (computer science);representational state transfer;requirement;requirements analysis;service composability principle;service-oriented architecture;simulation;web services description language;web service	Florian Haupt;Markus Fischer;Dimka Karastoyanova;Frank Leymann;Karolina Vukojevic-Haupt	2014	2014 IEEE 18th International Enterprise Distributed Object Computing Conference	10.1109/EDOC.2014.24	unified modeling language;real-time computing;business process execution language;computer science;service delivery framework;operating system;software engineering;service-oriented architecture;service design;database;rest;server	SE	-47.538702140003835	18.11475643558691	4940
dcc9e41d2879c1f0ed966d54ee32bc1d699d7565	the business intelligence value chain: data-driven decision support in a data warehouse environment: an exploratory study	data warehouses data mining decision support systems;decision support;decision maker;data mining;value chain;decision support systems;information processing;data access;database usage business intelligence value chain data driven decision support data warehouse data mining value chain process model;process model;business intelligence;data warehouses;data warehouse;data warehouses databases data mining data analysis information processing information analysis uncertainty testing investments adaptive systems;exploratory study	The recent introduction of a spate of data access applications, such as OLAP and data mining tools, has led to an increased interest on the part of both scholars and practitioners on how best to use and benefit from these tools. This paper reports on six exploratory case studies involving eight decision-makers and seven endusers. A process model based on the Value Chain is proposed and explained. Results show that database usage and information processing practices have indeed grown more sophisticated. Implications for practice and future research aimed at testing the Value Chain model	data access;data mining;exploratory testing;information processing;online analytical processing;process modeling	M. Kathryn Brohman;Michael Parent;M. Pearce;Michael R. Wade	2000		10.1109/HICSS.2000.926905	data access;data modeling;decision-making;data quality;decision support system;data model;value chain;computer science;data science;data warehouse;process modeling;data mining;database;business intelligence;exploratory research	DB	-78.85921724012945	8.101546116131408	4959
3bfadf979e2a5ac447563cd1db9a7279d3b55d48	a usable interface for location-based access control and over-the-air keying in tactical environments	encryption;graphical interface;user interface;feeds;unmanned aerial vehicle;military operations location based access control over the air keying tactical environments usable graphical interface unmanned aerial vehicles mobile tactical devices falconview mission planning;over the air;usable human machine interface for security;remotely operated vehicles;video broadcasting;secure uav video broadcast access control usable human machine interface for security;military communication;mission planning;receivers;access control planning encryption receivers shape feeds;shape;human machine interface;telecommunication security;location based access control;mobile handsets;planning;telecommunication security access control military communication mobile handsets remotely operated vehicles;access control;secure uav video broadcast	This paper presents a usable graphical interface for specifying and automatically enacting access control rules for applications that involve dissemination of data among mobile tactical devices. A specific motivating example is unmanned aerial vehicles (UAVs), where the mission planner or operator needs to control the conditions under which specific receivers can access the UAV's video feed. We implemented a prototype of this user interface as a plug-in for FalconView, a popular mission planning application.	access control;aerial photography;communications security;falconview;graphical user interface;location-based service;plug-in (computing);prototype;secure communication;unmanned aerial vehicle;video	Adam Petcher;Roger I. Khazan;Daniil M. Utin	2011	2011 - MILCOM 2011 Military Communications Conference	10.1109/MILCOM.2011.6127515	embedded system;simulation;engineering;computer security	Robotics	-33.5400860157866	22.45535813877614	4962
26d7cc83b5e1beeec8c120aa223c9d5126708584	hierarchical set decision diagrams and regular models	structural model;process calculi;decision diagram;systemes et controle;state space;symbolic model checking;algorithms and data structure	This paper presents algorithms and data structures that exploit a compositional and hierarchical specification to enable more efficient symbolic modelchecking. We encode the state space and transition relation using hierarchical Set Decision Diagrams (SDD) [9]. In SDD, arcs of the structure are labeled with sets, themselves stored as SDD. To exploit the hierarchy of SDD, a structured model representation is needed. We thus introduce a formalism integrating a simple notion of type and instance. Complex composite behaviors are obtained using a synchronization mechanism borrowed from process calculi. Using this relatively general framework, we investigate how to capture similarities in regular and concurrent models. Experimental results are presented, showing that this approach can outperform in time and memory previous work in this area.	arcs (computing);algorithm;c++;data structure;diagram;encode;heuristic (computer science);model checking;nusmv;open-source software;process calculus;smart;semantics (computer science);state space;symbolic simulation;time complexity	Yann Thierry-Mieg;Denis Poitrenaud;Alexandre Hamez;Fabrice Kordon	2009		10.1007/978-3-642-00768-2_1	discrete mathematics;influence diagram;computer science;state space;artificial intelligence;theoretical computer science;mathematics;algorithm	Logic	-11.8732791442044	23.416370291220375	4980
99c2eee2e3f6b2ce8ba5dee6667607639bee5441	scalable architectures with hardware extensions for low bitrate variable bandwidth realtime videocommunications (scalar)	image coding;visual communication;hardware bit rate bandwidth;multimedia systems;umea university acts project ac077 scalable architectures hardware extensions real time videocommunications scalar project europe ericsson telecom ab bang olufsen eid inesc image coding university of linkoping university of surrey multimedia systems university of genoa;real time systems image coding visual communication multimedia systems;real time systems	The SCALAR Project consortium is a balanced mix of university research groups, large multinational European companies (Ericsson Telecom AB (S), Bang & Olufsen (DK)) and SME (EID (P), INESC (P)). Three university research groups which participate in the consortium have been long involved in research on image coding problems in complementary areas (University of Linkoping (S), University of Surrey (UK)) and research on image processing, multimedia systems and architectures, real-time systems in particular (University of Genoa (I)). Umea University (S) is the main educational institute in North of Sweden. The SCALAR project started at the end of 1995 and the field test will end in the first half of 1999.		Giancarlo Parodi;Ivano Barbieri;Marco Raggio	1999		10.1109/ICIAP.1999.797767	simulation;telecommunications;computer science;multimedia;visual communication	Arch	-18.903701098280955	98.03871957763984	4981
ba2f28e5913923426925fd3b9c82258e49fa4399	programming simultaneous actions using common knowledge	distributed system;fault tolerant;byzantine agreement;synchronous system;common knowledge;large classes	This work applies the theory of knowledge in distributed systems to the design of efficient fault-tolerant protocols. We define a large class of problems requiring coordinated, simultaneous action in synchronous systems, and give a method of transforming specifications of such problems into protocols that areoptimal in all runs: these protocols are guaranteed to perform the simultaneous actions as soon as any other protocol could possibly perform them, given the input to the system and faulty processor behavior. This transformation is performed in two steps. In the first step we extract, directly from the problem specification, a high-level protocol programmed using explicit tests for common knowledge. In the second step we carefully analyze when facts become common knowledge, thereby providing a method of efficiently implementing these protocols in many variants of the omissions failure model. In the generalized omissions model, however, our analysis shows that testing for common knowledge is NP-hard. Given the close correspondence between common knowledge and simultaneous actions, we are able to show that no optimal protocol for any such problem can be computationally efficient in this model. The analysis in this paper exposes many subtle differences between the failure models, including the precise point at which this gap in complexity occurs.	algorithmic efficiency;distributed computing;fault tolerance;high- and low-level;np-hardness	Yoram Moses;Mark R. Tuttle	1987	Algorithmica	10.1007/BF01762112	fault tolerance;real-time computing;computer science;distributed computing;algorithm;common knowledge	Theory	-12.739192149392538	28.853791462925198	4982
27db85a45d1cecd326bef3f7b5a65dbc981d697b	demystifying cluster-based fault-tolerant firewalls	tolerancia falta;perimeter;stateful firewall cluster;protocols;replication;networks;cortafuego;complexity theory;fault tolerant;fault tolerance telecommunication traffic availability;availability;authorisation;disponibilidad;unwanted traffic firewalls networks fault tolerance;securite informatique;distributed computing;network performance;unwanted traffic;replicacion;perimetro;computer security;pare feu reseau;telecommunication traffic;fault tolerant system;internet;redundancy;unwanted traffic issue;fault tolerant systems;state replication;seguridad informatica;fault tolerance;telecommunication security;firewalls;sistema tolerando faltas;telecommunication traffic authorisation computer network reliability fault tolerance internet telecommunication security;calculo repartido;stateful firewall cluster cluster based fault tolerant firewall architecture security solution unwanted traffic issue internet state replication;systeme tolerant les pannes;perimetre;fires;disponibilite;calcul reparti;cluster based fault tolerant firewall architecture;tolerance faute;security solution;computer network reliability;firewall	Firewalls are perimeter security solutions that are useful for addressing the unwanted traffic issue. However, designers must also appropriately address the network performance, availability, and complexity problems that firewalls introduce. The authors survey existing cluster-based fault-tolerant firewall architectures and discuss their trade-offs in these three areas. They present a preliminary evaluation of these architectures and discuss the need for state replication in stateful firewall clusters. They also discuss the difficulties of providing a simple, performance, and fault-tolerant cluster-based firewall solution.	fault tolerance;firewall (computing);network performance;norm (social);perimeter;state (computer science);stateful firewall	Pablo Neira Ayuso;Rafael M. Gasca;Laurent Lefèvre	2009	IEEE Internet Computing	10.1109/MIC.2009.128	application firewall;fault tolerance;computer science;operating system;distributed computing;computer security;computer network	HPC	-57.05488644425707	76.55118129047446	4983
71e02e29604262f33b11a143657e144e0d70451f	nonclausal temporal deduction	temporal logic;first order temporal logic	We present a proof system for propositional temporal logic. This system is based on nonclausal resolution; proofs are natural and generally short. Its extension to first-order temporal logic is considered. Two variants of the system are described. The first one is for a logic with [7 (“always”), 0 (“sometime”), and 0 (“next”). The second variant is an extension of the first one to a logic with the additional operators U (“until”) and P (“precedes”). Each of these variants is proved complete.	first-order predicate;linear temporal logic;natural deduction;p (complexity);proof calculus	Martín Abadi;Zohar Manna	1985		10.1007/3-540-15648-8_1	modal logic;predicate logic;dynamic logic;zeroth-order logic;discrete mathematics;linear temporal logic;temporal logic;many-valued logic;interval temporal logic;intermediate logic;mathematics;algorithm;rule of inference	Logic	-13.24159761868229	14.486324386881787	4984
9e4faddc843578fbac20d388e10aa8b7438b28f1	from patching delays to infection symptoms: using risk profiles for an early discovery of vulnerabilities exploited in the wild		At any given time there exist a large number of software vulnerabilities in our computing systems, but only a fraction of them are ultimately exploited in the wild. Advanced knowledge of which vulnerabilities are being or likely to be exploited would allow system administrators to prioritize patch deployments, enterprises to assess their security risk more precisely, and security companies to develop intrusion protection for those vulnerabilities. In this paper, we present a novel method based on the notion of community detection for early discovery of vulnerability exploits. Specifically, on one hand, we use symptomatic botnet data (in the form of a set of spam blacklists) to discover a community structure which reveals how similar Internet entities behave in terms of their malicious activities. On the other hand, we analyze the risk behavior of end-hosts through a set of patch deployment measurements that allow us to assess their risk to different vulnerabilities. The latter is then compared to the former to quantify whether the underlying risks are consistent with the observed global symptomatic community structure, which then allows us to statistically determine whether a given vulnerability is being actively exploited in the wild. Our results show that by observing up to 10 days’ worth of data, we can successfully detect vulnerability exploitation with a true positive rate of 90% and a false positive rate of 10%. Our detection is shown to be much earlier than the standard discovery time records for most vulnerabilities. Experiments also demonstrate that our community based detection algorithm is robust against strategic adversaries.	algorithm;botnet;entity;existential quantification;exploit (computer security);feature extraction;malware;patch (computing);sensitivity and specificity;software deployment;spamming;system administrator;vulnerability (computing)	Chaowei Xiao;Armin Sarabi;Yang Liu;Bo Li;Mingyan Liu;Tudor Dumitras	2018			internet privacy;computer security;computer science;vulnerability	Security	-58.14179180564853	60.98246497986632	4989
a35a5c9c10dafa49ee592729d82dd95982a06c6a	leveraging peer-to-peer and ontologies for the extended enterprise	cob;collaboration;integration;distributed knowledge management;virtual office;information exchange;information quality;virtual enterprise;community of business;extended enterprise;p2p networks;peer to peer;peer to peer networks;ontology	In recent years, organisations are blurring their boundaries interacting with other organisations. This process fostered new business paradigms and organisational forms that transcend the previous static and closed competitive models and move to flexible and collaborative ways of working. Examples of new models are the extended enterprise (EE) and virtual enterprise (VE). For promoting those new organisational models are required adequate technologies enabling collaboration, integration and exchanging of information across heterogeneous and distributed sources. Moreover, in such environments, another important aspect to deal with is related to the 'quality' of information and knowledge exchanged. For fulfilling those requirements, we argue that a peer-to-peer (P2P) architecture implementing the 'virtual office' paradigm combined with adequate semantic supports can be an effective solution. This paper presents K-link+, a P2P system implemented in JXTA based on the concepts of 'community of business', 'virt...	extended enterprise;ontology (information science);peer-to-peer	Giuseppe Pirrò;Massimo Ruffolo;Domenico Talia	2008	IJBPIM	10.1504/IJBPIM.2008.024979	enterprise software;information exchange;computer science;knowledge management;software engineering;ontology;data mining;database;enterprise architecture;information quality;enterprise integration;management;world wide web;enterprise information system;enterprise life cycle;collaboration	Theory	-51.59950371396776	13.653178496792947	5000
b0855a284540648f14c15f7e83c8415d7f8c099a	the power of tree projections: when local consistency answers conjunctive queries		Enforcing local consistency is a well-known technique to simplify the evaluation of conjunctive queries. It consists of repeatedly taking the semijion between every pair of query atoms, until the procedure stabilizes. If some relation becomes empty, then the query has an empty answer. Otherwise, we cannot say anything in general, unless we have some information on the structure of the given query. In fact, a fundamental result in database theory states that the class of queries for which---on every database---local consistency entails global consistency is precisely the class of acyclic queries. #R##N#In the last few years, several efforts have been spent to define structural decomposition methods isolating larger classes of nearly-acyclic queries, yet retaining the same nice properties as acyclic ones. In this paper, we precisely characterize the power of local consistency procedures in the general framework of tree projections, where a query Q and a set W of views (i.e., resources that can be used to answer Q) are given, and where one looks for an acyclic hypergraph covering Q, and covered by W---all known structural decomposition methods are just special cases of this framework, defining their specific set of resources. We show that the existence of tree projections of certain subqueries is a necessary and sufficient condition to guarantee that local consistency allows the query to be answered efficiently, even without computing any tree projection. In particular, tight characterizations are given not only for the decision problem, but also when answers have to be computed.	conjunctive query;local consistency	Gianluigi Greco;Francesco Scarcello	2012	CoRR		boolean conjunctive query;data mining;database;range query;algorithm	DB	-24.739165475872646	11.237792679181474	5002
140877031d1a1dd4b558f24e227c4fbda28716a3	statistic-based dynamic complexity measurement for web service system		The existing research mainly concerns on the static complexity measurement for service-based system, but the dynamic features like execution behavior have been ignored. In this paper, we proposed a hierarchical measurement framework for evaluating the complexity of Web services from the dynamic aspect. At the level of single service, fluctuation rate is used to represent the QoS (Quality of Service) change during service invocation. Then, a cumulative distribution function is used to measure the dynamic complexity of service performance. At the system level, execution vectors and the corresponding probabilities can be counted according to the trace set of system dynamic executions. Subsequently, the complexity of dynamic execution behaviors can be calculated by the usage of entropy value. In addition, the rationality of above metrics has been validated by the studies on two real applications.		Chengying Mao	2016	Informatica (Slovenia)		real-time computing;simulation;computer science;distributed computing;computer security	Metrics	-24.820336945575526	62.72423676285386	5004
45afe3a94c5c29ff3c48455ceda13b949145ca80	student-centered design of a parser visualization tool	visualization;syntax trees	There exist many visualizations tools which visualize some aspect of the compiling process. They can be separated in two groups. On the one hand there are the ones which have a clear educational aim e.g. JFlap [3]. On the other hand, there are tools which display some aspects of the parsers generated with a particular generation tool e.g. ANTLRworks. The limitations of these tools are that they depend on a particular generation tool and they offer a partial view of the parsing process. In this situation when a student/teacher needs to visualize a different aspect, has to change both, the generation and visualization tool, adapting to the new syntax notation and graphical representations. We have developed VAST [1], an educational tool to display the syntax tree and its construction process within the compiling process. One of the main advantages of VAST is its independence from the parsing technique and the parser generator. We have used VAST with: LL parsers developed with ANTLR and LALR parsers developed with CUP. VAST has passed educational and usability evaluations. Although the results of the last evaluation [2] were positive, we realized that, due to the generic approach of VAST, the user has to perform many intermediate steps until he/she produces the visualizations.	antlr;cups;compiler;compiler-compiler;graphical user interface;jflap;lalr parser;ll parser;parse tree;parsing;usability;video ad serving template	Francisco J. Almeida-Martínez;Jaime Urquiza-Fuentes;Manuel Rubio-Sánchez;J. Ángel Velázquez-Iturbide	2010		10.1145/1822090.1822203	natural language processing;visualization;computer science;theoretical computer science;programming language	HCI	-29.035385049014796	25.322893749721498	5006
9aa6031e9144d50a61b132c2293deaded438bd1b	informationsbeschaffung zu sap-produkten und deren umfeld auf dem internet: informationen von anwendern, online-zeitschriften und informations-diensten				Gerhard Knolmayer	1996	Wirtschaftsinformatik			Theory	-100.77456428030447	27.56651237957533	5011
bfb044aae7e86f86d457319bca4bdb67a11ea002	integration of process monitoring and inspection based on agents and manufacturing features	inspection information process monitoring process inspection manufacturing features agent features production mode machining conditions dynamic inspection machining quality real time monitoring information intelligent software agents integrated framework information carrier connect monitoring;software agents inspection machining process monitoring production engineering computing;features machining monitoring inspection intelligent agents;inspection monitoring machining finishing planning sensors manufacturing	Small batch and multiple variety production mode and changing machining conditions call for dynamic inspection to guarantee machining quality. Dynamic inspection by considering real time monitoring information is a promising approach, but there is no available technology for integrating real time monitoring and inspection. To address this issue, this paper proposes a method of integrating monitoring and inspection based on intelligent software agents and manufacturing features. An agent-based approach is applied to develop an integrated framework, while manufacturing features are used as the information carrier to represent and connect monitoring and inspection information. Dynamic inspection is triggered according to the analysis results of real time monitoring. A prototype system has been developed to implement and validate the proposed method.	agent-based model;bridging (networking);feature recognition;intelligent agent;prototype;software agent	Changqing Liu;Yingguang Li;Weiming Shen	2014	Proceedings of the 2014 IEEE 18th International Conference on Computer Supported Cooperative Work in Design (CSCWD)	10.1109/CSCWD.2014.6846843	automated optical inspection	Robotics	-54.131383853456576	12.074063663685903	5014
91da6f1b58f6b0efb32fd145e8ce7d887b11d118	views over rdf datasets: a state-of-the-art and open challenges		Views on RDF datasets have been discussed in several works, nevertheless there is no consensus on their definition nor the requirements they should fulfill. In traditional data management systems, views have proved to be useful in different application scenarios such as data integration, query answering, data security, and query modularization. In this work we have reviewed existent work on views over RDF datasets, and discussed the application of existent view definition mechanisms to four scenarios in which views have proved to be useful in traditional (relational) data management systems. To give a framework for the discussion we provided a definition of views over RDF datasets, an issue over which there is no consensus so far. We finally chose the three proposals closer to this definition, and analyzed them with respect to four selected goals.	data security;requirement	Lorena Etcheverry;Alejandro A. Vaisman	2012	CoRR		computer science;data mining;database;information retrieval	DB	-37.408451941526884	4.824538736006072	5015
03f167cc9feb229f166e2f5d276af6efdf3096d0	cryptographic accumulators for authenticated hash tables	hash table;experimental evaluation;query answering;bilinear pairing;data structure;security protocol	Hash tables are fundamental data structures that optimally answer membership queries. Suppose a client stores n elements in a hash table that is outsourced at a remote server. Authenticating the hash table functionality, i.e., verifying the correctness of queries answered by the server and ensuring the integrity of the stored data, is crucial because the server, lying outside the administrative control of the client, can be malicious. We design efficient and secure protocols for optimally authenticating (non-)membership queries on hash tables, using cryptographic accumulators as our basic security primitive and applying them in a novel hierarchical way over the stored data. We provide the first construction for authenticating a hash table with constant query cost and sublinear update cost, strictly improving upon previous methods. Our first solution, based on the RSA accumulator, allows the server to provide a proof of integrity of the answer to a membership query in constant time and supports updates in O (nǫ log n) time for any fixed constant 0 < ǫ < 1, yet keeping the communication and verification costs constant. It also lends itself to a scheme that achieves different trade-offs—namely, constant update time and O(nǫ) query time. Our second solution uses an accumulator that is based on bilinear pairings to achieve O(nǫ) update time at the server while keeping all other complexities constant. Both schemes apply to two concrete data authentication models and an experimental evaluation shows good scalability. A preliminary version of this work was presented at the 15th ACM Conference on Computer and Communications Security (CCS), Alexandria, VA, 2008. Department of Computer Science, Brown University. Email: cpap@cs.brown.edu. Department of Computer Science, Brown University. Email: rt@cs.brown.edu. Department of Computer Science, Boston University and Department of Computer Science, Brown University. Email: nikos@cs.bu.edu. Research performed primarily while the author was with the Department of Computer Science at Aarhus University, Denmark.	accumulator (computing);bilinear filtering;communications security;computer science;correctness (computer science);cryptography;data structure;email;hash table;message authentication;scalability;server (computing);time complexity;verification and validation	Charalampos Papamanthou;Roberto Tamassia;Nikos Triandopoulos	2009	IACR Cryptology ePrint Archive		left-child right-sibling binary tree;exponential tree;kademlia;trie;theoretical computer science;mathematics;distributed computing;search tree;tree;ternary tree;algorithm	Theory	-35.18139516059857	76.58455230184924	5025
e6305e00746f75401fde3f4719f037a9fd183d7c	staying secure and unprepared: understanding and mitigating the security risks of apple zeroconf	bluetooth electronic mail authentication printers usability ip networks;electronic mail;printers;authentication;public key cryptography message authentication mobile computing;ip networks;bluetooth;usability;biometric technique security risk apple zeroconf usability oriented design zero configuration automatic service discovery plug and play technique core bluetooth framework multipeer connectivity bonjour man in the middle attack mitm attack apple issued public key certificate conflict detection approach	"""With the popularity of today's usability-oriented designs, dubbed Zero Configuration or ZeroConf, unclear are the security implications of these automatic service discovery, """"plug-and-play"""" techniques. In this paper, we report the first systematic study on this issue, focusing on the security features of the systems related to Apple, the major proponent of ZeroConf techniques. Our research brings to light a disturbing lack of security consideration in these systems' designs: major ZeroConf frameworks on the Apple platforms, including the Core Bluetooth Framework, Multipeer Connectivity and Bonjour, are mostly unprotected and popular apps and system services, such as Tencent QQ, Apple Handoff, printer discovery and AirDrop, turn out to be completely vulnerable to an impersonation or Man-in-the-Middle (MitM) attack, even though attempts have been made to protect them against such threats. The consequences are serious, allowing a malicious device to steal the user's SMS messages, email notifications, documents to be printed out or transferred to another device. Most importantly, our study highlights the fundamental security challenges underlying ZeroConf techniques: in the absence of any pre-configured secret across different devices, authentication has to rely on Apple-issued public-key certificate, which however cannot be properly verified due to the difficulty in finding a unique, nonsensitive and widely known identity of a human user to bind her to her certificate. To address this issue, we developed a suite of new techniques, including a conflict detection approach and a biometric technique that enables the user to speak out her certificate through 6 distinct, rare but pronounceable words to let those who know her voice verify her certificate. We performed a security analysis on the new protection and evaluated its usability and effectiveness using two user studies involving 60 participants. Our research shows that the new protection fits well with the existing ZeroConf systems such as AirDrop. It is well received by users and also providing effective defense even against recently proposed speech synthesis attacks."""	adversary (cryptography);authentication;biometrics;bluetooth;email;fits;file synchronization;malware;man-in-the-middle attack;plug and play;printer (computing);printing;public key certificate;public-key cryptography;service discovery;speaker recognition;speech synthesis;tencent qq;usability testing;zero-configuration networking;mdnsbrowser	Xiaolong Bai;Luyi Xing;Nan Zhang;XiaoFeng Wang;Xiaojing Liao;Tongxin Li;Shi-Min Hu	2016	2016 IEEE Symposium on Security and Privacy (SP)	10.1109/SP.2016.45	usability;computer science;authentication;internet privacy;bluetooth;world wide web;computer security	Security	-54.01735772151904	62.128053584497074	5029
4742eacadf63eb59572d4dc20cfe4f21ddaa0207	método de detección de párpados basado en un enfoque difuso de optimización multiobjetivo				Yuniol Alvarez-Betancourt;Miguel García-Silvente	2014	Computación y Sistemas			Crypto	-105.9107798259067	17.50222019799012	5042
442e80aec1db5e037cda704c15f8875f23a66f58	parallel jess	programming environments;parallel programming;expert system shells;java	Parallel production or rule-based systems, like a parallel version of Jess, are needed for real applications. The proposed architecture for such a system is based on a wrapper allowing the cooperation between several instances of Jess running on different computers. The system has been designed having in mind the final goal to speedup current P system simulators. Preliminary tests show its efficiency in this particular case and on classical benchmarks	benchmark (computing);computer;distributed computing;jess;message passing interface;p system;rule-based system;simulation;speedup;system migration	Dana Petcu	2005	The 4th International Symposium on Parallel and Distributed Computing (ISPDC'05)	10.1109/ISPDC.2005.38	parallel computing;real-time computing;computer science;theoretical computer science;operating system;distributed computing;programming language;java	Arch	-11.754418793763183	39.66404658419218	5045
8aff70fbff369570a11f703c5a69934eafaa9b73	hardness of computing the most significant bits of secret keys in diffie-hellman and related schemes	public key;diffie hellman key exchange;hidden number problem;diffie hellman	We show that computing the most signi cant bits of the secret key in a Di e-Hellman keyexchange protocol from the public keys of the participants is as hard as computing the secret key itself. This is done by studying the following hidden number problem: Given an oracle O ; (x) that on input x computes the k most signi cant bits of g + mod p, nd ; mod p. We present many other applications of this problem including: (1) MSB's in El-Gamal encryptions, Shamir Message passing scheme etc. are hard to compute. (2) Factoring with hints. Our results lead us to suggest a new variant of Di e-Hellman key exchange, for which we prove the most signi cant bit is hard to compute.	diffie–hellman key exchange;emoticon;google+;integer factorization;key (cryptography);message passing;most significant bit	Dan Boneh;Ramarathnam Venkatesan	1996		10.1007/3-540-68697-5_11	computer science;theoretical computer science;diffie–hellman key exchange;key generation;mathematics;distributed computing;key distribution;computer security	Crypto	-38.61554551906383	76.92522983141141	5046
a6e5b80a995bdf610823da3069d56aafb168bdd5	overtorrent: anticensorship without centralized servers	protocols;encryption;sockets;servers;internet;censorship	Anticensorship continues to be an arms race between citizens who use circumvention systems and the censors that find and block them. Many recently proposed circumvention systems have found ways to hide from deep-packet inspection (DPI), as well as active probing by the censors. However, many circumvention systems have trouble distributing the necessary resources to its users, and often use centralized distribution systems that can be easily blocked.	bittorrent;censoring (statistics);centralized computing;coat of arms;deep packet inspection;distributed hash table;network packet;peer-to-peer	Yu-Ju Lee;Eric Wustrow	2016	2016 14th Annual Conference on Privacy, Security and Trust (PST)	10.1109/PST.2016.7906961	communications protocol;the internet;computer science;censorship;internet privacy;world wide web;computer security;encryption;server	EDA	-56.681229001357394	66.33823206708693	5055
3b9cc8df409b154fcc4592bb190db8fe14fce4df	the presumed-either two-phase commit protocol	memory protocols;atomicity presumed either two phase commit protocol log piggybacking cost committing transactions performance advantages presumed abort protocol distributed transaction transaction processing;software performance evaluation;atomicity;two phase commit protocol;distributed transaction;distributed databases;transaction processing;distributed databases transaction processing software performance evaluation memory protocols;protocols tail peer to peer computing costs resource management database systems data structures;presumed abort;presumed commit	This paper describes the presumed-either two-phase commit protocol. Presumed-either exploits log piggybacking to reduce the cost of committing transactions. If timely piggybacking occurs, presumed-either combines the performance advantages of presumed-abort and presumed-commit. Otherwise, presumed-either behaves much like the widely-used presumed-abort protocol.	allocate-on-flush;best, worst and average case;piggybacking (security);stable storage;two-phase commit protocol	Gopi K. Attaluri;Kenneth Salem	2002	IEEE Trans. Knowl. Data Eng.	10.1109/TKDE.2002.1033784	three-phase commit protocol;commit;real-time computing;two-phase commit protocol;database transaction;transaction processing;distributed transaction;computer science;x/open xa;database;distributed computing;compensating transaction;serializability;distributed database;acid;transaction processing system;atomicity	DB	-22.12643449630266	49.03772643102179	5061
6d46e69ad38f026e218210c4b7472a10ca2066f1	classifying and filtering users by similarity measures for trust management in cloud environment		Trust represents an important issue for adopting cloud services. A trust management framework is essentially, about user rating. Hence, correctly addressing user feedback and filtering out malicious rating is a main step in providing reliable services. In order to process their feedback and calculate a reliable trust degree. Thus, new opportunities can be offered for the establishment of a trust relationship among involved entities. We propose a technique to process user rating by statistical methods. Then, we proceed to classify the users into different groups to detect malicious users. The users are grouped according to their rating by a k-means clustering technique, and the evaluation will show that the proposed solution gives better results than the traditional filtering solution.	authentication;authorization;cloud computing;cluster analysis;encryption;entity;feedback;interaction;k-means clustering;malware;requirement;service pack;sybil attack;trust (emotion);trust management (information system);trust management (managerial science)	Fatima Zohra Filali;Belabbes Yagoubi	2015	Scalable Computing: Practice and Experience		filter (signal processing);cloud computing;data mining;cluster analysis;computer science	Web+IR	-53.45150808484299	65.64613320936479	5062
16038bf8455ced2c9b902d2410d984fdb79db88f	migrating sgx enclaves with persistent state		Hardware-supported security mechanisms like Intel Software Guard Extensions (SGX) provide strong security guarantees, which are particularly relevant in cloud settings. However, their reliance on physical hardware conflicts with cloud practices, like migration of VMs between physical platforms. For instance, the SGX trusted execution environment (enclave) is bound to a single physical CPU. Although prior work has proposed an effective mechanism to migrate an enclave's data memory, it overlooks the migration of persistent state, including sealed data and monotonic counters; the former risks data loss whilst the latter undermines the SGX security guarantees. We show how this can be exploited to mount attacks, and then propose an improved enclave migration approach guaranteeing the consistency of persistent state. Our software-only approach enables migratable sealed data and monotonic counters, maintains all SGX security guarantees, minimizes developer effort, and incurs negligible performance overhead.	central processing unit;intel developer zone;network enclave;openvms;overhead (computing);trusted execution environment;while	Fritz Alder;Arseny Kurnikov;Andrew Paverd;N. Asokan	2018	2018 48th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)	10.1109/DSN.2018.00031	guard (information security);real-time computing;cryptography;mount;distributed computing;virtual machining;software;cloud computing;computer science;data loss	OS	-54.70637174151846	57.058181367833576	5064
dfaf3fba72cc45b25d1ebc8adac6d6037beeba62	mobility support in private networks using rpx	elektroteknik och elektronik;3g networks;mobile radio 3g mobile communication ip networks;wireless communication systems;private networks;null;connection request delay;network address translator;type of service;3g mobile communication;intelligent networks scalability proposals australia wireless communication tunneling delay space technology web and internet services network address translation;mobile radio;ip networks;ipv4 address space;packet transmission delay;packet transmission delay mobility support private networks ipv4 address space 3g networks wireless communication system ip in fqdn tunneling connection request delay;mobile ip;ip in fqdn tunneling;wireless communication system;mobility support;mobile user	The limited IPv4 address space has driven the cellular industry to start using IPv6 addresses for mobile users in 3G-networks. However, there is a potential threat to the success of 3G-network deployment, as the success will depend on the services offering to the end users. Currently, the overwhelming proportion of services resides in the IPv4 address space, which makes them inaccessible to users in the IPv6 address space. Thus, users cannot directly communicate with and access services without an intermediate translation mechanism. Previous studies on network address translation methods have shown that REBEKAH-IP with Port Extension, RPX is promising in that it provides excellent theoretical maximum scalability while supporting all types of services without limitations for the users in the network. However, the initial RPX proposal does not support host mobility to different networks, despite the fact that mobility is the most important feature of a wireless communication system. In this paper, we propose to extend the RPX scheme with a mobility support scheme based on mobile IP. RPX allows more than one host to use a single IPv4 address and therefore we have augmented Mobile IP with a new tunneling mechanism called IP-in-FQDN tunneling. The mechanism allows for unique mapping despite the sharing of IP addresses while maintaining the scalability of RPX. In addition, we present simulation results that indicate that the proposed scheme performs well in terms of scalability, connection request delay and packet transmission delay compared to mobile IP	address space;mobile ip;network address translation;network packet;private network;scalability;simulation;software deployment;tunneling protocol	Sanchai Rattananon;Björn Landfeldt;Aruna Seneviratne;Prawit Chumchu	2005	The IEEE Conference on Local Computer Networks 30th Anniversary (LCN'05)l	10.1109/LCN.2005.90	telecommunications;computer science;type of service;distributed computing;private network;mobile ip;ipv4 address exhaustion;computer network	Mobile	-12.144587250190764	90.49621905770225	5066
fe8aa600804cd96f3d6371cccadce5ba8426849b	automatic cross validation of multiple specifications: a case study	algebraic specification;software tool;formal specification;behavior modeling;temporal logic;software engineering;model checking;consistency checking;software component;cross validation;software specification	The problem of formal software specification has been addressed and discussed since the infancy of software engineering. However, among all the proposed solutions, none is universally accepted yet. Many different formal descriptions can in fact be given for the same software component; thus, the problem of determining the consistency relation among those descriptions becomes relevant and potentially critical. In this work, we propose a method for comparing two specific kinds of formal specifications of containers. In particular, we check the consistency of intensional behavior models with algebraic specifications. The consistency check is performed by generating a behavioral equivalence model from the intensional model, converting the algebraic axioms into temporal logic formulae, and then checking them against the model by using the NuSMV model checker. An automated software tool which encodes the problem as model checking has been implemented to check the consistency of recovered specifications of relevant Java classes.	cross-validation (statistics)	Carlo Ghezzi;Andrea Mocci;Guido Salvaneschi	2010		10.1007/978-3-642-12029-9_17	behavioral modeling;model checking;reliability engineering;software requirements specification;verification and validation;formal methods;temporal logic;computer science;software design;component-based software engineering;software engineering;software construction;formal specification;database;programming language;cross-validation	SE	-44.67538772526506	29.27603143175269	5067
52413619df825e7b1e88b8792695d7976e4a6687	interactions among dynamic sets of objects	specification;communication model;synchronisation;object orientation;interaction model;event;interaction constraints;communication	In this paper, we present an operator to model interactions among objects. Our proposal allows a variable number of participant objects in an interaction, and this number will be fixed during the execution of the model. This provides a very flexible interaction model based on synchronous interactions among several objects. Our interaction model is based on events and allows a multiple-way communication among objects. Concrete values of a communication are generated through constraints which are imposed locally on each participant object. The proposed interaction (and communication) model is very versatile and can be used as an abstract specification mechanism.	client–server model;dining philosophers problem;interaction;language of temporal ordering specification;server (computing);software development;symmetric multiprocessing	Jesús Torres;Juan Antonio Ortega;Miguel Toro	2003	Requirements Engineering	10.1007/s00766-002-0149-6	synchronization;real-time computing;models of communication;event;computer science;distributed computing;object-orientation;specification	SE	-32.084593761135835	34.35670866778606	5068
43433bc77fa6c5f25eaa3e745215ce0e8aebb7fc	dynamic voltage scaling for multitasking real-time systems with uncertain execution time	probability;low energy;dynamic voltage scaling;hard real time system;periodic tasks;multitasking;task scheduling;heuristic algorithm;energy saving;hard real time;real time systems	Dynamic voltage scaling (DVS) for real-time systems has been extensively studied to save energy. Previous studies consider the probabilistic distributions of tasks' execution time to assist DVS in task scheduling. These studies use probability information for intra-task frequency scheduling but do not sufficiently explore the opportunities for inter-task scheduling to save more energy. This paper presents a new approach to integrate intra-task and inter-task frequency scheduling for better energy savings in hard real-time systems with uncertain task execution time. Our approach has two steps: (a) We calculate statistically optimal frequency schedules for multiple periodic tasks using earliest deadline first (EDF) scheduling for processors that can change frequencies continuously. (b) For processors with a limited range of discrete frequencies, we further present a heuristic algorithm to construct frequency schedules. Our evaluation shows that our approach saves up to 23% more energy than existing solutions.	algorithm;central processing unit;computer multitasking;descriptive video service;dynamic voltage scaling;earliest deadline first scheduling;emoticon;heuristic (computer science);image scaling;real-time clock;real-time computing;run time (program lifecycle phase);schedule (computer science);scheduling (computing)	Changjiu Xian;Yung-Hsiang Lu	2006	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1145/1127908.1127998	heuristic;fair-share scheduling;fixed-priority pre-emptive scheduling;parallel computing;real-time computing;earliest deadline first scheduling;human multitasking;dynamic priority scheduling;computer science;probability;distributed computing;least slack time scheduling;statistics	Embedded	-5.451460778140598	58.920025161597	5071
fa0838a8cda5a7882e515cb390cc27bd898c9d44	hierarchies of spatially extended systems and synchronous concurrent algorithms	hierarchical structure;spatially extended systems;mathematical model;natural science	First, we study the general idea of a spatially extended system (SES) and argue that many mathematical models of systems in computing and natural science are examples of SESs. We examine the computability and the equational deenability of SESs and show that, in the discrete case, there is a natural sense in which an SES is computable if, and only if, it is deenable by equations. We look at a simple idea of hierarchical structure for SESs and, using respacings and retimings, we deene how one SES abstracts, approximates , or is implemented by another SES. Secondly, we study a special kind of SES called a synchronous concurrent algorithm (SCA). We deene the simplest kind of SCA with a global clock and unit delay which are computable and equationally deenable by primitive recursive equations over time. We focus on two examples of SCAs: a systolic array for convolution and a non-linear model of cardiac tissue. We investigate the hierarchical structure of SCAs by applying the earlier general concepts for the hierarchical structure of SESs. We apply the resulting SCA hierarchy to the formal analysis of both the implementation of a systolic array and the approximation of a biologically detailed model of cardiac tissue.	accessible surface area;approximation;computability;computable function;concurrent algorithm;convolution;linear model;mathematical model;nonlinear system;primitive recursive function;recursion;scsi enclosure services;systolic array	M. J. Poole;Arun V. Holden;J. V. Tucker	1998		10.1007/3-540-49254-2_6	discrete mathematics;mathematics;algorithm	Theory	-9.174872280349545	31.102789175350754	5082
9861ac4445b8b34002b225f78c3fe0361873e775	delay-based overlay construction in p2p video broadcast	data sharing;video streaming;peer to peer systems;streaming video;overlay networks;distributed computing;construction industry;p2p;video quality;video broadcasting;indexing terms;peer to peer system;media;overlay networks peer to peer systems video streaming distributed computation and control;distributed computation and control;servers;streaming media;video streaming peer to peer computing;overlay network;bandwidth;delay multimedia communication broadcasting peer to peer computing streaming media bandwidth video sharing intelligent networks jacobian matrices video signal processing;packet scheduling;video broadcast;peer to peer computing;delay based overlay construction;video streaming delay based overlay construction video broadcast peer to peer systems;free riding	We consider streaming video content over an overlay network of peer nodes. Each of the nodes employs a mesh-pull mechanism to organize the download of data units from its neighbours. We propose a novel algorithm for constructing the distribution overlay, where peers are arranged in neighbourhoods that exhibit similar latency values from the origin media server. Such an organization increases data sharing between neighbours in broadcast applications and reduces the play-out latency at a peer. Each of the nodes in the overlay is further equipped with a packet scheduling procedure that requests data units from neighbours in the order of their importance and their popularity within the neighbourhood. Finally, requesting peers share the upload bandwidth of a sending peer in proportion to their transmission rate to that peer in order to discourage free-riding in the system. Our simulation results show that the proposed mesh construction procedure provides improved performance in terms of frame-freeze and playback latency relative to a conventional approach where peer neighbours are selected at random. Corresponding gains in video quality for the media presentation are also registered due to the improved continuity of the playback experience.	algorithm;digital video;download;media server;network packet;overlay network;peer-to-peer;scheduling (computing);scott continuity;server (computing);simulation;streaming media;upload	Jacob Chakareski;Pascal Frossard	2009	2009 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2009.4960001	overlay network;computer science;internet privacy;world wide web;computer network	DB	-15.956184868417727	73.51710246970161	5093
bdca6a15b2fd6606410bfbb01f8ca703d4259c3d	ancestor excludable hierarchical id-based encryption revisited	ancestor excludable hierarchical identity-based encryption;identity-based cryptography;provable security	A hierarchical ID-based encryption (HIBE) allows a root Pri- vate Key Generator (PKG) to delegate private key generation and iden- tity authentication to lower-level PKGs. However, any ancestor in the path can generate a private key for any descendant node and thus decrypt the ciphertext. In an ancestor-excludable HIBE (AE-HIBE) scheme, an- cestors with a level less than the designated one can be excluded from a set of privileged ancestors who have the right to decrypt a ciphertext to a target node. We find that the functional definition and the concrete scheme proposed by Miyaji are flawed. To fix the problem, we introduce a new functional definition of AE-HIBE and present a new AE-HIBE scheme. The new scheme is proved to be ID-CPA secure in the random oracle and can be converted to ID-CCA security by applying a conver- sional method.	id-based encryption	Fan Zhang;Hua Guo;Zhoujun Li	2013		10.1007/978-3-642-38631-2_54	theoretical computer science;computer security;algorithm	Crypto	-40.46512637937227	76.30721043747079	5095
d451730433efe782f6aa81fe2d2dbb89c43542c5	root-cause analysis of san performance problems: an i/o path affine search approach	configuration information;storage area network;performance analysis storage area networks network servers switches application software algorithm design and analysis telecommunication traffic local area networks context modeling high speed networks;generic algorithm;i o path affine search approach;performance management;application software;building block;high speed networks;search algorithm;information search;reference algorithm;storage area networks;telecommunication traffic;network servers;computer network management storage area networks search problems;computer network management;performance analysis;root cause analysis;search problems;network management;switches;context modeling;algorithm design and analysis;local area networks;network management root cause analysis san performance i o path affine search approach storage area network configuration information search algorithm reference algorithm generic algorithm;san performance	We present a novel algorithm, called IPASS, for root cause analysis of performance problems in storage area networks (SANs). The algorithm uses configuration information available in a typical SAN to construct I/O paths, that connect between consumers and providers of the storage resources. When a performance problem is reported for a storage consumer in the SAN, IPASS uses the configuration information in an on-line manner to construct an I/O path for this consumer. As the path construction advances, IPASS performs an informed search for the root cause of the problem. The underlying rationale is that if the performance problem registered at the storage consumer is indeed related to the SAN itself, the root causes of the problem are more likely to be found on the relevant I/O paths within the SAN. We evaluate the performance of IPASS analytically and empirically, comparing it to known, informed and uninformed search algorithms. Our simulations suggest that IPASS scales 7 to 10 times better than the reference algorithms. Although our primary target domain is SAN, IPASS is a generic algorithm. Therefore, we believe that IPASS can be efficiently used as a building block for performance management solutions in other contexts as well.	air traffic control radar beacon system;design rationale;entity;generic programming;input/output;online and offline;prototype;scalability;search algorithm;simulation;software deployment;storage area network	David Breitgand;Ealan A. Henis;Edya Ladan-Mozes;Onn Shehory;Elena Yerushalmi	2005	2005 9th IFIP/IEEE International Symposium on Integrated Network Management, 2005. IM 2005.	10.1109/INM.2005.1440794	performance management;storage area network;telecommunications;computer science;theoretical computer science;operating system;data mining;distributed computing;computer security;computer network	OS	-8.333326282856707	68.49705464485106	5097
61b5b2f2114be6658d797d2aecde91437c9f3c34	the ieee softwae engineering stanldards process	ieee standards;professional practice;standards organizations;software engineering software standards standards development standards organizations ieee standards;software engineering;standards development;software standards	This review of ongoing standards development highlights the interaction between the IEEE standards-making process and the norms of professional practice in software engineering.		Fletcher J. Buckley	1984	IEEE Micro	10.1109/MM.1984.291220	computer science;dyspan;software engineering;software engineering professionalism;software requirements;software peer review	Visualization	-65.89007199316325	24.802955971740424	5098
f018807ae4fb709ee9fa76952f1a84c52d143ce8	protego: cloud-scale multitenant ipsec gateway		Virtual cloud network services let users have their own private networks in the public cloud. IPsec gateways are growing in importance accordingly as they provide VPN connections for customers to remotely access these private networks. Major cloud providers offer IPsec gateway functions to tenants using virtual machines (VMs) running a software IPsec gateway inside. However, dedicating individual IPsec gateway VMs to each tenant results in significant resource waste due to the strong isolation mechanism of VMs. In this paper, we design Protego, a distributed IPsec gateway service designed for multitenancy. By separating the control plane and the data plane of an IPsec gateway, Protego achieves high availability with active redundancy. Furthermore, Protego elastically scales in and out by seamlessly migrating IPsec tunnels between the data nodes without compromising their throughput. Our evaluation and simulation based on production data show that Protego together with a simple resource provisioning algorithm saves more than 80% of the resources compared with allocating independent VMs.	active redundancy;algorithm;cloud computing;control plane;forwarding plane;high availability;ipsec;multitenancy;provisioning;simulation;throughput;virtual machine;virtual private network	Jeongseok Son;Yongqiang Xiong;Kun Tan;Paul Wang;Ze Gan;Sue B. Moon	2017			computer network;parallel computing;cloud computing;ipsec;default gateway;multitenancy;computer science	OS	-15.411878583285478	81.9643251130487	5100
a3a909a7e94f5e29fc4cf1e298d18f189200f99e	impediments to regulatory compliance of requirements in contractual systems engineering projects: a case study	compliance of requirements;effort estimation;rail infrastructure system;legal requirements elicitation	Large-scale contractual systems engineering projects often need to comply with myriad government regulations and standards as part of contractual obligations. A key activity in the requirements engineering (RE) process for such a project is to demonstrate that all relevant requirements have been elicited from the regulatory documents and have been traced to the contract as well as to the target system components. That is, the requirements have met regulatory compliance. However, there are impediments to achieving this level of compliance due to such complexity factors as voluminous contract, large number of regulatory documents, and multiple domains of the system. Little empirical research has been conducted in the scientific community on identifying these impediments. Knowing these impediments is a driver for change in the solutions domain (i.e., creating improved or new methods, tools, processes, etc.) to deal with such impediments. Through a case study of an industrial RE project, we have identified a number of key impediments to achieving regulatory compliance in a large-scale, complex, systems engineering project. This project is an upgrade of a rail infrastructure system. The key contribution of the article is a number of hitherto uncovered impediments described in qualitative and quantitative terms. The article also describes an artefact model, depicting key artefacts and relationships involved in such a compliance project. This model was created from data gathered and observations made in this compliance project. In addition, the article describes emergent metrics on regulatory compliance of requirements that can possibly be used for estimating the effort needed to achieve regulatory compliance of system requirements.	device driver;emergence;internationalized domain name;requirement;requirements engineering;system requirements;systems engineering;visual artifact	Md. Rashed Iqbal Nekvi;Nazim H. Madhavji	2014	ACM Trans. Management Inf. Syst.	10.1145/2629432	systems engineering;engineering;operations management;management science	SE	-63.79522431381421	19.96842120444749	5109
5559ed2e3a7f559440d7308d1f9e661cb49d951f	towards a sdn-based architecture for analyzing network traffic in cloud computing infrastructures	network traffic engineering;network traffic analysis;software defined network;floodlight sdn controllers sdn based architecture cloud computing infrastructures network traffic monitoring tools ip address reusage virtual machines openstack cloud infrastructures;software defined network network traffic analysis network traffic engineering cloud computing;monitoring computer architecture servers cloud computing ip networks packet loss;cloud computing;virtual machines cloud computing software defined networking telecommunication traffic	Currently, network traffic monitoring tools do not fit well in the monitoring of cloud computing infrastructures. These tools are not integrated with the control plane of the cloud computing stack. This lack of integration causes a deficiency in the handling of the re-usage of IP addresses along virtual machines, a lack of adaption and reaction on highly frequent topology changes, and a lack of accuracy in the metrics gathered for the networking traffic flowing along the cloud infrastructure. The main contribution of this paper is to provide a novel SDN-based architecture to carry out the monitoring of network traffic in cloud infrastructures. The architecture in based on the integration between the SDN controller and the control plane of the cloud computing stack. The proposed architecture has been implemented using reference implementations such as OpenStack cloud infrastructures and FloodLight SDN controllers. A number of experiments have been executed in the prototype to validate the suitability and performance of the proposed architecture.	algorithm;angular defect;cloud computing;control plane;data center;experiment;high- and low-level;network packet;network traffic control;prototype;software-defined networking;traffic analysis;virtual machine	Enrique Chirivella-Perez;Juan Gutierrez-Aguado;Jose M. Alcaraz Calero;José M. Claver	2015	2015 23rd International Conference on Software, Telecommunications and Computer Networks (SoftCOM)	10.1109/SOFTCOM.2015.7314126	network traffic control;cloud computing;computer science;cloud testing;distributed computing;computer security;computer network	HPC	-14.945436383860642	82.3938972535053	5111
972c94a695b34d980954b5220167ddaa52e9d23f	collective abstractions and platforms for large-scale self-adaptive iot		"""On the way to the materialisation of the pervasive computing vision, the technological progress swelling from mobile computing and the Internet of Things (IoT) domain is already rich of missed opportunities. Firstly, coordinating large numbers of heterogeneous situated entities to achieve system-level goals in a resilient and self-adaptive way is complex and requires novel approaches to be seamlessly injected into mainstream distributed computing models. Secondly, achieving effective exploitation of computer resources is difficult, due to operational constraints resulting from current paradigms and uncomprehensive software infrastructures which hinder flexibility, adaptation, and smooth coordination of computational tasks execution. Indeed, building dynamic, context-oriented applications in small-or large-scale IoT with traditional abstractions is hard: even harder is to achieve opportunistic, QoS-and QoE-driven application task management across available hardware and networking infrastructure. In this insight paper, we analyse by the collective adaptation perspective the key directions of the impelling paradigm shift urged by forthcoming large-scale IoT scenarios. Specifically, we consider how collective abstractions and platforms can synergistically assist in such a transformation, by better capturing and enacting a notion of """"collective service"""" as well as the dynamic, opportunistic, and context-driven traits of space-time-situated computations."""		Roberto Casadei;Mirko Viroli	2018	2018 IEEE 3rd International Workshops on Foundations and Applications of Self* Systems (FAS*W)	10.1109/FAS-W.2018.00033	task management;situated;human–computer interaction;technological change;ubiquitous computing;software;macro;paradigm shift;mobile computing;computer science	HPC	-44.069316150416526	45.052136517385605	5119
41d9857c0e436dec8d9c9a05bd1e03f7c3e06aca	a note on subgroup security in pairing-based cryptography		Barreto~et al.\ (LATINCRYPT~2015) proposed a security notion, called subgroup security, for elliptic curves in pairing-based cryptography. They also claimed that, in some schemes, if an elliptic curve is subgroup-secure, the membership check, called full membership check, can be replaced by a cheaper membership check, called light membership check, which results in faster schemes than the original ones. However, they also noticed that some schemes will not maintain security if this replacement is done. It is unclear what schemes allow a secure replacement of the membership check. In this paper, we show a concrete example of insecurity in the sense of subgroup security in order to help developers understand what subgroup security is and what properties are actually preserved. In our conclusion, we recommend the developers to use the full membership check because it is a simple and general technique to securely implement schemes. If the developers use the light membership check for performance reasons, it is critical to carefully check that security is preserved.	elliptic curve cryptography;pairing-based cryptography	Tadanori Teruya	2018		10.1145/3197507.3197514	theoretical computer science;discrete mathematics;computer science;elliptic curve;cryptography;pairing;pairing-based cryptography	Security	-36.65960318286903	75.59370756740891	5124
f127865bb3e7d4f9d0ac1b94e4e49a9058ecd990	steps beyond international collaborations: our challenges in asian internet interconnection initiatives	available bandwidth;satellite communication;distance education;routing;ai3 project asian internet interconnection initiatives communication infrastructure wide project satellite communication systems r d network ku band satellite transponder udl routing asymmetric one way link bandwidth hub station distance education usability terrestrial links industrialization sustainable services;distance learning;international collaboration internet asia satellite communication educational technology acceleration system testing research and development transponders routing;acceleration;satellite links;research and development;telecommunication network routing internet distance learning satellite links research initiatives;internet;telecommunication network routing;network connectivity;system testing;research initiatives;internet application;educational technology;transponders;asia	The Internet has become a critical and dependable communication infrastructure in many countries. In Asia, there is still strong demand on the Internet development, because only the limited number of people can use the Internet without any difficulties. Asian Internet Interconnection Initiatives (AI3) has been established in 1995 by WIDE Project, aiming for developing technologies for accelerating the Internet development in Asia. Since 1995, the project has been constructing its own testbed network using satellite communication systems and this is only the international R&D network based on satellite communication system. Currently this network connects 17 member sites in 10 countries. Throughout our activity, technologies and engineering have been developed such as our infrastructure which uses Ku band satellite transponder for the Internet applications, UDL routing for asymmetric one way link to maximize available bandwidth from hub station to member sites, and series of experiments such as many challenges for distance education to confirm usability of our technology have been conducted. Now we are facing new issues raised such as expansion to use terrestrial links and industrialization of our developed technologies for sustainable services. In order to solve these issues, several new activities have been newly launched in our AI3 project. In this paper, we overviews our achievement in AI3 project, and discuss issues newly raised in our project.	autoit;communications satellite;dependability;experiment;interconnection;internet;ku band;link building;optical fiber cable;routing;software deployment;terrestrial television;testbed;transponder;usb hub;usability	Suguru Yamaguchi;Tomomitsu Baba;Jun Murai	2003		10.1109/SAINTW.2003.1210117	internet architecture board;distance education;educational technology;the internet;simulation;telecommunications;computer science;computer network	Networks	-19.124761739127305	91.23979821808854	5125
449776c50a4be25e1312bc4377a80b09edd3351f	spatiotemporal specification & verification of multimedia scenarios	document verification;spatiotemporal composition;multimedia scenario;spatiotemporal specification;multimedia scenarios	A Multimedia Application (MAP) consists of a set of media objects ordered in the spatial and temporal domains. In this paper we present an authoring & verification methodology for MAP documents development. We capitalize on a solid theoretical model for spatiotemporal compositions. The tool may be used both for prototyping and verification of multimedia presentations or spatiotemporal compositions in general. Regarding the authoring phase, emphasis was put on the flexible definition of spatial and temporal relationships of the participating entities. The verification procedures are supported by multiple tools allowing designers to preview their applications, in various ways: spatial layouts of the application window, temporal layouts, indicating the temporal duration and relationships among the participating objects and animation of the application.	entity;theory	Ioannis Kostalas;Timos K. Sellis;Michalis Vazirgiannis	1999			simulation;computer science;theoretical computer science;multimedia	HCI	-36.44433744613561	24.658643115930058	5126
660c3bc98552eafd8af9dc2492f7f5616bf3e88c	anais do wer06 - workshop em engenharia de requisitos, rio de janeiro, rj, brasil, julho 13-14, 2006	rio de janeiro		registered jack;winsock		2006				Logic	-103.53942066561576	19.664016323886695	5133
655f88756e6192597c722dccf7d52c764c8a14c4	industrie 4.0 - wünschenswertes, machbares und grenzen für technik, wirtschaft und mensch		Bim Wort genommen bedeutet eine Umsetzung der Ziele von Industrie 4.0, dass man durch den Einsatz von Informationstechnologie Prozesse maximal vernetzt und automatisiert. In die Vernetzung einbezogen werden dabei alle vertikal wie horizontal vorund nachgelagerten Prozesse: Produktionsprozesse, Logistikprozesse, Vertriebsprozesse, Unternehmensprozesse (neben Verwaltung auch Strategie, Planung und Steuerung). Alle Systeme und Prozesse sollen im Idealfall also Informationen austauschen, Prozesse planen und anstoßen sowie sich möglichst selbst steuern. Es wird somit ein ITIntegrationsgrad angestrebt, der nach jetzigem Stand der Erkenntnis nicht übertroffen werden kann.	eine and zwei;industry 4.0;internet explorer;maximal set	Detlev Buchholz	2016					-103.44453044371383	32.99111811142422	5136
46994ed97a3f65dd8d8c2e7b7e5c9383a3c4ffc3	using software defined networking to manage and control iec 61850-based systems	openflow;iec 61850;software defined networking;smart grid;monitoring;sflow	Display Omitted We describe the IEC 61850 communication model and provide an SDN-based framework.This framework mainly uses the OpenFlow, sFlow, and OVSDB protocols.It controls an IEC 61850 network, including resource analysis and management.It integrates traffic engineering techniques, such as QoS or traffic filtering. Smart Grid makes use of Information and Communications Technology (ICT) infrastructures for the management of the generation, transmission and consumption of electrical energy to increase the efficiency of remote control and automation systems. One of the most widely accepted standards for power system communication is IEC 61850, which defines services and protocols with different requirements that need to be fulfilled with traffic engineering techniques. In this paper, we discuss the implementation of a novel management framework to meet these requirements through control and monitoring tools that provide a global view of the network. With this purpose, we provide an overview of relevant Software Defined Networking (SDN) related approaches, and we describe an architecture based on OpenFlow that establishes different types of flows according to their needs and the network status. We present the implementation of the architecture and evaluate its capabilities using the Mininet network emulator.	software-defined networking	Elias Molina;Eduardo Jacob;Jon Matías;Naiara Moreira;Armando Astarloa	2015	Computers & Electrical Engineering	10.1016/j.compeleceng.2014.10.016	openflow;embedded system;real-time computing;computer science;engineering;software-defined networking;smart grid;computer network	SE	-20.78648327153856	81.29119325747436	5137
6bec206967c40928f76cbd42570e68049bc9188e	ein verfahren zur automatischen erstellung der dokumentation in automatisierungsprojekten				Jörg Gauger	1988				Vision	-100.28903855932276	24.49247932920497	5153
7b743215e4819b8e2100dde9ebf4cdde8b50347c	finding the source of type errors	type checking;polymorphism	It is a truism that most bugs are detected only at a great distance from their source. Although polymorphic type-checking systems like those in ML help greatly by detecting potential run-time type errors at compile-time, such systems are still not very helpful for locating the source of a type error. Typically, an error is reported only when the type-checker can proceed no further, even though the programmer's actual error may have occurred much earlier in the text. We describe an algorithm which appears to be quite helpful in isolating and explaining the source of type errors. The algorithm works by keeping track of the <i>reasons</i> the checker makes deductions about the types of variables.	algorithm;compile time;compiler;programmer;sensor;software bug;type safety;type system	Mitchell Wand	1986		10.1145/512644.512648	polymorphism;real-time computing;computer science;theoretical computer science;programming language;algorithm	PL	-57.75056315178862	38.92549638304408	5158
103771982b500e78e3ffe25aa3f675c464064925	kryptografie - verfahren, protokolle, infrastrukturen (3. aufl.)				Klaus Schmeh	2007				Vision	-98.5060566552041	22.203975055755667	5160
b58734935828ea9a2f9dfa852b800688a5bde5c8	parcoach: combining static and dynamic validation of mpi collective communications	debugging;collective;correctness;mpi;static analysis	Nowadays most scientific applications are parallelized based on MPI comm unications. Collective MPI communications have to be executed in the same order by all processes in their communicator and the same number of times, otherwise it is not conforming to the standard and a deadlock or other undefined behavior can occur. A s soon as the control-flow involving these collective operations becomes more complex, in particular including conditionals on process ran k , e suring the correction of such code is error-prone. We propose in this paper a static analysis to detect when such situation occur s, combined with a code transformation that prevents from deadlocking. We focus on blocking MPI collective operations in SPM D applications, assuming MPI calls are not nested in multithreaded regions. We show on several benchmarks the small impa ct on performance and the ease of integration of our techniques in the development process.		Emmanuelle Saillard;Patrick Carribault;Denis Barthou	2014	IJHPCA	10.1177/1094342014552204	correctness;parallel computing;computer science;message passing interface;theoretical computer science;operating system;distributed computing;programming language;debugging;static analysis	HPC	-19.54767580605496	39.88083612021006	5162
6596458461aaac524c698bcff71d688cdabcb74c	an agent-based approach for invoking service in environments with limited network bandwidth	protocols;simple object access protocol bandwidth protocols time factors xml payloads;limited bandwidth network;adaptive data compression agent based approach service invocation limited network bandwidth web service execution mode xml based format data fail risk agent based compression method software agent transmitted messages size redundant data;service agent;time factors;xml;bandwidth;payloads;axis2;xml data compression software agents web services;simple object access protocol;adaptive compression limited bandwidth network axis2 service agent;adaptive compression	In environments with limited network bandwidth, current web service execution mode will face challenges for its enormous demand for network bandwidth. The transmitted message which contains real payload and XML-based format data seems to be verbose. The response time and fail risk will be brought to increase as well. In this paper we address this issue and propose an agent-based compression method. A software agent is deployed between the requesters and invokers to execute services and return results to the requesters. The transmitted messages size is reduced by stripping off redundant data and adaptive data compression. We conduct a detailed evaluation of compression effectiveness and provide the results of execution time measurements. The effectiveness of the approach is proved by the experiment results and the conclusion is given.	agent-based model;algorithm;data compression;payload (computing);response time (technology);risk assessment;run time (program lifecycle phase);software agent;web service;xml	Xiaotao Li;Xiaohui Hu	2015	2015 IEEE International Conference on Web Services	10.1109/ICWS.2015.109	communications protocol;payload;xml;computer science;operating system;soap;database;distributed computing;world wide web;bandwidth	Visualization	-19.504310525947187	70.14889860458727	5165
710af58e64be5626483c43338a20f4feef184ed6	industry application of software development task measurement system: taskpit		To identify problems in a software development process, we have been developing an automated measurement tool called TaskPit, which monitors software development tasks such as programming, testing and documentation based on the execution history of software applications. This paper introduces the system requirements, design and implementation of TaskPit; then, presents two real-world case studies applying TaskPit to actual software development. In the first case study, we applied TaskPit to 12 software developers in a certain software development division. As a result, several concerns (to be improved) have been revealed such as (a) a project leader spent too much time on development tasks while he was supposed to be a manager rather than a developer, (b) several developers rarely used e-mails despite the company’s instruction to use e-mail as much as possible to leave communication records during development, and (c) several developers wrote too long e-mails to their customers. In the second case study, we have recorded the planned, actual, and self reported time of development tasks. As a result, we found that (d) there were unplanned tasks in more than half of days, and (e) the declared time became closer day by day to the actual time measured by TaskPit. These findings suggest that TaskPit is useful not only for a project manager who is responsible for process monitoring and improvement but also for a developer who wants to improve by him/herself. key words: processs measurement, system development, case study	documentation;email;emoticon;history of software;requirement;software developer;software development process;system requirements	Pawin Suthipornopas;Pattara Leelaprute;Akito Monden;Hidetake Uwano;Yasutaka Kamei;Naoyasu Ubayashi;Kenji Araki;Kingo Yamada;Ken-ichi Matsumoto	2017	IEICE Transactions		computer science	SE	-69.32873622647485	25.34146468308295	5175
f6a1fca397209650f88c0744001540a98a7f554b	algebraic laws for feature models	feature modeling;theorem prover;software product line	Software Product Lines (SPL) may be adopted by either bootstrapping existing software products into a SPL, or extending an existing SPL to encompass an additional software product. Program refactorings are usually applied for carrying out those tasks. The notion of SPL refactoring is an extension of the traditional definition of refactoring; it involves not only program refactorings, but also Feature Model (FM) refactorings, in order to improve configurability. However, FM refactorings are hard to define, due to the incompleteness of the refactoring catalogs developed as of today. In this paper, we propose a complete, sound catalog of algebraic laws, making up special FM refactorings that preserve configurability. This catalog is also defined as minimal, as one law cannot be derived from another one in the same catalog. In addition, a theory for FMs is presented, in the context of a theorem prover.	automated theorem proving;bootstrapping (compilers);code refactoring;fm broadcasting;feature model;graphical user interface;linear algebra;pandora fms;propositional proof system;reduction strategy (lambda calculus);semantics (computer science);software product line	Rohit Gheyi;Tiago Massoni;Paulo Borba	2008	J. UCS	10.3217/jucs-014-21-3573	computer science;theoretical computer science;software engineering;automated theorem proving;programming language;algorithm	SE	-45.37310848700819	28.0981990105586	5178
b0d04e542c46ffde42c466bbf5002b82cf4f4e64	consistency of uml class and statechart diagrams with state invariants		We present an approach and a tool to analyze the consistency of UML class and statechart diagrams containing state invariants automatically. UML class diagrams describe the structure of a system as a collection of classes while UML statechart diagrams describe its behavior. State invariants relate the active state configuration of a statechart with object instances described in a class diagram. We consider a UML statechart inconsistent if it contains unsatisfiable state invariants, that is, there are no object instances that can make a given invariant evaluate to true. To detect such inconsistencies, we translate a UML model containing class and statechart diagrams into the Web Ontology Language (OWL 2), and then use OWL 2 reasoning tools to infer the consistency and satisfiability of the translated diagrams. The approach is supported by an automatic translation tool and existing OWL 2 reasoners. We demonstrate our approach with an example design and evaluate its performance using large UML models.	class diagram;instance (computer science);machine translation;state diagram;uml state machine;web ontology language	Ali Hanzala Khan;Irum Rauf;Ivan Porres	2013		10.5220/0004320100140024	state diagram;reliability engineering;uml state machine	SE	-45.59442820035591	27.302400239561507	5181
294ecdaaa9f7e56c22599cb708598a45afb9b13e	traps, events, emulation, and enforcement: managing the yin and yang of virtualization-based security	debugging;virtual machine;virtualization;traps;iron;program analysis;security policy	We question current trends that attempt to leverage virtualization techniques to achieve security goals. We suggest that the security role of a virtual machine centers on being a policy interpreter rather than a resource provider. These two roles (security reference monitor and resource emulator) are currently conflated within the context of virtual machines and VMMs. We believe that this ``double-duty'' leads to both a significant performance impact as well as a bloated virtualization layer. Increased complexity reduces confidence that the code is elementary enough to verify or trust from a security perspective. Ironically, as more security-related functionality is shoved into a VM platform, the system becomes less trustworthy as it becomes increasingly trusted.  We argue that a principle reason for such an unfortunate situation is the lack of efficient hardware trapping mechanisms. We propose an architecture to help ameliorate this problem by transferring the security enforcement and program analysis roles from the virtualization component to a policy-directed FPGA.	application virtualization;cpu cache;central processing unit;debugging;emulator;field-programmable gate array;program analysis;reference monitor;simple network management protocol;trap (computing);virtual machine;yang	Sergey Bratus;Michael E. Locasto;Ashwin Ramaswamy;Sean W. Smith	2008		10.1145/1456482.1456491	program analysis;computer security model;cloud computing security;full virtualization;real-time computing;virtualization;computer science;security policy;virtual machine;operating system;security testing;debugging;iron;computer security	Security	-53.35940160927162	56.69673620869006	5195
75629b6329988ff06610699eb62db035ad09dc39	support facilities for development of parallel computational algorithms	parallel algorithm;parallel programming language;parallel computer;parallel programs	An approach to the development of parallel programs is described, which is based on algorithmic specification. Functional characteristics and structure of the system that supports the approach and is based on the parallel programming language SuperPascal are suggested. The system is designed for the development of parallel algorithms and for teaching parallel programming.	computation;constraint (mathematics);numerical analysis;numerical method;parallel algorithm;parallel computing;parallel programming model;programming language;transaction processing system;windows 95;workbench	S. I. Katkov;Igor V. Pottosin	2001	Programming and Computer Software	10.1023/A:1011098710626	computer architecture;embarrassingly parallel;computer science;theoretical computer science;massively parallel;parallel algorithm;algorithmic skeleton;programming language;parallel extensions;bulk synchronous parallel;cost efficiency;parallel programming model	HPC	-9.739672844330611	38.213655280199376	5198
f894ecb7cc3e414b90dbf456660ddc24f52f1dba	quantifying notes		We review several logics with propositional quantification.		Hans van Ditmarsch	2012		10.1007/978-3-642-32621-9_8		Logic	-12.122415250421295	11.082143658188716	5204
0c76646b49a7862af66cbd08d24def1ed2ef9de7	fine-grained visualization pipelines and lazy functional languages	functional abstraction;computer languages;surface extraction algorithms;pipeline model;lazy functional languages;service provider;programming language;laziness;computer graphic equipment;data processing;conceptual model;relational database;data mining;functional programming;fine grained visualization pipelines;expressive power;assembly;data visualisation;pipeline processing computer graphic equipment data visualisation functional languages functional programming;pipeline like function composition;indexation;functional languages;data visualization;us department of transportation;haskell fine grained visualization pipelines lazy functional languages programming language demand driven processing streaming form pipeline like function composition surface extraction algorithms functional abstraction;process model;streaming form;power system modeling;functional language;visual system;functional programming pipeline model laziness;buildings;demand driven processing;pipeline processing;haskell;data visualization pipeline processing us department of transportation buildings data processing computer languages power system modeling assembly data mining functional programming	The pipeline model in visualization has evolved from a conceptual model of data processing into a widely used architecture for implementing visualization systems. In the process, a number of capabilities have been introduced, including streaming of data in chunks, distributed pipelines, and demand-driven processing. Visualization systems have invariably built on stateful programming technologies, and these capabilities have had to be implemented explicitly within the lower layers of a complex hierarchy of services. The good news for developers is that applications built on top of this hierarchy can access these capabilities without concern for how they are implemented. The bad news is that by freezing capabilities into low-level services expressive power and flexibility is lost. In this paper we express visualization systems in a programming language that more naturally supports this kind of processing model. Lazy functional languages support fine-grained demand-driven processing, a natural form of streaming, and pipeline-like function composition for assembling applications. The technology thus appears well suited to visualization applications. Using surface extraction algorithms as illustrative examples, and the lazy functional language Haskell, we argue the benefits of clear and concise expression combined with fine-grained, demand-driven computation. Just as visualization provides insight into data, functional abstraction provides new insight into visualization	apl;algorithm;computation;freezing;functional programming;graphics pipeline;haskell;high- and low-level;imagery;lambda calculus;lazy evaluation;pipeline (computing);programming language;state (computer science);streaming media;anatomical layer;benefit	David J. Duke;Malcolm Wallace;Rita Borgo;Colin Runciman	2006	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2006.145	service provider;visual system;relational database;computer science;conceptual model;theoretical computer science;process modeling;data mining;database;assembly;programming language;functional programming;expressive power	Visualization	-29.30730152735517	20.710330173138956	5213
04b080b958c9997268592d9182ed71e8f1f88949	scope-aware data cache analysis for wcet estimation	analytical models;software metrics;wcet estimation;scope aware data cache analysis;cache storage;program diagnostics;memory block;memory access time;persistence analysis method;coarse grained memory access information;loop hierarchy structure;cache memory;abstract interpretation based methods;processor speed;software metrics cache storage data analysis program diagnostics;arrays;address analysis;data analysis;worst case execution time;data cache;estimation;arrays concrete estimation analytical models mathematical model equations safety;safety;mathematical model;worst case execution time data cache behavior prediction program analysis abstract interpretation cache memories real time applications;real time applications;program analysis;data cache behavior prediction;abstract cache state modeling scope aware data cache analysis wcet estimation processor speed memory access time access pattern analysis abstract interpretation based methods coarse grained memory access information address analysis persistence analysis method memory block loop hierarchy structure;abstract interpretation;real time application;abstract cache state modeling;access pattern analysis;concrete;cache memories	Caches are widely used in modern computer systems to bridge the increasing gap between processor speed and memory access time. On the other hand, presence of caches, especially data caches, complicates the static worst case execution time (WCET) analysis. Access pattern analysis (e.g., cache miss equations) are applicable to only a specific class of programs, where all array accesses must have predictable access patterns. Abstract interpretation-based methods (must/persistence analysis) determines possible cache conflicts based on coarse-grained memory access information from address analysis, which usually leads to significantly pessimistic estimation. In this paper, we first present a refined persistence analysis method which fixes the potential underestimation problem in the original persistence analysis. Based on our new persistence analysis, we propose a framework to combine access pattern analysis and abstract interpretation for accurate data cache analysis. We capture the dynamic behavior of a memory access by computing its temporal scope (the loop iterations where a given memory block is accessed for a given data reference) during address analysis. Temporal scopes as well as loop hierarchy structure (the static scopes) are integrated and utilized to achieve a more precise abstract cache state modeling. Experimental results shows that our proposed analysis obtains up to 74% reduction in the WCET estimates compared to existing data cache analysis.	abstract interpretation;access time;best, worst and average case;cas latency;cpu cache;clock rate;iteration;pattern recognition;persistence (computer science);run time (program lifecycle phase);worst-case execution time	Bach Khoa Huynh;Lei Ju;Abhik Roychoudhury	2011	2011 17th IEEE Real-Time and Embedded Technology and Applications Symposium	10.1109/RTAS.2011.27	program analysis;bus sniffing;uniform memory access;embedded system;estimation;cache-oblivious algorithm;parallel computing;real-time computing;cache coloring;concrete;cpu cache;cache;computer science;cache invalidation;operating system;clock rate;mathematical model;database;data analysis;programming language;cache algorithms;cache pollution;software metric;worst-case execution time	Embedded	-7.652776666138149	57.03339299458332	5226
4bf3f79333ebdc0484e0535d7878cfe5e6f0abbb	dynamic plugging of business processes in cross organizational workflow	ontology.;coordination;cross-organizational workflow;agent;business process	This paper shows how it is possible to use agents and ontologies to deal with dynamic coordination of business processes in Cross-Organizational Workflow (COW). The aim of COW is to support the cooperation between distributed and heterogeneous business processes running in different autonomous organizations to reach a common goal, corresponding to a value-added service. In this paper, we consider a particular case of COW called dynamic COW in which the different partners (organizations) involved in the COW are not necessarily known before its execution either because they are unknown at design-time or no more available at run-time. Consequently, coordination in dynamic COW raises two specific problems, which are: (i) partners finding able to realize an COW service (i.e. a service implementing a business process involved in an COW), and (ii) negotiation of COW services between partners. This paper first defines an agent-based architecture to support COW service execution. Then, it presents two specific agent-based mediation infrastructures introduced to deal with partners finding and negotiation between them and shows, for each of these infrastructures, how they use ontologies for automatically discovering COW services and choosing protocols to negotiate COW services.	agent-based model;autonomous robot;business process;ontology (information science)	Lotfi Bouzguenda;Rafik Bouaziz;Eric Andonoff	2008	IJCSA			AI	-51.96020922446851	14.120792867381684	5228
559914be3a3f8f9294172a9f4d5009acde1084d4	comparing the strength of diagonally nonrecursive functions in the absence of ∑20 induction	wkl;nonstandard models of arithmetic;mathematics and statistics;graph coloring;pairs;sigma 2 induction;ramseys theorem;diagonally nonrecursive functions;trees;dnr;reverse mathematics;jump operator;sets;injury priority arguments	We prove that the statement “there is a k such that for every f there is a k-bounded diagonally non-recursive function relative to f” does not imply weak König’s lemma over RCA0+BΣ 0 2. This answers a question posed by Simpson. A recursion-theoretic consequence is that the classic fact that every k-bounded diagonally non-recursive function computes a 2-bounded diagonally nonrecursive function may fail in the absence of IΣ2.	könig's lemma;primitive recursive function;recursion (computer science);simpson's rule;theory	François G. Dorais;Jeffry L. Hirst;Paul Shafer	2015	J. Symb. Log.	10.1017/jsl.2015.43	arithmetic;discrete mathematics;diagonally dominant matrix;reverse mathematics;graph coloring;mathematics;algorithm;algebra	Theory	-7.2320911718295005	14.994725677460021	5231
8c8468841bd8c2aff45d3c98e46a05f19bb20e11	parallel program performance evaluation and their behavior analysis on an openmp cluster	openmp cluster;computers;openmp api;software dsm cluster parallel programming performance evaluation openmp cluster openmp api shared memory multiprocessors behavior analysis openmp programming model;electronic mail;performance evaluation;application software;software performance evaluation workstation clusters distributed shared memory systems parallel programming performance evaluation application program interfaces;openmp programming model;software performance evaluation;software systems;parallel programming;software dsm cluster;programming model;memory architecture;shared memory multiprocessors;distributed shared memory systems;application program interfaces;workstations;performance analysis;access protocols;workstation clusters;parallel programs;behavior analysis;program processors;performance analysis application software workstations memory architecture program processors software systems computers parallel programming access protocols electronic mail;shared memory multiprocessor	The OpenMP API is an emerging standard for parallel programming on shared memory multiprocessors. In order to run an OpenMP program on a cluster, one feasible scheme is to translate the OpenMP program into a software DSM program, then execute it on the cluster. Evaluating the performance of OpenMP programs and analyzing their behavior will help support the OpenMP programming model on a software DSM cluster efficiently. In this paper, we use an experimental approach to investigate how the characteristics of the software DSM cluster and the translation together with the original program behavior determine the performance of OpenMP programs on software DSM clusters.	application programming interface;openmp;parallel computing;performance evaluation;programming model;shared memory	Fei Cai;Shaogang Wu;Longbing Zhang;Zhiming Tang	2004	IEEE International Symposium on Cluster Computing and the Grid, 2004. CCGrid 2004.	10.1109/CCGrid.2004.1336641	computer architecture;application software;parallel computing;workstation;computer science;operating system;programming paradigm;software system	HPC	-11.655748995975644	44.31093246319561	5233
9098278aed1458a869aee2e1b3bd51c7e089415d	analysis of code familiarity in module and functionality perspectives		Maintenance is one of the most important phases in the software life cycle. Usually, during this phase the assignment of tasks to developers is made based on the familiarity degree that they have with the source code related to each task. However, it is not simple nor immediate to infer the relationship between the developers and the source code, especially when it is considered the level of functionality, which may have code archives dispersed in many locations of a project. This work presents an approach to infer the familiarity between developer and source code considering the functionality perspective, a view more appropriate in a real context of software development. The approach was applied during the evaluation of real softwares. Through an analysis of the familiarity between the module and functionality perspectives, it was possible to perceive the gain of information that can be obtained to understand better how it is the familiarity distribution among members of a team.	archive;execution;inference;library (computing);reverse engineering;software development;software maintenance;software release life cycle;software repository;source code;stage level 1;stage level 2;xojo	Brian A Cary;Clare Cummins;Randhawa Muhammad Afzal;Anita Sharma	2018		10.1145/3275245.3275250	software engineering;software development process;reverse engineering;source code;software development;software maintenance;computer science	SE	-58.19449742765913	33.265335213360345	5234
17e2f16bd869c7c3b18dd8d0f013f3e45d2dc1dd	the projected tar and its application to conformance checking		Relational semantics of business process models have seen an uptake in various fields of application. As a prominent example, the Transition Adjacency Relation (TAR) has been used, for instance, to conduct conformance checking and similarity assessment. TAR is defined over the complete set of transitions of a Petri net and induces order dependencies between pairs of them. In this paper, we consider TAR in a more general setting, in which the order dependencies shall be derived only for a subset of projected transitions. We show how to derive this projected variant of TAR from the reachability graph of a reduced Petri net. We elaborate the projected TAR for conformance checking in a case from industry.	business process;concurrency (computer science);conformance testing;kripke semantics;operating system service management;petri net;reachability;state space	Johannes Prescher;Jan Mendling;Matthias Weidlich	2012			conformance checking;systems engineering;programming language;kripke semantics;reachability;petri net;theoretical computer science;computer science;adjacency list;business process modeling;tar;graph	Logic	-43.58566664404082	28.098060089579775	5236
4df95bc6207526eba0be9aec0d0fe26eccafb58c	intelligent agents as one framework for defining fusion requirements for complex adaptive systems	international security community;human factors community;control systems;communication system;electronic mail;heart;design and development;information understanding intelligent agents fusion requirements complex adaptive systems international defense community international security community network centric warfare network enabled capability networked communications systems military platforms service oriented architectures broad based enterprise service middleware capability human factors community human engineering community sensemaking shared awareness information sharing;information understanding;networked communications systems;collaboration;complex adaptive systems;complex adaptive system;service oriented architectures;defence industry;secure communication;military communication;shared awareness;information sharing;international defense community;human engineering community;software architecture;intelligent agents;human factors;network centric warfare;fusion requirements;adaptive systems;intelligent agent;sensemaking;middleware;national electric code;software architecture adaptive systems defence industry human factors middleware military communication military computing security;humans;broad based enterprise service middleware capability;military platforms;service oriented architecture;content addressable storage;security;intelligent agent adaptive systems humans control systems national electric code collaboration command and control systems heart content addressable storage electronic mail;command and control systems;military computing;network enabled capability	"""The international defense and security community is moving ahead at flank speed to realize the vision of network centric warfare (NCW; aka network-enabled capability (NEC) and etc). Extensive efforts are being put forth to define, design, and develop many of the technical components of such a capability, to include varieties of networked communications systems, various highly-capable military platforms, and """"service-oriented architectures"""" for a broad-based enterprise service middleware capability. To varying degrees, the human factors/human engineering community has also engaged in broad studies of """"sensemaking"""" and """"shared awareness"""" toward understanding some of the information-sharing and information- understanding paradigms put forward in the NCW literature."""	complex adaptive system;enterprise integration;human factors and ergonomics;intelligent agent;middleware;network-centric warfare;network-enabled capability;requirement;sensemaking;service-oriented architecture;service-oriented device architecture	James Llinas	2007	2007 10th International Conference on Information Fusion	10.1109/ICIF.2007.4408217	simulation;engineering;knowledge management;computer security	DB	-40.417467546794136	18.599665880734722	5237
2a01b019a1a36ccf4a4165eb355182a5576adfc9	understanding provenance black boxes	lineage;provenance;workflow management system;workflow management systems;coarse grained;usability	Current provenance stores associated with workflow management systems (WfMSs) capture enough coarse-grained information to describe which datasets were used and which processes were run. While this information is enough to rebuild a workflow run, it is not enough to facilitate user understanding. Because the data is manipulated via a series of black boxes, it is often impossible for a human to understand what happened to the data. In this work, we highlight the missing information that can assist user understanding. Unfortunately, provenance information is already very complex and difficult for a user to comprehend, which can be exacerbated by adding the extra information needed for deeper blackbox understanding. In order to alleviate this, we develop a model of provenance answers that follow a “roll up”, “drill down” strategy. We evaluate these techniques to determine if users have better understanding of provenance information. We show how this information can be captured by workflow management systems, and that the structures and information needed for this model are a negligible addition to standard provenance stores. Finally, we implement these techniques in a real provenance system, and evaluate implementation feasibility.	black box	Adriane Chapman;H. V. Jagadish	2009	Distributed and Parallel Databases	10.1007/s10619-009-7058-3	computer science;data mining;database;world wide web;workflow management system;workflow technology	DB	-60.739403336958716	53.18826638099714	5242
7731644e0591704455d10fd3d9f9bd342e1efecd	abstraction refinement for trace inclusion of data automata		ion Refinement for Trace Inclusion of Data Automata Radu Iosif, Adam Rogalewicz, and Tomáš Vojnar CNRS/Verimag, France and FIT BUT, Czech Republic radu.iosif@imag.fr, trogalew,vojnaru@fit.vutbr.cz Abstract. A data automaton is a finite automaton equipped with variables (counters) ranging over a multi-sorted data domain. The transitions of the automaton are controlled by first-order formulae, encoding guards and updates. We observe, in addition to the finite alphabet of actions, the values taken by the counters along a run of the automaton, and consider the data languages recognized by these automata. The problem addressed in this paper is the inclusion between the data languages recognized by such automata. Since the problem is undecidable, we give an abstraction-refinement semi-algorithm, proved to be sound and complete, but whose termination is not guaranteed. The novel feature of our technique is checking for inclusion, without attempting to complement one of the automata, i.e. working in the spirit of antichain-based non-deterministic inclusion checking for finite automata [1]. The method described here has various applications, ranging from logics of unbounded data structures, such as arrays or heaps, to the verification of real-time systems. A data automaton is a finite automaton equipped with variables (counters) ranging over a multi-sorted data domain. The transitions of the automaton are controlled by first-order formulae, encoding guards and updates. We observe, in addition to the finite alphabet of actions, the values taken by the counters along a run of the automaton, and consider the data languages recognized by these automata. The problem addressed in this paper is the inclusion between the data languages recognized by such automata. Since the problem is undecidable, we give an abstraction-refinement semi-algorithm, proved to be sound and complete, but whose termination is not guaranteed. The novel feature of our technique is checking for inclusion, without attempting to complement one of the automata, i.e. working in the spirit of antichain-based non-deterministic inclusion checking for finite automata [1]. The method described here has various applications, ranging from logics of unbounded data structures, such as arrays or heaps, to the verification of real-time systems.	algorithm;array data structure;automaton;complement (complexity);data domain;finite-state machine;first-order predicate;heap (data structure);re (complexity);real-time clock;real-time computing;refinement (computing);semiconductor industry;undecidable problem	Radu Iosif;Adam Rogalewicz;Tomás Vojnar	2014	CoRR		theoretical computer science;programming language;algorithm	Logic	-12.505317048658728	24.680167927014327	5243
0df7330a018d50115eb5cdb9b084829c483f8846	deploying digital media libraries in multi-service access networks	maximum delay constraint;access network;multiservice access network;digital media libraries;storage server placement problem;digital libraries;software libraries network servers bandwidth costs file servers delay switches multimedia systems videos strain measurement;bandwidth constraint digital media libraries multiservice access network data retention storage server placement problem sspa storage server placement algorithm unlimited bandwidth maximum delay constraint;unlimited bandwidth;storage area networks;data retention;network servers;bandwidth constraint;technology and engineering;digital media;storage server placement algorithm;telecommunication services digital libraries multimedia communication network servers storage area networks subscriber loops;sspa;subscriber loops;multimedia communication;telecommunication services;lower bound	A current important trend is the introduction of new services in the access and aggregation network, close to the end user. A major opportunity for such service enabled access networks is providing users with fast and reliable storage, allowing them to transparently access and share their digital media library anytime, anywhere, while guaranteeing data retention. An important issue in deploying such a service is where to put the storage servers, in order to minimize deployment cost without sacrificing performance. This paper presents and evaluates two algorithms for solving the storage server placement problem, minimizing the deployment cost while guaranteeing a low delay for accessing the digital media libraries from any access node. The base algorithm, referred to as SSPA (storage server placement algorithm), assumes the access and aggregation network has unlimited bandwidth. SSPA provides a lower bound on the required number of servers, respecting a maximum delay constraint. The extended algorithm, SSPA, solves the storage server placement problem, respecting both a maximum delay constraint and bandwidth constraints of the network links. It will be shown that the algorithms produce close to optimal results relatively fast	access network;anytime algorithm;cable internet access;data striping;digital media;digital subscriber line;file server;library (computing);network topology;server (computing);simulation;software deployment	Koert Vlaeminck;Filip De Turck;Bart Dhoedt;Piet Demeester	2006	Eighth IEEE International Symposium on Multimedia (ISM'06)	10.1109/ISM.2006.58	real-time computing;storage area network;digital library;telecommunications;computer science;telecommunications service;digital media;operating system;database;distributed computing;upper and lower bounds;world wide web;computer network;access network	Arch	-16.54439672600303	96.83826125618371	5245
07aef8af38ac0ed0f60238d205baf5ed6be5cc8b	hardware assisted control flow obfuscation for embedded processors	dynamic program;control flow graph;embedded system;memory access;control flow;on the fly;access control;obfuscation;theoretical foundation;software protection;embedded processor;reverse engineering	With more applications being deployed on embedded platforms, software protection becomes increasingly important. This problem is crucial on embedded systems like financial transaction terminals, pay-TV access-control decoders, where adversaries may easily gain full physical accesses to the systems and critical algorithms must be protected from being cracked. However, as this paper points out that protecting software with either encryption or obfuscation cannot completely preclude the control flow information from being leaked. Encryption has been widely studied and employed as a traditional approach for software protection, however, the control flow information is not 100% hidden with solely encrypting the code. On the other hand, pure software-based obfuscation has been proved inefficient to protect software due to its lack of theoretical foundation and considerable performance overhead introduced by complicated transformations. Moreover, even though obfuscation can prevent static reverse engineering, attacker can still successfully bypass the obfuscation by monitoring the dynamic program execution.To address all of these shortcomings, this paper presents a hardware assisted obfuscation technique that is capable of obfuscating the control flow information dynamically. Dynamic obfuscation changes memory access sequence on-the-fly and conceals recurrent instruction access sequences from being identified. Our scheme makes it provably difficult for the attacker to extract any useful information. Our results show that a high-level security protection is possible with only minor performance penalty. Finally, we show that our scheme can be implemented on embedded systems with very little hardware overhead.	algorithm;batch file;central processing unit;control flow;copy protection;elegant degradation;embedded system;encryption;experiment;hardware obfuscation;high- and low-level;high-level programming language;information leakage;obfuscation (software);overhead (computing);randomness;reverse engineering;spectral leakage	Xiaotong Zhuang;Tao Zhang;Hsien-Hsin S. Lee;Santosh Pande	2004		10.1145/1023833.1023873	embedded system;parallel computing;real-time computing;obfuscation;computer science;access control;operating system;distributed computing;programming language;control flow;computer security;reverse engineering;control flow graph	Security	-55.77837306038669	56.31113178425831	5249
e483ce2901e0c79ec0286831e0d8b654b3ea7ecf	real-time gang schedulings with workload models for parallel computers	real time constraints;processor scheduling concurrent computing job shop scheduling application software system software real time systems large scale systems parallel languages optimizing compilers software libraries;concurrent computing;software libraries;system software;application software;workload models;job shop scheduling;processor scheduling;gang scheduling;real time;simulation;time sharing;metric;parallel machines real time systems scheduling;large scale;scheduling algorithm;parallel systems;scheduling;real time scheduling;parallel computer;throughput real time gang scheduling workload models parallel computers job scheduling policy space sharing time sharing real time constraints metric task utilization workload simulation;parallel machines;parallel computers;space sharing;task scheduling;task utilization workload;optimizing compilers;parallel languages;non real time;real time gang scheduling;large scale systems;throughput;job scheduling policy;real time systems	Gang scheduling has recently been shown to be an effective task scheduling policy for parallel computers because it combines elements of space sharing and time sharing [10, 20]. In this paper, we propose new policies to enable gang scheduling to adapt to environments with real-time constraints. Our work, to our best knowledge, is the first attempt to address these real-time aspects with gang scheduling. Our system, guided by a metric called the task utilization workload, can schedule both real-time and non-real-time tasks at the same time. In this paper, we report simulation results obtained using a family of scheduling algorithms based on our proposed metric. Our scheme is designed for practical use with large scale industrial and commercial parallel systems. Preliminary simulation results also show that our proposed policy is effective for real-time scheduling and can schedule non-real-time tasks with fairness and good throughput.	algorithm;computer;fairness measure;gang scheduling;parallel computing;real-time clock;real-time computing;real-time operating system;real-time transcription;real-time web;regular expression;scheduling (computing);simulation;throughput;time-sharing	Jenq Kuen Lee;Chung-Der Lin;Yar-Wen Chang;Wei-Kuan Shih	1998		10.1109/ICPADS.1998.741027	fair-share scheduling;fixed-priority pre-emptive scheduling;job shop scheduling;parallel computing;real-time computing;earliest deadline first scheduling;gang scheduling;flow shop scheduling;concurrent computing;dynamic priority scheduling;computer science;rate-monotonic scheduling;operating system;two-level scheduling;deadline-monotonic scheduling;distributed computing;scheduling;lottery scheduling;round-robin scheduling;scheduling	HPC	-12.753739604652537	60.60254752585343	5253
011be91983b20887c8cecc768e0d96959689550d	nested coroutines for exception handling in modula-2	exception handling		coroutine;exception handling;modula-2	Gérard Padiou	1990	Structured Programming		computer science;programming language;modula-2;exception handling;coroutine	HCI	-23.32075803204263	32.59350536144699	5254
4b756154998bab5b7db2f9527cfa58724e77c406	security analysis of an efficient identity-based proxy signature in the standard model				Xiaoming Hu;Yinchun Yang;Jian Wang;Huajie Xu;WenAn Tan	2015	IEICE Transactions		standard model;cryptography;security analysis;statistics	Security	-41.652725083076994	77.34636748672058	5255
0ed354d5a12e149993de2a94a41471d505632571	quantum entanglement, non-locality and secure computation	secure computation algorithms;secure computation;quantum key distribution;cryptographic protocols;quantum computing quantum entanglement nanoscale devices protocols application software computer science security mathematics magnetic materials nanotechnology;nanoscale phenomenon;quantum entanglement cryptographic protocols quantum cryptography;nonlocal machines;nonlocal machines quantum entanglement secure computation algorithms nanoscale phenomenon protocol quantum key distribution;quantum cryptography;quantum entanglement;protocol	In this paper, we demonstrate that nanoscale phenomenon can be applied not only in device level but also in high layer applications, such as secure computation. We study the possibility of performing secure computation by building non-local machines based on quantum entanglement and non-locality, which are phenomena available only at the nanometer scale. Comparing with classical secure computation algorithms, the security of this protocol is based on physical laws, instead of any unproven mathematic conjecture.	action at a distance;algorithm;cryptographic primitive;cryptography;information sensitivity;locality of reference;quantum entanglement;secure multi-party computation	Yao-Hsin Chou;I-Ming Tsai;Chin-Shyurng Fahn;Shi-Jinn Horng;Sy-Yen Kuo	2007	2007 First International Conference on Quantum, Nano, and Micro Technologies (ICQNM'07)	10.1109/ICQNM.2007.17	quantum simulator;quantum nanoscience;quantum information;theoretical computer science;quantum network;quantum capacity;quantum technology;open quantum system;distributed computing;quantum sensor;quantum channel;quantum computer;quantum imaging;physics;quantum cryptography;quantum mechanics;quantum error correction	Crypto	-42.94660636679454	83.0229545342582	5279
1859725ebdf2cb7c1aac013064faab6a20fa0a10	die realisierung von architekturprinzipien für methodenbank-systeme im modellbanksystem mbs	realisierung von architekturprinzipien			Willi Klösgen;Wolfgang H Schwarz	1979		10.1007/978-3-642-67444-0_25		Theory	-100.08162669258569	28.05601445327469	5280
0ff819f4385d88859a304cbc981d8ffa9eb9e7ec	a three-layer meta-design model for addressing domain-specific customizations		Meta-design has been proposed as a model to design systems able to support End-User Development (EUD). Meta-design means “design for designers.” Differently than in traditional design, professional developers do not directly create a final application, but they build software environments thorough which non-technical end users, acting as co-designers, are enabled to shape up the application while they are using it. Allowing end users to participate to the creation of their applications, by modifying or even creating from scratch software artifacts, is very challenging. To make this possible, end users have to be provided with software environments customized to their specific domain, which they can easily understand and use. In order to cope with domain specificity, this chapter presents a new meta-design model that specifically addresses the customization to a domain of interest. Customization, performed by domain experts possibly in collaboration with professional developers, becomes the key activity to provide non-technical end users with software environments that are adequate to their knowledge and needs, thus allowing them to actually become co-designers of their applications. The model is illustrated by describing its successful application to the design of a mashup platform that allows end users to create new applications by integrating data and functionality taken from different resources. The customization of the platform to different domains, such as Cultural Heritage and Technology Enhanced Learning, is discussed.	multitier architecture	Carmelo Ardito;Maria Francesca Costabile;Giuseppe Desolda;Maristella Matera	2017		10.1007/978-3-319-60291-2_5	end-user development;mashup;end user;systems engineering;personalization;software;scratch;domain specificity;cultural heritage;computer science	NLP	-50.06363837218963	21.386610943093007	5283
be05a6a88a0d552939eba4612fb81713515907bd	on load balancing approaches for distributed object computing systems	distributed software development;web servers;scaling up;operating system;distributed object system;load balancing;distributed object computing;load balance;request redirection;javaspaces;article;fuzzy decision;java	Distributed object computing systems are widely envisioned to be the desired distributed software development paradigm in the near future due to the higher modularity and the capability of handling machine and operating system heterogeneity. Indeed, enabled by the tremendous advancements in processor and networking technologies, complex operations such as object serialization and data marshalling become very efficient, and thus, distributed object systems are being built for many different applications. As the system scales up (e.g., with larger number of server and client objects, and more machines), a judicious load balancing system is required to efficiently distribute the workload (e.g., the queries, messages/objects passing) among different servers in the system. Several such load balancing schemes are proposed recently in the literature. However, while the rationales and mechanisms employed are dramatically different, the relative strengths and weaknesses of these approaches are unknown, making it difficult for a practitioner to choose an appropriate approach for the problem at hand. In this paper, we describe in detail three representative approaches, which are all practicable, and present a quantitative comparison using a real experimental distributed object computing platform. Among these three approaches, namely, JavaSpaces based, request redirection based, and fuzzy decision based, we find that the fuzzy decision-based algorithm outperforms the other two considerably under a wide range of different practical scenarios.	algorithm;distributed computing;distributed object;java;load balancing (computing);marshalling (computer science);operating system;programming language;programming paradigm;serialization;server (computing);software development;tuple space	Lap-Sun Cheung;Yu-kwok Kwok	2004	The Journal of Supercomputing	10.1023/B:SUPE.0000009320.90845.0c	parallel computing;real-time computing;computer science;object request broker;load balancing;operating system;database;distributed computing;distributed object;distributed design patterns;programming language;computer security	HPC	-24.249902650939585	53.569197340276396	5285
f5ae8fbb960cac8983a9b6deb8bb89ebc9313177	processamento de informação em redes neurais sensoriais				Thiago Schiavo Mosqueiro	2015				NLP	-105.71093409446135	18.288075443758334	5293
3f3909c1544c82bbd81237e33a975ff509500a77	a framework for the decentralisation and management of collaborative applications in ubiquitous computing environments	trust;collaborative applications;groupware;instant messaging;context information;collaborative application;ubiquitous computing environments;content based networking;intermittent network connectivity;conference paper;computer architecture;instant messaging collaborative application trust community based management content based networking;network connectivity;collaborative activities management;collaborative activities management collaborative applications ubiquitous computing environments instant messaging intermittent network connectivity decentralized computing architectures;computer network management;electronic messaging;ubiquitous computing;environmental management collaboration ubiquitous computing access control application software computer architecture routing humans content management peer to peer computing;computer science;decentralized computing architectures;ubiquitous computing computer network management electronic messaging groupware;community based management;ubiquitous computing environment	When deploying collaborative applications such as instant messaging in ubiquitous computing environments significant enhancements can be afforded by offering additional context information, such as location information. However, such environments exert key challenges such as increased diversity of ownership and ad hoc, intermittent network connectivity that suits more decentralized computing architectures. This paper examines how a migration to a more decentralized collaborative architecture can be achieved together with a decentralization of the management of collaborative activities	decentralized computing;hoc (programming language);instant messaging;ubiquitous computing	Karl Quinn;Austin Kenny;Kevin Feeney;David Lewis;Declan O'Sullivan;Vincent P. Wade	2006	2006 IEEE/IFIP Network Operations and Management Symposium NOMS 2006	10.1109/NOMS.2006.1687677	computer science;knowledge management;distributed computing;trustworthy computing;world wide web;ubiquitous computing;collaborative software;computer network	HCI	-35.08855222316076	47.55114920750381	5294
d39dfbfc8d8e6c0503e092bffc981c978187ed92	open default theories over closed domains	ground term;unique name assumption;closed domains;free variable;arbitrary element;underlying theory;theory universe;open default theories;general case;open default;domain closure assumption;normal default theory	In this paper we compare two approaches to the meaning of a free variable x in an open default α(x) : β1(x), . . . , βm(x) γ(x) . The first treats x as a metavariable for the ground terms of the underlying theory, whereas the second treats it as a “name” of arbitrary elements of the theory universe. We show that, for normal default theories, under the domain closure assumption, the two approaches are equivalent. In the general case, the approaches are equivalent in the presence of both the domain closure assumption and the unique name assumption.	free variables and bound variables;metavariable;unique name assumption	Michael Kaminski	1999		10.1007/3-540-48747-6_19	combinatorics;discrete mathematics;mathematics	NLP	-8.25567685572494	10.33638057621254	5296
91285adb21a3be236e095d50ae4bc018f7f53f07	einfache authentifikation?				Detlef Kraus;Helmut Reimer	2016	Datenschutz und Datensicherheit - DuD	10.1007/s11623-016-0575-0		NLP	-96.92410779322788	22.79098114260542	5303
7d142887dcef3aa74cff7249a0012170f690ad94	contemporary research on real-time scheduling considered obsolete	design principle;resource constraint;software complexity;earliest deadline first;evaluation criteria;real time scheduling;real time computing;real time systems	The reason for research in scheduling is an economical one, viz., to optimise the utilisation of resources. Up to the present time, almost all interest is directed towards processor scheduling. Departing from the requirements holding for real-time computing, in this paper it is shown that maximum processor utilisation has become obsolete as an optimisation criterion for industrial real-time systems. It is also shown that the earliest-deadline-first discipline and certain modifications thereof provide a satisfactory and final answer to all real-life scheduling needs. To this end, all intrinsic properties of this discipline are compiled and discussed in order to show that it is the most advantageous scheme at hand, characterised by efficiency and allowing for predictable system behaviour. It is then pointed out how the method naturally extends to the scheduling of tasks having non-pre-emptable regions due to resource-access constraints. A sufficient condition is presented, which allows, at any arbitrary point in time and under observation of resource constraints, to check the feasible schedulability of the tasks competing for processor allocation. This condition applies to entirely non-pre-emptable tasks as well. Then, by taking industrial practice and actual cost relations into account, evaluation criteria and design principles for real-time computing systems are developed. The paper closes with pointing to those open optimisation questions, scheduling research ought to address if it wants to deal with practically relevant problems, viz., minimisation of software costs, software complexity, and complexity of schedules, synchronisation sequences, inter-task communication, etc. In other words, simplicity is to be maximised to enhance system dependability and predictability of system behaviour. © 2004 Elsevier Ltd. All rights reserved.	compiler;dependability;earliest deadline first scheduling;inter-process communication;mathematical optimization;programming complexity;real life;real-time clock;real-time computing;real-time transcription;requirement;scheduling (computing);viz: the computer game;windows nt processor scheduling	Wolfgang A. Halang	2004	Annual Reviews in Control	10.1016/j.arcontrol.2004.01.009	fair-share scheduling;fixed-priority pre-emptive scheduling;real-time computing;earliest deadline first scheduling;simulation;dynamic priority scheduling;computer science;engineering;rate-monotonic scheduling;artificial intelligence;two-level scheduling;control theory;mathematics;least slack time scheduling;round-robin scheduling;programming complexity	Embedded	-9.466064885674612	60.86073974729953	5306
7438bec80ca27afd553e9328c3dc48a056b7ecf4	business-to-business e-commerce: a transition model	business to business;electronic commerce;e commerce;management of change electronic commerce internet electronic data interchange;decision maker;electronic commerce business supply chain management supply chains companies economic forecasting consumer electronics outsourcing quality management internet;internet;management of change;industrial sector business to business e commerce transition model internet based companies traditional firms digital economy electronic commerce activities supply chain management clicks and mortar companies technological model seamless integration inter organizational processes five wave transitional model progressive steps business to business e commerce needs technological waves organizational underpinnings;digital economy;supply chain management;electronic data interchange;technological transition model	The creation of Internet-based companies is changing the way business is being carried out and increasing the pressure on traditional firms, which now need to adapt to the new challenges brought about by the so-called digital economy. Successful electronic commerce activities depend on the partners involved in the product or service being delivered. Firms able to communicate with their partners electronically for procurement, sales, or supply chain management have become what many call clicksand-mortar companies. An empirically based technological model that helps organizations understand the requirements of moving towards the seamless integration of intraand interorganizational processes is proposed. This five-wave transitional model accompanies key decision-makers through progressive steps that correspond to different business-to-business e-commerce needs and specifications. As organizations move along these technological waves, we can witness the gradual opening-up of new opportunities for carrying out business. This paper presents the model, its requirements and its technological and organizational underpinnings. The model is illustrated with examples from organizations in a major	convex function;e-commerce payment system;mortar methods;procurement;requirement;seamless3d	Louis A. Lefebvre;Luc Cassivi;Élisabeth Lefebvre	2001		10.1109/HICSS.2001.927034	e-commerce;decision-making;the internet;computer science;artifact-centric business process model;marketing;electronic data interchange;electronic business;management;world wide web;business process modeling;commerce	Web+IR	-75.74870943974337	6.265496517739403	5308
556c006b2d17eeae85fabf04881ec5ae98688ebe	balancing shared and distributed heaps on numa architectures		Due to the varying latencies between memory banks, efficient shared memory access is challenging on modern NUMA architectures. This has a major impact on the shared memory performance of parallel programs, particularly those written in languages with automatic memory management.	distributed shared memory	Malak Saleh Aljabri;Hans-Wolfgang Loidl;Philip W. Trinder	2014		10.1007/978-3-319-14675-1_1	parallel computing;theoretical computer science;memory management;memory bank;graph reduction;computer science;garbage collection;shared memory;heap (data structure)	Arch	-11.699704982130024	49.36660643591292	5328
d9ba9be878bed65481d86c6682ed63793caa2337	qos-enabled ipv6 emulation environment based on the open ims core	quality of service emulation media protocols ip networks bandwidth multimedia communication;quality of service ip networks multimedia communication;resource reservation;network control;control architecture;multimedia communication;ip networks;quality of service;ip multimedia subsystem;user to user audio session qos enabled ipv6 network control open ims core ip multimedia subsystem session control quality of service mapping application level qos parameters network level qos parameters qos control qos enabled policy charging control architecture	In this work we present an emulated environment that provides an IP Multimedia Subsystem (IMS) — based session control for applications and services, extended by Quality of Service (QoS) mapping and control mechanisms over the QoS-enabled IPv6 network. The mapping process transforms the application-level QoS parameters into the network-level QoS parameters, allowing the creation of a comprehensive laboratory environment incorporating session control, QoS control, and IPv6 network control functionalities. The key contribution of our model is the extension of the existing Open IMS Core solution, with the support for managing IPv6 network resources over the QoS-enabled Policy and Charging Control architecture. The laboratory environment capable for testing various IMS-based applications and services is presented. The functionality of the environment is verified on a user-to-user audio session with a successful network resource reservation scenario.	control system;emulator;ip multimedia subsystem;mathematical optimization;policy and charging rules function;quality of service;requirement;software performance testing	Tomislav Grgic;Niko Boskovic;Maja Matijasevic	2011	SoftCOM 2011, 19th International Conference on Software, Telecommunications and Computer Networks		real-time computing;mobile qos;quality of service;computer science;distributed computing;ip multimedia subsystem;computer network	Embedded	-14.36790830120191	93.13936025810007	5333
5ceb52520feab2d77f4ad0befb62a551a17e469f	wikis, semantics, and collaboration: symposium on collaboration analysis and reasoning systems, at the 2014 conference on collaboration technologies and systems	semantic web representations collaboration analysis reasoning systems collaborative sensemaking environments software design rapid response analytic situations software architecture semantic wiki architectures databases social tagging systems;encyclopedias electronic publishing semantic web semantics collaboration;web sites inference mechanisms semantic web social networking online software architecture	Designing software for collaborative sensemaking environments begins with a set of very challenging requirements. At a high level, the software needs to be flexible enough to support multiple lines of inquiry, contradictory hypotheses, and collaborative tasking by multiple analysts. It should also include support for managing evolving human/machine workflows and analytic products at various levels of strictness and formality, processing partial and ambiguous evidence arriving in streams, and developing explanatory scenarios based on both serendipitous and structured discovery. Eventually, it should support the analytic team as they evaluate multiple alternatives and converge on one or more consensus responses, while preserving the history and underlying reasoning. Finally, it should be delightful and simple to use, not require an inordinate degree of precision and exactness, and be quickly and inexpensively deployable in a variety of rapid-response analytic situations. It has not been possible thus far to create a single software architecture that adequately balances all these goals. However, we can shed useful light on this problem by looking at the experience of semantic wiki architectures: an emerging class of software that blends wikis, databases, social tagging systems, and Semantic Web representations.	collaborative software;converge;database;folksonomy;high-level programming language;recommender system;requirement;schedule (computer science);semantic web;sensemaking;software architecture;wiki	Mark Greaves	2014	2014 International Conference on Collaboration Technologies and Systems (CTS)	10.1109/CTS.2014.6867607	semantic computing;data web;computer science;knowledge management;semantic web;social semantic web;semantic web stack;database;world wide web;semantic analytics	SE	-46.70924852590969	4.774301302386336	5339
b3ab8e66135d9f3d15f6df3f02401be552b5a05d	an investigation of document partitions	citation analysis;analyse amas;analisis estadistico;modele mathematique;information retrieval;weighting;modelo matematico;graphs;analyse citation;cluster analysis;validity;statistical analysis;analyse statistique;tables data;automatic indexing;mathematical model;ponderation;relevance information retrieval;subject index terms	Diverses etudes ont montre recemment que la signification empirique de la repartition des documents retrouves par les relations de co-citations varie en fonction des criteres choisis pour determiner ces co-citations: le seuil de ponderation est determine par un niveau precis d'exhaustivite et de specificite de l'indexation, le seuil de similarite par un niveau specifique de la hierarchie par liaison unique d'association. Les resultats montrent que les memes repartitions «preferees» empiriquement peuvent etre detectees par deux strategies independantes: une analyse d'efficacite de la recherche par cluster et une analyse des regles de base du graphe. Ces resultats representent la premiere etape d'une recherche concue pour determiner si la signification statistique des repartitions des documents peut expliquer la signification empirique de ces memes repartitions. (INTD)		William M. Shaw	1986	Inf. Process. Manage.	10.1016/0306-4573(86)90006-3	relevance;computer science;mathematical model;weighting;cluster analysis;graph;citation analysis;information retrieval;validity;statistics	DB	-108.05976080903791	13.710112296531339	5344
4815250d4be82d81eea0970eac445215d8b533af	exploiting big data from mobile device sensor-based apps: challenges and benefits		"""In the world of big data, organizations are increasingly turning to mobile devices as new sources of data derived from continuously monitoring a wide range of processes and situations. Mobile devices can facilitate the gathering of data from a diverse set of internal and external 3 stakeholders, including employees, customers and citizens willing to """" donate """" their data. Moreover, this data can be used in myriad ways. For example, sensor data on how an automobile is driven is providing big benefits for organizations ranging from government agencies to insurers. Facilitated by the """" Internet of Things (IOT), """" donated sensor data generated from users' mobile devices provides important new opportunities. As originally conceived, the IOT includes all kinds of devices connected to the Internet and to each other, generating sensor-based signals independent of human intervention. As noted by Ashton, 6 """" We need to empower computers with Sensor data gathered from mobile devices generates big data, which can help organizations continuously monitor a wide range of processes in ways that prompt actions and generate value. However, generating reliable data from such devices is not straightforward and presents a variety of challenges. This article describes the challenges and provides guidelines based on a case study of """" Street Bump, """" a mobile device app that the City of Boston uses to facilitate road infrastructure management."""	big data;bump mapping;computer;internet of things;mobile device;sensor	Daniel E. O'Leary	2013	MIS Quarterly Executive		big data;engineering;internet privacy;computer security;mobile device	HCI	-42.52641991078897	50.46298416469787	5345
6fb6f26e4d5dd2d3c024bf514d1feed94b7793ac	gricol: a language for grid computing	significant role;complex computing experiment;grid computing;complex grid experiment;large-scale modeling experiment;efficient language;new possibility;programming languages;object oriented programming;computer experiment	The development of the grid has opened new possibilities for scientists and engineers to execute large-scale modeling experiments. This has stimulated the generation and development of tools for the creation and management of complex computing experiments in the grid. Among these, tools for the automation of the programming of experiments play a significant role. In this paper we present GriCoL, which we propose as a simple and efficient language for the description of complex grid experiments	experiment;grid computing	Natalia Currle-Linde;Michael M. Resch	2006	2006 7th IEEE/ACM International Conference on Grid Computing		computational science;computer experiment;semantic grid;computer science;theoretical computer science;end-user computing;programming language;object-oriented programming;drmaa;grid computing	HPC	-34.27562289941744	27.822499813833318	5347
14b4c605a5f53f8308eb3198ddde77d904b09a32	an ontology-based domain representation for plan-based controllers in a reconfigurable manufacturing system.	reconfigurable manufacturing systems;timeline based planning;knowledge based systems	The paper describes a knowledge-based control loop as the key feature of a generic control architecture for nodes in a Reconfigurable Transportation Systems (RTSs). In particular, two main aspects are presented: (i) the design of an ontologybased representation of information related to both internal configurations/capabilities of a node and the active connections with its neighbors in the plant; (ii) the definition of a relationship between the ontology and the abstract model exploited by a temporal planning and execution module to implement control strategies of a single node. The main contribution is in proposing a connection between ontology-based representation and planning and execution model. The goal is to enable dynamic inference of control models so as to adapt control strategies to mutating shop-floor scenarios.	agent architecture;automated planning and scheduling;closing (morphology);control system;mechatronics	Stefano Borgo;Amedeo Cesta;Andrea Orlandini;Alessandro Umbrico	2015			real-time computing;computer science;artificial intelligence;knowledge-based systems	Robotics	-38.6142815588935	20.885603651807205	5348
7185b274f24b9d46ca84cca34c8496c6fb8d142d	an impartial efficiency comparison of fd constraint systems	constraint system	CLP systems differ significantly in the efficiency of their execution so that a wrong choice for an application may be disastrous relative to the necessary performance. Thus, efficiency should be taken into account when choosing the best system for a particular application. In spite of this, there appeared to be no impartial efficiency information concerning CLP systems. In the research described here, we have compared, as fairly as possible, eight systems and provided detailed results concerning their speed and robustness. Eight systems were compared, four black box: ECLPS 3.5.2, Oz 2.0, the Ilog SOLVER 3.1 and B-Prolog 2.1; and four glass box: clp(FD) 2.21, CHR (built on top of ECLPS 3.5.2), SICStus 3#5 and IF/Prolog 5.0. These were compared with respect to the performance of their constraint propagation. For a fair comparison, two very different labeling strategies were used: the naive labeling and the first fail labeling. As far as possible, the variable and value ordering was kept the same for all the systems. In addition, for the robustness comparison, the garbage collection was kept switched on in all the systems. Seven (mostly well-known) benchmarks were used. The solutions (finding both the first answer and all possible answers) we chose were either supplied with the system, by the system implementers or first written by us and then improved by the implementers. All seven benchmarks were used in the efficiency tests but just two scalable ones were used for the robustness tests. Ilog was the fastest system and also extremely robust. clp(FD) was also very fast but did not scale well with respect to the number of FD variables. Oz, SICStus and IF/Prolog had very similar performance figures and were about two to three times faster than ECLPS. IF/Prolog worked particularly well for first solution search (even sometimes better than clp(FD) and Ilog). SICStus and IF/Prolog were more robust than clp(FD), Oz, and ECLPS. However, of the two, SICStus has the greater robustness. B-Prolog was comparable to clp(FD) when there was only a small number of FD variables although, as B-Prolog lacked garbage collection, the performance deteriorated rapidly as the number increased. The slowest system was CHR. Two reasons for this: (1) the version used had been built on top of ECLPS (which itself had poor results) and (2) it was not designed for efficiency but for defining adequate constraints solvers for particular problems on specific domains.	b-prolog;benchmark (computing);black box;constraint handling rules;fastest;garbage collection (computer science);ilog;local consistency;robustness (computer science);sicstus prolog;scalability;software propagation;white box (software engineering)	Antonio J. Fernández;Patricia M. Hill	1998		10.1007/3-540-49481-2_39	mathematical optimization;computer science;hybrid algorithm	AI	-20.638771251224583	15.058597858263491	5349
9f06c7ac2146a0445e501768f7196f88b4bfdff5	transforming acyclic programs	acyclic programs;operational semantics;termination;logic programs;terminating programs	An unfold/fold transformation system is a source-to-source rewriting methodology devised to improve the efficiency of a program. Any such transformation should preserve the main properties of the initial program: among them, termination. In the field of logic programming, the class of acyclic programs plays an important role in this respect, since it is closely related to the one of terminating programs. The two classes coincide when negation is not allowed in the bodies of the clauses. We prove that the Unfold/Fold transformation system defined by Tamaki and Sato preserves the acyclicity of the initial program. From this result, it follows that when the transformation is applied to an acyclic program, then the finite failure set for definite programs is preserved; in the case of normal programs, all major declarative and operational semantics are preserved as well. These results cannot be extended to the class of left-terminating programs without modifying the definition of the transformation.	directed acyclic graph;fold (higher-order function);logic programming;newman's lemma;operational semantics;rewriting	Annalisa Bossi;Sandro Etalle	1994	ACM Trans. Program. Lang. Syst.	10.1145/183432.183434	computer science;programming language;operational semantics;algorithm	PL	-15.770802910359103	21.95221643744338	5351
09ef6e312c3d3e2f8ed56303e040884604d7ce9c	route optimization and traffic management in atm networks using neural computing			atm turbo;artificial neural network	Aloke Chaudhuri	1998				HPC	-9.582247033039396	92.44598729595045	5361
9a31e5f66ef434953c0d1c386012be64379bbfd9	tractable cases of clean query answering under entity resolution via matching dependencies	matching dependencies;clean query answering;declarative entity resolution;tractable case;clean version;clean instance;clean answer;md enforcement;certain class;resulting class;chase-like procedure	Matching Dependencies (MDs) are a recent proposal for declarative entity resolution. They are rules that specify, given the similarities satisfied by values in a database, what values should be considered duplicates, and have to be matched. On the basis of a chase-like procedure for MD enforcement, we can obtain clean (duplicate-free) instances; possibly several of them. The clean answers to queries (which we call the resolved answers) are invariant under the resulting class of instances. Identifying the clean versions of a given instance is generally an intractable problem. In this paper, we show that for a certain class of MDs, the characterization of the clean instances is straightforward. This is an important result, because it leads to tractable cases of resolved query answering. Further tractable cases are derived by making connections with tractable cases of CQA.	algorithm;analysis of algorithms;cobham's thesis;computational complexity theory;constraint logic programming;database;datalog;free variables and bound variables;invariant (computer science);molecular dynamics;pattern matching;refinement (computing);rewriting;time complexity	Jaffer Gardezi;Leopoldo E. Bertossi	2012		10.1007/978-3-642-33362-0_14	data mining;database;mathematics;algorithm	DB	-24.291954978482625	11.082972391263823	5362
50615288b2edaf0f9c11c24a4ad278ab34233890	critical success factors for offshore software development outsourcing vendors: an empirical study	empirical study;cost saving;project manager;questionnaire survey;research paper;software development;systematic literature review;success factor;critical success factor	CONTEXT – Offshore software development outsourcing is a contractual business of high quality software production with significant cost-saving. OBJECTIVE – The objective of this research paper is to analyse the factors that influence software outsourcing clients in the selection of offshore software outsourcing vendors. METHOD – We have performed questionnaire surveys with 53 experts. We asked the participants to rank each success factor on a five-point scale to determine the perceived importance of each success factor. Our survey included success factors identified in the previous findings of systematic literature review study. RESULTS – Our study reveal both cost-saving and appropriate infrastructure as the most influential factors in the selection of outsourcing vendors. Our results also indicate that appropriate infrastructure, cost-saving and efficient project management are common success factors across different groups of practitioners. CONCLUSIONS – Cost-saving and appropriate infrastructure should be considered as the prime factors in the selection process of software development outsourcing vendors.	agile software development;display resolution;outsourcing;systematic review	Siffat Ullah Khan;Mahmood Niazi;Rashid Ahmad	2010		10.1007/978-3-642-13792-1_13	reliability engineering;questionnaire;systematic review;systems engineering;engineering;operations management;software development;knowledge process outsourcing;critical success factor;empirical research	SE	-71.74108085719675	21.559237485760796	5372
e2b9b8c0340a621133ff604516af9ce0a9fd7b7c	a hybrid time management approach to agent-based simulation	agent platform;time warp;agent based simulation;distributed agents;agent communication;time management;domain specificity;knowledge base	In this paper we describe a time management approach to distributed agent-based simulation. We propose a new time management policy by joining optimistic synchronization techniques and domain-specific knowledge based on agent communication protocols. With respect to our experimental results, we assume that our approach helps to prevent too optimistic event execution. Consequently, the probability of time consuming rollbacks is reduced in comparison to a pure time warp based solutions. The approach has been implemented as a synchronization service for the JADE agent platform SimJade. The paper concludes by the discussion of our experimental results and future improvements.	agent-based model;agent-based social simulation;algorithm;distributed computing;dynamic time warping;html5 in mobile devices;jade;load balancing (computing);logistics;multi-agent system;scalability	Dirk Pawlaszczyk;Ingo J. Timm	2006		10.1007/978-3-540-69912-5_28	real-time computing;simulation;computer science;distributed computing	AI	-39.7282377067776	21.19432552671961	5373
133c42ea59f24691539b0b460e60c84fb6233ddb	advances in computer systems architecture	291602 memory structures;671205 computer equipment	Virtual machines can enhance computer systems in a number of ways, including improved security, flexibility, fault tolerance, power efficiency, and performance. Virtualization can be done at the system level and the process level. Virtual machines can support high level languages as in Java, or can be implemented using a low level co-designed paradigm as in the Transmeta Crusoe. This talk will survey the spectrum of virtual machines and discuss important design problems and research issues. Special attention will be given to co-designed VMs and their application to performanceand power-efficient microprocessor design. Replica Victim Caching to Improve Reliability of In-Cache Replication	computer architecture;fault tolerance;hardware description language;high-level programming language;java;openvms;performance per watt;processor design;programming paradigm;systems architecture;transmeta crusoe;virtual machine	Moni Naor	2004		10.1007/b100354	parallel computing;computer hardware;programming language;memory map	Arch	-8.5042991599939	45.40434748012306	5377
0015eda37750570919a502387fe472cfd4f39efb	2nd international workshop on realising evidence-based software engineering (rebse-2)	software engineering	The REBSE international workshops are concerned with exploring the adaptation and use of the evidence-based paradigm in software engineering research and practice. The workshops address this goal through a mix of presentations and discussion, drawing upon ideas and experiences from other disciplines where appropriate.	experience;programming paradigm;software engineering	David Budgen;Barbara A. Kitchenham;Pearl Brereton;Mark Turner	2007	Second International Workshop on Realising Evidence-Based Software Engineering (REBSE '07)	10.1109/ICSECOMPANION.2007.6	engineering management;software engineering process group;computer science;systems engineering;engineering;social software engineering;software development;software engineering;software walkthrough	SE	-65.89040952740343	24.15879464258321	5383
d0f039e24136c17ac5ee3f1be392a622ac41e320	a new program to 13c nmr spectrum prediction based on tridimensional models (determinação estrutural e simulação de espectros de rmn13c de sesquiterpenos lactonizados utilizando métodos computacionais)				Fátima Maria Motter Magri	1999				Logic	-103.43919168789064	18.39014417366812	5384
77eefbd9c4510afb617e87d91638a959944064e3	generic framework for parallel and distributed processing of video-data	symmetric configuration;tratamiento paralelo;tratamiento datos;graph theory;distributed system;data transmission;record format;cluster computing;teoria grafo;haute performance;systeme reparti;traitement parallele;traitement flux donnee;multiprocessor;image databank;racimo calculadura;configuration symetrique;format enregistrement;view interpolation;distributed computing;data processing;video processing;traitement donnee;paralelisacion;flux donnee;flujo datos;parallel and distributed processing;flow models;data format;theorie graphe;multiple view;connected graph;modele ecoulement;configuracion simetrica;grappe calculateur;sistema repartido;senal video;signal video;transmission donnee;parallelisation;banco imagen;data flow processing;banque image;parallelization;alto rendimiento;software framework;video signal;calculo repartido;vue multiple;modele donnee;formato grabacion;multiprocesador;data flow;atomic processes;high performance;graphe connexe;calcul reparti;parallel processing;transmision datos;vista multiple;data models;grafo conexo;multiprocesseur	This paper presents a software framework providing a platform for parallel and distributed processing of video data on a cluster of SMP computers. Existing video-processing algorithms can be easily integrated into the framework by considering them as atomic processing tiles (PTs). PTs can be connected to form processing graphs that model the data flow. Parallelization of the tasks in this graph is carried out automatically using a pool-of-tasks scheme. The data format that can be processed by the framework is not restricted to image data, such that also intermediate data, like detected feature points, can be transferred between PTs. Furthermore, the processing can be carried out efficiently on special-purpose processors with separate memory, since the framework minimizes the transfer of data. We also describe an example application for a multi-camera view-interpolation system that we successfully implemented on the proposed framework.	algorithm;automatic parallelization;central processing unit;computer;dataflow;distributed computing;interpolation;parallel computing;software framework;symmetric multiprocessing	Dirk Farin;Peter H. N. de With	2006		10.1007/11942634_84	embedded system;data modeling;data flow diagram;parallel processing;parallel computing;multiprocessing;data processing;computer cluster;computer science;connectivity;graph theory;theoretical computer science;software framework;distributed computing;video processing;data transmission	HPC	-17.278411120430533	42.567265153672054	5398
d9a6b5b37d51350cc6b898ed27e71982bccf12ae	erhöhung der ortsauflösung von nahfeldmessungen durch eine inverse faltungsoperation			eine and zwei	Adam Tankielun;Sven Fisahn;Heyno Garbe;Werner John	2006			mathematical analysis;inverse;chemistry	Vision	-96.41204420836247	34.14473441487014	5400
d8ce07a131ad5ce064a02125aedf9ba64a1f8afb	an optimization framework for generalized relevance learning vector quantization with application to z-wave device fingerprinting		Z-Wave is low-power, low-cost Wireless Personal Area Network (WPAN) technology supporting Critical Infrastructure (CI) systems that are interconnected by government-to-internet pathways. Given that Z-wave is a relatively unsecure technology, Radio Frequency Distinct Native Attribute (RF-DNA) Fingerprinting is considered here to augment security by exploiting statistical features from selected signal responses. Related RF-DNA efforts include use of Multiple Discriminant Analysis (MDA) and Generalized Relevance Learning Vector Quantization-Improved (GRLVQI) classifiers, with GRLVQI outperforming MDA using empirically determined parameters. GRLVQI is optimized here for Z-Wave using a full factorial experiment with spreadsheet search and response surface methods. Two optimization measures are developed for assessing Z-Wave discrimination: 1) Relative Accuracy Percentage (RAP) for device classification, and 2) Mean Area Under the Curve (AUCM) for device identity (ID) verification. Primary benefits of the approach include: 1) generalizability to other wireless device technologies, and 2) improvement in GRLVQI device classification and device ID verification performance.	dna profiling;device fingerprint;learning vector quantization;linear discriminant analysis;low-power broadcasting;mathematical optimization;multiple discriminant analysis;radio fingerprinting;radio frequency;rapid refresh;relevance;response surface methodology;spreadsheet;z-wave	Trevor J. Bihl;Michael A. Temple;Kenneth W. Bauer	2017			speech recognition;learning vector quantization;computer science;information security;theoretical computer science;machine learning	ML	-47.30741210167116	83.68394287104059	5403
11f18d5b44de1bd813a833e353f115fc3bdce0eb	constructing formal rules to verify message communication in distributed systems	client server;specification and verification;distributed systems;reliable message passing	This study presents a method to construct formal rules used to run-time verify message passing between clients in distributed systems. Rules construction is achieved in four steps: (1) Visual specification of expected behavior of the sender, receiver, and network in sending and receiving a message, (2) Extraction of properties of sender, receiver, and network from the visual specification, (3) specification of constraints that should govern message passing in distributed systems, and (4) construction of verifier rules from the properties and the constraints. The rules are used to verify actual sender, receiver, and network behavior. Expected behavior of the client (process) is one that to be and the actual one is the behavior should be verified. The rules were applied to verify the behavior of client and servers that communicated with each other in order to compute Fibonacci numbers in parallel and some violations were discovered.	client (computing);distributed computing;message passing	Seyed Morteza Babamir	2011	The Journal of Supercomputing	10.1007/s11227-011-0553-0	parallel computing;real-time computing;computer science;operating system;database;distributed computing;message broker;programming language;computer security;client–server model	Networks	-31.672486620758534	34.38685692712894	5404
cb150a59a1ef90e74d4cee9d768dfbb858c67806	hpc-europa: towards uniform access to european hpc infrastructures	formal specification;grid computing;open systems;portals;resource allocation;security of data;european hpc infrastructures;gridsphere portal;hpc-europa portal;hpc-europa environment;accounting;interoperability;job specification description language;resource access;security;service access;uniform job submission interface;user access	One of the goals of the HPC-Europa project is to provide users with a Single Point of Access (SPA) to the resources of HPC centers in Europe. To this end, the HPC-Europa Portal is being built to provide transparent, uniform, flexible and intuitive user access to HPC-Europa resources. In this paper, we present a mechanism that enables end-users to transparently access the diverse services available in the HPC-Europa environment. The uniform job submission interface that uses this mechanism, utilizing the job specification description language (JSDL), is described. We also present the architecture of the SPA, based on the GridSphere portal framework. Finally, we discuss the various interoperability problems encountered, in particular those concerning job submission, security and accounting.	europa;hpc challenge benchmark;interoperability;job submission description language	Ariel Oleksiak;Alisdair Tullo;Paul Graham;Tomasz Kuczynski;Jarek Nabrzyski;Dawid Szejnfeld;Terry Sloan	2005	The 6th IEEE/ACM International Workshop on Grid Computing, 2005.		resource allocation;computer science;operating system;formal specification;database;open system;management;world wide web;computer security;grid computing	HPC	-32.75691897660952	52.73440443331277	5414
81fa92cb23cf3c20a659e00b05fcc4163fc763b3	content publishing framework for interactive paper documents	channel access;digital documents;new technology;multi threading;interactive paper;performance evaluation;life cycle;optimization technique;programming environment;data binding;demand driven;xml;xml document	This paper proposes a programming environment for Java thatprocesses network XML data in a demand-driven manner to returnquick initial responses. Our system provides a data binding tooland a tree operation package, and the programmer can easily handlenetwork XML data as tree-based operations using these facilities.For efficiency, demand-driven data binding allows the applicationto start the processing of a network XML document before thearrival of the whole data, and our tree operators are also designedto start the calculation using the initially accessible part of theinput data. Our system uses multithread technology forimplementation with optimization techniques to reduce runtimeoverheads. It can return initial responses quickly, and oftenshortens the total execution time due to the effects of latencyhiding and the reduction of memory usage. Compared with an ordinarytree-based approach, our system shows a highly improved responseand a 1-28% reduction of total execution time on the benchmarkprograms. It only needs 1-4% runtime overheads against theevent-driven programs.	accessibility;benchmark (computing);event-driven programming;integrated development environment;java platform, enterprise edition;join (sql);mathematical optimization;parsing;performance evaluation;programmer;run time (program lifecycle phase);thread (computing);tree traversal;xml	Masakazu Yamanaka;Kenji Niimura;Tomio Kamada	2005		10.1145/1096601.1096651	simple api for xml;xml;xml schema;streaming xml;computer science;xml framework;xml database;database;programming language;world wide web;efficient xml interchange	HPC	-19.127266368175107	34.05165912406779	5416
a600efcd71a8657de9c57cd46894bd358bf2107e	an evolutionary approach to multiprocessor scheduling of dependent tasks	systeme temps reel;algoritmo paralelo;multiprocessor scheduling;parallel algorithm;gestion labor;multiprocessor systems;processor scheduling;algoritmo genetico;algorithme parallele;optimisation combinatoire;mutual exclusion;gestion tâche;periodic tasks;precedence constraint;algorithme genetique;genetic algorithm;real time system;sistema tiempo real;ordonnancement processeur;task scheduling;combinatorial optimization;optimizacion combinatoria;real time systems;genetic alorithms	The scheduling of application tasks is a problem that occurs in all multiprocessor systems. This problem becomes even more complicated if the tasks are not independent but are interrelated by mutual exclusion and precedence constraints. This paper presents an approach for pre-runtime scheduling of periodic tasks on multiple processors for a real-time system that must meet hard deadlines. The tasks can be related to each other by mutual exclusion and precedence forming an acyclic graph. The proposed scheduler is based on Genetic Algorithms, which relieves the user from knowing how to construct a solution. Consequently, the paper focuses on the problem encoding, i.e., the representation of the problem by genes and chromosomes, and the derivation of an appropriate tness function. The main bene t of the approach is that it is scalable to any number of processors and can easily be extended to incorporate further requirements.	central processing unit;directed acyclic graph;genetic algorithm;iterative and incremental development;multiprocessing;multiprocessor scheduling;mutual exclusion;real-time clock;real-time computing;requirement;scalability;scheduling (computing);software release life cycle;time-triggered architecture	Roman Nossal-Tüyeni	1998	Future Generation Comp. Syst.	10.1016/S0167-739X(98)00041-7	parallel computing;real-time computing;genetic algorithm;mutual exclusion;combinatorial optimization;computer science;distributed computing;parallel algorithm;multiprocessor scheduling	Embedded	-13.115884866612435	61.935343801823194	5421
6e730056abe28c5e2e2c26a0979ce02279177448	discussing the difference between model driven architecture and model driven development in the context of supporting tools	analytical models;mda progress;design and development;context modeling object oriented modeling programming computer architecture computer aided software engineering software systems software quality automation productivity unified modeling language;two hemisphere model;software systems;software system design alternative solution;software development process;object oriented software development model driven architecture model driven development supporting tools context software development process software systems complexity software system design alternative solution case tool mda progress;software engineering;model driven development;software architecture;computer aided software engineering;rtu;business;software development;case tool;unified modeling language;riga technical university;object oriented software development;supporting tools context;component model;system development;software tools;software systems complexity;two hemisphere model model driven architecture model driven development tool chain;tool chain;programming;scientific publications;model driven architecture;object oriented modeling;scientific journal of rtu;software tools computer aided software engineering software architecture	The most current trends in the evolution of software development process are closely related with the increasing complexity of software systems. This motivates software engineers and system developers to find an alternative solution for software system design and development. In this case, the manner offered by OMG’s initiative, the Model Driven Architecture namely, represents a model-driven approach to software development. It also considers the further evolution of CASE tools, bringing their functionality to the next level. This paper investigates the variety of the CASE tools developed under an impact of MDA growing progress. The differences between Model Driven Architecture and Model Driven Development in the context of automatic capabilities for software development are also discussed in the paper. The example of possibility to define a set of tools proposed as a tool chain for development of software systems under the framework of MDA, the projection of two-hemisphere approach for object-oriented software development into authors’ developed component model of MDA is offered in the paper.	component-based software engineering;computer-aided software engineering;model-driven architecture;model-driven engineering;sms language;software development process;software engineer;software system;systems design;toolchain	Oksana Nikiforova;Antons Cernickins;Natalja Pavlova	2009	2009 Fourth International Conference on Software Engineering Advances	10.1109/ICSEA.2009.71	unified modeling language;reference architecture;software architecture;programming;model-driven architecture;verification and validation;computer science;systems engineering;engineering;package development process;social software engineering;software framework;component-based software engineering;software development;software design description;software engineering;process driven development;iterative and incremental development;software construction;software analytics;resource-oriented architecture;computer-aided software engineering;goal-driven software development process;software development process;software system;computer engineering	SE	-52.84435800562914	27.5938260635377	5426
eebab6351d556ee89d8d79bd589f1a9970d46695	personalizing information gathering for mobile database clients	query processing;mobile agents;mobile databases;mobile computer;mobile database;information gathering;wireless communication;asynchronous communication;data consistency and currency;materialized views;mobile agent;mobile computing;data consistency	Mobile agents are ideal for mobile computing environments because of their ability to support asynchronous communication and flexible query processing since tasks can be delegated to mobile agents when a mobile client is disconnected. This paper explores the use of mobile agents in personalizing information gathering for mobile database clients. Personalized data take the form of materialized views and personalization is provided in the form of view maintenance options. These options, expressed using an extended SQL Create View command, offer a finer grain of control and balance between data availability and currency, the amount of wireless communication and the cost of maintaining consistency. The paper defines recomputational consistency and introduces new levels of materialized view consistency to better characterize the mobile client view currency customizations.	database;evaluation function;materialized view;mobile agent;mobile computing;mobile device;personalization;release consistency;sql;system under test	Susan Weissman Lauzac;Panos K. Chrysanthis	2002		10.1145/508791.508803	mobile search;mobile web;mobile database;computer science;mobile technology;data mining;database;mobile business development;mobile computing;world wide web;mobile payment	DB	-31.294394212664848	44.40805533079863	5434
d20ecce4903b489154261753e74de60da5da1c00	a smooth combination of linear and herbrand equalities for polynomial time must-alias analysis	alias analysis;indexation;polynomial time	We present a new domain for analyzing must-equalities between address expressions. The domain is a smooth combination of Herbrand and affine equalities which enables us to describe field accesses and array indexing. While the full combination of uninterpreted functions with affine arithmetics results in intractable assertion checking algorithms, our restricted domain allows us to construct an analysis of address must-equalities that runs in polynomial time. We indicate how this analysis can be applied to infer access patterns in programs manipulating arrays and structs.	algorithm;alias analysis;assertion (software development);branch (computer science);linear algebra;lock (computer science);modulo operation;p (complexity);pointer (computer programming);polynomial;power of two;race condition;regular expression;time complexity	Helmut Seidl;Vesal Vojdani;Varmo Vene	2009		10.1007/978-3-642-05089-3_41	time complexity;alias analysis;computer science;programming language;algorithm	PL	-18.66764367019047	24.66091668883487	5439
0c13658a598d2e45caaca125b8b4007e296c5f78	a token-based synchronisation scheme for distributed real-time databases	transaction temps reel;distributed system;base donnee repartie;systeme reparti;distributed database;real time;real time transactions;base repartida dato;donnee repliquee;synchronisation;sistema repartido;synchronization;replicated data;temps reel;real time transaction;serializability;serialisabilite;tiempo real;sincronizacion	Ahstraet-Schedulers for real-time replicated database systems must satisfy two major requirements: transactions should meet their timing constraints, and mutual consistency of replicated data should be preserved. In this paper, we present a synchronization scheme, prove its correctness, and evaluate its performance in a distributed real-time database system. The algorithm adopts a token-based scheme for replication control and attempts to balance the urgency of real-time transactions with the conflict resolution policies. In addition, the algorithm employs epsilon-serializability (ESR), a new correctness criterion which is less stringent than conventional one-copy-serializability. The algorithm is flexible and very practical, since no prior knowledge of the data requirements or the execution time of each transaction is required.		Sang Hyuk Son;Spiros Kouloumbis	1993	Inf. Syst.	10.1016/0306-4379(93)90014-R	synchronization;real-time computing;computer science;operating system;database;distributed computing;suzuki-kasami algorithm;distributed database	DB	-23.835382068886435	47.06017314490103	5447
c67545e5c4f689227b47581ba4b6f258a5e6af36	introducing secure provenance in iot: requirements and challenges	wsn based iot;internet of things;ip based iot;provenance;rfid based iot	In current cyber-physical systems, IoT represents interconnection of highly heterogeneous networked entities, providing goods and services to a variety of domains including environment monitoring, energy management, health-care system, and industrial automation. However, in-spite of the advantages of global connectivity, the Internet of Things (IoT) encounter various security challenges and resource constraints including identity management, traceability, storing and processing of veracious sensory data. However, the security research in IoT so far have not focused on provenance and its usefulness in IoT. To keep data traces of IoT devices, provenance can play a vital role as it solves many issues related to data trustworthiness, decision-making, data reconciliation and data replication. In this paper, we have discussed the challenges on technical infrastructure of IP-based, WSN-based and RFID-based IoT. We have identified the possible ways to integrate secure provenance into IoT grounded on security issues and other resource constraints in IoT.	automation;big data;cyber-physical system;entity;identity management;interconnection;internet of things;radio-frequency identification;replication (computing);requirement;threat model;traceability;tracing (software);trust (emotion)	Sabah Suhail;Choong Seong Hong;Zuhaib Uddin Ahmad;Faheem Zafar;Abid Khan	2016	2016 International Workshop on Secure Internet of Things (SIoT)	10.1109/SIoT.2016.011	human–computer interaction;computer science;internet privacy;world wide web;computer security;internet of things	EDA	-44.622244555562396	50.753494460467486	5449
511afbd4c60ee8779f20caf6fc5f1ce95d73d27e	integrationskonzepte und -lösungen zur etablierung einer forschungsinfrastruktur für biobanken		Mit dem „BBMRI Catalog“ wurde das erste pan-europaische Biobanken-Repository entwickelt, das die Basis fur weitere Schritte zur Integration von Bioproben und assoziierten Informationen darstellt. Neben Fragen der Standardisierung und Harmonisierung von Daten und Metadaten stand die Konzeption und Umsetzung von Integrationsszenarien im Fokus. Das realisierte System verwaltet derzeit 345 registrierte Biobanken. Damit ist es die umfassendste Informationsquelle im Bereich Biobanking.	gesellschaft für informatik	Dominik Schmelcher	2013			library science;history	Vision	-104.09755966800756	33.93603856091798	5456
248a643e73c6fb0b75ae15ed5bf42b41f2d6b72f	using pvs to investigate incidents through the lens of distributed cognition	socio technical system;theorem proving;incident analysis	A systematic tool-based method is outlined that raises questions about the circumstances surrounding an incident: why it happened and what went wrong. The approach offers a practical and systematic way to apply a distributed cognition perspective to incident investigations, focusing on how available information resources (or the lack of them) may shape user action, rather than just on causal chains. This perspective supports a deeper understanding of the more systemic causes of incidents. The analysis is based on a higher order-logic model describing how information resources may have influenced the actions of those involved in the incident. The PVS theorem proving system is used to identify situations where available resources may afford unsafe user actions. The method is illustrated using a healthcare case study.	causal filter;distributed cognition;theorem proving system	Paolo Masci;Huayi Huang;Paul Curzon;Michael D. Harrison	2012		10.1007/978-3-642-28891-3_27	simulation;computer science;engineering;artificial intelligence;mathematics;management science;automated theorem proving;programming language;algorithm	NLP	-64.69168278565083	7.565213217611083	5459
4c97102fe8d31dc506af642ae0902251b1406a59	tuple centres for the coordination of internet agents	mars;multidisciplinary application;tuple space;local community;execution environment;runtime system;internet application;mobile agent;coordination model;legacy code	The paper presents the TUCSON coordination model for Internet applications based on network-aware (possibly mobile) agents. The model is based on the notion of tuple centre, an enhanced tuple space whose behaviour can be extended according to the application needs. Everv node of a TUCSON environment provides its local communication space, made up of a multiplicity of independently-programmable tuple centres. This makes it possible to embed global system properties into the space of components’ interaction, thus enabling flexible cooperation over space and time between agents, and permitting to easily face many issues critical to Internet applications, such as heterogeneity and dynamicity of the execution environments.	internet;tuple space	Andrea Omicini;Franco Zambonelli	1999		10.1145/298151.298231	mars exploration program;real-time computing;computer science;tuple space;mobile agent;database;distributed computing;legacy code	AI	-35.720559530084316	43.280650427910814	5464
a338bfcb5e0f20ce20204752c39a69772c2cecd8	objektorientiertes modellieren - sammlung und strukturierung von übungsaufgaben im informatikunterricht				Torsten Brinda	2000	LOG IN		multimedia;software engineering;engineering	Theory	-93.67718540459207	25.54797413543288	5467
275919f21d4811dc996a6a52e2dd85f40d71d1e5	imperative insertion sort			imperative programming;insertion sort	Christian Sternagel	2014	Archive of Formal Proofs		programming language;insertion sort;computer science	Crypto	-22.995868089040407	21.479998096662392	5469
0608ee2184b4bccfde474a2c6ba43b89bd085b6a	design and implementation of the kiosknet system	diminution cout;service information;securite;telecommunication sans fil;wifi;implementation;service telecommunication;mechanical backhaul;prototipo;ictd;delay tolerant networks;internet;design and implementation;telecomunicacion sin hilo;rural community;safety;defaillance;security architecture;telecommunication services;servicio informacion;delay tolerant network;cost effectiveness;conexidad;developing country;temps retard;wireless lan;failures;information service;delay time;connexite;implementacion;reduccion costes;connectedness;seguridad;tiempo retardo;architecture;prototype;reseau local sans fil;fallo;cost lowering;rural communication;wireless telecommunication	Rural Internet kiosks in developing countries can cost-effectively provide communication and e-governance services to the poorest sections of society. Unfortunately, a variety of technical and non-technical issues have caused most kiosk deployments to be unsustainable. KioskNet addresses the key technical problems underlying kiosk failure by using robust dasiamechanical backhaulpsila for connectivity, and by using low-cost and reliable kiosk controllers to support services delivered from one or more recycled PCs. KioskNet also addresses related issues such as security, user management, and log collection. In this paper, we describe the KioskNet system and outline its hardware, software, and security architectures. We describe a pilot deployment, and how we used lessons from this deployment to re-design our initial prototype.	e-governance;internet access;prototype;rural internet;software deployment	Shimin Guo;Mohammad Derakhshani;Hossein Falaki;Usman Ismail;Rowena Luk;Earl A. Oliver;Sumair Ur Rahman;Aaditeshwar Seth;Matei A. Zaharia;Srinivasan Keshav	2007	2007 International Conference on Information and Communication Technologies and Development	10.1016/j.comnet.2010.08.001	simulation;developing country;telecommunications;computer science;architecture;operating system;prototype;implementation;computer security;computer network	Robotics	-15.478232922689694	96.6037532046901	5470
310ba40ca10817f6aff62c795eb8dcb9866a1c39	total sets and objects in domain theory	domain theory	Abstract   Berger, U., Total sets and objects in domain theory, Annals of Pure and Applied Logic 60 (1993) 91-117. Total sets and objects generalizing total functions are introduced into the theory of effective domains of Scott and Ersov. Using these notions Kreisel's Density Theorem and the Theorem of Kreisel-Lacombe-Shoenfield are generalized. As an immediate consequence we obtain the well-known continuity of computable functions on the constructive reals as well as a domain-theoretic characterization of the Heriditarily Effective Operations.	domain theory	Ulrich Berger	1993	Ann. Pure Appl. Logic	10.1016/0168-0072(93)90038-F	combinatorics;mathematical analysis;discrete mathematics;topology;domain theory;mathematics;programming language;algorithm;algebra	Logic	-8.64999813438068	12.552408823575254	5472
06271f8efcc32970b1898db3402b9d44e675f306	architecture for symbolic object warehouse		Much information stored in current databases is not always present at necessary different levels of detail or granularity for Decision-Making Processes (DMP). Some organizations have implemented the use of central database Data Warehouse (DW) where information performs analysis tasks. This fact depends on the Information Systems (IS) maturity, the type of informational requirements or necessities the organizational structure and business own characteristic. A further important point is the intrinsic structure of complex data; nowadays it is very common to work with complex data, due to syntactic or semantic aspects and the processing type (Darmont et al., 2006). Therefore, we must design systems, which can to maintain data complexity to improve the DMP. OLAP systems solve the problem of present different aggregation levels and visualization for multidimensional data through cube’s paradigm. The classical data analysis techniques (factorial analysis, regression, dispersion, etc.) are applied to individuals (tuples or individuals in transactional databases). The classic analysis objects are not expressive enough to represent tuples, which contain distributions, logic rules, multivaluate attributes, and intervals. Also, they must be able to respect their internal variation and taxonomy maintaining the dualism between individual and class. Consequently, we need a new data type holding these characteristics. This is just the mathematical concept model introduced by Diday called Symbolic Object (SO). SO allows modeling physic entities or real world concepts. The former are the tuples stored in transactional databases and the latter are high entities obtained from expert’s analysis, automatic classification or some particular aggregation taken from analysis units (Bock & Diday, 2000). The SO concept helps construct the DW and it is an important development for Data Mining (DM): for the manipulation and analysis of aggregated information (Nigro & González Císaro, 2005). According to Calvanese, data integration is a central problem in the design of DWs and Decision Support Systems (Calvanese, 2003; Cali, et al., 2003); we make the architecture for Symbolic Object Warehouse construction with integrative goal. Also, it combines with Data Analysis tasks or DM. This paper is presented as follows: First, Background: DW concepts are introduced. Second, Main Focus divided into: SOs Basic Concepts, Construing SOs and Architecture. Third, Future Trends, Conclusions, References and Key Terms.	apple sos;capability maturity model;data mining;database;dreamwidth;entity;factor analysis;information systems;online analytical processing;programming paradigm;requirement;taxonomy (general)	Sandra Elizabeth González Císaro;Héctor Oscar Nigro	2009			online analytical processing;data type;tuple;data integration;theoretical computer science;decision support system;information system;data warehouse;data analysis;computer science	DB	-32.704877908361674	10.875400222643103	5473
c4f0c9c9fb069206071d8f1a377ee6eab596ec82	enhancing robust header compression over ieee 802 networks	encapsulation;optimal solution;ieee 802 network;communication system;rtp encapsulation robust header compression rohc packet ieee 802 network multimedia application voice over internet protocol voip 3g radio links real time transport protocol;transport protocols 3g mobile communication internet telephony multimedia communication radio links;robust header compression;3g radio links;multimedia application;rtp encapsulation;indexing terms;robustness ethernet networks wireless lan encapsulation bandwidth negative feedback access protocols radio link computer networks wireless personal area networks;internet telephony;computer networks;transport protocols;radio link;3g mobile communication;voice over internet protocol;wireless personal area networks;negative feedback;multimedia communication;access protocols;bandwidth;robustness;real time transport protocol;wireless lan;ethernet networks;computer network performance;rohc packet;voip;radio links	The use of ROHC (robust header compression) for multimedia applications as VoIP over 3G radio links has shown a good performance in reducing the overhead of RTP encapsulation. Some efforts have been made to use ROHC over IEEE 802 links, but the solution needs to be optimized. The use of ROHC out of its operating assumptions adds some additional problems. These problems occur when the padding is used within small packets. This paper introduces an optimized solution to send ROHC packets over IEEE 802 links	encapsulation (networking);framing (world wide web);overhead (computing);performance;point-to-point protocol;point-to-point (telecommunications);robust header compression	Samiha Ayed;Ana Carolina Minaburo;Laurent Toutain	2006	2006 IEEE International Conference on Wireless and Mobile Computing, Networking and Communications	10.1109/WIMOB.2006.1696371	real-time computing;telecommunications;computer science;voice over ip;computer network	Embedded	-4.572278191281853	94.27627332797715	5474
4e8d90f890ed5c325ed83b151f959ce643e98c5d	unconditionally secure key distribution in higher dimensions by depolarization	telecommunication security error correction matrix algebra quantum cryptography quantum entanglement;key management;gestion de claves;provable security;depolarization;entangled qubits;principe pauli;quantum physics law;entanglement purification;unconditional security;unconditional security depolarization entanglement purification local quantum operation pauli error phase error correction quantum key distribution shor preskill proof two way classical communication;physics quantum entanglement error correction error analysis resists purification information security art privacy councils;securite;information transmission;quantum information;correction erreur;eavesdropping attack;shor preskill proof;cryptanalyse;quantum key distribution;despolarizacion;shor preskill type argument;principio pauli;matrix algebra;indexing terms;phase error correction;cryptanalysis;two way classical communication;depolarisation;criptoanalisis;distribution cle;communication quantique;matrices;quantum information carrier;pauli principle;quantum key distribution unconditional secure key distribution depolarization prepare measure scheme n dimensional quantum particle eavesdropping attack pauli error quantum information carrier shor preskill type argument quantum physics law entangled qubits entanglement purification phase error correction;quantum physics;error correction;n dimensional quantum particle;safety;telecommunication security;higher dimensions;quantum communication;comunicacion cuantica;transmision informacion;correccion error;transmission information;quantum cryptography;local quantum operation;quantum entanglement;seguridad;prepare measure scheme;gestion cle;article;unconditional secure key distribution;pauli error;key distribution	This paper presents a prepare-and-measure scheme using N-dimensional quantum particles as information carriers where N is a prime power. One of the key ingredients used to resist eavesdropping in this scheme is to depolarize all Pauli errors introduced to the quantum information carriers. Using the Shor-Preskill-type argument, we prove that this scheme is unconditionally secure against all attacks allowed by the laws of quantum physics. For N=2n>2, each information carrier can be replaced by n entangled qubits. In this case, there is a family of eavesdropping attacks on which no unentangled-qubit-based prepare-and-measure (PM) quantum key distribution scheme known to date can generate a provably secure key. In contrast, under the same family of attacks, our entangled-qubit-based scheme remains secure whenever 2nges4. This demonstrates the advantage of using entangled particles as information carriers and of using depolarization of Pauli errors to combat eavesdropping attacks more drastic than those that can be handled by unentangled-qubit-based prepare-and-measure schemes	provable security;quantum entanglement;quantum information;quantum key distribution;quantum mechanics;qubit;shor's algorithm	Hoi Fung Chau	2005	IEEE Transactions on Information Theory	10.1109/TIT.2005.844076	depolarization;quantum information;theoretical computer science;mathematics	Crypto	-41.807270325256496	81.97628893763812	5475
07d3e849a30e14edb80652d1d5127805f4ef8329	emulation von netzwerkverhalten für skalierbarkeitstests ip-basierter audio-video-kommunikationssysteme			emulator	Robert Lübke	2015				Theory	-97.61858036160949	25.271279086765865	5479
925922269cfd0d4d46e94e61f1a2a3c0bcf3395c	privacy and consent in pervasive networks	legislation	Pervasive networks and location based systems have the potential to provide many new services. However the user of these services often has to provide personal information to allow the service to operate effectively. This article considers the problem of protecting personal information in this environment, and reports on the legislative and technical efforts being made to protect user privacy.	pervasive informatics;privacy	Nazir A. Malik;Allan Tomlinson	2009	Inf. Sec. Techn. Report	10.1016/j.istr.2009.10.002	privacy policy;computer science;data mining;internet privacy;computer security	HCI	-45.51249712151697	61.41973860791393	5484
d97be280211ac0903d88ac7586458c09841ce2ad	near: a novel energy aware replacement policy for stt-mram llcs		As the technology node shrinks, leakage power becomes a bottleneck for processor performance and memory capacity scalings. Spin Torque Transfer Magnetic Random Access Memory (STT-MRAM) has negligible leakage power, fast access speed, high integration density and non-volatility. Therefore, it is a promising candidate for the last level cache design. However, it suffers from high write energy and slow write speed. In the paper, we observe that the traditional cache replacement policy is not optimal when applied to STT-MRAM from the energy consumption perspective. So we propose a novel write energy aware cache replacement policy, which utilizes a MinHash function to identify the similarities between the cache line to be written back and candidates for the replacement. The cache line with the highest similarity is chosen as the victim. In addition, we propose a new metric for cache replacement considering both performance and write energy to improve the replacement policy further. The experimental results show that our proposed policy can reduce write energy by 33.6% on average compared to the state-of-the-art Least Recently Used (LRU) replacement policy with only 0.5% performance penalty and negligible hardware overhead.	bottleneck (engineering);cpu cache;magnetoresistive random-access memory;minhash;non-volatile memory;overhead (computing);random access;semiconductor device fabrication;spectral leakage;volatility	Chen Liu;Yuanqing Cheng;Ying Wang;Youguang Zhang	2018	2018 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2018.8350975	parallel computing;electronic engineering;cache;energy consumption;minhash;cpu cache;computer science;magnetoresistive random-access memory;cache algorithms;spin-transfer torque;bottleneck	Arch	-7.316356912821046	55.09279732356623	5485
6d952320955714e828f4b764a085f3fc96cf713e	exploiting non-intrusive monitoring in real-time embedded operating systems		Monitoring in embedded system software can have several uses, ranging from system characterization to run-time verification (RV). Traditional monitoring techniques require code instrumentation, imposing an overhead on system execution both in performance and timeliness. In real-time systems this is exarcebated by the need of new worst-case execution time estimation and schedulability analysis. In this paper we discuss how monitoring can be exploited in real-time embedded operating systems, via non-intrusive mechanisms.	best, worst and average case;embedded operating system;embedded system;overhead (computing);real-time clock;real-time computing;real-time operating system;real-time transcription;run time (program lifecycle phase);scheduling analysis real-time systems;worst-case execution time	Ricardo Pinto;José Rufino	2014			software;real-time computing;ranging;computer science;instrumentation (computer programming);embedded operating system	Embedded	-23.645820596981043	36.86321282882472	5487
34377e5b36ff8504bfe02df298d3c097cfa3ceda	an experimental evaluation and analysis of database cracking	sorting;adaptive indexing;database cracking;multi threaded algorithms	Database cracking has been an area of active research in recent years. The core idea of database cracking is to create indexes adaptively and incrementally as a side product of query processing. Several works have proposed different cracking techniques for different aspects including updates, tuple reconstruction, convergence, concurrency control, and robustness. Our 2014 VLDB paper “The Uncracked Pieces in Database Cracking” (PVLDB 7:97–108, 2013/VLDB 2014) was the first comparative study of these different methods by an independent group. In this article, we extend our published experimental study on database cracking and bring it to an up-to-date state. Our goal is to critically review several aspects, identify the potential, and propose promising directions in database cracking. With this study, we hope to expand the scope of database cracking and possibly leverage cracking in database engines other than MonetDB. We repeat several prior database cracking works including the core cracking algorithms as well as three other works on convergence (hybrid cracking), tuple reconstruction (sideways cracking), and robustness (stochastic cracking), respectively. Additionally to our conference paper, we now also look at a recently published study about CPU efficiency (predication cracking). We evaluate these works and show possible directions to do even better. As a further extension, we evaluate the whole class of parallel cracking algorithms that were proposed in three recent works. Altogether, in this work we revisit 8 papers on database cracking and evaluate in total 18 cracking methods, 6 sorting algorithms, and 3 full index structures. Additionally, we test cracking under a variety of experimental settings, including high selectivity (Low selectivity means that many entries qualify. Consequently, a high selectivity means, that only few entries qualify) queries, low selectivity queries, varying selectivity, and multiple query access patterns. Finally, we compare cracking against different sorting algorithms as well as against different main memory optimized indexes, including the recently proposed adaptive radix tree (ART). Our results show that: (1) the previously proposed cracking algorithms are repeatable, (2) there is still enough room to significantly improve the previously proposed cracking algorithms, (3) parallelizing cracking algorithms efficiently is a hard task, (4) cracking depends heavily on query selectivity, (5) cracking needs to catch up with modern indexing trends, and (6) different indexing algorithms have different indexing signatures.	central processing unit;computer data storage;concurrency (computer science);concurrency control;database;experiment;monetdb;parallel computing;password cracking;radix tree;robustness (computer science);selectivity (electronic);sorting algorithm;type signature;vldb;vergence	Felix Schuhknecht;Alekh Jindal;Jens Dittrich	2015	The VLDB Journal	10.1007/s00778-015-0397-y	computer science;sorting;data mining;database	DB	-20.07773356880834	49.377859639336094	5489
740eee5e7ae54231e5260e429ac5f3b9d75bed08	novel gossip-based multicast protocol with no global view information	distributed application;causal order distributed systems multicast scalability reliability;computers;distributed system;multicast protocols broadcasting multicast algorithms scalability computer science computer network reliability telecommunication network reliability application software buildings peer to peer computing;protocols;reliability;application level gossip based broadcast protocols;reliable propagation;causal ordering;argon;multicast protocols;reliable propagation gossip based multicast protocol application level gossip based broadcast protocols;gossip based multicast protocol;scalability;probabilistic logic;distributed systems;integrated circuits;multicast;gallium;causal order	Application-level gossip-based broadcast protocols are gaining significant popularity as an interesting alternative to deterministic broadcast algorithms for reliable propagation by providing an attractive tradeoff between reliability and scalability. In addition, many distributed applications require scalable, reliable and causally ordered delivery of broadcast messages to a large number of processes. However, despite the importance of the ordering guarantees, there exist only a few works to consider this requirement. Most early versions of these protocols rely on the assumption that each process knows every other process, or use virtual synchrony probabilistically by composing the gossip-style dissemination based on partially randomized individual views and K-committee protocol with a deterministic reliability. This paper presents an efficient gossip-based multicast protocol to guarantee causally-ordered delivery semantics based on the local view of every individual member consisting of a subset of members continuously changing, whose size never excesses a determined threshold, without virtual synchrony and K-committee.	distributed computing;existential quantification;gossip protocol;multicast;randomized algorithm;scalability;software propagation;virtual synchrony	Chayoung Kim;Jinho Ahn	2008	2008 Second International Conference on Future Generation Communication and Networking	10.1109/FGCN.2008.47	gossip protocol;atomic broadcast;computer science;theoretical computer science;distributed computing;computer network	HPC	-8.017655070712342	72.129143806124	5507
04618407340c72d29a4de5e542f6f464aff46cbe	a key management scheme for secure communications of advanced metering infrastructure in smart grid	advanced metering infrastructure (ami);key management scheme (kms);smart grid;transmission mode;cryptography	Advanced metering infrastructure (AMI) is an important component of the smart grid. The cyber security should be considered prior to the AMI system applications. To ensure confidentiality and integrality, a key management scheme (KMS) for a large amount of smart meters (SMs) and devices is required, which is not a properly solved problem until now. Compared with other systems, there are three specific features of AMI that should be carefully considered, including hybrid transmission modes of messages, storage and computation constraints of SMs, and unfixed participators in demand response (DR) projects. In order to deal with security requirements and considering the distinctive features, a novel KMS is proposed. First, the key management framework of an AMI system is constructed based on the key graph. Furthermore, three different key management processes are designed to deal with the hybrid transmission modes, including key management for unicast, broadcast, and multicast modes. Relatively simple cryptographic algorithms are chosen for key generation and refreshing policies due to the storage and computation constraints of SMs. Specific key refreshing policies are designed since the participators in a certain DR project are not fixed. Finally, the security and performance of the KMS are analyzed. According to the results, the proposed scheme is a possible solution for AMI systems.	algorithm;computation;computer security;confidentiality;cryptography;key generation;key management;multicast;requirement;smart meter;unicast	Jinshan Chen;Nian Liu;Wenxia Liu;Hong Luo	2011	IEEE Transactions on Industrial Electronics	10.1007/978-3-642-23220-6_55	embedded system;engineering;cryptography;mathematics;distributed computing;smart grid;computer security;statistics	Security	-48.6734407125951	79.05047407883094	5510
5233a606b9cbfecf99fa73e3526a652aedde659a	revisiting call-by-value bohm trees in light of their taylor expansion		The call-by-value λ-calculus can be endowed with permutation rules, arising from linear logic proof-nets, having the advantage of unblocking some redexes that otherwise get stuck during the reduction. We show that such an extension allows to define a satisfying notion of Böhm(-like) tree and a theory of program approximation in the call-byvalue setting. We prove that all λ-terms having the same Böhm tree are observationally equivalent, and characterize those Böhm-like trees arising as actual Böhm trees of λ-terms. We also compare this approach with Ehrhard’s theory of program approximation based on the Taylor expansion of λ-terms, translating each λ-term into a possibly infinite set of so-called resource terms. We provide sufficient and necessary conditions for a set of resource terms in order to be the Taylor expansion of a λ-term. Finally, we show that the normal form of the Taylor expansion of a λ-term can be computed by performing a normalized Taylor expansion of its Böhm tree. From this it follows that two λ-terms have the same Böhm tree if and only if the normal forms of their Taylor expansions coincide. We are honoured to dedicate this article to Corrado Böhm, whose brilliant pioneering work has been an inspiration to us all.	aharonov–bohm effect;approximation;böhm tree;database normalization;decision tree;denotational semantics;lambda calculus;linear logic;observational equivalence;operational semantics;plotkin bound;reduction (complexity);theory	Emma Kerinec;Giulio Manzonetto;Michele Pagani	2018	CoRR		permutation;mathematics;discrete mathematics;infinite set;evaluation strategy;lambda;lambda calculus;if and only if;taylor series;linear logic	Logic	-8.894232708881757	14.862672144463948	5513
6a1be2bb2928d607c8c0461a07c09382407bdaf1	tapestry: a fault-tolerant wide-area application infrastructure	location service;overlay routing;fault tolerant;content addressable network;satisfiability;large scale;design and implementation;overlay network;local search	"""This abstract describes progress of the Tapeswy wide-area overlay network infrastructure, current experimental results on muting efficiency, locality and fanh-tolerante, along with the design and implementation of a wide-area multicast system called Bayeux. Tapestry is an infrastructure approach to solving several key challenges in building novel large-scale network applications, including scalability, fault-tolerance, and adaptability in the wide-area. Tapestry solves these issues as an overlay network application infrastructure. It provides network location services in the form of mapping globally unique object IDs to server locations, and message muting services given the unique ID o f the destination node. Identifiers are randomly assigned 160 bit strings. Messages are routed using a hop-by-hop incremental m~rehing of node IDs to the destination node. Objects are dislributed acruss random embedded trees, and a fuuction maps each object-ID to the node-ID of its """"root node.'"""" Insertion involves usin 8 inter-node muting pointers to find the root, while placing pointers to the object on intervening hops. Queries also use routing pointers to find the destination root, but terminate when they intersect hops with the desired object pointers. For each possible suffix length, each node keeps muting links m its """"closest"""" neighbors with the common suffix that differs in the next digit. This results in good locality and distribution, and mutes with at most L o ~ ( # of Nodes) hops, where b is the base. This scheme simplifies corrupted pointer detection, scales via random object disIributinn, and has natural redundancy resulting in resilience against intermediate node failures and small network partitions. Compared to gimilar overlay systems like C.hord [3] and Contentaddressable Networks [2], Tapestry performs similarly or better in the key metrics of routing table size and expected logical hops hetween endpoints, shown in Table I. The key difference between Tapestry and CAN and Chord is that Tapeslry builds in an explicit correlalion between overlay topology distance and physical network latency. In both CAN and Chord, it is possible to take one overlay hop to a neighbor node, and traverse an unbounded nnmhar of hops in the physical network (both systems compensate by using heurist ic). In contrast, Tapestry's insertion algorithms select for each oveflsy hop the nearest nodes in network distance which satisfy the criteria. As a result, overlay distances correspond to physical network distances, and local searches do not incur long network travcrsals. In addition, Tapcslry's hierarchical cache of obj e t pointers makes a probabilistic argument, that a client searching for local copies of an object finds the nearest copy, and traverses a distance linearly proportional to its distance to that copy. In addition to providing efficient overlay routing and object location, Tapestry provides redundant mechanisms that leverage the increasing availability of resources such as computational power, magnetic storage and network bandwidth. In object location, redundancy occurs when incoming objects are hashed to produce multiple unique names, and references to the object are inserted Metr ic Tapestry i Chord C.A.N. J geyParame ter~ BaseB [ N o n e Dimension D [ Logic,,t Hot,, Lo~,(N) Lo92(. ~ O(d , N'') R o ~ s stoJe b, Lo~(N) Lo~,(N) O(d~"""	algorithm;chord diagram;embedded system;fault tolerance;hash function;hop-by-hop transport;insertion sort;locality of reference;magnetic storage;memory hierarchy;multicast;one-hot;overlay network;pointer (computer programming);randomness;routing table;scalability;server (computing);traverse;terminate (software);tree (data structure)	Ben Y. Zhao;John Kubiatowicz;Anthony D. Joseph	2002	Computer Communication Review	10.1145/510726.510755	fault tolerance;content addressable network;real-time computing;overlay network;computer science;local search;theoretical computer science;operating system;distributed computing;computer security;computer network;satisfiability	Networks	-11.094272482965447	73.44488875083539	5515
1a6d98d13ec3c366002aeddd02ac5d59f69a270d	buyer seller watermarking protocols issues and challenges - a survey	trust;watermarking;e commerce;fingerprinting;security;privacy;buyer seller watermarking	Advancements in computing and networking technologies have opened gateways for the content owners to produce and distribute their digital contents (e.g., audio/video/images) in a convenient and affordable manner. Despite all the advantages promised by the advancement in digital technology and widespread use of Internet, piracy of content is still big concern. Digital content can be easily copied without any quality loss. Content creators and owners are concerned about the consequences of illegal copying and distribution on a massive scale like loss of capital. As digital data can be duplicated and edited with great ease, this has led to Digital Rights Management (DRM) systems that can address the issues related to privacy and security of the digital contents. Digital watermarking is a promising technology employed by various DRM systems to achieve rights management. Buyer-Seller Watermarking (BSW) protocol integrates encryption, with digital watermarking and other techniques to ensure rights protection & security for seller as well as the buyer of the digital content. BSW protocols support copyright protection, piracy tracing, and privacy protection. Various approaches have been proposed for BSW protocols. In this context, the main contributions of this paper to the literature on BSW protocols are threefold: (i) it identifies the challenges in designing a BSW protocol; (ii) provides the taxonomy of existing approaches; and (iii) describes the strengths and weaknesses of the presented approaches by comparison and some open issues are highlighted.		Abid Khan;Farhana Jabeen;Farah Naz;Sabah Suhail;Mansoor Ahmed;Sarfraz Nawaz	2016	J. Network and Computer Applications	10.1016/j.jnca.2016.08.026	e-commerce;fingerprint;digital watermarking alliance;telecommunications;digital watermarking;computer science;information security;internet privacy;trustworthy computing;privacy;world wide web;computer security;computer network	Theory	-46.73416673021846	67.13128747400586	5521
8dd4b9abd53d8f039c6b26f272a08cc6d4be7ac5	formal specification of snmpv3 entities using action semantics	formal specification	This work presents a formal description of the structure and behaviour of SNMPv3 entities.          IETF documents describe the semantics of the Simple Network Management Protocol version          3 in an almost completely informal way. Our formal description is given using action          semantics, a completely formal yet verbose framework for the specification of programming          concepts. The purpose of our description is to specify management entities without          ambiguities, contributing to a better understanding of the NMPv3 framework that may          lead to automatic implementation and verification of agents and manager applications. Copyright          © 2004 John Wiley & Sons, Ltd.       	action semantics;entity;formal specification	Elias Procópio Duarte;Martin A. Musicante;Diógenes Cogo Furlan	2004	Int. Journal of Network Management	10.1145/1024511.1024513	formal system;formal methods;formal semantics;action semantics;formal verification;computer science;theoretical computer science;formal semantics;formal specification;database;programming language;operational semantics	DB	-42.82817740371033	27.930737637108752	5526
40ec5c090a274e4630adace9e031cd2a47d2b7da	concept integration as an approach to information systems design			information system;systems design	Gilles Falquet;Jacques Guyot;Marc Junet;Michel Léonard;R. Bursens;P. Crausaz;Ian Prince	1988			information architecture;system integration;systems engineering;information integration;systems design;information system;information engineering;management information systems;design technology;computer science	DB	-59.62388581015293	13.039590143275403	5528
748e7ec47f8ca1889c0839367ab78e042d5d0eea	a dynamic-priority based approach to fixing inconsistent feature models	feature modeling;feature model;inconsistency fixing;priority	In feature models' construction, one basic task is to ensure the consistency of feature models, which often involves detecting and fixing of inconsistencies in feature models. Several approaches have been proposed to detect inconsistencies, but few focus on the problem of fixing inconsistent feature models. In this paper, we propose a dynamic-priority based approach to fixing inconsistent feature models, with the purpose of helping domain analysts find solutions to inconsistencies efficiently. The basic idea of our approach is to first recommend a solution automatically, then gradually reach the desirable solution by dynamically adjusting priorities of constraints. To this end, we adopt the constraint hierarchy theory to express the degree of domain analysts' confidence on constraints (i.e. the priorities of constraints) and resolve inconsistencies among constraints. Two case studies have been conducted to demonstrate the usability and scalability of our approach.	constraint programming;feature model;hierarchy theory;scalability;sensor;solver;unified modeling language;usability;x11 color names	Bo Wang;Yingfei Xiong;Zhenjiang Hu;Haiyan Zhao;Wei Zhang;Hong Mei	2010		10.1007/978-3-642-16145-2_13	engineering;software engineering;machine learning;data mining;algorithm;feature model	SE	-58.00809442861093	30.47438579104042	5529
2d6bd939d7149ae6b8e583ede1015e9693cd094b	constructing an optimal spanning tree over a hybrid network with sdn and legacy switches	computers;control systems;protocols;measurement;network topology;storms;control systems measurement storms network topology protocols computers throughput;proceedings paper;floodlight sdn controller sdn hybrid network topology legacy switches optimal spanning tree ieee 802 11d distributed spanning tree protocol legacy network packet broadcast storm problem avoidance sdn controller estinet openflow network simulator;wireless lan broadcast communication protocols software defined networking switching networks trees mathematics;throughput	In this paper, we design and implement a scheme to build an optimal spanning tree over a hybrid network composed of SDN and legacy switches. SDN is an emerging network technology and gradually gaining adoption worldwide. However, during the technology transition period, which may last several years from now on, inevitably SDN switches and legacy switches will need to coexist in a network. In a legacy network with loops, the IEEE 802.11D distributed spanning tree protocol is used to construct a spanning tree to avoid the packet broadcast storm problem. In an SDN network, the SDN controller can use the collected global knowledge of the network topology to centrally construct a spanning tree to solve this problem. However, so far there is no scheme to integrate together the spanning trees separately built in multiple isolated legacy networks and isolated SDN networks to build an optimal spanning tree across the hybrid network. Our scheme presented in this paper is the first scheme designed and implemented for this purpose. In this paper, we used the EstiNet OpenFlow network simulator and emulator to simulate hybrid networks. Our scheme is implemented as a module of the Floodlight SDN controller to construct an optimal spanning tree over the simulated hybrid networks. Our simulation results show that our scheme can correctly generate an optimal spanning tree based on two performance metrics: the average path latency and the average path throughput of a spanning tree.	broadcast radiation;coexist (image);emulator;file spanning;network packet;network switch;network topology;openflow;simulation;software-defined networking;spanning tree protocol;throughput	Shie-Yuan Wang;Chia-Cheng Wu;Chih-Liang Chou	2015	2015 IEEE Symposium on Computers and Communication (ISCC)	10.1109/ISCC.2015.7405564	communications protocol;throughput;computer science;control system;theoretical computer science;distributed computing;distributed minimum spanning tree;storm;network topology;measurement;spanning tree protocol;computer network	Metrics	-11.915145002897463	83.27126646159644	5534
65c09ea2bdd65cad667f6720d6f261cdf92a5ff5	distributed atm switching topologies in tree-structured networks	cable television;teletrafic;commutation telecommunication;modelizacion;conmutacion telecomunicacion;networks;mode transfert asynchrone;amplifiers;telecommunication network;modelisation;network topology;teletrafico;telecommunication switching;red telecomunicacion;tree structure;reseau telecommunication;teletraffic;teledistribution;broadband access;switches;modeling;simulation model;teledistribucion;asynchronous transfer mode	HFC networks, today mostly used to broadcast traditional cable TV programs, will possibly play a major role in future broadband access network development. The structure of HFC networks is briefly described and bottlenecks in case of high load are identified. To improve performance and eliminate the bottleneck, the approach presented in this paper suggests the replacement of passive network elements like fiber nodes and signal amplifiers by ATM switches. This approach has been evaluated with respect to various network topologies and user scenarios. A simulation model used for the evaluation is presented within this paper and simulation results are discussed.	atm turbo;access network;amplifier;hybrid fibre-coaxial;internet access;network switch;network topology;scenario (computing);simulation	Urs Thürmann;Martina Zitterbart;Thomas Meuser	1998		10.1117/12.321884	electronic engineering;telecommunications;engineering;computer network	Networks	-8.599698994996592	85.33837459803665	5535
48f7eb1038cd80d00ff6b91cb297556c7ed58706	towards a tricksy group shilling attack model against recommender systems	group shilling attack;recommender systems;shilling attack	The robustness of recommender systems has drawn recently more and more attention of both industry and academia. Although a multitude of studies have been devoted to shilling attack modeling and detection, few of them focus on group shilling attack. The attackers in a shilling group work together to manipulate the output of the recom- mender system. Meanwhile, since the rating profiles in a shilling group are carefully designed, it is hard to detect them by traditional meth- ods. This paper presents a generative model to create shilling group in which every pair of attackers has high diversity. In particular, both strict and loose versions of group shilling attack generation algorithm are pro- posed. Experimental results on MovieLens data set demonstrate that the shilling group generated by the our model can not only exert large nega- tive effect to recommender systems, but also avoid the detection by the traditional methods.	attack model;recommender system	Youquan Wang;Zhiang Wu;Jie Cao;Changjian Fang	2012		10.1007/978-3-642-35527-1_56	simulation;data mining;computer security	Crypto	-53.35641219537054	65.05205664546482	5536
4772b4eb818da8439b10790d33ff866bcfb1ffad	on predicting the time taken to correct bug reports in open source projects	empirical study;atmospheric measurements;software maintenance;particle measurements;maintenance engineering;open source projects;public domain software;accuracy;computational modeling;predictive models computer science fault diagnosis linux testing particle measurements feedback software maintenance lab on a chip open source software;linear model;bug report correction time prediction open source projects ubuntu linux corrective maintenance process;corrective maintenance process;linux;predictive models;program debugging;bug report correction time prediction;correlation;software maintenance linux program debugging public domain software;ubuntu;open source	Existing studies on the maintenance of open source projects focus primarily on the analyses of the overall maintenance of the projects and less on specific categories like the corrective maintenance. This paper presents results from an empirical study of bug reports from an open source project, identifies user participation in the corrective maintenance process through bug reports, and constructs a model to predict the corrective maintenance effort for the project in terms of the time taken to correct faults. Our study focuses on 72482 bug reports from over nine releases of Ubuntu, a popular Linux distribution. We present three main results 1) 95% of the bug reports are corrected by people participating in groups of size ranging from 1 to 8 people, 2) there is a strong linear relationship (about 92%) between the the number of people participating in a bug report and the time taken to correct it, 3) a linear model can be used to predict the time taken to correct bug reports.	bug tracking system;coefficient;linear model;linux;open-source software;pa-risc;settling time;ubuntu version history	Prasanth Anbalagan;Mladen A. Vouk	2009	2009 IEEE International Conference on Software Maintenance	10.1109/ICSM.2009.5306337	maintenance engineering;real-time computing;computer science;engineering;operating system;software engineering;linear model;database;accuracy and precision;predictive modelling;programming language;empirical research;software maintenance;computational model;public domain software;correlation;linux kernel	SE	-63.32882580253083	35.93352808820242	5539
47263c4452be87063ff7b154f2929277db749c33	an online risk management strategy for voip enterprise infrastructures	risk management;security management;voip services;sip protocol;network management;security;risk mitigation;ip telephony	Telephony over IP has been widely deployed, supported by the standardization of VoIP signalling and media transfer protocols. This deployment has also led to the emergence of several security threats, including attacks inherited from the IP layer and attacks specific to the application layer. A large variety of security mechanisms has been proposed for addressing them, but these mechanisms may seriously degrade such a critical service. We propose in this paper an online risk management strategy for protecting VoIP infrastructures. The objective is to minimize the network exposure to security attacks while maintaining the quality of service, through the dynamic application of countermeasures. We describe our approach from the formalization of a dedicated risk model to its proof-of-concept implementation into an Asterisk VoIP server. We detail a portfolio of countermeasures and evaluate the performance of our solution with respect to different criteria, including the number of countermeasures, the risk threshold and the size of attack signatures.	antivirus software;captcha;configuration management;countermeasure (computer);digital signature;emergence;experiment;financial risk modeling;hoare logic;interaction;potentiometer;protection mechanism;quality of service;risk management;semantics (computer science);server (computing);simulation;software deployment;turing test;web strategy	Oussema Dabbebi;Remi Badonnel;Olivier Festor	2013	Journal of Network and Systems Management	10.1007/s10922-013-9282-4	network management;security management;risk management;computer science;voice over ip;session initiation protocol;internet privacy;computer security;computer network	Security	-50.149429197481346	57.99903962531558	5540
e95097904727c6dbd4d4d722e01a2ff2119bbaa8	efficiency of the conflict-preserving serializable scheduler for object-oriented databases	conflict preserving serializable scheduler;evaluation performance;fijacion;verrouillage;performance evaluation;evaluacion prestacion;locking;simultaneidad informatica;object oriented database system;concurrency;object oriented;scheduling;concurrency control;modele simulation;oriente objet;ordonamiento;controle concurrence;object oriented database;control concurrencia;systeme gestion base donnee;modelo simulacion;sistema gestion base datos;simultaneite informatique;orientado objeto;database management system;simulation model;ordonnancement;two phase locking protocol	Abstract#R##N##R##N#This paper discusses the concurrency control of the Object-Oriented Database System (OODBS), and presents a simulation model for the following basic elements which should be considered from the viewpoint of the data structure: (1) the entity should be modeled, based on the concept of object; (2) the definition of the attribute and the method should be inherited based on the class hierarchy; and (3) the composite object should be realized. Based on this model, performance evaluations are conducted on Two-Phase Locking protocol (2PL) and Conflict-Preserving Serializable scheduler (CPS). From investigations, so far, on the flat structure, CPS has shown better performance than 2PL. As a result of this study, however, it is shown that in the prominent data structure of OODBS, there exist access conflicts where the optimistic concurrency control such as CPS works effectively, as well as access conflicts where the pessimistic concurrency control such as 2PL works effectively.#R##N##R##N##R##N##R##N#Accordingly, this paper proposed a new scheduler called Delayed CPS (DCPS), which realizes an adequate control according to the type of access conflict. It is seen that CPS has good concurrency performance.	database;scheduling (computing);serializability	Shinichi Taniguchi;Shojiro Nishio;Nobuya Kubo	1995	Systems and Computers in Japan	10.1002/scj.4690260704	real-time computing;concurrency;computer science;two-phase locking;operating system;concurrency control;simulation modeling;database;distributed computing;programming language;object-oriented programming;scheduling	DB	-20.476283583710874	46.43090014549357	5544
c5975ffb95522fab5b5d02b49d76a0d5a56bf6ef	clickstream log acquisition with web farming	customer behavior;portals;electronic commerce;electronic commerce web sites portals customer profiles;customer relations;web server log clickstream log acquisition web farming customer interaction e business web site portal customer behavior customer profile personalized service;web sites;information gain;semi supervised document clustering;em;customer profiles;web server web mining portals customer profiles performance analysis data mining data engineering companies protocols html	Collecting customer interaction data on the e-business websites and portals will help to figure out customer behavior and build customer profile, and then perform personalized services. Traditional Web server log is hard to be associated with specific customer and impossible to log the complete actions and movements of customers across web-sites. Collecting clickstream log at the application layer with Web farming technology will help to seamlessly integrate Web usage data with other customer-related data. This model can be developed as a common plugin for most existing e-business websites and portals.	clickstream;electronic business;personalization;plug-in (computing);portals;server (computing);server log;usage data;web server	Jia Hu;Ning Zhong	2005	The 2005 IEEE/WIC/ACM International Conference on Web Intelligence (WI'05)	10.1109/WI.2005.47	e-commerce;web service;customer to customer;voice of the customer;customer relationship management;web mining;web development;clickstream;web standards;computer science;customer reference program;web log analysis software;web page;database;em;customer intelligence;kullback–leibler divergence;world wide web;consumer behaviour	DB	-39.03976185611334	10.138744127679741	5551
a6a5aaf85d2dde68728a8d0d170016b076b7e09f	a cross-layer key establishment scheme in wireless mesh networks	generation;technology;会议论文;secret;science technology;coding;wireless mesh networks;channel phase;computer science;computer science information systems;cross layer;channels;computer science theory methods;key establishment	Cryptographic keys are necessary to secure communications among mesh clients in wireless mesh networks. Traditional key establishment schemes are implemented at higher layers, and the security of most such designs relies on the complexity of computational problems. Extracting cryptographic keys at the physical layer is a promising approach with information-theoretical security. But due to the nature of communications at the physical layer, none of the existing designs supports key establishment if communicating parties are out of each other's radio range, and all schemes are insecure against man-in-the-middle attacks. This paper presents a cross-layer key establishment scheme where the established key is determined by two partial keys: one extracted at the physical layer and the other generated at higher layers. The analysis shows that the proposed cross-layer key establishment scheme not only eliminates the aforementioned shortcomings of key establishment at each layer but also provides a flexible solution to the key generation rate problem. © 2014 Springer International Publishing Switzerland.	mesh networking;wireless mesh network	Yuexin Zhang;Yang Xiang;Xinyi Huang;Li Xu	2014		10.1007/978-3-319-11203-9_30	generation;telecommunications;computer science;static key;key management;coding;key distribution;computer security;key encapsulation;computer network;technology	Mobile	-47.323244682323576	75.66301378556086	5556
4e106ca5e60acf7fe541c9f64d69bf6693693c98	a component framework for autonomous mobile robots	computer engineering;datorteknik	The major problem of robotics research today is that there is a barrier to entry into robotics research. Robot system software is complex and a researcher that wishes to concentrate on one particular problem often needs to learn about details, dependencies and intricacies of the complete system. This is because a robot system needs several different modules that need to communicate and execute in parallel. Today there is not much controlled comparisons of algorithms and solutions for a given task, which is the standard scientific method of other sciences. There is also very little sharing between groups and projects, requiring code to be written from scratch over and over again. This thesis proposes a general framework for robotics. By examining successful systems and architectures of past and present, yields a number of key properties. Some of these are ease of use, modularity, portability and efficiency. Even though there is much consensus on that the hybrid deliberate/reactive is the best architectural model that the community has produced so far, a framework should not stipulate a specific architecture. Instead the framework should enable the building of different architectures. Such a scheme implies that the modules are seen as common peers and not divided into clients and servers or forced into a set layering. Using a standardized middleware such as CORBA, efficient communication can be carried out between different platforms and languages. Middleware also provides network transparency which is valuable in distributed systems. Component-based Software Engineering (CBSE) is an approach that could solve many of the aforementioned problems. It enforces modularity which helps to manage complexity. Components can be developed in isolation, since algorithms are encapsulated in components where only the interfaces need to be known by other users. A complete system can be created by assembling components from different sources. Comparisons and sharing can greatly benefit from CBSE. A component-based framework called ORCA has been implemented with the following characteristics. All communication is carried out be either of three communication patterns, query, send and push. Communication is done using CORBA, although most of the CORBA code is hidden for the developer and can in the future be replaced by other mechanisms. Objects are transported between components in the form of the CORBA valuetype. A component model is specified that among other things include support for a state-machine. This also handles initialization and sets up communication. Configuration is achieved by the presence of an XML-file per component. A hardware abstraction scheme is specified that basically route the communication patterns right down to the hardware level. The framework has been verified by the implementation of a number of working systems.	algorithm;autonomous robot;common object request broker architecture;communication complexity;component-based software engineering;distributed computing;finite-state machine;hardware abstraction;middleware;mobile robot;modularity (networks);orca;robotics;software portability;usability;xml	Anders Orebäck	2004			outline of robotics;mobile robot;simulation;systems engineering;engineering;artificial intelligence;geography of robotics;future of robotics;educational robotics	Robotics	-40.960864755485126	20.89689708065113	5562
a465527a918d2f7b7b3ceac1f269e17d6e264f8f	lightweight and compromise-resilient message authentication in sensor networks	security properties;base stations;message authentication peer to peer computing polynomials resilience scalability wireless sensor networks computer science delay public key digital signatures;digital signatures;polynomials;sensor network;wireless sensor network;receivers;node compromises;public key;resilience;immediate authentication;node compromises message authentication wireless sensor networks communication authenticity immediate authentication;communication authenticity;scalability;message authentication;computer science;peer to peer computing;security;wireless sensor networks	Numerous authentication schemes have been proposed in the past for protecting communication authenticity and integrity in wireless sensor networks. Most of them however have following limitations: high computation or communication overhead, no resilience to a large number of node compromises, delayed authentication, lack of scalability, etc. To address these issues, we propose in this paper a novel message authentication approach which adopts a perturbed polynomial-based technique to simultaneously accomplish the goals of lightweight, resilience to a large number of node compromises, immediate authentication, scalability, and non-repudiation. Extensive analysis and experiments have also been conducted to evaluate the scheme in terms of security properties and system overhead.	computation;experiment;ibm notes;message authentication;non-repudiation;overhead (computing);polynomial;scalability	Wensheng Zhang;Nalin Subramanian;Guiling Wang	2008	IEEE INFOCOM 2008 - The 27th Conference on Computer Communications	10.1109/INFOCOM.2008.200	wireless sensor network;computer science;information security;authentication protocol;lightweight extensible authentication protocol;internet privacy;computer security;psychological resilience;computer network	Mobile	-50.25673022382289	76.59305458470139	5565
6dc4cc642aca0912b12a397163855b66371bcb99	scatter search: diseño básico y estrategias avanzadas	tecnologia electronica telecomunicaciones;scatter search;rev;universidad autonoma del estado de mexico;red de revistas cientificas de america latina y el caribe espanoa y portugal;uaem;grupo c;redalyc;cientifica;tecnologias;hemeroteca	Scatter Search es un método metaheurístico para resolver problemas de optimización. Aunque fue originalmente introducido en los años setenta, recientemente es cuando ha sido probado en numerosos problemas difíciles con gran éxito. Pertenece a la familia de los llamados Algoritmos Evolutivos, los cuales se distinguen por estar basados en la combinación de un conjunto de soluciones. Aunque presenta similitudes con los Algoritmos Genéticos, difiere de éstos en principios fundamentales, tales como el uso de estrategias sistemáticas en lugar de aleatorias. Scatter Search proporciona un marco flexible que permite el desarrollo de diferentes implementaciones con distintos grados de complejidad. En este trabajo se realiza una revisión del método desde sus orígenes hasta los aspectos más novedosos, que dan lugar a algoritmos más eficientes. Además, se ilustra la aplicación del método en la resolución del conocido problema de la mochila.	bibliothèque de l'école des chartes;cybernetics;estar project;evolutionary algorithm;evolutionary computation;genetic algorithm;global optimization;heuristic;john d. wiley;kelly criterion;lecture notes in computer science;library (computing);linear algebra;mathematical optimization;multimedia exchange network over satellite;multimodal interaction;naruto shippuden: clash of ninja revolution 3;natural language processing;nonlinear system;power-on reset;routing;springer (tank);tabu search;unique name assumption	Rafael Martí;Manuel Laguna	2003	Inteligencia Artificial, Revista Iberoamericana de Inteligencia Artificial		computer science;operating system;rev	ML	-108.21578221267326	16.734073657325773	5573
e06c829042c02fa3adc825b123ee5119f6269308	finite degree clones are undecidable		A clone of functions on a finite domain determines and is determined by its system of invariant relations (=predicates). When a clone is determined by a finite number of relations, we say that the clone is of finite degree. For each Minsky Machine M we associate a finitely generated clone C such that C has finite degree if and only if M halts, thus proving that deciding whether a given clone has finite degree is impossible.	register machine;undecidable problem	Matthew Moore	2018	CoRR			Logic	-5.088018287558516	15.592622436104707	5578
a400532cf65d7319b7190dcf2f15f3d106f79528	towards automated proofs for asymmetric encryption schemes in the random oracle model	provable security;security properties;automated proofs;asymmetric encryption;public key cryptosystem;hoare logic;security proof;chosen ciphertext security;automated verification;random oracle model;hoare logics	Chosen-ciphertext security is by now a standard security property for asymmetric encryption. Many generic constructions for building secure cryptosystems from primitives with lower level of security have been proposed. Providing security proofs has also become standard practice. There is, however, a lack of automated verification procedures that analyze such cryptosystems and provide security proofs. This paper presents an automated procedure for analyzing generic asymmetric encryption schemes in the random oracle model. It has been applied to several examples of encryption schemes among which the construction of Bellare-Rogaway 1993, of Pointcheval at PKC'2000 and REACT.	ciphertext;cryptosystem;encryption;mihir bellare;norton internet security;phillip rogaway;public-key cryptography;random oracle;react	Judicaël Courant;Marion Daubignard;Cristian Ene;Pascal Lafourcade;Yassine Lakhnech	2008		10.1145/1455770.1455817	random oracle;computer security model;40-bit encryption;plaintext-aware encryption;computer science;theoretical computer science;provable security;ciphertext indistinguishability;optimal asymmetric encryption padding;internet privacy;public-key cryptography;deterministic encryption;hoare logic;computer security;probabilistic encryption;56-bit encryption;attribute-based encryption	Crypto	-39.88080663376	76.03522728495273	5583
681f16ec238db3cbf0058456831ddcf5ca49aede	accelerating big data applications on tiered storage system with various eviction policies	plugs;performance evaluation;prototypes;acceleration;computer architecture;distributed databases;big data applications	Utilizing new type devices, such as SSD, to improve I/O performance of hybrid storage has become a tendency recently. Many efforts are made to apply the new type devices to hybrid storage in distributed environment, but most of them are confined to the specific file systems, such as HDFS. Besides, the low performance of HDFS descends the performance of hybrid storage. In this paper, we improve the performance of tiered storage system (one kind of hybrid storage system) in distributed environment with a plughable eviction framework considering that the data on each node is regularly accessed. On top of the eviction framework, we provide a couple of eviction policies, including LRU, LRFU, LIRS and ARC, covering different access patterns to accelerate the upper big data applications. Moreover, our design is general for all tiered storage systems. Then we evaluate the performance of our eviction framework through three widely-used big data applications and discover that LIRS can improve 30% hit ratio than most of other policies when running KMeans and PageRank, ARC can improve maximum 30% hit ratio than other policies when running complicated SQL applications, LRFU can always achieve relatively good performance when the configuration properties are set in reasonable range. We have implemented our prototype on Alluxio, which is a widely-used memory-centric distributed storage system. In addition, these eviction policies contributed by us have been merged into Alluxio and are already being in use.	apache hadoop;big data;clustered file system;computer data storage;hierarchical storage management;hit (internet);k-means clustering;lirs caching algorithm;memory hierarchy;pagerank;prototype;sql;solid-state drive;type system	Peng Shu;Rong Gu;Qianhao Dong;Chunfeng Yuan;Yihua Huang	2016	2016 IEEE Trustcom/BigDataSE/ISPA	10.1109/TrustCom.2016.0214	embedded system;real-time computing;engineering;operating system	OS	-14.47465996762519	53.38482366565552	5591
99c627f3b194034fb59d2c2713c617ecbf36fa3a	hermes: federating fog and cloud domains to support query evaluations in continuous sensing environments	raspberry pi;federated query evaluations;sensor networks;fog storage	As networked sensing devices continue to proliferate, the storage and processing capabilities of fog or edge devices have also increased substantially while offering low energy consumption and hardware costs. These complimentary technologies enable unique opportunities for performing decentralized analysis on the edges of the network while also leveraging the capabilities of public and private clouds for coordination and long-term storage. Our framework, HERMES, enables federated query evaluations that transition between cloud and fog nodes seamlessly. The system selectively samples from observational streams to reduce communication and memory consumption on fog nodes; our evaluation over a real-world observational dataset demonstrates an 89% reduction in dataset size while maintaining a mean absolute error of less than 0.25%.	approximation error;distance fog;edge device;federated identity;federated search	Matthew Malensek;Sangmi Lee Pallickara;Shrideep Pallickara	2017	IEEE Cloud Computing	10.1109/MCC.2017.26	real-time computing;simulation;wireless sensor network;computer science;operating system;world wide web;computer security	Mobile	-38.25954527935075	59.714675671737474	5598
1b32dbf577df7a0081facc40ab7f51a6ea96d8c7	catchup!: capturing and replaying refactorings to support api evolution	application program interfaces;computer aided software engineering;programming environments;software libraries;software maintenance;api evolution;api refactoring;application programming interfaces;integrated development environment;software library development	Library developers who have to evolve a library to accommodate changing requirements often face a dilemma: Either they implement a clean, efficient solution but risk breaking client code, or they maintain compatibility with client code, but pay with increased design complexity and thus higher maintenance costs over time. We address this dilemma by presenting a lightweight approach for evolving application programming interfaces (APIs), which does not depend on version control or configuration management systems. Instead, we capture API refactoring actions as a developer evolves an API. Users of the API can then replay the refactorings to bring their client software components up to date. We present CatchUp!, an implementation of our approach that captures and replays refactoring actions within an integrated development environment semi-automatically. Our experiments suggest that our approach could be valuable in practice.	application programming interface;code refactoring	Johannes Henkel;Amer Diwan	2005		10.1109/ICSE.2005.1553570	application programming interface;computer science;systems engineering;control system;operating system;software engineering;programming language	SE	-54.75042169507832	39.43802174828	5603
cb57d554f410c27740c994f40418c12809ee4f81	the lord of the shares: combining attribute-based encryption and searchable encryption for flexible data sharing		Secure cloud storage is considered one of the most important issues that both businesses and end-users are considering before moving their private data to the cloud. Lately, we have seen some interesting approaches that are based either on the promising concept of Symmetric Searchable Encryption (SSE) or on the well-studied field of Attribute-Based Encryption (ABE). In the first case, researchers are trying to design protocols where usersu0027 data will be protected from both internal and external attacks without paying the necessary attention to the problem of user revocation. On the other hand, in the second case existing approaches address the problem of revocation. However, the overall efficiency of these systems is compromised since the proposed protocols are solely based on ABE schemes and the size of the produced ciphertexts and the time required to decrypt grows with the complexity of the access formula. In this paper, we propose a protocol that combines both SSE and ABE in a way that the main advantages of each scheme are used. The proposed protocol allows users to directly search over encrypted data by using an SSE scheme while the corresponding symmetric key that is needed for the decryption is protected via a Ciphertext-Policy Attribute-Based Encryption scheme.		Antonis Michalas	2018	IACR Cryptology ePrint Archive		encryption;hybrid cryptosystem;computer network;cloud computing;cloud storage;revocation;symmetric-key algorithm;attribute-based encryption;computer science;cloud computing security	Crypto	-40.920521933758565	67.48980902549033	5611
b7bac51ea1ca35f2a0bf63b04d05e0e6b527efbb	"""sip based services and virtual home umts environment in the ist research project """"future"""""""	satellite communication;protocols;integrable system;session initiation protocol;radio direction finding;telecommunication terminals;3g mobile communication;multimedia communication;research initiatives;multimedia services;mobile user	The integration of a terrestrial UMTS network with a satellite platform represents one of the most attractive proposals to develop a functional Virtual Home UMTS environment for a generic mobile user provided with an “ad-hoc” dual mode terminal, which will be able to exploit the two integrated systems (hereafter referred to as “segments”) for a set of innovative multimedia services. At this purpose, the IST research project “FUTURE” (Functional UMTS Real Emulator) aims at studying and implementing an integrated Satellite and Terrestrial UMTS (S-UMTS, T-UMTS) demonstrator in order to develop new multimedia services provided “anywhere for everyone”, targeting, at the same time, to an efficient exploitation of the satellite transmission capabilities. This paper deals with the implementation and the demonstration of new services, based on the adoption of the SIP (Session Initiation Protocol), in the overall FUTURE scenario.	communications satellite;emulator;exploit (computer security);hoc (programming language);location-based service;simulation;terrestrial television	Filomena Del Sorbo;Giuseppe Lombardi;Dario Pompili	2002		10.1109/GLOCOM.2002.1189177	communications protocol;integrable system;simulation;telecommunications;computer science;operating system;session initiation protocol;communications satellite;computer network	DB	-16.216546379032497	90.80093824305969	5617
9c3974c0a52274359c9c6c17eccdecf45c44d353	time-triggered real-time computing	linking interface;composability;time-triggered;component reuse;distributed system;real-time;global time	Time-triggered (TT) distributed real-time computing systems are moving into the mainstream for the implementation of safety-critical applications in the aerospace and automotive sectors. This paper introduces the basic principles of a time-triggered real-time computing system, and elaborates on the benefits that can be gained from the availability of a global time base in general, and in specifying the linking interfaces of components in particular. It describes the concept of a temporal firewall that forms a fully specified operational interface of a component. The most important contribution of the TT-paradigm is the capability to precisely specify operational interfaces in the temporal domain and thus establish a sound basis for the composability of a design and the reuse of components in distributed real-time systems. Copyright c 2002 IFAC	composability;firewall (computing);programming paradigm;real-time clock;real-time computing;real-time transcription	Hermann Kopetz	2003	Annual Reviews in Control	10.1016/S1367-5788(03)00002-6	real-time computing;simulation;computer science;distributed computing	Embedded	-39.261054569551284	36.21115195999467	5618
807e08933c27ae7e6439713adc4d2f1ffa61c39a	"""""""frauenspezifische informatik"""" und frauenförderpläne: wie paßt das zusammen?"""			internet explorer	Maritta Heisel	1993				NLP	-101.31556387412809	27.883332488735185	5619
