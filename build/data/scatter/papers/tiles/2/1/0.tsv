id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
658b14d7f46e8e4e6e94671d00cb903270d4e316	adversarial active learning	active learning;human in the loop;secure machine learning	Active learning is an area of machine learning examining strategies for allocation of finite resources, particularly human labeling efforts and to an extent feature extraction, in situations where available data exceeds available resources. In this open problem paper, we motivate the necessity of active learning in the security domain, identify problems caused by the application of present active learning techniques in adversarial settings, and propose a framework for experimentation and implementation of active learning systems in adversarial contexts. More than other contexts, adversarial contexts particularly need active learning as ongoing attempts to evade and confuse classifiers necessitate constant generation of labels for new content to keep pace with adversarial activity. Just as traditional machine learning algorithms are vulnerable to adversarial manipulation, we discuss assumptions specific to active learning that introduce additional vulnerabilities, as well as present vulnerabilities that are amplified in the active learning setting. Lastly, we present a software architecture, Security-oriented Active Learning Testbed (SALT), for the research and implementation of active learning applications in adversarial contexts.	active learning (machine learning);algorithm;feature extraction;machine learning;software architecture;testbed	Brad Miller;Alex Kantchelian;Sadia Afroz;Rekha Bachwani;Edwin Dauber;Ling Huang;Michael Carl Tschantz;Anthony D. Joseph;J. Doug Tygar	2014		10.1145/2666652.2666656	robot learning;proactive learning;instance-based learning;error-driven learning;simulation;computer science;artificial intelligence;machine learning;active learning;synchronous learning	ML	18.30552924154122	-51.366527757422375	1004
67348cc0cbb3d0fd16c86eda6fb7d527c26bec15	meta-reasoning algorithm for improving analysis of student interactions with learning objects using supervised learning		Supervised learning (SL) systems have been used to automatically learn models for analysis of learning object (LO) data. However, SL systems have trouble accommodating data from multiple distributions and “troublesome” data that contains irrelevant features or noise—all of which are relatively common in highly diverse LO data. The solution is to break up the available data into separate areas and then take steps to improve models on areas containing troublesome data. Unfortunately, finding these areas in the first place is a far from trivial task that balances finding a single distribution with having sufficient data to support meaningful analysis. Therefore, we propose a BoU metareasoning (MR) algorithm that first uses semi-supervised clustering to find compact clusters with multiple labels that each support meaningful analyses. After clustering, our BoU MR algorithm learns a separate model on each such cluster. Finally, our BoU MR algorithm uses feature selection (FS) and noise correction (NC) algorithms to improve models on clusters containing troublesome data. Our experiments, using three datasets containing over 5000 sessions of student interactions with LOs, show that multiple models from BoU MR achieve more accurate analyses than a single model. Further, FS and NC algorithms are more effective at improving multiple models than a single model.	algorithm;cluster analysis;experiment;feature selection;interaction;relevance;sl (complexity);semiconductor industry;supervised learning	Lee Dee Miller;Leen-Kiat Soh	2013			machine learning;semi-supervised learning;instance-based learning;supervised learning;computer science;unsupervised learning;algorithm;active learning (machine learning);cluster analysis;artificial intelligence;learning classifier system;stability (learning theory);pattern recognition	ML	16.65373087270937	-40.52697940574899	1005
c62ff6da66db5dcf3ac1adf39de2cde0a916ab74	an interactive medical image segmentation system based on the optimal management of regions of interest using topological medical knowledge	regions of interest;structural model;computer aided diagnosis;topological structural model;experience report;interactive system;medical image processing;region of interest;hierarchical segmentation;medical image segmentation;hierarchical model;point of interest	This paper presents an original interactive system for efficient medical image segmentation in computer aided diagnosis. The main originality concerns the method used to manage, according to an a priori topological-based structural model, regions of interest (ROIs) within which computations can be constrained. The goal is then to avoid the processing of irrelevant image points, therefore improving and accelerating segmentations. In the case of a hierarchical modeling procedure, our ROI management method enables, for delineating a given medical structure, to optimally determine image points of interest by taking previously segmented structures into account. We propose a mathematical formulation of the method as well as a possible implementation within an interactive system. We also detail an experience report focussing on the segmentation of several abdominal structures from a CT image. It illustrates the behavior and the potential of our method.	ct scan;computation;image segmentation;interactivity;mathematics;medical image;medical records systems, computerized;point of interest;region of interest;relevance;biologic segmentation	Jean-Baptiste Fasquel;Vincent Agnus;Johan Moreau;Luc Soler;Jacques Marescaux	2006	Computer methods and programs in biomedicine	10.1016/j.cmpb.2006.04.004	computer vision;point of interest;simulation;computer science;segmentation-based object categorization;data mining;image segmentation;scale-space segmentation;hierarchical database model;region of interest	Vision	43.38922030389718	-75.22753338260064	1008
458309189e93af92a307ff944cd4f8643f09f35c	localization of human faces fusing color segmentation and depth from stereo	image segmentation;human detection human faces localization color segmentation depth from stereo stereo vision system color images depth map head model skin color information factory automation robot manipulator safety service robots;pattern recognition object detection;comunicacion de congreso;face localization;mobile robots;stereovision;computer vision;robot manipulator;information gathering;image segmentation stereo image processing face recognition image colour analysis factory automation robots industrial manipulators mobile robots;skin color;color segmentation;face recognition;image colour analysis;human detection;robots;stereo image processing;stereo vision;service robot;factory automation;color histograms;face detection;depth map;pattern recognition systems;pattern recognition computer vision;humans face detection image segmentation stereo vision image analysis image color analysis information analysis fusion power generation image generation layout;industrial manipulators;color image	Describes a method to localize faces in color images based on the fusion of the information gathered from a stereo vision system and the analysis of color images. Our method generates a depth map of the scene and tries to fit a head model taking into account the shape of the model and skin color information. The method is tailored for its use in factory automation applications where the detection and localization of humans is necessary for the completion or interruption of a particular task, such as robot manipulator safety or the interaction of service robots with humans.	comparison of command shells;data acquisition;depth map;heart rate variability;high- and low-level;image segmentation;interrupt;robot;sensor web;sparse matrix;stereopsis	Francesc Moreno;Juan Andrade-Cetto;Alberto Sanfeliu	2001	ETFA 2001. 8th International Conference on Emerging Technologies and Factory Automation. Proceedings (Cat. No.01TH8597)	10.1109/ETFA.2001.997729	facial recognition system;computer vision;simulation;computer science;stereopsis;computer graphics (images)	Robotics	49.94812204731304	-39.66701275312907	1009
169b4b89e6cea8b0078dd6e96b0faa8454353e86	torque evaluation method of spherical motors using six-axis force/torque sensor	torque rotors torque measurement induction motors wheels force robot sensing systems;spherical induction motor torque evaluation method spherical motors six axis force torque sensor multidegrees of freedom actuators output torque controllability mechatronic systems angular velocity torque measurement off axis simultaneous measurement rotor driver wheels omnidirectional mobile robots;torque controllability induction motors mechatronics mobile robots sensors	Spherical motors are classified as multi-degrees-of-freedom actuators. Several types of spherical motors are under investigation in the world because they will be useful for robots that currently use combination of 2 or 3 one-degree-of-freedom motors. However, most of the previous works mainly demonstrated how the motors rotate or how they output torque without giving enough specifications. Torque, velocity, efficiency, controllability, and response are important issues to determine how they can be used for robots or practical mechatronic systems. The objective of this paper is to propose a method to measure three-degrees-of-freedom torque of spherical motors while they rotate with arbitrary angular velocity, which can evaluate torque in practical situations and can be used to derive efficiency of the motor. These measurements are important for comparison among the implementations and improvement of the systems. The idea consists of two parts. One is torque measurement using a 6-axis force/torque sensor. It enables off-axis simultaneous measurement of torque. The other is an external 3-degrees-of-freedom rotor driver using wheels for omnidirectional mobile robots, which is installed between the rotor and the sensor. Detailed principles and a set of experimental measurements of our spherical induction motor demonstrating usefulness of the proposed method are described in this paper.	angularjs;inductive reasoning;mathematical induction;mechatronics;mobile robot;optic axis of a crystal;r.o.t.o.r.;torque;velocity (software development);wheels	Masaaki Kumagai	2016	2016 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2016.7487392	control engineering;electronic engineering;damping torque;torque sensor;stall torque;engineering;torque steering;control theory;coupling;direct torque control	Robotics	72.41735980671393	-21.56554013560932	1016
1cbb04b93ebc49231e2a7bf7e8711daa0ea0034b	optimal wiener interpolation filters for multiresolution coding of images	filtrado wiener;rate distortion;optimisation;interpolation;image coding;image processing;image resolution;filtre reponse impulsion finie;interpolacion;optimal filter;finite impulse response filter;rate distortion results optimal wiener interpolation filters multiresolution coding design approach optimization symmetric separable finite impulse response interpolation filters optimal inverse filters images four level progressive pyramid scheme;procesamiento imagen;wiener filters;decimation;optimal interpolation;traitement image;rate distortion theory;codificacion;filtro respuesta impulsion acabada;filtro optimal;finite impulse response;image reconstruction;wiener filtering;digital filters;coding;interpolation wiener filter finite impulse response filter matched filters design optimization statistics image reconstruction image coding testing rate distortion;fir filters;filtrage wiener;filtre optimal;rate distortion theory interpolation optimisation image coding wiener filters image resolution image reconstruction fir filters digital filters inverse problems;decimacion;codage;inverse problems	"""A design approach is presented which allows the optimization of coefficients for symmetric and separable finite impulse response (FIR) interpolation filters for multiresolution coding schemes. The interpolation filters serve as optimal inverse filters in the Wiener sense and are designed to match the characteristics of the specific filters used for decimation as well as for the statistics of """"typical"""" images to be reconstructed. Applied to the coding of test images in a four-level progressive pyramid scheme, the optimal interpolation filters generated substantially improved rate-distortion results compared to conventional filters."""	interpolation	Thomas Sikora	1997	IEEE Trans. Circuits Syst. Video Techn.	10.1109/76.564126	computer vision;mathematical optimization;image processing;interpolation;finite impulse response;control theory;mathematics;nearest-neighbor interpolation;statistics	Vision	47.11635700718007	-12.735602155091359	1018
37645ba2329df1131a1da6cb0c0f1d9fcaae732e	an unsupervised learning approach for classifying sequence data for human robotic interaction using spiking neural network	unsupervised learning;robotics;human robot interaction;classification;neuroscience;spiking neural network	The goal of this research is to enable robots to learn spatio-temporal patterns from a human's demonstration. We propose an approach based on Spiking Neural Networks. The method brings the following contributions: first, it enables the encoding of patterns in an unsupervised manner. Second, it requires a very small number of training examples. Third, the approach is invariant to scale and translation. We validated our method on a dataset of hand movement gestures representing the drawing of digits 0 to 9, in front of a camera. We compared the proposed approach with other standard pattern recognition approaches. The results indicate the superiority of proposed method over other approaches.	neural networks;pattern recognition;robot;spiking neural network;unsupervised learning	Banafsheh Rekabdar;Monica N. Nicolescu;Mircea Nicolescu	2015		10.1145/2701973.2702727	human–robot interaction;unsupervised learning;computer vision;biological classification;computer science;artificial intelligence;machine learning;robotics;spiking neural network	AI	21.66407464757171	-64.46138434766367	1023
09508ec2634d2a0e2be6278374a1e98f827e4de4	artificial skin ridges enhance local tactile shape discrimination	local shape;learning algorithm;support vector machines;tactile sensor;skin;ridged skin cover;bayes theorem;robotic hand;tactile sensing;touch;machine learning;algorithms;humans;prosthetic hand;support vector machine;neural networks computer;fingerprints;curvature discrimination;artificial neural network	One of the fundamental requirements for an artificial hand to successfully grasp and manipulate an object is to be able to distinguish different objects' shapes and, more specifically, the objects' surface curvatures. In this study, we investigate the possibility of enhancing the curvature detection of embedded tactile sensors by proposing a ridged fingertip structure, simulating human fingerprints. In addition, a curvature detection approach based on machine learning methods is proposed to provide the embedded sensors with the ability to discriminate the surface curvature of different objects. For this purpose, a set of experiments were carried out to collect tactile signals from a 2 × 2 tactile sensor array, then the signals were processed and used for learning algorithms. To achieve the best possible performance for our machine learning approach, three different learning algorithms of Naïve Bayes (NB), Artificial Neural Networks (ANN), and Support Vector Machines (SVM) were implemented and compared for various parameters. Finally, the most accurate method was selected to evaluate the proposed skin structure in recognition of three different curvatures. The results showed an accuracy rate of 97.5% in surface curvature discrimination.	algorithm;artificial neural network;artificial skin;bayes theorem;classification;coefficient;cross-correlation;embedded system;embedding;excretory function;experiment;fingerprint;fingerprints;fingertip dosing unit;hand;machine learning;naive bayes classifier;neural network software;ninety nine;online and offline;physical object;repeatability;requirement;simulation;speech discrimination tests;support vector machine;tactile sensor;sensor (device)	Saba Salehi;John-John Cabibihan;Shuzhi Sam Ge	2011		10.3390/s110908626	support vector machine;computer vision;computer science;engineering;artificial intelligence;machine learning;artificial neural network	AI	25.76570438808238	-67.30008213197361	1028
c785c2382edeb6efcd91da821958fd6162b56790	use of icesat glas data for forest disturbance studies in central siberia	geoscience laser altimeter system;remote sensing by laser beam;vegetation mapping;leaf area index;forestry;laser altimeter;information source;data collection;pulse emission;laser radar;forest disturbance;time elapse measurement;stand volume;returned signal;icesat glas data;optical radar;remote sensing;lidar data;biomass;ad 2003 09;field data;insect damaged forest stands;ad 2003 01;data collection icesat glas data forest disturbance lidar data laser altimeter distance determination time elapse measurement pulse emission geoscience laser altimeter system ad 2003 01 ad 2003 02 ad 2003 09 structural parameters biomass stand volume leaf area index returned signal information source fire insect damaged forest stands;fire;fires;laser radar pulse measurements laser theory instruments surface emitting lasers time measurement optical pulses signal processing buildings parameter estimation;vegetation mapping altimeters fires forestry optical radar remote sensing by laser beam;altimeters;structural parameters;ad 2003 02;distance determination	Lidar is a laser altimeter that determines the distance from the instrument to the physical surface by measuring the time elapsed between the pulse emission and the reflected return. The returned signal may identify multiple returns originating from trees, building and other objects and permits the calculation of their height. Studies using field data have shown that lidar data can provide estimates of structural parameters such as biomass, stand volume and leaf area index. NASA's ICESat Geoscience Laser Altimeter System (GLAS) was launched in January 2003 and collected data during February and September of that year. This study used GLAS data acquired over our study sites in central Siberia to examine the returned signal as a source of information about fire and insect damaged forest stands	information source;tree (data structure)	K. Jon Ranson;Guoqing Sun;Katalin A. Kovacs;Viatcheslav Kharuk	2004	IGARSS 2004. 2004 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2004.1370722	meteorology;lidar;biomass;hydrology;altimeter;leaf area index;fire;physics;remote sensing;data collection	Robotics	82.97415950442007	-63.09944987265496	1033
401aa5e26e4146d8b0d29352406d8c3f6bf4c081	dynamic ensemble of rough set reducts for data classification		Ensemble learning also named ensemble of multiple classi- fiers is one of the hot topics in machine learning. Ensemble learning can improve not only the accuracy but also the efficiency of the classification system. Constructing the component classifiers in ensemble learning is crucial, because it has direct influence on the performance of the clas- sification system. In the construction of component classifiers, it should be guaranteed that the constructed component classifiers possess certain accuracy and diversity. Based on the confidence degree of classifier, this paper presents an approach consisting of three steps to dynamically inte- grate rough set reducts. Firstly, multiple reducts are computed. Secondly, multiple component classifiers with certain diversity are trained on the different reducts. Finally, these component classifiers are integrated by adopting dynamic integration strategy. The experimental results show that the proposed algorithm is efficient and feasible.	rough set	Jun-Hai Zhai;Xizhao Wang;Hua-Chao Wang	2014		10.1007/978-3-319-11740-9_59	random subspace method;cascading classifiers;machine learning;pattern recognition;data mining;mathematics;ensemble learning	AI	10.806668031923694	-40.720251331696424	1034
237bd69daabed8f864484e321e06cb1baef9f1b2	refractory neural nets and vision - a deeper look			artificial neural network	Thomas C. Fall	2016			artificial neural network;artificial intelligence;computer science	Robotics	12.07235499376759	-27.01055320260331	1036
3e4294e7a003ed170336da57f7c00de07d271477	natural selection and algorithmic design of mrna	synonymous coding sequence;rna secondary structure;stability;rna;natural selection;molecular design;algorithm design	"""Messenger RNA (mRNA) sequences serve as templates for proteins according to the triplet code, in which each of the 4(3) = 64 different codons (sequences of three consecutive nucleotide bases) in RNA either terminate transcription or map to one of the 20 different amino acids (or residues) which build up proteins. Because there are more codons than residues, there is inherent redundancy in the coding. Certain residues (e.g., tryptophan) have only a single corresponding codon, while other residues (e.g., arginine) have as many as six corresponding codons. This freedom implies that the number of possible RNA sequences coding for a given protein grows exponentially in the length of the protein. Thus nature has wide latitude to select among mRNA sequences which are informationally equivalent, but structurally and energetically divergent. In this paper, we explore how nature takes advantage of this freedom and how to algorithmically design structures more energetically favorable than have been built through natural selection. In particular: (1) Natural Selection--we perform the first large-scale computational experiment comparing the stability of mRNA sequences from a variety of organisms to random synonymous sequences which respect the codon preferences of the organism. This experiment was conducted on over 27,000 sequences from 34 microbial species with 36 genomic structures. We provide evidence that in all genomic structures highly stable sequences are disproportionately abundant, and in 19 of 36 cases highly unstable sequences are disproportionately abundant. This suggests that the stability of mRNA sequences is subject to natural selection. (2) Artificial Selection--motivated by these biological results, we examine the algorithmic problem of designing the most stable and unstable mRNA sequences which code for a target protein. We give a polynomial-time dynamic programming solution to the most stable sequence problem (MSSP), which is asymptotically no more complex than secondary structure prediction. We show that the corresponding least stable sequence problem (LSSP) is NP-complete, and develop two heuristics for the construction of such sequences. We have implemented these algorithms, and present experimental results placing the high/low stability sequences in context with both wildtype and random encodings. Our implementation has already been applied to the design of RNA """"code-words"""" creating little or no secondary structure in RNA computing (Brenneman and Condon, 2001; Marathe et al., 2001), and we anticipate a variety of other applications of this work to sequence design problems (Skiena, 2001)."""	amino acids;arginine;base;clinical use template;codon (nucleotide sequence);control theory;dspace;dynamic programming;genetic selection;greater than;greedy algorithm;heuristics;managed security service;np-completeness;natural selection;nucleotides;organism;pierre robin syndrome;protein structure prediction;rna;rna, messenger;single linkage cluster analysis;substitution method;terminate (software);time complexity;transcription (software);triplet state;tryptophan;unstable medical device problem;wild type;algorithm;free energy	Barry Cohen;Steven Skiena	2003	Journal of computational biology : a journal of computational molecular cell biology	10.1089/10665270360688101	biology;algorithm design;natural selection;rna;stability;bioinformatics;nucleic acid secondary structure;genetics	Comp.	3.2425623484506567	-60.49714396445342	1040
d77bdf5ddb09e777da8a13b56b64a7d02b361d38	risk management in projects: the monte carlo approach versus pert	excel;project management;uncertainty;risk management;monte carlo methods gaussian distribution risk management correlation project management uncertainty planning;excel risk management monte carlo simulation pert;risk management monte carlo methods pert project management;planning;correlation;monte carlo simulation;gaussian distribution;monte carlo methods;pert approach project risk management monte carlo approach;pert	One of the most popular techniques to handle insecurities and risks in project management is the well-known PERT approach. But this approach contains some disadvantages that are not always clearly visible at first sight. In this contribution we show how to overcome these disadvantages of the PERT approach by using Monte Carlo simulation.	monte carlo method;program evaluation and review technique;risk management;simulation;whole earth 'lectronic link	Wolfgang Tysiak	2011	Proceedings of the 6th IEEE International Conference on Intelligent Data Acquisition and Advanced Computing Systems	10.1109/IDAACS.2011.6072904	project management;econometrics;risk management;statistics;monte carlo method	Robotics	27.18287282170613	-20.448352722606554	1044
3c318eedeef1e5dd92774e92a0d10eecd04b16ad	a network of networks processing model for image regularization	quadratic programming;metodo adaptativo;quadratic programming computer vision neural nets parallel processing adaptive signal processing learning artificial intelligence;degradation parallel processing sensor arrays pixel optimization methods computer networks biological system modeling computer architecture hardware pattern analysis;architecture systeme;image processing;learning;programmation quadratique;neural nets;analisis forma;adaptive control;procesamiento imagen;natural images;methode adaptative;indexing terms;traitement image;computer vision;aprendizaje;apprentissage;adaptive signal processing;adaptive method;arquitectura sistema;programacion cuadratica;pattern analysis;process model;learning artificial intelligence;reseau neuronal;system architecture;hardware implementation;red neuronal;quadratic programming network of networks model image regularization image formation local processing parallel processing adaptive processing early vision image analysis learning;parallel processing;analyse forme;neural network	We introduce a network of networks (NoN) model to solve image regularization problems. The method is motivated by the fact that natural image formation involves both local processing and globally coordinated parallel processing. Both forms are readily implemented using an NoN architecture. The modeling is very powerful in that it achieves high-quality adaptive processing, and it reduces the computational difference between inhomogeneous and homogeneous conditions. This method is able to provide fast, quality imaging in early vision, and its replicating structure and sparse connectivity readily lend themselves to hardware implementations.	computation;image formation;parallel computing;sparse matrix	Ling Guan;James A. Anderson;Jeffrey P. Sutton	1997	IEEE transactions on neural networks	10.1109/72.554202	adaptive filter;computer vision;index term;computer science;artificial intelligence;machine learning;process modeling;artificial neural network	Vision	60.15915375823495	-74.63425733922722	1050
3aeef59bddec9f99a820615394aeb6b0254ec56e	robust panel unit root tests for cross-sectionally dependent multiple time series	test hypothese;autocorrelacion;finite sample;analisis numerico;limit distribution;real exchange rate;metodo monte carlo;stochastic process;theorie approximation;65c05;analisis datos;stochastic method;normal distribution;loi limite;62m10;estimacion m;test hipotesis;62e17;echantillon fini;methode monte carlo;multiple time series;m estimation panel unit root test purchasing power parity;time series;curva gauss;distribucion estadistica;analyse numerique;approximation theory;raiz unitaria;data analysis;numerical analysis;distribution statistique;monte carlo experiment;monte carlo method;panel unit root test;statistical computation;serie temporelle;calculo estadistico;processus stochastique;serie temporal;loi normale;robust performance;taux change;methode stochastique;estimation m;exchange rate;analyse donnee;purchasing power parity;calcul statistique;unit root;racine unitaire;estimation statistique;proceso estocastico;latin american;estimacion estadistica;cross section dependence;m estimation;statistical estimation;60fxx;statistical distribution;gaussian distribution;tasa cambio;ley limite;autocorrelation;hypothesis test;metodo estocastico	Robust panel unit root tests are developed for cross-sectionally dependent multiple time series. The tests have limiting null distributions derived from standard normal distributions. A Monte Carlo experiment shows that the tests have better finite sample robust performance than existing tests. Some Latin American real exchange rates revealing many outlying observations are analyzed to check the purchasing power parity (PPP) theory.	time series	Dong Wan Shin;Sangun Park	2010	Computational Statistics & Data Analysis	10.1016/j.csda.2010.02.011	normal distribution;stochastic process;econometrics;calculus;mathematics;statistics	ML	33.071219048573816	-22.182397256574816	1051
8fe4281f21a671cd7eb12da2638a9108b2058d49	physiologic factor analysis (pfa) and parametric imaging of dynamic pet images	radioisotope scanning and imaging;time dependent;pet imaging;radioisotope scanning and imaging computerised tomography medical image processing;positron emission tomography;image analysis positron emission tomography kinetic theory pollution measurement blood biomedical imaging physics laboratories image generation data mining;factor analysis;medical image processing;region of interest;compartmental model;computerised tomography;compartment model;kinetics;neuroreceptor binding characteristics physiologic factor analysis nuclear medicine parametric imaging dynamic positron emission tomography compartment models tracer kinetics;animal studies	Physiologic factor analysis (PFA) has been applied to a set of dynamic positron emission tomography (PET)-generated images to extract fundamental, kinetic functions useful in compartmental modeling of PET data. The study was conducted to investigate PFA as a means of improving compartment models of tracer kinetics for the estimation of neuroreceptor binding characteristics from dynamic PET data. PFA derived factors avoid the problem of overlapping tissue types in compartmental estimates and also avoid errors in operator definition of regions of interest, since PFA is an automated method. Three factors were estimated: the first factor was identified as a sample of the free tracer in tissue compartment and accounted for a mean contribution of 41% to the total factor representation of the data; the second factor was identified as bound radioligand with a 33% mean contribution; and the third was identified as free tracer in blood with a 26% mean contribution. The PFA results obtained from the 14 human PET studies were compared to published results from animal studies using the same radioligand but where tissue samples were analyzed for radioactivity. The time-dependent behavior of compartmental activity in the two cases was similar. >	factor analysis;polyethylene terephthalate;predictive failure analysis	William J. Geckle;Zsolt Szabo	1992		10.1109/CBMS.1992.244968	radiology;computer science;nuclear medicine;factor analysis;multi-compartment model;animal studies;kinetics;medical physics;region of interest	Vision	43.64835460067899	-81.12390018417524	1053
ebc2fa0634b662345d19a4df93d3bf9e86e2ba77	online classification for object tracking based on superpixel		Abstract Treating object tracking as a binary classification problem has been greatly explored in recent years. State-of-the-art classification based trackers perform better robustness than many of the other existing trackers. In this paper, we propose a collaborative model by incorporating the local and holistic models together which is corresponding to discriminative and generative models in tracking-classification-framework. At the local level, an on-line Random Forest (RF) classifier is trained to distinguish the superpixels of the object from the background. A series of local superpixels are used to represent the target, so as to adapt the appearance variances. The discriminative model is used to classify superpixels in the next frame as either belonging to the object or background. A confidence map consisting of dependability and stability is formed to measure the probabilities of superpxiels pertaining to the target from the classifier. A modified mean-shift is proposed to work on the confidence map to find the peak, where is the position of the target. Meanwhile, a separate component for managing the training set dynamically is employed to control the updating of the RF model. At the global level, the target is represented by covariance matrix of multi-scale bounding boxes. The generative model is applied for protection measure, which can effectively reduce the drifts during tracking. Experimental results demonstrate that our method is effective and performs favorably in comparison to the state-of-the-art trackers.		Sixian Chan;Xiaolong Zhou;Shengyong Chen	2018	Neurocomputing	10.1016/j.neucom.2018.01.069	machine learning;discriminative model;robustness (computer science);covariance matrix;video tracking;artificial intelligence;generative model;random forest;bittorrent tracker;binary classification;pattern recognition;mathematics	Vision	42.654354965439126	-49.18383483804253	1061
2c52ee4fd47cef766e199a859415c1004c37ad88	a partial image encryption method with pseudo random sequences	image encryption;correlacion;encryption;securite informatique;low complexity;cifrado;computer security;sucesion seudo aleatoria;least significant bit;suite pseudoaleatoire;residual intelligence;cryptage;criptografia;cryptography;seguridad informatica;random sequence;partial encryption;pseudorandom sequence;cryptographie;information system;correlation;point of view;correlated data;pseudo random sequence;systeme information;sistema informacion	We propose an effective approach for partial image encryption with pseudo random sequences (PRS). It is known that an image can be considered as a combination of correlated and uncorrelated data as well as most of the perceptual information are present in the correlated data rather than the uncorrelated data. Hence, the amount of residual intelligence present in an encrypted image depends on the correlated data. It is, therefore, sufficient to encrypt the correlated data instead of encrypting the entire image in order to speed up the entire operation. From the perception point of view, the most significant bit (MSB) planes have high adjacent correlation between the pixels whereas the least significant bit (LSB) planes contain comparatively more uncorrelated data. PRS with simple hardware like m-sequences and Gold sequences have less correlation between the adjacent bits. These can therefore serve as a good alternative for partially encrypting the MSB planes with low complexity to provide security against casual listeners. It is observed from the results that the new approach is able to reduce the residual intelligence as would have been obtained by encrypting the entire image.		Y. V. Subba Rao;Abhijit Mitra;S. R. Mahadeva Prasanna	2006		10.1007/11961635_22	least significant bit;computer science;cryptography;theoretical computer science;random sequence;computer security;correlation;encryption;information system;algorithm;statistics	Vision	43.72583969734374	-11.321832607802572	1067
23b22385f5f9ba4d41b2fc7826e5abf79421fbb2	fiber optic sensors for temperature monitoring during thermal treatments: an overview	temperature monitoring;minimally invasive thermal treatments;info eu repo semantics article;fiber optic sensors;medical applications	During recent decades, minimally invasive thermal treatments (i.e., Radiofrequency ablation, Laser ablation, Microwave ablation, High Intensity Focused Ultrasound ablation, and Cryo-ablation) have gained widespread recognition in the field of tumor removal. These techniques induce a localized temperature increase or decrease to remove the tumor while the surrounding healthy tissue remains intact. An accurate measurement of tissue temperature may be particularly beneficial to improve treatment outcomes, because it can be used as a clear end-point to achieve complete tumor ablation and minimize recurrence. Among the several thermometric techniques used in this field, fiber optic sensors (FOSs) have several attractive features: high flexibility and small size of both sensor and cabling, allowing insertion of FOSs within deep-seated tissue; metrological characteristics, such as accuracy (better than 1 °C), sensitivity (e.g., 10 pm·°C(-1) for Fiber Bragg Gratings), and frequency response (hundreds of kHz), are adequate for this application; immunity to electromagnetic interference allows the use of FOSs during Magnetic Resonance- or Computed Tomography-guided thermal procedures. In this review the current status of the most used FOSs for temperature monitoring during thermal procedure (e.g., fiber Bragg Grating sensors; fluoroptic sensors) is presented, with emphasis placed on their working principles and metrological characteristics. The essential physics of the common ablation techniques are included to explain the advantages of using FOSs during these procedures.	angioplasty, balloon, laser-assisted;ct scan;clinical act of insertion;compliance behavior;conflict (psychology);cost effectiveness;eye;fever;fiber optic technology;frequency response;gain;hyperactive behavior;insertion mutation;interference (communication);kilohertz;microwave;neoplasms;optic atrophy, autosomal dominant;optic atrophy, hereditary, leber;optical fiber;psyllium 14.2 mg/ml oral suspension;radio frequency;solutions;tissue fiber;tomography, emission-computed;ultrasonography;general practice (field);monitoring temperature;radiofrequency ablation;sensor (device);thermal ablation therapy;tomography	Emiliano Schena;Daniele Tosi;Paola Saccomandi;Elfed Lewis;Taesung Kim	2016		10.3390/s16071144	electronic engineering;electrical engineering;analytical chemistry;fiber optic sensor;nanotechnology;biological engineering;physics	HCI	94.23894502974815	-21.161893766281615	1074
83f6fd8fa279c70282876370942ffedd3f32f366	deep brain stimulation therapies: a control-engineering perspective		Deep Brain Stimulation (DBS) is an established therapy for treating e.g. Parkinson's disease, essential tremor, as well as epilepsy. In DBS, chronic pulsatile electrical stimulation is administered to a certain target area of the brain through a surgically implanted lead. The stimuli parameters have to be properly tuned in order to achieve therapeutical effect that in most cases is alleviation of motor symptoms. Tuning of DBS currently is a tedious task since it is performed manually by medical personnel in a trial-and-error manner. It can be dramatically improved and expedited by means of recently developed mathematical models together with control and estimation technology. This paper presents a control engineering perspective on DBS, viewing it as a control system for minimizing the severity of the symptoms through coordinated manipulation of the stimuli parameters. The DBS model structure comprises a stimuli model, an activation model, and a symptoms model. Each of those is individualized from patient data obtained through medical imaging, electrical measurements, and objective symptom quantification. The proposed approach is illustrated by simulation and clinical data from an individualized DBS model being developed by the authors.	automatic control;brain implant;characteristic impedance;control engineering;control system;deep brain stimulation;direct-broadcast satellite;functional electrical stimulation;least fixed point;mathematical model;mathematical optimization;medical imaging;simulation	Ruben Cubo;Alexander Medvedev;Helena Andersson	2017	2017 American Control Conference (ACC)	10.23919/ACC.2017.7962938	control engineering;computer science;biomedical engineering;epilepsy;stimulation;deep brain stimulation;medical imaging;essential tremor	Robotics	34.14448003320644	-86.93680371764346	1075
5cc86232e4b1278e10b6a7e206bc34b8043db030	automated detection of sleep disorder-related events from polysomnographic data	sensors;event detection;segmentation;classification;noninvasive sensors sleep disorder related events polysomnographic data human bio signals sleep studies eeg signals alternative sensors;sleep;sleep monitoring;sleep electroencephalography medical disorders medical signal processing signal classification;monitoring;feature extraction;psg;biosignal;eeg;electromyography;electroencephalography;sleep feature extraction electroencephalography sensors electromyography monitoring event detection;classification sleep monitoring psg eeg biosignal segmentation feature extraction	This work presents our effort in analyzing human bio signals collected during sleep studies, to automatically detect events related to sleep disorders. We experiment with real sleep data collected using standard Polysomnography (PSG), and we detect events of interest from EEG signals, by segmenting the signal, extracting descriptive features from each segment, and applying supervised learning for classification. Our preliminary experimental results show that the event detection goal can be successfully achieved, while our methods are general enough to be directly applied to sleep data collected using alternative, noninvasive sensors.	biological anthropology;british informatics olympiad;electroencephalography;programmable sound generator;sensor;supervised learning	Hugo Espiritu;Vangelis Metsis	2015	2015 International Conference on Healthcare Informatics	10.1109/ICHI.2015.105	psychology;computer vision;speech recognition;communication	Robotics	12.872902949794506	-88.68069499485817	1078
e99f38c3af6b911a4c8f60f60cff1975cfdc5b96	graph topology inference based on transform learning	topology;symmetric matrices;learning systems;laplace equations;transforms;optimization;sparse matrices	The association of a graph representation to large datasets is one of key steps in graph-based learning methods. The aim of this paper is to propose an efficient strategy for learning the graph topology from signals defined over the vertices of a graph, under a signal band-limited (either exactly or only approximately so) assumption, which corresponds to signals having clustering properties. The proposed method is composed of two optimization steps. The first step consists in learning, jointly, the sparsifying orthonormal transform and the graph signal from the observed data. The solution of this joint problem is achieved through an iterative algorithm whose alternating intermediate solutions are expressed in closed form. The second step recovers the Laplacian matrix, and then the topology, from the knowledge of the sparsifying transform, through a convex optimization strategy which admits an efficient solution.	algorithm;bandlimiting;cluster analysis;convex optimization;graph (abstract data type);iterative method;laplacian matrix;mathematical optimization;topological graph theory	Stefania Sardellitti;Sergio Barbarossa;Paolo Di Lorenzo	2016	2016 IEEE Global Conference on Signal and Information Processing (GlobalSIP)	10.1109/GlobalSIP.2016.7905863	graph power;integral graph;mathematical optimization;factor-critical graph;combinatorics;directed graph;graph bandwidth;null graph;simplex graph;machine learning;mathematics;voltage graph;graph;spectral graph theory;quartic graph;complement graph;string graph;strength of a graph;adjacency matrix	ML	27.035730775757386	-40.45404279142869	1087
132d5724f8531aef54cadb79748929808ba685c0	handling occlusions with franken-classifiers	training detectors feature extraction materials standards buildings decision trees;caltech usa pedestrian datasets occlusion handling franken classifiers partially occluded pedestrian detection detection quality maximization occlusion specific classifiers training classifiers eth pedestrian datasets inria pedestrian datasets;occlusion;psi_visics;image classification;pedestrians;pedestrian detection;traffic engineering computing;occlusion object detection pedestrian detection;object detection;traffic engineering computing image classification object detection pedestrians	Detecting partially occluded pedestrians is challenging. A common practice to maximize detection quality is to train a set of occlusion-specific classifiers, each for a certain amount and type of occlusion. Since training classifiers is expensive, only a handful are typically trained. We show that by using many occlusion-specific classifiers, we outperform previous approaches on three pedestrian datasets, INRIA, ETH, and Caltech USA. We present a new approach to train such classifiers. By reusing computations among different training stages, 16 occlusion-specific classifiers can be trained at only one tenth the cost of one full training. We show that also test time cost grows sub-linearly.	addendum;bootstrapping (compilers);central processing unit;computation;desktop computer;emoticon;geforce 200 series;geforce 500 series;graphics processing unit;henry franken;naive bayes classifier;pixel;randomness	Markus Mathias;Rodrigo Benenson;Radu Timofte;Luc Van Gool	2013	2013 IEEE International Conference on Computer Vision	10.1109/ICCV.2013.190	computer vision;contextual image classification;speech recognition;computer science;machine learning	Vision	36.3977836958534	-46.61299672281927	1092
f7a6be26eff0698df6fcb6fdaad79715699fc8cd	salient band selection for hyperspectral image classification via manifold ranking	spectroscopy;biological patents;vector spaces;biomedical journals;manifolds;text mining;europe pubmed central;citation search;image classification;citation networks;band vectors hyperspectral image classification manifold ranking saliency detection hsi salient band selection methods band difference measurement;computer vision;machine learning;research articles;abstracts;feature extraction;hyperspectral imaging manifolds feature extraction adaptation models machine learning transforms computer vision;open access;transforms;life sciences;object detection feature extraction hyperspectral imaging image classification;clinical guidelines;stacked autoencoders saes band selection deep learning hyperspectral image hsi classification manifold ranking mr saliency;full text;hyperspectral imaging;adaptation models;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	Saliency detection has been a hot topic in recent years, and many efforts have been devoted in this area. Unfortunately, the results of saliency detection can hardly be utilized in general applications. The primary reason, we think, is unspecific definition of salient objects, which makes that the previously published methods cannot extend to practical applications. To solve this problem, we claim that saliency should be defined in a context and the salient band selection in hyperspectral image (HSI) is introduced as an example. Unfortunately, the traditional salient band selection methods suffer from the problem of inappropriate measurement of band difference. To tackle this problem, we propose to eliminate the drawbacks of traditional salient band selection methods by manifold ranking. It puts the band vectors in the more accurate manifold space and treats the saliency problem from a novel ranking perspective, which is considered to be the main contributions of this paper. To justify the effectiveness of the proposed method, experiments are conducted on three HSIs, and our method is compared with the six existing competitors. Results show that the proposed method is very effective and can achieve the best performance among the competitors.	deep learning;experiment;genetic selection;horizontal situation indicator;learning disorders;mitral valve insufficiency;radiotherapy, image-guided;scientific publication;manifold	Qi Wang;Jianzhe Lin;Yuan Yuan	2016	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2015.2477537	computer vision;contextual image classification;text mining;spectroscopy;manifold;vector space;feature extraction;computer science;hyperspectral imaging;machine learning;pattern recognition	AI	19.178710183115623	-47.892945340912206	1093
a026c5bf3749ae0321a4c1354e3552fc4ad37a1f	adaptive fuzzy control of switched objective functions in pursuit-evasion scenarios	learning artificial intelligence adaptive control dynamic programming fuzzy control game theory;dynamic programming;switches games navigation;fuzzy controller;game theory;homicidal chauffeur problem adaptive fuzzy control switched objective functions pursuit evasion games switching scheme dynamic programming reinforcement learning technique;reinforcement learning;fuzzy control;adaptive control;objective function;navigation;games;finite horizon;approximate dynamic programming;learning artificial intelligence;switches;adaptive fuzzy control;switching control	In recent efforts, the authors have derived simple switched control schemes that qualitatively yield an attractive performance in two player pursuit-evasion games. A drawback of these methods is that detailed knowledge of an opponent's dynamics and strategy is required to implement the switching controller. Furthermore, an objective evaluated over a finite horizon may not guide an agent to the target set. To circumvent this potential shortcoming, a switching scheme is proposed where an adaptive fuzzy controller chooses the best objective function from a predefined library to increase the agent's reachability. The methodology we present builds on the common approximate dynamic programming reinforcement learning technique. We give conditions for showing when the controller is applicable and give an implementation example with the Homicidal Chauffeur problem.	approximation algorithm;dynamic programming;evasion (network security);fuzzy control system;homicidal chauffeur problem;loss function;optimization problem;pursuit-evasion;reachability;reinforcement learning	Brian J. Goode;Andrew Kurdila;Mike Roan	2010	49th IEEE Conference on Decision and Control (CDC)	10.1109/CDC.2010.5717045	control engineering;games;game theory;navigation;simulation;adaptive control;network switch;computer science;dynamic programming;control theory;reinforcement learning	Robotics	55.849789275246216	-18.392066902358962	1100
770aa8475ef9426fc7a447f6f5fed1c455bdfe4a	weight-enhanced diversification in stochastic local search for satisfiability	institute for integrated and intelligent systems;faculty of science environment engineering and technology;artificial intelligence and image processing;080199	Intensification and diversification are the key factors that control the performance of stochastic local search in satisfiability (SAT). Recently, Novelty Walk has become a popular method for improving diversification of the search and so has been integrated in many well-known SAT solvers such as TNM and gNovelty. In this paper, we introduce new heuristics to improve the effectiveness of Novelty Walk in terms of reducing search stagnation. In particular, we use weights (based on statistical information collected during the search) to focus the diversification phase onto specific areas of interest. With a given probability, we select the most frequently unsatisfied clause instead of a totally random one as Novelty Walk does. Amongst all the variables appearing in the selected clause, we then select the least flipped variable for the next move. Our experimental results show that the new weightenhanced diversification method significantly improves the performance of gNovelty and thus outperforms other local search SAT solvers on a wide range of structured and random satisfiability benchmarks.	boolean satisfiability problem;diversification (finance);greedy algorithm;heuristic (computer science);local search (optimization);maxima and minima;randomness;smoothing;standard sea level;the nameless mod	Thach-Thao Duong;Duc Nghia Pham;Abdul Sattar;M. A. Hakim Newton	2013			computer science;artificial intelligence;machine learning;mathematics;algorithm	AI	27.70381457390101	-0.6941354974391585	1104
55fd22a0b7070d8bb650386ec8032f675c808a21	reactive path planning for 3-d autonomous vehicles	plane resolution linear navigation laws path planning in 3 d;autonomous vehicle;path planning;kinematics;linear functionals;navigation;visibility;navigation parameter space reactive path planning 3d autonomous vehicle flight path angle heading angle linear function visibility line angle deviation regulation heading regulation navigation proportionality factor two deviation parameter nonstraight path obstacle zone dangerous zone;path planning in 3 d;robots;mathematical model;linear navigation laws;visibility autonomous aerial vehicles collision avoidance;plane resolution;collision avoidance;vehicles;navigation vehicles path planning robots kinematics vehicle dynamics mathematical model;autonomous aerial vehicles;vehicle dynamics	This brief deals with the problem of path planning for 3-D vehicles. The problem is simplified by resolving the workspace into horizontal and vertical planes. The flight path angle and the heading angle are linear functions of the visibility line angles in the vertical and horizontal planes, respectively. Other terms for the deviation and heading regulation are added to the navigation law. The navigation law has two navigation proportionality factors and two deviation parameters corresponding to the horizontal and vertical planes. The method has the ability to generate non-straight paths to the goal. Obstacles and dangerous zones are avoided by correcting the path by changing the values of the parameters. The dynamic constraints are transformed to a restriction in the navigation parameters' space. Our results are illustrated using simulation.	autonomous robot;control theory;course (navigation);information architecture;linear function;mobile robot;motion planning;robotic mapping;simulation;workspace	Fethi Belkhouche;K. Bendjilali	2012	IEEE Transactions on Control Systems Technology	10.1109/TCST.2011.2111372	robot;computer vision;kinematics;navigation;vehicle dynamics;simulation;geodesy;visibility;mathematical model;motion planning;physics	Robotics	62.9392886459447	-18.746592457402254	1107
0476a295fc6f233c0a471dd8071dbbdc562534c7	a 1000 frames/s vision chip using scalable pixel-neighborhood-level parallel processing	memory management;image resolution;arrays;vision chip cmos image sensor image processing neighborhood processor np object tracking programmable single instruction multiple data;program processors image resolution parallel processing arrays delays memory management;program processors;parallel processing;delays	"""This paper presents a novel vision chip architecture based on pixel-neighborhood-level parallel processing. In the architecture, an 8-b RISC processing core is embedded in an <inline-formula> <tex-math notation=""""LaTeX"""">$8\times 8$ </tex-math></inline-formula> array of digital pixel sensors on the same focal plane. These neighborhood processors (NPs) are tiled in a 2-D array to form the final imager resolution. Program execution is carried out in parallel across the array of pixel-neighborhood processing cores, allowing for direct scalability in terms of resolution, without reduction in processing speed or frame rate. To accomplish this, a compact, low-complexity NP architecture along with a general-purpose, 8-b instruction set has been designed and implemented. A prototype vision chip containing an <inline-formula> <tex-math notation=""""LaTeX"""">$8 \times 10$ </tex-math></inline-formula> array of NPs with a <inline-formula> <tex-math notation=""""LaTeX"""">$64 \times 80$ </tex-math></inline-formula> resolution has been designed and fabricated in a 0.13-<inline-formula> <tex-math notation=""""LaTeX"""">$\mu \text{m}$ </tex-math></inline-formula> 1P8M CMOS fabrication process. The system is reprogrammable and can perform a wide range of image and video processing tasks. Several example algorithms are implemented and tested on the single-chip vision system to demonstrate the functionality of pixel-neighborhood-level parallelism, including 1000-frames/s object tracking."""	algorithm;boolean algebra;cmos;central processing unit;dot pitch;embedded system;focal (programming language);general-purpose markup language;high- and low-level;image processing;image sensor;np (complexity);parallel computing;pixel;prototype;real-time clock;scalability;semiconductor device fabrication;video processing	Joseph A. Schmitz;Mahir Kabeer Gharzai;Sina Balk&#x0131;r;Michael W. Hoffman;Daniel J. White;Nathan Schemm	2017	IEEE Journal of Solid-State Circuits	10.1109/JSSC.2016.2613094	embedded system;parallel processing;parallel computing;real-time computing;image resolution;computer hardware;computer science;operating system;digital image processing;memory management	Arch	42.49713563522159	-3.4246945885463322	1109
03df507b31691baeb7343d3eb70d048943e2d4f4	exploring the use of local descriptors for fish recognition in lifeclef 2015	info eu repo semantics conferenceobject;120304 inteligencia artificial	This paper summarizes the proposal made by the SIANI team for the LifeCLEF 2015 Fish task. The approach makes use of standard detection techniques, applying a multiclass SVM based classifier on large enough Regions Of Interest (ROIs) automatically extracted from the provided video frames. The selection of the detection and classification modules is based on the best performance achieved for the validation dataset consisting of 20 annotated videos. For that dataset, the best classification achieved for an ideal detection module, reaches an accuracy around 40%.	algorithm;fast fourier transform;multiclass classification;region of interest;support vector machine;unbalanced circuit	Jorge Cabrera-Gámez;Modesto Castrillón Santana;Antonio Carlos Domínguez-Brito;Daniel Hernández-Sosa;Josep Isern;Javier Lorenzo-Navarro	2015			computer science;artificial intelligence;data mining	Vision	28.198181982855505	-59.84048706832708	1112
7aec79e957d2a907055be54968b203befa9ecd4b	a friction model with velocity, temperature and load torque effects for collaborative industrial robot joints		In this paper, a comprehensive friction model for collaborative industrial robot joints is proposed which takes into account the velocity, temperature and load torque effects. The model indicates that the velocity and temperature have a strong influence on viscous friction nonlinearly, whereas load torque significantly influences the Coulomb friction linearly and causes a slight Stribeck effect. The friction characteristics were investigated on the collaborative robot joints which are equipped with temperature sensor and harmonic reducer. The proposed model has been verified successfully under wide velocity, temperature and load range. In addition, the permillage of the rated current of the motor is used to scale the value of joint torque, so that the model can be applied in common industrial robot.	algorithm;cobot;dynamic simulation;estimation theory;industrial robot;mobile robot;nonlinear system;online and offline;steady state;velocity (software development)	Liming Gao;Jianjun Yuan;Zhedong Han;Shuai Wang;Ning Wang	2017	2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2017.8206141	viscosity;industrial robot;control engineering;coulomb;reducer;torque;computer science;harmonic	Robotics	72.66654055629921	-19.448269603977245	1115
b4cf509336999512a14572959f389595d25a8401	anisotropic diffusion on complex tensor fields for polsar image filtering	polarimetric synthetic aperture radar anisotropic diffusion complex tensor fields polsar image filtering complex tensor image denoising pm model pde scalar image vector image speckle noises;tensile stress matrix decomposition symmetric matrices anisotropic magnetoresistance noise vectors gold;tensors electromagnetic fields electromagnetic wave scattering image denoising radar imaging radar polarimetry synthetic aperture radar;anisotropic diffusion polsar images denoising	This paper addresses the problem of denoising for complex tensor image. In particular, we extend the anisotropic diffusion, also known as PM model (Perona and Malik, 1990) for filtering images based on PDE, from scalar or vector images to complex tensor ones and apply the new method to remove speckle noises of PolSAR images.	anisotropic diffusion;noise reduction;vector graphics	Nan Xue;Gui-Song Xia;Liangpei Zhang	2014	2014 IEEE Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2014.6947287	computer vision;optics;nuclear magnetic resonance;anisotropic diffusion;physics	Vision	72.3218347492022	-67.00530424289605	1120
ee5bd0ab876cf319bdedd09c2da5843f6ae714cd	fast deep neural networks with knowledge guided training and predicted regions of interests for real-time video object detection		It has been recognized that deeper and wider neural networks are continuously advancing the state-of-the-art performance of various computer vision and machine learning tasks. However, they often require large sets of labeled data for effective training and suffer from extremely high computational complexity, preventing them from being deployed in real-time systems, for example vehicle object detection from vehicle cameras for assisted driving. In this paper, we aim to develop a fast deep neural network for real-time video object detection by exploring the ideas of knowledge-guided training and predicted regions of interest. Specifically, we will develop a new framework for training deep neural networks on datasets with limited labeled samples using cross-network knowledge projection which is able to improve the network performance while reducing the overall computational complexity significantly. A large pre-trained teacher network is used to observe samples from the training data. A projection matrix is learned to project this teacher-level knowledge and its visual representations from an intermediate layer of the teacher network to an intermediate layer of a thinner and faster student network to guide and regulate the training process. To further speed up the network, we propose to train a low-complexity object detection using traditional machine learning methods, such as support vector machine. Using this low-complexity object detector, we identify the regions of interest that contain the target objects with high confidence. We obtain a mathematical formula to estimate the regions of interest to save the computation for each convolution layer. Our experimental results on vehicle detection from videos demonstrated that the proposed method is able to speed up the network by up to 16 times while maintaining the object detection performance.	artificial neural network;computation;computational complexity theory;computer vision;convolution;deep learning;machine learning;network performance;neural networks;object detection;real-time clock;real-time computing;real-time transcription;region of interest;speedup;support vector machine	Wenming Cao;Jianhe Yuan;Zhihai He;Zhi Zhang;Zhiquan He	2018	IEEE Access	10.1109/ACCESS.2018.2795798	speedup;distributed computing;artificial neural network;computer science;computational complexity theory;computation;object detection;support vector machine;machine learning;projection (linear algebra);network performance;artificial intelligence	Vision	22.775642220933417	-50.26487660499811	1126
b137d0a9160d5f3155c634333d197ce07456736f	identification and control of a wastewater treatment pilot by catalytic ozonation	catalysis;continuous time systems;identification;ozonation (materials processing);transfer functions;wastewater treatment;absorbance;catalytic ozonation pilot optimization;catalytic ozonation process;continuous-time transfer function model;deliberate ozone overproduction;internal model control;operating costs;oxygen consumption;pollutant abatement;wastewater treatment pilot control;wastewater treatment pilot identification	This paper deals with the control of a wastewater treatment pilot by catalytic ozonation. In general, catalytic ozonation processes operate with a deliberate ozone overproduction to obtain a treated water which respects the discharge standards. But, in this case, the oxygen consumption is not optimal and the operating costs are important. The objective of this study focuses on the optimization of the catalytic ozonation pilot. A continuous-time transfer function model is identified to represent the pilot behavior, and an internal model control is proposed to obtain a significant abatement of the pollutant. In this application, the pollutant abatement is represented by the absorbance.	discharger;ergodic theory;function model;iso 10303;mathematical optimization;multi-compartment model;optimal control;reactor (software);transfer function	Manhal Abouzlam;Régis Ouvrard;Driss Mehdi;Florence Pontlevoy;Bertrand Gombert;Nathalie Karpel Vel Leitner;Sahidou O. B. Boukari	2012	2012 American Control Conference (ACC)		identification;catalysis;environmental engineering;corona;computer science;engineering;sewage treatment;transfer function;waste management;inductor	Robotics	57.73624241208678	-5.974851618730737	1129
0e8f17f53411cbc3c4f3b1494e9d37bfd90b5cd9	comparative study of clustering based colour image segmentation techniques	cluster validation;pattern clustering;image segmentation;image color analysis image segmentation clustering algorithms indexes partitioning algorithms clustering methods algorithm design and analysis;self organising feature maps image colour analysis image segmentation pattern clustering;colour space analysis;silhouette width cluster validation techniques clustering based colour image segmentation techniques image processing pattern recognition cie l a b color spaces rgb hsv color images k means clustering algorithm partitioning around medoids method pam kohonen self organizing maps method som dunn index;self organising feature maps;clustering;image colour analysis;pattern recognition image segmentation colour representations colour space analysis clustering cluster validation;pattern recognition;colour representations	Image segmentation is very essential and critical to image processing and pattern recognition. Various clustering based segmentation methods have been proposed. However, it is very difficult to choose the method best suited to the type of data. Therefore, the objective of this research was to compare the effectiveness of three clustering methods involving RGB, HSV and CIE L*a*b* color spaces and a variety of real color images. The methods were: K-means clustering algorithm, Partitioning Around Medoids method (PAM) and Kohonen's Self-Organizing Maps method (SOM). To evaluate these three techniques, the connectivity(C), the Dunn index (D) and the silhouette width (S) cluster validation techniques were used. For C, a lower value indicates a better technique and for D and S, a higher value indicates a better technique. Clustering algorithms were evaluated on natural images and their performance is compared. Results demonstrate that K-means and SOM were considered to be the most suitable techniques for image segmentation among CIE L*a*b* and HSV colour spaces.	algorithm;cluster analysis;color depth;color space;computability in europe;computer cluster;dunn index;image processing;image segmentation;k-means clustering;k-medoids;medoid;pattern recognition;teuvo kohonen	Samira Chebbout;Hayet Farida Merouani	2012	2012 Eighth International Conference on Signal Image Technology and Internet Based Systems	10.1109/SITIS.2012.126	correlation clustering;computer vision;computer science;canopy clustering algorithm;machine learning;segmentation-based object categorization;consensus clustering;pattern recognition;cure data clustering algorithm;mathematics;region growing;image segmentation;cluster analysis;scale-space segmentation	Vision	42.40661404643039	-68.34545904623677	1132
695668583e1381c8a4701817b1e11b4e6ad67714	dissecting deep neural networks for better medical image classification and classification understanding		"""Neural networks, in the context of deep learning, show much promise in becoming an important tool with the purpose assisting medical doctors in disease detection during patient examinations. However, the current state of deep learning is something of a """"black box"""", making it very difficult to understand what internal processes lead to a given result. This is not only true for non-technical users but among experts as well. This lack of understanding has led to hesitation in the implementation of these methods among mission-critical fields, with many putting interpretability in front of actual performance. Motivated by increasing the acceptance and trust of these methods, and to make qualified decisions, we present a system that allows for the partial opening of this black box. This includes an investigation on what the neural network sees when making a prediction, to both, improve algorithmic understanding, and to gain intuition into what pre-processing steps may lead to better image classification performance. Furthermore, a significant part of a medical expert's time is spent preparing reports after medical examinations, and if we already have a system for dissecting the analysis done by the network, the same tool can be used for automatic examination documentation through content suggestions. In this paper, we present a system that can look into the layers of a deep neural network and present the network's decision in a way that that medical doctors may understand. Furthermore, we present and discuss how this information can possibly be used for automatic reporting. Our initial results are very promising."""	artificial neural network;black box;computer vision;deep learning;documentation;internationalization and localization;mission critical;neural networks;neural network software;preprocessor	Steven Alexander Hicks;Michael Riegler;Konstantin Pogorelov;Kim V. Anonsen;Thomas de Lange;Dag Johansen;Mattis Jeppsson;Kristin Ranheim Randel;Sigrun Losada Eskeland;Pål Halvorsen	2018	2018 IEEE 31st International Symposium on Computer-Based Medical Systems (CBMS)	10.1109/CBMS.2018.00070	black box (phreaking);documentation;data mining;data science;artificial neural network;deep learning;visualization;contextual image classification;computer science;interpretability;intuition;artificial intelligence	Arch	30.13868261011526	-74.06306253138395	1134
930a9cf56997c70a24e6585aeb3d4bf9d4dec950	an improved particle swarm optimizer based on tabu detecting and local learning strategy in a shrunk search space	premature convergence;particle swarm optimization;detecting strategy;shrink search space;local learning;subregions	To improve the performance of the standard particle swarm optimization (PSO) which suffers from premature convergence and slow convergence speed, many PSO variants introduce lots of stochastic or aimless strategies to overcome the convergence problem. However, the mutual learning between elites particles is omitted, although which might be benefit to the convergence speed and, prevent the premature convergence. In this paper, we introduce DSLPSO, which integrates three novel strategies, specifically, tabu detecting, shrinking and local learning strategies, into PSO to overcome the aforementioned shortcomings. In DSLPSO, search space of each dimension is divided into many equal subregions. Then the tabu detecting strategy, which has good ergodicity for search space, helps the global historical best particle to detect a more suitable subregion, and thus help it jump out of a local optimum. The shrinking strategy enables DSLPSO to take optimization in a smaller search space and obtain a higher convergence speed. In the local learning strategy, a differential between two elites particles is used to increase solution accuracy. The experimental results show that DSLPSO has a superior performance in comparison with several other participant PSOs on most of the tested functions, as well as offering faster convergence speed, higher solution accuracy and stronger reliability.	mathematical optimization;sensor;swarm;tabu search	Xuewen Xia;Jingnan Liu;Zongbo Hu	2014	Appl. Soft Comput.	10.1016/j.asoc.2014.06.012	mathematical optimization;computer science;artificial intelligence;machine learning;mathematics;particle swarm optimization;premature convergence	Robotics	27.319791257328077	-3.9631887512729405	1137
6d82a6c0d1ec8c22a8db8500301a4f822495a3e5	multiple illuminant estimation from the covariance of colors	white balancing;color space;color constancy;computer vision;dichromatic reflection model;principal component analysis;multiple illuminant estimation	In this paper we present a single and a multiple illuminant estimation physics-based algorithm. Both algorithms are based on the mean projections maximization assumption and un-centered component analysis. The proposed assumption is validated for a large collection of images and later used to estimate the illuminant color. The multiple illuminant estimator assumes that the spectral power distribution of the light source is not the same for the whole scene, which is the case for a wide range of images. In such cases, our new multiple illuminant estimator recovers an accurate illuminants estimates map for each input image while maintaining speed. The evaluation of the proposed algorithms on different real image datasets is realized. The experimental results are satisfying; our algorithms maximize the trade-off between accuracy (illuminant estimation error) and computational complexity.	algorithm;color;computational complexity theory;level of detail;maxima and minima;maximal set;pixel;the matrix;time complexity	Elkhamssa Lakehal;Djemel Ziou;Mohamed Benmohammed	2017	J. Visual Communication and Image Representation	10.1016/j.jvcir.2017.03.013	computer vision;computer science;illuminant d65;mathematics;color balance;color space;color constancy;statistics;principal component analysis	Vision	53.78972678176449	-55.00142896862924	1144
b6aa97c3b449fd62397c7ea969d0efe6a80921c3	stable inverse dynamic curves	inverse statics;inverse dynamics;video game;2d curve;stable equilibrium;elastica	2d animation is a traditional but fascinating domain that has recently regained popularity both in animated movies and video games. This paper introduces a method for automatically converting a smooth sketched curve into a 2d dynamic curve at stable equilibrium under gravity. The curve can then be physically animated to produce secondary motions in 2d animations or simple video games. Our approach proceeds in two steps. We first present a new technique to fit a smooth piecewise circular arcs curve to a sketched curve. Then we show how to compute the physical parameters of a dynamic rod model (super-circle) so that its stable rest shape under gravity exactly matches the fitted circular arcs curve. We demonstrate the interactivity and controllability of our approach on various examples where a user can intuitively setup efficient and precise 2d animations by specifying the input geometry.		Alexandre Derouet-Jourdan;Florence Bertails-Descoubes;Joëlle Thollot	2010	ACM Trans. Graph.	10.1145/1882261.1866159	mathematical optimization;simulation;mathematics;inverse dynamics;computer graphics (images)	Graphics	66.87298646206744	-46.727807536476824	1145
968ceecbbdb21e66e361be996f03b0407ad2f9ba	using one-step perturbation to predict the effect of changing force-field parameters on the simulated folding equilibrium of a beta-peptide in solution	peptide folding;force field;free enthalpy calculation;one step perturbation;β peptide	Computer simulation using molecular dynamics is increasingly used to simulate the folding equilibria of peptides and small proteins. Yet, the quality of the obtained results depends largely on the quality of the force field used. This comprises the solute as well as the solvent model and their energetic and entropic compatibility. It is, however, computational very expensive to perform test simulations for each combination of force-field parameters. Here, we use the one-step perturbation technique to predict the change of the free enthalpy of folding of a beta-peptide in methanol solution due to changing a variety of force-field parameters. The results show that changing the solute backbone partial charges affects the folding equilibrium, whereas this is relatively insensitive to changes in the force constants of the torsional energy terms of the force field. Extending the cut-off distance for nonbonded interactions beyond 1.4 nm does not affect the folding equilibrium. The same result is found for a change of the reaction-field permittivity for methanol from 17.7 to 30. The results are not sensitive to the criterion, e.g., atom-positional RMSD or number of hydrogen bonds, that is used to distinguish folded and unfolded conformations. Control simulations with perturbed Hamiltonians followed by backward one-step perturbation indicated that quite large perturbations still yield reliable results. Yet, perturbing all solvent molecules showed where the limitations of the one-step perturbation technique are met. The evaluated methodology constitutes an efficient tool in force-field development for molecular simulation by reducing the number of required separate simulations by orders of magnitude.		Zhixiong Lin;Hongmei Liu;Wilfred F. van Gunsteren	2010	Journal of computational chemistry	10.1002/jcc.21534	chemistry;force field;computational chemistry;physical chemistry;physics		97.61313767213412	-3.9307285749857566	1147
81d3f9f6fedf8634564be9d20aa6d4d85584833d	a geometric framework for stochastic shape analysis		We introduce a stochastic model of diffeomorphisms, whose action on a variety of data types descends to stochastic evolution of shapes, images and landmarks. The stochasticity is introduced in the vector field which transports the data in the large deformation diffeomorphic metric mapping framework for shape analysis and image registration. The stochasticity thereby models errors or uncertainties of the flow in following the prescribed deformation velocity. The approach is illustrated in the example of finitedimensional landmark manifolds, whose stochastic evolution is studied both via the Fokker–Planck equation and by numerical simulations. We derive two approaches for inferring parameters of the stochasticmodel from landmark configurations observed at discrete time points. The first of the two approaches matches moments of the Fokker– Planck equation to sample moments of the data, while the second approach employs an expectation-maximization based algorithm using a Monte Carlo bridge sampling scheme to optimise the data likelihood. We derive and numerically test the ability of the two approaches to infer the spatial correlation length of the underlying noise.	expectation–maximization algorithm;image registration;large deformation diffeomorphic metric mapping;monte carlo method;numerical analysis;sampling (signal processing);shape analysis (digital geometry);simulation;stochastic process;velocity (software development)	Alexis Arnaudon;Darryl D. Holm;Stefan Sommer	2017	CoRR	10.1007/s10208-018-9394-z	econometrics;mathematical optimization;mathematics;statistics	ML	38.537732137911036	-25.82192943903657	1152
995659734595a5104d2005f29ebb8de24db8614a	mutual information expansion for studying the role of correlations in population codes: how important are autocorrelations	desciframiento;autocorrelacion;correlacion;calcul neuronal;neural computation;train potentiel action;62p20;decodage;decoding;developpement mathematique;30bxx;series expansion;62h20;aproximacion;informacion mutual;mathematical expansion;approximation;spike;manipulacion;information mutuelle;desarrollo serie;population coding;analyse correlation;mutual information;spike train;manipulation;code;theorie information;correlation;reseau neuronal;codigo;red neuronal;computacion neuronal;62b10;information theory;analisis correlacion;developpement serie;potentiel action;neuronal discharge;autocorrelation;neural network;correlation analysis;teoria informacion	The role of correlations in the activity of neural populations responding to a set of stimuli can be studied within an information theory framework. Regardless of whether one approaches the problem from an encoding or decoding perspective, the main measures used to study the role of correlations can be derived from a common source: the expansion of the mutual information. Two main formalisms of mutual information expansion have been proposed: the series expansion and the exact breakdown. Here we clarify that these two formalisms have a different representation of autocorrelations, so that even when the total information estimated differs by less than 1, individual terms can diverge. More precisely, the series expansion explicitly evaluates the informational contribution of autocorrelations in the count of spikes, that is, count autocorrelations, whereas the exact breakdown does not. We propose a new formalism of mutual information expansion, the Poisson exact breakdown, which introduces Poisson equivalents in order to explicitly evaluate the informational contribution of count autocorrelations with no approximation involved. Because several widely employed manipulations of spike trains, most notably binning and pooling, alter the structure of count autocorrelations, the new formalism can provide a useful general framework for studying the role of correlations in population codes.	approximation;autocorrelation;disintegration (morphologic abnormality);equivalent weight;information theory;mutual information;neural oscillation;population;product binning;qr code;semantics (computer science);series expansion;informational	A. Scaglione;Guglielmo Foffani;G. Scannella;Sergio Cerutti;Karen A. Moxon	2008	Neural Computation	10.1162/neco.2008.08-07-595	autocorrelation;series expansion;information theory;artificial intelligence;approximation;calculus;mathematics;mutual information;neural coding;code;correlation;artificial neural network;statistics;models of neural computation	ML	21.27413334825016	-71.41302232134097	1154
48de8ab0bd3fc4b7930c2c4d65915df45f1a978e	hair self shadowing and transparency depth ordering using occupancy maps	shadow mapping;opacity maps;deep shadow maps;publikationer;konferensbidrag;hair rendering;artiklar;rapporter;high frequency	This paper presents a method for quickly constructing a high-quality approximate visibility function for high frequency semi-transparent geometry such as hair. We can then reconstruct the visibility for any fragment without the expensive compression needed by Deep Shadow Maps and with a quality that is much better than what is attainable at similar framerates using Opacity Maps or Deep Opacity Maps. The memory footprint of our method is also considerably lower than that of previous methods. We then use a similar method to achieve back-to-front sorted alpha blending of the fragments with results that are virtually indistinguishable from depth-peeling and an order of magnitude faster.	alpha compositing;approximation algorithm;depth peeling;map;memory footprint;self-shadowing;semiconductor industry;shadow mapping	Erik Sintorn;Ulf Assarsson	2009		10.1145/1507149.1507160	computer science;high frequency;shadow mapping;computer graphics (images)	Graphics	65.82959875648675	-51.648536619454916	1160
7c0251ea1f5bc89ee7e833637fb2a8221724f72f	nonparametric likelihood inference for general autoregressive models	seasonal unit roots;martingale inference;brownian motion;autoregressive model;functional of brownian motion;seasonality;double unit roots;martingale difference sequence;finite sample properties;unit root;monte carlo;empirical likelihood	This paper shows how nonparametric likelihood inference for autoregressive models can be based on the family of “empirical” Cressie–Read statistics. The results of the paper apply to possibly nonstationary autoregressive models with innovations that form a martingale difference sequence, and can accommodate multiple and complex unit roots, as well as deterministic components. As an application, the paper considers nonparametric likelihood-based tests for seasonal unit roots and for double unit roots. Monte Carlo evidence seems to suggest that the proposed tests have competitive finite sample properties.	autoregressive model	Francesco Bravo	2010	Statistical Methods and Applications	10.1007/s10260-009-0122-3	econometrics;mathematical optimization;star model;brownian motion;mathematics;martingale difference sequence;autoregressive model;unit root;seasonality;statistics;monte carlo method	ML	30.95395743948363	-22.37838909952636	1167
83e97d16c17e1ae1587ba42580235c9b983142e0	copper clad laminate defects classification using inverse wavelets transform and support vector machine	copper;support vector machine;wavelet transform	Ferrous metal casting of manganese steel susceptible to austenization to develop minimum yield strength of about 75,000 psi and elongation of about 30% min. consisting essentially of: C-0.85 Mn-14 Si-0.	support vector machine;wavelet	Te-sheng Li	2007			wavelet transform;wavelet;inverse;support vector machine;yield (engineering);copper;casting (metalworking);discrete wavelet transform;materials science;pattern recognition;artificial intelligence	ML	87.76643046679864	-17.466298628661374	1168
37b3a77265ffb743794cb6ed3bf5df6713998d3f	expath: a database of comparative expression analysis inferring metabolic pathways for plants	gene expression regulation plant;plants;metabolic networks and pathways;databases genetic;animal genetics and genomics;zea mays;internet;life sciences general;arabidopsis;oryza;algorithms;transcriptome;microbial genetics and genomics;proteomics;computational biology;oligonucleotide array sequence analysis;microarrays;gene ontology;plant genetics genomics	In general, the expression of gene alters conditionally to catalyze a specific metabolic pathway. Microarray-based datasets have been massively produced to monitor gene expression levels in parallel with numerous experimental treatments. Although several studies facilitated the linkage of gene expression data and metabolic pathways, none of them are amassed for plants. Moreover, advanced analysis such as pathways enrichment or how genes express under different conditions is not rendered. Therefore, EXPath was developed to not only comprehensively congregate the public microarray expression data from over 1000 samples in biotic stress, abiotic stress, and hormone secretion but also allow the usage of this abundant resource for coexpression analysis and differentially expression genes (DEGs) identification, finally inferring the enriched KEGG pathways and gene ontology (GO) terms of three model plants: Arabidopsis thaliana, Oryza sativa, and Zea mays. Users can access the gene expression patterns of interest under various conditions via five main functions (Gene Search, Pathway Search, DEGs Search, Pathways/GO Enrichment, and Coexpression analysis) in EXPath, which are presented by a user-friendly interface and valuable for further research. In conclusion, EXPath, freely available at http://expath.itps.ncku.edu.tw , is a database resource that collects and utilizes gene expression profiles derived from microarray platforms under various conditions to infer metabolic pathways for plants.	gene expression;gene ontology term enrichment;gene regulatory network;inference;kegg;linkage (software);microarray;one thousand;oryza (plant);usability;genetic linkage	Chia-Hung Chien;Chi-Nga Chow;Nai-Yun Wu;Yi-Fan Chiang-Hsieh;Ping-Fu Hou;Wen-Chi Chang	2015		10.1186/1471-2164-16-S2-S6	biology;molecular biology;the internet;dna microarray;transcriptome;bioinformatics;proteomics;genetics	Comp.	-1.2294246434057512	-58.79594492972381	1173
38e1070adf0087991871dbc8169cbfcfe69b369b	weighted isotonic regression under the l1 norm	regression instance;fitting error;weighted isotonic regression;l1 norm;microarray analysis;weighted matching problem;isotonic regression;partial order;large-scale microarray data;l1 error measure;edge packing problem;isotonic regression problem	Isotonic regression, the problem of finding values that best fit given observations and conform to specific ordering constraints, has found many applications in biomedical research and other fields. When the constraints form a partial ordering, solving the problem under the L1 error measure takes O(n3) when there are n observations. The analysis of large-scale microarray data, which is one of the important tools in biology, using isotonic regression is hence expensive. This is because in microarray analysis, the same procedure is used for studying the fit of tens of thousands of genes to a given partial order. Fast estimation for the fitting error is therefore highly desired to reduce the number of regression instances through pruning. In this paper, we present approximation algorithms to the isotonic regression problem under the L1 error measure. We relate the problem to an edge packing problem and in the special case when the observations are not weighted, we relate it to a weighted matching problem.	approximation algorithm;curve fitting;isotonic regression;microarray;set packing;t-norm;taxicab geometry	Stanislav Angelov;Boulos Harb;Sampath Kannan;Li-San Wang	2006				ML	1.4046056322053213	-49.948012142282096	1174
08c56c95f04988e41a8a336ab48b751405aae32b	pseudo almost periodic solutions for fuzzy cellular neural networks with multi-proportional delays	fuzzy cellular neural networks;pseudo almost periodic solution;existence;global attractivity;multi-proportional delay;34c25;34k13;34k25	This paper deals with a class of fuzzy cellular neural networks with multi-proportional delays. By applying the contraction mapping fixed point theorem and differential inequality techniques, a set of easily verifiable sufficient conditions are established for the existence and global attractivity of a unique pseudo almost periodic solution for the model, which improve and supplement previously known researches. Moreover, a numerical example is given to illustrate the feasibility and application of the obtained results.	almost periodic function;contraction mapping;fixed point (mathematics);fixed-point theorem;formal verification;neural networks;numerical analysis;social inequality	Jiaxin Liang;Hao Qian;Bingwen Liu	2017	Neural Processing Letters	10.1007/s11063-017-9774-4	artificial intelligence;inequality;fuzzy logic;machine learning;mathematical optimization;cellular neural network;verifiable secret sharing;periodic graph (geometry);fixed-point theorem;mathematics;contraction mapping	ML	73.44715241229713	2.820685298968126	1178
173cc6a0649c621ad8cd6bf81e7eeaa3db12c141	the method of quantum clustering	support vector;hilbert space;scale space;clustering method;gaussian kernel;potential function;high dimension	We propose a novel clustering method that is an extension of ideas inherent to scale-space clustering and support-vector clustering. Like the latter, it associates every data point with a vector in Hilbert space, and like the former it puts emphasis on their total sum, that is equal to the scalespace probability function. The novelty of our approach is the study of an operator in Hilbert space, represented by the Schrödinger equation of which the probability function is a solution. This Schrödinger equation contains a potential function that can be derived analytically from the probability function. We associate minima of the potential with cluster centers. The method has one variable parameter, the scale of its Gaussian kernel. We demonstrate its applicability on known data sets. By limiting the evaluation of the Schrödinger potential to the locations of data points, we can apply this method to problems in high dimensions.	cluster analysis;data point;hilbert space;maxima and minima;potential method;quantum;scale space;schrödinger;support vector machine	David Horn;Assaf Gottlieb	2001			correlation clustering;constrained clustering;support vector machine;mathematical optimization;mathematical analysis;discrete mathematics;scale space;computer science;reproducing kernel hilbert space;mathematics;gaussian function;statistics;hilbert space	ML	3.29531856977367	-39.21859472480886	1179
5d2699d2fb7940b169b08173585cb6d27e64e2a1	networked architecture for distributed pc-based robot control systems	robot control;network architecture	An electric glass-melting furnace has a cylindrical orifice block formed of a single block of refractory metal and an orifice through which molten glass flows out of the furnace. The inner end of the block extends into the melting zone portion of the furnace, while the remainder of the block extends through the furnace lining and wall, with the extreme outer tip being flush with the outer face of a water jacket which surrounds the outer end portions of the block. The block has a reduced diameter portion adjacent the outer end of the block, so that the outer end face exposes minimal surface area to outside atmosphere. The direct contact of the cooling jacket with the outer end of the block keeps the temperature of this small mass of block low enough to avoid oxidation. No protective atmosphere is required for the exterior tip of the block.	control system;robot control	Yannick Dadji;Jochen Maass;Harald Michalik;Tobias Möglich;Nnamdi Kohn;J. Uwe Varchmin	2008			parallel computing;water jacket;computer science;body orifice;cylinder;robot control;composite material	Robotics	84.31990550966913	-19.235631373743697	1183
34cda44836496dd4fe7e22471af41d995977b568	sustained activities and retrieval in a computational model of the perirhinal cortex	computer model;novelty detection;partial information;visual working memory;working memory;dopamine;memory retrieval;perirhinal cortex	The perirhinal cortex is involved not only in object recognition and novelty detection but also in multimodal integration, reward association, and visual working memory. We propose a computational model that focuses on the role of the perirhinal cortex in working memory, particularly with respect to sustained activities and memory retrieval. This model describes how different partial informations are integrated into assemblies of neurons that represent the identity of an object. Through dopaminergic modulation, the resulting clusters can retrieve the global information with recurrent interactions between neurons. Dopamine leads to sustained activities after stimulus disappearance that form the basis of the involvement of the perirhinal cortex in visual working memory processes. The information carried by a cluster can also be retrieved by a partial thalamic or prefrontal stimulation. Thus, we suggest that areas involved in planning and memory coordination encode a pointer to access the detailed information encoded in the associative cortex such as the perirhinal cortex.	arm cortex-m;arm cortex-r;brodmann area 35;cerebral cortex;computation;computational model;delta modulation;dopamine hydrochloride;encode;memory disorders;multimodal interaction;novelty detection;outline of object recognition;pointer (computer programming);pointer <dog>;thalamic structure	Julien Vitay;Fred Henrik Hamker	2008	Journal of Cognitive Neuroscience	10.1162/jocn.2008.20147	psychology;computer simulation;cognitive psychology;interference theory;spatial memory;dopamine;neuroscience;long-term memory;developmental psychology;visual memory;semantic memory;working memory;methods used to study memory;visual short-term memory;recognition memory	ML	17.37834148992093	-76.08755284796268	1184
3cad935fafad60bd938a5a3011cad90cf2e4da96	increased theta oscillations during motor imagery in a subject with late-stage als		Non-invasive brain computer interface (BCI) has been successfully used to control cursors, helicopters and robotic arms. However, this technology is not widely adopted by people with late-stage amyotrophic lateral sclerosis (ALS) due to poor effectiveness. In this study, we attempt to assess the cognitive state of a completely locked-in ALS subject, and her ability to use motor imagery-based BCI for control. The subject achieves above chance level accuracies for both open loop (62.2%) and closed-loop (68.7%) 2-class movement vs. idle decoding. We also observe a prominent theta oscillation with peak frequency at 4.5 Hz during the experiments. Quantification shows that the theta oscillatory power increases during motor imagery tasks compared to idle tasks for both open-loop as well as closed-loop BCI tasks. Furthermore, for closed-loop sessions, theta oscillation power correlates positively with feedback accuracy during movement tasks, and negatively with feedback accuracy during idle tasks. Our study demonstrates the feasibility of motor imagery-based BCI for late-stage ALS subjects, and highlights the importance of feedback during BCI implementation.	amyotrophic lateral sclerosis;brain-computer interfaces;brain–computer interface;coat of arms;entity name part qualifier - adopted;experiment;guided imagery;hertz (hz);interface device component;lateral thinking;neural oscillation;quantitation;robot;robotic arm;theta model	Rosa Q. So;Tao Yang;Kok Soon Phua;Juanhong Yu;Valerie Toh;Wai Hoe Ng;Kai Keng Ang	2018	2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2018.8512411	brain–computer interface;task analysis;computer vision;amyotrophic lateral sclerosis;real-time computing;open-loop controller;artificial intelligence;robotic arm;electroencephalography;motor imagery;computer science;idle	Robotics	12.5840767895352	-82.11631846404877	1187
762efedb36630187d7874c140586bcde8ef86d3e	specimen specific, 3d modeling of the elbow - prediction of strain in the medial collateral ligament	kinematics biomechanics;biomechanics;kinematics;elbow strain ligaments sociology statistics orthopedic surgery educational institutions;cubic b spline models specimen specific 3d modeling elbow prediction strain medial collateral ligament cadaver elbows kinematic simulation software flexion extension axes congruent joint motion range of motion dissection ligament paths;cadaver collateral ligaments elbow joint humans imaging three dimensional models anatomic stress physiological	In this project 3D interactive models of twelve cadaver elbows are developed using the author's kinematic simulation software. The effective flexion-extension axes for each specimen's model are iteratively defined based upon congruent joint motion and individual limits in range-of-motion. Origins and insertions of both parts of the medial collateral ligament are digitized following careful dissection of each specimen. Ligament paths are then defined using cubic B-spline models of the principal fibers of each part, flexion extension motion of each elbow is carried out in real-time and the strain of each fiber model is calculated. Results indicate the existence of two distinct populations of medial collateral ligament - one whose anterior part stretches during flexion of the elbow and the other whose anterior part stretches during extension.	3d modeling;b-spline;biological specimen;cdisc send biospecimens terminology;cadaver;clinical act of insertion;cubic function;medial graph;national origin;population;real-time clock;simulation software;structure of collateral ligament;tissue fiber	William L. Buford;Joris W. Snijders;Vikas V. Patel;Cody M. Curry;Brian A. Smith	2012	2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/EMBC.2012.6346682	kinematics;engineering;biomechanics;physiology;engineering drawing;physics;anatomy;surgery	Robotics	30.068131656695307	-84.64462751972654	1193
075fc469c19ba26a7a3da2d6bfe0379bb3a37312	modelling exchange rate volatility	second order;volatility;modelo autorregresivo;chaos;chaotic dynamics;caos;orden 2;statistical model;autoregressive model;random walk hypothesis;random walk;modele statistique;modele lorenz;taux change;conditional variance;exchange rate;volatilite;modelo estadistico;exchange rate volatility;ordre 2;marcha aleatoria;volatibilidad;lorenz model;modele autoregressif;marche aleatoire;tasa cambio;modelo lorenz;variance;variancia	Two types of statistical models are empirically applied to test the pattern of volatility in the exchange rate markets. One considers the autoregressive models and tests the random walk hypothesis. The other considers the conditional variance process and tests the hypothesis of chaotic dynamics. Empirical results mostly support the random walk hypothesis and also the existence of Lorenz-type chaos.	autoregressive model;chaos theory;statistical model;volatility	Jati K. Sengupta;Raymond E. Sfeir	1997	Int. J. Systems Science	10.1080/00207729708929423	statistical model;econometrics;volatility;conditional variance;mathematics;variance;autoregressive model;stochastic volatility;random walk hypothesis;random walk;second-order logic;statistics	ML	33.37366271446759	-21.920445743896778	1201
b4dc0cf115f48cec31cb136141dabfe1b9595275	fast approximate natural gradient descent in a kronecker factored eigenbasis		Optimization algorithms that leverage gradient covariance information, such as variants of natural gradient descent (Amari, 1998), offer the prospect of yielding more effective descent directions. For models with many parameters, the covariance matrix they are based on becomes gigantic, making them inapplicable in their original form. This has motivated research into both simple diagonal approximations and more sophisticated factored approximations such as KFAC (Heskes, 2000; Martens & Grosse, 2015; Grosse & Martens, 2016). In the present work we draw inspiration from both to propose a novel approximation that is provably better than KFAC and amendable to cheap partial updates. It consists in tracking a diagonal variance, not in parameter coordinates, but in a Kronecker-factored eigenbasis, in which the diagonal approximation is likely to be more effective. Experiments show improvements over KFAC in optimization speed for several deep network architectures.	approximation algorithm;fisher information;formation matrix;generalization error;information geometry;iteration;mathematical optimization;stochastic gradient descent	Thomas George;César Laurent;Xavier Bouthillier;Nicolas Ballas;Pascal Vincent	2018			rendering (computer graphics);mathematical optimization;kronecker delta;covariance matrix;diagonal;mathematics;gradient descent;covariance	ML	24.53073144549533	-34.483990522395615	1204
62ac3c7ac5aa7871129053373e266e9646570813	boosting with a joint feature pool from different sensors	range data;mobile robot;real time;system performance;spatial correlation;distance metric;color vision	This paper introduces a new way to apply boosting to a joint feature pool from different sensors, namely 3D range data and color vision. The combination of sensors strengthens the systems universality, since an object category could be partially consistent in shape, texture or both. Merging of different sensor data is performed by computing a spatial correlation on 2D layers. An AdaBoost classifier is learned by boosting features competitively in parallel from every sensor layer. Additionally, the system uses new corner-like features instead of rotated Haar-like features, in order to improve real-time classification capabilities. Object type dependent color information is integrated by applying a distance metric to hue values. The system was implemented on a mobile robot and trained to recognize four different object categories: people, cars, bicycle and power sockets. Experiments were conducted to compare system performances between different merged and single sensor based classifiers. We found that for all object categories the classification performance is considerably improved by the joint feature pool.	adaboost;autonomous robot;boosting (machine learning);color vision;experiment;gentle;haar wavelet;initialization (programming);mobile robot;object type (object-oriented programming);performance;real-time clock;sensor;time-of-flight camera;universality probability;zcam	Dominik A. Klein;Dirk Schulz;Simone Frintrop	2009		10.1007/978-3-642-04667-4_7	mobile robot;computer vision;spatial correlation;metric;boosting methods for object categorization;computer science;artificial intelligence;machine learning;pattern recognition;computer performance;color vision	Robotics	32.041645240276374	-53.62315491943472	1208
7847f45bdb1cf6ba50688ddcdff69ffbc66aea6b	vertical cup-to-disc ratio measurement for diagnosis of glaucoma on fundus images	optic disc extraction;computer aided diagnosis;ocular disease;edge detection;cup to disc ratio;vertical profile;blood vessel;blindness;medical diagnostics;determination of cup edge;retinal fundus image;glaucoma;early detection;blood vessels;color image	Glaucoma is a leading cause of permanent blindness. Retinal fundus image examination is useful for early detection of glaucoma. In order to evaluate the presence of glaucoma, the ophthalmologists determine the cup and disc areas and they diagnose glaucoma using a vertical cup-to-disc ratio. However, determination of the cup area is very difficult, thus we propose a method to measure the cup-to-disc ratio using a vertical profile on the optic disc. First, the blood vessels were erased from the image and then the edge of optic disc was then detected by use of a canny edge detection filter. Twenty profiles were then obtained around the center of the optic disc in the vertical direction on blue channel of the color image, and the profile was smoothed by averaging these profiles. After that, the edge of the cup area on the vertical profile was determined by thresholding technique. Lastly, the vertical cup-to-disc ratio was calculated. Using seventy nine images, including twenty five glaucoma images, the sensitivity of 80% and a specificity of 85% were achieved with this method. These results indicated that this method can be useful for the analysis of the optic disc in glaucoma examinations.	canny edge detector;channel (digital image);color image;concordance (publishing);edge detection;level of measurement;sensitivity and specificity;smoothing;thresholding (image processing);virtual retinal display	Yuji Hatanaka;Atsushi Noudo;Chisako Muramatsu;Akira Sawada;Takeshi Hara;Tetsuya Yamamoto;Hiroshi Fujita	2010		10.1117/12.843775	computer vision;edge detection;color image;optics	Vision	37.12328676602618	-76.09781347594826	1209
c6dbec9496485c6c9c838f6f2b82f402414c10e9	joint fingerprinting and decryption method for color images based on quaternion rotation with cipher quaternion chaining	collusion attack;joint fingerprinting and decryption;traitor tracing;fingerprinting;quaternions	Quaternion-based joint fingerprinting and decryption method has been proposed.The proposed method uses a cipher based on quaternion rotations.Imperceptibility of fingerprints embedded in color images has been achieved.The proposed method was tested against noise, compression, and collusion attacks.The proposed fingerprinting method has high robustness against collusion attacks. This paper addresses the problem of unauthorized redistribution of multimedia content by malicious users (pirates). In this method three color channels of the image are considered a 3D space and each component of the image is represented as a point in this 3D space. The distribution side uses a symmetric cipher to encrypt perceptually essential components of the image with the encryption key and then sends the encrypted data via multicast transmission to all users. The encryption involves rotation, and translation of points in 3D color space using quaternion algebra. Each user has a unique decryption key which is different from the encryption key. The differences between the common encryption key and the individual user's decryption key cause the decrypted image to contain minor changes which are user's fingerprint. A computer-based simulation was conducted to examine the method's robustness against noise, compression, and collusion attacks.	cipher;cryptography;fingerprint (computing);quaternions and spatial rotation	Bartosz Czaplewski	2016	J. Visual Communication and Image Representation	10.1016/j.jvcir.2016.06.006	fingerprint;theoretical computer science;mathematics;internet privacy;computer security;quaternion	Vision	38.19612758573366	-11.53106972932551	1211
5ed4a0fa1abcfb82c7915da491d76c9a3affcafa	preconditioning methods for improved convergence rates in iterative reconstructions	preconditionnement;iterative method;radiology;poisson noise;filtering;filtrage;medical imagery;poisson noise 2d frequency domain filtering medical diagnostic imaging convergence rates iterative reconstructions tomographic inversion problem steepest descent algorithms residuals;rapid technique;processing;convergence;tecnica rapida;image processing;image resolution;high spatial frequency;radiology and nuclear medicine;genie biomedical;convergence rates;biomedical radiography;relacion convergencia;improvement;tomography 550602 medicine external radiation in diagnostics 1980;performance;filtrado;iterative reconstructions;filters;tomographic inversion problem;scattering;preconditioning;tomocentelleografia;taux convergence;convergence rate;attenuation;mathematical logic;positron emission tomography;metodo iterativo;iterative reconstruction;iterative methods;reconstruction image;accuracy;residuals;biomedical engineering;inverse problem;diagnostic techniques;reconstruccion imagen;exploration radioisotopique;methode domaine frequence;methode iterative;frequency domain method;image reconstruction;amelioration;tomographie;computerized tomography;computerised tomography;2d frequency domain filtering;imagerie medicale;technique rapide;radionuclide study;iterative methods computerised tomography image reconstruction;mejoria;algorithms;medicine;precondicionamiento;ingenieria biomedica;nuclear medicine;optimization;convergence iterative methods image reconstruction tomography equations filters filtering image resolution attenuation scattering;imageneria medical;metodo dominio frecuencia;steepest descent algorithms;tomografia;frequency domain;iteration method;exploracion radioisotopica;tomography;tomoscintigraphie;medical diagnostic imaging;steepest descent	Because of the characteristics of the tomographic inversion problem, iterative reconstruction techniques often suffer from poor convergence rates-especially at high spatial frequencies. By using preconditioning methods, the convergence properties of most iterative methods can be greatly enhanced without changing their ultimate solution. To increase reconstruction speed, spatially invariant preconditioning filters that can be designed using the tomographic system response and implemented using 2-D frequency-domain filtering techniques have been applied. In a sample application, reconstructions from noiseless, simulated projection data, were performed using preconditioned and conventional steepest-descent algorithms. The preconditioned methods demonstrated residuals that were up to a factor of 30 lower than the assisted algorithms at the same iteration. Applications of these methods to regularized reconstructions from projection data containing Poisson noise showed similar, although not as dramatic, behavior.	algorithm;convergence (action);gradient descent;iteration;iterative method;iterative reconstruction;preconditioner;shot noise;tomographic reconstruction	Neal H. Clinthorne;Tin-Su Pan;Ping-Chun Chiao;W. Leslie Rogers;John A. Stamos	1993	IEEE transactions on medical imaging	10.1109/42.222670	computer vision;mathematical optimization;radiology;computer science;mathematics;iterative method;tomography;optics	Visualization	52.29800064927609	-79.41639072854997	1213
abc10e5ef20f8d77cd7c3a574b51e860ca39379e	scale invariant detection and tracking of elongated structures	urban environment;obstacle detection;rate of change;interest points;visual motion;time to contact;particle filter;scale invariance	This paper describes a method for the detection of tracking of elongated structures that is robust under changes of scale and orientation. This method is based on extending the concept of scale invariant natural interest points to include elongated ridge structures. An operator is proposed that directly detects ridge points and provides an estimation of their elongation and orientation. A tracking process is used to follow elongated features over time and to robustly observe changes in scale and orientation. Changes in scale are used to directly estimate time to contact. Experimental results demonstrate that the method works well in cluttered scenes that are typical of urban environments.	algorithm;interest point detection;particle filter;repeatability;sensor;viewing angle	Amaury Nègre;James L. Crowley;Christian Laugier	2008		10.1007/978-3-642-00196-3_60	computer vision;simulation;particle filter;computer science;scale invariance	Vision	51.417614768119556	-38.786266240614445	1216
e6c3eb3e47da4030cf279868f80cac3915d5c7fa	sensitive analysis of radial basis function networks for fault tolerance purposes	system reliability;analisis sensibilidad;fiabilite systeme;analisis sistema;fault tolerant;fiabilidad sistema;fault tolerant system;radial basis function;fonction radiale base;radial basis function network;sensitivity analysis;sistema tolerando faltas;system analysis;analyse sensibilite;systeme tolerant les pannes;analyse systeme;reseau neuronal;red neuronal;neural network	This paper introduces the concept of sensitivity in radial basis function networks. By applying a fault methodology combined with the information provided by the sensitivity of the performance error to faulty elements, faulting selection method can be simplified. In addition, the relation established between the sensitivity and a measure of the system fault tolerance permit to determine the most critical neural elements in the sense of fault tolerance. The theoretical predictions are verified by simulation experiments on two groups of problems -classification and approximation problems. In , this paper presents the application of sensitivity analysis for determining the most critical neural elements in the sense of fault tolerance.	fault tolerance;radial (radio);radial basis function	Xavier Parra;Andreu Català	1999		10.1007/BFb0098214	fault tolerance;computer science;artificial intelligence;stuck-at fault;machine learning;artificial neural network;algorithm	Vision	10.0282951405133	-28.66959128483894	1217
2a22857a8328c3cb33e88631b1cf78be40b1dd8d	false-name manipulations in weighted voting games	weighted voting games	Weighted voting is a classic model of cooperation among agents in decision-making domains. In such games, each player has a weight, and a coalition of players wins the game if its total weight meets or exceeds a given quota. A player’s power in such games is usually not directly proportional to his weight, and is measured by a power index, the most prominent among which are the Shapley–Shubik index and the Banzhaf index. In this paper, we investigate by how much a player can change his power, as measured by the Shapley–Shubik index or the Banzhaf index, by means of a false-name manipulation, i.e., splitting his weight among two or more identities. For both indices, we provide upper and lower bounds on the effect of weight-splitting. We then show that checking whether a beneficial split exists is NP-hard, and discuss efficient algorithms for restricted cases of this problem, as well as randomized algorithms for the general case. We also provide an experimental evaluation of these algorithms. Finally, we examine related forms of manipulative behavior, such as annexation, where a player subsumes other players, or merging, where several players unite into one. We characterize the computational complexity of such manipulations and provide limits on their effects. For the Banzhaf index, we describe a new paradox, which we term the Annexation Non-monotonicity Paradox.	agent-based model;computation;computational complexity theory;index (publishing);multi-agent system;np-hardness;non-repudiation;randomized algorithm;stable marriage problem;time complexity	Haris Aziz;Yoram Bachrach;Edith Elkind;Mike Paterson	2011	J. Artif. Intell. Res.	10.1613/jair.3166	simulation;computer science;mathematics;mathematical economics	AI	-3.172326011639891	0.6266211947569517	1218
4c2154cc8f4c5f153f2c40f15f60c30a35a0be0e	quantization of the rolling-body problem with applications to motion planning	quantized systems;robot hand;body of revolution;low complexity;reachable set;motion planning;scientific communication;rolling bodies;communication channels	The problem of manipulation by low–complexity robot hands is a key issue since many years. The performance of simplified hardware manipulators relies on the exploitation of nonholonomic effects that occur in rolling. Beside this issue, more recently, the attention of the scientific community has been devoted to the problems of finite capacity communication channels and of constraints on the the complexity of computation. Quantization of controls proved to be efficient for dealing with such kind of limitations. With this in mind, we consider the rolling of a pair of smooth convex objects, one on top of the other, under quantized control. The analysis of the reachable set is performed by exploiting the geometric nature of the system which helps in reducing to the case of a group acting on a manifold. The cases of a plane, a sphere and a body of revolution rolling on an arbitrary surface are treated in detail.	computation;context of computational complexity;exploit (computer security);motion planning;quantization (signal processing)	Yacine Chitour;Alessia Marigo;Benedetto Piccoli	2005	Systems & Control Letters	10.1016/j.sysconle.2005.02.012	control engineering;simulation;control theory;mathematics;motion planning;channel	Robotics	69.66102063111762	-19.455066675933995	1219
51704a52d5a70a426524d8bf48396b20b395c0d7	constructing intermediate concepts by decomposition of real functions	decomposition method;learning from examples	In learning from examples it is often useful to expand an attribute-vector representation by intermediate concepts. The usual advantage of such structuring of the learning problem is that it makes the learning easier and improves the comprehensibility of induced descriptions. In this paper, we develop a technique for discovering useful intermediate concepts when both the class and the attributes are real-valued. The technique is based on a decomposition method originally developed for the design of switching circuits and recently extended to handle incompletely speci ed multi-valued functions. It was also applied to machine learning tasks. In this paper, we introduce modi cations, needed to decompose real functions and to present them in symbolic form. The method is evaluated on a number of test functions. The results show that the method correctly decomposes fairly complex functions. The decomposition hierarchy does not depend on a given repertoir of basic functions (background knowledge).	distribution (mathematics);machine learning	Janez Demsar;Blaz Zupan;Marko Bohanec;Ivan Bratko	1997		10.1007/3-540-62858-4_75	discrete mathematics;theoretical computer science;decomposition;machine learning;mathematics	AI	2.141392498221293	-29.150194968728684	1221
2231a83bbe5d6c85b14f39d4bee7bf1346339b77	energy minimization and molecular dynamics studies of asn-102 elastase	hydrogen bond;enzyme activity;serine protease;enzyme;enzyme mechanism;molecular dynamic;energy minimization;active site;md simulation	Four isomeric forms of the Asn-102 PPE (D102N mutant according to the emerging protocol, [Knowles, Science, 236 (1987) 1252-1258]) have been investigated using energy minimization (EM) and molecular dynamics (MD) techniques. MD simulation data for 175 ps are reported for each form (in total 700 ps for about 2500 atoms). The His-57 N epsilon-protonated forms are calculated to be more stable than the N delta-protonated ones. The active site region of the most stable form is very similar to that found in the D102N rat trypsin enzyme [Craik et al., Science, 237 (1987) 909-913]. Conformations of the active sites and their hydrogen bond patterns are presented for each of these forms and are compared with the structure of the native enzyme active site. The pH dependent activity of the D102N derivative is discussed.		Bogdan Lesyng;Edgar F. Meyer	1987	Journal of computer-aided molecular design	10.1007/BF01677045	biochemistry;stereochemistry;enzyme;molecular dynamics;enzyme assay;chemistry;active site;computational chemistry;hydrogen bond;energy minimization	EDA	9.81804274481826	-61.786162742134586	1224
0f72d31199ff247e4dfe17248e1881147874fbee	mr prostate segmentation via distributed discriminative dictionary (ddd) learning	health research;uk clinical guidelines;biological patents;biological tissues;image segmentation;sparse dictionary learning;europe pubmed central;medical image processing biological organs biological tissues biomedical mri feature extraction gaussian distribution image segmentation learning artificial intelligence;deformable segmentation prostate segmentation magnetic resonance image sparse dictionary learning;citation search;biological organs;prostate segmentation;magnetic resonance image;uk phd theses thesis;feature extraction;medical image processing;life sciences;deformable segmentation;learning artificial intelligence;uk research reports;medical journals;gaussian distribution;europe pmc;biomedical research;dictionaries image segmentation shape active appearance model standards deformable models silicon;biomedical mri;magnetic resonance prostate segmentation dice ratio tissue differentiation prostate boundary optimal separation linear discriminant analysis discriminative feature space minimum redundancy maximum relevance feature selection tissue discriminative power discriminative fashion deformable model nonparametric fashion image appearance sparse dictionary learning method popular active appearance model nongaussian distribution mri distributed discriminative dictionary learning;bioinformatics	Segmenting prostate from MR images is important yet challenging. Due to non-Gaussian distribution of prostate appearances in MR images, the popular active appearance model (AAM) has its limited performance. Although the newly developed sparse dictionary learning method[1, 2] can model the image appearance in a non-parametric fashion, the learned dictionaries still lack the discriminative power between prostate and non-prostate tissues, which is critical for accurate prostate segmentation. In this paper, we propose to integrate deformable model with a novel learning scheme, namely the Distributed Discriminative Dictionary (DDD) learning, which can capture image appearance in a non-parametric and discriminative fashion. In particular, three strategies are designed to boost the tissue discriminative power of DDD. First, minimum Redundancy Maximum Relevance (mRMR) feature selection is performed to constrain the dictionary learning in a discriminative feature space. Second, linear discriminant analysis (LDA) is employed to assemble residuals from different dictionaries for optimal separation between prostate and non-prostate tissues. Third, instead of learning the global dictionaries, we learn a set of local dictionaries for the local regions (each with small appearance variations) along prostate boundary, thus achieving better tissue differentiation locally. In the application stage, DDDs will provide the appearance cues to robustly drive the deformable model onto the prostate boundary. Experiments on 50 MR prostate images show that our method can yield a Dice Ratio of 88% compared to the manual segmentations, and have 7% improvement over the conventional AAM.	active appearance model;anterior descending branch of left coronary artery;automatic acoustic management;body tissue;dictionaries, pharmaceutic;dictionary [publication type];feature selection;feature vector;linear discriminant analysis;machine learning;relevance;silo (dataset);sparse dictionary learning;sparse matrix;biologic segmentation	Yanrong Guo;Yiqiang Zhan;Yaozong Gao;Jianguo Jiang;Dinggang Shen	2013	2013 IEEE 10th International Symposium on Biomedical Imaging	10.1109/ISBI.2013.6556613	normal distribution;computer vision;radiology;feature extraction;computer science;magnetic resonance imaging;machine learning;pattern recognition;image segmentation	Vision	43.5430510561484	-77.44925148666992	1226
66b295b60d95176ee78de21b0e7d90bc6a5c6bc8	multi-focus image fusion using biochemical ion exchange model	structural similarity index measure;image fusion;multi focus;spatial frequency;biochemical ion exchange model	An effective and efficient multi-focus image fusion technique is presented for creating a more informative composite image for further human or machine perceptions. The proposed technique is based on a novel theory called biochemical ion exchange model (BIEM) which stems from the mechanism of the mineral nutrient uptake mechanism of the root. The source images with different focuses are considered to be the inputs of BIEM which is proposed in this paper, respectively, and the pixels can be viewed as the ions with different valences and polarities. The spatial frequency is chosen as the determinant of the valence. The final fused image can be obtained under the influence of BIEM and the new SSIM metric. Experimental results and relevant analysis indicate that the proposed fusion technique is promising and has remarked superiorities over other current typical fusion ones.	image fusion	Weiwei Kong;Yang Lei	2017	Appl. Soft Comput.	10.1016/j.asoc.2016.11.033	artificial intelligence;spatial frequency;image fusion	Robotics	17.839895777596933	-60.49251064051488	1227
36a64ec02ff3cbe8f155bf9f96f09d4dd44a1f58	phenotypic characterization of glioblastoma identified through shape descriptors	image segmentation;tissues;cancer;shape analysis;magnetic resonance imaging;computing systems	This paper proposes quantitatively describing the shape of glioblastoma (GBM) tissue phenotypes as a set of shape features derived from segmentations, for the purposes of discriminating between GBM phenotypes and monitoring tumor progression. GBM patients were identified from the Cancer Genome Atlas, and quantitative MR imaging data were obtained from the Cancer Imaging Archive. Three GBM tissue phenotypes are considered including necrosis, active tumor and edema/invasion. Volumetric tissue segmentations are obtained from registered T1˗weighted (T1˗WI) postcontrast and fluid-attenuated inversion recovery (FLAIR) MRI modalities. Shape features are computed from respective tissue phenotype segmentations, and a Kruskal-Wallis test was employed to select features capable of classification with a significance level of p < 0.05. Several classifier models are employed to distinguish phenotypes, where a leave-one-out cross-validation was performed. Eight features were found statistically significant for classifying GBM phenotypes with p <0.05, orientation is uninformative. Quantitative evaluations show the SVM results in the highest classification accuracy of 87.50%, sensitivity of 94.59% and specificity of 92.77%. In summary, the shape descriptors proposed in this work show high performance in predicting GBM tissue phenotypes. They are thus closely linked to morphological characteristics of GBM phenotypes and could potentially be used in a computer assisted labeling system. © (2016) COPYRIGHT Society of Photo-Optical Instrumentation Engineers (SPIE). Downloading of the abstract is permitted for personal use only.		Ahmad Chaddad;Christian Desrosiers;Matthew Toews	2016		10.1117/12.2209121	computer vision;bioinformatics;magnetic resonance imaging;shape analysis;image segmentation;cancer	Vision	35.99638140413904	-77.13441028360292	1228
041ec182985eb5efe4362793571896ed0cb8f279	predicting fold novelty based on protonet hierarchical classification	genomique;genomics;proteine;estructura;genomica;methode;hierarchical classification;identification;classification hierarchique;number;identificacion;proteina;classification automatique;nombre;automatic classification;protein;metodo;clasificacion automatica;clasificacion jerarquizada;method;structure;numero	MOTIVATION Structural genomics projects aim to solve a large number of protein structures with the ultimate objective of representing the entire protein space. The computational challenge is to identify and prioritize a small set of proteins with new, currently unknown, superfamilies or folds.   RESULTS We develop a method that assigns each protein a likelihood of it belonging to a new, yet undetermined, structural superfamily. The method relies on a variant of ProtoNet, an automatic hierarchical classification scheme of all protein sequences from SwissProt. Our results show that proteins that are remote from solved structures in the ProtoNet hierarchy are more likely to belong to new superfamilies. The results are validated against SCOP releases from recent years that account for about half of the solved structures known to date. We show that our new method and the representation of ProtoNet are superior in detecting new targets, compared to our previous method using ProtoMap classification. Furthermore, our method outperforms PSI-BLAST search in detecting potential new superfamilies.	amino acid sequence;blast;comparison and contrast of classification schemes in linguistics and metadata;computation;peptide sequence;protomap (proteomics);superfamily;swiss-prot;scop;sensor;staphylococcal protein a;uniprot	Ilona Kifer;Ori Sasson;Michal Linial	2005	Bioinformatics	10.1093/bioinformatics/bti135	identification;structure;genomics;method;numero sign;computer science;bioinformatics;artificial intelligence;mathematics;grammatical number;algorithm	Comp.	-3.5630362179787594	-55.353955109545865	1231
79d09f8fcbe502c096e3b0b2b12c9e0ecf46654a	perceptual based stereoscopic content analysis using salient information, dense disparity maps, and modified random walk framework	motion pictures;video signal processing;resistance stereo image processing motion pictures immune system humans production;cinematography;resistance;video signal processing cinematography stereo image processing;dynamic visual importance analysis perceptual based stereoscopic content analysis tool salient information random walk framework 3d movie production 3d effects human perception behavior 3d content dense disparity map estimation static analyzer static visual importance analysis dynamic salient analyzer video frames;stereo image processing;immune system;production;humans	Stereoscopic content analysis tools are needed in 3D movie production to create exciting 3D effects while maintaining the visual comfort of viewers. Working practices in 3D movie production show that the behavior of human perception plays an important rule for determining the degree of comfort of a 3D content. In this paper, we propose perceptual based methods for stereoscopic content analysis. A modified random walk framework is proposed for dense disparity map estimation. A static analyzer that is useful for distinguishing blurred objects from clear ones is used for analyzing static visual importance. And a dynamic salient analyzer that is useful for distinguishing objects with single move or repeat moves in video frames is used for analyzing dynamic visual importance. These methods give us the flexibility to combine human perceptual factors for stereoscopic content analysis.	binocular disparity;map;static program analysis;stereoscopy	Wei-Jia Huang;An-Chun Luo;Wen-Chao Chen;Wei-Hao Huang	2012	2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops	10.1109/CVPRW.2012.6238902	computer vision;immune system;computer science;multimedia;cinematography;resistance;computer graphics (images)	Vision	58.752786379221796	-55.91867650886064	1237
f24e0532520b6c8eb4dcd1a6a48dea14f7b5eba2	design of focal brain cooling system for suppressing epileptic seizures		Epilepsy is a group of diseases caused by excessive neuronal activities, and one-quarter of the patients do not become seizure-free by the existing treatments. The potential treatments include focal brain cooling, which aims to cool the region where the excessive neuronal activities begin. We are developing a focal brain cooling system. The system delivers cold saline to a cranially implanted cooling device. The outflow is cooled by a Peltier device and pumped for circulation. The Peltier device and the pump are activated only when a seizure is predicted. In this research, the length of time for cooling the brain was calculated with a computational fluid dynamics (CFD)-based model of the focal brain cooling system. As a result, it takes less than 10 minutes for the average temperature 2 mm below the cooling device to reach 25.0 °C. It is much shorter than the time from seizure prediction to seizure onset when an existing algorithm for prediction is used.	algorithm;cns disorder;computation;computational fluid dynamics;computer cooling;cool - action;epilepsy;focal (programming language);hydrodynamics;implants;less than;onset (audio);patients;salineos;seizures;pump (device)	Kei Hata;Koichi Fujiwara;Manabu Kano;Takao Inoue;Sadahiro Nomura;Hirochika Imoto;Michiyasu Suzuki	2017	2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2017.8036817	electronic engineering;water cooling;epilepsy;thermoelectric cooling;computer science	Visualization	18.975058397843956	-82.69761781216968	1240
e69ddfa5ee9c5093f64498afd7decffa29151f40	three-dimensional euclidean distance transformation and its application to shortest path planning	shortest path planning;shortest path;mathematical morphology;image processing;time complexity;distance transformation;euclidean distance;three dimensional;distance transform	In this paper, we present a novel method to obtain the three-dimensional Euclidean distance transformation (EDT) in two scans of the image. The shortest path can be extracted based on the distance maps using the minimum value tracing. The EDT is obtained correctly and e5ciently in a constant time for arbitrary types of images, including the existence of obstacles. By adopting the new dynamically rotational mathematical morphology, we not only guarantee the collision-free in the shortest path, but also reduce the time complexity dramatically. ? 2003 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.	algorithm;euclidean distance;event dispatching thread;map;mathematical morphology;motion planning;pattern recognition;preprocessor;shortest path problem;time complexity	Frank Y. Shih;Yi-Ta Wu	2004	Pattern Recognition	10.1016/j.patcog.2003.08.003	time complexity;three-dimensional space;computer vision;mathematical optimization;combinatorics;minkowski distance;mathematical morphology;constrained shortest path first;image processing;computer science;euclidean shortest path;yen's algorithm;euclidean distance;mathematics;geometry;euclidean distance matrix;distance transform;shortest path problem;distance;distance;k shortest path routing;shortest path faster algorithm	AI	48.392086749196274	-62.369638074767416	1247
e02c8689092d43a6f0416766c633d8c21ec2e2c6	adaptive estimation of normals and surface area for discrete 3-d objects: application to snow binary data from x-ray tomography	x ray tomography adaptive filtering discrete geometry distance map normal vectors snow surface area;sensitivity and specificity;filtering;rendu image;optimisation;image measurement problem;champ vectoriel;object distance map;image processing;modelo 3 dimensiones;imaging three dimensional;capsula convexa;normal vector field;x ray imaging;restitucion imagen;snow;adaptive filtering;projection method adaptive normal estimation adaptive surface area estimation discrete 3d object snow binary data x ray tomography normal vector field rendering image measurement problem local convexity degree gradient vector field analysis object distance map surface voxel optimization digitization effect;gradient method;modele 3 dimensions;digitizing;filtrado adaptable;geometry;projection method;convexite;procesamiento imagen;three dimensional model;radiographic image enhancement;computational method;tomography x ray;numerisation;qualite image;traitement image;three dimensional;voxel;convexidad;neige;enveloppe convexe;algorithme;signal processing computer assisted;methode gradient;algorithm;campo vectorial;feedback;smoothing methods;adaptive filters;cluster analysis;hydrological techniques snow tomography rendering computer graphics gradient methods optimisation geophysical signal processing x ray imaging;imagerie rx;shape;methode projection;triangulacion;distance map;fenomeno meteorologico;measurement problem;metodo gradiente;geophysical signal processing;image quality;discrete geometry;metodo proyeccion;reproducibility of results;phenomene meteorologique;discrete 3d object;image rendering;local convexity degree;algorithms artificial intelligence cluster analysis feedback imaging three dimensional information storage and retrieval pattern recognition automated radiographic image enhancement radiographic image interpretation computer assisted reproducibility of results sensitivity and specificity signal processing computer assisted tomography x ray;gradient methods;x ray radiography	Estimating the normal vector field on the boundary of discrete three-dimensional objects is essential for rendering and image measurement problems. Most of the existing algorithms do not provide an accurate determination of the normal vector field for shapes that present edges. Here, we propose a new and simple computational method in order to obtain accurate results on all types of shapes, whatever their local convexity degree. The presented method is based on the gradient vector field analysis of the object distance map. This vector field is adaptively filtered around each surface voxel using angle and symmetry criteria so that as many relevant contributions as possible are accounted for. This optimizes the smoothing of digitization effects while preserving relevant details of the processed numerical object. Thanks to the precise normal field obtained, a projection method can be proposed to immediately derive the surface area from a raw discrete object. An empirical justification of the validity of such an algorithm in the continuous limit is also provided. Some results on simulated data and snow images from X-ray tomography are presented, compared to the Marching Cubes and Convex Hull results, and discussed.	algorithm;binary data;computation;convex hull;diagnostic radiologic examination;distance transform;estimated;gradient;marching cubes;normal (geometry);numerical analysis;numerical method;physical object;smoothing (statistical technique);thermodynamics;tomography;voxel	Frédéric Flin;Jean-Bruno Brzoska;David Coeurjolly;Romeu André Pieritz;Bernard Lesaffre;Cécile Coleou;Pascal Lamboley;Olivier Teytaud;Gérard L Vignoles;Jean-François Delesse	2005	IEEE Transactions on Image Processing	10.1109/TIP.2005.846021	adaptive filter;discrete geometry;computer vision;image processing;computer science;mathematics;geometry;tomography	Visualization	53.96845587890742	-73.31690038396934	1254
3b6bf72c0b78a47de76b4618b94cc867cfa76e96	repeated movie viewings produce similar local activity patterns but different network configurations		People seek novelty in everyday life, but they also enjoy viewing the same movies or reading the same novels a second time. What changes and what stays the same when re-experiencing a narrative? In examining this question with functional neuroimaging, we found that brain activity reorganizes in a hybrid, scale-dependent manner when individuals processed the same audiovisual narrative a second time. At the most local level, sensory systems (occipital and temporal cortices) maintained a similar temporal activation profile during the two viewings. Nonetheless, functional connectivity between these same lateral temporal regions and other brain regions was stronger during the second viewing. Furthermore, at the level of whole-brain connectivity, we found a significant rearrangement of network partition structure: lateral temporal and inferior frontal regions clustered together during the first viewing but merged within a fronto-parietal cluster in the second. Our findings show that repetition maintains local activity profiles. However, at the same time, it is associated with multiple network-level connectivity changes on larger scales, with these changes strongly involving regions considered core to language processing.	activity diagram;dna sequence rearrangement;electroencephalography;epilepsy, temporal lobe;functional neuroimaging;large;lateral computing;lateral thinking;merge;movies;network partition;resting state fmri;frontal lobe	Michael Andric;Susan Goldin-Meadow;Steven L. Small;Uri Hasson	2016	NeuroImage	10.1016/j.neuroimage.2016.07.061	psychology;artificial intelligence;communication;social psychology	HCI	19.00830253956247	-76.05922820457822	1259
4df455b0e64aaf1431ec476359b8ebea497b07a3	genome trees from conservation profiles	evolution molecular;evolutionary history;animals;phylogeny;conserved sequence;proteins;genome;humans;data handling;genome sequence	"""The concept of the genome tree depends on the potential evolutionary significance in the clustering of species according to similarities in the gene content of their genomes. In this respect, genome trees have often been identified with species trees. With the rapid expansion of genome sequence data it becomes of increasing importance to develop accurate methods for grasping global trends for the phylogenetic signals that mutually link the various genomes. We therefore derive here the methodological concept of genome trees based on protein conservation profiles in multiple species. The basic idea in this derivation is that the multi-component """"presence-absence"""" protein conservation profiles permit tracking of common evolutionary histories of genes across multiple genomes. We show that a significant reduction in informational redundancy is achieved by considering only the subset of distinct conservation profiles. Beyond these basic ideas, we point out various pitfalls and limitations associated with the data handling, paving the way for further improvements. As an illustration for the methods, we analyze a genome tree based on the above principles, along with a series of other trees derived from the same data and based on pair-wise comparisons (ancestral duplication-conservation and shared orthologs). In all trees we observe a sharp discrimination between the three primary domains of life: Bacteria, Archaea, and Eukarya. The new genome tree, based on conservation profiles, displays a significant correspondence with classically recognized taxonomical groupings, along with a series of departures from such conventional clusterings."""	cluster analysis;eukaryota;gene duplication abnormality;genome;handling (psychology);homology (biology);numerous;phylogenetics;subgroup;taxonomy (general);trees (plant);triple modular redundancy;informational;statistical cluster	Fredj Tekaia;Edouard Yeramian	2005	PLoS Computational Biology	10.1371/journal.pcbi.0010075	biology;whole genome sequencing;bioinformatics;group method of data handling;conserved sequence;genetics;phylogenetics;genome	Comp.	2.896198713349664	-59.30111409520807	1260
4c642f082bb54e1b74ce2ace5e9afba207b0e2ef	compact neural networks based on the multiscale entanglement renormalization ansatz		This paper demonstrates a method for tensorizing neural networks based upon an efficient way of approximating scale invariant quantum states, the Multi-scale Entanglement Renormalization Ansatz (MERA). We employ MERA as a replacement for the fully connected layers in a convolutional neural network and test this implementation on the CIFAR-10 and CIFAR-100 datasets. The proposed method outperforms factorization using tensor trains, providing greater compression for the same level of accuracy and greater accuracy for the same level of compression. We demonstrate MERA layers with 14000 times fewer parameters and a reduction in accuracy of less than 1% compared to the equivalent fully connected layers, scaling like O(N).	artificial neural network;convolutional neural network;image scaling;quantum entanglement;quantum state	Andrew Hallam;Edward Grant;Vid Stojevic;Simone Severini;Andrew G. Green	2018			tensor;computer science;mathematical optimization;renormalization;ansatz;quantum entanglement;artificial neural network;scale invariance;factorization;quantum state	NLP	23.599886673719023	-51.79012046330197	1263
eac0da58d9cc8ecbf8ca36a5e878e7c2e8912fce	novel distribution model of transformed coefficients in video coding using quad-tree structured block partitioning		Today’s video coding standard such as high efficiency video coding uses a full quad-tree structured block partitioning, so the underlying statistics of transformed coefficients becomes more complicated to estimate than the previous standards due to the coding structure. However, a statistical distribution of transformed residue is important for a design of a smart encoder. Thus, in this paper, we present a theoretic analysis of a distribution of transformed coefficients produced from an encoder using different transform sizes, and derive a probability density function (pdf) for the estimation. The proposed density model provides a more accurate distribution model than the conventional pdfs. Parameters are theoretically estimated, and rate-distortion model is established from the proposed pdf. We also apply the proposed method to a rate control problem to show the efficiency of the proposed density model. Our experimental results show that the proposed method is better capable of modeling the mixed sources of multiple-type transform coefficients occurred from the quad-tree coding structure of transform and provides an accurate estimate in rate control.		Je-Won Kang	2017	Multidim. Syst. Sign. Process.	10.1007/s11045-016-0438-8	simulation;theoretical computer science;mathematics;statistics;sum of absolute transformed differences	OS	44.535562574408836	-15.58623061595505	1270
b187ac562445af422d32e8e395c99ddfbf1fd103	stability of homogeneous nonlinear systems with sampled-data inputs	sampled-data systems;nonlinear systems;homogeneity;lyapunov stability;lyapunov functions	The main goal of this article is to use properties of homogeneous systems for addressing the problem of stability for a class of nonlinear systems with sampled-data inputs. This nonlinear strategy leads to several kinds of stability, i.e. local asymptotic stability, global asymptotic stability or global asymptotic set stability, depending on the sign of the degree of homogeneity. The results are illustrated with the case of the double integrator. © 2017 Elsevier Ltd. All rights reserved.	control theory;emulator;feedback;nonlinear system	Emmanuel Bernuau;Emmanuel Moulay;Patrick Coirault	2017	Automatica	10.1016/j.automatica.2017.07.048	mathematical optimization;control theory;lyapunov stability;circle criterion;homogeneity (statistics);nonlinear system;mathematics;double integrator;homogeneous;lyapunov function;exponential stability	Embedded	70.46847067451071	0.9009385009811631	1271
699cdfafdaa7a08a9a91d072f9385308c757f053	analyzing manufacturing process knowledge flows with kofi.	knowledge flow	A valve assembly embodied in cartridge form for ease of manufacture, repair, and replacement and for retrofitting existing valve housings. The cartridge operates with shear on/off operation and includes a valve element having elastomeric properties and a configuration arranged to develop positive shut-off and seal action to prevent leakage in both the flow circuit and along an operating stem.		Oscar M. Rodríguez-Elias;Alberto L. Morán;Jaqueline I. Lavandera;Aurora Vizcaíno	2008			computer science;knowledge management;process management	Robotics	82.02369460893503	-18.45318250760934	1272
0ff2d75b34c5cf9a956251490ad395313ef743dd	synthesizing goal-directed actions from a library of example movements	humanoid robot;spline;motion control;splines mathematics humanoid robots learning artificial intelligence motion control regression analysis robot dynamics;learning framework;training;humanoid robot goal directed actions example movement learning framework fifth order splines cartesian space robot movement robot motion locally weighted regression robot dynamics;example movement;data mining;splines mathematics;trajectory;humanoid robots;cartesian space;robots;robot movement;libraries humanoid robots physics computing hidden markov models orbital robotics robotics and automation cybernetics laboratories computer science data engineering;robot motion;regression analysis;goal directed actions;learning artificial intelligence;robot dynamics;locally weighted regression;fifth order splines;timing	We present a new learning framework for synthesizing goal-directed actions from example movements. The approach is based on the memorization of training data and locally weighted regression to compute suitable movements for a large range of situations. The proposed method avoids making specific assumptions about an adequate representation of the task. Instead, we use a general representation based on fifth order splines. The data used for learning comes either from the observation of events in the Cartesian space or from the actual movement execution on the robot. Thus it informs us about the appropriate motion in the example situations. We show that by applying locally weighted regression to such data, we can generate actions having proper dynamics to solve the given task. To test the validity of the approach, we present simulation results under various conditions as well as experiments on a real robot.	action potential;experiment;institute for operations research and the management sciences;interpolation;non-regression testing;programming by demonstration;robot;simulation;spline (mathematics)	Ales Ude;Marcia Riley;Bojan Nemec;Andrej Kos;Tamim Asfour;Gordon Cheng	2007	2007 7th IEEE-RAS International Conference on Humanoid Robots	10.1109/ICHR.2007.4813857	computer vision;simulation;computer science;humanoid robot;artificial intelligence	Robotics	62.24632755321724	-24.904081036606932	1273
4221683cfd0efb673e5bfdbbe2090afceeb9ba98	heart attack risk detection using bagging classifier	cdss heart attack detection bagging classification;heart;bagging;medical computing data analysis decision support systems learning artificial intelligence;bagging cardiac arrest reactive power heart classification algorithms decision support systems;cardiac arrest;heart attack risk detection cdss clinical decision support system dataset official permissions ensemble machine learning classification algorithm cardiovascular diseases bagging classifier;decision support systems;classification algorithms;reactive power	Cardiovascular diseases in the world are the most common cause of death. Our study aims to predict the rate of heart attack risk for individuals using the Bagging Method, an ensemble Machine Learning classification algorithm. For this reason, a questionnaire has been prepared to collect the relevant data. After obtaining the official permissions, the questionnaires are applied to the patients who have had a heart attack. By this way a predefined dataset is created to be used in the classification algorithms. In the applications, heart attack risk can be detected for an individual by using powerful ensemble classifiers. Additionally, in cross validation process the proposed model shows a high-performance in regression. Therefore, this suggested Clinical Decision Support System (CDSS) enables to take some precautions before a heart attack.	algorithm;clinical decision support system;cross-validation (statistics);ensemble kalman filter;machine learning	Faruk Bulut	2016	2016 24th Signal Processing and Communication Application Conference (SIU)	10.1109/SIU.2016.7496164	computer science;machine learning;pattern recognition;data mining	ML	6.339001110250315	-77.32588707093824	1281
6a31487b6927fab6b3fe93b8449f2186589c91f2	bayesian robot system identification with input and output noise	bayes estimation;entrada salida;high dimensional regression;rigid body;high dimensionality;system with n degrees of freedom;approximation algorithm;variational bayesian;robotics;multidimensional analysis;parameter identification;multi degree of freedom;input output;identificacion sistema;estimacion bayes;mpi fur intelligente systeme;analyse n dimensionnelle;rigid body dynamics;machine learning;system identification;automatic detection;factor analysis;negative feedback;systeme n degres liberte;solid dynamic;algoritmo aproximacion;model based control;estimacion parametro;analisis n dimensional;robotica;dynamique solide;robotique;parameter estimation;estimation parametre;reseau neuronal;variational bayesian methods;input noise;algorithme approximation;robot dynamics;sistema n grados libertad;dinamico solido;abt schaal;red neuronal;identification systeme;piecewise linear system;estimation bayes;neural network;entree sortie	For complex robots such as humanoids, model-based control is highly beneficial for accurate tracking while keeping negative feedback gains low for compliance. However, in such multi degree-of-freedom lightweight systems, conventional identification of rigid body dynamics models using CAD data and actuator models is inaccurate due to unknown nonlinear robot dynamic effects. An alternative method is data-driven parameter estimation, but significant noise in measured and inferred variables affects it adversely. Moreover, standard estimation procedures may give physically inconsistent results due to unmodeled nonlinearities or insufficiently rich data. This paper addresses these problems, proposing a Bayesian system identification technique for linear or piecewise linear systems. Inspired by Factor Analysis regression, we develop a computationally efficient variational Bayesian regression algorithm that is robust to ill-conditioned data, automatically detects relevant features, and identifies input and output noise. We evaluate our approach on rigid body parameter estimation for various robotic systems, achieving an error of up to three times lower than other state-of-the-art machine learning methods.	actuator device component;addresses (publication format);algorithm;algorithmic efficiency;calculus of variations;computer-aided design;condition number;estimation theory;factor analysis;inference;input/output;linear system;machine learning;muscle rigidity;negative feedback;nonlinear system;numerous;piecewise linear continuation;population parameter;robot (device);system identification	Jo-Anne Ting;Aaron D'Souza;Stefan Schaal	2011	Neural networks : the official journal of the International Neural Network Society	10.1016/j.neunet.2010.08.011	multidimensional analysis;input/output;rigid body;simulation;rigid body dynamics;system identification;computer science;artificial intelligence;machine learning;control theory;mathematics;robotics;estimation theory;factor analysis;negative feedback;approximation algorithm;artificial neural network	Robotics	59.70314619651647	-16.212478157034447	1282
46ea018c7a2a863b96be5554dcac025b8a52d81e	fuzzy k-minpen clustering and k-nearest-minpen classification procedures incorporating generic distance-based penalty minimizers		Abstract. We discuss a generalization of the fuzzy (weighted) k-means clustering procedure and point out its relationships with data aggregation in spaces equipped with arbitrary dissimilarity measures. In the proposed setting, a data set partitioning is performed based on the notion of points’ proximity to generic distance-based penalty minimizers. Moreover, a new data classification algorithm, resembling the k-nearest neighbors scheme but less computationally and memory demanding, is introduced. Rich examples in complex data domains indicate the usability of the methods and aggregation theory in general.	cluster analysis;data aggregation;k-means clustering;k-nearest neighbors algorithm;machine learning;usability	Anna Cena;Marek Gagolewski	2016		10.1007/978-3-319-40581-0_36	mathematical optimization;combinatorics;discrete mathematics;machine learning;mathematics	ML	3.655408096721443	-39.27130694098267	1283
28983c9d0499ddad891af220e983a203d399bf4b	automatic detection of distorted plethysmogram pulses in neonates and paediatric patients using an adaptive-network-based fuzzy inference system	neonates;receiver operator characteristic;artefact detection;adaptive network based fuzzy inference system;confidence interval;health care provider;heart rate;paediatrics;automatic detection;feature extraction;fuzzy inference system;roc curve;pulse oximetry;plethysmogram;oxygen saturation false alarms;oxygen saturation;fuzzy system	Despite the fact that pulse oximetry has become an essential technology in respiratory monitoring of neonates and paediatric patients, it is still fraught with artefacts causing false alarms resulting from patient or probe movement. As the shape of the plethysmogram has always been considered as a useful visual indicator for determining the reliability of SaO(2) numerical readings, automation of this observation might benefit health care providers at the bedside. We observed that the systolic upstroke time (t(1)), the diastolic time (t(2)) and heart rate (HR) extracted from the plethysmogram pulse constitute features, which can be used for detecting normal and distorted plethysmogram pulses. We developed a technique for classifying plethysmogram pulses into two categories: valid and artefact via implementations of fuzzy inference systems (FIS), which were tuned using an adaptive-network-based fuzzy inference system (ANFIS) and receiver operating characteristics (ROC) curves analysis. Features extracted from a total of 22,497 pulse waveforms obtained from 13 patients were used to systematically optimise the FIS. A further 2843 waveforms obtained from another eight patients were used for testing the system, and visually classified into 1635 (58%) valid and 1208 (42%) distorted segments. For the optimum system, the area under the ROC curve was 0.92. The system was able to classify 1418 (87%) valid segments and 897 (74%) distorted segments correctly. The calculations of the system's performance showed 87% sensitivity, 81% accuracy and 74% specificity. In comparison with the 95% confidence interval (CI) thresholding method, the fuzzy system showed higher specificity (P=0.008,P<0.01), and no significant difference was found between the two methods in terms of sensitivity (P=0.720,P>0.05) and accuracy (P=0.053,P>0.05). We therefore conclude that the algorithm used in this system has some potential in detecting valid and distorted plethysmogram pulse. However, further evaluation is needed using larger patient groups.	adaptive neuro fuzzy inference system;categories;classification;confidence intervals;diastole;extraction;fuzzy control system;fuzzy logic;infant, newborn;inference engine;interval arithmetic;large;morphologic artifacts;numerical analysis;oximetry, pulse;oxygen saturation measurement;patients;plethysmography;receiver operating characteristic;receiver operator characteristics;sensitivity and specificity;sensor;serial ata;thresholding (image processing);algorithm	Suliman Yousef Belal;Azzam Fouad George Taktak;Andrew John Nevill;Stephen Andrew Spencer;David Roden;Sharon Bevan	2002	Artificial intelligence in medicine	10.1016/S0933-3657(01)00099-9	computer science;machine learning;data mining;receiver operating characteristic;fuzzy control system;statistics	AI	16.25994598424939	-87.81062960937764	1288
74810e67ec319eb207bf42ce0d90024f157f8045	camerra: an analysis tool for the computation of conformational dynamics by evaluating residue-residue associations		A computational method which extracts the dominant motions from an ensemble of biomolecular conformations via a correlation analysis of residue-residue contacts is presented. The algorithm first renders the structural information into contact matrices, then constructs the collective modes based on the correlated dynamics of a selected set of dynamic contacts. Associated programs can bridge the results for further visualization using graphics software. The aim of this method is to provide an analysis of conformations of biopolymers from the contact viewpoint. It may assist a systematical uncovering of conformational switching mechanisms existing in proteins and biopolymer systems in general by statistical analysis of simulation snapshots. In contrast to conventional correlation analyses of Cartesian coordinates (such as distance covariance analysis and Cartesian principal component analysis), this program also provides an alternative way to locate essential collective motions in general. Herein, we detail the algorithm in a stepwise manner and comment on the importance of the method as applied to decoding allosteric mechanisms. © 2018 Wiley Periodicals, Inc.		Quentin R. Johnson;Richard J. Lindsay;Tongye Shen	2018	Journal of computational chemistry	10.1002/jcc.25192	chemistry;mathematical optimization;covariance matrix;principal component analysis;computation;residue (complex analysis);biopolymer	Comp.	13.573679075589283	-56.55692572499905	1289
a429cb2d14a981a48f58b9ace2cd83c3e5599b8f	multiple classifier combination for hyperspectral remote sensing image classification	mahalanobis distance;classifier combination;decision tree;classifier ensemble;generic algorithm;bagging;classifier system;image classification;evidence theory;multiple classifiers;boosting;combining classifier;multiple classifier combination;hyperspectral remote sensing;classification accuracy;multiple classifier system;experience base	Multiple classifier combination is used to hyperspectral remote sensing image classification in this paper, and some classifier combination algorithms are experimented. Based on a brief introduction to multiple classifier system and general algorithms, a modified evidence combination algorithm is adopted to handle evidence with high inconsistency, and a hierarchical multiple classifier system is designed to integrate the advantages of multiple classifiers. Using the OMIS hyperspectral remote sensing image as the study data, training samples manipulation approaching including boosting and bagging, together with parallel and hierarchical combination schemes are experimented. Mahalanobis distance classifier, MLPNN, RBFNN, SVM and J4.8 decision tree are selected as member classifiers. In the multiple classifier combination scheme based on training samples, both boosting and bagging can enhance the classification accuracy of any individual classifier, and boosting performs a bit better than bagging when the same classifier is used. In classification ensemble using multiple classifier combination approaches, both parallel combination using modified evidence theory and hierarchical classifier system can obtain higher accuracy than any individual member classifiers. So it can be concluded that multiple classifier combination is suitable for hyperspectral remote sensing image classification.		Peijun Du;Wei Zhang;Hao Sun	2009		10.1007/978-3-642-02326-2_6	margin classifier;quadratic classifier;machine learning;linear classifier;pattern recognition;data mining	HCI	13.33601180652801	-43.543450294762216	1292
775ead36904641137d086c918280877d92091732	nonnegative matrix factorization based decomposition for time series modelling		We propose a novel method of time series decomposition based on the non-negative factorization of the Hankel matrix of time series and apply this method for time series modelling and prediction. An interim (surrogate) model of time series is built from the components of the time series using random cointegration, while the best cointegration is selected using a nature-inspired optimization method (Artificial Bee Colony). For modelling of cointegrated time series we use the ARX (AutoRegressive with eXogenous inputs) model. The results of modelling using the historical data (daily highest price) of Su0026P 500 stocks from 2009 are presented and compared against stand-alone ARX models. The results are evaluated using a variety of metrics (RMSE, MAE, MAPE, Pearson correlation, Nash-Suttcliffe efficiency coefficient, etc.) as well as illustrated graphically using Taylor and Target diagrams. The results show a 51–98% improvement of prediction accuracy (depending upon accuracy metric used). The proposed time series modelling method can be used for variety applications (time series denoising, prediction, etc.).	non-negative matrix factorization;time series	Tatjana Sidekerskiene;Marcin Wozniak;Robertas Damaševičius	2017		10.1007/978-3-319-59105-6_52	artificial intelligence;cointegration;statistics;metzler matrix;computer science;hankel matrix;pattern recognition;decomposition of time series;autoregressive model;euler's factorization method;matrix decomposition;non-negative matrix factorization	AI	8.13788891978409	-19.814781874839603	1297
7cd742c83301390c789096d62d27a53a848e627c	image denoising based on morf and minimization total variation	watermarking;color image watermarking;human vision;image coding;color masking;watermarking degradation distortion measurement psnr humans quantization discrete cosine transforms colored noise image color analysis brightness;image colour analysis;peak signal to noise ratio;hvs based imperceptibility measurement;quality measures;watermarking image coding image colour analysis;just noticeable difference;just noticeable color differences hvs based imperceptibility measurement color image watermarking color masking;color image;just noticeable color differences	Imperceptibility of embedded watermark reflects the fidelity of watermarked image. Based on the color masking and Watson's JND (just noticeable differences) model, a calculation method of JNCD (just noticeable color differences) is proposed. Then the masking weight for each element of color image is calculated using the JNCD and a new quality measure of watermarked color image is proposed, i.e. CPSNR (color image peak signal to noise ratio). The quality of watermarked color image calculated by CPSNR is closer to the sensitivity of human vision than PSNR.	noise reduction	Chengwu Lu	2007		10.1109/SNPD.2007.299	just-noticeable difference;color histogram;computer vision;color normalization;color depth;color image;peak signal-to-noise ratio;digital watermarking;computer science;multimedia;color balance;histogram equalization;computer graphics (images)	EDA	60.475785090029	-63.43835743492932	1302
5285dd1bfcd134f0681aa27286fcc3dfdfe41aad	on the analysis of the quantum-inspired evolutionary algorithm with a single individual	global solution;evolutionary computation;probability;simulated annealing;theoretical analysis;quantum inspired evolutionary algorithm;algorithm design and analysis evolutionary computation quantum computing testing space exploration principal component analysis genetic algorithms simulated annealing guidelines research and development;search problems;probabilistic mechanism quantum inspired evolutionary algorithm theoretical analysis segment process onemax problem global search scheme local search scheme;quantum computing;search problems evolutionary computation probability quantum computing;local search	This paper discusses the reason why QEA works and verifies how QEA works. The theoretical analysis of the simplified model of the segment process of QEA shows that QEA with a single individual for OneMax problem guarantees the global solution in terms of expected running number of generations. The analysis for exploration shows clearly that QEA starts with a global search scheme and changes automatically into a local search scheme as generation advances because of its inherent probabilistic mechanism, which leads to a good balance between exploration and exploitation. For comparison purpose, simulated annealing is considered with three test functions. The results support the conclusions derived from the theoretical analysis of QEA with a single individual.	binary number;computer cooling;distribution (mathematics);evolutionary algorithm;fitness function;hill climbing;local search (optimization);mathematical optimization;maxima and minima;quantum;random number generation;randomized algorithm;scheduling (computing);simulated annealing;string (computer science);verification and validation	Kuk-Hyun Han;Jong-Hwan Kim	2006	2006 IEEE International Conference on Evolutionary Computation	10.1109/CEC.2006.1688636	mathematical optimization;simulated annealing;computer science;local search;theoretical computer science;machine learning;probability;mathematics;quantum computer;evolutionary computation	Robotics	27.69719075492323	-6.8295526142229095	1306
4832d7cd181681515aaa88a2258ddc69d8891e60	is it possible to find a pattern of k for the nearest neighbor?	nearest neighbor		k-nearest neighbors algorithm	D. S. Rodríquez;Francisco J. Ferrer-Troyano;José Cristóbal Riquelme Santos;Jesús S. Aguilar-Ruiz	2002				Vision	9.111620618465977	-36.74335362542195	1309
89dfe2e626bf98fafd97a90ab1177847578fff97	the impact of motion dimensionality and bit cardinality on the design of 3d gesture recognizers	sampling rate;gesture toolkit;euclidean distance;gesture dimensionality;bit cardinality;angular cosine distance;3d gestures;hausdorff;classifiers;dynamic time warping;gesture recognition;bit depth	The interactive demands of the upcoming ubiquitous computing era have set off researchers and practitioners toward prototyping new gesture-sensing devices and gadgets. At the same time, the practical needs of developing for such miniaturized prototypes with sometimes very low processing power and memory resources make practitioners in high demand of fast gesture recognizers employing little memory. However, the available work on motion gesture classifiers has mainly focused on delivering high recognition performance with less discussion on execution speed or required memory. This work investigates the performance of today’s commonly used 3D motion gesture recognizers under the effect of different gesture dimensionality and bit cardinality representations. Specifically, we show that few sampling points and low bit depths are sufficient for most motion gesture metrics to attain their peak recognition performance in the context of the popular Nearest-Neighbor classification approach. As a practical consequence, 16x faster recognizers working with 32x less memory while delivering the same high levels of recognition performance are being reported. We present recognition results for a large gesture corpus consisting in nearly 20,000 gesture samples. In addition, a toolkit is provided to assist practitioners in optimizing their gesture recognizers in order to increase classification speed and reduce memory consumption for their designs. At a deeper level, our findings suggest that the precision of the human motor control system articulating 3D gestures is needlessly surpassed by the precision of today’s motion sensing technology that unfortunately bares a direct connection with the sensors’ cost. We hope this work will encourage practitioners to consider improving the performance of their prototypes by careful analysis of motion gesture representation rather than by throwing more processing power and more memory into the design. & 2012 Elsevier Ltd. All rights reserved.	control system;finite-state machine;gesture recognition;sampling (signal processing);ubiquitous computing	Radu-Daniel Vatavu	2013	Int. J. Hum.-Comput. Stud.	10.1016/j.ijhcs.2012.11.005	computer vision;speech recognition;color depth;computer science;artificial intelligence;operating system;dynamic time warping;euclidean distance;gesture recognition;hausdorff space;sampling	HCI	0.7605624200646208	-82.64503847613007	1310
1b51ac37da95e147d593999ffbde236aec595bff	modeling of resilience properties in oscillatory biological systems using parametric time petri nets, supplementary information		Automated verification of living organism models allows us to gain previously unknown knowledge about underlying biological processes. In this paper, we show the benefits to use parametric time Petri nets in order to analyze precisely the dynamic behavior of biological oscillatory systems. In particular, we focus on the resilience properties of such systems. This notion is crucial to understand the behavior of biological systems (e.g. the mammalian circadian rhythm) that are reactive and adaptive enough to endorse major changes in their environment (e.g. jet-lags, day-night alternating work-time). We formalize these properties through parametric TCTL and demonstrate how changes of the environmental conditions can be tackled to guarantee the resilience of living organisms. In particular, we are able to discuss the influence of various perturbations, e.g. artificial jet-lag or components knock-out, with regard to quantitative delays. This analysis is crucial when it comes to model elicitation for dynamic biological systems. We demonstrate the applicability of this technique using a simplified model of circadian clock.	biological system;petri net	Alexander Andreychenko;Morgan Magnin;Katsumi Inoue	2015		10.1007/978-3-319-23401-4_20	simulation	SE	7.934234448625298	-66.58651717293903	1311
44a8cf16a0977f260238443df87e24075b0729a9	a directionally selective small target motion detecting visual neural network in cluttered backgrounds		Discriminating targets moving against a cluttered background is a huge challenge, let alone detecting a target as small as one or a few pixels and tracking it in flight. In the insect's visual system, a class of specific neurons, called small target motion detectors (STMDs), have been identified as showing exquisite selectivity for small target motion. Some of the STMDs have also demonstrated direction selectivity which means these STMDs respond strongly only to their preferred motion direction. Direction selectivity is an important property of these STMD neurons which could contribute to tracking small targets such as mates in flight. However, little has been done on systematically modeling these directionally selective STMD neurons. In this paper, we propose a directionally selective STMD-based neural network for small target detection in a cluttered background. In the proposed neural network, a new correlation mechanism is introduced for direction selectivity via correlating signals relayed from two pixels. Then, a lateral inhibition mechanism is implemented on the spatial field for size selectivity of the STMD neurons. Finally, a population vector algorithm is used to encode motion direction of small targets. Extensive experiments showed that the proposed neural network not only is in accord with current biological findings, i.e., showing directional preferences but also worked reliably in detecting the small targets against cluttered backgrounds.	algorithm;artificial neural network;biological neural networks;cns disorder;detectors;encode;experiment;hypothalamic area, lateral;lateral computing;lateral thinking;motion detector;neuron;pixel;population vector;selectivity (electronic);simulation	Hongxin Wang;Jigen Peng;Shigang Yue	2018	IEEE transactions on cybernetics	10.1109/TCYB.2018.2869384	artificial neural network;pixel;encode;machine learning;lateral inhibition;computer vision;artificial intelligence;mathematics;population;detector	Vision	20.513839025126234	-66.12713100217677	1313
7768dd34f0ccd8be45fc60a2e8c0bb25b352cafc	single- and multi-objective genetic programming: new runtime results for sorting	optimization algorithm design and analysis evolutionary computation sorting genetic programming runtime complexity theory;conference paper;computational complexity analysis multiobjective genetic programming single objective genetic programming bloat problem sorting problem;sorting computational complexity genetic algorithms	In genetic programming, the size of a solution is typically not specified in advance and solutions of larger size may have a larger benefit. The flexibility often comes at the cost of the so-called bloat problem: individuals grow without providing additional benefit to the quality of solutions, and the additional elements can block the optimisation process. Consequently, problems that are relatively easy to optimise can not be handled by variable-length evolutionary algorithms. In this article, we present several new bounds for different single- and multi-objective algorithms on the sorting problem, a problem that typically lacks independent and additive fitness structures.	computational complexity theory;evolutionary algorithm;genetic programming;mathematical optimization;sorting;utility functions on indivisible goods	Markus Wagner;Frank Neumann	2014	2014 IEEE Congress on Evolutionary Computation (CEC)	10.1109/CEC.2014.6900310	evolutionary programming;genetic programming;computer science;theoretical computer science;machine learning;genetic representation;algorithm	Theory	24.817429650017058	-1.7024307412977187	1317
9031ef8e7691a4d9c6355f017dc1135fa4feac79	amid: accurate magnetic indoor localization using deep learning	deep learning;magnetic landmark;recurrence plot	Geomagnetic-based indoor positioning has drawn a great attention from academia and industry due to its advantage of being operable without infrastructure support and its reliable signal characteristics. However, it must overcome the problems of ambiguity that originate with the nature of geomagnetic data. Most studies manage this problem by incorporating particle filters along with inertial sensors. However, they cannot yield reliable positioning results because the inertial sensors in smartphones cannot precisely predict the movement of users. There have been attempts to recognize the magnetic sequence pattern, but these attempts are proven only in a one-dimensional space, because magnetic intensity fluctuates severely with even a slight change of locations. This paper proposes accurate magnetic indoor localization using deep learning (AMID), an indoor positioning system that recognizes magnetic sequence patterns using a deep neural network. Features are extracted from magnetic sequences, and then the deep neural network is used for classifying the sequences by patterns that are generated by nearby magnetic landmarks. Locations are estimated by detecting the landmarks. AMID manifested the proposed features and deep learning as an outstanding classifier, revealing the potential of accurate magnetic positioning with smartphone sensors alone. The landmark detection accuracy was over 80% in a two-dimensional environment.	academia (organization);artificial neural network;biological neural networks;classification;conflict (psychology);deep learning;eighty;estimated;experiment;extraction;fingerprint;indoor positioning system;magnetic positioning;operability;particle filter;sensor web;smartphone;sensor (device)	Namkyoung Lee;Sumin Ahn;Dongsoo Han	2018		10.3390/s18051598	computer vision;artificial neural network;engineering;electronic engineering;inertial measurement unit;deep learning;landmark;indoor positioning system;particle filter;earth's magnetic field;classifier (linguistics);artificial intelligence	HCI	6.2322560456760865	-83.86424329591784	1321
6d3c1bd2b68fd8eec979a0e0b5fa622eb05b2457	on infinite horizon switched lqr problems with state and control constraints	suboptimality;infinite horizon constrained optimal control;discrete time switched and hybrid systems;lqr	This paper studies the Discrete-Time Switched LQR problem over an infinite time horizon, subject to polyhedral constraints on state and control inputs. Specifically, we aim to find an infinite-horizon hybridcontrol sequence, i.e., a sequence of continuous and discrete (switching) control inputs, that minimizes an infinite-horizon quadratic cost function, subject to polyhedral constraints on state and (continuous) control input. The overall constrained, infinite-horizon problem is split into two subproblems: (i) an unconstrained, infinite-horizon problem and (ii) a constrained, finite-horizon one. We derive a stationary suboptimal policy for problem (i) with analytical bounds on its optimality, and develop a novel formulation of problem (ii) as a Mixed-Integer Quadratic Program. By introducing the concept of a safe set, the solutions of the two subproblems are combined to achieve the overall control objective. Through the connection between (i) and (ii) it is shown that, by proper choice of the design parameters, the error of the overall suboptimal solution can be made arbitrarily small. The approach is tested on a numerical example. © 2012 Elsevier B.V. All rights reserved.	computation;initial condition;linear system;loss function;numerical analysis;polyhedron;quadratic programming;stationary process	Maximilian Balandat;Wei Zhang;Alessandro Abate	2012	Systems & Control Letters	10.1016/j.sysconle.2012.01.011	control engineering;mathematical optimization;linear-quadratic regulator;control theory;mathematics	AI	62.53932963158499	0.38136972037527916	1323
27d0bc3cae26442082cc8862eface8b91ea7b597	helping ai to play hearthstone using neural networks		This paper presents a winning solution to the AAIA'17 Data Mining Challenge. The challenge focused on creating an efficient prediction model for digital card game Hearthstone. Our final solution is an ensemble of various neural network models, including convolutional neural networks.	artificial neural network;convolutional neural network;data mining	Lukasz Grad	2017	2017 Federated Conference on Computer Science and Information Systems (FedCSIS)	10.15439/2017F561	artificial neural network;artificial intelligence;data mining;computer science;machine learning;convolutional neural network;feature extraction	AI	10.554952192495637	-26.92174154312616	1327
5c5b84a0df998ff0eabe552ce36bac6690a22349	compatibility of interval fuzzy preference relations with the cowa operator and its application to group decision making	cowa operator;compatibility;group decision making;interval fuzzy preference relation	We develop a new compatibility for the interval fuzzy preference relations based on the continuous ordered weighted averaging (COWA) operator and use it to determine the weights of experts in group decision making (GDM). We define some concepts of the compatibility degree and the compatibility index for the two interval fuzzy preference relations based on the COWA operator. We study some desirable properties of the compatibility index and investigate the relationship between the each expert’s interval fuzzy preference relation and the synthetic interval fuzzy preference relation. The prominent characteristic of the compatibility index based on the COWA operator is that it can deal with the compatibility of all the arguments by using a controlled parameter considering the attitude of decision maker rather than the compatibility of the simply two points in intervals. To determine the experts’ weights in the GDM with the interval fuzzy preference relations, we propose an optimal model based on the criterion of minimizing the compatibility index. In the end, we give a numerical example to develop the new approach to GDM with interval fuzzy preference relations. Communicated by G. Acampora. L. Zhou · Y. He · H. Chen (B) School of Mathematical Sciences, Anhui University, Hefei 230601, Anhui, China e-mail: huayouc@126.com L. Zhou e-mail: shuiqiaozlg@126.com Y. He e-mail: 812256307@qq.com J. Liu School of Business, Anhui University, Hefei 230601, Anhui, China e-mail: liujinpei2009@gmail.com	email;entity–relationship model;fuzzy set;numerical analysis;preference learning;synthetic intelligence	Ligang Zhou;Yingdong He;Huayou Chen;Jinpei Liu	2014	Soft Comput.	10.1007/s00500-013-1201-9	mathematical optimization;discrete mathematics;group decision-making;artificial intelligence;mathematics;compatibility	AI	-2.7471597947116346	-20.58823182011686	1328
91baa2bd7eca12346909cc9821b45e581af234ef	data mining in hiv-aids surveillance system	surveillance data;surveillance system;data mining;hiv aids;reporting delay	The Human Immunodeficiency Virus (HIV) is an infectious agent that attacks the immune system cells. Without a strong immune system, the body becomes very susceptible to serious life threatening opportunistic diseases. In spite of the great progresses on medication and prevention over the last years, HIV infection continues to be a major global public health issue, having claimed more than 36 million lives over the last 35 years since the recognition of the disease. Monitoring, through registries, of HIV-AIDS cases is vital to assess general health care needs and to support long-term health-policy control planning. Surveillance systems are therefore established in almost all developed countries. Typically, this is a complex system depending on several stakeholders, such as health care providers, the general population and laboratories, which challenges an efficient and effective reporting of diagnosed cases. One issue that often arises is the administrative delay in reports of diagnosed cases. This paper aims to identify the main factors influencing reporting delays of HIV-AIDS cases within the portuguese surveillance system. The used methodologies included multilayer artificial neural networks (MLP), naive bayesian classifiers (NB), support vector machines (SVM) and the k-nearest neighbor algorithm (KNN). The highest classification accuracy, precision and recall were obtained for MLP and the results suggested homogeneous administrative and clinical practices within the reporting process. Guidelines for reductions of the delays should therefore be developed nationwise and transversally to all stakeholders.	acquired immunodeficiency syndrome;artificial neural network;bayesian network;complex system;data mining;hiv infections;health care;hematological disease;immune system;immunologic deficiency syndromes;k-nearest neighbors algorithm;laboratory;memory-level parallelism;naive bayes classifier;precision and recall;single linkage cluster analysis;support vector machine;infectious agent	Alexandra Oliveira;Brígida Mónica Faria;A. Rita Gaio;Luís Paulo Reis	2017	Journal of Medical Systems	10.1007/s10916-017-0697-4	simulation;medicine;pathology;computer science;artificial intelligence;data mining;computer security	ML	5.854196928126252	-77.16729025750995	1333
9f1fffaedf0f9c27bd2c736cdc4b27a7fad1dfd7	phase unwrapping with phase-singularity spreading	costs phase estimation synthetic aperture radar interferometry large scale systems electrons reflection electromagnetic interference electromagnetic scattering proposals;conventional network programming method phase unwrapping method phase singularity spreading fractional phase compensators digital elevation maps;digital elevation map;network programming;geophysical techniques;phase unwrapping	We propose a novel phase unwrapping method where we spread the singularity in phase map with fractional phase compensators. We find that the obtained digital elevation maps have higher quality than those obtained in conventional network programming method. In addition, the calculation cost is very small. We present the basic idea and the processing procedure.	computer network programming;in-phase and quadrature components;instantaneous phase;map;technological singularity	Akira Hirose;Ryo Yamaki	2006	2006 IEEE International Symposium on Geoscience and Remote Sensing	10.1109/IGARSS.2006.325	computer science;optics;computer network programming;remote sensing	Arch	83.10414420269254	-68.17899715380534	1340
6eb75f2ef055aa959117c488ec6349a2317ecef9	position extraction from a discrete sliding-mode observer for sensorless control of ipmsms	angle tracking observer;control systems;oscillations;permanent magnet synchronous machine;oscillators;speed feedback;sampling frequency;phase shift;variable structure systems;observers;sensorless control;angle tracking observer ato;interior permanent magnet synchronous machine ipmsm;ipmsm drive system discrete sliding mode observer ipmsm sensorless control discrete smo interior permanent magnet synchronous machine sensorless control back electromagnetic force high speed applications rotor position extraction methods speed feedback algorithm inverse tangent method angle tracking observer ato extended back emf matlab simulink;discrete smo;control system;feedback;sensorless machine control;permanent magnet machines;ipmsm sensorless control;rotors;interior permanent magnet synchronous machine sensorless control;position measurement;back electromagnetic force;parameter uncertainty;positional information;variable structure systems feedback inverse problems observers permanent magnet machines sensorless machine control synchronous motor drives;ipmsm drive system;matlab simulink;stators;speed feedback algorithm;rotor position extraction methods;ato;synchronous motor drives;rotors observers oscillators position measurement noise control systems stators;speed feedback angle tracking observer ato interior permanent magnet synchronous machine ipmsm position extraction sliding mode observer smo;high speed;high speed applications;position extraction;sliding mode observer smo;inverse tangent method;discrete sliding mode observer;extraction method;measurement noise;sliding mode observer;noise;inverse problems;extended back emf	Sliding-mode observers (SMOs) offer a promising solution for sensorless control of interior permanent magnet synchronous machines (IPMSMs) due to their excellent robustness to system structure and parameter uncertainty. However, in practical applications, it is challenging for an SMO to achieve a perfect estimation for the back electromagnetic force (EMF) using a finite or relatively lower sampling frequency, especially for high-speed applications. Phase shift, magnitude variation, and heavy noise in the estimated back EMF will cause unexpected errors in rotor position extraction. Thus, advanced rotor position extraction methods are needed to obtain position information from the estimated back EMF. This paper proposes a novel estimated speed feedback algorithm to work together with the conventional inverse tangent method and angle tracking observer (ATO) to extract the rotor position from the extended back EMF obtained from a discrete SMO. The extracted position has reduced oscillations compared to that obtained from traditional methods. The proposed position extraction methods are validated by simulations in MATLAB Simulink as well as experiments on a practical IPMSM drive system.	algorithm;computation;eclipse modeling framework;experiment;matlab;r.o.t.o.r.;sampling (signal processing);sequential minimal optimization;simulation;simulink;steady state	Yue Zhao;Wei Qiao;Long Wu	2012	2012 IEEE International Symposium on Industrial Electronics	10.1109/ISIE.2012.6237159	control engineering;electronic engineering;engineering;control theory	Robotics	71.88085448025753	-13.77794780570507	1341
60dd6ce19216e301b43b7795277df49654ab61e7	decoding visual object categories from temporal correlations of ecog signals	it cortex;decoding;temporal coding;object category;ecog	How visual object categories are represented in the brain is one of the key questions in neuroscience. Studies on low-level visual features have shown that relative timings or phases of neural activity between multiple brain locations encode information. However, whether such temporal patterns of neural activity are used in the representation of visual objects is unknown. Here, we examined whether and how visual object categories could be predicted (or decoded) from temporal patterns of electrocorticographic (ECoG) signals from the temporal cortex in five patients with epilepsy. We used temporal correlations between electrodes as input features, and compared the decoding performance with features defined by spectral power and phase from individual electrodes. While using power or phase alone, the decoding accuracy was significantly better than chance, correlations alone or those combined with power outperformed other features. Decoding performance with correlations was degraded by shuffling the order of trials of the same category in each electrode, indicating that the relative time series between electrodes in each trial is critical. Analysis using a sliding time window revealed that decoding performance with correlations began to rise earlier than that with power. This earlier increase in performance was replicated by a model using phase differences to encode categories. These results suggest that activity patterns arising from interactions between multiple neuronal units carry additional information on visual object categories.	categories;electrocorticography;encode (action);epilepsy;high- and low-level;interaction;neural oscillation;neuroscience discipline;patients;physical object;temporal lobe;time series;two-phase locking;visual objects;decoding;electrode	Kei Majima;Takeshi Matsuo;Keisuke Kawasaki;Kensuke Kawai;Nobuhito Saito;Isao Hasegawa;Yukiyasu Kamitani	2014	NeuroImage	10.1016/j.neuroimage.2013.12.020	computer science;artificial intelligence;machine learning;communication	HCI	17.4286305571229	-75.81373842550263	1343
335eb47a50194170579a5214029c0ca8caf3f721	association between cerebral glutamate and human behaviour: the sensation seeking personality trait	pedestrian safety;personality trait;poison control;injury prevention;hippocampus;human behaviour;magnetic resonance spectroscopy;safety literature;traffic safety;injury control;home safety;injury research;safety abstracts;human factors;anterior cingulate cortex;occupational safety;safety;brain imaging;safety research;personality traits;accident prevention;violence prevention;sensation seeking;glutamate;bicycle safety;poisoning prevention;falls;ergonomics;animal studies;suicide prevention	INTRODUCTION Brain imaging studies have linked the anterior cingulate cortex (ACC) to motivation, drive, and personality traits like novelty and sensation seeking. Animal studies have shown glutamatergic neurotransmission to be important in ACC function as well as motivated behaviour. However, the role of glutamate in related personality traits like sensation seeking has not been investigated in humans.   METHODS The associations between sensation seeking personality scores and absolute glutamate concentrations in the ACC and the hippocampal region measured by 3-Tesla proton magnetic resonance spectroscopy (1H-MRS) were investigated.   RESULTS ACC glutamate concentration was negatively correlated with the sensation seeking sum score and the experience seeking subscore. A weak negative correlation was also observed between the hippocampal glutamate and the sensation seeking sum score. The reexamination of the glutamate concentration after 4 weeks revealed a similar relationship with sensation seeking.   DISCUSSION Although preliminary, the results are in line with the key role of the ACC for motivation and executive control and with the impact of glutamate on novelty related behaviour observed in animal experiments. The role of the hippocampus for novelty processing is discussed. Glutamate measurement with 1H-MRS may facilitate the understanding of biological underpinnings of personality traits and psychiatric diseases associated with dysfunctions in motivation and drive.	antisocial personality disorder;cerebral cortex;childhood cerebral astrocytoma;cingulate cortex;experiment;glutamic acid;host-seeking behavior;magnetic resonance spectroscopy;medical imaging;mental association;mental disorders;minimal recursion semantics;negative feedback;protons;synaptic transmission;trait	Jürgen Gallinat;Dieter Kunz;Undine E. Lang;Peter Neu;Nuhad Kassim;T. Kienast;Frank Seifert;Florian Schubert;Malek Bajbouj	2007	NeuroImage	10.1016/j.neuroimage.2006.10.004	psychology;psychiatry;neuroscience;developmental psychology;suicide prevention;human factors and ergonomics;injury prevention;sensation seeking;glutamate receptor;hippocampus;big five personality traits;social psychology	NLP	18.18554085967251	-79.03192284036477	1346
462fcd6d8094dc915748d0ef624282f462efe7b6	a review of learning with deep generative models from perspective of graphical modeling		This document aims to provide a review on learning with deep generative models (DGMs), which is an highly-active area in machine learning and more generally, artificial intelligence. This review is not meant to be a tutorial, but when necessary, we provide self-contained derivations for completeness. This review has two features. First, though there are different perspectives to classify DGMs, we choose to organize this review from the perspective of graphical modeling, because the learning methods for directed DGMs and undirected DGMs are fundamentally different. Second, we differentiate model definitions from model learning algorithms, since different learning algorithms can be applied to solve the learning problem on the same model, and an algorithm can be applied to learn different models. We thus separate model definition and model learning, with more emphasis on reviewing, differentiating and connecting different learning algorithms. We also discuss promising future research directions.	algorithm;artificial intelligence;artificial neural network;graph (discrete mathematics);graphical model;graphical user interface;latent variable;loss function;machine learning;optimization problem;rewrite (programming);speedup;unicode;variational principle	Zhijian Ou	2018	CoRR		machine learning;generative grammar;artificial intelligence;mathematics	ML	20.613774741287568	-46.01559295941628	1349
3226e439b57549febc385903bf992e8bb484e3bb	automatic classification of diabetic macular edema using a modified completed local binary pattern (clbp)		Diabetic macular edema is the leading cause of visual loss for patients with diabetic retinopathy, a complication of diabetes. Early screening and treatment has been shown to prevent blindness in diabetic retinopathy and diabetic macular edema. The Early Treatment Diabetic Retinopathy Study (ETDRS) and the Diabetic Macular Edema Disease Severity Scale are the common screening standards based on the distance of exudates from the fovea. Instead of focusing on the macula region, this research adopts a global approach using texture classification to grade the fundus images into three stages: normal, moderate diabetic macular edema and severe diabetic macular edema. The proposed algorithm starts with a modified completed Local Binary Pattern (CLBP) to extract the image local gray level for all RGB channels. The obtained feature vector will then be fed into a multiclass Support Vector Machine (SVM) for classification. The 100 fundus images selected to be utilized for training and testing set were taken from MESSIDOR and these images were reviewed by an ophthalmologist for cross-validation. The algorithm using the CLBP demonstrates a sensitivity of 67% with a specificity of 30% while the proposed modified CLBP yields a higher sensitivity and specificity of 80% and 70% respectively.	algorithm;cross-validation (statistics);euclidean distance;feature vector;grayscale;multidimensional digital pre-distortion;sensitivity and specificity;support vector machine;test set;the 100	S. T. Lim;M. K. Ahmed;S. L. Lim	2017	2017 IEEE International Conference on Signal and Image Processing Applications (ICSIPA)	10.1109/ICSIPA.2017.8120570	computer science;artificial intelligence;macular edema;pattern recognition;diabetic retinopathy;local binary patterns;statistical classification;retinopathy;blindness;fundus (eye)	Vision	34.79214823869749	-75.24986213605798	1352
0fad6f3789fd1cf56603e00c793682dd9da2dfba	"""to which extend is the """"neural code"""" a metric ?"""	spiking network. neural code. gibbs distribution.;neural code;gibbs distribution	Here is proposed a review of the different choices to structure spike trains, using deterministic metrics. Temporal constraints observed in biological or computational spike trains are first taken into account The relation with existing neural codes (rate coding, rank coding, phase coding, ..) is then discussed. To which extend the “neural code” contained in spike trains is related to a metric appears to be a key point, a generalization of the Victor-Purpura metric family being proposed for temporal constrained causal spike trains.	action potential;causal filter;code;computation;neural coding	Bruno Cessac;Horacio Rostro-González;Juan Carlos Vasquez;Thierry Viéville	2008	CoRR			ML	22.044552118371804	-71.48480684096937	1354
2d5342c00e40a978ad4eb6ff2429edebe9ec2dd0	two steps multi-temporal non-local means for sar images	geophysical image processing;noise reduction synthetic aperture radar speckle noise image denoising noise measurement time series analysis;multitemporal image denoising multitemporal nonlocal means sar image multitemporal synthetic aperture radar image nlm method multitemporal data nlm operator maximum likelihood estimate binary weight iterative nl means spatial pixels image redundancy;maximum likelihood estimation;iterative methods;multi temporal sar images;radar imaging;non local means nlm;image denoising;multi temporal sar images image denoising non local means nlm;radar imaging geophysical image processing geophysical techniques image denoising iterative methods maximum likelihood estimation;geophysical techniques	This paper presents a denoising approach for multi-temporal Synthetic aperture radar (SAR) images based on Non-Local Means (NLM) method. To exploit redundancy existing in multi-temporal images, we develop a new strategy of NLM for multi-temporal data. Instead of directly overspreading the NLM operator from one image to temporal images, a two steps weighted average is proposed in this paper. The first step is a maximum likelihood estimate with binary weights on temporal pixels and the second step is iterative NL means on spatial pixels. Experiments in this paper illustrate that the proposed method can effectively exploit image redundancy and denoise multi-temporal images.	iterative method;netware loadable module;noise reduction;non-local means;pixel	Xin Su;Charles-Alban Deledalle;Florence Tupin;Hong Sun	2012	2012 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2012.6351106	computer vision;pattern recognition;iterative method;maximum likelihood;radar imaging;non-local means;remote sensing	Robotics	68.62959319435701	-65.72643375821995	1358
943745937a28f8ecb95ee1f7fb2968759b8ef006	ensemble approach for the classification of imbalanced data	119999;medical and health sciences;linear regression;data mining;boosting;gradient based optimisation;random forest;prediction accuracy;ensemble classifier;griffith health faculty;menzies health institute qld;decision trees;population and social health research program	Ensembles are often capable of greater prediction accuracy than any of their individual members. As a consequence of the diversity between individual base-learners, an ensemble will not suffer from overfitting. On the other hand, in many cases we are dealing with imbalanced data and a classifier which was built using all data has tendency to ignore minority class. As a solution to the problem, we propose to consider a large number of relatively small and balanced subsets where representatives from the larger pattern are to be selected randomly. As an outcome, the system produces the matrix of linear regression coefficients whose rows represent random subsets and columns represent features. Based on the above matrix we make an assessment of how stable the influence of the particular features is. It is proposed to keep in the model only features with stable influence. The final model represents an average of the base-learners, which are not necessarily a linear regression. Test results against datasets of the PAKDD-2007 data-mining competition are presented.	coefficient;column (database);data mining;overfitting;randomness;the matrix	Vladimir Nikulin;Geoffrey J. McLachlan;Shu-Kay Ng	2009		10.1007/978-3-642-10439-8_30	random forest;computer science;linear regression;artificial intelligence;machine learning;decision tree;pattern recognition;data mining;boosting;statistics	ML	14.21769077254081	-41.342541471701175	1362
907858eb5aca962d3812f6b2ec6f666010f110b3	imprecise specification of ill-known functions using gradual rules	computacion informatica;fuzzy data;fuzzy rules;rule based;reference point;ciencias basicas y experimentales;grupo a	Functional laws may be known only at a finite number of points, and then the function is completed by interpolation techniques obeying some smoothness conditions. We rather propose here to specify constraints by means of gradual rules for delimiting areas where the function may lie between known points. The more general case where the known points of the function are imprecisely located is also dealt with. The use of gradual rules for expressing constraints on the closeness with respect to reference points leads to interpolation graphs that are imprecise but still crisp. We thus propose a refinement of the rule-based representation that enables the handling of fuzzy interpolation graphs.	categorization;centrality;constraint (mathematics);data mining;delimiter;fuzzy concept;interpolation;logic programming;obedience (human behavior);overshoot (signal);refinement (computing);sparse matrix;time series	Sylvie Galichet;Didier Dubois;Henri Prade	2004	Int. J. Approx. Reasoning	10.1016/j.ijar.2003.08.002	rule-based system;discrete mathematics;computer science;artificial intelligence;mathematics;algorithm	Logic	-0.6892067815983035	-25.20898877629718	1363
7ae6ea183bc01272b4125561e96edeaf3cdb801b	coding of 3d videos based on visual discomfort	rate distortion;degradation;measurement;visualization indexes measurement videos three dimensional displays rate distortion degradation;indexes;visualization;three dimensional displays;visual discomfort rate distortion optimization depth map 3d video perceptual quality image fidelity;videos	We propose a rate-distortion optimization method for 3D videos based on visual discomfort estimation. We calculate visual discomfort in the encoded depth maps using two indexes: temporal outliers (TO) and spatial outliers (SO). These two indexes are used to measure the difference between the processed depth map and the ground truth depth map. These indexes implicitly depend on the amount of edge information within a frame and on the amount of motion between frames. Moreover, we fuse these indexes considering the temporal and spatial complexities of the content. We test the proposed method on a number of videos and compare the results with the default rate-distortion algorithms in the H.264/AVC codec. We evaluate rate-distortion algorithms by comparing achieved bit-rates, visual degradations in the depth sequences and the fidelity of the depth videos measured by SSIM and PSNR.	algorithm;codec;data rate units;depth map;distortion;ground truth;h.264/mpeg-4 avc;mathematical optimization;peak signal-to-noise ratio;rate–distortion optimization;spatial network;structural similarity;vienna development method	Dogancan Temel;Ghassan Al-Regib	2013	2013 Asilomar Conference on Signals, Systems and Computers	10.1109/ACSSC.2013.6810515	computer vision;computer science;multimedia;computer graphics (images)	Vision	44.5144469610944	-18.785632424381852	1372
47381c58cf95e67296d0a6d242086151c4cb3696	expert system based on artificial neural networks for content-based image retrieval	high dimensionality;image clustering;fuzzy art;hsv joint histogram;gray level co occurrence matrix;content based image retrieval;artificial neural network;expert system	Clustering technique is essential for fast retrieval in large database. In this paper, new image clustering technique based on artificial neural networks is proposed for content-based image retrieval. Fuzzy-ART mechanism maps high-dimensional input features into the output neuron. Joint HSV histogram and average entropy computed from gray-level co-occurrence matrices in the localized image region is employed as input feature elements. Original Fuzzy-ART suffers unnecessary increase of the number of output neurons when the noise input is presented. Modified FuzzyART mechanism resolves the problem by differently updating the committed node and uncommitted node, and checking the vigilance test again. To show the validity of the proposed algorithm, experiment results on image clustering performance and comparison with original Fuzzy-ART are presented in terms of recall rates. q 2005 Elsevier Ltd. All rights reserved.	algorithm;artificial neural network;cluster analysis;content-based image retrieval;database;expert system;map;neuron	Sang-Sung Park;Kwang-Kyu Seo;Dong-Sik Jang	2005	Expert Syst. Appl.	10.1016/j.eswa.2005.04.027	visual word;computer science;artificial intelligence;machine learning;data mining;expert system;artificial neural network	AI	13.752328171687918	-30.854878144117833	1374
29770c627c0e3ff68dd6cf6d9d1e342daf074289	locality preserving projections	locality preserving projection;linear approximation;variational problem;data representation;high dimensional data;information processing;principal com ponent analysis;local linear embedding	Many problemsin informationprocessinginvolve someform of dimensionalityreduction.In this paper , we introduceLocality PreservingProjections(LPP).Thesearelinear projective mapsthat ariseby solving a variationalproblemthatoptimally preservestheneighborhoodstructure of thedataset. LPPshouldbeseenasanalternati ve to PrincipalComponentAnalysis (PCA) – a classicallinear techniquethat projectsthe dataalongthe directionsof maximalvariance. Whenthe high dimensionaldataliesona low dimensionalmanifoldembeddedin theambient space,the Locality PreservingProjectionsare obtainedby finding the optimal linearapproximationsto theeigenfunctionsof theLaplaceBeltrami operatoron the manifold. As a result, LPP sharesmany of the datarepresentationpropertiesof nonlineartechniquessuchasLaplacian Eigenmapsor Locally Linear Embedding.Yet LPP is linear andmore crucially is definedeverywherein ambientspaceratherthanjust on the trainingdatapoints. This is borneout by illustrative exampleson some highdimensionaldatasets.	locality of reference;nonlinear dimensionality reduction	Xiaofei He;Partha Niyogi	2003			mathematical optimization;discrete mathematics;topology;information processing;computer science;mathematics;external data representation;linear approximation;clustering high-dimensional data	ML	26.135502525631964	-41.36996783788284	1379
79c21a730440f8e4e103323df2396bf01218d5ed	a fast iterated conditional modes algorithm for water–fat decomposition in mri	fats;algorithms animals computer simulation fats image processing computer assisted linear models magnetic resonance imaging markov chains mice reproducibility of results water whole body imaging;stability criteria;animals;optimisation;mice;whole body imaging;iterated conditional modes icm;magnetic resonance imaging mri;iterated conditional mode;clinical application;probability density function;stability tracking;water fat;water biomedical mri fats gradient methods markov processes medical image processing optimisation parameter estimation;indexing terms;image processing computer assisted;magnetic resonance image;markov random field;accuracy;nonhomogeneous media;high resolution mouse datasets iterated conditional mode algorithm mri magnetic resonance imaging two phased approach three point water fat decomposition problem background masked markov random field energy model field inhomogeneity high performance optimization median based initialization algorithm adaptive gradient based scheme parametric configuration;error propagation;background masking;medical image processing;magnetic resonance imaging;pixel;markov random field mrf;median value initialization;reproducibility of results;field inhomogeneity;gradient methods;algorithms;water fat background masking field inhomogeneity iterated conditional modes icm magnetic resonance imaging mri markov random field mrf median value initialization stability tracking;markov processes;parameter estimation;linear models;high performance;computer simulation;pixel nonhomogeneous media stability criteria magnetic resonance imaging accuracy probability density function;water;markov chains;biomedical mri	Decomposition of water and fat in magnetic resonance imaging (MRI) is important for biomedical research and clinical applications. In this paper, we propose a two-phased approach for the three-point water-fat decomposition problem. Our contribution consists of two components: 1) a background-masked Markov random field (MRF) energy model to formulate the local smoothness of field inhomogeneity; 2) a new iterated conditional modes (ICM) algorithm accounting for high-performance optimization of the MRF energy model. The MRF energy model is integrated with background masking to prevent error propagation of background estimates as well as improve efficiency. The central component of our new ICM algorithm is the stability tracking (ST) mechanism intended to dynamically track iterative stability on pixels so that computation per iteration is performed only on instable pixels. The ST mechanism significantly improves the efficiency of ICM. We also develop a median-based initialization algorithm to provide good initial guesses for ICM iterations, and an adaptive gradient-based scheme for parametric configuration of the MRF model. We evaluate the robust of our approach with high-resolution mouse datasets acquired from 7T MRI.	algorithm;blinded;computation;computation (action);estimated;fatty acid glycerol esters;gradient;image resolution;iterated conditional modes;iterated function;iteration;iterative method;magnetic resonance imaging;mandibular right second molar tooth;markov chain;markov random field;mathematical induction;mathematical optimization;name;pixel;platelet glycoprotein 4, human;policy;propagation of uncertainty;simplicity (adhesive);software propagation;xfig	Fangping Huang;Sreenath Narayan;David L. Wilson;David H. Johnson;Guo-Qiang Zhang	2011	IEEE Transactions on Medical Imaging	10.1109/TMI.2011.2125980	computer vision;water;markov chain;mathematical optimization;probability density function;index term;propagation of uncertainty;magnetic resonance imaging;machine learning;linear model;mathematics;accuracy and precision;markov process;estimation theory;pixel;statistics	Vision	52.37342992331262	-76.6000980389345	1382
6a94d05c714768f70673457e0f18a1f9e59c26fc	harnessing real-world depth edges with multiflash imaging	object recognition;rendering computer graphics edge detection cameras;edge detection;nonphotorealistic rendering;geometric feature;stylized images real world depth edges multiflash imaging geometric features capture setup modification portable self contained device multiple strategically positioned flashes depth discontinuities depth edge recovery 2d intensity edge detection 3d cameras imaging geometry rendering;cameras pixel light sources image edge detection layout assembly geometry engines brightness histograms;medical imaging;object recognition depth edges nonphotorealistic rendering medical imaging;rendering computer graphics;depth edges;cameras	A method for capturing geometric features of real-world scenes relies on a simple capture setup modification. The system might conceivably be packaged into a portable self-contained device. The multiflash imaging method bypasses 3D geometry acquisition and directly acquires depth edges from images. In the place of expensive, elaborate equipment for geometry acquisition, we use a camera with multiple strategically positioned flashes. Instead of having to estimate the full 3D coordinates of points in the scene (using, for example, 3D cameras) and then look for depth discontinuities, our technique reduces the general 3D problem of depth edge recovery to one of 2D intensity edge detection. Our method could, in fact, help improve current 3D cameras, which tend to produce incorrect results near depth discontinuities. Exploiting the imaging geometry for rendering provides a simple and inexpensive solution for creating stylized images from real scenes. We believe that our camera will be a useful tool for professional artists and photographers, and we expect that it will also let the average user easily create stylized imagery. This article is available with a short video documentary on CD-ROM.	cd-rom;contain (action);edge detection;guided imagery;preparation;video clip	Ramesh Raskar;Kar-Han Tan;Rogério Schmidt Feris;Matthew Turk;James Kobler;Jingyi Yu	2005	IEEE Computer Graphics and Applications	10.1109/MCG.2005.10	medical imaging;computer vision;simulation;edge detection;computer science;cognitive neuroscience of visual object recognition;computer graphics (images)	Graphics	60.162481354476725	-50.26662113383711	1384
296a2bd03bf17fe2753322b165918ae8717fc873	investigating and exploiting the bias of the weighted hypervolume to articulate user preferences	theory and practice;pareto front;search algorithm;weight distribution;evolutionary multiobjective optimization;preference articulation;dynamic program;user preferences;hypervolume indicator;exact algorithm	Optimizing the hypervolume indicator within evolutionary multiobjective optimizers has become popular in the last years. Recently, the indicator has been generalized to the weighted case to incorporate various user preferences into hypervolume-based search algorithms. There are two main open questions in this context: (i) how does the specified weight influence the distribution of a fixed number of points that maximize the weighted hypervolume indicator? (ii) how can the user articulate her preferences easily without specifying a certain weight distribution function?  In this paper, we tackle both questions. First, we theoretically investigate optimal distributions of μ points that maximize the weighted hypervolume indicator. Second, based on the obtained theoretical results, we propose a new approach to articulate user preferences within biobjective hypervolume-based optimization in terms of specifying a desired density of points on a predefined (imaginary) Pareto front. Within this approach, a new exact algorithm based on dynamic programming is proposed which selects the set of μ points that maximizes the (weighted) hypervolume indicator. Experiments on various test functions show the usefulness of this new preference articulation approach and the agreement between theory and practice.	biconnected component;distribution (mathematics);dynamic programming;exact algorithm;experiment;imaginary time;mathematical optimization;optimizing compiler;pareto efficiency;search algorithm;user (computing)	Anne Auger;Johannes Bader;Dimo Brockhoff;Eckart Zitzler	2009		10.1145/1569901.1569980	mathematical optimization;computer science;artificial intelligence;weight distribution;multi-objective optimization;machine learning;mathematics;search algorithm	AI	20.88494213728565	-4.628024405757769	1385
edb939c04aa9dc86049cd7383a7c88eabc112674	an optimized model for estimation of muscle contribution and human joint torques from semg information	electromyography biological tissues biomechanics;biological tissues;motion generation optimized model muscle contribution estimation human joint torques semg information hill model based technique human elbow torque optimization process activation levels;muscles torque fatigue robots joints modeling resistance;adult elbow joint electromyography humans male models biological muscle skeletal torque;biomechanics;electromyography	This paper develops a Hill model based technique to estimate human elbow torque from sEMG measurements. Some new parameters are included in the optimization process in order to improve the resulting estimated torque. These parameters correspond to activation levels of muscles involved in motion generation. They have not previously been used in other works dealing with this kind of model. Results from experiments with several subjects in different movement conditions and using the new optimized parameters lead to some conclusions about the generality of the optimized models and the influence of the new parameters on the improvement of the estimation.	experiment;mathematical optimization;muscle;sema5b gene	Diana R. Bueno;Luis Montano	2012	2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/EMBC.2012.6346686	simulation;engineering;biomechanics;physiology;anatomy;surgery	Robotics	69.74884129578385	-28.363167784912626	1387
19712dc022db5d1eb6fa7b9abe6fa9661c0c6c4d	global stabilisation of the pvtol aircraft with lateral force coupling and bounded inputs	global stabilisation;control algorithm;saturacion;input saturation;global stabilization;programme commande;etude experimentale;small parameter method;stabilisation globale;aeronef;nonlinear control;methode petit parametre;aeronave;pvtol aircraft;vtol aircraft;commande non lineaire;control program;estabilizacion global;vertical take off and landing;programa mando;bounded inputs;non linear control;robustness;saturations;metodo pequeno parametro;estudio experimental;avion adav;saturation;control no lineal;aircraft	Global stabilisation of the PVTOL aircraft with lateral force coupling and bounded inputs D.J. López-Araujo a , A. Zavala-Río a , I. Fantoni b , S. Salazar b & R. Lozano b a IPICYT , Camino a la Presa San José 2055, Lomas 4a., Sección 78216, San Luis Potosí, Mexico b Université de Technologie de Compiègne CNRS , UMR 6599 Heudiasyc, BP 20529, 60200 Compiègne, France Published online: 23 Jun 2010.	camino;lateral thinking;linear algebra	Daniela Juanita López-Araujo;Arturo Zavala-Río;Isabelle Fantoni;Sergio Salazar;Rogelio Lozano	2010	Int. J. Control	10.1080/00207171003758778	control engineering;simulation;nonlinear control;engineering;control theory;mathematics;saturation;robustness	DB	74.89202012908652	-5.223655947516816	1390
fb9832c69a86dce484408fb2e885ba7d7c9157bd	a knee arthroscopy simulator: design and validation	teaching biomedical education bone surgery;bone;surgery;skills assessment knee arthroscopy simulator;surgery joints training instruments face force measurement;biomedical education;expert surgeons high fidelity physical knee arthroscopy simulator arthroscopic surgery learning process risk free environment modular replaceable elements instrument position hand motion;teaching	Many challenges exist when teaching and learning arthroscopic surgery, carrying a high risk of damaging the joint during the learning process. To minimize risk, the use of arthroscopy simulators allows trainees to learn basic skills in a risk-free environment before entering the operating room. A high-fidelity physical knee arthroscopy simulator is proposed to bridge the gap between surgeons and residents. The simulator is composed of modular and replaceable elements and can measure applied forces, instrument position and hand motion, in order to assess performance in real time. A construct validity study was conducted in order to assess the performance improvement of novices after practicing with the simulator. In addition, a face validity study involving expert surgeons indicated that the simulator provides a realistic scenario suitable for teaching basic skills. Future work involves the development of better metrics to assess user performance.	arthroscopy;health care;operating room;patients;simulators	Abelardo Escoto;Fraser Le Ber;Ana Luisa Trejos;Michael D. Naish;Rajnikant V. Patel;Marie-Eve LeBel	2013	2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2013.6610848	education;simulation;medicine;engineering;biological engineering;surgery	Robotics	39.82652129345405	-37.45060683334064	1399
ad6fa62ca4a93ed0556df04098947192dc3bc5fe	pose estimation of rigid transparent objects in transparent clutter	table cleaning pose estimation rigid transparent objects transparent clutter human environments recognition cluttered environments active depth sensor object segmentation 2d edge analysis microsoft kinect pr2 robot robotic grasping;clutter;image segmentation;edge detection;pose estimation clutter edge detection image segmentation;three dimensional displays image edge detection estimation clutter cost function solid modeling image segmentation;pose estimation	Transparent objects are ubiquitous in human environments but, due to their special interaction with light, very few vision methods exist to identify them. We propose a new algorithm for recognition and pose estimation of rigid transparent objects which can deal with overlapping instances and cluttered environments. Using an active depth sensor for segmentation of the objects and 2d edge analysis for pose estimation, we are able to provide accurate identification and position. The proposed method is evaluated on a Microsoft Kinect and also on a PR2 robot. Results show that the algorithm is robust and accurate enough for robotic grasping and that it can be used in practical applications like table cleaning.	3d pose estimation;algorithm;clutter;kinect;pixel;plasma cleaning;range imaging;robot	Ilya Lysenkov;Vincent Rabaud	2013	2013 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2013.6630571	computer vision;simulation;pose;edge detection;3d pose estimation;computer science;articulated body pose estimation;clutter;image segmentation;computer graphics (images)	Robotics	50.580148730760406	-40.92496195470996	1407
d29a231c1a4af46ad7f0cda934ade50812c66593	automated sizing of analog circuits based on genetic algorithm with parameter orthogonalization procedure	analog circuits;genetic algorithm	This paper presents a method for the automated sizing of analog circuits using genetic algorithm (GA). For the rapid and efficient exploration of GA, we introduce the idea of search space sphering and dimension reduction with principal component analysis (PCA). The potential capability of the system is demonstrated through the automated sizing of wide-swing current mirror circuit. Experimental results show that the system with PCA successfuly generates higher-performance circuits under given evaluation function on average, and the dimensionally reduction method further improves the performance of solutions.	analogue electronics;automation;current mirror;dimensionality reduction;evaluation function;genetic algorithm;informatics;principal component analysis;robotics;software release life cycle;transistor	Masanori Natsui;Yoshiaki Tadokoro	2008			mathematical optimization;electronic engineering;genetic algorithm;analogue electronics;computer science;artificial intelligence;machine learning	EDA	15.07590437677354	-21.78549599654378	1414
16087f102829c21ad0ead0edd6814ac56b2afbc5	metaheuristics based on grasp and vns for solving the traveling purchaser problem	parallel algorithms costs parallel processing traveling salesman problems computer science electronic mail performance analysis algorithm design and analysis genetic algorithms upper bound;optimisation;dynamic load balancing;parallel algorithm;resource allocation;heuristic programming;combinatorial optimization problem;randomised algorithms;greedy randomized adaptive search procedure;heuristic programming parallel algorithms search problems randomised algorithms optimisation combinatorial mathematics resource allocation;variable neighborhood search;search problems;parallel implementation;performance distributed parallel metaheuristics greedy randomized adaptive search procedure variable neighborhood search combinatorial optimization problem traveling purchaser problem parallel algorithms master worker models distributed models independent models dynamic load balance static load balance;combinatorial mathematics;parallel algorithms	This paper presents several strategies for sequential and parallel implementations of the Greedy Randomized Adaptive Search Procedure (GRASP) and the Variable Neighborhood Search (VNS) applied to a combinatorial optimization problem known as Traveling Purchaser Problem (TPP). The high potential of these metaheuristics and their hybrid versions (GRASP+VNS) is shown through the comparison with an algorithm Tabu Search for the TPP, that has presented the best results for this problem so far.	combinatorial optimization;grasp;greedy algorithm;greedy randomized adaptive search procedure;mathematical optimization;metaheuristic;optimization problem;randomized algorithm;tpp;tabu search;traveling purchaser problem;variable neighborhood search	Luiz Satoru Ochi;Leonardo Soares Vianna;Mozar B. da Silva;Lucia M. de Assumpcao Drummond	2001		10.1109/ICPADS.2002.1183409	mathematical optimization;parallel computing;computer science;theoretical computer science;machine learning;parallel algorithm	AI	22.191027125902416	1.9914381048499268	1419
58616eaf5b549a1afe7296061b6d0c91823e391e	identification of dynamic models in complex networks with prediction error methods - basic methods for consistent module estimates	computacion informatica;grupo de excelencia;ciencias basicas y experimentales	The problem of identifying dynamical models on the basis of measurement data is usually considered in a classical open-loop or closed-loop setting. In this paper, this problem is generalized to dynamical systems that operate in a complex interconnection structure and the objective is to consistently identify the dynamics of a particularmodule in thenetwork. For a known interconnection structure it is shown that the classical prediction error methods for closed-loop identification can be generalized to provide consistent model estimates, under specified experimental circumstances. Two classes of methods considered in this paper are the direct method and the joint-IO method that rely on consistent noise models, and indirect methods that rely on external excitation signals like two-stage and IV methods. Graph theoretical tools are presented to verify the topological conditions under which the several methods lead to consistent module estimates. © 2013 Elsevier Ltd. All rights reserved.	agent-based model;complex network;direct method in the calculus of variations;dynamical system;interconnection	Paul M. J. Van den Hof;Arne G. Dankers;Peter S. C. Heuberger;Xavier Bombois	2013	Automatica	10.1016/j.automatica.2013.07.011	econometrics;mathematics;statistics	ML	61.066873721432124	3.773423763679702	1422
30f302e0282a2df85fc370af17392831dae2d272	fracture analysis of brittle materials based on nonlinear fem and application in arch dam with fractures		Current fracture analysis models based on fracture mechanics or continuum damage mechanics are still limited in the application to three-dimensional structure. Based on deformation reinforcement theory coming from elastoperfect plastic theory, unbalanced force is proposed to predict initiation and propagation of cracks. Unbalanced force is the driving force of time-dependent deformation according to Perzyna’s viscoplasticity theory. It is also related to the damage driving force in viscoplastic damagemodel. The distribution of unbalanced force indicates cracks initiation area, while its direction predicts possible cracks propagation path. Uniaxial compression test of precrack specimen is performed as verification to this method.The trend and distribution of cracks are in good agreement with numerical results, proving that unbalanced force is feasible and effective for fracture analysis. The method is applied in fracture analysis of Xiaowan high arch dam, which is subjected to some cracks in dam due to the temperature control program.The results show that the deformation and stress of cracks and the stress characteristics of dam are insensitive to grouting of cracks. The existing cracks are stable and dam heel is still the most possible cracking position.		Yuanwei Pan;Yaoru Liu;Zhixiong Cui;Xin Chen;Qiang Yang	2013	J. Applied Mathematics	10.1155/2013/658160	geotechnical engineering;mathematical optimization;deformation (mechanics);brittleness;viscoplasticity;finite element method;arch dam;compression (physics);damage mechanics;fracture mechanics;mathematics	SE	85.62361620102924	-12.206220378050466	1424
33a9217209523ee8d14c33dd05c90db12269824e	an intelligent method to discover transition rules for cellular automata using bee colony optimisation	fysisk geografi;urban simulation;transition rules;ca;datavetenskap datalogi;computer science;bee colony optimisation;physical geography	This paper presents a new, intelligent approach to discover transition rules for geographical cellular automata CA based on bee colony optimisation BCO–CA that can perform complex tasks through the cooperation and interaction of bees. The artificial bee colony miner algorithm is used to discover transition rules. In BCO–CA, a food source position is defined by its upper and lower thresholds for each attribute, and each bee searches the best upper and lower thresholds in each attribute as a zone. A transition rule is organised when the zone in each attribute is connected to another node by the operator ‘And’ and is linked to a cell status value. The transition rules are expressed by the logical structure statement ‘IF-Then’, which is explicit and easy to understand. Bee colony optimisation could better avoid the tendency to be vulnerable to local optimisation through local and global searching in the iterative process, and it does not require the discretisation of attribute values. Finally, The BCO–CA model is employed to simulate urban development in the Xi’an-Xian Yang urban area in China. Preliminary results suggest that this BCO approach is effective in capturing complex relationships between spatial variables and urban dynamics. Experimental results indicate that the BCO–CA model achieves a higher accuracy than the NULL and ACO–CA models, which demonstrates the feasibility and availability of the model in the simulation of complex urban dynamic change.	automata theory;cellular automaton;mathematical optimization;production (computer science)	Jianyi Yang;Guoan Tang;Min Cao;Rui Zhu	2013	International Journal of Geographical Information Science	10.1080/13658816.2013.823498	simulation;calcium;computer science;artificial intelligence;machine learning	Robotics	19.565847143476066	-7.615469391445202	1426
3af7295abf876f11f2a9c37c5aed14c7c1d62778	multiple samples acgh analysis for rare cnvs detection	health research;uk clinical guidelines;biological patents;health informatics;europe pubmed central;citation search;uk phd theses thesis;life sciences;uk research reports;medical journals;europe pmc;biomedical research;biomedicine general;bioinformatics	DNA copy number variations (CNV) constitute an important source of genetic variability. The standard method used for CNV detection is array comparative genomic hybridization (aCGH). We propose a novel multiple sample aCGH analysis methodology aiming in rare CNVs detection. In contrast to the majority of previous approaches, which deal with cancer datasets, we focus on constitutional genomic abnormalities identified in a diverse spectrum of diseases in human. Our method is tested on exon targeted aCGH array of 366 patients affected with developmental delay/intellectual disability, epilepsy, or autism. The proposed algorithms can be applied as a post–processing filtering to any given segmentation method. Thanks to the additional information obtained from multiple samples, we could efficiently detect significant segments corresponding to rare CNVs responsible for pathogenic changes. The robust statistical framework applied in our method enables to eliminate the influence of widespread technical artifact termed ‘waves’.	array-based comparative genomic hybridization;autistic disorder;childhood absence epilepsy;congenital abnormality;copy number polymorphism;dna copy number variations;developmental delay (disorder);intellectual disability;neoplasms;nucleic acid hybridization;patients;spatial variability;algorithm;biologic segmentation	Maciej Sykulski;Tomasz Gambin;Magdalena Bartnik;Katarzyna Derwinska;Barbara Wisniowiecka-Kowalnik;Paweł Stankiewicz;Anna Gambin	2011		10.1186/2043-9113-3-12	health informatics;alternative medicine;medical research;medicine;bioinformatics;nursing	Comp.	3.988403007541205	-70.75269838971683	1436
e2b4beef9d97ea9ebb5bf5bdf04c5f475fc5714d	a review on leaf temperature sensor: measurement methods and application		Leaf temperature is the guarantee for the plant to carry out the life activities and closely related to plants’ healthy growth and crops’ planting management. The accurate measurement of leaf temperature is significant to understand the physiological condition, guide farmland irrigation, select variety and forecast production, etc. The development of plant leaf temperature measurement and requirements of application in recent years at home and abroad were briefly summarized and reviewed in the paper. Firstly, the status of application research and achievements of leaf temperature were introduced from the methods of measurement and scientific experiments. Then it analyzed and compared the principle, advantages and disadvantages and measurement of several common methods in detail including thermal resistance measurement, thermocouple measurement, infrared temperature measurement, infrared thermal imaging measurement and the leaf temperature model. At last, some problems urgently needed to be solved and the development direction of the field were presented, which could provide a reference for the further study of the leaf temperature sensor.		Lu Yu;Wenli Wang;Xin Zhang;Wengang Zheng	2015		10.1007/978-3-319-48357-3_21	temperature measurement;environmental science;ecology;thermocouple;agronomy;thermal resistance	Robotics	85.95104871109373	-52.96572291675058	1437
5d8c876001e03612ffcccbe3d7e6460961e03399	cognition située chez le singe capucin	inhibicion;relativization;relativisacion;mammalia;tool;relativisation;analisis espacial;mono;herramienta;vertebrata;intelligence artificielle;monkey;cognition;singe;cognicion;primates;artificial intelligence;inteligencia artificial;inhibition;spatial analysis;outil;analyse spatiale	The presently-disclosed invention utilizes sensed pitched rate and sensed vertical acceleration to prevent aberrant pitch rate command outputs as a function of lead compensated sensed acceleration, controlling the vertical acceleration by sensing when a maximum is approached. When the lead compensated acceleration exceeds a first predetermined acceleration factor, the presently-sensed pitch rate replaces the pre-programmed maximum pitch rate factor as the temporary maximum rate, to temporarily prevent additional vertical acceleration due to increasing pitch rate. This apparatus requires no airspeed input and does not interfere with flight performance characteristics during normal flight operations, allowing higher torque servos to be used, and thus improves overall performance.	cognition;geforce 6 series	Michel Jean Dubois;Pascal Carlier	2005	Revue d'Intelligence Artificielle	10.3166/ria.19.253-264	cognition;computer science;artificial intelligence;spatial analysis	NLP	-1.1813128579907235	-79.53930521698648	1449
47bb4d62f2ab716b6ae4cee11f7a36f0dea0f1b8	patient-specific simulation of carotid artery stenting using computational fluid dynamics	contrast enhanced;phase contrast;computation fluid dynamics;carotid artery stenting;finite element;internal carotid artery;magnetic resonance angiography;experimental validation;blood flow;deformable model;carotid artery stenosis	"""An image-based computational methodology to predict the outcome of carotid artery stenting procedures is presented. Anatomically realistic models are reconstructed from contrast-enhanced magnetic resonance angiography images using deformable models. Physiologic flow conditions are obtained from phase-contrast magnetic resonance angiography data. Finite element flow calculations are obtained before and after modifying the anatomical models in order to simulate stenting procedures. The methodology was tested on image data from a patient with carotid artery stenosis. Significant changes in the blood flow through the common carotid and internal carotid artery were found after conducting a """"virtual stenting"""" intervention. Pending experimental validation, this methodology may potentially be used to plan and optimize vascular stenting procedures on a patient-specific basis."""		Juan R. Cebral;Rainald Löhner;Orlando Soto;Peter L. Choyke;Peter J. Yim	2001		10.1007/3-540-45468-3_19	radiology;medicine;blood flow;phase contrast microscopy;finite element method;surgery;cardiology	Robotics	38.58733319399648	-84.7494991015039	1452
47c93f6710b3bf762cd6f60c496262d72fa27111	optimal space subdivision for parallel approximate nearest neighbour determination	silicon;approximation algorithms;computer vision;estimation;clustering algorithms;computer science;clustering methods	Many computer vision tasks rely on correspondence between features. SIFT and related descriptors represent features as high-dimensional vectors, which are used to compute a distance between features for matching. Calculating these distances is expensive in large datasets, so approximate nearest neighbour (ANN) approaches are used. ANN schemes that can be efficiently parallelised have been proposed, some of which divide up high-dimensional vectors into lower-dimensional subspaces. However, the feature descriptors computed by SIFT and similar methods are not computed in a homogeneous manner. There are clear statistical patterns within and between the components of feature vectors, and in this work we show that these patterns can have a strong effect on approximate nearest neighbor searching algorithms that are based on space subdivision.	analysis of algorithms;approximation algorithm;computation;computer vision;feature vector;k-nearest neighbors algorithm;nearest neighbor search;random effects model;scale-invariant feature transform;search algorithm;subdivision surface;time complexity;tuple space	Huan Feng;Steven Mills;David M. Eyers;Xiaolong Shen;Zhiyi Huang	2015	2015 International Conference on Image and Vision Computing New Zealand (IVCNZ)	10.1109/IVCNZ.2015.7761548	computer vision;estimation;computer science;theoretical computer science;machine learning;pattern recognition;mathematics;cluster analysis;silicon;approximation algorithm;statistics	Vision	40.77495823249131	-55.98571056130787	1454
ba913f8b6ff5828deb4ae5fe2b7211770f6fdf2d	deep q learning based high level driving policy determination		With the commercialization of various Driver Assistance Systems (DAS), those vehicles have some autonomous functions like Smart Cruise Control (SCC) and Lane Keeping System (LKS). It is believed that autonomous driving can be achieved by combining the DAS functions in the limited situations such as on highways. However, in order to coordinate the DAS functions for autonomous driving, a supervisor is needed to select an appropriate DAS function. In this paper, we propose a method for training a supervisor that selects proper DAS by deep reinforcement learning. The driving policy operates based on camera images and LIDAR data that are accessible in autonomous vehicles. Therefore, deep reinforcement learning network model is designed to analyze both camera image and LIDAR data. This system aims to drive in simulated traffic situation of highway without collision and with high speed. Unlike the systems which learn how to throttle, brake and steering directly, the proposed method can guarantee safe driving because the learned driving policy is based on the existing commercialized DAS functions. In order to verify the algorithms, a simulation tool is developed using Unity for highway environment with multiple vehicles and autonomous driving performance is compared with the proposed supervisor.	algorithm;autonomous car;autonomous robot;machine learning;modal logic;network model;q-learning;reinforcement learning;simulation;unity	Haixia Xie;Hayoung Kim	2018	2018 IEEE Intelligent Vehicles Symposium (IV)	10.1109/IVS.2018.8500645	q-learning;advanced driver assistance systems;reinforcement learning;network model;artificial neural network;control engineering;cruise control;brake;computer science;supervisor	Robotics	52.61040602312637	-30.8768169611369	1459
9242de4b2d8f23473c70b14481f7a6a607171439	a smart robot's geometric model inverting system	control systems;systeme commande;systeme redondant;real time;robotics;mathematical inversion;inversion matematica;redundant system;temps reel;tiempo real;geometric model;robotique;sistema redundante;inversion mathematique	Abstract#R##N##R##N#This article deals with a real-time model inverting system. The mathematical inverting problem is no longer solved with a mathematical algorithm but through the design of the system control itself. The control algorithm is issued from human knowledge. The stability of this system is proved, and the properties are discussed. This system can be used where a geometric model should be inverted in real time, especially for redundant robots. (c) 1995 John Wiley & Sons, Inc.	geometric modeling;robot	J. Zhou;Philippe Coiffet	1995	J. Field Robotics	10.1002/rob.4620120905	computer science;engineering;artificial intelligence;geometric modeling;robotics;algorithm	Robotics	69.95965208965343	-17.98757714975074	1461
c79ca337671b07c71b72436b9cd020cb51a4d91f	evaluation and calculation of dynamics in environmental impact assessment	sustainable manufacturing;product lifecycle;sustainable production;environmental analysis;discrete event simulation	In ten years customers will select products not only based on price and quality but also with strong regard to the product value environmental footprint, including for example the energy consumed. Customers expect transparency in the product realization process, where most products are labeled with their environmental footprint. Vigorous companies see this new product value as an opportunity to be more competitive. In order to effectively label the envi-ronmental impact of a product, it is pertinent for companies to request the envi-ronmental footprint of each component from their suppliers. Hence, companies along the product lifecycle require a tool, not only to facilitate the computing of the environmental footprint, but also help reduce/balance the environmental impact during the lifecycle of the product. This paper proposes to develop a procedure that companies will use to evaluate, improve and externally advertise their product’s environmental footprint to customers.		Björn Johansson;Jon Andersson;Erik Lindskog;Jonatan Berglund;Anders Skoogh	2012		10.1007/978-3-642-40352-1_18	environmental scanning;engineering;marketing;operations management;product lifecycle;product design specification;product management;new product development;commerce;product engineering	HCI	1.9499440053013863	-7.444011717007457	1463
1c0e51f812d01f9a857eab9fb18cfc7184ae6352	an ai walk from pharmacokinetics to a marketing		• Artificial Neural Networks (ANNs): Multilayer Perceptron (MLP), Finite Impulse Response (FIR) Neural Network, Elman Network, Self-Oganizing Maps (SOMs) and Adaptive Resonance Theory (ART). • Other clustering algorithms: K-Means, Expectation-Maximization (EM) algorithm, Fuzzy C-Means (FCM), Hierarchical Clustering Algorithms (HCA). • Generalized Auto-Regressive Conditional Heteroskedasticity (GARCH). • Support Vector Regression (SVR). • Collaborative filtering techniques. • Reinforcement Learning (RL) methods.	adaptive resonance theory;artificial neural network;cluster analysis;collaborative filtering;expectation–maximization algorithm;finite impulse response;fuzzy cognitive map;hierarchical clustering;k-means clustering;multilayer perceptron;quad flat no-leads package;recurrent neural network;reinforcement learning;support vector machine	José David Martín-Guerrero;Emilio Soria-Olivas;Paulo J. G. Lisboa;Antonio J. Serrano	2009				ML	11.310561802386745	-25.433024271827055	1464
a99d2b8ec308d333ef7e81df08d3dab1ffa8d21b	filtered lyapunov functions and their applications in the stability analysis and control of nonlinear systems	lyapunov methods;control systems;design tool;uncertain systems;uncertainty;lyapunov function;nonlinear control systems;interconnected systems;stabilization;differential equation;filtered lyapunov function;satisfiability;stability;lyapunov method stability analysis nonlinear control systems control systems nonlinear systems interconnected systems control system analysis differential equations digital filters uncertainty;interconnected system;stability analysis;nonlinear control system;differential equation nonlinear control system stability analysis filtered lyapunov function uncertain system;filtered lyapunov functions;robustness;differential equations;stabilization filtered lyapunov functions interconnected systems;nonlinear system;uncertain system;filtering theory;iir filters;noise;uncertain systems differential equations filtering theory lyapunov methods noise nonlinear control systems stability	In this paper, we introduce a new type of Lyapunov functions in a general framework particularly suitable for the analysis and control of systems with noise and uncertainty. These Lyapunov functions may depend on parameters possibly satisfying differential equations. The main differences with respect to classical Lyapunov functions and classical tools for the design of composite Lyapunov functions are discussed through examples. A design tool for the design of composite filtered Lyapunov functions is given, and examples show improvements over existing literature.	design tool;lyapunov fractal;noise (electronics);nonlinear system	Stefano Battilotti	2008	IEEE Transactions on Automatic Control	10.1109/TAC.2007.914269	control engineering;mathematical optimization;control-lyapunov function;nonlinear system;lyapunov function;control system;lyapunov equation;control theory;mathematics;lyapunov redesign;lyapunov exponent;lyapunov optimization;differential equation;statistics	EDA	67.65753175837263	-1.0943577565663432	1465
32872b4e6bb936d1088fea132c49921b6e6c7203	distributed joint sensor registration and target tracking via sensor network		Abstract This paper deals with distributed registration of a sensor network for target tracking in the presence of false and/or missed measurements. Each sensor acquires measurements of the target position in local coordinates, having no knowledge about the relative positions (referred to as drift parameters) of its neighboring nodes. A distributed Bernoulli filter is run over the network to compute in each node a local posterior target density. Then a suitable cost function, expressing the discrepancy between the local posteriors in terms of averaged Kullback–Leibler divergence, is minimized with respect to the drift parameters for sensor registration purposes. In this way, a computationally feasible optimization approach for joint sensor registration and target tracking is devised. Finally, the effectiveness of the proposed approach is demonstrated through simulation experiments on both tree networks and networks with cycles, as well as with both linear and nonlinear sensors.	sensor web	Lin Gao;Giorgio Battistelli;Luigi Chisci;Ping Wei	2019	Information Fusion	10.1016/j.inffus.2018.05.003	local coordinates;wireless sensor network;divergence;artificial intelligence;mathematics;nonlinear system;pattern recognition;control theory;bernoulli's principle	Vision	53.35235852240955	3.3934187961131337	1471
6e206164bdfedc5c82b6af984b09663b848e88f1	anomaly detection in crowded scenes using log-euclidean covariance matrix		In this paper, we propose an approach for anomaly detection in crowded scenes. For this purpose, two important types of features that encode motion and appearance cues are combined with the help of covariance matrix. Covariance matrices are symmetric positive definite (SPD) matrices which lie in the Riemannian manifold and are not suitable for Euclidean operations. To make covariance matrices suitable for use in the Euclidean space, they are converted to log-Euclidean covariance matrices (LECM) by using log-Euclidean framework. Then LECM features created in two different ways are used with one-class SVM to detect abnormal events. Experiments carried out on an anomaly detection benchmark dataset and comparison made with previous studies show that successful results are obtained.	anomaly detection;benchmark (computing);cma-es;encode;euclidean distance;experiment;receiver operating characteristic;support vector machine	Efsun Sefa Sezer;Ahmet Burak Can	2018		10.5220/0006618402790286	anomaly detection;covariance matrix;machine learning;euclidean geometry;artificial intelligence;computer science	Vision	35.770123391168426	-51.58315508765791	1481
bdebc5a4a3aac8b0ebe0a991a88122527ce2498a	h∞ control for discrete-time singular markovian jump systems based on the novel bounded real lemma	discrete time;linear matrix inequality lmi;singular markovian jump systems;期刊论文;h control;bounded real lemma	This paper considers the H∞ control problem for the discrete-time singular Markovian jump systems with partially unknown transition probabilities. First, a new formulation of the bounded real lemma (BRL) for discrete-time singular Markovian jump systems is given by referencing slack variables technique, which ensures the considered systems to be regular, causal and stochastically stable with given H∞ performance index γ. Then, based on this new BRL, the desired controller gains are also presented by solving a set of strict linear matrix inequalities. A numerical example is provided to show the effectiveness and less conservativeness of the proposed methods.		Jianhua Wang;Qingling Zhang;Xiaoxu Liu	2015	Int. J. Systems Science	10.1080/00207721.2014.948947	mathematical optimization;discrete time and continuous time;discrete mathematics;control theory;mathematics	Theory	67.68711588471584	1.2170575916909958	1483
021c54e496d60feb3ddff222f263d2f6cd7df382	communication-efficient sparse regression: a one-shot approach		"""We devise a one-shot approach to distributed sparse regression in the high-dimensional setting. The key idea is to average """" debiased """" or """" desparsified """" lasso estimators. We show the approach converges at the same rate as the lasso as long as the dataset is not split across too many machines. We also extend the approach to generalized linear models."""	generalized linear model;lasso;sparse matrix	Jason D. Lee;Yuekai Sun;Qiang Liu;Jonathan E. Taylor	2015	CoRR		econometrics;machine learning;mathematics;statistics	ML	26.56070506453438	-32.59203762579	1485
ef220905b7b08551e58fb2812463d84071e22767	additive noise analysis on microarray data via svm classification	svm classification;microarray data;biology computing;support vector machines;cancer;disease related gene identification;additive noise;support vector machines biology computing lab on a chip learning artificial intelligence pattern classification;disease related gene identification additive noise analysis microarray data svm classification support vector machines;genetics;additive noise data analysis support vector machines support vector machine classification working environment noise condition monitoring genetics performance analysis noise measurement testing;accuracy;noise level;degeneration;environmental variation;classification algorithms;pattern classification;diseases;lab on a chip;differentially expressed gene;support vector machine;learning artificial intelligence;classification accuracy;noise;additive noise analysis	Microarray technology has been broadly used for monitoring the expression levels of thousands of genes simultaneously, providing the opportunities of identifying disease-related genes by finding differentially expressed genes in different conditions. However, a great challenge of analyzing microarray data is the significant noise brought by different experimental settings, laboratory procedures, genetic heterogeneity among samples, and environmental variations among different patients, and so on. This paper attempts to analyze the influence of these noises on each gene by measuring the changes of classification performance. We assume each gene in microarray data includes an independently distributed unknown uniform noise. Thus, we add a compensated noise back to each gene and test whether the classification accuracy of a linear support vector machine (SVM) improves. If the accuracy does increase, then we believe such noise does exist and degenerate the relation of this gene to the disease status. Through extensive experiments on several public microarray data, we found such added noises can improve the classification accuracy in several genes and the results are relatively consistent, indicating our method can be used to analyze the noise pattern in microarray experiments, and also discover potential important gene markers.	additive model;additive white gaussian noise;experiment;fits;feature selection;machine learning;microarray;support vector machine	Zejin Jason Ding;Yanqing Zhang	2010	2010 IEEE Symposium on Computational Intelligence in Bioinformatics and Computational Biology	10.1109/CIBCB.2010.5510725	statistical classification;support vector machine;computer science;bioinformatics;machine learning;data mining	Comp.	8.420056248927548	-48.089728934411234	1492
cf09e1321f99ada36fc239471d6b2c653a0aaf53	effects of quinidine on short qt syndrome variant 2 in the human ventricle: a modelling and simulation study		The short QT syndrome (SQTS) is a rare cardiac disorder associated with an abnormally short QT interval and an increased risk of ventricular arrhythmias and sudden cardiac death (SCD). Gain-of-function mutation to potassium channels mediating the slow delayed rectifier current, IKs, underlie KCNQ1-linked SQTS variant 2 (SQT2), in which treatment with sodium, calcium and potassium channel blocking class Ia anti-arrhythmic agents has demonstrated some efficacy. This study used computational modelling and simulation to gain mechanistic insights into the actions the clinical drug, quinidine, in the setting of SQT2. The ten Tusscher et al. human ventricle model was modified to incorporate KCNQ1 V307L mutation-induced changes to IKs based on experimentally observed data: wild type (WT) and SQT2 mutant conditions were studied. Actions of quinidine were simulated by implementing a simple pore block theory to simulate the drug blocking effects on IKr, IKs, Ito, INa, ICaL, INaCa and INaL, which were modelled using IC50 and Hill coefficient. Cell models were incorporated into one-dimensional (1D) model that considered the intrinsic electrical heterogeneity in the left ventricle. At a clinically therapeutic concentration of 10μΜ quinidine, the action potential duration (APD) was significantly increased, and the QT interval on the pseudo-ECG was prolonged. This study helps to better understand the underlying mechanisms of pharmacological therapy, and provides further evidence that quinidine is a suitable treatment for the SQT2 phenotype.	action potential;auditory processing disorder;blocking (computing);coefficient;experiment;qt (software);rectifier (neural networks);simulation	Cunjin Luo;Kuanquan Wang;Henggui Zhang;Yang Liu;Yong Xia	2017	2017 Computing in Cardiology (CinC)		short qt syndrome;wild type;potassium channel;quinidine;ventricle;sudden cardiac death;qt interval;cardiology;internal medicine;medicine;short qt interval	AI	11.609049380221258	-69.07072830770349	1496
93cd7ace6c756d4ef9765fdea80e94c12601ae40	unsupervised domain adaptation for 3d keypoint prediction from a single depth scan		In this paper, we introduce a novel unsupervised domain adaptation technique for the task of 3D keypoint prediction from a single depth scan/image. Our key idea is to utilize the fact that predictions from different views of the same or similar objects should be consistent with each other. Such view consistency provides effective regularization for keypoint prediction on unlabeled instances. In addition, we introduce a geometric alignment term to regularize predictions in the target domain. The resulting loss function can be effectively optimized via alternating minimization. We demonstrate the effectiveness of our approach on real datasets and present experimental results showing that our approach is superior to state-of-the-art generalpurpose domain adaptation techniques.	3d modeling;alternating turing machine;domain adaptation;loss function	Xingyi Zhou;Arjun Karpur;Chuang Gan;Linjie Luo;Qi-Xing Huang	2017	CoRR		domain adaptation;regularization (mathematics);mathematics;artificial intelligence;pattern recognition	Vision	28.12146573743804	-48.69168336217651	1498
0d3fc86d8a3b7c0af07230bf339a55a7c136065f	developing remote sensing methodology to distinguish urban built-up areas and bare land in mafikeng town, south africa	remote sensing normalized difference impervious surface index normalized difference water index normalized difference bare soil index normalized difference built up area index normalized difference vegetation index urban policy planning decisions land management urban land cover feature extraction knn classifier urban land cover classification supervised classification algorithms landsat 8 oli data ndsi ndwi ndisi ndbai ndvi south africa mafikeng town urban bare land urban built up areas;remote sensing vegetation mapping land surface radiometry satellites urban areas earth;vegetation mapping feature extraction geophysical image processing image classification land cover land use planning;indices spectral separability urban mapping mafikeng	Application of remote sensing technologies in mapping urban land cover still poses a challenge among urban planners. The aim of this study was to develop remote sensing methodology for distinguishing bare surface and built-up area in Mafikeng, South Africa. Several indices were developed to depict various urban features including NDVI, NDBAI, NDISI, NDWI and NDSI using Landsat 8-OLI data. Different supervised classification algorithms were independently tested to determine their ability in extracting the urban land cover classes. Field survey was conducted to gather ground truth data for accuracy assessment. The classification results proved that KNN was effective in not only increasing the classification accuracy, but also in making the classification of urban land cover features more visible and distinguishable than the other classifiers. The results demonstrate the potential of KNN classifier and combination of several indices to accurately map urban land cover features that can be used as input to land management and urban policy planning decisions.	ground truth;k-nearest neighbors algorithm;machine learning;supervised learning	Lobina G. Palamuleni;Naledzani N. Ndou	2014	2014 IEEE Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2014.6946906	hydrology;remote sensing	HCI	79.16867992266269	-57.7602671241563	1502
73412e244b70614e3c7349897144bb74952edee3	robust optimization-based calculation of invariant trajectory representations for point and rigid-body motion		Invariant representations of demonstrated motion trajectories provide context-independent motion models that can be used in motion recognition and generalization applications such as robot programming by demonstration. In practice, the use of invariant representations is still limited because their numerical calculation from a demonstrated trajectory is complicated by sensitivity to measurement noise and singularities, yielding inaccurate invariant functions that do not correspond well with the original trajectory. This paper improves the calculation of invariant representations for point and rigid-body motions by reformulating their calculation as an optimization problem that minimizes the error between the trajectory reconstructed from the invariant representation and the measured trajectory. Robustness against noise and singularities is ensured through the addition of regularization terms on the invariants. Simulations and real motion experiments show that the accuracy of the calculated invariant representations greatly improves with respect to standard smoothing methods. These results encourage future developments of motion recognition and generalization applications based on invariant trajectory representations.		Maxim Vochten;Tinne De Laet;Joris De Schutter	2018	2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2018.8593540	computer vision;algorithm;gravitational singularity;robust optimization;robustness (computer science);artificial intelligence;noise measurement;smoothing;rigid body;invariant (mathematics);trajectory;computer science	Robotics	50.807356732545394	-50.31864037989527	1504
080fd65904acc9eb9955da6639f58a83375b11db	brain connectivity hyper-network for mci classification		Brain connectivity network has been used for diagnosis and classification of neurodegenerative diseases, such as Alzheimer's disease (AD) as well as its early stage, i.e., mild cognitive impairment (MCI). However, conventional connectivity network is usually constructed based on the pairwise correlation among brain regions and thus ignores the higher-order relationship among them. Such information loss is unexpected because the brain itself is a complex network and the higher-order interaction may contain useful information for classification. Accordingly, in this paper, we propose a new brain connectivity hyper-network based method for MCI classification. Here, the connectivity hyper-network denotes a network where an edge can connect more than two brain regions, which can be naturally represented with a hyper-graph. Specifically, we first construct connectivity hyper-networks from the resting-state fMRI time series using sparse representation modeling. Then, we extract three sets of the brain-region specific features from the connectivity hyper-networks, and exploit a manifold regularized multi-task feature selection method to jointly select the most discriminative features. Finally, we use multi-kernel support vector machine (SVM) for classification. The experimental results demonstrate the efficacy of our proposed method for MCI classification with comparison to the conventional connectivity network based methods.	alzheimer's disease;brain diseases;classification;cognition disorders;complex network;computer multitasking;feature selection;greater than;hyperactive behavior;limited stage (cancer stage);mild cognitive disorder;neurodegenerative disorders;numerous;rest;resting state fmri;sensorineural hearing loss (disorder);sparse approximation;sparse matrix;support vector machine;time series;disease classification;manifold	Biao Jie;Dinggang Shen;Daoqiang Zhang	2014	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/978-3-319-10470-6_90	computer science;artificial intelligence;machine learning;pattern recognition	ML	23.632765607029803	-78.26944026423742	1507
93a9e70bb3f189ef12db89985f73d7b457415bd3	biologically inspired adaptive mobile robot search with and without gradient sensing	robot sensing systems;optimisation;yuragi;bacterial chemotaxis;specialized random walks;mobile robot;adaptive control;biological fluctuation;mobile robots;biological fluctuation biologically inspired adaptive mobile robot search biological creatures bacterial chemotaxis levy walk specialized random walks fractal movement trajectories random search optimisation yuragi;trajectory;random walk;biological creatures;random search optimisation;mathematical model;biologically inspired adaptive mobile robot search;mobile robots microorganisms robot kinematics intelligent robots robot sensing systems fluctuations organisms chemical hazards educational technology biological system modeling;search problems;fractal movement trajectories;levy walk;search problems adaptive control mobile robots optimisation;random search;microorganisms;noise;robot kinematics	Many biologically inspired approaches have been investigated in relation with researches on mobile robot(s) that can effectively locate targets that induce gradient information. Here, we concentrate on realizing an adaptive searching behavior in mobile robot that is simple, yet effective with and without sensing the gradient information. We are interested in two searching behaviors found in biological creatures: bacterial chemotaxis, probably the simplest yet effective gradient sources searching behavior found in living creatures; and Levy walk, specialized random walks with fractal movement trajectories that optimize random search for sparsely and randomly distributed target(s). Our approach is to implement and combine the two searching behaviors based on “yuragi”, a Japanese word for biological fluctuation.	computer performance;experiment;fractal;gradient descent;lévy flight;mobile robot;quantum fluctuation;random search;randomness	Surya Girinatha Nurzaman;Yoshio Matsumoto;Yutaka Nakamura;Satoshi Koizumi;Hiroshi Ishiguro	2009	2009 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2009.5353998	mobile robot;computer vision;simulation;adaptive control;computer science;artificial intelligence;mathematics	Robotics	28.306578403562394	-9.685386263126569	1509
dcb8a48a1df24ea8042fb675f5460a8585e80171	a benchmark for comparing precision medicine methods in thyroid cancer diagnosis using tissue microarrays		Motivation The aim of precision medicine is to harness new knowledge and technology to optimize the timing and targeting of interventions for maximal therapeutic benefit. This study explores the possibility of building AI models without precise pixel-level annotation in prediction of the tumor size, extrathyroidal extension, lymph node metastasis, cancer stage and BRAF mutation in thyroid cancer diagnosis, providing the patients' background information, histopathological and immunohistochemical tissue images.   Results A novel framework for objective evaluation of automatic patient diagnosis algorithms has been established under the auspices of the IEEE International Symposium on Biomedical Imaging 2017- A Grand Challenge for Tissue Microarray Analysis in Thyroid Cancer Diagnosis. Here, we present the datasets, methods and results of the challenge and lay down the principles for future uses of this benchmark. The main contributions of the challenge include the creation of the data repository of tissue microarrays; the creation of the clinical diagnosis classification data repository of thyroid cancer; and the definition of objective quantitative evaluation for comparison and ranking of the algorithms. With this benchmark, three automatic methods for predictions of the five clinical outcomes have been compared, and detailed quantitative evaluation results are presented in this paper. Based on the quantitative evaluation results, we believe automatic patient diagnosis is still a challenging and unsolved problem.   Availability and implementation The datasets and the evaluation software will be made available to the research community, further encouraging future developments in this field. (http://www-o.ntust.edu.tw/cvmi/ISBI2017/).   Contact cweiwang@mail.ntust.edu.tw.   Supplementary information Supplementary data are available at Bioinformatics online.		Ching-Wei Wang;Yu-Ching Lee;Evelyne Calista;Fan Zhou;Hongtu Zhu;Ryohei Suzuki;Daisuke Komura;Shumpei Ishikawa;Shih-Ping Cheng	2018	Bioinformatics	10.1093/bioinformatics/btx838	precision medicine;computer science;bioinformatics;thyroid cancer;tissue microarray	Comp.	31.74322628299259	-80.57177713404414	1511
40ae6812e9e1df5f490a36ece25f112a544f1d62	maximum entropy principle with imprecise side-conditions ii: crisp discrete solutions	fuzzy set;maximum entropy principle;probability density function;crisp solution;discrete probability distributions;probability distribution;fuzzy constraints;maximum entropy	In this paper we consider the maximum entropy principle with imprecise side-conditions, where the imprecise side-conditions are modeled as fuzzy sets. In a previous paper our solution produced fuzzy discrete probability distributions and fuzzy probability density functions. In this paper we consider only discrete probability distributions and we have the constraint that the solution must be crisp (non-fuzzy).	principle of maximum entropy	James J. Buckley	2006	Soft Comput.	10.1007/s00500-004-0454-8	probability distribution;mathematical optimization;probability mass function;mathematical analysis;imprecise probability;convolution of probability distributions;maximum entropy probability distribution;principle of maximum entropy;mathematics;maximum entropy thermodynamics;uniform distribution;kullback–leibler divergence;joint probability distribution;cross entropy;k-distribution;typical set;statistics	Logic	0.4783616878411103	-20.16717640799278	1512
687185deb4956bb2bc5c6337559c764ff5910988	myoelectric signs of fatigue and force failure during endured isometric contractions in elderly			isometric projection	Alessandro Rosponi;Francesco Felici;Ilenia Bazzucchi;Paola Sbriccoli;Luigi Fattorini;Gian Carlo Filligoi	2003	Int. J. Comp. Sci. Sport		isometric exercise;psychology;anesthesia	Robotics	11.344236161206366	-82.23058459975394	1515
f80582efb039253a4f92461bf096e509ec5a3bfe	a variational approach to edge detection	edge detection;variational approach	l’tlt: probIc:rn of dztc~ctin~ iri:,c:il::ity c!~arlges in images is canonical in \ isIon. lXi;e del,ectlou opc.1 :ltc,rs arc typ]Lally designed to optimally estimate first or second derival.ivr over some (usually small) support. Other criteria such as output signal to noise ratio or bandwidth have also been argued for. This paper describes an attempt to formulate a set of edge detection criteria that capture as directly as possible the desirable properties of the detector. Variational techniques are used to find 2 solution over the space of all possible functions. The first criterion is that the detector have low probability of error i.e. failing to mark edges or falsely marking non-edges. The second is that the marked points should be as close as possible to the centre of the true edge. The third criterion is that there should be low probability of more than one response to a single edge. The third criterion is claimed to be new, and it became necessary when an operator designed using the first two criteria was found to have excessive multiple responses. The edge model that will be considered here is 2 one-dlmensional step edge in white Gaussian noise although the same technique has been applied to an extended impulse or ridge profile. The result is a one dimensional operator that approximates the first derivative of a Gaussian. Its extension to two dimensions is also discussed.	bandwidth (signal processing);edge detection;failure;item unique identification;signal-to-noise ratio;variational principle	John F. Canny	1983			mathematical optimization;edge detection;computer science;statistics	Vision	53.3594497033523	-64.97415856318818	1516
105d8e97e7037d6af6bc2edf613cd4a3a7b37534	generalized crowding for genetic algorithms	bayesian network;markov chain analysis;premature convergence;probabilistic crowding;bepress selected works;simulated annealing;theoretical analysis;experiments;genetic algorithm;genetic algorithms;niching;deterministic crowding;genetic algorithms niching deterministic crowding probabilistic crowding markov chain analysis bayesian networks experiments;empirical evaluation;bayesian networks;markov chain	Crowding is a technique used in genetic algorithms to preserve diversity in the population and to prevent premature convergence to local optima. It consists of pairing each offspring with a similar individual in the current population (pairing phase) and deciding which of the two will remain in the population (replacement phase). The present work focuses on the replacement phase of crowding, which usually has been carried out by one of the following three approaches: Deterministic, Probabilistic, and Simulated Annealing. These approaches present some limitations regarding the way replacement is conducted. On the one hand, the first two apply the same selective pressure regardless of the problem being solved or the stage of the genetic algorithm. On the other hand, the third does not apply a uniform selective pressure over all the individuals in the population, which makes the control of selective pressure over the generations somewhat difficult. This work presents a Generalized Crowding approach that allows selective pressure to be controlled in a simple way in the replacement phase of crowding, thus overcoming limitations of the other approaches. Furthermore, the understanding of existing approaches is greatly improved, since both Deterministic and Probabilistic Crowding turn out to be special cases of Generalized Crowding. In addition, the temperature parameter used in Simulated Annealing is replaced by a parameter called scaling factor that controls the selective pressure applied. Theoretical analysis using Markov chains and empirical evaluation using Bayesian networks demonstrate the potential of this novel Generalized Crowding approach.	bayesian network;crowding;genetic algorithm;image scaling;local optimum;markov chain;premature convergence;simulated annealing	Severino F. Galán;Ole J. Mengshoel	2010		10.1145/1830483.1830620	mathematical optimization;genetic algorithm;computer science;artificial intelligence;machine learning;bayesian network	AI	28.482341566617485	-4.849777857603609	1517
7daa8885b4ef1f466059738e78d16bebbc54bfd7	producing interpretable local models in parametric cmac by regularization	tagaki-sugeno fuzzy model;cerebellar model articulation controller;regularized parametric;producing interpretable local model;linear local model;nonlinear system;parametric cmac;state estimation;unknown nonlinear process;cmac model;nonlinear process;system identification;regularization;cmac	Cerebellar model articulation controller (CMAC) has been widely applied to modeling and control due to its attractive features such as fast training speed and parsimonious structure. A parametric CMAC is a CMAC model with its constant weights replaced by linear functional weights or linear local models, i.e., a type of Tagaki-Sugeno fuzzy model. This paper proposes a regularized parametric CMAC, and investigates how its linear local models are able to approximate the local linearity of the nonlinear system to be modeled by using regularization techniques and how the regularized parametric CMAC can be successfully applied in modeling a nonlinear process for state estimation of unknown nonlinear processes. Experimental results on the approximation ability and interpretability of the regularized parametric CMAC and its application to nonlinear state estimation have been presented to demonstrate the advantages of the regularized parametric CMAC.	matrix regularization;one-key mac	John Q. Gan;Eric M. Rosales	2007	KES Journal		regularization;mathematical optimization;system identification;computer science;machine learning;cmac	ML	15.463550690545755	-28.881104148082525	1518
7cf00a9e71a57cdb15d51a40a37e54212d71a3a3	selecting the shortest itinerary in a cloud-based distributed mobility network		New Internet technologies can considerably enhance contem porary traffic control and management systems (TCMS). Such systems need to process increasing volumes of data available in clouds, and so new algorithms an d techniques for statistical data analysis are required. A very important problem f or cloud-based TCMS is the selection of the shortest itinerary, which requires rou te comparison on the basis of historical data and dynamic observations. In the paper we compare two nonoverlapping routes in a stochastic graph. The weights of the edges are considered to be independent random variables with unknown distribution s. Only historical samples of the weights are available, and some edges may have com mon samples. Our purpose is to estimate the probability that the weight of the first route is greater than that of the second one. We consider the resampling estimator of he probability in the case of small samples and compare it with the parametric p lug-in estimator. The analytical expressions for the expectations and variances of the proposed estimators are derived, which allow theoretical evaluation of the esti ma ors’ quality. The experimental results demonstrate that the resampling estima tor is a suitable alternative to the parametric plug-in estimator. This problem is very im portant for a vehicle decision-making procedure to choose route from the availab le lternatives.	algorithm;cloud computing;data mining;electrical connector;plug-in (computing);recommender system;resampling (statistics);tor messenger	Jelena Fiosina;Maksims Fiosins	2013		10.1007/978-3-319-00551-5_13	computer science;machine learning;statistics	ML	28.803578816084276	-19.740849093775132	1519
16b0124c7544072b79beb1d792469d50ad5a4d75	induced current magnetic resonance electrical impedance tomography with z-gradient coil	frequency 0 khz to 1 khz surface potential measurements electromagnetic induction z helmholtz configurations circular coil configurations finite element method fem magnetic flux density measurement mr phase imaging techniques surface electrodes electrical current electrical conductivity medical imaging method z gradient coil mreit magnetic resonance electrical impedance tomography;surface potential biomedical mri electric impedance imaging electrical conductivity finite element analysis magnetic flux;coils magnetic resonance imaging conductivity conductors tomography finite element analysis magnetic resonance	Magnetic Resonance Electrical Impedance Tomography (MREIT) is a medical imaging method that provides images of electrical conductivity at low frequencies (0-1 kHz). In MREIT, electrical current is applied to the body via surface electrodes and corresponding magnetic flux density is measured by means of Magnetic Resonance (MR) phase imaging techniques. By utilizing the magnetic flux density measurements and surface potential measurements images of true conductivity distribution can be reconstructed. In order to overcome difficulties regarding current application via surface electrodes, Induced Current MREIT (ICMREIT) have been proposed in the past. In ICMREIT, electrical currents and corresponding magnetic flux density are generated in the object through electromagnetic induction by means of externally placed coils driven with time varying currents. In this study, use of z-gradient, z-Helmholtz, and circular coil configurations in ICMREIT are proposed and investigated. Finite Element Method (FEM) is used to solve the forward problem of ICMREIT. Consequently, excitation performances and clinical applicability of different coil configurations are analyzed.	ampere;cardiography, impedance;characteristic impedance;coil device component;current source device component;electric conductivity;electrical current;excitation;finite element method;flux;gradient;imaging techniques;kilohertz;magnetic resonance imaging;medical imaging;nominal impedance;performance;scanning systems;simulation;electric impedance;electrode;tomography	Hasan H. Eroglu;B. Murat Eyuboglu	2014	2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/EMBC.2014.6943797	search coil;electronic engineering;electrical resistivity tomography;electromagnetic coil;nuclear magnetic resonance	Robotics	92.85821115276215	-23.196171587201604	1520
0f9bd0d528603654de2687d3ae2472a522607ee3	semantics-aware visual localization under challenging perceptual conditions		Visual place recognition under difficult perceptual conditions remains a challenging problem due to changing weather conditions, illumination and seasons. Long-term visual navigation approaches for robot localization should be robust to these dynamics of the environment. Existing methods typically leverage feature descriptions of whole images or image regions from Deep Convolutional Neural Networks. Some approaches also exploit sequential information to alleviate the problem of spatially inconsistent and non-perfect image matches. In this paper, we propose a novel approach for learning a discriminative holistic image representation which exploits the image content to create a dense and salient scene description. These salient descriptions are learnt over a variety of datasets under large perceptual changes. Such an approach enables us to precisely segment the regions of an image which are geometrically stable over large time lags. We combine features from these salient regions and an off-the-shelf holistic representation to form a more robust scene descriptor. We also introduce a semantically labeled dataset which captures extreme perceptual and structural scene dynamics over the course of 3 years. We evaluated our approach with extensive experiments on data collected over several kilometers in Freiburg and show that our learnt image representation outperforms off-the-shelf features from the deep networks and hand-crafted features.	boosting (machine learning);convolutional neural network;discriminative model;experiment;holism;machine vision;robotic mapping;visual descriptor	Tayyab Naseer;Gabriel L. Oliveira;Thomas Brox;Wolfram Burgard	2017	2017 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2017.7989305	visualization;convolutional neural network;discriminative model;robustness (computer science);feature extraction;perception;image segmentation;exploit;computer vision;computer science;pattern recognition;artificial intelligence	Robotics	31.20088635189365	-50.72126131774449	1521
6466d88fa7c5963bb30f832008c07903aebced0b	monocular 3d shape reconstruction using deep neural networks	shape three dimensional displays solid modeling training image segmentation estimation image reconstruction;traffic engineering computing image reconstruction image segmentation neural nets optimisation;low dimensional latent shape space monocular 3d shape reconstruction deep neural networks silhouette based 3d reconstruction viewpoint estimation optimization image segmentation object shape	This paper presents a novel approach to reconstructing the 3D shape of an object from a single image. The approach combines deep neural networks with a silhouette-based 3D reconstruction process. The optimal 3D shape is sought efficiently inside an extremely low-dimensional latent shape space, and the viewpoint and the object shape are jointly optimized based on the result of image segmentation. Evaluation of this approach shows a nearly 20 percent performance gain in viewpoint estimation subsequent to the optimization.	3d reconstruction;artificial neural network;autostereogram;deep learning;image segmentation;mathematical optimization	Qing Rao;Lars Krüger;Klaus C. J. Dietmayer	2016	2016 IEEE Intelligent Vehicles Symposium (IV)	10.1109/IVS.2016.7535403	active shape model;computer vision;geography;machine learning;segmentation-based object categorization;pattern recognition;image segmentation;scale-space segmentation	Vision	30.239831618417913	-49.46345003197047	1522
a5981a5ec7bb99b3f75bea728c1bd92b6cc483f5	analysis on the collaboration between global search and local search in memetic computation	analytical models;optimisation;theoretical model;quasi basin class;generic model;optimization technique;search space;memetics algorithm design and analysis search problems optimization collaboration analytical models computational modeling;computer model;collaboration;subthreshold seeker;memetics;subthreshold seeker global search local search memetic algorithms quasi basin class;memetic algorithm;global search;objective function;computational modeling;real world application;search problems optimisation;memetic algorithms;global optimization;subthreshold seeker global search local search memetic computation optimization exploration exploitation coordination exploration exploitation synergy quasibasin class memetic algorithm;optimization;search problems;local search;article;algorithm design and analysis;analytical model	The synergy between exploration and exploitation has been a prominent issue in optimization. The rise of memetic algorithms, a category of optimization techniques which feature the explicit exploration-exploitation coordination, much accentuates this issue. While memetic algorithms have achieved remarkable success in a wide range of real-world applications, the key to successful exploration-exploitation synergies still remains obscure as conclusions drawn from empirical results or theoretical derivations are usually quite algorithm specific and/or problem dependent. This paper aims to provide a theoretical model that can depict the collaboration between global search and local search in memetic computation on a broad class of objective functions. In the proposed model, the interaction between global search and local search creates a set of local search zones, in which the global optimal points reside, within the search space. Based on such a concept, the quasi-basin class (QBC) which categorizes problems according to the distribution of their local search zones is adopted. The subthreshold seeker, taken as a representative archetype of memetic algorithms, is analyzed on various QBCs to develop a general model for memetic algorithms. As the proposed model not only well describes the expected time for a simple memetic algorithm to find the optimal point on different QBCs but also consists with the observations made in previous studies in the literature, the proposed model may reveal important insights to the design of memetic algorithms in general.	algorithm design;average-case complexity;baseline (configuration management);computer simulation;evolutionary computation;exploit (computer security);genetic algorithm;local search (constraint satisfaction);local search (optimization);mathematical optimization;memetic algorithm;memetics;search algorithm;synergy;theory;web search engine	Jih-Yiing Lin;Ying-Ping Chen	2011	IEEE Transactions on Evolutionary Computation	10.1109/TEVC.2011.2150754	mathematical optimization;computer science;artificial intelligence;machine learning;mathematics;memetic algorithm;global optimization	AI	23.730269097745254	-6.786356389961159	1525
1e701d720c0c98b9071ff92bae51ead3072ed3ae	nfd technique for efficient and secured information hiding in low resolution images	baja resolucion;text;replacement;steganographie;remplacement;image resolution;fonction polynomiale;information hiding;low resolution;distributed computing;basse resolution;texte;steganography;esteganografia;internet;calculo repartido;reemplazo;funcion polinomial;polynomial function;texto;resolution image;calcul reparti	A new steganography algorithm that supports transmission of huge information with minimal embedding is proposed. This approach uses the Newton's Forward Difference (NFD) Technique for mapping the text file/s onto a low resolution image file. A polynomial function is derived to represent the mapping of the text bits with the bit positions on the host image file. This polynomial is represented as bits and is embedded in the image file that is transmitted. The bit replacements made in the host image file are negligible compared to those in the existing embedding techniques.		S. N. Sivanandam;C. K. Gokulnath;K. Prasanna;S. Rajeev	2004		10.1007/978-3-540-30555-2_53	image resolution;computer science;theoretical computer science;distributed computing;programming language;algorithm;statistics;computer graphics (images)	ML	44.280206145139665	-11.814186691755781	1529
7010eebae6dc0171a1e6125d9748ba457579f8ef	efficient and robust shape matching for model based human motion capture	shape matching;human motion	In this paper we present a robust and efficient shape matching approach for Marker-less Motion Capture. Extracted features such as contour, gradient orientations and the turning function of the shape are embedded in a 1-D string. We formulate shape matching as a Linear Assignment Problem and propose to use Dynamic Time Warping on the string representation of shapes to discard unlikely correspondences and thereby to reduce ambiguities and spurious local minima. Furthermore, the proposed cost matrix pruning results in robustness to scaling, rotation and topological changes and allows to greatly reduce the computational cost. We show that our approach can track fast human motions where standard articulated Iterative Closest Point algorithms fail.	algorithmic efficiency;assignment problem;computation;computational complexity theory;correspondence problem;dynamic time warping;embedded system;experiment;gradient descent;image scaling;iterative closest point;iterative method;maxima and minima;motion capture;string searching algorithm	Gerard Pons-Moll;Laura Leal-Taixé;Tri Truong;Bodo Rosenhahn	2011		10.1007/978-3-642-23123-0_42	computer vision;mathematical optimization;computer science;shape analysis;mathematics;geometry	Vision	49.174843831182756	-49.92269852392236	1531
39518d5e2a4132c9c24cc7f5c5eeed90b22619e8	image segmentation on gpgpus: a cellular automata-based approach	gpus;image segmentation;opencl;fuzzy connectedness;cellular automata	Image segmentation is one of the most difficult tasks in image processing and plays a critical role in the analysis of medical images used for diagnosis and treatment. With the decreased hardware costs and improvements in computing power of many-core architectures, there is an opportunity to both improve upon image segmentation algorithms and to make this technology more accessible. This paper describes our on-going research efforts to implement efficient image segmentation algorithms on graphical processing units (GPUs). A focused case study was performed with a suitable algorithm based on Cellular Automata, a parallel computational technique. Preliminary segmentation results are shown to validate our approach. Plans to improve the algorithm by making it more robust to noise and more efficient on GPU architectures are discussed. Our use of graph theoretic techniques and their implementation on GPUs will have broad application to other areas requiring computationally intensive calculations, as found in many problems involving modeling and simulation.	automata theory;cellular automaton;general-purpose computing on graphics processing units;image segmentation	Irving Olmedo;Yessika Guerra Perez;James Ford Johnson;Lakshman Raut;David H. K. Hoe	2013			cellular automaton;computer vision;simulation;computer science;theoretical computer science;machine learning;segmentation-based object categorization;image segmentation;algorithm	Vision	41.570411521007884	-30.733710481353395	1535
111f2dffc5b0f5a517d7d4317f9465c95840af59	detection of charcoal rot (macrophomina phaseolina) toxin effects in soybean (glycine max) seedlings using hyperspectral spectroscopy		Abstract Charcoal rot caused by the fungal pathogen Macrophomina phaseolina is an important disease of soybean and the use of resistant cultivars is recommended to manage the disease. Since symptoms, including leaf wilt, typically occur as soybeans reach maturity, screening varieties for tolerance to charcoal rot can be time-consuming, requiring nearly an entire growing season. In this study, soybean seedlings (V1) were exposed to a culture filtrate of M. phaseolina containing toxin(s) produced by the fungal pathogen. The effect on the seedlings was measured with hyperspectral spectroscopy on leaves. Two spectrometers integrated with a fiber optic light source and a 6.35 mm diameter probe yielding 480–850 nm and 900–2400 nm ranges after preprocessing were used. The spectra of the untreated plants measured at 0 h, 4 h, 8 h, 12 h, and 24 h post-exposure to the fungal extract were nearly indistinguishable. In contrast, the toxin-treated plants had significantly different spectra from the untreated plants at each of the 4 h, 8 h, 12 h, and 24 h measurements. Reflectance increased in the NIR (900–2400 nm) region with extended exposure to the fungal extract. This change was most noticeable in the 1450 nm and 1940 nm wavebands. Across the spectra, the 24 h reflectance was significantly higher than that of 12 h, which was significantly higher than those of 8 h, 4 h, and 0 h. Jeffries-Matusita (JM) distance, quantifying class separability, was used as a feature selection method and the 24 h measurement had the highest JM distance values, which indicated good separability. Based on JM Distance the most sensitive wavebands were in the regions of 1370–2400 nm. A ratio of the reflectance at 0 h to reflectance at the other times for each of the wavebands was calculated. The ratio curves had two noticeable troughs centered on 1450 nm and 1940 nm, with respective ratios of 0.47 and 0.32 for the 24 h measurement. The 1940 nm ratio at 24 h was proposed as a relative measure of charcoal rot susceptibility of soybean varieties. A ratio of 1.0 indicated no susceptibility with lower ratios indicating greater susceptibility to charcoal rot toxin(s). This study has implications in terms of developing tools to screen for soybean varieties tolerant to charcoal rot and potentially for other biotic or abiotic factors that induce foliar wilting.	link rot;software rot	Ameer H. Al-Ahmadi;Arjun Subedi;Guangxing Wang;Ruplal Choudhary;Ahmad Fakhoury;Dennis G. Watson	2018	Computers and Electronics in Agriculture	10.1016/j.compag.2018.04.013	artificial intelligence;computer vision;wilting;engineering;charcoal;botany;reflectivity;macrophomina phaseolina	HCI	85.80864833489775	-54.32136650287058	1542
33314b6a2cbf11835ed4b0083ea0743609cd82c3	a driving behaviour model of electrical wheelchair users		In spite of the presence of powered wheelchairs, some of the users still experience steering challenges and manoeuvring difficulties that limit their capacity of navigating effectively. For such users, steering support and assistive systems may be very necessary. To appreciate the assistance, there is need that the assistive control is adaptable to the user's steering behaviour. This paper contributes to wheelchair steering improvement by modelling the steering behaviour of powered wheelchair users, for integration into the control system. More precisely, the modelling is based on the improved Directed Potential Field (DPF) method for trajectory planning. The method has facilitated the formulation of a simple behaviour model that is also linear in parameters. To obtain the steering data for parameter identification, seven individuals participated in driving the wheelchair in different virtual worlds on the augmented platform. The obtained data facilitated the estimation of user parameters, using the ordinary least square method, with satisfactory regression analysis results.	acclimatization;aging;assumed;cdisc adas-cog - commands summary score;control system;digital photo frame;intention - mental process;least-squares analysis;observable;ordinary least squares;population parameter;power (psychology);real-time web;regression analysis;user interface device component;virtual world;interest;motorized wheelchair	Stevine Obura Onyango;Yskandar Hamam;Karim Djouani;Boubaker Daachi;Nico Steyn	2016	Computational intelligence and neuroscience	10.1155/2016/7189267	simulation	Robotics	70.25063112541699	-27.28345741318917	1546
5020c26665af9e39839203d8de0a862660eed99e	robust visual tracking via mcmc-based particle filtering	histograms;hog;joints;visualization;target tracking visualization histograms joints robustness covariance matrix proposals;different real datasets mcmc based particle filtering visual tracking framework mcmc based particle algorithm informative likelihood color based observation model detection confidence density histograms of oriented gradients hog descriptor posterior distribution estimation tracking problem global system;image colour analysis;particle filtering;robustness;mcmc;target tracking;particle filtering numerical methods image colour analysis;visual tracking;proposals;hog visual tracking particle filtering mcmc;particle filtering numerical methods;covariance matrix	We present in this paper a new visual tracking framework based on the MCMC-based particle algorithm. Firstly, in order to obtain a more informative likelihood, we propose to combine the color-based observation model with a detection confidence density obtained from the Histograms of Oriented Gradients (HOG) descriptor. The MCMC-based particle algorithm is then employed to estimate the posterior distribution of the target state to solve the tracking problem. The global system has been tested on different real datasets. Experimental results demonstrate the robustness of the proposed system in several difficult scenarios.	algorithm;image gradient;information;markov chain monte carlo;particle filter;video tracking	Dung Nghi Truong Cong;François Septier;Christelle Garnier;Louahdi Khoudour;Yves Delignon	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6288173	computer vision;econometrics;covariance matrix;visualization;particle filter;markov chain monte carlo;eye tracking;computer science;histogram;mathematics;statistics;robustness	Robotics	45.17823630230094	-48.7900535990017	1564
6399b66fa2b88874ea81740572365dd7e2e10c64	a survey on perception methods for human-robot interaction in social robots		For human–robot interaction (HRI), perception is one of the most important capabilities. This paper reviews several widely used perception methods of HRI in social robots. Specifically, we investigate general perception tasks crucial for HRI, such as where the objects are located in the rooms, what objects are in the scene, and how they interact with humans. We first enumerate representative social robots and summarize the most three important perception methods from these robots: feature extraction, dimensionality reduction, and semantic understanding. For feature extraction, four widely used signals including visual-based, audio-based, tactile-based and rang sensors-based are reviewed, and they are compared based on their advantages and disadvantages. For dimensionality reduction, representative methods including principle component analysis (PCA), linear discriminant analysis (LDA), and locality preserving projections (LPP) are reviewed. For semantic understanding, conventional techniques for several typical applications such as object recognition, object tracking, object segmentation, and speaker localization are discussed, and their characteristics and limitations are also analyzed. Moreover, several popular data sets used in social robotics and published semantic understanding results are analyzed and compared in light of our analysis of HRI perception methods. Lastly, we suggest important future work to analyze fundamental questions on perception methods in HRI.	human–robot interaction;social robot	Haibin Yan;Marcelo H. Ang;Aun Neow Poo	2014	I. J. Social Robotics	10.1007/s12369-013-0199-6	computer vision;artificial intelligence	Robotics	24.64353893034081	-58.69362627612909	1566
90d8a1165e8d053cba0224681c8d27668cac33a6	convergence of population dynamics in symmetric routing games with a finite number of playersz	learning algorithm;convergence;evolutionary computation;game theory;telecommunication traffic evolutionary computation game theory learning artificial intelligence telecommunication congestion control telecommunication network routing;nash equilibrium;traffic assignment;routing;probability density function;evolutionary based learning algorithm population dynamics symmetric routing games traffic assignment problem congestion games equivalent global optimization problem;telecommunication congestion control;data mining;equivalent global optimization problem;telecommunication traffic;telecommunication network routing;population dynamic;games;population dynamics;artificial intelligence;global optimization;learning artificial intelligence;evolutionary based learning algorithm;symmetric routing games;congestion game;convergence routing costs read only memory topology communication networks context round robin roads telecommunication traffic;congestion games;traffic assignment problem	Routing games, as introduced in the pioneering work of Orda, Rom and Shimkin (1993), are very closely related to the traffic assignment problems as already studied by Wardrop and to congestion games, as introduced by Rosenthal. But they exhibit more complex behavior: often the equilibrium is not unique, and computation of equilibria is typically harder. They cannot be transformed in general into an equivalent global optimization problem as is the case with congestion games and in the traffic assignment problem which possess a potential under fairly general conditions. In this paper we study convergence of various learning schemes to an equilibrium in the problem of routing games. We are able to considerably extend previous published results [1] that were restricted to routing into two parallel links. We study evolutionary-based learning algorithms and establish their convergence for general topologies.	algorithm;assignment problem;computation;global optimization;machine learning;mathematical optimization;network congestion;optimization problem;population dynamics;read-only memory;routing	Eitan Altman;Vijay Kamble;Vivek S. Borkar	2009	2009 International Conference on Game Theory for Networks	10.1109/GAMENETS.2009.5137459	mathematical optimization;simulation;computer science;distributed computing	ECom	-3.4250218863579565	3.3393561531098954	1571
2f7c06027748bff5828a27af419da570360d0387	segmenting 3d surface scan data of the human body by 2d projection	whole body;proyeccion;two dimensional shape;forme bidimensionnelle;representation tridimensionnelle;forma;cuerpo;body;three dimensional shape;corps entier;hombre;exploracion;segmentation;forma tridimensional;projective plane;forma bidimensional;algorithme;algorithm;plano proyectivo;shape;forme tridimensionnelle;projection;human body;espacio proyectivo;human;plan projectif;balayage;corps;projective space;superficie;surface;three dimensional representation;scanning;forme;espace projectif;segmentacion;representacion tridimensional;cuerpo entero;homme;algoritmo	This paper presents a segmentation algorithm for 3-D whole body surface scan data. The algorithm is based on 2-D projection of 3-D data and has achieved good results in a number of limited surface shapes. The method has been successfully employed to extract the torso, arm, and leg segments of the human body.		Peng Li;Brian D. Corner;Steven Paquette	2000		10.1117/12.380038	topology;mathematics;geometry;cartography	HCI	49.567304657553386	-59.30136522784489	1579
173606431c904583b55bcbabb165be0368ab259a	the emerging jpeg-2000 security (jpsec) standard	watermarking;image coding;service provider;iso iec jpeg 2000 security standard;data compression;iso;information security;e commerce;iso standards;code standards;iec standards;jpeg 2000 image coding digital imagery security iso iec jpeg 2000 security standard;streaming media;security of data data compression iec standards image coding iso standards;cryptography;digital imagery security;draft international standard;digital image;lts1;digital images;jpeg 2000 image coding;security of data;digital images cryptography data security iec standards iso standards code standards streaming media watermarking image coding information security;internal standard;data security	The emergence of digital imaging applications is accelerating the need for security of digital imagery. The emerging international standard ISO/IEC JPEG-2000 security (JPSEC) is designed to provide security for digital imagery, and in particular digital imagery coded with the JPEG-2000 image coding standard. This paper provides an overview of the JPSEC standard, including a description of its basic architecture and examples of its use	digital image;digital imaging;emergence;jpeg 2000;secure digital	John G. Apostolopoulos;Susie Wee;Frédéric Dufaux;Touradj Ebrahimi;Qibin Sun;Zhishou Zhang	2006	2006 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2006.1693476	computer science;information security;internet privacy;world wide web;computer security;digital image	Arch	38.54196305040027	-14.795634754426727	1582
1194a762d9ce2440b1f5fdc4f75a45cb46bde5e6	entropy-based representation of image information	feature encoding;scale space;color images;pattern recognition;entropy;color image;active vision	Loss of information in images undergoing fine-to-coarse transformations is analysed by using an approach based on the theory of irreversible processes. In the case of grey level images, entropy variation along scales is used to characterize basic, lowlevel information and to identify perceptual components of the image, such as shape and texture. Here an extension of the approach to colour images is proposed. Spatiochromatic information is defined, which depends on cross-interactions between the different colour channels. Examples illustrating the use of spatio-chromatic information are presented, related to pattern recognition and active vision.	active vision;grayscale;interaction;pattern recognition	Mario Ferraro;Giuseppe Boccignone;Terry Caelli	2002	Pattern Recognition Letters	10.1016/S0167-8655(02)00099-5	computer vision;entropy;scale space;active vision;color image;computer science;artificial intelligence;pattern recognition;mathematics	Vision	43.45130834279727	-63.37998900465125	1586
121c9d45c8c900f9113d845e20ccd347a45224db	prediction-based vision for robot control	forecasting;structure methods;robot sensing systems;control systems;image recognition;sensor systems;processing;nist;general and miscellaneous mathematics computing and information science;image processing;simulation 990210 supercomputers 1987 1989;path planning;data processing;timing properties;computerized control systems;operation;orbital robotics;artificial intelligent;accuracy;control system;robot control;feedback loop;work;environment;solid modeling;robots;computerized simulation;pattern recognition;artificial intelligence;robot control robot sensing systems path planning control systems sensor systems nist solid modeling orbital robotics feedback loop image recognition;process simulation;vision;images;knowledge base	46 T he domain of robot sensing has much more structure than that of general sensing. A robot sensing system must operate within time and accuracy limits usually mandated by the application. It commonly does this by precomputing as much information as possible about the robot's environment and the objects in it, and storing this information as a model of the world. In most cases, this knowledge corresponds to statistical or structural methods of identifying objects in images, but is encoded in such a way as to be useful only for recognition or object location.' As robot tasks become more complicated, this approach becomes less viable. A more general approach to modeling is required when task and path planning must be effected at runtime rather than fixed beforehand. And when unknown objects must be handled or when the environment becomes too complicated, the simple methods break down. As more flexibility is required of the system, more generality is required of the models. Fortunately, in industrial robotics environments a good source of models is usually available-the computer-aided design files used to define the objects. A designer can take advantage of this geometrically complete information to substantially enhance the capabilities of a robot sensing system. In addition to modeling the geometry of objects, a sensing system must usually model the environment surrounding the objects and account for the many instances of each object that might appear in the world and the changes that might be made to each of them. This makes representation and modeling substantially more than	computer-aided design;industrial robot;motion planning;precomputation;robot control;robotics;run time (program lifecycle phase)	Michael Shneier;Ronald Lumia;Martin Herman	1987	Computer	10.1109/MC.1987.1663660	operation;vision;computer vision;simulation;nist;forecasting;computer science;control system;artificial intelligence;processing;feedback loop;robot control;solid modeling;work	Robotics	52.06790200663678	-30.888514329475864	1587
09da8d56351dff51389d764cbc94bc733ee3129c	adversarial adaptation of synthetic or stale data		Two types of data shift common in practice are 1. transferring from synthetic data to live user data (a deployment shift), and 2. transferring from stale data to current data (a temporal shift). Both cause a distribution mismatch between training and evaluation, leading to a model that overfits the flawed training data and performs poorly on the test data. We propose a solution to this mismatch problem by framing it as domain adaptation, treating the flawed training dataset as a source domain and the evaluation dataset as a target domain. To this end, we use and build on several recent advances in neural domain adaptation such as adversarial training (Ganin et al., 2016) and domain separation network (Bousmalis et al., 2016), proposing a new effective adversarial training scheme. In both supervised and unsupervised adaptation scenarios, our approach yields clear improvement over strong baselines.	domain adaptation;framing (world wide web);overfitting;randomized algorithm;software deployment;supervised learning;synthetic data;synthetic intelligence;test data;unsupervised learning	Young-Bum Kim;Karl Stratos;Dongchan Kim	2017		10.18653/v1/P17-1119	machine learning;natural language processing;artificial intelligence;adversarial system;computer science	NLP	19.852372395547917	-49.288610548819236	1599
79e37cce06e4715eb1c74b5a4b47b5210a560ddf	using camera motion to identify types of american football plays	image motion analysis;image segmentation;supervised learning;information extraction;video signal processing;american football plays;camera motion parameters;cameras games data mining system testing machine assisted indexing image analysis tracking image segmentation robustness supervised learning;data mining;camera motion;motion information;video cameras;football;games;image segmentation camera motion parameters american football plays motion information;video signal processing image motion analysis video cameras sport image segmentation;system testing;robustness;image analysis;sport;classification accuracy;machine assisted indexing;cameras;tracking	This paper presents a method that uses camera motion parameters to recognise 7 types of American Football plays. The approach is based on the motion information extracted from the video and it can identify short and long pass plays, short and long running plays, quaterback sacks, punt plays and kickoff plays. This method has the advantage that it is fast and it does not require player or hall tracking. The system was trained and tested using 782 plays and the results show that the system has an overall classification accuracy of 68%.		Mihai Lazarescu;Svetha Venkatesh	2003		10.1109/ICME.2003.1221583	computer vision;image analysis;simulation;computer science;sport;machine learning;tracking;multimedia;image segmentation;supervised learning;system testing;information extraction;robustness	HCI	39.512788815219906	-47.57936261008613	1605
2bfe2bbe7c96e695c16360570bfdd9f1a1fd2970	rpc-based orthorectification for satellite images using fpga	field-programmable gate array (fpga);orthorectification;rational polynomial coefficient (rpc)	Conventional rational polynomial coefficients (RPC)-based orthorectification methods are unable to satisfy the demands of timely responses to terrorist attacks and disaster rescue. To accelerate the orthorectification processing speed, we propose an on-board orthorectification method, i.e., a field-programmable gate array (FPGA)-based fixed-point (FP)-RPC orthorectification method. The proposed RPC algorithm is first modified using fixed-point arithmetic. Then, the FP-RPC algorithm is implemented using an FPGA chip. The proposed method is divided into three main modules: a reading parameters module, a coordinate transformation module, and an interpolation module. Two datasets are applied to validate the processing speed and accuracy that are achievable. Compared to the RPC method implemented using Matlab on a personal computer, the throughputs from the proposed method and the Matlab-based RPC method are 675.67 Mpixels/s and 61,070.24 pixels/s, respectively. This means that the proposed method is approximately 11,000 times faster than the Matlab-based RPC method to process the same satellite images. Moreover, the root-mean-square errors (RMSEs) of the row coordinate (ΔI), column coordinate (ΔJ), and the distance ΔS are 0.35 pixels, 0.30 pixels, and 0.46 pixels, respectively, for the first study area; and, for the second study area, they are 0.27 pixels, 0.36 pixels, and 0.44 pixels, respectively, which satisfies the correction accuracy requirements in practice.	algorithm;coefficient;field-programmability;field-programmable gate array;fixed-point arithmetic;interpolation imputation technique;matlab;on-board data handling;orthophoto;personal computer;pixel;plant roots;polynomial;remote procedure call;requirement	Rongting Zhang;Guoqing Zhou;Guangyun Zhang;Xiang Zhou;Jingjin Huang	2018		10.3390/s18082511	electronic engineering;field-programmable gate array;computer vision;satellite;engineering;orthophoto;artificial intelligence	Robotics	58.66495332972776	-36.82638381451364	1607
96d0362b8665d6a23c9989e57fdf0943c2ca3e85	three-dimensional topographic mapping with aster stereo data in rugged topography	contraste;modele numerique elevation;photogrammetry;correlacion;teledetection spatiale;errors;wave reflection;interpolation;erreur;aster;topography earth;emission thermique;relief;space remote sensing;canadian rocky mountains;north america;canada ouest;america del norte;topographie;amerique du nord;snow;rocky mountains;pouvoir reflecteur;pente versant;image matching;lago;lakes;reflexion onde;poder reflector;modele numerique terrain;reflexion onda;stereoscopy;western canada;correction;emision termica;cartographie;digital elevation model;indexing terms;topography;topographic map;three dimensional;digital terrain models;neige;ground control point;corrections;error analysis;radiometry;canada;nube;accuracy;macizo montanoso;teledeteccion espacial;modelo;cartografia;precision;canada oeste;reflectance;propagacion;error propagation;stereo bundle adjustment geophysical measurement technique land surface terrain mapping satellite remote sensing three dimensional topography aster stereo method rugged terrain mountainous area digital elevation model dem stereo pair infrared radiometry high relief canadian rocky mountains;clouds;interpolation method;pixel;rocheuses canadiennes;photogrammetry geophysical techniques terrain mapping topography earth;high relief areas;correccion;surfaces lakes digital elevation models reflection radiometry error correction calibration image matching clouds snow;cartography;slope angle;digital elevation models;etalonnage;modele;charged couple device;terrain mapping;radiometrie;error;dem;lac;correlation;nuage;mountains;models;calibration;thermal emission;nieve;topografia;propagation;geophysical techniques;bundle adjustment;massif montagneux	Digital elevatio models (DEMs) were generated from a stereo pair using the Advanced Spaceborne Thermal Emission and Reflection Radiomete (ASTER) backward and nadir images with 27.7 intersection angles ( = 0 6) over a high relief area of the Canadian Rocky Mountains. Fifteen ground control points were a good compromise to compute the stereo-bundle adjustment when they are only 25–30 m precise to avoid their error propagation in the modeling. It enabled to keep accuracy on the order of one pixel (15 m). DEM accuracy was then evaluated along the full process and as a function of different parameters. Applying the calibration of charge-coupled devices in order to reduce the striping effects improves the DEM accuracy by a factor of 10%. The major problems in the image matching were the clouds, snow, lakes, and occluded/shadowed areas, which generated mismatched areas and “artificial” relief in the lakes (1000–2000 m) during the correlation process and the automatic interpolation method. Postprocessing the DEM with semiautomatic three-dimensional tools improved its accuracy by a factor of 10%. The final results (LE68 and LE90 of 28 and 51 m, respectively) were obtained with the interactive correction of lake elevations, but without taking into account the large mismatched areas, which were specific to this challenging mountainous study site. The final results demonstrated that the DEM was almost linearly correlated with the terrain slopes, and 20-m LE68 can be obtained on a medium topography. The bidirectional reflectance distribution function and local topography effects should then be corrected on the ASTER stereo images to improve DEM extraction in an iterative processing method.	advanced spaceborne thermal emission and reflection radiometer;bidirectional reflectance distribution function;bundle adjustment;charge-coupled device;data striping;decade (log scale);digital elevation model;image registration;interpolation;iterative method;pixel;propagation of uncertainty;rugged computer;software propagation;topography	Thierry Toutin	2002	IEEE Trans. Geoscience and Remote Sensing	10.1109/TGRS.2002.802878	meteorology;digital elevation model;topography;mathematics;optics;physics;statistics;remote sensing	Vision	77.4851540392469	-63.2372786455791	1616
df871ed235ffb77a9b60c791508e2b545d84aab4	new iterative cone beam ct reconstruction software: parameter optimisation and convergence study	computerized axial tomography;tomodensitometria;genie biomedical;cbct;convergence rate;iterative algorithm;down sampled data;cone beam;data analysis;tomodensitometrie;biomedical engineering;cone beam computed tomography;edge recovery;image reconstruction;image quality;ingenieria biomedica;reconstruction algorithm;iteration method;article	Cone beam computed tomography (CBCT) provides a volumetric image reconstruction from tomographic projection data. Image quality is the main concern for reconstruction in comparison to conventional CT. The reconstruction algorithm used is clearly important and should be carefully designed, developed and investigated before it can be applied clinically. The Multi-Instrument Data Analysis System (MIDAS) tomography software originally designed for geophysical applications has been modified to CBCT image reconstruction. In CBCT reconstruction algorithms, iterative methods offer the potential to generate high quality images and would be an advantage especially for down-sampling projection data. In this paper, studies of the CBCT iterative algorithms implemented in MIDAS are presented. Stability, convergence rate, quality of reconstructed image and edge recovery are suggested as the main criteria for monitoring reconstructive performance. Accordingly, the selection of relaxation parameter and number of iterations are studied in detail. Results are presented, where images are reconstructed from full and down-sampled cone beam CT projection data using iterative algorithms. Various iterative algorithms have been implemented and the best selection of the iteration number and relaxation parameters are investigated for ART. Optimal parameters are chosen where the errors in projected data as well as image errors are minimal.	algorithm;ct scan;cone beam computed tomography;display resolution;image quality;iteration;iterative method;iterative reconstruction;lagrangian relaxation;linear programming relaxation;mathematical optimization;population parameter;projections and predictions;rate of convergence;sampling (signal processing);sampling - surgical action;vergence;volumetric display	W. Qiu;J. R. Tong;C. N. Mitchell;Tom Marchant;Paulette Spencer;Christopher J. Moore;M. Soleimani	2010	Computer methods and programs in biomedicine	10.1016/j.cmpb.2010.03.012	iterative reconstruction;computer vision;mathematical optimization;mathematics;iterative method;medical physics	Vision	51.993537981190414	-79.69344668010379	1617
5a582488a2c288b6f9371079b82939375ffa9606	pmx: automated protein structure and topology generation for alchemical perturbations	software;free energy calculations;molecular dynamics;protein stability;molecular dynamics simulation;thermostability;proteins;protein conformation;mutations;gene library;thermodynamics;amino acids;computational biology;alchemy;mutation;automation	Computational protein design requires methods to accurately estimate free energy changes in protein stability or binding upon an amino acid mutation. From the different approaches available, molecular dynamics-based alchemical free energy calculations are unique in their accuracy and solid theoretical basis. The challenge in using these methods lies in the need to generate hybrid structures and topologies representing two physical states of a system. A custom made hybrid topology may prove useful for a particular mutation of interest, however, a high throughput mutation analysis calls for a more general approach. In this work, we present an automated procedure to generate hybrid structures and topologies for the amino acid mutations in all commonly used force fields. The described software is compatible with the Gromacs simulation package. The mutation libraries are readily supported for five force fields, namely Amber99SB, Amber99SB*-ILDN, OPLS-AA/L, Charmm22*, and Charmm36.	amino acids;anatomy, regional;charmm;computation;force field (chemistry);gromacs;libraries;molecular dynamics;network topology;opls;pmx (technology);protein, organized by structure;simulation;throughput;free energy;mutation analysis	Vytautas Gapsys;Servaas Michielssens;Daniel Seeliger;Bert L. de Groot	2015		10.1002/jcc.23804	mutation;crystallography;protein structure;molecular dynamics;chemistry;alchemy;genomic library;automation;computational chemistry;physics	Comp.	11.81943250453172	-60.315937643841345	1621
63cfd84ecff1ef9727a003f2cf2979e0723604af	magnetohydrodynamic flows in a hairpin duct under a magnetic field applied perpendicular to the plane of flow	hairpin duct;magnetohydrodynamics mhd;liquid metal;numerical simulation	This numerical study examines three-dimensional liquid–metal magnetohydrodynamic flows in a hairpin-shaped electrically-conducting duct with a square cross-section under a uniform magnetic field applied perpendicular to the flow plane. Predicted is detailed information on the fluid velocity, pressure, current, and electric potential in the magnetohydrodynamic duct flows. Higher velocities are observed in the side layers in the inflow and outflow channels, yielding ‘M-shaped’ velocity profiles. More specifically, in the present study the axial velocity in the side layer near the partitioning wall is higher than that near the outer walls because of the current features therein. In the transition segment, a large velocity recirculation is observed at the entrance of the outflow channel (above the end of partitioning wall) caused by the flow separation, yielding complicated distributions of the electric potential and current therein. Cases with different Hartmann numbers are examined. The non-dimensional pressure gradient is smaller in case of higher Hartmann number, while the pressure in each case almost linearly decreases along the main flow direction, except in the transition segment. In the present study, the electromagnetic characteristics of the liquid metal flows are examined in terms of the electro-motive and electric-field components of the current with an aim to analyze the interdependency of the flow variables.		Xuejiao Xiao;Chang Nyung Kim	2014	Applied Mathematics and Computation	10.1016/j.amc.2014.04.049	computer simulation;classical mechanics;liquidmetal;optics;physics	Theory	87.78041219461214	-10.504262846840787	1622
84a74ef8680b66e6dccbc69ae80321a52780a68e	facial expression recognition	optical flow;hopfield network;facial expression;frequency analysis;image processing;feature vector;neural network;feature extraction;learning artificial intelligence	Facial expressions are the facial changes in response to a person’s internal emotional states, intentions, or social communications. Facial expression analysis has been an active research topic for behavioral scientists since the work of Darwin in 1872 [21, 26, 29, 83]. Suwa et al. [90] presented an early attempt to automatically analyze facial expressions by tracking the motion of 20 identified spots on an image sequence in 1978. After that, much progress has been made to build computer systems to help us understand and use this natural form of human communication [5, 7, 8, 17, 23, 32, 43, 45, 57, 64, 77, 92, 95, 106–108, 110]. In this chapter, facial expression analysis refers to computer systems that attempt to automatically analyze and recognize facial motions and facial feature changes from visual information. Sometimes the facial expression analysis has been confused with emotion analysis in the computer vision domain. For emotion analysis, higher level knowledge is required. For example, although facial expressions can convey emotion, they can also express intention, cognitive processes, physical effort, or other intraor interpersonal meanings. Interpretation is aided by context, body gesture, voice, individual differences, and cultural factors as well as by facial configuration and timing [11, 79, 80]. Computer facial expression analysis systems need to analyze the facial actions regardless of context, culture, gender, and so on.	computer vision;darwin;facial recognition system;regular expression	Ying-li Tian;Takeo Kanade;Jeffrey F. Cohn	2011		10.1007/978-0-85729-932-1_19	psychology;computer vision;facial action coding system;communication;social psychology;face hallucination	Graphics	-2.17145598549332	-75.80244198827407	1624
18d6a245e76ebb24b4ff26534f2597763060d60c	compressive sensing based classification in the presence of intra-and inter-signal correlation		In this letter, we investigate the problem of classification with high-dimensional data using low-dimensional random projections in the presence of inter- and intra-signal correlations. Each sensor is assumed to compress its high-dimensional (Gaussian) signal vector using random projections in a multisensor setting. In order to quantify the classification performance with compressed data, we consider the Bhattacharya distance as the performance metric. In the presence of intra-signal correlation at a given sensor, the degradation in the Bhattacharya distance with compressed data is shown to be nonlinear with the compression ratio in contrast to the case when there is no intra-signal correlation. In the presence of inter-signal correlation, the degradation in the Bhattacharya distance with compressed data depends on whether or not an identical projection matrix is used to compress data at multiple sensors.	compressed sensing;data compression;elegant degradation;nonlinear system;random projection;sensor	Thakshila Wimalajeewa;Pramod K. Varshney	2018	IEEE Signal Processing Letters	10.1109/LSP.2018.2860254	pattern recognition;fold (higher-order function);artificial intelligence;mathematics;compressed sensing;nonlinear system;correlation;projection (linear algebra);signal reconstruction;principal component analysis;bhattacharyya distance	Metrics	31.145814953941677	-40.38154878156747	1629
63fab21aea89f441028ba075e457057f81b6d5e1	improved detection of human respiration using data fusion basedon a multistatic uwb radar	uwb radar;kalman filter;data fusion;multistatic;respiration detection	This paper investigated the feasibility for improved detection of human respiration using data fusion based on a multistatic ultra-wideband (UWB) radar. UWB-radar-based respiration detection is an emerging technology that has great promise in practice. It can be applied to remotely sense the presence of a human target for through-wall surveillance, post-earthquake search and rescue, etc. In these applications, a human target’s position and posture are not known a priori. Uncertainty of the two factors results in a body orientation issue of UWB radar, namely the human target’s thorax is not always facing the radar. Thus, the radial component of the thorax motion due to respiration decreases and the respiratory motion response contained in UWB radar echoes is too weak to be detected. To cope with the issue, this paper used multisensory information provided by the multistatic UWB radar, which took the form of impulse radios and comprised one transmitting and four separated receiving antennas. An adaptive Kalman filtering algorithm was then designed to fuse the UWB echo data from all the receiving channels to detect the respiratory-motion response contained in those data. In the experiment, a volunteer’s respiration was correctly detected when he curled upon a camp bed behind a brick wall. Under the same scenario, the volunteer’s respiration was detected based on the radar’s single transmitting-receiving channels without data fusion using conventional algorithm, such as adaptive line enhancer and single-channel Kalman filtering. Moreover, performance of the data fusion algorithm was experimentally investigated with different channel combinations and antenna deployments. The experimental results show that the body orientation issue for human respiration detection via UWB radar can be dealt well with the multistatic UWB radar and the Kalman-filter-based data fusion, which can be applied to improve performance of UWB radar in real applications.	algorithm;experiment;kalman filter;poor posture;radar;radial (radio);transmitter;ultra-wideband	Hao Lv;Fugui Qi;Yonghui Zhang;Teng Jiao;Fulai Liang;Zhao Li;Jianqi Wang	2016	Remote Sensing	10.3390/rs8090773	kalman filter;man-portable radar;continuous-wave radar;radar lock-on;telecommunications;fire-control radar;passive radar;bistatic radar;machine learning;sensor fusion;remote sensing	Robotics	85.32100319098787	-41.83786359663119	1630
08fe9bb0abd063cd2f9d3f675f323a34030c6802	convolutional neural network simplification based on feature maps selection	structure simplification;convolutional neural network;separability analysis;feature maps selection	We present a feature maps selection method for convolutional neural network (CNN) which can keep the classifier performance when CNN is used as a feature extractor. This method aims to simplify the last subsampling layer of CNN by cutting the number of feature maps with Linear Discriminant Analysis (LDA). It is shown that our method can stabilize the classification accuracy and achieve runtime reduction by removing some feature maps of the last subsampling layer which have worst separability. And the result also lay the foundation for further simplification of CNN.	artificial neural network;chroma subsampling;convolutional neural network;level of detail;linear discriminant analysis;linear separability;map;randomness extractor;text simplification	Ting Rui;Junhua Zou;You Zhou;Jianchao Fei;Chengsong Yang	2016	2016 IEEE 22nd International Conference on Parallel and Distributed Systems (ICPADS)	10.1109/ICPADS.2016.0161	computer science;artificial intelligence;machine learning;pattern recognition;convolutional neural network	Robotics	24.198825964513468	-51.86594553716559	1634
dfd05fa97c6575df4ba37e13d500570eb894b442	parallel layout construction algorithm for irregular shape packing problems	no fit polygon placement problems optimization collision free region;simulated annealing bin packing boolean functions;parallel layout construction algorithm serial algorithm speed improvement bin packing problems simulated annealing algorithm parallel version polygons boolean operations collision free region new layout protrudes item geometry shipbuilding industries wood industries garmen industries cutting problems irregular shape packing problems	Cutting and packing problems arise in a variety of industries, including garment, wood and shipbuilding. Irregular shape packing is a special case which admits irregular items and is much more complex due to the geometry of items. In order to ensure that items do not overlap and no item from the layout protrudes from the container, the collision free region concept was adopted. It represents all possible translations for a new item to be inserted into a container with already placed items. To construct a feasible layout, collision free region for each item is determined through a sequence of Boolean operations over polygons. In order to improve the speed of the algorithm, a parallel version of the layout construction was proposed and it was applied to a simulated annealing algorithm used to solve bin packing problems. Tests were performed in order to determine the speed improvement of the parallel version over the serial algorithm.	bin packing problem;boolean algebra;boolean operations on polygons;run time (program lifecycle phase);sequential algorithm;set packing;simulated annealing	André Kubagawa Sato;Thiago de Castro Martins;Marcos de Sales Guerra Tsuzuki	2012	The 6th International Conference on Soft Computing and Intelligent Systems, and The 13th International Symposium on Advanced Intelligence Systems	10.1109/SCIS-ISIS.2012.6505041	mathematical optimization;mathematics	Robotics	19.455120842011905	3.565979418030874	1636
0f32ee7322f019f76693b4c46ec38662aa6375cd	matching of quasi-periodic time series patterns by exchange of block-sorting signatures	evaluation performance;performance evaluation;execution time;point to point;implementation;evaluacion prestacion;database;signal periodique;base dato;novelty detection;time series;electrocardiographie;algorithme;algorithm;accuracy;electrocardiography;electrocardiografia;precision;pattern matching;identification;serie temporelle;base de donnees;serie temporal;pattern recognition;dtw;temps execution;identificacion;periodic signal;concordance forme;reconnaissance forme;reconocimiento patron;tiempo ejecucion;implementacion;detection de rupture;block sorting signature;senal periodica;shape exchange;electrocardiogram;algoritmo	We propose a novel method for quasi-periodic time series patterns matching, through signature exchange between the two patterns. The signature is obtained through sorting of the time series on magnitude. The advantage is that the difficult task of comparing the two patterns can be easily performed as a result of this exchange: The original time series is compared (point to point matching) to the reconstructed time series obtained through the reverse process, using the other time series signature. The matching is such that periods in one time series are put into correspondence with periods in the other time series, even if the time series is of different basic patterns and/or different lengths. The method is simple to implement and requires no parameters. It was compared to the very appreciated DTW algorithm on execution time, space and accuracy. Due to the quasi-periodic nature of the electrocardiogram, the tests were performed on ECG traces, selected from the Massachusetts Institute of Technology – Beth Israel Hospital (MITBIH) public database. Results show that the proposed method outperforms DTW on all aspects. This suggests that our method could be a good alternative to the classical DTW technique for quasi-periodic signals comparison. Specific applications are foreseen for our method: Novelty detection and person identification using ECG. 2007 Elsevier B.V. All rights reserved.	algorithm;burrows–wheeler transform;database;evert willem beth;novelty detection;quasiperiodicity;run time (program lifecycle phase);sorting;time series;tracing (software);type signature	Bachir Boucheham	2008	Pattern Recognition Letters	10.1016/j.patrec.2007.11.004	speech recognition;computer science;artificial intelligence;dynamic time warping;accuracy and precision;algorithm;statistics	DB	19.817507639057386	-61.382903466582185	1641
6d30a9a46575e08ff38737ee1842cdc2b895a719	on turbo-compression and modeling of black and white images	distributed algorithms;compression algorithm;empirical study;image coding;data compression;distributed turbo compression modeling image pixel statistics image rows entropy nonstationary conditional distributions turbo codes compression bounds black and white images lossless compression;image coding pixel entropy turbo codes probability distribution councils statistical distributions compression algorithms parity check codes transform coding;lossless compression;turbo codes;statistical distributions;entropy codes;statistical distributions image coding turbo codes data compression entropy codes distributed algorithms;conditional distribution;turbo code	This paper considers lossless compression of black and white images using recently developed distributed turbo compression techniques. An empirical study of image pixel statistics is initially conducted in order to apply turbo compression algorithms to distributed compression of image rows. In particular, it is noted that while the entropy of the pixels throughout the image does not vary much, the distribution and conditional distributions of pixel values are nonstationary. Corresponding compression bounds are noted, appropriate coding schemes based on turbo-codes are developed and simulation results are reported.	algorithm;data compression;lossless compression;pixel;simulation;turbo code	Isabel Deslauriers;Jan Bajcsy	2004	2004 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2004.1326634	data compression;lossy compression;color cell compression;distributed algorithm;data compression ratio;turbo code;speech recognition;block truncation coding;computer science;entropy encoding;theoretical computer science;serial concatenated convolutional codes;mathematics;lossless compression;tunstall coding;adaptive coding;context-adaptive binary arithmetic coding;texture compression;statistics;coding theory	Robotics	49.87793295967261	-12.46066607457116	1643
ec9301e1701e73bd868f281a500bd06736b07550	constructive modeling of frep solids using spline volumes	real time;collaborative cad;solid modeling;design process history;design rationale;value function;space mapping	We present an approach to constructive modeling of FRep solids [2] defined by real-valued functions using 4D uniform rational cubic Bspline volumes as primitives. While the first three coordinates are used to represent the spatial component of the volume to be sculpted, the fourth coordinate is used as a scalar, which corresponds to a function value or a volume density. Thus, the shape can be manipulated by changing the scalar control coefficients of the spline volume. This modeling process is interactive as the isosurface can be polygonized and visualized in real time. The distance property we obtain, combined with the properties of the spline volumes, allow us to use the resulting 3D solid as a leaf of a constructive tree and to apply to it set-theoretic, blending and other operations defined using R-functions [2]. Additional deformations can be achieved by moving arbitrary points in the coordinate space and applying space mapping at any level of the constructive tree. The final constructive solid is defined by a single real-valued function evaluated by the tree traversing procedure.	alpha compositing;b-spline;coefficient;cubic function;function representation;isosurface;set theory;space mapping;spline (mathematics)	Benjamin Schmitt;Alexander A. Pasko;Christophe Schlick	2001		10.1145/376957.377003	mathematical optimization;discrete mathematics;space mapping;mathematics;geometry;function representation;bellman equation;solid modeling;design rationale	Graphics	67.92180050371769	-42.44497607501023	1647
2081f16295721bedb80264af07c180ffc3e6b627	modeling distinct sources of neural variability driving neuroprosthetic control	decoding;trajectory;statistics;neurons;calibration;sociology;data models	Many closed-loop, continuous-control brain-machine interface (BMI) architectures rely on decoding via a linear readout of noisy population neural activity. However, recent work has found that decomposing neural population activity into correlated and uncorrelated variability reveals that improvements in cursor control coincide with the emergence of correlated neural variability. In order to address how correlated and uncorrelated neural variability arises and contributes to BMI cursor control, we simulate a neural population receiving combinations of shared inputs affecting all cells and private inputs affecting only individual cells. When simulating BMI cursor-control with different populations, we find that correlated activity generates faster, straighter cursor trajectories, yet sometimes at the cost of inaccuracies. We also find that correlated variability can be generated from either shared inputs or quickly updated private inputs. Overall, our results suggest a role for both correlated and uncorrelated neural activity in cursor control, and potential mechanisms by which correlated activity may emerge.	architecture as topic;brain neoplasms;brain–computer interface;cursor (databases);emergence;heart rate variability;neural ensemble;neural oscillation;neuroprosthetics;population;simulation;spatial variability	Preeya Khanna;Vivek R. Athalye;Suraj Gowda;Rui M. Costa;Jose M. Carmena	2016	2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2016.7591377	data modeling;real-time computing;calibration;computer science;trajectory;machine learning;communication;physics;statistics	SE	16.112034270915515	-71.87089899767264	1650
2a2cd47d6f1ec0a3e24634977b143dfd17e45dd7	a novel technique for image-velocity computation	delay lines;picture processing;analog vlsi image velocity computation image processing delay lines image velocity image area;picture processing delay lines;analog vlsi;delay lines very large scale integration propagation delay hardware image processing digital signal processing read write memory pixel optical distortion optical computing	A novel technique for image-velocity computation is described. Using one set of intersecting delay lines that are physically perpendicular to each other, the two perpendicular components of image velocity can be computed separately if the image area is known. To compute the image velocity without knowledge of the image area, two sets of intersecting delay lines are required. This technique is suitable for analog VLSI implementation.	analog-to-digital converter;computation;delay line memory;velocity (software development);very-large-scale integration	Chu Phoon Chong;C. Andre T. Salama;Kenneth C. Smith	1992	IEEE Trans. Circuits Syst. Video Techn.	10.1109/76.157163	computer vision;feature detection;analog image processing;image processing;computer science;theoretical computer science;digital image processing	EDA	42.34234518054929	-3.49009174901135	1651
14d6ddb48d1b8a593665576d7e25f17be1447b2e	recognizing human actions using multiple features	eigenvalues and eigenfunctions;graph theory;image recognition;euclidian coordinates;fuses;vocabulary;local spatiotemporal volumes;video retrieval;imaging variations;testing;matrix algebra;humans videos vocabulary fuses image recognition lighting shape laplace equations matrix converters testing;computer vision;symmetric matrices;laplace equations;video recognition;shape;multiple features;single feature based representation;video retrieval eigenvalues and eigenfunctions feature extraction graph theory image recognition image representation matrix algebra;three dimensional displays;image representation;feature extraction;quantized vocabulary;human action recognition;graph laplacian matrix;matrix converters;fiedler embedding;humans;lighting;video retrieval human action recognition multiple features video recognition single feature based representation imaging variations local spatiotemporal volumes quantized vocabulary euclidian coordinates graph laplacian matrix fiedler embedding;videos	In this paper, we propose a framework that fuses multiple features for improved action recognition in videos. The fusion of multiple features is important for recognizing actions as often a single feature based representation is not enough to capture the imaging variations (view-point, illumination etc.) and attributes of individuals (size, age, gender etc.). Hence, we use two types of features: i) a quantized vocabulary of local spatio-temporal (ST) volumes (or cuboids), and ii) a quantized vocabulary of spin-images, which aims to capture the shape deformation of the actor by considering actions as 3D objects (x, y, t). To optimally combine these features, we treat different features as nodes in a graph, where weighted edges between the nodes represent the strength of the relationship between entities. The graph is then embedded into a k-dimensional space subject to the criteria that similar nodes have Euclidian coordinates which are closer to each other. This is achieved by converting this constraint into a minimization problem whose solution is the eigenvectors of the graph Laplacian matrix. This procedure is known as Fiedler embedding. The performance of the proposed framework is tested on publicly available data sets. The results demonstrate that fusion of multiple features helps in achieving improved performance, and allows retrieval of meaningful features and videos from the embedding space.	cuboid;embedded system;entity;greedy embedding;laplacian matrix;spatiotemporal database;vocabulary	Jingen Liu;Saad Ali;Mubarak Shah	2008	2008 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2008.4587527	fuse;computer vision;feature extraction;shape;computer science;graph theory;machine learning;pattern recognition;lighting;mathematics;geometry;software testing;symmetric matrix	Vision	36.03118333532288	-53.9668240804422	1653
b3c1590bb3f891d955d3bd19d9aa521d894cbf65	an improved fast harmony search algorithm for identification of hydrogeological parameters	algorithm design and analysis optimization heuristic algorithms parameter estimation genetic algorithms search problems convergence;improved fast harmony search;global searching;convergence;geophysics;hydrogeological parameters identification;ifhs algorithm;modflow algorithm;solution vectors;convergence rate;parameter identification;optimization problems;hydrological techniques geophysics groundwater;fast harmony search algorithm;heuristic methods;heuristic algorithms;parameter identification fast harmony search algorithm hydrogeological parameters;complex parameter identification problems;genetic algorithm;genetic algorithms;groundwater;optimization;search problems;parameter estimation;groundwater systems;harmony search;hydrogeological parameters;algorithm design;algorithm design and analysis;hydrological techniques;complex parameter identification problems hydrogeological parameters improved fast harmony search ifhs algorithm optimization problems solution vectors convergence rate modflow algorithm hydrogeological parameters identification heuristic methods global searching groundwater systems;heuristic algorithm	This paper develops an improved fast harmony search (IFHS) algorithm for solving optimization problems. The proposed IFHS algorithm employs novel methods for generating new solution vectors and expanding the scale of new solution vectors to enhance accuracy and convergence rate of harmony search (HS) algorithm. Moreover, the IFHS algorithm combined with MODFLOW is successfully used to solve the problem of hydrogeological parameters identification. The results show that the proposed algorithm, compared with other heuristic methods, has more powerful ability of global searching and faster convergence rate for complex parameter identification problems of groundwater systems.	genetic algorithm;hs algorithm;harmony search;heuristic;mathematical optimization;rate of convergence;search algorithm	Qian-kun Luo;Jianfeng Wu;Yun Yang	2011	2011 Seventh International Conference on Natural Computation	10.1109/ICNC.2011.6022401	mathematical optimization;geography;artificial intelligence;machine learning	Robotics	28.151251342915657	-4.94705463826243	1658
5f6ab4543cc38f23d0339e3037a952df7bcf696b	video2vec: learning semantic spatio-temporal embeddings for video representation	computer engineering;zero shot learning;optical network units;semantics;computer architecture;optical imaging;logic gates;deep learning;semantic embedding;feature extraction;action recognition;video representation;encoding;semantic retrieval	We propose to learn semantic spatio-temporal embeddings for videos to support high-level video analysis. The first step of the proposed embedding employs a deep architecture consisting of two channels of convolutional neural networks (capturing appearance and local motion) followed by their corresponding Gated Recurrent Unit encoders for capturing longer-term temporal structure of the CNN features. The resultant spatio-temporal representation (a vector) is used to learn a mapping via a multilayer perceptron to the word2vec semantic embedding space, leading to a semantic interpretation of the video vector that supports high-level analysis. We demonstrate the usefulness and effectiveness of this new video representation by experiments on action recognition, zero-shot video classification, and “word-to-video” retrieval, using the UCF-101 dataset.	artificial neural network;convolutional neural network;encode;encoder;experiment;high- and low-level;memory-level parallelism;multilayer perceptron;resultant;semantic interpretation;time series;video clip;video content analysis;word2vec	Sheng-hung Hu;Yikang Li;Baoxin Li	2016	2016 23rd International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2016.7899735	computer vision;semantic computing;logic gate;feature extraction;computer science;theoretical computer science;machine learning;optical imaging;semantics;deep learning;encoding	Vision	26.90638262259659	-52.28283823702961	1659
863ff61028eb038e8d3e59a94ddc8ce73a3501a0	real-time force optimization in parallel kinematic chains under inequality constraints	kinematics;linear programming;quadratic programming;robots;cooperating manipulators;inequality constraints;mechanical hands;parallel kinematic chains;quadratic optimization;real-time control;real-time force optimisation;underdetermined force system;walking machines;control systems;optimal control;linear program;real time control;real time systems;constraint optimization;actuators;real time;torque;topology	The real-time control of cooperating manipulators, mechanical hands, and walking machines involves the optimization of an underdetermined force system subject to both equality and inequality constraints. The inequality constraints arise as a result of passive frictional contacts in systems that depend on these for force transmission, or when taking into account the limited torque or force capability of the actuators. Since the results of the force optimization are used to provide force or torque setpoints to the actuators, they must be obtained in real time. A technique is presented for solving a quadratic optimization problem with equality and inequality constraints for this application. The technique is compared with linear programming to show its superior performance in terms of speed and the quality of the solution	kinematic chain;mathematical optimization;real-time transcription;social inequality	Meyer Nahon;Jorge Angeles	1991		10.1109/ROBOT.1991.131956	kinematic chain;robot;control engineering;mathematical optimization;kinematics;real-time control system;optimal control;computer science;linear programming;manipulator;comparative research;control theory;mathematics;constraint;quadratic programming;force;cooperation	Robotics	65.11077078836321	-21.11879675002121	1664
3ab72e1f92c93ae88b9abf47dd8ba38c1f72c025	hierarchical fuzzy intelligent controller for gymnastic bar actions	hierarchical control system;linguistic knowledge expression;fuzzy intelligent control			Junji Nishino;Akihiro Tagawa;Haruhiko Shirai;Tomohiro Odaka;Hisakazu Ogura	1999	JACIII	10.20965/jaciii.1999.p0106	computer science;artificial intelligence;neuro-fuzzy;machine learning;hierarchical control system	Robotics	3.761926429549433	-25.494699538017322	1665
1f279b55612f2e6a0d987fcfa7112d108321ddd3	global mittag-leffler synchronization of fractional-order neural networks via impulsive control	fractional-order neural networks;impulse;mittag-leffler synchronization;lyapunov direct method;m-matrix	This paper aims at analyzing the impulsive synchronization of fractional-order neural works. Firstly, in view of control theory, by constructing a suitable impulsive response system with the designed controller, the synchronization error system between the drive system and the corresponding response system is given. Afterwards, based on the theory of impulsive differential equation, the theory of fractional differential equation, Lyapunov direct method, and inequality techniques, some effective sufficient criteria are established to guarantee the global Mittag-Leffler stability for the synchronization error system. Finally, several simulation examples are designed to demonstrate the effectiveness and feasibility of the obtained results.	artificial neural network;complex systems;control system;control theory;direct method in the calculus of variations;lyapunov fractal;nonlinear system;samuel j leffler;simulation;social inequality	Xujun Yang;Chuandong Li;Tingwen Huang;Qiankun Song;Junjian Huang	2017	Neural Processing Letters	10.1007/s11063-017-9744-x	control theory;inequality;artificial intelligence;machine learning;impulse (physics);m-matrix;synchronization;differential equation;artificial neural network;lyapunov function;mathematics;control theory	ML	73.14171441945774	1.8397245734721455	1672
5c8ad432c9dee0c87a777daffca6e3c46ff2837e	gpr signal denoising and target extraction with the ceemd method	target extraction gpr signal denoising ceemd method time analysis method complete ensemble empirical mode decomposition method ground penetrating radar signal processing gaussian white noise mode mixing problem signal to noise ratio hilbert huang transform spectral spatial resolution;ground penetrating radar;time and frequency analysis complete ensemble empirical mode decomposition ceemd ground penetrating radar gpr signal signal processing;principal component analysis;signal resolution;signal to noise ratio;time frequency analysis;time frequency analysis awgn decomposition ground penetrating radar hilbert transforms radar detection signal resolution;signal denoising;ground penetrating radar signal resolution time frequency analysis principal component analysis signal denoising signal to noise ratio	In this letter, we apply a time and frequency analysis method based on the complete ensemble empirical mode decomposition (CEEMD) method in ground-penetrating radar (GPR) signal processing. It decomposes the GPR signal into a sum of oscillatory components, with guaranteed positive and smoothly varying instantaneous frequencies. The key idea of this method relies on averaging the modes obtained by empirical mode decomposition (EMD) applied to several realizations of Gaussian white noise added to the original signal. It can solve the mode-mixing problem in the EMD method and improve the resolution of ensemble EMD (EEMD) when the signal has a low signal-to-noise ratio. First, we analyze the difference between the basic theory of EMD, EEMD, and CEEMD. Then, we compare the time and frequency analysis with Hilbert-Huang transform to test the results of different methods. The synthetic and real GPR data demonstrate that CEEMD promises higher spectral-spatial resolution than the other two EMD methods in GPR signal denoising and target extraction. Its decomposition is complete, with a numerically negligible error.	algorithm;ensemble kalman filter;fast fourier transform;frequency analysis;hilbert–huang transform;iteration;kriging;noise reduction;numerical analysis;signal processing;signal reconstruction;signal-to-noise ratio;smoothing;synthetic intelligence;white noise	Jing Li;Cai Liu;Zhaofa Zeng;Lingna Chen	2015	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2015.2415736	speech recognition;time–frequency analysis;ground-penetrating radar;telecommunications;computer science;signal-to-noise ratio;physics;quantum mechanics;principal component analysis	EDA	81.1637301250274	-40.031674175109096	1675
0ceae64600380929657b500d930e63c033dcb010	deriving biased classifiers for better roc performance.	roc analysis	Induction of classiiers is an important task in the eld of data mining. Classiiers are often evaluated based on their predictive accuracy, but there are disadvantages associated with this measure: it may not be appropriate for the context in which the classiier will be deployed. ROC analysis is an alternative evaluation technique that makes it possible to evaluate how well classiiers will perform given certain misclassiication costs and class distributions. Given a set of classiiers, it also provides a method for constructing a hybrid classiier that optimally uses the available classiiers according to speciic properties of the deployment context. Now in some cases it is possible to derive multiple classiiers from a single one, in a cheap way, and such that these classiiers focus on diierent areas of the ROC diagram, such that a hybrid classiier with better overall ROC performance can be constructed. This principle is quite generally applicable; here we describe a method to apply it to decision tree classiiers. An experimental evaluation illustrates the usefulness of the technique.	data mining;decision tree;diagram;receiver operating characteristic;software deployment	Hendrik Blockeel;Jan Struyf	2002	Informatica (Slovenia)		computer science;machine learning;receiver operating characteristic	ML	11.572419186720838	-37.512160061930864	1677
8c8de3cc57bb803fb75046216b3b92d78c8a434d	h∞ model reduction for continuous-time switched stochastic hybrid systems	switched systems;commande multimodele;continuous time;switched system;sistema hibrido;lyapunov function;exponential stability;stochastic hybrid system;ℋ performance;mean square;mean square exponential stability;control multimodelo;multimodel control;dwell time;h performance;systeme ordre reduit;stochastic system;linear matrix inequality;model reduction;commande stochastique;performance model;hybrid system;stochastic control;control estocastico;stochastic systems;article;reduced order systems;systeme hybride	This article deals with the problem of computing an approximation system for a continuous-time switched stochastic system, such that the ℋ∞ gain of the error system is less than a prescribed scalar. By using the average dwell-time approach and the piecewise Lyapunov function technique, a sufficient condition is first proposed, which guarantees the error system to be mean-square exponentially stable with a weighted ℋ∞ performance. Then, the model reduction is solved by using the projection approach, which casts the model reduction into a sequential minimisation problem subjected to linear matrix inequality constraints by employing the cone complementary linearisation algorithm. Finally, a numerical example is provided to illustrate the effectiveness of the proposed theory.	hybrid system	Ligang Wu;Daniel W. C. Ho;James Lam	2009	Int. J. Systems Science	10.1080/00207720902989312	control engineering;mathematical optimization;stochastic control;linear matrix inequality;lyapunov function;control theory;dwell time;mathematics;exponential stability;hybrid system	Logic	72.83775005146447	-2.657431831682833	1683
0ff44c14f03f86c4e4e839b775b5ebc1e8f5e7dc	learning optimal redistribution mechanisms through neural networks		We consider a social setting where p public resources/objects are to be allocated among n competing and strategic agents so as to maximize social welfare (the objects should be allocated to those who value them the most). This is called allocative efficiency (AE). We need the agents to report their valuations for obtaining these resources, truthfully referred to as dominant strategy incentive compatibility (DSIC). Typically, we use auction-based mechanisms to achieve AE and DSIC. However, due to Green-Laffont Impossibility Theorem, we cannot ensure budget balance in the system while ensuring AE and DSIC. That is, the net transfer of money cannot be zero. This problem has been addressed by designing a redistribution mechanism so as to ensure minimum surplus of money as well as AE and DSIC. The objective could be to minimize surplus in expectation or in the worst case and these p objects could be homogeneous or heterogeneous. Designing of such mechanisms is non-trivial. Especially, designing redistribution mechanisms which perform well in expectation becomes analytically challenging for heterogeneous settings. In this paper, we take a completely different, data-driven approach. We train a neural network to determine an optimal redistribution mechanism based on given settings with both the objectives, optimal in expectation and optimal in the worst case. We also propose a loss function to train a neural network to optimize worst case. We design neural networks with the underlying rebate functions being linear as well as nonlinear in terms of bids of the agents. Our networks’ performances are same as the theoretical guarantees for the cases where it has been solved. We observe that a neural network based redistribution mechanism for homogeneous settings which uses nonlinear rebate functions outperforms linear rebate functions when the objective is optimal in expectation. Our approach also yields an optimal in expectation redistribution mechanism for heterogeneous settings.	allocative efficiency;artificial neural network;best, worst and average case;care-of address;loss function;money;network planning and design;neural network software;nonlinear system;performance;procedural generation;randomness;value (ethics)	P. Manisha;C. V. Jawahar;Sujit Gujar	2018			computer science;machine learning;artificial neural network;mathematical optimization;artificial intelligence;deep learning;strategic dominance;incentive compatibility;valuation (finance);allocative efficiency;arrow's impossibility theorem;redistribution (cultural anthropology)	AI	-1.5302488491961301	0.9843702172719918	1684
3b6927780662e2672f45ab3c5327d1b476b828b5	an emotion recognition system based on rough set theory	rough set theory;emotion recognition;pattern recognition;rough set;affective computing	Affective computing is becoming a more and more important topic in intelligent computing technology. Emotion recognition is one of the most important topics in affective computing. It is always performed on face and voice information with such technology as ANN, fuzzy set, SVM, HMM, etc. In this paper, an emotion recognition system based on rough set theory (ERSBRS) is proposed, which take facial features as input data. Our simulation experiment results show that rough set theory and method are effective in emotion recognition, and high recognition rate is resulted.	emotion recognition;rough set;set theory	Yongkai Yang;Guoyin Wang;Peijun Chen	2006			rough set;computer science;artificial intelligence;machine learning;pattern recognition;affective computing	Robotics	8.711098569243251	-94.80307887251344	1686
01a0d19026da8c96dd92cfb3e47651ba2a94bd81	resolve reactive robot control with perturbed constraints using a second order cone programming approach		As a modular and reactive control approach, constraint-based programming helps us to formulate and solve complex robotic tasks in a systematic way. In different fields ranging from industrial manipulators to humanoids, robots are supposed to work in an uncertain environment. However, how to address uncertainties is missing in the state-of-the-art of different constraint-based programming frameworks. In this paper, we introduce a Second Order Cone Programming (SOCP) approach to integrate constraints with norm bounded uncertainties. The proposed SOCP is convex and through simulations with controlled uncertainty level, we can clearly tell that the proposed approach guarantees the constraints satisfaction compared to the state-of-the-art.	best, worst and average case;coefficient;conic optimization;jacobian matrix and determinant;plasma cleaning;qp state machine frameworks;robot control;second-order cone programming;simulation	Yuquan Wang;Lihui Wang	2017	2017 13th IEEE Conference on Automation Science and Engineering (CASE)	10.1109/COASE.2017.8256091	kinematics;mathematical optimization;second-order cone programming;robot control;bounded function;ranging;modular design;mathematics	Robotics	63.19356357935721	-22.321219880167973	1687
46a842a15cf4b326354959702ac7a5001562e982	estimation of healthy and fibrotic tissue distributions in de-cmr incorporating cine-cmr in an em algorithm		Delayed Enhancement (DE) Cardiac Magnetic Resonance (CMR) allows practitioners to identify fibrosis in the myocardium. It is of importance in the differential diagnosis and therapy selection in Hypertrophic Cardiomyopathy (HCM). However, most clinical semiautomatic scar quantification methods present high intra- and interobserver variability in the case of HCM. Automatic methods relying on mixture model estimation of the myocardial intensity distribution are also subject to variability due to inaccuracies of the myocardial mask. In this paper, the CINE-CMR image information is incorporated to the estimation of the DE-CMR tissue distributions, without assuming perfect alignment between the two modalities nor the same label partitions in them. For this purpose, we propose an expectation maximization algorithm that estimates the DE-CMR distribution parameters, as well as the conditional probabilities of the DE-CMR labels with respect to the labels of CINE-CMR, with the latter being an input of the algorithm. Our results show that, compared to applying the EM using only the DE-CMR data, the proposed algorithm is more accurate in estimating the myocardial tissue parameters and obtains higher likelihood of the fibrosis voxels, as well as a higher Dice coefficient of the subsequent segmentations.	expectation–maximization algorithm	Susana Merino-Caviedes;Lucilio Cordero-Grande;M. Teresa Sevilla-Ruiz;Ana Revilla-Orodea;M. Teresa Pérez Rodríguez;César Palencia de Lara;Marcos Martín-Fernández;Carlos Alberola-López	2017		10.1007/978-3-319-75541-0_6	voxel;expectation–maximization algorithm;mixture model;sørensen–dice coefficient;data mining;tissue distributions;conditional probability;magnetic resonance imaging;hypertrophic cardiomyopathy;computer science;pattern recognition;artificial intelligence	ML	42.136569757971756	-79.53136595700843	1689
969575a87d98c9eb39f0fd3adf425166aedfd877	k-t cspi: a dynamic mri reconstruction framework for combining compressed sensing and parallel imaging	image sampling;spatial temporal total variation;nonlinear grappa technique;compressed sensing based dynamic methods;compressed sensing;aliased dynamic images;satisfiability;undersampled k space data;magnetic resonance image;acceleration;parallel imaging;sensitivity;mr imaging;dynamic cardiac cine mri;coils;dynamic mri reconstruction framework;image reconstruction;medical image processing;magnetic resonance imaging;total variation;dynamic mri;parallel imaging techniques;medical image processing biomedical mri compressed sensing image reconstruction image sampling image sequences;sampling strategy;dynamic cardiac cine mri dynamic mri reconstruction framework image reconstruction compressed sensing based dynamic methods parallel imaging techniques aliased dynamic images undersampled k space data nonlinear grappa technique;spatial temporal total variation dynamic mri compressed sensing nonlinear grappa;image reconstruction magnetic resonance imaging compressed sensing coils acceleration sensitivity;nonlinear grappa;biomedical mri;image sequences	In this paper, we propose a new dynamic MR image reconstruction technique that combines the compressed sensing-based dynamic methods with parallel imaging techniques to achieve high accelerations. The method decouples the reconstruction process into two sequential steps. In the first step, a series of aliased dynamic images is reconstructed using a CS method from the highly undersampled £-space data. In the second step, the missing £-space data for the original image are reconstructed by the nonlinear GRAPPA technique. The sampling strategy for each step is thereby designed independently such that the incoherent undersampling requirement for CS and structured undersampling requirement for parallel imaging can be satisfied simultaneously. Experimental results demonstrate that the proposed method improves the reconstruction quality of dynamic cardiac cine MRI over the state-of-the-art method.	compressed sensing;iterative reconstruction;nonlinear system;sampling (signal processing);undersampling	Yihang Zhou;Yuchou Chang;Dong Liang;Leslie Ying	2012	2012 9th IEEE International Symposium on Biomedical Imaging (ISBI)	10.1109/ISBI.2012.6235571	iterative reconstruction;acceleration;computer vision;mathematical optimization;radiology;medicine;sensitivity;computer science;magnetic resonance imaging;dynamic contrast-enhanced mri;mathematics;nuclear magnetic resonance;compressed sensing;total variation;satisfiability	Vision	49.961519746004285	-80.80350355299558	1691
acd26bef1d2cb5ab75a221b88cb6d67fd0ef16fe	constructing fuzzy measures in expert systems	uncertainty principle;fuzzy measure;expert systems;neural networks;uncertainty;sugeno measures;fuzzy relation;uncertainty principles;modal logic;choquet integral;dempster shafer theory;statistical inference;possibility theory;genetic algorithm;genetic algorithms;fuzzy measures;neural network;expert system;sugeno integral	Abstract   This paper is an overview of results regarding various representations of fuzzy measures and methods for constructing fuzzy measures in the context of expert systems, which were obtained by the authors and their associates during the last three years. Included are methods for constructing fuzzy measures by various transformations, by extension, by statistical inference, and by various data-driven methods based either on the Sugeno-integral or the Choquet-integral and using neural networks, genetic algorithms, or fuzzy relation equations.	expert system;fuzzy concept;fuzzy measure theory	George J. Klir;Zhenyuan Wang;David Harmanec	1997	Fuzzy Sets and Systems	10.1016/S0165-0114(97)00174-7	genetic algorithm;defuzzification;adaptive neuro fuzzy inference system;fuzzy classification;computer science;artificial intelligence;neuro-fuzzy;machine learning;fuzzy measure theory;data mining;mathematics;fuzzy associative matrix;expert system;fuzzy set operations;artificial neural network	AI	1.9683821714744638	-25.477129225906584	1695
3473e7aadf33d95a7c01fac902046bc795e36dd5	a knowledge-based methodology for tuning analytical models	forecasting;computer aided analysis;prevision;representacion conocimientos;logic programming computer aided analysis knowledge based systems;knowledge based system;linguistic variable;procesamiento informacion;metodologia;systeme accord;adjustment;base connaissance;macroeconometric forecasting model analytical model tuning knowledge based methodology logic programming system;logical programming;prise decision;forecasting model;decision maker;methodologie;reglage;variable linguistique;analytical models predictive models decision making economic forecasting statistical analysis macroeconomics finance humans history logic programming;model error;logic programming;programmation logique;information processing;base conocimiento;systeme gestion base donnee;logic programs;reglaje;methodology;knowledge representation;traitement information;toma decision;representation connaissances;programacion logica;sistema gestion base datos;database management system;knowledge based systems;analytical model;knowledge base	"""Many computer-based analytical models for decision-making and forecasting have been developed in recent years, particularly in the areas of economics and finance. Analytic models have an important limitation which has restricted their use: a model cannot anticipate every factor that may be important in making a decision. Some analysts attempt to compensate for this limitation by making heuristic adjustments to the model in order to """"tune"""" the results. Tuning produces a model forecast that is consistent with intuitive expectations, and maintains the detail and structure of the analytic model. This is a very difficult task unless the user has expert knowledge of the model and the task domain. This paper describes a new methodology, called knowledge-based tuning, that allows a human analyst and a knowledge-based system to collaborate in adjusting an analytic model. Such a methodology makes the model more acceptable to a decision-maker, and offers the potential of improving the decisions that either an analyst or a model can make alone. In knowledge-based tuning, subjective judgments about missing factors are specified by the analyst in terms of linguistic variables. These linguistic variables and knowledge of the model error history are used by the tuning system to infer a specific model adjustment. A logic programming system was developed that illustrates the tuning methodology for a macroeconometric forecasting model that empirically demonstrates how the predictability of the model can be improved. *Department of Computer Science, Polytechnic University **Department of Computer Science, St. John's University. To appear in IEEE Transactions on Systems, Man, and Cybernetics, Volume 21, Number 2, March, 1991. A KNOWLEDGE-BASED METHODOLOGY FOR TUNING ANALYTICAL MODELS 2 Biosketch of Authors Roy S. Freedman received his BS and MS in Mathematics in 1975, his MS in Electrical Engineering in 1978, and his PhD in Mathematics in 1979, all from Polytechnic University. He joined the Hazeltine Corporation Research Laboratories in 1979, and was responsible for the software engineering of several well-known AI systems. In 1985 he returned to Polytechnic University as an Associate Professor. As a consultant, Dr. Freedman has done extensive work for a broad range of clients in the financial services and telecommunication industries, including the New York Stock Exchange, Equitable Life, MCI, Chemical Bank, Intelligent Technology Inc. (Tokyo), Grumman Aerospace, and Hazeltine Corporation. In 1989, he founded Inductive Solutions, Inc., a firm that produces tools for building embedded knowledge-based systems. Dr. Freedman has published over thirty papers in artificial intelligence and software engineering as well as two books, Programming Concepts with the Ada Language and Programming with APSE Software Tools, both now published by McGraw-Hill. He has been an Associate Editor of the journal IEEE Expert for the past four years, and is a member of the IEEE Computer Society, AAAI, ACM, and SIAM. Dr. Freedman can be reached at the Department of Electrical Engineering and Computer Science at Polytechnic University, 333 Jay Street, Brooklyn, NY 11201 or at Inductive Solutions, Inc., 380 Rector Place, Suite 4A, New York, NewYork 10280. Gerald J. Stuzin is an Associate Professor of Computer Science at St. John's University. He obtained his Ph.D. from Polytechnic University in 1989, and an MBA in Economics from New York University in 1972. Since 1969, Dr. Stuzin has worked as an economic consultant at American Can Company, CBS, Inc., Union Carbide, and the International Forecast Group. He is presently a consultant at Business Planning Systems, an economics consulting firm. His current interests are in developing intelligent analytic tools. Dr. Stuzin can be reached at the Department of Computer Science at St. John’s University, Jamaica, New York 11471. A KNOWLEDGE-BASED METHODOLOGY FOR TUNING ANALYTICAL MODELS"""	apse;ada;admissible numbering;analytical engine;artificial intelligence;book;computer science;dr. zeus inc.;electrical engineering;embedded system;game-maker;glossary of computer graphics;heuristic;ieee systems, man, and cybernetics society;intelligent agent;knowledge-based systems;logic programming;mathematical model;programming tool;software engineering	Roy S. Freedman;Gerald J. Stuzin	1991	IEEE Trans. Systems, Man, and Cybernetics	10.1109/21.87083	decision-making;forecasting;computer science;artificial intelligence;knowledge-based systems;machine learning;errors-in-variables models;methodology;logic programming;operations research;algorithm	DB	3.0646384823572355	-15.98569656869691	1696
23d3979cb212117aa98c6eca2afc51ea7461ca7a	nefrl: a new neuro-fuzzy system for episodic reinforcement learning tasks	episodic reinforcement learning tasks;pole balancing task nefrl neurofuzzy system episodic reinforcement learning tasks learning algorithm probability;learning algorithm;fuzzy neural nets;probability;fuzzy neural networks neural networks fuzzy systems information technology machine learning fellows unsupervised learning computer architecture stochastic processes gradient methods;learning artificial intelligence fuzzy neural nets fuzzy systems;reinforcement learning;input output;domain knowledge;neuro fuzzy;neurofuzzy system;neuro fuzzy system;nefrl;pole balancing task;learning artificial intelligence;fuzzy systems	In this paper, we propose a new neuro-fuzzy system for episodic reinforcement learning tasks, NEFRL. While NEFRL has all benefits of a neuro-fuzzy architecture, it has the additional advantage that it can learn with a numerical evaluation of performance and there is no need for training input-output pairs. Also, we show that the learning algorithm of this system converges with probability one to a local maximum of the average numerical performance signal. Our experimental results for the pole-balancing task show the power of this system even without any prior domain knowledge.	algorithm;fuzzy control system;maxima and minima;neuro-fuzzy;numerical analysis;reinforcement learning	Babak Behsaz;Reza Safabakhsh	2007	2007 Frontiers in the Convergence of Bioscience and Information Technologies	10.1109/FBIT.2007.139	multi-task learning;error-driven learning;computer science;artificial intelligence;machine learning;pattern recognition;learning classifier system	ML	15.59979970712626	-30.06883301726203	1699
77eec8280d62a38643bce4003d5cc7c3f4e63f39	a novel planar layered representation for 3dtv and freeview tv applications	minimisation;three dimensional displays rendering computer graphics nonlinear distortion geometry fitting labeling solid modeling;depth image based rendering model fitting multiview plus depth graph cut;image segmentation;planar assignments novel planar layered representation 3dtv freeview tv applications 3d content textural distortion geometric distortion novel view rendering energy minimization based framework 3d range data object like segmentation texture information;digital television;image texture;object detection digital television image representation image segmentation image texture minimisation;image representation;object detection	A novel planar layered representation for 3D content is proposed. The proposed representation aims to decouple the geometric and textural distortion related artifacts in novel view rendering. In this perspective, an energy minimization-based framework is utilized to fit planar models from 3D range data. The planar model assignments provide object-like segmentation of the scene. By rendering the corresponding texture information of the planar assignments, the proposed representation is finalized. In novel view rendering applications, the proposed representation shows minimum artifacts with different characteristics with respect to the conventional approaches. The experimental results of the proposed representation are quite promising for various datasets.	3d television;distortion;energy minimization	Burak Özkalayci;A. Aydin Alatan	2013	2013 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)	10.1109/ICMEW.2013.6618402	image texture;computer vision;minimisation;digital television;rendering;computer science;multimedia;image segmentation;computer graphics (images)	Robotics	56.86198299647411	-53.93558895740719	1703
606b34547b0edc5b4fba669d1a027961ee517cdd	power assist method for hal-3 using emg-based feedback controller	motion control;adaptive control humans torque exoskeletons legged locomotion leg power generation signal generators control systems hip;medical robotics;feedback;handicapped aids;feedback handicapped aids medical robotics electromyography torque control motion control;power assist walking power assist method hal 3 hybrid assistive leg emg electromyography exoskeletal robotics lower limb gait disorder motion control phase sequence control torque control myoelectricity signals;lower limb;electromyography;control method;torque control	We have developed the exoskeletal robotics suite HAL (Hybrid Assisitve Leg) which is integrated with human and assists suitable power for ‘lower limb of people with goit disorder. This study proposes the method of assist motion and ossist torque to realize a power assist corresponding to the operator’s intention. In the method of a,ssist motion, we adopted Phase Sequence control which generates a series of assist m,otions hi/ transiti.n,q .some simple basic motions called Phase. w e used the feedback controller to adjust the assist torque to m.ointain myoelectricity signals which were generated while performing the power assist uiolking. The experiment xsults showed the effective power assist according to operator’s intention b y using these control methods.	control theory;electromyography;hal;robotics suite	Hiroaki Kawamoto;Suwoong Lee;Shigehiro Kanbe;Yoshiyuki Sankai	2003		10.1109/ICSMC.2003.1244649	motion control;simulation;control theory;feedback	Robotics	70.03124343360156	-26.40877709554989	1711
90bc2cf14fb185dc83280274eefcf6256f5c333b	synchronization in networked mass-spring-damper oscillator systems	eigenvalues and eigenfunctions;graph theory;oscillations;complex dynamics;oscillators;shock absorbers;matrix theory;exact solution;dynamic system;numerical simulations synchronization networked mass spring damper oscillator systems nonlinear spring interaction algebraic graph theory matrix theory harmonic motion chain networked mechanical systems;harmonic motion;symmetric matrices;synchronisation;chain networked mechanical systems;springs mechanical;springs;numerical analysis;algebra;synchronisation algebra graph theory numerical analysis shock absorbers springs mechanical;synchronization;networked mass spring damper oscillator systems;algebraic graph theory;numerical simulations;nonlinear spring interaction;oscillators frequency synchronization nonlinear dynamical systems optical coupling control systems springs mechanical systems nonlinear control systems graph theory chaos;mechanical systems;stability theory;numerical simulation;harmonic analysis	This brief paper considers synchronization dynamics in networked mass-spring-damper (MSD) oscillator systems with nonlinear spring interaction. Based on stability theory on dynamical system, algebraic graph theory, and some matrix theory, an exact solution of synchronization state for such networked oscillator systems is derived analytically. It is shown that the networked MSD oscillator systems can be synchronized to a simple harmonic motion, even if the isolated oscillator is chaotic or others complex dynamics. Finally, the results are applied to a typical chain networked mechanical systems composing of six MSD oscillators and numerical simulations are given to verify the correctness of the theoretical results.	algebraic graph theory;complex dynamics;computational fluid dynamics;correctness (computer science);dynamical system;nonlinear system;numerical analysis;numerical linear algebra;simulation	Shan Cheng;Lan Xiang;Jin Zhou	2010	IEEE ICCA 2010	10.1109/ICCA.2010.5524382	classical mechanics;control engineering;synchronization;graph theory;harmonic analysis;control theory;mathematics;oscillation	Embedded	77.69699316443783	1.738280892277331	1713
17f47ba73651091e8e3deb3d72ddf9f76c9b6532	resiliency and robustness of alternative shape-based image retrieval	discrepancies resiliency robustness shape based image retrieval techniques multimedia similarity retrieval uncertainty shape representation techniques performance shape based object retrieval recall precision noise corner points human perception retrieval accuracy mbc tpvas minimum bounding circle touch point vertex angle sequence;similarity retrieval;shape representation;human factors;multimedia databases;multimedia databases image retrieval relevance feedback human factors;shape based image retrieval;relevance feedback;human perception;image retrieval shape measurement uncertainty databases noise shaping humans robust stability noise robustness computer science multimedia systems;image retrieval	The shape of an object is an important feature for image and multimedia similarity retrieval. However, as a consequence of uncertainty, shape representation techniques may sometimes work well only in certain environments, and their performance may depend crucially on the quality of the technique used to represent the shapes. In this study, we focus on shape-based object retrieval under various uncertainty scenarios and conduct a comparison study on four techniques. We measure the effectiveness of the similarity retrieval of the four different shape representation methods (in terms of recall and precision) under the following situations: (1) in the presence of noise in the database, (2) when the exact corner points are unknown, and (3) factoring in the human perception of similarity. Our results show that the similarity retrieval accuracy of our method [MBC-TPVAS (Minimum Bounding Circle with Touch-Point Vertex-Angle Sequence)] is better than that of the other methods under uncertainty and discrepancies.	image retrieval	Maytham Safar;Cyrus Shahabi;Cheng-Hai Tan	2000		10.1109/IDEAS.2000.880605	computer vision;visual word;image retrieval;computer science;theoretical computer science;data mining;perception;information retrieval	Vision	40.85858809922766	-62.03958635276735	1716
69af0b7b95946091ba76c64b6908a6fa46f00d8d	robust control for markovian jumping discrete-time systems	sistema lineal;riccati equation;systeme commande;sistema control;boucle reaction etat;disturbance rejection;concepcion sistema;control h infinito;systeme discret;proceso markov;state feedback;robust control;rejet perturbation;linear system;time varying system;systeme incertain;control system;equation riccati;system design;processus markov;systeme parametre variable;estabilidad estocastica;stabilite stochastique;markov process;recuazamiento pertubacion;control robusta;bucle realimentacion estado;ecuacion riccati;sistema parametro variable;sistema discreto;systeme lineaire;commande robuste;sistema incierto;stochastic stability;uncertain system;h infinite control;conception systeme;discrete time system;discrete system;commande h infini	In this paper, we investigate the H control problem for a class of linear discrete-time systems with Markovian jumping parameters. The jumping parameters considered here is modelled by adiscrete-time Markov process. Ourattentionisfocused onthedesign of linear state feedback controller such that both stochastic stability and a prescribed H performance are required to be achieved when the real system under consideration has different types of uncertainty. Sufficient conditions are proposed to solve the above problem, which are in terms of a set of solutions of coupled algebraic Riccati inequalities. An example is given to show the potential of the proposed techniques.	robust control	Peng Shi;El Kebir Boukas;Ramesh K. Agarwal	1999	Int. J. Systems Science	10.1080/002077299291912	robust control;control engineering;control system;discrete system;calculus;riccati equation;control theory;mathematics;markov process;linear system;systems design	Robotics	72.5824668839416	-2.8704140443208708	1723
ae4144f2977c05e56501c705825efa2a34aad242	adaptive output regulation of a class of discrete-time nonlinear systems based on output feedback and nn feedforward control	output feedback control;feedforward neural networks;nonlinear control systems;adaptive control;discrete time systems;discrete time;stability adaptive control causality discrete time systems feedback feedforward neural nets neurocontrollers nonlinear control systems;output feedback;stability;output regulation;artificial neural networks;control system;nonlinear systems;feedback;design method;adaptive systems;stability analysis;tracking control;kyp lemma a class system discrete time nonlinear system control system ofsp system causality problem adaptive output feedback control system neural network feedforward control;feedforward control;feedforward neural nets;neurocontrollers;nonlinear system;causality;output feedback adaptive systems artificial neural networks feedforward neural networks nonlinear systems stability analysis	In this paper, a design method for adaptive output tracking control of discrete-time nonlinear systems is dealt with. The proposed method ensures the stability of the resulting control system by an adaptive output feedback based on OFSP properties of the controlled system and achieves the output tracking by an adaptive NN feedforward control. Since the discrete-time OFSP system must have a relative degree of 0, it possibly results in the causality problem. We will solve this problem by considering an equivalent control using the available output signals and develop an adaptive output feedback control system with an adaptive NN feedforward. The stability of the obtained control system will be analyzed through the discrete-time nonlinear version of KYP lemma.	block cipher mode of operation;causality;control system;farkas' lemma;feed forward (control);feedback;feedforward neural network;nonlinear system	Ikuro Mizumoto;Naoki Tanaka;Toru Tokimatsu	2010	49th IEEE Conference on Decision and Control (CDC)	10.1109/CDC.2010.5717981	control engineering;discrete time and continuous time;von neumann stability analysis;real-time computing;causality;nonlinear control;stability;design methods;adaptive control;nonlinear system;computer science;control theory;feedback;mathematics;feed forward	Robotics	65.11840824737956	-5.9808294405857865	1728
c02bb9d32151c89f290378d64a048908d642f4c5	on hysteresis and air gap disturbance in current and voltage mode feed-forward control of variable reluctance actuators	magnetic hysteresis;voltage control;electric current control;first principle physical model hysteresis air gap disturbance current mode control voltage mode control feedforward force control variable reluctance actuators error generator force tracking error linearization law hysteresis compensation parametric hysteresis operator high fidelity actuator model magnetic material behavior actuator structure;feedforward;magnetic hysteresis force actuators voltage control atmospheric modeling magnetic cores magnetomechanical effects;actuators;force;compensation;magnetomechanical effects;linearisation techniques;atmospheric modeling;magnetic cores;voltage control actuators compensation electric current control feedforward force control linearisation techniques;force control	Current and voltage mode control approaches in feed-forward force control of variable reluctance actuators are analyzed and compared. Two major error generators, the unknown air gap variation and hysteresis, are investigated together with the quantification of their influence on the final force tracking error, and it is shown that voltage control has fundamental advantages over current control. Furthermore, linearization laws with hysteresis compensation based on the parametric hysteresis operator are proposed and compared on a high-fidelity actuator model which is derived from the behavior of magnetic materials, the actuator structure, and first principle physical models.	air gap (networking);current source;eclipse modeling framework;feed forward (control);feedback;feedforward neural network;hysteresis;noise (radio);quantum decoherence;simulation	A. Katalenic;C. M. M. van Lierop;P. P. J. van den Bosch	2011	IEEE Conference on Decision and Control and European Control Conference	10.1109/CDC.2011.6161384	control engineering;magnetic hysteresis;atmospheric model;electronic engineering;engineering;control theory;force;feed forward;actuator	Robotics	71.98793011304993	-12.47997177921983	1736
155be171bcbe8df6f9a5d4305edceca5334bf6b1	temporal range registration for unmanned ground and aerial vehicles	urban environment;range data;unmanned aerial vehicle;military radar;unmanned ground vehicle;global position system;mobile robots;remotely operated vehicles;3d registration;military aircraft;iterative methods;navigation;land vehicles road vehicles unmanned aerial vehicles global positioning system laser radar nist iterative algorithms robotics and automation iterative closest point algorithm robustness;range image;iterative methods radar imaging mobile robots navigation remotely operated vehicles military aircraft military radar road vehicles image registration multidimensional signal processing;image registration;radar imaging;multidimensional signal processing;iterative closest point;field data;positional information;vehicle position estimation temporal range registration unmanned ground vehicles unmanned aerial vehicles 3d range image registration global positioning system navigation iterative closest point algorithm scanning ladar rangefinder successive range images ladar images hybrid image registration approach air to ground registration feature based method;road vehicles	An iterative temporal registration algorithm is presented in this paper for registering 3D range images obtained from unmanned ground and aerial vehicles traversing unstructured environments. We are primarily motivated by the development of 3D registration algorithms to overcome both the unavailability and unreliability of Global Positioning System (GPS) within required accuracy bounds for unmanned ground vehicle (UGV) navigation. After suitable modifications to the well-known iterative closest point (ICP) algorithm, the modified algorithm is shown to be robust to outliers and false matches during the registration of successive range images obtained from a scanning LADAR rangefinder on the UGV. Towards registering LADAR images from the UGV with those from an unmanned aerial vehicle (UAV) that flies over the terrain being traversed, we then propose a hybrid registration approach. In this approach to air to ground registration to estimate and update the position of the UGV, we register range data from two LADARs by combining a feature-based method with the aforementioned modified ICP algorithm. Registration of range data guarantees an estimate of the vehicle's position even when only one of the vehicles has GPS information. Temporal range registration enables position information to be continually maintained even when both vehicles can no longer maintain GPS contact. We present results of the registration algorithm in rugged terrain and urban environments using real field data acquired from two different LADARs on the UGV.	aerial photography;unmanned aerial vehicle	Raj Madhavan;Tsai Hong;Elena Messina	2004		10.1109/ROBOT.2004.1307552	multidimensional signal processing;remotely operated underwater vehicle;mobile robot;computer vision;navigation;simulation;computer science;image registration;iterative method;radar imaging;iterative closest point;remote sensing	Robotics	53.742451991671324	-36.843338445851984	1738
043da3e8e663692e22e4b6aa96dee62af8fc2dfb	measuring human assessed complexity in synthetic aperture sonar imagery using the elo rating system		Performance of automatic target recognition from synthetic aperture sonar data is heavily dependent on the complexity of the beamformed imagery [1, 2]. Operators describe the task of explaining image complexity in the context of minehunting as a difficult problem. Several mechanisms can contribute to this including: unwanted vehicle dynamics, complex acoustic channels, the presence of biological activity, the bathymetry of the scene, and the presence of natural and manmade clutter. One often has an intuitive sense of what makes a particular image difficult, but determining a reliable mathematical formula for the task has proven elusive. In order to understand the impact of the environmental complexity on automatic image perception, researchers have taken approaches rooted in information theory [3, 4, 5] and heuristics [6]. Despite these efforts, a quantitative measure for complexity has not been related to the phenomenology from which it is derived. By using subject matter experts (SMEs) we will derive a complexity metric for a set of imagery which is associated to the perceived complexity. The goal of this work is to develop an understanding of how several common information theoretic and heuristic measures are related to the SME perceived complexity in synthetic aperture sonar imagery. We use the following approach to develop an SME-based complexity metric. First, an ensemble of 10 meter x 10 meter images were cropped from a SAS data set that spans multiple environments. Second, we choose to have operators compare pairs of images from a finite set and have them rank the images in one of three categories with respect to minehunting: image A is more complex than image B, image B is more complex than image A, or images A and B have the same complexity. We do this because it is difficult for humans to rank order large sets of information (Miller’s Law) [7]. Many methods exist to estimate the rank order of a set of pair-wise comparisons. One such method, Elo ranking, was originally developed for rank ordering chess players based on the outcome of matches (win, loose, or draw) [8]. Thus, finally, we translate these comparisons into a complexity metric using Elo ranking. The Elo method produced a plausible rank ordering across the broad dataset, despite some disagreement between operators on which conditions lead to the most complex environment (e.g. coral versus sand ripples). Heuristic and information theoretical metrics were then compared to the Elo score metric. The metrics with highest degree of correlation were those relating to spatial information, e.g. variations in pixel intensity, with an R-squared value of approximately 0.8. However, this agreement was dependent on the scale from which the spatial variation was measured, with only optimal kernel sizes being demonstrated here. We will also be presenting a comparison between the Elo score and many other metrics including lacunarity, image compression, and entropy.	acoustic cryptanalysis;automatic target recognition;bathymetry;clutter;coefficient of determination;entropy (information theory);fractal dimension;heuristic (computer science);image compression;information theory;learning to rank;pixel;sas;sonar (symantec);self-information;sports rating system;subject matter expert turing test;subject-matter expert;synthetic intelligence	Brian Reinhardt;Isaac Gerg;Daniel Brown;Joonho Park	2018	CoRR		artificial intelligence;pattern recognition;machine learning;spatial analysis;image compression;information theory;computer science;spatial variability;lacunarity;synthetic aperture sonar;heuristics;automatic target recognition	Vision	63.79816265609631	-66.3752492901955	1740
150c9e8790b82383cd9d31013344f6a1f0ef24c6	coding of stroke-based animations	qa75 electronic computers computer science szamitastechnika;monte carlo markov chain;szamitogeptudomany;image sequence	We present a way of rendering painting-like images and sequences by our Stochastic Painting-based SBR technique. We avoid disturbing artifacts by coding the images in a painting-like way. If we code the resulting stroke sequence, than the level of error comes not from the artifacts but from the painting process. For this reason, if we generate high quality paintings, we can transfer the image without disturbing coding errors. The painting method incorporates some novel properties like dynamic Monte Carlo Markov Chain optimization, multiscale edge gradient following or grayscale stroke templates. The painting technique inherits the properties of the Paintbrush Transformation, like well-defined contours, acceptable distortion and a painting-like view with no fine details below a limit. Our goal is to produce a painting -like output, which contains the stroke-series and the motion data in a losslessly compressed form. This way the painted video contains no compression artifacts (while the painting-like impression remains). The compression scheme of the stroke-series with motion data could also be suitable for compressing painting-like image sequences produced with other painting techniques.	artifact (software development);compression artifact;contour line;display resolution;distortion;gradient;grayscale;lossless compression;markov chain monte carlo;mathematical optimization;monte carlo method;paintbrush;standard business reporting	Levente Kovács;Tamás Szirányi	2004			computer vision;simulation;markov chain monte carlo;computer science;statistics;computer graphics (images)	Graphics	55.581137528978275	-59.50391444591882	1743
855bfc17e90ec1b240efba9100fb760c068a8efa	facial expression recognition using tracked facial actions: classifier performance analysis	human computer interaction;visual face tracking;3d deformable models;dynamic facial expression recognition;facial actions	In this paper, we address the analysis and recognition of facial expressions in continuous videos. More precisely, we study classifiers performance that exploit head pose independent temporal facial action parameters. These are provided by an appearance-based 3D face tracker that simultaneously provides the 3D head pose and facial actions. The use of such tracker makes the recognition poseand textureindependent. Two different schemes are studied. The first scheme adopts a dynamic time warping technique for recognizing expressions where training data are given by temporal signatures associated with different universal facial expressions. The second scheme models temporal signatures associated with facial actions with fixed length feature vectors (observations), and uses some machine learning algorithms in order to recognize the displayed expression. Experiments quantified the performance of different schemes. These were carried out on CMU video sequences and home-made video sequences. The results show that the use of dimension reduction techniques on the extracted time series can improve the classification performance. Moreover, these experiments show that the best recognition rate can be above 90%. & 2012 Elsevier Ltd. All rights reserved.	algorithm;dimensionality reduction;dynamic time warping;electronic signature;experiment;feature vector;image warping;kernel (operating system);linear discriminant analysis;machine learning;nonlinear dimensionality reduction;nonlinear system;principal component analysis;profiling (computer programming);real-time computing;six degrees of separation;time complexity;time series	Fadi Dornaika;Abdelmalik Moujahid;Bogdan Raducanu	2013	Eng. Appl. of AI	10.1016/j.engappai.2012.09.002	computer vision;speech recognition;computer science;three-dimensional face recognition;face hallucination	AI	31.97109823601965	-57.787161016248945	1744
5f61e333b317b513b42df888b4dbdbaad4ce002f	simulation of a hydraulic system duplicating the human circulatory system	hydraulic system;mock circulatory system;modeling;simulation	Mock circulatory system has been an essential tool for ventricular assist device VAD developing. It can replicate the characteristics of the natural human circulatory system, such as the flow and pressure of the aortic root. In this study a new method to establish a mock circulatory system using basic hydraulic elements is proposed. The system consists of a self-designed centrifugal pump, one electromagnetic switch valve, three vessels with a certain volume and a throttling valve. All of the hydraulic elements are modeled by means of fluid dynamics and ideal gas law. Once combining the components to form a system, numerical calculation is employed to get the results. Then the results are compared to the data obtained from the human circulatory system to validate the effectiveness of the new mock circulatory system. It can be found that this system is simple but valuable for VAD evaluating.	simulation	Feng Huang;Wenwei Qian;Xiao-dong Ruan;Xin Fu	2013		10.1007/978-3-642-40852-6_43	structural engineering;engineering	Robotics	82.37622089257759	-13.010378253077834	1747
496311ebd7424b6b1e9550f3bd941e88d42bbdef	a new approach to power system disturbance assessment using wide-area postdisturbance records		This paper presents an empirical wavelet transform (EWT) based approach to perform postmortem analysis of wide-area measurement (WAM) based signals. The commonly used empirical mode decomposition (EMD) has limitations such as mode mixing, sensitivity to noise, and sampling rate. The decomposition provided by EWT is more consistent as compared to EMD. The modes revealed by the EWT help in extracting dynamic patterns of different power system disturbances. The dynamic patterns extracted through EWT-based decomposition are further used as inputs to a data-mining tool known as random forest, to build a wide-area disturbance classifier (WADC) model. The efficient mode extraction quality of the EWT-based signal processing tool is analyzed for WAM data recorded on Northern Grid of Indian Power System. The performance of the WADC is validated on IEEE 39-bus New England test system. The results provide improved performance in terms of decomposition quality and classification accuracy.	data mining;hilbert–huang transform;random forest;sampling (signal processing);signal processing;wavelet transform	Manas Kumar Jena;Bijaya Ketan Panigrahi;Subhransu Ranjan Samantaray	2018	IEEE Transactions on Industrial Informatics	10.1109/TII.2017.2772081	wavelet;wavelet transform;computer science;grid;hilbert–huang transform;control engineering;sampling (signal processing);random forest;signal processing;feature extraction;pattern recognition;artificial intelligence	SE	18.660960926861975	-90.58595192019968	1751
b84721bd73ef250e0c78c157d1f8fd6ba0f3b4f4	underwater imaging using a 1 × 16 cmut linear array	transmission performance;capacitive micro machined ultrasonic transducer linear array;underwater imaging;synthetic aperture focusing technique	A 1 × 16 capacitive micro-machined ultrasonic transducer linear array was designed, fabricated, and tested for underwater imaging in the low frequency range. The linear array was fabricated using Si-SOI bonding techniques. Underwater transmission performance was tested in a water tank, and the array has a resonant frequency of 700 kHz, with pressure amplitude 182 dB (μPa·m/V) at 1 m. The -3 dB main beam width of the designed dense linear array is approximately 5 degrees. Synthetic aperture focusing technique was applied to improve the resolution of reconstructed images, with promising results. Thus, the proposed array was shown to be suitable for underwater imaging applications.	capacitive micromachined ultrasonic transducers;charge-coupled device;frequency band;international system of units;kilohertz;numerical aperture;obstacle avoidance;pyschological bonding;resonance;synthetic data;transducer;ultrasonics (sound);voltage;width	Rui Zhang;Wendong Zhang;Changde He;Yongmei Zhang;Jinlong Song;Chenyang Xue	2016	Sensors	10.3390/s16030312	electronic engineering;acoustics;engineering;optics	Robotics	91.93388639762168	-22.117755126803093	1755
46cd724695b5a0cd2d91a1663afaaa6e07717776	computational prediction of species-specific malonylation sites via enhanced characteristic strategy		Motivation Protein malonylation is a novel post-translational modification (PTM) which orchestrates a variety of biological processes. Annotation of malonylation in proteomics is the first-crucial step to decipher its physiological roles which are implicated in the pathological processes. Comparing with the expensive and laborious experimental research, computational prediction can provide an accurate and effective approach to the identification of many types of PTMs sites. However, there is still no online predictor for lysine malonylation.   Results By searching from literature and database, a well-prepared up-to-data benchmark datasets were collected in multiple organisms. Data analyses demonstrated that different organisms were preferentially involved in different biological processes and pathways. Meanwhile, unique sequence preferences were observed for each organism. Thus, a novel malonylation site online prediction tool, called MaloPred, which can predict malonylation for three species, was developed by integrating various informative features and via an enhanced feature strategy. On the independent test datasets, AUC (area under the receiver operating characteristic curves) scores are obtained as 0.755, 0.827 and 0.871 for Escherichia coli ( E.coli ), Mus musculus ( M.musculus ) and Homo sapiens ( H.sapiens ), respectively. The satisfying results suggest that MaloPred can provide more instructive guidance for further experimental investigation of protein malonylation.   Availability and Implementation http://bioinfo.ncu.edu.cn/MaloPred.aspx .   Contact jdqiu@ncu.edu.cn.   Supplementary information Supplementary data are available at Bioinformatics online.	amino acid sequence;area under curve;base excision repair;benchmark (computing);bioinformatics;biological processes;biological science disciplines;computation;database;decipher prostate cancer test;elfacos ow 100;escherichia coli;experiment;genetic translation process;geographic information systems;homo sapiens;house mice;kerrison predictor;lysine;muscle;natural science disciplines;ptms gene;pathologic processes;polynomial texture mapping;post-translational protein processing;propel;proteomics;receiver operating characteristic;acetylsalicylic acid lysinate;computer science;vedolizumab	Li-na Wang;Shao-Ping Shi;Hao-Dong Xu;Ping-Ping Wen;Jianding Qiu	2017	Bioinformatics	10.1093/bioinformatics/btw755	computer science;bioinformatics	Comp.	9.123859282018264	-56.49514849311727	1758
985030e1b1dff816a9aafa5669aa6c3b3bc87cb9	gait parameter estimation from a miniaturized ear-worn sensor using singular spectrum analysis and longest common subsequence	geriatrics;b800 medical technology;c600 sports science;time 20 min gait parameter estimation miniaturized ear worn sensor singular spectrum analysis gait analysis triaxial accelerometer swing stance stride times raw signals signal periodicity gait parameter extraction gait pattern changes heel contact toe off high speed camera force plate instrumented treadmill healthy adults walking;singular spectrum analysis ssa body sensor networks e ar sensor gait longest common subsequence lcss;ear;acceleration large hadron collider market research matrix decomposition oscillators silicon;gait analysis;accelerometers;medical signal processing;biomedical equipment;medical signal processing accelerometers biomedical equipment ear gait analysis geriatrics	This paper presents a new approach to gait analysis and parameter estimation from a single miniaturized ear-worn sensor embedded with a triaxial accelerometer. Singular spectrum analysis combined with the longest common subsequence algorithm has been used as a basis for gait parameter estimation. It incorporates information from all axes of the accelerometer to estimate parameters including swing, stance, and stride times. Rather than only using local features of the raw signals, the periodicity of the signals is also taken into account. The hypotheses tested by this study include: 1) how accurate is the ear-worn sensor in terms of gait parameter extraction compared to the use of an instrumented treadmill; 2) does the ear-worn sensor provide a feasible option for assessment and quantification of gait pattern changes. Key gait events for normal subjects such as heel contact and toe off are validated with a high-speed camera, as well as a force-plate instrumented treadmill. Ten healthy adults walked for 20 min on a treadmill with an increasing incline of 2% every 2 min. The upper and lower limits of the absolute errors using 95% confidence intervals for swing, stance, and stride times were obtained as 35.5 ±3.99 ms, 36.9 ±3.84 ms, and 17.9 ±2.29 ms, respectively.	algorithm;confidence intervals;embedded system;embedding;estimation theory;gait analysis;heel;longest common subsequence problem;maxima and minima;population parameter;quantitation;quasiperiodicity;sensor;singular;spectrum analysis;swing (java);traffic enforcement camera;treadmill, device;accelerometers	Delaram Jarchi;Charence Wong;Richard M Kwasnicki;Ben W. Heller;Garry A. Tew;Guang-Zhong Yang	2014	IEEE Transactions on Biomedical Engineering	10.1109/TBME.2014.2299772	embedded system;effect of gait parameters on energetic cost;electronic engineering;simulation;gait analysis;medicine;engineering;geriatrics;physiology;accelerometer	Visualization	11.78258579255317	-85.64786935963	1763
0ad958c0db14ba8977bbdde5c1b02b60f49ad57f	phase congruency detects corners and edges	feature detection;stereo matching;image registration	There are many applications such as stereo matching, motion tracking and image registration that require so called ‘corners’ to be detected across image sequences in a reliable manner. The Harris corner detector is widely used for this purpose. However, the response from the Harris operator, and other corner operators, varies considerably with image contrast. This makes the setting of thresholds that are appropriate for extended image sequences difficult, if not impossible. This paper describes a new corner and edge detector developed from the phase congruency model of feature detection. The new operator uses the principal moments of the phase congruency information to determine corner and edge information. The resulting corner and edge operator is highly localized and has responses that are invariant to image contrast. This results in reliable feature detection under varying illumination conditions with fixed thresholds. An additional feature of the operator is that the corner map is a strict subset of the edge map. This facilitates the cooperative use of corner and edge information.	computer stereo vision;corner detection;edge detection;feature detection (computer vision);feature detection (web development);harris affine region detector;image registration;phase congruency;stereoscopy	Peter Kovesi	2003			corner detection;computer vision;feature detection;edge detection;computer science;image registration;pattern recognition;feature detection;mathematics	Vision	44.849093686786404	-55.35058085988794	1766
26080b8d62fd201dba62811283fc6e3654dd3301	gabor wavelets transform and extended nearest feature space classifier for face recognition	image sampling;image downsampling gabor wavelets transform extended nearest feature space classifier face recognition image representation face identification linear discriminant analysis;image classification;linear discriminate analysis;wavelet transforms face recognition wavelet analysis kernel principal component analysis neural networks frequency convolution space technology linear discriminant analysis;null;feature space;image sampling face recognition wavelet transforms image classification image representation;wavelet transforms;hybrid approach;face recognition;image representation;action recognition;gabor wavelets;dimensional reduction;similarity measure	This paper proposes new hybrid approaches for face recognition. Gabor wavelets representation of face images is an effective approach for both facial action recognition and face identification. The performance of dimensionality reduction and linear discriminant analysis on the downsampled Gabor waveletfaces can increase the discriminant ability. The nearest feature space is extended to various similarity measures. In our experiments, proposed Gabor waveletfaces combined with extended nearest feature space classifier shows very good performance, which can achieve 99 % maximum correct recognition rate on ORL data set without any preprocessing step.	decimation (signal processing);dimensionality reduction;experiment;facial recognition system;feature vector;gabor filter;gabor wavelet;linear discriminant analysis;pixel;preprocessor;principal component analysis;return loss	Jianke Zhu;Mang I Vai;Peng Un Mak	2004	Third International Conference on Image and Graphics (ICIG'04)	10.1109/ICIG.2004.71	facial recognition system;computer vision;contextual image classification;feature vector;computer science;machine learning;pattern recognition;mathematics;gabor wavelet;wavelet transform	Vision	34.14081572980678	-58.44981124662725	1770
6f27d236e11ffd47897736589a7fca4143cdcc57	exploring the complexity of a proposed recursive measure of recombinational distance		When studying evolutionary systems, either from the natural world or artificially constructed using simulated populations, researchers must be able to quantify the genotypic differences that are observed. With the simple genetic algorithm employing both a unary mutation operator and a binary recombination operator to maintain variation in the population, it is exceedingly difficult to quantify the distance between elements of the chromosome space with an approach that is truly representative of the distance that would need to be traversed by the evolutionary mechanism. Although evaluation function dependence and the binary arity of the recombination operator both contribute to this difficulty, it is possible to redefine the function of recombination in such a way as to facilitate the computation of a more representative measurement of the distance the genetic algorithm would need to traverse to create a specific chromosome from a given population. The recursive approach presented here entails the definition of unary recombination operators and ultimately results in a technique for calculating the recombinational distance between chromosomes with a time complexity that is improved logarithmically over a simplistic approach.	bitwise operation;computation;evaluation function;evolutionary systems;genetic algorithm;operand;population;recursion (computer science);simulated annealing;simulation;speedup;traverse;time complexity;unary operation	Robert Collier;Mark Wineberg	2010			machine learning;recursion;artificial intelligence;mathematics	AI	24.365395286013033	-8.100555604278698	1771
6080ff4afca9da10cdfdbb7de0249572424504c4	building statistical atlas of white matter fiber tract based on vector/tensor field reconstruction in diffusion tensor mri	diffusion tensor parameter;statistical atlas;white matter fiber tract;interactive roi setting;modeling method;diffusion tensor tractography;fiber tract modeling method;tract structure;key role;tensor field reconstruction;diffusion tensor;probability density	The diffusion tensor tractography has drawbacks such as low objectivity by interactive ROI setting and fiber-crossing. For coping with such problems, we are constructing a statistical atlas of white matter fiber tracts, in which probability density maps of tract structures are stored with diffusion tensor parameters on spatially normalized brain data. In building the atlas, our fiber tract modeling method plays a key role, which is based on a novel approach of vector/tensor field reconstruction avoiding fiber-crossings. In this abstract, we describe the modeling method, our statistical atlas, and the preliminary results.	tract (literature)	Yoshitaka Masutani;Shigeki Aoki;Osamu Abe;Mariko Yoshida;Haruyasu Yamada;Harushi Mori;Kenji Ino;Kuni Ohtomo	2005		10.1007/11595755_11	computer vision;normalization (statistics);artificial intelligence;computer science;tensor field;probability density function;pattern recognition;white matter;atlas (anatomy);diffusion mri;tractography	Vision	44.305073042352014	-78.14726657073541	1780
38807ea88de439a8ed6333115ec0d7ffb137dbae	projective rectification-based view interpolation for multiview video coding and free viewpoint generation	horizontal line;free viewpoint video;interpolation;image coding;free viewpoint generation;cost function;video signal processing;projective rectification based view interpolation algorithm;image matching;view interpolation;visual communication;video compression;fundamental matrix;video coding image matching interpolation matrix algebra;interpolation video coding cameras signal processing algorithms computer displays video compression cost function video signal processing visual communication layout;layout;matrix algebra;view rectification multiview video coding view interpolation;view rectification;multiview video coding;video coding;estimation;pixel;computer displays;fundamental matrix projective rectification based view interpolation algorithm multiview video coding free viewpoint generation matched epipolar line horizontal line camera;signal processing algorithms;encoding;matched epipolar line;cameras;camera	A projective rectification-based view interpolation algorithm is developed for multiview video coding and free viewpoint video. It first calculates the fundamental matrix between two views without using any camera parameter. The two views are then resampled to have horizontal and matched epipolar lines. One-dimensional disparity is estimated next, which is used to interpolate the image for an intermediate viewpoint. After unrectification, the interpolated view can be displayed directly for free viewpoint video purpose. It can also be used as a reference to encode data of an intermediate camera. Experimental results show that the interpolated views can be 3 dB better than existing method. Video coding results illustrate that the method can provide up to 1.3 dB improvement over JMVC.	algorithm;binocular disparity;data compression;decibel;encode;epipolar geometry;fundamental matrix (computer vision);image rectification;interpolation;multiview video coding	Xiaoyu Xiu;Jie Liang	2009	2009 Picture Coding Symposium	10.1109/PCS.2009.5167348	data compression;layout;computer vision;estimation;horizontal line test;interpolation;computer science;mathematics;multimedia;fundamental matrix;pixel;encoding;statistics;visual communication;multiview video coding;computer graphics (images)	Vision	44.91529702955587	-18.37426293195515	1783
1b28c64b56ed8d231b04ec14cc13a59f6ccb9226	a fast and robust algorithm for fighting behavior detection based on motion vectors	motion vectors;fighting behavior;average maximum violence index;image analysis;surveillance scene	In this paper, we propose a fast and robust algorithm for fighting behavior detection based on Motion Vectors (MV), in order to solve the problem of low speed and weak robustness in traditional fighting behavior detection. Firstly, we analyze the characteristics of fighting scenes and activities, and then use motion estimation algorithm based on block-matching to calculate MV of motion regions. Secondly, we extract features from magnitudes and directions of MV, and normalize these features by using Joint Gaussian Membership Function, and then fuse these features by using weighted arithmetic average method. Finally, we present the conception of Average Maximum Violence Index (AMVI) to judge the fighting behavior in surveillance scenes. Experiments show that the new algorithm achieves high speed and strong robustness for fighting behavior detection in surveillance scenes.	algorithm	Jianbin Xie;Wei Yan;Peiqin Li;Zhaowen Zhuang	2011	TIIS	10.3837/tiis.2011.11.018	computer vision;image analysis;simulation;computer science;artificial intelligence	Robotics	43.28033230913801	-44.667731920693974	1784
1434ee22eca85baca5bbff31013402b0bc872f9b	mrf solutions for probabilistic optical flow formulations	optimisation;nonlinear optics;image motion analysis;probability;maximum flow;performance evaluation;uncertainty;venus;optical filters;optimisation image sequences motion estimation image representation computer vision markov processes probability;markov random fields;image sequence optical flow probability brightness image representation motion estimation markov random fields linear clique maximum flow;motion estimation;least squares approximation;linear clique;computer vision;brightness;markov ran dom field;image representation;image sequence;image motion analysis brightness nonlinear optics uncertainty motion estimation optical filters computer vision optical devices least squares approximation venus;optical flow;markov processes;iteration method;optical devices;image sequences	In this paper we propose an efficient, non-iterative method for estimating optical flow. We develop a probabilistic framework that is appropriate for describing the inherent uncertainty in the brightness constraint due to errors in image derivative computation. We separate the flow into two one-dimensional representations and pose the problem of flow estimation as one of solving for the most probable configuration of one-dimensional labels in an Markov Random Fields (MRF) with linear clique potentials. The global optimum for this problem can be efficiently solved for using the maxflow computation in a graph. We develop this formulation and describe how the use of the probabilistic framework, the parametrisation and the MRF formulation together enables us to capture the desirable properties for flow estimation, especially preserving motion discontinuities. We demonstrate the performance of our algorithm and compare our results with that of other algorithms described in the performance evaluation paper of Barron et. al [2].	algorithm;computation;global optimization;iterative method;markov chain;markov random field;maximum flow problem;optical flow;performance evaluation;statistical model	Sébastien Roy;Venu Madhav Govindu	2000		10.1109/ICPR.2000.903724	nonlinear optics;maximum flow problem;computer vision;mathematical optimization;uncertainty;computer science;machine learning;motion estimation;probability;optical flow;optical filter;mathematics;iterative method;markov process;least squares;brightness;statistics	Vision	53.6693089253974	-54.3644623478056	1792
ac7d281b2f6c8dd507035d3c68d5fd763ae10010	theoretical and empirical investigations on difficulty in structure learning by estimation of distribution algorithms	structure learning;probability;search space;search problems genetic algorithms learning artificial intelligence probability;search space structure learning estimation of distribution algorithm evolutionary algorithm genetic algorithm probabilistic model;probabilistic model;estimation of distribution algorithm;genetic algorithm;electronic design automation and methodology genetic algorithms evolutionary computation probability distribution encoding cybernetics genetic mutations sampling methods buildings information science;genetic algorithms;search problems;evolutionary algorithm;learning artificial intelligence	Estimation of distribution algorithms (EDAs) are population based evolutionary algorithms derived from genetic algorithms (GAs) . EDAs build probabilistic models of promising solutions to guide further exploration of the search space. They have been considered to behave in similar way to GAs. In this paper, we show their different behaviors and difficulties in applications of EDAs by designing an EDA difficult function in which schemata that are not consistent with problem structure sometimes overwhelm those that are.	electronic design automation;estimation of distribution algorithm;evolutionary algorithm;genetic algorithm	Miwako Tsuji;Masaharu Munetomo;Kiyoshi Akama	2006	2006 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2006.384384	quality control and genetic algorithms;genetic programming;mathematical optimization;probabilistic analysis of algorithms;genetic algorithm;estimation of distribution algorithm;cultural algorithm;computer science;artificial intelligence;theoretical computer science;machine learning;evolutionary algorithm;genetic representation;memetic algorithm;intelligent control;evolutionary computation;search algorithm;population-based incremental learning	Robotics	27.77615490560719	-8.57866186543615	1795
f5e6e8964d8615f2502f53d2b2303c6b968660f1	x-ray fluorescence computed tomography with polycapillary focusing	liposomal iodine nanoparticles polycapillary lens x ray fluorescence ct image reconstruction;fluorescence x ray imaging nanoparticles image reconstruction biomedical image processing probes computed tomography;biomedical optical imaging computerised tomography diagnostic radiography diseases fluorescence image reconstruction iodine nanomedicine nanoparticles numerical analysis patient diagnosis x ray fluorescence analysis;image reconstruction liposomal iodine nanoparticles polycapillary lens x ray fluorescence ct;x ray fluorescence analysis biomedical optical imaging computerised tomography diagnostic radiography diseases fluorescence image reconstruction iodine nanomedicine nanoparticles numerical analysis patient diagnosis;polycapillary focusing imaging performance x ray fluorescence signals focal spot x ray cones submillimeter focal spot polycapillary lens roi region of interest reconstruction x ray fluorescence computed tomography method iodine concentration distribution imaging fluorescence yield photoelectric absorption human diseases patient therapy patient diagnosis molecular probes intravascular contrast liposomal iodine nanoparticles;imaging performance x ray fluorescence signals focal spot x ray cones submillimeter focal spot polycapillary lens roi region of interest reconstruction x ray fluorescence computed tomography method iodine concentration distribution imaging fluorescence yield photoelectric absorption human diseases patient therapy patient diagnosis molecular probes intravascular contrast liposomal iodine nanoparticles polycapillary focusing	Liposomal iodine nanoparticles (LINPs) have a long half-life and provide an excellent intravascular contrast. The nanoparticles can be functionalized as molecular probes for biological targets to facilitate numerous preclinical studies for translation toward diagnosis and therapy of various human diseases. Iodine has a K-edge at 33 keV due to the photoelectric absorption of photons, which emit X-ray fluorescence at 28 keV with a fluorescence yield of 0.88. Detections of the characteristic X-rays can be used for the imaging of iodine concentration distribution in an object. In this paper, we propose an X-ray fluorescence computed tomography method for reconstruction of a LINPs distribution over a region of interest (ROI) in a small animal. X-rays are focused onto a submillimeter focal spot utilizing a polycapillary lens, generating a pair of X-ray cones in the animal. This focused beam irradiates LINPs, the most strongly at the focal spot. Then, the focal spot can be scanned over an ROI in the object to produce X-ray fluorescence signals. From measured fluorescence data, a reliable image reconstruction can be achieved with a high spatial resolution. Numerical simulation studies are performed to demonstrate the superior imaging performance of this methodology.	ct scan;cobalt oxide nanoparticles;focal (programming language);iterative reconstruction;numerical method;photoelectric effect;region of interest;sensor;simulation;tomography	Wenxiang Cong;Yan Xi;Ge Wang	2014	IEEE Access	10.1109/ACCESS.2014.2359831	computed tomography;computer science;distributed computing;analytical chemistry;x-ray fluorescence	Vision	46.74218832836769	-84.97878579754914	1796
363777481b142a4bd6e9eb811c607106fdd1addd	an output tracking control strategy for unknown nonlinear systems	invariant set;state space;tracking control;dynamic neural network;nonlinear system;neural network	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	artificial neural network;control theory;francis;neural network software;nonlinear system;optimal control;primary source;quadratic equation;setpoint (control system);simulation	Zhongjing Wang;Mei Qing	2004	Applied Artificial Intelligence	10.1080/08839510490250060	nonlinear system;computer science;state space;machine learning	Robotics	50.79749847949986	-3.395921965316977	1797
71b9bf9ea9df7d8390cc979037609ffe8fc3a1b6	applying a hybrid targeted estimation of distribution algorithm to feature selection problems		This paper presents the results of applying a hybrid Targeted Estimation of Distribution Algorithm (TEDA) to feature selection problems with 500 to 20,000 features. TEDA uses parent fitness and features to provide a target for the number of features required for classification and can quickly drive down the size of the selected feature set even when the initial feature set is relatively large. TEDA is a hybrid algorithm that transitions between the selection and crossover approaches of a Genetic Algorithm (GA) and those of an Estimation of Distribution Algorithm (EDA) based on the reliability of the estimated probability distribution. Targeting the number of features in this way has two key benefits. Firstly, it enables TEDA to efficiently find good solutions for cases with low signal to noise ratios where the majority of available features are not associated with the given classification task. Secondly, due to the tendency of TEDA to select the smallest promising feature sets, it builds compact classifiers and is able to evaluate populations more quickly than other approaches.	dexter (malware);estimation of distribution algorithm;feature selection;fitness function;flying-spot scanner;genetic algorithm;hybrid algorithm;population;software release life cycle	Geoffrey Neumann;David Cairns	2013		10.5220/0004553301360143	mathematical optimization;machine learning;pattern recognition	Metrics	10.111658280745425	-43.393176385941736	1799
dacd0794d7cc7d55ff4f815e94ca503f3fd9450f	research on the comprehensive traffic state evaluation model linked with drivers’ perception under the vehicle networking	t s model;anfis;driver s perception;traffic state evaluation;vehicle networking	Unified judgment standards and methods are often adopted to identify traffic state in certain road network based on traffic flow parameters. However, drivers often have different perceptions about the traffic state on different road sections, since their expectations on traffic state vary more or less from each other on different road sections. In particular, under the vehicle networking, out of considerations for safety and other relevant factors, requirements for the correlation and coordination of running vehicles have also raised significantly. Therefore, it is necessary to take driver’s perception about the driving conditions of certain road sections into consideration to adjust the release of traffic state. This paper has provided a comprehensive traffic state evaluation model linked with driver’s perception under the vehicle networking. The authors first establish an ANFIS model based on the T–S model and then conduct statistical analysis on drivers’ perceptions about certain traffic state. At last, the authors use the results of statistical analysis as regulatory factors to amend the parameters input through ANFIS. Through simulation, this paper has demonstrated that the model established has a high rate of convergence, a high identification precision and the generalization ability to conduct researches on the identification of traffic state.	adaptive neuro fuzzy inference system;algorithm;futures studies;iteration;library (computing);machine learning;mean squared error;network congestion;rate of convergence;requirement;simulation	Guang-Yu Zhu;Gao-na Lu;Chen-Guang Yang;Peng Zhang	2014	Personal and Ubiquitous Computing	10.1007/s00779-014-0801-4	simulation;adaptive neuro fuzzy inference system;computer science;artificial intelligence;computer security	Metrics	8.438459259295653	-13.275765618473605	1803
ad4b4fd51e711f190d1537dc80ec88ca0af0c151	fuzzy systems for multicriteria decision making	multicriteria decision making;fuzzy system	One of the techniques used to support decisions in uncertain environments is the Fuzzy TOPSIS method. However, from crisp data, this method considers only one fuzzy set in their analysis, besides being a strictly mathematical optimization technique. This article proposes extensions to the original Fuzzy TOPSIS, exploring two distinct versions: to increase the method with the necessary resources for the mathematics process to consider the membership values of the input data in more than one fuzzy set and to aggregate to method the empiric knowledge of an expert represented through fuzzy rules. In such case, the method, named by Fuzzy F-TOPSIS (Fuzzy Flexible TOPSIS), is proposed with the objective of improving the Fuzzy TOPSIS ability to deal with uncertainty through the combination of the mathematical process involved in the original Fuzzy TOPSIS with the expert empirical knowledge. A case study is presented to validate the proposal.	aggregate data;data (computing);decision support system;experiment;fuzzy concept;fuzzy control system;fuzzy set;mathematical optimization;norm (social);rule-based system;tops	Fabio J. J. Santos;Heloisa A. Camargo	2010	CLEI Electron. J.		membership function;defuzzification;adaptive neuro fuzzy inference system;fuzzy transportation;type-2 fuzzy sets and systems;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;data mining;fuzzy associative matrix;fuzzy set operations;fuzzy control system	AI	-3.200298225410162	-20.591522562284307	1805
49f1a5790eccbdec2ca27931b050e361634fac17	a local search embedded genetic algorithm for simplified protein structure prediction	lattice model hybrid algorithm local search genetic algorithm protein structure prediction hp model;artificial intelligence and image processing not elsewhere classified;institute for integrated and intelligent systems;conference output;faculty of science environment engineering and technology;hydrophobicity;psp local search embedded genetic algorithm protein structure prediction tabu based local search population based genetic algorithm low resolution ab initio search method hydrophobic polar energy model face centred cubic lattice search stagnation randomly generated individuals random walk randomised solution improvement;proteins;random processes;genetic algorithms;search problems;amino acids proteins genetic algorithms sociology statistics lattices prediction algorithms;search problems genetic algorithms hydrophobicity proteins random processes;artificial intelligence and image processing;080199	No single algorithm suits the best for the protein structure prediction problem. Therefore, researchers have tried hybrid techniques to mix the power of different strategies to gain improvements. In this paper, we present a hybrid search framework that embeds a tabu-based local search within a population based genetic algorithm. We applied our hybrid algorithm on simplified protein structure prediction problem. We use a low-resolution ab initio search method with the hydrophobic-polar energy model and face-centred-cubic lattice. Within the genetic algorithm, we apply local search in two different situations: i) only once at the beginning and ii) every time at search stagnation. At the beginning, we apply local search to improve the randomly generated individuals and use them as an initial population for the genetic algorithm. Later, we apply local search after applying a random-walk at situations where the genetic algorithm gets stuck. In both cases, the use of local search is to improve the randomised solutions quickly. We experimentally show that our hybrid approach outperforms the state-of-the-art approaches.	ab initio quantum chemistry methods;cubic function;embedded system;experiment;genetic algorithm;hybrid algorithm;local search (optimization);memetic algorithm;procedural generation;protein structure prediction;tabu search	Mahmood A. Rashid;M. A. Hakim Newton;Tamjidul Hoque;Abdul Sattar	2013	2013 IEEE Congress on Evolutionary Computation	10.1109/CEC.2013.6557688	beam search;stochastic process;mathematical optimization;genetic algorithm;cultural algorithm;beam stack search;tabu search;computer science;bioinformatics;artificial intelligence;local search;hill climbing;machine learning;hydrophobe;iterated local search;mathematics;incremental heuristic search;best-first search;difference-map algorithm;statistics;guided local search;binary search algorithm;search algorithm;population-based incremental learning	AI	26.911896966818205	-2.0043055317143397	1808
a09d5070bddf51c8dc0d326692bd86811165d7e1	design of electric simulation system based on dsp for modular dual-joint of space robot	mobile robots aerospace robotics control engineering computing digital signal processing chips;hardware circuit platform;digital signal processing orbital robotics computational modeling space missions circuit simulation system testing computer simulation digital signal processing chips computer interfaces circuit testing;can bus;circuit design;mobile robots;chip;digital signal processor dsp;modular dual joint;digital signal processor dsp joint electric simulation system jess modular dual joint space robot can bus;joint electric simulation system jess;space robotics;aerospace robotics;mathematical model;space robot;digital signal processor;space robot joints;digital signal processing chips;dynamic simulation;control engineering computing;electric simulation system design;circuit design electric simulation system design modular dual joint space robot joints hardware circuit platform dsp chip;dsp chip	This paper studies and designs a joint electric simulation system (JESS), which has been used for the simulation of testing and communicating of the space robot joints. In the design of the JESS, a hardware circuit platform based on the core of DSP chip manufactured by TI Corp is designed and implemented, and it completes the circuit design of three routes of CAN BUS. To increase the reliability of CAN BUS, the two routes of CAN BUS that communicate with the central control computer adopt redundancy technique for each other. Furthermore, the mathematic model of motional mechanism of the space robot joints has been established and transformed as software codes to run in the actual joint controller and simulate the motion conditions of the space robot's actual joints. As the most important component of the electric testing simulation system of the space robot, the JESS participates in testing of the space robot system with dynamical simulation computer and the central computer, detects the correctness of communicating information among the electric components and interfaces of the space robot system, and simulates the space robot on-orbit mission capability.	algorithm;can bus;circuit design;code;complex programmable logic device;correctness (computer science);digital signal processing;digital signal processor;dynamical simulation;electronic circuit;logic control;robot;servo;system on a chip	Xiang Chen;Hanxu Sun;Qingxuan Jia;Ping Ye;Wei Xi	2006	2006 IEEE International Conference on Robotics and Biomimetics	10.1109/ROBIO.2006.340321	embedded system;dynamic simulation;digital signal processor;electronic engineering;simulation;computer science;engineering	Robotics	64.47386412362897	-28.84222338944916	1809
c1e576af3399a5770045ffe014c1f318342307d7	application of a novel tool for diagnosing bile acid diarrhoea	sensitivity and specificity;female;bile acid diarrhea bad;fermentome;middle aged;male;bile acids and salts;diarrhea;faims;diagnosis computer assisted;colitis ulcerative;adult;bile acid malabsoprtion bam;volatile organic compounds;reproducibility of results;algorithms;steatorrhea;humans;electronic nose;r medicine general;aged	Bile acid diarrhoea (BAD) is a common disease that requires expensive imaging to diagnose. We have tested the efficacy of a new method to identify BAD, based on the detection of differences in volatile organic compounds (VOC) in urine headspace of BAD vs. ulcerative colitis and healthy controls. A total of 110 patients were recruited; 23 with BAD, 42 with ulcerative colitis (UC) and 45 controls. Patients with BAD also received standard imaging (Se75HCAT) for confirmation. Urine samples were collected and the headspace analysed using an AlphaMOS Fox 4000 electronic nose in combination with an Owlstone Lonestar Field Asymmetric Ion Mobility Spectrometer (FAIMS). A subset was also tested by gas chromatography, mass spectrometry (GCMS). Linear Discriminant Analysis (LDA) was used to explore both the electronic nose and FAIMS data. LDA showed statistical differences between the groups, with reclassification success rates (using an n-1 approach) at typically 83%. GCMS experiments confirmed these results and showed that patients with BAD had two chemical compounds, 2-propanol and acetamide, that were either not present or were in much reduced quantities in the ulcerative colitis and control samples. We believe that this work may lead to a new tool to diagnose BAD, which is cheaper, quicker and easier that current methods.	anterior descending branch of left coronary artery;antivirus software;bile acid measurement;bile fluid;diarrhea;electronic nose;epistaxis;experiment;flatulence;foxes;gas chromatography;gas chromatography-mass spectrometry;halitosis;instrument - device;ions;iontophoresis;isopropyl alcohol;linear discriminant analysis;microsoft outlook for mac;organic chemicals;patients;programming paradigm;quantity;subgroup;thioctic acid;uc browser;ulcer;ulcerative colitis;volatile organic compounds;weatherstar;acetamide;confirmation - responselevel;rapid diagnosis	James A. Covington;Eric W. Westenbrink;Nathalie Ouaret;Ruth Harbord;Catherine Bailey;Nicola O'Connell;James Cullis;Nigel Williams;Chuka U. Nwokolo;Karna D. Bardhan;Ramesh P. Arasaradnam	2013		10.3390/s130911899	electronic nose;chemistry;computer science	Comp.	11.930123285399041	-64.43172867450438	1811
e9315d5aeeaa1beb745f604abeef0925c988ce99	solar generation prediction using the arma model in a laboratory-level micro-grid	distributed power generation;autoregressive moving average processes;medium term solar forecasting solar generation prediction arma model laboratory level microgrid solar generation forecasting model ucla smart grid energy research center smerc test platform solar forecasting providers autoregressive moving average model historical solar radiation data system advisor model sam historical solar generation data system identification toolbox matlab platform error analysis;solar power stations;solar power stations autoregressive moving average processes distributed power generation error analysis load forecasting smart power grids;load forecasting;forecasting predictive models mathematical model data models laboratories solar radiation satellites;error analysis;smart power grids	The goal of this article is to investigate and research solar generation forecasting in a laboratory-level micro-grid, using the UCLA Smart Grid Energy Research Center (SMERC) as the test platform. The article presents an overview of the existing solar forecasting models and provides an evaluation of various solar forecasting providers. The auto-regressive moving average (ARMA) model and the persistence model are used to predict the future solar generation within the vicinity of UCLA. In the forecasting procedures, the historical solar radiation data originates from SolarAnywhere. System Advisor Model (SAM) is applied to obtain the historical solar generation data, with inputting the data from SolarAnywhere. In order to validate the solar forecasting models, simulations in the System Identification Toolbox, Matlab platform are performed. The forecasting results with error analysis indicate that the ARMA model excels at short and medium term solar forecasting, whereas the persistence model performs well only under very short duration.	error analysis (mathematics);matlab;persistence (computer science);simulation;system identification	Rui Huang;Tiana Huang;Rajit Gadh;Na Li	2012	2012 IEEE Third International Conference on Smart Grid Communications (SmartGridComm)	10.1109/SmartGridComm.2012.6486039	meteorology;simulation;engineering;artificial intelligence	Robotics	9.483879770875575	-18.211142607697568	1813
fbdf5bd0b695803419cd96635f92122393ae23e3	improving multiclass ilp by combining partial rules with winnow algorithm: results on classification of dopamine antagonist molecules	partial match;multiclass problem;winnow algorithm;original rule;suitable class;partial rule;dopamine antagonist molecule;10-fold cross validation;new example;multiclass real-world problem;important partial match;whole rule	In this paper, we propose an approach which can improve Inductive Logic Programming in multiclass problems. This approach is based on the idea that if a whole rule cannot be applied to an example, some partial matches of the rule can be useful. The most suitable class should be the class whose important partial matches cover the example more than those from other classes. Hence, the partial matches of the rule, called partial rules, are first extracted from the original rules. Then, we utilize the idea of Winnow algorithm to weight each partial rule. Finally, the partial rules and the weights are combined and used to classify new examples. The weights of partial rules also show another aspect of the knowledge which can be discovered from the data set. In the experiments, we apply our approach to a multiclass real-world problem, classification of dopamine antagonist molecules. The experimental results show that the proposed method gives the improvement over the original rules and yields 88.58% accuracy by running 10-fold cross validation.	algorithm;c4.5 algorithm;cross-validation (statistics);dopamine;experiment;inductive logic programming;method (computer programming);multiclass classification;winnow (algorithm)	Sukree Sinthupinyo;Cholwich Nattee;Masayuki Numao;Takashi Okada;Boonserm Kijsirikul	2004		10.1007/978-3-540-71009-7_50	machine learning;data mining;mathematics;algorithm	ML	9.840567163915807	-46.241379097989345	1814
56b406f3e00148b0268c3bcf0770340a4525b41c	autokey: human assisted key extraction	partial differential equation;spline;human interaction;image composition;color space;edge detection;efficient algorithm;motion estimation;key;data representation;inverse problem;object extraction;alpha value;block matching;image compositing	Key extraction is an inverse problem of finding the foreground, the background, and the alpha from an image and some hints. Although the chromakey solves this for a limited case (single background color), this is often too restrictive in practical situations. When the extraction from arbitrary background is necessary, this is currently done by a time consuming manual task. In order to reduce the operator load, attempts have been made to assist operators using either color space or image space information. However, existing approaches have their limitations. Especially, they leave too much work to operators. In this paper, we present a key extraction algorithm which for the first time, addresses the problem quantitatively. We first derive a partial differential equation that relates the gradient of an image to the alpha values. We then describe an efficient algorithm that provides the alpha values as the solution of the equation. Along with our accurate motion estimation technique, it produces correct alpha values almost everywhere, leaving little work to operators. We also show that a careful design of the algorithm and the data representation greatly improves human interaction. At every step of the algorithm, human interaction is possible and it is intuitive. CR Categories: I.3.3 [Computer Graphics]: Picture / Image Generation; I.4.6 [Image Processing]: Segmentation Edge and feature detection; I.4.7 [Image Processing]: Feature Measurement; I.5.2 [Pattern Recognition]: Design Methodology Feature evaluation and selection. Additional	algorithm;color space;computation;computer graphics;data (computing);feature detection (computer vision);feature detection (web development);gradient;image processing;key frame;morphing;motion estimation;pattern recognition;signal-to-noise ratio;user interface;web colors	Tomoo Mitsunaga;Taku Yokoyama;Takashi Totsuka	1995		10.1145/218380.218450	spline;computer vision;mathematical optimization;interpersonal relationship;edge detection;computer science;inverse problem;pattern recognition;motion estimation;mathematics;external data representation;key;color space;partial differential equation	Graphics	53.16957423241175	-62.886965175481436	1816
d735984e0be5e89dba91473202d3b9ccbea58b6b	automatic segmentation of newborn brain mri using mathematical morphology	mathematical morphology;biological tissues;brain;white matter;image segmentation;newborns brain mri automatic segmentation mathematical morphology signal to noise ratio intracranial cavity cerebellum brainstem cortical gray matter central gray matter myelinated white matter unmyelinated white matter cerebrospinal fluid manual segmentations;image segmentation pediatrics magnetic resonance imaging manuals brain biomedical imaging morphology;automatic segmentation;magnetic resonance image;paediatrics;medical image processing;paediatrics biological tissues biomedical mri brain image segmentation mathematical morphology medical image processing;mathematical morphology image segmentation magnetic resonance imaging brain imaging newborns;brain structure;gray matter;brain imaging;signal to noise ratio;biomedical mri;cerebrospinal fluid	We propose a novel algorithm for the segmentation of newborn brain MRI, based on mathematical morphology. The algorithm combines image information with high-level anatomical knowledge to deal with the difficulties of newborn brain MRI segmentation (lower signal-to-noise ratio, reduced contrast and different brain structure compared to the adult brain). It robustly segments the brain globally (intracranial cavity, cerebellum, brainstem and the two hemispheres) and at tissue level (cortical gray matter, central gray matter, myelinated white matter, unmyelinated white matter and cerebrospinal fluid). Important advantages compared to existing methods are that the proposed algorithm does not require any manual interaction and that it does not require an atlas, whose construction would be tedious and time-consuming. Experimental results show good agreement with expert manual segmentations and qualitative superiority to state-of-the-art methods in the literature.	algorithm;high- and low-level;mathematical morphology;signal-to-noise ratio	Laura Gui;Radoslaw Lisowski;Tamara Faundez;Petra S. Huppi;François Lazeyras;Michel Kocher	2011	2011 IEEE International Symposium on Biomedical Imaging: From Nano to Macro	10.1109/ISBI.2011.5872810	computer vision;mathematical morphology;radiology;medicine;pathology;computer science;magnetic resonance imaging;image segmentation;signal-to-noise ratio;anatomy;neuroimaging	Vision	41.935142845771544	-78.5932098235258	1818
fda5a43b77f4b9d70240c2ec88829d2e375d2548	edge and corner preserving smoothing for artistic imaging	texture;image processing;0705p;imagen fija;etude experimentale;0130c;photography;photographic image;0768;traitement image;photographie;fixed image;imagen borrosa;image photographique;smoothing;blurred image;imaging;imagen fotografica;formation image;image fixe;image floue;4230v	What visually distinguishes a painting from a photograph is often the absence of texture and the sharp edges: in many paintings, edges are sharper than in photographic images while textured areas contain less detail. Such artistic effects can be achieved by filters that smooth textured areas while preserving, or enhancing, edges and comers. However, not all edge preserving smoothers are suitable for artistic imaging. This study presents a generalization of the well know Kuwahara filter aimed at obtaining an artistic effect. Theoretical limitations of the Kuwahara filter are discussed and solved by the new nonlinear operator proposed here. Experimental results show that the proposed operator produces painting-like output images and is robust to corruption of the input image such as blurring. Comparison with existing techniques shows situations where traditional edge preserving smoothers that are commonly used for artistic imaging fail while our approach produces good results.	smoothing	Giuseppe Papari;Nicolai Petkov;Patrizio Campisi	2007		10.1117/12.702864	computer vision;image processing;photography;optics;texture;smoothing;computer graphics (images)	Vision	58.76155519462478	-60.83982943611005	1827
9c94c54ee0fb64f27844efd93e0aed0d9b8ccea7	mobile stigmergic markers for navigation in a heterogeneous robotic swarm	self organization;infrared	We study self-organized navigation in a heterogeneous robotic swarm consisting of two types of robots: small wheeled robots, called foot-bots, and flying robots that can attach to the ceiling, called eye-bots. The task of foot-bots is to navigate back and forth between a source and a target location. The eye-bots are placed in a chain on the ceiling, connecting source and target using infrared communication. Their task is to guide foot-bots, by giving local directional instructions. The problem we address is how the positions of eye-bots and the directional instructions they give can be adapted, so that they indicate a path that is efficient for foot-bot navigation, also in the presence of obstacles. We propose an approach of mutual adaptation, where foot-bots move according to eye-bot instructions, and eye-bots move according to observed foot-bot behavior. Our solution is inspired by pheromone based navigation of ants, as eye-bots serve as mobile stigmergic markers for foot-bot navigation. We evaluate the system’s performance in a range of simulation experiments.	bell test experiments;dos;experiment;mobile robot;pathfinding;robot;self-organization;simulation;stigmergy;swarm robotics	Frederick Ducatelle;Gianni A. Di Caro;Alexander Förster;Luca Maria Gambardella	2010		10.1007/978-3-642-15461-4_44	computer vision;self-organization;simulation;infrared;computer science;mobile robot navigation	Robotics	56.880542809241845	-23.809899026540258	1828
4bc86a4df15df83728cad555027df68a3803cc09	construction of a mean surface for the variability study of the cornea	mean model;interpolation;medical imaging technique mean surface variability study human cornea corneal surface mean cornea global factor volume minimization surface disparity iterative algorithm disparity map mean disparity value registration accuracy;surface topography;surface morphology;accuracy;image color analysis;surface topography interpolation cornea surface morphology image color analysis accuracy;atlas;geometric modeling;elevation model;cornea geometric modeling mean model elevation model atlas;cornea;medical image processing image registration iterative methods	In this study, we present an algorithm to build a mean surface, applied to the human cornea, for the study of variability within a population. Due to the smoothness of the corneal surface, there is no anatomical anchor. The main challenge is to match several surfaces from different subjects to build the mean cornea. The key idea is to use a registration step based on a global factor: the volume minimization between two surfaces. We then compute the surface disparity after registration. An iterative algorithm minimizes this disparity to determine the best possible matching. The algorithm re-samples the registered surfaces on a common grid to compute the mean surface. Finally, we compute a disparity map and a mean disparity value after registration to estimate the registration accuracy and to compare our method to the existing one.	algorithm;amplifier;bean scripting framework;binocular disparity;heart rate variability;iterative method;tomography	Arnaud Polette;Edouard Auvinet;Jean-Luc Mari;Isabelle Brunette;Jean Meunier	2014	2014 Canadian Conference on Computer and Robot Vision	10.1109/CRV.2014.51	computer vision;interpolation;geometric modeling;atlas;accuracy and precision;statistics	Vision	45.569733818022065	-77.40266886307606	1831
7bca33f9f85aa4289f886f5faabdcf88581e3ffd	robust quaternion-based nonlinear output feedback control of a quadrotor hover system		A robust nonlinear output feedback control method is presented, which achieves three degree of freedom (3-DOF) attitude control of a hover system test bed. The proposed control method formally incorporates the practical limitations in the voltage control inputs to the control actuators (i.e., the quadrotor propellers). In addition, the control law is designed to compensate for uncertainty in the hover system dynamic model, including input-multiplicative parametric uncertainty resulting from uncertain drag and friction coefficients in the propellers' dynamic model. To reduce the computational requirement in the closed-loop system, constant feedforward estimates of the input-multiplicative uncertainty are utilized in lieu of adaptive parameter estimates. Eschewing the high-gain feedback requirement that is characteristic of standard sliding mode observer methods, the proposed control method utilizes a bank of dynamic filters, which operates as a velocity estimator in the closed-loop system. A rigorous error system development and Lyapunov-based stability analysis are presented to prove that the proposed output feedback control law achieves asymptotic 3-DOF attitude control in the presence of parametric input uncertainty and unmodeled dynamics. Experimental results are provided to demonstrate the performance of the attitude control method using the Quanser 3-DOF hover system test bed.	block cipher mode of operation;coefficient;encoder;feedback;feedforward neural network;lyapunov fractal;mathematical model;nonlinear system;optimal control;system dynamics;system testing;testbed;velocity (software development)	Derek Hoffman;Muhammad Rehan;William MacKunis;Mahmut Reyhanoglu	2017	2017 IEEE 56th Annual Conference on Decision and Control (CDC)	10.1109/CDC.2017.8264379	degrees of freedom (statistics);estimator;computer science;attitude control;drag;control theory;feed forward;system testing;nonlinear system;parametric statistics	Robotics	65.96029741000066	-10.518947670551206	1832
bcd2cee66ec6750aa5bea17b135e373600201f3d	study on human slip and fall gaits based on 3d gait analysis system	gait;computer;body response coordination ability;3d gait analysis system;slip and fall probability;total time	Slip and fall is a serious problem which affects the health and safety of people, and it has become a hot topic in the ergonomics and biomedicine fields in recent years. The causes of slip and fall accidents including external causes and internal causes. And it is the body response coordination ability under the condition of instability that is one of the important internal causes and plays a key role in causing slip and fall accidents. On the sports psychology, total time (TT) is defined as the sum of reaction time and movement time and it can be used to measure the body response coordination ability. Slip and fall probability (FP) is the frequency of occurrence of slip and fall accidents. When external conditions are consistent, to a certain extent, different FP reflects the difference of body response coordination ability. Theoretically, TT and FP should have a certain relationship, but the detail is unknown. With the development of computer technology, the 3D gaits analysis system has appeared and the study of slip and fall accidents was promoted depending on its powerful functions. Based on the 3D gaits analysis system, this paper innovatively listed the topic as study content and got the study result: the relationship between TT and FP is significant correlation under the 0.01 level. By using the datum, images and videos exported from the system, this paper conducted the gait analysis and verified the reliability of the correlation: different TT lead to different foot-ground contact force, thus lead to different body response coordination ability, namely FP. Therefore, it is very effective to use the 3D gait analysis system to study the slip and fall accidents.	computer;experiment;gait analysis;geodetic datum;human factors and ergonomics;instability;slip (programming language);spss	Junxia Zhang;Na Yin;Juan Ge	2014	Journal of Multimedia	10.4304/jmm.9.3.356-362	simulation;gait	HCI	12.503742941200013	-80.8444195292612	1838
d371eaf881734754d6c228dc2659fd83269c82eb	speeding up fractal image compression by working in karhunen-loeve transform space	fractals image coding karhunen loeve transforms neural networks quantization bismuth usa councils principal component analysis discrete cosine transforms decoding;quantization;fractals;image coding;neural networks;data compression;decoding;neural nets;bismuth;fractal image;generalisation artificial intelligence data compression image coding fractals karhunen loeve transforms neural nets;usa councils;feature vector;karhunen loeve transforms;image compression;discrete cosine transforms;principal component analysis;kd tree;generalisation artificial intelligence;transform coefficients;network generality fractal image image compression karhunen loeve transform encoding neural network feature vector quantization transform coefficients;encoding;karhunen loeve transform;network generality;neural network;fractal image compression	The main weakness of fractal image compression is its long encoding time needed to search the entire domain pool to find the best domain-range mapping. To solve the problem, some solutions were proposed but most of them do not employ neural networks (only the use of Kohonen SOM for clustering was reported). The paper proposes a new method based on Karhunen-Loeve transform (PCA networks), which attempts to use neural networks' well-known adaptability in order to find a good feature vector for a block. Performance regarding network generality, quantization of the transform coefficients, comparison with DCT and kd-tree search, was explored. Results prove that the proposed method slightly outperforms state-of-the-art methods.	fractal compression;image compression	Macarie Breazu;Gavril Toderean;Daniel Volovici;Mihaela Iridon	1999		10.1109/IJCNN.1999.833504	data compression;feature vector;quantization;fractal;image compression;computer science;theoretical computer science;machine learning;bismuth;pattern recognition;k-d tree;mathematics;fractal transform;fractal compression;karhunen–loève theorem;artificial neural network;encoding;principal component analysis	Vision	43.11478917058	-13.466678826845039	1845
43622bcec794cd0e7667ff9bc290022ab80674aa	different failure signatures of multiple tlp and hbm stresses in an esd robust protection structure	n p n transistor;tension electrica;concepcion asistida;computer aided design;modelo cuerpo homano;protective device;fiabilidad;reliability;bipolar transistor;contrainte electrique;transistor bipolaire;impulsion electrique;dispositivo proteccion;transistor n p n;human body model;electrostatic discharge;transistor bipolar;impulso electrico;decharge electrostatique;fiabilite;defaillance;conception assistee;modele corps humain;electric pulse;failures;fallo;dispositif protection;electric stress	The failure signatures of a grounded-base NPN bipolar ESD protection under multiple TLP and HBM stresses are analyzed. For this particular device having a graded collector region, multiple TLP or HBM stresses result in different types of defects. OBIC techniques and TCAD simulations are used to thoroughly analyze the involved physical mechanisms. * Corresponding author. nguitard@laas.fr Tel: +33 (0)561 336 489; Fax: +33 (0)561 336 208	antivirus software;fax;high bandwidth memory;hotspot (wi-fi);optical beam-induced current;polish notation;simulation;snapback (electrical);task parallelism;thermal management (electronics);traffic collision avoidance system;transistor;uniform resource identifier	Nicolas Guitard;Fabien Essely;David Trémouilles;Marise Bafleur;Nicolas Nolhier;Philippe Perdu;André Touboul;Vincent Pouget;Dean Lewis	2005	Microelectronics Reliability	10.1016/j.microrel.2005.07.030	electronic engineering;electrostatic discharge;telecommunications;engineering;electrical engineering;computer aided design;reliability;bipolar junction transistor	Arch	91.04706496236518	-9.849994720608953	1846
8409ac07733379f0fc1a4d62f8d799b42f76ccb2	stochastic gradient method with accelerated stochastic dynamics		In this paper, we propose a novel technique to implement stochastic gradient methods, which are beneficial for learning from large datasets, through accelerated stochastic dynamics. A stochastic gradient method is based on mini-batch learning for reducing the computational cost when the amount of data is large. The stochasticity of the gradient can be mitigated by the injection of Gaussian noise, which yields the stochastic Langevin gradient method; this method can be used for Bayesian posterior sampling. However, the performance of the stochastic Langevin gradient method depends on the mixing rate of the stochastic dynamics. In this study, we propose violating the detailed balance condition to enhance the mixing rate. Recent studies have revealed that violating the detailed balance condition accelerates the convergence to a stationary state and reduces the correlation time between the samplings. We implement this violation of the detailed balance condition in the stochastic gradient Langevin method and test our method for a simple model to demonstrate its performance.	algorithmic efficiency;detailed balance;gradient method;sampling (signal processing);stationary process;stationary state;stochastic process	Masayuki Ohzeki	2015	CoRR		continuous-time stochastic process	ML	27.377073503409566	-28.85537979691216	1847
a4d3c70ef48291616d2fb9efabf91270c574970f	a site-based method for prediction of protein orthologous relations			homology (biology);sequence homology	Hsuan-Chao Chiu;Yuh-Jyh Hu	2004			a-site;biology;bioinformatics	Comp.	1.7154045064232104	-63.558940948706805	1849
69a5e6f7d006847f7f69e7fdbcdc2dc464f3fe38	parameter identification methods for metamodeling simulations	parameter estimation metamodeling least squares approximation mathematical model taxonomy least squares methods laboratories contracts system identification concrete;least squares approximation;contracts;parameter identification;system identification;least square;taxonomy;mathematical model;a priori information;technical report;parameter estimation;metamodeling;least squares methods;concrete	A metamodel is a mathematical approximation of the system relationships defined by a high fidelity model or simulation. This paper presents methods that support new procedures that expanded the set of available metamodel representations be yond the traditional least squares formulation and added the capability to use dynamical metamodels. These methods compliment a new taxonomy of metamodel structures and procedures that separated the metamodeling process into a set of sequential decisions based on a priori information. This work was supported in part by The USAF Rome Laboratory Contract F30602-94-C-0110.	approximation;least squares;metamodeling;simulation	Don Caughlin	1996		10.1145/256562.256805	econometrics;mathematical optimization;computer science;mathematics;least squares;taxonomy;statistics	Graphics	34.49898035813941	-16.70528772165104	1854
222d2e000477d17a7506d98b581ef608b8fca9d8	a new auto-focusing method based on wbdct for many object situations	focusing;wbdct;focusing distance measurement discrete cosine transforms lenses cameras digital images security;evaluation function;image processing;object detection discrete cosine transforms focusing image processing;long short object distance situation;discrete cosine transform;long object distance situation;distance measurement;weighted blocking discrete cosine transform;discrete cosine transforms;lenses;block discrete cosine transform;security;digital images;cameras;weighted blocking discrete cosine transform auto focusing method wbdct long object distance situation long short object distance situation image clarity evaluation function;auto focusing method;object detection;image clarity evaluation function	There are two situations in auto focusing: one is the long-object-distance situation; the other is the long-short-object-distance situation. The curve of image clarity evaluation function under the long-short-object-distance situation usually has multiple peaks. A new method based on weighted blocking discrete cosine transform (WBDCT) is proposed for evaluating the image clarity in this paper. In the method, images are divided into some blocks of the same size. Then, each block is given the corresponding weighted value and is performed by discrete cosine transform (DCT). Finally, the clarity evaluation function is calculated. The method is tested in the long-object-distance situation and the long-short-object-distance situation, respectively. Experimental results indicate that the proposed method can effectively focus the images under the two situations.	blocking (computing);discrete cosine transform;evaluation function	Ke Han;Xue-mei Jiang;Quan Feng;Xiu-chang Zhu	2008	2008 Ninth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing	10.1109/SNPD.2008.79	computer vision;image processing;computer science;artificial intelligence;discrete cosine transform;evaluation function;lens;digital image;algorithm;computer graphics (images)	SE	55.51692581980541	-62.878545329425044	1855
64fcf209748ce114db0c7e844adc2ebb75b8e5be	transform-invariant pca: a unified approach to fully automatic facealignment, representation, and recognition	face face recognition image recognition training image reconstruction principal component analysis probes;image recognition;image coding;training image in plane transformations;training;classification methods;invariant descriptors;face coding;tipca technique;hog;local binary pattern;face alignment;optimal eigenspace;probes;gef;ground truth alignment;handcrafted alignment;gabor energy filter;principal component analysis face recognition image coding image reconstruction image representation;face recognition;face representation;image representation;image reconstruction;eigenfaces;principal component analysis;intrinsic human face structures;fully automatic face alignment;lbp;optimized coding;svm;face;principal component analysis face alignment face coding face recognition eigenfaces;support vector machine;src;transform invariant pca;sparse representation based classification;histogram of oriented gradient;mean square error minimization;facial landmarks;handcrafted alignment transform invariant pca fully automatic face alignment face representation face recognition tipca technique intrinsic human face structures training image in plane transformations optimal eigenspace mean square error minimization feret facial image ensemble eigenspace representation optimized coding local binary pattern lbp invariant descriptors histogram of oriented gradient hog gabor energy filter gef classification methods sparse representation based classification src support vector machine svm ground truth alignment face coding image reconstruction facial landmarks;eigenspace representation;feret facial image ensemble	We develop a transform-invariant PCA (TIPCA) technique which aims to accurately characterize the intrinsic structures of the human face that are invariant to the in-plane transformations of the training images. Specially, TIPCA alternately aligns the image ensemble and creates the optimal eigenspace, with the objective to minimize the mean square error between the aligned images and their reconstructions. The learning from the FERET facial image ensemble of 1,196 subjects validates the mutual promotion between image alignment and eigenspace representation, which eventually leads to the optimized coding and recognition performance that surpasses the handcrafted alignment based on facial landmarks. Experimental results also suggest that state-of-the-art invariant descriptors, such as local binary pattern (LBP), histogram of oriented gradient (HOG), and Gabor energy filter (GEF), and classification methods, such as sparse representation based classification (SRC) and support vector machine (SVM), can benefit from using the TIPCA-aligned faces, instead of the manually eye-aligned faces that are widely regarded as the ground-truth alignment. Favorable accuracies against the state-of-the-art results on face coding and face recognition are reported.		Weihong Deng;Jiani Hu;Jiwen Lu;Jun Guo	2014	IEEE transactions on pattern analysis and machine intelligence	10.1109/TPAMI.2013.194	facial recognition system;support vector machine;computer vision;computer science;machine learning;pattern recognition;mathematics	Vision	33.64500257375519	-53.84115559537275	1857
71f243b2f48caf47404b084c0c83f2cda5c83bf0	inertial-aided rolling shutter relative pose estimation		Relative pose estimation is a fundamental problem in computer vision and it has been studied for conventional global shutter cameras for decades. However, recently, a rolling shutter camera has been widely used due to its low cost imaging capability and, since the rolling shutter camera captures the image line-by-line, the relative pose estimation of a rolling shutter camera is more difficult than that of a global shutter camera. In this paper, we propose to exploit inertial measurements (gravity and angular velocity) for the rolling shutter relative pose estimation problem. The inertial measurements provide information about the partial relative rotation between two views (cameras) and the instantaneous motion that causes the rolling shutter distortion. Based on this information, we simplify the rolling shutter relative pose estimation problem and propose effective methods to solve it. Unlike the previous methods, which require 44 (linear) or 17 (nonlinear) points with the uniform rolling shutter camera model, the proposed methods require at most 9 or 11 points to estimate the relative pose between the rolling shutter cameras. Experimental results on synthetic data and the public PennCOSYVIO dataset show that the proposed methods outperform the existing methods.	3d pose estimation;algorithm;angularjs;computer vision;distortion;experiment;movie projector;nonlinear system;simultaneous localization and mapping;synthetic data;synthetic intelligence;velocity (software development)	Chang-Ryeol Lee;Kuk-Jin Yoon	2017	CoRR		angular velocity;pattern recognition;shutter;artificial intelligence;rolling shutter;pose;computer science;inertial frame of reference;computer vision;distortion	Vision	54.016894342280374	-41.776767671648074	1859
e5385401ab7753343f559470d00680ce7ff6d24e	a blind robust image watermarking using interest points and iwt	interest points;state code;image watermarking;blind watermark	In this paper, we propose a blind robust image watermarking algorithm based on a state code using local invariant areas. Integer Wavelet Transform (IWT) is used to embed watermark into local areas around interest points detected by SURF decoder, that allowing to increase the robustness of the proposed approach against various attacks. For watermark extraction, both local areas and state coding are used to extract the watermark without requiring to the original image or the original watermark. The results of different simulations applied show the robustness and effectiveness of the proposed approach, it can provide sufficient robustness against both traditional signal processing attacks and geometric attacks.	algorithm;digital watermarking;interest point detection;signal processing;simulation;speeded up robust features;wavelet transform	Ali Behloul	2014		10.1145/2668260.2668305	computer vision;theoretical computer science;watermark;computer security	ML	41.33643812810435	-10.51468808652185	1861
323485dcfcdda0917fc7ee14f274d851b9972ae3	implementation of scale and rotation invariant on-line object tracking based on cuda	cuda;classifier updating;gpgpu;object tracking	Object tracking is a major technique in image processing and computer vision. Tracking speed will directly determine the quality of applications. This paper presents a parallel implementation for a recently proposed scale- and rotation-invariant on-line object tracking system. The algorithm is based on NVIDIA's Graphics Processing Units (GPU) using Compute Unified Device Architecture (CUDA), following the model of single instruction multiple threads. Specifically, we analyze the original algorithm and propose the GPU-based parallel design. Emphasis is placed on exploiting the data parallelism and memory usage. In addition, we apply optimization technique to maximize the utilization of NVIDIA's GPU and reduce the data transfer time. Experimental results show that our GPGPU-based method running on a GTX480 graphics card could achieve up to 12X speed-up compared with the efficiency equivalence on an Intel E8400 3.0GHz CPU, including I/O time.		Quan Miao;Guijin Wang;Xinggang Lin	2011	IEICE Transactions	10.1587/transinf.E94.D.2549	computer vision;parallel computing;computer science;theoretical computer science;video tracking;general-purpose computing on graphics processing units;computer graphics (images)	Vision	43.447003742300055	-36.48192537800296	1864
5d5bebcc1a40ae508be304bd6d9302f5c9cff76b	hesitant fuzzy hamacher aggregation operators and their application to multiple attribute decision making	期刊论文	Hamacher product is a t-norm and Hamacher sum is a t-conorm. They are good alternatives to algebraic product and algebraic sum, respectively. Nevertheless, it seems that most of the existing hesitant fuzzy aggregation operators are based on the algebraic operations. In this paper, we utilize Hamacher operations to develop some hesitant fuzzy aggregation operators. Then, we have utilized these operators to develop some approaches to solve the hesitant fuzzy multiple attribute decision making problems. Finally, a practical example for evaluating the customer satisfaction of e-commerce websites is given to verify the developed approach and to demonstrate its practicality and effectiveness.	review aggregator	Liyong Zhou;Xiaofei Zhao;Guiwu Wei	2014	Journal of Intelligent and Fuzzy Systems	10.3233/IFS-130939	discrete mathematics;computer science;data mining;mathematics	AI	-2.5343286063542143	-20.745068010197606	1866
4d600e61c30541bdf12f92a10f356a14f525b1a3	the use of adaptive negotiation by a shopping agent in agent-mediated electronic commerce	commerce electronique;metodo adaptativo;feed forward;anticipacion;multiagent system;sobrecarga;anticipation;comercio electronico;feedforward;multi agent system;guerra;software agent;alimentacion maquina;methode adaptative;intelligence artificielle;information overload;boucle anticipation;agent logiciel;software agents;ciclo anticipacion;war;internet;surcharge;agent mediated electronic commerce;adaptive method;machine feed;alimentation machine;artificial intelligence;algorithme evolutionniste;algoritmo evolucionista;inteligencia artificial;agent architecture;evolutionary algorithm;evolutionary process;reseau neuronal;sistema multiagente;overload;red neuronal;guerre;electronic trade;artificial neural network;systeme multiagent;neural network	Software agents could help buyers and sellers to combat information overload and expedite specific stages of the online buying process. On the other hand, in a multi-agent system such as an agent-mediated electronic commerce, it is desirable that the agents try to adapt to the environment by learning or by an evolutionary process, thus doing an anticipation of the interaction with the other agents. The paper presents a shopping agent architecture, SmartAgent, whose role is to assist users when doing electronic shopping, in the Internet. The agent has a learning capability implemented by a feed-forward artificial neural network that allows him to model the other agent's negotiation strategy, thus doing an adaptive negotiation in order to make a better deal.	e-commerce	Mihaela Oprea	2003		10.1007/3-540-45023-8_57	simulation;computer science;artificial intelligence;software agent;feed forward;artificial neural network	AI	23.151866335548036	-12.186651030169001	1867
8626aa4842c36f2e7013d2e4f75e90d255344406	a novel predictive direct torque controller for induction motor drives	torque;predictive control;mean inverter switching frequency predictive direct torque controller pdtc induction motor drives predictive switching table load torque observer perturbation kalman filter flux estimation rapid control prototyping station speed trajectory tracking disturbance rejection dynamic responses;kalman filters;prediction algorithms;rapid prototyping direct torque control dtc induction motor im kalman filter kf load torque observer predictive control pc;torque stators induction motors kalman filters prediction algorithms torque control predictive control;induction motors;trajectory control dynamic response induction motor drives kalman filters machine control observers perturbation techniques torque control;stators;torque control	A predictive direct torque controller (PDTC) for induction motors (IM) is proposed. It combines the direct torque control (DTC) and the predictive control (PC), and uses a predictive switching table to enhance the overall performance of the motor. A new type of PC is adopted for speed regulation with the use of a load torque observer, the torque being considered an unknown perturbation. A Kalman filter (KF) is used for reliable flux estimation. The validity of the proposed controller was experimentally confirmed on a rapid control prototyping station. The obtained results have proven superiority of the proposed control with respect to the speed trajectory tracking, torque and flux dynamic responses, and disturbance rejection. Also, a lower current distortion was observed with PDTC, in comparison with the regular DTC, due to increased mean inverter switching frequency.	clock rate;distortion;experiment;kalman filter;power inverter;rapid control prototyping;rejection sampling	Mohand A. Ouhrouche;Rachid Errouissi;Andrzej M. Trzynadlowski;Kambiz Arab Tehrani;Ammar Benzaioua	2016	IEEE Transactions on Industrial Electronics	10.1109/TIE.2016.2558140	kalman filter;control engineering;electronic engineering;prediction;engineering;electrical engineering;control theory;induction motor;torque;vector control;model predictive control;torque motor;direct torque control	Robotics	69.69694444365594	-11.677585321693666	1871
7610d32dd8096d6506e821932c04310a4b5c1a4b	robust rotation synchronization via low-rank and sparse matrix decomposition		This paper deals with the rotation synchronization problem, which arises in global registration of 3D point-sets and in structure from motion. The problem is formulated in an unprecedented way as a “low-rank and sparse” matrix decomposition that handles both outliers and missing data. A minimization strategy, dubbed R-GoDec, is also proposed and evaluated experimentally against state-of-the-art algorithms on simulated and real data. The results show that R-GoDec is the fastest among the robust algorithms.	algorithm;experiment;fastest;missing data;point set registration;sparse matrix;structure from motion	Federica Arrigoni;Andrea Fusiello;Beatrice Rossi;Pasqualina Fragneto	2015	CoRR		mathematical optimization;theoretical computer science;machine learning;mathematics	ML	51.915396806692335	-48.596836057500205	1875
d124b0d9df8631b66e811c50b6a7c7c1dab74637	a dynamic clustering algorithm based on small data set	statistical character;cluster algorithm;pattern clustering data analysis;pattern clustering;small data set;class ratio;large dataset;presses;probability character;dynamic clustering algorithm;data mining;probability character dynamic clustering algorithm small data set class rotation class ratio statistical character;dynamic clustering;data analysis;heuristic algorithms;spatial databases;class rotation;clustering algorithms;clustering algorithms heuristic algorithms couplings probability partitioning algorithms optimization methods merging computer graphics data visualization educational institutions;couplings;algorithm design and analysis	The traditional clustering algorithms are designed for large dataset or vary large dataset. It is not easy to cluster the small dataset because of the loss of the statistical character and probability character. In this paper, the class ration is introduced, based on the class ratio, the dynamic clustering algorithm is proposed. The dataset are divided into all possible classes, and the class ratios are computed, the min class ratio is chosen and the clustering about the min class ratio is the best clustering. With the experiments, the schema is an effective way for the clustering of small data sets.	algorithm;cluster analysis;experiment;maxima and minima	Tao Peng;Minghua Jiang;Ming Hu	2009	2009 Sixth International Conference on Computer Graphics, Imaging and Visualization	10.1109/CGIV.2009.78	complete-linkage clustering;correlation clustering;constrained clustering;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;cluster analysis;single-linkage clustering;brown clustering;dbscan;biclustering;affinity propagation;clustering high-dimensional data	Vision	2.9406067329379475	-40.3945550638183	1876
00e1589f762bd021385063a2c9db5848a7b64ac8	computational analysis of rodent spinal cpg			central pattern generator;computation	William Erik Sherwood	2014		10.1007/978-1-4614-7320-6_40-2	rodent;cancer research;cpg site;biology	Logic	1.948877312240088	-64.37657398549776	1879
2efb749e95ee6b16675713f73018eefc3dbb1d2a	activity-based temporal segmentation for videos of interacting objects using invariant trajectory features	motion analysis;cameras hidden markov models trajectory sliding mode control video surveillance object detection gunshot detection systems phase detection indexing shape;image motion analysis;image segmentation;invariant trajectory feature;video signal processing feature extraction image motion analysis image segmentation markov processes object detection tracking;video signal processing;hidden markov model;visual semantics;hmm;video tracking;hierarchical semimarkov chain;distance measurement;visualization;computational modeling;hidden markov models;trajectory;squash video sequence;feature extraction;indexation;object tracking;content based approach;activity based temporal video segmentation;pattern classification;hmm activity based temporal video segmentation invariant trajectory feature content based approach object tracking visual semantics hierarchical semimarkov chain image motion analysis squash video sequence hidden markov model;markov processes;pattern classification video signal processing hidden markov models motion analysis;cameras;tracking;object detection;markov chain	This paper presents a content-based approach for temporal segmentation of videos. Tracked objects are characterized by their 2D trajectories which are used in a meaningful way to model visual semantics, i.e., the observed single video object activities and their interactions. To this end, hierarchical semi-Markov chains (SMCs) are computed in order to take into account the temporal causalities of object motions. Object movements are characterized using local invariant features computed from the curvature and velocity values while interactions are represented by the temporal evolution of the distance between objects. We have evaluated our method on squash video sequences, and have favorably compared with other methods including hidden Markov models (HMMs).	hidden markov model;interaction;markov chain;semiconductor industry;velocity (software development)	Alexandre Hervieu;Patrick Bouthemy;Jean-Pierre Le Cadre	2008	2008 15th IEEE International Conference on Image Processing	10.1109/ICIP.2008.4712478	computer vision;computer science;machine learning;video tracking;pattern recognition;hidden markov model	Vision	38.38950875527194	-48.649555079131325	1883
b2a739990a129ad8b3d2a61b83435b00b1cdbec2	multi-label classification via feature-aware implicit label space encoding		To tackle a multi-label classification problem with many classes, recently label space dimension reduction (LSDR) is proposed. It encodes the original label space to a low-dimensional latent space and uses a decoding process for recovery. In this paper, we propose a novel method termed FaIE to perform LSDR via Feature-aware Implicit label space Encoding. Unlike most previous work, the proposed FaIE makes no assumptions about the encoding process and directly learns a code matrix, i.e. the encoding result of some implicit encoding function, and a linear decoding matrix. To learn both matrices, FaIE jointly maximizes the recoverability of the original label space from the latent space, and the predictability of the latent space from the feature space, thus making itself feature-aware. FaIE can also be specified to learn an explicit encoding function, and extended with kernel tricks to handle non-linear correlations between the feature space and the latent space. Extensive experiments conducted on benchmark datasets well demonstrate its effectiveness.	benchmark (computing);dimensionality reduction;effective method;experiment;feature vector;kernel (operating system);multi-label classification;nonlinear system;serializability	Zijia Lin;Guiguang Ding;Mingqing Hu;Jianmin Wang	2014			machine learning;multi-label classification;artificial intelligence;kernel (linear algebra);encoding (memory);decoding methods;matrix (mathematics);pattern recognition;dimensionality reduction;computer science;feature vector;predictability	ML	24.272573180878194	-44.610187737061494	1884
05cd39c2264ed9c28e0172ac538ee582609262ea	ray casting algebraic surfaces using the frustum form	raytracing;algebraic surfaces;computational efficiency;visible line surface algorithms i 3 7 three dimensional graphics and realism;i 3 7 three dimensional graphics and realism;ray casting	We propose an algorithm for interactive ray-casting of algebraic surfaces of high degree. A key point of our approach is a polynomial form adapted to the view frustum. This so called frustum form yields simple expressions for the Bernstein form of the ray equations, which can be computed efficiently using matrix products and pre-computed quantities. Numerical root-finding is performed using B-spline and Bézier techniques, and we compare the performances of recent and classical algorithms. Furthermore, we propose a simple and fairly efficient anti-aliasing scheme, based on a combination of screen space and object space techniques. We show how our algorithms can be implemented on streaming architectures with single precision, and demonstrate interactive frame-rates for degrees up to 16.	aliasing;b-spline;bernstein polynomial;bézier curve;glossary of computer graphics;numerical linear algebra;performance;phil bernstein;precomputation;ray casting;root-finding algorithm;single-precision floating-point format;spatial anti-aliasing;streaming algorithm;tuple space;viewing frustum	Martin Reimers;Johan Seland	2008	Comput. Graph. Forum	10.1111/j.1467-8659.2008.01133.x	ray tracing;topology;computer science;ray casting;mathematics;geometry;algebraic surface;algorithm;computer graphics (images)	Graphics	66.6721193817012	-50.391895516768926	1885
6e3fef9334f67274d9dd029f4cbe60fb8dbd62c8	accelerating depth image-based rendering using gpu	analisis contenido;image tridimensionnelle;metodo adaptativo;rendu image;representation graphique;multimedia;illumination;restitucion imagen;luminance;securite informatique;surface rendering;methode adaptative;hardware accelerator;classification;surface reponse;computer security;content analysis;compression image;image compression;graphics hardware;object oriented;eclairage;seguridad informatica;adaptive method;superficie respuesta;image rendering;graphic processing unit;grafo curva;tridimensional image;oriente objet;lighting;depth image based rendering;analyse contenu;response surface;eclairement;orientado objeto;clasificacion;graphics;imagen tridimensional;alumbrado;compresion imagen;luminancia	In this paper, we propose a practical method for hardwareaccelerated rendering of the depth image-based representation (DIBR) object, which is defined in MPEG-4 Animation Framework eXtension (AFX). The proposed method overcomes the drawbacks of the conventional rendering, i.e. it is slow since it is hardly assisted by graphics hardware and surface lighting is static. Utilizing the new features of modern graphic processing unit (GPU) and programmable shader support, we develop an efficient hardware-accelerated rendering algorithm of depth image-based 3D object. Surface rendering in response of varying illumination is performed inside the vertex shader while adaptive point splatting is performed inside the fragment shader. Experimental results show that the rendering speed increases considerably compared with the software-based rendering and the conventional OpenGL-based rendering	afx windows rootkit 2003;algorithm;graphics hardware;graphics processing unit;hardware acceleration;mathematical optimization;opengl;shader;texture splatting;volume rendering	Man Hee Lee;In Kyu Park	2006		10.1007/11848035_74	computer vision;tiled rendering;tessellation;simulation;shader;image-based modeling and rendering;fragment processing;content analysis;3d rendering;rendering;computer science;parallel rendering;lighting;real-time rendering;texture memory;alternate frame rendering;software rendering;image-based lighting;computer graphics (images)	Graphics	62.96809379841929	-48.45608102041498	1886
aa380b94d66417a3a0333711001de54cb90f4de2	a fixed transformation of color images for dichromats based on similarity matrices	color space mapping;dichromat s visual correction;color space;three dimensional;similarity matrix;color image	A novel method is developed for the dichromat's visual correction. This scheme includes three steps. Firstly, two similarity matrices are established respectively for the normal three-dimensional color space and the color plane in which dichromats can distinguish all the colors. Then a 3D-2D mapping relationship is searched based on these similarity matrices. Finally, the original color image is transformed to a new one which is suitable for dichromats. The experiments on both color test images and real images demonstrate the ability of the scheme for color blindness correction. With the fixed transformation of color space, this scheme may have capability to help training dichromats to recognize most colors.		Yinhui Deng;Yuanyuan Wang;Yu Ma;Jibin Bao;Xiaodong Gu	2007		10.1007/978-3-540-74171-8_103	color histogram;three-dimensional space;computer vision;icc profile;color model;color quantization;similarity matrix;color normalization;color depth;color image;computer science;rgb color space;color difference;mathematics;color balance;color space;computer graphics (images)	Vision	54.86859741364461	-61.94348165452054	1890
fa1040057bc67a078aa91bcf83b8ee90efaa93ed	identification of sudden cardiac arrest using the pan-tompkins algorithm	patient diagnosis;heart disease;pan tompkins algorithm;electrocardiogram ecg;heart electrocardiography cardiac arrest muscles blood databases;cardiology;sudden cardiac arrest;mit bih database sudden cardiac arrest identification pan tompkins algorithm heart electrical system malfunctions blood heart disease risk factors ecg r peaks;pan tompkins algorithm electrocardiogram ecg sudden cardiac death sudden cardiac arrest;risk factors;electrocardiography;sudden cardiac death;diseases;patient diagnosis cardiology diseases electrocardiography medical signal processing;medical signal processing;electrocardiogram	In this paper, a method to identify sudden cardiac arrest is described based on the Pan-Tompkins Algorithm. Sudden cardiac arrest (SCA) is a condition in which the heart suddenly and unexpectedly stops beating. Sudden cardiac death occurs when the heart's electrical system malfunctions. The SCA happens, when blood stops flowing to the brain and other vital organs. People who have heart disease are at increased risk for SCA. However, most SCAs happen in people who appear healthy and have no known heart disease or other risk factors for SCA. 4-5 min. of ECG of any patient is enough to detect possibility of SCD. Pan-Tompkins algorithm was used to find number of R-peaks in the ECG of a SCA patient and normal person. This method is compared to other methods. The MIT BIH database has been utilized for evaluating the algorithm.	algorithm;bounding interval hierarchy;multistage interconnection networks;risk factor (computing);slowly changing dimension	V. Vijjaya;K. Kishan Rao;P. Sahrudai	2012	2012 UKSim 14th International Conference on Computer Modelling and Simulation	10.1109/UKSim.2012.23	intensive care medicine;medicine;medical emergency;cardiology	Robotics	16.526538015202632	-87.08498970989578	1891
349add5d848303b49c24664963120768c16a12b0	a three-phase approach to solving the bidline problem	quality of life;cutting stock;set partitions;circadian rhythm;set partitioning;integer programming;aircrew scheduling;integer program;bidline generation	This paper describes a three-phase approach to solving the bidline generation problem within airline flightcrew scheduling. Phase 1 builds “patterns” from existing pairings. Phase 2 builds bidlines from the “patterns” found in Phase 1 and solves a set partitioning problem to generate a final schedule. If Phase 2 fails to cover enough of the scheduled work, Phase 3 is used to fill in the uncovered pairings. Along with this methodology is a new rule set that tries to improve the work–rest schedule for flightcrews by trying to take into account circadian rhythms. This new rule set is an attempt at addressing some of the flightcrews's quality of life issues. Copyright Kluwer Academic Publishers 2004	problem solving	Jeffery D. Weir;Ellis L. Johnson	2004	Annals OR	10.1023/B:ANOR.0000019093.93633.bc	mathematical optimization;simulation;quality of life;integer programming;computer science;operations management;mathematics;algorithm;circadian rhythm	AI	15.285913726781136	3.6558565869285524	1892
2f6dc64dcb5f7b86c69b683b5cf9864376b96eaa	stabilising the camera-to-fixation point distance in active vision	eficacia sistema;vision ordenador;image processing;etude experimentale;performance systeme;procesamiento imagen;movie camera;robotics;system performance;traitement image;tracking movable target;computer vision;algorithme;algorithm;camara;fixation;camera to fixation point distance;robotica;vision ordinateur;poursuite;robotique;vision active;estudio experimental;tracking;persecucion y continuacion;camera;active vision;algoritmo	Abstract   This paper proposes a method for the determination of the camera-to-fixation point (CFP) distance using active vision strategies. The proposed approach employs an advantage of fixation; namely that the problem becomes object centred. An accurate estimate of the CFP distance is necessary for (1) establishing correspondence between consecutive images and (2) tracking and predicting where the tracked features are in the subsequent images. The mathematical formulation as well as the strategies used for the correction of the CFP distance are discussed. The advantage associated with this strategy is that the correction of the CFP distance only requires the 2-D location of the feature points.	active vision	Chiou Peng Lam;Geoff A. W. West;Svetha Venkatesh	1998	Pattern Recognition	10.1016/S0031-3203(97)00114-3	fixation;computer vision;simulation;active vision;image processing;computer science;tracking;robotics;computer graphics (images)	Vision	49.2113410321951	-57.6142071730463	1897
daac03f26842c58675f69f3eba60933faf7fc800	feature selection based on maximal neighborhood discernibility		Neighborhood rough set has been proven to be an effective tool for feature selection. In this model, the positive region of decision is used to evaluate the classification ability of a subset of candidate features. It is computed by just considering consistent samples. However, the classification ability is not only related to consistent samples, but also to the ability to discriminate samples with different decisions. Hence, the dependency function, constructed by the positive region, cannot reflect the actual classification ability of a feature subset. In this paper, we propose a new feature evaluation function for feature selection by using discernibility matrix. We first introduce the concept of neighborhood discernibility matrix to characterize the classification ability of a feature subset. We then present the relationship between distance matrix and discernibility matrix, and construct a feature evaluation function based on discernibility matrix. It is used to measure the significance of a candidate feature. The proposed model not only maintains the maximal dependency function, but also can select features with the greatest discernibility ability. The experimental results show that the proposed method can be used to deal with heterogeneous data sets. It is able to find effective feature subsets in comparison with some existing algorithms.	feature selection;maximal set	Changzhong Wang;Qiang He;Ming-Wen Shao;Qinghua Hu	2018	Int. J. Machine Learning & Cybernetics	10.1007/s13042-017-0712-6	feature (computer vision);matrix (mathematics);feature selection;evaluation function;machine learning;data set;rough set;artificial intelligence;pattern recognition;distance matrix;mathematics	ML	6.74537288323046	-42.10035961643512	1898
ee8eebe4d86c9749a4cf40adf279ec1b4d049c39	a high invariance motion representation for skeleton-based action recognition	multi value;skeletal representation;orthogonal group;relative geometry;computation cost	Human action recognition is very important and significant research work in numerous fields of science, for example, human–computer interaction, computer vision and crime analysis. In recent years, relative geometry features have been widely applied to the description of relative relation of body motion. It brings many benefits to action recognition such as clear description, abundant features etc. But the obvious disadvantage is that the extracted features severely rely on the local coordinate system. It is difficult to find a bijection between relative geometry and skeleton motion. To overcome this problem, many previous methods use relative rotation and translation between all skeleton pairs to increase robustness. In this paper we present a new motion representation method. It establishes a motion model based on the relative geometry with the aid of special orthogonal group SO(3). At the same time, we proved that this motion representation method can establish a bijection between relative geometry and motion of skeleton pairs. After the motion representation method in this paper is used, the computation cost of action recognition reduces from the two-way relative motion (motion from A to B and B to A) to one-way relative motion (motion from A to B or B to A) between any skeleton pair, namely, permutation problem Pn2 is simplified into combinatorics problem Cn2. Finally, the experimental results of the three motion datasets are all superior to present skeleton-based action recognition methods.		Songrui Guo;Huawei Pan;Guanghua Tan;Lin Chen;Chunming Gao	2016	IJPRAI	10.1142/S021800141650018X	computer vision;structure from motion;orthogonal group;artificial intelligence;motion estimation;mathematics;geometry;motion field	Vision	53.645070969208525	-48.68998469959616	1901
24297838c2e3f55715f14402a35528ac871991bf	the measurement of production efficiency in scientific journals through stochastic frontier analysis models: application to quantitative economics journals	efficiency;62f10;scientific production;d24;frontier production models;production;productivity;62j99;90b30	The importance of a scientific journal is usually established by considering the number of citations received by the papers that the journal publishes. In this way, the number of citations received by a scientific journal can be considered as a measure of the total production of the journal. In this paper, in order to obtain measures of the efficiency in the production process, the approach provided by stochastic frontier analysis (SFA) is considered, and econometric models are proposed. These models estimate a frontier production, which is the maximum achievable number of citations to the journal based on its resources. The efficiency can then be measured by considering the difference between the actual production and the estimated frontier. This approach is applied to the measurement of the productive efficiency of the journals of the JCR social sciences edition database, which belong simultaneously to the areas of “economics” and “social sciences, mathematical methods”. © 2013 Elsevier Ltd. All rights reserved.	econometric model;journal citation reports	Francisco J. Ortega;Jose M. Gavilan	2013	J. Informetrics	10.1016/j.joi.2013.09.004	econometrics;productivity;data envelopment analysis;efficiency;operations research	AI	-0.5694491100611736	-13.349927021182301	1903
6502922fb33ff271b5df8041fc3e318bd1eee21a	sustainable building design: a review on recent metaheuristic methods		A notable portion of the total primary energy is consumed by today’s buildings in developed countries. In recent years, decision makers and planners are facing increased pressure to respond more effectively to a number of energy-related issues and conflicts. Therefore, this article strives to make a technical review of all relevant research applying simulation-based optimization methods to sustainable building design problems. A summary of the application of common heuristic and meta-heuristic methods to different fields of sustainable building design is given.	metaheuristic	Somayeh Asadi;Zong Woo Geem	2015		10.1007/978-3-319-13826-8_11	systems engineering;engineering;transport engineering;management science	HCI	11.359511739987765	-4.663500282602456	1904
f5991b6ff443890c695e3d0cbdc38f4a78a89f9c	an improved class of estimators of a finite population quantile in sample surveys	auxiliary variable;information auxiliaire;difference type estimator;population finie;cuantila;matematicas aplicadas;mathematiques appliquees;auxiliary information;sample surveys;ecuesta estadistica;finite population;finite population quantile;sample survey;quantile;estimateur type difference;applied mathematics;sondage statistique;poblacion finita;estimateur horvitz thompson;horvitz thompson estimator	This work proposes a general class of estimators for a finite population quantile using auxiliary information. This information is provided by the population means of auxiliary variables. The optimum estimator in this class is derived. This result is supported with a numerical example.		Antonio Arcos;María del Mar Rueda;Juan Francisco Muñoz	2007	Appl. Math. Lett.	10.1016/j.aml.2006.04.014	econometrics;mathematical optimization;quantile;survey sampling;mathematics;statistics	DB	32.45916729572842	-22.4669607944487	1905
a36d8c79010e11b308fb90ffb52a86acff07a88b	malicious web domain identification using online credibility and performance data by considering the class imbalance issue		Purpose – Malicious web domain identification is of significant importance to the security protection of Internet users. With online credibility and performance data, this paper aims to investigate the use of machine learning techniques for malicious web domain identification by considering the class imbalance issue (i.e., there are more benign web domains than malicious ones). Design/methodology/approach – We propose an integrated resampling approach to handle class imbalance by combining the Synthetic Minority Over-sampling TEchnique (SMOTE) and Particle Swarm Optimisation (PSO), a population-based meta-heuristic algorithm. We use the SMOTE for over-sampling and PSO for under-sampling. Findings – By applying eight well-known machine learning classifiers, the proposed integrated resampling approach is comprehensively examined using several imbalanced web domain datasets with different imbalance ratios. Compared to five other well-known resampling approaches, experimental results confirm that the proposed approach is highly effective. Practical implications – This study not only inspires the practical use of online credibility and performance data for identifying malicious web domains, but also provides an effective resampling approach for handling the class imbalance issue in the area of malicious web domain identification. Originality/value – Online credibility and performance data is applied to build malicious web domain identification models using machine learning techniques. An integrated resampling approach is proposed to address the class imbalance issue. The performance of the proposed approach is confirmed based on real-world datasets with different imbalance ratios.	algorithm;heuristic (computer science);machine learning;malware;mathematical optimization;opaque pointer;oversampling;particle swarm optimization;resampling (statistics);sampling (signal processing)	Zhongyi Hu;Raymond Chiong;Ilung Pranata;Yukun Bao;Yuqing Lin	2018	CoRR	10.1108/IMDS-02-2018-0072	management science;the internet;data mining;information security;oversampling;engineering;population;resampling;credibility;undersampling;particle swarm optimization	SE	6.082575557013921	-38.8472561116984	1907
e09266c3fa2aa18c0b15463e9430a1304b93efc9	towards human activity reasoning with computational logic and deep learning		We approach the problem of human action recognition in videos by distinguishing between simple and complex actions. To recognize simple actions, we take advantage of the latest advances with 3D convolutional networks, which are able to offer a generic video snippet descriptor. For the complex ones, which involve interaction between more than one individual, we use the recognized simple human actions of the previous step to generate Event Calculus theories. This way, we aim to achieve a high-level human action understanding, combining the opaque effectiveness of deep learning and the transparent reasoning of computational logic. Our experimental results on a benchmark activity recognition dataset encourage further research towards this direction.	activity recognition;benchmark (computing);computational logic;convolutional neural network;deep learning;event calculus;high- and low-level;theory	Ioannis Prapas;Georgios Paliouras;Alexander Artikis;Nicolas Baskiotis	2018		10.1145/3200947.3201051	machine learning;deep learning;activity recognition;computer science;event calculus;computational logic;snippet;inductive logic programming;artificial intelligence	AI	24.17869732852039	-53.92606250360914	1908
27351ed90aa670a244ad4fbb7e15b5fd906bee05	a salt and pepper noise image filtering method using pcnn	pulsed coupled neural network;salt and pepper noise;filter;extended window median filter	Based on Pulse Coupled Neural networks,an effective salt and pepper noise image filtering method is proposed.Synchronous pulses were burst by using the similar groups of neurons in a PCNN,whereby the noise pixels are detected;and the neuron parameter-estimation method was given.Then it is considered that a noise pixel has the most similar with neighbor non-noise pixels,a filtering method called extended window median filter was put forward,which filtered the noise in a noise image.Simulation results show that the proposed method has excellent filtering performance for the noise images of different noise intensity,and has the more obvious advantage than the corresponding median filters method.	pulse-coupled networks;salt-and-pepper noise	Rencan Nie;Shaowen Yao;Dongming Zhou;Xiang Li	2013		10.1007/978-3-642-37502-6_120	gaussian noise;median filter;image noise;computer vision;electronic engineering;engineering;machine learning;salt-and-pepper noise	Vision	56.44320502571854	-65.61213930313981	1910
1c8add7c926eef9403ac4d0a2651c2cd92ad1e72	simple but effective tree structures for dynamic programming-based stereo matching	image segmentation;dynamic program;energy function;stereo matching;tree structure	This work describes a fast method for computing dense stereo correspondences that is capable of generating results close to the state-of-the-art. We propose running a separate disparity computation process in each image pixel. The idea is to root a tree graph on the pixel whose disparity needs to be reconstructed. The tree thereby forms an individual approximation of the standard four-connected grid for this specific pixel. An exact optimum of a predefined energy function on the applied tree structure is determined via dynamic programming (DP), and the root pixel is assigned to the disparity of optimal costs. We present two simple tree structures that allow for the efficient calculation of all trees’ optima with only four scanline-based DP passes. These simple trees are designed to capture all pixels of the reference frame and incorporate horizontal and vertical smoothness edges in order to weaken the scanline streaking problem inherent in DP-based approaches. We evaluate our results using the Middlebury test set. Our algorithm currently ranks at the eighth position of approximately 30 algorithms in the Middlebury database. More importantly, it is the currently best-performing method that does not use image segmentation and is significantly faster than most competing algorithms. Our method needs less than a second to determine the disparity map for typical stereo pairs.	algorithm;approximation;approximation algorithm;belief propagation;binocular disparity;computation;computer stereo vision;dynamic programming;image segmentation;map;mathematical optimization;optimization problem;pixel;reference frame (video);scan line;software propagation;test set;tree structure;yang	Michael Bleyer;Margrit Gelautz	2008			computer vision;computer science;machine learning;pattern recognition;image segmentation;tree structure	Vision	53.535923011318424	-59.12826624431357	1911
8ca29096d5828e9608166c4b7eadf2bdde8d35a1	new algorithms for optimal binary vector quantizer design	constrained optimization;hamming codes vector quantisation image coding linear codes;image coding;hamming codes;binary image;hamming codes optimal binary vector quantizer design algorithms subjective quality binary images vq linear codes image coding;algorithm design and analysis image coding image segmentation frequency pixel vector quantization image edge detection image quality linear code digital images;linear codes;design optimization;vq;linear code;binary images;algorithms;vector quantizer;vector quantisation;subjective quality;optimal binary vector quantizer design	New algorithms are proposed for designing optimal binary vector quantizers. These algorithms aim to avoid the problem of the generalized Lloyd method of easily getting trapped into a poor local minimum. To improve the subjective quality of vector-quantized binary images, a constrained optimal binary VQ framework is proposed. Within this framework, the optimal VQ design can be done via an interesting use of linear codes.	algorithm;binary image;bit array;code;maxima and minima;quantization (signal processing);vector quantization	Xin Wu;Yang Fang	1995		10.1109/DCC.1995.515503	mathematical optimization;constrained optimization;binary image;computer science;theoretical computer science;pattern recognition;mathematics	Theory	44.29477028308052	-12.9758064033894	1912
207fff903427b72b6207f18a8506a275fbf2e17a	a sparse optimization method for masking effect removal in noise synthetic aperture radar	compressed sensing;chirp;frequency measurement;sparse optimization;synthetic aperture radar sar compressed sensing inverse problem masking effect noise radar sparse optimization;inverse problem;synthetic aperture radar sar;radar imaging;masking effect;noise radar;signal to noise ratio;synthetic aperture radar airborne radar compressed sensing frequency measurement image reconstruction inverse problems optimisation radar imaging radar interference;synthetic aperture radar radar imaging chirp frequency measurement signal to noise ratio optimization methods;airborne noise sar data sparse optimization method masking effect removal synthetic aperture radar noise sar imaging process noise waveforms high level sidelobes matched filtering compressed sensing inverse problem random frequency measurements;optimization methods;synthetic aperture radar	In this paper, we propose a sparse optimization method to suppress the masking effect of noise synthetic aperture radar (SAR) imaging. The transmitted noise waveforms have large fluctuations in the spectrum, which results in high-level sidelobes in the range direction. Weak targets can be masked by the sidelobes of strong targets when traditional matched filtering (MF) is applied. Inspired by the idea of compressed sensing (CS), we formulate the noise SAR imaging process as solving an inverse problem from incomplete random frequency measurements, and then reconstruct the noise SAR image using sparse optimization method. Experimental results on real airborne noise SAR data are presented to show the effectiveness of the proposed method.	airborne ranger;compressed sensing;high- and low-level;mathematical optimization;sparse approximation;sparse matrix;synthetic data;texture filtering	Xiao Dong;Yunhua Zhang;Wenshuai Zhai	2016	2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)	10.1109/IGARSS.2016.7730778	computer vision;synthetic aperture radar;geology;inverse problem;radar imaging;inverse synthetic aperture radar;compressed sensing;signal-to-noise ratio;chirp;remote sensing	EDA	72.69286433573552	-66.96615297602452	1913
0403a5d37e953d98bbdc3344c2bcf4b62c78a45a	relational clustering by symmetric convex coding	distance function;relational data;generic model;euclidean distance;graph partitioning;theoretical analysis;experimental evaluation	Relational data appear frequently in many machine learning applications. Relational data consist of the pairwise relations (similarities or dissimilarities) between each pair of implicit objects, and are usually stored in relation matrices and typically no other knowledge is available. Although relational clustering can be formulated as graph partitioning in some applications, this formulation is not adequate for general relational data. In this paper, we propose a general model for relational clustering based on symmetric convex coding. The model is applicable to all types of relational data and unifies the existing graph partitioning formulation. Under this model, we derive two alternative bound optimization algorithms to solve the symmetric convex coding under two popular distance functions, Euclidean distance and generalized I-divergence. Experimental evaluation and theoretical analysis show the effectiveness and great potential of the proposed model and algorithms.	algorithm;cluster analysis;euclidean distance;graph partition;machine learning;mathematical optimization	Bo Long;Zhongfei Zhang;Xiaoyun Wu;Philip S. Yu	2007		10.1145/1273496.1273568	combinatorics;discrete mathematics;relational model;statistical relational learning;metric;relational database;graph partition;machine learning;euclidean distance;mathematics;conjunctive query	ML	22.08339353118225	-41.87239230442537	1922
0c40604d72b43d48a9cb07c9aaccde15c37229f0	combined formulation of doxorubicin-arg-gly-asp (rgd) and modified pegylated plga-encapsulated nanocarrier improves anti-tumor activity	encapsulation;drugs;nanoparticles;nanomedicine;drug delivery;drug delivery systems;cancer;biodiffusion;integrin;tumours;proteins;tumor;conducting polymers;molecular biophysics;rgd;nanoparticles drugs cancer dna peptides tumors educational institutions;tumours biodiffusion cancer cellular biophysics conducting polymers drug delivery systems drugs encapsulation molecular biophysics nanomedicine nanoparticles proteins;nanoparticle rgd tumor drug delivery integrin;nanoparticle;cellular biophysics;doxorubicin arg gly asp formulation arg gly asp peptide passive targeting moieties active targeting moieties malignant tumor cell lines dox loading efficiency plga nanoparticles drug burst release effect polymeric matrix resultant nanoparticles nanoparticle targeting ability integrin expressing cancer cells inducing apoptosis specific malignant cancer cells multifunctional nanoparticles malignant integrin expressing cancer treatment dispersion poly ethylene glycol nanoparticle surface solvent diffusion techniques conjugated poly lactic co glycolic acid antitumor activity modified pegylated plga encapsulated nanocarrier formulation	In this formulation, Doxorubicin (Dox) was conjugated to Poly (lactic-co-glycolic acid) (PLGA), and formulated via the solvent-diffusion techniques into nanoparticles. The surface of the nanoparticles was subsequently linked with Poly (ethylene glycol) (PEG) and Arg-Gly-Asp (RGD) peptide to achieve both passive and active targeting moieties. The nanoparticles were then tested against several malignant tumor cell lines. The conjugation increased loading efficiency of Dox to PLGA nanoparticles (the encapsulation efficiency was over 85%) and alleviated the drug burst release effect substantially. The drug was released from the polymeric matrix in a sustained release manner over a period of 12 days. The resultant nanoparticles were spherically uniform and well-dispersed. The nanoparticle targeting ability was proven through strong affinity to various integrin-expressing cancer cells, and much less affinity to the low integrin expression cancer cells. The nanoparticles also showed high efficacy in inducing apoptosis in specific malignant cancer cells. Taken together, these multifunctional nanoparticles hold potential to treat malignant integrin-expressing cancers.	doxing;encapsulation (networking);multi-function printer;parsing expression grammar;processor affinity;rgd;resultant	Stanley Moffatt;Richard Cristiano;Rose Boyle	2012	2012 IEEE International Conference on Bioinformatics and Biomedicine Workshops	10.1109/BIBMW.2012.6470261	computer science;nanoparticle;nanomedicine;molecular biophysics	Visualization	11.30436244905186	-63.908237240474755	1931
aa003d240574e77573a5eaa22ea534e2f1ddb32f	how well do line drawings depict shape	data collection;large data sets;computer graphic;shape perception;line drawings;surface model;ground truth;non photorealism	This paper investigates the ability of sparse line drawings to depict 3D shape. We perform a study in which people are shown an image of one of twelve 3D objects depicted with one of six styles and asked to orient a gauge to coincide with the surface normal at many positions on the object's surface. The normal estimates are compared with each other and with ground truth data provided by a registered 3D surface model to analyze accuracy and precision. The paper describes the design decisions made in collecting a large data set (275,000 gauge measurements) and provides analysis to answer questions about how well people interpret shapes from drawings. Our findings suggest that people interpret certain shapes almost as well from a line drawing as from a shaded image, that current computer graphics line drawing techniques can effectively depict shape and even match the effectiveness of artist's drawings, and that errors in depiction are often localized and can be traced to particular properties of the lines used. The data collected for this study will become a publicly available resource for further studies of this type.		Forrester Cole;Kevin Sanik;Douglas DeCarlo;Adam Finkelstein;Thomas A. Funkhouser;Szymon Rusinkiewicz;Manish Singh	2009	ACM Trans. Graph.	10.1145/1531326.1531334	computer vision;technical drawing;ground truth;computer science;artificial intelligence;mathematics;statistics;data collection;computer graphics (images)	Graphics	56.460286547332764	-48.40200359676765	1935
e6704cb4f33388b92477a0b0463cfb60a2ed3c0a	possibility interval-valued vague soft expert sets and its similarity measure		In this paper, the authors aim to extend the notion of interval-valued vague soft sets to establish the concept of possibility interval-valued vague soft expert sets. The set theoretic operations of this concept and other related concepts are introduced. The algebraic properties of this notion such as the laws of commutativity, associativity and De Morgan are established and verified. Lastly, the similarity measure between this set is introduced and illustrated using a hypothetical example related to texture synthesis.	similarity measure;vagueness	Ganeshsree Selvachandran;Sunil Jacob John	2017	IJFSA	10.4018/IJFSA.2017010106	data mining	NLP	-2.144490301596799	-23.12694588285798	1941
f53bd700ec16373cb523d941f17ec8c93b8c7f9e	a pso-inspired multi-robot search algorithm independent of global information	robot sensing systems;relative coordinate system multirobot search algorithm mobile robot pso inspired search algorithm cartesian geometry;search problems mobile robots multi robot systems particle swarm optimisation;mobile robot;multirobot search algorithm;robot kinematics robot sensing systems particle swarm optimization force manganese;repulsion multi robot search pso inspired relative coordinate system;search algorithm;mobile robots;relative coordinate system;pso inspired;force;cartesian geometry;manganese;particle swarm optimizer;pso inspired search algorithm;particle swarm optimization;multi robot systems;repulsion;physical environment;search problems;particle swarm optimisation;multi robot search;coordinate system;robot kinematics	This paper presents an algorithm that coordinates mobile robots to find the desired targets without depending on precise global information. We compare the abstract solution space in PSO and physical environment that robots will explore. According to similarities and differences between them, we introduce a PSO-inspired search algorithm to guide robots to complete the search mission. Moreover, a novel method based on Cartesian geometry for unifying relative coordinate systems will be adopted to improve system's robustness and efficiency.	cartesian closed category;feasible region;mobile robot;particle swarm optimization;robustness (computer science);search algorithm	Qian Zhu;Alei Liang;Haibing Guan	2011	2011 IEEE Symposium on Swarm Intelligence	10.1109/SIS.2011.5952586	mathematical optimization;simulation;machine learning;physics	Robotics	59.089575516469125	-22.211657909514695	1944
91ce523893c189f27ea57ae89c02357018521a8c	hybrid forward resampling and volume rendering	discrete rendering;discrete sample;spherical splats;resampling problem;resampling practice;perspectively projected splats;discrete object;minified voxels;hybrid forward resampling;volume rendering;efficient perspective splatting;accurate perspective reconstruction filter;perspective projection	The transforming and rendering of discrete objects, such as traditional images (with or without depths) and volumes, can be considered as resampling problem – objects are reconstructed, transformed, filtered, and finally sampled on the screen grids. In resampling practices, discrete samples (pixels, voxels) can be considered either as infinitesimal sample points (simply called points) or samples of a certain size (splats). Resampling can also be done either forwards or backwards in either the source domain or the target domain. In this paper, we present a framework that features hybrid forward resampling for discrete rendering. Specifically, we apply this framework to enhance volumetric splatting. In this approach, minified voxels are taken simply as points filtered in screen space; while magnified voxels are taken as spherical splats. In addition, we develop two techniques for performing accurate and efficient perspective splatting. The first one is to efficiently compute the 2D elliptical geometry of perspectively projected splats; the second one is to achieve accurate perspective reconstruction filter. The results of our experiments demonstrate both the effectiveness of antialiasing and the efficiency of rendering using this	brookgpu;computer science;display resolution;experiment;glossary of computer graphics;hidden surface determination;image warping;nyquist–shannon sampling theorem;pixel;reconstruction filter;resampling (statistics);research data archiving;sampling (signal processing);shading;spatial anti-aliasing;stereoscopy;texture mapping;texture splatting;volume rendering;volumetric display;voxel	Xiaoru Yuan;Minh X. Nguyen;Hui Xu;Baoquan Chen	2003		10.2312/VG/VG03/119-128	computer vision;mathematical optimization;mathematics;computer graphics (images)	Graphics	64.66778484042688	-53.25351491588657	1946
ff5dfc8a3da3f610b2a37e5eca2984f22bcb6469	improvement of information transfer rates using a hybrid eeg-nirs brain-computer interface with a short trial length: offline and pseudo-online analyses	eeg;nirs;brain-computer interface;information transfer rate;pseudo-online	Electroencephalography (EEG) and near-infrared spectroscopy (NIRS) are non-invasive neuroimaging methods that record the electrical and metabolic activity of the brain, respectively. Hybrid EEG-NIRS brain-computer interfaces (hBCIs) that use complementary EEG and NIRS information to enhance BCI performance have recently emerged to overcome the limitations of existing unimodal BCIs, such as vulnerability to motion artifacts for EEG-BCI or low temporal resolution for NIRS-BCI. However, with respect to NIRS-BCI, in order to fully induce a task-related brain activation, a relatively long trial length (≥10 s) is selected owing to the inherent hemodynamic delay that lowers the information transfer rate (ITR; bits/min). To alleviate the ITR degradation, we propose a more practical hBCI operated by intuitive mental tasks, such as mental arithmetic (MA) and word chain (WC) tasks, performed within a short trial length (5 s). In addition, the suitability of the WC as a BCI task was assessed, which has so far rarely been used in the BCI field. In this experiment, EEG and NIRS data were simultaneously recorded while participants performed MA and WC tasks without preliminary training and remained relaxed (baseline; BL). Each task was performed for 5 s, which was a shorter time than previous hBCI studies. Subsequently, a classification was performed to discriminate MA-related or WC-related brain activations from BL-related activations. By using hBCI in the offline/pseudo-online analyses, average classification accuracies of 90.0 ± 7.1/85.5 ± 8.1% and 85.8 ± 8.6/79.5 ± 13.4% for MA vs. BL and WC vs. BL, respectively, were achieved. These were significantly higher than those of the unimodal EEG- or NIRS-BCI in most cases. Given the short trial length and improved classification accuracy, the average ITRs were improved by more than 96.6% for MA vs. BL and 87.1% for WC vs. BL, respectively, compared to those reported in previous studies. The suitability of implementing a more practical hBCI based on intuitive mental tasks without preliminary training and with a shorter trial length was validated when compared to previous studies.	bl (logic);baseline dental cement;baseline (configuration management);book citation index;brain neoplasms;brain-computer interfaces;conflict (psychology);electroencephalography phase synchronization;elegant degradation;emoticon;emulator;experiment;fints;hemodynamics;interface device component;interleukin receptor common gamma subunit;length of trial;maxima and minima;mental disorders;metabolic process, cellular;microsoft word for mac;morphologic artifacts;neuroimaging;online and offline;programming paradigm;pseudo brand of pseudoephedrine;relaxation;spectroscopy, near-infrared;water consumption;write combining;writing commons;recurrent childhood brain stem glioma	Jaeyoung Shin;Do-Won Kim;Klaus-Robert Müller;Han-Jeong Hwang	2018		10.3390/s18061827	brain–computer interface;electronic engineering;engineering;electroencephalography;temporal resolution;information transfer;neuroimaging;5s;artificial intelligence;pattern recognition;trial length	ML	12.918608676710804	-91.89692459132422	1951
aebbbe4a3d528a011c51143c5c9406a16540bcb8	a stereo video object segmentation algorithm based on motion detection and disparity	mathematical morphology;image segmentation;image matching;object segmentation stereo video motion detection disparity estimation;object segmentation motion detection motion segmentation video sequences noise change detection algorithms estimation;stereo image processing;stereo image processing image matching image segmentation mathematical morphology object detection;mathematic morphology post processing stereo video object segmentation motion disparity research hotpot motion information edging features moving object extraction video sequences eight neighbor motion detection change detection mask stereo matching edging information;object detection	With the development of the stereo video, the technology of the stereo video object segmentation has gradually become a research hotpot. In this paper, a stereo video object segmentation algorithm based on motion detection and disparity is proposed, which utilizes the information of motion, disparity and edging features to extract moving objects from video sequences. Firstly, the eight-neighbor motion detection is used to get the change detection mask (CDM) of moving objects. Secondly, the stereo matching is adopted to get the disparity map which will then be used to modify the CDM. Finally, the edging information and mathematic morphology post-processing are applied to get the accurate video object. Experimental results demonstrate that the proposed algorithm can successfully extract moving objects from stereo video sequences whose background have other moving objects.	algorithm;computer stereo vision;conceptual schema;galaxy morphological classification;map;stereopsis;video post-processing	Lingyun Wang;Zhaohui Li;Dongmei Li	2012	2012 3rd IEEE International Conference on Network Infrastructure and Digital Content	10.1109/ICNIDC.2012.6418774	computer stereo vision;stereo cameras;computer vision;geography;segmentation-based object categorization;video tracking;pattern recognition;scale-space segmentation;computer graphics (images)	Vision	48.34638009974346	-53.389905539311016	1952
5ecfeaf642cf7bcfc94f1f06e2715f22a3ab68f8	texture feature coding method for classification of liver sonography	diagnostic tool;maximum likelihood;texture classification;texture features;system performance;texture analysis;classification system;chronic liver disease;quantitative image analysis;ultrasonography;co occurrence matrix	This paper introduces a new texture analysis method called texture feature coding method (TFCM) for classification of ultrasonic liver images. The TFCM transforms a gray-level image into a feature image in which each pixel is represented by a texture feature number (TFN) coded by TFCM. The TFNs obtained are used to generate a TFN histogram and a TFN co-occurrence matrix (CM), which produces texture feature descriptors for classification. Four conventional texture analysis methods that are gray-level CM, texture spectrum, statistical feature matrix and fractal dimension, are used also to classify liver sonography for comparison. The supervised maximum likelihood (ML) classifiers implemented by different type texture features are applied to discriminate ultrasonic liver images into three disease states that are normal liver, liver hepatitis and cirrhosis. The 30 liver sample images proven by needle biopsy are used to train the ML system that classify on a set of 90 test sample images. Experimental results show that the ML classifier together with TFCM texture features outperforms one with the four conventional methods with respect to classification accuracy.	analysis method;chemical and drug induced liver injury;classification;co-occurrence matrix;document-term matrix;feature model;fibrosis;fractal dimension;medical ultrasound;pixel;tribe floodnet;ultrasonics (sound);ultrasonography	Ming-Huwi Horng;Yung-Nien Sun;Xi-Zhang Lin	1996	Computerized medical imaging and graphics : the official journal of the Computerized Medical Imaging Society	10.1007/BFb0015537	computer vision;computer science;ultrasonography;pattern recognition;computer performance;maximum likelihood;co-occurrence matrix;statistics	ML	34.704158577691096	-74.70168559323004	1955
21debe3e4df6346e024eb2eb4d08c04715828049	bias estimation for angle-only sensors in distributed multi-target tracking systems		This paper describes a method of automatic sensor bias calculation for angle only sensor models in a target tracking scenario. It is assumed that separate Kalman filters are employed by each sensor and no measurements of known landmarks are available. Accurate bias estimation is achieved through the use of pseudo measurements of slant range from each sensor to the target and pseudo measurements of each sensor's bias. The covariance intersection (CI) algorithm is used to produce a pseudo measurement of slant range. This pseudo measurement of range allows pseudo measurements of sensor bias to be calculated based on each sensor's residuals and Kalman gains. Substantially improved tracking performance is demonstrated when estimating and accounting for constant biases on each sensor.	algorithm;covariance intersection;extended kalman filter;image sensor;kalman filter;tracking system	Sean R. Martin;Cameron K. Peterson	2017	2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2017.8206308	kalman filter;artificial intelligence;computer vision;slant range;covariance intersection;computer science;tracking system	Robotics	55.44408173942935	-37.041918486546244	1959
cddee67c70ad76ad6505fe7148effdb2e4cac7b5	generalized h⃗∞ model reduction for stable two-dimensional discrete systems	finite frequency ff;two dimensional 2 d systems;model reduction;article;generalized h norm	For model reduction, the approximation performance sometimes needs to be enhanced over a specific frequency range. Motivated by this fact, the paper investigates generalized $$H_{\infty }$$H? model reduction for stable two-dimensional (2-D) discrete systems represented by the Roesser model and the Fornasini---Machesini local state-space model, respectively. The generalized $$H_{\infty }$$H? norm of 2-D systems is introduced to evaluate the approximation error over a specific finite frequency (FF) domain. In light of the 2-D generalized Kalman---Yakubovich---Popov lemmas, sufficient conditions in terms of linear matrix inequalities are derived for the existence of a stable reduced-order model satisfying a specified generalized $$H_{\infty }$$H? level. Several examples are provided to illustrate the effectiveness and advantages of the proposed method. Compared with most of the existing 2-D model reduction results, the proposed method has the following merits: (1) The generalized $$H_\infty $$H? model reduction problems are considered for both important types of 2-D models, and no structural assumption is made for the plant model, so that our method has a broader applicable scope. (2) The reduced-order model is guaranteed to be stable, and an upper bound on the generalized $$H_{\infty }$$H? error is provided. Moreover, no frequency weighting function is needed. (3) The proposed method is applicable for 2-D model reduction with multiple FF specifications.		Xianwei Li;James Lam;Kie Chung Cheung	2016	Multidim. Syst. Sign. Process.	10.1007/s11045-014-0306-3	mathematical optimization;mathematical analysis;discrete mathematics;calculus;control theory;mathematics	DB	69.81912691310016	4.056234926208789	1961
92b0774d6e2c06aa1de026c7d81c996c8a560abd	increasing accuracy of pulse transit time measurements by automated elimination of distorted photoplethysmography waves	photoplethysmography (ppg);pressure pulse waves (pws);pulse transit time (ptt);algorithm	Photoplethysmography (PPG) is a widely available non-invasive optical technique to visualize pressure pulse waves (PWs). Pulse transit time (PTT) is a physiological parameter that is often derived from calculations on ECG and PPG signals and is based on tightly defined characteristics of the PW shape. PPG signals are sensitive to artefacts. Coughing or movement of the subject can affect PW shapes that much that the PWs become unsuitable for further analysis. The aim of this study was to develop an algorithm that automatically and objectively eliminates unsuitable PWs. In order to develop a proper algorithm for eliminating unsuitable PWs, a literature study was conducted. Next, a ‘7Step PW-Filter’ algorithm was developed that applies seven criteria to determine whether a PW matches the characteristics required to allow PTT calculation. To validate whether the ‘7Step PW-Filter’ eliminates only and all unsuitable PWs, its elimination results were compared to the outcome of manual elimination of unsuitable PWs. The ‘7Step PW-Filter’ had a sensitivity of 96.3% and a specificity of 99.3%. The overall accuracy of the ‘7Step PW-Filter’ for detection of unsuitable PWs was 99.3%. Compared to manual elimination, using the ‘7Step PW-Filter’ reduces PW elimination times from hours to minutes and helps to increase the validity, reliability and reproducibility of PTT data.	activated partial thromboplastin time measurement;conflict (psychology);excretory function;filter (signal processing);license;metal gear solid: peace walker;morphologic artifacts;oximetry, pulse;patients;photoplethysmography;population parameter;pulse pressure;pulse transit time;srgn gene;sensitivity and specificity;algorithm;orders - hl7publishingdomain;standards characteristics	Marit H. N. van Velzen;Arjo J. Loeve;Sjoerd P. Niehof;Egbert G. Mik	2017		10.1007/s11517-017-1642-x	simulation;telecommunications;engineering;surgery	HCI	16.023054757540503	-85.43354637008228	1966
ab438d2d0ac7de08c464c6255d9b6da85044c666	optimising noisy objective functions	stochastic algorithm;pure random search;global optimisation;objective function;random noise;noisy objective function;random search	This discussion paper considers the use of stochastic algorithms for solving global optimisation problems in which function evaluations are subject to random noise. An idea is outlined for discussion at the forthcoming Stochastic Global Optimisation 2001 workshop in Hanmer in June; we propose that a noisy version of pure random search be studied.		David W. Bulger;H. Edwin Romeijn	2005	J. Global Optimization	10.1007/s10898-004-9969-x	mathematical optimization;random field;random search;stochastic optimization;machine learning;random function;mathematics;statistics	Theory	31.017230753628873	0.7578152500446367	1967
58306f011478c9ce2649f0c65a996e5f405dd8e3	diffusion in random networks: impact of degree distribution		"""Word of Mouth (WoM) is known as a powerful marketing force, as numerous empirical studies reveal that consumers' purchasing decisions are based on the advice of those in their social networks rather than on direct advertising. A recent international survey by Nielsen reports that 92% of consumers around the world count on recommendations from friends and family more than all other forms of advertising. With technological advances in online communications that enable consumers to easily share their experience with their """"friends,"""" the effect and importance of WoM has only grown. In this new era, firms not only harness the power of WoM, but also improve its efficacy by targeting consumers based on the wealth of information available about their online activities. In particular, firms can utilize information on connections among consumers (1) to predict the diffusion trajectory (for both time and cost), and (2) to devise effective seeding strategies to impact the trajectory. In this work, we provide a theoretical framework to study the diffusion process for a general class of network models and drive insights on the impact of heterogeneity in the degree of connections on the cost and speed of diffusion as well as on optimal seeding strategies.  To this end, we study a diffusion process of a new product that spreads through the contacts that adopters make with their neighbors. In particular, we assume that an adopter makes contact with each of her neighbors according to an independent Poisson process with rate γ. We assume that the network underlying the connections is a random network with a given degree distribution. This general class of network models has been extensively used in the study of social networks, and it serves as the network model when the firm's knowledge about the pattern of connections is limited to the degree of each agent, rather that having access to the identity of every neighbor of an agent.  In our setting, the firm incurs a fixed cost c for each contact by an adopter. Further, we assume that the firm has a budget for seeding. In particular, prior to the adoption process, he can directly contact a fraction q > 0 of agents, who become adopters. The firm decides who to seed with the goal of minimizing the total cost or the total time to reach his target proportion of adopters. Targeting agents based on social network information is a common practice. It has been studied in network economics literature in monopolistic settings as well as in competitive settings, mainly under the assumption that the firm has complete information about the network structure. However, in our setting the firm can only target based on the degree of an agent, since he does not have access to more detailed information. Seeding agents based on their degrees seems to be more practical (as it requires the firm to acquire much less information), and empirically it has been shown to be effective.  Summary of Our Contributions: In the above setting,  (1) We compute the cost and time to reach any adoption proportion q < s < 1, for any general bounded degree distribution in the limit as the number of agents grows. To the best of our knowledge, this is the first exact characterization of the diffusion process for such a general class of degree distributions. The other exact characterizations are for the special cases of a complete network, which is equivalent to the Bass model, and a one-dimensional grid (i.e., a cycle).  (2) Using our exact characterization, we study the impact of degree distribution on the cost and time to reach any adoption proportion q < s < 1 and demonstrate a trade-off between contact cost and speed. In particular, we show that lower variability in the degree distribution results in lower cost. Fixing the average degree k ∈ N, the most cost-efficient network (to reach any adoption fraction s) is the k-regular network. The impact of degree distribution on timing is more involved, as it depends on the target adoption level (i.e., s) as well as higher moments of the degree distribution. However, our numerical analysis suggests that unless the target level is very high (e.g., s = 0.9), higher variance improves the speed.  (3) We also study the problem of optimal seeding that the firm faces for a network with a given degree distribution. Somewhat contrary to the general wisdom, we show that the optimal strategy does not necessarily entail seeding high degree agents. In fact, we prove that for the objective of minimizing cost (to achieve any target level of adoption), the optimal strategy is to maximally seed low degree agents. For the objective of minimizing time, the optimal strategy depends on the target level s and on the seeding budget q. We present examples illustrating that the optimal strategy can be to seed a mixture of high and low degree agents.  (4) In the absence of seeding, diffusion has a very slow start simply because there are not enough adopters to make contacts. We also study diffusion in such a setting by assuming that the diffusion starts with a single (randomly selected) agent. We characterize the cost and time it takes to reach α log(n) adopters, where n is the number of agents in the network, and α > 0 is a constant. We call this phase of diffusion the early adoption regime. We provide comparative statics with respect to the degree distribution in the early adoption regime, and we show that the cost is independent of degree distribution. Further, the time to diffuse to α log(n) adopters only depends on the first two moments of the degree distribution. Fixing the average degree, the time decreases as variance increases.1"""	basolateral sorting signal;cost efficiency;degree distribution;face;flow network;heart rate variability;network model;numerical analysis;purchasing;random graph;randomness;sample variance;seeding;social network;tcp congestion control;cellular targeting	Vahideh Manshadi;Sidhant Misra;Scott Rodilitz	2018		10.1145/3230654.3230661	econometrics;fixed cost;complete information;degree distribution;total cost;distributed computing;comparative statics;network economics;monopolistic competition;social network;computer science	ECom	-2.3499779924679483	-3.0767971173290003	1969
488cdfd2730c646b3c81bfd7f37f74939849810c	bayesian active object recognition via gaussian process regression	object recognition;optimisation;regression analysis bayes methods gaussian processes object recognition optimisation;gaussian processes;bayes methods;mutual information bayesian active object recognition gaussian process regression camera parameter image features likelihood object classes bayesian state estimation problem sequential optimization problem;regression analysis;cameras object recognition mutual information optimization entropy training bayesian methods	This paper is concerned with a Bayesian approach of actively selecting camera parameters in order to recognize a given object from a finite set of object classes. Gaussian process regression is applied to learn the likelihood of image features given the object classes and camera parameters. In doing so, the object recognition task can be treated as Bayesian state estimation problem. For improving the recognition accuracy and speed, the selection of appropriate camera parameters is formulated as a sequential optimization problem. Mutual information is considered as optimization criterion, which aims at maximizing the information from camera observations or equivalently at minimizing the uncertainty of the state estimate.	action potential;active object;bayesian network;kriging;mathematical optimization;mutual information;optimization problem;outline of object recognition;recursion (computer science);reinforcement learning;robot;robotic arm;simulation;spatial variability;statistical classification	Marco F. Huber;Tobias Dencker;Masoud Roschani;Jürgen Beyerer	2012	2012 15th International Conference on Information Fusion		machine learning;pattern recognition;mathematics;3d single-object recognition;statistics	Robotics	36.60162494069265	-40.178911072058455	1970
f84a8947cead00599381fedd3c5acf2a5c2a76bd	from the discrete kinetic theory to modelling open systems of active particles	matematicas aplicadas;mathematiques appliquees;theorie cinetique;teoria sistema;teoria cinetica;closed system;kinetic theory;systems theory;closed and open systems;theorie systeme;active particles;open system;applied mathematics	This work deals with a methodological development of the kinetic theory for open systems of active particles with discrete states. It essentially refers to the derivation of mathematical tools which provide the guidelines for modelling open systems in different fields of applied sciences. After a description of closed systems, mathematical frameworks suitable for depicting the evolution of open systems are proposed. Finally, some research perspectives towards modelling are outlined. c © 2007 Elsevier Ltd. All rights reserved.	closed system;mathematical model;open system (computing)	I. Brazzoli	2008	Appl. Math. Lett.	10.1016/j.aml.2007.02.018	kinetic theory;open and closed systems in social science;applied mathematics;calculus;mathematics;closed system;open system;thermodynamics;systems theory;quantum mechanics	AI	85.18852385027964	2.7148387224938544	1971
66a947bc077b01a1abbe87ce11227845a83c96d7	a joint matrix factorization approach to unsupervised action categorization	matrix factorization;weizmann data set matrix factorization unsupervised action categorization action video sequences action representation learning model 2d action matrices;video signal processing;learning model;bismuth;prototypes;action representation;video signal processing image sequences matrix decomposition;video sequences;time series;distance measurement;joint matrix factorization action categorization;time series analysis;spatial distribution;matrix decomposition;feature extraction;pixel;joint matrix factorization;prototypes matrix decomposition tensile stress video sequences matrix converters symmetric matrices computer science surveillance feature extraction electroencephalography;action categorization;image sequences	In this paper, a novel unsupervised approach to mining categories from action video sequences is presented. This approach consists of two modules: action representation and learning model. Videos are regarded as spatially distributed dynamic pixel time series, which are quantized into pixel prototypes. After replacing the pixel time series with their corresponding prototype labels, the video sequences are compressed into 2D action matrices. We put these matrices together to form an multi-action tensor, and propose the joint matrix factorization method to simultaneously cluster the pixel prototypes into pixel signatures, and matrices into action classes. The approach is tested on public and popular Weizmann data set, and promising results are achieved.	action potential;categorization;cluster analysis;data mining;java media framework;pixel;prototype;quantization (signal processing);time series;type signature;unsupervised learning	Peng Cui;Fei Wang;Lifeng Sun;Shiqiang Yang	2008	2008 Eighth IEEE International Conference on Data Mining	10.1109/ICDM.2008.59	computer vision;computer science;machine learning;time series;pattern recognition;mathematics;matrix decomposition;statistics	Vision	36.64490621459046	-47.82826537202107	1973
998cb367603038192ee76859870a3b7feffa51d4	hand gesture recognition method based on hog-lbp features for mobile devices		Abstract Hand Gesture recognition becomes a challenging task in computer vision field especially after the appearance of mobile devices. Although gesture recognition algorithms have been widely applied to the human-mobile interaction systems, it is difficult to meet the real time requirements and the robustness in different lightening conditions and backgrounds. This paper provides a description of a practical investigation into ways of improving hand gesture recognition algorithms for computationally limited handheld Android devices. We thoroughly researched possible approaches and find out how they affect the mobile device in terms of execution time and energy consumption. We have identified a combined HOG-LBP methodology whose performance improved upon the detection rate of other systems. In this paper, we present a static hand gesture recognition system for mobile devices by combining the histogram of oriented gradients (HOG) and local binary pattern (LBP) features, which can accurately detect hand poses. With the help of combining HOG and LBP features, we achieved a detection rate of approximately 92% on the enhanced NUS dataset I. This combination performs a better result than the results obtained when using only LBP or HOG features in term of recognition rate. However, in term of execution and detection time, LBP and HOG perform better than the combined HOG-LBP.		Houssem Lahiani;Mahmoud Neji	2018		10.1016/j.procs.2018.07.259	local binary patterns;gesture recognition;histogram of oriented gradients;machine learning;android (operating system);robustness (computer science);mobile device;computer vision;computer science;artificial intelligence	Robotics	27.69863756595367	-57.45598941749248	1977
9f41e178e0244694f162e82e2f07febc2917fd61	an analysis of camera noise	traitement signal;ccd camera;poisson noise;vision ordenador;ley poisson;vidicon cameras;charge coupled device;image processing;ionization sensors;sistema informatico;procesamiento imagen;direction dependent stationary electronic noise sources;computer system;charge coupled devices;traitement image;computer vision;camera noise;adaptive signal processing;ccd;noise source;signal processing;source bruit;filtro adaptable;camara ccd;camera ccd;vision ordinateur;systeme informatique;vidicon;charged couple device;fuente ruido;filtre adaptatif;adaptive signal processing computer vision camera noise ionization sensors charge coupled device vidicon cameras direction dependent stationary electronic noise sources;loi poisson;procesamiento senal;cameras semiconductor device noise optical noise ionization signal processing optical sensors charge coupled image sensors layout signal processing algorithms optical signal processing;adaptive filter;cameras;poisson distribution;noise cameras charge coupled devices computer vision;noise	The class of cameras that are based on ionization sensors, which includes the most common charge-coupled device (CCD) and vidicon cameras, is examined. Camera signals are shown to be corrupted by direction-dependent stationary electronic noise sources and fluctuations due to the statistical nature of the sensing process. The authors develop and test a model of the inherent noises in cameras. These results are confirmed by measurement, and they suggest a locally stationary model of noise for adaptive signal processing. >		Robert A. Boie;Ingemar J. Cox	1992	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/34.141557	computer vision;image processing;computer science;signal processing;charge-coupled device	Vision	61.39712642648663	-58.539220791924066	1978
b9bba20e79083ca40c9be1c435c74442709587d3	a new robust technique for optimal control of chemical engineering processes	genetic operator;computacion informatica;non linear systems;grupo de excelencia;chemical engineering;optimal control;ciencias basicas y experimentales;quimica;genetic algorithm;genetic algorithms;non linear system	A new optimal control technique is presented to provide good quality, robust solutions for chemical engineering problems, which are generally non-linear, and highly constrained. The technique neither uses any input of feasible control solution, nor any auxiliary condition. The technique generates optimal control by applying the genetic operations of selection, crossover, and mutation on an initial population of random, binary-coded deviation vectors. Each element of a deviation vector corresponds to a control stage, and is a deviation from some “mean” control value randomized initially for that stage. The deviation, and the mean control value map on to the actual discrete step value of control at that stage. The mapping is logarithmic in beginning, but is later allowed to alternate with a linear one. The genetic operations are periodically followed by the replacement of mean control values by a newly available optimal control solution, and by the size-variation of control domain between its limits. The optimal control technique is successfully tested on four challenging problems of chemical engineering. © 2003 Elsevier Ltd. All rights reserved.	control theory;crossover (genetic algorithm);nonlinear system;optimal control;randomized algorithm	Simant R. Upreti	2004	Computers & Chemical Engineering	10.1016/j.compchemeng.2003.09.003	control engineering;linear-quadratic-gaussian control;mathematical optimization;genetic algorithm;nonlinear system;computer science;engineering;artificial intelligence;machine learning;control theory	SE	32.62416805601463	-5.933865643883291	1979
11608f935502346a28907a5a092ace4a18caa2df	hybrid approach to face recognition system using principle component and independent component with score based fusion process		Hybrid approach has a special status among Face Recognition Systems as they combine different recognition approaches in an either serial or parallel to overcome the shortcomings of individual methods. This paper explores the area of Hybrid Face Recognition using score based strategy as a combiner/fusion process. In proposed approach, the recognition system operates in two modes: training and classification. Training mode involves normalization of the face images (training set), extracting appropriate features using Principle Component Analysis (PCA) and Independent Component Analysis (ICA). The extracted features are then trained in parallel using Back-propagation neural networks (BPNNs) to partition the feature space in to different face classes. In classification mode, the trained PCA BPNN and ICA BPNN are fed with new face image(s). The score based strategy which works as a combiner is applied to the results of both PCA BPNN and ICA BPNN to classify given new face image(s) according to face classes obtained during the training mode. The proposed approach has been tested on ORL and other face databases; the experimented results show that the proposed system has higher accuracy than face recognition systems using single feature extractor. KEYWORD: Face Recognition, Feature Extractor, Hybrid System, ICA, PCA, Neural Network, Score based strategy.	artificial neural network;database;diplexer;facial recognition system;feature vector;hybrid system;independent computing architecture;independent component analysis;principal component analysis;randomness extractor;return loss;software propagation;test set	Trupti M. Kodinariya	2014	CoRR		speech recognition;computer science;machine learning;pattern recognition	AI	26.728189625950375	-62.7694314711735	1981
1b497b4cfca8583792feddddb5e348189831cf56	a maximum set of (26, 6)-connected digital surfaces	analisis imagen;image processing;procesamiento imagen;traitement image;analyse combinatoire;image analysis;analyse image;analisis combinatorio;combinatorial analysis	In the class $\mathcal{H}$ of (26,6)–connected homogeneous digital spaces on R3 we find a digital space EU with the largest set of digital surfaces in that class. That is, if a digital objet S is a digital surface in any space $E \epsilon \mathcal{H}$ then S is a digital surface in EU too.		Jose C. Ciria;Angel de Miguel;Eladio Domínguez;Angel R. Francés;Antonio J Quintero	2004		10.1007/978-3-540-30503-3_22	computer vision;image analysis;image processing;computer science;mathematics;geometry	Graphics	50.693044462972324	-60.68977258776872	1984
127b9404d361e251ffd5e915a06ae05907096c7f	analysis of the imageries from a chinese cube satellite for polar observations		The research for the global climate changes call for high quality satellite data and imageries regarding Polar Regions. In recent years, the development of immerging Earth-Observation micro/nano satellite technology provides new data source for polar region observations. The STU-2A is a newly-developed nano satellite specializing in polar region observation activities. It is a 3U CubeSat of 2.9kg with a size of 30×10×10 cm and carrying an earth observation camera, and was launched on Sept. 25th, 2015 and be deployed in a 470 × 485km, 97.3-degree inclination Sun Synchronous Orbit (SSO). During the Antarctic summer of 2015/16, it has obtained images covering different ocean and coastal regions including Amundsen Sea, Ross Sea and Vincennes Bay. These images were used to analyze the change of ice sheet and sea ice and have played a role in the navigation task of the Xuelong vessel, a Chinese polar research ship. This satellite provides 100m resolution visible-light true color images, better than the MODIS data, which can only reach a maximum resolution of 250m. As the camera was specially designed for the polar region which has an environment of low solar elevation and high surface reflectance, it eliminates the oversaturation problem of the MODIS sensors and can provide high quality images. Based on data analysis and assessment, it is confirmed that this satellite data can meet the demand of glacier and sea ice observation. This paper will discuss the Cubesat design configuration, the payload camera design, and present its application in Antarctic glacier and sea ice observation.	color depth;display resolution;gnu nano;google summer of code;sensor	Yanmei Zhang;Xiao Cheng	2017	2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)	10.1109/IGARSS.2017.8127763	remote sensing;ice sheet;sea ice;elevation;computer science;glacier;earth observation;image resolution;sun-synchronous orbit;climate change	Embedded	79.78864121718985	-61.55043161603111	1985
0bf7f95fdcc3d3daff9f25d9af69e85763687d4b	neural network application in financial area		Forecasting financial data is an extremely important issue and it is a good opportunity to demonstrate the capabilities of the neural networks. The objective of this study is to develop a neural network model for forecasting the direction of movement of financial data one step forward. The architecture of a neural network uses four different technical indicators, which are based on the raw data and the current day of the week. The training method is algorithm with back propagation of the error. The program realization and experimental results are considered in this article.	algorithm;artificial neural network;backpropagation;network model;software propagation;teaching method	Roumen Trifonov;Dilyana Budakova;Galya Pavlova	2017		10.1145/3134302.3134336	architecture;artificial neural network;computer science;data mining;raw data;finance;backpropagation	ML	8.46085540694914	-21.323482493293614	1986
5e393b23a51b84f33e83298888e78e283ea9e077	high-performance fpga architecture for multi-line beamforming in ultrasound applications	real time processing;fpga;ultrasound system;high frame rate;beamformer	"""Nowadays, several applications are requested to process huge amount of data with tight temporal constraints to produce """"real time"""" results. This is the case of ultrasound (US) tomographic (B-mode) images used in diagnostics. A B-mode image is typically composed by many tens of vertical lines. Each line is calculated by the so-called beamformer (BF) that combines the echoes received from tens of small piezoelectric transducers integrated in the US probe. The BF processes massive quantity of data (several Gb/s) and is implemented in fast digital devices like the FPGAs. In the standard approach, the B-mode image is formed on a line-by-line basis, i.e. one line per pulse repetition interval (PRI). This limits the achievable image frame rate (IFR) to few tens of Hz, which is not suitable to detail fast, important phenomena like the heart valve movement. In the so-called plane-wave approach, the data needed to produce all of the B-mode image lines are acquired in a single PRI. In this case, the IFR can be in the order of kHz, but a more complex BF is needed. In this paper, we present a Multi-Line BF architecture, capable of real-time processing tens of lines per PRI. The BF was implemented in the research ultrasound system ULA-OP 256. A sample B-mode sequence obtained at IFR = 600Hz is presented."""	beamforming;brainfuck;decimation (signal processing);field-programmable gate array;instrument flight rules;piezoelectricity;real-time clock;real-time computing;tomography;transducer;vertical blanking interval	Valentino Meacci;Luca Bassi;Stefano Ricci;Enrico Boni;Piero Tortoli	2016	2016 Euromicro Conference on Digital System Design (DSD)	10.1109/DSD.2016.111	embedded system;real-time computing;telecommunications;computer science;field-programmable gate array	Graphics	44.17870469161229	-3.108195988172	1989
048722fe8f1264818c82c4a3dbd3f2472eeb0353	active matrix liquid crystal displays (amlcds)			active matrix;active-matrix liquid-crystal display	Mervyn Rose	2012		10.1007/978-3-540-79567-4_95	crystallography;active matrix;liquid-crystal display;materials science	HCI	94.34874249096534	-13.854163119132716	1990
737d3d61a2bf7801688cb08651dd7cf58b73f18b	underwater robot with a buoyancy control system based on the spermaceti oil hypothesis development of the depth control system	underwater robot;robot sensing systems;water depth;robot sensing systems petroleum wire heating cooling robot control;underwater vehicles;peltier effect;underwater vehicles microcomputers mobile robots peltier effect pressure sensors;spermaceti oil hypothesis;mobile robots;microcomputer spermaceti oil paraffin wax sperm whale underwater robot depth control circuit pressure sensor;heating;peltier element;microcomputer;wire;buoyancy control system;pressure sensor;paraffin wax underwater robot buoyancy control system spermaceti oil hypothesis depth control system robot control system microcomputer sperm whale peltier element;depth control circuit;control system;petroleum;robot control;robot control system;depth control system;pressure sensors;spermaceti oil;microcomputers;sperm whale;cooling;paraffin wax	The goal of this paper is to develop a robot control system using a microcomputer for an underwater robot with a buoyancy control system based on the spermaceti oil hypothesis. Sperm whales have a spermaceti organ in their head that is filled with spermaceti oil. It is said that sperm whales control their buoyancy by melting or coagulating spermaceti oil. In the previous papers proposed an underwater robot with a buoyancy control device using its theory. The robot was radio controlled and it had no sensor. Therefore, the robot could not detect water depth. Also, it was not able to control its own depth. In addition, the robot could not dive because it had no cooling system. In this paper, we built a depth control circuit using a microcomputer and pressure sensor. The pressure sensor detected the robot's depth, and the microcomputer controlled the robot's position. And, we made a new buoyancy control device using a peltier element. A peltier element was used for heating and cooling systems of the paraffin wax. Then, we experimented with the robot control. As a result, we confirmed that when we ordered a target water depth, the robot dived, floated and maintained the depth itself by detecting its own position. In addition, we confirmed that when a disturbance occurs, the robot returned to the former depth.	computer cooling;control system;microcomputer;radio control;robot control;sensor	T. Inoue;Koji Shibuya;A. Nagano	2010	2010 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2010.5653493	control engineering;simulation;computer science;engineering;control system;artificial intelligence;pressure sensor;microcomputer;marine engineering	Robotics	76.32258537643011	-23.56816799349976	1991
c3215d9abaa3a28fadf657620fcd5c6732b83086	bayesian geometric model for line network extraction from satellite images	satellite images;monte carlo methods terrain mapping bayes methods feature extraction markov processes topography earth simulated annealing;topography earth;image segmentation;monte carlo markov chain;bayes methods;reversible jump monte carlo markov chain algorithm geographical data bayesian geometric model unsupervised line network extraction stochastic geometry method satellite images linear structure presence detection markov object process network topological properties simulated annealing optimization rjmcmc algorithm hydrographic network extraction;bayesian methods;random variables;simulated annealing;data mining;markov object process;topological properties;hydrographic network extraction;geology;reversible jump;bayesian methods solid modeling satellites roads data mining pixel simulated annealing random variables image segmentation geology;roads;geographical data;feature extraction;satellites;pixel;solid modeling;rjmcmc algorithm;satellite image;stochastic geometry method;geometric model;terrain mapping;markov processes;bayesian geometric model;unsupervised line network extraction;reversible jump monte carlo markov chain algorithm;network topological properties;monte carlo methods;simulated annealing optimization;stochastic geometry;linear structure presence detection;bayesian model	This paper presents a two-step algorithm to perform an unsupervised extraction of line networks from satellite images, within a stochastic geometry framework. First, we propose a new operator, providing a measure of the possibility of linear structure presence on each image pixel. Second, we propose a Bayesian model in order to extract the line network from the operator output. The prior model, a Markov object process, incorporates the topological properties of the network through interactions between objects, while the line operator answers are taken into account in the likelihood. Optimization is realized by simulated annealing using a reversible jump Monte Carlo Markov chain algorithm. An application to hydrographic network extraction is presented.	algorithm;bayesian network;geometric modeling;interaction;monte carlo method;pixel;reversible-jump markov chain monte carlo;simulated annealing	Caroline Lacoste;Xavier Descombes;Josiane Zerubia;Nicolas Baghdadi	2004	2004 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2004.1326607	random variable;computer vision;variable-order bayesian network;simulated annealing;markov chain monte carlo;feature extraction;bayesian probability;computer science;geometric modeling;machine learning;pattern recognition;mathematics;image segmentation;markov process;solid modeling;bayesian inference;satellite;pixel;statistics;stochastic geometry;monte carlo method	Robotics	61.58246423130532	-72.40237532738473	1993
7e767de6142acde126edbf72b1af9670bb14bd0a	3d qsar selectivity analyses of carbonic anhydrase inhibitors: insights for the design of isozyme selective inhibitors		A 3D QSAR selectivity analysis of carbonic anhydrase (CA) inhibitors using a data set of 87 CA inhibitors is reported. After ligand minimization in the binding pockets of CA I, CA II, and CA IV isoforms, selectivity CoMFA and CoMSIA 3D QSAR models have been derived by taking the affinity differences (DeltapKi) with respect to two CA isozymes as independent variables. Evaluation of the developed 3D QSAR selectivity models allows us to determine amino acids in the respective CA isozymes that possibly play a crucial role for selective inhibition of these isozymes. We further combined the ligand-based 3D QSAR models with the docking program AUTODOCK in order to screen for novel CA inhibitors. Correct binding modes are predicted for various CA inhibitors with respect to known crystal structures. Furthermore, in combination with the developed 3D QSAR models we could successfully estimate the affinity of CA inhibitors even in cases where the applied scoring function failed. This novel strategy to combine AUTODOCK poses with CoMFA/CoMSIA 3D QSAR models can be used as a guideline to assess the relevance of generated binding modes and to accurately predict the binding affinity of newly designed CA inhibitors that could play a crucial role in the treatment of pathologies such as tumors, obesity, or glaucoma.		Alexander Weber;Markus Böhm;Claudiu T. Supuran;Andrea Scozzafava;Christoph A. Sotriffer;Gerhard Klebe	2006	Journal of chemical information and modeling	10.1021/ci600298r	carbonic anhydrase;isozyme;chemistry;amino acid;quantitative structure–activity relationship;autodock;biochemistry	ML	9.84349986939892	-60.855634626605145	1996
dd015994b2d22332c1b24659c09ff1cf0a829bdb	effects of clumping on modelling lidar waveforms in forest canopies	clumping effects crown archetypes lai function crown macrostructure function radiative transfer lidar waveform modelling forest canopies realistic trees leaf area index forest canopies;radiative transfer;clumping;canopy structure;vegetation;optical radar;clumping lidar waveforms radiative transfer canopy structure lai;laser radar vegetation remote sensing biological system modeling solid modeling educational institutions scattering;lidar waveforms;lai;vegetation geophysical techniques optical radar;geophysical techniques	Empirical relations are frequently used to derive leaf area index (LAI). Such relations often make assumptions that make it hard to link the derived LAI to realistic trees and forest canopies. In previous work we developed a set of analytical expressions to describe LiDAR waveforms with only a limited number of assumptions based on radiative transfer. These expressions were a function of crown macro-structure and LAI. The expressions were successfully tested when applied on crown archetypes, but showed significant error when applied to more realistic crowns. In this study, we analyse the effect of clumping on inferring LAI from realistic trees. Despite the potential of the expressions to detect subtle changes in LAI, absolute inferred LAI values can be significantly off. However, the strong correlation between true and inferred LAI (R2 >; 0.97) for the two test cases in this study, allows for calibration of the inferred LAI values.	bus bunching;crown group;test case	Kim Calders;Philip Lewis;Mathias Disney;Jan Verbesselt;John Armston;Martin Herold	2012	2012 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2012.6350693	radiative transfer;hydrology;physics;vegetation;remote sensing	Embedded	86.2496020786174	-62.95464856140342	1999
ae489181458e512610218eac9826a7a54652a890	prefrontal cortex transcranial direct current stimulation via a combined high definition and conventional electrode montage: a fem modeling studying		The document that should appear here is not currently available.	appear here;finite element method;montagejs;transcranial direct-current stimulation	Dennis Q. Truong;Abhishek Datta;Jiansong Xu;Felipe Fregni;Marom Bikson	2012	2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/EMBC.2012.6347509	cognitive psychology;neuroscience;developmental psychology	DB	24.964580322257245	-83.9593648444732	2000
a9178155f72c7bf07861dd96fbf922a1ee2718a9	a robust adaptive tracking control method for a rehabilitative walker using random parameters	rehabilitative training walker;adaptive control;random parameters;reliable tracking control	ABSTRACTThe present paper investigates stochastic modelling and a new nonlinear reliable tracking control method for a rehabilitative training walker. The stochastic model is constructed by considering random parameters. A new nonlinear tracking method against actuator fault is proposed based on redundant degree of freedom and a state feedback controller is designed by exploiting an adaptive control technique. It is proved that the mean square of the trajectory tracking error can be made arbitrarily small by choosing appropriate design parameters. As an application, simulation results confirm the effectiveness of the proposed method and verify that the walker with random parameters can provide safe sequential motion when one wheel actuator is at fault.	amiga walker;fault (technology);lyapunov fractal;mean squared error;nl (complexity);open desktop workstation;robot;simulation	Hongbin Chang;Ping Sun;Shuoyu Wang	2017	Int. J. Control	10.1080/00207179.2016.1209562	control engineering;simulation;adaptive control;engineering;control theory;mathematics	Robotics	65.60144903202509	-9.767938793673267	2002
890b044d347da4a2ad158c6c568fa2e98e956853	multiple surface identification and matching in magnetic resonance images	magnetism;magnetic resonance imaging	An iterative algorithm is presented for simultaneous deformation of multiple curves and surfaces to an MRI, with inter-surface constraints and self-intersection avoidance. The resulting robust segmentation, combined with local curvature matching, automatically creates surfaces of MRI datasets with a common mapping to surface parametric space.© (1994) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.	resonance	David A. Macdonald;David Avis;Alan C. Evans	1994		10.1117/12.185176	computer vision;mathematical optimization;mathematics;nuclear magnetic resonance	Vision	45.47241691852211	-76.57186948730956	2004
0d959fa0c554c0c2004a3cbba978e881870e723e	face recognition via adaptive discriminant clustering	databases;xm2vts face database;training;image classification;visual databases face recognition image classification;indexing terms;adaptive classification face recognition adaptive discriminant analysis k means algorithm umist database xm2vts face database training data cluster;data clustering;discriminant analysis;training data;artificial neural networks;face recognition;training data cluster;discriminant analysis adaptive classification face recognition;umist database;adaptive discriminant analysis;adaptive classification;clustering algorithms;k means algorithm;face;face recognition testing training data linear discriminant analysis clustering algorithms databases principal component analysis iterative algorithms informatics artificial intelligence;visual databases	This paper presents a methodology that tackles the face recognition problem by accommodating multiple clustering steps. At each clustering step, the test and training faces are projected to a discriminant space and the projected training data are partitioned into clusters using the k-means algorithm. Then a subset of the training data clusters is selected, based on how similar the faces in these clusters are to the test face. In the clustering step that follows a new discriminant space is defined by processing this subset and both the test and training data are projected to this space. This process is repeated until one final cluster is selected and the most similar, to the test face, face class contained is set as the identity match. The UMIST and XM2VTS face databases have been used to evaluate the algorithm and results indicate that the proposed framework provides a promising solution to the face recognition problem.	algorithm;cluster analysis;database;discriminant;facial recognition system;k-means clustering	Marios Kyperountas;Anastasios Tefas;Ioannis Pitas	2008	2008 15th IEEE International Conference on Image Processing	10.1109/ICIP.2008.4712362	facial recognition system;correlation clustering;fuzzy clustering;computer science;machine learning;pattern recognition;data mining;cluster analysis;linear discriminant analysis;single-linkage clustering;artificial neural network	Vision	15.299964317208023	-44.83276141871307	2006
3be831de3e4307e4ffcb5417cecd53f81c56c5c4	designing simple nonlinear filters using hysteresis of single recurrent neurons for acoustic signal recognition in robots	digital signal processing;neural networks;nonlinear filter;discrete time;low pass filter;non speech recognition;autonomous robots;filter design;transient dynamics;walking robot;band pass filter;walking robots;recurrent neural network;autonomous robot;neural network;dynamic properties	In this article we exploit the discrete-time dynamics of a single neuron with self-connection to systematically design simple signal filters. Due to hysteresis effects and transient dynamics, this single neuron behaves as an adjustable low-pass filter for specific parameter configurations. Extending this neuro-module by two more recurrent neurons leads to versatile highand band-pass filters. The approach presented here helps to understand how the dynamical properties of recurrent neural networks can be used for filter design. Furthermore, it gives guidance to a new way of implementing sensory preprocessing for acoustic signal recognition in autonomous robots.	acoustic cryptanalysis;acoustic fingerprint;artificial neural network;autonomous robot;filter design;hysteresis;low-pass filter;neuron;preprocessor;recurrent neural network	Poramate Manoonpong;Frank Pasemann;Christoph Kolodziejski;Florentin Wörgötter	2010		10.1007/978-3-642-15819-3_50	nonlinear filter;discrete time and continuous time;low-pass filter;computer science;recurrent neural network;digital signal processing;machine learning;control theory;band-pass filter;filter design;artificial neural network	Robotics	42.47343597770416	-28.962166251996113	2010
3a747bf3092baff5e0a358e29de62bc468e31723	attention enhancement system using virtual reality and eeg biofeedback	simulation ordinateur;virtual reality electroencephalography biological control systems medical treatment biomedical imaging pediatrics psychology drugs medical services space technology;realite virtuelle;realidad virtual;electroencefalografia;desordre hyperactivite deficit attention;behavioural sciences computing;therapie cognitive;patient rehabilitation;cognitive behavior therapy;virtual reality;medical signal processing virtual reality electroencephalography feedback patient treatment patient rehabilitation behavioural sciences computing;behavior problems;intelligence artificielle;learning disabled;electroencephalographie;academic performance;intelligence quotient;feedback;terapia cognitiva;cognitive therapy;cociente intelectual;attention deficit hyperactivity disorder;immersive interactive technology attention enhancement system virtual reality eeg biofeedback attention deficit hyperactivity disorder adhd children childhood syndrome attention span impulsiveness hyperactivity learning disabilities behavioral problems patient treatment psychostimulant medication cognitive behavior therapy drugs inattentiveness disruptive behaviors academic performance iq scores rehabilitation health care delivery distractions concentration ability;patient treatment;artificial intelligence;simulacion computadora;inteligencia artificial;electroencephalography;quotient intellectuel;computer simulation;medical signal processing;health care	Attention Deficit Hyperactivity Disorder (ADHD) is a childhood syndrome characterized by short attention span, impulsiveness, and hyperactivity, which often lead ? to learning disabilities and various behavioral problems. For the treatment of ADHD, medication and cognitive-behavior therapy is applied in recent years. Although psychostimulant medication has been widely used for many years, current findings suggest that, as the sole treatment for ADHD, it is an inadequate form of intervention in that parents don’t want their child to use drug and the effects are limited to the period in which the drugs are physiologically active. On the other hand, EEG biofeedback treatment studies for ADHD have reported promising results not only in significant reductions in hyperactive, inattentive, and disruptive behaviors, but also improvements in academic performance and IQ scores. However it is too boring for children to finish the whole treatment. The recent increase in computer usage in medicine and rehabilitation has changed the way health care is delivered. Virtual Reality technology provides specific stimuli that can be used in removing distractions and providing environments that get the subjects’ attention and increasing their ability to concentrate. And Virtual Reality technology can hold a patient’s attention for a longer period of time than other methods can, because VR is immersive, interactive and imaginal. Based on these aspects, we developed Attention Enhancement System (AES) using Virtual Reality technology and EEG biofeedback for assessing and treating ADHD children as well as increasing the attention span of children who have attention difficulty.	auditory processing disorder;electroencephalography;neurofeedback;virtual reality	Baek Hwan Cho;Jong-Min Lee;Jeonghun Ku;Dong Pyo Jang;J. S. Kim;In-Young Kim;Jang-Han Lee;Sun Il Kim	2002		10.1109/VR.2002.996518	computer simulation;intelligence quotient;electroencephalography;computer science;artificial intelligence;feedback;virtual reality;health care	HCI	9.524668559501862	-92.09109553099611	2011
0d16420a7b081c5b849fe048df98e3e666556c35	gaussian kernels for density estimation with compositional data	computadora;eocene;tratamiento datos;hebrides interieures;computers;europa;lave;densite;fanerozoico;normal distribution;ordinateur;europa del oeste;grande bretagne;escocia;simulacion numerica;data processing;tertiary;traitement donnee;densidad;great britain;compositional data;smoothing parameter;lava;cenozoique;algorithme;eoceno;ile de skye;density estimation;royaume uni;paleogene;theory;united kingdom;simulation numerique;kernel density estimate;teoria;western europe;gaussian kernel;simplex;reino unido;isometric log ratio;bandwidth;hebrides;paleogeno;algorithms;ecosse;density;europe;tertiaire;phanerozoic;inner hebrides;cenozoico;terciario;phanerozoique;scotland;hebridas;digital simulation;theorie;europe ouest;algoritmo;gran bretana;cenozoic;hebridas interiores	Common simplifications of the bandwidth matrix cannot be applied to existing kernels for density estimation with compositional data. In this paper, kernel density estimation methods are modified on the basis of recent developments in compositional data analysis and bandwidth matrix selection theory. The isometric log-ratio normal kernel is used to define a new estimator in which the smoothing parameter is chosen from the most general class of bandwidth matrices on the basis of a recently proposed plug-in algorithm. Both simulated and real examples are presented in which the behaviour of our approach is illustrated, which shows the advantage of the new estimator over existing proposed methods.	compositional data	J. E. Chacón;G. Mateu-Figueras;J. A. Martín-Fernández	2011	Computers & Geosciences	10.1016/j.cageo.2009.12.011	normal distribution;paleogene;kernel density estimation;density estimation;data processing;density;geology;tertiary;mathematics;paleontology;phanerozoic;cenozoic;gaussian function;variable kernel density estimation;theory;bandwidth;simplex;statistics;lava	ML	34.07294985532609	-23.922618451789763	2014
48e84fc874466d38062b80c1f7b2d9b0a2b511a4	convergence of a class of cooperative standard cellular neural network arrays	eigenvalues and eigenfunctions;piecewise linear;limit set dichotomy;standard cellular neural networks;convergence;piecewise linear techniques;cellular neural network;cooperative dynamical systems;dynamic system;periodic boundary condition;set theory;cellular neural nets;satisfiability;asymptotic stability;equilibrium point;limit set;artificial neural networks;vectors;cooperative systems;set theory asymptotic stability cellular neural nets convergence cooperative systems piecewise linear techniques;standard cellular neural networks convergence cooperative dynamical systems limit set dichotomy;integrated circuit interconnections;artificial neural networks neurons convergence vectors integrated circuit interconnections cellular neural networks eigenvalues and eigenfunctions;nearest neighbor;neurons;cellular neural networks;neuronal activity;asymptotic stability cooperative standard cellular neural network arrays nonsymmetric standard cellular neural network array cooperative interconnections three segment piecewise linear neuron activation one dimensional cell linking cloning template nearest neighbor interconnections periodic boundary conditions squashing effect eventually strongly monotone cooperative scnn array unstable saddle type equilibrium point dynamical properties limit set dichotomy convergence properties;artificial neural network;dynamic properties	This paper considers a nonsymmetric standard (S) cellular neural network (CNN) array with cooperative (nonnegative) interconnections between neurons and a typical three-segment piecewise-linear (PL) neuron activation. The CNN is defined by a one-dimensional cell-linking (irreducible) cloning template with nearest-neighbor interconnections and has periodic boundary conditions. The flow generated by the SCNN is monotone but, due to the squashing effect of the horizontal segments in the PL activations, is not eventually strongly monotone (ESM). A new method for addressing convergence of the cooperative SCNN array is developed, which is based on the two main tools: (1) the concept of a frozen saddle, i.e., an unstable saddle-type equilibrium point (EP) enjoying certain dynamical properties that hold also for an asymptotically stable EP (a sink); (2) the analysis of the order relations satisfied by the sinks and saddle-type EPs. The analysis permits to show a fundamental result according to which any pair of ordered EPs of the SCNN contains at least a sink or a frozen saddle. On this basis it is shown that the flow generated by the SCNN enjoys a LIMIT SET DICHOTOMY and convergence properties analogous to those valid for ESM flows. Such results hold in the case where the SCNN displays either a local diffusion or a global propagation behavior.	artificial neural network;cell signaling;cellular neural network;control theory;dynamical system;encapsulated postscript;expectation propagation;irreducibility;neuron;periodic boundary conditions;software propagation;monotone	Mauro Di Marco;Mauro Forti;Massimo Grazzini;Luca Pancioni	2012	IEEE Transactions on Circuits and Systems I: Regular Papers	10.1109/TCSI.2011.2169913	mathematical optimization;combinatorics;cellular neural network;discrete mathematics;computer science;control theory;mathematics;artificial neural network	Theory	75.29159452710456	3.365156732990765	2015
27db7bf18f6d5f4a498e277ecf59d2955733c86a	understanding deep networks with gradients	training;neural nets feature extraction image classification;visualization;computational modeling;distributed representation deep neural networks convolutional neural networks neuron relevance measure high prediction gradient units classification task performance loss low gradient units;deep learning;robustness;biological neural networks;conferences;biological neural networks visualization training conferences computational modeling robustness	Existing methods for understanding the inner workings of convolutional neural networks have relied on visualizations, which do not describe the connections between the layers and units of the network. We introduce the prediction gradient as a measure of a neuron's relevance to prediction. Using this quantity, we study a relatively small convolutional neural network and make three observations. First, there exists a small number of high prediction-gradient units, which upon removal, severely impact the ability of the network to classify correctly. Second, this performance loss generalizes spans multiple classes, and is not mirrored by removing low-gradient units. Third, the distributed representation of the neural network prevents performance from being impacted until a critical number of units are destroyed, the number depending highly on the prediction gradient of the units removed. These three observations validate the utility of the prediction gradient in identifying important units in a neural network. We finally use the prediction gradient in order to generate and study adversarial examples.	artificial neural network;convolutional neural network;image gradient;neuron;relevance	Henry Z. Lo;Wei Ding	2015	2015 IEEE International Conference on Data Mining Workshop (ICDMW)	10.1109/ICDMW.2015.227	nervous system network models;visualization;types of artificial neural networks;computer science;artificial intelligence;machine learning;data mining;time delay neural network;deep learning;computational model;robustness	ML	20.402300600064223	-51.802068854095175	2017
a9d82aa004e2701973b557f4391dca77a72d6c49	learning with a friendly interactive robot for service tasks in hospital environments	mobile robot;local system;autonomic system;learning methods;robot control;trajectory generation;trajectory tracking	Our work addresses the problem of mobile robots control and programming. We present FIRST, a Friendly Interactive Robot for Service Tasks, designed to carry heavy loads in hospitals. A learning method is used to teach it the set of all the trajectories it has to follow. During the learning phase, a human operator teleoperates the robot while its sensors are activated. So the trajectory is built according to these data. The telebperation is easy and accurate thanks to several locomotion modes of the robot, specially a crab mode. A “Learning Computer” helps the operator in any phase of the learning mode. As the missions feasibility must be assumed, each trajectory will be checked before used. Furthermore, any learned trajectory can be linked to a trajectory generated with the help of another method, provided that all files are of the same kind. Eventually, while performing the mission, the robot compares the measurements provided by the localization system and the learned points in order to ensure an accurate trajectory tracking.	crab (cipher);mobile robot;sensor	Catherine Rafflin;Alain Fournier	1996	Auton. Robots	10.1007/BF00240652	mobile robot;robot learning;computer vision;simulation;computer science;artificial intelligence;social robot;robot control;multimedia;ubiquitous robot;mobile robot navigation;personal robot;local system	Robotics	57.44963420208763	-28.489282622642065	2020
c20c150c710ce77a6f551fd8006a83d56652121e	pencil curve tracing via virtual digitizing	virtual digitizing;pencil curve tracing	Pencil-curve machining, which is a single-pass ball-endmilling along a concave edge on a die surface, is widely employed in die-surface machining. The cutter-path used for pencil-curve machining, which is the trajectory of the “ball-center point” of a ball-endmill sliding along a concave-edge region on the die surface, is called pencil-curve. Presented in the paper is a pencil-curve tracing algorithm in which “concave-type” sharp edges are computed from a “virtually digitized” model of the tool-envelope surface. The resulting “initial” pencil-curves are then refined by applying a series of fairing operations. Illustrative examples and methods for enhancing accuracy are also presented. The proposed pencil-curve tracing algorithm has been successfully implemented in a commercial CAM system specialized in die-machining and in the CAD/CAM system CATIA®		Jung W. Park;Yun C. Chung;Bo Hyoung Kim;Byoung Kyu Choi	1998			simulation;engineering;engineering drawing;computer graphics (images)	Visualization	69.33358239819326	-37.953849786529084	2022
84ed7b55e86cb0b1a65cac13aa6636e96e45172c	automated vision-based system for inspecting glue route quality in harddisk drive top cover assembly		An automated vision-based system for inspecting glue route quality in harddisk drive top cover assembly is proposed. It consists of three units: user interface, processing and material handling. In the image processing unit, the image is aligned and then the glue route is extracted. Four partial dark-field images were obtained from each plate. For each image, the reference coordinate system is first determined to construct a rigid-body transformation from predefined template images. Two binary templates were used to identify the defective route in the test image. Our system is efficient and can achieve a very high accuracy.	hard disk drive;image processing;material handling;standard test image;user interface	Wirat Rajchawong;Pakorn KaewTrakulPong	2011			embedded system;simulation	Robotics	60.68402779697063	-39.381821171075515	2031
b81f8b2daca90ad671f66c8dc3cd3d7f7dcdd153	on the uniform sampling of cielab color space and the number of discernible colors	3d close packed hexagonal grid;computational color imaging;sampling;perceptually uniform color space	This paper presents a useful algorithmic strategy to sample uniformly the CIELAB color space based on close packed hexagonal grid. This sampling scheme has been used successfully in different research works from computational color science to color image processing. The main objective of this paper is to demonstrate the relevance and the accuracy of the hexagonal grid sampling method applied to the CIELAB color space. The second objective of this paper is to show that the number of color samples computed depends on the application and on the color gamut boundary considered. As demonstration, we use this sampling to support a discussion on the number of discernible colors related to a JND.	color space;sampling (signal processing)	Jean-Baptiste Thomas;Philippe Colantoni;Alain Trémeau	2013		10.1007/978-3-642-36700-7_5	color co-site sampling;color gradient;color histogram;sampling;computer vision;icc profile;color model;color quantization;hsl and hsv;color depth;chromaticity;rgb color space;color difference;mathematics;color balance;optics;color space;statistics;computer graphics (images)	Vision	61.93289656966289	-59.92045522000562	2032
92edc0baa77bf6ff565bc079794e1ba36ceb7f0c	a new wavelet domain block matching algorithm for real-time object tracking	video signal processing;image matching;real time;wavelet transforms;feature vector;euclidean distance measure real time object tracking block matching algorithm image sequences tree expansion wavelet packet transform feature vectors video sequences search method image denoising mahalanobis discriminant measure computational complexity;wavelet packet transform;object tracking;video signal processing real time systems tracking image sequences wavelet transforms image matching tree searching;tree searching;wavelet domain object detection wavelet transforms image edge detection discrete wavelet transforms matched filters testing optical noise image color analysis image processing;tracking;block matching algorithm;real time systems;image sequences	This paper describes a new real-time algorithm for tracking user-selected objects in a sequence of images based on a new block matching algorithm in wavelet domain. In our algorithm, the amplitude of coefficients in the best basis tree expansion of un-decimated wavelet packet transform is used as the feature vectors (FVs). Real-time object tracking have been achieved using a new search technique for finding the hest match among FVs of the reference block and FVs of the search area in the wavelet domain. Our experimental results show that the algorithm is robust to various object deformations and noisy video sequences.	block-matching algorithm;coefficient;feature vector;real-time locating system;real-time transcription;robustness (computer science);wavelet packet decomposition	Mahdi Amiri;Hamid R. Rabiee;Farid Behazin;Mohammad Khansari	2003		10.1109/ICIP.2003.1247406	wavelet;computer vision;harmonic wavelet transform;feature vector;second-generation wavelet transform;continuous wavelet transform;computer science;machine learning;curvelet;video tracking;pattern recognition;cascade algorithm;mathematics;block-matching algorithm;tracking;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;fast wavelet transform;lifting scheme;wavelet transform	Robotics	40.798255680113314	-58.97665678401202	2033
302dfc1f32c835248f57e3775676d89304d9865c	computing the local continuity order of optical flow using fractional variational method	edge detection;signal processing;image analysis;optical flow;variational method	We introduce variational optical flow computation involving priors with fractional order differentiations. Fractional order differentiations are typical tools in signal processing and image analysis. The zero-crossing of a fractional order Laplacian yields better performance for edge detection than the zero-crossing of the usual Laplacian. The order of the differentiation of the prior controls the continuity class of the solution. Therefore, using the square norm of the fractional order differentiation of optical flow field as the prior, we develop a method to estimate the local continuity order of the optical flow field at each point. The method detects the optimal continuity order of optical flow and corresponding optical flow vector at each point. Numerical results show that the Horn-Schunck type prior involving the  n  +  ***  order differentiation for 0  ***   n  is suitable for accurate optical flow computation.	calculus of variations;optical flow;scott continuity;variational method (quantum mechanics)	Koji Kashu;Yusuke Kameda;Atsushi Imiya;Tomoya Sakai;Yoshihiko Mochizuki	2009		10.1007/978-3-642-03641-5_12	computer vision;mathematical optimization;mathematical analysis;image analysis;edge detection;computer science;variational method;calculus;signal processing;optical flow;mathematics	Vision	54.41800478594544	-70.74518192992042	2034
12e8c97f4b5b862ed97c23bd3e166ed790680b64	a strand specific high resolution normalization method for chip-sequencing data employing multiple experimental control measurements	health research;uk clinical guidelines;biological patents;europe pubmed central;citation search;physiological cellular and medical topics;computational biology bioinformatics;uk phd theses thesis;life sciences;algorithms;uk research reports;medical journals;biochemistry and molecular biology;europe pmc;biomedical research;biokemi och molekylarbiologi;bioinformatics	High-throughput sequencing is becoming the standard tool for investigating protein-DNA interactions or epigenetic modifications. However, the data generated will always contain noise due to e.g. repetitive regions or non-specific antibody interactions. The noise will appear in the form of a background distribution of reads that must be taken into account in the downstream analysis, for example when detecting enriched regions (peak-calling). Several reported peak-callers can take experimental measurements of background tag distribution into account when analysing a data set. Unfortunately, the background is only used to adjust peak calling and not as a pre-processing step that aims at discerning the signal from the background noise. A normalization procedure that extracts the signal of interest would be of universal use when investigating genomic patterns. We formulated such a normalization method based on linear regression and made a proof-of-concept implementation in R and C++. It was tested on simulated as well as on publicly available ChIP-seq data on binding sites for two transcription factors, MAX and FOXA1 and two control samples, Input and IgG. We applied three different peak-callers to (i) raw (un-normalized) data using statistical background models and (ii) raw data with control samples as background and (iii) normalized data without additional control samples as background. The fraction of called regions containing the expected transcription factor binding motif was largest for the normalized data and evaluation with qPCR data for FOXA1 suggested higher sensitivity and specificity using normalized data over raw data with experimental background. The proposed method can handle several control samples allowing for correction of multiple sources of bias simultaneously. Our evaluation on both synthetic and experimental data suggests that the method is successful in removing background noise.	binding sites;biopolymer sequencing;c++;chip-on-chip;dna binding site;database normalization;downstream (software development);hepatocyte nuclear factor 3-alpha;interaction;largest;linear regression body surface area formula for infants and children;max;motif;peak calling;preprocessor;reading (activity);repetitive region;sensitivity and specificity;sensor;sequence number;strand (programming language);synthetic intelligence;transcription factor;throughput;transcription (software);study of epigenetics	Stefan Enroth;Claes R. Andersson;Robin Andersson;Claes Wadelius;Mats G. Gustafsson;Jan Komorowski	2011		10.1186/1748-7188-7-2	biology;medical research;computer science;bioinformatics;data science;data mining	Comp.	4.125963074080298	-53.78032679002058	2035
5e466eeaa417ce57981911f96285baa6385efbac	finite approximations to coherent choice	admissibility;analisis sensibilidad;optimisation;admissibilite;closure;computacion informatica;credal set;capsula convexa;optimizacion;prise de decision;intelligence artificielle;fonction perte;funcion perdida;enveloppe convexe;admisibilidad;numerical analysis;lower prevision;extreme point;ciencias basicas y experimentales;sensitivity analysis;loss function;analyse sensibilite;artificial intelligence;optimization;inteligencia artificial;cerradura;grupo a;convex hull;e admissibility;toma decision;choice function;fermeture;maximality	This paper studies and bounds the effects of approximating loss functions and credal sets on choice functions, under very weak assumptions. In particular, the credal set is assumed to be neither convex nor closed. The main result is that the effects of approximation can be bounded, although in general, approximation of the credal set may not always be practically possible. In case of pairwise choice, I demonstrate how the situation can be improved by showing that only approximations of the extreme points of the closure of the convex hull of the credal set need to be taken into account, as expected. 2008 Elsevier Inc. All rights reserved.	approximation algorithm;coherence (physics);convex function;convex hull;loss function	Matthias C. M. Troffaes	2009	Int. J. Approx. Reasoning	10.1016/j.ijar.2008.07.001	extreme point;choice function;numerical analysis;convex hull;calculus;closure;mathematics;sensitivity analysis;algorithm;loss function	AI	1.2366215282121045	-16.061915226587153	2037
914a1ebd1411853a2a22bd76034ba8d2d728c386	fans economy and all-pay auctions with proportional allocations		In this paper, we analyze an emerging economic form, called fans economy, in which a fan donates money to the host and gets allocated proportional to the amount of his donation (normalized by the overall amount of donation). Fans economy is the major way live streaming apps monetize and includes a number of popular economic forms ranging from crowdfunding to mutual fund. We propose an auction game, coined all-pay auctions with proportional allocation (APAPA), to model the fans economy and analyze the auction from the perspective of revenue. Comparing to the standard all-pay auction, which normally has no pure Nash-Equilibrium in the complete information setting, we solve the pure Nash-Equilibrium of the APAPA in closed form and prove its uniqueness. Motivated by practical concerns, we then analyze the case where APAPA is equipped with a reserve and show that there might be multiple equilibria in this case. We give an efficient algorithm to compute all equilibria in this case. For either case, with or without reserve, we show that APAPA always extracts revenue that 2approximates the second-highest valuation. Furthermore, we conduct experiments to show how revenue changes with respect to different reserves.	algorithm;crowdfunding;experiment;monetization;money;nash equilibrium;streaming media;value (ethics);virtual economy	Pingzhong Tang;Yulong Zeng;Song Zuo	2017			mathematical optimization;computer science;common value auction;microeconomics	AI	-2.529581015558961	-3.5904110075950886	2038
6936fca3af3da13c1ac9f32c6aef530d9517981f	the systematic annotation of the three main gpcr families in reactome	receptors g protein coupled;ligands;signal transduction;orphan nuclear receptors;humans;databases factual;databases protein	Reactome is an open-source, freely available database of human biological pathways and processes. A major goal of our work is to provide an integrated view of cellular signalling processes that spans from ligand-receptor interactions to molecular readouts at the level of metabolic and transcriptional events. To this end, we have built the first catalogue of all human G protein-coupled receptors (GPCRs) known to bind endogenous or natural ligands. The UniProt database has records for 797 proteins classified as GPCRs and sorted into families A/1, B/2 and C/3 on the basis of amino acid sequence. To these records we have added details from the IUPHAR database and our own manual curation of relevant literature to create reactions in which 563 GPCRs bind ligands and also interact with specific G-proteins to initiate signalling cascades. We believe the remaining 234 GPCRs are true orphans. The Reactome GPCR pathway can be viewed as a detailed interactive diagram and can be exported in many forms. It provides a template for the orthology-based inference of GPCR reactions for diverse model organism species, and can be overlaid with protein-protein interaction and gene expression datasets to facilitate overrepresentation studies and other forms of pathway analysis. Database URL: http://www.reactome.org.	amino acid sequence;amino acids;annotation;cell signaling;classification;clinical use template;diagram;digital curation;gene expression;gene regulatory network;iuphar/bps guide to pharmacology;inference;interaction;ligands;metabolic process, cellular;open-source software;pathway analysis;reactome: a database of reactions, pathways and biological processes.;transcription, genetic;uniprot;uniform resource locator;biological signaling	Bijay Jassal;Steven Jupe;Michael Caudy;Ewan Birney;Lincoln Stein;Henning Hermjakob;Peter D'Eustachio	2010		10.1093/database/baq018	biology;bioinformatics;data mining;ligand;signal transduction	Comp.	-0.5490349068311965	-60.51297564849463	2041
271480d44bf8e0cf8107c76b1814c0eee5b8842c	optimization of multiple vehicle routing problems using approximation algorithms	cluster computing;approximate algorithm;vehicle routing problem;optimal routing;genetic algorithm;k means clustering	This paper deals with generating of an optimized route for multiple Vehicle routing Problems (mVRP). We used a methodology of clustering the given cities depending upon the number of vehicles and each cluster is allotted to a vehicle. kMeans clustering algorithm has been used for easy clustering of the cities. In this way the mVRP has been converted into VRP which is simple in computation compared to mVRP. After clustering, an optimized route is generated for each vehicle in its allotted cluster. Once the clustering had been done and after the cities were allocated to the various vehicles, each cluster/tour was taken as an individual Vehicle Routing problem and the steps of Genetic Algorithm were applied to the cluster and iterated to obtain the most optimal value of the distance after convergence takes place. After the application of the various heuristic techniques, it was found that the Genetic algorithm gave a better result and a more optimal tour for mVRPs in short computational time than other Algorithms due to the extensive search and constructive nature of the algorithm.	approximation algorithm;cluster analysis;computation;genetic algorithm;heuristic;iteration;k-means clustering;optimization problem;time complexity;vehicle routing problem	R. Nallusamy;K. Duraiswamy;R. Dhanalakshmi;P. Parthiban	2009	CoRR		correlation clustering;constrained clustering;mathematical optimization;data stream clustering;simulation;genetic algorithm;k-medians clustering;computer cluster;computer science;destination-sequenced distance vector routing;canopy clustering algorithm;vehicle routing problem;machine learning;cure data clustering algorithm;k-medoids;k-means clustering	AI	17.705948564773415	0.5494609474273503	2044
4f22ef601f1398e4b5c451d399dc24ed903bcc5e	a multiple-phase train trajectory optimization method under real-time rail traffic management	real time systems force integrated circuits trajectory optimization junctions;netherlands;case studies;real time systems optimisation rail traffic railways;real time information;train trajectory optimization;train path envelope;gauss pseudospectral method;energy consumption;optimization;dutch corridor multiple phase train trajectory optimization method real time rail traffic management real time traffic plan train path envelope target time speed point train trajectory planning problem energy consumption minimization gauss pseudospectral method optimization problem;railroad traffic control;multiple phase optimization model train trajectory optimization train path envelope;multiple phase optimization model;trajectory control	This paper presents a multiple-phase train trajectory optimization method under real-time rail traffic management. A real-time traffic plan gives for each train a Train Path Envelope consisting of a set of positions with a target time and speed point or window. The resulting train trajectory planning problem is formulated as a multiple-phase optimization model with minimizing the energy consumption as objective. The Gauss Pseudospectral Method is adopted to solve the optimization problem. A case study on a 50 km long Dutch corridor shows the validity of the approach for real-time application and demonstrates the influence of the Train Path Envelope on train trajectory planning.	algorithm;automated planning and scheduling;automated theorem proving;computation;gauss pseudospectral method;gradient;hierarchical task network;mathematical optimization;microsoft windows;monte carlo method;online and offline;online optimization;optimization problem;program optimization;real life;real-time clock;real-time computing;real-time transcription;trajectory optimization	Pengling Wang;Rob M. P. Goverde;Miaozhang Zhu	2015	2015 IEEE 18th International Conference on Intelligent Transportation Systems	10.1109/ITSC.2015.130	control engineering;trajectory optimization;simulation;engineering;operations management	Robotics	13.855919299748964	1.19164023989128	2045
4ec82a32cfc866c1d9748f92f9d3cbac397dabc5	simulation of adaptive control of a tubular chemical reactor		The paper deals with continuous-time adaptive control of a tubular chemical reactor with the countercurrent cooling as a nonlinear single input – single output process. The nonlinear model of the reactor is approximated by an external linear model with parameters estimated via corresponding delta model. The control system structure with two feedback controllers is considered. The resulting controllers are derived using the polynomial approach. The method is tested on a mathematical model of the tubular chemical reactor. INTRODUCTION Tubular chemical reactor are units frequently used in chemical industry. From the system theory point of view, tubular chemical reactors belong to a class of nonlinear distributed parameter systems with mathematical models described by sets of nonlinear partial differential equations (NPDRs). The methods of modelling and simulation of such processes are described e.g. in (Luyben 1989), (Ingham et al. 1994) and (Dostál et al. 2008). It is well known that the control of chemical reactors, and, tubular reactors especially, often represents very complex problem. The control problems are due to the process nonlinearity, its distributed nature, and high sensitivity of the state and output variables to input changes. Evidently, the process with such properties is hardly controllable by conventional control methods, and, its effective control requires application some of advanced methods. One possible method to cope with this problem is using adaptive strategies based on an appropriate choice of a continuous-time external linear model (CT ELM) with recursively estimated parameters. These parameters are consequently used for parallel updating of controller‘s parameters. Some results obtained in this field were presented by authors of this paper e.g. in (Dostál et al. 2004). For the CT ELM parameter estimation, either the direct method or application of an external delta model with the same structure as the CT model can be used, e.g. (Middleton and Goodwin 1990) or (Mukhopadhyay et al. 1992). Although delta models belong into discrete models, they do not have such disadvantageous properties connected with shortening of a sampling period as discrete z-models. In addition, parameters of delta models can directly be estimated from sampled signals. Moreover, it can be easily proved that these parameters converge to parameters of CT models for a sufficiently small sampling period (compared to the dynamics of the controlled process), as shown in (Stericker and Sinha 1993). This paper deals with continuous-time adaptive control of a tubular chemical reactor with a countercurrent cooling. With respect to practical possibilities of a measurement and control, the mean reactant temperature temperature is chosen as the controlled output, and, the coolant flow rate as the control input. The nonlinear model of the reactor is approximated by a CT external linear model with a structure chosen on the basis of computed controlled output step responses. The control structure with two feedback controllers is considered, e.g. (Dostál et al. 2007). The resulting controllers are derived using the polynomial approach (Kučera 1993) and the pole assignment method, e.g. (Bobál et al. 2005). The method is tested on a mathematical model of a tubular chemical reactor. MODEL OF THE REACTOR An ideal plug-flow tubular chemical reactor with a simple exothermic consecutive reaction 1 2 k k A B C → → in the liquid phase and with the countercurrent cooling is considered. Heat losses and heat conduction along the metal walls of tubes are assumed to be negligible, but dynamics of the metal walls of tubes are significant. All densities, heat capacities, and heat transfer coefficients are assumed to be constant. Under above assumptions, the reactor model can be described by five PDRs in the form 1 A A r A c c v k c t z ∂ ∂ + = − ∂ ∂ (1) 1 2 B B r A B c c v k c k c t z ∂ ∂ + = − ∂ ∂ (2)	approximation algorithm;ct scan;coefficient;computer cooling;control flow;control system;converge;delta-sigma modulation;direct method in the calculus of variations;estimation theory;linear model;mathematical model;nonlinear system;polynomial;reactor (software);recursion;sampling (signal processing);simulation;systems theory;word lists by frequency	Petr Dostál;Jiri Vojtesek;Vladimir Bobal	2012		10.7148/2012-0419-0425	distributed parameter system;chemical reactor;control theory;adaptive control;linear model;partial differential equation;nonlinear system;control theory;mathematics;control system	AI	57.651596114629854	2.263960682086868	2049
6b80db884195fa0e5d74ed7d4684f5ff8f1f75e1	a computational approach to the real option management of network contracts for natural gas pipeline transport capacity	math programming;operations management practice;spread options;operations management finance interface;natural gas pipelines;real options;capacity valuation;sensitivities;energy related operations;commodity and energy conversion assets;heuristics;monte carlo simulation;petroleum natural gas industries	Commodity merchants use real option models to manage their operations. A central element of such a model is its underlying operating policy. We focus on network contracts for the transport capacity of natural gas pipelines, specific energy conversion assets. Practitioners commonly manage these contracts as portfolios of spread options. Although computationally fast, we show that this approach in general is heuristic, due to the suboptimality of its operating policy. We propose a different computationally efficient real option approach based on an optimal operating policy, integrating linear optimization and Monte Carlo simulation. We use our approach to benchmark the spread option approach in a numerical study based on market data and realistic instances. We find that our approach can substantially improve on the contract valuations computed by the spread option approach, especially for contracts with flexibility in their allowed capacity usage. We also show that the optimal operating policy of contracts with this flexibility is of the greedy type, while the one of contracts without it in general is not. However, we observe that greedy optimization yields a near optimal operating policy for the latter type of contracts with a substantially reduced computational effort. A version of our model was recently implemented by a major international energy trading company. Our model can also be employed to efficiently estimate the contract value sensitivities (the “Greeks”), which merchants use for financial hedging purposes. Potentially, our work has wider significance for the merchant management of other commodity conversion assets with payoffs determined by solving capacity constrained optimization models.	algorithmic efficiency;benchmark (computing);computation;constrained optimization;greedy algorithm;heuristic;linear programming;mathematical optimization;monte carlo method;numerical analysis;pipeline (computing);simulation	Nicola Secomandi;Mulan X. Wang	2012	Manufacturing & Service Operations Management	10.1287/msom.1120.0383	financial economics;actuarial science;economics;marketing;operations management;heuristics;statistics;monte carlo method	ML	4.350219864377807	-6.665794436060171	2056
29920deae786584b9e3d8f720cf4872e678078ff	an asymptotically optimal algorithm for the max k-armed bandit problem	asymptotic optimality	We present an asymptotically optimal algorithm for the maxvariant of thek-armed bandit problem. Given a set ofk slot machines, each yielding payoff from a fixed (but unknown) distribution, we wish to allocate trials to the machines so as to maximize the expected maximum payoff received over a series of n trials. Subject to certain distributional assumptions, we show that O ( ln( δ ) ln(n) 2 2 ) trials are sufficient to identify, with probability at least 1− δ, a machine whose expected maximum payoff is within of optimal. This result leads to a strategy for solving the problem that is asymptotically optimal in the following sense: the gap between the expected maximum payoff obtained by using our strategy for n trials and that obtained by pulling the single best arm for all n trials approaches zero as n →∞.	asymptotically optimal algorithm;multi-armed bandit	Matthew J. Streeter;Stephen F. Smith	2006			mathematical optimization;computer science	Theory	23.26974998704597	-16.899837268825056	2058
c3c62953ca8c07044c5d9949df36aa03de58b750	adaptive neural nets for generation of artificial earthquake precursors	forecasting;damping;time dependent;prevision;landers;sismo;neural networks;north america;seismology;america del norte;neural nets;artificial earthquake precursor;case studies;amerique du nord;artificial neural networks earthquakes neural networks adaptive filters geology testing damping least squares approximation adaptive signal processing;helium;real time;training;seisme;time window;learning artificial intelligence seismology earthquakes geophysics computing neural nets;least squares approximation;californie;testing;fenomeno precursor;california united states usa;etats unis;earthquakes;estados unidos;real time mode;algorithme;artificial neural networks;california;adaptive filters;adaptive neural net;adaptive signal processing;foreshock seismicity seismology earthquake prediction technique forecasting geophysics computing adaptive neural net neural network artificial earthquake precursor method california united states usa real time mode landers time dependent attribute moving time window danger function training;neural net;geophysics computing;geology;foreshock seismicity;estudio caso;time dependent attribute;danger function;etude cas;algorithms;moving time window;learning artificial intelligence;reseau neuronal;phenomene precurseur;prediction;method;red neuronal;precursors;neural network;algoritmo;earthquake prediction technique	A novel methodology for generation of artificial earthquake precursors was tested on Southern California earthquake data in reverse and real time modes. When it was tried as a real time generator of earthquake precursors, it successfully predicted the June, 1992, Landers earthquake. The methodology is based on the use of adaptive neural nets (ANN) that process a set of time-dependent attributes calculated in a moving time-window. The most important of them is a danger function. The structure of the neural net is defined by the properties of input data in the moving time window. Thus, the neural net continuously adapts its structure to the time variant properties of the input attributes. The main problem the authors encountered in training the neural net on the earthquake data was the small size of the training set compared to the number of parameters that describe the structure of the ANN. To prevent instability and over-fitting in the training session, the authors used a technique similar to the damping method in least squares approximation. >	artificial neural network	Fred Aminzadeh;Simon Katz;Keiti Aki	1994	IEEE Trans. Geoscience and Remote Sensing	10.1109/36.338361	adaptive filter;meteorology;seismology;artificial intelligence;machine learning;artificial neural network	EDA	10.572865425260266	-21.30736493200649	2060
d3d3333ceddbb491c6eb4d5926b271d4a61a8ac6	feature selection for the prediction of tropospheric ozone concentration using a wrapper method	air pollution forecasting;meteorological factors;artificial neural networks;backwazd elimination;turkey;istanbul;variable sensitivity analysis	High concentrations of ozone (O3) in the lower troposphere increase global warming, and thus affect climatic conditions and human health. Especially in metropolitan cities like Istanbul, ozone level approximates to security levels that may threaten human health. Therefore, there are many research efforts on building accurate ozone prediction models to develop public warning strategies. The goal of this study is to construct a tropospheric (ground) ozone prediction model and analyze the effectiveness of air pollutant and meteorological variables in ozone prediction using artificial neural networks (ANNs). The air pollutant and meteorological variables used in ANN modeling are taken from monitoring stations located in Istanbul. The effectiveness of each input feature is determined by using backward elimination method which utilizes the constructed ANN model as an evaluation function. The obtained results point out that outdoor temperature (OT) and solar irradiation (SI) are the most important input features of meteorological variables, and total hydrocarbons (THC), nitrogen dioxide (NO2) and nitric oxide (NO) are those of air pollutant variables. The subset of parameters found by backward elimination feature selection method that provides the maximum prediction accuracy is obtained with six input features which are OT, SI, NO2, THC, NO, and sulfur dioxide (SO2) for both validation and test sets.	artificial neural network;cmos;evaluation function;feature selection;stepwise regression	Cemal Okan Sakar;Goksel Demir;Olcay Kursun;Huseyin Ozdemir;Gökmen Altay;Senay Yalçin	2011	Intelligent Automation & Soft Computing	10.1080/10798587.2011.10643157	computer science;machine learning;artificial neural network	AI	10.379599931183636	-19.041234362514096	2061
e63995ce7825de160143ad6bee1d1ebc79733c7e	waveform-based multi-stimulus coding for brain-computer interfaces based on steady-state visual evoked potentials		Multiple stimulus coding plays an important role in a steady-state visual evoked potential (SSVEP)-based brain-computer interface (BCI). In conventional SSVEP-based BCIs, multiple visual stimuli are modulated with different properties such as frequencies and/or phases. However, the number of properties that can be assigned to visual stimuli rendered on a computer monitor is always limited by its refresh rate, leading to a system with limited commands or functions. To alleviate this issue, this study proposes a novel waveform-based stimulus coding method that uses modulation waveforms to differentiate resulting SSVEPs. In this paper, the discriminability of 12-class SSVEPs modulated by three types of waveforms (i.e., rectangle, sinusoidal and triangle waveforms) and four frequencies (i.e., 12, 13, 14, and 15 Hz) was investigated by computing its classification accuracy. The results showed the SSVEPs modulated by different waveforms can be successfully distinguished when using the state-of-the-art canonical correlation analysis (CCA)-based method with an average accuracy of 92.31%. This result suggests that the proposed method has great potential to significantly increase the number of functions in an SSVEP-based BCI system.	brain–computer interface;computer monitor;modulation;refresh rate;steady state;waveform	Yutaro Tanji;Masaki Nakanishi;Kaori Suefusa;Toshihisa Tanaka	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8462246	stimulus (physiology);modulation;artificial intelligence;evoked potential;pattern recognition;electroencephalography;refresh rate;frequency-shift keying;waveform;visual perception;computer science	Robotics	13.761763812087134	-93.34200520290386	2062
7794abbedaf362250416d607d21d0328978abfe3	g4ldb: a database for discovering and studying g-quadruplex ligands	ligands;internet;drug design;期刊论文;g quadruplexes;molecular docking simulation;databases chemical	The G-quadruplex ligands database (G4LDB, http://www.g4ldb.org) provides a unique collection of reported G-quadruplex ligands to streamline ligand/drug discovery targeting G-quadruplexes. G-quadruplexes are guanine-rich nucleic acid sequences in human telomeres and gene promoter regions. There is a growing recognition for their profound roles in a wide spectrum of diseases, such as cancer, diabetes and cardiovascular disease. Ligands that affect the structure and activity of G-quadruplexes can shed light on the search for G-quadruplex-targeting drugs. Therefore, we built the G4LDB to (i) compile a data set covering various physical properties and 3D structure of G-quadruplex ligands; (ii) provide Web-based tools for G-quadruplex ligand design; and (iii) to facilitate the discovery of novel therapeutic and diagnostic agents targeting G-quadruplexes. G4LDB currently contains >800 G-quadruplex ligands with ∼4000 activity records, which, to our knowledge, is the most extensive collection of its kind. It offers a user friendly interface that can meet a variety of data inquiries from researchers. For example, ligands can be searched for by name, molecular properties, structures, ligand activities and so on. Building on the reported data, the database also provides an online ligand design module that can predict ligand binding affinity in real time.	academy;access network;adobe streamline;cardiovascular diseases;cell nucleus;compiler;diabetes mellitus;drug discovery;forty nine;guanine;homologous gene;homology modeling;interface device component;ligand binding;ligands;nar 2;natural science disciplines;nucleic acids;physical phenomenon or property;processor affinity;promoter regions, genetic;rna;societies, scientific;usability;version;bacterium n06;cellular targeting;supercomputer;telomere	Qian Li;Jun-Feng Xiang;Qian-Fan Yang;Hong-Xia Sun;Ai-Jiao Guan;Yalin Tang	2013		10.1093/nar/gks1101	biology;the internet;bioinformatics;ligand;drug design	ML	0.5766016561195367	-60.389642154086694	2064
dd79dd895126eeec7e597d6f8825d63ca12df701	extraction of disease-related genes from pubmed paper using word2vec		Finding disease-related genes is important in drug discovery. Many genes are involved in the disease, and many studies have been conducted and reported for each disease. However, it is very costly to check these one by one. Therefore, machine learning is a suitable method to address this problem. By extracting study results from research papers by text mining, it is possible to make use of that knowledge. In this research, we aim to extract disease-related genes from PubMed papers using word2vec, which is a text mining method. The method extracts the top 10 genes whose known disease genes and vectors are close to those obtained by word2vec. Based on these, genes other than known disease-related genes are extracted and used as disease-related genes. We conducted experiments using schizophrenia, and confirmed the likelihood of this disease-related gene using xgboost. Pattern 1: Only known genes. Pattern 2: Pattern 1 plus disease-related genes extracted in this study. Pattern 3: Pattern 1 plus the same number of random genes. Using these three patterns, we performed a xgboost with microarray data and compared the classification accuracy. The result was that Pattern 2 had the highest accuracy. Therefore, we could extract genes with using genes related to disease by our method.	experiment;machine learning;microarray;pubmed;text mining;word2vec;xgboost	Takahiro Koiwa;Hayato Ohwada	2017		10.1145/3156346.3156355	drug discovery;word2vec;gene;microarray analysis techniques;biology;text mining;bioinformatics	ML	7.8835684749988	-49.547004795308325	2065
cd68d347850d795add43228c0436bb4573c6cecd	a new steerable pressure force for parametric deformable models	gradient vector flow;convergence rate;cardiac mri;force field;magnetic resonance imaging;image analysis;cardiovascular magnetic resonance imaging;deformable model;active contour model	Active contour models have been widely used in various image analysis applications. Despite their usefulness, there are problems limiting their utility, such as capture range, concavity conformation, and convergence rate. This paper presents a new pressure-like force that not only improves contour convergence rate, but also encourages contours to conform to concave regions. Unlike the traditional pressure force, this new force does not require users' input for the force direction and is steerable according to the image content. Better convergence rate as well as force normalization consistency of this new force are presented when compared with those of the gradient vector flow force field on synthetic images. Accuracies of these two methods are compared against the manual markups on a set of cardiac MRI images. Moreover, results on a MRI image smoothed at different levels demonstrate the robustness of this new force to noise.© (2011) COPYRIGHT Society of Photo-Optical Instrumentation Engineers (SPIE). Downloading of the abstract is permitted for personal use only.		Jun Kong;Lee A. D. Cooper;Ashish Sharma;Tahsin M. Kurç;Daniel J. Brat;Joel H. Saltz	2011		10.1117/12.877700	computer vision;mathematical optimization;image analysis;simulation;magnetic resonance imaging;force field;active contour model;rate of convergence	Robotics	46.293229230103215	-74.00077913766398	2068
b50198b3550a0a2ebf96874962b8ecf63132804d	an unsupervised anomaly-based detection approach for integrity attacks on scada systems	unsupervised detection;consistent inconsistent scada patterns;cyber warfare;scada systems;intrusion detection system	Supervisory Control and Data Acquisition (SCADA) systems are a core part of industrial systems, such as smart grid power and water distribution systems. In recent years, such systems become highly vulnerable to cyber attacks. The design of efficient and accurate data-driven anomaly detection models become an important topic of interest relating to the development of SCADA-specific Intrusion Detection Systems (IDSs) to counter cyber attacks. This paper proposes two novel techniques: (i) an automatic identification of consistent and inconsistent states of SCADA data for any given system, and (ii) an automatic extraction of proximity detection rules from identified states. During the identification phase, the density factor for the k-nearest neighbours of an observation is adapted to compute its inconsistency score. Then, an optimal inconsistency threshold is calculated to separate inconsistent from consistent observations. During the extraction phase, the well-known fixed-width clustering technique is extended to extract proximity-detection rules, which forms a small and most-representative data set for both inconsistent and consistent behaviours in the training data set. Extensive experiments were carried out both on real as well as simulated data sets, and we show that the proposed techniques provide significant accuracy and efficiency in detecting cyber attacks, compared to three wellknown anomaly detection approaches. Crown Copyright © 2014 Published by Elsevier Ltd. All rights reserved.	anomaly detection;anomaly-based intrusion detection system;automatic identification and data capture;cluster analysis;crown group;data acquisition;experiment;k-nearest neighbors algorithm;sensor;test set;unsupervised learning;whole earth 'lectronic link	Abdulmohsen Almalawi;Xinghuo Yu;Zahir Tari;Adil Fahad;Ibrahim Khalil	2014	Computers & Security	10.1016/j.cose.2014.07.005	intrusion detection system;computer science;data mining;database;cyberwarfare;computer security	AI	5.676077439205854	-36.68743301264523	2070
922b40fe0b70f71359cdaa21b94dcfa173f0ab17	blind estimation of locations and time offsets for distributed recording devices	blind alignment;generalized cross correlation;asynchronous recording	This paper presents a blind technique to estimate locations and recording time offsets of distributed recording devices from asynchronously recorded signals. In our method, locations of sound sources and recording devices, and the recording time offsets are estimated from observed time differences of arrivals (TDOAs) by decreasing the mean squared errors. The auxiliary-function-based updates guarantee the monotonic decrease of the objective function at each iteration. The TDOAs are estimated by the generalized cross correlation technique. The validity of our approach is shown by experiments in real environment, where locations of seven sound sources and eight microphones and eight time offsets were estimated from signals recorded by four stereo IC recorders in reverberant rooms.		Keisuke Hasegawa;Nobutaka Ono;Shigeki Miyabe;Shigeki Sagayama	2010		10.1007/978-3-642-15995-4_8	electronic engineering;real-time computing;geography;telecommunications	HCI	50.17351886417153	3.608423209852932	2071
db9afccd7541458e9b6e74864f26eb9e1a2338aa	synthetic video generation with camera motion patterns to evaluate sprite generation	cameras sprites computer image generation psnr mpeg 4 standard motion estimation image resolution video compression computer science spirals;video object;zigzag pattern;image segmentation;psnr;image resolution;generic algorithm;video signal processing;sprite image;sprite quality;video compression;spiral pattern;motion estimation;sprite generation sythetic video generation;synthetic video generation;camera motion;mpeg 4 standard;image generation;general methods;sprites computer;peak signal to noise ratio;spirals;motion parameter estimation;video signal processing image segmentation motion estimation;ground truth;camera motion pattern;computer science;parameter estimation;zoom pattern;sythetic video generation;earthquake pattern;sprite quality synthetic video generation camera motion pattern sprite generation sprite image zigzag pattern spiral pattern earthquake pattern zoom pattern peak signal to noise ratio motion parameter estimation;cameras;sprite generation	There is no proper objective method of evaluating sprite generation methods due to absence of the ground truth sprite images. In this paper, we propose several camera motion patterns to generate synthetic videos from original image. Our camera motion patterns include zigzag, spiral, earthquake, and zoom patterns. Subsequently, we apply sprite generation algorithm on the synthetic video. Objective evaluation is performed by comparing original image and generated sprite image based on Peak-Signal-to-Noise Ratio (PSNR), size and error on motion parameter estimation. Our results indicate that our picture PSNR, size, and error on motion parameters are good indication of the sprite quality.	algorithm;estimation theory;ground truth;peak signal-to-noise ratio;spiral model;sprite (computer graphics);synthetic data;synthetic intelligence	Yi Chen;Ramazan Savas Aygün	2009	2009 First International Conference on Advances in Multimedia	10.1109/MMEDIA.2009.33	computer vision;simulation;peak signal-to-noise ratio;computer science;statistics;computer graphics (images)	Vision	58.14636613154016	-56.51649968766746	2073
37d26d71ea2f4dd50ad96432b062cdef62a7f371	estimating pig weights from images without constraint on posture and illumination		Abstract This paper proposes an image based pig weight estimation method different from the previous works in three ways. The first difference is no constraint on pig posture and image capture environment, reducing the stress of the pigs. The second one is that the features obtained from 2D images are used without depending on 3D depth information. And the third is that our estimation model is constructed by exploiting the recent advances in machine learning. Besides the pig area size which had been a major feature parameter for the estimation, two new features, curvature and deviation, are introduced because those are related with the postures, thus being able to quantify the weight adjustment. A set of experiments are conducted to investigate how the performance is affected by the combination of the features and the neural network configurations. By using 477 training and 103 test images, the average estimation error of 3.15 kg was achieved, and the coefficient of determination of the model was R 2 = 0.79 .	poor posture	Kyungkoo Jun;Sijung Kim;Hyun Wook Ji	2018	Computers and Electronics in Agriculture	10.1016/j.compag.2018.08.006	image processing;computer vision;coefficient of determination;curvature;artificial neural network;artificial intelligence;engineering	Vision	48.07478035337383	-45.52735277090742	2080
681eb62c3f03c77c9dbb133069090f6fe66d80cc	single facility collection depots location problem with random weights		In this paper we consider the single facility collection depots location problem in the plane where n weighted demand points and p collection depots are given. The service of a demand point consists of the travel from the facility to the demand point and returning back to the facility through a collection depot which provides the shortest route. The weights are drawn from independent continuous distributions, and the objective is to find the location such that the probability that the maximum weighted distance of the round trip exceeds a threshold is minimized. The problem is formulated and analyzed. Computational experience is reported.	computation	Dongyan Chen;Chan He;Senlin Wu	2016	Operational Research	10.1007/s12351-015-0200-2	mathematical optimization;operations management;1-center problem	Theory	16.45490741659663	4.178098730216805	2081
e7b8085e432b36d0b1fb0605bfe04bfc032da0d0	image fusion and evaluation of geological disaster based on remote sensing	remote sensing;image fusion	The investigation of geological disaster in our article locates in southern Australia, which is characterized by wide range, high relief, inaccessibility and other unfavorable factors. Multi-spectral ETM+ and SPOT 5 pan images were selected as the remote sensing data source, and Brovey transform (BT), intensity-hue-saturation (IHS), principal component analysis (PCA), high-pass filtering (HPF) and modified Gram-Schmidt (MGS) methods were used for image fusion. A comparison has been conducted between the resultant fusion images to assess the image quality both in subjective and objective evaluation. The results show that, the MGS method is the optimal image fusion method for geological disaster interpretation, and can provide abundant textural and spectral information in interpreting such geological disasters as landslide, rock fall and debris flow.	image fusion	Juan Wang;Huajun Wang;Yinghao Li;Hairui Chen	2014	iJOE		image fusion	Mobile	77.44147311662365	-59.89004487794647	2084
07cd381ee511c099e3a18ac0083c222b7451f706	extracting image features from mpeg-2 compressed stream	desciframiento;image features;transformation cosinus;image processing;decodage;decoding;edge detection;codigo longitud variable;variable length code;procesamiento imagen;codigo bloque;traitement image;deteccion contorno;brightness;detection contour;codificacion;brillance;code longueur variable;feature extraction;mpeg 2;transformacion coseno;coding;pattern recognition;code bloc;reconnaissance forme;cosine transform;reconocimiento patron;brillantez;block code;codage	In this paper, we propose a new image feature extraction algorithm in the compression domain. To minimize the decompression process the proposed feature extraction algorithm executes only the parsing process to the compressed bit stream. Then, by just decoding dctjc_size in the MPEG-2 bit stream, we can determine ifthere exists any abrupt brightness change between two DCT blocks. According to the Huffman table for the MPEG-2 encoder, as the difference of the DC values between two succeeding DCT blocks increases, it yields longer coded bits. That is, the length of the coded DC value is proportional to the brightness change between two succeeding DCT blocks. Therefore, one can detect an edge feature between DCT blocks by just decoding the information regarding the number of bits assigned to the difference of the DC values. To demonstrate the usefulness ofthe proposed feature extraction method, we apply the detected edge features to find scene changes in the MPEG-2 compressed bit stream.	algorithm;bitstream;data compression;discrete cosine transform;encoder;feature (computer vision);feature extraction;huffman coding;mpeg-2;parsing	Chee Sun Won;Dong Kwon Park;Seong-Joon Yoo	1998		10.1117/12.298451	block code;computer vision;speech recognition;edge detection;image processing;feature extraction;variable-length code;computer science;discrete cosine transform;mpeg-2;coding;feature;brightness;computer graphics (images)	Vision	46.00935037394905	-13.889524246391188	2088
2c3f332a2c5688511482141168046e6c26502126	characterization of flexible radio-frequency spiral inductors on a plastic substrate	flexible;quality factor;plastic substrate;self resonant frequency;characterization;spiral inductor	This paper describes the design and fabrication of flexible radiofrequency inductors integrated on a polyethylene terephthalate substrate. Experimental and modeling results for the RF responses of flexible inductors are reported. Investigations and analysis have been conducted on the effects of layout and process parameters on the frequency responses of inductance, quality factor and self-resonant frequency of the spiral inductors. The influence of bending strain on the performance of spiral inductors is also investigated based on measurement and modeling results. The analysis provides guidelines for designing the flexible spiral inductors towards the flexible monolithic microwave integrated circuits on a plastic substrate.	frequency response;integrated circuit;microwave;polyethylene terephthalate;radio frequency;resonance;spiral model;thickness (graph theory)	Guoxuan Qin;Hao Liu;Yanmeng Xu;Mengjiao Dang;Jianguo Ma;Zhenqiang Ma;Xuejiao Chen;Tao Luo	2016	IEICE Electronic Express	10.1587/elex.13.20160690	electronic engineering;engineering;electrical engineering;q factor;engineering drawing;physics	EDA	89.28017956789145	-15.413150654978077	2091
53af40f76d3a2aedf854477fd669f40069fb402a	goal-directed inductive matrix completion	matrix completion	Matrix completion (MC) with additional information has found wide applicability in several machine learning applications. Among algorithms for solving such problems, Inductive Matrix Completion(IMC) has drawn a considerable amount of attention, not only for its well established theoretical guarantees but also for its superior performance in various real-world applications. However, IMC based methods usually place very strong constraints on the quality of the features(side information) to ensure accurate recovery, which might not be met in practice. In this paper, we propose Goal-directed Inductive Matrix Completion(GIMC) to learn a nonlinear mapping of the features so that they satisfy the required properties. A key distinction between GIMC and IMC is that the feature mapping is learnt in a supervised manner, deviating from the traditional approach of unsupervised feature learning followed by model training. We establish the superiority of our method on several popular machine learning applications including multi-label learning, multi-class classification, and semi-supervised clustering.	algorithm;cluster analysis;computation;coordinate descent;feature learning;inductive reasoning;machine learning;mathematical optimization;memory controller;multi-label classification;multiclass classification;nonlinear system;polynomial kernel;semi-supervised learning;semiconductor industry;stochastic gradient descent	Si Si;Kai-Yang Chiang;Cho-Jui Hsieh;Nikhil Rao;Inderjit S. Dhillon	2016		10.1145/2939672.2939809	computer science;artificial intelligence;machine learning;data mining;mathematics;algorithm	ML	22.721177485707486	-43.03400533982971	2092
768df18a4fc2e63d1a94291428166b09ee4855aa	new discontinuity-induced bifurcations in chua's circuit	chua s circuit;discontinuity;bifurcation;stability;equilibrium	Chua's circuit, an archetypal example of nonsmooth dynamical systems, exhibits mostly discontinuous bifurcations. More complex dynamical phenomena of Chua's circuit are presented here due to discontinuity-induced bifurcations. Some new kinds of classical bifurcations are revealed and analyzed, including the coexistence of two classical bifurcations and bifurcations of equilibrium manifolds. The local dynamical behavior of the boundary equilibrium points located on switch boundaries is found to be determined jointly by the Jacobian matrices evaluated before and after switching. Some new discontinuous bifurcations are also observed, such as the coexistence of two discontinuous and one classical bifurcation.	chua's circuit;reflections of signals on conducting lines	Shihui Fu;Qishao Lu;Xiangying Meng	2015	I. J. Bifurcation and Chaos	10.1142/S021812741550090X	stability;discontinuity;pitchfork bifurcation;calculus;control theory;mathematics;statistics	EDA	80.80442414983713	3.8126555588236157	2094
10b97d4b7aac7bd819cca683f68ab48e017d9f7b	3d shape registration using regularized medial scaffolds	global registration;assignment graph;novel method;initial alignment close;medial scaffold;medial axis;global optimal;shape registration;practical known method;medial structure;regularized medial scaffolds;graph matching;iterative closest point;image registration;point cloud;graph theory;mesh generation;global optimization;noise shaping	This work proposes a method for global registration based on matching 3D medial structures of unorganized point clouds or triangulated meshes. Most practical known methods are based on the iterative closest point (ICP) algorithm, which requires an initial alignment close to the globally optimal solution to ensure convergence to a valid solution. Furthermore, it can also fail when there are points in one dataset with no corresponding matches in the other dataset. The proposed method automatically finds an initial alignment close to the global optimal by using the medial structure of the datasets. For this purpose, we first compute the medial scaffold of a 3D dataset: a 3D graph made of special shock curves linking special shock nodes. This medial scaffold is then regularized exploiting the known transitions of the 3D medial axis under deformation or perturbation of the input data. The resulting simplified medial scaffolds are then registered using a modified graduated assignment graph matching algorithm. The proposed method shows robustness to noise, shape deformations, and varying surface sampling densities.	algorithm;apache axis;emoticon;ground truth;iteration;iterative closest point;iterative method;loss function;matching (graph theory);maxima and minima;medial graph;minimum bounding box;point cloud;robustness (computer science);sampling (signal processing)	Ming-Ching Chang;Frederic Fol Leymarie;Benjamin B. Kimia	2004	Proceedings. 2nd International Symposium on 3D Data Processing, Visualization and Transmission, 2004. 3DPVT 2004.	10.1109/TDPVT.2004.1335423	computer vision;mathematical optimization;theoretical computer science;mathematics	Vision	49.83998555759121	-51.595521063326636	2097
b73ce2d9a245ae848d5ac0e906b055791f3a1a69	gpumafia: efficient subspace clustering with mafia on gpus	efficient subspace;subspace clustering;high-dimensional data;mafia subspace;magnitude speedup;practical datasets;gpu version;data analytics;multi-dimensional data;computational complexity;data point dimension	Clustering, i.e., the identification of regions of similar objects in a multi-dimensional data set, is a standard method of data analytics with a large variety of applications. For high-dimensional data, subspace clustering can be used to find clusters among a certain subset of data point dimensions and alleviate the curse of dimensionality.#R##N##R##N#In this paper we focus on the MAFIA subspace clustering algorithm and on using GPUs to accelerate the algorithm. We first present a number of algorithmic changes and estimate their effect on computational complexity of the algorithm. These changes improve the computational complexity of the algorithm and accelerate the sequential version by 1---2 orders of magnitude on practical datasets while providing exactly the same output. We then present the GPU version of the algorithm, which for typical datasets provides a further 1---2 orders of magnitude speedup over a single CPU core or about an order of magnitude over a typical multi-core CPU. We believe that our faster implementation widens the applicability of MAFIA and subspace clustering.		Andrew V. Adinetz;Jiri Kraus;Jan Meinke;Dirk Pleiter	2013		10.1007/978-3-642-40047-6_83	correlation clustering;constrained clustering;data stream clustering;parallel computing;computer science;theoretical computer science;canopy clustering algorithm;machine learning;cure data clustering algorithm;data mining;cluster analysis	EDA	-2.278456742599108	-40.124681172010824	2099
1da386297f809df3db2754f56d9a87b7ae80b8f1	method for classifying cardiac arrhythmias using photoplethysmography	signal detection blood flow measurement cardiology diseases medical signal processing photoplethysmography signal classification;cardiac disease cardiac arrhythmia classification photoplethysmography mobile computing miniature devices wearable technologies clinical applications wearable devices wearable sensing electrocardiogram photoplethysmograph signals heart electrical activities ambulatory ppg monitoring arrhythmia detection ventricular tachycardia ventricular premature contraction normal sinus rhythm supraventricular premature contraction ablation therapy ventricular fibrillation cardiac death;electrocardiography feature extraction heart rate variability testing biomedical monitoring sensors support vector machines	Advances in mobile computing and miniature devices have contributed to the accelerated development of wearable technologies for clinical applications. The new trend of wearable technologies has fostered a growth of interest for sensors that can be easily integrated into wearable devices. In particular, photoplethysmography (PPG) is especially suitable for wearable sensing, as it is low-cost, noninvasive, and does not require wet electrodes like the electrocardiogram. Photoplethysmograph signals contain rich information about the blood pulsating variation which is strongly related to the electrical activities of the heart. Therefore, in this paper we hypothesize that the ambulatory PPG monitoring could be employed for arrhythmia detection and classification. This paper presents a method for classifying ventricular premature contraction (VPC) and ventricular tachycardia (VT) from normal sinus rhythm (NSR) and supraventricular premature contraction (SVPC) recorded in patients going through ablation therapy for arrhythmia. Although occasional VPCs are benign, the increase in the frequency of VPC events may lead to VT, which in turn,could evolve into ventricular fibrillation and sudden cardiac death. Therefore the accurate measurement of VPC frequency and early detection of VT events becomes essential for patients with cardiac disease.	artificial cardiac pacemaker;classification;computation (action);early diagnosis;electrocardiography;expectation propagation;extraction;gain;heart diseases;heart rate variability;implantable cardioverter-defibrillator;mobile computing;neutral sidebent rotated;nonlinear system;patients;photoplethysmography;premature ventricular contractions;pulse (signal processing);spectral density;sudden cardiac death;support vector machine;tachycardia;ventricular fibrillation;virtual private cloud;wearable computer;wearable technology;electrode;sensor (device)	Luisa F. Polanía;Lalit K. Mestha;David T. Huang;Jean-Philippe Couderc	2015	2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2015.7319899	medicine;biological engineering;medical emergency;cardiology	EDA	9.52292513561322	-87.87433184545237	2102
8e88c3eb72bd7b5af0c20462ba1f70c5d18ece76	non-negative matrix factorization for topic modeling		In this abstract, a new formulation of the Non-negative Matrix Factorization problem for topic modelingwill be presented. It allows the user to iteratively improve the topic model with a higher level of detail than current NMF-based approaches such as [2], achieving a higher performance than other weakly-supervised LDA-based methods.	level of detail;local-density approximation;non-negative matrix factorization;topic model	Alberto Purpura	2018			discrete mathematics;topic model;non-negative matrix factorization;mathematics	Web+IR	29.315001289464423	-39.639377624964766	2107
edd3397c0d906498b05a0e7b5ca9d83c2a667234	the vehicle routing problem with restricted mixing of deliveries and pickups	heuristic;routing;backhauling;logistics;transportation;ha33 management science;optimization	The vehicle routing problem with deliveries and pickups is one of the main problems within reverse logistics. This paper focuses on an important assumption that divides the literature on the topic, namely the restriction that all deliveries must be completed before pickups can be made. A generalised model is presented, together with a mathematical formulation and its resolution. The latter is carried out by adopting a suitable implementation of the reactive tabu search metaheuristic. Results show that significant savings can be achieved by allowing a mixture of delivery and pickup loads on-board and yet not incurring delays and driver inconvenience.	vehicle routing problem	Gábor Nagy;Niaz A. Wassan;Saïd Salhi	2013	J. Scheduling	10.1007/s10951-012-0296-7	logistics;transport;mathematical optimization;routing;heuristic;backhaul;computer science	Theory	15.348512384379971	1.4826953414723243	2111
ba474bc2222ce2853f4cb575607c8cca9d3fc037	an activity canyon characterization of the pharmacological topography	computer applications in chemistry;theoretical and computational chemistry;computational biology bioinformatics;documentation and information in chemistry	BACKGROUND Highly chemically similar drugs usually possess similar biological activities, but sometimes, small changes in chemistry can result in a large difference in biological effects. Chemically similar drug pairs that show extreme deviations in activity represent distinctive drug interactions having important implications. These associations between chemical and biological similarity are studied as discontinuities in activity landscapes. Particularly, activity cliffs are quantified by the drop in similar activity of chemically similar drugs. In this paper, we construct a landscape using a large drug-target network and consider the rises in similarity and variation in activity along the chemical space. Detailed analysis of structure and activity gives a rigorous quantification of distinctive pairs and the probability of their occurrence.   RESULTS We analyze pairwise similarity (s) and variation (d) in activity of drugs on proteins. Interactions between drugs are quantified by considering pairwise s and d weights jointly with corresponding chemical similarity (c) weights. Similarity and variation in activity are measured as the number of common and uncommon targets of two drugs respectively. Distinctive interactions occur between drugs having high c and above (below) average d (s). Computation of predicted probability of distinctiveness employs joint probability of c, s and of c, d assuming independence of structure and activity. Predictions conform with the observations at different levels of distinctiveness. Results are validated on the data used and another drug ensemble. In the landscape, while s and d decrease as c increases, d maintains value more than s. c ∈ [0.3, 0.64] is the transitional region where rises in d are significantly greater than drops in s. It is fascinating that distinctive interactions filtered with high d and low s are different in nature. It is crucial that high c interactions are more probable of having above average d than s. Identification of distinctive interactions is better with high d than low s. These interactions belong to diverse classes. d is greatest between drugs and analogs prepared for treatment of same class of ailments but with different therapeutic specifications. In contrast, analogs having low s would treat ailments from distinct classes.   CONCLUSIONS Intermittent spikes in d along the axis of c represent canyons in the activity landscape. This new representation accounts for distinctiveness through relative rises in s and d. It provides a mathematical basis for predicting the probability of occurrence of distinctiveness. It identifies the drug pairs at varying levels of distinctiveness and non-distinctiveness. The predicted probability formula is validated even if data approximately satisfy the conditions of its construction. Also, the postulated independence of structure and activity is of little significance to the overall assessment. The difference in distinctive interactions obtained by s and d highlights the importance of studying both of them, and reveals how the choice of measurement can affect the interpretation. The methods in this paper can be used to interpret whether or not drug interactions are distinctive and the probability of their occurrence. Practitioners and researchers can rely on this identification for quantitative modeling and assessment.	activity diagram;activity tracker;analog;apache axis;chemical similarity;chemical space;class;computation;drug interactions;function (biology);greater than;illness (finding);interaction;interpretation (logic);mathematics;mental association;pharmacology;probability;quantitation;specification;topography;transitional region;weight	Varsha S. Kulkarni;David J. Wild	2016		10.1186/s13321-016-0153-3	biology;toxicology;bioinformatics	Comp.	8.41818099339864	-59.695963130639264	2114
676c343dfc1a8794141d7455a66eb551155a1935	bidirectional selection cooperation approach based on multi-agent	protocols;collaborative work;multi agent system;design engineering;processor scheduling;cooperation;contract net protocol;collaboration;contracts;multi agent;collaboration protocols costs multiagent systems design engineering processor scheduling forward contracts broadcasting collaborative work mechatronics;forward contracts;powders;detonator production schedule cooperation bidirectional selection cooperation contract net multi agent;schedules;communication cost;detonator production schedule;production scheduling;broadcasting;mechatronics;bidirectional selection cooperation;contract net;multiagent systems	A bidirectional selection cooperation approach based on Multi-Agent is presented, in which some parameters, such as the ability value, communication costs and so on, are introduced into the classical contract net protocol. More importantly, the bidirectional selection mechanism between assign task agent and perform task agent is introduced into collaboration for the first time. These parameters reduce the communication consumption and balance the resource load among all Agents. The bidirectional selection cooperation increases cooperation efficiency and ensures the reliability and correctness of the whole Multi-Agent system. At last, taking detonator production schedule as an example, it proves that the bidirectional selection cooperation approach prolongs the system lifetime, improves the system load and increases the accomplished tasks in unit time.	contract net protocol;correctness (computer science);load (computing);multi-agent system;parametric design;simulation;software agent	Xuan Wang;Jing-long Yang	2010	The 2010 14th International Conference on Computer Supported Cooperative Work in Design	10.1109/CSCWD.2010.5471959	communications protocol;powder;simulation;schedule;computer science;artificial intelligence;multi-agent system;distributed computing;scheduling;contract net protocol;cooperation;broadcasting;forward contract;collaboration	Robotics	-0.46083842089050725	2.53027194479213	2117
b3e33ea52a41111aa3422a2187a84f01c0900868	reversals of fortune	genome rearrangement;genetic map;objective function;comparative mapping;exact algorithm;partial order	The objective function of the genome rearrangement problems allows the integration of other genome-level problems so that they may be solved simultaneously. Three examples, all of which are hard: 1) Orientation assignment for unsigned genomes. 2 ) Ortholog identification in the presence of multiple copies of genes. 3) Linearisation of‘partially ordered genomes. The comparison of traditional genetic maps by rearrangement algorithms poses all these problems. We combine heuristics for the first two problems with an exact algorithm for the third to solve a moderate-sized instance comparing maps of cereal genomes.	exact algorithm;heuristic (computer science);homology (biology);hybrid genome assembly;loss function;map;optimization problem;regular expression	David Sankoff;Chunfang Zheng;Aleksander Lenert	2005		10.1007/11554714_12	partially ordered set;mathematical optimization;bioinformatics;machine learning;mathematics;genetics	Theory	0.3643928575343228	-50.9087023356826	2119
13313124277b42dc61096edd170e74b87592ddab	image denoising via sparse and redundant representations over learned dictionaries	alternative method;gaussian noise;iterative method;traitement signal;evaluation performance;decomposition valeur singuliere;k svd algorithm;discrete cosine transform dct;performance evaluation;image processing;bayesian treatment;learning;redundancia;image databank;bayes methods;evaluacion prestacion;persecusion adaptativa;localization;transformation cosinus discrete;singular value decomposition;poursuite adaptative;ruido gaussiano;sparse redundant representations;additive noise;image database;ruido aditivo;procesamiento imagen;bruit additif;estimation a posteriori;awgn;localizacion;k svd;indexing terms;qualite image;traitement image;discrete cosine transform;a posteriori estimation;reduccion ruido;metodo iterativo;algorithme;aprendizaje;etat actuel;algorithm;apprentissage;dictionnaire;localisation;redundancy;estimacion a posteriori;methode alternative;maximum a posteriori map estimation;methode iterative;discrete cosine transforms;signal processing;noise reduction;metodo alternativo;image quality;banco imagen;state of the art;dictionaries;banque image;bruit gaussien;reduction bruit;map estimation;representacion parsimoniosa;matching pursuit;dictionary learning;estado actual;suppression bruit image;calidad imagen;decomposicion valor singular;homogeneous gaussian additive noise;image denoising;zero mean white noise;sparse representation;procesamiento senal;bayesian reconstruction;diccionario;sparse representations;redondance;dis crete cosine transform;representation parcimonieuse;algoritmo;visual databases	We address the image denoising problem, where zero-mean white and homogeneous Gaussian additive noise is to be removed from a given image. The approach taken is based on sparse and redundant representations over trained dictionaries. Using the K-SVD algorithm, we obtain a dictionary that describes the image content effectively. Two training options are considered: using the corrupted image itself, or training on a corpus of high-quality image database. Since the K-SVD is limited in handling small image patches, we extend its deployment to arbitrary image sizes by defining a global image prior that forces sparsity over patches in every location in the image. We show how such Bayesian treatment leads to a simple and effective denoising algorithm. This leads to a state-of-the-art denoising performance, equivalent and sometimes surpassing recently published leading alternative denoising methods	additive white gaussian noise;arabic numeral 0;body dysmorphic disorders;body of uterus;concentrate dosage form;data dictionary;deploy;dictionary [publication type];generalization (psychology);hl7publishingsubsection <operations>;handling (psychology);k-svd;large;noise reduction;normal statistical distribution;omeprazole;online advertising;scientific publication;singular value decomposition;snowflake vitreoretinal degeneration;sparse matrix;text corpus;utility functions on indivisible goods;algorithm	Michael Elad;Michal Aharon	2006	IEEE Transactions on Image Processing	10.1109/TIP.2006.881969	image quality;gaussian noise;additive white gaussian noise;computer vision;feature detection;index term;internationalization and localization;image processing;k-svd;computer science;machine learning;signal processing;discrete cosine transform;noise reduction;sparse approximation;mathematics;iterative method;redundancy;singular value decomposition;non-local means;statistics;matching pursuit	Vision	53.98818009276493	-74.22098361665125	2121
44cd5ce475eef4a0425899c9d756ee3206e77bc5	a multi-layer inference approach to reconstruct condition-specific genes and their regulation	transcription genetic;breast neoplasms;female;cell adhesion molecules;systems biology;gene regulatory networks;transcription factors;cold temperature;gene expression regulation fungal;gene expression regulation neoplastic;antigens neoplasm;arabidopsis;gene expression regulation;humans;acclimatization;gene expression profiling	UNLABELLED An important topic in systems biology is the reverse engineering of regulatory mechanisms through reconstruction of context-dependent gene networks. A major challenge is to identify the genes and the regulations specific to a condition or phenotype, given that regulatory processes are highly connected such that a specific response is typically accompanied by numerous collateral effects. In this study, we design a multi-layer approach that is able to reconstruct condition-specific genes and their regulation through an integrative analysis of large-scale information of gene expression, protein interaction and transcriptional regulation (transcription factor-target gene relationships). We establish the accuracy of our methodology against synthetic datasets, as well as a yeast dataset. We then extend the framework to the application of higher eukaryotic systems, including human breast cancer and Arabidopsis thaliana cold acclimation. Our study identified TACSTD2 (TROP2) as a target gene for human breast cancer and discovered its regulation by transcription factors CREB, as well as NFkB. We also predict KIF2C is a target gene for ER-/HER2- breast cancer and is positively regulated by E2F1. The predictions were further confirmed through experimental studies.   AVAILABILITY The implementation and detailed protocol of the layer approach is available at http://www.egr.msu.edu/changroup/Protocols/Three-layer%20approach%20 to % 20reconstruct%20condition.html.	acclimatization;cold temperature;context-sensitive language;gene expression;gene regulatory network;inference;kinesin-like protein kif2c;layer (electronics);mammary neoplasms;numerous;reverse engineering;silo (dataset);synthetic intelligence;systems biology;transcription factor;transcription (software);transcription, genetic;transcriptional regulation	Ming Wu;Li Liu;Hussein Hijazi;Christina Chan	2013	Bioinformatics	10.1093/bioinformatics/btt186	biology;cell adhesion molecule;gene regulatory network;regulation of gene expression;acclimatization;bioinformatics;gene expression profiling;genetics;systems biology;transcription factor	Comp.	5.395184866379194	-58.733879899664466	2122
8c7c95a75fd78b64fadf19c782025bd70c14cac5	reliable simulation with input uncertainties using an interval-based approach	decision making;random processes;simulation;statistical distributions;interval arithmetic;random interval-based approach;reliable simulation mechanism;robust decision making support;statistical distribution;uncertain probability	Uncertainty associated with input parameters and models in simulation has gained attentions in recent years. The sources of uncertainties include lack of data and lack of knowledge about physical systems. In this paper, we present a new reliable simulation mechanism to help improve simulation robustness when significant uncertainties exist. The new mechanism incorporates variabilities and uncertainties based on imprecise probabilities, where the statistical distribution parameters in the simulation are intervals instead of precise real numbers. The mechanism generates random interval variates to model the inputs. Interval arithmetic is applied to simulate a set of scenarios simultaneously in each simulation run. To ensure that the interval results bound those from the traditional real-valued simulation, a generic approach is also proposed to specify the number of replications in order to achieve the desired robustness. This new reliable simulation mechanism can be applied to address input uncertainties to support robust decision making.	interval arithmetic;simulation	Ola Ghazi Batarseh;Yan Wang	2008	2008 Winter Simulation Conference		probability distribution;stochastic process;mathematical optimization;computer science;machine learning;mathematics;statistics	Robotics	29.36058374134439	-17.087723363574295	2123
08f2c665b3280081aa5c8aac8b614e8382dbe792	depth perception of distorted stereoscopic images	databases;3d quality of experience distorted stereoscopic images perceptual quality depth information stereoscopic natural images depth quality absolute category rating protocol acr protocol monocular cues spatial quality stereoscopic cues asymmetric distortions label depth polarizations depth perception difficulty index dpdi image content dependency distortion type dependency;visual perception protocols quality of experience stereo image processing;transform coding;distortion;asymmetric distortion depth perception stereoscopic image 3d image image quality assessment quality of experience;three dimensional displays;image quality;solid modeling;stereo image processing;three dimensional displays image quality distortion stereo image processing transform coding databases solid modeling	How to measure the perceptual quality of depth information in stereoscopic natural images, especially images undergoing different types of symmetric and asymmetric distortions, is a fundamentally important issue that is not well understood. In this paper, we present two of our recent subjective studies on depth quality. The first one follows the absolute category rating (ACR) protocol that is widely used in general image quality assessment research. We find that traditional approaches such as ACR is problematic in this scenario because monocular cues and the spatial quality of images have strong impacts on the depth quality scores given by subjects, making it difficult to single out the actual contributions of stereoscopic cues in depth perception. To overcome this problem, we carry out the second subjective study where depth effect is synthesized at different depth levels before various types and levels of symmetric and asymmetric distortions are applied. Instead of following the traditional approach, we ask subjects to identify and label depth polarizations, and a Depth Perception Difficulty Index (DPDI) is developed based on the percentage of correct and incorrect subject judgements. We find this approach highly effective at quantifying depth perception induced by stereo cues and observe a number of interesting effects regarding image content dependency, distortion type dependency, and the impacts of symmetric versus asymmetric distortions. We believe that these are useful steps towards building comprehensive 3D quality-of-experience models for stereoscopic images.	depth perception;distortion;image quality;stereoscopy	Jiheng Wang;Shiqi Wang;Zhou Wang	2015	2015 IEEE 17th International Workshop on Multimedia Signal Processing (MMSP)	10.1109/MMSP.2015.7340832	image quality;computer vision;transform coding;distortion;telecommunications;computer science;mathematics;multimedia;solid modeling;computer graphics (images)	Vision	63.42916489537967	-62.85872587627832	2124
31edf992ae5628992089f21c85ea5cdc5f9f93ae	detector adaptation by maximising agreement between independent data sources	training collection;dynamic programming;detectors;supervised annotation;information resources;image segmentation;skin pixel detection;image resolution;information retrieval;dynamic programming algorithm;skin;thermal infrared;image classification;training data;skin pixel detection detector adaptation independent data sources training collection supervised annotation thermal infrared visual imagery dynamic programming algorithm shadow pixel;feedback;infrared imaging;heuristic algorithms;training data mutual information face detection infrared detectors infrared imaging skin image segmentation heuristic algorithms feedback information resources;mutual information;object detection dynamic programming image classification image resolution;face detection;infrared detectors;detector adaptation;object detection;independent data sources;shadow pixel;visual imagery	Traditional methods for creating classifiers have two main disadvantages. Firstly, it is time consuming to acquire, or manually annotate, the training collection. Secondly, the data on which the classifier is trained may be over-generalised or too specific. This paper presents our investigations into overcoming both of these drawbacks simultaneously, by providing example applications where two data sources train each other. This removes both the need for supervised annotation or feedback, and allows rapid adaptation of the classifier to different data. Two applications are presented: one using thermal infrared and visual imagery to robustly learn changing skin models, and another using changes in saturation and luminance to learn shadow appearance parameters.	algorithm;binary image;color space;mutual information;pixel	Ciarán Ó Conaire;Noel E. O'Connor;Alan F. Smeaton	2007	2007 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2007.383448	computer vision;computer science;machine learning;dynamic programming;pattern recognition	Vision	38.058823350379086	-64.56399995850049	2126
960ed3f450e1f750c0b64139cf4c8c45323aa911	multi-objective differential evolution algorithm for multi-label feature selection in classification	differential evolution;multi objective;multi label feature selection;classification	Multi-label feature selection is a multi-objective optimization problem in nature, which has two conflicting objectives, i.e., the classification performance and the number of features. However, most of existing approaches treat the task as a single objective problem. In order to meet different requirements of decision-makers in real-world applications, this paper presents an effective multi-objective differential evolution for multi-label feature selection. The proposed algorithm applies the ideas of efficient non-dominated sort, the crowding distance and the Pareto dominance relationship to differential evolution to find a Pareto solution set. The proposed algorithm was applied to several multi-label classification problems, and experimental results show it can obtain better performance than two conventional methods. abstract environment.	algorithm;differential evolution;feature selection;multi-label classification	Yong Zhang;Dun-Wei Gong;Miao Rong	2015		10.1007/978-3-319-20466-6_36	differential evolution;mathematical optimization;biological classification;computer science;machine learning;pattern recognition;mathematics	ML	10.218393963001734	-43.61786183328758	2127
59ee98c7cf7f47dcb609db4030f0942d87791f61	he-tree: a framework for detecting changes in clustering structure for categorical data streams	ingenieria del conocimiento;hierarchical system;cluster algorithm;real time decision making;change detection;streaming;ingenierie connaissances;bepress selected works;base donnee tres grande;categorical data clustering;structure arborescente;data stream;real time;metodo arborescente;processus metier;systeme hierarchise;prise de decision;algorithme numerique;sistema jerarquizado;transmission en continu;monitoring;estructura arborescente;data stream mining;temps reel;tree structure;estructura datos;numerical algorithm;number of clusters;tiempo real;algoritmo numerico;proceso oficio;tree structured method;structure donnee;methode arborescente;monitorage;transmision fluyente;data stream mining categorical data clustering change detection;very large databases;monitoreo;toma decision;data structure;data stream minting;categorical data;business process;donnee categorielle;dato categorico;knowledge engineering	Analyzing clustering structures in data streams can provide critical information for real-time decision making. Most research in this area has focused on clustering algorithms for numerical data streams, and very few have proposed to monitor the change of clustering structure. Most surprisingly, to our knowledge, no work has been proposed on monitoring clustering structure for categorical data streams. In this paper, we present a framework for detecting the change of primary clustering structure in categorical data streams, which is indicated by the change of the best number of clusters (Best K) in the data stream. The framework uses a Hierarchical Entropy Tree structure (HE-Tree) to capture the entropy characteristics of clusters in a data stream, and detects the change of Best K by combining our previously developed BKPlot method. The HE-Tree can efficiently summarize the entropy property of a categorical data stream and allow us to draw precise clustering information from the data stream for generating high-quality BKPlots. We also develop the time-decaying HE-Tree structure to make the monitoring more sensitive to recent changes of clustering structure. The experimental result shows that with the combination of the HE-Tree and the BKPlot method we are able to promptly and precisely detect the change of clustering structure in categorical data streams.	ace;approximation algorithm;cns;categorical variable;cluster analysis;energy citations database;ibm notes;level of measurement;norm (social);numerical analysis;primary clustering;real-time locating system;sensor;tree (data structure);tree structure;wiki	Keke Chen;Ling Liu	2009	The VLDB Journal	10.1007/s00778-009-0134-5	correlation clustering;constrained clustering;determining the number of clusters in a data set;data stream clustering;data structure;categorical variable;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;consensus clustering;knowledge engineering;cure data clustering algorithm;data mining;database;hierarchical control system;tree structure;data stream mining;cluster analysis;business process;brown clustering;change detection;algorithm;clustering high-dimensional data;conceptual clustering	DB	-3.1667140542435117	-32.832245515700826	2134
e586e35d00853925a5555f89b59c7c04d1a6ac78	conditioning analysis of missing data estimation for large sensor array	image restoration;estimation algorithm;ccd image sensors;computer vision;experimental results conditioning analysis missing data estimation large sensor arrays deblurring denoising image restoration ccd sensor arrays butting technique regularization theory noise signal to noise ratio blur kernel estimation;sensor array;computer vision image restoration ccd image sensors;missing data;signal to noise ratio;kernel estimate;data analysis kernel sensor arrays image restoration noise reduction algorithm design and analysis charge coupled devices charge coupled image sensors image converters signal analysis	Optimal missing data estimation algorithms including deblurring and denoising are designed to restore images captured from large CCD sensor arrays using butting technique, where 1 to 2 columns of data are missed at the butting edge. We developed consistency method with separable deblurring to estimate the missing data. This method converts an ill-posed restoration problem into a well-posed one by making few assumptions based on regularization theory. Under the condition that no noise is inserted, and the separable blur kernel is exactly known, the consistency method can deblur the original image and at the same time estimate the missing column(s) exactly. However, this algorithm becomes unstable when large noise is inserted or inaccurate estimation of blur kernel is made. Conditioning analysis is used to quantify the amount of ill condition of the blur kernel when the assumptions are relaxed to different levels, which provides a solid measurement on how stable the system will remain knowing the signal-to-noise ratio and the inaccuracy of the blur kernel estimation. Experimental results from different approaches are compared.	algorithm;circuit restoration;column (database);control theory;deblurring;gaussian blur;kernel (operating system);missing data;noise reduction;signal-to-noise ratio;well-posed problem	Hairong Qi;Wesley E. Snyder	2000		10.1109/CVPR.2000.854918	image restoration;computer vision;missing data;computer science;pattern recognition;mathematics;signal-to-noise ratio;sensor array;statistics	Vision	58.73584607147259	-71.66057280979048	2137
83151907b2278b6915474bcc49e0a5396c7f054a	a bootstrap approach for lower injury levels of the risk curves	bootstrap method;sample size;statistical simulation;injury risk criterion;confidence interval;censored data;survival analysis;bootstrapping;asymptotic normal;numerical simulation	Survival analysis is widely applied to develop injury risk curves from biomechanical data. To obtain more accurate estimation of confidence intervals of parameters, bootstrap method was evaluated by a designed simulation process. Four censoring schemes and various sample sizes were considered to investigate failure time parameters corresponding to low-level injury probabilities. In the numerical simulations, the confidence interval ranges developed by bootstrapping were about two-third of the corresponding ranges calculated by asymptotical normal approximation and showed highest reduction for censored datasets with smaller sample size (≤ 40). In analysis of two experimental datasets with reduced sample sizes and mixed censored data, it was shown that the bootstrapping reduce significantly the confidence intervals as well. The results presented in this study recommend using bootstrapping in development of more accurate confidence intervals for risk curves in injury biomechanics, which consequently will lead to better regulations and safer vehicle designs.		Yuan-Chiao Lu;Costin D. Untaroiu	2012	Computer methods and programs in biomedicine	10.1016/j.cmpb.2011.03.015	computer simulation;sample size determination;bootstrapping;econometrics;confidence interval;computer science;pattern recognition;mathematics;survival analysis;censoring;bootstrapping;robust confidence intervals;statistics	NLP	29.601331211771573	-21.74632308113118	2139
f102148a58bad0da7b4513cae1138642f4c5c445	static fracture tolerance of human metatarsal in being run over by robot		Human foot injury due to being run over by a robot is a hazardous event caused by robot motion. This study aims to provide information for the safety design of robots in terms of the risk of foot fracture. A model for scaling the fracture loads of alternate specimens (bear metatarsals) to those of human metatarsals considering the effects of soft tissue is proposed and validated. Using the model, fracture tolerance of metatarsals of a human female foot of statistically typical size is predicted to be 1.24 kN at 10% fracture probability, and 0.53 kN at 1% fracture probability with 95% confidence.	image scaling;industrial robot;intrusion tolerance;mobile robot;robot	Tatsuo Fujikawa;Yoichi Asano;Tetsuya Nishimoto;Rie Nishikata	2017	2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2017.8206618	artificial intelligence;computer vision;orthodontics;computer science;foot injury;foot fracture;robot	Robotics	73.62308767945184	-29.65603718559003	2146
8147decfc04108f60617f6c100befd3e8a6d7c37	deepasl: kinetic model incorporated loss for denoising arterial spin labeled mri via deep residual learning		Arterial spin labeling (ASL) allows to quantify the cerebral blood flow (CBF) by magnetic labeling of the arterial blood water. ASL is increasingly used in clinical studies due to its noninvasiveness, repeatability and benefits in quantification. However, ASL suffers from an inherently low-signal-to-noise ratio (SNR) requiring repeated measurements of control/spin-labeled (C/L) pairs to achieve a reasonable image quality, which in return increases motion sensitivity. This leads to clinically prolonged scanning times increasing the risk of motion artifacts. Thus, there is an immense need of advanced imaging and processing techniques in ASL. In this paper, we propose a novel deep learning based approach to improve the perfusion-weighted image quality obtained from a subset of all available pairwise C/L subtractions. Specifically, we train a deep fully convolutional network (FCN) to learn a mapping from noisy perfusion-weighted image and its subtraction (residual) from the clean image. Additionally, we incorporate the CBF estimation model in the loss function during training, which enables the network to produce high quality images while simultaneously enforcing the CBF estimates to be as close as reference CBF values. Extensive experiments on synthetic and clinical ASL datasets demonstrate the effectiveness of our method in terms of improved ASL image quality, accurate CBF parameter estimation and considerably small computation time during testing.	computation;deep learning;display resolution;estimation theory;experiment;image quality;loss function;noise reduction;repeatability;signal-to-noise ratio;synthetic intelligence;time complexity	Cagdas Ulas;Giles Tetteh;Stephan Kaczmarz;Christine Preibisch;Bjoern H. Menze	2018		10.1007/978-3-030-00928-1_4	artificial intelligence;computer science;pattern recognition;computer vision;residual;repeatability;cerebral blood flow;noise reduction;image quality;deep learning;estimation theory;spin-½	Vision	45.36873582406208	-82.30407931953174	2154
aefcc2fbdd24d5ecd0b254a667e52c3268c4cb42	an approach to pattern recognition by evolutionary computation		Evolutionary Computation has been inspired by the natural phenomena of evolution. It provides a quite general heuristic, exploiting few basic concepts: reproduction of individuals, variation phenomena that affect the likelihood of survival of individuals, inheritance of parents features by offspring. EC has been widely used in the last years to effectively solve hard, non linear and very complex problems.#N#Among the others, EC–based algorithms have also been used to tackle#N#classification problems. Classification is a process according to which  an object is attributed to one of a finite set of classes or, in other  words, it is recognized as belonging to a set of equal or similar entities, identified by a label. Most likely, the main aspect of classification concerns the generation of prototypes to be used to recognize unknown patterns. The role of prototypes is that of representing patterns belonging to the different classes defined within a given problem. For most of the problems of practical interest, the generation of such prototypes is a very hard problem, since a prototype must be able to represent patterns belonging to the same class, which may be significantly dissimilar each other. They must also be able to discriminate patterns belonging to classes different from the one that they represent. Moreover, a prototype should contain the minimum amount of information required to satisfy the requirements just mentioned. The research presented in this thesis, has led to the definition of an EC–based framework to be used for prototype generation. The defined framework does not provide for the use of any particular kind of prototypes. In fact, it can generate any kind of prototype once an encoding scheme for the used prototypes has been defined. The generality of the framework can be exploited to develop many applications. The framework has been employed to implement two specific applications for prototype generation. #N#The developed applications have been tested on several data sets and the results compared with those obtained by other approaches previously presented in the literature.	evolutionary computation;pattern recognition	Francesco Fontanella	2006			artificial intelligence;data mining;mathematics;algorithm	Vision	2.236253324596951	-37.52557602284109	2157
577d20d7582e94f7b1161e79d5e2ee62e23fb52b	from a gaussian mixture model to additive fuzzy system	nonadditive fuzzy systems;takagi sugeno fuzzy model;probability;takagi sugeno;gaussian processes;integral equations;probability gaussian processes fuzzy set theory fuzzy systems;generalized fuzzy model gfm;probabilistic system;functional equivalence;indexing terms;fuzzy set theory;fuzzy sets theory gaussian mixture model additive fuzzy systems probabilistic system generalized fuzzy model takagi sugeno fuzzy model;distribution function;generalized fuzzy models;gaussian mixture model;gaussian mixture models;takagi sugeno models;probabilistic systems;mamdani larsen models;gaussian mixture model gmm;additive fuzzy systems;choquet fuzzy integral;takagi sugeno models additive fuzzy systems conditional means functional equivalence gaussian mixture models generalized fuzzy models mamdani larsen models probabilistic systems;q measure and lambda measure;fuzzy systems fuzzy sets uncertainty gallium nitride mathematical model distribution functions monopoly fuzzy set theory possibility theory;fuzzy systems;fuzzy system;fuzzy model;conditional means	This work explores how a kind of probabilistic system, namely the Gaussian mixture model (GMM), can be translated to an additive fuzzy system. We will prove the mathematical equivalence between the conditional mean of a GMM, and the defuzzified output of a generalized fuzzy model (GFM). The relationship between a GMM and a GFM, and the conditions for GMM to GFM translation will be made explicit in the form of theorems. The work will then extend to special cases of the GFM, specifically the Mamdani-Larsen and Takagi-Sugeno fuzzy models. The possibility of reverse translation, that is, from a GFM to a GMM will also be discussed. Finally, we will consider the generality of a GMM, specifically how it can approximate other distribution functions.	fuzzy control system;mixture model;utility functions on indivisible goods	Ming-Tao Gan;Madasu Hanmandlu;Ai Hui Tan	2005	IEEE Trans. Fuzzy Systems	10.1109/TFUZZ.2004.841728	mathematical optimization;discrete mathematics;computer science;artificial intelligence;machine learning;mixture model;mathematics;fuzzy control system;statistics	Embedded	0.5973932825064255	-21.094303745246002	2161
3ed1cb38cf3e2820a5ef0de66a2c82127f84eb5f	co-evolution positions and rules for antigenic variants of human influenza a/h3n2 viruses	influenza a virus h3n2 subtype;influenza virus;decision tree;influenza human;antigenic variation;amino acid;rule based;influenza a;genetic diversity;genetics;computational biology bioinformatics;influenza viruses;models molecular;biological evolution;hemagglutinins viral;influenza a virus;seasonality;hemagglutination inhibition;prediction accuracy;algorithms;humans;combinatorial libraries;information gain;computer appl in life sciences;article;public health;microarrays;bioinformatics	"""In pandemic and epidemic forms, avian and human influenza viruses often cause significant damage to human society and economics. Gradually accumulated mutations on hemagglutinin (HA) cause immunologically distinct circulating strains, which lead to the antigenic drift (named as antigenic variants). The """"antigenic variants"""" often requires a new vaccine to be formulated before each annual epidemic. Mapping the genetic evolution to the antigenic drift of influenza viruses is an emergent issue to public health and vaccine development We developed a method for identifying antigenic critical amino acid positions, rules, and co-mutated positions for antigenic variants. The information gain (IG) and the entropy are used to measure the score of an amino acid position on hemagglutinin (HA) for discriminating between antigenic variants and similar viruses. A position with high IG and entropy implied that this position is highly correlated to an antigenic drift. Nineteen positions with high IG and high genetic diversity are identified as antigenic critical positions on the HA proteins. Most of these antigenic critical positions are located on five epitopes or on the surface based on the HA structure. Based on IG values and entropies of these 19 positions on the HA, the decision tree was applied to create a rule-based model and to identify rules for predicting antigenic variants of a given two HA sequences which are often a vaccine strain and a circulating strain. The predicting accuracies of this model on two sets, which consist of a training set (181 hemagglutination inhibition (HI) assays) and an independent test set (31,878 HI assays), are 91.2% and 96.2% respectively. Our method is able to identify critical positions, rules, and co-mutated positions on HA for predicting the antigenic variants. The information gains and the entropies of HA positions provide insight to the antigenic drift and co-evolution positions for influenza seasons. We believe that our method is robust and is potential useful for studying influenza virus evolution and vaccine development."""	amino acids;aves;concept drift;decision tree;emergence;epitopes;evolution, molecular;hemagglutination inhibition tests;hemagglutinin;hereditary diseases;influenza a virus, h3n2 subtype;influenza virus vaccine;kullback–leibler divergence;logic programming;mutation;name;orthomyxoviridae;rule (guideline);test set;vaccine development	Jhang-Wei Huang;Chwan-Chuen King;Jinn-Moon Yang	2009	BMC Bioinformatics	10.1186/1471-2105-10-S1-S41	original antigenic sin;biology;h5n1 genetic structure;amino acid;dna microarray;public health;hemagglutination assay;computer science;bioinformatics;virology;decision tree;immunology;kullback–leibler divergence;microbiology;evolution of influenza;genetics;genetic diversity;antigenic variation;seasonality;antigenic shift;antigenic drift	Comp.	3.6922646504192014	-61.87367416776358	2166
02594ab5ea8567e3464625eedbe7e7fee9791b2e	pre and post-hoc diagnosis and interpretation of malignancy from breast dce-mri.		We propose a new method for breast cancer screening from DCE-MRI based on a post-hoc approach that is trained using weakly annotated data (i.e., labels are available only at the image level without any lesion delineation). Our proposed post-hoc method automatically diagnosis the whole volume and, for positive cases, it localizes the malignant lesions that led to such diagnosis. Conversely, traditional approaches follow a pre-hoc approach that initially localises suspicious areas that are subsequently classified to establish the breast malignancy – this approach is trained using strongly annotated data (i.e., it needs a delineation and classification of all lesions in an image). Another goal of this paper is to establish the advantages and disadvantages of both approaches when applied to breast screening from DCE-MRI. Relying on experiments on a breast DCE-MRI dataset that contains scans of 117 patients, our results show that the post-hoc method is more accurate for diagnosing the whole volume per patient, achieving an AUC of 0.91, while the pre-hoc method achieves an AUC of 0.81. However, the performance for localising the malignant lesions remains challenging for the post-hoc method due to the weakly labelled dataset employed during training. ∗Corresponding author Email address: gabriel.maicas@adelaide.edu.au (Gabriel Maicas ) This work was partially supported by the Australian Research Council project (DP180103232). IR acknowledges the Australian Research Council: ARC Centre for Robotic Vision (CE140100016) and Laureate Fellowship (FL130100102) Preprint submitted to Journal of Medical Image Analysis September 26, 2018 ar X iv :1 80 9. 09 40 4v 1 [ cs .C V ] 2 5 Se p 20 18	dce/rpc;email;experiment;hoc (programming language);image analysis;medical image computing;reinforcement learning;the australian	Gabriel Maicas;Andrew P. Bradley;Jacinto C. Nascimento;Ian D. Reid;Gustavo Carneiro	2018	CoRR		pattern recognition;lesion;breast cancer screening;computer science;artificial intelligence;malignancy	ML	32.01219376973974	-76.29161677565179	2167
f30e2e5fe447f00930329302a8711efaf5e06596	an aco algorithm for the 3d bin packing problem in the steel industry		This paper proposes a new Ant Colony Optimization (ACO) algorithm for the three-dimensional (3D) bin packing problem with guillotine cut constraint, which consists of packing a set of boxes into a 3D set of bins of variable dimensions. The algorithm is applied to a real-world problem in the steel industry. The retail steel cut consists on how to cut blocks of steel in order to satisfy the clients orders. The goal is to minimize the amount of scrap metal and consequently reduce the stock of steel blocks. The proposed ACO algorithm searches for the best orders of the boxes and it is guided by a heuristic that determines the position and orientation for the boxes. It was possible to reduce the amount of scrap metal by 90% and to reduce the usage of raw material by 25%.	algorithm;bin packing problem;set packing	Miguel Espinheira Silveira;Susana M. Vieira;João Miguel da Costa Sousa	2013		10.1007/978-3-642-38577-3_55	mathematical optimization;bin packing problem	Theory	19.100575573284	2.61394683417518	2173
8b13808881016020db71d5fdb63b5fc24d3e07a6	high resolution full spherical videos	minimisation;viewer implementation;environment maps;geometric envelopes;high resolution;image resolution;motion pictures;video signal processing;computer graphics;geometry;virtual reality;multi head cameras;layout;videos motion pictures cameras computer graphics layout rendering computer graphics virtual environment streaming media computer science image generation;image resolution video signal processing authoring systems virtual reality synchronisation video cameras minimisation image registration geometry;authoring systems;video authoring;viewer implementation high resolution full spherical videos video authoring video viewing high resolution immersive video multi head cameras camera alignment nodal point seamlessly synchronized video stream stitching image registration technique geometric envelopes objective function minimization compounded image synthesis algorithm environment maps discrepancy;objective function;synchronisation;image synthesis;video coding;high resolution full spherical videos;nodal point;compounded image synthesis algorithm;image generation;discrepancy;streaming media;camera alignment;video cameras;image registration;objective function minimization;seamlessly synchronized video stream stitching;image registration technique;high resolution immersive video;computer science;virtual environment;rendering computer graphics;video viewing;cameras;videos	We describe algorithms for authoring and viewing high resolution immersive videos. Given a set of cameras designed to be aligned more or less at the same nodal point, we first present a process for stitching seamlessly synchronized streams of videos into a single immersive video corresponding to the video of the abstract multi-head camera. We describe a general registration technique onto geometric envelopes based on minimizing a novel appropriate objective function, and detail our compounded image synthesis algorithm of multi-head cameras. Several new environment maps with low discrepancy are presented. Finally, we give details on the viewer implementation. Experimental results on both immersive real and synthetic videos are shown.	360-degree video;algorithm;camera resectioning;codec;discrepancy function;image resolution;image stitching;loss function;map;optimization problem;pixel;rendering (computer graphics);synthetic intelligence;view synthesis	Frank Nielsen	2002		10.1109/ITCC.2002.1000397	computer vision;computer science;multimedia;computer graphics (images)	Vision	58.43692724814107	-53.47910893710516	2174
92a73c01db0514c0aed4c90937f1a5d592f3c44b	investigation of two novel approaches for detection of sulfate ion and methane dissolved in sediment pore water using raman spectroscopy	methane;raman spectroscopy;ccl4 extraction;ccl 4 extraction;lcof;sulfate ion	The levels of dissolved sulfate and methane are crucial indicators in the geochemical analysis of pore water. Compositional analysis of pore water samples obtained from sea trials was conducted using Raman spectroscopy. It was found that the concentration of SO42- in pore water samples decreases as the depth increases, while the expected Raman signal of methane has not been observed. A possible reason for this is that the methane escaped after sampling and the remaining concentration of methane is too low to be detected. To find more effective ways to analyze the composition of pore water, two novel approaches are proposed. One is based on Liquid Core Optical Fiber (LCOF) for detection of SO42-. The other one is an enrichment process for the detection of CH4. With the aid of LCOF, the Raman signal of SO42- is found to be enhanced over 10 times compared to that obtained by a conventional Raman setup. The enrichment process is also found to be effective in the investigation to the prepared sample of methane dissolved in water. By CCl4 extraction, methane at a concentration below 1.14 mmol/L has been detected by conventional Raman spectroscopy. All the obtained results suggest that the approach proposed in this paper has great potential to be developed as a sensor for SO42- and CH4 detection in pore water.	carbon tetrachloride;gene ontology term enrichment;ions;methane;optical fiber;preparation;raman scattering;sampling (signal processing);sediment;spectroscopy, near-infrared;spectrum analysis, raman;sulfate ion;x-ray emission spectroscopy	Zengfeng Du;Jing Chen;Wangquan Ye;Jinjia Guo;Xin Zhang;Ronger Zheng	2015		10.3390/s150612377	raman spectroscopy;chemistry;methane;analytical chemistry;environmental chemistry;organic chemistry;inorganic chemistry	HCI	96.22541709744301	-17.64615217566407	2177
862d16b586da66829fcc0119332e15548e47947c	calibration method for multiple 2d lidars system	laser radar calibration three dimensional displays optimization feature extraction simultaneous localization and mapping;optical radar cad calibration intelligent transportation systems;cad model calibration method multiple 2d lidar system reference calibration multilidar calibration intelligent vehicle platform poss v underground parking lot sensor geometry	Many robotic and mobile mapping systems have been developed using multiple 2D LIDARs (briefly multi-LIDAR system) to sense environment. In such systems, extrinsic calibration of all LIDARs is essential for making collaborative use of the data from different sensors. This research aims at developing a calibration method for multi-LIDAR systems at the general scene, such as an outdoor place or an underground parking-lot, without modification to environment by putting calibration targets. In this paper, the calibration method is proposed by aligning the 3D data of different LIDARs. They are concerned at two-levels: 1) reference calibration, i.e. finding the transformation from a reference LIDAR to the platform frame; 2) multi-LIDAR calibration, i.e. finding the LIDARs' relative geometries by referring to the reference one. The method is examined in calibrating the multiple 2D LIDARs on an intelligent vehicle platform POSS-V, where the data collected through a driving in an underground parking-lot are registered to find sensors' geometry. Calibration accuracy is examined by comparing with a CAD model of the scene, which was measured by using a total station.	3d modeling;computer-aided design;mobile mapping;multi-storey car park;robot;sensor	Mengwen He;Huijing Zhao;Jinshi Cui;Hongbin Zha	2014	2014 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2014.6907296	computer vision;simulation;remote sensing	Robotics	55.813634637456104	-40.17829858697157	2182
75bd878b2e07b39cc6914f5c3b817c4f70a32848	interpolation and averaging of multi-compartment model images		Multi-compartment diffusion models (MCM) are increasingly used to characterize the brain white matter microstructure from diffusion MRI. We address the problem of interpolation and averaging of MCM images as a simplification problem based on spectral clustering. As a core part of the framework, we propose novel solutions for the averaging of MCM compartments. Evaluation is performed both on synthetic and clinical data, demonstrating better performance for the “covariance analytic” averaging method. We then present an MCM template of normal controls constructed using the proposed interpolation.	cluster analysis;interpolation;level of detail;multi-chip module;multi-compartment model;spectral clustering;synthetic data;synthetic intelligence	Renaud Hédouin;Olivier Commowick;Aymeric Stamm;Christian Barillot	2015		10.1007/978-3-319-24571-3_43	econometrics;mathematical optimization;mathematics;statistics	Vision	48.4724894817399	-79.92971200536145	2186
a40250adc45cb7b9f1f2e58f143683a60dd03812	distributed multiresolution discrete fourier transform and its application to watermarking	digital watermarking;singular value decomposition;distributed multiresolution discrete fourier transform;discrete fourier transform;multiresolution analysis	The Fourier transform is undoubtedly one of the most valuable and frequently used tools in signal processing and analysis but it has some limitations. In this paper, we rectify these limitations by proposing a newer version of Fourier transform, namely, Distributed Multiresolution Discrete Fourier Transform (D-MR-DFT) and its application in digital watermarking. The core idea of the proposed watermarking scheme is to decompose an image into four frequency sub-bands using D-MR-DFT and then singular values of every sub-band are modified with the singular values of the watermark. The experimental results show better visual imperceptibility and resiliency of the proposed scheme against intentional or unintentional variety of attacks.	digital watermarking;discrete fourier transform;discrete cosine transform;discrete wavelet transform;nl-complete;numerical aperture;scheme;signal processing;singular value decomposition	Gaurav Bhatnagar;Balasubramanian Raman	2010	IJWMIP	10.1142/S021969131000347X	multiresolution analysis;discrete hartley transform;constant q transform;mathematical optimization;discrete-time fourier transform;mathematical analysis;discrete mathematics;hartley transform;s transform;harmonic wavelet transform;second-generation wavelet transform;short-time fourier transform;continuous wavelet transform;digital watermarking;fractional fourier transform;discrete sine transform;discrete fourier transform;mathematics;discrete fourier transform;fourier analysis;singular value decomposition;algorithm	Vision	41.32993686857279	-8.994530549964098	2187
e23da6e1b461baf9845e0e3c7aebeb5df4a85fe6	designing rbf neural networks with weighted mean subtractive clustering algorithms	weighted mean subtractive clustering;cluster algorithm;pattern clustering;classification algorithm;neural networks;cluster centers rbf network subtractive clustering;subtractive clustering;ionosphere;radial basis function networks;accuracy;radial basis function;clustering algorithms algorithm design and analysis neural networks ionosphere classification algorithms signal processing algorithms accuracy;rbf neural network;signal processing;classification algorithms;pattern classification;cluster center;clustering algorithms;network architecture;radial basis function networks pattern classification pattern clustering;signal processing algorithms;classification accuracy;classification accuracy rbf neural network weighted mean subtractive clustering cluster center radial basis function weighted mean method network architecture;rbf network;algorithm design;algorithm design and analysis;cluster centers;neural network;weighted mean method	In this paper, weighted mean subtractive clustering algorithms are proposed to find cluster centers of the dataset. Then the found cluster centers act as the centers of radial basis functions. In weighted mean subtractive clustering algorithms, subtractive clustering is used to find center prototypes and then weighted mean methods are used to create new centers. Three weighted mean methods are tried to create more effective centers. Comparative experiments were executed between subtractive clustering and three weighted mean subtractive clustering algorithms on five benchmark datasets. Next, the performance of RBF neural networks set with the proposed algorithms was studied. The experimental results suggest that all three weighted mean subtractive clustering algorithms can find more accurate centers and can be successfully applied to design RBF neural networks. The RBF neural networks determined by weighted mean subtractive clustering algorithms have rather simpler network architecture but with slightly lower classification accuracy than ones determined by subtractive clustering algorithm.	algorithm;artificial neural network;benchmark (computing);cluster analysis;determining the number of clusters in a data set;experiment;mean squared error;network architecture;radial (radio);radial basis function	Junying Chen;Zhe Li	2011	2011 Seventh International Conference on Natural Computation	10.1109/ICNC.2011.6022115	correlation clustering;k-medians clustering;computer science;machine learning;pattern recognition;data mining;cluster analysis	ML	4.196897313703146	-39.04478430995924	2190
7129c7fca4867c556094b4eed06e602d16bccc1d	m-type smoothing spline anova for correlated data	metodo cuadrado menor;sistema lineal;metodo directo;classification automatique statistiques;metodo correlacion;metodo estadistico;methode moindre carre;analyse multivariable;donnee longitudinale;analisis numerico;log likelihood;empirical study;estimacion robusta;regression function;covariance analysis;methode empirique;theorie approximation;multivariate analysis;fonction regression;least squares method;41a15;approximation numerique;65f05;funcion regresion;data smoothing;correlated data longitudinal data nonparametric regression resistant smoothing parameter robust;log vraisemblance;correlation method;estimation robuste;62f35;metodo empirico;62e17;estimation non parametrique;virus;empirical method;variance analysis;62g08;metodo penalidad;outlier;etude methode;human immunodeficiency virus;statistical method;regression non parametrique;matrix inversion;estudio metodo;distribucion estadistica;virus immunodeficience humaine;smoothing parameter;inversion matriz;estimation algorithm;statistical regression;linear system;62jxx;robust estimation;aproximacion numerica;analyse numerique;resistant smoothing parameter;algorithme;discriminant analysis;analyse discriminante;approximation theory;robust;observacion aberrante;algorithm;non parametric estimation;non parametric regression;analisis discriminante;aproximacion esplin;penalty method;smoothing methods;methode penalite;iteraccion;numerical analysis;analyse covariance;distribution statistique;65d07;62h30;methode statistique;spline approximation;approximation spline;analisis variancia;regresion estadistica;methode lissage;62j10;analyse correlation;algebra lineal numerica;algebre lineaire numerique;robust method;estimacion parametro;lissage donnees;nonparametric regression;iteration;simian human immunodeficiency virus;inversion matrice;observation aberrante;analisis multivariable;numerical linear algebra;method study;numerical approximation;estimacion no parametrica;parameter estimation;estimation parametre;analisis covariancia	This paper concerns outlier robust non-parametric regression with smoothing splines for data that are possibly correlated. We define a robust smoother as the minimizer of a penalized robustified log likelihood. Our estimation algorithm uses iteratively reweighted least squares to estimate the regression function. We develop two types of robust methods for joint estimation of the smoothing parameters and the correlation parameters: indirect methods and direct methods, terms borrowed from the related generalized smoothing spline literature. The indirect methods choose those parameters by conveniently approximating the distribution of the working data at each iteration as Gaussian. The direct methods estimate those parameters to minimize an estimate of the loss between the truth and the final estimated regression. Indirect methods are computationally more efficient, but our empirical studies suggest that direct methods result in more accurate estimates. Finally, the methods are applied to a data set from a macaque Simian-Human Immunodeficiency Virus (SHIV) challenge study.	smoothing spline	Anna Liu;Li Qin;John Staudenmayer	2010	J. Multivariate Analysis	10.1016/j.jmva.2010.06.001	econometrics;calculus;mathematics;linear discriminant analysis;empirical research;nonparametric regression;statistics;smoothing	HCI	33.05969846983423	-23.540893726720764	2193
bfbc00da5f23bdfefb291584a9eda9ccc15a0cba	on estimating local shape using contact sensing		Ž . In this paper we investigate an exploratory procedure EP for determining the local shape or curvature of objects with curved surfaces, using contact sensing by a Ž . dexterous robotic agent. The EP is based on rolling a probe a finger on the surface of Ž . an unknown object. Using the known geometry of the probe tip fingertip and the sensed displacement of contact point, we use kinematic equations for two rigid bodies in contact to estimate the curvature of the unknown object at a point. We demonstrate this approach with simulations and analyze its sensitivity to noise caused by sensing and measurement errors. This local information can then be used in the global object shape reconstruction algorithms. 2000 John Wiley & Sons, Inc.	algorithm;displacement mapping;expectation propagation;john d. wiley;robot;simulation	M. Charlebois;Kamal K. Gupta;Shahram Payandeh	2000	J. Field Robotics	10.1002/1097-4563(200012)17:12%3C643::AID-ROB1%3E3.0.CO;2-8	control engineering;mathematics	Robotics	62.474931601087	-34.34891568839923	2196
18fd83544c1a4036bc3e6f735ca74cc7393d08e2	mechanics, planning, and control for tapping	impact;robot manipulator;impulsive manipulation;robotic manipulation;sensitivity analysis;motion planning;planning and control;feedback control	We present analysis and experimental demonstration of manipulation by tapping. Our problem domain is positioning planar parts on a support surface by a sequence of taps; each tap imparts some initial velocities to the object which then slides until it comes to rest due to friction. We formulate the mechanics of tapping for circular axisymmetric objects, show how to plan a single tap and a sequence of taps to reach a goal configuration, and present feedback control methods to robustly accomplish positioning tasks. With these methods, we have experimentally demonstrated positioning tasks using tapping, including high-precision positioning tasks in which the object is positioned more precisely than the tapping actuator. We show stability and sensitivity analysis in support of these results.	control theory;discrepancy function;displacement mapping;experiment;feedback;ibm notes;jacobian matrix and determinant;jason;numerical analysis;powell's method;problem domain;robotics	Wesley H. Huang;Matthew T. Mason	2000	I. J. Robotics Res.	10.1177/02783640022067841	control engineering;simulation;computer science;engineering;artificial intelligence;control theory;feedback;motion planning;impact;sensitivity analysis	Robotics	64.0980848190748	-21.245288328285184	2200
4316d33e53135f9aa5957b0d0d4402bced0a1c03	tone dependent color error diffusion	quantization;optimisation;color rendition;tone dependent color error diffusion;color;color rendition tone dependent color error diffusion grayscale error diffusion halftoning worms tone dependent error diffusion quantization errors error filter weights error filter thresholds human visual system model error filters;worms;error filters;human visual system model;gray scale;satellite broadcasting;quantisation signal;error filter weights;nonlinear distortion;grayscale error diffusion halftoning;human visual system;image colour analysis;signal processing;tone dependent error diffusion;pixel;error filter thresholds;quantization errors;gray scale quantization color frequency satellite broadcasting humans pixel nonlinear distortion signal processing laboratories;visual perception;humans;visual perception image colour analysis quantisation signal optimisation filtering theory;optimum design;frequency;filtering theory;color image;quantization error	Conventional grayscale error diffusion halftoning produces worms and other objectionable artifacts. Tone dependent error diffusion (Li, P. and Allebach, J.P. Proc. SPIE Color Imaging, vol.4663, p.310-21, 2002) reduces these artifacts by controlling the diffusion of quantization errors based on the input graylevel. Li and Allebach designed error filter weights and thresholds for each (input) graylevel with optimization based on a human visual system (HVS) model. We extend tone dependent error diffusion to color. In color error diffusion, what color to render becomes a major concern in addition to finding optimal dot patterns. We present a visually optimum design approach for input level (tone) dependent error filters (for each color plane). The resulting halftones reduce traditional error diffusion artifacts and achieve greater accuracy in color rendition.	error diffusion;grayscale;holomatix rendition;human visual system model;mathematical optimization;optimal design;quantization (signal processing)	Vishal Monga;Brian L. Evans	2004	2004 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2004.1326491	computer vision;speech recognition;quantization;computer science;signal processing;human visual system model;error diffusion;computer graphics (images)	Visualization	58.58568847326598	-63.79385628907444	2201
38e404299660f10ef70ad088156c1928b44a435f	dolop-database of bacterial lipoproteins	lipoprotein;information exchange;sequence analysis;bacterial genome;knowledge base	UNLABELLED Bacterial lipoproteins and lipid modification are gaining importance owing to their essential nature, roles in pathogenesis and interesting commercial applications. We have created an exclusive knowledge base for bacterial lipoproteins by processing information from 510 entries to provide a list of 199 distinct lipoproteins with relevant links to molecular details. Features include functional classification, predictive algorithm for query sequences, primary sequence analysis and lists of predicted lipoproteins from 43 completed bacterial genomes along with interactive information exchange facility.   AVAILABILITY The website called Database Of bacterial LipOProteins (DOLOP) is available at http://www.mrc-lmb.cam.ac.uk/genomes/dolop along with additional information on the biosynthetic pathway, supplementary material and other related figures.	electronic supplementary materials;gene regulatory network;genome, bacterial;information exchange;knowledge base;lipid metabolism disorders;lipoproteins;question (inquiry);sequence analysis;algorithm	M. Madan Babu;K. Sankaran	2002	Bioinformatics	10.1093/bioinformatics/18.4.641	biology;knowledge base;information exchange;computer science;bioinformatics;sequence analysis;world wide web;genetics;bacterial genome size	Comp.	-0.9077918371069077	-61.018563523676605	2203
6fac0e63869bec7ed4dbcde3249098091a0d39bb	3d face recognition algorithm of alignment and fitting	data processing;facial recognition systems	For 3D face recognition area, Design a algorithm follows a morphable model approach. To align the scan with a model, ICP and spin-images are used also referred to as registration. Deformation is done by a nonrigid ICP algorithm to fit the model with the scan. From the fitted model a geometry image and a normal image is generated. The developed algorithm is tested by several measurements. From the results of these measurements, it could be concluded that the algorithm is robust and reliable.	algorithm;align (company);facial recognition system;robustness (computer science);three-dimensional face recognition	Yifei Wu;Yao Cheng;Nan Yang	2015		10.1117/12.2197158	computer vision;computer science;machine learning;pattern recognition;three-dimensional face recognition;3d single-object recognition;eigenface	Vision	48.29839400267549	-50.12886861576149	2204
fa14f448d150fd928055c77dfbf55836ccfbf665	least-squares and minimum chi-square estimation in a discrete weibull model	count data;least squares method;discrete weibull distribution;empirical cumulative distribution function;minimum chi square;stochastic reliability	AbstractIn this work, we investigate the properties of least-squares and minimum Chi-square methods for the point estimation of the two parameters characterizing a discrete Weibull distribution. The first method, inflected into three variants, is based on the empirical cumulative distribution function and provides a closed analytical expression for each estimate. The second method is based on the minimization of the well-known chi square statistic, which provides a numerical solution. A Monte Carlo simulation study empirically assesses the performance of the methods; two applications on real data show how the inferential techniques practically work.	least squares;mathematical model	Alessandro Barbiero	2017	Communications in Statistics - Simulation and Computation	10.1080/03610918.2016.1263733	empirical distribution function;weibull distribution;econometrics;mathematical optimization;chi distribution;count data;mathematics;least squares;statistics	ML	29.18668310705561	-21.80579675951326	2213
e10193a0dc565cb4a37f53a14699b39411b67bd4	visual servoing feedback based robust regulation of nonholonomic wheeled mobile robots	nonholonomic kinematic systems;visual servoing cameras feedback mobile robots pose estimation position control robot kinematics robust control;visual servoing regulation nonholonomic mobile robot;mobile robot;mobile robots cameras robot vision systems visualization kinematics;stabilization;mobile robots;robust control;indexing terms;kinematics;stabilization visual servoing feedback based robust regulation nonholonomic wheeled mobile robots visual servoing regulation nonholonomic mobile robots monocular camera nonholonomic kinematic systems visual feedback two phase technique robust controller mobile robot image pose orientation regulation depth information precise visual parameters image frame inertial frame;monocular camera;wheeled mobile robot;visual servoing feedback based robust regulation;visualization;feedback;robot vision;nonholonomic mobile robot;position control;nonholonomic mobile robots;robust controller;orientation regulation;regulation;precise visual parameters;depth information;visual feedback;two phase technique;image frame;visual servoing;inertial frame;mobile robot image pose;robot vision systems;cameras;visual servoing regulation;nonholonomic wheeled mobile robots;robot kinematics;pose estimation	This paper investigated the visual servoing regulation of nonholonomic mobile robots with monocular camera. Nonholonomic kinematic systems with visual feedback are uncertain and more involved in comparison with common kinematic systems. Two-phase technique was used to present a robust controller that enabled the mobile robot image pose and the orientation regulation despite the lack of depth information and the lack of precise visual parameters. The most interesting feature of this paper is that the problem was discussed in the image frame and the inertial frame, which made the problem easy and useful. The stabilization of the system by using the proposed method was rigorously proved. The simulation was given to show the effectiveness of the presented controllers.	mobile robot;robust control;simulation;two-phase locking;visual servoing	Chaoli Wang	2011	2011 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2011.5979924	control engineering;mobile robot;computer vision;computer science;artificial intelligence;control theory;visual servoing	Robotics	60.88118681990252	-32.21390229448652	2215
592ee4939db7ec12a0e317e7b5a9354d1831c3b5	pseudo-stereo vision system: a detailed study	images separation;tecnologia industrial tecnologia mecanica;tecnologia electronica telecomunicaciones;complex image;robot manipulator;stereo vision;single camera;camera calibration;tecnologias;mirrors alignment;grupo a;real time application;pseudo stereovision	In this paper, a new stereovision system based on mirrors is presented. It is composed of three mirrors, a beam-splitter and a camera. It is called Pseudo-Stereo Vision System (PSVS) and can be used in real time applications. Two parallel virtual cameras are created with the geometric properties and parameters of the real camera. PSVS captures, in one shot, a complex image, created by the superposition of the left and right views of the system. The apparatus has no moving parts, low cost and double resolution compared with other monocular systems based on mirrors. It can be constructed in any dimension covering any type of camera, length of baseline and accuracy of depth measurements. The design and construction details of the system as well as the appearing refraction phenomena to the apparatus are analytically presented. Analytical expressions are derived for the calculation of mirrors dimensions, minimum common view distance and minimum length of baseline. Mirrors alignment method is also described. Equations providing the Cartesian coordinates of a point in space, taking into consideration refraction phenomena to beam-splitter and camera calibration parameters, are proved. Two new methods for the separation of complex images to pairs of left and right images using gray scale or color cameras are explained and the first real experimental results are illustrated. Finally, experimental results, where the PSVS is mounted on the end effector of a PUMA 761 robotic manipulator are presented.	baseline (configuration management);camera resectioning;demultiplexer (media file);grayscale;lumpers and splitters;mind;robot end effector;servo;stereopsis;visual servoing;webcam	Theodore P. Pachidis;John N. Lygouras	2005	Journal of Intelligent and Robotic Systems	10.1007/s10846-005-0932-y	computer vision;camera auto-calibration;camera resectioning;simulation;computer science;stereopsis	Robotics	56.72332030363049	-49.88683530795454	2220
5a51087a7b4dd57ec10cb8b351a4de7da5292966	using the karhunen-loe've transformation in the back-propagation training algorithm	eigenvalues and eigenfunctions;optimized production technology;learning rate;convergence;transformacion matematica;neural networks;matriz correlacion;neural nets;relacion convergencia;mathematical transformation;simulacion numerica;synthetic noisy image;taux convergence;convergence rate;segmentation;matrix algebra;eigenvector;synthetic noisy image segmentation pattern recognition karhunen loe ve transformation back propagation training algorithm major eigenvectors correlation matrix;karhunen loe ve transformation;physics computing;computer networks;eigenvalue;vector propio;learning systems;major eigenvectors;optical propagation;backpropagation algorithm;simulation numerique;valor propio;back propagation training algorithm;transformation mathematique;transforms;pattern recognition;back propagation algorithm;associative memory;algorithme retropropagation;matrice correlation;valeur propre;circuits;reseau neuronal;neural networks convergence dispersion optical propagation optical feedback optimized production technology computer networks physics computing associative memory circuits;correlation matrix;dispersion;back propagation;optical feedback;red neuronal;vecteur propre;transforms eigenvalues and eigenfunctions learning systems matrix algebra neural nets pattern recognition;training algorithm;eigenvectors;neural network;numerical simulation	A novel training approach based on the back-propagation algorithm is introduced. In the proposed approach, initially, a set of training vectors is obtained by applying the Karhunen-Loe've transform on the training patterns. The training is first started in the direction of the major eigenvectors of the correlation matrix of the training patterns and then continues by gradually including the remaining components, in their order of significance. With this approach, the number of computations is significantly reduced and the learning rate is improved. The performance of this method is compared with the standard back-propagation algorithm in segmenting a synthetic noisy image.		Heidar A. Malki;Ali M. Reza	1991	IEEE transactions on neural networks	10.1109/72.80306	eigenvalues and eigenvectors;computer science;artificial intelligence;theoretical computer science;machine learning;mathematics;artificial neural network;statistics	Vision	16.514375705284387	-28.239376799438606	2223
eaf817b684018a0357ab0541b13356fa72cb4db6	benchmarking a hybrid de-rhc algorithm on real world problems	evolutionary computation;heuristic algorithms;heuristic algorithms optimization benchmark testing evolutionary computation algorithm design and analysis search problems relays;optimization;search problems;relays;algorithm design;algorithm design and analysis;benchmark testing;heuristic algorithm;evolutionary computing;differential evolution hybrid de rhc algorithm real world problem optimization metaheuristic algorithm evolutionary algorithm continuous function random hill climber	Continuous optimization is one of the most active research lines in evolutionary and metaheuristic algorithms. Through CEC 2005 to CEC 2010 competitions, many different algorithms have been proposed to solve continuous problems. The advances on this type of problems are of capital importance as many real-world problems from very different domains (biology, engineering, data mining, etc.) can be formulated as the optimization of a continuous function. For this reason, we have proposed a hybrid DE-RHC algorithm that combines the search strength of Differential Evolution with the explorative ability of a Random Hill Climber, which can help the Differential Evolution algorithm to reach new promising areas in difficult fitness landscapes, such as those than can be found on real-world problems. To evaluate this approach, the benchmark problems proposed in the “Testing Evolutionary Algorithms on Real-world Numerical Optimization Problems” CEC 2011 special session have been considered.	benchmark (computing);climber (beam);continuous optimization;data mining;differential evolution;evolutionary algorithm;mathematical optimization;metaheuristic;recueil des historiens des croisades	Antonio LaTorre;Santiago Muelas;José María Peña Sánchez	2011	2011 IEEE Congress of Evolutionary Computation (CEC)	10.1109/CEC.2011.5949730	algorithm design;mathematical optimization;meta-optimization;genetic algorithm;criss-cross algorithm;interactive evolutionary computation;cultural algorithm;computer science;machine learning;evolutionary algorithm;min-conflicts algorithm;imperialist competitive algorithm;algorithm;metaheuristic;evolutionary computation	ML	25.09334980358623	-2.4527007119914153	2230
59ffca26c9d1f881e4e37e7c5af626d72d858dc4	relationship of sars-cov to other pathogenic rna viruses explored by tetranucleotide usage profiling	rabbits;animals;multivariate analysis;phylogeny;distance measure;genome viral;nucleotides;severe acute respiratory syndrome;sars virus;rna viruses;power method;computational biology bioinformatics;single stranded;microsatellite repeats;factor analysis;hemorrhagic disease virus rabbit;cattle;open reading frame;coronavirus bovine;algorithms;humans;neighbor joining;rna viral;combinatorial libraries;correlation coefficient;computer appl in life sciences;article;gene expression regulation viral;dinucleotide repeats;gene expression profiling;genome sequence;microarrays;coronavirus 229e human;bioinformatics;open reading frames	The exact origin of the cause of the Severe Acute Respiratory Syndrome (SARS) is still an open question. The genomic sequence relationship of SARS-CoV with 30 different single-stranded RNA (ss RNA) viruses of various families was studied using two non-standard approaches. Both approaches began with the vectorial profiling of the tetra-nucleotide usage pattern V for each virus. In approach one, a distance measure of a vector V, based on correlation coefficient was devised to construct a relationship tree by the neighbor-joining algorithm. In approach two, a multivariate factor analysis was performed to derive the embedded tetra-nucleotide usage patterns. These patterns were subsequently used to classify the selected viruses. Both approaches yielded relationship outcomes that are consistent with the known virus classification. They also indicated that the genome of RNA viruses from the same family conform to a specific pattern of word usage. Based on the correlation of the overall tetra-nucleotide usage patterns, the Transmissible Gastroenteritis Virus (TGV) and the Feline CoronaVirus (FCoV) are closest to SARS-CoV. Surprisingly also, the RNA viruses that do not go through a DNA stage displayed a remarkable discrimination against the CpG and UpA di-nucleotide (z = -77.31, -52.48 respectively) and selection for UpG and CpA (z = 65.79,49.99 respectively). Potential factors influencing these biases are discussed. The study of genomic word usage is a powerful method to classify RNA viruses. The congruence of the relationship outcomes with the known classification indicates that there exist phylogenetic signals in the tetra-nucleotide usage patterns, that is most prominent in the replicase open reading frames.	city of heroes;coefficient;congruence of squares;coronavirus, feline;cost per action;cyproterone acetate;embedded system;embedding;existential quantification;factor analysis;frame (physical object);neighbor joining;nucleotides;open reading frames;open reading frame;phylogenetics;rna viruses;reading frames (nucleotide sequence);severe acute respiratory syndrome;single-stranded rna;thoracic gas volume;transmissible gastroenteritis virus;algorithm;replicase	Yee Leng Yap;Xue Wu Zhang;Antoine Danchin	2003	BMC Bioinformatics	10.1186/1471-2105-4-43	open reading frame;biology;bioinformatics;virology;genetics;phylogenetics	Comp.	3.800117572226798	-60.8435948249062	2239
cab09b36c3ff00f6ac924346df7faf0c3c86a270	palmprint recognition using 3-d information	hand;analisis imagen;charge coupled image sensors;palmprint matching;surface type;processus gauss;image recognition;haute performance;gaussian curvature;image processing;illumination;image databases;gaussian processes;debit information;biometrics access control;dimension reduction;biometrie;authentication;extraction forme;luminance;biometrics;database;biometria;tipo dato;base dato;structured light;contracts;courbure;image recognition biometrics access control feature extraction gaussian processes;indice aptitud;palmprint classification;data type;feature matching;high precision;data mining;classification;indice informacion;charge coupled devices;3d palmprint database;authentification;reduction dimension;illumination variations;counterfeiting;indice aptitude;3d structural information;autenticacion;extraccion forma;biometrics image recognition charge coupled devices contracts feature extraction counterfeiting lighting charge coupled image sensors data mining image databases;capability index;feature extraction;precision elevee;score level fusion strategy;imaging;gaussian curvature image;surface curvature;base de donnees;precision elevada;pattern recognition;mano;alto rendimiento;information rate;palmprint recognition;reduccion dimension;curvatura;formation image;curvature;image analysis;lighting;formacion imagen;mean curvature image;reconnaissance forme;gaussian process;extraction caracteristique;palm surface;main;3 d palmprint recognition;illumination variations palmprint recognition biometric characteristics palm surface feature extraction feature matching 2d palmprint images 3d depth information 3d structural information structured light imaging mean curvature image gaussian curvature image surface type score level fusion strategy palmprint matching palmprint classification 3d palmprint database anticounterfeiting capability;reconocimiento patron;biometric characteristics;type donnee;mean curvature;proceso gauss;3d depth information;structured light imaging	Palmprint has proved to be one of the most unique and stable biometric characteristics. Almost all the current palmprint recognition techniques capture the 2-D image of the palm surface and use it for feature extraction and matching. Although 2-D palmprint recognition can achieve high accuracy, the 2-D palmprint images can be counterfeited easily and much 3-D depth information is lost in the imaging process. This paper explores a 3-D palmprint recognition approach by exploiting the 3-D structural information of the palm surface. The structured light imaging is used to acquire the 3-D palmprint data, from which several types of unique features, including mean curvature image, Gaussian curvature image, and surface type, are extracted. A fast feature matching and score-level fusion strategy are proposed for palmprint matching and classification. With the established 3-D palmprint database, a series of verification and identification experiments is conducted to evaluate the proposed method. The results demonstrate that 3-D palmprint technique has high recognition performance. Although its recognition rate is a little lower than 2-D palmprint recognition, 3-D palmprint recognition has higher anticounterfeiting capability and is more robust to illumination variations and serious scrabbling in the palm surface. Meanwhile, by fusing the 2-D and 3-D palmprint information, much higher recognition rate can be achieved.	algorithm;align (company);biometrics;data acquisition;experiment;feature extraction;fingerprint;google code-in;matching (graph theory);region of interest;structured light	David Zhang;Guangming Lu;Wei Li;Lei Zhang;Nan Luo	2009	IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)	10.1109/TSMCC.2009.2020790	computer vision;image analysis;speech recognition;image processing;computer science;pattern recognition;lighting;gaussian process;authentication	Vision	44.448010000734605	-60.75262957870349	2242
637939c0e0dd864c1ea3134107bc15abcbb9b032	on a hydrodynamic permeability of a system of coaxial partly porous cylinders with superhydrophobic surfaces		Abstract The paper considers a Stokes–Brinkman’s system with varying viscosity that describes the continuous flow of viscous incompressible liquid along an ensemble of partially porous cylindrical particles using the cell approach. The analytical solution for the considered system was derived and analyzed for a particular case of Brinkman’s viscosity which illustrates the presence of superhydrophobic surfaces in a porous system. Some numerical validation of the derived results are done and the hydrodynamic permeability of the porous system was calculated and analyzed depending on geometrical and physicochemical parameters. Our analysis of the problem shows that the bigger the impermeable core the less the coefficient of hydrodynamic permeability what agrees with the physical process of the filtration. In addition, the bigger the specific permeability of porous layer the greater the hydrodynamic permeability of the porous medium.		Anatoly Filippov;Yulia Koroleva	2018	Applied Mathematics and Computation	10.1016/j.amc.2018.06.034	mathematical analysis;viscosity;porous medium;compressibility;mathematics;filtration;permeability (electromagnetism);fluid dynamics;mechanics;coaxial;porosity	Robotics	91.75053520565626	2.332983337355896	2250
f45c07dae0ff1d01f51a23bbeaf9b3a547db8e04	linear fuzzy clustering techniques with missing values and their application to local principal component analysis	eigenvalues and eigenfunctions;financial data processing;principal compo nent analysis;principal component analysis covariance matrix data mining clustering algorithms prototypes vectors fuzzy sets partitioning algorithms eigenvalues and eigenfunctions scattering;fuzzy covariance matrix linear fuzzy clustering techniques local principal component analysis fuzzy c varieties clustering;prototypes;scattering;indexing terms;data mining;fuzzy set theory;fuzzy sets;financial data processing fuzzy set theory principal component analysis covariance matrices data analysis;incomplete data;fuzzy clustering;data analysis;local structure;vectors;covariance matrices;principal component analysis;fuzzy covariance matrix;clustering algorithms;fuzzy c varieties;local principal component analysis;fuzzy c varieties clustering;missing values;linear fuzzy clustering techniques;covariance matrix;partitioning algorithms;principal component	In this paper, we propose two methods for partitioning an incomplete data set with missing values into several linear fuzzy clusters by extracting local principal components. One is an extension of fuzzy c-varieties clustering that can be regarded as the algorithm for the local principal component analysis of fuzzy covariance matrices. The other is a simultaneous application of fuzzy clustering and principal component analysis of fuzzy correlation matrices. Both methods estimate prototypes ignoring only missing values and they need no preprocessing of data such as the elimination of samples with missing values or the imputation of missing elements. Numerical examples show that the methods provide useful tools for interpretation of the local structures of a database.	algorithm;cluster analysis;database;fuzzy clustering;fuzzy cognitive map;fuzzy set;geo-imputation;iteration;iterative method;loss function;mathematical optimization;missing data;numerical linear algebra;optimization problem;preprocessor;principal component analysis;randomness;requirement;scale-invariant feature transform	Katsuhiro Honda;Hidetomo Ichihashi	2004	IEEE Transactions on Fuzzy Systems	10.1109/TFUZZ.2004.825073	fuzzy clustering;fuzzy classification;computer science;fuzzy number;machine learning;pattern recognition;data mining;mathematics;fuzzy set;statistics;principal component analysis	ML	2.4263705711736674	-39.60958179285461	2252
5a33850e3008286e63644ad71c8c803e9fd0ed9b	temporal segmentation of environmental map based on changing rate of activity areas and improvement of tracking method using lrf	trajectory history clustering algorithms joining processes legged locomotion;pedestrians human robot interaction image motion analysis laser ranging mobile robots object tracking path planning;human activity areas temporal environmental map segmentation activity changing rate tracking method lrf moving object tracking laser range finders environmental map segmentation route guidance service autonomous mobile robot pedestrian tracking human activity histories	This paper presents improved methods of tracking moving objects by using laser range finders located in the environment and of segmenting environmental maps weighted by the degree of human activity. The accuracy of our previously proposed method of detecting individuals needing route guidance service, which involves the use of an autonomous mobile robot, depends on the accuracy of pedestrian tracking. Therefore, we herein propose a method that can track a pedestrian even if he or she cannot be directly detected due to occlusion. In addition, since environmental maps depict the human activity histories in particular areas, which vary depending upon the environmental situation, such histories should be divided according to environmental situation to enable more accurate analysis of human activity areas. Thus, we also propose a method of determining the times at which environmental maps should be divided, which is based on calculating the average rate of change in each grid section. Two experiments were conducted to evaluate the usefulness of the proposed methods, and the results are presented herein.	autonomous robot;experiment;hidden surface determination;map;mobile robot;sensor;stationary process;the times	Naoki Uenoyama;Mihoko Niitsuma	2015	2015 IEEE/SICE International Symposium on System Integration (SII)	10.1109/SII.2015.7404954	computer vision;simulation;geography;cartography	Robotics	51.046240918993924	-37.067422073691226	2258
62fd305085d46d40f96ac15329640b761d1f766a	a perceptual image hashing algorithm for hybrid document security		In order to create an automatic document security system one needs to secure the textual content but also the graphical content of the document. This paper proposes a hashing algorithm capable of securing the graphical parts of paper and digital documents with unprecedented performance and a very small digest. The main challenge for such an algorithm is that of stability, in particular with respect to print and scan noise. We define the generic notion of stability and how to evaluate it. To achieve such performance we use both dense local information and global descriptors. We have tested our method on two datasets totaling nearly 45000 images.	algorithm;authentication;basic block;color mapping;computation;cryptographic hash function;graphical user interface;grayscale;instability;photocopier	Sébastien Eskenazi;Boris Bodin;Petra Gomez-Krämer;Jean-Marc Ogier	2017	2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)	10.1109/ICDAR.2017.126	computer vision;artificial intelligence;computer science;perception;search engine indexing;colors of noise;hash function;perceptual hashing	Vision	38.31354568755677	-63.30105795064031	2259
454e531342ac2cc1a433032d4329a165cce7ab11	an adaptive evolutionary algorithm for volterra system identification	error reduction;evolutionary computation;system identification;volterra systems;evolutionary algorithm;nonlinear system;real coded genetic algorithm;polynomial approximation;evolutionary computing	In this paper a real-coded genetic algorithm (GA) for Volterra system identification is presented. The adaptive GA method suggested here addresses the problem of determining the proper Volterra candidates which closely model the identified nonlinear system. A variable length GA chromosomes will encode the coefficients of the selected candidates. A number of candidates with the highest correlation with the output are selected to undergo the first evolution ‘‘era’’. The candidates with the least significant contribution in the error reduction process are removed during evolution. Then the next set of candidates are applied into the next era until a solution is found. The proposed GA method handles the issues of detecting the proper Volterra candidates and calculating the associated coefficients as a nonseparable process. The proposed algorithms has produced excellent results in modeling different nonlinear systems. 2004 Elsevier B.V. All rights reserved.	coefficient;encode;evolutionary algorithm;genetic algorithm;nonlinear system;sensor;software release life cycle;system identification	Hazem M. Abbas;Mohamed M. Bayoumi	2005	Pattern Recognition Letters	10.1016/j.patrec.2004.08.020	mathematical optimization;system identification;computer science;machine learning;control theory;mathematics;evolutionary computation	AI	32.66618252895537	-6.071043167091089	2266
fe20ad7ec79760abe74982add1f9358526c08d29	face recognition using both geometric features and pca/lda	mouth;testing;linear discriminate analysis;face recognition principal component analysis linear discriminant analysis eyes performance analysis dispersion energy measurement mouth ear testing;geometric feature;feature vector;eyes;face recognition;ear;energy measurement;face recognitionpcaldageometric features;principal component analysis;performance analysis;principle component analysis;dispersion;linear discriminant analysis	To upgrade the performance of the face recognition system, we propose a complex method using the facial geometric feature and principle component analysis/linear discriminant analysis (PCA/LDA). The existing PCA/LDA method can not reflect the facial contour line exactly because it measures similarity according to the degree of physical dispersion. In order to overcome this defect, we measured the distance between the eyes and mouth, and then used this as a feature vector for the energy of each domain within a face, such as the eyes, ears, and chin, to recalculate the similarity originally calculated by the existing PCA/LDA technique when the test and training images differ widely. An evaluation of 400 ORL database face images by using the proposed method confirmed that the recognition rate was increased by approximately 4% in the proposed method compared to that in the existing PCA/LDA method.	contour line;facial recognition system;feature vector;linear discriminant analysis;local-density approximation;principal component analysis;return loss;software bug	Young-Jun Song;Young Gil Kim;Nam Kim;Jae-Hyeong Ahn	2007	Sixth International Conference on Advanced Language Processing and Web Information Technology (ALPIT 2007)	10.1109/ALPIT.2007.18	computer vision;speech recognition;pattern recognition;mathematics	Robotics	33.23172020776785	-59.43336926903683	2267
3027d54fac4be0071e27d6376168c3b1cb3c6bbd	content-based 3d neuroradiologic image retrieval: preliminary results	3d image;indexing 3d neuroradiologic image retrieval content based image retrieval robotics institute cmu multimodal 3d images mr images ct images image similarity anatomical structures human brain;anatomical structure;cmu;multimodal 3d images;mr images;computed tomography;3d imaging;query processing;information retrieval;image matching;biomedical nmr;image indexing;testing;indexing and retrieval;computer vision;robotics institute;medical image;indexing;medical information systems;medical image processing;image retrieval content based retrieval indexing robots computed tomography anatomical structure humans information retrieval testing computer vision;3d neuroradiologic image retrieval;robots;visual features;indexing visual databases medical information systems query processing medical image processing biomedical nmr image matching;humans;content based image retrieval;ct images;content based retrieval;human brain;bilateral symmetry;anatomical structures;visual databases;image retrieval;image similarity	A content-based 3D neuroradiologic image retrieval system is being developed at the Robotics Institute of CMU. The special characteristics of this system include: 1) directly dealing with multimodal 3D images (MR/CT); 2) image similarity based on anatomical structures of the human brain; 3) combining both visual and collateral information for indexing and retrieval. A testbed has been implemented for using detected salient visual features for indexing and retrieving 3D images.	image retrieval;multimodal interaction;robotics;testbed	Yanxi Liu;William E. Rothfus;Takeo Kanade	1998		10.1109/CAIVD.1998.646037	computer vision;visual word;computer science;multimedia;information retrieval	Vision	33.2436937600698	-83.34600995361266	2269
136ba0ca4b105c9e8ddb7825f0a1f55aa06d1fcd	crossnets : a new approach to complex learning		We propose a novel neural network structure called CrossNets, which considers architectures on directed acyclic graphs. This structure builds on previous generalizations of feed forward models, such as ResNets, by allowing for all forward cross connections between layers (both adjacent and non-adjacent). The addition of cross connections among the network increases information flow across the whole network, leading to better training and testing performances. The superior performance of the network is tested against four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN. We conclude with a proof of convergence for Crossnets to a local minimum for error, where weights for connections are chosen through backpropagation with momentum.	artificial neural network;backpropagation;benchmark (computing);directed acyclic graph;graph (discrete mathematics);information flow (information theory);mnist database;maxima and minima;performance	Chirag Agarwal;Mehdi Sharifzadeh;Joe Klobusicky;Dan Schonfeld	2017	CoRR		computer science	ML	16.950505852432155	-31.693029910141167	2272
64dfdc5045e5370b30b72c384b27b5dc51beace6	robust adaptive control for prescribed performance tracking of constrained uncertain nonlinear systems		Abstract This paper proposes a robust adaptive control strategy for a class of state-constrained uncertain nonlinear systems with prescribed transient and steady-state behavior. The prescribed tracking performance can be characterized by constraints on an output tracking error. Both state and output constraints are achieved by bounding integral barrier Lyapunov functions in the backstepping procedure. A robust adaptive term is designed to compress auxiliary system uncertainties without the knowledge of their bounds. The satisfaction of control constraints and tracking error convergence are verified by theoretical analysis and are illustrated by simulation results.		Tairen Sun;Yongping Pan	2019	J. Franklin Institute	10.1016/j.jfranklin.2018.09.005	mathematical optimization;fold (higher-order function);backstepping;adaptive control;control theory;tracking error;lyapunov function;nonlinear system;mathematics;convergence (routing)	Robotics	67.78184456602308	-1.497740721540576	2275
17a8e68bf2ff9cd12a4758248a4d0a83424bdea9	artificial bee colony algorithm for power plant optimization		"""The mathematical model of a fleet of power plants can be optimized with respect to energy production. This involves the solution of a mixed integer problem. Traditionally these problems are solved by linearization of the continuous and non-linear parts and subsequent application of a Simplex-type algorithm. In order to handle nonlinearities, so-called """"biomimetic"""" optimization algorithms can be applied. As an example, we are proposing an approach to first model power plant blocks with fast Neural Networks. Afterwards, we optimize the operation of multi-block power plants over a period of time using an Artificial Bee Colony Algorithm."""	artificial bee colony algorithm;biomimetics;mathematical model;mathematical optimization;neural networks;nonlinear system	Friedrich Biegler-König	2013		10.7148/2013-0788	artificial bee colony algorithm	EDA	32.47200304797626	-3.9357427753415344	2276
0a814adfac0896d91eca6b126bfe2cf1de99455d	order of arrival estimation via uhf-uwb rfid	tracking technique order of arrival estimation uhf uwb rfid conveyor belt automatic supply chain management optical systems ultra high frequency radio frequency identification wireless technologies ultra wide band;radiofrequency identification estimation vectors belts indexes distance measurement sorting;uwb sorting localization rfid;ultra wideband communication conveyors estimation theory object tracking particle filtering numerical methods radiofrequency identification	The estimation of the order-of-arrival for items on a conveyor belt is a fundamental operation for automatic supply chain management. Classical solutions are based on optical systems, which are accurate but do not guarantee the transmission of a high amount of data through the barcode, or on ultra-high frequency (UHF) radio frequency identification (RFID), which does not guarantee high-accuracy sorting. Among wireless technologies, ultra-wide band (UWB) provides high-accuracy localization. In this paper, we introduce a joint UHF and UWB RFID system that allows identification and tracking of passive tags attached to objects for high-accuracy order-of-arrival estimation of goods on conveyor belts. A tracking technique based on particle filtering is used for order-of-arrival estimation. Results for a case of study show accuracy of the proposed system for various settings.	''the legend of zelda:;barcode;estimation theory;particle filter;radio frequency;radio-frequency identification;signal processing;sorting;succession;ultra high frequency;ultra-wideband	Stefania Bartoletti;Nicolo Decarli;Anna Guerra;Francesco Guidi;Davide Dardari;Andrea Conti	2014	2014 IEEE International Conference on Communications Workshops (ICC)	10.1109/ICCW.2014.6881185	embedded system;telecommunications	Robotics	49.635672175194365	-0.21761506909924466	2282
eb28d021969de1d3586be3138ff0fd2c267d813c	improvement of ghost imaging on orbital angular momentum by using jpeg & linear block codes	image coding;linear codes;imaging system;image compression;image transmission;peak signal to noise ratio;joint photographic expert group;compression ratio;ghost imaging;image transmission orbital angular momentum ghost imaging image compression;transform coding imaging image coding block codes image communication psnr reliability;linear codes block codes image coding;block codes;psnr ghost imaging orbital angular momentum jpeg linear block codes joint photographic experts group peak signal to noise ratio;orbital angular momentum	Ghost imaging has attracted a lot of attentions recently, but the trade-off between the reliability and the transmitting rate becomes the biggest challenge for ghost imaging. In this paper, we present a method to improve the transmitting rate with the unchanged reliability for ghost-imaging system based on entanglement. In the scheme, we first encode the image by Joint Photographic Experts Group (JPEG) algorithm before transmitting an image, and then encode the JPEG bits stream using linear block codes to overcome the lossy information by compression. The transmitting rate can be enhanced by compression, and the reliability can be kept by linear block codes. By experiments, the results show that when the compression ratio of JPEG is beyond the coding efficiency of linear block codes, we can get some gains of the image with Peak Signal-to-Noise Ratio(PSNR) increase. Compared with direct transmission bit stream, our scheme can improve the PSNR 16dB, using 1.225 times of the transmission times.	algorithm;algorithmic efficiency;angular momentum operator;angularjs;bitstream;encode;experiment;ghost imaging;jpeg;linear code;lossy compression;molecular orbital;moving picture experts group;peak signal-to-noise ratio;quantum entanglement;transmitter	Xiaoliang Dong;Fei Cao;Shengmei Zhao;Baoyu Zheng	2011	2011 International Conference on Wireless Communications and Signal Processing (WCSP)	10.1109/WCSP.2011.6096918	lossless jpeg;computer vision;telecommunications;mathematics;optics	Vision	44.13649488543927	-18.21728743060753	2285
696d3f1f6ee9abcb5df68494654072bff1ebd989	consideration on pattern-separating function in a generalized random nerve net consisting of two layers	physiological models biocybernetics neural nets neurophysiology;red nerviosa;neural nets;reseau nerveux;firing rate pattern separating function random nerve net feedforward inhibitory connections mean values variances;forma descarga;electrophysiology;functional dependency;biocybernetics;modele simulation;optical fiber theory reliability engineering brain modeling hamming distance particle separators information processing reliability theory feedforward systems;electrofisiologia;nervous network;modelo simulacion;neurophysiology;electrophysiologie;simulation model;physiological models;discharge pattern;mode decharge	A two-layered random nerve net with feed-fonvard inhibitory connections has the pattern-separating function, which separates input patterns in the sense of overlapping rate. The function depends on mean values and variances of various distributions specifying the nerve net. It also depends on the firing rate of the input patterns and the second layer. A theory is derived to investigate the pattern-separating function in the two-layered random nerve net. Then the effects of the mean values and variances of distributions on the pattern-separating function are considered collectively, using the derived theory. The influences of the firing rate, both of the input patterns and the second layer, on the function are also investigated. As a result, it is shown that the pattern-separating function is largely influenced by the aforementioned mean values, variances, and firing rates. It is also revealed that an excellent nerve net in the pattern-separating function can be obtained when the mean values and variances of distributions are selected properly. Simultaneously, it is pointed out that the pattern-separating function is enhanced hy controlling the firing rate of the second layer to a small value. Furthermore, it is shown that the input patterns are remarkably separated when they have large firing rates.		Toyoshi Torioka;Nobuhiko Ikeda	1990	IEEE Trans. Systems, Man, and Cybernetics	10.1109/21.57274	electrophysiology;simulation;computer science;artificial intelligence;machine learning;simulation modeling;biocybernetics;functional dependency;neurophysiology;artificial neural network;statistics	ML	19.733607911170985	-71.24855957854498	2287
8edb825b52072eec377acd494af4e28e529eac1c	a flexible approach to ranking with an application to mba programs	modelizacion;agregacion;multicriteria analysis;sorting;modele lineaire;tria;modelo lineal;multiple criteria;aggregation;modelisation;multi dimensional;hierarchical classification;mixed integer program;programacion mixta entera;programa aplicacion;ranking;application program;programme application;ranking sorting mixed integer programming multiple criteria mba programs;triage;linear model;preferencia;classification hierarchique;agregation;programmation partiellement en nombres entiers;mixed integer programming;preference;analisis multicriterio;analyse multicritere;modeling;clasificacion jerarquizada;mba programs	We develop a model for flexibly ranking multi-dimensional alternatives/units into preference classes via Mixed Integer Programming. We consider a linear aggregation model, but allow the criterion weights to vary within pre-specified ranges. This allows the individual alternatives/units to play to their strengths. We illustrate the use of the model by considering the Financial Times Global MBA Program rankings and discuss the implications. We argue that in many applications neither the data nor the weights or the aggregation model itself is precise enough to warrant a complete ranking, providing an argument for sorting or what we call flexible ranking. 2009 Elsevier B.V. All rights reserved.	financial times;integer programming;linear programming;microsoft lumia;sorting	Murat Köksalan;Tayyar Büyükbasaran;Özgür Özpeynirci;Jyrki Wallenius	2010	European Journal of Operational Research	10.1016/j.ejor.2009.02.034	mathematical optimization;combinatorics;systems modeling;integer programming;ranking;sorting;linear model;mathematics;algorithm;statistics	AI	-0.11515441867918225	-15.613797981996443	2288
864b722fdd68f95e4a60b82255e9dfc3f19a914d	improving h.264 video coding through block oriented transforms	prediction method;rate distortion;image coding;image resolution;video coding block codes image resolution transforms;encoding transforms discrete cosine transforms bit rate automatic voltage control pixel image coding;rate distortion criterion;block oriented transforms;mpeg4 avc;bit rate;rate distortion theory;video coding;prediction methods;automatic voltage control;image orientation;rate distortion theory video coding image orientation analysis transforms prediction methods;discrete cosine transforms;pixel;transforms;integer transforms;h 264 video coding;rate distortion criterion h 264 video coding block oriented transforms mpeg4 avc integer transforms;encoding;block codes;image orientation analysis	This paper introduces a pre-processing stage for the video coder H.264/MPEG4-AVC that takes advantage of the orientation of the blocks. Contrary to most of the proposed solutions in the literature, it is not the transform that adapts to the signal but the signal that is pre-processed to dasiafitpsila the transform. Residual blocks are straightened out to horizontal or vertical axes with the help of circular shifts applied at the pixel level. These shifts allow to simulate pseudo-rotations in the blocks. They are defined for all the partition sizes (16times16 to 4times4) and the two integer transforms of H.264 (8times8 and 4times4). Size and orientation for the blocks are evaluated with the help of a selection based on a rate-distortion criterion. This pre-processing stage applied on residual frames of a H.264 coder can improve its rate-distortion performances.	circular shift;data compression;distortion;h.264/mpeg-4 avc;performance;pixel;preprocessor;simulation	Antoine Robert;Isabelle Amonou;Béatrice Pesquet-Popescu	2008	2008 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2008.4607532	block code;computer vision;image resolution;orientation;rate–distortion theory;computer science;theoretical computer science;mathematics;pixel;encoding;statistics	Vision	45.71541163548214	-17.19090583237294	2294
089c522d90bd9da30cae9a535f25a3f2e0d5c6b3	face recognition based on weber symmetrical local graph structure			facial recognition system	Jucheng Yang;Lingchao Zhang;Yuan Wang;Tingting Zhao;Wenhui Sun;Dong Sun Park	2018	TIIS	10.3837/tiis.2018.04.019	distributed computing;facial recognition system;computer science;graph	Vision	30.113690464901346	-57.307716921106376	2303
6d8ea2883ed5272136f3af1797b80e2785733b8a	formal-language-theoretic optimal path planning for accommodation of amortized uncertainties and dynamic effects	supervisory control;optimal path planning;path planning;robot navigation;regular language;optimization problem;wheeled mobile robot;finite state automata;experimental validation;global optimization;estimation error;finite state machine;formal language	We report a globally-optimal approach to robotic path plann ing under uncertainty, based on the theory of quantitative m easures of formal languages. A significant generalization to the lan guage-measure-theoretic path planning algorithm ν is presented that explicitly accounts for average dynamic uncertainties and estimation errors in plan execution. The notion of the navig tion automaton is generalized to include probabilistic uncontrollabl e transitions, which account for uncertainties by modeling a d planning for probabilistic deviations from the computed policy in the co urse of execution. The planning problem is solved by casting i in the form of a performance maximization problem for probabilist ic finite state automata. In essence we solve the following op timization problem: Compute the navigation policy which maximizes the probability of reaching the goal, while simultaneously min i izing the probability of hitting an obstacle. Key novelties of the proposed approach include the modeling of uncertaintie s using the concept of uncontrollable transitions, and the solution of the ensuing optimization problem using a highly efficient se arch-free combinatorial approach to maximize quantitative measures of p robabilistic regular languages. Applicability of the algo rithm in various models of robot navigation has been shown with experimental validation on a two-wheeled mobile robotic platform (SEGWA Y RMP 200) in a laboratory environment.	amortized analysis;automata theory;automated planning and scheduling;automaton;cartesian closed category;central processing unit;decomposition (computer science);expectation–maximization algorithm;finite-state machine;formal language;inter-process communication;mathematical optimization;maxima and minima;motion planning;optimization problem;partially observable system;recursion;regular language;risk management plan;robot;robotic mapping;run time (program lifecycle phase);simulation;time complexity;workspace	Ishanu Chattopadhyay;Anthony Cascone;Asok Ray	2010	CoRR		optimization problem;mathematical optimization;formal language;simulation;regular language;computer science;artificial intelligence;machine learning;mathematics;motion planning;supervisory control;finite-state machine	Robotics	22.455438872348225	-16.443554028149713	2304
9019e4fa98ef224ce2dee6aa981a6c1a1ddcb5de	exponential stability of impulsive discrete systems with multiple delays	exponential stability;halanay inequality;multiple delays;discrete systems	The stability problem of impulsive discrete systems with multiple delays is studied. By means of the Lyapunov stability theory and discrete-time Halanay-type inequality technique, we develop sufficient conditions for the exponential stability for the impulsive discrete systems with multiple delays, which involves multiple delays not only at non-impulsive time instants but also at impulsive time instants. The results are extended to two special discrete systems: the delayed discrete systems with time delays at only impulsive time instants and the delayed discrete systems with time delays at only non-impulsive time instants. Finally, the validity of the obtained results is shown by a numerical example.	discrete system;lyapunov fractal;numerical analysis;social inequality;time complexity	Yuanqiang Chen;Renbi Tian	2013	JNW	10.4304/jnw.8.11.2564-2571	mathematical optimization;exponential stability	ML	70.11986372826242	0.95525344377492	2307
0544b202c20435938f2816f6ae5de0d639ed351e	a methodology for extracting objective color from images	burn scar color measurement;skin lesion;range scanner;color calibration;image processing;reflectivity;color;realistic rendering;geometry;calibration feature extraction computer vision image colour analysis realistic images rendering computer graphics image texture;layout;indexing terms;three dimensional;image texture;computer vision;skin color;automatic detection;image edge detection;image colour analysis;range image;feature extraction;color image edge detection indoor environments production facilities solid modeling reflectivity geometry cameras layout computer vision;indoor environment;solid modeling;production facilities;color image enhancement;indoor environments;color correction;realistic images;three dimensional range images;light calibration;range scanner color calibration color correction computer vision image processing light calibration realistic rendering three dimensional range images surface orientation real images burn scar color measurement color image enhancement;rendering computer graphics;real images;calibration;cameras;surface orientation;color image	We present a methodology for correcting color images taken in practical indoor environments, such as laboratories, factories, and studios, that explicitly models illuminant location, surface reflectance and geometry, and camera responsivity. We explicitly model surfaces by taking our color images with corresponding registered three-dimensional (3-D) range images, which provide surface orientation and location information for every point in the scene. We automatically detect regions where color correction should not be applied, such as specularities, coarse texture regions, and jump edges. This correction results in objective color measures of the imaged surfaces. This kind of integrated, comprehensive system of color correction has not existed until now. i.e., it is the first of its kind in computer vision. We demonstrate results of applying this methodology to real images for applications in photorealistic rerendering, skin lesion detection, burn scar color measurement, and general color image enhancement. We also have tested the method under different lighting configurations and with three different range scanners.	cicatrix;color image;computer vision;estimated;image editing;laboratory;lambertian reflectance;map;physical object;registration;scar (physics);shading;texture mapping	Mark W. Powell;Sudeep Sarkar;Dmitry B. Goldgof;Krassimir Ivanov	2004	IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)	10.1109/TSMCB.2004.832177	color histogram;image texture;layout;three-dimensional space;false color;computer vision;calibration;color quantization;index term;color normalization;color image;image processing;feature extraction;computer science;reflectivity;color balance;solid modeling;real image;computer graphics (images)	Vision	57.468814188498236	-48.967766561205046	2320
4ca4c187c0ac629851f465fc42c8b7a14267c99c	an evolutionary path relinking approach for the quadratic multiple knapsack problem	constrained optimization;quadratic multiple knapsack;responsive threshold search;path relinking;evolutionary computing	The quadratic multiple knapsack problem (QMKP) is a challenging combinatorial optimization problem with numerous applications. In this paper, we propose the first evolutionary path relinking approach (EPR) for solving the QMKP approximately. This approach combines advanced features both from the path relinking (PR) method and the responsive threshold search algorithm. Thanks to the tunneling property which allows a controlled exploration of infeasible regions, the proposed EPR algorithm is able to identify very high quality solutions. Experimental studies on the set of 60 well-known benchmarks and a new set of 30 large-sized instances show that EPR outperforms several state-ofthe-art algorithms. In particular, it discovers 10 improved results (new lower bounds) and matches the best known result for the remaining 50 cases. More significantly, EPR demonstrates remarkable efficacy on the 30 new larger instances by easily dominating the current best performing algorithms across the whole instance set. Key components of the algorithm are analyzed to shed lights on their impact on the proposed approach.	benchmark (computing);combinatorial optimization;computation;display resolution;diversification (finance);epr paradox;evolutionary algorithm;genetic algorithm;high- and low-level;knapsack problem;local search (optimization);mathematical optimization;optimization problem;quadratic assignment problem;search algorithm;tunneling protocol	Yuning Chen;Jin-Kao Hao;Fred Glover	2016	Knowl.-Based Syst.	10.1016/j.knosys.2015.10.004	mathematical optimization;constrained optimization;computer science;algorithm	AI	25.524591545599513	0.6512774506093482	2324
207f8f7b1c44630f8dc1619fd7bec405140c818f	ucp-networks: a directed graphical representation of conditional utilities	condition dependence;model generation;utility function;additive model;graphical representation	We propose a directed graphical representation of utility functions, called UCP-networks, that combines aspects of two existing preference models: generalized additive models and CP-networks. The network decomposes a utility function into a number of additive factors, with the directionality of the arcs reflecting conditional dependence in the underlying (qualitative) preference ordering under a ceteris paribus interpretation. The CP-semantics ensures that computing optimization and dominance queries is very efficient. We also demonstrate the value of this representation in decision making. Finally, we describe an interactive elicitation procedure that takes advantage of the linear nature of the constraints on “tradeoff weights” imposed by a UCP-network.	additive model;emi (protocol);graphical user interface;mathematical optimization;utility functions on indivisible goods	Craig Boutilier;Fahiem Bacchus;Ronen I. Brafman	2001			combinatorics;computer science;machine learning;mathematics;additive model;statistics	AI	-2.4279456811270483	-17.428901531694585	2328
bd193eafcc30baddce3a914878ede03796a1fff2	stock fluctuations anomaly detection based on wavelet modulus maxima	wavelet transforms fourier analysis investment risk management stock markets;wavelet analysis;wavelet modulus maxima method;investments;finance;stock market;fluctuations;uncertainty;economic forecasting;anomaly detection;information technology;government;risk management;industries;abnormal;investment;stock markets;stability;modulus maxima stock abnormal wavelet;wavelet transforms;multiscale wavelet transform;wavelet transform;abnormal stock analysis;time series analysis;point mutation;fourier analysis;stock;investment risk;stock fluctuations anomaly detection;frequency domain;modulus maxima;fourier analysis stock fluctuations anomaly detection investment risk stock market wavelet modulus maxima method abnormal stock analysis multiscale wavelet transform;wavelet;fluctuations stock markets investments wavelet analysis stability finance government economic forecasting information technology uncertainty;irregular sampling;singular point	Stock fluctuations anomaly increase the uncertainty and investment risk in the stock market, is an important element in financial research. In this paper, wavelet modulus maxima method is used in the detection of abnormal stock analysis. It is obtained based on the irregular sampling in the multi-scale wavelet transform. It overcomes the localized limitation about traditional Fourier analysis in time and frequency domains. Experimental results show that the wavelet modulus maxima method can not only depict the position of the point mutation in the signals but also capture the singular points of the stock unusual fluctuations quickly and accurately.	anomaly detection;cutting stock problem;fourier analysis;maxima;modulus of continuity;sampling (signal processing);wavelet transform	Zhijun Fang;Guihua Luo;Shenghua Xu;Fengchang Fei	2009	2009 International Conference on Business Intelligence and Financial Engineering	10.1109/BIFE.2009.89	financial economics;actuarial science;economics;commerce	DB	4.336541164339898	-14.817903705129828	2329
d25a82b18fd3653baeb7d05dfeb0f5f28c3ac71d	a non-deterministic grammar inference algorithm applied to the cleavage site prediction problem in bioinformatics	virus genome;grammar inference algorithm;accuracy rate;nondeterministic algorithm;cleavage site prediction problem;order independent language;report sensibility;potivirus genome;non-deterministic grammar inference algorithm;cleavage site	We report results on applying the OIL (Order Independent Language) grammar inference algorithm to predict cleavage sites in polyproteins from translation of Potivirus genome. This non-deterministic algorithm is used to generate a group of models which vote to predict the occurrence of the pattern. We built nine models, one for each cleavage site in this kind of virus genome and report sensibility, specificity, accuracy for each model. Our results show that this technique is useful to predict cleavage sites in the given task with accuracy rates higher than 95%.	bioinformatics;deterministic algorithm;grammar induction;nondeterministic algorithm;sensitivity and specificity	Gloria Inés Alvarez;Jorge Hernán Victoria;Enrique Bravo;Pedro García	2010		10.1007/978-3-642-15488-1_23	computer science;bioinformatics;machine learning;algorithm	ML	1.95587386951802	-55.56161695542667	2339
a9e3a4f1d71c7fb72dbea6afd308a3c2eb0e89cf	noise adaptation in integrate-and-fire neurons	traitement signal;time dependent;tecnologia electronica telecomunicaciones;fonction potentiel;computacion informatica;analisis estadistico;error function;probability density function;integrate and fire;grupo de excelencia;leaky integrate and fire;dynamical system;stochastic system;systeme dynamique;gaussian white noise;fonction densite;statistical analysis;density function;ciencias basicas y experimentales;signal processing;funcion densidad;analytical method;analyse statistique;funcion potencial;funcion error;stochastic integral;fonction erreur;integrate and fire neuron;root mean square;sistema dinamico;reseau neuronal;tecnologias;sistema estocastico;grupo a;potential function;procesamiento senal;red neuronal;systeme stochastique;neural network	The statistical spiking response of an ensemble of identically prepared stochastic integrate-and-fire neurons to a rectangular input current plus gaussian white noise is analyzed. It is shown that, on average, integrate-and-fire neurons adapt to the root-mean-square noise level of their input. This phenomenon is referred to as noise adaptation. Noise adaptation is characterized by a decrease in the average neural firing rate and an accompanying decrease in the average value of the generator potential, both of which can be attributed to noise-induced resets of the generator potential mediated by the integrate-and-fire mechanism. A quantitative theory of noise adaptation in stochastic integrate-and-fire neurons is developed. It is shown that integrate-and-fire neurons, on average, produce transient spiking activity whenever there is an increase in the level of their input noise. This transient noise response is either reduced or eliminated over time, depending on the parameters of the model neuron. Analytical methods are used to prove that nonleaky integrate-and-fire neurons totally adapt to any constant input noise level, in the sense that their asymptotic spiking rates are independent of the magnitude of their input noise. For leaky integrate-and-fire neurons, the long-run noise adaptation is not total, but the response to noise is partially eliminated. Expressions for the probability density function of the generator potential and the first two moments of the potential distribution are derived for the particular case of a nonleaky neuron driven by gaussian white noise of mean zero and constant variance. The functional significance of noise adaptation for the performance of networks comprising integrate-and-fire neurons is discussed.	acclimatization;action potential;arabic numeral 0;artificial neuron;biological neuron model;neurons;noise (electronics);normal statistical distribution;sample variance;transient noise;white noise	Michael E. Rudd;Lawrence G. Brown	1997	Neural Computation	10.1162/neco.1997.9.5.1047	probability density function;telecommunications;artificial intelligence;signal processing;mathematics;stochastic resonance;artificial neural network;statistics	ML	20.89363131851614	-71.82143317968982	2344
6b4565e08437dba25d143ecab6ec82deb3f423ce	identification of electrical activity of the brain associated with changes in behavioural performance	evaluation performance;analisis componente principal;recording;performance evaluation;cognitive psychology;systeme nerveux central;electroencefalografia;no response;actividad electrica;evaluacion prestacion;encefalo;classification;activite electrique;electroencephalographie;sistema nervioso central;enregistrement;single channel;encephale;principal component analysis;analyse composante principale;electroencephalography;work in progress;clasificacion;central nervous system;registro;electrical activity;brain vertebrata	Brain-behavior research and practice in autism, especially over the past 20 years, has provided a more in-depth understanding of the multiple brain areas affected in autism. Neuropsychological studies and computerized technology have aided in understanding the cerebral organization and psychological structure of the functions of learning: attention, executive function, perception, memory, speech, language, sensory-motor and mood. These functions are dependent upon biological (structural-electrical-chemical) and external, environmental/social cues and together present a very dynamic process throughout development.	machine perception	A. Salguero Beltran;Maria Petrou;G. Barrett;B. Dickson	2001		10.1007/3-540-44732-6_30	recording;electroencephalography;biological classification;artificial intelligence;central nervous system;work in process;principal component analysis	HCI	13.191644667336877	-79.02949079912729	2351
a7330d56cdafe8c4a8672d9ffc3bded8990f9c86	goes-r advanced baseline imager (abi) and geostationary lightning mapper (glm) calibration/validation from a field campaign perspective		To ensure the post-launch performance of the National Oceanic and Atmospheric Administration's (NOAA) next generation geostationary optical Earth observing sensors a post-launch validation field campaign was developed. The GOES-R field campaign was focused to support post-launch validation of the Advanced Baseline Imager (ABI) and Geostationary Lightning Mapper (GLM). This effort will discuss an overview of the GOES-R field campaign, field campaign reference measurements to validate ABI SI traceability and GLM flash detection efficiency, geophysical measurements for ABI product validation, as well as preliminary results. The GOES-R field campaign enabled the collection of reference data that provided advanced post-launch validation capabilities and fostered new perspectives in the post-launch validation and monitoring of NOAA's next generation of operational environmental satellites.	application binary interface;generalized linear model;image sensor;lightning;next-generation network;traceability	Francis Padula;Steven J. Goodman;Aaron Pearlman;Changyong Cao	2017	2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)	10.1109/IGARSS.2017.8126955	traceability;computer vision;artificial intelligence;remote sensing;calibration;satellite;geostationary orbit;computer science;hyperspectral imaging;lightning	Robotics	80.82882987719385	-62.231068180010375	2353
7c23c337d1e756b66a20f0915549e54a07dfbb46	generalization characteristics of complex-valued feedforward neural networks in relation to signal coherence	feedforward neural network;feedforward neural networks;generalization error;neurons coherence biological neural networks vectors feedforward neural networks signal to noise ratio;supervised learning;neural nets;activation function;degree of freedom;supervised learning complex valued neural network function approximation generalization;signal processing function approximation generalisation artificial intelligence learning artificial intelligence neural nets;imaging system;vectors;function approximation;signal processing;neural dynamics generalization characteristics complex valued feedforward neural networks signal coherence radar systems coherent imaging systems real valued feedforward neural networks function approximation temporal signal interpolation amplitude phase type activation function bivariate real valued neural networks dual univariate real valued neural networks learning;coherence;neurons;generalisation artificial intelligence;learning artificial intelligence;signal to noise ratio;generalization;biological neural networks;complex valued neural network;neural network	Applications of complex-valued neural networks (CVNNs) have expanded widely in recent years-in particular in radar and coherent imaging systems. In general, the most important merit of neural networks lies in their generalization ability. This paper compares the generalization characteristics of complex-valued and real-valued feedforward neural networks in terms of the coherence of the signals to be dealt with. We assume a task of function approximation such as interpolation of temporal signals. Simulation and real-world experiments demonstrate that CVNNs with amplitude-phase-type activation function show smaller generalization error than real-valued networks, such as bivariate and dual-univariate real-valued neural networks. Based on the results, we discuss how the generalization characteristics are influenced by the coherence of the signals depending on the degree of freedom in the learning and on the circularity in neural dynamics.	activation function;approximation algorithm;artificial neural network;coherence (physics);contingency table;dual;exhibits as topic;experiment;feed forward (control);feedforward neural network;generalization (psychology);generalization error;imaginary time;interpolation imputation technique;neural networks;numerical analysis;rs-232;small	Akira Hirose;Shotaro Yoshida	2012	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2012.2183613	feedforward neural network;computer science;artificial intelligence;machine learning;pattern recognition;artificial neural network	ML	19.75669911595838	-28.999312902839634	2356
95fce4b15819a06f128d1f8312e1f90c0b939833	path planning for unmanned vehicles using ant colony optimization on a dynamic voronoi diagram	aco;voronoi diagram;unmanned vehicles;path planning;time of arrival;ant colony optimization;shortest path	The objective is an unmanned vehicle is to complete a given mission, i.e., to arrive at a given destination safely within a prespecified time. As a result, the basic pathplanning problem discussed in this paper is the problem of finding a short (fuel-optimal) and collision-free path between the two states in an environment with static or dynamic obstacles [13]. For a static environment with perfect information, the traditional path-finding algorithms such as A-star [4] can provide an optimal solution. However, for a dynamic environment with uncertain factors or gradually changing topology, the conventional optimization algorithm may not be applicable. Instead, an adaptive and robust algorithm is favored. Abstract	a* search algorithm;ant colony optimization algorithms;mathematical optimization;motion planning;robustness (computer science);uncrewed vehicle;unmanned aerial vehicle;voronoi diagram	Yaohang Li;Tao Dong;Marwan Bikdash;Yongduan Song	2005			voronoi diagram;adaptive algorithm;ant colony optimization algorithms;k shortest path routing;motion planning;time of arrival;shortest path problem;mathematical optimization;perfect information;computer science	Robotics	53.8640680668593	-23.854150698741808	2361
1985de1691024c1ca6bd416e0b2007be6aaa0bce	using local models to control movement	local model	This paper explores the use of a model neural network for motor learning. Steinbuch and Taylor presented neural network designs to do nearest neighbor lookup in the early 1960s. In this paper their nearest neighbor network is augmented with a local model network, which fits a local model to a set of nearest neighbors. The network design is equivalent to local regression. This network architecture can represent smooth nonlinear functions, yet has simple training rules with a single global optimum. The network has been used for motor learning of a simulated arm and a simulated running machine.	artificial neural network;fits;global optimization;lookup table;network architecture;network planning and design;nonlinear system	Christopher G. Atkeson	1989			probabilistic neural network;computer science;artificial intelligence;machine learning;pattern recognition;time delay neural network;network simulation	ML	17.855369333869405	-24.380704308151365	2363
4537c0d365880338923f16974005507582e45740	an intrinsic algorithm for computing geodesic distance fields on triangle meshes with holes	drntu engineering computer science and engineering;journal article;discrete geodesics;geodesic distance field;intrinsic algorithm;global optimization;optimization;paths;triangle mesh;article	As a fundamental concept, geodesics play an important role in many geometric modeling applications. However, geodesics are highly sensitive to topological changes; a small topological shortcut may result in a significantly large change of geodesic distance and path. Most of the existing discrete geodesic algorithms can only be applied to noise-free meshes. In this paper, we present a new algorithm to compute the meaningful approximate geodesics on polygonal meshes with holes. Without the explicit hole filling, our algorithm is completely intrinsic and independent of the embedding space; thus, it has the potential for isometrically deforming objects as well as meshes in high dimensional space. Furthermore, our method can guarantee the exact solution if the surface is developable. We demonstrate the efficacy of our algorithm in both real-world and synthetic models.	algorithm;distance (graph theory);triangle mesh	Dao Thi Phuong Quynh;Ying He;Shi-Qing Xin;Zhonggui Chen	2012	Graphical Models	10.1016/j.gmod.2012.04.009	combinatorics;solving the geodesic equations;topology;geodesic map;computer science;triangle mesh;mathematics;geometry;global optimization	HCI	68.9152392846481	-44.06124519387515	2364
bc2b158a2ebe95bb7d5a00d587640ff212001d87	new strategy of modeling inversion layer characteristics in mos structure for ulsi applications	quantum mechanical effects qmes modeling inversion layer mos structure;surface potential;density of state;valence band;mos structure quantum mechanical effects qmes modeling inversion layer;quantum mechanics;threshold voltage;surface layer;quantum mechanical effects qmes;inversion layer;modeling;mos structure;metal oxide semiconductor field effect transistor	With the development of ULSI silicon technology, metal oxide semiconductor field effect transistor (MOSFET) devices are scaling down to nanometer regime. Energy of carriers in inversion layer in MOS structure is quantized and consequently, the physics and then the transport characteristics of inversion layer carriers are different from those in semi-classical theory. One essential matter is that the widely used concept of conduction band (valence band as well) effective density-of-states is no longer valid in quantized inversion layer. In this paper, an alternative concept, called surface layer effective density-of-states, is used to model the characteristics of MOS structure including threshold voltage, carrier sheet density, surface potential as well as capacitance.	depletion region;field effect (semiconductor);first quantization;image scaling;quantization (signal processing);semiconductor industry;transistor;very-large-scale integration	Yutao Ma;Zhijian Li;Litian Liu	2001	Science in China Series : Information Sciences	10.1007/BF02714568	systems modeling;valence band;surface layer;density of states;threshold voltage;quantum mechanics	EDA	93.0385211500032	-11.967194347912951	2366
c434ea20f006ac34dc470662b64c2289e5b9227c	non-linear dimensionality reduction using fuzzy lattices	fuzzy lattices;facial expression recognition;schrodinger equation;schrodinger equation face recognition feature extraction fuzzy set theory gaussian processes;facial activities;three dimension space;nonlinear dimensionality reduction;kinetic energy;ke;nonlinear dimensionality reduction nonlinear feature extraction facial expression recognition facial activities three dimension space ke kinetic energy schrodinger equation gaussian membership functions nearest neighbourhood elements fuzzy lattices;nearest neighbourhood elements;nonlinear feature extraction;gaussian membership functions	The proposed method is based on extraction of non-linearity from the nearest neighbourhood elements of image. To detect non-linearity, relation between the nearest neighbourhood elements of the image, have been expressed in terms of Gaussian membership functions. All the elements of the image are connected with the nearest neighbourhood elements with some membership degree of the Gaussian functions. It results in the formation of number of fuzzy lattices. The lattices have been expressed in the form of Schrodinger equation, to find the kinetic energy (KE) used, corresponding to change occurring in the facial activity of a person. Finally, the KE embedded in three dimension space is used to distinguish non-linear changes during occurrence of various facial activities. Experimental results show that proposed method is effective in recognition of facial expression as it focuses on extracting the non-linear features corresponding to contours of maximum energy which are appearing because of different expressions.	nonlinear dimensionality reduction	Rajiv Kapoor;Rashmi Gupta	2013	IET Computer Vision	10.1049/iet-cvi.2012.0097	schrödinger equation;mathematical analysis;discrete mathematics;topology;computer science;machine learning;kinetic energy;mathematics;nonlinear dimensionality reduction	Vision	35.449235003691555	-59.71682779719944	2367
2ba4142906cc05e449d973b03951d1e7ada7c556	developmental learning of complex syntactical song in the bengalese finch: a neural network model	pere;father;modelizacion;song;dato observacion;ruido aleatorio;feedforward;development;chant;learning;noyau moteur;zebra finch;bruit aleatoire;relapse;estudio comparativo;nif;simulation;hvc;desarrollo;hopfield neural nets;simulacion;boucle anticipation;proceso adquisicion;acquisition process;approche deterministe;padre;recurrence;aprendizaje;deterministic approach;modelisation;etude comparative;observacion;birdsong;random noise;apprentissage;ciclo anticipacion;motor nucleus;reseau neuronal hopfield;recurrencia;developpement;comparative study;enfoque determinista;observation;reseau neuronal recurrent;nucleo motor;donnee observation;recurrent neural nets;recurrent neural network;neural network model;reseau neuronal;modeling;recidive;red neuronal;observation data;processus acquisition;recaida;canto;noise;neural network	We developed a neural network model for studying neural mechanisms underlying complex syntactical songs of the Bengalese finch, which result from interactions between sensori-motor nuclei, the nucleus HVC (HVC) and the nucleus interfacialis (NIf). Results of simulations are tested by comparison with the song development of real young birds learning the same songs from their fathers. The model shows that complex syntactical songs can be reproduced from the simple interaction between the deterministic dynamics of a recurrent neural network and random noise. Features of the learning process in the simulations show similar trends to those observed in empirical data on the song development of real birds. These observations suggest that the temporal note sequences of songs take the form of a dynamical process involving recurrent connections in the network of the HVC, as opposed to feedforward activities, the mechanism proposed in the previous model.	artificial neural network;biological neural networks;cell nucleus;feedforward neural network;finch feather ab.ige:acnc:pt:ser:qn;holographic versatile card;interaction;network model;neuroscience information framework;noise (electronics);nucleus rtos;recurrent neural network;simulation;dorsal raphe nucleus	Yuichi Yamashita;Miki Takahasi;Tetsu Okumura;Maki Ikebuchi;Hiroko Yamada;Madoka Suzuki;Kazuo Okanoya;Jun Tani	2008	Neural networks : the official journal of the International Neural Network Society	10.1016/j.neunet.2008.03.003	systems modeling;canto;computer science;noise;artificial intelligence;recurrent neural network;machine learning;comparative research;deterministic system;observation;feed forward;artificial neural network	ML	20.569794276678888	-70.2997314746862	2368
460b096142e693b79ef09e49d2bf760ef2a834e3	self-localization in the robocup environment	vision system;autolocalisation;navegacion;robot movil;obstaculo;position;systeme vision;soccer;estudio comparativo;localization;posicion;localizacion;robotics;orientation;transformacion hough;robocup;etude comparative;captador medida;navigation;measurement sensor;capteur mesure;localisation;robot mobile;football;comparative study;robotica;orientacion;hough transformation;hough transform;transformation hough;robotique;indoor installation;instalacion interior;installation interieure;moving robot;obstacle;futbol	Knowing the position and orientation of a mobile robot situated in an environment is a critical element for effectively accomplishing complex tasks requiring autonomous navigation. Techniques for robot self-localization have been extensively studied in the past, but an effective general solution does not exist, and it is often necessary to integrate different methods in order to improve the overall result. In this paper we present a self-localization method that is based on the Hough Transform for matching a geometric reference map with a representation of range information acquired by the robot’s sensors. The technique is adequate for indoor office-like environments, and specifically for those environments that can be represented by a set of segments. We have implemented and successfully tested this method in the RoboCup environment and we consider this a good benchmark for its use in office-like environments populated with unknown and moving obstacles (e.g. persons moving around).	autonomous robot;benchmark (computing);critical graph;high- and low-level;hough transform;image noise;logic programming;map matching;mobile robot;norm (social);population;reflection mapping;sensor;situated	Luca Iocchi;Daniele Nardi	1999		10.1007/3-540-45327-X_26	hough transform;computer vision;simulation;computer science;robotics;mobile robot navigation	Robotics	57.32996062746856	-33.03236586864734	2371
e7312e4f66a19860b2168751bd0b90e6fe2897a3	probabilistic neural network based method for fault diagnosis of analog circuits	optimal method;data association;genetics;feature vector;analog circuits;wavelet packet decomposition;probabilistic neural network;fault diagnosis	A novel method for fault diagnosis of analog circuits with tolerance based on wavelet packet decomposition (WP) and probabilistic neural networks (PNN) is proposed in the paper. The fault feature vectors are extracted after feasible domains on the basis of WP decomposition of responses of a circuit is solved. Then by fusing various uncertain factors into probabilistic operations, parameters and structures of PNNs for diagnose faults are obtained based on genetic optimization method leading to best detection of faults. Finally, simulations indicated that PNN classifiers can correctly 7% more than BPNN of the test data associated with our sample circuits.	integrated circuit;probabilistic neural network	Yanghong Tan;Yigang He;Meirong Liu	2007		10.1007/978-3-540-72395-0_71	probabilistic neural network;feature vector;analogue electronics;computer science;machine learning;pattern recognition;wavelet packet decomposition;algorithm	ML	22.646839302659025	-24.14348047618236	2372
73c4fac7866f5001a13ad582c81f1b401fdc524c	mtibase: a database for decoding microrna target sites located within cds and 5′utr regions from clip-seq and expression profile datasets		MicroRNAs (miRNAs) play an important role in the regulation of gene expression. Previous studies on miRNA functions mainly focused on their target sites in the 3' untranslated regions (UTRs) of mRNAs. However, increasing evidence has revealed that miRNAs can also induce mRNA degradation and mediate translational repression via complementary interactions with the coding sequence (CDS) and 5'UTR of mRNAs. In this study, we developed a novel database, MtiBase, to facilitate the comprehensive exploration of CDS- and 5'UTR-located miRNA target sites identified from cross-linking immunoprecipitation sequencing (CLIP-Seq) datasets and to uncover their regulatory effects on mRNA stability and translation from expression profile datasets. By integrating 61 Argonaute protein-binding CLIP-Seq datasets and miRNA target sites predicted by five commonly used programs, we identified approximately 4 400 000 CDS-located and 470 000 5'UTR-located miRNA target sites. Moreover, we evaluated the regulatory effects of miRNAs on mRNA stability and translation using the data from 222 gene expression profiles, and 28 ribosome-protected fragment sequencing, and six pulsed stable isotope labeling with amino acids in culture. Finally, the effects of SNPs on the functions of miRNA target sites were systematically evaluated. Our study provides a useful tool for functional studies of miRNAs in regulating physiology and pathology. Database URL: http://mtibase.sysu.edu.cn.	3' untranslated regions;5' untranslated regions;amino acids;biopolymer sequencing;cross-linking immunoprecipitation high-throughput sequencing;elegant degradation;gene expression regulation;gene expression profiling;interaction;isotope labeling;isotopes;micrornas;open reading frames;repression, psychology;ribosomes;sequence number;single nucleotide polymorphism;translational repression;uniform resource locator;mrna degradation;physiological aspects;stable isotope	Zhi-Wei Guo;Chen Xie;Jian-Rong Yang;Junhao Li;Jianhua Yang;Limin Zheng	2015		10.1093/database/bav102	biology;molecular biology;bioinformatics;genetics	Comp.	4.467382831034941	-60.69388861255044	2373
2db4fcb27f2b53b0ce95bfc6c38a449e18e8803b	gain parameters based complex-valued backpropagation algorithm for learning and recognizing hand gestures	biological neural networks neurons backpropagation convergence joints backpropagation algorithms;gesture recognition backpropagation feedforward neural nets;feedforward neural network gain parameters complex valued backpropagation algorithm learning hand gesture recognition	In this paper, an improved complex-valued backpropagation algorithm with gain parameters is proposed. It is then employed to train a complex-valued feedforward neural network with one hidden layer. The well-trained complex-valued neural network is finally applied to deal with the recognition problem of 26 hand gestures. The results of experiment clearly show that much better performance can be achieved by our improved complex-valued backpropagation algorithm than some existing methods.	algorithm;artificial neural network;backpropagation;feedforward neural network;whole earth 'lectronic link	Yuanshan Liu;Tingwen Huang;Tingwen Huang	2014	2014 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2014.6889685	rprop;feedforward neural network;computer science;artificial intelligence;machine learning;pattern recognition;multilayer perceptron	ML	15.628752118663549	-28.99708411980701	2381
54686bcbe3ba5689b85c9f6b7fa82b4da37debf4	face recognition based on binary template matching	illumination;image matching;local adaptation;face recognition;binary template matching;facial expression;template matching;binary edge map;image similarity	In this paper, a novel face recognition method based on binary face edges is presented to deal with the illumination problem. The Binary Face Edge Map (BFEM) is extracted using the Locally Adaptive Threshold (LAT) algorithm. Based on BEFM, a new image similarity metric is proposed. Experimental results show that face recognition rates of 76.32% and 82.67% are achieved respectively on 798 AR images and 150 Yale images with changed lighting conditions and facial expression variations when one sample per subject is used as the target image. The proposed method takes less time for image matching and outperforms some existing face recognition approaches, especially in changed lighting conditions.	facial recognition system;template matching	Jiatao Song;Beijing Chen;Zheru Chi;Xuena Qiu;Wei Wang	2007		10.1007/978-3-540-74171-8_115	facial recognition system;computer vision;template matching;computer science;machine learning;pattern recognition;three-dimensional face recognition;mathematics;facial expression	Vision	36.08564832718135	-58.64377722858106	2382
47366cb4005f4def463f49b0d3003c270eda9893	possible role of cooperative action of nmda receptor and gaba function in developmental plasticity	developmental plasticity;spike time dependent plasticity;stdp;cortical plasticity;critical period;neocortex;synaptic competition;visual cortex;nmda receptor	The maturation of cortical circuits is strongly influenced by sensory experience during a restricted critical period. The developmental alteration in the subunit composition of NMDA receptors (NMDARs) has been suggested to be involved in regulating the timing of such plasticity. However, this hypothesis does not explain the evidence that enhancing GABA inhibition triggers a critical period in the visual cortex. Here, to investigate how the NMDAR and GABA functions influence synaptic organization, we examine an spike-timing-dependent plasticity (STDP) model that incorporates the dynamic modulation of LTP, associated with the activity- and subunit-dependent desensitization of NMDARs, as well as the background inhibition by GABA. We show that the competitive interaction between correlated input groups, required for experience-dependent synaptic modifications, may emerge when both the NMDAR subunit expression and GABA inhibition reach a sufficiently mature state. This may suggest that the cooperative action of these two developmental mechanisms can contribute to embedding the spatiotemporal structure of input spikes in synaptic patterns and providing the trigger for experience-dependent cortical plasticity.	anatomical maturation;cerebral cortex;desensitization (telecommunications);embedding;linux test project (ltp);long-term potentiation;modulation;n-methyl-d-aspartate receptors;n-methylaspartate;precipitating factors;synaptic package manager;gamma-aminobutyric acid;hypersensitivity desensitization	Shigeru Kubota;Tatsuo Kitajima	2010	Journal of Computational Neuroscience	10.1007/s10827-010-0212-0	psychology;synaptic plasticity;nmda receptor;neuroplasticity;developmental plasticity;homosynaptic plasticity;neuroscience;synaptic augmentation;developmental psychology;metaplasticity;nonsynaptic plasticity;spike-timing-dependent plasticity;communication;critical period;synaptic scaling	ML	18.561063321866733	-72.14054959787472	2386
50b258609e67359ae9db6e70efd9c9063728e417	identify critical genes in development with consistent h3k4me2 patterns across multiple tissues	gene expression bioinformatics proteins stem cells computational biology genomics;genomics;ieee transactions;stem cells;development;protein protein interaction differentiation development histone modification;gene expression;differentiation;lysine 4 residue pattern recognition methods epigenetic events gene functions protein protein interaction networks embryonic stem cells epithelial tissue nonblood connective tissue nervous tissue muscular tissue h3k4me2 distribution patterns mouse nervous tissue cells human cd4 t cells transcription starting sites histone 3 distribution pattern;proteins;protein protein interaction;histone modification;computational biology;proteins biological tissues cellular biophysics genetics molecular biophysics;bioinformatics	Histone modification is an important epigenetic event which plays essential roles in cell differentiation and tissue development. Recent studies show that a unique dimethylation of lysine 4 residue on histone 3 (H3K4me2) distribution pattern around transcription starting sites (TSS) of genes marks tissue specific genes in human CD4 + T cells and mouse nervous tissue cells. However, existence of this pattern has not been widely tested and its implication remains unclear. In this paper, we study the H3K4me2 distribution patterns across six different cell lines from five major tissue types (including muscular tissue, nervous tissue, non-blood connective tissue, blood, and epithelial tissue) as well as embryonic stem cells. We define a metric `tail length' to quantitatively describe H3K4me2 distribution patterns around the TSS. While confirming the previous observations, we also identified a group of 217 genes with ubiquitous long-tail H3K4me2 patterns in all the tested tissues and the embryonic stem cells (ESC). Further analyses confirmed that these genes are critical for development, and highly interactive with other tissue specific genes as evinced by protein-protein interaction networks, suggesting their critical regulatory functions. Our results suggest that rich information on gene functions and epigenetic events can be revealed using pattern recognition methods.	body tissue;cell differentiation process;connective tissue;cultured cell line;embryonic stem cells;epithelium;histocompatibility testing;histogenic process;histone h3 dimethyl lys4;histones;logical connective;long tail;lysine;muscle;nerve tissue;pattern recognition;toxic shock syndrome;transcription (software);protein protein interaction;study of epigenetics	Nan Meng;Raghu Machiraju;Kun Huang	2015	IEEE/ACM Transactions on Computational Biology and Bioinformatics	10.1109/TCBB.2015.2430340	protein–protein interaction;biology;genomics;molecular biology;gene expression;bioinformatics;histone;differentiation;genetics;cellular differentiation	ML	5.019308137643126	-62.746032748818685	2388
259655b76cd9f4a02669911e2d3c1ebf82238c0f	pseudo boosted deep belief network		A computationally efficient method to improve classification performance of a Deep Belief Network(DBN) is introduced. In the Pseudo Boost Deep Belief Network(PB-DBN), top layers are boosted while lower layers of the base classifiers share weights for feature extraction. PB-DBN maintains the same time complexity as a DBN with fast convergence to optimality by introducing the mechanism of pseudo boost. Experiments in classification show that after only a few iterations, the PB-DBN has higher accuracy than a classic DBN.	algorithmic efficiency;deep belief network;feature extraction;iteration;petabyte;time complexity	Tiehang Duan;Sargur N. Srihari	2016		10.1007/978-3-319-44781-0_13	time complexity;pattern recognition;artificial intelligence;machine learning;feature extraction;deep belief network;computer science;convergence (routing)	ML	19.038482803084246	-39.56814936578172	2394
9b6237de0f333d6d49c57a9d5e95ab549416d8ab	narrow fingerprint template synthesis by clustering minutiae descriptors			fingerprint;minutiae	Zhiqiang Hu;Dongju Li;Tsuyoshi Isshiki;Hiroaki Kunieda	2017	IEICE Transactions		computer vision;fingerprint verification competition;pattern recognition;computer science;artificial intelligence;cluster analysis;fingerprint;minutiae	Vision	34.78280298956662	-61.90998189120969	2398
2ac1823f384a20be34ad5a54b1382ec35a919750	an adaptive time-frequency filtering algorithm for multi-component lfm signals based on generalized s-transform	time frequency filtering;multi component lfm signal;time frequency analysis filtering filtering algorithms signal to noise ratio transforms noise reduction;generalized s transform time frequency filtering multi component lfm signal time frequency concentration characteristic;signal denoising feature extraction filters frequency modulation;time frequency concentration characteristic;generalized s transform;original signal features adaptive time frequency filtering algorithm multicomponent lfm signal cohen class bilinear time frequency distribution methods phase information clustered energy time frequency region extraction method time frequency distribution spectrum random noise components inverse generalized s transform signal denoising;ta engineering general civil engineering general	Recent studies show that Cohen class bilinear time-frequency distribution methods do not have satisfactory denoising performance when analyzing multi-component LFM signals. This paper has constructed a new adaptive time-frequency filtering factor and has proposed an adaptive time-frequency filtering algorithm based on generalized S-transform. Firstly, the time-frequency distribution is obtained by transforming the time domain signals to time-frequency domain by using generalized S-transform, which is followed by calculating instantaneous frequency based on the phase information from the time-frequency distribution. Secondly, the time-frequency distribution regions occupied by clustered energy of effective signal are identified through time-frequency region extraction method and all time-frequency distribution spectrum out of the regions are removed. Thirdly, a novel TF filtering factor is constructed by the time-frequency concentration characteristic to restrain the random noise components in the regions of effective signal. Finally, the filtered signals are retrieved by using inverse generalized S-transform. Simulation results demonstrate that the proposed filtering algorithm has satisfactory performances for signal denoising which most features of original signal can be remained.	algorithm;bilinear filtering;bilinear time–frequency distribution;content-control software;instantaneous phase;noise (electronics);noise reduction;performance;s transform;signal processing;signal-to-noise ratio;simulation;tf–idf;time–frequency analysis;time–frequency representation	Dianwei Wang;Jing Wang;Ying Liu;Zhijie Xu	2015	2015 21st International Conference on Automation and Computing (ICAC)	10.1109/IConAC.2015.7314000	filter;speech recognition;frequency domain;statistics	Robotics	81.35474437030334	-40.19179053728699	2400
2f9745db9b56714dd8028b84aaf2ca1ab306e9f2	multi-objective optimization of reliability-redundancy allocation problem with cold-standby strategy using nsga-ii			mathematical optimization;multi-objective optimization	Mostafa Abouei Ardakan;Mohammad Taghi Rezvan	2018	Rel. Eng. & Sys. Safety	10.1016/j.ress.2017.12.019		DB	18.312170669672746	-3.366024683424616	2404
fddcb2c5733f3b93f356c4dc9ef32f0208205499	tracking control for inverted orbital stabilization of inertia wheel pendulum—trajectory generation, stability analysis, and experiments	relay control;second order;underactuated system;robust tracking controller design inverted orbital stabilization inertia wheel pendulum trajectory generation stability analysis second order sliding mode control tracking control problem open loop system nonminimum phase system oscillations scalar output underactuated system amplitude frequency reference model two relay controller;oscillations;frequency control;oscillators;reference model;robust control;open loop systems;variable structure systems;wheels trajectory oscillators friction steady state relays equations;orbital stability;trajectory;trajectory generation;control system synthesis;variable structure systems control system synthesis frequency control open loop systems pendulums relay control robust control tracking trajectory control;stability analysis;tracking control;relays;pendulums;friction;trajectory control;tracking;sliding mode control;wheels;steady state;non minimum phase system	A second order sliding mode control is developed to solve the tracking control problem for an inertia wheel pendulum. The desired trajectory is centered at the upright position where the open-loop system becomes a non-minimum-phase system. We have recently proved that a two relay controller produces oscillations at the scalar output of an underactuated system where the desired amplitude and frequency are reached by choosing its gains. In this paper we go much further and develop a reference model, based on two-relay controller, to generate a set of desired trajectories for the inertia wheel pendulumallowing us to design a robust tracking controller. Performance issues of the constructed controller are illustrated in an experimental study.	experiment;minimum phase;molecular orbital;prototype;reference model;relay;scroll wheel;underactuation	Victor Juárez;Luis T. Aguilar;Rafael Iriarte	2011	2011 8th International Conference on Electrical Engineering, Computing Science and Automatic Control	10.1109/ICEEE.2011.6106680	control engineering;simulation;engineering;control theory	Robotics	65.93692808561325	-15.814032843430912	2408
0d9decf6e3ce9c4f2239b9f0731ab9107ade9b7e	mind the class weight bias: weighted maximum mean discrepancy for unsupervised domain adaptation		In domain adaptation, maximum mean discrepancy (MMD) has been widely adopted as a discrepancy metric between the distributions of source and target domains. However, existing MMD-based domain adaptation methods generally ignore the changes of class prior distributions, i.e., class weight bias across domains. This remains an open problem but ubiquitous for domain adaptation, which can be caused by changes in sample selection criteria and application scenarios. We show that MMD cannot account for class weight bias and results in degraded domain adaptation performance. To address this issue, a weighted MMD model is proposed in this paper. Specifically, we introduce class-specific auxiliary weights into the original MMD for exploiting the class prior probability on source and target domains, whose challenge lies in the fact that the class label in target domain is unavailable. To account for it, our proposed weighted MMD model is defined by introducing an auxiliary weight for each class in the source domain, and a classification EM algorithm is suggested by alternating between assigning the pseudo-labels, estimating auxiliary weights and updating model parameters. Extensive experiments demonstrate the superiority of our weighted MMD over conventional MMD for domain adaptation.	discrepancy function;domain adaptation;expectation–maximization algorithm;experiment;glossary of computer graphics;mikumikudance	Hongliang Yan;Yukang Ding;Peihua Li;Qilong Wang;Yong Xu;Wangmeng Zuo	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2017.107	machine learning;artificial intelligence;kernel (linear algebra);open problem;prior probability;pattern recognition;computer science;domain adaptation;expectation–maximization algorithm	Vision	23.242156845459363	-43.067302451558746	2412
2dfeb511bc23bfb4219b322a8336f4abcc3294c6	prediction unit depth selection based on statistic distribution for hevc intra coding	variance distribution hevc intra prediction pu depth selection;variances distribution;image coding;complexity theory;standards;prediction unit depth selection;tree codes;reference encoder;prediction algorithms;pu depth selection;coding unit;partition decision;pixel blocks;compression performance;hevc;intra prediction;video coding;statistical distributions;prediction theory;ultra hd videos;computational complexity;variance distribution;statistic distribution;coding tree structure;encoding prediction algorithms video coding complexity theory algorithm design and analysis image coding standards;hevc intra coding;encoding;video coding standard;ultra hd videos prediction unit depth selection statistic distribution hevc intra coding video coding standard compression performance computational complexity coding tree structure partition decision mode decision coding unit pixel blocks variances distribution encoding reference encoder coding performance loss;mode decision;block codes;algorithm design and analysis;video coding block codes computational complexity prediction theory statistical distributions tree codes;coding performance loss	The new video coding standard, HEVC, achieves better compression performance relative to H.264 at the expense of a significant increase in computational complexity. It adopts coding tree structure, and implements partition and mode decision for each coding unit (CU) and prediction unit (PU). For intra coding, five levels of PUs from 64×64 to 4×4 should be searched and each PU selects one best mode from 35 candidate modes, which is very computation intensive. Therefore, reducing the number of levels for PUs, i.e., PU depth selection, is an efficient way to decrease intra coding complexity. In this paper, we fully investigate the relationship between the variances of different sizes of pixel blocks and the best partition of PUs. It is shown that 90th percentile of the variances distribution can be used to distinguish large PUs and small PUs. Furthermore, 16×16 PU search can be disabled for smooth area when variances of 64×64 pixel blocks are smaller than the 90th percentile of variances distribution of 64×64 PUs and this 90th percentile is smaller than an absolute threshold. Meanwhile, further split can be disabled if the variance of the current PU is smaller than 70th percentile of the variances of its parent-level PUs when the current PU is 16×16 or 8×8. The experimental results show that the proposed algorithm can reduce 55% encoding time of reference encoder HM10.0 at all Intra setting on average with negligible coding performance loss less than 1.22% BD-Rate for HD or Ultra-HD videos.	algorithm;blu-ray;computation;computational complexity theory;data compression;encoder;h.264/mpeg-4 avc;high efficiency video coding;huffman coding;ibm systems network architecture;intra-frame coding;one-class classification;pixel;tip (unix utility);tree structure;video coding format	Xue Li;Xiaoyun Zhang;Yunyu Shi;Zhiyong Gao	2014	2014 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)	10.1109/ICMEW.2014.6890719	block code;probability distribution;algorithm design;real-time computing;prediction;computer science;theoretical computer science;pattern recognition;computational complexity theory;encoding;statistics	Robotics	46.74436269859427	-19.359377825879555	2419
d6557855f03aa582cf0ecd8a666b0bb182d835e8	point image representation for efficient detection of vehicles		This paper presents method of image conversion into point image representation. Conversion is carried out with the use of small image gradients. Layout of binary values of point image representation is in accordance with the edges of objects comprised in the source image. Vehicles are detected through analysis of the detection field state. The state of the detection field is determined on the basis of the sum of the edge points within the detection field. The proposed method of vehicle detection is fast and simple computationally. Vehicle detection with the use of image conversion into point image representation is efficient and can by used in real-time processing. Experimental results are provided.		Zbigniew Czapla	2015		10.1007/978-3-319-26227-7_65	computer vision;mathematical optimization;feature detection;pattern recognition	Vision	40.9666006281719	-54.234442337287184	2426
906125c0d51b328d9ab4e54bb7b49592a5763648	computerized mouse pupil size measurement for pupillary light reflex analysis	randomized hough transform;image processing;functional testing;pupil size;clinical diagnosis;pupillary light reflex;mean absolute difference;infrared;region growing;object detection	Accurate measurement of pupil size is essential for pupillary light reflex (PLR) analysis in clinical diagnosis and vision research. Low pupil-iris contrast, corneal reflection, artifacts and noises in infrared eye imaging pose challenges for automated pupil detection and measurement. This paper describes a computerized method for pupil detection or identification. After segmentation by a region-growing algorithm, pupils are detected by an iterative randomized Hough transform (IRHT) with an elliptical model. The IRHT iteratively suppresses the effects of extraneous structures and noise, yielding reliable measurements. Experimental results with 72 images showed a mean absolute difference of 3.84% between computerized and manual measurements. The inter-run variation for the computerized method (1.24%) was much smaller than the inter-observer variation for the manual method (7.45%), suggesting a higher level of consistency of the former. The computerized method could facilitate PLR analysis and other non-invasive functional tests that require pupil size measurements.		Wei Lu;Jinglu Tan;Keqing Zhang;Bo Lei	2008	Computer methods and programs in biomedicine	10.1016/j.cmpb.2008.01.002	computer vision;infrared;image processing;computer science;machine learning;functional testing;region growing;mean difference;statistics;computer graphics (images)	Vision	39.700400848454414	-76.26815566856013	2433
ce90632d814821a0977b6472843a90d258e88548	enhancing duplicate collection detection through replica boundary discovery	documento electronico;web documents;fonction potentiel;web pages;red www;analisis datos;edge detection;base donnee tres grande;chainage donnee;reseau web;service web;web based information system;large data sets;web service;data mining;similitude;deteccion contorno;document electronique;detection contour;data analysis;internet;fouille donnee;decouverte connaissance;data link;funcion potencial;similarity;world wide web;descubrimiento conocimiento;analyse donnee;similitud;very large databases;potential function;busca dato;electronic document;servicio web;document similarity;knowledge discovery;ligazon datos	Web documents are widely replicated on the Internet. These replicated documents bring potential problems to Web based information systems. So replica detection on the Web is an indispensable task. The challenge is to find these duplicated collections from a very large data set with limited hardware resources in acceptable time. In this paper, we first introduce the notion of replica boundary to roughly reflect the situation of the replicas; then we propose an effective and efficient approach to discover the boundary of the replicas. The advantages of the proposed approach include: first, it dramatically reduces pair-wise document similarity computation, making it much faster than traditional replicated document detection approaches; second, it can identify the boundary of the replicated collections accurately, demonstrating to what extent two collections are replicated. On two web page sets containing 24 million and 30 million Web pages respectively, we evaluated the accuracy of the approach.	computation;directory (computing);fast software encryption;information system;internet;web page;world wide web	Zhigang Zhang;Weijia Jia;Xiaoming Li	2006		10.1007/11731139_42	web service;the internet;edge detection;similarity;data link;computer science;similitude;web page;data mining;database;knowledge extraction;data analysis;world wide web	Web+IR	-3.9564597770744743	-33.27055443821367	2435
777c27eafcbe64501a8a7b3327a4f6c7e0c28583	biomimetic dexterous hands: human like multi-fingered robotics hand control	motion control;robot hand;thumb force robot sensing systems humans grasping;manipulation skills biomimetic robot hands;dexterous manipulators;design and implementation;manipulation skills;motion control biomimetics dexterous manipulators;biomimetic dexterous robotics hand multifingered robot robot hand control skillful manipulation behavior bio oriented finger fingertip force computation knowledge based approach;biomimetic;robot hands;biomimetics;knowledge base	For advanced and skillful manipulation behaviors and dexterities, an innovative phase of robotics hands has been developed recently with Biomimeticly oriented functionalities. Within this manuscript, we shall survey in details biomimetic based dexterous robotics hands designs and implementations. In particular, the study focuses on a number of developments that have been evolving and taking place over the last decade in recent development and technologies related to this vital filed of research. In terms of bio-oriented fingers, the article, furthermore, looks in details at the complicated issue of fingertips forces computation and distribution, and how they are balanced using a knowledge based approach for BIOMIMETIC Dexterous Robotics Hands.	algorithm;biomimetics;british informatics olympiad;computation;robotics;venue (sound system)	Ebrahim Mattar	2012	2012 UKSim 14th International Conference on Computer Modelling and Simulation	10.1109/UKSim.2012.35	control engineering;simulation;engineering;artificial intelligence	Robotics	66.98654730382985	-27.874985245321295	2436
b9f9ca118e19bd482315857938ea02cc9e90b6f9	a novel subnetwork alignment approach predicts new components of the cell cycle regulatory apparatus in plasmodium falciparum	escherichia coli;computational biology bioinformatics;plasmodium falciparum;algorithms;cell cycle;combinatorial libraries;protein interaction maps;computer appl in life sciences;protozoan proteins;microarrays;bioinformatics	"""According to the World Health organization, half the world's population is at risk of contracting malaria. They estimated that in 2010 there were 219 million cases of malaria, resulting in 660,000 deaths and an enormous economic burden on the countries where malaria is endemic. The adoption of various high-throughput genomics-based techniques by malaria researchers has meant that new avenues to the study of this disease are being explored and new targets for controlling the disease are being developed. Here, we apply a novel neighborhood subnetwork alignment approach to identify the interacting elements that help regulate the cell cycle of the malaria parasite Plasmodium falciparum. Our novel subnetwork alignment approach was used to compare networks in Escherichia coli and P. falciparum. Some 574 P. falciparum proteins were revealed as functional orthologs of known cell cycle proteins in E. coli. Over one third of these predicted functional orthologs were annotated as """"conserved Plasmodium proteins"""" or """"putative uncharacterized proteins"""" of unknown function. The predicted functionalities included cyclins, kinases, surface antigens, transcriptional regulators and various functions related to DNA replication, repair and cell division. The results of our analysis demonstrate the power of our subnetwork alignment approach to assign functionality to previously unannotated proteins. Here, the focus was on proteins involved in cell cycle regulation. These proteins are involved in the control of diverse aspects of the parasite lifecycle and of important aspects of pathogenesis."""	antigens, heterophile;cell (microprocessor);cell cycle control;cell division;cellular phone;cessation of life;contract agreement;cyclins;dna replication;economic burden;high-throughput computing;homology (biology);interaction;leukemia, b-cell;malaria vaccines;plasmodium falciparum ag:acnc:pt:bld:qn:if;sequence alignment;subnetwork;throughput;transcription, genetic	Hong Cai;Changjin Hong;Timothy G. Lilburn;Armando L. Rodriguez;Sheng Chen;Jianying Gu;Rui Kuang;Yufeng Wang	2013		10.1186/1471-2105-14-S12-S2	biology;dna microarray;bioinformatics;virology;cell cycle;escherichia coli;genetics	Comp.	5.320993788738484	-60.18240243291791	2443
5f3abafa6da4699689b4524bfb91a32307246b80	eos mls forward model polarized radiative transfer for zeeman-split oxygen lines	baja presion;absorption;correlacion;teledetection spatiale;microwave limb sounder mls;absorcion;radiance;microwave limb sounder;radiative transfer;space remote sensing;magnetic field;atmospheric composition;relative orientation;oxygene;forward model;oxygen;polarization;hyperfrequence;direccion;campo magnetico;orientation;temperatura;cross polarization;zeeman earth observing system eos aura forward model microwave limb sounder mls;oxigeno;microwaves;atmospheric measuring apparatus;polarizacion;radiacion;algorithme;teledeteccion espacial;modelo;champ magnetique;eos;geomagnetic field;direction;rayonnement;zeeman effect atmospheric measuring apparatus oxygen artificial satellites aerospace instrumentation atmospheric composition atmospheric temperature atmospheric spectra radiative transfer;zeeman effect;artificial satellites;orientacion;algorithms;basse pression;modele;o sub 2 earth observing system microwave limb sounder aura stellite eos mls forward model polarized radiative transfer oxygen spectral line zeeman splitting collisional broadening radiation polarization mode limb radiance atmospheric temperature atmospheric composition spectral line position spectral line strength 118 75 ghz;low pressure;mls;luminance energetique;polarisation;correlation;temperature;earth observing system eos aura;atmospheric spectra;earth observing system;earth observing system multilevel systems polarization temperature geomagnetism atmospheric modeling electromagnetic wave absorption magnetic fields spectroscopy receiving antennas;zeeman;models;atmospheric temperature;aerospace instrumentation;champ geomagnetique;radiation;aura;algoritmo	This work supplements the Earth Observing System (EOS) Microwave Limb Sounder (MLS) clear-sky unpolarized forward model with algorithms for modeling polarized emission from the Zeeman-split 118.75-GHz O/sub 2/ spectral line. The model accounts for polarization-dependent emission and for correlation between polarizations with complex, 2/spl times/2 intensity and absorption matrices. The oxygen line is split into three Zeeman components by the interaction of oxygen's electronic spin with an external magnetic field, and the splitting is of order /spl plusmn/0.5 MHz in a typical geomagnetic field. Zeeman splitting is only significant at pressures low enough that collisional broadening (/spl sim/1.6 MHz/hPa) is not very large by comparison. The polarized forward model becomes significant for MLS temperature retrievals at pressure below 1.0 hPa and is crucial at pressures below /spl sim/0.03 hPa. Interaction of the O/sub 2/ molecule with the radiation field depends upon the relative orientation of the radiation polarization mode and the geomagnetic field direction. The model provides both limb radiances and the derivatives of these radiances with respect to atmospheric temperature and composition, as required by MLS temperature retrievals. EOS MLS views the atmospheric limb at 118.75 GHz with a pair of linear-cross-polarized, 100-kHz-resolution, 10-MHz-wide spectrometers. The antennas of the associated receivers are scanned to view rays with tangent heights from the Earth's surface to 0.001 hPa. Comparisons of the modeled MLS radiances with measurements show generally good agreement in line positions and strengths, however residuals in the line centers at the highest tangent heights are larger than desired and still under investigation.	algorithm;computational resource;coupling (physics);eos;line level;microwave;null (sql);polarization (waves);polarizer;precomputation;set splitting problem;software propagation;time complexity	Michael J. Schwartz;William G. Read;W. Van Snyder	2006	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2005.862267	meteorology;polarization;zeeman effect;optics;physics;quantum mechanics;remote sensing	Visualization	83.75878671122848	-64.69652656285857	2444
36b83eb3839fa842536f74b95b59c12482348d7d	machine learning algorithm selection for forecasting behavior of global institutional investors	economic forecasting;forecasting behavior;early warning system;long term;forecasting problem;stock market;various important issue;global institutional investors;learning (artificial intelligence);pattern classification;korean stock market;short term forecasting;lag l classifier;global institutional investor behavior forecasting;stock markets;various machine;proper machine;global institutional investor;investment;machine learning algorithm selection;machine learning;empirical study;institutional investors	Recently Son et al. [32] proposed early warning system (EWS) monitoring the behaviors of global institutional investors (GII) against their possible massive pullout from the local emerging stock market. They used machine learning algorithm for lag l classifier to forecast the behavior of GII. The main aim of this article is to implement various machine learning algorithms in constructing the EWS and to compare their performances to select the proper one. Our results address various important issues for machine learning forecasting problem. In particular, a proper machine learning algorithm will be recommended for both long term and short term forecasting. This is empirically studied for the Korean stock market.	algorithm selection;internet;machine learning;performance	Jae Joon Ahn;Suk Jun Lee;Kyong Joo Oh;Tae Yoon Kim;Hyoung Yong Lee;Min Sik Kim	2009	2009 42nd Hawaii International Conference on System Sciences	10.1109/HICSS.2009.812	actuarial science;computer science;marketing;warning system;institutional investor;empirical research;commerce	ML	5.749159353027543	-19.047950152007573	2452
4a414ef78342e4023e5cff458bf15ac5ae749f89	gradient descent learning for quaternionic hopfield neural networks		A Hopfield neural network (HNN) is a neural network model with mutual connections. A quaternionic HNN (QHNN) is an extension of HNN. Several QHNN models have been proposed. The hybrid QHNN utilizes the noncommutativity of quaternions. It has been shown that hybrid QHNNs with the Hebbian learning rule outperformed QHNNs in noise tolerance. The Hebbian learning rule, however, is a primitive learning algorithm, and it is necessary that we study advanced learning algorithms. Although the projection rule is one of a few promising learning algorithms, it is restricted by network topologies and cannot be applied to hybrid QHNNs. In the present work, we propose gradient descent learning, which can be applied to hybrid QHNNs. We compare the performance of gradient descent learning with that of projection rule. Results showed that the gradient descent learning outperformed projection rule in noise tolerance in computer simulations. For small training-pattern sets, hybrid QHNNs with gradient descent learning produced the best performances. QHNNs did so for large training-pattern set. In future, gradient descent learning will be extended to QHNNs with different network topology and activation function.	activation function;algorithm;artificial neural network;computer simulation;gradient descent;hebbian theory;hopfield network;learning rule;machine learning;network model;network topology;performance	Masaki Kobayashi	2017	Neurocomputing	10.1016/j.neucom.2017.04.025	mathematical optimization;types of artificial neural networks;artificial intelligence;recurrent neural network;backpropagation;machine learning;mathematics;hopfield network	ML	15.305572903067036	-28.213795063983582	2457
dd1f218a17c6de42adc91178044d8f292bf0a147	satellite hyperspectral remote sensing for estimating estuarine and coastal water quality	estuarine water quality;colored dissolved organic matter;contraste;hyperion;australasie;spatial gradients;teledetection spatiale;queensland australia;radiative transfer model;underwater light field satellite hyperspectral remote sensing estuarine water quality coastal water quality hyperion data eastern australia moreton bay southern queensland spatial gradients optical depth water quality bathymetry substrate composition turbid river inputs humic river inputs open ocean flushing optical properties substrate reflectance spectra colored dissolved organic matter chlorophyll suspended matter radiative transfer models;australie;modele numerique;batimetria;propriete optique;coastal waters;high resolution;geochemistry;humic river inputs;satellite hyperspectral remote sensing;space remote sensing;rivers;resolution spatiale;calidad agua;concentracion;substrat;optical noise;pouvoir reflecteur;satellites hyperspectral sensors remote sensing sea measurements optical sensors optical noise australia spatial resolution testing earth;color;oceanographic techniques oceanographic regions sediments geochemistry;earth;light field;underwater light field;poder reflector;land application;estuario;hyperspectral sensors;oceanographic regions;testing;depth;optical depth;matrix inversion;indexing terms;problema inverso;water quality;sediments;chlorophylle;coastal environment;substrate reflectance spectra;radiative transfer models;teledeteccion espacial;estuaire;haute resolution;baie moreton;suspended matter;inverse problem;queensland;reflectance;coastal water quality;turbid river inputs;bahia moreton;rio;remote sensing;satellites;bruit;riviere;medio litoral;optical properties;eo 1;alta resolucion;profundidad;satellite remote sensing;hyperspectral remote sensing;australasia;couleur;earth observation;etalonnage;southern queensland;hyperion data;imaging spectrometry;optical sensors;estuaries;substrate composition;substrates;airborne remote sensing;transfert radiatif;profondeur;numerical models	The successful launch of Hyperion in November 2000 bridged the gap between the high-resolution (spatial and spectral) airborne remote sensing and the lower resolution satellite remote sensing. Although designed as a technical demonstration for land applications, Hyperion was tested for its capabilities over a range of water targets in Eastern Australia, including Moreton Bay in southern Queensland. Moreton Bay was the only Australian Earth Observing 1 (EO-1) Hyperion coastal site used for calibration/validation activities. This region was selected due to its spatial gradients in optical depth, water quality, bathymetry, and substrate composition. A combination of turbid and humic river inputs, as well as the open ocean flushing, determines the water quality of the bay. The field campaigns were coincident with Hyperion overpasses, retrieved inherent optical properties, apparent optical properties, substrate reflectance spectra, and water quality parameters. Environmental noise calculations demonstrate that Hyperion has sufficient sensitivity to detect optical water quality concentrations of colored dissolved organic matter, chlorophyll, and suspended matter in the complex waters of Moreton Bay. A methodology was developed integrating atmospheric and hydrooptical radiative transfer models (MODTRAN-4, Hydrolight) to estimate the underwater light field. A matrix inversion method was applied to retrieve concentrations of chlorophyll, colored dissolved organic matter, and suspended matter, which were comparable to those estimated in the field on the days of the overpass.	airborne ranger;bathymetry;gradient;hyperion;image resolution;inverse transform sampling;light field	Vittorio E. Brando;Arnold G. Dekker	2003	IEEE Trans. Geoscience and Remote Sensing	10.1109/TGRS.2003.812907	meteorology;image resolution;hydrology;physics;water remote sensing;remote sensing	Robotics	83.29692717936375	-63.646433986828654	2463
15d1582c8b65dbab5ca027467718a2c286ddce7a	on robust face recognition via sparse coding: the good, the bad and the ugly	170205 neurocognitive patterns and neural networks;local image patches;multiple region descriptors;010200 applied mathematics;visual databases face recognition gaussian processes image coding minimisation neural nets;sparse signals;spatial relations;l1 minimisation;080109 pattern recognition and data mining;080106 image processing;080104 computer vision;holistic descriptors;080100 artificial intelligence and image processing;gaussian mixture models;implicit probabilistic technique;sparse autoencoder neural network;010401 applied statistics;sr encoding;local sr approach;exyaleb datasets;080602 computer human interaction;ar datasets;image deformations;010406 stochastic analysis and modelling;feret datasets;sparse representation;sparse coding;face verification scenario;closed set identification applications;banca datasets;chokepoint datasets;local sr approach robust face recognition sparse coding sparse representation holistic descriptors closed set identification applications face verification scenario sr encoding local image patches sparse signals multiple region descriptors spatial relations image deformations l 1 minimisation sparse autoencoder neural network sann implicit probabilistic technique gaussian mixture models chokepoint datasets banca datasets exyaleb datasets feret datasets ar datasets;robust face recognition;sann	In the field of face recognition, Sparse Representation (SR) has received considerable attention during the past few years. Most of the relevant literature focuses on holistic descriptors in closed-set identification applications. The underlying assumption in SR-based methods is that each class in the gallery has sufficient samples and the query lies on the subspace spanned by the gallery of the same class. Unfortunately, such assumption is easily violated in the more challenging face verification scenario, where an algorithm is required to determine if two faces (where one or both have not been seen before) belong to the same person. In this paper, we first discuss why previous attempts with SR might not be applicable to verification problems. We then propose an alternative approach to face verification via SR. Specifically, we propose to use explicit SR encoding on local image patches rather than the entire face. The obtained sparse signals are pooled via averaging to form multiple region descriptors, which are then concatenated to form an overall face descriptor. Due to the deliberate loss spatial relations within each region (caused by averaging), the resulting descriptor is robust to misalignment and various image deformations. Within the proposed framework, we evaluate several SR encoding techniques: l1-minimisation, Sparse Autoencoder Neural Network (SANN), and an implicit probabilistic technique based on Gaussian Mixture Models. Thorough experiments on AR, FERET, exYaleB, BANCA and ChokePoint datasets show that the proposed local SR approach obtains considerably better and more robust performance than several previous state-of-the-art holistic SR methods, in both verification and closed-set identification problems. The experiments also show that l1-minimisation based encoding has a considerably higher computational cost when compared to SANN-based and probabilistic encoding, but leads to higher recognition rates.	ar (unix);artificial neural network;autoencoder;computation;computational complexity theory;concatenation;experiment;feret (facial recognition technology);facial recognition system;holism;information retrieval;mixture model;molecular descriptor;neural coding;randomized algorithm;sparse matrix	Yongkang Wong;Mehrtash Tafazzoli Harandi;Conrad Sanderson	2014	IET Biometrics	10.1049/iet-bmt.2013.0033	spatial relation;computer vision;computer science;machine learning;pattern recognition;mixture model;sparse approximation;data mining;neural coding	Vision	28.787574861204842	-47.21220991993853	2466
6f342b7bb6ff5d0147835eb89d8dacd0019c2ffa	hybrid sampling strategy-based multiobjective evolutionary algorithm for process planning and scheduling problem	期刊论文;hybrid sampling;multiobjective optimization;evolutionary algorithm;process planning and scheduling;vector evaluated genetic algorithm vega	Process planning and scheduling (PPS) is a most important, practical but very intractable problem in manufacturing systems. Many research works use multiobjective evolutionary algorithm (MOEA) to solve such problems; however, they cannot achieve satisfactory results in both quality and speed. This paper proposes a hybrid sampling strategy-based evolutionary algorithm (HSS-EA) to deal with PPS problem. HSS-EA tactfully unites the advantages of vector evaluated genetic algorithm (VEGA) and a sampling strategy according to Pareto dominating and dominated relationship-based fitness function (PDDR-FF). The sampling strategy of VEGA prefers the edge region of the Pareto front and PDDR-FF-based sampling strategy has the tendency converging toward the central area of the Pareto front. These two mechanisms preserve both the convergence rate and the distribution performance. Numerical comparisons show that the convergence performance of HSS-EA is better than NSGA-II and SPEA2 while the distribution performance is slightly better or equivalent, and the efficiency is obviously better than they are.	automated planning and scheduling;computation;computational complexity theory;crossover (genetic algorithm);evolutionary algorithm;fitness function;general purpose serial interface;genetic algorithm;high-speed serial interface;job shop scheduling;local search (optimization);moea framework;makespan;multi-objective optimization;numerical analysis;numerical method;pareto efficiency;performance;rate of convergence;sampling (signal processing);scheduling (computing);time complexity;tournament selection	Wenqiang Zhang;Mitsuo Gen;Jungbok Jo	2014	J. Intelligent Manufacturing	10.1007/s10845-013-0814-2	mathematical optimization;simulation;computer science;engineering;artificial intelligence;multi-objective optimization;evolutionary algorithm	AI	22.57695430047405	-1.0130292046702143	2468
68243004242ab2803b9fedd1f75f8a592d4069eb	grouping crowd-sourced mobile videos for cross-camera tracking	video signal processing;sorting;mobile camera networks;video tracking;computer vision;video signal processing image reconstruction mobile handsets object tracking sorting video cameras;video cameras;youtube public adoption camera equipped mobile phones video upload online viewing wide area surveillance systems event reconstruction end to end method automatic cross camera tracking crowd sourced mobile video data video sorting overlapping space time groups intercamera relationships tracked objects;image reconstruction;object tracking;crowd source video;mobile handsets;cameras videos visualization target tracking correlation youtube;scene understanding;scene understanding computer vision crowd source video mobile camera networks video tracking	Public adoption of camera-equipped mobile phones has given the average observer of an event the ability to capture their perspective and upload the video for online viewing (e.g. YouTube). When traditional wide-area surveillance systems fail to capture an area or time of interest, crowd-sourced videos can provide the information needed for event reconstruction. This paper presents the first end-to-end method for automatic cross-camera tracking from crowd-sourced mobile video data. Our processing (1) sorts videos into overlapping space-time groups, (2) finds the inter-camera relationships from objects within each view, and (3) provides an end user with multiple stabilized views of tracked objects. We demonstrate the system's effectiveness on a real dataset collected from YouTube.	crowdsourcing;digital camera;end-to-end encryption;global network;map projection;match moving;mathematical optimization;mobile phone;temporal logic;upload;virtual reality headset	Nathan Frey;Matthew Antone	2013	2013 IEEE Conference on Computer Vision and Pattern Recognition Workshops	10.1109/CVPRW.2013.120	video compression picture types;computer vision;video production;computer science;video capture;video tracking;professional video camera;multimedia;video processing;computer graphics (images)	Vision	47.39162662885893	-45.43011006001016	2469
daa4cfde41d37b2ab497458e331556d13dd14d0b	multi-view constrained local models for large head angle facial tracking	shape training mathematical model face magnetic heads active appearance model;magnetic heads;training;active appearance model;real time systems face recognition;shape;real time face tracking multiview constrained local models large head angle facial tracking facial point detection car driving setting global shape model shape space division strategies;mathematical model;face	We propose Multi-View Constrained Local Models - a simple but effective technique for improving facial point detection under large head angles, such as in a car driving setting. Our approach combines a global shape model with separate sets of response maps targeted at different head angles, indexed on the shape model parameters. We explore shape-space division strategies and show that, as well as outperforming the traditional method, our approach also provides a marked speed-up which demonstrates the suitability of this technique for real-time face tracking.	algorithmic efficiency;map;real-time clock;yaws	Georgia Rajamanoharan;Timothy F. Cootes	2015	2015 IEEE International Conference on Computer Vision Workshop (ICCVW)	10.1109/ICCVW.2015.128	active shape model;face;computer vision;active appearance model;simulation;shape;mathematical model;mathematics;geometry;computer graphics (images)	Vision	47.4977397722153	-48.75151719223094	2481
c7d9914b3bdf320c4d80893017441e6bbfd48634	a new ignition model for spark-ignited engine simulations	parallel computing;engine performance;engine combustion;computer model;spark ignition;computation fluid dynamics;computational fluid dynamics;spark ignition modeling;spatial distribution;parallel computer;navier stokes equation;engine combustion simulations	The amount of spark energy deposited into the combustion chamber is key to an optimum ignition as one can end up with misfires when this energy is low or with other undesired effects on engine performance and byproducts when it is high. Experimentally, up to now, no one has been able to correlate the combustion outcome accurately to the spark parameters in a controllable way. Theoretical investigation and computer modeling is leading to a better understanding of how spark flames propagate. A new computational approach to ignition dynamics is presented here for spark-ignited (SI) engine combustion simulations. Our computational model, using the MPI communication library, attempts to solve temporal and spatial equations of the electromagnetic (EM) equations in conjunction with the well-known Navier-Stokes equations of the standard KIVA-3 engine code. The interaction between the gas and the flame (plasma) kernel in the spark region is computed through the momentum and energy exchange between these two fields. Preliminary results show a distinct spatial distribution of physical quantities at the flame front and within the inflammation zone. A slight change in the spark discharge current has significant impact on the combustion and emissions. Enhanced accuracy of spark ignition modeling might help us better compute the early flame propagation and its influence on the cyclic variability of engines, potentially leading to design of new spark plugs.	simulation	O. Yaar	2001	Parallel Computing	10.1016/S0167-8191(00)00094-6	computer simulation;parallel computing;simulation;computational fluid dynamics;computer science;ignition timing;homogeneous charge compression ignition	HPC	92.91547926874085	-0.16296751120174394	2483
f694bc37599e19a68d69fb7a43628b28725171c0	sroc: a speaker recognition with data decision level fusion method in cloud environment	speaker recognition;sroc;d-s evidence theory;data fusion;cloud computing	Speaker recognition as one of biometrics techniques is to recognize speaker’s identity. A robust method for speaker recognition with high accuracy of recognition rate is the aim for all relevant researchers. With the rapid development of cloud computing, many complicated tasks in speaker recognition system, such as machine learning, data processing, and information recording, can be implemented in cloud. Meanwhile, speaker recognition can be used as a security and privacy service which are extremely important for the popularization of cloud computing. In this paper, we propose a data decision level fusion method for speaker recognition in cloud environment. After introducing the basic processing of speech recognition process and describing the composition of the speech recognition system based on Dempster-Shafer Evidence Theory of decision level data fusion method, we present a speaker recognition system of cloud computing (SROC) architecture. Through experimental evaluation, SROC can improve the recognition rate and accuracy greatly.	speaker recognition	Ning Jiang;Meikang Qiu;Wenyun Dai	2017	Signal Processing Systems	10.1007/s11265-015-1100-7	speaker recognition;speaker diarisation;speech recognition;intelligent character recognition;computer science;pattern recognition	Robotics	29.876199109842794	-63.45863983510942	2485
5222ac8c5ee4de85189bf0ac562964c4bf4cbda6	inference for circular distributions and processes	data augmentation;qa mathematics;wind directions;time series;wrapped processes;markov chain monte carlo;markov chain monte carlo methods;directional data;time series data;circle;em algorithm	One approach to defining models for circular data and processes has been to take a standard Euclidean model and to wrap it around the circle. This generates rich families of circular models but creates difficulties for inference. Using data augmentation ideas which have previously been applied to this problem in the framework of an EM algorithm, we demonstrate the power and flexibility of Markov chain Monte Carlo methods to fit such classes of models to circular data. The precision of the technique is confirmed through simulated examples, and then applications are given to multivariate and time series data of wind directions.		Stuart Coles	1998	Statistics and Computing	10.1023/A:1008930032595	econometrics;markov chain;hybrid monte carlo;markov chain monte carlo;machine learning;time series;monte carlo molecular modeling;mathematics;statistics;monte carlo method	HPC	28.624603393851885	-28.190077830790248	2486
2c23d4d8fdebf4180ec4d5fa273cafa1dbb37d74	sinuosity pattern recognition of road features for segmentation purposes in cartographic generalization	cartographic generalization;visvalingam whyatt algorithm;automatic segmentation;segmentation;algorithme;algorithm;douglas peucker algorithm;line segmentation;pattern recognition;reconnaissance forme;reconocimiento patron;effective area;segmentacion;algoritmo	In line generalization, results depend very much on the characteristics of the line in question. For this reason it would be useful to obtain an automatic segmentation and enrichment of lines in order to apply to each section the best algorithm and the most appropriate parameter. In this paper, we present a line segmentation methodology based on a sinuosity pattern recognition measured by means of the effective-area as derived from the Visvalingam-Whyatt algorithm. Sections are determined by applying the Douglas-Peucker algorithm to a shape signature of the line: an effective-area/length space representation. An experiment is carried out with a set of 24 road features from a 1:25000 scale map with a recommendation of the value of some parameters and a procedure for the automated search of that defined as natural number of sections. This procedure is based in the search of zones of stability in a graph of the number of sections when applying Douglas-Peucker to the shape signature. The results are positively assessed by an independent group of experts.	cartographic generalization;cartography;pattern recognition	José Luis García Balboa;Francisco Javier Ariza-López	2009	Pattern Recognition	10.1016/j.patcog.2009.02.004	computer vision;ramer–douglas–peucker algorithm;computer science;artificial intelligence;machine learning;segmentation-based object categorization;pattern recognition;scale-space segmentation;segmentation;algorithm;cartographic generalization	Vision	40.47936395141133	-69.52764331395343	2490
3806a4d8394772c434a89bed6a699303d2609f71	fuzzy lyapunov decentralized control of takagi-sugeno interconnected descriptors	lyapunov methods;closed loop system;decentralized parallel distributed compensation controller;takagi sugeno model;nonlinear system stabilization;takagi sugeno;lyapunov function;closed loop systems;nonlinear control systems;probability density function;interconnected systems;fuzzy control;parallel distributed compensation;stability closed loop systems compensation decentralised control distributed control fuzzy control interconnected systems linear matrix inequalities lyapunov methods nonlinear control systems;data mining;stability;nonlinear systems;linear matrix inequality;compensation;decentralised control;pd control;takagi sugeno interconnected descriptor;stability condition;stability analysis;decentralized control;fuzzy lyapunov decentralized control;nonlinear system;linear matrix inequality fuzzy lyapunov decentralized control takagi sugeno interconnected descriptor nonlinear system stabilization closed loop system decentralized parallel distributed compensation controller;fuzzy control distributed control takagi sugeno model fuzzy systems nonlinear systems stability interconnected systems fuzzy sets control system synthesis lyapunov method;distributed control;linear matrix inequalities	This paper deals with decentralized stabilization of nonlinear systems composed of interconnected Takagi-Sugeno fuzzy descriptors. To ensure the stability of the overall closed-loop system, a set of decentralized Parallel Distributed Compensations (PDC) controllers is employed. The stability conditions are then derived into Linear Matrix Inequalities (LMI) using a fuzzy Lyapunov function for less conservatism. Nevertheless, it contains decision parameters that are not available in practice. So the LMIs are casted into relaxed quadratic conditions using simple assumptions. Finally, a numerical example is proposed to illustrate the effectiveness of the suggested decentralized approach.	distributed control system;linear matrix inequality;lyapunov fractal;nonlinear system;numerical analysis;programme delivery control	Dalel Jabri;Kevin Guelton;Noureddine Manamanni;Mohamed Naceur Abdelkrim	2009	2009 IEEE Symposium on Computational Intelligence in Control and Automation	10.1109/CICA.2009.4982780	control engineering;mathematical optimization;control theory;mathematics	Embedded	65.53628241981264	-3.6703367782159018	2491
db97d447a6fe027f23f7642482928954ea101062	descriptors for amino acids using molsurf parametrization	qsar;quantum chemistry;amino acid	This work describes a new set of amino acid descriptors based on ab initio quantum mechanical calculations and MolSurf technology. These descriptors have been applied to two dipeptide data sets using partial least squares as the statistical engine. Statistically significant models for both data sets have been developed. The results from the derived peptide QSAR models are easy to interpret in terms of the theoretically computed MolSurf parameters of physicochemical nature. Q 1998 John Wiley & Sons, Inc. J Comput Chem 19: 51]59, 1998	ab initio quantum chemistry methods;john d. wiley;partial least squares regression;q-chem;quantitative structure–activity relationship;quantum mechanics	Ulf Norinder;Peter Svensson	1998	Journal of Computational Chemistry	10.1002/(SICI)1096-987X(19980115)19:1%3C51::AID-JCC4%3E3.0.CO;2-Y	biochemistry;stereochemistry;amino acid;chemistry;organic chemistry;computational chemistry;quantitative structure–activity relationship;quantum chemistry;quantum mechanics	Vision	12.549374300422132	-58.20343369238984	2505
4490ec1f074b398a48707bed48b79bcc3c7932f2	distributed mopso with dynamic pareto front driven population analysis for tsp problem	travelling salesman problems ant colony optimisation pareto optimisation particle swarm optimisation;particle swarm optimization;optimization cities and towns sociology statistics educational institutions particle swarm optimization genetic algorithms;statistics;cities and towns;genetic algorithms;optimization;tsp problem pareto approach multi objective optimization mopso;pareto dominance distributed mopso dynamic pareto front driven population analysis tsp problem multiobjective concept traveling salesman problem np hard problem single objective based methods particle swarm optimization pso ant colony optimization aco multiobjective particles swarm optimization;sociology	This paper describe the use of Multi-Objective concept to solve the Traveling Salesman Problem (TSP). The traveling salesman problem is defined as an NP-hard problem. The resolution of this kind of problem is based firstly on exact methods and after that is based on single objective based methods as Particle Swarm Optimization (PSO) and Ant Colony Optimization (ACO). Firstly, a short description of the Multi-objective Particles swarm optimization (MOPSO) is given as an efficient technique to use for many real problems. Based on the concept of Pareto dominance, the process of implementation of the algorithm consists of two stages. First, when executing a multi-objective Particle Swarm Optimization (MOPSO), a ranking operator is applied to the population in a predefined iteration to build an initial archive using ε-dominance The TSP problem is characterized by two contradictory objectives as minimize the total distance traveled by a particle and minimize the total time. An experimental study is conducted in this paper. A comparative study with other algorithms existing in the literature has shown a better performance of our algorithm (pMOPSO).	algorithm;ant colony optimization algorithms;archive;experiment;iteration;mathematical optimization;np-hardness;pareto efficiency;particle swarm optimization;travelling salesman problem	Raja Fdhila;Walid Elloumi;Tarek M. Hamdani	2014	2014 6th International Conference of Soft Computing and Pattern Recognition (SoCPaR)	10.1109/SOCPAR.2014.7008022	extremal optimization;2-opt;mathematical optimization;multi-swarm optimization;ant colony optimization algorithms;meta-optimization;genetic algorithm;computer science;artificial intelligence;machine learning;mathematics;particle swarm optimization;3-opt;metaheuristic	Robotics	25.104468031001396	-2.7159314265167134	2515
136b23635c7b959824b0bcb0b5ebc5741faa2570	quality control in manufacturing oligo arrays: a combinatorial design approach	dna microarray;quality control;combinatorial design	"""The advent of the DNA microarray technology has brought with it the exciting possibility of simultaneously observing the expression levels of all genes in an organism. One such microarray technology, called """"oligo arrays"""", manufactures short single strands of DNA (called probes) onto a glass surface using photolithography. An altered or missed step in such a manufacturing protocol can adversely affect all probes using this failed step, and is in general impossible to disentangle from experimental variation when using such a defective array. The idea of designing special quality control probes to detect a failed step was first formulated by Hubbell and Pevzner. We consider an alternative formulation of this problem and use a combinatorial design approach to solve it. Our results improve over prior work in guaranteeing coverage of all protocol steps and in being able to tolerate a greater number of unreliable probe intensities."""	combinatorial optimization;dna microarray format;microarray analysis;oligo primer analysis software;oligonucleotides	Rimli Sengupta;Martin Tompa	2001	Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing		bioinformatics;error detection and correction;dna microarray;mathematics;combinatorial design	Embedded	-0.07693263885791263	-53.4696309574674	2521
100f24f48c03ea5c3f09bdd6454fe19dcca4a153	multi-agent model for fatigue control in large offshore wind farm	simulink;offshore wind farm;distributed estimation;automatic agent controller synthesis;fatigue;uppaal tiga;wind power plants;agent modeling;wind turbines;wind power;biological system modeling;maintenance cost;control network;fatigue wind farms wind turbines communication system control large scale systems life estimation communication networks wind energy automatic control network synthesis;controller synthesis;large scale;multi agent systems;community networks;wind speed;mathematical model;wind power delivery;self organization;wind turbines fatigue multi agent systems offshore installations wind power plants;offshore installations;offshore wind farm multi agent model fatigue wind turbine;fatigue distribution;wind turbine;wind power generation;large offshore wind farm;simulink multi agent model fatigue control large offshore wind farm wind turbine fatigue fatigue distribution wind power delivery automatic agent controller synthesis uppaal tiga;multi agent model;wind farms;wind turbine fatigue;wind farm;fatigue control	To control wind turbine fatigue and optimize the fatigue distribution for offshore wind farm, a control network model is proposed based on Multi-Agent theory. A typical model of large-scale offshore wind farm is described. Power fatigue of individual wind turbine is defined. In offshore wind farm, a fatigue distribution estimation approach is studied. The Multi-Agent network is modeled based on the current communication network. All wind turbines act as independent agents, and can self-organize into wind power delivery groups adaptively according to the real wind direction. A solution for automatic agent controller synthesis using Uppaal Tiga and Simulink is presented. The fatigue distribution can be optimized to prolong the service time of a wind farm. This approach can also save some of the maintenance cost due to the expensive logistic visit by helicopters. Finally, a typical simulation result illustrates the feasibility of this multi-agent model.	multi-agent system;network model;self-organization;server farm;simulation;simulink;telecommunications network;texas instruments graphics architecture;uppaal;unbalanced circuit	Rongyong Zhao;Yongqing Su;Torben Knudsen;Thomas Bak;WenZhong Shen	2008	2008 International Conference on Computational Intelligence and Security	10.1109/CIS.2008.131	wind power;simulation;computer science;artificial intelligence;multi-agent system	Robotics	35.69406402978475	-0.693451100360818	2522
cef1afe0209fd9d3a8904e707374f05e67b120cb	the wireless controlled unmanned vehicle system with vision system	vision system			Hee Chang Moon;Woon-Sung Lee;Jung Ha Kim	2003			embedded system;wireless;xm1216 small unmanned ground vehicle;engineering;machine vision	Robotics	56.76810686440481	-31.296089866617773	2523
e76a1d91939677fe959446f491bb4ece7d285a41	image and video dehazing using view-based cluster segmentation	dark channel prior;video dehazing;view based cluster segmentation;gmm;image dehazing	To avoid distortion in sky regions and make the sky and white objects clear, in this paper we propose a new image and video dehazing method utilizing the view-based cluster segmentation. Firstly, GMM(Gaussian Mixture Model)is utilized to cluster the depth map based on the distant view to estimate the sky region and then the transmission estimation is modified to reduce distortion. Secondly, we present to use GMM based on Color Attenuation Prior to divide a single hazy image into K classifications, so that the atmospheric light estimation is refined to improve global contrast. Finally, online GMM cluster is applied to video dehazing. Extensive experimental results demonstrate that the proposed algorithm can have superior haze removing and color balancing capabilities.	algorithm;color balance;depth map;distortion;google map maker;mixture model	Feng Yu;Chunmei Qing;Xiangmin Xu;Bolun Cai	2016	2016 Visual Communications and Image Processing (VCIP)	10.1109/VCIP.2016.7805512	computer vision;simulation;generalized method of moments;statistics;computer graphics (images)	Vision	57.40192531960496	-60.209437830871295	2524
ffd10f50f68ef557f1cdb3c41914a31b761d8c7d	hybrid robust support vector machines for regression with outliers	least squares support vector machines for regression ls svmr;robust statistics;weighted ls svmr;support vector regression;outliers;support vector machines for regression;robust support vector regression networks;support vector machine;data preprocessing;least squares support vector machine	In this study, a hybrid robust support vector machine for regression is proposed to deal with training data sets with outliers. The proposed approach consists of two stages of strategies. The first stage is for data preprocessing and a support vector machine for regression is used to filter out outliers in the training data set. Since the outliers in the training data set are removed, the concept of robust statistic is not needed for reducing the outliers’ effects in the later stage. Then, the training data set except for outliers, called as the reduced training data set, is directly used in training the non-robust least squares support vector machines eywords: utliers upport vector machines for regression east squares support vector machines for egression (LS-SVMR) for regression (LS-SVMR) or the non-robust support vector regression networks (SVRNs) in the second stage. Consequently, the learning mechanism of the proposed approach is much easier than that of the robust support vector regression networks (RSVRNs) approach and of the weighted LS-SVMR approach. Based on the simulation results, the performance of the proposed approach with non-robust LS-SVMR is superior to the weighted LS-SVMR approach when the outliers exist. Moreover, the performance of the non-r eighted LS-SVMR obust support vector regression networks proposed approach with	data pre-processing;least squares;preprocessor;simulation;support vector machine;test set	Chen-Chia Chuang;Zne-Jung Lee	2011	Appl. Soft Comput.	10.1016/j.asoc.2009.10.017	support vector machine;least squares support vector machine;least trimmed squares;computer science;machine learning;pattern recognition;data mining;relevance vector machine;structured support vector machine;robust regression	ML	13.251013561184445	-40.73279643513015	2525
8e709abc4633e537ee8423b46817c77a8ee9e6a9	manipulator grasping and pushing operations	classical mechanics;theoretical analysis;industrial robots;remote handled;programming tool	The primary goal of this research is to develop theoretical tools for analysis, synthesis, and application of primitive manipulator operations. The primary method is to extend and apply traditional tools of classical mechanics. The results are of such a general nature that they address many different aspects of industrial robotics, including effector and sensor design, planning and programming tools, and design of auxiliary equipment. Some of the manipulator operations studied are: 1. Grasping an object. The object will usually slide and rotate during the period between first contact and prehension. 2. Placing an object. The object may slip slightly in the fingers upon contact with the table as the base aligns with the table. 3. Pushing. Often the final stage of mating two parts involves pushing one object into the other. In each of these operations the motion of the object is determined partly by the manipulator and partly by frictional forces. Hence the theoretical analysis focuses on the problem of partially constrained motion with friction. When inertial forces are dominated by frictional forces, we find that the fundamental motion of the object-whether it will rotate, and if so in what direction-may be determined by inspection. In many cases the motion may be predicted in detail, and in any case it is possible to find bounds on the motion. With these analytical tools it is sometimes possible to predict the outcome of a given manipulator operation, or, on the other hand, to plan an operation producing a given desired outcome. Thesis Supervisors: Tomas Lozano-Perez and Berthold K. P. Horn Titles: Assistant Professor and Associate Professor, respectively, of Computer Science and Electrical Engineering	automated planning and scheduling;computer science;electrical engineering;industrial robot;programming tool;robotics;star trek: first contact	Matthew Thomas Mason	1982			control engineering;simulation;computer science;artificial intelligence	Robotics	64.06634493714884	-26.08525429267135	2531
d51239ad31a8df8c6992d8ee5d1ab7f041416f71	pectoral muscle detection in mammograms using local statistical features	mammography;pectoral muscle;non-gaussianity	Mammography is a primary imaging method for breast cancer diagnosis. It is an important issue to accurately identify and separate pectoral muscles (PM) from breast tissues. Hough-transform-based methods are commonly adopted for PM detection. But their performances are susceptible when PM edges cannot be depicted by straight lines. In this study, we present a new pectoral muscle identification algorithm which utilizes statistical features of pixel responses. First, the Anderson–Darling goodness-of-fit test is used to extract a feature image by assuming non-Gaussianity for PM boundaries. Second, a global weighting scheme based on the location of PM was applied onto the feature image to suppress non-PM regions. From the weighted image, a preliminary set of pectoral muscles boundary components is detected via row-wise peak detection. An iterative procedure based on the edge continuity and orientation is used to determine the final PM boundary. Our results on a public mammogram database were assessed using four performance metrics: the false positive rate, the false negative rate, the Hausdorff distance, and the average distance. Compared to previous studies, our method demonstrates the state-of-art performance in terms of four measures.	body tissue;breast carcinoma;entity name part qualifier - adopted;hausdorff dimension;hough transform;iterative method;mammary neoplasms;mammography;myalgia;pectoralis muscles;performance;pixel;scott continuity;algorithm	Li Liu;Qian Liu;Wei Lu	2014	Journal of Digital Imaging	10.1007/s10278-014-9676-1	computer vision;simulation;mathematics	Vision	38.395660050084075	-77.64823469480105	2542
0845fa90e4a76b705e8c7ae1a79fba15c2eafc97	the analysis and control for singular ecological-economic model with harvesting and migration		To keep the resources renewable, a singular ecological-economic model is proposed for the populations with harvesting and migration. The local stability and the dynamic behavior of the model are studied. Singular induced bifurcation appears when economic interest is zero, which is different from the ordinary differential models. In order to apply variable structure control to eliminate these complex behaviors, the singular model is transformed into a single-input and single-output model with parameter varying within definite intervals. And then, a variable structure controller is designed to make the model stable. Finally, an inshore-offshore fishery model is given to illustrate the proposed method, and some numerical simulations are shown to demonstrate the control results.	bifurcation theory;computer simulation;numerical analysis;population	Qingling Zhang;Hong Niu;Lichun Zhao;Fenglan Bai	2012	J. Applied Mathematics	10.1155/2012/973869	mathematical optimization;mathematical analysis;control theory;mathematics	Robotics	78.39021445563785	3.556424597286883	2543
a6dac74de1c154413159524a27d91b68471f40bd	feedback-control modeling for cellualr response mechanisms based on a gene regulatory networks under radiotherapy	radiotherapy;stress response;cellular networks;double strand break;single cell;cellular network;defense mechanism;signaling pathway;kinetics;modeling;gene regulatory network;feedback control;dna damage	In response to genome stresses, cell can trigger the self-defensive mechanisms by regulating the vital genes and their complicated signal pathways. To illustrate the celluar response in fighting against DNA damage under radiotherapy, a feedback-control model of P53 stress response networks is proposed at single cell level. The kinetics of Double Strand Breaks(DSBs) generation and repair, ARF and ATM activation, P53-MDM2 regulation, toxins degradation, as well as feedback-control to ion radiation (IR) dose are presented.	atm turbo;acoustic radiation force;elegant degradation;gene regulatory network;kinetics internet protocol;strand (programming language)	Jinpeng Qi;Shihuang Shao;Zhihai Rong	2009		10.1145/1543834.1543977	cellular network;computer science;bioinformatics	Comp.	6.147635404785648	-64.4902769282862	2545
c85f698c8c73d4cf7acbe073ff53cbf10200feed	fuzzy thick rubber model for cerebral surface extraction in neonatal brain mr images	fuzzy thick rubber model;gyral index;image segmentation;rubber;computer aided diagnosis;root mean square error;fuzzy control;niobium;data mining;fuzzy set theory;magnetic resonance image;force;brain modeling;mr imaging;magnetic resonance;feature extraction;medical image processing;root mean squared error;indexation;transmission line measurements;rubber brain modeling pediatrics image segmentation transmission line measurements deformable models fuzzy control magnetic resonance volume measurement labeling;mean square error methods;neonatal brain mr image;medical image processing biomedical mri feature extraction fuzzy set theory mean square error methods;cerebral surface extraction;gyral index fuzzy thick rubber model cerebral surface extraction neonatal brain mr image computer aided diagnosis magnetic resonance image fuzzy control root mean squared error;partial volume effect;biomedical mri;spatial resolution	Cerebral surface extraction plays a fundamental role of computer aided diagnosis (CAD) for neonatal brain magnetic resonance (MR) images. However, cerebral sulci of the neonatal brains is complexity folded, and it is difficult to extract complete cerebral contour from MR images due to the limitation of spatial resolution and partial volume effect (PVE). This paper proposes a novel method to extract the cerebral contour based on fuzzy thick rubber model (TRM). The TRM is deformed by using fuzzy control schemes so that the digitally synthesized MR images from the deforming TRM are identical to the given MR images. By synthesizing the MR images with respect to PVE, the proposed method is able to extract the cerebral contour with sub-voxel accuracy. The proposed method was applied to 7 subjects whose revised ages were from -17 days to 34 days. The root-mean-squared-error between the extracted contour and the manually delineated contour by two physicians was 1.09 ± 0.48 mm from the truth contour. And, to demonstrate the clinical effective, gyral index was calculated using the extracted cerebral contour.	computer-aided design;contour line;federal enterprise architecture;fuzzy control system;resonance;voxel	Syoji Kobashi;Takuma Oshiba;Kumiko Ando;Reiichi Ishikura;Seturo Imawaki;Shozo Hirota;Yutaka Hata	2009	2009 IEEE International Conference on Fuzzy Systems	10.1109/FUZZY.2009.5276881	computer vision;computer science;artificial intelligence;magnetic resonance imaging;machine learning;mean squared error	Robotics	41.099890756558594	-76.20092399238966	2550
18b7c2e0ce4a10b7eb6a9bb6dd838c797961f060	fractional quaternion zernike moments for robust color image copy-move forgery detection		In this paper, fractional Zernike moments (FrZMs) for complex signals are generalized to fractional quaternion Zernike moments (FrQZMs) for quaternion signal processing in a holistic manner by the quaternion algebra. We first present the definition of FrQZMs and an efficient implementation algorithm for speeding up the computation of FrQZMs through FrZMs of each component of the quaternion signal. The performance of the proposed FrQZMs is evaluated by considering robust color image copy–move forgery detection. The proposed robust copy–move forgery-detection algorithm considers the FrQZMs as a feature and a modified PatchMatch algorithm as a feature matching algorithm. Experimental results on two publicly available data sets (FAU and GRIP data set) have demonstrated that the proposed FrQZM-based algorithm can achieve an overall better performance than the state-of-the-art algorithms, especially in some additional operation cases.	algorithm;authentication;color image;computation;feature model;fractional fourier transform;holism;patchmatch;signal processing	Beijing Chen;Ming Yu;Qingtang Su;Hiuk Jae Shim;Yun Q. Shi	2018	IEEE Access	10.1109/ACCESS.2018.2871952	quaternion;computer vision;signal processing;robustness (computer science);feature extraction;color image;zernike polynomials;computer science;blossom algorithm;quaternion algebra;distributed computing;artificial intelligence	Vision	34.38267850246559	-59.23073587003901	2551
0f3950e89175e0b7ef2fdd15510f02d8748c386b	generating video textures by ppca and gaussian process dynamical model	probabilistic principal component analysis;computer graphics;dynamic model;autoregressive process;computer graphic;computer vision;dimensionality reduction;gaussian process;video texture;ppca	"""Video texture is a new type of medium which can provide a continuous, infinitely varying stream of video images from a recorded video clip. It can be synthesized by rearranging the order of frames based on the similarities between all pairs of frames. In this paper, we propose a new method for generating video textures by implementing probabilistic principal components analysis (PPCA) and Gaussian Process Dynamical model (GPDM). Compared to the original video texture technique, video texture synthesized by PPCA and GPDM has the following advantages: it might generate new video frames that have never existed in the input video clip before; the problem of """"dead-end"""" is totally avoided; it could also provide video textures that are more robust to noise."""	dynamical system;gaussian process	Wentao Fan;Nizar Bouguila	2009		10.1007/978-3-642-10268-4_94	computer vision;computer science;machine learning;gaussian process;autoregressive model;computer graphics;motion compensation;statistics;dimensionality reduction;computer graphics (images)	Vision	59.958712709050744	-48.627396582196916	2554
c7be28abac89d08e74125dd680661b1993b7d7f8	clustering using an autoassociator: a case study in network event correlation.	feedforward neural network;novelty detection;event correlation;cluster system	An autoassociator is a feedforward neural network that has the same number of input and output units. The goal of the autoassociator is very simple; to reconstruct its input at the output layer. Despite their simplicity, autoassociators have previously been shown to be quite successful on the task of Novelty Detection applied to industrial and military domains. The purpose of this paper is to test their utility on the more general task of clustering. In particular, we apply a clustering version of the autoassociator to the domain of Network Event Correlation. The results suggest that autoassociators are indeed useful as clustering systems. They were able to successfully correlate similar types of network alerts and have the added advantage of being fast once trained, a crucial feature when used for Network Event Correlation.	artificial neural network;autoencoder;cluster analysis;event correlation;feedforward neural network;input/output;novelty detection	Reuben L Smith;Nathalie Japkowicz;Maxwell G. Dondo	2005			feedforward neural network;probabilistic neural network;computer science;event correlation;machine learning;pattern recognition;data mining	ML	13.563003904504226	-35.11426830734879	2564
59272a787f8e35bcadf3203a349e23fb297526d3	diffusion map for clustering fmri spatial maps extracted by independent component analysis	pattern clustering;independent component analysis;pattern clustering biomedical mri independent component analysis medical image processing;diffusion map based clustering dimensionality reduction similarity matrix correlation matrices spatial map clustering ica independent component analysis functional magnetic resonance imaging fmri spatial map extraction;medical image processing;spatial maps clustering diffusion map dimensionality reduction functional magnetic resonance imaging fmri independent component analysis;correlation magnetic resonance imaging principal component analysis educational institutions brain measurement;biomedical mri	Functional magnetic resonance imaging (fMRI) produces data about activity inside the brain, from which spatial maps can be extracted by independent component analysis (ICA). In datasets, there are n spatial maps that contain p voxels. The number of voxels is very high compared to the number of analyzed spatial maps. Clustering of the spatial maps is usually based on correlation matrices. This usually works well, although such a similarity matrix inherently can explain only a certain amount of the total variance contained in the high-dimensional data where n is relatively small but p is large. For high-dimensional space, it is reasonable to perform dimensionality reduction before clustering. In this research, we used the recently developed diffusion map for dimensionality reduction in conjunction with spectral clustering. This research revealed that the diffusion map based clustering worked as well as the more traditional methods, and produced more compact clusters when needed.	cluster analysis;computer cluster;diffusion map;dimensionality reduction;independent component analysis;resonance;similarity measure;spatial anti-aliasing;spectral clustering;voxel	Tuomo Sipola;Fengyu Cong;Tapani Ristaniemi;Vinoo Alluri;Petri Toiviainen;Elvira Brattico;Asoke K. Nandi	2013	2013 IEEE International Workshop on Machine Learning for Signal Processing (MLSP)	10.1109/MLSP.2013.6661923	diffusion map;independent component analysis;correlation clustering;computer vision;computer science;machine learning;consensus clustering;pattern recognition;mathematics;cluster analysis;spectral clustering	ML	0.8182828976553792	-42.59893362072009	2570
cf562b648ee2a758c9b7e63c23d9771bc0a5ccc8	organ localization and identification in thoracic ct volumes using 3d cnns leveraging spatial anatomic relations		In this paper, we present a model to obtain prior knowledge for organ localization in CT thorax images using three dimensional convolutional neural networks (3D CNNs). Specifically, we use the knowledge obtained from CNNs in a Bayesian detector to establish the presence and location of a given target organ defined within a spherical coordinate system. We train a CNN to perform a soft detection of the target organ potentially present at any point, x e [ r,Θ,Φ ] T . This probability outcome is used as a prior in a Bayesian model whose posterior probability serves to provide a more accurate solution to the target organ detection problem. The likelihoods for the Bayesian model are obtained by performing a spatial analysis of the organs in annotated training volumes. Thoracic CT images from the NSCLC–Radiomics dataset are used in our case study, which demonstrates the enhancement in robustness and accuracy of organ identification. The average value of the detector accuracies for the right lung, left lung, and heart were found to be 94.87p, 95.37p, and 90.76p after the CNN stage, respectively. Introduction of spatial relationship using a Bayes classifier improved the detector accuracies to 95.14p, 96.20p, and 95.15p, respectively, showing a marked improvement in heart detection. This workflow improves the detection rate since the decision is made employing both lower level features (edges, contour etc) and complex higher level features (spatial relationship between organs). This strategy also presents a new application to CNNs and a novel methodology to introduce higher level context features like spatial relationship between objects present at a different location in images to real world object detection problems.		Rajath Elias Soans;James A. Shackleford	2018		10.1117/12.2293801	spatial contextual awareness;convolutional neural network;robustness (computer science);object detection;deep learning;posterior probability;pattern recognition;bayes classifier;artificial intelligence;bayesian inference;computer science	Vision	32.336024916574395	-76.05456380149167	2572
c7689c5339ef25c1aacabd3b024aa0746ac951f0	high-performance state feedback, robust, and output feedback stabilizing control-a systematic design algorithm	eigenvalues and eigenfunctions;sistema lineal;systeme mimo;scale model;systeme commande;sistema control;boucle reaction etat;mimo system;modele reduit;concepcion sistema;state feedback robust control output feedback control systems riccati equations stochastic processes automatic control robust stability difference equations differential algebraic equations;systeme invariant;stabilization;state feedback;robust control;matrix algebra;invarying system;indexing terms;strong stability;linear system;output feedback;matrix algebra state feedback robust control compensation control system synthesis eigenvalues and eigenfunctions;sistema invariante;modelo reducido;control system;robust stability;sistema mimo;compensation;estabilizacion;system synthesis;control system synthesis;system design;synthese systeme;boucle reaction sortie;sintesis sistema;control robusta;bucle realimentacion estado;stabilisation;low controller order high performance state feedback output feedback stabilizing control systematic design algorithm;feedback system;systeme lineaire;commande robuste;state feedback control;high performance;conception systeme;bucle realimentacion salida	Improves a fundamentally new design approach which can generally realize the robustness properties of the state feedback control for the first time, by guaranteeing the resulting feedback system stability. To the best of the authors' knowledge, this improved design also provides the first simple and systematic design procedure and design solution to the strong stabilization problem-stabilize feedback system by a stable output feedback compensator. Finally, high-performance high robustness, and low-controller order can all be systematically achieved by this design.	advanced configuration and power interface;algorithm;block cipher mode of operation;negative feedback;robustness (computer science)	Chia-Chi Tsui	1999	IEEE Trans. Automat. Contr.	10.1109/9.751350	robust control;full state feedback;control engineering;electronic engineering;index term;control system;control theory;feedback;mathematics;linear system;scale model;systems design	Security	70.88071625410805	-3.221059296418031	2574
d48f6e7f0ec1faa662bfb961923cac5df109bead	biofunctionalization of polymeric surfaces	biological patents;biomedical journals;text mining;europe pubmed central;citation search;citation networks;programmable logic arrays proteins peptides adhesives electronic countermeasures polymers films;biofunctionalization migratory receptor cxcr4 mesenchymal stromal cells short rgd peptides cell migration pla based microcarriers cell delivery tissue engineering native tissues decellularized polymer scaffold cell deposited ecm collagen coating migratory potential cell attachment surface modifications serum proteins bioactive signals ecm molecules polylactic acid biomaterials cell adhesion protein like recombinamers extracellular matrix proteins peptides carbodiimide chemistry biomolecules polyesters controlled hydrolysis functionalization approaches bioactive cues surface modification strategies tissue regeneration synthetic polymeric biomaterials polymeric surfaces;research articles;abstracts;open access;life sciences;clinical guidelines;full text;tissue engineering adhesion biochemistry biomechanics biomedical materials cell motility conducting polymers molecular biophysics proteins;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	Most of the synthetic polymeric biomaterials used for biomedical applications lack of functional groups able to specifically instruct cells to unlock their potential for tissue regeneration. Surface modification strategies are able to overcome this limitation by introducing bioactive cues. In this study, several functionalization approaches are analyzed. Wet chemical methods such as controlled hydrolysis of polyesters followed by biomolecules grafting by carbodiimide chemistry are simple and versatile approaches, able to succesfully improve the bioactivity of devices with virtually any architecture. Grafting of short peptides, extracellular matrix proteins (ECM) or engineered protein-like recombinamers are promising techniques to improve cell adhesion to biomaterials, including polylactic acid (PLA) and its derivatives. ECM molecules and recombinamers can present more effectively bioactive signals, even in presence of competing, nonadhesive serum proteins. Besides adhesion, surface modifications intended to improve cell attachment, play a role on other cell responses, such as migratory potential. Collagen coating were shown to enhance the expression of the migratory receptor CXCR4 in mesenchymal stromal cells, when compared to short RGD peptides, while the modality of functionalization (covalent vs. physisorbed) tuned the rate of cell migration from PLA-based microcarriers. This multiple effects have to be taken into account when designing biomaterials for cell delivery and tissue engineering. Furthermore, as we aim to recapitulate in vitro the complexity of native tissues, alternative strategies based on the generation of decellularized polymer scaffold rich in cell-deposited ECM are proposed.	attachments;biomaterials;body tissue;cell adhesion;cell-matrix junction;coating excipient;covalent interaction;extracellular matrix proteins;modality (human–computer interaction);natural regeneration;polyesters;polylactic acid (substance);polymer;programmable logic array;rgd;sim lock;synthetic intelligence;tissue engineering;transplanted tissue;tissue regeneration	Miguel A. Mateos-Timoneda;Riccardo Levato;Xavier Punet;Irene Cano;Oscar Castano;Elisabeth Engel	2015	2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2015.7318715	text mining;medical research;computer science;nanotechnology;biological engineering	Visualization	11.183921991473246	-65.79020115400274	2576
46e1ad976b222245b1fa99956c93dc46b4fb3ef1	hmm and wt fusion for face identification	hidden markov model;wavelet transform	This paper describes the original FaMar method of user's identification. The method bases on the fusion of Wavelet Transform (WT) and Hidden Markov Models (HMM), which is used for three parts of the face (eyes, nose, and mouth) separately.	hidden markov model;markov chain;wavelet transform	Janusz Bobulski	2005		10.1007/3-540-32390-2_89	speech recognition;computer science;machine learning;pattern recognition;hidden markov model;wavelet transform	Vision	31.613024020779985	-59.44045271881362	2580
7e31ac4997ccf770d606de8f123d0ee933d4f5b3	effects of fragile and semi-fragile watermarking on iris recognition system		Security is an important issue in biometric recognition systems. In recent years, many researchers proposed to use watermarking to improve the security of biometric systems, but some people concern whether the embedded watermarks will influence recognition results. In this paper, we investigate the effects of several fragile and semi-fragile watermarking methods on the iris recognition performance. Experimental results demonstrate that, even images are fully embedded, fragile watermarking methods nearly have no effects on the recognition performance, while semi-fragile watermarking methods which embed watermark in the visually important components of images have larger effects on the recognition performance than the semi-fragile watermarking methods that embed watermark in the visually unimportant components of images. And embedding parameters, such as embedding strength and watermark length, also have some influences on the recognition results.	iris recognition;semiconductor industry	Zairan Wang;Jing Dong;Wei Wang;Tieniu Tan	2014		10.1007/978-3-319-19321-2_13	artificial intelligence;theoretical computer science;computer vision;watermark;iris recognition;computer science;digital watermarking;biometrics	EDA	37.145772144547784	-11.445783186075378	2584
21cc620416a37fef9b0b0b899fd2f44fcd68a24b	development of a two dof needle driver for ct-guided needle insertion-type interventional robotic system		We present a compact and lightweight two Degrees of Freedom (DOF) needle driver to be applied to a teleoperated needle insertion-type interventional robotic system. The interventional slave manipulator is located beside the patient bed inside a CT scanner room. Physicians manipulate a master device, separately located in the control room, in order to control needle placement. The needle driver provides one translational motion and one rotational motion to a biopsy needle, and the needle is easily detachable from the needle driver. We performed several experiments to measure the repeatability and the insertion force of the needle to evaluate the basic performance of the proposed needle driver. The maximum error of the repeatability was 0.16 mm and the standard deviation was 0.0058 mm. The maximum insertion force of the needle was about 2.2 kgf, which meets the minimum insertion force of the needle for needle insertion-type interventions. Therefore, we expect the suggested needle driver is suitable to the clinical environment for CT-guided needle insertion-type interventions.	ct scan;experiment;repeatability;robot	Ki Young Kim;Hyun Soo Woo;Jang Ho Cho;Yong Koo Lee	2017	2017 26th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)	10.1109/ROMAN.2017.8172344	artificial intelligence;computer vision;computer science;teleoperation;manipulator;biopsy	Robotics	73.55222513728991	-28.839098097380038	2588
3ed028e0d217acd084d916d9577352925b680036	large-scale dna memory based on the nested pcr	theoretical model;dna memory;data embedding;scaling up;large scale;theoretical capacity;nested pcr;dna computing;combinatorial optimization	A DNA Memory with over 10 million (16.8 M) addresses was achieved. The data embedded into a unique address was correctly extracted through an addressing processes based on nested PCR. The limitation of the scaling-up of the proposed DNA memory is discussed by using a theoretical model based on combinatorial optimization with some experimental restrictions. The results reveal that the size of the address space of the DNA memory presented here may be close to the theoretical limit. The high-capacity DNA memory can be also used in cryptography (steganography) or DNA ink. In decoding process, multiple data with different addresses can be also simultaneously accessed by using the mixture of some address primers.	address space layout randomization;combinatorial optimization;cryptography;dna computing;embedded system;mathematical optimization;steganography;theory	Masahito Yamamoto;Satoshi Kashiwamura;Azuma Ohuchi;Masashi Furukawa	2008	Natural Computing	10.1007/s11047-008-9076-x	nested polymerase chain reaction;combinatorial optimization;computer science;bioinformatics;theoretical computer science;genetic memory;genetics;dna computing;algorithm	Metrics	-3.4356050492213894	-52.86023669585473	2589
5834cd7772448785cd3c13cc1a685fb454f3a53f	prediction error estimation: a comparison of resampling methods	prediccion;genomique;genomics;model selection;pronostic;prediction error;classification tree;small sample size;selection;genomica;methode;linear discriminate analysis;small samples;discriminant analysis;estimation erreur;pronostico;risk estimation;error estimation;erreur estimation;mean square error;nearest neighbor;estimacion error;echantillon;error estimacion;feature selection;sample;prediction model;cross validation;estimation error;resampling method;seleccion;signal to noise ratio;metodo;prognosis;prediction;method;muestra;leave one out	MOTIVATION In genomic studies, thousands of features are collected on relatively few samples. One of the goals of these studies is to build classifiers to predict the outcome of future observations. There are three inherent steps to this process: feature selection, model selection and prediction assessment. With a focus on prediction assessment, we compare several methods for estimating the 'true' prediction error of a prediction model in the presence of feature selection.   RESULTS For small studies where features are selected from thousands of candidates, the resubstitution and simple split-sample estimates are seriously biased. In these small samples, leave-one-out cross-validation (LOOCV), 10-fold cross-validation (CV) and the .632+ bootstrap have the smallest bias for diagonal discriminant analysis, nearest neighbor and classification trees. LOOCV and 10-fold CV have the smallest bias for linear discriminant analysis. Additionally, LOOCV, 5- and 10-fold CV, and the .632+ bootstrap have the lowest mean square error. The .632+ bootstrap is quite biased in small sample sizes with strong signal-to-noise ratios. Differences in performance among resampling methods are reduced as the number of specimens available increase.   SUPPLEMENTARY INFORMATION A complete compilation of results and R code for simulations and analyses are available in Molinaro et al. (2005) (http://linus.nci.nih.gov/brb/TechReport.htm).	compiler;cross reactions;cross-validation (statistics);decision tree;estimated;feature selection;linear discriminant analysis;mean squared error;model selection;r language;resampling (statistics);sample size;signal-to-noise ratio;simulation;single linkage cluster analysis;specimen;trees (plant)	Annette M. Molinaro;Richard Simon;Ruth M. Pfeiffer	2005	Bioinformatics	10.1093/bioinformatics/bti499	selection;econometrics;genomics;method;prediction;sample;decision tree learning;computer science;mean squared prediction error;pattern recognition;mathematics;mean squared error;predictive modelling;signal-to-noise ratio;feature selection;k-nearest neighbors algorithm;cross-validation;model selection;statistics	ML	9.357813467867336	-49.736315819249704	2592
d53f5a7ea7b6efcc25716095fbfde6c361a40bca	improvement on velocity estimation of an extended object		Conventional point object model based automotive radar tracking system assumes at most one detection received from the target object at a time. However, in real applications for an extended object, such as a passenger car, located within a close range to a high-resolution radar or LIDAR, the system usually receives multiple reflections from different parts of the object. This can introduce large bias into a velocity estimation performed by a point object model based tracking system. Doppler-azimuth profile based approach accounts for the cluster of detections and could give a very accurate velocity vector of the extended object. However, depending on the position and orientation of the object, the linear equation set could be ill-conditioned, in which case the estimated velocity will suffer from substantial error. In this paper, we first propose a new approach to estimate the heading of a moving object using principle component analysis based on the detection cluster trajectory. We then propose an approach to fuse the three above-mentioned velocity estimators as each estimator faces challenges in different situations. Road data from a 77GHz radar is used for performance illustration.	condition number;course (navigation);image resolution;linear equation;principal component analysis;radar;reflection (computer graphics);sensor;tracking system;velocity (software development)	Jifeng Ru;Cuichun Xu	2017	2017 20th International Conference on Information Fusion (Fusion)	10.23919/ICIF.2017.8009882	computer vision;artificial intelligence;estimator;computer science;radar tracker;object model;radar;lidar;linear equation;tracking system;trajectory	Robotics	53.86149290098984	-38.44177335826128	2597
f2acb4019a45f582c91f4b0a73cd66bfe6c9f2ee	parametric lattice boltzmann method	numerical stability;navier stokes equations;lattice boltzmann method	The discretized equilibrium distributions of the lattice Boltzmann method are presented by using the coefficients of the Lagrange interpolating polynomials that pass through the points related to discrete velocities and using moments of the Maxwell–Boltzmann distribution. The ranges of flow velocity and temperature providing positive valued distributions vary with regulating discrete velocities as parameters. New isothermal and thermal compressible models are proposed for flows of the level of the isothermal and thermal compressible Navier–Stokes equations. Thermal compressible shock tube flows are simulated by only five on-lattice discrete velocities. Two-dimensional isothermal and thermal vortices provoked by the Kelvin–Helmholtz instability are simulated by the parametric models.	lattice boltzmann methods	Jae Wan Shim	2017	J. Comput. Physics	10.1016/j.jcp.2017.02.057	classical mechanics;mathematical optimization;boltzmann relation;lattice boltzmann methods;mathematics;thermodynamics;physics;numerical stability;algorithm	Theory	90.09347319014226	4.0945979637947705	2598
f8d905bacab6c15b235a4041bc406b7c8407ccbf	aluminum production modeling - a nonlinear bilevel programming approach	non linear programming;elaboration aluminium;modele mathematique;fusion minerai;programacion no lineal;programmation non lineaire;nonlinear;modelo matematico;igneous electrolysis;product model;algorithme;algorithm;fusion mineral;electrolyse ignee;elaboracion aluminio;bilevel;mathematical model;production planning;optimization;smelting;aluminium production;bilevel programming;electrolisis ignea;programming;algoritmo	This paper develops a nonlinear bilevel programming model of an aluminium smelter that is capable of representing all the major production processes. The model encompasses all the areas of the smelter which operates in a multilevel way. However, as shown, it can be reduced quite simply to a bilevel programming problem. The problem specification involves nonlinearities with respect to the variables and the presence of ratios among the constraints. The problem is also characterized by a two-way material flow in the production process. A relatively simple solution algorithm that facilitates the attainment of the global optimum is developed. While the problem specification appears daunting, a number of simplifications can be made, due to nature of the problem, allowing its quick solution. While the model developed in this paper together with the solution algorithm appear simple, considerable effort was required to identify the fundamental relationships in the aluminium smelting process, quantify them, and then develop appropriate expressions to represent them. It is believed that the solution of this class of nonlinear bilevel programming problems will have implications for other applications.		Miles G. Nicholls	1995	Operations Research	10.1287/opre.43.2.208	smelting;programming;mathematical optimization;nonlinear system;nonlinear programming;operations management;mathematical model;mathematics;operations research;bilevel optimization;algorithm	Robotics	8.316162327900235	-4.351284631148359	2605
dba3ec4420a0bcca3264f75f4c975cabdbb1af74	"""""""edutainment 2017"""" a visual and semantic representation of 3d face model for reshaping face in images"""		Reshaping faces is an interesting thing and a common demand in today’s media industry and cosmetic industry. Especially, the reshaped face images contribute a lot to special effects industry and plastic-beauty industry. In this paper, we provide a novel parametric representation of 3D face model that describes the face shape as a linear combination of semantic and non-semantic bases. The semantic (local) bases correspond to each individual face part and they are used to semantically edit each face part, while the non-semantic bases explain other shape variations to span the subspace of the face shape. First, we build a sparse and spatially localized parametric face model from a dataset of 3D face models by sparse principal component analysis. Second, to define the semantic bases, we train a regression model to correlate semantically significant values like nose height and mouth width. Finally, the bases of the resulting parametric face model is orthogonalized to all defined semantic bases by the Gram–Schmidt algorithm for generating the novel parametric representation of 3D face model. The novel representation can be applied in reshaping face in the images. The experiment results demonstrate that our representation not only retains the accuracy for 3D face reconstruction but also provides users a user-friendly tool to edit facial parts for desired facial shapes.	algorithm;artificial neural network;convolution;convolutional neural network;educational entertainment;face detection;illumination (image);lu decomposition;mathematical optimization;nonlinear programming;nonlinear system;principal component analysis;real-time clock;real-time computing;schmidt decomposition;sparse matrix;usability	Jiang Du;Dan Song;Yan-Long Tang;Ruofeng Tong;Min Tang	2018	J. Visualization	10.1007/s12650-018-0476-4		Vision	61.85981618488174	-46.58130209510022	2610
27487f6c410133b2c6a9be6e3c61e349b2feada8	the effect of error in gridded digital elevation models on the estimation of topographic parameters	digital elevation model;topography;error analysis;statistical analysis;error propagation;spatial distribution;routing algorithm;distributed hydrological model;topographic index;monte carlo simulation	Digital elevation models (DEMs) provide the basic information required to characterise the topographic attributes of terrain. The primary derived topographic parameters associated with DEMs are slope and aspect. Slope and aspect maps are used in a wide variety of applications. Slope and aspect can be used to calculate other significant topographic parameters such as upslope area and topographic index. The topographic index, in turn, can be used by distributed hydrological models to characterise the spatial distribution of terrain moisture. Many algorithms have been developed to calculate slope, aspect and upslope area from DEMs e specifically from gridded DEMs e but little work has gone into determining the uncertainty in these parameters, or the affect of this uncertainty in further applications. The accuracy of these parameters is dependent both on the algorithm and on the errors associated with the DEM itself. Since it is almost impossible to model all the errors associated with a given slope/aspect algorithm and since a DEM is normally only provided with a single rms error, simple error propagation is not adequate to determine the error associated with the derived topographic parameters. A more rigorous method of determining the affect of DEM errors on derived topographic parameters is with statistical analysis using Monte Carlo simulation and error realisations of the DEMs. In this research we demonstrate that the error sensitivity of slope decreases as the number of neighbours used in the algorithm increases, hence steepest neighbour algorithms, which are common in hydrology are more sensitive to DEM error than algorithms that use four or more neighbours. In contrast, the average error sensitivity of aspect to DEM error is not dependent on the algorithm used. However, while the mean variability of this sensitivity was lower for the steepest neighbour algorithms, their errors were spread over a greater variety of slopes while the eight neighbour algorithms had errors confined to flat regions. The error sensitivity of upslope area and topographic index is related to the use of steepest neighbour flow routing algorithm. 2005 Elsevier Ltd. All rights reserved.	algorithm;digital elevation model;map;monte carlo method;propagation of uncertainty;routing;simulation;software propagation;spatial variability;topography	Lynn D. Raaflaub;Michael J. Collins	2006	Environmental Modelling and Software	10.1016/j.envsoft.2005.02.003	digital elevation model;hydrology;propagation of uncertainty;topography;mathematics;statistics;remote sensing;monte carlo method	HCI	86.58129633675091	-59.143151777799076	2611
b474b209b1e138377b42835d42c1f04e9fe4fa47	hybridnet: integrating model-based and data-driven learning to predict evolution of dynamical systems		The robotic systems continuously interact with complex dynamical systems in the physical world. Reliable predictions of spatiotemporal evolution of these dynamical systems, with limited knowledge of system dynamics, are crucial for autonomous operation. In this paper, we present HybridNet, a framework that integrates data-driven deep learning and model-driven computation to reliably predict spatiotemporal evolution of a dynamical systems even with in-exact knowledge of their parameters. A data-driven deep neural network (DNN) with Convolutional LSTM (ConvLSTM) as the backbone is employed to predict the time-varying evolution of the external forces/perturbations. On the other hand, the model-driven computation is performed using Cellular Neural Network (CeNN), a neuro-inspired algorithm to model dynamical systems defined by coupled partial differential equations (PDEs). CeNN converts the intricate numerical computation into a series of convolution operations, enabling a trainable PDE solver. With a feedback control loop, HybridNet can learn the physical parameters governing the system’s dynamics in real-time, and accordingly adapt the computation models to enhance prediction accuracy for time-evolving dynamical systems. The experimental results on two dynamical systems, namely, heat convection-diffusion system, and fluid dynamical system, demonstrate that the HybridNet produces higher accuracy than the state-of-the-art deep learning based approach.	algorithm;artificial neural network;autonomous robot;cellular neural network;complex dynamics;computation;control system;convolution;convolutional neural network;deep learning;dynamical system;evolution;feedback;interaction;internet backbone;long short-term memory;model-driven integration;numerical analysis;numerical partial differential equations;real-time clock;real-time transcription;solver;system dynamics	Yun Long;Xueyuan She;Saibal Mukhopadhyay	2018			cellular neural network;computer science;machine learning;dynamical system;artificial intelligence;dynamical systems theory;deep learning;system dynamics;computation;artificial neural network;solver	ML	17.97622494334064	-24.99093664196274	2618
536807e72623afe885abc82a6ad075a2e48f8c17	optimizing feature selection in video-based recognition using max-min ant system for the online video contextual advertisement user-oriented system	nearest neighbor classifier nnc;contextual advertising;dwt;video based face recognition vbfr;feature selection fs;pzmi;max min ant system mmas;face recognition fr	Abstract The online-advertising has been grown to focus on multimedia interactive model with through the Internet. Our Online Video Advertisement User-oriented (OVAU) system combined the machine learning model for face recognition from camera, multimedia streaming protocols, and video meta-data storage technology. face recognition (FR) is an importance phase which can to enhance the performance of our system. Feature Selection (FS) problem for FR is solved by MMAS-FS algorithms based-on PZMI and DWT features. The features set are represented by digraph G ( E , V ). Each node used to show the features, and the ability to choose a combination of features is presented the edges connecting between two adjacent nodes. The heuristic information extracted from the selected feature vector as antu0027s pheromone. The feature subset optimal is selected by the shortest length features and best presentation of classifier. The best subset used to classify the face recognition used Nearest Neighbor Classifier (NNC). The experiments were analyzed on FS shows that our algorithm can be easily applied without the priori information of features. The execution assessed of our calculation is more effective than previous approaches for Video-based recognition based on FS problem.	ant colony optimization algorithms;computer data storage;contextual advertising;data storage technology;directed graph;discrete wavelet transform;experiment;facial recognition system;feature selection;feature vector;heuristic;internet;machine learning;nearest neighbour algorithm;online advertising;optimizing compiler;statistical classification;video clip	Bao Nguyen Le;Dac-Nhuong Le;Gia Nhu Nguyen;Vikrant Bhateja;Suresh Chandra Satapathy	2017	J. Comput. Science	10.1016/j.jocs.2016.10.016	facial recognition system;feature selection;the internet;feature (machine learning);computer science;k-nearest neighbors algorithm;ant;feature vector;advertising;heuristic;machine learning;artificial intelligence;pattern recognition	AI	38.3919710117455	-61.844184534457625	2619
6be0e7e1995aceb7f4d5dcda4f6d382361ed354b	quality metric-based fitness function for robust watermarking optimisation with bees algorithm	optimisation discrete cosine transforms image watermarking;discrete cosine transforms embedding strength parameters per block image watermarking dct domain fitness function optimisation problem peak signal to noise ratio psnr quality metric bees algorithm attacks;dct domain;discrete cosine transforms;peak signal to noise ratio;attacks;embedding strength parameters;psnr quality metric;optimisation problem;bees algorithm;per block image watermarking;fitness function	The design of a robust watermarking technique has been always suffering from the conflict between the watermark robustness and the quality of the watermarked image. In this study, the embedding strength parameters for per-block image watermarking in the discrete cosine transform (DCT) domain are optimised. A fitness function is proposed to best suit the optimisation problem. The optimum solution is selected based on the quality and the robustness achieved using that solution. For a given image block, the peak-signal-to-noise ratio (PSNR) is used as a quality metric to measure the imperceptibility for the watermarked block. However, the robustness cannot be measured for a single watermark bit using traditional metrics. The proposed method uses the PSNR quality metric to indicate the degree of robustness. Hence, optimum embedding in terms of quality and robustness can be achieved. To demonstrate the effectiveness of the proposed approach, a recent watermarking technique is modified, and then used as the embedding method to be optimised. The Bees algorithm is selected as the optimisation method and the proposed fitness function is applied. Experimental results show that the proposed method provides enhanced imperceptibility and robustness under different attacks.	bees algorithm;fitness function;mathematical optimization	Assem M. Abdelhakim;Hassan I. Saleh;Amin M. Nassar	2016	IET Image Processing	10.1049/iet-ipr.2015.0379	mathematical optimization;discrete mathematics;peak signal-to-noise ratio;computer science;theoretical computer science;bees algorithm;mathematics;fitness function	ML	42.241074301508945	-10.36646806614666	2620
d0e77d4283142c8af8c096012d73a9b00367f47e	use it or lose it: selective memory and forgetting in a perpetual learning machine		In a recent article we described a new type of dee p neural network– a Perpetual Learning Machine (PLM) – which is capable of learning ‘on the fly’ like a brain by existing in a state of Perpetual Stochastic Gradient Descent (PSGD). Here, by simulating the process of practice, we demonstrate both selective memory and selective forgetting when we introduce statistical recall biases during PSGD. Frequently recalled memo ries are remembered, whilst memories recalled rarely are forgotten. This results in a ‘use it or lose it’ stimulus driven memory process that is similar to human memory.	artificial neural network;dead-end elimination;simulation;stochastic gradient descent;while	Andrew J. R. Simpson	2015	CoRR		artificial intelligence;forgetting	ML	15.748852350020506	-73.29304520984095	2624
dcced5603fec6a21230079bd1a0804ccc04896bc	using radial nmr profiles to characterize pore size distributions	brain;oscillators;signal attenuation;numerical analysis;numerical integration;algorithms;diseases and disorders	Extracting information about axon diameter distributions in the brain is a challenging task which provides useful information for medical purposes; for example, the ability to characterize and monitor axon diameters would be useful in diagnosing and investigating diseases like amyotrophic lateral sclerosis (ALS) or autism. Three families of operators are defined by Ozarslan, whose action upon an NMR attenuation signal extracts the moments of the pore size distribution of the ensemble under consideration; also a numerical method is proposed to continuously reconstruct a discretely sampled attenuation profile using the eigenfunctions of the simple harmonic oscillator Hamiltonian: the SHORE basis. The work presented here extends Ozarlan’s method to other bases that can offer a better description of attenuation signal behaviour; in particular, we propose the use of the radial Spherical Polar Fourier (SPF) basis. Testing is performed to contrast the efficacy of the radial SPF basis and SHORE basis in practical attenuation signal reconstruction. The robustness of the method to additive noise is tested and analysed. We demonstrate that a low-order attenuation signal reconstruction outperforms a higher-order reconstruction in subsequent moment estimation under noisy conditions. We propose the simulated annealing algorithm for basis function scale parameter estimation. Finally, analytic expressions are derived and presented for the action of the operators on the radial SPF basis (obviating the need for numerical integration, thus avoiding a spectrum of possible sources of error).	additive white gaussian noise;algorithm;basis function;estimation theory;hamiltonian (quantum mechanics);lateral computing;lateral thinking;numerical integration;numerical method;radial (radio);signal reconstruction;simulated annealing;utility functions on indivisible goods	Rachid Deriche;John Treilhard	2012		10.1117/12.910673	mathematical optimization;telecommunications;numerical analysis;numerical integration;computer science;optics;oscillation	ML	85.84889462891988	-0.08685552111976684	2628
d90a4c1f03265d507a6da346a76c2379ce0c4fd7	the method of automatic knuckle image acquisition for continuous verification systems				Rafal Doroz	2018	Symmetry	10.3390/sym10110624		Logic	37.610405348979825	-91.30697435124725	2629
07f14b6105c0c8d09c3d0ad6d14651d394c9465f	superpixel segmentation using gaussian mixture model		Superpixel segmentation partitions an image into perceptually coherent segments of similar size, namely, superpixels. It is becoming a fundamental preprocessing step for various computer vision tasks because superpixels significantly reduce the number of inputs and provide a meaningful representation for feature extraction. We present a pixel-related Gaussian mixture model (GMM) to segment images into superpixels. GMM is a weighted sum of Gaussian functions, each one corresponding to a superpixel, to describe the density of each pixel represented by a random variable. Different from previously proposed GMMs, our weights are constant, and Gaussian functions in the sums are subsets of all the Gaussian functions, resulting in segments of similar size and an algorithm of linear complexity with respect to the number of pixels. In addition to the linear complexity, our algorithm is inherently parallel and allows fast execution on multi-core systems. During the expectation-maximization iterations of estimating the unknown parameters in the Gaussian functions, we impose two lower bounds to truncate the eigenvalues of the covariance matrices, which enables the proposed algorithm to control the regularity of superpixels. Experiments on a well-known segmentation dataset show that our method can efficiently produce superpixels that adhere to object boundaries better than the current state-of-the-art methods.	coherence (physics);computational complexity theory;computer vision;estimated;expectation–maximization algorithm;feature extraction;google map maker;iteration;leukemic hematopoietic stem cell;mixture model;multi-core processor;multiprocessing;normal statistical distribution;numerous;open-source software;parallel computing;pixel;preprocessor;segmentation action;semi-continuity;silo (dataset);truncation;weight function;biologic segmentation	Zhihua Ban;Jianguo Liu;Li Cao	2018	IEEE Transactions on Image Processing	10.1109/TIP.2018.2836306	pattern recognition;scale-space segmentation	ML	51.69261483405214	-70.39276606282162	2630
831de70d32af09835d727597fd895fc644e1a2cb	unexpected observations after mapping longsage tags to the human genome	chromosome mapping;computational biology bioinformatics;gene library;genome human;human genome;polymorphism;error rate;algorithms;sequence tagged sites;humans;cross validation;splice variant;combinatorial libraries;public libraries;computer appl in life sciences;genome sequence;microarrays;bioinformatics	SAGE has been used widely to study the expression of known transcripts, but much less to annotate new transcribed regions. LongSAGE produces tags that are sufficiently long to be reliably mapped to a whole-genome sequence. Here we used this property to study the position of human LongSAGE tags obtained from all public libraries. We focused mainly on tags that do not map to known transcripts. Using a published error rate in SAGE libraries, we first removed the tags likely to result from sequencing errors. We then observed that an unexpectedly large number of the remaining tags still did not match the genome sequence. Some of these correspond to parts of human mRNAs, such as polyA tails, junctions between two exons and polymorphic regions of transcripts. Another non-negligible proportion can be attributed to contamination by murine transcripts and to residual sequencing errors. After filtering out our data with these screens to ensure that our dataset is highly reliable, we studied the tags that map once to the genome. 31% of these tags correspond to unannotated transcripts. The others map to known transcribed regions, but many of them (nearly half) are located either in antisense or in new variants of these known transcripts. We performed a comprehensive study of all publicly available human LongSAGE tags, and carefully verified the reliability of these data. We found the potential origin of many tags that did not match the human genome sequence. The properties of the remaining tags imply that the level of sequencing error may have been under-estimated. The frequency of tags matching once the genome sequence but not in an annotated exon suggests that the human transcriptome is much more complex than shown by the current human genome annotations, with many new splicing variants and antisense transcripts. SAGE data is appropriate to map new transcripts to the genome, as demonstrated by the high rate of cross-validation of the corresponding tags using other methods.	biopolymer sequencing;cross-validation (statistics);exons;libraries;matching;public library;rna splicing;scientific publication;silo (dataset);tag (metadata);tail;tails;transcript;transcriptome;antisense therapy;mapped;sage extract;serial analysis of gene expression;triangulation	Céline Keime;Marie Sémon;Dominique Mouchiroud;Laurent Duret;Olivier Gandrillon	2007	BMC Bioinformatics	10.1186/1471-2105-8-154	serial analysis of gene expression;biology;polymorphism;human genome;molecular biology;whole genome sequencing;dna microarray;word error rate;computer science;bioinformatics;genomic library;alternative splicing;sequence-tagged site;genetics;cross-validation	Comp.	1.1433078604145641	-55.3842365946525	2631
317af917a9c82cc0932dbd3c61f12e787d1f1173	invariant features for searching in protein fold databases	search and retrieval;classification algorithm;spherical harmonics;spherical harmonic;web interface;data model;group integration;invariants;protein structure;structure comparison;d wigner matrices;algebraic method;protein search and retrieval;protein folding;protein protein similarity;structural similarity;invariant feature	The tremendous growth of 3D data models available on the internet requires more skills for fast retrieval and classification algorithms. Especially, the problem of finding structural similarities between proteins automatically, in order to predict their functional similarity is a challenging task. In this paper a new algebraic method for structural comparison between proteins based on invariant features computed by group integration with spherical harmonics and D-Wigner matrices is proposed. Our goal is to achieve good classification without alignment by using intrinsic, pose invariant features. We compare our method to DALI, PRIDE and the Gauss Integral-method in a classification and search task. Additionally we provide a web interface to test the proposed method.	algorithm;algorithmic efficiency;computation;data model;database;feature selection;lib sh;requirement;scop;user interface	Maja Temerinac-Ott;Marco Reisert;Hans Burkhardt	2007	Int. J. Comput. Math.	10.1080/00207160701351937	theoretical computer science;machine learning;pattern recognition;mathematics;spherical harmonics	ML	14.520851107115785	-57.064801964572574	2633
d485b1469f3d2b1eefdd544159f8fdb2b3de357d	motion jpeg2000 coding scheme based on human visual system for digital cinema	discrete information;estimation mouvement;transformacion discreta;discrete wavelet transform;storage access;image processing;informacion discreta;estimacion movimiento;subband decomposition;information discrete;procesamiento imagen;ondita discreta;discrete time;motion estimation;efecto contraste;traitement image;memory access;rate control;haute frequence;masquage;subbanda;descomposicion subbanda;human visual system;enmascaramiento;subband;discrete wavelet;discrete transformation;acces memoire;ondelette discrete;acceso memoria;aparato visual;masking;contrast effect;appareil visuel;decomposition sous bande;human vision system;tiempo discreto;temps discret;alta frecuencia;visual system;transformation discrete;sous bande;high frequency;motion jpeg2000;effet contraste	In this paper, the properties of human visual system such as insensitivity to high temporal frequency and contrast masking are exploited to improve the subjective coding efficiency of the Motion JPEG2000. Specifically, we detect motion information using DWT (Discrete Wavelet Transform) coefficients of the JPEG2000 and then determine the importance of the corresponding high frequency components based on contrast masking effect. If the motion turns out to be imperceptible by the human vision system, then we discard the corresponding subband data. Proposed scheme yields higher subjective quality at lower bit-rate than the conventional Motion JPEG2000 coding scheme. Also, the proposed algorithm needs lower memory access and complexity than the previous enhanced coding scheme using motion estimation.		Jong Han Kim;Sang Beom Kim;Chee Sun Won	2006		10.1007/11949534_87	computer vision;discrete time and continuous time;visual system;telecommunications;image processing;quarter-pixel motion;computer science;high frequency;masking;motion estimation;human visual system model;discrete wavelet transform;contrast effect	Graphics	46.30171623524997	-14.285354673120088	2637
6a0fd88268c81ab470d8c01159522e200686a1dd	automatic segmentation of subfigure image panels for multimodal biomedical document retrieval	semiconducteur ii vi;image features;cluster algorithm;medical imagery;4230;decision tree;genie biomedical;systeme aide decision;information retrieval;0130c;automatic segmentation;compose binaire;logique floue;cadmium sulfides;imagerie;biomedical imaging;optimization method;segmentation;fuzzy logic controller;cadmium sulfure;metodo optimizacion;algorithme;fuzzy logic;temps calcul;imagery;ii vi semiconductors;recherche documentaire;particle swarm optimizer;biomedical engineering;cd s;recherche information;computational complexity;busqueda documental;region of interest;particle swarm optimization;clinical decision support;automatic annotation;decision support systems;imagineria medica;methode optimisation;imagerie medicale;cds;algorithms;document retrieval;imagineria;tiempo computacion;computation time;binary compounds;segmentacion	Biomedical images are often referenced for clinical decision support (CDS), educational purposes, and research. They appear in specialized databases or in biomedical publications and are not meaningfully retrievable using primarily text-based retrieval systems. The task of automatically finding the images in an article that are most useful for the purpose of determining relevance to a clinical situation is quite challenging. This task can be done by automatically annotating images extracted from scientific publications with respect to their usefulness for CDS. As an important step toward achieving this goal, we proposed figure image analysis for contentbased image retrieval (CBIR) techniques. Extracted image features from the entire image and relevant local image regions can then be associated with identified biomedical concepts extracted from the meta-text in figure captions and discussion in the full text for improved hybrid (text and image) retrieval of biomedical articles. A challenge toward this goal is separating individual panels from a multi-panel figure that is often found as a single image in the biomedical article. In a previous study, the feasibility of automatically classifying images by usefulness (utility) in finding evidence was explored using supervised machine learning and achieved 84.3% accuracy using image captions for modality and 76.6% accuracy combining captions and image data for utility from articles over 2 years from a clinical journal. However, the figures images in this study had to be manually segmented into individual panels. In this work we present methods that add make robust our previous efforts reported here that, though successful, were limited in their scope and were unable to meet the challenges of segmenting figure illustrations, graphs, and charts. For the latter, we present a novel particle swarm optimization (PSO) clustering algorithm to locate related figure components Results from preliminary evaluation are very promising with the area under the ROC curve at 94.9% for regular (non-illustration) figure images and 92.1% accuracy for illustration images. More intensive tests are in progress to evaluate impact of automatic figure panel segmentation and use of ROI in image annotation and retrieval.	algorithm;automatic image annotation;autostereogram;chart;clinical decision support system;cluster analysis;content-based image retrieval;database;document retrieval;experiment;fitness function;graph (discrete mathematics);image analysis;machine learning;mathematical optimization;modality (human–computer interaction);multimodal interaction;particle swarm optimization;phase-shift oscillator;receiver operating characteristic;region of interest;relevance;scientific literature;supervised learning;text-based (computing)	Beibei Cheng;Sameer K. Antani;R. Joe Stanley;George R. Thoma	2011		10.1117/12.873685	fuzzy logic;document retrieval;computer vision;computer science;artificial intelligence;machine learning;decision tree;credit default swap;computational complexity theory;particle swarm optimization;segmentation;feature;algorithm;region of interest	Vision	39.87808966789623	-69.44171039805485	2638
5ca5b2cbbce69f59286cc8cc895b332e49d209e3	rate-distortion optimization with adaptive weighted distortion in high efficiency video coding	encoding optimization video coding psnr algorithm design and analysis transform coding color;video coding;distortion;image colour analysis;video coding distortion image colour analysis;h 264 mpeg 4 avc rate distortion optimization high efficiency video coding adaptive weighted distortion optimization algorithm rdo process coding efficiency color components distortion weight encoding complexity adaptive weighted distortion optimization method hevc reference software hm 8 0 coding schemes	This paper presents an adaptive weighted distortion optimization algorithm used in the Rate-Distortion Optimization (RDO) process of the High Efficiency Video Coding (HEVC). RDO is an important tool to improve the coding efficiency. Usually the distortion weights of different color components are equal or predetermined. In this paper, an adaptive weighted distortion optimization algorithm is introduced to improve the coding efficiency. The distortion weight is estimated according to the previous coded pictures belonging to the same temporal level, such that encoding complexity is almost unchanged. With the proposed adaptive weighted distortion optimization method, on average about 3.3% and up to 10.6% bit-saving are obtained based on the latest HEVC reference software, HM-8.0 and the corresponding common test conditions. The proposed algorithm can also be applied to other coding schemes such as H.264/MPEG-4 AVC.	adaptive algorithm;algorithmic efficiency;data compression;distortion;h.264/mpeg-4 avc;high efficiency video coding;mathematical optimization;raster document object;rate–distortion optimization;remote data objects	Bin Li;Jizheng Xu;Houqiang Li	2013	2013 IEEE International Symposium on Circuits and Systems (ISCAS2013)	10.1109/ISCAS.2013.6571885	computer vision;mathematical optimization;distortion;telecommunications;computer science;theoretical computer science;context-adaptive variable-length coding;coding tree unit;mathematics;context-adaptive binary arithmetic coding;multiview video coding	Vision	45.97461914219395	-18.127244733407775	2648
e4b6e9c034d545f529ff94d61810e8d73e55b9e1	an enhanced data visualization method for diesel engine malfunction classification using multi-sensor signals	biological patents;biomedical journals;text mining;multi sensor signals;europe pubmed central;citation search;malfunction classification;citation networks;diesel engine;research articles;abstracts;open access;data visualization;life sciences;clinical guidelines;feature subset score;full text;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	The various multi-sensor signal features from a diesel engine constitute a complex high-dimensional dataset. The non-linear dimensionality reduction method, t-distributed stochastic neighbor embedding (t-SNE), provides an effective way to implement data visualization for complex high-dimensional data. However, irrelevant features can deteriorate the performance of data visualization, and thus, should be eliminated a priori. This paper proposes a feature subset score based t-SNE (FSS-t-SNE) data visualization method to deal with the high-dimensional data that are collected from multi-sensor signals. In this method, the optimal feature subset is constructed by a feature subset score criterion. Then the high-dimensional data are visualized in 2-dimension space. According to the UCI dataset test, FSS-t-SNE can effectively improve the classification accuracy. An experiment was performed with a large power marine diesel engine to validate the proposed method for diesel engine malfunction classification. Multi-sensor signals were collected by a cylinder vibration sensor and a cylinder pressure sensor. Compared with other conventional data visualization methods, the proposed method shows good visualization performance and high classification accuracy in multi-malfunction classification of a diesel engine.	akaike information criterion;cylinder seal;data visualization;diesel fuel;experiment;extraction;fss - fatigue interferes with physical functioning;flying-spot scanner;imagery;leigh disease;necrotizing encephalopathy, infantile subacute, of leigh;nonlinear dimensionality reduction;nonlinear system;numerous;pattern recognition;pressure sensor device component;relevance;silo (dataset);subgroup;t-distributed stochastic neighbor embedding	Yiqing Li;Yu Ping Wang;Yanyang Zi;Mingquan Zhang	2015		10.3390/s151026675	text mining;medical research;computer science;bioinformatics;engineering;data science;data mining;information retrieval;data visualization	ML	10.857046477653073	-72.58409552067161	2650
b44a3be6ae054fe57b5181ec07cd110ee22d88db	evaluation of volitional control of hand with vertical force assist device for high tetraplegia	force muscles spinal cord injury injuries shoulder robots neck;patient rehabilitation;biomechanics;user acceptance volitional control evaluation vertical force assist device high tetraplegia motor function sensory function rehabilitative interventions residual volitional control;injuries;diseases;neurophysiology;patient rehabilitation biomechanics biomedical equipment diseases injuries medical control systems neurophysiology;biomedical equipment;adult biomechanical phenomena female hand humans male middle aged quadriplegia range of motion articular rehabilitation self help devices spinal cord injuries volition;medical control systems	Individuals with high tetraplegia have lost motor and sensory function below the neck. This population is highly disabled and requires some method of assistance to accomplish activities of daily living. To date, rehabilitative interventions available in the clinic are limited and have not been designed to exploit residual volitional control. The goal of this study was to validate the presence of volitional control at the hand in individuals with high tetraplegia when the arm is supported against gravity with a vertical assist device. Any volitional control is pertinent to the development of rehabilitative interventions for high tetraplegia because it can be utilized to improve the overall performance and user acceptance of the device. The results from this study provide evidence toward the possibility of an individual with high tetraplegia being able to position their hand and generate forces volitionally in a functional workspace if the weight of their arm is supported by a vertical assist device.	disabled persons;esthesia;quadriplegia;rehabilitation therapy;relevance;workspace	Swarna Solanki;Robert F. Kirsch	2012	2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/EMBC.2012.6346185	physical medicine and rehabilitation;engineering;biomechanics;physical therapy;physiology;neurophysiology;surgery	Robotics	12.886847640678434	-82.80216792540944	2653
f7311cc0d49ca5b9cb337cc52d83691ad124cd17	adaptive cfar for space-based multichannel sar–gmti	geophysical image processing;space based radar;texture;phase noise;phase noise radar clutter spaceborne radar radar cross section;remote sensing by radar;radar cross section;texture distribution space based multichannel sar gmti clutter data statistics multichannel space based synthetic aperture radar ground moving target indication adaptive constant false alarm rate detector space based sar gmti clutter measurements airborne based sar gmti clutter measurements adaptive cfar detector radarsat 2 data adaptive cfar algorithm heterogeneous clutter;compound distribution;synthetic aperture radar geophysical image processing geophysical techniques object detection radar clutter radar imaging remote sensing by radar;cfar;radar imaging;radar detection;radar clutter;texture cfar compound distribution phase noise radar clutter radar detection space based radar;object detection;geophysical techniques;spaceborne radar;synthetic aperture radar	This paper studies the statistics of clutter data measured with a multichannel space-based synthetic aperture radar (SAR) for the purpose of ground moving target indication (GMTI) and presents an algorithm that implements an adaptive constant false-alarm rate (CFAR) detector. It discusses the differences between airborne and space-based SAR-GMTI clutter measurements and proposes how to develop the statistics of the latter from widely published results for the former. Based upon one of the differences, it proposes an adaptive CFAR detector and demonstrates detection using measured RADARSAT-2 data. The adaptive CFAR algorithm does not require a texture distribution for heterogeneous clutter.	airborne ranger;algorithm;clutter;constant false alarm rate;moving target indication;synthetic data	Ishuwa C. Sikaneta;Christoph H. Gierull	2012	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2012.2195320	computer vision;space-based radar;synthetic aperture radar;constant false alarm rate;compound probability distribution;texture;radar imaging;phase noise;radar cross-section;physics;remote sensing	Visualization	72.86915251978778	-64.0023860707416	2654
05d9d219d99d9183f0c629c76121e94c6a005911	hevc fast fme algorithm using ime rd-costs based error surface fitting scheme	error surface fitting;sub accuracy pixel;hevc;fast block matching;fme	Motion Estimation (ME), which is composed of integer motion estimation (IME) and fractional motion estimation (FME), is the most computational intensive module in HEVC encoding procedure. In this paper, a new fast fractional pixel motion search method, that is based on a six-parameter two-dimension error surface model, is proposed. In our proposal, by solving the over-determined equations, nine integer-pixel rate-distortion costs (RDC), including the best integer-pixel search candidate and its eight neighboring integer pixels, are used to estimate the six parameters in the model. Then, we can obtain the minimal position on the fitted error surface equation, which is the quarter-pixel accurate search center. We provided three kinds of search patterns in the quarter-pixel search stage, which could take a tradeoff between the computational complexity and the prediction accuracy. Experimental results demonstrate that, as compared with HM reference software (HM-15.0), the three proposed FME patterns could reduce 35.1%, 29.4%, and 22.5% encoding time, while the corresponding compression efficiency losses in terms of BDBR are 3.04%, 0.79%, and 0.43%, respectively.	computation;computational complexity theory;distortion;formal methods europe;fractional fourier transform;high efficiency video coding;input method;motion estimation;multivariate interpolation;pixel;quarter-pixel motion;ruby document format;search algorithm	Yunpeng Li;Zhenyu Liu;Xiangyang Ji;Dongsheng Wang	2016	2016 Visual Communications and Image Processing (VCIP)	10.1109/VCIP.2016.7805574	computer vision;mathematical optimization;theoretical computer science;mathematics	Vision	47.715603136670175	-19.40905625982505	2659
703a39d40b869093e187924a884d107b487bf376	in vivo automated quantification of thermally damaged human tissue using polarization sensitive optical coherence tomography	machine learning;optical coherence tomography;thermal damaged tissue (human skin)	Over the decades numerous technologies have been performed for the quantification of skin injuries, but their poor sensitivity, specificity and accuracy limits their applications. Optical coherence tomography (OCT) can be potential tool for the identification but the modern high-speed OCT system acquires huge amount of data, which will be very time-consuming and tedious process for human interpretation. Our proposed method opens the possibility of fully automated quantitative analysis based on morphological features of thermally damaged tissue, which will become biomarker for the removal of non-viable skin. The proposed method is based on multi-level ensemble classifier by dissociating morphological features (A-line, B-scan, phase images) extracted from Polarization Sensitive Optical Coherence Tomography (PS-OCT) images. Our proposed classifier attained the average sensitivity, specificity and accuracy is 92.22%, 87.2% and 92.5%, respectively, in detecting the thermally damaged human skin. Moreover, we show that our classifier is one of the best possible classifier based on features extracted from PS-OCT images, which demonstrates the significance of PS-OCT data in detecting abnormality in human skin.		Kavita Dubey;Vishal Srivastava;Krishna Dalal	2018	Computerized medical imaging and graphics : the official journal of the Computerized Medical Imaging Society	10.1016/j.compmedimag.2018.01.002	computer vision;in vivo;artificial intelligence;optical coherence tomography;biomarker (medicine);medicine	Vision	35.14240078567853	-76.2728694837963	2662
61dc996a4f50603f601e11f91c1588d2f42a533e	adaptive analytical model-based control for si engine air–fuel ratio	time varying systems adaptive control delay system engines model based control;analytical models;modelizacion;linear systems;ford 2 l 14 engine;predictive control;optimisation;relacion aire combustible;state space methods;control predictiva;filtro kalman;optimizacion;rapport air combustible;carburant moteur;operating conditions;ignition;state space controller;time delays;filtre kalman;adaptive control;spark ignition;systeme commande adaptative modele reference;kalman filters;adaptive controller;espace etat;time varying systems;programmable control;commande predictive;online optimization;time varying systems adaptive control control system synthesis delays ignition internal combustion engines kalman filters linear systems predictive control state space methods;kalman filter;mezcla carburante;motor explosion interna;spark ignition engine air fuel ratio control;control lineal;delay system;time delay;time varying system;modelisation;carburante motor;model reference adaptive control systems;fuel mixture;ford 2 l 14 engine adaptive controller state space controller model predictive controller spark ignition engine air fuel ratio control linear model based controller time varying delay compensation time delays kalman filter;condition operatoire;engines;current measurement;internal combustion engines;systeme a retard;state space method;control adaptativo;methode espace etat;moteur combustion interne;control system synthesis;internal combustion engine;systeme parametre variable;state space;linear model;model based control;commande adaptative;sparks;commande lineaire;programmable control adaptive control analytical models engines sparks ignition predictive models delay current measurement velocity measurement;predictive models;optimization;temps retard;process model;melange carburant;sistema parametro variable;delay time;sistema con retardo;time varying delay;espacio estado;state space model;velocity measurement;condicion operatoria;linear control	An adaptive, state-space, model-predictive controller for spark ignition (SI) engine air-fuel ratio control is developed and presented in this brief. The linear model-based controller is an analytical controller that does not require online optimization. The model parameters for the predictive controller and the time-varying delay compensation are adapted based on the current measured engine speed and load. These process model parameters and time delays are identified from engine operating data. A Kalman filter is used to estimate the model and disturbance states of the system. The controller is demonstrated on a Ford 2-L I4 engine.	algorithm;broadcast delay;electronic control unit;engine control unit;feed forward (control);feedforward neural network;kalman filter;linear model;mathematical optimization;online optimization;operating point;pid;process modeling;state space	Kenneth R. Muske;James C. Peyton Jones;E. M. Franceschi	2008	IEEE Transactions on Control Systems Technology	10.1109/TCST.2007.912243	kalman filter;control engineering;open-loop controller;simulation;adaptive control;engineering;control theory	Robotics	72.06387928231393	-7.526809986211747	2663
ef6c1550b6547713c94a87fd192c17a94f960d9b	an advanced stochastic time-cost tradeoff analysis based on a cpm-guided genetic algorithm		This article presents an advanced stochastic time-cost tradeoff (ASTCT) method that performs time-cost tradeoff analysis by identifying optimal set(s) of construction methods for activities, hence reducing the project completion time and cost simultaneously. ASTCT involves a stochastic time-cost tradeoff analysis method based on a critical path method (CPM)-guided genetic algorithm (GA). It makes use of CPM schedule data exported from a project management software, and alternative construction methods obtained from estimators (i.e., normal and accelerated durations and costs) for each activity. It simulates schedule networks, identifies an optimal set of GA parameters (i.e., population size, crossover rate, mutation rate, and stopping rule), implements several GA cycles, and computes near-optimal solution(s) exhaustively. This study is of value to practitioners because ASTCT improves the computation time, reliability, and usability of existing GA-based time-cost tradeoff methods. The study is also of relevance to researchers because it facilitates experiments using different GA parameters expeditiously. Two test cases verify the usability and validity of the computational methods.	computation;critical path method;experiment;genetic algorithm;relevance;software release life cycle;test case;time complexity;usability	Hyung-Guk Lee;Chang-Yong Yi;Dong-Eun Lee;David Arditi	2015	Comp.-Aided Civil and Infrastruct. Engineering	10.1111/mice.12148	mathematical optimization;simulation;computer science;artificial intelligence;machine learning;data mining;algorithm;statistics	SE	17.60994472415049	-7.807702813514449	2664
0a7f35d2c9380729cc46793630f769a498bdd798	planning trajectories in dynamic environments using a gradient method	navegacion;trajectoire;algorithm performance;soccer;gradient method;path planning;degree of freedom;intelligence artificielle;robotics;planification trajectoire;methode gradient;dynamic environment;navigation;prevencion esquiva colision;trajectory;metodo gradiente;resultado algoritmo;football;prevention esquive collision;performance algorithme;robotica;artificial intelligence;trayectoria;collision avoidance;robotique;inteligencia artificial;futbol	In this article we propose an extension for a path planning method based on the LPN-algorithm to have better performance in a very dynamic environment. The path planning method builds a navigation function that drives the robot toward the goal avoiding the obstacles. The basic method is very fast and efficient for a robot with few degrees of freedom. The proposed extension integrates the obstacle dynamics in the planning method to have better performances in very dynamic environments. Experiments have shown the effectiveness of the proposed extension in a very dynamic environment, given by RoboCup	algorithm;automated planning and scheduling;computation;experiment;gradient method;loss function;motion planning;navigation function;performance;robot;velocity (software development)	Alessandro Farinelli;Luca Iocchi	2003		10.1007/978-3-540-25940-4_28	navigation;simulation;computer science;gradient method;artificial intelligence;trajectory;motion planning;degrees of freedom;robotics	Robotics	60.455405770591135	-21.126845132173333	2665
3eae7e69464c308b54e9fd05068461d7da6cb925	extreme lower probabilities	engineering;52b11;fuzzy set;procesamiento informacion;loi probabilite;ley probabilidad;mathematics and statistics;combinatorial problems;conjunto difuso;ensemble flou;satisfiability;ingenierie;permutation;combinatorial problem;probleme combinatoire;problema combinatorio;imprecise probability;lower probabilities;extreme points;extreme point;probability distribution;imprecise probabilities;permutacion;information processing;ingenieria;sistema difuso;systeme flou;60a99;traitement information;non additive measures;fuzzy system;14n10;58j70	We consider lower probabilities on finite possibility spaces as models for the uncertainty about the state. These generalizations of classical probabilities can have some interesting properties; for example: k-monotonicity, avoiding sure loss, coherence, permutation invariance. The sets formed by all the lower probabilities satisfying zero or more of these properties are convex. We show how the extreme points and rays of these sets-the extreme lower probabilities-can be calculated and we give an illustration of our results.		Erik Quaeghebeur;Gert de Cooman	2008	Fuzzy Sets and Systems	10.1016/j.fss.2007.11.020	extreme point;combinatorics;discrete mathematics;information processing;mathematics;fuzzy control system;statistics;algebra	NLP	1.3784954911310316	-16.554800888352464	2668
73864c4cf19e4e19fc0e835fdb04a04f990ac5f4	developing sbml beyond level 2: proposals for development	level 2;computer model;systems biology markup language;level 1;model composition	The Systems Biology Markup Language (SBML) is an XMLbased exchange format for computational models of biochemical networks. SBML Level 2, whose definition was established in June 2003, includes several enhancements to the original Level 1. This paper includes a brief overview of Level 2. Several proposals are under development to extend SBML to create Level 3. These include diagrams, 2-D and 3-D spatial characteristics, arrays, model composition and multi-component chemical species. This paper describes the current proposals for the last two features.	cpu cache;computational model;diagram;markup language;open-source religion;requirement;sbml;systems biology;transduction (machine learning)	Andrew Finney	2004		10.1007/978-3-540-25974-9_21	computer simulation;natural language processing;sbml;simulation;computer science;theoretical computer science;systems biology ontology	AI	-0.6096537082396742	-65.51002290907272	2673
42465348d5cd85a497943cc45761b77d7e48e608	volatility hedging model for precious metal futures returns		This study attempts to evaluate appropriately the optimal hedge ratio and hedging effectiveness between spot and futures returns of precious metals with a special concern in gold, silver, and platinum. We employ the Markov switching dynamic copula model to measure the dependence structure between spot and futures returns of these precious metals and then evaluate the hedging strategies. Evidence from this study can bring about the contribution to the discussion on this area.	futures studies;volatility	Roengchai Tansuchat;Paravee Maneejuk;Songsak Sriboonchitta	2016		10.1007/978-3-319-49046-5_57	precious metal;hedge (finance);financial economics;economics;futures contract;volatility (finance);markov chain	Robotics	-0.10316189041751744	-8.918678671991765	2674
a3f919a9442532b69bd932b65595f61ced246820	development of an active micromixer by dielectrophrosis particle manipulating	microelectrodes;dielectrophoresis;dielectrophoresis force;mems process;micropiv technology;fluids;microelectrodes electrophoresis masks microchannel flow;active micromixer;photomask;mixing performance;dep;traveling wave;fluids electric fields dielectrophoresis microelectrodes force permittivity;electrophoresis;frequency 10 mhz;micromixer chip;microchannel;electrode array chip;particle manipulating;force;chip;micromixer dep particle manipulating;polymer particle;electric fields;conductivity;microchannel flow;voltage input;frequency 100 khz;particle manipulating micromixer;flow phenomena;frequency 10 mhz active micromixer particle manipulating micromixer mems process photomask electrode array chip microchannel dielectrophoresis force polymer particle conductivity traveling wave dep micromixer chip mixing performance voltage input flow phenomena micropiv technology frequency 100 khz;masks;traveling wave dep;biomedical application;permittivity;micromixer	A particle manipulating micromixer was developed successfully in this study. The present device was fabricated by simply MEMS process. Only two photomasks, one for electrode array chip and another for micro-channel, were needed to manufacture the micromixer. Dielectrophoresis (DEP) force was used to manipulate the particles in the micro-channel. In this experience, polymer particles with diameter of 1 n m were tested in the fluid with conductivity of 0.10mS/cm. The results show that the symmetrical vortices were observed at frequency of 100 kHz. The traveling wave DEP (twDEP) occurred when the input frequency was 10MHz. Micro-mixer chip which can stir fluid to achieve mixing was fabricated successfully by combining an electro array chip and Y-type micro-channel. The mixing performance depends on the voltage input. The flow phenomena were obtained and analyzed by micro-PIV technology. In the future, different cells can be manipulated by the present micromixer to achieve the rapid mixing with different driving frequency. It is useful in the biological or biomedical application.	executable space protection;microelectromechanical systems;polymer;vortex	Szu-I Yeh;Ching-Jiun Lee;Yu-Jui Fan;Cheng-Peng Liu;Horn-Jiunn Sheen	2010	2010 IEEE 5th International Conference on Nano/Micro Engineered and Molecular Systems	10.1109/NEMS.2010.5592492	chip;materials science;dielectrophoresis;microchannel;electrophoresis;microelectrode;analytical chemistry;data execution prevention;electric field;conductivity;wave;photomask;force;physics;quantum mechanics;permittivity;fluid mechanics	Visualization	91.6898151037346	-16.82069995794125	2679
1c0f0922d7c783346cece8dcf6c326d790aa2bc5	directional statistical gabor features for texture classification		Abstract In texture classification, methods using multi-resolution directional (MRD) filters such as Gabor have not often shown significantly better performance than simple methods using local binary patterns, although they have a robust theoretical background and high computational complexity. We expect that this is because such methods usually make use of only the modulus parts of complex-valued MRD-filtered images and do not fully utilize their phase parts and other directional information. This letter presents a rotation-invariant feature using four types of directional statistics obtained from both the modulus and phase parts of Gabor-filtered images. First, modulus statistics, scale-shift cross-correlations, and orientation-shift cross-correlations are computed over all directions for each pixel of Gabor-filtered images, and global autocorrelations are computed over all pixels of each Gabor-filtered image. Global means and standard deviations for the three types of directional statistics and directional means and standard deviations for the global autocorrelations are then computed to form a feature vector. Experimental results with Brodatz, STex, CUReT, KTH-TIPS, UIUC, UMD, ALOT, and Kylberg databases show that the proposed method yields excellent performance compared with several conventional methods.		Nam Chul Kim;Hyun Joo So	2018	Pattern Recognition Letters	10.1016/j.patrec.2018.05.010	local binary patterns;artificial intelligence;computer vision;directional statistics;mathematics;standard deviation;pixel;modulus;feature vector;computational complexity theory;pattern recognition	Vision	36.78446137520791	-59.917535730792686	2680
806bf27919283dd4db3aef8a64e053ca5052840d	the large-scale crowd behavior perception based on spatio-temporal viscous fluid field	eigenvalues and eigenfunctions;spatiotemporal phenomena force eigenvalues and eigenfunctions analytical models vectors tracking abstracts;pattern clustering;video surveillance;image motion analysis;crowd motion pattern crowd behavior analysis spatio temporal feature;video surveillance eigenvalues and eigenfunctions feature extraction image motion analysis matrix algebra pattern clustering;matrix algebra;feature extraction;latent dirichlet allocation model large scale crowd behavior perception spatio temporal viscous fluid field automatic surveillance video analysis crowd motion modeling crowd motion pattern modelling crowd behavior apperance large scale crowd events spatio temporal variation matrix video signal local fluctuation eigenvalue analysis principal fluctuation extraction abstract fluid field interaction force shear force codebook neighboring pixel clustering spatio temporal features	Over the past decades, a wide attention has been paid to crowd control and management in the intelligent video surveillance area. Among the tasks for automatic surveillance video analysis, crowd motion modeling lays a crucial foundation for numerous subsequent analysis but encounters many unsolved challenges due to occlusions among pedestrians, complicated motion patterns in crowded scenarios, etc. Addressing the unsolved challenges, the authors propose a novel spatio-temporal viscous fluid field to model crowd motion patterns by exploring both appearance of crowd behaviors and interaction among pedestrians. Large-scale crowd events are hereby recognized based on characteristics of the fluid field. First, a spatio-temporal variation matrix is proposed to measure the local fluctuation of video signals in both spatial and temporal domains. After that, eigenvalue analysis is applied on the matrix to extract the principal fluctuations resulting in an abstract fluid field. Interaction force is then explored based on shear force in viscous fluid, incorporating with the fluctuations to characterize motion properties of a crowd. The authors then construct a codebook by clustering neighboring pixels with similar spatio-temporal features, and consequently, crowd behaviors are recognized using the latent Dirichlet allocation model. The convincing results obtained from the experiments on published datasets demonstrate that the proposed method obtains high-quality results for large-scale crowd behavior perception in terms of both robustness and effectiveness.	closed-circuit television;cluster analysis;codebook;experiment;latent dirichlet allocation;pixel;quantum fluctuation;the matrix;the unsolved;video content analysis	Hang Su;Hua Yang;Shibao Zheng;Yawen Fan;Sha Wei	2013	IEEE Transactions on Information Forensics and Security	10.1109/TIFS.2013.2277773	computer vision;simulation;feature extraction;computer science;machine learning	Vision	38.414426208680865	-47.46725667885943	2689
719fd645c4da6575ab0e774891ba30d7dfcc53aa	loam: lidar odometry and mapping in real-time		We propose a real-time method for odometry and mapping using range measurements from a 2-axis lidar moving in 6-DOF. The problem is hard because the range measurements are received at different times, and errors in motion estimation can cause mis-registration of the resulting point cloud. To date, coherent 3D maps can be built by off-line batch methods, often using loop closure to correct for drift over time. Our method achieves both low-drift and low-computational complexity without the need for high accuracy ranging or inertial measurements. The key idea in obtaining this level of performance is the division of the complex problem of simultaneous localization and mapping, which seeks to optimize a large number of variables simultaneously, by two algorithms. One algorithm performs odometry at a high frequency but low fidelity to estimate velocity of the lidar. Another algorithm runs at a frequency of an order of magnitude lower for fine matching and registration of the point cloud. Combination of the two algorithms allows the method to map in real-time. The method has been evaluated by a large set of experiments as well as on the KITTI odometry benchmark. The results indicate that the method can achieve accuracy at the level of state of the art offline batch methods.	algorithm;benchmark (computing);closing (morphology);closing the loop;cloud computing;coherence (physics);computation;computational complexity theory;distortion;experiment;kalman filter;map;motion estimation;odometry;online and offline;point cloud;real-time clock;real-time computing;real-time transcription;simultaneous localization and mapping;velocity (software development)	Ji Zhang;Sanjiv Singh	2014		10.15607/RSS.2014.X.007	computer science;computer vision;point cloud;order of magnitude;artificial intelligence;inertial frame of reference;simultaneous localization and mapping;motion estimation;ranging;odometry;lidar	Robotics	52.75496183448741	-45.44298710330465	2692
a2955d93e9874621d54144d7f88eae29cbd045f3	increasing threat of landfalling typhoons in the western north pacific between 1974 and 2013		Abstract Long-term changes between 1974 and 2013 were investigated in western North Pacific typhoons making landfall in East and Southeast Asia. Landfalling typhoon parameters, including the percentage of typhoons making landfall, the annual mean landfall intensity (LFI), and the annual accumulated power dissipation index at land, all increased significantly (at the 99% confidence level), by 14%, 17%, and 94%, respectively, over the study period. The increase in probability of a typhoon making landfall was attributed to an eastward shift of the typhoon genesis location. The LFI was decomposed into the product of the intensification rate and intensification duration. The product reproduced variations in the observed LFI well, and the correlation coefficient was high at 0.82. Although the intensification duration decreased slightly, an unprecedented increase in the intensification rate was observed, this increased the LFI. Warming of the upper ocean in the western North Pacific typhoon main intensification region, giving a higher tropical cyclone heat potential, yielded better oceanic conditions and overcame the worsening atmospheric conditions (increasing vertical wind shear), allowing typhoons to intensify. The increase in the annual accumulated power dissipation index was mainly caused by the increase in the LFI, and the annual number of typhoons and typhoon duration contributed much less. Increasing typhoon landfalling activities might heighten the threat posed by typhoons to populations and infrastructure in coastal regions.		Xingru Feng;Mingjie Li;Baoshu Yin;Dezhou Yang;Hongwei Yang	2018	Int. J. Applied Earth Observation and Geoinformation	10.1016/j.jag.2018.01.006	hydrology;tropical cyclone;climatology;geography;wind shear;landfall;typhoon	Logic	83.86923333133704	-56.588986597291324	2693
426b6d514dc4fe551b9db85d1139ec9502fd264c	gender determination using a support vector machine variant	ikee lib auth gr;conference proceedings articles;βκπ;ικee;websearch;support vector;hilbert space;optimization problem;bkp;vectors optimization kernel support vector machines training face error analysis;auth;kernel fisher discriminant analysis;βιβλιοθήκη και κέντρο πληροφόρησης;ιδρυματικό καταθeτήριο;support vector machine;απθ;library and information center;aristotle university of thessaloniki ικee;ikee;institutional repository;kernel fisher discriminant analysis gender determination support vector machine variant fisher discriminant ratio nonlinear decision surfaces arbitrary hilbert spaces mercer kernels;support vector machines feature extraction hilbert spaces human computer interaction optimisation pattern classification	In this paper a modified class of Support Vector Machines (SVMs) inspired from the optimization of Fisher's discriminant ratio is presented. Moreover, we present a novel class of nonlinear decision surfaces by solving the proposed optimization problem in arbitrary Hilbert spaces defined by Mercer's kernels. The effectiveness of the proposed approach is demonstrated by comparing it with the standard SVMs and other classifiers, like Kernel Fisher Discriminant Analysis (KFDA) in gender determination.	decision boundary;hilbert space;linear discriminant analysis;linear separability;mathematical optimization;nonlinear system;optimization problem;support vector machine	Stefanos P. Zafeiriou;Anastasios Tefas;Ioannis Pitas	2008	2008 16th European Signal Processing Conference		mathematical optimization;kernel fisher discriminant analysis;machine learning;pattern recognition;mathematics	AI	23.15894212379474	-39.72200165437995	2695
3bc197a296198d9a7feb530d41f81912091f97e3	expressive power of first-order recurrent neural networks determined by their attractor dynamics	attractors;turing machines;neural computation;learning;spatiotemporal patterns;evolving systems;ω languages;analog computation;expressive power;recurrent neural networks	We characterize the attractor-based expressive power of several models of recurrent neural networks.The deterministic rational-weighted networks are Muller Turing equivalent.The deterministic real-weighted and evolving networks recognize the class of B C ( ź 2 0 ) neural ω languages.The nondeterministic rational and real networks recognize the class of Σ 1 1 neural ω-languages. We provide a characterization of the expressive powers of several models of deterministic and nondeterministic first-order recurrent neural networks according to their attractor dynamics. The expressive power of neural nets is expressed as the topological complexity of their underlying neural ω-languages, and refers to the ability of the networks to perform more or less complicated classification tasks via the manifestation of specific attractor dynamics. In this context, we prove that most neural models under consideration are strictly more powerful than Muller Turing machines. These results provide new insights into the computational capabilities of recurrent neural networks.	analog computer;artificial intelligence;artificial neural network;british informatics olympiad;computational complexity theory;discharger;emergence;first-order predicate;hd radio;model of computation;patterns of evolution;recurrent neural network;recursion;sigmoid function;spatiotemporal pattern;synaptic package manager;synaptic weight;topological graph theory;turing machine	Jérémie Cabessa;Alessandro E. P. Villa	2016	J. Comput. Syst. Sci.	10.1016/j.jcss.2016.04.006	cellular neural network;types of artificial neural networks;computer science;turing machine;artificial intelligence;recurrent neural network;machine learning;time delay neural network;deep learning;programming language;attractor;expressive power;algorithm;models of neural computation	AI	17.914695098235576	-68.75154648765486	2696
9a8523a15e74a99408858d61b031476c3642cf7e	a soa-based ia vulnerability configuration management in e-government	configuration management	Liquid fuel is distributed in a fluidized bed of refractory particles through the gas constriction plate of a fluidized bed incinerator. The fuel is supplied from a distribution system of circulation pipes situated well below the constriction plate and passed to the fluidized bed through risers by means of a small amount of driving agent, preferably steam, of high pressure supplied from a separate distribution system independently of the primary combustion air and the fluidizing gas. The driving agent and the fuel are passed into the risers through nozzles at the lower end of the risers.	configuration management;e-government	Namho Yoo	2007			waste management;combustion;e-government;fluidized bed;incineration;liquid fuel;nozzle;environmental science	SE	84.22652677171315	-16.987354178055593	2699
0b7b5ce624863b0051b85f7e709923fe67575178	gff2ps: visualizing genomic annotations	computer program;interfase usuario;representation graphique;visualizacion;genome annotation;user interface;representacion grafica;annotation;secuencia nucleotido;nucleotide sequence;sequence nucleotide;visualization;descripcion;visualisation;genome;interface utilisateur;genoma;description;programa computador;graphics;programme ordinateur	gff2psis a program for visualizing annotations of genomic sequences. The program takes the annotated features on a genomic sequence in GFF format as input, and produces a visual output in PostScript. While it can be used in a very simple way, it also allows for a great degree of customization through a number of options and/or customization files.	general feature format;postscript	Josep F. Abril;Roderic Guigó	2000	Bioinformatics	10.1093/bioinformatics/16.8.743	biology;visualization;computer science;bioinformatics;theoretical computer science;world wide web;genetics	Comp.	-4.38808421609995	-56.84846082234379	2704
18c948e89d7beea427290f5ec24631ebe2d510c3	non-monotone differential evolution	non monotone differential evolution;differential evolution;premature convergence;search space;evolutionary algorithms;global optimization;evolutionary algorithm	The Differential Evolution algorithm uses an elitist selection, constantly pushing the population in a strict downhill search, in an attempt to guarantee the conservation of the best individuals. However, when this operator is combined with an exploitive mutation operator can lead to premature convergence to an undesired region of attraction. To alleviate this problem, we propose the Non-Monotone Differential Evolution algorithm. To this end, we allow the best individual to perform some uphill movements, greatly enhancing the exploration of the search space. This approach further aids algorithm's ability to escape undesired regions of the search space and improves its performance. The proposed approach utilizes already computed pieces of information and does not require extra function evaluations. Experimental results indicate that the proposed approach provides stable and reliable convergence.	differential evolution;premature convergence;search algorithm;selection (genetic algorithm);monotone	Michael G. Epitropakis;Vassilis P. Plagianakos;Michael N. Vrahatis	2008		10.1145/1389095.1389195	differential evolution;mathematical optimization;computer science;artificial intelligence;evolutionary algorithm;mathematics;algorithm;premature convergence	ML	26.461182111090338	-4.297234557702551	2705
049194674f32a155e6c209821968cd67508a7662	careful seeding based on independent component analysis for k-means clustering	pattern clustering;electronic mail;measurement;k means;k means clustering careful seeding independent component analysis;independent component analysis;clustering algorithms algorithm design and analysis iris independent component analysis accuracy electronic mail measurement;accuracy;pattern clustering independent component analysis;seeding k means k means independent component analysis;clustering algorithms;iris;k means clustering;algorithm design and analysis;seeding;careful seeding	The k-means method is a widely used clustering technique because of its simplicity and speed. However, the clustering result depends heavily on the chosen initial value. In this report, we propose a seeding method with independent component analysis for the k-means method. Using a benchmark dataset, we evaluate the performance of our proposed method and compare it with other seeding methods.	algorithm;benchmark (computing);cluster analysis;computer cluster;independent component analysis;k-means clustering	Takashi Onoda;Miho Sakai;Seiji Yamada	2010	2010 International Conference on Multimedia Technology	10.1109/WI-IAT.2010.102	correlation clustering;computer science;machine learning;pattern recognition;data mining;k-means clustering	Robotics	4.367825213272258	-39.826382515262125	2706
fcc56e791854b186871050d8d3694091ebb39bb5	hdr-cielab and hdr-ipt: simple models for describing the color of high-dynamic-range and wide-color-gamut images.		Proposed method – hdr-CIELAB and hdr-IPT • Addressing HDR questions – Hard intercepts at zero luminance/lightness – Uncertain applicability for color brighter than diffuse white • Replacing the power-function nonlinearities in CILAB and IPT with a more physiologically plausible hyperbolic function – Michaelis-Menten equation	information processes and technology	Mark D. Fairchild;David R. Wyble	2010			computer vision;computer science;artificial intelligence;lightness;luminance;high dynamic range;hyperbolic function;gamut	Vision	62.21501482776121	-60.304446366489714	2710
813f514932e0e0d2b40e908316d4213b2649f511	coded df-expression for binary and multi-valued picture	bit plane;hierarchical structure;zirconium;image coding;information systems;image processing;data compression;multivalued picture;decoding;natural images;trees mathematics;data representation;three dimensional;binary coding;information systems computer science image processing data structures decoding data compression zirconium;data structures;trees mathematics computerised picture processing data compression encoding;computerised picture processing;computer science;binary pictures;depth first picture expression;encoding;compact quadtree;bit plane multivalued picture computerised picture processing depth first picture expression compact quadtree data compression data representation binary pictures binary coding gray image coding;gray image coding	The depth-first picture expression (DF-expression) is a compact quadtree expression providing a high data compression capability. It is primarily a hierarchically structured data representation for binary pictures. The authors first review the basic ideas of the DF-expression, and its potential data-compression competence. Then the practical problem is studied of how much reduction can be achieved data by converting the expression to a binary coding. It is also shown that there are two directions in the DF-expression strategy for the gray-image coding. One is the bit-plane based, and the other is the three-dimensional (three-valued) DF-expressions. For most of the natural images, the bit-plane based strategy is more effective than the three-dimensional one. The reason for that is discussed. >	bitwise operation;direction finding	Eiji Kawaguchi;Rin-ichiro Taniguchi	1988		10.1109/ICPR.1988.28464	data compression;three-dimensional space;computer vision;binary code;discrete mathematics;bit plane;image processing;computer science;theoretical computer science;zirconium;external data representation;information system;encoding;statistics	Robotics	39.53262186233722	-18.571402029573367	2712
9d498a5eabf57e4db2458a46cf7adb895d58ed80	a fuzzy-based possibility measure approach for multi-expert multi-criteria selection problem	consensus;decision making programming companies supply chains joints mathematical model;satisfaction functions;possibility measure;fuzzy set theory;supply chain management decision making fuzzy set theory mathematical programming;fuzzy logic;multi expert;fuzzy environment fuzzy based possibility measure approach multiexpert multicriteria selection problem supply chain coordination scc production and operations management pom goal programming approach memc decision making problem;mathematical programming;goal programming;ccsd method;supply chain coordination;supply chain management;ccsd method supply chain coordination multi expert consensus possibility measure fuzzy logic goal programming satisfaction functions	Nowadays, supply chain coordination (SCC) is considered by researchers and practitioners as one of the active research topics in production and operations management (POM). This paper addresses the development of a fuzzy-based possibility measure with goal programming approach dedicated to selection problem. More specifically, the selection problem is formulated as a multi-expert multi-criteria (MEMC) decision making problem and solved by combing consensus-based possibility measure with goal programming in a fuzzy environment. To illustrate the applicability of the proposed approach, a simple example is presented and the numerical results analyzed.	agent-based model;common criteria;goal programming;numerical analysis;real life;selection algorithm	Idris Igoulalene;Lyès Benyoucef	2013	2013 IEEE International Conference on Systems, Man, and Cybernetics	10.1109/SMC.2013.131	fuzzy logic;mathematical optimization;supply chain management;consensus;computer science;artificial intelligence;goal programming;mathematics;fuzzy set;fuzzy set operations	Robotics	-4.015105604972744	-17.120542860243596	2717
489baaf7ca4b6ed0ff3cef48a8b078ca09b4c080	video question answering using a forget memory network		Visual question answering combines the fields of computer vision and natural language processing. It has received much attention in recent years. Image question answering (Image QA) targets to automatically answer questions about visual content of an image. Different from Image QA, video question answering (Video QA) needs to explore a sequence of images to answer the question. It is difficult to focus on the local region features which are related to the question from a sequence of images. In this paper, we propose a forget memory network (FMN) for Video QA to solve this problem. When the forget memory network embeds the video frame features, it can select the local region features that are related to the question and forget the irrelevant features to the question. Then we use the embedded video and question features to predict the answer from multiple-choice answers. Our proposed approaches achieve good performance on the MovieQA [21] and TACoS [28] dataset.		Yuanyuan Ge;Youjiang Xu;Yahong Han	2017		10.1007/978-981-10-7299-4_33	natural language processing;question answering;information retrieval;artificial intelligence;computer science	NLP	26.84002700113964	-53.39997153442678	2720
908c9036e622e82e88d932fce2a24dff22c556be	jbiowh: an open-source java framework for bioinformatics data integration	database management systems;pharmaceutical preparations;databases as topic;user computer interface;computational biology;programming languages	The Java BioWareHouse (JBioWH) project is an open-source platform-independent programming framework that allows a user to build his/her own integrated database from the most popular data sources. JBioWH can be used for intensive querying of multiple data sources and the creation of streamlined task-specific data sets on local PCs. JBioWH is based on a MySQL relational database scheme and includes JAVA API parser functions for retrieving data from 20 public databases (e.g. NCBI, KEGG, etc.). It also includes a client desktop application for (non-programmer) users to query data. In addition, JBioWH can be tailored for use in specific circumstances, including the handling of massive queries for high-throughput analyses or CPU intensive calculations. The framework is provided with complete documentation and application examples and it can be downloaded from the Project Web site at http://code.google.com/p/jbiowh. A MySQL server is available for demonstration purposes at hydrax.icgeb.trieste.it:3307. Database URL: http://code.google.com/p/jbiowh.	application programming interface;bioinformatics;cpu (central processing unit of computer system);central processing unit;data sources;database validation plan;desktop computer;documentation;handling (psychology);high-throughput computing;java platform, enterprise edition;java programming language;java tea;kegg;list of java apis;mysql;national center for biotechnology information;open-source software;parser;parsing;programmer device component;published database;question (inquiry);relational database;server (computing);throughput;type iii site-specific deoxyribonuclease;uniform resource locator	Roberto Vera;Yasset Pérez-Riverol;Sonia Pérez;Balázs Ligeti;Attila Kertész-Farkas;Sándor Pongor	2013		10.1093/database/bat051	computer science;bioinformatics;data mining;database;world wide web;information retrieval;database design	DB	-3.334659259626436	-59.5670963923373	2721
b3a0bf4e7a95c08ea24e1a6f32a35a551fa99545	rigid-body point-based registration: the distribution of the target registration error when the fiducial registration errors are given	distribution;target registration error tre;rigid body;statistical independence;standard deviation;target registration error;mean square;higher order;fiducial registration error fre;registration;root mean square;coordinate system	"""Medical guidance systems often employ several data sources using different coordinate systems. In order to map positions from one coordinate system to the other, these guidance systems usually employ rigid-body point-based registration, using pairs of fiducial points: pairs which describe the same physical positions, but in different coordinate systems. The customary test for the quality of the registration is the fiducial registration error (FRE), which is the root-mean-square of the mismatch between the fiducials in each pair (after the registration). The FRE, however, does not give an answer to the question which is usually of interest, and that is the accuracy at a """"target"""" point which is not part of the set of fiducial points. The statistics of the target registration error (TRE) have been studied before and approximate expressions were derived, but those expressions require as input the unknown true fiducial positions. In the present paper, it is proven that by replacing these unknowable true positions with the known measured positions in the expression for mean-square TRE, a higher order approximation is achieved. In other words, it is shown that more accurate estimates are obtained by using less accurate, but available, inputs. Furthermore, in previous approximations FRE and TRE were shown to be statistically independent, whereas here, due to the higher approximation level, it is shown that a slight dependence exists. Thus, the knowledge of FRE can in fact be employed to improve predictions of the TRE statistics. These results are supported by simulations and hold even for fiducial localization error (FLE) distributions with large standard deviations."""	approximation algorithm;estimated;fiducial markers;fiducial marker;guidance system;image registration;muscle rigidity;simulation;tre;customary;registration - actclass	A. Seginer	2011	Medical image analysis	10.1016/j.media.2011.01.001	distribution;independence;computer vision;rigid body;higher-order logic;root mean square;coordinate system;mathematics;standard deviation;statistics	Vision	45.69430331560379	-80.8327207110138	2723
36e0bb263be632702e59a2b31eae0b8854eaacb9	practice summary: flight trajectory optimization	dynamic programming;flight management system;flight cost optimization	We developed and implemented a novel algorithm to control the trajectory of an aircraft flight to reduce the costs it incurs. Our algorithm computes the values for the parameters to be entered into the aircraft's flight management system to minimize the fuel and schedule-adherence costs incurred on the flight. We implemented it in a prototype software system that is currently in use at GE Aviation Services.	program optimization;trajectory optimization	Srinivas Bollapragada;Joel Klooster;MacKenzie Cumings	2013	Interfaces	10.1287/inte.2013.0694	mathematical optimization;free flight;simulation;computer science;engineering;operations management;dynamic programming;aeronautics;mathematics;flight management system	ML	13.808780996478603	1.0138172761128024	2725
ac7ca92abc8c19e324e10b808c337ca08b548d91	preliminary results of an early clinical experience with the acrobottm system for total knee replacement surgery	total knee replacement	"""Early clinical experience with a """"hands-on"""" robotic system for total knee replacement surgery is presented. The system consists of a pre-operative CT based planning software, a small special purpose robot called Acrobot (active constraint robot) mounted on a gross positioning device and special leg fixtures. The surgeon guides the Acrobot under active constraint control, which constrains the motion into a predefined region, and thus allows surfaces of the bones to be machined safely and with high accuracy. A non-invasive anatomical registration method is used. The system was clinically tested on 7 patients with encouraging results."""		Matjaz Jakopec;Simon J. Harris;Ferdinando Rodriguez y Baena;Paula Gomes;Justin Cobb;Brian L. Davies	2002		10.1007/3-540-45786-0_32	computer science;physical therapy;mathematics;surgery	Robotics	39.61614368088329	-84.70968432395752	2729
9de1a8d2f0189589d12dc5993e595d638ce33606	detection of laser marks in retinal images	medical image processing diseases eye image recognition laser applications in medicine;image recognition;eye;automatic retinal diagnostics laser marks detection retinal images eye diseases diabetic retinopathy laser based cauterization automated retinal diagnostic systems;retinal images;laser applications in medicine;laser marks;medical image processing;diabetic retinopathy;diseases;laser treatment retinal images diabetic retinopathy laser marks;laser treatment;retina lasers diabetes retinopathy image color analysis classification algorithms measurement by laser beam	Eye diseases such as diabetic retinopathy are sometimes treated with recourse to laser based cauterization. The marks left by the action of the laser on the surface of the retina can cause misbehaviors of automated retinal diagnostic systems. It is therefore highly desirable to be able to detect the presence of laser marks left behind by such treatments and use that information in the first steps of automatic retinal diagnostics to prevent further unnecessary processing. In this paper we present an attempt to detect the presence of laser marks using an automated detection algorithm and present some preliminary results on its performance.	algorithm	Joao Miguel Pires Dias;Carlos Manta Oliveira;Luís Alberto da Silva Cruz	2013	Proceedings of the 26th IEEE International Symposium on Computer-Based Medical Systems	10.1109/CBMS.2013.6627864	computer vision;medicine;computer science;diabetes mellitus	Embedded	36.87538214347196	-76.4952708522214	2732
678b513ad138b4eb81cc6dda90b595a26e1a77f7	a novel visual word assignment model for content-based image retrieval		Visual bag of words model have been applied in the recent past for the purpose of content-based image retrieval. In this paper, we propose a novel assignment model of visual words for representing an image patch. In particular, a vector is used to represent an image patch with its elements denoting the affinities of the patch to belong to a set of closest/most influential visual words. We also introduce a dissimilarity measure, consisting of two terms, for comparing a pair of image patches. The first term captures the difference in affinities of the patches to belong to the common set of influential visual words. The second term checks the number of visual words which influences only one of the two patches and penalizes the measure accordingly. Experimental results on the publicly available COIL-100 image database clearly demonstrates the superior performance of the proposed content-based image retrieval (CBIR) method over some similar existing approaches.	content-based image retrieval;visual word	Anindita Mukherjee;Soman Chakraborty;Jaya Sil;Ananda S. Chowdhury	2016		10.1007/978-981-10-2104-6_8	natural language processing;computer vision;visual word;information retrieval	Vision	34.00626906894573	-54.118119554526224	2734
9115e9a1101336d743a581e26527c40bccd55eb2	simulation of free surface compressible flows via a two fluid model	compressible flow;wave breaking;fluid dynamics;equation of state;numerical analysis;free surface	The purpose of this communication is to discuss the simulation of a free surface compressible flow between two fluids , typically air and water. We use a two fluid model with the same velocity, pressure and temperature for both phases. In such a numerical model, the free surface becomes a thin three dimen sional zone. The present method has at least three advantage s: (i) the free-surface treatment is completely implicit; (ii ) it can naturally handle wave breaking and other topological chang es in the flow; (iii) one can easily vary the Equation of States (EOS ) of each fluid (in principle, one can even consider tabulated EOS ). Moreover, our model is unconditionally hyperbolic for reas onable EOS. Introduction One of the challenges in Computational Fluid Dynamics (CFD) is to determine efforts exerted by waves on coastal str uctures. Such flows can be quite complicated and in particular when the sea is rough, wave breaking can lead to flows that cannot be described by basic models like e.g. free surface Euler or Navier-Stokes equations. In a free surface model, the bound ary between the gas (air) and the liquid (water) is a surface. The liquid flow is assumed to be incompressible, while the gas is r epresented by a media, above the liquid, where the pressure is c onstant (the atmospheric pressure in general). Such a descrip tion ∗Address all correspondence to this author. is known to be valid as far as propagation in the open sea of waves with moderate amplitude is concerned. Clearly it is no t satisfactory when waves either break or hit coastal structu es like offshore platforms, jetties, piers, breakwaters tc. . . . In this work, our goal is to investigate a two-fluid model for this kind of problem. It belongs to the family of averaged mod els, that is although the two fluids considered are not miscib le, there exists a length scale ε in order that each averaging volume (of sizeε3) contain representative samples of each of the fluids. Once the averaging process is done, it is assumed that the two fluids share, locally, the same pressure, temperature and ve locity. Such models are called homogeneous models in the litera ture. They can be seen as limiting case of more general two-flu id models where the two fluids could have different temperature s and velocities [1]. The influence of the presence of air in wave impacts is a difficult topic. While it is usually thought that the presence of air softens the impact pressures, recent results by Bullock et a l. [2] show that the cushioning effect due to aeration via the incre ased compressibility of the air-water mixture is not necessaril y a dominant effect. First of all, air may become trapped or entrain ed in the water in different ways, for example as a single bubble trapped against a wall, or as a column or cloud of small bubble s. In addition, it is not clear which quantity is the most approp riate to measure impacts. For example some researchers pay mor e attention to the pressure impulse than to pressure peaks. Th e pressure impulse is defined as the integral of pressure over t he short duration of impact. Bagnold [3], for example, noticed that 1 Copyright c © 2008 by ASME the maximum pressure and impact duration differed from one identical wave impact to the next, even in carefully control led laboratory experiments, while the pressure impulse appear s to be more repeatable. For sure, the simple one-fluid models whi ch are commonly used for examining the peak impacts are no longe r appropriate in the presence of air. There are few studies dea ling with two-fluid models. An exception is the work by Peregine an d his collaborators. Wood, Peregrine & Bruce [4] used the pres sure impulse approach to model a trapped air pocket. Peregri ne & Thais [5] examined the effect of entrained air on a particular kind of violent water wave impact by considering a filling flow. Bullock et al. [6] found pressure reductions when compa ring wave impact between fresh and salt water where, due to the different properties of the bubbles in the two fluids, the aer ation levels are much higher in salt water than in fresh. H. Bredmos e recently performed numerical experiments on a two-fluid sys tem which is quite similar to the one we will use below. He develop d a finite volume solver for aerated flows named Flair [7]. Mathematical model In this section we present the equations which govern the motion of two phase mixtures in a computational domain Ω. First of all, we need to introduce the notation which will be used throughout this article. We use superscripts ± to denote any quantity which is related to liquid and gas respectively . For example,α+ andα− denote the volume fraction of liquid and gas and obviously satisfy the condition α+ + α− = 1. Then, we have the following classical quantities: ρ±, ~u, p, e, E, ~g which denote the density of each phase, the velocity field vector, t he pressure, the internal & total energy and the acceleration d ue to gravity correspondingly. Conservation of mass (one equation for each phase), momentum and energy lead to the four following equations: (αρ)t + ∇ · (αρ~u) = 0, (1) (ρ~u)t + ∇ · ( ρ~u⊗~u+ pI ) = ρ~g, (2) ( ρE ) t + ∇ · ( ρH~u ) = ρ~g·~u, (3) whereρ := α+ρ+ + α−ρ− (the total density), H := E + p ρ (the specific enthalpy), E = e+ 1 2|~u| 2 (the total energy). This system can be seen as the single energy and infinite drag limit of the more conventional six equations model [1]. The above system contains five unknowns αρ,~u, p andE and only four governing equations (1) (3). In order to close the system, we need t o provide the so-called equation of state (EOS) p = p±(ρ±,e±). The construction of the EOS will be discussed below. It is possible to rewrite these equations as a system of balance laws ∂w ∂ t + ∇ ·F (w) = S(w), (4) wherew(x,t) : Rd×R+ 7→Rm is the vector of conservative variables (in the present study d = 2 or 3 andm= 5), F (w) is the advective flux function andS(w) the source term. The conservative variables in the 2D case are defined as follows: w = (wi)i=1 := (α ρ,αρ, ρu, ρv, ρE). (5) The flux projection on the normal direction ~n = (n1,n2) can be expressed in physical and conservative variables F ·~n= (αρun,αρun,ρuun+ pn1,ρvun+ pn2,ρHun) = ( w1 w3n1+w4n2 w1 +w2 ,w2 w3n1 +w4n2 w1 +w2 ,w3 w3n1 +w4n2 w1 +w2 + pn1, w4 w3n1+w4n2 w1 +w2 + pn2,(w5 + p) w3n1 +w4n2 w1 +w2 )	computation;computational fluid dynamics;dirac delta function;eos;euler;experiment;finite volume method;mathematical model;navier–stokes equations;normal (geometry);numerical analysis;numerical method;rewrite (programming);simulation;software propagation;solver;velocity (software development)	Frédéric Dias;Denys Dutykh;Jean-Michel Ghidaglia	2008	CoRR		classical mechanics;statistical physics;breaking wave;compressible flow;fluid parcel;numerical analysis;equation of state;free surface;thermodynamics;physics;fluid mechanics;fluid dynamics	Robotics	90.1305356150149	4.156890205608209	2744
20c935d7afb63933210a4476ef0fa09b92d8f7fe	the covering number in learning theory	covering number;fourier transform;function space;reproducing kernel hilbert space;word learning;learning theory;mercer kernel	Learning Theory studies learning objects from random samples. The main question is: How many samples do we need to ensure an error bound with certain con ̄dence? To answer this question, the covering numbers or entropy numbers play an essential role, as shown by Vapnik, Poggio, CuckerSmale, and many others. For kernel machine learning such as the Support Vector Machine, a Reproducing Kernel Hilbert Space associated with a Mercer kernel K is often used and the covering number of a ball BR of such a space (as a subset IK(BR) of C(X)) is needed. We show how to estimate the covering number N(IK(BR); ́). Our estimates are based on the regularity of the kernel function K. For convolution type kernels K(x; t) = k(x ¡ t) on [0; 1], we provide estimates depending on the decay of the Fourier transform of k. In particular, when k̂ decays exponentially, we have	algorithmic learning theory;convolution;hilbert space;kernel method;machine learning;naruto shippuden: clash of ninja revolution 3;support vector machine;tomaso poggio	Ding-Xuan Zhou	2002	J. Complexity	10.1006/jcom.2002.0635	kernel;fourier transform;kernel method;mathematical optimization;mathematical analysis;discrete mathematics;string kernel;kernel embedding of distributions;radial basis function kernel;function space;kernel principal component analysis;learning theory;reproducing kernel hilbert space;mathematics;variable kernel density estimation;positive-definite kernel;polynomial kernel	ML	20.447914563905258	-33.197018530932354	2746
095dc9c78c6ed04a3ee7d60636bfe448dc120f2b	mirrored variants of the (1, 2)-cma-es compared on the noiseless bbob-2010 testbed	benchmarking;stochastic algorithm;search space;statistical significance;fast local search;evolution strategy;stochastic local search;black box optimization;local search	Derandomization by means of mirroring has been recently introduced to enhance the performances of (1,λ)-Evolution-Strategies (ESs) with the aim of designing fast robust local search stochastic algorithms. This paper compares on the BBOB-2010 noiseless benchmark testbed two variants of the (1,2)-CMA-ES where the mirroring method is implemented. Independent restarts are conducted till a total budget of 104 D function evaluations per trial is reached, where D is the dimension of the search space. The results show that the improved variants increase the success probability on 5 (respectively 7) out of 24 test functions in 20D and at the same time are significantly faster on 9 (10) functions in 20D by a factor of about 2-3 (2-4) for a target value of 10-7 while in no case, the baseline (1,2)-CMA-ES is significantly faster on any tested target function value in 5D and 20D.	baseline (configuration management);benchmark (computing);disk mirroring;distribution (mathematics);local search (optimization);performance;randomized algorithm;testbed	Anne Auger;Dimo Brockhoff;Nikolaus Hansen	2010		10.1145/1830761.1830772	mathematical optimization;computer science;local search;theoretical computer science;machine learning;mathematics;statistical significance;evolution strategy;benchmarking	HCI	23.91103269013054	-5.684441362916756	2749
549aba73c224fc7747c73d825b8c84c34bfe672b	deriving and visualizing uncertainty in kinetic pet modeling	computer and information science;data och informationsvetenskap	Kinetic modeling is the tool of choice when developing new positron emission tomography (PET) tracers for quantitative functional analysis. Several approaches are widely used to facilitate this process. While all these approaches are inherently different, they are still subject to uncertainty arising from various stages of the modeling process. In this paper we propose a novel approach for deriving and visualizing uncertainty in kinetic PET modeling. We distinguish between intraand inter-model uncertainties. While intra-model uncertainty allows us to derive uncertainty based on a single modeling approach, inter-model uncertainty arises from the differences of the results of different approaches. To derive intra-model uncertainty we exploit the covariance matrix analysis. The inter-model uncertainty is derived by comparing the outcome of three standard kinetic PET modeling approaches. We derive and visualize this uncertainty to exploit it as a basis for changing model input parameters with the ultimate goal to reduce the modeling uncertainty and thus obtain a more realistic model of the tracer under investigation. To support this uncertainty reduction process, we visually link abstract and spatial data by introducing a novel visualization approach based on the ThemeRiver metaphor, which has been modified to support the uncertainty-aware visualization of parameter changes between spatial locations. We have investigated the benefits of the presented concepts by conducting an evaluation with domain experts.	ct scan;eurographics;machine learning;matrix analysis;multi-compartment model;polyethylene terephthalate;semiconductor industry;subject-matter expert;tomography	Tan Khoa Nguyen;Alexander Bock;Anders Ynnerman;Timo Ropinski	2012		10.2312/VCBM/VCBM12/107-114	computer vision;simulation;computer science;data science	Robotics	47.60269905046962	-80.45705662026658	2750
479d43ef0dea7bdfbf5f21448b23e0df89f8a2d4	local symmetries of shapes in arbitrary dimension	level sets;computational geometry;computational geometry computer vision object oriented programming;object oriented programming;computer vision;reference systems;extreme point;3 d shape analysis;gradient vector local symmetries of shapes arbitrary dimension object centered reference system salient characteristics edge strength function local minima absolute gradient conic section;local minima;the edge strength function;symmetry axis;shape robustness skeleton fires level set electric shock mathematics computer vision smoothing methods	In [TSP], level curves of a function , called “the edge strength function,” defined for 2–dimensional shapes, are interpreted as successively smoother versions of the initial shape boundary. The local minima of the absolute gradient along the level curves of are shown to be a robust criterion for determining the shape skeleton. More generally, at an extremal point of along a level curve, the level curve is locally symmetric with respect to the gradient vector . That is, at such a point, the level curve is approximately a conic section whose one of the principal axes coincides with the gradient vector. Thus, the locus of the extremal points of along the level curves determines the axes of local symmetries of the shape. In this paper, we extend this method to shapes of arbitrary dimension and illustrate it by applying to 3D shapes.	gradient;locus;maxima and minima	Sibel Tari;Jayant Shah	1998		10.1109/ICCV.1998.710857	computer vision;extreme point;mathematical optimization;topology;computational geometry;level set;maxima and minima;mathematics;geometry;object-oriented programming	Vision	49.53203961800061	-66.61621369760998	2751
b780a4fe85b1f42fb43fc2c3e88e9e1e995f9cd4	aid decision of chinese traditional patent medicine based on manifold ranking	manifold ranking;aid decision model;manifolds;training;testing;chinese patent medicine;data mining;medical disorders;medical computing;aid decision method aid decision model testing aid decision model training stroke patient learning process symptom chinese patent medicine relation rational chinese patent medicine use western physician aid clinical prescriptions chinese patent medicine utility traditional chinese medicine knowledge clinical practice disease treatment manifold ranking chinese traditional patent medicine;patents;patents manifolds data models testing diseases training;decision support systems;diseases;patient treatment;patient treatment data mining decision support systems diseases medical computing medical disorders;tcm;manifold ranking tcm chinese patent medicine aid decision model data mining;data models	Chinese Patent Medicine is widely used to treat many disease in China since better efficacy can be achieved in the clinical practice. However, because of lack of Traditional Chinese Medicine (TCM) knowledge, many western physicians are confusing with the utility of Chinese Patent Medicine, which are exactly the main force of clinical prescriptions. Therefore, the aid decision method is urgent and necessary to help the western physicians rationally use Chinese Patent Medicine. In this paper, Manifold Ranking (MR) is employed to developing the aid decision model of Chinese Patent Medicine. There are two stages in the aid decision model Firstly, the underlying relation between the symptoms and the Chinese Patent Medicines is obtained based on MR during the learning process. Secondly, given the symptoms of a new patient, the aid decision of Chinese Patent Medicine is able to be given during the decision process. 115 patients with stroke are taken as the examples of training and testing the aid decision models. The experimental results reveal that Chinese Patent Medicine is able to be differentiated and the high accuracy of aid decision is also obtained based on some symptoms.	software patent	Yufeng Zhao;Liyun He;Baoyan Liu;Ruili Huo;Xianghong Jing	2013	2013 IEEE International Conference on Bioinformatics and Biomedicine	10.1109/BIBM.2013.6732707	data modeling;decision support system;manifold;computer science;data mining;software testing	SE	5.011646867300662	-76.47127304411967	2755
6441f07ecb147f289fd37095bba3d69eead82a2f	detection of self-paced movement intention from pre-movement electroencephalogram signals with hilbert transform		The movement related cortical potential (MRCP) is a well-known neural signature of human's self-paced movement intention, which can be exploited by future neuroprosthesis. Most existing studies have explored the amplitude representation for the movement intention. In this paper we investigate the Hilbert transformed MRCP, which implicitly includes phase information complementarily to amplitude information, for the detection of self-paced upper-limb movement intention. On the datasets in which 5 healthy subjects executed a self-initiated upper limb center-out reaching task in three sessions, we have evaluated the detection model with Hilbert transformed MRCP as features and the state-of-art one with the original MRCP amplitude as features. Results show that the Hilbert transformed MRCP based detector is more accurate than the original MRCP based one.	cholangiopancreatography, magnetic resonance;detectors;electroencephalography;execution;hilbert space;hilbert transform;movement disorders;neuroprosthesis	Hong Zeng;ChangCheng Wu;Aiguo Song;Baoguo Xu;Huijun Li;Pengcheng Wen;Jia Liu	2017	2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2017.8037265	computer vision;artificial intelligence;computer science;hilbert transform;amplitude;detector	SE	15.557547481660091	-92.1496011499604	2757
9e02d13c22643c775b8c243e910686097a7dd476	structural analysis of source code collected from programming contests	outlier codes source code structural analysis programming contests international olympiad for informatics ioi international collegiate programming contest icpc algorithmic problems programming codes koi korea olympiad for informatics east asia regional contest blind test cases weka data mining tool source code score prediction static analysis block tree structure data mining classifier naïve bayes 10 fold cross validation test;source code software data mining pattern classification program diagnostics software metrics;educational institutions programming complexity theory harmonic analysis data mining sensitivity	Programming contests such as the International Olympiad for Informatics (IOI) and the International Collegiate Programming Contest (ICPC) are effective for encouraging young and bright programmers. These contests require contestants to complete a few tasks (between three and nine) related to algorithmic problems within a limited time. For this study, we collected a set of 2,400 programming codes submitted to the KOI (Korea Olympiad for Informatics) in 2011 and 2012 as well as 2,300 programming codes submitted at the preliminary contest session for the ICPC in 2009, 2011, and 2012 at the East-Asia regional contest. Because submitted source codes were evaluated with blind test cases, we can define a criteria to separate the high- and low-scoring students in the order of their respective scores. The main objective of this paper is to reveal the relationship between the task's proposed features, its difficulty, the school grade (elementary, middle-, and high-school), and the score. We do so with the data-mining tool WEKA. The ultimate goal of this study is to predict the score of some particular code with static analysis. We propose a simple and straightforward complexity measure based on the block-tree structure. We considered the high scoring student group as a positive class and the low scoring student group as negative class. The performance of the data mining classifier named Naïve Bayes are evaluated based on 10-fold cross validation test. We decided that the meaningful classification for a harmonic mean of sensitivity and specificity is empirically larger than 0.6 empirically. Among the codes acquired through the KOI, we found a set of outlier codes that attempt to reply with the correct response to receive extra points. Among the codes acquired through the ICPC, we discovered that good collegiate programmers (i.e., Those with high score) attempt to keep their code more compact, both lexically and structurally. We used WEKA to analyze the code using code-features proposed in this study, and the results are detailed quantitatively.	acm international collegiate programming contest;acceptance testing;algorithm;blum axioms;code;computational complexity theory;data mining;international olympiad in informatics;koi-18;naive bayes classifier;programmer;sensitivity and specificity;static program analysis;structural analysis;test case;tree structure;weka	Bokuk Park;Haesung Tak;Hwan-Gue Cho	2014	2014 IEEE International Conference on Computer and Information Technology	10.1109/CIT.2014.171	simulation;computer science;artificial intelligence;data science;operating system;machine learning;data mining;database;programming language;computer security;statistics	SE	6.692273420096669	-40.30681039387369	2759
77f250a4dffe7e5b07f54802765fd809c768601e	implementation of a moving target tracking algorithm using eye-ris vision system on a mobile robot	segmentation;robotic;target tracking;motion detection;analog system	A moving target tracking algorithm is proposed here and implemented on the Anafocus Eye-RIS vision system, which is a compact and modular platform to develop real time image processing applications. The algorithm combines moving-object detection with feature extraction in order to identify the specific target in the environment. The algorithm was tested in a mobile robotics experiment in which a robot, with the Eye-RIS mounted on it, pursued another one representing the moving target, demonstrating its performance and capabilities.	algorithm;eb-eye;experiment;focal (programming language);feature extraction;image processing;mobile robot;object detection;real-time clock;relational interface system;robotics	Fethullah Karabiber;Paolo Arena;Luigi Fortuna;Sebestiano De Fiore;Guido Vagliasindi;Sabri Arik	2011	Signal Processing Systems	10.1007/s11265-010-0504-7	embedded system;computer vision;simulation;tracking system;computer science;segmentation	Robotics	45.36278555033364	-35.8908835810976	2762
0e2b9d8a2dfdaec02ee4895fd14b503706f157f5	a novel video denoising algorithm in communication base station monitoring system	gaussian noise;filtering;noise reduction filtering algorithms filtering gaussian noise monitoring algorithm design and analysis;spatio temporal combination base station monitoring system monitoring video denoising non local means filter;adaptive filter novel video denoising algorithm communication base station monitoring system nonlocal means spatial domain filter method temporal domain denoising spatiotemporal combination method;filtering algorithms;monitoring;noise reduction;video signal processing adaptive filters image denoising;algorithm design and analysis	The basic theory of video denoising is firstly systematically studied. Non-local means spatial domain filter method and its advantages compared to other traditional methods are discussed in detail. On this basis, taking the issue of temporal domain denoising into account, a novel spatio-temporal combination method for video denoising is proposed. What is more, adaptive filter strength is introduced to present a modified spatio-temporal combination method. Experiments show that both algorithms have achieved good results.	adaptive filter;algorithm;experiment;noise reduction;non-local means;video denoising	Sheng Qiang;Zhanmin Zhou	2014	IECON 2014 - 40th Annual Conference of the IEEE Industrial Electronics Society	10.1109/IECON.2014.7048840	step detection;computer vision;electronic engineering;computer science;machine learning;non-local means;video denoising	Vision	60.42259917474031	-68.05557786884917	2764
6eb807ae65f393e2b1544619bd7faf7985897b07	simultaneous stereo-motion fusion and 3-d motion tracking	feature correspondence estimation;filtering;covariance analysis;cost function;feature extraction sensor fusion image matching motion estimation stereo image processing tracking maximum likelihood estimation iterative methods adaptive kalman filters covariance analysis;maximum likelihood;image matching;kalman filters;geometry;kalman filter;motion estimation;covariance;feature matching;maximum likelihood estimation;observation noise;motion tracking;iterative methods;3 d motion tracking;adaptive filters;motion parameter estimates;chromium;feature extraction;stereo image processing;adaptive kalman filters;stereo pairs;fusion power generation;stereo motion fusion;parameter estimation;feature correspondence estimation stereo motion fusion 3 d motion tracking maximum likelihood adaptive iterated extended kalman filtering stereo pairs 3 d feature matches motion parameter estimates covariance observation noise cost function occlusion related problems;sensor fusion;extended kalman filter;3 d feature matches;occlusion related problems;cameras;tracking;tracking maximum likelihood estimation kalman filters filtering adaptive filters fusion power generation motion estimation parameter estimation cost function;adaptive iterated extended kalman filtering	:L . I N T R O DUCTI0 N Several researchers have proposed extended Kalman filtering (EKF) for tracking 3-D motion parameters from long stereo sequences using 2-D or 3-D feature correspondences as observations [1], [2], [3]. These formulations treat the estimation of the 2-D or 3-D feature correspondences firom pairs of frames and tracking of the 3-D motion parameters more-or-less separately. Statistical data association techniques [5] or deterministic stereo-motion fusion using dynamic programming [6] have been used in the literature to estimate 3-D feature matches. However feature matching is itself an ill-posed problem, and without some stronger regularization constraints erroneous matches may be found, resulting in the divergence of the EKF. To this effect, in this paper we propose a simultaneous framework which combines the maximum likelihood (ML) estimation of 3-D feature correspondences with extended Kalman filtering for 3-D motion tracking. The ML correspondence estimation algorithm imposes consistency of the estimated feature matches with the projected 3-D motion parameters obtained from the EKF. However, unlike the previous approaches where This work is supported in part by a National Science Foundation IUCRC grant and a New York State Science and Technology Foundation grant. to the Center for Electronic Imaging Systems at the University of Rochester, and a grant by Eastman Kodak Company. the feature matching step is initialized by the predicted estimate from the EKF, the new ML approach imposes this consistency constraint as part of the cost function. At each time instant, several iterations of the ML and the EKF algorithms are interleaved such that the EKF makes use of the feature correspondences computed by the ML step, and the ML step uses the 3-D motion parameters updated by the EKF. That is, we have an ilerated EKF (IEKF) which uses improved observationis a t each iteration as well as an improved linearization. The algorithm advances to the next time sample when the ML cost function can no longer be reduced. In section 11, we discuss the stereo imaging geometry and the motion kinematics. In Section 111, the ML step of the algorithm is introduced. The formulation of an adaptive EKF, where the variances of the observation noise are adjusted by the value of the ML cost function to address occlusion related problems, is discussed in Section IV. In Section V, we compare the results of the combined algorithm with those obtained by treating the two-steps separately. 2. I M A G I N G A N D M O T I O N MODELS Imaging Model: Fig. 1 shows a 3-D world coordinate system C W and two camera coordinate systems CL and C R that are fixed on the left and right cameras, respectively, with their z-axes pointing along the 01)tical axis of the cameras. Let fl = f,. = f denote the focal length of both cameras. Consider two other coordinate systems CO, the object coordinate system, whose origin PO coincides with the center of rotation (that is unknown), and C S , the structure coordinate system, whose origin is located a t a known point on the object. It is assumed that CS and CO are related by a translation d. Let a point P;(t) = ( X i ( t ) , Y , ( t ) , Z i ( t ) ) in the world coordinate system CW be represented as Pi,(t) = (Xi~(t),Yi,(t),z,~(t)) in C R and P i , ( t ) = ( X i , ( t ) , Yi, ( t ) , Zi, (t)) in CL, respectively. Then, Pt,(t) = R,Pt(t) + Tr, (1) Pt,(t) = RlPt(t) + T I , ( 2 ) where R, and RI represent the rotation matrices, and T, and TI denote the translation vectors indicating the 2277 0-7803-2431 -5/95 $4.00	3d computer graphics;algorithm;binary prefix;correspondence problem;dynamic programming;extended kalman filter;focal (programming language);isometric projection;iteration;matrix regularization;optic axis of a crystal;well-posed problem	Yücel Altunbasak;A. Murat Tekalp;Gozde Bozdagi Akar	1995		10.1109/ICASSP.1995.479945	kalman filter;computer vision;computer science;machine learning;pattern recognition;mathematics;maximum likelihood;statistics	Vision	55.19289196380079	-41.295735777171295	2765
129438df671ab4b388f7a6b6e6a959f0c710659c	iterative design of feedback and feedforward controller with input saturation constraint for building temperature control	temperature control;optimization algorithm design and analysis buildings adaptive control atmospheric modeling temperature control meteorology;adaptive control;temperature control feedback feedforward iterative learning control;optimization;atmospheric modeling;meteorology;algorithm design and analysis;four room testbed system iterative design feedback controller feedforward controller input saturation constraint building temperature control building temperature regulation application iterative learning control ilc strategy nonrepetitive disturbance components constrained optimization problem constrained ilc problem iterative feedback tuning problem;buildings	In this paper, an iterative design of feedforward and feedback controller is proposed for a building temperature regulation application. An iterative learning control (ILC) strategy is utilized to eliminate the influence of repetitive disturbance and an iteratively tuned feedback controller is applied to reduce the influence of non-repetitive disturbance components. With constraints from input saturation, the controller design is formulated into a constrained optimization problem. To reduce the complexity of the problem, an alternating procedure is proposed to solve the optimization problem, separating the original problem into two parts, namely, constrained ILC problem and iterative feedback tuning problem. The proposed algorithm is demonstrated by simulations on a four-room testbed system.	algorithm;constrained optimization;constraint (mathematics);control theory;feed forward (control);feedforward neural network;iteration;iterative design;iterative method;mathematical optimization;optimization problem;simulation;system camera;testbed	Cheng Peng;Wenlong Zhang;Masayoshi Tomizuka	2016	2016 American Control Conference (ACC)	10.1109/ACC.2016.7525087	control engineering;algorithm design;atmospheric model;mathematical optimization;adaptive control;computer science;engineering;temperature control;control theory;mathematics	Robotics	60.93758743411901	-3.121369261405771	2766
3181220f151ebecdc818aeef68ba00e247512b3f	quality of compressed medical images	software;diagnostic imaging;computed tomography;data compression;radiology information systems;index;wavelet compression;radiographic image enhancement;image processing computer assisted;light;artifacts;image enhancement;lossy;medical image;image compression;jpeg;quality evaluation;magnetic resonance;peak signal to noise ratio;magnetic resonance imaging;image quality;model;joint photographic expert group;jpeg2000;compression ratio;algorithms;humans;mammography;computed radiography;ct images;blocking artifact;sliding window;tomography x ray computed	Previous studies have shown that Joint Photographic Experts Group (JPEG) 2000 compression is better than JPEG at higher compression ratio levels. However, some findings revealed that this is not valid at lower levels. In this study, the qualities of compressed medical images in these ratio areas (∼20), including computed radiography, computed tomography head and body, mammographic, and magnetic resonance T1 and T2 images, were estimated using both a pixel-based (peak signal to noise ratio) and two 8 × 8 window-based [Q index and Moran peak ratio (MPR)] metrics. To diminish the effects of blocking artifacts from JPEG, jump windows were used in both window-based metrics. Comparing the image quality indices between jump and sliding windows, the results showed that blocking artifacts were produced from JPEG compression, even at low compression ratios. However, even after the blocking artifacts were omitted in JPEG compressed images, JPEG2000 outperformed JPEG at low compression levels. We found in this study that the image contrast and the average gray level play important roles in image compression and quality evaluation. There were drawbacks in all metrics that we used. In the future, the image gray level and contrast effect should be considered in developing new objective metrics.	blocking (computing);ct scan;fda quality metrics terminology;grayscale;image compression;image quality;intelligent character recognition;international normalized ratio;jpeg 2000;joint photographic experts group;medical imaging;mice, inbred icr;microsoft windows;minkowski portal refinement;morphologic artifacts;national supercomputer centre in sweden;pgrmc1 protein, human;peak signal-to-noise ratio;pixel;radiography;resonance;signal-to-noise ratio;x-ray computed tomography;funding grant;observers;research grants	Ya-Hui Shiao;Tzong-Jer Chen;Keh-Shih Chuang;Cheng-Hsun Lin;Chun-Chao Chuang	2007	Journal of Digital Imaging	10.1007/s10278-007-9013-z	computer vision;radiology;medicine;computer science;magnetic resonance imaging;jpeg;quantization;computer graphics (images)	HCI	57.033536597406	-79.85206121580515	2767
37cd8e4d1fdfe918d36a3a94d8ee5b73781a74f1	simple analytical model and efficiency improvement of polysilicon solar cells with porous silicon at the backside	silicon;back surface;grain boundary;evaluation performance;metodo analitico;capa espesa;corriente fotoelectrica;photoelectric current;performance evaluation;porosite;efecto fotovoltaico;polycristal;materiau poreux;polysilicon solar cells;evaluacion prestacion;surface arriere;multicouche;celula solar;backside reflector;reflected light;superficie atras;joint grain;effet photovoltaique;reflector;polycrystal;multiple layer;optoelectronic device;photovoltaic effect;light absorbed;solar cell;porous material;luz reflejada;analytical method;8105r;capa multiple;porous silicon;policristal;photoconductivity;courant photoelectrique;photoconductivite;methode analytique;material poroso;dispositif optoelectronique;cellule solaire;limite grano;silicium;porosity;8460j;porosidad;reflecteur;thick film;silicio;dispositivo optoelectronico;fotoconductividad;couche double;analytical model;lumiere reflechie;double layers;couche epaisse	The present study developed a simple analytical model to simulate the performance of polysilicon solar cells with porous silicon (PS) layer at the backside. It analytically solved the complete set of equations necessary for the determination of the photocurrent generated under the effect of the reflected light. It also investigated the contribution of the light absorbed by the PS layer and explored the effect that the latter’s number of double porosities and high porosity have had on photovoltaic parameters. The findings suggest that the photovoltaic parameters increase with the number of double porosities that the layer might have in a given structure. When the PS layer is formed by three-double porosity layers 20%/80% and for a 5 mm-thick film c-Si, the backside reflector gives a total improvement of about 2.65 mA/cm in photocurrent density and 1.4% in cell efficiency. This improvement can even be of much more important for well passivated grain boundaries and back contact of solar cells. & 2010 Elsevier Ltd. All rights reserved.	back-illuminated sensor;bean scripting framework;cell (microprocessor);distributed bragg reflector;simulation;solar cell;terrestrial television;wafer (electronics)	Abdessalem Trabelsi;Monem Krichen;Abdelaziz Zouari;Adel Ben Arab	2011	Microelectronics Journal	10.1016/j.mejo.2010.08.012	photovoltaic effect;electronic engineering;grain boundary;reflection;photoconductivity;porosity;optics;silicon;physics	Robotics	92.58995281419088	-10.775460674586677	2771
197cc85cdc133449aa1755c0f98a9b3219c4af0b	principal component analysis of the start-up transient and hidden markov modeling for broken rotor bar fault diagnosis in asynchronous machines	automatic control;articulo;principal component analysis;broken rotor bar fault diagnosis;hidden markov modeling	This article presents a novel computational method for the diagnosis of broken rotor bars in three phase asynchronous machines. The proposed method is based on Principal Component Analysis (PCA) and is applied to the stator's three phase start-up current. The fault detection is easier in the start-up transient because of the increased current in the rotor circuit, which amplifies the effects of the fault in the stator's current independently of the motor's load. In the proposed fault detection methodology, PCA is initially utilized to extract a characteristic component, which reflects the rotor asymmetry caused by the broken bars. This component can be subsequently processed using Hidden Markov Models (HMMs). Two schemes, a multiclass and a one-class approach are proposed. The efficiency of the novel proposed schemes is evaluated by multiple experimental test cases. The results obtained indicate that the suggested approaches based on the combination of PCA and HMMs, can be successfully utilized not only for identifying the presence of a broken bar but also for estimating the severity (number of broken bars) of the fault.	hidden markov model;markov chain;principal component analysis;r.o.t.o.r.	George K. Georgoulas;Mohammed Obaid Mustafa;Ioannis P. Tsoumas;Jos&#x00E9; Antonino-Daviu;Vicente Climente-Alarc&#x00F3;n;Chrysostomos D. Stylios;George Nikolakopoulos	2013	Expert Syst. Appl.	10.1016/j.eswa.2013.06.006	computer science;machine learning;automatic control;control theory;principal component analysis	ML	37.087543486211686	-30.40160756352538	2774
5d40abd5c470b5461349de0c8ae9c7aaee5a2ecf	a predictive learning framework for monitoring aggregated performance indicators over business process events		"""In many application contexts, a business process' executions are subject to performance constraints expressed in an aggregated form, usually over predefined time windows, and detecting a likely violation to such a constraint in advance could help undertake corrective measures for preventing it. This paper illustrates a prediction-aware event processing framework that addresses the problem of estimating whether the process instances of a given (unfinished) window w will violate an aggregate performance constraint, based on the continuous learning and application of an ensemble of models, capable each of making and integrating two kinds of predictions: single-instance predictions concerning the ongoing process instances of w, and time-series predictions concerning the """"future"""" process instances of w (i.e. those that have not started yet, but will start by the end of w). Notably, the framework can continuously update the ensemble, fully exploiting the raw event data produced by the process under monitoring, suitably lifted to an adequate level of abstraction. The framework has been validated against historical event data coming from real-life business processes, showing promising results in terms of both accuracy and efficiency."""	aggregate data;algorithm;business process;complex event processing;concept drift;microsoft windows;performance;pixel density;predictive learning;real life;scalability;sensor;single-instance storage;stationary process;time series	Alfredo Cuzzocrea;Francesco Folino;Massimo Guarascio;Luigi Pontieri	2018		10.1145/3216122.3216143	data stream mining;abstraction;business process;performance indicator;computer science;data mining;complex event processing;predictive learning	DB	0.01773550407530734	-35.66054670726105	2781
9a0fea32d0520d56691e193d8c240cd499204cd7	a generative spatial clustering model for random data through spanning trees	space exploration;geographical areas generative spatial clustering model random data spanning trees spatial data analysis spatially constrained clustering probabilistic regionalization algorithm bayesian generative spatial product partition model markov chain monte carlo algorithm random walk edge deletion synthetic data municipalities regionalization cancer incidence rates;spatial graphical model spatially constrained clustering bayesian spatial model;stochastic processes;yttrium;probability distribution;clustering algorithms;trees mathematics bayes methods cancer data analysis geography markov processes monte carlo methods pattern clustering random processes;data models;partitioning algorithms;yttrium partitioning algorithms clustering algorithms data models stochastic processes probability distribution space exploration	When performing analysis of spatial data, there is often the need to aggregate geographical areas into larger regions, a process called regionalization or spatially constrained clustering. These algorithms assume that the items to be clustered are non-stochastic, an assumption not held in many applications. In this work, we present a new probabilistic regionalization algorithm that allows spatially varying random variables as features. Hence, an area highly different from its neighbors can still be considered a member of their cluster if it has a large variance. Our proposal is based on a Bayesian generative spatial product partition model. We build an effective Markov Chain Monte Carlo algorithm to carry out a random walk on the space of all trees and their induced spatial partitions by edges' deletion. We evaluate our algorithm using synthetic data and with one problem of municipalities regionalization based on cancer incidence rates. We are able to better accommodate the natural variation of the data and to diminish the effect of outliers, producing better results than state-of-art approaches.	aggregate data;bayesian network;cluster analysis;computer cluster;constrained clustering;file spanning;incidence matrix;markov chain monte carlo;monte carlo algorithm;monte carlo method;probabilistic database;synthetic data	Leonardo Vilela Teixeira;Renato Martins Assunção;Rosangela Helena Loschi	2015	2015 IEEE International Conference on Data Mining	10.1109/ICDM.2015.106	probability distribution;correlation clustering;stochastic process;data modeling;constrained clustering;econometrics;data stream clustering;fuzzy clustering;computer science;canopy clustering algorithm;space exploration;yttrium;machine learning;cure data clustering algorithm;mathematics;cluster analysis;statistics	DB	-2.812695780603031	-43.36163037249547	2788
8c7a8085116c48986e72a62dc2d17e041d271739	thesis: multiple kernel learning for object categorization		Object Categorization is a challenging problem, especially when the images have clutter background, occlusions or different lighting conditions. In the past, many descriptors have been proposed which aid object categorization even in such adverse conditions. Each descriptor has its own merits and de-merits. Some descriptors are invariant to transformations while the others are more discriminative [1, 2]. Past research has shown that, employing multiple descriptors rather than any single descriptor leads to better recognition [3, 4]. The problem of learning the optimal combination of the available descriptors for a particular classification task is studied. Multiple Kernel Learning (MKL) framework has been developed for learning an optimal combination of descriptors for object categorization. Existing MKL formulations often employ block l-1 norm regularization which is equivalent to selecting a single kernel from a library of kernels [5, 6, 7, 8, 9]. Since essentially a single descriptor is selected, the existing formulations maybe suboptimal for object categorization. A MKL formulation based on block l-∞ norm regularization has been developed, which chooses an optimal combination of kernels as opposed to selecting a single kernel. A Composite Multiple Kernel Learning(CKL) formulation based on mixed l-∞ and l-1 norm regularization has been developed. These formulations end in Second Order Cone Programs(SOCP). Other efficient alternative algorithms for these formulation have been implemented. Empirical results on benchmark datasets show significant improvement using these new MKL formulations.	adaboost;algorithm;benchmark (computing);caltech 101;categorization;clutter;codebook;cone;convex optimization;dhrystone;kernel (operating system);manifold regularization;math kernel library;mathematical optimization;matrix regularization;multiple kernel learning;optimization problem;pascal	Dinesh Govindaraj	2016	CoRR		mathematical optimization;machine learning;pattern recognition;mathematics	ML	26.195537251030352	-44.631982804529294	2793
ca44ba3f2120b124febed3e1de62276c3ff1b946	an iot solution for online monitoring of anesthetics in human serum based on an integrated fluidic bioelectronic system		In this paper, we present the design, the implementation and the validation of a novel Internet of Things  (IoT) drug monitoring system for the online continuous and simultaneous detection of two main anesthetics, e.g., propofol and paracetamol, in undiluted human serum. The described full system consists of a custom-built electronic Raspberry Pi (RPi) based Printed Circuit Board (PCB) that drives and reads out the signal from an electrochemical sensing platform integrated into a fluidic system. Thanks to the  Polydimethylsiloxane (PDMS) fluidic device, the analyzed sample is automatically fluxed on the sensing site. The IoT network is supported by a Cloud system, which allows the doctor to control and share all the patient's data through a dedicated Android application and a smart watch. The validation closes with the first ever demonstration that our system successfully works for the simultaneous monitoring of propofol and paracetamol in undiluted human serum by measuring the concentration trends of these two drugs in fluxing conditions over time.	acetaminophen;anesthesiology;dimethylpolysiloxanes;drug monitoring;internet of things;intrinsic drive;isoelectric point;pdms;patients;printed circuit board device component;propofol;raspberry pi 3 model b (latest version);reading (activity);smartwatch	Francesca Stradolini;Abuduwaili Tuoheti;Tugba Kilic;Sofia Lydia Ntella;Nadia Tamburrano;Zijian Huang;Giovanni De Micheli;Danilo Demarchi;Sandro Carrara	2018	IEEE Transactions on Biomedical Circuits and Systems	10.1109/TBCAS.2018.2855048	embedded system;pi;smartwatch;cloud computing;control engineering;android (operating system);fluidics;computer science;internet of things	Mobile	6.137149406113741	-90.64252606317486	2797
326d1e2121354eb0527bc48472f69397a8b9f04b	fast bundle algorithm for multiple-instance learning	drugs;microwave integrated circuits;kernel compounds microwave integrated circuits drugs computational modeling support vector machines optimization;multiple instance;kernel;compounds;integrated circuit;support vector machines;convex programming;nonconvex optimization;multiple instance learning;computer model;bundle method;multiple instance ranking;medicine and science artificial intelligence machine learning nonsmooth optimization bundle methods multiple instance learning ranking;computational chemistry;multiple instance loss functions;artificial intelligent;computational modeling;bundle algorithm;machine learning;ranking;loss function;bundle methods;linear time subgradient based methods;linear time;pattern classification;gradient methods;computational chemistry bundle algorithm multiple instance learning multiple instance classification multiple instance ranking multiple instance loss functions smooth nonconvex optimization problems linear time subgradient based methods support vector machines nonconvex bundle method;smooth nonconvex optimization problems;artificial intelligence;optimization;pattern classification convex programming gradient methods learning artificial intelligence;medicine and science;support vector machine;nonsmooth optimization;learning artificial intelligence;algorithms artificial intelligence humans neural networks computer pattern recognition automated support vector machines;nonconvex bundle method;multiple instance classification	We present a bundle algorithm for multiple-instance classification and ranking. These frameworks yield improved models on many problems possessing special structure. Multiple-instance loss functions are typically nonsmooth and nonconvex, and current algorithms convert these to smooth nonconvex optimization problems that are solved iteratively. Inspired by the latest linear-time subgradient-based methods for support vector machines, we optimize the objective directly using a nonconvex bundle method. Computational results show this method is linearly scalable, while not sacrificing generalization accuracy, permitting modeling on new and larger data sets in computational chemistry and other applications. This new implementation facilitates modeling with kernels.	algorithm;attempt;bilinear filtering;choose (action);code;computation;computational chemistry;computational complexity theory;computer multitasking;exhibits as topic;feature selection;generalization (psychology);holographic principle;information;isoenzymes;large;least squares;linear bounded automaton;loss function;mathematical optimization;model selection;multiple-instance learning;quadratic programming;scalability;sensorineural hearing loss (disorder);softmax function;subderivative;subgradient method;support vector machine;time complexity;mil unit of measure	Charles Bergeron;Gregory M. Moore;Jed Zaretzki;Curt M. Breneman;Kristin P. Bennett	2012	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2011.194	support vector machine;mathematical optimization;convex optimization;computer science;machine learning;pattern recognition;mathematics	ML	21.921062149175373	-37.72340556587029	2802
9326a1a8ba2c6395ee3e79b6c23748fad7602557	the relationship between and/or search and variable elimination	bottom up;variable elimination;search space;top down;graphical model	In this paper we compare search and inference in graphical models through the new framework of AND/OR search. Specifically, we compare Variable Elimination (VE) and memoryintensive AND/OR Search (AO) and place algorithms such as graph-based backjumping and no-good and good learning, as well as Recursive Conditioning[7] and Value Elimination[2] within the AND/OR search framework.	algorithm;algorithmic efficiency;backjumping;breadth-first search;control theory;depth-first search;graphical model;recursion (computer science);static variable;top-down and bottom-up design;variable elimination	Robert Mateescu;Rina Dechter	2005			mathematical optimization;computer science;backjumping;machine learning;top-down and bottom-up design;jump search;data mining;mathematics	AI	21.476077978720156	-13.930227964658558	2808
3bae57db0b7959a10c19ea49d4bd45d0c1497892	recognition of bolt quality base on elman neural network by ant colony optimization algorithm		The quality and working condition of the bolt anchoring system determine the safety performance of the whole construction project to a large extent. In order to ensure the effect of the bolt in the support system engineering and prevent the occurrence of major disasters, testing of the bolt quality becomes particularly important. The paper presents a way of identifying the bolt quality that use ant colony algorithm to optimize Elman neural network. The weights and thresholds of Elman neural network are optimized by using the ant colony algorithm and the Elman neural network model to recognition bolt quality is established. The results show that the Elman neural network by ant colony algorithm has better recognition effect and higher prediction precision than the Elman neural network and the Elman neural network by genetic algorithm.	ant colony optimization algorithms;approximation error;artificial neural network;genetic algorithm;network model;software bug;software release life cycle;systems engineering	W L Di;Xiao-Yun Sun;Mingming Wang;Hui Xing;Jingna Liu	2018	2018 International Conference on Machine Learning and Cybernetics (ICMLC)	10.1109/ICMLC.2018.8527051	artificial intelligence;genetic algorithm;machine learning;artificial neural network;pattern recognition;ant colony optimization algorithms;computer science;convergence (routing)	Robotics	11.795231261070025	-21.450963128199103	2813
5a0d8bac74a61c5de1f618554c0b2ab90c7b70ae	an actuated horizontal plane model for insect locomotion		We analyze a simple three degree of freedom model for running in the horizontal plane: the lateral leg spring (LLS) model with an actuated hip. The leg attachment point within the body is actuated to produce a force in the elastic leg that matches experimental force profiles. While the resulting bipedal model accurately reproduces force profiles by construction, we find that a single leg cannot reproduce the moment profiles and yawing variations seen in an insect equipped with a tripod of legs. We find stability is strongly dependent upon foot placement and that the leg attachment point motion within the body differs from that seen in the insect.		John Schmitt	2005		10.1007/3-540-26415-9_106	classical mechanics;horizontal plane;tripod (photography);physics	Robotics	74.67472979670747	-23.504569071366614	2815
d4d7868c25b0a2b4f878ad3fcfd35b1a4a2b0112	high dynamic range imaging using camera arrays		While High Dynamic Range (HDR) image generation using computational approaches has seen widespread adoption in consumer photography, computational approaches for HDR video creation are only just beginning to emerge. In this paper, we present an algorithm for HDR video generation using short baseline camera arrays (cameras placed close together) with varying exposures. Our main contributions are an algorithm for disparity estimation that specifically targets short baseline camera arrays and an alignment algorithm that minimizes visual artifacts even in the presence of disparity errors. Finally, we present results using our algorithm on real world videos captured using a camera array to demonstrate the success of this application.	algorithm;baseline (configuration management);binocular disparity;glossary of computer graphics;high dynamic range;range imaging;visual artifact	Kalpana Seshadrinathan;Oscar Nestares	2017	2017 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2017.8296376	computer vision;radiometry;dynamic range;artificial intelligence;high dynamic range;visual artifact;visualization;merge (version control);computer science;high-dynamic-range imaging;photography	Robotics	60.302317176173204	-55.513240626331836	2816
cacb9e856c251cc7bccb920fcaa08e981cea8ed0	applications of tensor theory to object recognition and orientation determination	image recognition;object recognition;tensile stress;image processing;tensor applications;bepress selected works;image sequence analysis;affine transformation image processing image sequence analysis pattern recognition tensor applications;manufacturing automation;data mining;three dimensional;tensile stress object recognition position measurement fourier transforms image processing image recognition equations image sequence analysis pattern recognition manufacturing automation;feature extraction;affine transformation;fourier transforms;transforms;position measurement;mathematical model;orthogonal projection;pattern recognition;linear equations;object identification;tensor applications affine transformation image processing image sequence analysis pattern recognition	A method is developed by which images resulting from orthogonal projection of rigid planar-patch objects arbitrarily oriented in three-dimensional (3-D) space may be used to form systems of linear equations which are solved for the affine transform relating the images. The technique is applicable to complete images and to unlabeled feature sets derived from images, and with small modification may be used to transform images of unknown objects such that they represent images of those objects from a known orientation, for use in object identification. No knowledge of point correspondence between images is required. Theoretical development of the method and experimental results are presented. The method is shown to be computationally efficient, requiring O(N) multiplications and additions where, depending on the computation algorithm, N may equal the number of object or edge picture elements.	algorithm;algorithmic efficiency;computation;distortion;handling (psychology);linear equation;muscle rigidity;outline of object recognition;physical object;projection defense mechanism;randomness;system of linear equations;cell transformation	David Cyganski;John A. Orr	1985	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.1985.4767722	fourier transform;three-dimensional space;computer vision;discrete mathematics;image processing;feature extraction;computer science;cognitive neuroscience of visual object recognition;pattern recognition;mathematical model;affine transformation;mathematics;geometry;linear equation;stress;orthographic projection	Vision	53.42464565366978	-51.216511952731324	2825
6da5fa1abb8d7b48bca89bbc64bb5ed031fa2dc6	evidence-based object tracking via global energy maximization	dynamic programming;image sequences;motion estimation;table lookup;coarse-to-fine dynamic programming;evidence-based object tracking;global energy maximization;lookup-table form;robust algorithm;cost function;parameter space;hough transform;object tracking;lookup table	This paper describes a robust algorithm for arbitrary object tracking in long image sequences. This technique extends the dynamic Hough transform proposed in our earlier work to detect arbitrary shapes undergoing affine motion. The proposed tracking algorithm processes the whole image sequence globally. First, the object boundary is represented in lookup-table form, and we then perform an operation that estimates the energy of the motion trajectory in the parameter space. We assign an extra term in our cost function to incorporate smoothness of deformation. The object is actually rigid, so by ‘deformation’ we mean changes due to rotation or scaling of the object. There is no need for training or initialization, and an efficient implementation can be achieved with coarse-to-fine dynamic programming and pruning. The method, because of its evidence-based nature, is robust under noise and occlusion.	dynamic programming;expectation–maximization algorithm;hidden surface determination;hough transform;image noise;image scaling;lookup table;loss function	John N. Carter;Pelopidas Lappas;Robert I. Damper	2003		10.1109/ICME.2003.1221279	hough transform;computer vision;mathematical optimization;lookup table;intelligent decision support system;rotation;shape;scaling;computer science;speech;theoretical computer science;pruning;dynamic programming;video tracking;motion estimation;mathematics;tracking;parameter space;estimation theory;intersymbol interference;statistics;robustness	Vision	48.9663736907302	-51.247562706159854	2827
237adae4a0d0bc2487006aff4db86343d26e7dbb	mixtures of factor analyzers with common factor loadings: applications to the clustering and visualization of high-dimensional data	modelizacion;base dato multidimensional;component covariance matrices;pattern clustering;analyse amas;pattern clustering covariance matrices data visualisation;model based density estimation;common factor;base de donnees multidimensionnelle;visualizacion;medicion densidad;estimacion densidad;density measurement;analisis forma;grande dimension;estimation densite;data visualization matrix decomposition maximum likelihood estimation density functional theory covariance matrix load modeling equations clustering algorithms pattern analysis statistics;matrice covariance;intelligence artificielle;matriz covariancia;large dimension;maximum likelihood estimation;multidimensional database;component covariance matrices factor analyzer mixture common factor loadings data clustering data visualization model based density estimation;data clustering;modelisation;density functional theory;density estimation;data visualisation;visualization;model based clustering normal mixture models mixtures of factor analyzers common factor loadings;cluster analysis;hidden markov models;normal mixture;visualisation;mixtures of factor analyzers;matrix decomposition;covariance matrices;high dimensional data;normal mixture models;data visualization;statistics;factor analyzer mixture;clustering algorithms;mesure densite;artificial intelligence;common factor loadings;analisis cluster;teoria mezcla;pattern analysis;inteligencia artificial;gran dimension;mixture theory;load modeling;modeling;theorie melange;model based clustering;analyse forme;covariance matrix	Mixtures of factor analyzers enable model-based density estimation to be undertaken for high-dimensional data, where the number of observations n is not very large relative to their dimension p. In practice, there is often the need to further reduce the number of parameters in the specification of the component-covariance matrices. To this end, we propose the use of common component-factor loadings, which considerably reduces further the number of parameters. Moreover, it allows the data to be displayed in low--dimensional plots.	factor analysis;specification;mixture	Jangsun Baek;Geoffrey J. McLachlan;Lloyd K. Flack	2010	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2009.149	econometrics;computer science;machine learning;mathematics;cluster analysis;data visualization;statistics	Visualization	34.42848002080848	-25.467749126826575	2835
00a6cc0db7fcf8d03c3c888b3016c59b1388b4c1	revealing the trick of taming chaos by weak harmonic perturbations	nonlinear oscillations;chaos suppressing;taming chaos	Taming chaos by weak harmonic perturbations has been a hot topic in recent years [Lima & Pettini, 1990; Braiman & Goldhirsch, 1991; Salerno, 1991; Loskutov & Shishmarev, 1992; Filatrella et al., 1993; Chacón & Bejarano, 1993; Qu et al., 1995; Li & Cherm, 1996; Yang et al., 1996; Chacón et al., 1997; Kiss & Hudson, 2001; Chacón, 2001a]. Let us consider a nonautonomous differential equation which generates chaos. The amplitude of the forcing term is chosen such that the solution of this equation is chaotic. However, such chaotic behavior can often be tamed and knocked down into periodic behavior by a very weak second perturbation which is harmonic to the primary forcing term. Such a phenomenon is called as “taming chaos” or “chaos suppressing” and has been studied with great interest [Lima & Pettini, 1990; Braiman & Goldhirsch, 1991; Salerno, 1991; Loskutov & Shishmarev, 1992; Filatrella et al., 1993; Chacón & Bejarano, 1993; Quet al., 1995; Li & Chern, 1996; Yang et al., 1996; Chacón et al., 1997; Kiss & Hudson, 2001; Chacón, 2001b] where it is introduced as an effective method to tame chaotic behavior without any feedback. Actually it could be observed in great surprise or wonder, because such chaotic behavior drastically disappears and is changed into periodic behavior by only an extremely weak harmonic perturbation. Taming chaos has been studied by many scientists since 1990. Although many researchers insist that taming chaos is a useful method as a nonfeedback chaos control, we imagine that the actual reason for the interest in taming chaos is because of the anticipation of something surprising or mysterious for this phenomenon, and the drastic change that happens by a weak perturbation.	chaos control;chaos theory;effective method;hudson;rule 184;stellar classification;tame;word lists by frequency;yang	Naohiko Inaba;Munehisa Sekikawa;Tetsuro Endo;Takashi Tsubouchi	2003	I. J. Bifurcation and Chaos	10.1142/S0218127403008272	classical mechanics;control of chaos;theoretical physics;mechanics;physics	AI	80.86684883362916	2.679301689140864	2840
34cc2561baf5a607ae6ed35208c31ed63b88f788	syqual: a platform for qualitative modelling and simulation of biological systems	drugs;systems biology;biological system modeling;eda;computational modeling;mathematical model;biological systems;predictive models;qualitative modeling	Qualitative modelling in systems biology is increasingly adopted as it allows predicting important properties of biological systems even when quantitative information of such systems are unknown. Even though different tools for qualitative modelling have been recently proposed, their lack of automatism and their unstructured simulation core limit their applicability to non-complex biological networks. This paper presents SyQUAL, a platform for qualitative modelling and simulation of biological systems. It consists of two main layers: a Web-based framework that allows users to (i) import models described in the standard Systems Biology Markup Language (SBML), (ii) easily define properties to observe, and (iii) run simulations by hiding the underlying layer, that is, a SystemC-based core simulator that allows simulating the systems through a discrete event-based model of computation at different levels of details. The paper shows how SyQUAL has been applied to identify the attractors and to analyse the system robustness/sensitivity under perturbations of the Colitis-associated Colon Cancer (CAC) network.	biological system;colon classification;common access card;markup language;model of computation;sbml;simulation;systemc	Rosario Distefano;Nickolas Goncharenko;Franco Fummi;Rosalba Giugno;Gary D. Badery;Nicola Bombieri	2016	2016 IEEE International High Level Design Validation and Test Workshop (HLDVT)	10.1109/HLDVT.2016.7748270	modelling biological systems;simulation;computer science;bioinformatics;theoretical computer science;mathematical model;predictive modelling;computational model;systems biology	Embedded	5.5960127022572115	-68.17456682032194	2843
17bdab55c5832286448bcf9222c2abb1a5b96ccc	relative transfer function estimation exploiting spatially separated microphones in a diffuse noise field		Many multi-microphone speech enhancement algorithms require the relative transfer function (RTF) vector of the desired speech source, relating the acoustic transfer functions of all array microphones to a reference microphone. In this paper, we propose a computationally efficient method to estimate the RTF vector in a diffuse noise field, which requires an additional microphone that is spatially separated from the microphone array, such that the spatial coherence between the noise components in the microphone array signals and the additional microphone signal is low. Assuming this spatial coherence to be zero, we show that an unbiased estimate of the RTF vector can be obtained. Based on real-world recordings experimental results show that the proposed RTF estimator outperforms state-of-the-art estimators using only the microphone array signals in terms of estimation accuracy and noise reduction performance.	acoustic cryptanalysis;algorithm;algorithmic efficiency;coherence (physics);microphone;noise reduction;speech enhancement;transfer function	Nico Gößling;Simon Doclo	2018	2018 16th International Workshop on Acoustic Signal Enhancement (IWAENC)	10.1109/IWAENC.2018.8521295		EDA	83.82436439038185	-35.659711898476004	2844
282da482ee8c511a44f94cafb3906058864a2b34	parallel visualization algorithms: performance and architectural implications	distributed memory systems;data visualization rendering computer graphics hardware vehicles image generation layout silicon workstations computer graphics ray tracing;volume rendering;parallel programming;data distribution;interconnection network;data visualisation;shared memory systems;ray tracing;shear warp volume rendering parallel visualization algorithms shared address space multiprocessors distributed interconnection network physically distributed main memory temporal locality hierarchical radiosity algorithm parallelized ray casting volume renderer optimized ray tracer;hierarchical radiosity;message passing;ray tracing shared memory systems distributed memory systems parallel algorithms data visualisation rendering computer graphics parallel programming;rendering computer graphics;ray casting;parallel algorithms	Recently, a new class of scalable, shared-address-space multiprocessors has emerged. Like message-passing machines, these multiprocessors have a distributed interconnection network and physically distributed main memory. However, they provide hardware support for efficient implicit communication through a shared address space, and they automatically exploit temporal locality by caching both local and remote data in a processor's hardware cache. In this article, we show that these architectural characteristics make it much easier to obtain very good speedups on the best known visualization algorithms. Simple and natural parallelizations work very well, the sequential implementations do not have to be fundamentally restructured, and the high degree of temporal locality obviates the need for explicit data distribution and communication management. We demonstrate our claims through parallel versions of three state-of-the-art algorithms: a recent hierarchical radiosity algorithm by Hanrahan et al. (1991), a parallelized ray-casting volume renderer by Levoy (1992), and an optimized ray-tracer by Spach and Pulleyblank (1992). We also discuss a new shear-warp volume rendering algorithm that provides the first demonstration of interactive frame rates for a 256/spl times/256/spl times/256 voxel data set on a general-purpose multiprocessor.<<ETX>>	address space;algorithm;cache (computing);cache coherence;coherence (physics);computer data storage;computer graphics;general-purpose modeling;interconnection;locality of reference;message passing;multiprocessing;parallel computing;programming paradigm;radiosity (computer graphics);ray casting;ray tracing (graphics);real-time computing;real-time locating system;rendering (computer graphics);scalability;volume rendering;voxel	Jaswinder Pal Singh;Anoop Gupta;Marc Levoy	1994	Computer	10.1109/2.299410	ray tracing;parallel computing;message passing;computer science;theoretical computer science;operating system;ray casting;parallel algorithm;volume rendering;data visualization;computer graphics (images)	Arch	68.3216849408344	-51.29592958740446	2846
3b8e5d24f6ab442f982992ce7e254ffd825c9fef	analysis of eye-tracking experiments performed on a tobii t60	gaze;suivi regard;eye;etude experimentale;0130c;user study;mirada;0705r;regard;accuracy;visualization;eyes;precision;visualization technique;data visualization;visualisation donnee;eye tracking;oeil;eye gaze	Commercial eye-gaze trackers have the potential to be an important tool for quantifying the benefits of new visualization techniques. The expense of such trackers has made their use relatively infrequent in visualization studies. As such, it is difficult for researchers to compare multiple devices – obtaining several demonstration models is impractical in cost and time, and quantitative measures from real-world use are not readily available. In this paper, we present a sample protocol to determine the accuracy of a gaze-tacking device.	bittorrent tracker;eb-eye;experiment;eye tracking;fixed point (mathematics);pixel;tracking system;video post-processing	Chris Weigle;David C. Banks	2008		10.1117/12.768424	computer vision;simulation;eye tracking;computer science;accuracy and precision;data visualization;computer graphics (images)	Visualization	54.24066905424085	-45.26694290574302	2849
8398f7c6c8d4c66602200446d6d0af69d468df03	btw: a web server for boltzmann time warping of gene expression time series	software;partition function;time warp;genomics;euclidean distance;gene expression data;time series;data mining;gene expression;time factors;internet;speech recognition;time series data;algorithms;humans;sequence alignment;user computer interface;correlation coefficient;dynamic time warping;gene expression profiling	UNLABELLED Dynamic time warping (DTW) is a well-known quadratic time algorithm to determine the smallest distance and optimal alignment between two numerical sequences, possibly of different length. Originally developed for speech recognition, this method has been used in data mining, medicine and bioinformatics. For gene expression time series data, time warping distance is arguably a more flexible tool to determine genes having similar temporal expression, hence possibly related biological function, than either Euclidean distance or correlation coefficient--especially since time warping accommodates sequences of different length. The BTW web server allows a user to upload two tab-separated text files A,B of gene expression data, each possibly having a different number of time intervals of different durations. BTW then computes time warping distance between each gene of A with each gene of B, using a recently developed symmetric algorithm which additionally computes the Boltzmann partition function and outputs Boltzmann pair probabilities. The Boltzmann pair probabilities, not available with any other existent software, suggest possible biological significance of certain positions in an optimal time warping alignment.   AVAILABILITY http://bioinformatics.bc.edu/clotelab/BTW/.	alignment;bioinformatics;boltzmann machine;coefficient;data mining;dynamic time warping;euclidean distance;function (biology);gene expression;numerical analysis;partition function (mathematics);probability;server (computer);server (computing);speech recognition;symmetric-key algorithm;tablet dosage form;time complexity;time series;upload;web server	Fabrizio Ferrè;Peter Clote	2006	Nucleic Acids Research	10.1093/nar/gkl162	biology;genomics;bioinformatics;dynamic time warping;time series;genetics	Comp.	-1.7415799960702194	-55.43813397725882	2852
0a661ce507a0afb6123e2dd658c5120bb658b7a2	parameter optimization methods based on computational intelligence techniques in context of sustainable computing		In sustainable computing techniques, we always need to solve lots of optimization problems like design, planning and control, which are extremely hard. Conventional mathematical optimization techniques are computationally difficult. Recent advances in computational intelligence have resulted in an increasing number of nature inspired metaheuristic optimization techniques for effectively solve these complex problems. Mainly, the algorithms which are based on the principle of natural biological evolution and/or collective behavior of swarm have shown a promising performance and are becoming more and more popular nowadays. Most of these algorithms have their some set of parameters. The performance of these algorithm is highly depends on optimal parameter value settings. Prior to running these algorithms, the user must have values of different parameters, such as population size, parameters related to selection, and crossover probability, number of generations etc. That is energy and resource consuming. In this paper we summarize the work in computational intelligence based parameter setting techniques, and discuss related methodological issues. Further we discuss how parameter tuning affects the performance and/or robustness of metaheuristic algorithms and also discusses parameter tuning taxonomy.	computation;computational intelligence;shadow volume	Pankaj Upadhyay;Jitender Kumar Chhabra	2017		10.1007/978-3-319-53153-3_6	robustness (computer science);computer science;green computing;swarm behaviour;meta-optimization;metaheuristic;crossover;machine learning;artificial intelligence;optimization problem;computational intelligence	AI	22.571778162655214	-5.1912829359777435	2854
07a0462796c5509a1edfcb5de112706c310bf225	homogeneity measures for multiphase level set segmentation of brain mri	rate of convergence;biological tissues;biomedical measurements;brain;image segmentation;image resolution;brain mri;tissue homogeneity;level set;shape measurement;usa councils;quadratic homogeneity measure;level set magnetic resonance imaging image segmentation biomedical measurements shape measurement biomedical engineering usa councils measurement standards robustness area measurement;biomedical engineering;medical image processing;magnetic resonance imaging;image quality;variational segmentation;image spatial resolution;biomedical image processing;area measurement;robustness;tissue contrast quadratic homogeneity measure brain mri variational segmentation multiple level set functions image spatial resolution image quality tissue homogeneity;tissue contrast;measurement standards;medical image processing biological tissues biomedical mri brain image resolution image segmentation;biomedical mri;multiple level set functions;spatial resolution	This paper presents a new homogeneity measure for variational segmentation with multiple level set functions. We propose to modify the quadratic homogeneity measure to trade off the convexity of the function against a faster rate of convergence. We tested in two series of experiments the performance of this new homogeneity force at converging to appropriate partitioning of brain MRI data sets, over a large range of image spatial resolution and image quality, in terms of tissue homogeneity and contrast	calculus of variations;experiment;image quality;rate of convergence;segmentation fault	Elsa D. Angelini;Ting Song;Andrew F. Laine	2006	3rd IEEE International Symposium on Biomedical Imaging: Nano to Macro, 2006.	10.1109/ISBI.2006.1625024	computer vision;radiology;image resolution;computer science;magnetic resonance imaging;mathematics;medical physics	Vision	43.90137920436029	-75.29583846580616	2858
b920c63f14a6bb17fab77739a84904037a8cd666	remote sensing instruments used for measurement and model validation of optical parameters of atmospheric aerosols	distributed data;remote sensing instrument;aerosol optical depth;network measurement;cimel;column integrated aod comparisons remote sensing instruments model validation atmospheric aerosols dramatical climate changing real time surface level retrieval atmospheric composition optical coefficients aerosol optical depth particle size distribution surface network measurements mie scattering relative humidity vertical profile multiwavelength optical parameters column integrated vertical profiling measurements multiwavelength light detection ranging sunphotometer ceilometer tapered element oscillating microbalance column integrated parameters cmaq model planetary boundary layer;instruments;remote sensing aerosols air pollution atmospheric optics humidity mie scattering particle size;particulate matter pm less than 2 5 mu hbox m in diameter hbox pm _ 2 5;atmospheric measurements;new york state;particle size;climate change;real time;raman light detection and ranging lidar aerosol optical depth aod ceilometer cimel community multiscale air quality cmaq particulate matter pm less than 2 5 mu hbox m in diameter hbox pm _ 2 5;model performance;particle size distribution;backscatter;laser radar;ceilometer;vertical profile;community multiscale air quality cmaq;model validation;mie scattering;particulate matter;community multiscale air quality;tapered element oscillating microbalance;raman light detection and ranging lidar;atmospheric optics;boundary layer;air pollution;remote sensing;humidity;stability condition;atmospheric aerosols;relative humidity;optical sensors;vertical distribution;atmospheric modeling;optical sensor;air quality;aerosols atmospheric modeling laser radar instruments atmospheric measurements backscatter optical sensors;planetary boundary layer;aerosol optical depth aod;aerosols	With the dramatical climate changing that we are facing today, atmospheric monitoring is of major importance. Several atmospheric monitoring instruments are already being deployed for real-time surface-level retrieval of atmospheric composition, optical coefficients, particulate matter less than 2.5 μm in diameter (PM2.5), aerosol optical depth (AOD), and particle size distribution. However, these measurements are, in general, very cost intensive and can realistically be deployed only over very limited areas. Therefore, it is very important that advanced modeling methods be employed to fill these gaps and provide air quality predictions that can be used for forecasts as well as a better understanding of the interplay of meteorology, atmospheric emissions, and chemistry. In particular, for the New York State area, the New York State Department of Environmental Conservation uses Community Multiscale Air Quality (CMAQ) model to couple meteorology to local emissions, and there is intense interest in trying to assess the model performance beyond current surface network measurements. In particular, a deeper understanding of boundary layer processes can be made by experimentally exploring the vertical distribution model forecasts to better understand the underlying causes when model forecast results are not accurate. To this end, we develop a comprehensive Mie-scattering-based procedure including the effects of relative humidity that allows us to convert CMAQ aerosol distribution data into vertical-profile multiwavelength optical parameters that can be compared to column-integrated and vertical-profiling measurements to assess model performance and point to areas where the model is deficient. In particular, we make use of multiwavelength light detection and ranging (LIDAR), sunphotometer, ceilometer, and existing tapered element oscillating microbalance (TEOM) measurements to assess various vertical and column-integrated parameters of the CMAQ model under different stability conditions. In particular, we find that, for cases where the planetary boundary layer (PBL) is stable, the column-integrated AOD comparisons are in good agreement unlike the days with dynamic PBL. This is also consistent with observations that the TEOM PM2.5 trends are closely followed by the CMAQ model during these stable conditions. On the other hand, significant errors between the surface CMAQ PM2.5 and TEOM measurements can occur which can be traced to unphysically high particulate concentration profiles distributed too close to the surface not seen in ceilometer/LIDAR profiles. Finally, we note that, even for the stable cases, the multiwavelength optical depth data show that, for sufficiently low wavelengths, the column AOD in the model is underestimated, illustrating that there is a general underestimation of ultrafine (Aitken) particulates which can dramatically affect health.	cmaq;coefficient;experiment;minimally invasive education;planetary scanner;real-time clock;subsurface scattering	Viviana Vladutescu;YongHua Wu;Barry M. Gross;Fred Moshary;Samir Ahmed;Reginald A. Blake;Mohammad Razani	2012	IEEE Transactions on Instrumentation and Measurement	10.1109/TIM.2011.2178664	meteorology;lidar;optical depth;atmospheric model;air quality index;particulates;mie scattering;ceilometer;atmospheric sciences;particle-size distribution;boundary layer;sensor;relative humidity;humidity;regression model validation;particle size;climate change;atmospheric optics;backscatter;physics;remote sensing;air pollution;planetary boundary layer	Metrics	86.38816240777932	-62.46725231348393	2863
d3d920a44ed67a9e69ae5231d1149048a820aa74	a novel robot for arm motor therapy with homogeneous mechanical properties	perceived impedance arm motor therapy homogeneous mechanical properties robotic platforms human motor control h man degree of freedom planar device design cost effectiveness principle portability principle ease of control principle planar h shaped cable differential mechanism constant jacobian homogeneous perceived inertia mechanical design performance evaluation;impedance;tensile stress;force;robots pulleys force impedance haptic interfaces tensile stress friction;medical robotics design engineering end effectors jacobian matrices manipulator kinematics;pulleys;robots;haptic interfaces;friction	Robotic platforms developed to assist conventional motor therapy and to investigate human motor control have received an increasing interest over the past decades. However, for most of the proposed solutions, bulkiness and expensiveness have limited the use of such devices to specialized clinics that can afford their cost. This paper presents the H-Man, a two degree-of-freedom planar device designed according to three main principles: cost-effectiveness, portability and ease of control. The key component of the device is a planar H-shaped cable differential mechanism which ensures a constant Jacobian and homogeneous perceived inertia over the entire workspace. The paper presents the mechanical design as well as the performance evaluation in terms of perceived impedance.	characteristic impedance;haptic technology;intrinsic safety;jacobian matrix and determinant;microcontroller;performance evaluation;robot;software portability;time-scale calculus;workspace	Paolo Tommasino;K. C. Welihena Gamage;Lorenzo Masia;Charmayne M. L. Hughes;Domenico Campolo	2014	2014 13th International Conference on Control Automation Robotics & Vision (ICARCV)	10.1109/ICARCV.2014.7064502	robot;control engineering;simulation;engineering;artificial intelligence;electrical impedance;friction;control theory;stress;pulley;force;quantum mechanics	Robotics	72.19238927385695	-25.57812832761106	2865
ad592e07ae75e7aecdde67d88d45d493a02cd46d	steganalysis of lsb matching exploiting high-dimensional correlations between pixel differences	spatial domain steganography methods;cover images;spatial lsb matching steganography;histograms;steganography histograms statistical distributions pixel gray scale maximum likelihood estimation information science computer industry noise reduction wavelet domain;image coding;steganalysis;high dimensionality;image matching;information hiding;stego images;lsb matching;linear discriminate analysis;transform coding;joints;gray scale;feature vector;jpeg steganography;steganography;statistical distributions;jpeg steganography fisher linear discrimination analysis spatial lsb matching steganography grayscale images cover images stego images spatial domain steganography methods;fisher linear discrimination analysis;computational complexity;pixel;grayscale images;steganography image matching;pixel differences;pixel differences information hiding steganography steganalysis lsb matching	In this paper, a new steganalytic method exploiting high-dimensional correlations between pixel differences is proposed, which is designed to detect the presence of spatial LSB matching steganography in grayscale images. The proposed steganalytic method is based on a Fisher LDA(Linear discrimination analysis) classifier trained on feature vectors corresponding to cover-images and stego-images,and the distinguishing features are built from marginal and joint statistics of pixel differences. Experimental results show that the proposed method exhibits excellent performances for the detection of LSB matching steganography in grayscale images. Moreover, it has a low computational complexity and fast computational speed.The proposed steganalytic method can also be applied to the detection of other spatial-domain steganography methods,and the idea can be used for reference while designing steganalytic methods of JPEG steganography.	algorithm;coefficient;computational complexity theory;discrete cosine transform;grayscale;jpeg;least significant bit;linear discriminant analysis;marginal model;performance;pixel;statistical model;steganalysis;steganography;utility functions on indivisible goods	Tao Zhang;Yan Zhang;Wenxiang Li;Xijian Ping	2009	2009 Fifth International Conference on Natural Computation	10.1109/ICNC.2009.195	computer vision;speech recognition;pattern recognition;mathematics	Vision	37.21547611294075	-62.01262730905955	2867
b56ffbf0f38c21a1db10c1f51f1340b46000356d	semiparametric analysis of the additive hazards model with informatively interval-censored failure time data		Regression analysis of failure time data has been discussed by many authors and for this, one of the commonly used models is the additive hazards model, for which some inference procedures have been developed for various types of censored data. In this paper, a much general type of censored data, case K informatively interval-censored data, is considered for which there does not seem to exist an established inference procedure. For the problem, a joint modeling approach that involves a two-step estimation procedure and the sieve maximum likelihood estimation is presented. The proposed estimators of regression parameters are shown to be consistent and asymptotically normal, and a simulation study conducted suggests that the proposed procedure works well for practical situations. In addition, an application is provided. © 2018 Elsevier B.V. All rights reserved.	censoring (statistics);interval arithmetic;semiparametric model;simulation;utility functions on indivisible goods	Shuying Wang;Chunjie Wang;Peijie Wang;Jianguo Sun	2018	Computational Statistics & Data Analysis	10.1016/j.csda.2018.03.011	maximum likelihood;estimator;regression analysis;econometrics;statistics;mathematics;censoring (statistics);inference	AI	29.590042395636733	-22.923700120594123	2871
f3af02c6647ca476a1686196c0a4f8edc10bc37c	detection of new drug indications from electronic medical records	silicon;drugs;diseases;biomedical measurement;natural language processing;bioinformatics	Drug repositioning — detection of new uses of existing drugs — is an emerging trend in pharmaceutical industry. It essentially is a multiple aspect process of analyzing large-scale heterogeneous data for exploiting advantage of off-targets of the existing drugs. Three kinds of omics, phenomic and drug data are often integrated and used to study drug repositioning. The recent prevalence of electronic medical records (EMRs) makes it become an extremely significant resource of phenomic data for drug repositioning in the post-market stage. However, there is still no generic process and method to this end. This work aims to establish such a process and method. The paper addresses the solution of the first two problems in this complex process.	omics;regular expression	Tran-Thai Dang;Phetnidda Ouankhamchan;Tu-Bao Ho	2016	2016 IEEE RIVF International Conference on Computing & Communication Technologies, Research, Innovation, and Vision for the Future (RIVF)	10.1109/RIVF.2016.7800298	pharmacology;medicine;data mining;biological engineering	Robotics	-2.9825527292519656	-66.1871309044612	2875
fb04d7a2e53554b47773388a6a461e77c21dd198	do incumbents improve service quality in response to entry? evidence from airlines' on-time performance	quality competition;theoretical model;grupo de excelencia;entry;price competition;administracion de empresas;airline industry;economia y empresa;southwest;airlines;entry threat;grupo a;on time performance;service quality	We examine if and how incumbent firms respond to entry and entry threats using nonprice modes of competition. Our analysis focuses on airline service quality. We find that incumbent on-time performance OTP actually worsens in response to entry, and even entry threats, by Southwest Airlines. Since Southwest is both a top-performing airline in OTP and a low-cost carrier LCC, we conjecture that this response by incumbents may be due to a cost-cutting strategy that allows for intense postentry price competition along with preentry deterrence, or it may be due to a postentry differentiation strategy along with preentry accommodation. Further analysis of entry and entry threats by other airlines is inconclusive, providing evidence that is partially consistent with both hypotheses. Nonetheless, the phenomenon of worsening OTP can only be observed when the potential entrant is a LCC Southwest, Jet Blue, and AirTran.#R##N##R##N#Data, as supplemental material, are available at  http://dx.doi.org/10.1287/mnsc.2014.1918 .#R##N##R##N#This paper was accepted by by Bruno Cassiman, business strategy.		Jeffrey T. Prince;Daniel H. Simon	2015	Management Science	10.1287/mnsc.2014.1918	economics;marketing;operations management;advertising;management;service quality	HPC	-2.719346251405906	-8.80377026877334	2876
c30555c7f03b43a5e390a29cbfa5d123908db589	on the distribution free continuous-review inventory model with a service level constraint	service level;continuous review;inventory	Article history: Received 17 August 2009 Accepted 31 August 2010 Available online 9 September 2010	inventory theory	M. Mahdi Tajbakhsh	2010	Computers & Industrial Engineering	10.1016/j.cie.2010.08.022	reorder point;reliability engineering;inventory;service level;economics;operations management;welfare economics	AI	2.8289396969534764	-7.616689982261663	2878
ce94a722d1778df982a4af3669c3aa5c6f15ad7c	numerical study of polarimetric bistatic scattering dependence on sea spectrum at low wind speed at l- and c bands		With the current and planned scientific missions related to ocean observation, there is a need to understand the relevant scattering physics and sensitivity so as to provide a physical basis for retrieving geophysical information from ocean-scattered signals. Sea spectrum is one of the factors to be determined in the retrieval. Since difference in sea spectrum manifests itself mostly at sea waves of intermediate and large scales, it is desirable to include sea waves of all scales (large, intermediate, and small scales) at the same numerical simulation. In this study, we extend the stochastic second degree iterative algorithm with sparse matrix and Chebyshev approximation (SSD-SM-Cheby) method that we previously developed to the analysis of sensitivity of bistatic scattering pattern upon sea spectrum at both L and C bands at low wind speed. Four popular sea spectra are considered. Results are provided and discussed.	algorithm;approximation theory;computer simulation;iterative method;polarimetry;scattering theory;solid-state drive;sparse matrix	Jingsong Yang;Yang Du;Jiancheng Shi;Ruitao Gao	2017	2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)	10.1109/IGARSS.2017.8127234	spectral line;wind speed;geodesy;approximation theory;remote sensing;bistatic radar;sea surface temperature;surface wave;polarimetry;computer science;scattering	Arch	83.420746118816	-67.13665823663023	2880
ee8733555caa963274c3826b4eb6c98b7da751ee	delete-2 jackknife-after-bootstrap in regression	influential observation;masking;swamping;cook s distance;jackknife after bootstrap	In this study, we propose a delete-2 jackknife-after-bootstrap method to refine the cut-offs for the well-known diagnostic measure Cook’s distance when the data have multiple influential data points with masking and swamping effects. The performance of the proposed method is compared with one of the most recent approaches in the literature through real world examples as well as a designed simulation study. Copyright © 2014 John Wiley & Sons, Ltd.	bootstrapping (statistics);data point;jackknife resampling;john d. wiley;mask (computing);simulation;whole earth 'lectronic link	Ufuk Beyaztas;Aylin Alin	2014	Quality and Reliability Eng. Int.	10.1002/qre.1711	econometrics;cook's distance;computer science;masking;data mining;mathematics;influential observation;statistics	AI	28.66795436200231	-23.782004968984104	2881
2f7c4cbd6ac373a0cb3c0a8bd358d1d75078105e	a density-based dynamic od estimation method that reproduces within-day congestion dynamics	density based dynamic od estimation method;detectors;estimation theory;highway network density based dynamic od estimation method within day congestion dynamics original destination estimation procedure traffic engineering problems;estimation method;within day congestion dynamics;road traffic;highway network;loading;dynamic origin destination estimator;estimation detectors load modeling cities and towns data models road transportation loading;cib_traffic;origin and destination;estimation;mathematical models;traffic density;original destination estimation procedure;demand estimation;traffic models;cities and towns;traffic engineering problems;traffic engineered;traffic counts;road transportation;load modeling;road traffic estimation theory;data models	The OD estimation procedure is of paramount importance in traffic engineering problems. However, existing procedures are built upon the well-known static OD estimation methods, with the result that in practice the estimated OD flows sometimes hardly reproduce current traffic conditions. In this paper we identify the reasons that can cause this problem. Further, we propose a novel density-based OD estimation procedure that outperforms traditional dynamic estimation approaches, which tend to misinterpret the data, confusing free-flow and congestion regimes. The advantage of this approach is shown by performing demand estimation on the highway network around the city of Antwerp, Belgium.	network congestion;software propagation;synthetic intelligence;vii;whole earth 'lectronic link	Rodric Frederix;Francesco Viti;Chris M. J. Tampère	2010	13th International IEEE Conference on Intelligent Transportation Systems	10.1109/ITSC.2010.5625220	simulation;geography;transport engineering;forensic engineering	Visualization	9.494293435987597	-11.555192283413522	2885
4ef4d0305df88272fed4ded4a80c65a57bc304e4	speech recognition interface using dual channel post-filter based on transfer function estimation for noise reduction	transfer function;noise reduction;speech recognition	This paper proposes the accurate transfer function estimation method to improve the performance of dual channel post-filter for noise reduction in speech recognition interface. In case of applying Wiener filer based post filtering, cross-power spectrum representing cross-correlation of inputted noises having higher coherence is computed in estimating transfer function. At this time, least mean square algorithm to make noise characteristics equally is applied after we assume that respective channels have different noise characteristics. This enhances the performance of noise reduction rate because this enable to satisfy the general assumption that noise characteristics of respective channels have same one. Experimental evaluation is done using real car noisy speech databases, which showed better performance than other's works.	noise reduction;speech recognition;transfer function	Heungkyu Lee;June Kim	2006			gradient noise;gaussian noise;computer vision;speech recognition;value noise;computer science;noise measurement;noise;noise reduction;transfer function	EDA	83.63070336609945	-34.000090334936495	2894
12c5179c88f6ebf42a5b965a7440461ff67eaed8	double refinement network for efficient indoor monocular depth estimation		Monocular Depth Estimation is an important problem of Computer Vision that may be solved with Neural Networks and Deep Learning nowadays. Though recent works in this area have shown significant improvement in accuracy, state-of-the-art methods require large memory and time resources. The main purpose of this paper is to improve performance of the latest solutions with no decrease in accuracy. To achieve this, we propose a Double Refinement Network architecture. We evaluate the results using the standard benchmark RGB-D dataset NYU Depth v2. The results are equal to the current state-of-the-art, while frames per second rate of our approach is significantly higher (up to 15 times speedup per image with batch size 1), RAM per image is significantly lower.	artificial neural network;autostereogram;benchmark (computing);computer vision;deep learning;network architecture;random-access memory;real-time transcription;refinement (computing);speedup	Nikita Durasov;Mikhail Romanov;Valeriya Bubnova;Anton Konushin	2018	CoRR			Vision	28.096434513083306	-53.5203165772558	2896
83968597173ff4d3f16fa4d0502a481b24e039b1	parallel local search for solving constraint problems on the cell broadband engine (preliminary results)	shared memory;local search algorithm;search space;cell broadband engine;artificial intelligent;linear time;parallel implementation;combinatorial optimization;local search	We explore the use of the Cell Broadband Engine (Cell/BE for short) for combinatorial optimization applications: we present a parallel version of a constraint-based local search algorithm that has been implemented on a multiprocessor BladeCenter machine with twin Cell/BE processors (total of 16 SPUs per blade). This algorithm was chosen because it fits very well the Cell/BE architecture and requires neither shared memory nor communication between processors, while retaining a compact memory footprint. We study the performance on several large optimization benchmarks and show that this achieves mostly linear time speedups, even sometimes super-linear. This is possible because the parallel implementation might explore simultaneously different parts of the search space and therefore converge faster towards the best sub-space and thus towards a solution. Besides getting speedups, the resulting times exhibit a much smaller variance, which benefits applications where a timely reply is critical.	cell (microprocessor);central processing unit;combinatorial optimization;converge;fits;ibm bladecenter;local search (optimization);mathematical optimization;memory footprint;multiprocessing;search algorithm;shared memory;time complexity	Salvador Abreu;Daniel Diaz;Philippe Codognet	2009		10.4204/EPTCS.5.8	beam search;mathematical optimization;combinatorial optimization;tabu search;computer science;local search;theoretical computer science;hill climbing;machine learning;iterated local search;incremental heuristic search;best-first search;combinatorial search;guided local search;state space search;search algorithm	HPC	22.05698265705119	2.901103685005642	2898
d89364d3fc566279522e2c1d33219c2a09370af7	impurities in elliptical quantum corrals	kondo rkky interactions;corail quantique;quantum corrals;itinerant model;rkky interaction;03 67 pp;entanglement;von neumann entropy;interaction superechange;kondo effect;confinement;modele itinerant;quantum corral;interaction rkky;03 67 pp 03 67 a;singlet state;magnetic impurities;entropie;superexchange interactions;etat singulet;modelo itinerante;73 21 b quantum corrals;entropy;impurete magnetique;estado singular;7321;03 67 a;effet kondo;73 21 b	Elliptical quantum corrals present interesting properties due to the combination of confinement and focalizing properties. We show here that two magnetic impurities located at the foci of the systems experience an enhanced interaction, as compared to the one they would have in an open surface. These impurities interact via a superexchange AF interaction J with the surface electrons in the ellipse. For small J they are locked in a singlet state, which weakens for larger values of this parameter. When J is much larger than the hopping parameter of the electrons in the ellipse, both spins decorrelate and form a singlet with the nearest electron, thus presenting a confined RKKY–Kondo transition. We can also interpret this behavior via the entanglement or von Neumann entropy between the localized impurities and the itinerant electrons of the ellipse: for small J the entropy is nearly zero while for large J it is maximum. r 2008 Elsevier Ltd. All rights reserved. PACS: 03.67.Pp; 03.67. a; 73.21. b	af-heap;anisotropic filtering;electron;frequency-hopping spread spectrum;kosterlitz–thouless transition;picture archiving and communication system;quantum entanglement;singlet state	K. Hallberg;M. Nizama;J. d'Albuquerque e Castro	2008	Microelectronics Journal	10.1016/j.mejo.2008.01.041	entropy;quantum electrodynamics;condensed matter physics;physics;quantum mechanics	AI	93.78418514222338	-8.129190766141331	2903
91d8b4dbc42bf02b9814c79d9efaf277c1035c52	computational methods in neuroengineering	biomedical engineering;neurosciences;humans;computational biology	"""Copyright © 2013 Chang-Hwan Im et al. This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. Neuroengineering is an emerging discipline in the field of medical and biological engineering with the aim of understanding , modulating, enhancing, or repairing neuronal systems , which are obviously the most complex systems of the human body. During the last decade or so, neuroengineering has been growing rapidly and expanded its applications from interpreting and processing neuronal signals to interfacing the neural systems with external devices to restore lost functions. Despite its short history, neuroengineering has now become one of the most important topics in the current biomedical engineering research. As in other interdisci-plinary fields, computational methods have played key roles in the development of neuroengineering. Considering the aforementioned trends, it seems natural that neuroengineering is selected as the theme of this special issue. This special issue includes eleven high-quality, peer-reviewed articles that might provide researchers in the field of neuroscience, engineering, psychology, and computational sciences with the current state-of-the-art knowledge of this emerging interdisciplinary research field. The paper """" Trial-by-trial adaptation of movements during mental practice under force field """" by M. N. Anwar and S. H. Khan studied how motor imagery influences trial-to-trial learning in a robot based adaptation task. The results showed that reaching movements performed with motor imagery have relatively a more focused generalization pattern and a higher learning rate in training direction. The paper """" Evaluation of EEG features in decoding individual finger movements from one hand """" by R. Xiao and L. Ding investigates the existence of a broadband feature in EEG to discriminate individual fingers from one hand, with the significantly higher average decoding accuracy than guess level by the spectral principal component analysis (PCA). The paper """" Coercively adjusted auto regression model for forecasting in epilepsy EEG """" by S.-H. Kim et al. proposes a coercively adjusted auto regression (CA-AR) method to forecast future values from a multivariate epilepsy EEG time series with higher accuracy and improved computational efficiency. The paper """" A mixed L2 norm regularized HRF estimation method for rapid event-related fMRI experiments """" by Y. Lei et al. presents a new regularization framework to identify trial-specific BOLD responses in extremely rapid event-related fMRI experiments, where BOLD responses are heavily overlapped from adjacent …"""	acclimatization;bioengineering;complex systems;computation (action);computational science;copyright;electroencephalography;engineering psychology;epilepsy;experiment;fingers, unit of measurement;force field (chemistry);generalization (psychology);guided imagery;license;movement;nanorana aenea;neuroscience discipline;principal component analysis;projections and predictions;time series;biomedical engineering field;fmri	Chang-Hwan Im;Lei Ding;Yiwen Wang;Sung-Phil Kim	2013		10.1155/2013/617347	psychology;computational biology;simulation;speech recognition;medicine;pathology;computer science;artificial intelligence;machine learning;mathematics;communication;algorithm;statistics	ML	22.603125468948313	-89.23507496686251	2908
42a5a9ce7802cda4e572ea28372407866cebcda3	continuous behaviour knowledge space for semantic indexing of video content	classifier fusion;video databases;semantic indexing;decision tree;behavior knowledge space model;video retrieval;biomedical imaging;indexing and retrieval;video content indexing and retrieval classifier fusion behavior knowledge space;heterogeneous classifier;indexing;image color analysis;statistical computing;eurecom ecole d ingenieur telecommunication centre de recherche graduate school research center communication systems;video content retrieval;multimedia communication;statistics;videocontent indexing and retrieval;video retrieval content based retrieval decision making indexing video databases;speech recognition;k nearest neighbor;image texture analysis;rendering computer graphics;decision trees;content based retrieval;semantic indexing behavior knowledge space model heterogeneous classifier decision making video content retrieval;indexing content based retrieval speech recognition biomedical imaging image color analysis image texture analysis rendering computer graphics multimedia communication statistics decision trees;video content indexing and retrieval;behavior knowledge space	In this paper we introduce a new method for fusing classifier outputs. It is inspired from the behavior knowledge space model with the extra ability to work on continuous input values. This property allows to deal with heterogeneous classifiers and in particular it does not require to make any decision at the classifier level. We propose to build a set of units, defining a knowledge space, with respect to classifier output spaces. A new sample is then classified with respect to the unit it belongs to and some statistics computed on each unit. Several methods to create cells and make the final decision are proposed and compared to k-nearest neighbor and decision tree schemas. The evaluation is conducted on the task of video content retrieval which will reveal the efficiency of our approach	binary classification;decision tree;digital video;fuzzy concept;heterogeneous system architecture;k-means clustering;k-nearest neighbors algorithm;knowledge space;statistical classification	Fabrice Souvannavong;Benoit Huet	2006	2006 9th International Conference on Information Fusion	10.1109/ICIF.2006.301568	computer vision;computer science;multimedia;information retrieval	DB	39.120665591988164	-62.53546088796001	2910
769faff89d14c548d65de6d3f8342bcd51b4cebe	mdl denoising revisited	transformation ondelette;gaussian noise;traitement signal;evaluation performance;mdl criterion;predictive universal coding mdl denoising earlier minimum description length wavelet based denoising clustering problem model index subband dependent coefficient distributions;minimum description length mol principle;performance evaluation;threshold detection;data compression;noise reduction vectors signal processing wavelet coefficients robustness gaussian noise data compression design methodology signal design wavelet domain;ondelette;signal design;evaluacion prestacion;signal analysis;universal coding;subband decomposition;subband dependent coefficient distributions;analisis de senal;indexing terms;critere mdl;reduccion ruido;wavelet transforms;refinement method;codage predictif;detection seuil;deteccion umbral;earlier minimum description length;vectors;model index;descomposicion subbanda;signal processing;noise reduction;codage universel;robustesse;minimum description length;signal classification;codificacion universal;reduction bruit;codificacion predictiva;classification signal;robustness;denoising;transformacion ondita;denoising minimum description length mdl principle wavelets;decomposition sous bande;classification automatique;mdl criterio;methode raffinement;wavelet transforms signal denoising;wavelet domain;automatic classification;predictive universal coding;procesamiento senal;clasificacion automatica;metodo afinamiento;predictive coding;mdl denoising;wavelets;wavelet coefficients;wavelet denoising;analyse signal;wavelet transformation;minimum description length mdl principle;signal denoising;robustez;design methodology;wavelet based denoising;clustering problem	We refine and extend an earlier minimum description length (MDL) denoising criterion for wavelet-based denoising. We start by showing that the denoising problem can be reformulated as a clustering problem, where the goal is to obtain separate clusters for informative and noninformative wavelet coefficients, respectively. This suggests two refinements, adding a code-length for the model index, and extending the model in order to account for subband-dependent coefficient distributions. A third refinement is the derivation of soft thresholding inspired by predictive universal coding with weighted mixtures. We propose a practical method incorporating all three refinements, which is shown to achieve good performance and robustness in denoising both artificial and natural signals.	cluster analysis;coefficient;information;minimum description length;noise reduction;refinement (computing);thresholding (image processing);universal code (data compression);wavelet	Teemu Roos;Petri Myllymäki;Jorma Rissanen	2009	IEEE Transactions on Signal Processing	10.1109/TSP.2009.2021633	computer vision;speech recognition;computer science;signal processing;pattern recognition;noise reduction;mathematics;statistics	Vision	53.988298618000655	-67.3684515505277	2912
d3caf7786d74a8aa7abbf29f7375730df5c98f3d	reversible data hiding based compressible privacy preserving system for color image		In digital era, privacy preservation and data size reduction are important issues and many applications handle them simultaneously. In this paper, authors introduce a novel application of reversible data hiding to protect privacy sensitive region in a color image while reducing its file size. The proposed work introduces entropy as a new performance criterion along with distortion, capacity for reversible data hiding. Evaluation metric of the proposed method is a file size of watermarked and losslessly compressed image. The proposed method preserves privacy and controls rise in image entropy by reversible data hiding.	color image;data compression;distortion;entropy (information theory);kerrison predictor;lossless compression;privacy;watermark (data file)	Vaibhav B. Joshi;Mehul S. Raval;Minoru Kuribayashi	2017	Multimedia Tools and Applications	10.1007/s11042-017-5230-8	computer vision;information hiding;computer science;color image;artificial intelligence;file size;distortion;lossless compression	HCI	39.84402903881933	-11.870440643239249	2914
a6835fe6c6c20b0ba81403dae020d82ad187185f	is measurement-based feedback still better for quantum control systems?	ecuacion estocastica;controle mode;capability of control;stochastic equation;quantum system;coherent control;control theory;uncertain systems;bucle abierto;measurement;mode control;open loop control;master equation;commande boucle fermee;indice aptitud;closed loop control;equation stochastique;journal;modelisation;indice aptitude;systeme incertain;open loop;feedback;stochastic processes;systeme quantique;master equations;commande boucle ouverte;capability index;sistema cuantico;processus stochastique;stochastic master equation;quantum control;boucle ouverte;calcul quantique;boucle reaction;equation maitresse;quantum computing;information gain;feedback control;quantum feedback control	In this paper, we put forward a fundamental question concerning feedback control of quantum systems: Is measurement-based feedback control still better than openloop control? In contrast to the classical control theory, the answer is far from obvious. This is because measurement-based feedback needs measurement to reduce the system uncertainty, whereas the measurement on a quantum system will inevitably increase the system uncertainty in turn. In fact, there is a complicated tradeoff between the uncertainty introduced and the information gained by the measurement on a quantum system. To investigate this fundamental problem, we will only focus on a typical model of coherent control mode with and without the decoherence term in the paper. By establishing some fundamental limits on the performances of both the open-loop and measurement-based feedback controls, we will demonstrate via simulation that the measurement-based feedback control of quantum systems is still superior to the open-loop control in some sense for the typical model under consideration.	coherence (physics);coherent control;control system;control theory;feedback;like button;microsoft foundation class library;performance;quantum decoherence;quantum system;simulation;stochastic process	Bo Qi;Lei Guo	2010	Systems & Control Letters	10.1016/j.sysconle.2010.03.008	control engineering;open-loop controller;stochastic process;electronic engineering;control theory;feedback;mathematics;master equation	Robotics	72.77841073684161	-8.161859624925432	2915
8b13796b07439e9e6f765fe4e8f927daa76aef55	a fast and parametric torque distribution strategy for four-wheel-drive energy-efficient electric vehicles	energy efficiency;torque;control allocation electric vehicle torque distribution;cornering maneuvers energy efficient electric vehicles fast parametric torque distribution four wheel drive controlled drivetrains over actuated systems wheel torque yaw moment drivetrain power loss driving range optimal torque distribution parametric optimization problem vehicle speed torque demand electric vehicle demonstrator driving cycles;electric vehicles;optimization;tires;wheels electric drives electric vehicles power transmission mechanical torque motors;torque wheels energy efficiency optimization tires electric vehicles;wheels	Electric vehicles (EVs) with four individually controlled drivetrains are over-actuated systems, and therefore, the total wheel torque and yaw moment demands can be realized through an infinite number of feasible wheel torque combinations. Hence, an energy-efficient torque distribution among the four drivetrains is crucial for reducing the drivetrain power losses and extending driving range. In this paper, the optimal torque distribution is formulated as the solution of a parametric optimization problem, depending on the vehicle speed. An analytical solution is provided for the case of equal drivetrains, under the experimentally confirmed hypothesis that the drivetrain power losses are strictly monotonically increasing with the torque demand. The easily implementable and computationally fast wheel torque distribution algorithm is validated by simulations and experiments on an EV demonstrator, along driving cycles and cornering maneuvers. The results show considerable energy savings compared to alternative torque distribution strategies.		Arash Moradinegade Dizqah;Basilio Lenzo;Aldo Sorniotti;Patrick Gruber;Saber Fallah;Jasper De Smet	2016	IEEE Transactions on Industrial Electronics	10.1109/TIE.2016.2540584	control engineering;electronic differential;damping torque;torque sensor;transmission;stall torque;engineering;automotive engineering;torque steering;control theory;efficient energy use;torque;physics;direct torque control	Robotics	56.309219455593514	-12.77232553343834	2923
569aa3c80789b91ffd65ddda550dd7f8b4617f65	connections between robustness, precision, and storage requirements in statistical design of filters	statistique;image processing;non linear filter;optimal filtering;optimal filter;procesamiento imagen;nonlinear filter;traitement image;data storage;filtro optimal;design and implementation;almacenamiento;robustesse;stockage;statistics;robustness;filtro no lineal;filtre optimal;storage;filtre non lineaire;nonlinear filtering;estadistica;robustez	The practical design and implementation of statistically optimal nonlinear filters must take into account the estimation precision and the robustness as factors contributing to the final filter performance. Therefore, the choice of a filter becomes in practice a tradeoff between the optimal filter performance and the precision or robustness of the designed filter. Moreover, the computational limits and especially the storage space limits have sometimes an important contribution to the filter choice. The present paper analyzes the connections between estimation precision, robustness and storage space. These connections are used to derive a consistent approach for selecting the best filters, in which a filter is considered 'better' than another one only if it outperforms the other filter under all possible circumstances. Examples and applications are presented throughout the paper.© (1999) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.	requirement;robustness (computer science)	Octavian Valeriu Sarca;Jaakko Astola	1999		10.1117/12.341074	adaptive filter;control engineering;computer science;control theory;filter design;statistics	EDA	63.08836897635905	-67.98826451718566	2926
51e95da85a91844ee939147c6f647f749437f42c	multilabel svm active learning for image classification	learning;support vector machines;active learning;image classification;image;multilabel;classification;support vector machines support vector machine classification image classification labeling humans computer vision learning systems image retrieval indexing image databases;computer vision;active;learning artificial intelligence support vector machines image classification computer vision realistic images;realistic images;svm;learning artificial intelligence;real world image support vector machine multilabel svm active learning image classification computer vision max loss strategy mean max loss strategy artificial data	Image classification is an important task in computer vision. However, how to assign suitable labels to images is a subjective matter, especially when some images can be categorized into multiple classes simultaneously. Multilabel image classification focuses on the problem that each image can have one or multiple labels. It is known that manually labelling images is time-consuming and expensive. In order to reduce the human effort of labelling images, especially multilabel images, we proposed a multilabel SVM active learning method. We also proposed two selection strategies: Max Loss strategy and Mean Max Loss strategy. Experimental results on both artificial data and real-world images demonstrated the advantage of proposed method.	categorization;computer vision;image	Xuchun Li;Lei Wang;Eric Sung	2004	2004 International Conference on Image Processing, 2004. ICIP '04.	10.1109/ICIP.2004.1421535	support vector machine;computer vision;computer science;machine learning;pattern recognition;automatic image annotation	Vision	34.07622689617348	-54.216657508246136	2931
a3feec3d412f990a61ade08887ba05ef7e27df4e	solving reliability redundancy allocation problems with orthogonal simplified swarm optimization	rails;rails reliability optimization;reliability;mixed integer nonlinear programming reliability redundancy allocation problem rrap simplified swarm optimization algorithm sso;swarm intelligence computational complexity gas turbines integer programming large scale systems redundancy;optimization;mpi reliability redundancy allocation problems rrap orthogonal simplified swarm optimization sso penalty guided strategy orthogonal array test series system series parallel system complex system bridge system gas turbine system overspeed protection system reliability maximization mixed integer programming problem np hard problems maximum possible improvement	This study applies a penalty guided strategy and the orthogonal array test (OA) based on the Simplified Swarm Optimization algorithm (SSO) to solve the reliability redundancy allocation problems (RRAP) in the series system, the series-parallel system, the complex (bridge) system, and the overspeed protection of gas turbine system. For several decades, the RRAP has been one of the most well known techniques. The maximization of system reliability, the number of redundant components, and the reliability of corresponding components in each subsystem have to be decided simultaneously with nonlinear constraints, acting as one difficulty for the use of the RRAP. In other words, the objective function of the RRAP is the mixed-integer programming problem with the nonlinear constraints. The RRAP is of the class of NP-hard. Hence, in this paper, the SSO algorithm is proposed to solve the RRAP and improve computation efficiency for these NP-hard problems. There are four RRAP problems used to illustrate the applicability and the effectiveness of the SSO. The experimental results are compared with previously developed algorithms in literature. Moreover, the maximum-possible-improvement (MPI) is used to measure the amount of improvement of the solution found by the SSO to the previous solutions. According to the results, the system reliabilities obtained by the proposed SSO for the four RRAP problems are as well as or better than the previously best-known solutions.	computation;expectation–maximization algorithm;integer programming;loss function;mathematical optimization;np-hardness;nonlinear programming;nonlinear system;optimization problem;series-parallel graph;single sign-on;swarm	Wei-Chang Yeh;Yuk Ying Chung;Yun-Zhi Jiang;Xiangjian He	2015	2015 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2015.7280420	mathematical optimization;simulation;computer science;machine learning;reliability;algorithm	EDA	17.24415014615579	-2.2448007624219026	2932
1b3099c3267e977c2d6cb5f8a7c4d09ee03a6e2a	semi-supervised multiview embedding for hyperspectral data classification	kernel canonical correlation analysis;hyperspectral images;multiview learning;classification;semi supervised learning;feature extraction	In this paper, a method for semi-supervised multiview feature extraction based on the multiset regularized kernel canonical correlation analysis (kCCA) is proposed for the classification of hyperspectral images. The covariance matrix of this type of data is naturally composed of distinct blocks of spectral channels, which in turn compose the hypercube. To reduce the dimensionality of the data and extract discriminant features taking advantage of this particular structure, a multiview feature extraction method is applied prior to the classification. The proposed scheme exploits both the labels (as a distinct view on the data) and unlabeled pixels into the computation of cross-correlations and regularizations terms. First, we propose a technique to automatically obtain the segmentation of the spectral profile, based on the correlation between channels. Then, the multiset kernel canonical correlation analysis is applied to find a latent space which represents mutually correlated projected views and labels. Experiments on three real hyperspectral images with two linear classifiers and comparisons to stateof-the-art feature extraction methods show the benefits of this approach, which provides classification accuracies equal or superior to those obtained by training classifiers on the original input space but with only a fraction of the original data dimensionality. & 2014 Elsevier B.V. All rights reserved.	algorithmic efficiency;computation;computer vision;cross-correlation;data (computing);dimensionality reduction;discriminant;feature extraction;kernel (operating system);laplacian matrix;linear discriminant analysis;linear separability;mathematical morphology;model selection;pixel;semi-supervised learning;semiconductor industry;stellar classification;test set;texture mapping;verification and validation	Michele Volpi;Giona Matasci;Mikhail F. Kanevski;Devis Tuia	2014	Neurocomputing	10.1016/j.neucom.2014.05.010	semi-supervised learning;computer vision;feature extraction;biological classification;computer science;machine learning;pattern recognition;mathematics;dimensionality reduction	AI	28.32579003193052	-44.87213561738244	2933
27e0bf3c793d950c7ec6b41be94f7c038543de7f	subspace tracking from missing and outlier corrupted data		We study the related problems of subspace tracking in the presence of missing data (ST-miss) as well as robust subspace tracking with missing data (RST-miss). Here “robust” refers to robustness to sparse outliers. In recent work, we have studied the RST problem without missing data. In this work, we show that simple modifications of our solution approach for RST also provably solve ST-miss and RST-miss under weaker and similar assumptions respectively. To our knowledge, our result is the first complete guarantee for both ST-miss and RST-miss. This means we are able to show that, under assumptions on only the algorithm inputs (input data and/or initialization), the output subspace estimates are close to the true data subspaces at all times. Our guarantees holds under mild and easily interpretable assumptions, and handle time-varying subspaces (unlike all previous work). We also show that our algorithm and its extensions are fast and have competitive experimental performance when compared with existing methods.	algorithm;intel matrix raid;missing data;robustness (computer science);sparse matrix	Praneeth Narayanamurthy;Sarah Knorr;Namrata Vaswani	2018	CoRR		linear subspace;machine learning;missing data;initialization;robustness (computer science);outlier;artificial intelligence;mathematics;algorithm;subspace topology	ML	26.322225498791806	-36.407273916603685	2935
fd90eee5637639623323b737dedc49d9c609128d	triple content-based ontology (tricot) for xml dissemination		Disclosed is a mirror-driving mechanism for a single-lens reflex camera, comprising a first mirror holding frame pivotally mounted on a mirror box, a second mirror holding frame pivotally mounted on the first mirror holding frame, a second mirror urging spring for urging the second mirror holding frame in a direction away from the first mirror holding frame and for urging the second mirror holding frame in a direction bringing it resilient contact with the back of the first mirror holding frame, and a second mirror defining member in engagement with an actuating pin provided on the second mirror holding frame. The resilient member which forms the second mirror defining member comprises a wire spring or a flat spring.	xml	Mirella M. Moro;Deise de Brum Saccol;Renata de Matos Galante	2009			engineering drawing;data mining;xml;information retrieval;computer science;ontology;monad (category theory);mirror box	DB	82.84188264882707	-20.274899666899383	2936
a7cb69d2e8f8f00bc49c5631b6812a8ffd9b3b79	a dynamic programming approach to variable rate speech transmission	dynamic programming;cost function;variable rate;filters;prediction algorithms;dynamic program;distortion measurement;speech enhancement;bit rate;linear predictive coding;dynamic programming filters linear predictive coding distortion measurement speech enhancement bit rate cost function prediction algorithms pulse compression methods white noise;white noise;pulse compression methods	For many sounds, the successive frames of a speech signal do not differ significantly, and they can be represented by the same set of parameters (PARCOR coefficients). This paper examines the possibility of transmitting only some of the new parameters and replacing the rest by the parameters already transmitted. Several frames are considered, and all the possible decision sequences (paths) are examined by using a dynamic programming approach. The path which minimizes a preselected cost function is chosen for transmission, resulting in a reduced average data rate.		Panos Papamichalis;Thomas P. Barnwell	1980		10.1109/ICASSP.1980.1171010	linear predictive coding;speech recognition;prediction;harmonic vector excitation coding;computer science;machine learning;dynamic programming;speech coding;mathematics;white noise;statistics	Networks	48.78914441923115	-9.157244174025767	2947
b12ae0a7e67c792cdda3aa4181a59d955db6f048	treatment of systematic errors in the processing of wide-angle sonar sensor data for robotic navigation	tratamiento datos;engineering;navegacion;maps;robot movil;errors;labeling scheme;processing;erreur;general and miscellaneous mathematics computing and information science;metodologia;mapa;systematic error;analisis forma;information processing labelling systematic errors sonar sensor data robotic navigation pixel assignments;logic;computers computerized models computer programs 1987 1989;sonar navigation sensor systems robot sensing systems ultrasonic transducers sonar applications logic testing computer vision concurrent computing ultrasonic imaging;data processing;robot navigation;traitement donnee;mobile robots;robotics;etiquetage;carte;methodologie;etiquetaje;captador medida;error analysis;navigation;measurement sensor;capteur mesure;robot mobile;range finders 420200 engineering facilities equipment techniques;signal processing;robots;information processing;pattern recognition;labelling;robotica;sonar error analysis mobile robots navigation pattern recognition signal processing;pattern analysis;robotique;error;methodology;measuring instruments;autonomous robot;moving robot;analyse forme;logique;logica;sonar;robot hermies iib	A methodology has been developed for the treatment of systematic errors that arise in the processing of sparse sensor data. A detailed application of this methodology to the construction, from wide-angle sonar sensor data, of navigation maps for use in autonomous robotic navigation is presented. In the methodology, a four-valued labeling scheme and a simple logic for label combination are introduced. The four labels Conflict, Occupied, Empty, and Unknown are used to mark the cells of the navigation maps. The logic allows for the rapid updating of these maps as new information is acquired. Systematic errors are treated by relabeling conflicting pixel assignments. Most of the new labels are obtained from analyses of the characteristic patterns of conflict that arise during the information processing. The remaining labels are determined by imposing an elementary consistent-labeling condition. >	robotic mapping;sonar (symantec)	Martin Beckerman;E. M. Oblow	1990	IEEE Trans. Robotics and Automation	10.1109/70.54730	computer vision;navigation;simulation;data processing;information processing;computer science;engineering;artificial intelligence;processing;systematic error;signal processing;methodology;robotics;logic;sonar	Robotics	52.38885743596644	-32.88874937181407	2951
e3562db0e93a32e2c6a70a8ce8b721ca0440e2b1	internal transfer learning for improving performance in human action recognition for small datasets		Human action recognition nowadays plays a key role in varieties of computer vision applications. Many computer vision methods focus on algorithms designing classifiers with handcrafted features which are complex and inflexible. In this paper, we focus on the human action recognition problem and utilize 3D convolutional neural networks to automatically extract both spatial and temporal features for classification. Specifically, in order to address the training problems with small data sets, we propose an internal transfer learning strategy adapted to this framework, by incorporating the sub-data classification method into transfer learning. We evaluate our method on several data sets and obtain promising results. With the proposed strategy, the performance of human action recognition is improved obviously.	algorithm;artificial neural network;computer vision;convolutional neural network	Tian Wang;Yang Chen;Mengyi Zhang;Jie Chen;Hichem Snoussi	2017	IEEE Access	10.1109/ACCESS.2017.2746095	transfer of learning;convolutional neural network;machine learning;small data;computer science;convolution;data set;artificial intelligence	Vision	25.251073420742117	-51.6781156646847	2953
00496a28745ed514100a814ac5ba32e6882fe341	image enhancement method via blur and noisy image fusion	image motion analysis;photometric calibration;brightness transfer function;image fusion;image restoration;image enhancement image fusion cameras degradation layout photometry calibration image restoration brightness transfer functions;brightness transfer function low light imaging motion blur noise filtering photometric calibration;information presentation;noise measurement;brightness;image enhancement;motion blur;photometry;photometry calibration filtering theory image denoising image enhancement image fusion image motion analysis image restoration noise;transfer function;image color analysis;noise filtering image enhancement method blur image fusion noisy image fusion image degradations brightness transfer function estimation photometric calibration short exposed image long exposed image motion blur;pixel;noise filtering;fusion rule;image denoising;low light;calibration;filtering theory;low light imaging;noise	We present an image enhancement algorithm based on fusing the visual information present in two images of the same scene, captured with different exposure times. The main idea is to exploit the differences between the image degradations that affect the two images. On one hand the short-exposed image is less affected by motion blur, whereas the long-exposed image is less affected by noise. Different fusion rules are designed for the luminance and chrominance components such that to preserve the desirable properties from each input image. We also present a method for estimating the brightness transfer function between the input images for photometric calibration of the short-exposed image with respect to the long-exposed image. As no global blur PSF is assumed, our method can deal with blur from both camera and object motions. We demonstrate the algorithm by a series of experiments and simulations.	algorithm;experiment;gaussian blur;image editing;image fusion;simulation;transfer function	Marius Tico;Kari Pulli	2009	2009 16th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2009.5413626	filter;image texture;image restoration;computer vision;feature detection;calibration;binary image;photometry;image processing;computer science;noise measurement;noise;gaussian blur;transfer function;image fusion;top-hat transform;brightness;pixel;computer graphics (images)	Vision	57.76818818392549	-61.51420929095729	2957
a73357d35c6dffc26e363562f14154521e84af73	micromechanical thermal converter device based on polyimide-fixed island structure	mems;gaas;hemt;micromechanical thermal converter	Design technology and characterization of new GaAs island based Micromechanical Thermal Converter (MTC) device are presented. The MTC device introduced consists of pHEMT as a microwave heater and thin film polySi/Ni resistor as a temperature sensor monolithically integrated on polyimide-fixed 1 μm thick GaAs/AlGaAs island structure. The preliminary experimental results in the device electro-thermal conversion evaluation are demonstrated.		Tibor Lalinsky;M. Krnac;S. Hascik;Z. Mozolova;L. Matay;Ivan Kostic;P. Hrkut;Jiri Jakovenko;Miroslav Husak	2003	International Journal of Computational Engineering Science	10.1142/S146587630300171X	materials science;generally accepted auditing standards;ceramic materials;electronic engineering;high-electron-mobility transistor;electrical engineering;nanotechnology;microelectromechanical systems;physics	Visualization	91.60320611061043	-15.181577764957392	2960
225348b9778e7aca601a986ab2d046f688b7428f	output feedback control of a class of stochastic hybrid systems	reconfiguration;sistema lineal;controle mode;reconfigurable control;output feedback control;reconfiguracion;jump process;reconfigurable system;control h2;computacion informatica;optimisation h2;sintesis control;sistema hibrido;systeme markov;control h infinito;mode control;proceso markov;reconfigurable architectures;static output feedback;closed feedback;grupo de excelencia;commande boucle fermee;stochastic hybrid system;state dependence;linear system;output feedback;h2 control;systeme incertain;processus saut;markovian system;retroaccion;markovian jumping parameters;saut;optimizacion h2;retroaction;ciencias basicas y experimentales;processus markov;synthese commande;h2 h control;estabilidad estocastica;bucle realimentacion cerrada;stabilite stochastique;dependencia del estado;markov process;salto;feedback regulation;hybrid system;jump linear system;commande h2;commande retour sortie;proceso salto;systeme lineaire;jumping;sistema incierto;stochastic stability;sistema markov;uncertain system;h2 optimization;architecture reconfigurable;h infinite control;control synthesis;systeme hybride;bucle realimentacion salida;commande h infini;dependance de l etat	This paper deals with static output feedback control of a class of reconfigurable systems with Markovian Parameters and state-dependent noise. The main contribution is to formulate conditions for multi-performance design related to this class of stochastic hybrid systems. The specifications and objectives under consideration include stochastic stability, H2 and H∞ performances. Another problem related to a more general class of stochastic hybrid systems, known as Markovian Jump Linear Systems (MJLS), is also addressed. This problem concerns the mode-independent output feedback control of MJLS. The obtained results are illustrated on a numerical example. c © 2008 Elsevier Ltd. All rights reserved.	algorithm;block cipher mode of operation;fault detection and isolation;feedback;h2 database engine;hybrid system;linear system;multiplicative noise;numerical analysis;performance;reconfigurable computing	Samir Aberkane;Jean Christophe Ponsart;Mickael Rodrigues;Dominique Sauter	2008	Automatica	10.1016/j.automatica.2007.09.021	control engineering;simulation;engineering;control reconfiguration;control theory;mathematics;markov process;linear system;hybrid system	Robotics	73.03380865841888	-3.1434220807321864	2961
a17ecbbaeb3d63374c45045817d6e4451472a28d	recursive diameter prediction for calculating merchantable volume of eucalyptus clones using multilayer perceptron		A very common problem in forestry is the realization of the forest inventory. The forest inventory is very important because it allows the trading of medium- and long-term timber to be extracted. On completion , the inventory is necessary to measure different diameters and total height to calculate their volumes. However, due to the high number of trees and their heights, these measurements are an extremely time consuming and expensive. In this work, a new approach to predict recursively diameters of eucalyptus trees by means of Multilayer Perceptron artificial neural networks is presented. By taking only three diameter measures at the base of the tree, diameters are predicted recursively until they reach the value of 4 cm, with no previous knowledge of total tree height. The training was conducted with only 10% of the total trees planted site, and the remaining 90% of total trees were used for testing. The Smalian method was used with the predicted diameters to calculate merchantable tree volumes. To check the performance of the model, all experiments were compared with the least square polynomial approximator and the diameters and volumes estimates with both methods were compared with the actual values measured. The performance of the proposed model was satisfactory when predicted diameters and volumes are compared to actual ones.	algorithmic efficiency;artificial neural network;computation;experiment;human error;inventory;multilayer perceptron;polynomial;quad flat no-leads package;recursion (computer science);the forest;tree (data structure)	Fabrízzio Alphonsus A. M. N. Soares;Edna Lúcia Flôres;Christian Dias Cabacinha;Gilberto Arantes Carrijo;Antônio Cláudio Paschoarelli Veiga	2012	Neural Computing and Applications	10.1007/s00521-012-0823-7	mathematical optimization;simulation;mathematics	ML	11.65303378428577	-19.64649245992338	2963
d05f266f5442977563b3995872300cb6207ee9c0	sparse pca for high-dimensional data with outliers	software and technology for statistics measurement analysis statistics;dimension reduction;grupo de excelencia;outlier detection;ciencias basicas y experimentales;matematicas;robustness;algorithm outliers simulations parameters robust design computation	A new sparse PCA algorithm is presented which is robust against outliers. The approach is based on the ROBPCA algorithm which generates robust but nonsparse loadings. The construction of the new ROSPCA method is detailed, as well as a selection criterion for the sparsity parameter. An extensive simulation study and a real data example are performed, showing that it is capable of accurately finding the sparse structure of datasets, even when challenging outliers are present. In comparison with a projection pursuit based algorithm, ROSPCA demonstrates superior robustness properties and comparable sparsity estimation capability, as well as significantly faster computation time.	algorithm;computation;principal component analysis;simulation;sparse pca;sparse matrix;time complexity	Mia Hubert;Tom Reynkens;Eric Schmitt;Tim Verdonck	2016	Technometrics	10.1080/00401706.2015.1093962	robust statistics;econometrics;anomaly detection;statistics;robustness;dimensionality reduction	ML	27.18501191525127	-34.099580276680086	2965
4fc763cc525a50bf66ff8d89e3c224e0d8eab889	real-time 3d interactive segmentation of echocardiographic data through user-based deformation of b-spline explicit active surfaces	b spline explicit active surfaces;active contours;user interaction;interactive segmentation	Image segmentation is an ubiquitous task in medical image analysis, which is required to estimate morphological or functional properties of given anatomical targets. While automatic processing is highly desirable, image segmentation remains to date a supervised process in daily clinical practice. Indeed, challenging data often requires user interaction to capture the required level of anatomical detail. To optimize the analysis of 3D images, the user should be able to efficiently interact with the result of any segmentation algorithm to correct any possible disagreement. Building on a previously developed real-time 3D segmentation algorithm, we propose in the present work an extension towards an interactive application where user information can be used online to steer the segmentation result. This enables a synergistic collaboration between the operator and the underlying segmentation algorithm, thus contributing to higher segmentation accuracy, while keeping total analysis time competitive. To this end, we formalize the user interaction paradigm using a geometrical approach, where the user input is mapped to a non-cartesian space while this information is used to drive the boundary towards the position provided by the user. Additionally, we propose a shape regularization term which improves the interaction with the segmented surface, thereby making the interactive segmentation process less cumbersome. The resulting algorithm offers competitive performance both in terms of segmentation accuracy, as well as in terms of total analysis time. This contributes to a more efficient use of the existing segmentation tools in daily clinical practice. Furthermore, it compares favorably to state-of-the-art interactive segmentation software based on a 3D livewire-based algorithm.	algorithm;b-spline;contribution;image analysis;image segmentation;livewire segmentation technique;medical image computing;medical imaging;programming paradigm;real-time clock;real-time transcription;segmentation action;synergy;biologic segmentation	Daniel Barbosa;Brecht Heyde;Maja Cikes;Thomas Dietenbeck;Piet Claus;Denis Friboulet;Olivier Bernard;Jan D'hooge	2014	Computerized medical imaging and graphics : the official journal of the Computerized Medical Imaging Society	10.1016/j.compmedimag.2013.10.002	computer vision;simulation;livewire segmentation technique;computer science;machine learning;segmentation-based object categorization;image segmentation;scale-space segmentation	Vision	45.47028447064381	-76.79499108373571	2966
f8b6be1fdaa0a2dee1bdddddfd0e183df2a1afbf	atmospheric water vapor and cloud liquid water retrieval over the arctic ocean using satellite passive microwave sensing	water vapor;radiative transfer equation;teledetection spatiale;errors;q;surface temperature;erreur;instruments;radiative transfer;passive microwave;instrumentation;space remote sensing;neural networks;amsr e;research vessels;neural nets;ocean temperature;facteur q;remote sensing atmospheric humidity atmospheric measuring apparatus neural nets oceanographic regions;information retrieval;wentz algorithm atmospheric water vapor cloud liquid water arctic ocean satellite passive microwave sensing neural networks special sensor microwave imager ssm i instrument advanced microwave scanning radiometer earth observing system amsr e instrument;instrumentacion;arctic ocean;atmospheric water vapor;advanced microwave scanning radiometer earth observing system amsr e instrument;hyperfrequence;factor q;vapeur eau;oceanographic regions;atmospheric humidity;microwave imaging;navire recherche;ocean arctique;image sensors;canal;microwave radiometry;regresion;oceano artico;winds;microwaves;atmospheric measuring apparatus;special sensor microwave imager;arctic;algorithme;satellite broadcasting;clouds arctic satellite broadcasting microwave radiometry ocean temperature information retrieval neural networks sea surface content based retrieval image sensors;radiometry;nube;sea surface;teledeteccion espacial;buque oceanografico;regression;sea surface temperature;advanced microwave scanning radiometer;eos;numerical integration;chenal;clouds;remote sensing;viento;contenido en agua;satellite passive microwave sensing;wentz algorithm;algorithms;temperature surface;water algorithms microwave radiometry neural networks nns;neural networks nns;water content;radiometrie;vent;ssm i;error;transfert radiatif;nuage;reseau neuronal;earth observing system;microwave radiometer;latitude;channels;ssm i instrument;content based retrieval;water;red neuronal;cloud liquid water;teneur eau;neural network;algoritmo;vapor agua	New algorithms for total atmospheric water vapor content (Q) and total cloud liquid water content (W) retrieval from satellite microwave radiometer data, based on neural networks (NNs) and applicable for high-latitude open-water areas, were developed. For algorithm development, a radiative transfer equation numerical integration was carried out for Special Sensor Microwave/Imager (SSM/I) and Advanced Microwave Scanning Radiometer-Earth Observing System (AMSR-E) channel characteristics for nonprecipitating conditions over the open ocean. Sets of sea surface temperatures less than 15°C, surface winds, and radiosonde (r/s) reports collected by Russian research vessels served as input data for integration. It was shown that NNs perform better than the conventional regression techniques. Q retrieval algorithms were validated both for the SSM/I and AMSR-E instruments using satellite radiometric measurements collocated in space and time with polar station r/s data. The resulting SSM/I and AMSR-E retrieval errors proved to be 1.09 and 0.90 kg/m2 correspondingly. For SSM/I Q retrievals, the algorithms were compared with the Wentz global operational algorithm. This comparison demonstrated the advantages of NN-based polar regional algorithms in comparison with the Wentz global one. The retrieval errors proved to be 1.34 and 1.90 kg/m2 ( ~ 40% worse) for the NN and Wentz algorithms correspondingly.	algorithm;artificial neural network;incidence matrix;jan bergstra;memory-level parallelism;microwave;nl (complexity);numerical analysis;numerical integration;observable;simulation;terabyte	Leonid P. Bobylev;Elizaveta Zabolotskikh;Leonid M. Mitnik;Maia L. Mitnik	2010	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2009.2028018	meteorology;radiative transfer;sea surface temperature;atmospheric sciences;artificial neural network;physics;remote sensing	Visualization	83.92993642216261	-63.54857700515413	2970
c62d6ff1abd4a828e6b645c59ed72712637f1fd7	a new novel local search integer-programming-based heuristic for pcb assembly on collect-and-place machines	heuristic;vehicle routing;integer programming;generalized tsp;printed circuit board;local search	This paper presents the development of a novel vehicle-routing-based algorithm for optimizing component pick-up and placement on a collect-and-place type machine in printed circuit board manufacturing. We present a two-phase heuristic that produces solutions of remarkable quality with respect to other known approaches in a reasonable amount of computational time. In the first phase, a construction procedure is used combining greedy aspects and solutions to subproblems modeled as a generalized traveling salesman problem and quadratic assignment problem. In the second phase, this initial solution is refined through an iterative framework requiring an integer programming step. A detailed description of the heuristic is provided and extensive computational results are presented.	computation;greedy algorithm;heuristic;integer programming;iterative method;local search (optimization);printed circuit board;printing;quadratic assignment problem;time complexity;travelling salesman problem;two-phase locking;vehicle routing problem	Anupam Seth;Diego Klabjan;Placid M. Ferreira	2016	Math. Program. Comput.	10.1007/s12532-015-0095-1	mathematical optimization;greedy algorithm;heuristic;integer programming;computer science;local search;machine learning;mathematics;printed circuit board	AI	18.743916915814022	3.271871313296519	2972
6fec646a12acfaeaf91aa5068460cc229469007a	idep: an integrated web application for differential expression and pathway analysis of rna-seq data	bioinformatics;differential gene expression;pathway analysis;rna-seq;web application	RNA-seq is widely used for transcriptomic profiling, but the bioinformatics analysis of resultant data can be time-consuming and challenging, especially for biologists. We aim to streamline the bioinformatic analyses of gene-level data by developing a user-friendly, interactive web application for exploratory data analysis, differential expression, and pathway analysis. iDEP (integrated Differential Expression and Pathway analysis) seamlessly connects 63 R/Bioconductor packages, 2 web services, and comprehensive annotation and pathway databases for 220 plant and animal species. The workflow can be reproduced by downloading customized R code and related pathway files. As an example, we analyzed an RNA-Seq dataset of lung fibroblasts with Hoxa1 knockdown and revealed the possible roles of SP1 and E2F1 and their target genes, including microRNAs, in blocking G1/S transition. In another example, our analysis shows that in mouse B cells without functional p53, ionizing radiation activates the MYC pathway and its downstream genes involved in cell proliferation, ribosome biogenesis, and non-coding RNA metabolism. In wildtype B cells, radiation induces p53-mediated apoptosis and DNA repair while suppressing the target genes of MYC and E2F1, and leads to growth and cell cycle arrest. iDEP helps unveil the multifaceted functions of p53 and the possible involvement of several microRNAs such as miR-92a, miR-504, and miR-30a. In both examples, we validated known molecular pathways and generated novel, testable hypotheses. Combining comprehensive analytic functionalities with massive annotation databases, iDEP (http://ge-lab.org/idep/) enables biologists to easily translate transcriptomic and proteomic data into actionable insights.		Steven X. Ge	2018		10.1186/s12859-018-2486-6		Comp.	-0.11815464875107384	-58.79224911283522	2977
ccecf2617b675035306ca566c176f22c79a5282e	a low complexity model for predicting slice loss distortion for prioritizing h.264/avc video	h 264 avc;slice discard;video coding;unequal error protection uep;slice loss;slice prioritization;cmse prediction	The cumulative mean squared error (CMSE) is a widely used measure of distortion introduced by a slice loss. We propose a low-complexity and low-delay generalized linear model for predicting CMSE contributed by the loss of individual H.264/AVC encoded video slices. We train the model over a video database by using a combination of video factors that are extracted during the encoding of the current frame, without using any data from future frames in the group of pictures (GOP). We then analyze the accuracy of the CMSE prediction model using cross-validation and correlation coefficients. We prioritize the slices within a GOP based on their predicted CMSE values. The performance of our model is evaluated by applying unequal error protection, using rate compatible punctured convolutional codes, to the prioritized slices over noisy channels. We also demonstrate an application of our slice prioritization by implementing a slice discard scheme, where the slices are dropped from the router when the network experiences congestion. The simulation results show that (i) the slice CMSE prediction model performs well for varying GOP structures, GOP lengths, and encoding bit rates, and (ii) the peak signal-to-noise ratio and video quality metric performance of an unequal error protection algorithm using slices prioritized by the predicted CMSE is similar to that of the measured CMSE values for different videos and channel signal-to-noise. We also extend the GOP-level slice prioritization to frame-level slice prioritization and show its performance over noisy channels.	additive white gaussian noise;algorithm;coefficient;convolutional code;cross-validation (statistics);cubic function;distortion;generalized linear model;group of pictures;h.264/mpeg-4 avc;mean squared error;network congestion;peak signal-to-noise ratio;router (computing);simulation	Seethal Paluri;Kashyap K. R. Kambhatla;Barbara A. Bailey;Pamela C. Cosman;John D. Matyjas;Sunil Kumar	2014	Multimedia Tools and Applications	10.1007/s11042-014-2334-2	real-time computing;telecommunications;computer science;theoretical computer science	ML	48.287158599411086	-16.55571115127718	2978
a6c4f0ea4441f0d524f23e57ef33ee44011845a6	improvement of modal matching image objects in dynamic pedobarography using optimization techniques	computacional vision;matching;dynamic pedobarography;fem;medical imaging;deformable objects;optimization techniques.;modal analysis	The paper presents an approach for matching objects in dynamic pedobarography image sequences, based on finite element modeling and modal analysis. The determination of correspondences between objects’ nodes is here improved using optimization techniques and, because the elements number of each object is not necessarily the same, a new algorithm to match excess nodes is proposed. This new matching algorithm uses a neighborhood criterion and can overcome some disadvantages that the usual matching “one to one” in various applications can have. The proposed approach allows the determination of correspondences between 2D or 3D objects and will be here considered in dynamic pedobarography images.	algorithm;finite element method;local search (optimization);mathematical optimization;modal logic;offset binary;pedobarography	João Manuel R. S. Tavares;Luísa Ferreira Bastos	2009		10.1142/9789812834461_0019	matching;medical imaging;computer vision;simulation;computer science;finite element method;modal analysis	Vision	50.11746331169323	-52.97619973273767	2979
063369e82c65dc558b231148a3c9447b0ba40abf	a project scheduling method based on human resource availability	project scheduling;human resource	Apparatus comprising a vertically movable arm member pivoted at one end about a vertical axis and a chucking head mounted at the other distal end thereof, for engagement with and holding an apertured article, said chucking head being arranged to rotate the chucked article and whereby apertured articles being successively fed to a loading position are subjected to a revolving movement and an upward movement while being transferred to an unloading position via a working position where they are subjected to a desired treatment.	schedule (project management)	Lizi Xie;Junchao Xiao;Dapeng Liu;Qing Wang	2008			resource management;resource leveling;systems engineering;computer science;scheduling (production processes);schedule (project management);deadline-monotonic scheduling;distributed computing;resource allocation;human resources	Robotics	10.043377583762947	3.932225918132477	2984
5cbcfa2d24cebb13991aa797f9e370c19040992f	use of the artificial vision for nutritional diagnosis of nitrogen, phosphorus, potassium and manganese in corn (utilização da visão artificial para diagnóstico nutricional de nitrogênio, fósforo, potássio e manganês em milho)			computer vision	Liliane Maria Romualdo	2013				AI	85.2728903722802	-47.823130436405364	2991
f6f12e0fbfce067d02445abde76be0522e4db329	online multiple targets detection and tracking from mobile robot in cluttered indoor environments with depth camera	depth camera;robot navigation;online tracking	"""Indoor environment is a common scene in our everyday life, and detecting and tracking multiple targets in this environment is a key component for many applications. However, this task still remains challenging due to limited space, intrinsic target appearance variation, e.g. full or partial occlusion, large pose deformation, and scale change. In the proposed approach, we give a novel framework for detection and tracking in indoor environments, and extend it to robot navigation. One of the key components of our approach is a virtual top view created from an RGB-D camera, which is named ground plane projection (GPP). The key advantage of using GPP is the fact that the intrinsic target appearance variation and extrinsic noise is far less likely to appear in GPP than in a regular side-view image. Moreover, it is a very simple task to determine free space in GPP without any appearance learning even from a moving camera. Hence GPP is very di®erent from the top-view image obtained from a ceiling mounted camera. We perform both object detection and tracking in GPP. Two kinds of GPP images are utilized: gray GPP, which represents the maximal height of 3D points projecting to each pixel, and binary GPP, which is obtained by thresholding the gray GPP. For detection, a simple connected component labeling is used to detect footprints of targets in binary GPP. For tracking, a novel Pixel Level Association (PLA) strategy is proposed to link the same target in consecutive frames in gray GPP. It utilizes optical °ow in gray GPP, which to our best knowledge has never been done before. Then we \back project"""" the detected and tracked objects in GPP to original, sideview (RGB) images. Hence we are able to detect and track objects in the side-view (RGB) images. Our system is able to robustly detect and track multiple moving targets in real time."""	algorithm;computation;connected-component labeling;graph partition;head-mounted display;map projection;maximal set;mobile robot;object detection;pixel;programmable logic array;robotic mapping;sensor;thresholding (image processing)	Yu Zhou;Yinfei Yang;Meng Yi;Xiang Bai;Wenyu Liu;Longin Jan Latecki	2014	IJPRAI	10.1142/S0218001414550015	computer vision;simulation	Vision	51.90047748417767	-42.33668599195709	2994
07a9c938001282199ca56fba83cf08254977c140	quantification trees	pattern classification;learning artificial intelligence;decision trees	"""In many applications there is a need to monitor how a population is distributed across different classes, and to track the changes in this distribution that derive from varying circumstances, an example such application is monitoring the percentage (or """"prevalence"""") of unemployed people in a given region, or in a given age range, or at different time periods. When the membership of an individual in a class cannot be established deterministically, this monitoring activity requires classification. However, in the above applications the final goal is not determining which class each individual belongs to, but simply estimating the prevalence of each class in the unlabeled data. This task is called quantification. In a supervised learning framework we may estimate the distribution across the classes in a test set from a training set of labeled individuals. However, this may be sub optimal, since the distribution in the test set may be substantially different from that in the training set (a phenomenon called distribution drift). So far, quantification has mostly been addressed by learning a classifier optimized for individual classification and later adjusting the distribution it computes to compensate for its tendency to either under-or over-estimate the prevalence of the class. In this paper we propose instead to use a type of decision trees (quantification trees) optimized not for individual classification, but directly for quantification. Our experiments show that quantification trees are more accurate than existing state-of-the-art quantification methods, while retaining at the same time the simplicity and understandability of the decision tree framework."""	big data;decision tree learning;deterministic algorithm;experiment;machine learning;mathematical optimization;optimizing compiler;random forest;statistical classification;supervised learning;test set	Letizia Milli;Anna Monreale;Giulio Rossetti;Fosca Giannotti;Dino Pedreschi;Fabrizio Sebastiani	2013	2013 IEEE 13th International Conference on Data Mining	10.1109/ICDM.2013.122	computer science;artificial intelligence;machine learning;decision tree;data mining;mathematics;statistics	ML	14.824073888790883	-39.23935811799948	2996
99280e5ad3844ed2890b22eb71b7345fd6905148	d-stability and delay-independent stability of monotone nonlinear systems with max-separable lyapunov functions	lyapunov methods;linear systems;asymptotic stability;nonlinear systems;stability analysis;robustness;delays	Stability properties of monotone nonlinear systems with max-separable Lyapunov functions are considered in this paper, motivated by the following observations. First, recent results have shown that such Lyapunov functions are guaranteed to exist for asymptotically stable monotone systems on compact sets. Second, it is well-known that, for monotone linear systems, asymptotic stability implies the stronger properties of D-stability and robustness with respect to time-delays. This paper shows that similar properties hold for monotone nonlinear systems that admit max-separable Lyapunov functions. In particular, a notion of D-stability for monotone nonlinear systems and delay-independent stability will be discussed. The theoretical results are illustrated by means of examples.	linear system;lyapunov fractal;nonlinear system;monotone	Bart Besselink;Hamid Reza Feyzmahdavian;Henrik Sandberg;Mikael Johansson	2016	2016 IEEE 55th Conference on Decision and Control (CDC)	10.1109/CDC.2016.7798745	mathematical optimization;von neumann stability analysis;discrete mathematics;nonlinear system;lyapunov function;lyapunov equation;control theory;mathematics;lyapunov redesign;linear system;lyapunov exponent;robustness	Robotics	70.22219932335467	2.22213166705896	3000
dd128d34a709934c7f82420ae312ad5a31baecac	the multiattribute group decision making method based on aggregation operators with interval-valued 2-tuple linguistic information	magdm;aggregation operator;interval valued 2 tuple;2 tuple	Many researchers have proposed lots of 2-tuple linguistic models and applied them to multiattribute group decision making (MAGDM) problems. Based on the definition of an interval 2-tuple linguistic variable given by Lin et al. in 2005 [28], this paper puts forward the interval-valued 2-tuple linguistic variable and introduces its score and accuracy functions for comparison between interval-valued 2-tuples. In addition, some aggregation operators of interval-valued 2-tuples, together with their properties, are introduced. Finally, two numerical examples are presented to illustrate the application of proposed aggregation operators in MAGDM. The results indicate that the interval-valued 2-tuple expresses preferences of decision maker more naturally than 2-tuples and aggregation operators proposed can easily aggregate the interval-valued 2-tuple information denoted by different multi-granularity linguistic term sets. © 2012 Elsevier Ltd. All rights reserved.	aggregate data;decision problem;numerical analysis	Huimin Zhang	2012	Mathematical and Computer Modelling	10.1016/j.mcm.2012.01.001	discrete mathematics;mathematics;algorithm	AI	-2.9716999347808715	-20.76251641103328	3004
2d1400f6e5d472c8115f47df6cffc280dec40b01	generation bidding game with potentially false attestation of flexible demand	signal image and speech processing;demand response;smart grids;quantum information technology spintronics;false demand attestation	With the onset of large numbers of energy-flexible appliances, in particular plug-in electric and hybrid-electric vehicles, a significant portion of electricity demand will be somewhat flexible and accordingly may be responsive to changes in electricity prices. In the future, this increased degree of demand flexibility (and the onset of only short-term predictable intermittent renewable supply) will considerably exceed present level of uncertainty in day-ahead prediction of assumed inelastic demand. For such a responsive demand idealized, we consider a deregulated wholesale day-ahead electricity marketplace wherein bids by generators (or energy traders) are determined through a Nash equilibrium via a common clearing price (i.e., no location marginality). This model assumes the independent system operator (ISO) helps the generators to understand how to change their bids to improve their net revenue based on a model of demand-response. The model of demand-response (equivalently, demand-side bidding day ahead) is based on information from load-serving entities regarding their price-flexible demand. We numerically explore how collusion between generators and loads can manipulate this market. The objective is to learn how to deter such collusion, e.g., how to set penalties for significant differences between stated and actual demand, resulting in higher energy prices that benefit certain generators.	entity;nash equilibrium;numerical analysis;onset (audio);plug-in (computing);sysop;terminator 2: judgment day;traders;zero-day (computing)	Yuquan Shan;Jayaram Raghuram;George Kesidis;David J. Miller;Anna Scaglione;Jeff Rowe;Karl N. Levitt	2015	EURASIP J. Adv. Sig. Proc.	10.1186/s13634-015-0212-3	demand patterns;demand forecasting;telecommunications;computer science;demand management;smart grid;derived demand;demand curve;market demand schedule	AI	-0.16280079106192333	-4.127079392699949	3006
078f6dd6590b5688f5d3c0a0901f198cedeb6bf2	image-based spatio-temporal modeling and view interpolation of dynamic events	image based modeling and rendering;cmu;construccion arquitectura tecnologia ambiental;computacion informatica;scene flow;view interpolation;grupo de excelencia;discrete time;voxel models;non rigid motion;view synthesis;ciencias basicas y experimentales;time use;space carving;spatio temporal models;tecnologias;spatio temporal view interpolation;dynamic scenes;ray casting;discrete sampling	We present an approach for modeling and rendering a dynamic, real-world event from an arbitrary viewpoint, and at any time, using images captured from multiple video cameras. The event is modeled as a nonrigidly varying dynamic scene, captured by many images from different viewpoints, at discrete times. First, the spatio-temporal geometric properties (shape and instantaneous motion) are computed. The view synthesis problem is then solved using a reverse mapping algorithm, ray-casting across space and time, to compute a novel image from any viewpoint in the 4D space of position and time. Results are shown on real-world events captured in the CMU 3D Room, by creating synthetic renderings of the event from novel, arbitrary positions in space and time. Multiple such recreated renderings can be put together to create retimed fly-by movies of the event, with the resulting visual experience richer than that of a regular video clip, or switching between images from multiple cameras.	algorithm;interpolation;ray casting;rendering (computer graphics);synthetic data;video clip;view synthesis	Sundar Vedula;Simon Baker;Takeo Kanade	2005	ACM Trans. Graph.	10.1145/1061347.1061351	computer vision;discrete time and continuous time;simulation;image-based modeling and rendering;computer science;ray casting;computer graphics (images)	Graphics	59.945253707165165	-48.79133755587805	3007
39dd26fda764664f749bf1108c39a8984b4fabcd	a cross-selection instance algorithm		Motivated by the idea of cross-validation, a novel instance selection algorithm is proposed in this paper. The novelties of the proposed algorithm are that (1) it cross selects the important instances from the original data set with a committee, (2) it can deal with the problem of selecting instance from large data sets. We experimentally compared our algorithm with five state-of-the-art approaches which are CNN, ENN, RNN, MCS, and ICF on 3 artificial data sets and 6 UCI data sets, including 4 large data sets, ranking from 130K to 4898K in size. The experimental results show that the proposed algorithm is very efficient and effective, especially on large data sets.	apache hadoop;big data;cloud computing;cross-validation (statistics);experiment;mapreduce;multi categories security;random neural network;regular expression;selection algorithm;windows firewall	Jun-Hai Zhai;Ta Li;Xizhao Wang	2016	Journal of Intelligent and Fuzzy Systems	10.3233/IFS-151792	computer science;artificial intelligence;machine learning;pattern recognition;data mining	ML	12.737148089004648	-40.52014846173465	3008
9862c3d4d2e2a1a233f47bee425e4ea5cd477c4d	tightening torque estimation of a screw tightening robot	screw driving process screw tightening robot ac servomotors tightening torque estimation electrical current real time system;robot sensing systems;manipulators;electrical current;estimation method;feeds;real time;ac generators;ac motors;current measurement;assembling;servomechanisms;process control;screw driving process;servomotors;assembling manipulators industrial manipulators process control torque control servomotors real time systems ac motor drives;fasteners;real time system;tightening torque estimation;ac servomotors;industrial manipulators;robotics and automation;fasteners ac motors servomotors servomechanisms robot sensing systems ac generators robotics and automation torque control feeds current measurement;screw tightening robot;real time systems;torque control;ac motor drives	A screw tightening robot with AC servomotors, without any torque sensor was developed for a variety of screws. In order to detect the accurate tightening torque, we developed a new torque estimation method by measuring the electrical current of the motor and calculating it in real time. The screw-driving process is controlled by the 4th joint's motion of the robot. Thus, this robot can tighten screws accurately and reliably without extra cost.	robot	M. Matsumura;S. Itou;Hideharu Hibi;M. Hattori	1995		10.1109/ROBOT.1995.525572	control engineering;ac motor;embedded system;torque sensor;computer science;engineering;process control;control theory;electric current;servomotor	Robotics	73.93191402855908	-20.010605015104748	3023
691cf67a09f2faeeaa46a07d0b3719cab18f52ae	estimating lighting environments based on shadow area in an omni-directional image	image based lighting;lighting environment estimation;omni directional image	To create realistic CG images, the information about the lighting is very important. There are two ways to estimate the information of the light source. One is a direct measurement method using images captured with a fish-eye lens or a spherical mirror[Debevec 1998], and the other is an indirect measurement method to estimate positions and intensities of the light sources from the shadow information of objects[Sato et al. 2003]. In the direct measurement method, by concerning pixels of the captured image as light sources having corresponding intensities, it is possible to estimate the lighting environment densely. However, for a high-intensity light source like the sun, the dynamic range of the camera is insufficient, and the radiant intensity of the light source cannot be accurately estimated. So, we propose a method that combines a direct measurement technique and an indirect measurement method. In our proposed method, the light source information of the high-intensity area in the captured image is estimated by indirect measurement method. In the experiments using real images, even for outdoor scenes that contain the high-intensity light source like the sun, the measurement of the light source environment could be performed by the proposed method. Also, it was confirmed that images including realistic shadows equivalent to real images could be created.	cg (programming language);dynamic range;experiment;pixel;radiant ai	Masashi Baba;Kesuke Haruta;Shinsaku Hiura	2016		10.1145/2945078.2945166	computer vision;computer science;image-based lighting;computer graphics (images)	Vision	57.71017364548908	-49.45439058534439	3026
33697430dfa21e66a1a8e3adec909b7afc9de486	volatility forecasting for low-volatility portfolio selection in the us and the korean equity markets		AbstractWe consider the problem of low-volatility portfolio selection which has been the subject of extensive research in the field of portfolio selection. To improve the currently existing techniques that rely purely on past information to select low-volatility portfolios, this paper investigates the use of time series regression techniques that make forecasts of future volatility to select the portfolios. In particular, for the first time, the utility of support vector regression and its enhancements as portfolio selection techniques is provided. It is shown that our regression-based portfolio selection provides attractive outperformances compared to the benchmark index and the portfolio defined by a well-known strategy on the data-sets of the Su0026P 500 and the KOSPI 200.	volatility	Saejoon Kim	2018	J. Exp. Theor. Artif. Intell.	10.1080/0952813X.2017.1354083	machine learning;replicating portfolio;financial economics;post-modern portfolio theory;artificial intelligence;computer science;application portfolio management;portfolio insurance;portfolio optimization;volatility (finance);portfolio;modern portfolio theory	ECom	6.027281881553475	-18.11236215904538	3030
c950882fb575a0536417dead6fac88bab3e591b4	stability and performance analysis of bit-stream-based feedback control systems	stability criteria;control systems;control systems noise decoding feedback control stability criteria modulation;decoding;dc servo mechanism bit stream delta sigma modulator two level dynamic quantizer sliding mode analysis;variable structure systems circuit stability delta sigma modulation feedback quantisation signal three term control;hardware resources bit stream based feedback control systems delta sigma modulator two level dynamic quantizer quantizer step size stability condition bs based control system sliding mode analysis sliding manifold optimum quantizer gain quantization noise bs based proportional integral differential controllers bs pid controllers optimal dynamic quantizer information rate channel interfaces;feedback control;noise;modulation	Bit stream (BS)-based feedback control systems often use a delta-sigma modulator (ΔΣ-M) as a two-level dynamic quantizer to encode and decode feedback signals. The stability and performance of such systems are critically dependent on the selection of the quantizer step size. This paper derives the stability condition of a BS-based control system in terms of the quantizer step size using sliding mode analysis. It is proved that the quantized system is equivalent to the original system on the sliding manifold under ideal sliding. However, the presence of the quantizer in a ΔΣ-M introduces noise into the system that often degrades the performance of the overall system. This paper therefore determines the optimum quantizer gain, i.e., the upper and lower boundaries of the quantizer, which maintains the stability and reduces the quantization noise. BS-based proportional- integral-differential (BS-PID) controllers are designed using the proposed optimal dynamic quantizer and implemented using three different realizations. Simulation results show that the optimal quantizer significantly reduces the noise within the system bandwidth. The effectiveness of the BS-PID controller with the optimal quantizer is further demonstrated using the experimental prototype of a dc servomechanism by designing a BS-PID controller. The experimental results from a laboratory prototype illustrate that the BS-PID controller gives identical performance to a traditional PID controller and effectively controls the system while consuming less information rate (channel interfaces) and hardware resources.	bitstream;code;control system;delta-sigma modulation;encode;feedback;image noise;pid;profiling (computer programming);prototype;quantization (signal processing);sampling (signal processing);simulation	Dhafer J. Almakhles;Akshya K. Swain;Nitish D. Patel	2015	IEEE Transactions on Industrial Electronics	10.1109/TIE.2014.2381161	control engineering;electronic engineering;computer science;engineering;noise;control system;control theory;feedback;modulation	Mobile	67.14653912283559	-3.50209325068868	3035
2f77a8a81ef683d9ce54c5923e0b8f7f8926f5a6	doppler spectrum of microwaves at forward scattering from the sea surface		The work is aimed at developing a new approach to bistatic remote sensing of the sea surface. The basis of the new approach is the analysis of the shape of the Doppler spectrum of the signal reflected in the forward direction from the sea surface. The Doppler spectrum contains more information about the reflecting surface than the scattering cross section, since the cross section is an integral characteristic of the Doppler spectrum. The advantage of the Doppler spectrum is also its greater versatility when comparing the measurements of various instruments comparing to the scattering cross section. The paper presents the final expressions for the width and shift of the Doppler spectrum of microwaves reflected by the sea surface during bistatic sensing in the case of a moving receiver and a transmitter. An important feature of the proposed approach is the consideration of the antennas patterns of the receiving and transmitting antennas. And it is taken into account that the antennas patterns of the receiver and transmitter can be different and asymmetrical. The correct choice and use of antennas with different patterns allows directly expressing the sea surface parameters of interest from the measured characteristics of the Doppler spectrum.	cross section (geometry);microwave;transmitter	Yury A. Titchenko;Vladimir Karaev	2018	IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2018.8517326	remote sensing;scattering cross-section;transmitter;doppler effect;forward scatter;bistatic radar;surface wave;scattering;computer science;optics;microwave	Embedded	83.14538283662088	-67.62604412187791	3036
aef4c2b51f59828aa48c49c273ea915939fb285a	attribute reduction algorithm research based on rough core and back elimination	computers;back elimination;machine learning algorithm;rough core;uncertainty;rough set theory;rough reduction superset;knowledge system reduction set;reasoning capability;testing;uncertainty handling;data mining;artificial intelligent;attribute reduction;machine learning algorithms uncertainty knowledge based systems degradation artificial intelligence pattern recognition set theory decision making educational institutions computer science;research paper;machine learning;uncertain information;knowledge systems;classification algorithms;pattern recognition;uncertainty handling data mining learning artificial intelligence rough set theory;artificial intelligence;rough reduction superset attribute reduction algorithm rough core back elimination machine learning algorithm artificial intelligence pattern recognition rough set theory uncertain information reasoning capability knowledge system reduction set;entropy;learning artificial intelligence;algorithm design and analysis;attribute reduction algorithm	Machine learning algorithms are known to degrade in performance when facing with many features that are not necessary in the field of artificial intelligence and pattern recognition. Rough set theory is a new effective tool in dealing with vagueness and uncertainty information. Attribute reduction is one of the most important concepts in rough set theory and application research. Once it gets the whole reduction set, the reasoning capability with multi attributes absent can behave well. There are few research papers on whole reduction set of knowledge system. Thus how to get the whole reduction set is worth a problem to research. In this paper, we first get the rough core, then calculate superset of rough reduction, finally get the whole really rough reduction set with back elimination. Experiment shows that the attribute reduction algorithm based on core and back elimination is effective.	algorithm;artificial intelligence;knowledge-based systems;machine learning;pattern recognition;rough set;set theory;vagueness	Guojun Zhang;Enmin Song;Guangzhi Ma;Wei Zhang	2008	2008 The 9th International Conference for Young Computer Scientists	10.1109/ICYCS.2008.485	statistical classification;algorithm design;entropy;rough set;uncertainty;computer science;artificial intelligence;knowledge-based systems;machine learning;pattern recognition;data mining;software testing	AI	-1.6729850618198683	-27.999376671585914	3039
0a36a5f70bf5d690db4bdb54a179701783e02e26	stability analysis and controller synthesis for discrete uncertain singular fuzzy systems with distinct difference matrices in the rules	discrete singular system;t s fuzzy model;linear matrix inequality lmi;stability analysis;parallel distributed compensation pdc	This paper mainly studies an extended discrete singular fuzzy model incorporating the multiple difference matrices in the rules and discusses its stability and design issues. By embracing additional algebraic constraint, traditional discrete Takagi-Sugeno (T-S) fuzzy model can be extended to a generalised discrete singular Takagi-Sugeno (GDST-S) model with individual difference matrices Ei in the locally singular models, where it can describe a larger class of physical or non-linear systems. Based on the linear matrix inequality (LMI) approach, we focus on deriving some explicit stability and design criteria expressed by the LMIs for the regarded system. Thus, the stability verification and controller synthesis can be performed by the current LMI tools. Finally, some illustrative examples are given to illustrate the effectiveness and validity of the proposed approach.	fuzzy control system	Chih-Peng Huang	2014	Int. J. Systems Science	10.1080/00207721.2012.757382	control engineering;mathematical optimization;von neumann stability analysis;control theory;mathematics	Logic	68.25308606856814	-0.811538339048703	3042
1755c5971845231c91ccc06b3fca45b8e672b3af	multivariable control for regulating high pressure centrifugal compressor with variable speed and igv	pressure control angular velocity control blades compressors high pressure techniques linear quadratic gaussian control multivariable control systems pi control predictive control;valves turbines mathematical model velocity control discharges electric blades;power plant multivariable control optimal control model predictive control linear quadratic regulator;single loop pi controller high pressure centrifugal compressor regulation igv multivariable control system control signals rotational speed inlet guide vane linear quadratic gaussian control with integral action model predictive control mpc controllers lqgi controllers standard proportional integral controller compressor discharge pressure	The objective of this paper is to develop a multivariable control system for a class of centrifugal compressors, which exploit as control signals both the rotational speed and the Inlet Guide Vane (IGV). Linear Quadratic Gaussian control with Integral action (LQGI) and Model Predictive Control (MPC) are investigated. The LQGI and MPC controllers are compared to a standard proportional integral (PI) controller, to regulate the discharge pressure of the compressor. The control algorithms are simulated and compared in different operating scenarios. Results demonstrate that the proposed multivariabe control schemes provide better performance than the single-loop PI controller, thus motivating the use of IGV for control purposes.	algorithm;centrifugal governor;control system;discharger;feedback	Toufik Bentaleb;Alessio Cacitti;Sergio De Franciscis;Andrea Garulli	2014	2014 IEEE Conference on Control Applications (CCA)	10.1109/CCA.2014.6981393	control engineering;electronic engineering;controller;engineering;automatic control;control theory	Robotics	58.59034336318224	-8.078550649126583	3043
3d14267090ebf9ed17c1135ea91fe73321346910	nonlinear multilayered representation of graph-signals	signal decomposition;weighted graphs;nonlocal total variation;multiscale representations;hierarchical models	We propose a nonlinear multiscale decomposition of signals defined on the vertex set of a general weighted graph. This decomposition is inspired by the hierarchical multiscale (BV,L 2) decomposition of Tadmor, Nezzar, and Vese (Multiscale Model. Simul. 2(4):554–579, 2004). We find the decomposition by iterative regularization using a graph variant of the classical total variation regularization (Rudin et al, Physica D 60(1–4):259–268, 1992). Using tools from convex analysis, and in particular Moreau’s identity, we carry out the mathematical study of the proposed method, proving the convergence of the representation and providing an energy decomposition result. The choice of the sequence of scales is also addressed. Our study shows that the initial scale can be related to a discrete version of Meyer’s norm (Meyer, Oscillating Patterns in Image Processing and Nonlinear Evolution Equations, 2001) which we introduce in the present paper. We propose to use the recent primal-dual algorithm of Chambolle and Pock (J. Math. Imaging Vis. 40:120–145, 2011) in order to compute both the minimizer of the graph total variation and the corresponding dual norm. By applying the graph model to digital images, we investigate the use of nonlocal methods to the multiscale decomposition task. Since the only assumption needed to apply our method is that the input data is living on a graph, we are also able to tackle the task of adaptive multiscale decomposition of irregularly sampled data sets within the same framework. We provide in particular examples of 3-D irregular meshes and point clouds decompositions.	adaptive grammar;convex analysis;convex optimization;diffusing update algorithm;digital data;digital image;dual norm;image processing;interaction;iterative method;manifold regularization;mathematical optimization;matrix regularization;nonlinear system;nonlocal lagrangian;optimization problem;point cloud;qr decomposition;quantifier (logic);total variation denoising;triangle mesh	Moncef Hidane;Olivier Lézoray;Abderrahim Elmoataz	2012	Journal of Mathematical Imaging and Vision	10.1007/s10851-012-0348-9	mathematical optimization;combinatorics;discrete mathematics;topology;mathematics;geometry;statistics	Vision	53.85320196523526	-71.48828918630277	3047
d65d5e98e7a7b6fa360b3f7147566aa5ef3a41f0	automated lesion detectors in retinal fundus images	microaneurysm detector;bright lesion detector;computer aided diagnosis;multiscale analysis;hemorrhage detector;cartoon texture decomposition;variational segmentation;retinal fundus image;wavelets	Diabetic retinopathy (DR) is a sight-threatening condition occurring in persons with diabetes, which causes progressive damage to the retina. The early detection and diagnosis of DR is vital for saving the vision of diabetic persons. The early signs of DR which appear on the surface of the retina are the dark lesions such as microaneurysms (MAs) and hemorrhages (HEMs), and bright lesions (BLs) such as exudates. In this paper, we propose a novel automated system for the detection and diagnosis of these retinal lesions by processing retinal fundus images. We devise appropriate binary classifiers for these three different types of lesions. Some novel contextual/numerical features are derived, for each lesion type, depending on its inherent properties. This is performed by analysing several wavelet bands (resulting from the isotropic undecimated wavelet transform decomposition of the retinal image green channel) and by using an appropriate combination of Hessian multiscale analysis, variational segmentation and cartoon+texture decomposition. The proposed methodology has been validated on several medical datasets, with a total of 45,770 images, using standard performance measures such as sensitivity and specificity. The individual performance, per frame, of the MA detector is 93% sensitivity and 89% specificity, of the HEM detector is 86% sensitivity and 90% specificity, and of the BL detector is 90% sensitivity and 97% specificity. Regarding the collective performance of these binary detectors, as an automated screening system for DR (meaning that a patient is considered to have DR if it is a positive patient for at least one of the detectors) it achieves an average 95-100% of sensitivity and 70% of specificity at a per patient basis. Furthermore, evaluation conducted on publicly available datasets, for comparison with other existing techniques, shows the promising potential of the proposed detectors.	bl (logic);bands;binary classification;calculus of variations;channel (digital image);detectors;diabetes mellitus;diabetic retinopathy;early diagnosis;exudate;hemorrhage;hessian;microaneurysm;numerical analysis;patients;retina;retinal diseases;sensitivity and specificity;stationary wavelet transform	Isabel N. Figueiredo;S. Kumar;Carlos Manta Oliveira;João Diogo Ramos;Björn Engquist	2015	Computers in biology and medicine	10.1016/j.compbiomed.2015.08.008	wavelet;computer vision;medicine;mathematics;statistics;computer graphics (images)	Vision	35.68956627068127	-76.40474228989754	3050
de414e99ede95fbc8336147c2e65bdc08c3bbbd3	decentralized disturbance attenuating output-feedback trackers for large-scale nonlinear systems	large scale nonlinear systems;output feedback;decentralized system;large scale;decentralized tracking;disturbance attenuation;controller design;nonlinear system	"""The problem of decentralized output-feedback tracking with disturbance attenuation is addressed for a new class of large-scale and minimum-phase nonlinear systems. Common assumptions like matching and growth conditions are not required for the underlying decentralized system with a diagonal structure. An observer-based decentralized controller design is presented. The proposed decentralized output-feedback laws achieve asymptotic tracking and internal Lagrange stability when the disturbance inputs disappear, and, guarantee external stability in the presence of disturbance inputs. These external stability properties include Sontag's ISS and iISS conditions and standard L""""2-gain property."""	block cipher mode of operation;nonlinear system	Zhong-Ping Jiang	2002	Automatica	10.1016/S0005-1098(02)00039-0	control engineering;simulation;nonlinear system;decentralised system;engineering;control theory	Robotics	68.0117459765868	-4.098250755257845	3051
093e6c44f3641141460ea24f93820beec659299b	iterative learning control of linear discrete-time multivariable systems	sistema lineal;sistema 2 dimensiones;bucle cerrado;iterative method;systeme commande;sistema control;uncertain systems;systeme multivariable;learning;systeme discret;teoria sistema;iterative learning control;adaptive control;systeme 2 dimensions;discrete time systems;discrete time;system theory;linear system;metodo iterativo;aprendizaje;systeme incertain;control system;apprentissage;control adaptativo;systems theory;system synthesis;methode iterative;2 d system theory;synthese systeme;theorie systeme;robustesse;sistema multivariable;closed loop;commande adaptative;sintesis sistema;multivariable system;output error;robustness;boucle fermee;sistema discreto;systeme lineaire;sistema incierto;uncertain system;two dimensional system;discrete time system;discrete system;robustez	The authors present an iterative learning control rule for linear discrete-time multivariable systems. The control rule assures a zero output error for the whole desired trajectory after only one learning iteration. The robustness of the control rule is studied and two numerical examples are used to validate the new control rule.	iterative method	Yong Fang;Tommy W. S. Chow	1998	Automatica	10.1016/S0005-1098(98)00091-0	control engineering;adaptive control;control system;calculus;control theory;mathematics;systems theory	Robotics	72.51986259828158	-4.314433961547129	3054
fddc33fa7d44edba61ff9b470120f06b8eb56804	landmark-based inductive model for robust discriminative tracking		The appearance of an object could be continuously changing during tracking, thereby being not independent identically distributed. A good discriminative tracker often needs a large number of training samples to fit the underlying data distribution, which is impractical for visual tracking. In this paper, we present a new discriminative tracker via the landmark-based inductive model (Lim) that is non-parametric and makes no specific assumption about the sample distribution. With an undirected graph representation of samples, the Lim locally approximates the soft label of each sample by a linear combination of labels on its nearby landmarks. It is able to effectively propagate a limited amount of initial labels to a large amount of unlabeled samples. To this end, we introduce a local landmarks approximation method to compute the cross-similarity matrix between the whole data and landmarks. And a soft label prediction function incorporating the graph Laplacian regularizer is used to diffuse the known labels to all the unlabeled vertices in the graph, which explicitly considers the local geometrical structure of all samples. Tracking is then carried out within a Bayesian inference framework where the soft label prediction value is used to construct the observation model. Both qualitative and quantitative evaluations on 65 challenging image sequences including the benchmark dataset and other public sequences demonstrate that the proposed algorithm outperforms the state-of-the-art methods.	algorithm;approximation;benchmark (computing);bittorrent tracker;discriminative model;expanded memory;graph (abstract data type);graph (discrete mathematics);inductive reasoning;laplacian matrix;similarity measure;video tracking	Yuwei Wu;Mingtao Pei;Min Yang;Yang He;Yunde Jia	2014		10.1007/978-3-319-16814-2_21	artificial intelligence;pattern recognition;graph (abstract data type);discriminative model;linear combination;eye tracking;independent and identically distributed random variables;laplacian matrix;computer science;bayesian inference;sampling distribution	AI	46.52588174505527	-50.70100218151891	3058
375912d7f1d2640173e6b7c8e5d46b1560c90127	haplotyping for disease association: a combinatorial approach	phylogeny;genetics;biochemistry;np complete problem;discrete mathematics;organisms;genomics;combinatorics;molecular biophysics;forensics;genotypes;satisfiability;bioinformatics;decision problem	"""We consider a combinatorial problem derived from haplotyping a population with respect to a , either recessive or dominant. Given a set of individuals, partitioned into healthy and diseased, and the corresponding sets of genotypes, we want to infer """"bad"""" and """"good"""" haplotypes to account for these genotypes and for the disease. Assume, for example, that the disease is recessive. Then, the resolving haplotypes must consist of bad and good haplotypes so that 1) each genotype belonging to a diseased individual is explained by a pair of bad haplotypes and 2) each genotype belonging to a healthy individual is explained by a pair of haplotypes of which at least one is good. We prove that the associated decision problem is NP-complete. However, we also prove that there is a simple solution, provided that the data satisfy a very weak requirement."""	decision problem;fits;graph coloring;intel high definition audio;mathematical optimization;np-completeness;optimization problem;phylogenetics	Giuseppe Lancia;R. Ravi;Romeo Rizzi	2008	IEEE/ACM Transactions on Computational Biology and Bioinformatics	10.1145/1371585.1371593	biology;organism;genomics;np-complete;bioinformatics;decision problem;genotype;genetics;satisfiability;molecular biophysics	Comp.	1.4328658557050977	-51.17206908720379	3070
bb6dea3179df0eda8768560057938630e5ac1c61	enhancing effectiveness of density-based outlier mining	outlier mining;machine learning algorithms;complexity theory;asc;density measurement;medical signal detection;signal analysis;space complexities;density based outlier mining;density based;intrusion detection;data mining;average series cost;data mining computational complexity;space complexities density based outlier mining density similarity neighbor based outlier factor algorithm data mining similar density series average series cost;distance measurement;prediction methods;computational complexity;similar density series;information processing;dsnof;space complexity;clustering algorithms data mining intrusion detection information processing proposals prediction methods density measurement algorithm design and analysis medical signal detection signal analysis;clustering algorithms;sds;density similarity neighbor based outlier factor algorithm;dsnof outlier mining density based sds asc;proposals;algorithm design and analysis	Outlier mining is an important work of data mining and a density-similarity-neighbor based outlier factor (DSNOF) algorithm is proposed to indicate the degree of outlier-ness of an object. The proposed algorithm calculates the densities of an object and its neighbors and constructs the similar density series (SDS) in the neighborhood of the object. Based on the SDS, the proposed algorithm computes the average series cost (ASC) of the object and the DSNOF of the object can be obtained according to the ASC of the object and those of the neighbors of the object. The experiments are performed on the synthetic and the real datasets. The experiments results verify that the proposed algorithm not only can detect outlier more effectively and but also do not increase the time and the space complexities.	algorithm;dspace;data mining;earthbound;experiment;synthetic data;ti advanced scientific computer	Hui Cao;Gangquan Si;Wenzhi Zhu;Yanbin Zhang	2008	2008 International Symposiums on Information Processing	10.1109/ISIP.2008.67	computer science;machine learning;pattern recognition;data mining	ML	-0.2674140592875913	-38.92378866120984	3075
02e27fbe6b66fc8860adfb28df46ddd3edd92741	scale invariant value computation for reinforcement learning in continuous time		Natural learners must compute an estimate of future outcomes that follow from a stimulus in continuous time. Critically, the learner cannot in general know a priori the relevant time scale over which meaningful relationships will be observed. Widely used reinforcement learning algorithms discretize continuous time and use the Bellman equation to estimate exponentially-discounted future reward. However, exponential discounting introduces a time scale to the computation of value, implying that the relative values of various states depend on how time is discretized. This is a serious problem in continuous time as successful learning requires prior knowledge of the solution. We discuss a recent computational hypothesis, developed based on work in psychology and neuroscience, for computing a scale-invariant timeline of future events. This hypothesis efficiently computes a model for future time on a logarithmically-compressed scale. Here we show that this model for future prediction can be used to generate a scale-invariant power-law-discounted estimate of expected future reward. The scale-invariant timeline could provide the centerpiece of a neurocognitive framework for reinforcement learning in continuous time. Introduction In reinforcement learning, an agent learns how to optimize its actions from interacting with the environment, aiming to maximize temporally-discounted future reward. In order to navigate the environment, the agent perceives stimuli that define different states. The stimuli are experienced embedded in continuous time with temporal relationships that the agent needs to learn in order to learn the optimal action policy. Temporal discounting is well justified by numerous behavioral experiments on humans and animals (see e.g. Kurth-Nelson, Bickel, and Redish (2012)) and it is useful in numerous practical applications (see e.g. Mnih et al. (2015)). If the value of a state is defined as expected future reward discounted with an exponential function of future time, value can be updated in a recursive fashion, following the Bellman equation (Bellman, 1957). The Bellman equation is a foundation of highly successful and widely used modern reinforcement learning approaches such as dynamic programming and temporal difference (TD) learning (Sutton and Barto, 1998). Copyright c © 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Exponential temporal discounting is not scale-invariant When using the Bellman equation (or exponential discounting in general), values assigned to the states will depend on the chosen discretization of the temporal axis in a non-linear fashion. Consequently the ratio of the values attributed to the states changes as a function of the chosen temporal resolution and the base of the exponential function. To illustrate this let us define the value of a state s observed at time t as a sum of expected rewards r discounted with an exponential function:	algorithm;andrew barto;apache axis;artificial intelligence;bellman equation;c date and time functions;computation;digital physics;discretization;dynamic programming;embedded system;experiment;interaction;machine learning;nonlinear system;recursion;reinforcement learning;temporal difference learning;temporal logic;time complexity;timeline	Zoran Tiganj;Karthik H. Shankar;Marc W. Howard	2017				AI	22.60196763098005	-19.241312436275283	3080
eccec952da11d7375849b3fdda2ac00fb5f91865	using confusion matrices and confusion graphs to design ensemble classification models from large datasets	classification trees;ensemble classification;pvn classification;data mining;artificial neural networks;confusion graph;confusion matrix;predictive data mining	Classification modeling is one of the methods commonly employed for predictive data mining. Ensemble classification is concerned with the creation of many base models which are combined into one model for purposes of increasing classification performance. This paper reports on a study which was conducted to establish whether the use of information in the confusion matrix of a single classification model could be used as a basis for the design of ensemble base models that provide high predictive performance. Positiveversus-negative (pVn) classification was studied as a method of base model design. Confusion graphs were used as input to an algorithm that determines the classes for each base model. Experiments were conducted to compare the levels of diversity provided by all-classes-at-once (ACA) and pVn base models using a statistical measure of dis-similarity. Experiments were also conducted to compare the performance of pVn ensembles, ACA ensembles, and single kclass models using classification trees and multi-layer perceptron artificial neural networks. The experimental results demonstrated that even though ACA base models provide a higher level of diversity than pVn base models, the diversity does result in higher predictive performance. The experimental results also demonstrated that pVn ensemble models can provide predictive performance that is higher than that of single k-class models and ACA ensemble models.	confusion matrix	Patricia E. N. Lutu	2011		10.1007/978-3-642-23544-3_23	confusion matrix;computer science;machine learning;pattern recognition;data mining;artificial neural network;statistics	ML	12.019193849611282	-40.759052221320374	3082
fce5bc434a909d6d480fda7cb04cac3d278f8eeb	indexing ensembles of exemplar-svms with rejecting taxonomies	support vector machines data structures image classification image segmentation indexing object detection;rejection strategy evaluation phase joint distribution exemplar classifier scores exemplar svm indexing taxonomy object detection segmentation task indexing data structure image patches;vegetation;media;visualization;indexing;data structures;taxonomy;taxonomy indexing object detection visualization vegetation data structures media;object detection	Ensembles of Exemplar-SVMs have been used for a wide variety of tasks, such as object detection, segmentation, label transfer and mid-level feature learning. In order to make this technique effective though a large collection of classifiers is needed, which often makes the evaluation phase prohibitive. To overcome this issue we exploit the joint distribution of exemplar classifier scores to build a taxonomy capable of indexing each Exemplar-SVM and enabling a fast evaluation of the whole ensemble. We experiment with the Pascal 2007 benchmark on the task of object detection and on a simple segmentation task, in order to verify the robustness of our indexing data structure with reference to the standard Ensemble. We also introduce a rejection strategy to discard not relevant image patches for a more efficient access to the data.	benchmark (computing);computation;convolution;data structure;fast fourier transform;fastest;feature learning;object detection;rejection sampling;taxonomy (general);time complexity;vector quantization	Federico Becattini;Lorenzo Seidenari;Alberto Del Bimbo	2016	2016 14th International Workshop on Content-Based Multimedia Indexing (CBMI)	10.1109/CBMI.2016.7500241	computer vision;search engine indexing;media;visualization;computer science;machine learning;pattern recognition;data mining;world wide web;vegetation;taxonomy	Vision	31.59812108785558	-45.763246535368715	3094
12e9a953520744f907902677b2e826e8c349fab9	frame complexity prediction for h.264/avc rate control	linear prediction rate control h 264 avc r d model;rate distortion;network bandwidth;video encoder;complexity theory;psnr;data compression;r d model;rate distortion model;h 264 avc;automatic voltage control object oriented modeling encoding rate distortion predictive models centralized control video compression quantization computer science mpeg 4 standard;video compression;h 264 avc rate control;target bit estimation frame complexity prediction h 264 avc rate control video encoder network bandwidth video compression linear prediction theory rate distortion model liner rate complexity model rate quantization model;joints;target bit estimation;rate quantization model;visual quality;linear predictive;quantisation signal;rate control;video coding;video coding data compression quantisation signal telecommunication standards;frame complexity prediction;automatic voltage control;streaming media;telecommunication standards;mathematical model;linear prediction;linear prediction theory;liner rate complexity model;encoding	Rate control regulates the output bit rate of a video encoder in order to obtain optimum visual quality within the available network bandwidth and to maintain buffer fullness within a specified tolerance range. In this paper, we propose a novel rate control scheme for H.264/AVC video compression with a number of new features. We first introduce a calculation approach of frame complexity based on the linear prediction theory. Then, we propose a joint Rate-Distortion model which is an integration of a liner rate-complexity model and an exponential rate-quantization model. Finally, we develop an effective target bit estimation approach. Experimental results show that, compared with JVT-W042, our scheme achieves more accurate rate regulation, provides robust buffer control, efficiently reduces frame skipping, and improves visual quality.	data compression;distortion;encoder;h.264/mpeg-4 avc;time complexity	Ling Tian;Yu Sun;Shixin Sun	2009	2009 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2009.5202432	data compression;real-time computing;computer science;theoretical computer science;statistics	Robotics	47.081729658094794	-17.771360619101124	3095
042a6c70dcbad4d83ae87ace6f532a578c126a26	the trials of hula hooping by a musculo-skeletal humanoid kojiro nearing dancing motions using the soft spine	humanoid robot;humanoid robots humanities;humanoid robots;humanities;dancing musculoskeletal humanoid kojiro hula hooping humanoid robots supple spine structure;humans torso robot sensing systems art materials mathematical model	Many types of humanoid robots have been developed, but it is hard to find one with supple spine structure. To act in a proper manner with sophisticated, human-like motions, we believe that humanoid robots should make use of their spines effectively. To find out how to use the spine flexibly and elegantly, we focused on dancing, the art of beautiful movements. Dancing demands you to develop flexible and well coordinated movements of the torso, so we decided to pick up a movement of the sort and achieve it by the musculo-skeletal humanoid KOJIRO, in order to improve the skills needed for dancing. We took hula hooping as an ideal sample, and after generalizing the principal of hula hooping motion by an exclusive hula-hooping robot, we will describe the trials of hula hooping by KOJIRO as the first step toward dancing humanoids(Fig.1).	2.0;angularjs;dendritic spine;humanoid robot;uncontrolled format string	Yuriko Kakehashi;Tamon Izawa;Takuma Shirai;Yuto Nakanishi;Kei Okada;Masayuki Inaba	2011	2011 11th IEEE-RAS International Conference on Humanoid Robots	10.1109/Humanoids.2011.6100902	simulation;computer science;humanoid robot;artificial intelligence	Robotics	66.4847343961137	-26.50555481117386	3098
bc343856f722b0ed569485eef972997e74c2207f	texture classification based on fuzzy system applied to the cork industry	fuzzy system		cork encoding;fuzzy control system	Manuel João O. Ferreira;João L. Monteiro	1999			cork;computer vision;artificial intelligence;fuzzy control system;computer science	ML	3.670130721664167	-25.295741176235776	3107
1c0d14d90c53f3dc3f4a7e6415f3137f7257f6ee	computational inference of compound-induced anti-inflammatory effects across time in an adjuvant-induced arthritis rat model.	time course;gene expression data;dynamic bayesian network;rat model;cell adhesion molecule;inflammatory disease;experimental validation;adjuvant induced arthritis	A number of diseases, such as arthritis and cardiovascular disorders impacting the lives of many people have strong inflammatory components. To elucidate the antiinflammatory mechanism of a Novartis compound, time-course gene expression data were collected from joint tissue in an adjuvant-induced arthritis rat model, with and without compound treatment. The gene expression data were then analyzed using a dynamic Bayesian network (DBN) inference algorithm. Due to the nature of the biological experiments, a prescreening method to select a gene list and a data pair-up method to prepare the data were developed prior to applying DBN. From the inference analysis, a well-known cell adhesion molecule implicated in several inflammatory diseases was suggested as a potential direct target of the compound. In addition, a biologically plausible downstream pathway, modulated by the compound, was revealed. Experimental validation studies to confirm the compound direct target are in progress.	algorithm;computation;downstream (software development);dynamic bayesian network;experiment;gene expression programming;gene regulatory network;modulation	Jing Yu;Gabriel Helmlinger;Muriel Saulnier;Anna Georgieva	2006			medicine;pathology;bioinformatics;immunology	Comp.	8.034115814226157	-60.91412495435683	3110
072471a21eaad4179df7416cfa2835236a6e9bb9	path following for a quadrotor using dynamic extension and transverse feedback linearization	vectors acceleration heuristic algorithms nonlinear dynamical systems dynamics manifolds level set;motion control;path planning;nonlinear dynamical systems;stability;feedback;linearisation techniques;desired motion dynamic extension transverse feedback linearization path following controller quadrotor vehicle nonclosed embedded curves desired points stabilization nonlinear dynamic model linear system;helicopters;stability feedback helicopters linearisation techniques motion control nonlinear dynamical systems path planning	This work presents a path following controller for a quadrotor vehicle. A smooth, dynamic, feedback controller is designed that allows the quadrotor to follow both closed and non-closed embedded curves while maintaining a desired speed, a desired acceleration or while stabilizing desired points along the curves. The nonlinear dynamic model of the quadrotor is transformed into a linear system via a coordinate and feedback linearization transformation. Once transformed, a path following controller is designed that guarantees invariance of the path while enforcing the desired motion along the path.	control theory;embedded system;linear system;mathematical model;nonlinear system;simulation;transverse wave	Adeel Akhtar;Steven Lake Waslander;Christopher Nielsen	2012	2012 IEEE 51st IEEE Conference on Decision and Control (CDC)	10.1109/CDC.2012.6425945	control engineering;motion control;mathematical optimization;stability;engineering;control theory;feedback;mathematics;motion planning;feedback linearization	Robotics	64.82685105470497	-17.548499344233914	3114
a9f1e06b55fd9fb64f229ff1cd110cf7a6c60a14	study of chemical characteristics investigation and depth direction sterilization effect of a soil by ozone treatment	microorganisms soil agriculture electron tubes chemicals nitrogen;chemicals;soil agriculture crops fertilisers food safety microorganisms ozonation materials processing;chemical characteristic of a soil ozone treatment agriculture sterilization sanitization;sterilization;bacteria chemical characteristic depth direction sterilization effect soil ozone treatment crops pesticide chemical fertilizer food safety agriculture organic farming;chemical characteristic of a soil;sanitization;agriculture;electron tubes;soil;microorganisms;nitrogen;ozone treatment	As for securing a yield and quality of crops in limited farmland environment like Japan, a very high technique has been required. The problem to remark to spreading in large quantities of a pesticide and mass use of chemical fertilizer brought it at the same time. This may threaten food safety. In recent years agriculture being aware of reducing a pesticide and organic farming has been studied. However, it is a clear fact that insects exist, and countermeasures should be necessary. We have been studied about ozone treatment of a soil. We confirmed that ozone treatment in a field of agriculture was an effective soil application method. In this study, it was investigated how a soil chemically changed by ozone treatment and in addition, it was confirmed that a sterilization effect was provided by penetration of ozone to the depth direction of a soil. In ozone treatment of topsoil, it was able to sterilized 99% of bacteria of 5cm in depth of a surface.	countermeasure (computer)	Shin-Ichi Aoqui;Noriko Horibe;Fumiaki Mitsugi;Kazuhiro Nagahama	2014	2014 IIAI 3rd International Conference on Advanced Applied Informatics	10.1109/IIAI-AAI.2014.55	environmental science;environmental engineering;no-till farming;agronomy;waste management	EDA	86.2514743010059	-51.730355075310186	3115
a13b2424e1ea95f5a385da03dd80886d32e89d07	automated method to compute evans index for diagnosis of idiopathic normal pressure hydrocephalus on brain ct images	brain;skull;medical diagnostics;dementia	The early diagnosis of idiopathic normal pressure hydrocephalus (iNPH) considered as a treatable dementia is important. The iNPH causes enlargement of lateral ventricles (LVs). The degree of the enlargement of the LVs on CT or MR images is evaluated by using a diagnostic imaging criterion, Evans index. Evans index is defined as the ratio of the maximal width of frontal horns (FH) of the LVs to the maximal width of the inner skull (IS). Evans index is the most commonly used parameter for the evaluation of ventricular enlargement. However, manual measurement of Evans index is a time-consuming process. In this study, we present an automated method to compute Evans index on brain CT images. The algorithm of the method consisted of five major steps: standardization of CT data to an atlas, extraction of FH and IS regions, the search for the outmost points of bilateral FH regions, determination of the maximal widths of both the FH and the IS, and calculation of Evans index. The standardization to the atlas was performed by using linear affine transformation and non-linear wrapping techniques. The FH regions were segmented by using a three dimensional region growing technique. This scheme was applied to CT scans from 44 subjects, including 13 iNPH patients. The average difference in Evans index between the proposed method and manual measurement was 0.01 (1.6%), and the correlation coefficient of these data for the Evans index was 0.98. Therefore, this computerized method may have the potential to accurately compute Evans index for the diagnosis of iNPH on CT images.	ct scan	Noriyuki Takahashi;Toshibumi Kinoshita;Tomomi Ohmura;Eri Matsuyama;Hideto Toyoshima	2017		10.1117/12.2251322	medical diagnosis	Vision	37.313740551551376	-79.2925385748281	3119
011cf878c790bca36fb1cfaee6018576c0071556	autonomous excavation using a rope shovel		Abstract#R##N##R##N#This paper describes the development of a system to automate the digging cycle of an electric rope shovel, a machine which is widely used in open-pit mining. Achieving optimal digging performance requires path planning strategies to ensure dipper (bucket) filling, as well as methods to detect when to disengage the dipper from the bank. To this end, techniques to detect and avert dipper stall, and the online estimation of dipper “full-ness,” are described along with in-field experimental results using a one-seventh scale-model electric rope shovel. Over 100 autonomous excavation cycles, in a variety of digging conditions, resulted in cycle times consistent with a human operator, but with lower overall motor loading. © 2006 Wiley Periodicals, Inc.		Matthew Dunbabin;Peter I. Corke	2006	J. Field Robotics	10.1002/rob.20132	simulation;engineering;forensic engineering	Robotics	54.07069620569136	-17.54928668542317	3122
4861e225a86ba09db27b0cd6ef8a5515016978d0	thermal radiation effect on an unsteady magnetohydrodynamic flow past inclined porous heated plate in the presence of chemical reaction and viscous dissipation	viscous dissipation;porous medium;finite difference method;heat transfer;thermal radiation;chemical reaction;mhd flow;mass transfer	An analysis is made for the unsteady magnetohydrodynamic (MHD) flow of a viscous, incompressible, electrically conducting fluid in a porous medium. Considering the viscous dissipative term in energy equation which is important in free convective flow. The coupled non-linear partial differential equations are solved by using an implicit finite difference method of Crank Nicolson type. The effects of chemical reaction, viscous dissipation and radiation on velocity, temperature and concentrations are discussed. More importantly, the results of numerical solution of the present study agree well with the analytical solution of the earlier study in a particular case (i.e. without viscous dissipation). It is interesting to note that the Hartmann number reduces the velocity at all points of the flow domain as expected. An increase in viscous dissipation contributes slightly but uniformly to the rise of temperature as well as velocity distribution. 2013 Elsevier Inc. All rights reserved.	crank (person);crank–nicolson method;finite difference method;nonlinear system;numerical partial differential equations;pipelines;thermal grease;velocity (software development)	Rabi Narayan Barik;Gouranga Charan Dash	2014	Applied Mathematics and Computation	10.1016/j.amc.2013.09.077	classical mechanics;chemical reaction;finite difference method;thermal radiation;thermodynamics;heat transfer;porous medium;physics;mass transfer		89.96489505305283	3.999664938611407	3129
b1ed63260323c7df021645198e0e47ea5311436a	in-situ multi-view multi-scattering stochastic tomography	photonics scattering aerosols cameras sun estimation atmospheric modeling	To recover the three dimensional (3D) volumetric matter distribution in an object, the object is imaged from multiple directions and locations. Using these images, tomographic computations seek the distribution. When scattering is significant and under constrained irradiance, tomography must explicitly account for off-axis scattering. Furthermore, tomographic recovery must function when imaging is done in-situ, as occurs in medical imaging and ground-based atmospheric sensing. We formulate tomography that handles arbitrary orders of scattering, using a Monte-Carlo model. The model is highly parallelizable in our formulation. This can enable large scale rendering and recovery of volumetric scenes having a large number of variables. We solve stability and conditioning problems that stem from radiative transfer modeling in-situ.	apache axis;computation;medical imaging;merge sort;monte carlo method;tomography	Vadim Holodovsky;Yoav Y. Schechner;Anat Levin;Aviad Levis;Amit Aides	2016	2016 IEEE International Conference on Computational Photography (ICCP)	10.1109/ICCPHOT.2016.7492869	computer vision;optics;physics;remote sensing	Vision	60.46225291319765	-51.755029162606505	3134
22333aeccdf004b7b2b8fb9e5ec0c654777592dc	model order selection for multiple cooperative swarms clustering using stability analysis	swarm intelligence;model order selection;data clustering;particle swarm optimization;cooperative swarms	Extracting different clusters of the given data is an appealing topic in swarm intelligence applications. This paper introduces multiple cooperative swarms and single swarm clustering approaches and provides mathematical descriptions explaining why the former approach outperform the other one. Moreover, the stability analysis is proposed to obtain the model order of the data using multiple cooperative swarms clustering approach. The proposed clustering approach is evaluated using three data sets and its performance is compared with that of other clustering techniques.	algorithm;british informatics olympiad;cluster analysis;scalability;swarm intelligence	Abbas Ahmadi;Fakhri Karray;Mohamed S. Kamel	2008	2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational Intelligence)	10.1016/j.ins.2010.10.010	correlation clustering;mathematical optimization;multi-swarm optimization;fuzzy clustering;swarm intelligence;computer science;artificial intelligence;machine learning;cure data clustering algorithm;data mining;cluster analysis;particle swarm optimization	Vision	3.448913212605013	-42.078446580623734	3138
512ce44b37625e6dfb939c6bd7c83d829b71a591	data mining for quality control: burr detection in the drilling process	burr detection;data mining;machine learning;drilling process	Drilling process is one of the most important operations in aeronautic industry. It is performed on the wings of the aeroplanes and its main problem lies with the burr generation. At present moment, there is a visual inspection and manual burr elimination task subsequent to the drilling and previous to the riveting to ensure the quality of the product. These operations increase the cost and the resources required during the process. The article shows the use of data mining techniques to obtain a reliable model to detect the generation of burr during high speed drilling in dry conditions on aluminium Al 7075-T6. It makes possible to eliminate the unproductive operations in order to optimize the process and reduce economic cost. Furthermore, this model should be able to be implemented later in a monitoring system to detect automatically and on-line when the generated burr is out of tolerance limits or not. The article explains the whole process of data analysis from the data preparation to the evaluation and selection	data mining;online and offline;visual inspection	Susana Ferreiro;Basilio Sierra;Itziar Irigoien;Eneko Gorritxategi	2011	Computers & Industrial Engineering	10.1016/j.cie.2011.01.018	drilling;computer science;engineering;operations management;machine learning;forensic engineering;engineering drawing	SE	53.904734795655635	-17.573787464805672	3142
4b6a1ff3794de860d3502894e7d174de1a2e914f	wavelet-based denoising and independent component analysis for improving multi-group inference in fmri data	sensitivity and specificity;control group;measurement;fmri;signal separation;gaussian processes;shape noise reduction measurement smoothing methods wavelet transforms wavelet domain noise;active region;inference mechanisms;shape accuracy;independent component analysis;wavelet transforms;smoothing methods;shape;gaussian smoothing;medical image processing;noise reduction;shape accuracy wavelet based denoising independent component analysis multigroup inference fmri gaussian smoothing signal separation;shape metrics;image denoising;denoising;wavelet domain;shape metrics fmri wavelets denoising 3 d;wavelets;3 d;wavelet denoising;wavelet transforms biomedical mri gaussian processes image denoising independent component analysis inference mechanisms medical image processing smoothing methods;noise;biomedical mri;wavelet based denoising;multigroup inference	Denoising is amongst the most challenging steps involved in analyzing fMRI data. The conventionally used Gaussian smoothing improves the SNR at the cost of spatial sensitivity and specificity. We briefly describe a 3-D framework for wavelet based fMRI analysis that includes denoising and signal separation followed by a detailed illustration of the benefits and improvements when applied to multi-group (healthy/patient) fMRI studies. We utilize a novel shape metric to highlight the accuracy of the shape of activation regions obtained through different processing frameworks. The proposed algorithm results in higher specificity and increased shape accuracy which in turn is likely to be more sensitive to important differences in the patient and control group.	algorithm;gaussian blur;independent component analysis;map;noise reduction;sensitivity and specificity;signal-to-noise ratio;smoothing;wavelet	Siddharth Khullar;Andrew Michael;Nicolle M. Correa;Tülay Adali;Stefi A. Baum;Vince D. Calhoun	2011	2011 IEEE International Symposium on Biomedical Imaging: From Nano to Macro	10.1109/ISBI.2011.5872444	computer vision;computer science;pattern recognition;noise reduction;mathematics;statistics	Visualization	44.2388826355349	-79.05210156670175	3147
31fd69325b1129daf7cdc0318e5d3f654dec55e6	technical developments for mr-guided microwave thermocoagulation therapy of liver tumors	real time;mr imaging;clinical study;liver tumor	We have started clinical studies of MR-guided thermocoagulation therapy of liver tumors. Through this therapy for two years, we have developed new instruments, such as a filter to reduce noise in MRI from the microwave coagulator, a new MR-compatible electrode for easy detection of the tip position and an MR-compatible endoscopic system for trans-diaphragmatic approach to liver tumors just below the diaphragm. Concerning software, a program was modified for the real-time display of MR temperature map with a scale bar. A navigation software, 3D Slicer was customized to add real-time image navigation capability. The re-sliced images in the two perpendicular planes complemented the limitations of real-time MR image, which is taken in 2-3 seconds. These technical developments play important roles for more accurate, safer and easier treatment.	microwave	Shigehiro Morikawa;Toshiro Inubushi;Yoshimasa Kurumi;Shigeyuki Naka;Koichiro Sato;Tohru Tani;Nobuhiko Hata;Viswanathan Seshan;Hasnine A. Haque	2002		10.1007/3-540-45787-9_7	radiology;pathology;computer science	NLP	39.33241839236497	-86.46279965016235	3152
d1477e7a555f6ecfe3cec4dc19faf5d92b2b758b	the relation between the roc curve and the cmc	false reject rate;biometrics access control;system performance;receiver operating characteristic curve;frr receiver operating characteristic curve roc curve cumulative match curve cmc identification system performance false accept rate far false reject rate;sensitivity analysis;pattern matching;roc curve;cumulant;pattern matching biometrics access control sensitivity analysis;false accept rate;biometrics databases probes sorting search engines bismuth computer vision system performance computer displays testing	The cumulative match curve (CMC) is used as a measure of 1: m identification system performance. It judges the ranking capabilities of an identification system. The receiver operating characteristic curve (ROC curve) of a verification system, on the other hand, expresses the quality of a 1:1 matcher. The ROC plots the false accept rate (FAR) of a 1:1 matcher versus the false reject rate (FRR) of the matcher. We show that the CMC is also related to the FAR and FRR of a 1:1 matcher, i.e., the matcher that is used to rank the candidates by sorting the scores. This has as a consequence that when a 1:1 matcher is used for identification, that is, for sorting match scores from high to low, the CMC does not offer any additional information beyond the FAR and FRR curves. The CMC is just another way of displaying the data and can be computed from the FAR and FRR.	1:1 pixel mapping;biometrics;discrepancy function;identification scheme;performance evaluation;receiver operating characteristic;sorting;systems design	Ruud M. Bolle;Jonathan H. Connell;Sharath Pankanti;Nalini K. Ratha;Andrew W. Senior	2005	Fourth IEEE Workshop on Automatic Identification Advanced Technologies (AutoID'05)	10.1109/AUTOID.2005.48	econometrics;pattern recognition;mathematics;statistics	ECom	40.85219133063684	-62.24555579455977	3156
9b86844c375695d2ccb1a04c79cf7aa91a50bc3a	noncode 2016: an informative and valuable data source of long non-coding rnas		NONCODE (http://www.bioinfo.org/noncode/) is an interactive database that aims to present the most complete collection and annotation of non-coding RNAs, especially long non-coding RNAs (lncRNAs). The recently reduced cost of RNA sequencing has produced an explosion of newly identified data. Revolutionary third-generation sequencing methods have also contributed to more accurate annotations. Accumulative experimental data also provides more comprehensive knowledge of lncRNA functions. In this update, NONCODE has added six new species, bringing the total to 16 species altogether. The lncRNAs in NONCODE have increased from 210 831 to 527,336. For human and mouse, the lncRNA numbers are 167,150 and 130,558, respectively. NONCODE 2016 has also introduced three important new features: (i) conservation annotation; (ii) the relationships between lncRNAs and diseases; and (iii) an interface to choose high-quality datasets through predicted scores, literature support and long-read sequencing method support. NONCODE is also accessible through http://www.noncode.org/.	annotation;information;reduced cost;non-t, non-b, calla positive childhood acute lymphoblastic leukemia	Yi Zhao;Hui Li;Shuangsang Fang;Yue Kang;Wei Wu;Yajing Hao;Ziyang Li;Dechao Bu;Ninghui Sun;Michael Q. Zhang;Runsheng Chen	2016		10.1093/nar/gkv1252	biology;bioinformatics	Comp.	-0.04045354582254258	-58.08823408851145	3165
471b517260af5a25a80a8a22b008aa4eb0379855	wheeze detection using convolutional neural networks		In this paper, we propose to use convolutional neural networks for automatic wheeze detection in lung sounds. We present convolutional neural network based approach that has several advantages compared to the previous approaches described in the literature. Our method surpasses the standard machine learning models on this task. It is robust to lung sound shifting and requires minimal feature preprocessing steps. Our approach achieves 99% accuracy and 0.96 AUC on our datasets.	convolutional neural network;neural networks	Kirill Kochetov;Evgeny Putin;Svyatoslav Azizov;Ilya Skorobogatov;Andrey Filchenkov	2017		10.1007/978-3-319-65340-2_14	machine learning;convolutional neural network;preprocessor;wheeze;deep learning;computer science;lung sound;pattern recognition;artificial intelligence	NLP	30.86650733349155	-75.02665932195448	3169
98e6844c8e29df73d55c99480ba5eff25ce74577	a noise estimation algorithm with rapid adaptation for highly nonstationary environments	probability speech enhancement acoustic noise random noise parameter estimation spectral analysis smoothing methods;noise estimation;frequency dependence;probability;working environment noise noise level speech enhancement signal to noise ratio floors speech analysis frequency estimation frequency dependence smoothing methods testing;look ahead;speech enhancement;estimation algorithm;power spectrum;random noise;smoothing methods;acoustic noise;speech enhancement noise estimation algorithm nonstationary environments noisy speech power spectrum time dependent smoothing factor frequency dependent smoothing factor signal presence probability local minimum estimation algorithm;parameter estimation;spectral analysis	A noise estimation algorithm is proposed for highly nonstationary noise environments. The noise estimate is updated by averaging the noisy speech power spectrum using a time and frequency dependent smoothing factor, which is adjusted based on signal presence probability in subbands. Signal presence is determined by computing the ratio of the noisy speech power spectrum to its local minimum, which is computed by averaging past values of the noisy speech power spectra with a look-ahead factor. The local minimum estimation algorithm adapts very quickly to highly non-stationary noise environments. This was confirmed with formal listening tests that indicated that our noise estimation algorithm when integrated in speech enhancement was preferred over other noise estimation algorithms.	algorithm;maxima and minima;noise shaping;smoothing;spectral density;speech enhancement;stationary process	Sundarrajan Rangachari;Philipos C. Loizou;Yi Hu	2004	2004 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2004.1325983	gradient noise;gaussian noise;noise spectral density;speech recognition;colors of noise;value noise;noise temperature;computer science;noise measurement;noise;noise;probability;mathematics;noise figure;estimation theory;noise floor;spectral density;phase noise;noise;statistics;salt-and-pepper noise	EDA	82.78773275280378	-33.745844280246	3170
d5f663e8fcd7b4ee2eec37e9a89114d382a596ba	image reconstruction by a ground-based sar system	image reconstruction signal processing algorithms radar scattering radar polarimetry data acquisition optical reflection diffraction stacking aerospace simulation data mining;diffraction;optical reflection;aerospace simulation;data mining;radar scattering;stacking;radar polarimetry;image reconstruction;signal processing;system development;signal processing algorithms;data acquisition	In order to evaluate the SAR signal processing algorithm for stereographic SAR, indoor experiment using Ground-based SAR system was conducted. We used two SAR processing algorithms, namely diffraction stacking (back projection) and F-K (ω-k) migration. The both algorithms can reconstruct 3-D images of targets, but showed differences. F-K migration is much faster and reconstructs better images, but needs larger memory space. Keywords--stereographic SAR, diffraction stacking, back projection, F-K maigrartion, ω-k migration Introduction	algorithm;dspace;iterative reconstruction;signal processing;stacking;stereoscopy	Ralf Herrmann;Motoyuki Sato	2005	Proceedings. 2005 IEEE International Geoscience and Remote Sensing Symposium, 2005. IGARSS '05.	10.1109/IGARSS.2005.1525817	iterative reconstruction;computer vision;continuous-wave radar;synthetic aperture radar;stacking;bistatic radar;signal processing;pulse-doppler radar;optics;diffraction;data acquisition;radar imaging;physics;remote sensing	Embedded	76.10214907195717	-66.33853633603997	3171
b29f6a0e183966d4a3adf91678a7210a1bd266c6	exploiting smoothness in statistical learning, sequential prediction, and stochastic optimization	sequential prediction;computer engineering;computer science exploiting smoothness in statistical learning;mehrdad;and stochastic optimization michigan state university rong jin mahdavi;educational technology	In the last several years, the intimate connection between convex optimization and learning problems, in both statistical and sequential frameworks, has shifted the focus of algorithmic machine learning to examine this interplay. In particular, on one hand, this intertwinement brings forward new challenges in reassessment of the performance of learning algorithms including generalization and regret bounds under the assumptions imposed by convexity such as analytical properties of loss functions (e.g., Lipschitzness, strong convexity, and smoothness). On the other hand, emergence of datasets of an unprecedented size, demands the development of novel and more efficient optimization algorithms to tackle large-scale learning problems. rnThe overarching goal of this thesis is to reassess the smoothness of loss functions in statistical learning, sequential prediction/online learning, and stochastic optimization and explicate its consequences. In particular we examine how smoothness of loss function could be beneficial or detrimental in these settings in terms of sample complexity, statistical consistency, regret analysis, and convergence rate, and investigate how smoothness can be leveraged to devise more efficient learning algorithms.	machine learning;program optimization;stochastic optimization	Mehrdad Mahdavi	2014	CoRR		econometrics;mathematical optimization;educational technology;algorithmic learning theory;empirical risk minimization;computer science;online machine learning;machine learning;mathematics;statistics;generalization error	ML	23.247866865438976	-33.12474734533098	3173
f2b102b70cb4caf902cc2b31e0caf2ea5c8bb9c1	motion-based object detection using 3d wave digital filters	moving object;filtering;image motion analysis;3d wave digital filter;linear multidimensional filter;integrated circuit;transfer functions;wave digital filters image motion analysis integrated circuit design multidimensional digital filters multidimensional signal processing object detection;velocity filter;information filtering;digital filters filtering theory transfer functions filtering fourier transforms information filters information filtering;real time processing;wave digital filters;three dimensional;hardware architecture;integrated circuit design;design and implementation;digital filters;fourier transforms;multidimensional signal processing;visual scene;motion based object detection;system development;three dimensional signal processing;wave digital filter;real time processing motion based object detection 3d wave digital filter three dimensional signal processing visual scene linear multidimensional filter integrated circuit design hardware architecture velocity filter;information filters;multidimensional digital filters;filtering theory;object detection	This paper is focused on the separation of three-dimensional signals due to different velocity or directional components, more precisely detecting moving objects in visual scenes by means of linear multidimensional filters. While soundness of results obtained by similar systems developed in the past has been convincing, interest in those systems declined to the reduced computational power of contemporary computers. Recent advances in computation speed as well as design and implementation of integrated circuits and hardware architectures allow hardware realization of the above mentioned velocity filters for real-time processing. We present wave digital filter (WDF) structures adapted to the detection of objects moving in a certain direction at a given speed. These filters have been successfully tested on sequences showing crossing pedestrians recorded by a vehicle-mounted camera.	computation;computer;digital filter;integrated circuit;object detection;real-time clock;sensor;velocity (software development);wigner distribution function	Sam Schauland;Jörg Velten;Anton Kummert	2008	2008 8th IEEE International Conference on Computer and Information Technology	10.1109/CIT.2008.4594786	multidimensional signal processing;filter;fourier transform;three-dimensional space;computer vision;digital filter;computer science;integrated circuit;hardware architecture;transfer function;integrated circuit design;computer graphics (images)	Robotics	57.728286062729566	-41.66546898760367	3174
1f43804bfdb0fa5cb84507b4a8c6c0840e87f018	multilogistic regression by means of evolutionary product-unit neural networks	libre mercado;modelizacion;evaluation performance;test statistique;performance evaluation;funcion no lineal;learning;maximum likelihood;competitividad;evaluacion prestacion;optimal method;test estadistico;performance;maximum vraisemblance;statistical test;non linear function;regression model;optimization method;evolutionary neural network;rest;logistic regression;proceso adquisicion;metodo optimizacion;acquisition process;repos;marche concurrentiel;aprendizaje;modelisation;hypermedia;accuracy;software architecture;regresion logistica;modelo regresion;apprentissage;precision;multi class classification;multilogistic regression;modele regression;regression logistique;signal classification;fonction non lineaire;methode optimisation;competitiveness;classification signal;algorithme evolutionniste;descanso;algoritmo evolucionista;open market;evolutionary algorithm;rendimiento;classification automatique;reseau neuronal;automatic classification;competitivite;modeling;clasificacion automatica;red neuronal;hipermedia;processus acquisition;maxima verosimilitud;architecture logiciel;evolutionary neural networks;neural network	We propose a multilogistic regression model based on the combination of linear and product-unit models, where the product-unit nonlinear functions are constructed with the product of the inputs raised to arbitrary powers. The estimation of the coefficients of the model is carried out in two phases. First, the number of product-unit basis functions and the exponents' vector are determined by means of an evolutionary neural network algorithm. Afterwards, a standard maximum likelihood optimization method determines the rest of the coefficients in the new space given by the initial variables and the product-unit basis functions previously estimated. We compare the performance of our approach with the logistic regression built on the initial variables and several learning classification techniques. The statistical test carried out on twelve benchmark datasets shows that the proposed model is competitive in terms of the accuracy of the classifier.	adaboost;algorithm;architecture as topic;artificial neural network;basis function;benchmark (computing);biological neural networks;coefficient;experiment;interaction;logistic model tree;logistic regression;machine learning;mathematical optimization;memory-level parallelism;network model;neural network simulation;nonlinear system;numerous;occam's razor;olami–feder–christensen model;perceptron;power (psychology);programming paradigm;silo (dataset);statistical classification	César Hervás-Martínez;Francisco J. Martínez-Estudillo;Mariano Carbonero-Ruz	2008	Neural networks : the official journal of the International Neural Network Society	10.1016/j.neunet.2007.12.052	computer science;artificial intelligence;machine learning;evolutionary algorithm;mathematics;accuracy and precision;artificial neural network;algorithm;statistics	ML	10.407945401256391	-31.186819261934936	3175
87cafeb32119f81621c8098b0b5ada9cb6c15022	what makes a problem hard for xcs	classifier system;learning classifier system	Despite two decades of work learning classi er systems researchers have had relatively little to say on the subject of what makes a problem diÆcult for a classi er system. Wilson's accuracy-based XCS, a promising and increasingly popular classi er system, is, we feel, the natural rst choice of classi er system with which to address this issue. To make the task more tractable we limit our considerations to a restricted, but very important, class of problems. Most signi cantly, we consider only single step reinforcement learning problems and the use of the standard binary/ternary classi er systems language. In addition to distinguishing several dimensions of problem complexity for XCS, we consider their interactions, identify bounding cases of diÆculty, and consider complexity metrics for XCS. Based on these results we suggest a simple template for ternary single step test suites to more comprehensively evaluate classi er systems.		Tim Kovacs;Manfred Kerber	2000		10.1007/3-540-44640-0_7	margin classifier;margin;artificial intelligence;machine learning;mathematics;learning classifier system;algorithm	Theory	18.799446693596796	-32.948432708421805	3178
214e9b8200771f9647938f1e4d8a28defc1b9749	a new method of generating robot-used maps	map generating;path planning;service robots;discrete points coordinates;servomechanisms industrial robots path planning;data mining;straight lines;general methods;global positioning system;industrial robots;servomechanisms;servo control;robot used maps generation;circle arcs;robot;robot kinematics service robots servosystems path planning robot control intelligent robots least squares methods computational intelligence information security error correction;robot servo control map generating;robot servo control robot used maps generation discrete points coordinates straight lines circle arcs;robot kinematics;robot servo control;servosystems	In this paper, a new map-generating method was introduced, this method was based on the information of the coordinates of discrete points, to generate straight lines and circle arcs, which could simplify the robot servo control, the experiment proved that the method has high precision and small errors.	robot;servo	Lan Wang;Qian Zhang	2009	2009 International Conference on Computational Intelligence and Security	10.1109/CIS.2009.217	robot;computer vision;servo control;global positioning system;computer science;artificial intelligence;control theory;motion planning;robot kinematics	Robotics	62.08099793296241	-21.83903193882049	3181
570b2ee085a86ca644b1d471139def8d0ee15e93	aesthetic visual quality assessment of paintings	painting;art;image processing;prior knowledge;testing;visual quality assessment;machine learning problem;classification;visual quality;human subjects;questionnaire survey;aesthetics;visualization;paintings;machine learning;aesthetic visual quality assessment;painting images;quality assessment painting humans art image coding digital images feature extraction machine learning bridges computer vision;feature extraction;visual quality assessment aesthetics classification feature extraction;visual features;computational visual features aesthetic visual quality assessment paintings digital images machine learning problem painting images;computational visual features;visual perception;humans;digital image;learning artificial intelligence;visual perception art image processing learning artificial intelligence painting;digital images	This paper aims to evaluate the aesthetic visual quality of a special type of visual media: digital images of paintings. Assessing the aesthetic visual quality of paintings can be considered a highly subjective task. However, to some extent, certain paintings are believed, by consensus, to have higher aesthetic quality than others. In this paper, we treat this challenge as a machine learning problem, in order to evaluate the aesthetic quality of paintings based on their visual content. We design a group of methods to extract features to represent both the global characteristics and local characteristics of a painting. Inspiration for these features comes from our prior knowledge in art and a questionnaire survey we conducted to study factors that affect human's judgments. We collect painting images and ask human subjects to score them. These paintings are then used for both training and testing in our experiments. Experimental results show that the proposed work can classify high-quality and low-quality paintings with performance comparable to humans. This work provides a machine learning scheme for the research of exploring the relationship between aesthetic perceptions of human and the computational visual features extracted from paintings.	adaboost;color vision;computation;digital image;experiment;goto;high- and low-level;humans;image retrieval;jpeg;judgment (mathematical logic);machine learning;mind;naive bayes classifier;pixel;procedural generation;recommender system;use case survey	Congcong Li;Tsuhan Chen	2009	IEEE Journal of Selected Topics in Signal Processing	10.1109/JSTSP.2009.2015077	visual rhetoric;computer vision;image processing;painting;computer science;machine learning;multimedia;digital image	Web+IR	23.14980292832369	-58.119954328002144	3183
ccc2df08a332c4d8a98c3a0781aec6ab1e883937	robotic understanding of scene contents and spatial constraints		The aim of this paper is to create a model which is able to be used to accurately identify objects as well as spacial relationships in a dynamic environment. This paper proposed methods to train a deep learning model which recognizes unique objects and positions of key items in an environment. The model requires a low amount of images compared to others and also can recognize multiple objects in the same frame due to the utilization of region proposal networks. Methods are also discussed to find the position of recognized objects which can be used for picking up recognized items with a robotic arm. The system utilizes logic operations to be able to deduct how different objects relate to each other in regard to their placement from one another based off of the localization technique. The paper discusses how to create spacial relationships specifically.		Dustin Wilson;Fujian Yan;Koustuv Sinha;Hongsheng He	2018		10.1007/978-3-030-05204-1_10	boolean algebra;convolutional neural network;deep learning;object detection;computer vision;robotic arm;computer science;artificial intelligence	Vision	46.9653965399145	-39.150140153136185	3190
00088b938c4c5c91c8ede5e43e9ae6209a8268f7	l∞ error and bandwidth selection for kernel density estimates of large data	bandwidth selection;coresets;kernel density estimates	Kernel density estimates are a robust way to reconstruct a continuous distribution from a discrete point set. Typically their effectiveness is measured either in <i>L</i><sub>1</sub> or <i>L</i><sub>2</sub> error. In this paper we investigate the challenges in using <i>L</i><sub>∞</sub> (or worst case) error, a stronger measure than <i>L</i><sub>1</sub> or <i>L</i><sub>2</sub>. We present efficient solutions to two linked challenges: how to evaluate the <i>L</i><sub>∞</sub> error between two kernel density estimates and how to choose the bandwidth parameter for a kernel density estimate built on a subsample of a large data set.	best, worst and average case;kernel density estimation	Yan Zheng;Jeff M. Phillips	2015		10.1145/2783258.2783357	kernel;kernel density estimation;econometrics;mathematical optimization;kernel embedding of distributions;mathematics;variable kernel density estimation;statistics	ML	30.389556483412132	-29.323922679832187	3191
c0a90a33e1082b0e6d4c133bb4d5f9dcd3fcbbf1	a new scheme for daily peak wind gust prediction using machine learning	support vector regression;wind energy;kernel principal component analysis	A major challenge in meteorology is the forecasting of winds owing to their highly chaotic nature. However, wind forecasts, and in particular daily peak wind gust forecasts, provide the public with a general sense of the risks associated with wind on a given day and are useful in decision making. Additionally, such knowledge is critical for wind energy production. Currently, no operational daily peak wind gust product exists. As such, this project will seek to develop a peak wind gust prediction scheme based on output from an operational numerical weather prediction model. Output from the North American Mesoscale (NAM) model will be used in a support vector regression (SVR) algorithm trained to predict daily peak wind gusts for ten cities commonly impacted by hazardous wind gusts (cities in the Midwest and central Plains) and with interests in wind energy. Output from a kernel principal component analysis of the fully three-dimensional atmosphere as characterized by the NAM forecasts will be used to predict peak wind gusts for each location at 36 hours lead time. Ultimately, this initial product will lead to the development of a more robust prediction scheme that could one day transition into an operational forecast model. © 2014 The Authors. Published by Elsevier B.V. Selection and peer-review under responsibility of scientific committee of Missouri University of Science and Technology.	algorithm;chaos theory;kernel principal component analysis;machine learning;nam;north american mesoscale model;numerical analysis;numerical weather prediction;support vector machine	Andrew Mercer;Jamie Dyer	2014		10.1016/j.procs.2014.09.059	simulation	ML	9.550448107715175	-18.552431180855795	3201
2f6f24b76091b1331149f079aa7ca562f9e6e011	genccs: a correlated group difference approach to contrast set mining	contrast set mining;contrast set;search space;attribute-value pair;large datasets;data mining task;real datasets;correlated group difference approach;contrast set mining technique;discerning difference;correlated group difference	Contrast set mining has developed as a data mining task which aims at discerning differences amongst groups. These groups can be patients, organizations, molecules, and even time-lines. distinguishes one from the other. A contrast set is a conjunction of attribute-value pairs that differ significantly in their distribution across groups. The search for contrast sets can be prohibitively expensive on relatively large datasets because every combination of attribute-values must be examined, causing a potential exponential growth of the search space. In this paper, we introduce the notion of a correlated group difference (CGD) and propose a contrast set mining technique that utilizes mutual information and all confidence to select the attribute-value pairs that are most highly correlated, in order to mine CGDs. Our experiments on real datasets demonstrate the efficiency of our approach and the interestingness of the CGDs discovered.	attribute–value pair;data mining;experiment;mutual information;rough set;time complexity	Mondelle Simeon;Robert J. Hilderman	2011		10.1007/978-3-642-23199-5_11	bioinformatics;artificial intelligence;data mining;mathematics	ML	4.544935589983215	-49.19251216015224	3207
d6d347cb0f74637a9bd29db30e3927303fdf10f6	observing the ocean surface topography at high-resolution by the swot (surface water and ocean topography) mission		This paper presents the oceanographic objectives and plans for calibration and validation, science investigations and applications for the Surface Water and Ocean Topography (SWOT) Mission. SWOT, an international mission planned for launch in 2021, will make high-resolution observations of the ocean surface topography as a next-generation altimetry mission utilizing the technique of radar interferometry.	image resolution;radar;topography	Lee-Lueng Fu;Rosemary Morrow	2018	IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2018.8518929	remote sensing;surface water;radar;altimeter;swot analysis;interferometry;computer science;ocean surface topography	Embedded	80.44313893476263	-62.850814626291175	3208
93f4c69558af2daa21e80f9cae6d59881e192d90	hyperchaos synchronization using pso-optimized rbf-based controllers to improve security of communication systems		This paper studies the Lorenz hyperchaos synchronization and its application to improve the security of communication systems. Two methods are proposed to synchronize the general forms of hyperchaotic systems, and their performance in secure communication application is verified. These methods use the radial basis function (RBF)-based neural controllers for this purpose. The first method uses a standard RBF neural controller. Particle swarm optimization (PSO) algorithm is used to derive and optimize the parameters of the RBF controller. In the second method, with the aim of increasing the robustness of the RBF controller, an error integral term is added to the equations of RBF neural network. For this method, the coefficients of the error integral component and the parameters of RBF neural network are also derived and optimized via PSO algorithm. For better comparison, the proposed methods and an optimal PID controller optimized by PSO are applied to the Lorenz hyperchaotic system for secure communication. Simulation results show the effectiveness and superiority of the proposed methods in both performance and robustness in comparison with the PID controller.	algorithm;artificial neural network;coefficient;lorenz cipher;mathematical optimization;memory controller;nonlinear programming;nonlinear system;optimal control;pid;particle swarm optimization;phase-shift oscillator;radial (radio);radial basis function;secure communication;simulation	Mansour Sheikhan;Reza Shahnazi;Sahar Garoucy	2011	Neural Computing and Applications	10.1007/s00521-011-0774-4	computer science;machine learning;control theory	EDA	73.00011039491558	0.25655495108803367	3210
d689f11553dfeb0ae3a3ab9075eb8a54daedc68f	object localization: selection of optimal reference objects	reference point;object localization;context dependent	The quality of an object localization depends essentially on the adequate selection of a suitable reference. In most computational approaches developed so far only the distance between the located object and a potential reference object has been used as a decision criterion. However many other criteria have to be considered for a cognitive plausible selection of adequate reference points. In this paper we investigate how object and context dependent properties, like referentiality, visual salience, functional dependencies, or prior knowledge, innuence the quality of a reference object. Each factor is quantitatively determined and scaled by relevance to a certain context. The scaling permits the necessary comparability of the diierent quality criteria. Finally, on the basis of these factors a computational model is presented which permits a context dependent determination of the optimal reference object in a particular spatial connguration.	computation;computational model;functional dependency;image scaling;relevance	Klaus-Peter Gapp	1995		10.1007/3-540-60392-1_34	context-dependent memory	Vision	-1.6285282975264512	-77.77198383712245	3212
1f3869a4d27f46f803574881cb34d12623026e06	research on 3d reconstruction procedure of marked points for large workpiece measurement	photogrammetry;optimisation;workpiece measurement;3d measurements;image processing;iterative optimization strategy;digital camera;close range photogrammetry;digital close range photogrammetry;engineering projects;digital cameras;iterative methods;nonlinear distortion;bundle adjustment method;three dimensional displays;image reconstruction;3d reconstruction procedure;marked point reconstruction;mathematical model;optimization;computerised instrumentation;camera calibration;large workpiece;2d image sequence;calibration;3d measurement;3d reconstruction;cameras;bundle adjustment;image sequences	Digital close-range photogrammetry has been used increasedly in various engineering projects. By using digital cameras along with image processing, camera calibration and bundle adjustment method, measurements with high precision can be achieved. Based on analyzing the characteristics of 3D measurement for large workpiece and its actual demand, a procedure of reconstructing marked points is proposed in this paper. In the procedure, the initial value of camera parameters and 3D coordinates of marked points is computed by optimizing alternately and spreading from the selected images to the other. The 3D coordinates of all marked points are obtained by using bundle adjustment method, in which camera parameters and 3D coordinates of all marked points are optimized at the same time. Camera calibration is the key to get high-precision 3D information from a sequence of 2D images in close-range photogrammetry. In this procedure, iterative optimization strategy is used for camera calibration, which can improve the accuracy of 3D coordinates gradually. The procedure is effective and flexible. The experimental results proved that this method can meet the needs of 3D measurements and other practical engineering applications.	3d reconstruction;bundle adjustment;camera resectioning;control point (mathematics);digital camera;experiment;image processing;iterative method;mathematical optimization;photogrammetry;precondition;requirement;system of measurement	Zhanli Li;Yuan Wang	2009	2009 Fifth International Conference on Information Assurance and Security	10.1109/IAS.2009.55	computer vision;camera auto-calibration;simulation;geography;bundle adjustment;engineering drawing;pinhole camera model	Vision	55.15756730228234	-50.44565065235082	3214
3b9e3b9e466e0605ac4649b1f67107c86476735f	recursive identification approach of multivariable nonlinear dynamic systems based on evolving fuzzy hammerstein models		In this paper a novel approach for evolving fuzzy identification of nonlinear Multi-Input Multi-Output (MIMO) dynamic systems, is proposed. The adopted methodology is based on state space multivariable Hammerstein models, in which, the static nonlinearity block is approximated by an evolving Takagi-Sugeno fuzzy inference system and the linear dynamic block is represented by a state space linear model obtained through a Recursive Eigensystem Realization Algorithm (RERA). Computational and experimental results from the online identification of a multivariable dynamic system with a complex combined nonlinearity and an Evaporator Process with threeinput and three-output, which are benchmarks widely cited in the literature, illustrate the efficiency and applicability of the proposed approach.	approximation algorithm;benchmark (computing);computation;dynamical system;dynamical systems theory;eigensystem realization algorithm;inference engine;linear model;mimo;nonlinear system;recursion (computer science);state space;universal approximation theorem	Bettina Haase;Ginalber Luiz de Oliveira Serra	2018	2018 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2018.8491606	linear model;fuzzy logic;dynamical system;multivariable calculus;state space;control theory;nonlinear system;eigensystem realization algorithm;computer science;markov process	Robotics	60.30600118266416	-5.682463374994477	3217
81aa7d343f36c0a88e00d273a2ba0e2934b3948b	a multi-view face recognition system	feature detection;face recognition;affine transformation;face modeling;template matching	In many automatic face recognition systems, posture constraining is a key factor preventing them from application. In this paper, a series of strategies-will be described to achieve a system which enables face recognition under varying pose. These approaches include the multi-view face modeling, the threshold image based face feature detection, the affine transformation based face posture normalization and the template matching based face identification. Combining all of these strategies, a face recognition system with the pose invariance is designed successfully. Using a 75MHZ Pentium PC and with a database of 75 individuals, 15 images for each person, and 225 test images with various postures, a very good recognition rate of 96.89% is obtained.	facial recognition system;feature detection (computer vision);feature detection (web development);free viewpoint television;pattern recognition;poor posture;template matching	Yongyue Zhang;Zhenyun Peng;Suya You;Guangyou Xu	1997	Journal of Computer Science and Technology	10.1007/BF02943172	facial recognition system;computer vision;face detection;speech recognition;object-class detection;template matching;computer science;pattern recognition;feature detection;three-dimensional face recognition;affine transformation;3d single-object recognition	Vision	32.699180459635606	-58.471961425953424	3219
bd84c430249e0759d13cee7d9051f08451f0c6be	hetero-labeled lda: a partially supervised topic model with heterogeneous labels		We propose Hetero-Labeled LDA (hLLDA), a novel semi-supervised topic model, which can learn from multiple types of labels such as document labels and feature labels (i.e., heterogeneous labels), and also accommodate labels for only a subset of classes (i.e., partial labels). This addresses two major limitations in existing semi-supervised learning methods: they can incorporate only one type of domain knowledge (e.g. document labels or feature labels), and they assume that provided labels cover all the classes in the problem space. This limits their applicability in real-life situations where domain knowledge for labeling comes in different forms from different groups of domain experts and some classes may not have labels. hLLDA resolves both the label heterogeneity and label partialness problems in a unified generative process. hLLDA can leverage different forms of supervision and discover semantically coherent topics by exploiting domain knowledge mutually reinforced by different types of labels. Experiments with three document collections–Reuters, 20 Newsgroup and Delicious– validate that our model generates a better set of topics and efficiently discover additional latent topics not covered by the labels resulting in better classification and clustering accuracy than existing supervised or semisupervised topic models. The empirical results demonstrate that learning from multiple forms of domain knowledge in a unified process creates an enhanced combined effect that is greater than a sum of multiple models learned separately with one type of supervision.	algorithm;cluster analysis;coherence (physics);problem domain;real life;semi-supervised learning;semiconductor industry;supervised learning;taxonomy (general);topic model;unified framework;unified process	Dongyeop Kang;Youngja Park;Suresh Chari	2014		10.1007/978-3-662-44848-9_41	computer science;machine learning;pattern recognition;data mining	ML	22.602559956660595	-44.01105312004633	3226
d266c89d6c22e91e1ba288a61b6c0957c4c84f8d	a heuristic for emergency operations scheduling with lead times and tardiness penalties	scheduling;mixed integer programming;heuristics;supply chain management	We study the operations scheduling problem encountered in the process of making and distributing emergency supplies. The lead times of a multi-echelon process, including shipping time, assembly time, and waiting time for raw materials must be explicitly modeled. The optimization problem is to find an inventory allocation and a production/assembly plan together with a shipping schedule for inbound supplies and outbound deliveries so that the total tardiness in customer order fulfillment is minimized. We define the problem as a mixed integer programming model, perform a structure analysis of the problem, and then propose a new search heuristic for the problem. This proposed heuristic finds a feasible solution to the problem by solving a series of linear programming relaxation problems, and is able to terminate quickly. Observations from an extensive empirical study are reported.	heuristic;scheduling (computing)	Lei Lei;Kangbok Lee;Hui Dong	2016	European Journal of Operational Research	10.1016/j.ejor.2015.10.005	mathematical optimization;real-time computing;supply chain management;integer programming;computer science;operations management;cutting stock problem;heuristics;mathematics;scheduling	Robotics	14.419650613994476	2.523900706228217	3227
6f4e23abb68796a7030af0ead5d6e7a4cfa09b4f	autonomous gpr surveys using the polar rover yeti		The National Science Foundation operates stations on the ice sheets of Antarctica and Greenland to investigate Earth's climate history, life in extreme environments, and the evolution of the cosmos. Understandably, logistics costs predominate budgets due to the remote locations and harsh environments involved. Currently, manual ground-penetrating radar (GPR) surveys must preceed vehicle travel across polar ice sheets to detect subsurface crevasses or other voids. This exposes the crew to the risks of undetected hazards. We have developed an autonomous rover, Yeti, specifically to conduct GPR surveys across polar ice sheets. It is a simple four-wheel-drive, battery-powered vehicle that executes autonomous surveys via GPS waypoint following. We describe here three recent Yeti deployments, two in Antarctica and one in Greenland. Our key objective was to demonstrate the operational value of a rover to locate subsurface hazards. Yeti operated reliably at −30 °C, and it has has good oversnow mobility and adequate GPS accuracy for waypoint-following and hazard georeferencing. It has acquired data on hundreds of crevasse encounters to improve our understanding of heavily crevassed traverse routes and to develop automated crevasse-detection algorithms. Importantly, it helped to locate a previously undetected buried building at the South Pole. Yeti can improve safety by decoupling survey personnel from the consequences of undetected hazards. It also enables higher-quality systematic surveys to improve hazard-detection probabilities, increase assessment confidence, and build datasets to understand the evolution of these regions. Yeti has demonstrated that autonomous vehicles have great potential to improve the safety and efficiency of polar logistics. © 2012 Wiley Periodicals, Inc.	kriging;rover (the prisoner)	James H. Lever;A. J. Delaney;Laura E. Ray;Eric Trautmann;L. A. Barna;A. M. Burzynski	2013	J. Field Robotics	10.1002/rob.21445	engineering;remote sensing	Robotics	54.18854793570583	-29.38170933741017	3233
f4071ee56341e333c50f44afcc5759a114b1a8d8	ptrider: a price-and-time-aware ridesharing system		Ridesharing is popular among travellers because it can reduce their travel costs, and it also holds the potential to reduce travel time, congestion, air pollution, and overall fuel consumption. Existing ridesharing systems (e.g., lyft, uberPOOL) often offer each traveler only one choice that aims to minimize system-wide vehicle travel distance or time. In this demonstration, we present a price-and-time-aware ridesharing system, termed as PTRider, which provides more options. It considers both pick-up time and price, so that travellers are able to choose the vehicle matching their preferences best. To answer the ridesharing request in real time, PTRider builds indexes on the road network and vehicles separately, and utilizes corresponding efficient matching methods. A real-life dataset that contains 432,327 trips extracted from 17,000 Shanghai taxis for one day (May 29, 2009) is used to demonstrate that PTRider can return various options for every ridesharing request in real time. PVLDB Reference Format: Lu Chen, Yunjun Gao, Zixian Liu, Xiaokui Xiao, Christian S. Jensen, and Yifan Zhu. PTRider: A Price-and-Time-Aware Ridesharing System. PVLDB, 11(12): 1938-1941, 2018. DOI: https://doi.org/10.14778/3229863.3236229	entity–relationship model;jensen's inequality;lu decomposition;li-chen wang;network congestion;real life;uptime	Lu Chen;Yunjun Gao;Zixian Liu;Xiaokui Xiao;Christian S. Jensen;Yifan Zhu	2018	PVLDB	10.14778/3229863.3236229	data mining;computer science	DB	11.261780541135087	-7.0512177956925175	3235
1bc734f600d47e6124780f00df86f4c6200055dc	using a big data database to identify pathogens in protein data space.		Current metagenomic analysis algorithms require significant computing resources, can report excessive false positives (type I errors), may miss organisms (type II errors / false negatives), or scale poorly on large datasets. This paper explores using big data database technologies to characterize very large metagenomic DNA sequences in protein space, with the ultimate goal of rapid pathogen identification in patient samples. Our approach uses the abilities of a big data databases to hold large sparse associative array representations of genetic data to extract statistical patterns about the data that can be used in a variety of ways to improve identification algorithms. Keywords-Bioinformatics; Big Data; Accumulo; Databases	algorithm;apache accumulo;big data;bioinformatics;database;metagenomics;sparse matrix	Ashley Mae Conard;Stephanie Dodson;Jeremy Kepner;Darrell O. Ricke	2015	CoRR		database;data mining;big data;false positive paradox;metagenomics;associative array;computer science	DB	2.117441558424481	-53.859840240020134	3241
d6c15eb31433c294cccdddd231cd6c613e1290bf	does your model weigh the same as a duck?	quantitative structure activity relationship;computer aided design;ligands;models molecular;drug design;protein binding;informatics;humans	Computer-aided drug design is a mature field by some measures, and it has produced notable successes that underpin the study of interactions between small molecules and living systems. However, unlike a truly mature field, fallacies of logic lie at the heart of the arguments in support of major lines of research on methodology and validation thereof. Two particularly pernicious ones are cum hoc ergo propter hoc (with this, therefore because of this) and confirmation bias (seeking evidence that is confirmatory of the hypothesis at hand). These fallacies will be discussed in the context of off-target predictive modeling, QSAR, molecular similarity computations, and docking. Examples will be shown that avoid these problems.	boat dock;chemical similarity;computation;correlation does not imply causation;docking (molecular);drug design;ergo proxy;hoc (programming language);interaction;living systems;predictive modelling;quantitative structure–activity relationship;small molecule;confirmation - responselevel	Ajay N. Jain;Ann E. Cleves	2012	Journal of computer-aided molecular design	10.1007/s10822-011-9530-1	biology;plasma protein binding;simulation;chemistry;computer science;bioinformatics;artificial intelligence;computer aided design;machine learning;ligand;quantitative structure–activity relationship;informatics;drug design	EDA	8.84827316257949	-59.65668228302428	3243
40ef831d19278e355b4ea11c1fea436970fc542c	retrieval of oceanic constituents from ocean color using simulated annealing	colored dissolved organic matter;absorption;oceans simulated annealing reflectivity absorption scanning probe microscopy optical sensors sea measurements optical scattering water sea surface;particulates;dissolving;oceans;ocean color;annealing;reflectivity;optical detectors;mean square;simulated annealing;chlorophylls;suspended particulate matter;colors;belgium;symposia;sea surface;reflectance;optical scattering;phytoplankton;pigments;optical sensors;scanning probe microscopy;optical sensor;organic materials;water;sea water;sea measurements;suspended sediments	The color of the sea is determined by the contents of the water, especially the concentrations of suspended particulate matter (SPM), phytoplankton pigments such as chlorophyll (CHL) and colored dissolved organic matter (CDOM). Reversely, optical sensors that measure the water-leaving reflectance spectra allow us to calculate the desired concentration products. In this paper, a method is introduced that is valid for both case 1 and 2 waters. To this end, model is fitted to reflectance spectra, using simulated annealing for optimizing the mean square of the reflectance over all spectra.	mean squared error;sensor;simulated annealing;super paper mario	Pieter Kempeneers;Sindy Sterckx;Walter Debruyn;Steve De Backer;Paul Scheunders;Youngje Park;Kevin Ruddick	2005	Proceedings. 2005 IEEE International Geoscience and Remote Sensing Symposium, 2005. IGARSS '05.	10.1109/IGARSS.2005.1526058	meteorology;particulates;oceanography;reflectivity;optics;mineralogy;physics;remote sensing	Embedded	85.12583792764725	-64.08042465620854	3248
ce9846ed510454c7e2bc6616d47b0c2fb4ba1dd3	extensions of kmeans-type algorithms: a new clustering framework by integrating intracluster compactness and intercluster separation	clustering algorithms linear programming partitioning algorithms algorithm design and analysis vectors games approximation methods;normal mutual information k means type algorithms clustering framework intracluster compactness intercluster separation k means type clustering data set partitioning cluster objects objective functions updating rules;pattern clustering data handling;kmeans clustering data mining feature weighting	Kmeans-type clustering aims at partitioning a data set into clusters such that the objects in a cluster are compact and the objects in different clusters are well separated. However, most kmeans-type clustering algorithms rely on only intracluster compactness while overlooking intercluster separation. In this paper, a series of new clustering algorithms by extending the existing kmeans-type algorithms is proposed by integrating both intracluster compactness and intercluster separation. First, a set of new objective functions for clustering is developed. Based on these objective functions, the corresponding updating rules for the algorithms are then derived analytically. The properties and performances of these algorithms are investigated on several synthetic and real-life data sets. Experimental studies demonstrate that our proposed algorithms outperform the state-of-the-art kmeans-type clustering algorithms with respect to four metrics: accuracy, RandIndex, Fscore, and normal mutual information.	algorithm;arabic numeral 0;categorical variable;cluster analysis;convergence (action);division by zero;experiment;k-means clustering;mutual information;nmi gene;non-maskable interrupt;palais–smale compactness condition;performance;physical object;rs-232;real life;rule (guideline);synthetic intelligence;statistical cluster	Xiaohui Huang;Yunming Ye;Haijun Zhang	2014	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2013.2293795	correlation clustering;constrained clustering;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;mathematics;cluster analysis;single-linkage clustering;brown clustering;dbscan;biclustering;affinity propagation;clustering high-dimensional data;conceptual clustering	ML	3.6002592454942146	-40.17474320122754	3254
4ee813258317124cf89d84d99a84d3033a41711a	as-rigid-as-possible mosaicking and serial section registration of large sstem datasets	animals;high resolution;imaging three dimensional;microscopy electron transmission;transmission electron microscopy;drosophila;scale invariant feature transform;drosophila melanogaster;least square;global optimization;databases factual;image mosaicing;open source software	MOTIVATION Tiled serial section Transmission Electron Microscopy (ssTEM) is increasingly used to describe high-resolution anatomy of large biological specimens. In particular in neurobiology, TEM is indispensable for analysis of synaptic connectivity in the brain. Registration of ssTEM image mosaics has to recover the 3D continuity and geometrical properties of the specimen in presence of various distortions that are applied to the tissue during sectioning, staining and imaging. These include staining artifacts, mechanical deformation, missing sections and the fact that structures may appear dissimilar in consecutive sections.   RESULTS We developed a fully automatic, non-rigid but as-rigid-as-possible registration method for large tiled serial section microscopy stacks. We use the Scale Invariant Feature Transform (SIFT) to identify corresponding landmarks within and across sections and globally optimize the pose of all tiles in terms of least square displacement of these landmark correspondences. We evaluate the precision of the approach using an artificially generated dataset designed to mimic the properties of TEM data. We demonstrate the performance of our method by registering an ssTEM dataset of the first instar larval brain of Drosophila melanogaster consisting of 6885 images.   AVAILABILITY This method is implemented as part of the open source software TrakEM2 (http://www.ini.uzh.ch/~acardona/trakem2.html) and distributed through the Fiji project (http://pacific.mpi-cbg.de).	anatomic structures;artifact (software development);biological specimen;displacement mapping;distortion;image resolution;image stitching;least-squares analysis;morphologic artifacts;muscle rigidity;neurobiology;open-source software;psychologic displacement;slpi protein, human;scanning electron microscopy;scott continuity;sectioning technique;silo (dataset);staining method;synaptic package manager;synaptic connectivity;transmission electron microscopy;registration - actclass	Stephan Saalfeld;Albert Cardona;Volker Hartenstein;Pavel Tomançak	2010		10.1093/bioinformatics/btq219	computer vision;transmission electron microscopy;simulation;image resolution;computer science;scale-invariant feature transform;least squares;global optimization;computer graphics (images)	Vision	47.71822694175294	-85.74612153809078	3255
418bb73a786e74978ccf7bb01a970fa4cbb2a7a6	determination of optimal general edge detectors by global minimization of a cost function	performance measure;numerical algorithmics;search engine;optimisation;algoritmo busqueda;optimizacion;cost function;edge detection;algorithme recherche;search algorithm;numerical optimization;prior knowledge;minimizacion funcion;simulated annealing;algoritmo genetico;global minimization;funcion coste;deteccion contorno;detection contour;recuit simule;algorithmique numerique;function minimization;point spread function;algorithme genetique;fonction cout;genetic algorithm;recocido simulado;optimization;algoritmico numerico;minimisation fonction	We present a numerical optimization technique based on genetic algorithms or simulated annealing functional search engines, and use it to optimize linear and nonlinear versions of Canny’s performance measure for general edge operators. No prior knowledge of the point-spread-function is assumed. We demonstrate that the results obtained are superior both for an ideal and a real edge to those arrived at by previous researchers.	canny edge detector;edge detection;genetic algorithm;global optimization;loss function;mathematical optimization;nonlinear system;numerical analysis;sensor;simulated annealing;web search engine	Shahryar Jalali;James F. Boyce	1995	Image Vision Comput.	10.1016/0262-8856(95)98863-O	computer vision;mathematical optimization;genetic algorithm;edge detection;simulated annealing;computer science;artificial intelligence;point spread function;mathematics;algorithm;search engine;search algorithm	Vision	28.196855250576984	1.7808764734448488	3256
e4f063b23fe5f50046a91e8c24a85630eb3ef076	nestly - a framework for running software with nested parameter choices and aggregating results	software;algorithms;humans;computational biology	UNLABELLED The execution of a software application or pipeline using various combinations of parameters and inputs is a common task in bioinformatics. In the absence of a specialized tool to organize, streamline and formalize this process, scientists must write frequently complex scripts to perform these tasks. We present nestly, a Python package to facilitate running tools with nested combinations of parameters and inputs. nestly provides three components. First, a module to build nested directory structures corresponding to choices of parameters. Second, the nestrun script to run a given command using each set of parameter choices. Third, the nestagg script to aggregate results of the individual runs into a CSV file, as well as support for more complex aggregation. We also include a module for easily specifying nested dependencies for the SCons build tool, enabling incremental builds.   AVAILABILITY Source, documentation and tutorial examples are available at http://github.com/fhcrc/nestly. nestly can be installed from the Python Package Index via pip; it is open source (MIT license).	adobe streamline;aggregate data;bioinformatics;build automation;choice behavior;complex text layout;directory (computing);documentation;increment;open-source software;population parameter;published directory;python package index;scons;mercaptopurine/methotrexate/vincristine	Connor McCoy;Aaron Gallagher;Noah G. Hoffman;Frederick A. Matsen	2013	Bioinformatics	10.1093/bioinformatics/bts696	computer science;bioinformatics;operating system;database;distributed computing	SE	-4.283109523816807	-58.98562013817805	3262
04fe50847d94b03ca1cca901a098469904cec14b	efficient random sampling for nonrigid feature matching	random sampling;feature matching;adaptive sampling;eigenvectors	This paper aims to match two sets of nonrigid feature points using random sampling methods. By exploiting the principle eigenvector of corres pondence-model-linkage, an adaptive sampling method is devised to efficiently deal with non-rigid matching problems.		Lixin Fan;Timo Pylvänäinen	2009		10.1007/978-3-642-10331-5_43	sampling;mathematical optimization;eigenvalues and eigenvectors;slice sampling;pattern recognition;mathematics;statistics	Vision	48.881169994035226	-51.32584696997385	3264
c9496236882aef941204f458cd020853b4c63d48	comprehensive classification and diversity assessment of atomic contacts in protein-small ligand interactions		"""Elucidating the molecular mechanisms of selective ligand recognition by proteins is a long-standing problem in drug discovery. Rapid increase in the availability of three-dimensional protein structural data indicates that a data-driven approach for finding the rules that govern protein-ligand interactions is increasingly attractive. However, this approach is not straightforward because of the complexity of molecular interactions and our inadequate understanding of the diversity of molecular interactions that occur during ligand recognition. Thus, we aimed to provide a comprehensive classification of the spatial arrangements of ligand atoms based on the local coordinates of each interacting """"protein fragment"""" consisting of three atoms with covalent bonds in each amino acid. We used a pattern recognition technique based on the Gaussian mixture model and found 13,519 patterns in the spatial arrangements of interacting ligand atoms, each of which was described as a Gaussian function of the local coordinates. Some typical well-known interaction patterns such as hydrogen bonds were ubiquitous in several hundred protein families, whereas others were only observed in a few specific protein families. After removing protein sequence redundancy from the data set, we found that 63.4% of ligand atoms interacted via one or more interaction patterns and that 25.7% of ligand atoms interacted without patterns, whereas the remainder had no direct interactions. The top 3115 major patterns included 90% of the interacting pairs of residues and ligand atoms with patterns, while the top 6229 included all of them."""	amino acids;covalent interaction;drug discovery;hydrogen;ligands;local coordinates;mixture model;normal statistical distribution;pattern recognition;protein fragment;protein family;rule (guideline)	Kota Kasahara;Matsuyuki Shirota;Kengo Kinoshita	2013	Journal of chemical information and modeling	10.1021/ci300377f	stereochemistry;chemistry;bioinformatics;nanotechnology	ML	8.410733240220043	-59.28618514196406	3266
f1f0789ed411e420d0c6f3a5d19ba78c88b668b6	open-loop tracking of rising and setting gnss radio-occultation signals from an airborne platform: signal model and statistical analysis	statistical analysis atmospheric humidity geophysical signal processing remote sensing satellite navigation;global positioning system atmospheric modeling signal to noise ratio atmospheric measurements extraterrestrial measurements satellites receivers;atmospheric humidity;statistical analysis;geophysical signal processing;remote sensing;satellite navigation;airborne remote sensing radio occultation gnss atmospheric remote sensing;open loop tracking ol tracking ro signal model signal refraction effects small excess phase estimation atmospheric bending satellite observation setting airborne receiver signal setting sampled signal processing tracking initialization higher elevation satellite signal to noise ratio complex correlation residual phase estimate variance minimum snr threshold experimental measurements bending angle measurements atmospheric water vapor profiles stratified atmosphere low elevation gnss signals propagation direction change atmospheric sounding technique gnss ro global navigation satellite system radio occultation statistical analysis signal model airborne platform setting gnss radio occultation signal rising gnss radio occultation signal	Global Navigation Satellite System (GNSS) radio occultation (RO) is an atmospheric sounding technique based upon the change in propagation direction of low-elevation GNSS signals through the stratified atmosphere. Atmospheric water vapor profiles can be retrieved from inverting the bending-angle measurements. Open-loop (OL) tracking, in which the received RO signal is cross-correlated with a model signal that does not include refraction effects, is applied to estimate the small excess phase due to atmospheric bending. OL tracking is demonstrated on rising as well as setting satellites observed from an airborne receiver. Setting signals are tracked by processing the sampled signal in reverse, allowing the initialization of tracking after the satellite has reached a higher elevation. A model has been developed to relate the signal to noise ratio (SNR) of the complex correlation to the variance in the residual phase estimate and is used to set a threshold on the minimum SNR for OL tracking. This model is shown to agree well with experimental measurements.	airborne ranger;automatic sounding;offset binary;radio occultation;satellite navigation;signal-to-noise ratio;software propagation	Kuo-Nung Wang;Paytsar Muradyan;James L. Garrison;Jennifer S. Haase;Brian J. Murphy;Ulvi Acikoz;Tyler Lulich	2013	2013 IEEE International Geoscience and Remote Sensing Symposium - IGARSS	10.1109/IGARSS.2013.6723550	meteorology;satellite navigation;geodesy;remote sensing	Mobile	80.72229262383993	-66.19993695359878	3268
4a562214bb49684e5812a852b13c85c8a9e62d63	eco-environmental evaluation to support environmental protection for twin taipei cities by landsat data		We utilize information derived from Landsat satellite images to produce eco-environmental vulnerability maps by considering four main factors: hydrometeorology, topography, land resources, and socioeconomics. Twin Taipei Cities are chosen for our study. Their eco-environmental vulnerability assessment (EVA) is first performed. Subsequently, classified regions in needs of environmental protection are proposed. We found that high vulnerable levels are observed in the Taipei Metropolis, consisting of Taipei City and its transboundary regions with New Taipei City as a result of high-density buildings and urbanization with low vegetation. Lowlands in the metropolitan area with concentration of impervious surface might lead to flood if heavy rainfall events occur. In contrast, the regions outside Taipei Metropolis, especially in southwest and southeast parts, have higher quality eco-environment due to better-protected forest areas with lower intervention of human activities. Information of suggested ecological zoning regions is useful for Twin Cities' governments to understand their eco-environmental vulnerability conditions.	ecology;map;metropolis;topography	Yuei-An Liou;Anh-Kim Nguyen	2018	IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2018.8517382	vegetation;twin cities;impervious surface;zoning;vulnerability assessment;hydrometeorology;urbanization;environmental protection;metropolitan area;computer science	Mobile	83.31665427865313	-55.43274814733899	3272
01bbd22360d7c376eba73035685f7b11f1f6116b	design and analysis of robust fault detection filter using lmi tools	optimization problem;filter design;linear matrix inequality lmi;linear matrix inequality;model uncertainty;fault detection filter;model matching;fault detection;robustness;linear time invariant	In this paper, the robust fault detection filter design problem for linear time invariant (LTI) systems with unknown inputs and modeling uncertainties is studied. The basic idea of our study is to formulate the robust fault detection filter design as a H∞ model-matching problem. A solution of the optimal problem is then presented via a linear matrix inequality (LMI) formulation. The main results include the formulation of robust fault detection filter design problems, the derivation of a sufficient condition for the existence of a robust fault detection filter and construction of a robust fault detection filter based on the iterative of LMI algorithm. © 2008 Elsevier Ltd. All rights reserved.	algorithm;fault detection and isolation;filter design;iterative method;linear matrix inequality;linear time-invariant theory;log management;numerical analysis;optimal design;social inequality;time complexity;time-invariant system	Jinchao Guo;Xinhan Huang;Yu Cui	2009	Computers & Mathematics with Applications	10.1016/j.camwa.2008.10.032	optimization problem;mathematical optimization;lti system theory;linear matrix inequality;control theory;mathematics;filter design;fault detection and isolation;robustness	Robotics	67.55995195917703	0.26995775551730883	3275
30674eab7dfdce7a634041daa0595b80776274ee	ambient assisted living technology to support older adults with dementia with activities of daily living: key concepts and the state of the art				Alex Mihailidis;Jennifer Boger;Stephen Czarnuch;Tizneem Jiancaro;Jesse Hoey	2012		10.3233/978-1-60750-837-3-304	activities of daily living;physical medicine and rehabilitation;physical therapy;dementia;medicine	HCI	7.773354279752629	-90.62951789938734	3277
dd56940970db71bd23e8f7074e88cab2aa54a650	landsat 9 thermal infrared sensor 2 preliminary stray light assessment		Although the Thermal Infrared Sensor 2 (TIRS-2) is a near-identical copy of the Landsat 8/TIRS-1 instrument, an important design change to the optical system was designed to mitigate the stray light issue that plagued the TIRS-1 instrument [1] 2, 3]. This change involved the addition of several baffles strategically placed within the optical telescope to block the stray light paths that were present in the TIRS-1 design. The specific optical changes were determined by first characterizing the TIRS-1 stray light paths on-orbit and then deriving a detailed optical model that was used to determine the locations and shapes of the mitigating baffles. The stray light design changes to the TIRS-2 instrument were confirmed through the initial thermal-vacuum characterization tests. Preliminary assessments of TIRS-2 indicate that the total stray light magnitude has been drastically reduced to a total magnitude of approximately 1% or less.	characterization test;image sensor;list of code lyoko episodes;optical fiber	Matthew Montanaro;Joel McCorkel;June Tveekrem;John Stauder;Allen Lunsford;Eric Mentzell;Jason Hair;Dennis Reuter	2018	IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2018.8519394	optical imaging;remote sensing;baffle;optical telescope;infrared;computer science;optics;stray light;light scattering;thermal;lens (optics)	Embedded	82.10674381696836	-64.12215879136691	3278
57f062a10a592f215c7f0b45557a75df94e67017	task allocation and path planning for collaborative autonomous underwater vehicles operating through an underwater acoustic network		Dynamic and unstructured multiple cooperative autonomous underwater vehicle (AUV) missions are highly complex operations, and task allocation and path planning are made significantly more challenging under realistic underwater acoustic communication constraints. This paper presents a solution for the task allocation and path planning for multiple AUVs under marginal acoustic communication conditions: a location-aided task allocation framework (LAAF) algorithm for multitarget task assignment and the grid-based multiobjective optimal programming (GMOOP) mathematical model for finding an optimal vehicle command decision given a set of objectives and constraints. Both the LAAF and GMOOP algorithms are well suited in poor acoustic network condition and dynamic environment. Our research is based on an existing mobile ad hoc network underwater acoustic simulator and blind flooding routing protocol. Simulation results demonstrate that the location-aided auction strategy performs significantly better than the well-accepted auction algorithm developed by Bertsekas in terms of task-allocation time and network bandwidth consumption. We also demonstrate that the GMOOP path-planning technique provides an efficient method for executing multiobjective tasks by cooperative agents with limited communication capabilities. This is in contrast to existing multiobjective action selection methods that are limited to networks where constant, reliable communication is assumed to be available.	acoustic cryptanalysis;motion planning	YueYue Deng;Pierre-Philippe J. Beaujean;Edgar An;Edward A. Carlson	2013	J. Robotics	10.1155/2013/483095	real-time computing;simulation;distributed computing	Robotics	55.808056977656086	-25.737451129516025	3279
12995debb6f551a7fe3ce8f9dcaad0c07cb249ac	multi stage promotional resource allocation for segment specific and spectrum effect of promotion for a product incorporating repeat purchase behavior	68t20;90c30;durable technology product;93b40;91b32;differentiated market promotion;spectrum effect;dynamic promotional effort allocation;innovation diffusion models;mass market promotion;repeat purchase;68q25;np hard;market segmentation;differential evolution algorithm	Promotion plays an important role in determining fate of a product. It voices product qualities and persuades potential customers to make purchases. In todays diversified markets every customer has individual needs and preferences. This calls for market segmentation which facilitates companies to adopt customer driven marketing. Product is also promoted using mass promotion to influence larger markets and segments with a spectrum effect. Existing literature in promotion resource allocation optimization evades distinction in two types of promotion strategies while allocating resources and primarily focuses on either of them statically. In this paper, we formulate an optimization model which dynamically allocates differentiated and mass promotional resources in segments to maximize sales under budgetary and minimum market share aspiration constraint on each segment as well as on total market share including repeat purchase behavior. Planning horizon is divided into multi-periods, adoption pattern is studied in each period	darknet market;mathematical optimization;numerical analysis;purchasing	Prerna Manik;Anshu Gupta;P. C. Jha	2015	IGTR	10.1142/S0219198915400216	economics;marketing;operations management;np-hard;microeconomics;market segmentation;commerce	AI	0.024115872652804245	-4.678873302121962	3280
a4de9f264b8f67163e86ad7ce06442f0a4801e2d	nonnegative canonical polyadic decomposition for tissue-type differentiation in gliomas	tensile stress tumors matrix decomposition imaging graphical models distribution functions informatics;l1 regularization non negative canonical polyadic decomposition magnetic resonance spectroscopic imaging glioma;sista	Magnetic resonance spectroscopic imaging (MRSI) reveals chemical information that characterizes different tissue types in brain tumors. Blind source separation techniques are used to extract the tissue-specific profiles and their corresponding distribution from the MRSI data. We focus on automatic detection of the tumor, necrotic and normal brain tissue types by constructing a 3D MRSI tensor from in vivo 2D-MRSI data of individual glioma patients. Nonnegative canonical polyadic decomposition (NCPD) is applied to the MRSI tensor to differentiate various tissue types. An in vivo study shows that NCPD has better performance in identifying tumor and necrotic tissue type in glioma patients compared to previous matrix-based decompositions, such as nonnegative matrix factorization and hierarchical nonnegative matrix factorization.	blind signal separation;body tissue;brain neoplasms;cheminformatics;glioma;histocompatibility testing;necrosis;non-negative matrix factorization;patients;source separation;video-in video-out;visually impaired persons;magnetic resonance spectroscopic imaging	Halandur Nagaraja Bharath;D. M. Sima;Nicolas Sauwen;Uwe Himmelreich;Lieven De Lathauwer;Sabine Van Huffel	2017	IEEE Journal of Biomedical and Health Informatics	10.1109/JBHI.2016.2583539	mathematical analysis;mathematics	ML	26.859602324485465	-78.32920892056488	3281
1a744c81f609ffc416428af6cf45d37f1184bb50	iterative reweighted algorithms for sparse signal recovery with temporally correlated source vectors	temporal correlation;mahalanobis distance;compressed sensing;sparse bayesian learning;sparse signal recovery;temporal correlation sparse signal recovery compressed sensing iterative reweighted l 2 algorithms sparse bayesian learning;multiple measurement vectors;iterative reweighted l 2 algorithms	Iterative reweighted algorithms, as a class of algorithms for sparse signal recovery, have been found to have better performance than their non-reweighted counterparts. However, for solving the problem of multiple measurement vectors (MMVs), all the existing reweighted algorithms do not account for temporal correlations among source vectors and thus their performance degrades significantly in the presence of the correlations. In this work we propose an iterative reweighted sparse Bayesian learning (SBL) algorithm exploiting the temporal correlations, and motivated by it, we propose a strategy to improve existing reweighted ℓ2 algorithms for the MMV problem, i.e. replacing their row norms with Mahalanobis distance measure. Simulations show that the proposed reweighted SBL algorithm has superior performance, and the proposed improvement strategy is effective for existing reweighted ℓ2 algorithms.	algorithm;computation;computer simulation;detection theory;iteration;iterative method;multiuse model view;sparse matrix	Zhilin Zhang;Bhaskar D. Rao	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5947212	iteratively reweighted least squares;computer vision;computer science;mahalanobis distance;machine learning;pattern recognition;mathematics;compressed sensing;statistics	Vision	78.3537927070229	-6.7793805498408455	3284
30f3f6ccdaad0abff05d61d528e0d9526b08bad6	adjustable speed surface subdivision	concepcion asistida;computer aided design;non uniform;anneau;aproximacion esplin;spline approximation;approximation spline;conception assistee;adjustable speed;subdivision;ring;anillo	We introduce a non-uniform subdivision algorithm that partitions the neighb orhood of an extraordinary point in the ratioσ : 1 − σ, whereσ ∈ (0, 1). We callσ the speedof the non-uniform subdivision and verify C continuity of the limit surface. For σ = 1/2, the Catmull-Clark limit surface is recovered. Other speeds are useful to vary the relative width of the polynomial spline rings generated from extraordinary nodes.	algorithm;catmull–clark subdivision surface;polynomial;scott continuity;spline (mathematics)	Kestutis Karciauskas;Jörg Peters	2009	Computer Aided Geometric Design	10.1016/j.cagd.2009.07.006	mathematical optimization;computer aided design;subdivision;mathematics;geometry;ring;algebra	Graphics	68.29500322467389	-40.302434872514446	3287
702e71ee487ba61e9c695c1c965c084411c9770a	reduction of wear at hot forging dies by using coating systems containing boron	multilayer;cracks;wear resistance;pacvd;hot forging;coating design	The near surface area of forging dies is exposed to high mechanical loads. Additionally thermal and chemical stresses appear during the hot forging process. Depending on the number of forged parts, several kinds of stresses occur in the near surface area, which lead to the initial failures of forging dies. Wear is the main reason for production downtimes with a ratio of 70%. Furthermore, thermal and mechanical cracks are typical causes for failures causes as well as plastic deformation. In order to reduce wear, the abrasion resistance of the forging die surface has to be increased. Hence, different methods like plasma nitriding and optional additional thin hard coatings (TiN, TiCN, TiC, TiBN and TiB2) were successfully examined. Recently developed Ti–B–N coatings in specific multilayer designs are thermally stable, wear-resistant and anti-adhesive regarding the sticking of work piece material. This paper presents the wear reduction possibilities of boron-containing multilayer coating systems applied to forging dies by using the plasma enhanced chemical vapor deposition treatment. A basic mechanical and analytical characterization of different coating systems is realized in the first stage of the project. Best qualified multilayer coating variants were applied to forging dies for experimental investigations. As a result of the tests, wear can be reduced significantly by using thermally stable boron multilayer coatings. To receive realistic wear values under production conditions, an automated forging process was used for testing. After 3,000 forged parts, the coatings were examined by tactile measurement, SEM and EDX analyses to characterize the occurring wear.		Bernd-Arno Behrens;Günter Bräuer;Hanno Paschke;Marcus Bistron	2011	Production Engineering	10.1007/s11740-011-0308-z	engineering;forging;wear;metallurgy;mechanical engineering	OS	87.36698097789603	-13.720891517285608	3291
3b20ae165689ee108bd34178179ba37a60e01e1a	continuous global optimization in multiview 3d reconstruction	minimisation;modelizacion;calculo de variaciones;image tridimensionnelle;minimization;vision ordenador;silhouette;image processing;resolution spatiale;multiview 3d reconstruction;resolucion espacial;convex programming;coupe graphe;image based_3d_reconstruction;procesamiento imagen;mesure niveau;optimum global;minimizacion;convex optimization;programmation convexe;global optimum;traitement image;computer vision;multiple view;modelisation;reconstruction image;corte grafo;calcul variationnel;reconstruccion imagen;continuous global minimization;convex function;graph cut;image reconstruction;level measurement;image based 3d reconstruction;vue multiple;tridimensional image;global optimization;vision ordinateur;modele donnee;subdivision;convex relaxation;modeling;optimo global;3d shape reconstruction;silueta;variational calculus;surface area;3d reconstruction;quantitative evaluation;imagen tridimensional;vista multiple;data models;medicion nivel;spatial resolution;programacion convexa	In this article, we introduce a new global optimization method to the field of multiview 3D reconstruction. While global minimization has been proposed in a discrete formulation in form of the maxflow-mincut framework, we suggest the use of a continuous convex relaxation scheme. Specifically, we propose to cast the problem of 3D shape reconstruction as one of minimizing a spatially continuous convex functional. In qualitative and quantitative evaluation we demonstrate several advantages of the proposed continuous formulation over the discrete graph cut solution. Firstly, geometric properties such as weighted boundary length and surface area are represented in a numerically consistent manner: The continuous convex relaxation assures that the algorithm does not suffer from metrication errors in the sense that the reconstruction converges to the continuous solution as the spatial resolution is increased. Moreover, memory requirements are reduced, allowing for globally optimal reconstructions at higher resolutions. We study three different energy models for multiview reconstruction, which are based on a common variational template unifying regional volumetric terms and on-surface photoconsistency. The three models use data measurements at increasing levels of sophistication. While the first two approaches are based on a classical silhouette-based volume subdivision, the third one relies on stereo information to define regional costs. Furthermore, this scheme is exploited to compute a precise photoconsistency measure as opposed to the classical estimation. All three models are compared on standard data sets demonstrating their advantages and shortcomings. For the third one, which gives the most accurate results, a more exhaustive qualitative and quantitative evaluation is presented.	3d reconstruction;algorithm;calculus of variations;convex function;convex optimization;cut (graph theory);experiment;global optimization;graph cuts in computer vision;linear programming relaxation;mathematical optimization;max-flow min-cut theorem;maxima and minima;maximum flow problem;minimum cut;numerical analysis;optimization problem;requirement;shadow volume;subdivision surface	Kalin Kolev;Maria Klodt;Thomas Brox;Selim Esedoglu;Daniel Cremers	2007	International Journal of Computer Vision	10.1007/s11263-009-0233-1	3d reconstruction;iterative reconstruction;convex function;data modeling;computer vision;minimisation;mathematical optimization;convex optimization;systems modeling;image resolution;cut;computer science;subdivision;surface area;mathematics;geometry;global optimum;silhouette;calculus of variations	Vision	51.52265465675051	-56.635903566720536	3293
bcc60edf26a17e0b0b5eed9b7d3065f5e5e997dc	a discrete-time model of american put option in an uncertain environment	optimal stopping problem;stochastic process;uncertainty modeling;discrete time;possibility measure;continuous time models;mathematical model;optimal stopping;american put option	Abstract   A discrete-time mathematical model for American put option with uncertainty is presented, and the randomness and fuzziness are evaluated by both probabilistic expectation and fuzzy expectation defined by a possibility measure from the viewpoint of fuzzy expectation, taking account of decision-maker’s subjective judgment. An optimality equation for the optimal stopping problem in a fuzzy stochastic process is derived and an optimal exercise time is given for the American put option. It is shown that the optimal fuzzy price is a solution of the optimality equation under a reasonable assumption. The permissible range of the writer’s (seller’s) optimal expected price in the American put option is presented, and the meaning and properties of the optimal expected prices are discussed in numerical examples. In a numerical example, the discrete-time approximation model is discussed for the continuous-time model.		Yuji Yoshida	2003	European Journal of Operational Research	10.1016/S0377-2217(02)00591-X	financial economics;stochastic process;mathematical optimization;actuarial science;optimal stopping;economics;mathematics;mathematical economics;statistics	DB	1.0257649000709423	-12.341310247997617	3303
a9d402daa2c215d3c1312c784e24fc556637b45b	ranking insertion, deletion and nonsense mutations based on their effect on genetic information	budding yeast;genetics medical;amino acid;disease;yeasts;non coding region;allele frequency;computational method;indel mutation;genetics;genetic variation;computational biology bioinformatics;codon nonsense;purifying selection;evolutionary conservation;algorithms;humans;combinatorial libraries;computational biology;computer appl in life sciences;mutagenesis insertional;mutation;single nucleotide polymorphism;microarrays;bioinformatics	Genetic variations contribute to normal phenotypic differences as well as diseases, and new sequencing technologies are greatly increasing the capacity to identify these variations. Given the large number of variations now being discovered, computational methods to prioritize the functional importance of genetic variations are of growing interest. Thus far, the focus of computational tools has been mainly on the prediction of the effects of amino acid changing single nucleotide polymorphisms (SNPs) and little attention has been paid to indels or nonsense SNPs that result in premature stop codons. We propose computational methods to rank insertion-deletion mutations in the coding as well as non-coding regions and nonsense mutations. We rank these variations by measuring the extent of their effect on biological function, based on the assumption that evolutionary conservation reflects function. Using sequence data from budding yeast and human, we show that variations which that we predict to have larger effects segregate at significantly lower allele frequencies, and occur less frequently than expected by chance, indicating stronger purifying selection. Furthermore, we find that insertions, deletions and premature stop codons associated with disease in the human have significantly larger predicted effects than those not associated with disease. Interestingly, the large-effect mutations associated with disease show a similar distribution of predicted effects to that expected for completely random mutations. This demonstrates that the evolutionary conservation context of the sequences that harbour insertions, deletions and nonsense mutations can be used to predict and rank the effects of the mutations.	alleles;amino acids;biopolymer sequencing;clinical act of insertion;codon (nucleotide sequence);codon, nonsense;computation;conserved sequence;deletion mutation;function (biology);gene frequency;hereditary diseases;indel mutation;insertion mutation;large;nonsense mutation;nucleotides;purification of quantum state;randomness;saccharomycetales;single nucleotide polymorphism	Amin Zia;Alan M. Moses	2011		10.1186/1471-2105-12-299	single-nucleotide polymorphism;mutation;biology;molecular biology;amino acid;dna microarray;bioinformatics;genetic variation;allele frequency;conserved sequence;genetics;negative selection	Comp.	4.03401256535933	-61.76700948991217	3306
9971e4f8fa7790573f47c3a1e4aafbf3907b708e	on the exponential stability of nonlinear delay systems with impulses			nonlinear system;time complexity	Zhongli You;JinRong Wang	2018	IMA J. Math. Control & Information	10.1093/imamci/dnw077	mathematics;mathematical optimization;control theory;nonlinear system;exponential stability	Robotics	71.14808582916966	-0.630157398829126	3310
615526383d6b05960aed65e44425b4e749d5c5fc	internal preload control of redundantly actuated parallel manipulators &#8211; backlash avoiding control	backlash;inverse dynamics;manipulator dynamics kinematics force control open loop systems parallel robots actuators feeds fasteners mechatronics chemical technology;feeds;parallel manipulator;manipulator dynamics;actuators;open loop systems;kinematics;general solution;parallel robots;model based control;fasteners;mechatronics;real time application;model based control redundant actuation inverse dynamics parallel manipulator backlash;chemical technology;redundant actuation;force control	Redundant actuation of parallel manipulators admits internal forces without generating end-effector forces (preload). Preload can be controlled in order to prevent backlash during the manipulator motion. Such controlis based on the inverse dynamics. The general solution of the inverse dynamics of redundantly actuated parallel manipulators is given. For the special case of simple over actuation an explicit solution is derived in terms of a single preload parameter. With this formulation a computational effcient open-loop preload control is developed and applied to the elimination of backlash. Its simplicity makes it applicable in real-time applications. The approach is exemplified for a planar 4RRR manipulator.	continuation;inverse dynamics;preload (software);real-time clock;real-time computing;robot end effector;sampling (signal processing);the matrix	Andreas Müller	2005	Proceedings of the 2005 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2005.1570239	control engineering;parallel manipulator;simulation;mechatronics;engineering;control theory	Robotics	70.59370345559715	-19.234079811935054	3314
f22a528239de86f1cca36105d86d74084200f2b1	the l-separation criterion for description of cs-independence models	directed acyclic graph;conditional independence;graphical model;conditional probability	Different conditional independence models have been proposed in literature; in this paper we consider models induced by conditional probabilities based on the definition of conditional cs-independence. These models need not comply with the symmetry property, so that they have not the graphoid structure. Hence, the well-known d-separation criterion for directed acyclic graphs may not be able to represent such independence models. Therefore, we introduce a new separation criterion called L-separation. We study its main properties and show how it allows to represent the above-mentioned independence models through directed acyclic graphs.		Barbara Vantaggi	2002	Int. J. Approx. Reasoning	10.1016/S0888-613X(01)00069-X	combinatorics;discrete mathematics;conditional independence;conditional probability;computer science;machine learning;bayesian network;mathematics;graphical model;directed acyclic graph;discriminative model;statistics	AI	25.113467660497523	-27.696933857535477	3316
55a67d77f00f0d000da2ad2d1f846150feaff133	feature-preserving procedural texture		This paper presents how to synthesize a texture in a procedural way that preserves the features of the input exemplar. The exemplar is analyzed in both spatial and frequency domains to be decomposed into feature and non-feature parts. Then, the non-feature parts are reproduced as a procedural noise, whereas the features are independently synthesized. They are combined to output a non-repetitive texture that also preserves the exemplar’s features. The proposed method allows the user to control the extent of extracted features and also enables a texture to edited quite effectively.	procedural programming;procedural texture	HyeongYeop Kang;JungHyun Han	2017	The Visual Computer	10.1007/s00371-017-1375-8	computer vision;speech recognition;computer science	Graphics	22.728898276410483	-55.452931245615225	3317
67721e451323bc4ca04b724d30e74b959a20b936	robot navigation with a polar neural map	motion control;robot navigation;kinematics and dynamics;navigation system	Michail G. Lagoudakis Department of Computer Science Duke University Durham, NC 27708 mgl@cs.duke.edu Anthony S. Maida Center for Advanced Computer Studies University of Southwestern Louisiana Lafayette, LA 70504 maida@cacs.usl.edu Neural maps have been recently proposed as an alternative method for mobile robot path planning (Glasius, Komoda, and Gielen 1995). However, these proposals are mostly theoretical and are primarily concerned with biological plausibility. Our purpose is to investigate their applicability on real robots. Information about the environment is mapped on a topologically ordered neural population. The di usion dynamics force the network into a unique equilibrium state that de nes the navigation landscape for the given target. A path from any initial position to the target (corresponding to the peak of the activation surface) is derived by a steepest ascent procedure. The gures below show an example on a 50 50 rectangular map (a. Environment, b. Contours of activation, c. Path). We attempted to implement the approach on a Nomad 200 mobile robot for sonar-based navigation. However, we found that the neural map requires reorganization in a polar topology that re ects the distribution of the sonar data points, the only source of information about the environment. The polar map covers the local circular area around at the robot. Sonar data points are mapped scaled to the physical robot size. At each step of the control loop, the dynamics of the map is used to derive the angular and radial displacement required to reach the target from the current con guration. A simpli ed example is shown below (bird's eye view). Sensor uncertainty and noise is handled by a sonar short-term memory and appropriate coordinate mapping for reuse. Motion control is based on an optimization procedure that combines ideas from Fox, Burgard, and Thrun (1997) and Hong et al (1996), and takes into account the kinematic and dynamic constraints of the robot. The complete architecture of the resulting local (sensor-based) navigation system is shown below. Partially supported by the Lilian-Boudouri Foundation in Greece. Copyright c 1999, American Association for Arti cial Intelligence (www.aaai.org). All rights reserved. I T	angularjs;bird's-eye view;computer science;control system;data point;displacement mapping;gradient descent;information source;long short-term memory;map;mathematical optimization;mobile robot;motion planning;naruto shippuden: clash of ninja revolution 3;neural ensemble;nomad 200;plausibility structure;radial (radio);robotic mapping;sonar (symantec);sensor;times ascent	Michail G. Lagoudakis;Anthony S. Maida	1999			motion control;mobile robot;avm navigator;robot control;mobile robot navigation;robot kinematics	AI	50.95493534654904	-29.619995939002635	3319
0cab9ca3f31cd692e921d33ed2c43a3828aabe52	variable stiffness treadmill (vst): a novel tool for the investigation of gait	rehabilitation variable stiffness treadmill vst gait analysis force stimulus walking surface load feedback mechanisms load feedback stimulus control gait kinematics;force foot kinematics equations mathematical model legged locomotion springs;patient rehabilitation feedback gait analysis	Locomotion is one of the human's most important functions that serve survival, progress and interaction. Gait requires kinematic and dynamic coordination of the limbs and muscles, multi-sensory fusion and robust control mechanisms. The force stimulus generated by the interaction of the foot with the walking surface is a vital part of the human gait. Although there have been many studies trying to decipher the load feedback mechanisms of gait, there is a need for the development of a versatile system that can advance research and provide new functionality. In this paper, we present the design and characterization of a novel system, called Variable Stiffness Treadmill (VST). The device is capable of controlling load feedback stimulus by regulating the walking surface stiffness in real time. The high range of available stiffness, the resolution and accuracy of the device, as well as the ability to regulate stiffness within the stance phase of walking, are some of the unique characteristics of the VST. We present experiments with healthy subjects in order to prove the concept of our device and preliminary findings on the effect of altered stiffness on gait kinematics. The developed system constitutes a uniquely useful research tool, which can improve our understanding of gait and create new avenues of research on gait analysis and rehabilitation.	control system;experiment;gait analysis;haptic technology;inverse kinematics;omnidirectional treadmill;perturbation theory;robust control;simulation;stiffness;video game rehabilitation	Andrew Barkan;Jeffrey Skidmore;Panagiotis K. Artemiadis	2014	2014 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2014.6907266	effect of gait parameters on energetic cost;simulation	Robotics	68.53700929937003	-26.125571240259465	3321
6a516cdba110e74429c8bc741b128b5c14a62237	deep-learning-based pipe leak detection using image-based leak features		In this paper, we propose a deep-learning-based pipe leak detection (PLD) technique using trajectory-based image features extracted from time-series acoustic data received from microphone sensor nodes. We developed root-mean-square-pattern and frequency-pattern images by reflecting the leakage signal characteristics in the time and frequency domains and used them for ensemble learning with the help of state-of-the-art residual networks. The experimental results obtained using the measured data of leakage signals in a laboratory-scale nuclear power plant environment are presented to validate the effectiveness of the proposed method for PLD. The results show that the proposed image features suitable for convolutional neural network-based deep-learning can provide reliable PLD performance in terms of classification accuracy despite the machine-driven complex noise environment.		Ji-Hoon Bae;Doyeob Yeo;D. Yoon;Se Won Oh;Gwan-Joong Kim;Nae-Soo Kim;Cheol Sig Pyo	2018	2018 25th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2018.8451489	convolutional neural network;residual;computer vision;feature (computer vision);deep learning;computer science;ensemble learning;leak;microphone;leakage (electronics);pattern recognition;artificial intelligence	Robotics	26.82011463032243	-63.49937134130434	3327
ec39af8af930046e549968b84054b831cf0254c3	semi-supervised interpolation in an anticausal learning scenario	independence of cause and mechanism;semi supervised learning;abt scholkopf;information geometry;anticausal learning;causality	According to a recently stated ‘independence postulate’, the distribution Pcause contains no information about the conditional Peffect|cause while Peffect may contain information about Pcause|effect. Since semi-supervised learning (SSL) attempts to exploit information from PX to assist in predicting Y from X, it should only work in anticausal direction, i.e., when Y is the cause and X is the effect. In causal direction, when X is the cause and Y the effect, unlabelled x-values should be useless. To shed light on this asymmetry, we study a deterministic causal relation Y = f(X) as recently assayed in InformationGeometric Causal Inference (IGCI). Within this model, we discuss two options to formalize the independence of PX and f as an orthogonality of vectors in appropriate inner product spaces. We prove that unlabelled data help for the problem of interpolating a monotonically increasing function if and only if the orthogonality conditions are violated – which we only expect for the anticausal direction. Here, performance of SSL and its supervised baseline analogue is measured in terms of two different loss functions: first, the mean squared error and second the surprise in a Bayesian prediction scenario.	anticausal system;baseline (configuration management);causal filter;causal inference;interpolation;loss function;mean squared error;pixel;semi-supervised learning;semiconductor industry;supervised learning	Dominik Janzing;Bernhard Schölkopf	2015	Journal of Machine Learning Research		semi-supervised learning;causality;computer science;artificial intelligence;machine learning;anticausal system;mathematics;information geometry;statistics	ML	21.06858098975163	-33.210829313084936	3330
5970d1ec5b858d5f95663335d03091196c0cebf3	a spatial summation model for image processing by artificial neural networks	image processing;neural nets;image data representation;noisy images;neural nets computerised picture processing;noise suppression;hidden processing unit;threshold logic;artificial neural networks;activation rule;hidden layer;impulsive noise removal;noise suppressing algorithm;spatial summation model;massively parallel decision tasks;input layer;robust performance;computerised picture processing;spatial summation;threshold logic activation rule artificial neural networks binary representation gray level image processing hidden layer hidden processing unit image data representation impulsive noise removal input layer massively parallel decision tasks noise suppressing algorithm noisy images real valued intensities robust performance spatial summation model;real valued intensities;gray level image processing;binary representation;artificial neural network;neural network	The representation of the data used in a computational algorithm should be coupled to the computing machine on which the algorithm is implemented. A model for representing image data for gray-level image processing by artificial neural networks is presented. An input layer converts an image with real-valued or quantized intensities to a binary representation. The input layer uses threshold logic as the activation rule. An image processing task is decomposed into a collection of massively parallel decision tasks, each of which is performed locally by a hidden processing unit. The output of the hidden layer, in binary form, can be converted to a resultant image with real-valued intensities by the output layer. An advantage of this approach is that it is well-matched to the decision and classification capabilities of neural networks. A noise-suppressing algorithm with robust performance is implemented to illustrate the efficacy of this model	artificial neural network;image processing	C. H. Chu	1990		10.1109/IJCNN.1990.137620	computer vision;summation;feature detection;binary number;image processing;computer science;theoretical computer science;machine learning;digital image processing;artificial neural network	Robotics	34.085450514402936	-39.6820564394804	3333
1bd4e84a58f4d9f88e90de058175374396f560dd	further results on almost disturbance decoupling with global asymptotic stability for nonlinear systems	systeme commande;sistema control;disturbance rejection;zero dynamics;global asymptotic stability;disturbance decoupling;stabilite asymptotique;almost disturbance decoupling;desacoplamiento;low gain;l2 gain;decouplage;rejet perturbation;asymptotic stability;linear system;control system;generalized integrators;high gain;system synthesis;synthese systeme;systeme non lineaire;recuazamiento pertubacion;sintesis sistema;decoupling;estabilidad asintotica;nonlinear system;sistema no lineal;non linear system	As a complement to some new breakthroughs on global almost disturbance decoupling problem with stability for nonlinear systems, in a recent note, we identified a class of unstable zero dynamics that are allowed to be affected by disturbances. The class of the unstable zero dynamics identified in that note is linear and have all the poles at the origin. In this paper, we enlarge such a class of zero dynamics to include any linear system with all its poles in the closed left-half plane. The condition on the way the disturbance affects this part of zero dynamics is also identified. This enlargement is due to a new scaling technique that views each pair of jw axis zeros as a ‘‘generalized integrator’’ and transforms the zero dynamics into a number of chains of ‘‘generalized integrators’’. ( 1999 Elsevier Science Ltd. All rights reserved.	apache axis;control theory;coupling (computer programming);image scaling;linear system;nonlinear system	Zongli Lin;Xiangyu Bao;Ben M. Chen	1999	Automatica	10.1016/S0005-1098(98)00203-9	control engineering;nonlinear system;engineering;control system;calculus;decoupling;control theory;mathematics;linear system;high-gain antenna	Robotics	70.26500919057153	-3.559315618130546	3339
499241dcfc4686720d3ce1f3b8d1b73c04f1fd14	cdte/cds core shell quantum dots for photonic applications	x ray diffraction;diffraction rx;nanocristal;x ray microscopy;caracteristique optique;punto cuantico;quantum dot;luminescence;suspension coloidal;quantum dots;experimental procedure;integrated optics;point quantique;powder x ray diffraction;luminiscencia;optica integrada;transmission electron microscopy;cdte;espectrometria emision;spectrometrie emission;optical characteristic;optical properties;microscopia rayos x;optique integree;microscopie rx;nanocrystal;suspension colloidale;caracteristica optica;colloidal suspension;emission spectrometry;difraccion rx	In this work, we show a simple experimental procedure to obtain CdTe/CdS (diameterZ3–7 nm) core shell nanocrystals in colloidal suspension. The nanocrystals were characterized by transmission electronic microscopy and powder X-ray diffraction analysis. The optical properties were determined by electronic absorption, excitation and emission spectroscopies and are discussed in terms of chemical changes that occur at the surface of the particles. q 2005 Elsevier Ltd. All rights reserved.	quantum dot	F. D. de Menezes;A. G. Brasil;W. L. Moreira;L. C. Barbosa;C. L. Cesar;Ricardo C Ferreira;P. M. A. de Farias;B. S. Santos	2005	Microelectronics Journal	10.1016/j.mejo.2005.04.005	nanotechnology;optics;quantum dot;x-ray crystallography;physics	Theory	93.57118833046206	-11.120577112662819	3341
0e13f1234e698f39cdb8ebeff3089f6280f921a8	the emergence of action sequences from spatial attention: insight from rodent-like robots		Animal behaviour is rich, varied, and smoothly integrated. One plausible model of its generation is that behavioural sub-systems compete to command effectors. In small terrestrial mammals, many behaviours are underpinned by foveation, since important effectors (teeth, tongue) are co-located with foveal sensors (microvibrissae, lips, nose), suggesting a central role for foveal selection and foveation in generating behaviour. This, along with research on primate visual attention, inspires an alternative hypothesis, that integrated behaviour can be understood as sequences of foveations with selection being amongst foveation targets based on their salience. Here, we investigate control architectures for a biomimetic robot equipped with a rodent-like vibrissal tactile sensing system, explicitly comparing a salience map model for action guidance with an earlier model implementing behaviour selection. Both architectures generate life-like action sequences, but in the salience map version higher-level behaviours are an emergent consequence of following a shifting focus of attention.	biomimetics;emergence;robot;sensor;smoothing;terrestrial television	Benjamin Mitchinson;Martin J. Pearson;Anthony G. Pipe;Tony J. Prescott	2012		10.1007/978-3-642-31525-1_15	psychology;computer vision;artificial intelligence;communication	Robotics	18.71948306591968	-66.82229579241019	3345
5b2d99d0396ed37d1e9043c5d9ac31cd368b00dd	individualized predictions of survival time distributions from gene expression data using a bayesian mcmc approach	overall survival;expression profile;diffuse large b cell lymphoma;high dimensionality;bayesian approach;prior distribution;cox regression;gene expression data;gene expression;confidence interval;survival time;posterior distribution;drug design;feature selection;patient survival	"""It has previously been demonstrated that gene expression data correlate with event-free and overall survival in several cancers. A number of methods exist that assign patients to different risk classes based on expression profiles of their tumor. However, predictions of actual survival times in years for the individual patient, together with confidence intervals on the predictions made, would provide a far more detailed view, and could aid the clinician considerably in evaluating different treatment options. Similarly, a method able to make such predictions could be analyzed to infer knowledge about the relevant disease genes, hinting at potential disease pathways and pointing to relevant targets for drug design. Here too, confidences on the relevance values for the individual genes would be useful to have.#R##N##R##N#Our algorithm to tackle these questions builds on a hierarchical Bayesian approach, combining a Cox regression model with a hierarchical prior distribution on the regression parameters for feature selection. This prior enables the method to efficiently deal with the low sample number, high dimensionality setting characteristic of microarray datasets. We then sample from the posterior distribution over a patients survival time, given gene expression measurements and training data. This enables us to make statements such as """"with probability 0.6, the patient will survive between 3 and 4 years"""". A similar approach is used to compute relevance values with confidence intervals for the individual genes measured.#R##N##R##N#The method is evaluated on a simulated dataset, showing feasibility of the approach. We then apply the algorithm to a publicly available dataset on diffuse large B-cell lymphoma, a cancer of the lymphocytes, and demonstrate that it successfully predicts survival times and survival time distributions for the individual patient."""	markov chain monte carlo	Lars Kaderali	2007		10.1007/978-3-540-71233-6_7	gene expression;confidence interval;prior probability;bayesian probability;bioinformatics;data mining;mathematics;posterior probability;feature selection;proportional hazards model;drug design;statistics	ML	7.825694796121063	-52.81040741201984	3346
b0f34b2ae27ccfb7ca4bb64d41a1bc9a55ce6e08	dispersion ratio based decision tree model for classification		Abstract In predictive tasks like classification, Information Gain (IG) based Decision Tree is very popularly used. However, IG method has some inherent problems like its preference towards choosing attributes with higher number of distinct values as the splitting attribute in case of nominal attributes and another problem is associated with imbalanced datasets. Most of the real-world datasets have many nominal attributes, and those nominal attributes may have many number of distinct values. In this paper, we have tried to point out these characteristics of the datasets while discussing the performance of our proposed approach. Our approach is a variant of the traditional Decision Tree model and uses a new technique called Dispersion_Ratio, a modification of existing Correlation Ratio (CR) method. The whole approach is divided into two phases - firstly, the dataset is discretised using a discretization module and secondly, the preprocessed dataset is used to build a Dispersion Ratio based Decision Tree model. The proposed method does not prefer the attributes with many unique values and indifferent about class distribution. It performs better than previously proposed CR based Decision Tree (CRDT) Model since an efficient discretization module has been added with it. We have evaluated the performance of our approach on some benchmark datasets from various domains to demonstrate the effectiveness of the proposed technique and also compared our model with Information Gain, Gain Ratio and Gini Index based models. Result shows that the proposed model outperforms other models in majority of the cases that we have considered in our experiment.	decision tree model	Smita Roy;Samrat Mondal;Asif Ekbal;Maunendra Sankar Desarkar	2019	Expert Syst. Appl.	10.1016/j.eswa.2018.08.039	decision tree model;decision tree;dispersion (optics);information gain ratio;machine learning;artificial intelligence;discretization;correlation ratio;computer science	ML	7.865087503681629	-41.72568191050195	3347
5ecb022258d4f397431d92058cac97e92bcac635	toward free-living walking speed estimation using gaussian process-based regression with on-body accelerometers and gyroscopes	bayes methods;gaussian processes;accelerometers;biocontrol;body sensor networks;gait analysis;gyroscopes;least squares approximations;nonlinear estimation;nonparametric statistics;regression analysis;velocity measurement;bayesian linear regression;gaussian process based regression;rms prediction error;free living walking speed estimation;ground truth walking speed;least squares regression;linear correlation;nonlinear nonparametric regression framework;on-body accelerometers;on-body sensor;treadmill walking speed estimation;tri-axial gyroscopes;accelerometers;gaussian processes;gyroscopes;walking speed estimation	Walking speed is an important determinant of energy expenditure. We present the use of Gaussian Process-based Regression (GPR), a non-linear, non-parametric regression framework to estimate walking speed using data obtained from a single on-body sensor worn on the right hip. We compare the performance of GPR with Bayesian Linear Regression (BLR) and Least Squares Regression (LSR) in estimating treadmill walking speeds. We also examine whether using gyroscopes to augment accelerometry data can improve prediction accuracy. GPR shows a lower average RMS prediction error when compared to BLR and LSR across all subjects. Per subject, GPR has significantly lower RMS prediction error than LSR and BLR (p≪0.05) with increasing training data. The addition of tri-axial gyroscopes as inputs reduces RMS prediction error (p≪0.05 per subject) when compared to using only acclerometers. We also study the effect of using treadmill walking data to predict overground walking speeds and that of combining data from more than one person to predict overground walking speed. A strong linear correlation exists (rX,Y = .8861) between overground walking speeds predicted from treadmill data and ground truth walking speed measured. Combining treadmill data from multiple subjects with similar height characteristics improved the prediction capability of GPR for overground walking speeds as measured by correllation between ground truth and GP-predicted values (rX,Y = .8204 with combined data).	bayesian network;gaussian process;ground truth;gyroscope;kriging;least squares;nonlinear system;triangular function	Harshvardhan Vathsangam;B. Adar Emken;Donna Spruijt-Metz;Gaurav S. Sukhatme	2010	2010 4th International Conference on Pervasive Computing Technologies for Healthcare		gaussian process;least squares;statistics	Robotics	12.460928871456439	-86.07155688699001	3348
184d67cee2d05f249517ffd2886f0bc17c24260c	weighted kpca degree of homogeneity amended nonclassical receptive field inhibition model for salient contour extraction in low-light-level image	nonclassical receptive field inhibition model;kernel;wkpca;degree of homogeneity;edge detection;computational procedure;crf;dh hemts;night vision image weighted kpca degree of homogeneity nonclassical receptive field inhibition model salient contour extraction low light level image receptive field neuron modulation night vision information processing weighted kernel principal component analysis degree of homogeneity complex natural scene wkpca algorithm computational procedure ncrf contour elements computational visual model contour detection;visualization;night vision image;vectors kernel dh hemts computational modeling covariance matrices visualization principal component analysis;computational modeling;principal component analysis edge detection night vision;vectors;night vision;complex natural scene;covariance matrices;principal component analysis;dh;contour detection;ncrf;night vision information processing;receptive field;weighted kernel principal component analysis;wkpca algorithm;low light level image;salient contour extraction;computational visual model;lll image;contour elements;neuron;weighted kpca degree of homogeneity;modulation	The stimulus response of the classical receptive field (CRF) of neuron in primary visual cortex is affected by its periphery [i.e., non-CRF (nCRF)]. This modulation exerts inhibition, which depends primarily on the correlation of both visual stimulations. The theory of periphery and center interaction with visual characteristics can be applied in night vision information processing. In this paper, a weighted kernel principal component analysis (WKPCA) degree of homogeneity (DH) amended inhibition model inspired by visual perceptual mechanisms is proposed to extract salient contour from complex natural scene in low-light-level image. The core idea is that multifeature analysis can recognize the homogeneity in modulation coverage effectively. Computationally, a novel WKPCA algorithm is presented to eliminate outliers and anomalous distribution in CRF and accomplish principal component analysis precisely. On this basis, a new concept and computational procedure for DH is defined to evaluate the dissimilarity between periphery and center comprehensively. Through amending the inhibition from nCRF to CRF by DH, our model can reduce the interference of noises, suppress details, and textures in homogeneous regions accurately. It helps to further avoid mutual suppression among inhomogeneous regions and contour elements. This paper provides an improved computational visual model with high-performance for contour detection from cluttered natural scene in night vision image.	algorithm;area striata structure;computation;conditional random field;contour line;corticotropin-releasing hormone;information processing;inspiration function;interference (communication);kernel principal component analysis;modulation;neuron;night vision;photic stimulation;receptive aphasia (finding);visual cortex;visual modeling;zero suppression	Yi Zhang;Jing Han;Jiang Yue;Lianfa Bai	2014	IEEE Transactions on Image Processing	10.1109/TIP.2014.2317987	computer vision;kernel;visualization;edge detection;computer science;machine learning;pattern recognition;mathematics;computational model;receptive field;principal component analysis;modulation	Vision	36.52904969708401	-57.59085402820484	3349
b9fc62c82b555873b3d96f6a7104f33403c0d9c0	neurosphere segmentation in brightfield images	vignetting	The challenge of segmenting neurospheres (NSPs) from brightfield images includes uneven background illumination (vignetting), low contrast and shadow-casting appearance near the well wall. We propose a pipeline for neurosphere segmentation in brightfield images, focusing on shadow-casting removal. Firstly, we remove vignetting by creating a synthetic blank field image from a set of brightfield images of the whole well. Then, radial line integration is proposed to remove the shadow-casting and therefore facilitate automatic segmentation. Furthermore, a weighted bi-directional decay function is introduced to prevent undesired gradient effect of line integration on NSPs without shadow-casting. Afterward, multiscale Laplacian of Gaussian (LoG) and localized region-based level set are used to detect the NSP boundaries. Experimental results show that our proposed radial line integration method (RLI) achieves higher detection accuracy over existing methods in terms of precision, recall and F-score with less computational time.	algorithm;blob detection;f1 score;gradient;illumination (image);radial (radio);shadow volume;synthetic data;time complexity;whole earth 'lectronic link	Jierong Cheng;Wei Xiong;Shue-Ching Chia;Joo-Hwee Lim;Shvetha Sankaran;Sohail Ahmed	2014		10.1117/12.2043365	computer vision;optics;vignetting;physics;computer graphics (images)	Vision	46.95495432333	-66.77468425613552	3350
7c3e70a8dfa6bf8d980c255a50e4c4518af62369	multi-level adaptive switching filters for highly corrupted images	false alarm;miss detection;impulse noise;switching filter;adaptive median filter;noise detection;directional correlation;bdnd	This paper proposes a multi-level adaptive switching filter (MASF) for the recovery of highly corrupted images.The adaptive technique is employed in both the restoration and detection stages.Most design parameters in the proposed algorithm are self-adaptive.Several modifications on MASF are also introduced to provide better and more robust results. The performance of a switching filter is highly dependent on its detection accuracy. Inspired by adaptive median filter methodology, this paper proposes a multi-level adaptive switching filter (MASF) for the recovery of highly corrupted images. In particular, the adaptive technique is employed in all the detective, edge-preserving and restorative stages in order to improve noise discrimination and suppression simultaneously. Most critical design parameters in the proposed algorithm, e.g., the limit of window-expansion size and the noise range, are self-adaptive so as to retain simple implementation and high computational efficiency. Furthermore, several effective modifications on both stages of the MASF, such as the convergence factor, switching initialization and spatial adaptive weighting, are also introduced in order to provide better and more robust results. Monte-Carlo simulations show that the MASF outperforms many existing state-of-the-art algorithms in terms of both visual and quantitative results.	adaptive switching	Hsien-Hsin Chou;Ling-Yuan Hsu;Hwai-Tsu Hu	2015	J. Visual Communication and Image Representation	10.1016/j.jvcir.2015.05.007	adaptive filter;kernel adaptive filter;impulse noise;control theory	Vision	56.69322414155513	-66.07818012024923	3358
24f098a9309606482ac87e02f41b2404fbf56f5c	a dynamic span storing method for hidden surface removal	cognitive modeling with heuristics;hidden surface removal;machine learning;learning strategy;heuristics	A dynamic span storing method is presented to reduce the large space requirement of the conventional z-buffer algorithm for hidden surface removal, but retain the simplicity to calculate spans of polygons. The method is also applied to the conventional scan-line algorithm using span coherence, resulting in a new scan-line algorithm having a much simpler control in span manipulation. A new polygon filling scheme is also presented for span generation, which handles singularities properly in all cases.	algorithm;cache coherence;hidden surface determination;scan line;scanline rendering;z-buffering	J. Zhang	1989		10.1145/75427.75445	hidden surface determination;computer science;artificial intelligence;theoretical computer science;heuristics;machine learning;algorithm	Graphics	67.87100674573679	-49.48786329432384	3360
204145482e2106d440664178f515dd0f778f7955	intelligent control of showers in solar heating systems and gas to water economy	solar heaters and gas heaters;water savings;engineering education;humanitarian technology;water waste	The following article presents a solution to the water waste in showers heated either by solar or gas energy. Analyzing such structures, there is a considerable physical distance between those heating devices or hot water trunks and the douche. Showers that are equipped with such devices are known by a big water flow, having cold water dropped off for a period of time coming from unheated pipes or water trunks. Considering the current environmental conditions, as water saving, the amount of water dropped which is not used for bathing is a problem. The Proposition is heating up the first water flow electrically, it can be used. Assisted by the Arduino Platform, the prototype verifies the water temperature inside the shower through a sensor and controls a heating system to heat the water or not electrically. Before turning on the shower the user sets the temperature using two buttons and a LCD display. In order to guarantee the minimum energy use, the circuit keeps the heat up only if the water temperature the gets to the shower is less than the one set. Appling the prototype in large scale can end the waste caused by these systems so they can be totally sustainable.	arduino;intelligent control;prototype;water cooling	C. R. Lopes;M. R. P. Oliveira;R. E. Silva;T. V. Rodrigues;V. A. D. Souza;W. S. C. Morais	2016	2016 IEEE Global Humanitarian Technology Conference (GHTC)	10.1109/GHTC.2016.7857375	storage water heater;environmental engineering;hydrology;engineering;water cooler;waste management	Mobile	55.88329630364486	-13.784858037648538	3362
4338b6137ee0d02f4bebd14e9a2430c837590209	automatic eyeglasses removal from face images	face recognition statistical analysis visual databases monte carlo methods markov processes;pixel face detection humans eyes computer vision image resolution filling detectors statistical analysis image databases;algorithms artificial intelligence eyeglasses face female humans image enhancement image interpretation computer assisted information storage and retrieval male pattern recognition automated photography reproducibility of results sensitivity and specificity signal processing computer assisted subtraction technique user computer interface;eye region detection;eyeglasses localization;find and replace;image editing;face recognition;statistical analysis;statistical analysis automatic eyeglasses removal face images intelligent image editing face synthesis system eyeglasses detection eyeglasses localization markov chain monte carlo method;markov chain monte carlo methods;monte carlo method;image processing techniques;markov processes;intelligent image editing;monte carlo methods;approaches to learning;eyeglasses removal;visual databases	In this paper, we present an intelligent image editing and face synthesis system that automatically removes eyeglasses from an input frontal face image. Although conventional image editing tools can be used to remove eyeglasses by pixel-level editing, filling in the deleted eyeglasses region with the right content is a difficult problem. Our approach works at the object level where the eyeglasses are automatically located, removed as one piece, and the void region filled. Our system consists of three parts: eyeglasses detection, eyeglasses localization, and eyeglasses removal. First, an eye region detector, trained offline, is used to approximately locate the region of eyes, thus the region of eyeglasses. A Markov-chain Monte Carlo method is then used to accurately locate key points on the eyeglasses frame by searching for the global optimum of the posterior. Subsequently, a novel sample-based approach is used to synthesize the face image without the eyeglasses. Specifically, we adopt a statistical analysis and synthesis approach to learn the mapping between pairs of face images with and without eyeglasses from a database. Extensive experiments demonstrate that our system effectively removes eyeglasses.	detectors;experiment;eye;eyeglasses;fill;global optimization;image editing;internationalization and localization;level editor;markov chain monte carlo;monte carlo method;online and offline;pixel;the void (virtual reality);urination	Chenyu Wu;Ce Liu;Harry Shum;Ying-Qing Xu;Zhengyou Zhang	2003	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2004.1262319	facial recognition system;computer vision;speech recognition;computer science;mathematics;statistics;monte carlo method;computer graphics (images)	Vision	44.39834042711328	-48.86843575320974	3373
775d097adee2be1fe3cb87feb50d417266ca1458	wireless sensor network based e-health system: implementation and experimental results	geriatrics;data fusion wireless sensor network e health system elderly person home sensor network health monitoring multisensor data analysis;wireless sensor networks mobile communication medical services monitoring sensor systems thermal sensors;e health system;sensor systems;elektroteknik och elektronik;mobile sensors;wireless sensor networks radio active positioning radio passive positioning e health;electrical engineering electronic engineering information engineering;thermal sensors;health system;turn off;senior citizen;data fusion;sensor network;wireless sensor network;health monitoring;home sensor network;radio active positioning;data analysis;medical services;monitoring;elderly person;medical information systems;security and privacy;radio passive positioning;mobile communication;wireless sensor networks geriatrics health care home automation medical information systems patient monitoring sensor fusion;health status;multisensor data analysis;elderly people;patient monitoring;e health;sensor fusion;wireless sensor networks;everyday life;home automation;health care	With the increasing number senior citizens, E-health is targeted for home use with the special requirements of being usable in everyday life and low cost. A wireless sensor network application is proposed here for 24 hour constant monitoring without disturbing daily activities of elderly people and their caretakers. In the system proposed, both fixed and body (mobile) sensors are used. Since not every elder likes to have a sensor board attached to him/her, and in many cases, he/she may not carry the sensor; the home sensor network independently would have the ability to monitor the health status and living environment based on the multisensor data analysis and fusion. A mixed positioning algorithm is proposed to determine where the elderly person is. The purpose of the positioning is to help the system to determine the person's activities and further to make decisions about his/her health status. The system could take care of two types of the basic needs of an elderly person: everyday needs as abnormal events and emergency alarms to doctors and caretakers through telephone, SMS and e-mail, and day to day requirements such as taking of medicine, having lunch, turn off the microwave oven, and so on. At same time, the system is sensitive to security and privacy issues.	24-hour clock;algorithm;care-of address;computer security;email;microwave;privacy;requirement;sensor web	Hairong Yan;Hongwei Huo;Youzhi Xu;Mikael Gidlund	2010	IEEE Transactions on Consumer Electronics	10.1109/TCE.2010.5681102	embedded system;wireless sensor network;telecommunications;computer science;engineering;geriatrics;computer security	Mobile	7.401233999242181	-88.31061829934796	3376
45eeb327d8485e47b8e579a1a29b2e6243691100	representing local shape geometry from multi-view silhouette perspective: a distinctive and robust binary 3d feature		Abstract Local geometry description is of central importance in 3D computer vision and robotics, while the design of a distinctive and robust binary descriptor still remains a challenge at present. This paper tackles this problem by proposing to utilize the silhouette cue from multiple viewpoints to represent local shape geometry, forming a new binary feature called rotational silhouette maps (RSM). Key to the RSM descriptor are the leverage of multi-view information for comprehensive characterization paired with the silhouette method for binary and robust feature encoding. Specifically, an RSM is computed for the radius neighbors of a keypoint, at which a local reference frame (LRF) is first constructed to attain rotation invariance. Then, the local shape is rotated in the LRF multiple times to capture the multi-view information. For each view, a silhouette map is generated via projection. By concatenating all the silhouette maps, the RSM feature is computed. Extensive experiments are deployed on three standard datasets, where descriptiveness and robustness with respect to Gaussian noise, shot noise, varying mesh resolutions, the number of keypoints, keypoint localization errors, clutter and occlusion are assessed based on a comparison with nine state-of-the-art descriptors. The results reveal the overall superiority of the proposed descriptor in terms of distinctiveness and robustness.		Siwen Quan;Jie Ma;Tao Ma;Fangyu Hu;Bin Fang	2018	Sig. Proc.: Image Comm.	10.1016/j.image.2018.03.015	robustness (computer science);computer vision;shot noise;silhouette;artificial intelligence;gaussian noise;local reference frame;computer science;geometry;binary number	Vision	41.5832257693299	-55.467754624500216	3377
4a3a4626ac8ab787515dd9905aaafc75bee8931f	planar microcoils array applied to magnetic beads based lab-on-chip for high throughput applications	magnetophoresis;charge carrier processes;optimal method;microfluidics coils finite element analysis lab on a chip;coils magnetic separation charge carrier processes arrays microfluidics throughput;joule heating;high throughput particles;planer coils;chip;arrays;microcoils array;first order;magnetic separation;coils;microfluidics;lab on a chip;finite element analysis;power consumption;microfluidic structure magnetic beads high throughput application lab on chip application throughput capacity in channel planar microcoils array array scanning scheme power consumption joule heating finite element analysis software;high throughput;fea;lab on chip;magnetic bead;fea lab on chip magnetophoresis microcoils array planer coils high throughput particles;throughput	In some magnetic beads based lab-on-chip (LoC) applications, such as purification and fast cell detection, high throughput capacity is required. In this paper, we propose an optimization method for the implementation of in-channel planar microcoils array. By generating more dispersed trapping centers and exploiting the array scanning scheme, the problem of interaction among magnetic beads is controlled and both power consumption and Joule heating are reduced. Simulation results by Finite Element Analysis software show that under the first order optimization, the proposed topology saves 69% power, while keeps approximate total trapping area, compared with the conventional topologies. The microfluidic structure combining the proposed coils array and operation scheme is suitable for high throughput LoC applications.	approximation algorithm;current source;federal enterprise architecture;finite element method;joule;mathematical optimization;purification of quantum state;simulation;throughput	Yushan Zheng;Sara Bekhiche;Mohamad Sawan	2011	2011 IEEE International Symposium of Circuits and Systems (ISCAS)	10.1109/ISCAS.2011.5938073	electronic engineering;lab-on-a-chip;computer science;electrical engineering;finite element method	Arch	90.16962005179157	-16.302245466382026	3379
0f1d9434ef36e95d626aecbb2c79aa739bd7a828	a multistage approach for blind separation of convolutive speech mixtures	sound quality;traitement signal;calidad sonora;filtering;evaluation performance;filtrage;methode section divisee;qualite sonore;performance evaluation;signal estimation;separacion ciega;estimated binary mask;evaluacion prestacion;time frequency;filtrado;ideal binary mask;microfono;independent component analysis;reduccion ruido;algorithme;algorithm;blind separation;cepstral analysis;masquage;independent component analysis ica;analyse cepstrale;enmascaramiento;signal processing;noise reduction;convolutive mixtures;musical noise;estimacion senal;reduction bruit;separacion senal;analyse composante independante;separation aveugle;masking;cepstral smoothing;separation source;ideal binary mask ibm;rapport signal bruit;relacion senal ruido;analisis componente independiente;signal to noise ratio;source separation;convolutive mixture;procesamiento senal;estimation signal;microphone;algoritmo;multistage method	In this paper, we propose a novel algorithm for the separation of convolutive speech mixtures using two-microphone recordings, based on the combination of independent component analysis (ICA) and ideal binary mask (IBM), together with a post-filtering process in the cepstral domain. Essentially, the proposed algorithm consists of three steps. First, a constrained convolutive ICA algorithm is applied to separate the source signals from two-microphone recordings. In the second step, we estimate the IBM by comparing the energy of corresponding time-frequency (T-F) units from the separated sources obtained with the convolutive ICA algorithm. The last step is to reduce musical noise caused typically by T-F masking using cepstral smoothing. The performance of the proposed approach is evaluated based on both reverberant mixtures generated using a simulated room model and real recordings. The proposed algorithm offers considerably higher efficiency, together with improved speech quality while producing similar separation performance as compared with a recent approach.	algorithm;blind signal separation;cepstrum;independent computing architecture;independent component analysis;microphone;multistage amplifier;smoothing	Tariqullah Jan;Wenwu Wang;DeLiang Wang	2009	2009 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1016/j.specom.2011.01.002	filter;independent component analysis;speech recognition;time–frequency analysis;computer science;signal processing;masking;noise reduction;sound quality;signal-to-noise ratio;statistics	Robotics	81.18284286674744	-32.104679250702716	3381
54c5d0973a653738ea6ffea6457f72c611836965	an extended bayesian framework for atrial and ventricular activity separation in atrial fibrillation	electrocardiography signal to noise ratio kalman filters harmonic analysis time frequency analysis adaptation models lead	An extended nonlinear Bayesian filtering framework is introduced for the analysis of atrial fibrillation (AF), in particular with single-channel electrocardiographical (ECG) recordings. It is suitable for simultaneously tracking the fundamental frequency of atrial fibrillatory waves (f-waves), and separating signals, linked to atrial and ventricular activity, during AF. In this framework, high-power ECG components, i.e., Q, R, S, and T waves, are modeled using sum of Gaussian functions. The atrial activity dynamical model is instead based on a trigonometrical function, with a fundamental frequency (the inverse of the dominant atrial cycle length), and its harmonics. The state variables of both dynamical models (QRS-T and f-waves) are hidden and, then estimated, sample by sample, using a Kalman smoother. Remarkably, the scheme is capable of separating ventricular and atrial activity signals, while contemporarily tracking the atrial fundamental frequency in time. The proposed method was evaluated using synthetic signals. In 290 ECGs in sinus rhythm from the PhysioNet PTB Diagnostic ECG Database, the P-waves were replaced with a synthetic f-wave. Broadband noise at different signal-to-noise ratio (SNR) (from 0 to 40 dB) was added to study the performance of the filter, under different SNR conditions. The results of the study demonstrated superior results in atrial and ventricular signal separation when compared with traditional average beat subtraction (ABS), one of the most widely used method for QRS-T cancellation (normalized mean square error = 0.045 for extended Kalman smoother (EKS) and 0.18 for ABS, SNR improvement was 21.1 dB for EKS and 12.2 dB for ABS in f-wave extraction). Various advantages of the proposed method have been addressed and demonstrated, including the problem of tracking the fundamental frequency of f-waves (root mean square error (RMSE) $\approx 0.03\pm 0.005$  Hz for gradually changing frequency at SNR=15 dB) and of estimating robust QT/RR values during AF (RMSE $\approx 0.008\pm 0.001$ at SNR = 10 dB, $R^2 = 0.99$).	anisotropic filtering;atrial fibrillation;atrial premature complexes;decibel;dynamical system;estimated;heart atrium;hertz (hz);kalman filter;mean squared error;nasal sinus;nonlinear system;normal statistical distribution;rapid refresh;root mean square;signal-to-noise ratio;synthetic intelligence;ventricular fibrillation	Ebadollah Kheirati Roonizi;Roberto Sassi	2017	IEEE Journal of Biomedical and Health Informatics	10.1109/JBHI.2016.2625338	pattern recognition;atrial fibrillation;artificial intelligence;computer science;bayesian probability	Vision	20.52287002157807	-89.82210652545973	3384
e5a47a5d72cf5e0def139413507e04691d212cb9	interactive detection of 3d models of building's roofing for the estimation of the solar energy potential	building integrated photovoltaics;interactive systems;power engineering computing;3d modelling;3d models interactive detection;aerial photographs;building roofing;building selection;cartographic map;disparity computation;disparity measurement algorithms;image segmentation;interactive system;solar energy potential estimation;standard software packages	The paper presents the work in progress of the design and implementation of an interactive system for the detection of the building's roofing characteristic. For each of its pitches data concerning height, shape, orientation, slope and useful area are estimated at different precision levels. The system operates on a cartographic map and two pre-processed aerial photographs that are aligned for building selection, image segmentation and 3D modelling. Each building roofing is automatically classified and its features are used for disparity computation from two stereoscopic views for precise 3D modelling. Different disparity measurement algorithms are being experimented to measure their accuracy based on reference test buildings. The 3D model is the input to standard software packages for solar energy potential estimation.	3d modeling;aerial photography;algorithm;binocular disparity;cartography;computation;epipolar geometry;hough transform;image segmentation;interactivity;signal-to-noise ratio;stereoscopy	S. Brofferio;D. Passoni;M. E. Gaetani;M. Spertino	2006	2006 14th European Signal Processing Conference		computer vision;simulation;engineering;engineering drawing	Vision	57.5252923726901	-47.931824265449805	3385
12caec65e939cd0233ceb97fcb5b8674168f0882	navigation and mapping in large unstructured environments	autonomous vehicle;path planning;occupancy grid;kalman filter;data association;multi dimensional;obstacle avoidance;autonomous navigation	In this paper we address the problem of autonomous navigation in very large unstructured environments. A new hybrid metric map (HYMM) structure is presented that combines feature maps with other metric representations in a consistent manner. The global feature map is partitioned into a set of connected local triangular regions (LTRs), which provide a reference for a detailed multidimensional description of the environment. The HYMM framework permits the combination of efficient feature-based simultaneous localization and mapping (SLAM) algorithms for localization with, for example, occupancy grid maps for tasks such as obstacle avoidance, path planning or data association. This fusion of feature and grid maps has several complementary properties; for example, grid maps can assist data association and can facilitate the extraction and incorporation of new landmarks as they become identified from multiple vantage points. In this paper we also present a path-planning technique that efficiently maintains the estimated cost of traversing each LTR. The consistency of the SLAM algorithm is investigated with the introduction of exploration techniques to guarantee a certain measure of performance for the estimation process. Experimental results in outdoor environments are presented to demonstrate the performance of the algorithms proposed. KEY WORDS—AUTHOR: PLEASE PROVIDE The International Journal of Robotics Research Vol. 23, No. 4, April 2004, pp. xxx-xxx, DOI: 10.1177/0278364904042203 ©2004 Sage Publications	algorithm;autonomous robot;correspondence problem;map;motion planning;obstacle avoidance;robotics;simultaneous localization and mapping;titcoin	José E. Guivant;Eduardo Mario Nebot;Juan I. Nieto;Favio R. Masson	2004	I. J. Robotics Res.	10.1177/0278364904042203	kalman filter;computer vision;simulation;computer science;artificial intelligence;motion planning;transport engineering;obstacle avoidance	Robotics	52.155695347960425	-37.7177283300088	3388
6595e60db384301b29bf54228fea4a1ebf844244	the generalization performance of learning machine with na dependent sequence	covering number;generalization performance;expected error;bound;rough set theory;relacion convergencia;sampling error;convergencia uniforme;taux convergence;intelligence artificielle;convergence rate;negative association;sequence learning;convergence uniforme;uniform convergence;machine learning;theorie ensemble approximatif;theoretical analysis;na sequence;empirical error;artificial intelligence;sample error;inteligencia artificial;learning machine;erm	The generalization performance is the main purpose of machine learning theoretical research. This note mainly focuses on a theoretical analysis of learning machine with negatively associated dependent input sequence. The explicit bound on the rate of uniform convergence of the empirical errors to their expected error based on negatively associated dependent input sequence is obtained by the inequality of Joag-dev and Proschan. The uniform convergence approach is used to estimate the convergence rate of the sample error of learning machine that minimize empirical risk with negatively associated dependent input sequence. In the end, we compare these bounds with previous results		Bin Zou;Luoqing Li;Jie Xu	2006		10.1007/11795131_82	sampling error;sequence learning;computer science;artificial intelligence;machine learning;mathematics;stability;statistics;generalization error	AI	20.09079065153871	-30.81226413751606	3392
5af9f27cd7316990b82e5097d655972293ba08d5	indication of slowly moving ground targets in non-gaussian clutter using multi-channel synthetic aperture radar	slowly moving ground vehicle;generalised eigensystem theory;optimal solution;nongaussian background clutter;stationary background urban clutter image intensity suppression;standard space time adaptive processing algorithm;complex valued weights;optimum steering vector;coherence matrices;slowly moving ground target indication;correlation parameters;synthetic aperture radar adaptive signal processing discrete fourier transforms eigenvalues and eigenfunctions filtering theory optimisation radar clutter radar imaging;quadratic form ratio;discrete fourier transform;parameter space;slowly moving ground vehicle slowly moving ground target indication multichannel synthetic aperture radar images target to clutter ratio direct maximisation nongaussian background clutter complex valued weights generalised eigensystem theory quadratic form ratio coherence matrices optimum steering vector discrete fourier transform optimal solution parameter space correlation parameters standard space time adaptive processing algorithm stationary background urban clutter image intensity suppression;target to clutter ratio direct maximisation;multichannel synthetic aperture radar images	The problem of how best to maximise the ratio of mean target intensity to mean background intensity for slowly moving targets in sets of multi-channel synthetic aperture radar images is discussed for highly non-Gaussian background clutter. The problem is formulated as a direct maximisation of target-to-clutter ratio thus giving a true maximisation of that ratio. Complex-valued weights derived using generalised eigensystem theory are used to maximise the ratio of quadratic forms representing the mean intensity of the target and background derived from their coherence matrices. For two to four channels it is shown that when the target is highly coherent an optimum steering vector is a discrete Fourier transform. For more than two channels it is shown that the optimal solution is only valid within a subspace of the whole parameter space defined by the correlation parameters of the background clutter. Images from a publically released ground moving target indicator dataset are filtered using the results for three channels. The method outperform a standard space-time adaptive processing algorithm in suppressing the stationary background urban clutter image intensity relative to the image intensity because of a known slowly moving ground vehicle. Moreover, the steering vector is much simpler to implement.		Brian Barber;Jon Barker	2012	IET Signal Processing	10.1049/iet-spr.2011.0157	stationary target indication;electronic engineering;moving target indication;discrete fourier transform;control theory;mathematics;parameter space;statistics	Robotics	74.05082103338594	-67.39008448989021	3394
882c8c97e240a9a3fb27fcf6d67b4ff7ad2a72b0	event-based optimization with non-stationary uncertainties to save energy costs of hvac systems in buildings	energy conservation;human comfort event based optimization nonstationary uncertainties energy costs savings hvac systems lagrangian relaxation framework time dependent variables events definition time independent events time independent actions event based policy q learning method energy savings;cost reduction;hvac building management systems cost reduction energy conservation;hvac;building management systems;event based optimization hvac system lagrangian relaxation price based coordination;q factor cooling optimization buildings uncertainty meteorology steady state	Building accounts for nearly 40% of global energy consumption, and about 40% of that is consumed by HVAC systems. A typical way of saving HVAC energy cost is to formulate and solve the HVAC operation problem which minimizes the HVAC energy cost in 24 hours ahead. Traditionally, the problem is solved by using time-based approaches where decisions are calculated and executed at each discrete time instant. In this paper, an innovative event-based approach is developed in the Lagrangian relaxation framework so that the decisions are only calculated and executed on an “as needed” basis to reduce computational requirements and extend device lifetimes. Developing such an event-based approach is challenging since with a finite time horizon of 24 hours and non-stationary uncertainties in weather, cooling load, etc., there is no steady-state solution. Events and actions are therefore time-dependent, causing the policy space to be extremely large. Our key idea to overcome this difficulty is to 1) include time-dependent variables that affect decisions in the definition of events so that events and actions will become time-independent and the size of event-based policy will be reduced significantly; and 2) develop a Q-learning method based on events within the Lagrangian relaxation framework to obtain the optimal actions. Numerical results demonstrate significant reductions of computational efforts as compared with time-based approaches with similar levels of energy savings and human comfort.	computation;computer cooling;lagrangian relaxation;linear programming relaxation;mathematical optimization;numerical method;optimization problem;q-learning;requirement;stationary process;steady state;time complexity	Biao Sun;Peter B. Luh;Qing-Shan Jia;Bing Yan	2013	2013 IEEE International Conference on Automation Science and Engineering (CASE)	10.1109/CoASE.2013.6654055	mathematical optimization;simulation;engineering;operations management	Robotics	3.864411846584097	3.9070103227213164	3395
1c1f957d85b59d23163583c421755869f248ceef	robust facial landmark detection under significant head poses and occlusion	shape face robustness predictive models mathematical model estimation;regression analysis face recognition performance evaluation pose estimation probability;local shapes robust facial landmark detection head poses in the wild images unified robust cascade regression framework landmark occlusion prediction landmark location prediction occlusion estimation binary occlusion vector prediction supervised regression method landmark visibility probabilities occlusion pattern performance improvement local appearances	"""There have been tremendous improvements for facial landmark detection on general """"in-the-wild"""" images. However, it is still challenging to detect the facial landmarks on images with severe occlusion and images with large head poses (e.g. profile face). In fact, the existing algorithms usually can only handle one of them. In this work, we propose a unified robust cascade regression framework that can handle both images with severe occlusion and images with large head poses. Specifically, the method iteratively predicts the landmark occlusions and the landmark locations. For occlusion estimation, instead of directly predicting the binary occlusion vectors, we introduce a supervised regression method that gradually updates the landmark visibility probabilities in each iteration to achieve robustness. In addition, we explicitly add occlusion pattern as a constraint to improve the performance of occlusion prediction. For landmark detection, we combine the landmark visibility probabilities, the local appearances, and the local shapes to iteratively update their positions. The experimental results show that the proposed method is significantly better than state-of-the-art works on images with severe occlusion and images with large head poses. It is also comparable to other methods on general """"in-the-wild"""" images."""	algorithm;handel;image resolution;iteration;unified model	Yue Wu;Qiang Ji	2015	2015 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2015.417	computer vision;speech recognition;pattern recognition	Vision	30.365161297267058	-49.368093863122255	3398
ceee0723a3aacf006a84081bee0afafedde0aba4	ledaps: mapping north american disturbance from the landsat record	landsat;tratamiento datos;carbon;vegetation mapping;teledetection spatiale;carbono;change detection;forestry;space remote sensing;atmospheric correction;north america;atmospheric composition;america del norte;amerique du nord;proyecto;pouvoir reflecteur;thematic mapper;poder reflector;data processing;imagerie;traitement donnee;north american;time series;forest disturbance;modaps;tm;algorithme;carbone;teledeteccion espacial;imagery;analyse serie temporelle;reflectance;ecosysteme;time series analysis;ecosistema;ecosystems;teledeteccion multiespectral;remote sensing;carbon compounds;multispectral remote sensing;satellites remote sensing reflectivity ecosystems adaptive systems north america atmospheric modeling detection algorithms data mining frequency;correction atmospherique;algorithms;etm;atmospheric techniques;imagineria;projet;carbon compounds forestry vegetation mapping atmospheric techniques atmospheric composition;ad 2004 ledaps project landsat ecosystem disturbance adaptive processing system forest disturbance regrowth mapping north america landsat satellite imagery carbon modeling activity decadal landsat geocover data image time series atmospheric correction change detection algorithm modis land processing system architecture moderate resolution imaging spectroradiometer modaps architecture modis adaptive processing system mss multispectral scanner tm thematic mapper etm enhanced tm beta surface reflectance product;teledetection multispectrale;projects;ledaps;mss;extraction;algoritmo	"""The Landsat Ecosystem Disturbance Adaptive Processing System (LEDAPS) project is creating a record of forest disturbance and regrowth for North America from the Landsat satellite record, in support of the carbon modeling activities. LEDAPS relies on the decadal Landsat GeoCover data set supplemented by dense image time series for selected locations. Imagery is first atmospherically corrected to surface reflectance, and then change detection algorithms are used to extract disturbance area, type, and frequency. Reuse of the MODIS Land processing system (MODAPS) architecture allows rapid throughput of over 1500 MSS, TM, and ETM+ scenes. Initial (""""Beta"""") surface reflectance products are currently available for testing, and initial disturbance products will be available by the end of 2004."""	algorithm;ecosystem;throughput;time series	Robert E. Wolfe;Jeffrey G. Masek;Nazmi El Saleous;Forrest G. Hall	2004	IGARSS 2004. 2004 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2004.1368929	data processing;hydrology;time series;statistics;remote sensing	Embedded	80.88998178316608	-59.86284309935304	3406
3c97c32ff575989ef2869f86d89c63005fc11ba9	face detection with the faster r-cnn		While deep learning based methods for generic object detection have improved rapidly in the last two years, most approaches to face detection are still based on the R-CNN framework [11], leading to limited accuracy and processing speed. In this paper, we investigate applying the Faster RCNN [26], which has recently demonstrated impressive results on various object detection benchmarks, to face detection. By training a Faster R-CNN model on the large scale WIDER face dataset [34], we report state-of-the-art results on the WIDER test set as well as two other widely used face detection benchmarks, FDDB and the recently released IJB-A.	authorization;benchmark (computing);deep learning;face detection;object detection;reverse polish notation;test set	Huaizu Jiang;Erik G. Learned-Miller	2017	2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)	10.1109/FG.2017.82	simulation;computer science;artificial intelligence;machine learning	Vision	28.807199125265097	-53.95655851163714	3410
3203be404119ad5a192c5bd0ea4e957187d6158e	visual simulation of shockwaves	estensibilidad;finite volume method;parallel algorithm;shared memory;physics based modeling;explosion;finite volume methods;computer graphics;memoria compartida;architecture memoire;processeur multicoeur;non viscous fluid;visual simulation;paralelisacion;procesador multinucleo;pam sonoro;nonlinear acoustics;bang sonique;algorithme parallele;modelisation;fluide non visqueux;methode volume fini;memory architecture;parallelisation;aeroacoustique;animation;fluide compressible;parallelization;compressible fluid;aeroacoustics;acoustique non lineaire;explosions;multicore processor;extensibilite;scalability;turbulent flow;fluid simulation;fluido compresible;sonic boom;computer animation;ecoulement turbulent;shock waves;shockwaves;onde choc;infographie;physically based modeling;memoire partagee;fluido no viscoso;parallel algorithms;animation par ordinateur	We present an efficient method for visual simulations of shock phenomena in compressible, inviscid fluids. Our algorithm is derived from one class of the finite volume method especially designed for capturing shock propagation, but offers improved efficiency through physically-based simplification and adaptation for graphical rendering. Our technique is well suited for parallel implementation on multicore architectures and is also capable of handling complex, bidirectional object-shock interactions stably and robustly. We describe its applications to various visual effects, including explosion, sonic booms and turbulent flows.	algorithm;augmented reality;finite volume method;graphical user interface;interaction;level of detail;multi-core processor;simulation;software propagation;turbulence;visual effects	Jason Sewall;Nico Galoppo;Georgi Tsankov;Ming C. Lin	2008		10.1016/j.gmod.2009.03.002	simulation;computer science;parallel algorithm;algorithm;computer graphics (images)	Graphics	71.29731006885183	-48.57980235494629	3411
d56c722ca02fe32034df38ef8a01597a19041d3d	face recognition from 2d and 3d images using 3d gabor filters	range data;3d imaging;rotation invariant;gabor filter;rotation invariance;face recognition;hausdorff distance;least trimmed squares;facial expression;gabor wavelets;3d gabor filter;least trimmed square hausdorff distance;3d spherical gabor filter;invariant feature	To recognize faces with different facial expressions or varying views from only one stored prototype per person is challenging. This paper presents such a system based on both 3D range data as well as the corresponding 2D gray-level facial images. The traditional 3D Gabor filter (3D TGF) is explored in the face recognition domain to extract expression-invariant features. To extract view-invariant features, a rotation-invariant 3D spherical Gabor filter (3D SGF) is proposed. Furthermore, a two-dimensional (2D) Gabor histogram is employed to represent the Gabor responses of the 3D SGF for solving the missing-point problem caused by self-occlusions under large rotation angles. The choice of 3D Gabor filter parameters for face recognition is examined as well. To match a given test face with each model face, the Least Trimmed Square Hausdorff Distance (LTS-HD) is employed to tackle the possible partial-matching problem. Experimental results based on our face database involving 80 persons have demonstrated that our approach outperforms the standard Eigenface approach and the approach using the 2D Gabor-wavelets representation.	3d film;facial recognition system	Yingjie Wang;Chin-Seng Chua	2005	Image Vision Comput.	10.1016/j.imavis.2005.07.005	facial recognition system;gabor transform;stereoscopy;hausdorff distance;computer vision;speech recognition;least trimmed squares;pattern recognition;mathematics;facial expression;gabor wavelet;statistics	Vision	40.150247819114256	-57.44471124435443	3416
21af6ed44421803d8277a254a61ce62c928993c7	a two-stage corrective markov model for activities of daily living detection		In this paper we propose a two-stage, supervised statistical model for detecting the activities of daily living (ADL) from sensor data streams. In the first stage each activity is modeled separately by a Markov model where sensors correspond to states. By modeling each sensor as a state we capture the absolute and relational temporal features of the atomic activities. A novel data segmentation approach is proposed for accurate inferencing at the first stage. To boost the accuracy, a second stage consisting of a Hidden Markov Model is added that serves two purposes. Firstly, it acts as a corrective stage, as it learns the probability of each activity being incorrectly inferred by the first stage, so that they can be corrected at the second stage. Secondly, it introduces inter-activity transition information to capture possible time-dependent relationships between two contiguous activities. We applied our method to three ADL datasets to show its suitability to this domain.	experiment;heart rate variability;hidden markov model;image noise;markov chain;sensor;statistical model;supervised learning;test case	Love Kalra;Xinghui Zhao;Axel J. Soto;Evangelos E. Milios	2012		10.1007/978-3-642-28783-1_21	simulation	AI	22.143463584641243	-57.295837070056464	3418
fa410df01627ae0c66d66d8aa7bda4c8bd07c7ba	evaluation of internal carotid artery segmentation by insightsnap	level sets;arteries;automatic segmentation;level set;cervical carotid;scientific discovery;clinical decision making;atherosclerosis;segmentation;medical image;internal carotid artery;blood;radiometric corrections;open source	Quantification of cervical carotid geometry may facilitate improved clinical decision making and scientific discovery. We set out to evaluate the ability of InsightSNAP (ITK-SNAP), an open-source segmentation program for 3D medical images (http://www.itksnap.org, version 1.4), to semi-automatically segment internal carotid arteries. A sample of five individuals (three normal volunteers, and two diseased patients) were imaged with an MR exam consisting of a MOTSA TOF MRA image volume and multiple black blood images acquired with different contrast weightings. Comparisons were made to a manual segmentation created during simultaneous evaluation of the MOTSA image and the various black blood images (typically PD-weighted, T1-weighted, and T2-weighted). These individuals were selected as a training set to determine acceptable parameters for ITK-SNAP's semi-automatic level sets segmentation method. The conclusion from this training set was that the initial thresholding (assigning probabilities to the intensities of image pixels) in the image pre-processing step was most important to obtaining an acceptable segmentation. Unfortunately no consistent trends emerged in how this threshold should be chosen. Figures of percent over- and under-segmentation were computed as a means of comparing the hand segmented and semi-automatically segmented internal carotids. Overall the under-segmentation by ITK-SNAP (voxels included in the manual segmentation but not in the semiautomated segmentation) was 10.94% ± 6.35% while the over-segmentation (voxels excluded in the manual segmentation but included in the semi-automated segmentation) was 8.16% ± 4.40% defined by reference to the total number of voxels included in the manual segmentation.© (2007) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.		Emily L. Spangler;Christopher Brown;John A. Roberts;Brian E. Chapman	2007		10.1117/12.709954	computer vision;radiology;medicine;pathology	Robotics	38.93756435025446	-80.6526303155393	3421
62546d4fa8663a087a3a7fe068c806d0e4610cf8	who needs qp for linear mpc anyway?	optimal solution;quadratic programming;predictive control;invariant set;quadratic program;constraint optimization;constrained control;model matching;optimization;invariant sets;constraints	Conventional MPC uses quadratic programming (QP) to minimise, on-line, a cost over n linearly constrained control moves. However, stability constraints often require the use of large n thereby increasing the on-line computation, rendering the approach impracticable in the case of fast sampling. Here, we explore an alternative that requires a fraction of the computational cost (which increases only linearly with n), and propose an extension which, in all but a small class of models, matches to within a fraction of a percent point the performance of the optimal solution obtained through QP. The provocative title of the paper is intended to point out that the proposed approach o9ers a very attractive alternative to QP-based MPC. ? 2002 Published by Elsevier Science Ltd.	algorithmic efficiency;approximation algorithm;computation;converge;iteration;mathematical optimization;monte carlo method;newton's method;online and offline;quadratic programming;sampling (signal processing);simulation	Basil Kouvaritakis;Mark Cannon;J. Anthony Rossiter	2002	Automatica	10.1016/S0005-1098(01)00263-1	control engineering;mathematical optimization;control theory;mathematics;quadratic programming;model predictive control	ML	61.3830930469216	-1.190507877483753	3423
3bdf45df571e9be08391885133576c7bc98df542	quantum filtering equations for a system driven by nonclassical fields		Using Gardiner and Collet’s input-output model and the concept of cascade system, we determine the filtering equation for a quantum system driven by light in some specific nonclassical states. The quantum system and electromagnetic field are described by making use of quantum stochastic unitary evolution. We consider two examples of the nonclassical states of field: a combination of vacuum and single photon states and a mixture of two coherent states. The stochastic evolution conditioned on the results of the photon counting and quadrature measurements is described.	quantum	Anita Dabrowska	2018	Open Syst. Inform. Dynam.	10.1142/S1230161218500075	quantum mechanics;photon counting;quantum stochastic calculus;filter (signal processing);photon;quantum system;coherent states;electromagnetic field;mathematics;quantum	Robotics	88.90527003672169	-0.1063974558609326	3426
f607777cfd95fb0991cbeb0f216df320f5ddea2b	biped walking stabilization based on gait analysis	humanoid robot;motion control;legged locomotion;foot;wabian 2r biped walking stabilization control gait analysis biped humanoid robot human like foot mechanism mimicking medial longitudinal arch foot arch structure arch function force disturbance stepping strategy human walking strategy foot landing point com mechanical energy;joints;stability;mechanical energy;robot vision;humanoid robots;gait analysis;humans;stability humanoid robots legged locomotion motion control robot vision;legged locomotion foot mechanical energy humans educational institutions joints	This paper describes a walking stabilization control based on gait analysis for a biped humanoid robot. We have developed a human-like foot mechanism mimicking the medial longitudinal arch to clarify the function of the foot arch structure. To evaluate the arch function through walking experiments using a robot, a walking stabilization control should also be designed based on gait analysis. Physiologists suggest the ankle, hip and stepping strategies, but these strategies are proposed by measuring human beings who are not “walking” but “standing” against force disturbances. Therefore, first we conducted gait analysis in this study, and we modeled human walking strategy enough to be implemented on humanoid robots. We obtained following two findings from gait analysis: i) a foot-landing point exists on the line joining the stance leg and the projected point of CoM on the ground, and ii) the distance between steps is modified to keep mechanical energy at the landing within a certain value. A walking stabilization control is designed based on the gait analysis. Verification of the proposed control is conducted through experiments with a human-sized humanoid robot WABIAN-2R. The experimental videos are supplemented.	experiment;gait analysis;humanoid robot;medial graph;motion capture;sensor;shoes;stepping level;the walking dead: season two;wearable computer	Kenji Hashimoto;Yuki Takezaki;Hiromitsu Motohashi;Takuya Otani;Tatsuhiro Kishi;Hun-ok Lim;Atsuo Takanishi	2012	2012 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2012.6225253	control engineering;simulation;computer science;engineering;humanoid robot;artificial intelligence;control theory	Robotics	67.33996646743256	-24.51111591599197	3428
320239c2facc9425c1d7ca5fe38b5656f4e0b503	kolmogorov complexity of finite sequences and recognition of different preictal eeg patterns	data recording;sequences;pattern recognition electroencephalography epilepsy fractals yield estimation limit cycles chaos state estimation electrodes surgery;fractals;data compression;nonlinear properties;data string compressibility;chaos;intracranial eeg recordings kolmogorov complexity finite sequences preictal eeg patterns quantitative interpretation epilepsy ictal segments postictal segments data complexity algorithmic information content data string compressibility linear properties nonlinear properties dynamical system;algorithmic information content;dynamic system;yield estimation;state estimation;intracranial eeg recordings;information content;quantitative interpretation;dynamical system;electrodes;ictal segments;preictal eeg patterns;intracranial eeg;kolmogorov complexity;limit cycles;computational complexity;surgery;data complexity;postictal segments;pattern recognition;medical signal processing electroencephalography sequences data recording computational complexity data compression pattern recognition;linear properties;electroencephalography;finite sequences;medical signal processing;epilepsy	The problem of an adequate quantitative interpretation of epileptic EEG recordings is of great importance in the understanding, recognition and treatment of epilepsy. In recent years, much effort has been made to develop computerized methods which can characterize different interictal, ictal and postictal stages. The main issue of whether there exist a preictal phenomenon is unresolved. In this paper, we address this issue making use of the most basic representation of data complexity, namely the algorithmic information content. In general this measure, also known as Kolmogorov complexity, represents the compressibility of the data strings. It can also be used to describe properties (linear and nonlinear) of the underlying dynamical system. We analyze Kolmogorov complexity and related characteristics of intracranial EEG recordings containing preictal, ictal and postictal segments. >	electroencephalography;kolmogorov complexity	Arthur Petrosian	1995		10.1109/CBMS.1995.465426	theoretical computer science;dynamical system;machine learning;statistics	Crypto	19.838036906137486	-86.51891445681409	3430
814bbcda60b333268dcb191e9a36e66a286e6513	probabilistic multihypothesis trackerwith an evolving poisson prior	target tracking hysteresis standards joints complexity theory sensors clutter;clutter;complexity theory;standards;hysteresis;sensors;joints;linear complexity probabilistic multihypothesis tracker poisson prior multitarget tracking algorithm conditional independence assumption measurement model data association prior track quality measure track management decision pmht markov chain hyperparameter exponential complexity poisson distribution poisson model parameter;target tracking;target tracking computational complexity markov processes poisson distribution sensor fusion	The probabilistic multihypothesis tracker (PMHT) is an efficient multitarget tracking algorithm that performs data association under a conditional independence assumption. A key part of the measurement model is the data-association prior, which can be used as a track quality measure for track management decisions. The original PMHT makes this prior an unknown fixed parameter. The PMHT with hysteresis extended the measurement model by adding a Markov chain hyperparameter to the prior, but this came at the cost of exponential complexity in the number of targets. This complexity comes as a consequence of the normalization of the prior. This article shows that the PMHT data-association model is equivalent to assuming that targets create a Poisson-distributed number of measurements; an alternative PMHT is derived that deals directly with the Poisson model parameters and retains linear complexity in the number of targets.	algorithm;clutter;computation;computational complexity theory;correspondence problem;hysteresis;markov chain;monte carlo method;simulation;smoothing;time complexity	Sam Davey	2015	IEEE Transactions on Aerospace and Electronic Systems	10.1109/TAES.2014.120633	hysteresis;sensor;machine learning;pattern recognition;control theory;mathematics;clutter;physics;statistics	Vision	38.91989874449181	-26.699805154423505	3432
41f53301f390411df633d23deb644526568ad6f1	agnostic boosting	agnostic learning;realistic setting;minimal error;distribution p;weak agnostic learner;hypothesis class f;efficient algorithm;hypothesis h;agnostic boosting;probability distribution;weak agnostic learning	We extend the boosting paradigm to the realistic setting of agnostic learning, that is, to a setting where the training sample is generated by an arbitrary (unknown) probability distribution over examples and labels. We deene a-weak agnostic learner with respect to a hypothesis class F as follows: given a distribution P it outputs some hypothesis h 2 F whose error is at most erP(F) + , where erP (F) is the minimal error of an hypothesis from F under the distribution P (note that for some distributions the bound may exceed a half). We show a boosting algorithm that using the weak agnostic learner computes a hypothesis whose error is at most maxfc1()er(F) c 2 () ; g, in time polynomial in 1==. While this generalization guarantee is significantly weaker than the one resulting from the known PAC boosting algorithms, one should note that the assumption required for-weak agnostic learner is much weaker. In fact, an important virtue of the notion of weak agnostic learning is that in many cases such learning is achieved by eecient algorithms.	approximation algorithm;boosting (machine learning);polynomial;probabilistic turing machine;probably approximately correct learning;programming paradigm	Shai Ben-David;Philip M. Long;Yishay Mansour	2001		10.1007/3-540-44581-1_33	artificial intelligence;machine learning;mathematics;algorithm;statistics	ML	19.86456045582069	-32.875029064125755	3434
9aba3bc2489f997a1a1abe45fb084bf54841a8dd	imu-based virtual road profile sensor for vehicle localization	inertial sensor signals;localization;road profile estimation;unknown input kalman filter;virtual measurement	A road profile can be a good reference feature for vehicle localization when a Global Positioning System signal is unavailable. However, cost effective and compact devices measuring road profiles are not available for production vehicles. This paper presents a longitudinal road profile estimation method as a virtual sensor for vehicle localization without using bulky and expensive sensor systems. An inertial measurement unit installed in the vehicle provides filtered signals of the vehicle's responses to the longitudinal road profile. A disturbance observer was designed to extract the characteristic features of the road profile from the signals measured by the inertial measurement unit. Design synthesis based on a Kalman filter was used for the observer design. A nonlinear damper is explicitly considered to improve the estimation accuracy. Virtual measurement signals are introduced for observability. The suggested methodology estimates the road profile that is sufficiently accurate for localization. Based on the estimated longitudinal road profile, we generated spectrogram plots as the features for localization. The localization is realized by matching the spectrogram plot with pre-indexed plots. The localization using the estimated road profile shows a few meters accuracy, suggesting a possible road profile estimation method as an alternative sensor for vehicle localization.	backup;biological science disciplines;conflict (psychology);damper device component;drug vehicle;estimated;global positioning system;image registration;index;internationalization and localization;kalman filter;matching;nonlinear system;repeatability;sensor;spectrogram;suspensions;tree accumulation;sensor (device)	Juhui Gim;Changsun Ahn	2018		10.3390/s18103344	electronic engineering;engineering;inertial measurement unit;control engineering	Robotics	56.62547138251677	-36.65797146840257	3435
cc87be03710f435c2a8eab3fafd62750d32f5a31	solving large-scale 0-1 knapsack problem by the social-spider optimisation algorithm		This paper uses the social-spider optimisation (SSO) algorithm to solve large-scale 0-1 knapsack problems. The SSO algorithm is based on the simulation of cooperative behaviour of social-spiders. In SSO algorithm, individuals emulate a group of spiders which interact to each other based on the biological laws of the cooperative colony. The algorithm considers two different search agents (spiders): males and females. Depending on gender, each individual is conducted by a set of different evolutionary operators which mimic different cooperative behaviour which are typically found in the colony. The experiment results show that the social-spider optimisation algorithm can be an efficient alternative for large-scale 0-1 knapsack problems.	algorithm;knapsack problem;mathematical optimization	Guo Zhou;Ruixin Zhao;Yongquan Zhou	2018	IJCSM	10.1504/IJCSM.2018.10016491	mathematical optimization;mathematics;operator (computer programming);knapsack problem;social spider;algorithm	Vision	25.556637278609223	-5.664661414002538	3437
92a8d2e667452242df82d82fcd70eb5e1fee597e	distributionally robust optimization with applications to risk management		Many decision problems can be formulated as mathematical optimization models. While deterministic optimization problems include only known parameters, real-life decision problems almost invariably involve parameters that are subject to uncertainty. Failure to take this uncertainty under consideration may yield decisions which can lead to unexpected or even catastrophic results if certain scenarios are realized. While stochastic programming is a sound approach to decision making under uncertainty, it assumes that the decision maker has complete knowledge about the probability distribution that governs the uncertain parameters. This assumption is usually unjustified as, for most realistic problems, the probability distribution must be estimated from historical data and is therefore itself uncertain. Failure to take this distributional modeling risk into account can result in unduly optimistic risk assessment and suboptimal decisions. Furthermore, for most distributions, stochastic programs involving chance constraints cannot be solved using polynomial-time algorithms. In contrast to stochastic programming, distributionally robust optimization explicitly accounts for distributional uncertainty. In this framework, it is assumed that the decision maker has access to only partial distributional information, such as the firstand second-order moments as well as the support. Subsequently, the problem is solved under the worst-case distribution that complies with this partial information. This worst-case approach effectively immunizes the problem against distributional modeling risk. The objective of this thesis is to investigate how robust optimization techniques can be used for quantitative risk management. In particular, we study how the risk of large-scale derivative portfolios can be computed as well as minimized, while making minimal assumptions about the probability distribution of the underlying asset returns. Our interest in derivative portfolios stems from the fact that careless investment in derivatives can yield large losses or even bankruptcy. We show that by employing robust optimization techniques we are able to capture the substantial risks involved in derivative investments. Furthermore, we investigate how distributionally robust chance constrained programs can be reformulated or approximated as tractable optimization problems. Throughout the thesis, we aim to derive tractable models that are scalable to industrial-size problems.	approximation algorithm;best, worst and average case;cobham's thesis;decision problem;decision theory;mathematical optimization;real life;risk assessment;risk management;robust optimization;scalability;stationary process;stochastic programming;time complexity	Steve Zymler	2010			robust optimization;risk management;management science;engineering	ML	5.072142462982154	-4.7679054397660305	3438
feb132e2ef8707175365f74df362c7f119106e96	theoretical calculations on c30h12 bowl-shaped hydrocarbons: nmr shielding constants, stability, and aromaticity	aromaticity;fullerene;ab initio calculations;dft;nmr	Ž . ABSTRACT: Density Functional Theory DFT calculations at the B3LYPr631G level have been performed on four bowl-shaped polyaromatic Ž . Ž . Ž . hydrocarbons of C H molecular formula 1]4 showing C 1 , C 2 and 4 , 30 12 3 2 v Ž . and C 3 symmetries. The geometrical and electronic properties of the 2 h compounds studied have been analyzed to explain their relative stability. NMR chemical shifts parameters for the atoms and Nucleus Independent Chemical Ž . 13 Shifts NICSs for the rings were calculated using the GIAO method. The C and H chemical shifts calculated are in very good agreement with the experimental data. Q 1999 John Wiley & Sons, Inc. J Comput Chem 20: 1412]1421, 1999	density functional theory;john d. wiley;q-chem	Santiago Melchor Ferrer;José Molina Molina	1999	Journal of Computational Chemistry	10.1002/(SICI)1096-987X(199910)20:13%3C1412::AID-JCC8%3E3.0.CO;2-F	stereochemistry;chemistry;fullerene;organic chemistry;discrete fourier transform;computational chemistry;ab initio quantum chemistry methods;aromaticity	Theory	97.29336215782006	-5.00063184124165	3442
65cb83957fa76d26fce2d1cab5c98e41c5839e1c	invariant grey-scale features for 3d sensor-data	object recognition;transformation group;geometry pattern recognition biomedical imaging robustness image processing computer science image recognition image motion analysis lubricating oils image segmentation;geometry;medical image analysis;computational complexity;topological deformations invariant grey scale features 3d sensor data transformation group 3d objects 3d euclidean motion rigid objects articulated objects;monte carlo methods object recognition geometry computational complexity;monte carlo methods;invariant feature	In this paper a technique for the construction of invariant features of 3D sensor-data is proposed. Invariant greyscale features are characteristics of grey-scale sensor-d ata which remain constant if the sensor-data is transformed according to the action of a transformation group. The proposed features are capable of recognizing 3D objects independent of their orientation and position, which can be used e. g. in medical image analysis. The computation of the proposed invariants needs no preprocessing like filtering, segmentation, or registration. After the introductio n f the general theory for the construction of invariant featur es for 3D sensor-data, the paper focuses on the special case of 3D Euclidean motion which is typical for rigid 3D objects. Due to the fact that we use functions of local support the calculated invariants are also robust with respect to in dependent Euclidean motion, articulated objects, and even topological deformations. The complexity of the method is linear in the data-set size which may be to high for large 3D objects. Therefore approaches for the acceleration of the computation are given. First experimental results for artificial 3D objects are presented in the paper to demonstrate the invariant properties of the proposed features.	3d computer graphics;3d scanner;algorithm;computation;connectionism;cosmographiae introductio;distributed computing;experiment;grayscale;image analysis;image retrieval;image segmentation;medical image computing;medical imaging;nonlinear system;preprocessor;resonance;sampling (signal processing)	Marc Schael;Sven Siggelkow	2000		10.1109/ICPR.2000.906129	computer vision;topology;cognitive neuroscience of visual object recognition;mathematics;geometry;3d single-object recognition;computational complexity theory;monte carlo method	Vision	42.03143496762571	-58.46114062779987	3444
384cc424fc386fb30b4b15ab24532e3e08b7c68f	from corners to rectangles — directional road sign detection using learned corner representations		In this work we adopt a novel approach for the detection of rectangular directional road signs in single frames captured from a moving car. These signs exhibit wide variations in sizes and aspect ratios and may contain arbitrary information, thus making their detection a challenging task with applications in traffic sign recognition systems and vision-based localization. Our proposed approach was originally presented for additional traffic sign detection in small image regions and is generalized to full image frames in this work. Sign corner areas are detected by four ACF-detectors (Aggregated Channel Features) on a single scale. The resulting corner detections are subsequently used to generate quadrangle hypotheses, followed by an aggressive pruning strategy. A comparative evaluation on a database of 1500 German road signs shows that our proposed detector outperforms other methods significantly at close to real-time runtimes and yields thrice the very low error-rate of the recent MS-CNN framework while being two orders of magnitude faster.	acf;autostereogram;comparison and contrast of classification schemes in linguistics and metadata;image processing;image segmentation;quadrangle (geography);real-time clock;requirement;sensor;traffic sign recognition	Thomas Wenzel;Ta-Wei Chou;Steffen Brüggert;Joachim Denzler	2017	2017 IEEE Intelligent Vehicles Symposium (IV)	10.1109/IVS.2017.7995851	traffic sign recognition;order of magnitude;quadrangle;detector;aspect ratio (image);feature extraction;computer vision;artificial intelligence;engineering;communication channel	Vision	41.321414123404224	-42.99903797058259	3447
c045167d25c3f5eadd6eba17a944b0fb8f7c1bdb	mining information extraction models for hmtdb annotation	biology computing;data mining genetic mutations dna bioinformatics pathology genomics databases biological system modeling proteins biological information theory;information extraction;information retrieval;hmtdb database data mining information extraction models hmtdb annotation genome sequencing discovered genes proteins biological process biological information structured representations domain relevant entities extraction rules manually labelled texts;text analysis;text extraction;data mining;genetics;proteins;text analysis biology computing data mining genetics information retrieval proteins;genome sequence;biological process	Advances of genome sequencing techniques have risen an overwhelming increase in the literature on discovered genes, proteins and their role in biological processes. However, the biomedical literature remains a greatly unexploited source of biological information. Information extraction (IE) techniques are necessary to map this information into structured representations that allow facts relating domain-relevant entities to be automatically recognized. In this paper, we present a framework that supports biologists in the task of automatic extraction of information from texts. The framework integrates a data mining module that discovers extraction rules from a set of manually labelled texts. Extraction models are subsequently applied in an automatic mode on unseen texts. We report an application to a real-world dataset composed by publications selected to support biologists in the annotation of the HmtDB database	association rule learning;biological database;data mining;dictionary;entity;information extraction;mined;named-entity recognition;performance;preprocessor;sensitivity and specificity;taxonomy (general);unsupervised learning;whole genome sequencing	Margherita Berardi;Donato Malerba;Marcella Attimonelli	2006	Sixth IEEE International Conference on Data Mining - Workshops (ICDMW'06)	10.1109/ICDMW.2006.113	text mining;whole genome sequencing;computer science;bioinformatics;data mining;knowledge extraction;biological process;information extraction;information retrieval	DB	-2.6973815197908677	-63.65969983619683	3452
4929adafa2d1fa8fcab96c50a94fb507b4e15cb0	spike-time robotics: a rapid response circuit for a robot that seeks temporally varying stimuli	neurorobotics;braitenberg vehicles;spike time robotics;resonating circuits;rat animat;spiking neural networks;autonomous robotics	In this paper we describe a spiking neural circuit inspired by the pyramidalinterneuron network gamma (PING) circuit modeled by Whittington and colleagues [1]. The spiking network controls a rat animat – a rodent-inspired robot that can autonomously explore and map its environment. We demonstrate how the neural controller directs the rat animat‟s movement towards temporal stimuli of the appropriate frequency using an approach based on Braitenberg Vehicles. The circuit responds robustly (after four cycles) when first detecting a light pulsing at 1 Hz, and rapidly (after one-to-three cycles) when primed by recent experiences with the same frequency. This study is the first to demonstrate a biologically-inspired spike-based robot that is both robust and rapid in detecting and responding to temporal dynamics in the environment. It provides the basis for further studies of biologically-inspired spike-based	action potential;animat;autonomous robot;braitenberg vehicle;futures studies;interaction;neurorobotics;sensor;software studies;spiking neural network	Janet Wiles;David Ball;Scott Heath;Chris Nolan;Peter Stratton	2010	Austr. J. Intelligent Information Processing Systems		braitenberg vehicle;neurorobotics;simulation;artificial intelligence	Robotics	18.39804176926332	-65.99334883567113	3453
61d729d47ab3742297a0235c63fc1dadb6ea5eaa	compressive passive millimeter-wave imaging	engineering;compressed sensing;psnr;passive millimeter wave imaging;image reconstruction bayesian methods reconstruction algorithms compressed sensing psnr millimeter wave technology;bayes methods;sparse reconstruction passive millimeter wave imaging compressive sensing bayesian methods;reconstruction algorithms;bayesian methods;focal planes;bayesian method;imaging system;image compression;sparse reconstruction passive millimeter wave imaging system compressive sensing principle encoded mask focal plane incoherent measurement imaged scene bayesian reconstruction algorithm original image estimation;compressive sensing;image reconstruction;sparse reconstruction;millimetre wave imaging bayes methods compressed sensing focal planes image reconstruction;millimeter wave;reconstruction algorithm;millimeter wave technology;millimetre wave imaging	In this paper, we present a novel passive millimeter-wave (PMMW) imaging system designed using compressive sensing principles. We employ randomly encoded masks at the focal plane of the PMMW imager to acquire incoherent measurements of the imaged scene. We develop a Bayesian reconstruction algorithm to estimate the original image from these measurements, where the sparsity inherent to typical PMMW images is efficiently exploited. Comparisons with other existing reconstruction methods show that the proposed reconstruction algorithm provides higher quality image estimates. Finally, we demonstrate with simulations using real PMMW images that the imaging duration can be dramatically reduced by acquiring only a few measurements compared to the size of the image.	algorithm;compressed sensing;focal (programming language);image sensor;randomness;simulation;sparse matrix	S. Derin Babacan;Martin Luessi;Leonidas Spinoulas;Aggelos K. Katsaggelos;Nachappa Gopalsami;Thomas W. Elmer;Ryan Ahern;Shaolin Liao;Apostolos C. Raptis	2011	2011 18th IEEE International Conference on Image Processing	10.1109/ICIP.2011.6116227	computer vision;bayesian probability;computer science;mathematics;compressed sensing;statistics	Vision	59.64710807261491	-57.89101467063538	3455
6266f779367266afb4668b00754bd769030e934e	dispersed-pulse codebook and its application to a 4 kb/s speech coder	algebraic codes;convolution;speech coding;convolution speech coding linear predictive coding algebraic codes vocoders;linear predictive coding;vocoders;bit rate communication industry pulse generation industrial training performance evaluation background noise speech coding gsm process design target tracking;4 kbit s dispersed pulse codebook speech coder celp coder excitation vector generation dispersion vectors convolution algebraic codevector signed pulses coding distortion objective evaluation algebraic codebook subjective evaluation speech coding low bit rate coding;subjective evaluation	This paper presents a dispersed-pulse codebook for CELP coder. This codebook generates an excitation vector by convoluting dispersion vectors with signed pulses in an algebraic codevector. The dispersion vectors are obtained through training so the coding distortion to be reduced. An objective evaluation result shows that the coding distortion with this codebook is smaller than that with an algebraic codebook. The dispersed-pulse codebook is applied to a 4 kb/s CELP coder. Subjective evaluation results show that: (1) the fundamental performance of the 4 kb/s coder is equivalent to that of G.726 32 kb/s coder, and (2) the performances of the 4k b/s coder under some error and background noise conditions are equivalent to those of G.729 8 kb/s coder.	codebook;data rate units;speech coding	Kazutoshi Yasunaga;Hiroyuki Ehara;Koji Yoshida;Toshiyuki Morii	2000		10.1109/ICASSP.2000.861926	dictionary coder;linear predictive coding;speech recognition;harmonic vector excitation coding;computer science;theoretical computer science;speech coding;mathematics;convolution;code-excited linear prediction	HCI	48.11924663712185	-8.37263353862146	3456
08f87ac6e8526ea8fc3199ff8fb3fe8e144ad0f7	a algorithm for identifying disease genes by incorporating the subcellular localization information into the protein-protein interaction networks	databases;cancer;training;proteins;protein engineering;bioinformatics	Disease gene identification is a key step to understand the cellular mechanisms associated with a specific disease. Compared with biological experiments, computational predictions of disease genes are cheaper and more effortless. Many computational methods are used to detect causal genes for diseases on the protein-protein interaction (PPI) networks generated by the high-throughput technology. However, the accuracy of these methods need to be improved due to the false interactions in the PPI data. To deal with the challenge, other methods are proposed via the integration of biological information from different sources with the PPI networks. In this work, a new algorithm AIDG is developed to predict disease genes. First, the weighted PPI networks are built by incorporating the protein subcellular localization information into the human PPI networks. Next, all of disease candidate genes are scored in terms of a iteration function. Finally, they are ranked on descending order of their scores. The top candidates are considered as potential disease genes. The results from the leave-one-out crossing validation (LOOCV) show that AIDG outperforms other similar methods like DADA and ToppNet.	algorithm;causal filter;computation;cross-validation (statistics);disease gene identification;experiment;high-throughput computing;interaction;iteration;multi-compartment model;online mendelian inheritance in man;pixel density;semantic network;sorting;throughput	Xiwei Tang;Xiaohua Hu;Xuejun Yang;Yuan Sun	2016	2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)	10.1109/BIBM.2016.7822537	biology;computer science;bioinformatics;machine learning;data mining;protein engineering;cancer	Comp.	6.463950075770418	-56.015064378733335	3457
174f1e4b52f409a9e81089f7d1524378b17fa75c	detection of causality between process variables based on industrial alarm data using transfer entropy	transfer entropy;binary alarm series;alarm design;causality analysis;期刊论文	In modern industrial processes, it is easier and less expensive to configure alarms by software settings rather than by wiring, which causes the rapid growth of the number of alarms. Moreover, because there exist complex interactions, in particular the causal relationship among different parts in the process, a fault may propagate along propagation pathways once an abnormal situation occurs, which brings great difficulty to operators to identify its root cause immediately and to take proper actions correctly. Therefore, causality detection becomes a very important problem in the context of multivariate alarm analysis and design. Transfer entropy has become an effective and widely-used method to detect causality between different continuous process variables in both linear and nonlinear situations in recent years. However, such conventional methods to detect causality based on transfer entropy are computationally costly. Alternatively, using binary alarm series can be more computational-friendly and more direct because alarm data analysis is straightforward for alarm management in practice. The methodology and implementation issues are discussed in this paper. Illustrated by several case studies, including both numerical cases and simulated industrial cases, the proposed method is demonstrated to be suitable for industrial situations contaminated by noise.	causality;computation;existential quantification;interaction;nonlinear system;numerical analysis;software propagation;transfer entropy;wiring	Weijun Yu;Fan Yang	2015	Entropy	10.3390/e17085868	transfer entropy;computer science;machine learning;data mining;statistics	SE	13.065912839995832	-13.783976769687328	3458
01b11b554a4e539013d2942b2ab264f1af1e6968	posterior probability based multi-classifier fusion in pedestrian detection		This paper presents a novel method for pedestrian detection at mea- surement level. At feature extraction stage, we use Histogram of Oriented Gradi- ent to describe the feature of pedestrian and non-pedestrian. To decrease the time cost, we reduce the dimension by using PCA. The base classifiers used in poste- rior probability based multi-classifier fusion are posterior probability based SVM, Na¨ ive Bayesian and Minimum Distance Classifier, respectively. To estimate the accuracy of fusion result, stratified cross-validation is used. Experimental results on pedestrian databases prove the efficiency of this work.	pedestrian detection	Jialu Zhao;Yan Chen;Xuanyi Zhuang;Yong Xu	2013		10.1007/978-3-319-01796-9_35	speech recognition;machine learning;pattern recognition	Vision	28.104129687081038	-59.46300100541459	3460
d6dab7c4e9ba089c15d5d7f9c44805c02c854452	ray tracing for simulating reflection phenomena in sar images	analytical models;backward ray tracing;radar reflectivity map;ray tracing solid modeling optical scattering layout optical reflection light scattering radar imaging radar scattering analytical models light sources;image processing;optical reflection;reflectivity;light scattering;geometry;terrasar x;layout;quasi perfect specular reflection;persistent scatterer;remote sensing by radar;institut fur methodik der fernerkundung;radar scattering;terrasar x sar simulation ray tracing persistent scatterers;persistent scatterers;persistent scatterer analysis sar images backward ray tracing radar reflectivity map image generation terrasar x imagery bounce levels afterwards reflection effects quasi perfect specular reflection;specular reflection;afterwards reflection effects;image generation;sar images;optical scattering;model building;geophysical signal processing;electromagnetic wave reflection;radar imaging;solid modeling;sar image;ray tracing;persistent scatterer analysis;optical sensors;sar simulation;building model;synthetic aperture radar electromagnetic wave reflection geophysical signal processing image processing ray tracing reflectivity remote sensing by radar;reflection;bounce levels;buildings;radar;light sources;synthetic aperture radar;terrasar x imagery	This paper presents an approach for using backward ray tracing for simulating radar reflectivity maps. After explaining the simulation concept which consists of three parts - modeling, sampling and image generation - two applications are presented for showing the performance of the simulator. At first reflection contributions like single or multiple bounce are simulated for a modeled building and compared to a real TerraSAR-X image. It shows how the capability of separating different bounce levels in different layers can support the interpretation of the SAR image. Afterwards reflection effects caused by quasi-perfect specular reflection are detected for another building model by means of geometrical analysis. This is thought as input for advanced Persistent Scatterer (PS) Analysis, since PS typically appear due to such reflection effects.	glossary of computer graphics;map;ray tracing (graphics);sampling (signal processing);simulation	Stefan Auer;Stefan Hinz;Richard Bamler	2008	IGARSS 2008 - 2008 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2008.4780143	computer vision;image processing;light scattering;optics;physics;remote sensing	Robotics	79.36874827876483	-67.71240728797358	3461
5c09c5764a03002a2bb002985e33aa2729f1400b	correlating interactions with gene expressions to detect protein complexes in protein interaction networks	dna;proteins gene expression dna algorithm design and analysis polymers educational institutions image edge detection;ppi protein complexes protein interaction networks gene expression data;gene expression;proteins;image edge detection;out module weight gene expression protein complex detection protein protein interaction networks macromolecular complexes cells ppi networks cige algorithm maximal full connected subgraph core graph seed node in module weight;polymers;proteins genetics graph theory molecular biophysics;algorithm design and analysis	In protein-protein interaction networks, proteins combine into macromolecular complexes to execute essential functions in the cells, such as replication, transcription, protein transport. Considering the certain rate of false positive and false negative interactions, we take a confidence probability on interactions and correlate interactions and gene expression data to assign weights to edges in PPI networks. Then we propose the CIGE algorithm to detecting protein complexes from protein interaction networks. Our algorithm takes a maximal full-connected sub-graph as core graph of a seed node, and decides whether a node belongs to a protein complex through judging in-module weight and out-module weight between core graph and nodes out of core graph. Experiment results show that our algorithm has an excellent performance in both accuracy and hit rate.	algorithm;interaction;maximal set;pixel density;replication (computing);sensor;transcription (software)	Huaxiong Yao;Mengxiao Cui;Wei Li;Ziwei Wang;Yuxiang Zhu	2014	2014 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)	10.1109/BIBM.2014.6999279	biology;algorithm design;gene expression;bioinformatics;theoretical computer science;machine learning;genetics;dna	Comp.	7.01878582331946	-56.541830560472	3465
0835124cf6e9708c639b3c0af882d346b2b8fb67	separation of estimation and control for decentralized stochastic control systems	discrete time systems;optimal control;decision theory;decentralized control;stochastic control;separation theorem	"""1. Introdllclion SFPAR,aTION Of estimation and control for decentralized discrete-time stochastic control systems will be discussed in this paper. Since research on optimal stochastic control problems started about 1960. separation between estimation and control has been one of the importanl"""" topics in the field. In [1.21 it has been shown that for the standard LQG (linear system, quadratic cost function, and Ganssian noise) problem. the design of the optimal controller is separated into the design of optimal estimator and thal of optimal controller for an equivalent deterministic problem. One generalization of this structural property of the optimal controller is the property that an optimal control law exists in a subclass of control laws which depend only on the expected value of the current state given the past and current data[3.41. A condition for this kind of separation is given in [3,41 . These results have been called 'separation theorems'. The concept of sufficient statislic has also very deep relation with separation[5]. The mildest separation property of stochastic control problems will probably be the one that an optimal control law exists in a subclass of control laws which depend only on the conditional probability distribution of the current state given the past and current data. This property has been used and/or discussed by many researchers [e,g. [6 9]). Traditional control problems, including those in the abovementioned papers, have a common fnndamental assumption of centralized information, i.e.. the assumption that there is only one control station with perfect memory in the system. Recently, however, as the control of large scale complex systems drew attention of control engineers, discussions started on what would happen if this fundamental assumption was replaced by the one that there are plural control stations with different information on the system or that the control station has not perfect memory. Reference [101 includes a large list of papers on these types of systems, so-called decentralized control systems. See also [11.121 . There are some works on separation properties for decentralized control systems. Witsenhausen[13] has introduced and discussed some concepts related to separation with the help of a fairly general discrete-time model. Several assertions on separation are also stated in [13] without proofs. It has been shown [14 16] that for systems with one-step delay sharing infer-"""	centralized computing;complex systems;control flow;control theory;distributed control system;linear system;loss function;optimal control;separation kernel;stochastic control	Tsuneo Yoshikawa;Hiroaki Kobayashi	1978	Automatica	10.1016/0005-1098(78)90052-3	control engineering;mathematical optimization;witsenhausen's counterexample;optimal control;stochastic control;decision theory;decentralised system;control theory;mathematics;separation principle;statistics	Robotics	64.28004783404647	0.3623957840363024	3468
17cb3e5f3e62aadd97b1626a61ee736e41854fae	smooth morphological transformation of ct and mr medical data	mr imaging;human brain	The presented method is the application of distortion correction approaches in the proccessing of multimodal medical data. The method allows reliable correction of distorted MR images, transformation between MR and CT images and consequent information blending among any desired multimodal data sets (CT, MR, human brain atlas).		Vojtech Jankovic;Eugen Ruzický;Ludovít Niepel	1993		10.1007/3-540-57233-3_91	computer science	NLP	45.35940982077401	-80.3573105997006	3476
360c50864b98d1cef244a929599182268da036aa	approximate z transform using higher-order integrators and its applications in sampled-data control systems	systeme commande;sistema control;z transformation;sampled data control;mando numerico;fonction polynomiale;aproximacion;transformation z;systeme echantillonne;aplicacion espacial;continuous system;commande numerique;transformacion z;higher order;approximation;stability;vehiculo;control system;vehicule;integrator;robustesse;integrador;stability analysis;sistema muestreado;robustness;digital control;vehicle;funcion polinomial;stabilite;polynomial function;application spatiale;sampled system;estabilidad;space application;integrateur;robustez	In this paper, we first clarify the difference between the approximate z transform and the discrete equivalent of a continuous system using higher-order integrators. It is shown that a 1/ ts factor needs to be included for the approximate z transform but not for the discrete equivalent. We further apply the approximate z transform to facilitate the stability analysis of sampled-data control systems, with or without uncertain parameters, ft is shown in this paper that the approximate z transform greatly simplifies the stability analysis of a sampled-data control system, which is regarded as rather difficult ( if not impossible) to handle because of its transcendental nature. The results can be easily obtained and show reasonably good approximations with this approach. Several examples are used to illustrate the effectiveness of this new method.	control system	Chi-Hsu Wang;Chen-Chien James Hsu;Wei-Yen Wang	1998	Int. J. Systems Science	10.1080/00207729808929551	control engineering;z-transform;von neumann stability analysis;integrator;higher-order logic;s transform;stability;digital control;control system;approximation;control theory;mathematics;algorithm;robustness	Logic	72.16467622149243	-5.17324272713517	3483
da240c232f161830895fc0e8175b9c1687d250b6	lyapunov-based control of macro-micro manipulator considering bending and torsional vibrations	lyapunov methods;closed loop system;microcontrollers;control systems;bending;manipulators;torsional strain lyapunov based control macro micro manipulator bending torsional vibrations positioning controller macroarm distributed parameter model output feedback controller lyapunov method asymptotic stability closed loop system pd feedback motor angles;distributed parameter systems manipulators position control lyapunov methods asymptotic stability torsion bending feedback closed loop systems microcontrollers pd control;closed loop systems;strain control;motor angles;vibration control lyapunov method manipulator dynamics control systems orbital robotics output feedback asymptotic stability strain control capacitive sensors safety;positioning controller;manipulator dynamics;asymptotic stability;torsion;orbital robotics;output feedback;distributed parameter model;feedback;lyapunov method;pd control;position control;torsional vibrations;macro micro manipulator;vibration control;distributed parameter systems;safety;torsional strain;pd feedback;output feedback controller;lyapunov based control;macroarm;capacitive sensors	This paper presents a Lyapunov-based positioning controller for a macro-micro manipulator suppressing the bending and torsional vibrations of the macro arm. On the basis of the distributed parameter model, the output feedback controller is constructed using the Lyapunov method. The asymptotic stability in the neighborhood of the desired states of the closed-loop system is proven. The proposed controller consists of the PD feedback of the motor angles and a feedback of the torsional strain at the tip of the macro arm. Some simulations are performed to show the effectiveness of the proposed controller.	block cipher mode of operation;control theory;lyapunov fractal;simulation	Yoshifumi Morita;Motoyuki Kimata;Hiroyuki Ukai;Hisashi Kando;Fumitoshi Matsuno	2005	Proceedings of the Fifth International Workshop on Robot Motion and Control, 2005. RoMoCo '05.	10.1109/ROMOCO.2005.201444	control engineering;electronic engineering;engineering;control theory;lyapunov redesign	Robotics	68.06921273098085	-15.423965139799048	3486
6b773c307698a1c1a83c5d58ae7a8e24ff81e5b1	joint learning of gene functions - a bayesian network model approach	bayesian network;yeast;gene function prediction;machine learning;gene function	In this paper, we develop a machine learning system for determining gene functions from heterogeneous data sources using a Weighted Naive Bayesian network (WNB). The knowledge of gene functions is crucial for understanding many fundamental biological mechanisms such as regulatory pathways, cell cycles and diseases. Our major goal is to accurately infer functions of putative genes or Open Reading Frames (ORFs) from existing databases using computational methods. However, this task is intrinsically difficult since the underlying biological processes represent complex interactions of multiple entities. Therefore, many functional links would be missing when only one or two sources of data are used in the prediction. Our hypothesis is that integrating evidence from multiple and complementary sources could significantly improve the prediction accuracy. In this paper, our experimental results not only suggest that the above hypothesis is valid, but also provide guidelines for using the WNB system for data collection, training and predictions. The combined training data sets contain information from gene annotations, gene expressions, clustering outputs, keyword annotations, and sequence homology from public databases. The current system is trained and tested on the genes of budding yeast Saccharomyces cerevisiae. Our WNB model can also be used to analyze the contribution of each source of information toward the prediction performance through the weight training process. The contribution analysis could potentially lead to significant scientific discovery by facilitating the interpretation and understanding of the complex relationships between biological entities.		Xutao Deng;Huimin Geng;Hesham H. Ali	2006	Journal of bioinformatics and computational biology	10.1142/S0219720006001928	biology;computer science;bioinformatics;machine learning;bayesian network;data mining;statistics	ML	5.958021966365956	-56.32901447291683	3488
b420269b8b8542a5f2380e72dc8e114ab776cc23	an improved sat formulation for the social golfer problem	finite geometry;sports scheduling;design theory;combinatorial optimization	Abstract The Social Golfer Problem (SGP) is a sports scheduling problem that exhibits a lot of symmetry and has recently attracted significant attention. In this paper, we first revisit an existing SAT encoding for the SGP and correct some of its clauses. We then propose a change in the encoding that significantly reduces the number of variables for all instances. We achieve considerable performance improvements when solving many SGP instances with common SAT solvers using local search and complete backtracking. This makes SAT formulations a more promising approach for solving the SGP than previously.		Markus Triska;Nysret Musliu	2012	Annals OR	10.1007/s10479-010-0702-5	mathematical optimization;finite geometry;combinatorial optimization;artificial intelligence;mathematics;designtheory;algorithm	AI	22.991175740041523	3.6698603136794667	3493
a8a58412420f5719c6d3ae7dd6057bda6ae7f16f	high resolution sparse estimation of exponentially decaying signals	sparse signal modeling;parameter estimation sparse signal modeling spectral analysis sparse reconstruction;dictionaries damping frequency estimation signal to noise ratio estimation sparse matrices optimization;sparse reconstruction;parameter estimation;spectral analysis;simulated data high resolution sparse estimation exponentially decaying signals dictionary matrices finely spaced frequency grid computationally cumbersome optimization problem dictionary learning approach arbitrary damping factors;sannolikhetsteori och statistik;signal resolution learning artificial intelligence matrix algebra optimisation	We consider the problem of sparse modeling of a signal consisting of an unknown number of exponentially decaying sinusoids. Since such signals are not sparse in an oversampled Fourier matrix, earlier approaches typically exploit large dictionary matrices that include not only a finely spaced frequency grid but also a grid over the considered damping factors. The resulting dictionary is often very large, resulting in a computationally cumbersome optimization problem. Here, we instead introduce a novel dictionary learning approach that iteratively refines the estimate of the candidate damping factor for each sinusoid, thus allowing for both a quite small dictionary and for arbitrary damping factors, not being restricted to a grid. The performance of the proposed method is illustrated using simulated data, clearly showing the improved performance as compared to previous techniques.	damping factor;dictionary;machine learning;mathematical optimization;optimization problem;sparse matrix	Johan Sward;Stefan Ingi Adalbjornsson;Andreas Jakobsson	2014	2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2014.6854998	speech recognition;k-svd;computer science;machine learning;sparse approximation;mathematics;estimation theory;statistics	Robotics	78.76796246122912	-10.031568744745718	3503
1a7d11090c5b17db6d4f182ee961702eb68b8fb0	inverse kinematics analysis for incompletely restrained parallel wire mechanisms	incompletely restrained parallel wire mechanism inverse kinematics analysis incompletely restrained parallel wire mechanisms end effector multiple wires 3d positioning 3d end effector orientation dynamic equations;incompletely restrained parallel wire mechanisms;incompletely restrained parallel wire mechanism;multiple wires;pensions;inverse kinematics analysis;dynamic equations;kinematics wire cranes equations intelligent systems machine intelligence machinery systems engineering and theory pensions mechanical factors;kinematics;manipulator kinematics;three dimensional;systems engineering and theory;mechanical factors;wire;3d positioning;general solution;machine intelligence;end effector;dynamic equation;intelligent systems;cranes;3d end effector orientation;inverse kinematics;machinery;inverse problems;dynamic properties;inverse problems manipulator kinematics	This paper discusses parallel wire mechanisms where a n end-effector of the mechanism i s suspended by multiple wires. This mechanism enables not only three dimensional positioning but also three dimensional orientating of the end-effector, unlike typical wire suspension type mechanism such as overhead crane. To discuss the parallel wire mechanism, two forms of basic dynamic equations are presented. T h e n dynamical properties of the mechanism is described based o n the basic equations. This paper mainly issues a n inverse kinematics problem for general incompletely restrained type parallel wire mechanism. T o distinguish some parallel wire mechanisms including the incompletely restrained type, the paper f irs t defines constraint states of the end-effector, then parallel wire mechanisms are classified in to three categories based o n the definition. Each characteristic of the mechanism corresponding t o the categories is described. Especially, incompletely restrained type parallel wire mechanism i s mainly discussed an the view point of inverse kinematics problem. Af ter arguing a general solution f o r the inverse kinematics problem of the incompletely restrained type parallel wire mechanism, a n actual mechanism of the incompletely restrained type mechanism i s presented and the inverse kinematics problem for this mechanism is analyzed.	dynamical system;inverse kinematics;overhead (computing);robot end effector;time-scale calculus	Motoji Yamamoto;Akira Mohri	2000		10.1109/IROS.2000.894654	control engineering;three-dimensional space;kinematics;machine;robot end effector;intelligent decision support system;computer science;inverse problem;artificial intelligence;inverse kinematics;control theory;mathematics;engineering drawing	Robotics	69.301247557603	-21.647376277560404	3508
db6df9ac98bf7119048f0196f048d38da097aab2	aigif: adaptively integrated gradient and intensity feature for robust and low-dimensional description of local keypoint			gradient	Songlin Du;Takeshi Ikenaga	2017	IEICE Transactions		mathematics;machine learning;artificial intelligence;computer vision	Vision	32.340469909153775	-53.79492830432897	3513
908180006d325597b5360b2eb59c4dadf223b675	reconstructing regulatory networks in streptomyces using evolutionary algorithms	regulatory network reconstruction discrete modelling technique continuous modelling technique gene expression data s coelicolor gene regulatory network parameter optimisation network structure gene regulation antibiotics production streptomyces coelicolor soil dwelling bacterium organism bacteria life cycle biological systems biological network reconstruction evolutionary algorithm;evolutionary computation;gene expression mathematical model antibiotics production proteins biological system modeling data models;genetics;microorganisms antibacterial activity evolutionary computation genetics;antibacterial activity;network reconstruction streptomyces coelicolor gene regulatory networks;microorganisms	Reconstructing biological networks is vital in developing our understanding of nature. Biological systems of particular interest are bacteria that can produce antibiotics during their life cycle. Such an organism is the soil dwelling bacterium Streptomyces coelicolor. Although some of the genes involved in the production of antibiotics in the bacterium have been identified, how these genes are regulated and their specific role in antibiotic production is unknown. By understanding the network structure and gene regulation involved it may be possible to improve the production of antibiotics from this bacterium. Here we use an evolutionary algorithm to optimise parameters in the gene regulatory network of a sub-set of genes in S. coelicolor involved in antibiotic production. We present some of our preliminary results based on real gene expression data for continuous and discrete modelling techniques.	biological network;biological system;evolutionary algorithm;gene regulatory network;object lifetime	Spencer Angus Thomas;Yaochu Jin;Emma Laing;Colin P. Smith	2013	2013 13th UK Workshop on Computational Intelligence (UKCI)	10.1109/UKCI.2013.6651283	biology;bioinformatics;microbiology	Comp.	6.145977494461522	-60.8415920517497	3518
3b41a4f960d73be2648f525a66a05f3d5e8c8ab4	semi-supervised learning for tree-structured ensembles of rbf networks with co-training	unlabeled data;image recognition;object recognition;reconocimiento imagen;vision ordenador;multi view learning;supervised learning;analisis datos;metodo arborescente;dempster shafer theory of evidence;fonction base radiale;binary hierarchical classifier;reconnaissance objet;data mining;semi supervised learning;computer vision;multiple view;radial basis function networks;decomposition method;data analysis;hierarchical classification;radial basis function;radial basis function network;teoria dempster shafer;fouille donnee;tree structure;dempster shafer theory;reconnaissance image;pattern recognition;classification hierarchique;vue multiple;tree structured method;analyse donnee;vision ordinateur;reconocimiento de objetos;co training;methode arborescente;reconnaissance forme;visual object recognition;apprentissage supervise;reseau neuronal;reconocimiento patron;classification accuracy;funcion radial base;aprendizaje supervisado;clasificacion jerarquizada;rbf network;busca dato;red neuronal;vista multiple;theorie dempster shafer;neural network;output space decomposition	Supervised learning requires a large amount of labeled data, but the data labeling process can be expensive and time consuming, as it requires the efforts of human experts. Co-Training is a semi-supervised learning method that can reduce the amount of required labeled data through exploiting the available unlabeled data to improve the classification accuracy. It is assumed that the patterns are represented by two or more redundantly sufficient feature sets (views) and these views are independent given the class. On the other hand, most of the real-world pattern recognition tasks involve a large number of categories which may make the task difficult. The tree-structured approach is an output space decomposition method where a complex multi-class problem is decomposed into a set of binary sub-problems. In this paper, we propose two learning architectures to combine the merits of the tree-structured approach and Co-Training. We show that our architectures are especially useful for classification tasks that involve a large number of classes and a small amount of labeled data where the single-view tree-structured approach does not perform well alone but when combined with Co-Training, it can exploit effectively the independent views and the unlabeled data to improve the recognition accuracy.		Mohamed Farouk Abdel Hady;Friedhelm Schwenker;Günther Palm	2010	Neural networks : the official journal of the International Neural Network Society	10.1016/j.neunet.2009.09.001	semi-supervised learning;computer science;artificial intelligence;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;supervised learning;artificial neural network	ML	10.257289075481486	-33.186153489957206	3519
8e095f6e0a95aa27308b7dddafbb428427efb84b	stability regions for maintaining efficiency in data envelopment analysis	optimal solution;analisis sensibilidad;analisis envolvimiento datos;efficiency;stability region;prise decision;operations research;stability;eficacia;data envelopment analysis;sensitivity analysis;necessary and sufficient condition;data envelopment analysis dea;efficacite;analyse sensibilite;decision making unit;stabilite;toma decision;data envelope analysis;estabilidad;analyse enveloppement donnee	This paper develops a procedure for performing a sensitivity analysis of the efficient decision making units (DMUs) within the Charnes et al. (CCR) [European Journal of Operational Research 2 (1978) 429–444] model of data envelopment analysis (DEA). The procedure yields an exact `input stability region' and `output stability region' within which the efficiency of a specific efficient DMU remains unchanged. Such stability regions are simply characterized by optimal solutions of modified CCR models which are easily computed. In contrast to existing sufficient conditions for the preservation of efficiency under changes in inputs or outputs, the paper provides both necessary and sufficient conditions for an efficient DMU to remain efficient. The procedure is illustrated by numerical examples. Somewhat surprisingly, for real world data sets, for most of the efficient DMUs, the amount of some individual input can be infinitely increased when keeping other inputs and all outputs constant. This indicates that these efficient DMUs are located at extreme positions.	data envelopment analysis	Lawrence M. Seiford;Joe Zhu	1998	European Journal of Operational Research	10.1016/S0377-2217(97)00103-3	econometrics;economics;computer science;operations management;data envelopment analysis;mathematics;operations research	Theory	0.041833936977285546	-14.720170675347807	3520
11584ae96682ad1ce61ee2179f9df97d1290e492	parameter adaptive control algorithms - a tutorial	closed loop parameter estimation;adaptive control;identification;digital control;model structure determination	An introduction is given to adaptive (self-tuning) control algorithms with recursive parameter estimation, which have obtained increasing attention in recent years. These algorithms result from combinations of recursive parameter estimation algorithms and easy to design control algorithms. Firstly a short review is given on proper recursive parameter estimation methods, including their application in a closed loop. This is followed by the design equations for various control algorithms and ways for d.c.-value estimation and for offset compensation. Various explicit and implicit combinations can be designed with different properties of the resulting adaptive control algorithms for both deterministic and stochastic disturbances. Their convergence properties are discussed. Simulation examples are presented and examples for the adaptive control of an air conditioner and a pH-process are shown. The introduction of a third feedback level for coordination and supervision is considered. Finally further problems are discussed. â€ Purchase Export Previous article Next article Check if you have access through your login credentials or your institution.	algorithm;credential;estimation theory;login;recursion;self-tuning;simulation	Rolf Isermann	1982	Automatica	10.1016/0005-1098(82)90002-4	identification;control engineering;mathematical optimization;digital control;adaptive control;computer science;engineering;control theory;mathematics	ML	66.42026320846551	-5.655626228838685	3527
62d6b91cb3454f597ea9f7691759f86c40c715ca	fractional wavelet scattering network and applications		Objective: This study introduces a fractional wavelet scattering network (FrScatNet), which is a generalized translation invariant version of the classical wavelet scattering network. Methods: In our approach, the FrScatNet is constructed based on the fractional wavelet transform (FRWT). The fractional scattering coefficients are iteratively computed using FRWTs and modulus operators. The feature vectors constructed by fractional scattering coefficients are usually used for signal classification. In this paper, an application example of the FrScatNet is provided in order to assess its performance on pathological images. First, the FrScatNet extracts feature vectors from patches of the original histological images under different orders. Then we classify those patches into target (benign or malignant) and background groups. And the FrScatNet property is analyzed by comparing error rates computed from different fractional orders, respectively. Based on the above pathological image classification, a gland segmentation algorithm is proposed by combining the boundary information and the gland location. Results: The error rates for different fractional orders of FrScatNet are examined and show that the classification accuracy is improved in fractional scattering domain. We also compare the FrScatNet-based gland segmentation method with those proposed in the 2015 MICCAI Gland Segmentation Challenge and our method achieves comparable results. Conclusion: The FrScatNet is shown to achieve accurate and robust results. More stable and discriminative fractional scattering coefficients are obtained by the FrScatNet in this paper. Significance: The added fractional order parameter is able to analyze the image in the fractional scattering domain.		Li Liu;Jiasong Wu;Dengwang Li;Lotfi Senhadji;Huazhong Shu	2018	IEEE Transactions on Biomedical Engineering	10.1109/TBME.2018.2850356	wavelet transform;wavelet;computer vision;time–frequency analysis;image segmentation;artificial intelligence;feature vector;fractional wavelet transform;scattering;computer science;algorithm;contextual image classification	Vision	34.380212377748464	-74.81582505926858	3530
28e88a5c0a6a0dd2c9972d05382689b76e61cd5d	the effect of supply uncertainty in price-setting newsvendor models	price dependent newsvendor model;journal;supply uncertainty;emergency supply option;stochastic comparisons	We consider a price-setting newsvendor model in which a firm needs to make joint inventory and pricing decisions before the selling season. The supply process is uncertain such that the received quantity is the product of the order quantity and a random yield rate. Two cost structures are investigated, the in-house production case in which the firm pays for the input quantity and the procurement case in which the firm pays for the quantity received only. Our objective is to investigate the effect of yield randomness on optimal decisions and expected profit. By using the theory of stochastic comparisons, we find that under both cost structures, a less variable yield rate leads to a lower optimal price and a higher expected profit. Moreover, we show that in the in-house production case, a stochastically larger yield rate also results in a lower optimal price and a higher profit, but this is not true in the procurement case. Examples show that the effect of supply uncertainty on optimal order quantity is not universal.	newsvendor model	Minghui Xu;Ye Lu	2013	European Journal of Operational Research	10.1016/j.ejor.2012.12.008	financial economics;newsvendor model;extended newsvendor model;economics;microeconomics;law of supply;commerce	Robotics	1.2416480473599534	-5.255466436255424	3531
f4369c8bdf56bf5a0aaa18c801bf54d0600f0709	single-facility huff location problems on networks	location on networks;dc optimization;huff location models;interval analysis	Huff location problems have been extensively analyzed within the field of competitive continuous location. In this work, two Huff location models on networks are addressed, by considering that users go directly to the facility or they visit the facility in their way to a destination. Since the problems are multimodal, a branch and bound algorithm is proposed, in which two different bounding strategies, based on Interval Analysis and DC optimization, are used and compared. Computational results are given for the two bounding procedures, showing that problems of rather realistic size can be solved in reasonable time. Copyright Springer Science+Business Media New York 2014	high-frequency direction finding	Rafael Blanquero;Emilio Carrizosa;Amaya Nogales-Gómez;Frank Plastria	2014	Annals OR	10.1007/s10479-013-1445-x	mathematical optimization;artificial intelligence;operations management;mathematics;interval arithmetic;operations research	Theory	16.494328131955385	2.3709564864900416	3533
6b454d302bda6766264d0de1d4c7d8f37566e642	variable background active contour model for computer-aided delineation of nodules in thyroid ultrasound images	patient diagnosis;level sets;active contour;image segmentation;computer aided diagnosis;ultrasound;computer aided delineation;thyroid ultrasound images;biological organs;level set;tumours;biomedical imaging;thyroid nodules;active contours;ultrasound us;noise robustness;ultrasound imaging;ultrasound us active contour models computer aided diagnosis level sets thyroid nodules;contour noise robustness;active contours ultrasonic imaging level set biomedical imaging image edge detection cancer computed tomography ultrasonography medical diagnostic imaging biomedical informatics;hypoechoic thyroid nodules variable background active contour model computer aided delineation thyroid ultrasound images active contour model contour noise robustness;medical image processing;thyroid nodule;computerized tomography;active contour models;variable background active contour model;tumours biological organs biomedical ultrasonics image segmentation medical image processing;hypoechoic thyroid nodules;biomedical ultrasonics;active contour model;artificial intelligence computer simulation humans image interpretation computer assisted models biological pattern recognition automated reproducibility of results sensitivity and specificity thyroid nodule ultrasonography	This paper presents a computer-aided approach for nodule delineation in thyroid ultrasound (US) images. The developed algorithm is based on a novel active contour model, named variable background active contour (VBAC), and incorporates the advantages of the level set region-based active contour without edges (ACWE) model, offering noise robustness and the ability to delineate multiple nodules. Unlike the classic active contour models that are sensitive in the presence of intensity inhomogeneities, the proposed VBAC model considers information of variable background regions. VBAC has been evaluated on synthetic images, as well as on real thyroid US images. From the quantification of the results, two major impacts have been derived: 1) higher average accuracy in the delineation of hypoechoic thyroid nodules, which exceeds 91%; and 2) faster convergence when compared with the ACWE model.	active contour model;contour line;convergence (action);name;quantitation;synthetic intelligence;thyroid nodule;vaginal birth after cesarean;algorithm	Dimitrios E. Maroulis;Michalis A. Savelonas;Dimitrios K. Iakovidis;Stavros A. Karkanis;Nikos Dimitropoulos	2007	IEEE Transactions on Information Technology in Biomedicine	10.1109/TITB.2006.890018	medical imaging;computer vision;radiology;medicine;pathology;computer science;level set;active contour model	Vision	40.33518859766265	-76.00993003994465	3534
076e81498a4f455f9439f8fdcc2868c7c6f83182	regularized online learning of pseudometrics	online content based music retrieval scenario;pseudometric online learning regularization;unsupervised learning;mahalanobis distance;unsupervised learning computer aided instruction content based retrieval geometry internet music;online content based music retrieval scenario regularized online learning pseudometrics mahalanobis distance unsupervised data;regularized online learning;computer aided instruction;training;geometry;pseudometric;euclidean distance;online learning;regularization;laplace equations;feedback;computational modeling;unsupervised data;internet;feedback music information retrieval content based retrieval euclidean distance optimization methods internet unsupervised learning principal component analysis laplace equations algorithm design and analysis;principal component analysis;music information retrieval;pseudometrics;optimization;content based retrieval;music;sparse matrices;algorithm design and analysis;content based music retrieval;optimization methods	We present a regularized approach for online learning of a pseudometric in the form of a Mahalanobis distance. We express the problem as an optimization that learns on the current labeled instance whilst favoring a solution of a predefined form. Our focus is on regularization. Our formulation takes up a flexible form allowing for scenarios ranging from traditional L2 regularization to regularization to a prior estimated from unsupervised data. We apply our method to an online content-based music retrieval scenario (e.g. personalized internet radio). Here the user provides information on his listening preferences via online feedback for each song that is played. By updating a pseudometric given this feedback, the algorithm optimizes a transformation that maps the user's preferred songs closer together and undesired songs far from these preferred songs.	algorithm;manifold regularization;map;mathematical optimization;matrix regularization;online machine learning;personalization;web content;while	Yvonne Moh;Joachim M. Buhmann	2010	2010 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2010.5495245	unsupervised learning;regularization;computer vision;computer science;mahalanobis distance;machine learning;pattern recognition;music;euclidean distance;feedback;mathematics;principal component analysis	Robotics	26.059118054948417	-41.46917794867283	3536
7f8545ac3532f05f07ac9669111b86704a5d8ee2	localized projection learning	kernel machine;approximate nearest neighbor;k nearest neighbor	It is interesting to compare different criteria of kernel machines. In this paper, the following is made: 1) To cope with the scaling problem of projection learning, we propose a dynamic localized projection learning using k nearest neighbors, 2) The localized method is compared with SVM from some viewpoints, and 3) Approximate nearest neighbors are demonstrated their usefulness in such a localization. As a result, it is shown that SVM is superior to projection learning in many classification problems in its optimal setting but the setting is not easy.		Kazuki Tsuji;Mineichi Kudo;Akira Tanaka	2010		10.1007/978-3-642-14980-1_8	machine learning;pattern recognition;data mining;mathematics;k-nearest neighbors algorithm	ML	22.540813104989432	-40.971027160708886	3537
5565b7fefcbc8072e3165dbc4023693f66ff0172	web-based tool compilation of analytical equations for groundwater management applications		Abstract The INOWAS platform provides a compilation of free web-based tools for groundwater management. All tools are running on a web server and can be accessed via standard web browsers. The implemented analytical equations enable the assessment of saltwater intrusion induced by pumping or sea level rise, the calculation of travel time through unconfined aquifers and the evaluation of pumping-induced river drawdown. The groundwater mounding calculator can be used to estimate the rise of groundwater levels underneath infiltration basins. To determine the contaminant concentration downgradient of a constant source, an analytical tool solving the advection-dispersion equation can be utilized. All tools are incorporated into a decision support environment. The user is provided with detailed online support that contains the theoretical background of the tools, possible applications and examples.		Jana Glass;Ramandeep Jain;Ralf Junghanns;Jana Sallwey;Thomas Fichtner;Catalin Stefan	2018	Environmental Modelling and Software	10.1016/j.envsoft.2018.07.008	decision support system;computer science;hydrology;web application;web server;infiltration (hydrology);groundwater;saltwater intrusion;aquifer;sea level rise	SE	96.2842992892628	0.9203896192305745	3540
fb5ac93293c09f16568b5269dd1b1f46cd01384c	bayesian network retrieval discrimination criteria model based on unbalanced information		Unbalanced sample data are usually ignored in the process of case matching, but these data also lead to misclassification during case matching. To solve this problem, a discrimination criteria model based on the Bayesian network and corresponding algorithm is proposed in our paper. The Bayesian network cost sensitivity learning in this model uses the minimization theorem of loss function. We also introduce a ROC curve to evaluate the performance of the retrieval model and verify the validity of the model by using diagnostic data for clinical heart disease. Our results indicate that this method can effectively eliminate the cost sensitivity of imbalanced datasets and improve the accuracy of the retrieval results.	bayesian network	Man Xu;Dan Gan;Jiang Shen;Bang An	2018		10.1007/978-3-030-03649-2_21	bayesian network;computer science;artificial intelligence;pattern recognition	Vision	12.192673097345429	-39.18981096879121	3544
4d472c9f78b7d13acd27ea941cf74b8b6dcef6b7	hmulab: a biomedical hybrid multi-label classifier based on multiple linear regression	neighbor score hmulab multi label learning multiple linear regression hybrid method feature score;classification algorithms loss measurement prediction algorithms linear regression optimization informatics	Many biomedical classification problems are multi-label by nature, e.g., a gene involved in a variety of functions and a patient with multiple diseases. The majority of existing classification algorithms assumes each sample with only one class label, and the multi-label classification problem remains to be a challenge for biomedical researchers. This study proposes a novel multi-label learning algorithm, hMuLab, by integrating both feature-based and neighbor-based similarity scores. The multiple linear regression modeling techniques make hMuLab capable of producing multiple label assignments for a query sample. The comparison results over six commonly-used multi-label performance measurements suggest that hMuLab performs accurately and stably for the biomedical datasets, and may serve as a complement to the existing literature.	algorithm;cardinality;coefficient;complement system proteins;computation;graphics processing unit;linear iga bullous dermatosis;machine learning;multi-label classification;multiple personality disorder;nevus sebaceous;numerous;patients;silo (dataset);sparse matrix;the matrix;vector quantization;algorithm	Pu Wang;Ruiquan Ge;Xuan Xiao;Manli Zhou;Fengfeng Zhou	2017	IEEE/ACM Transactions on Computational Biology and Bioinformatics	10.1109/TCBB.2016.2603507	machine learning;computer science;linear regression;classifier (linguistics);linear classifier;informatics;bioinformatics;statistical classification;artificial intelligence;pattern recognition	Comp.	9.822723538059657	-47.70777154065416	3546
208b4a0637b4b60b2406be7112563ed2b30e6b09	learning a restricted bayesian network for object detection	belief networks;computational complexity;image classification;object detection;statistical analysis;bayesian network;np complete learning structure;image classification;log-likelihood ratio function;object detection;sparse structure;statistical dependency	Many classes of images have the characteristics of sparse structuring of statistical dependency and the presence of conditional independencies among various groups of variables. Such characteristics make it possible to construct a powerful classifier by only representing the stronger direct dependencies among the variables. In particular, a Bayesian network compactly represents such structuring. However, learning the structure of a Bayesian network is known to be NP complete. The high dimensionality of images makes structure learning especially challenging. This paper describes an algorithm that searches for the structure of a Bayesian network based classifier in this large space of possible structures. The algorithm seeks to optimize two cost functions: a localized error in the log-likelihood ratio function to restrict the structure and a global classification error to choose the final structure of the network. The final network structure is restricted such that the search can take advantage of pre-computed estimates and evaluations. We use this method to automatically train detectors of frontal faces, eyes, and the iris of the human eye. In particular, the frontal face detector achieves state-of-the-art performance on the MIT-CMU test set for face detection.	algorithm;bayesian network;conditional entropy;face detection;object detection;precomputation;sensor;sparse matrix;statistical classification;test set	Henry Schneiderman	2004	Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.	10.1109/CVPR.2004.141	computer vision;contextual image classification;face detection;conditional independence;variable-order bayesian network;likelihood-ratio test;computer science;machine learning;pattern recognition;bayesian network;data mining;computational complexity theory;statistics	Vision	32.380940796401774	-41.45847584192783	3552
e04c7d5459e6bb91a8843a805ea2a6ebd1ef86a4	noise robustness in the auditory representation of speech signals	zero crossing rate;noisy environments;neural nets;compressive nonlinearity;signal analysis;natural speech vowels auditory representation speech signals biological sensory systems wavelet transform compressive nonlinearity neural representation locally averaged zero crossing rates encoding mutual suppressive interactions robustness noisy environments;auditory system;locally averaged zero crossing rates;contracts;speech enhancement;noise robustness;wavelet transforms;interference suppression;wavelet transform;ear;wavelet transforms acoustic noise interference suppression neural nets speech analysis and processing;signal processing;acoustic noise;noise robustness speech enhancement wavelet transforms signal processing biological information theory signal analysis ear acoustic noise contracts educational institutions;speech signals;neural representation;speech analysis and processing;biological information theory;natural speech vowels;robustness;sensory system;auditory representation;biological sensory systems;encoding;mutual suppressive interactions	A common sequence of operations in the early stages of most biological sensory systems is a wavelet transform followed by a compressive nonlinearity. The contribution of these operations to the formation of robust and perceptually significant representations in the auditory system is explored. It is demonstrated that the neural representation of a complex signal such as speech is derived from a highly reduced version of its wavelet transform, specifically, from the distribution of its locally averaged zero-crossing rates along the temporal and scale axes. It is shown analytically that such encoding of the wavelet transform results in mutual suppressive interactions across its different scale representations. Suppression in turn endows the representation with enhanced spectral peaks and superior robustness in noisy environments. Examples using natural speech vowels are presented to illustrate the results. >		Kuansan Wang;Shihab A. Shamma;William J. Byrne	1993		10.1109/ICASSP.1993.319306	speech recognition;computer science;signal processing;wavelet transform	ML	-3.799128544442808	-91.28769014591789	3562
d4288ec77156cadd9ee98c6be58c1f73d9e3e978	flame object segmentation by an improved frame difference method	object recognition;video streaming fires flames image segmentation interference suppression object recognition polymerisation smoke detectors video cameras;video streaming;image segmentation;flames;frame difference;fires polymers image color analysis image segmentation cameras streaming media interference;fire alarm;flame segmentation;interference;fire prevention flame object segmentation frame difference method fire alarm system fire source pattern recognition fisheye camera visual sensor early detection polymerization flash point location video streaming interference suppression moving object recognition dissociative flash point thresholding method contour early warning;interference suppression;streaming media;video cameras;smoke detectors;image color analysis;polymers;polymerisation;fires;image segmentation fire alarm frame difference flame segmentation;cameras	This paper in view of the characteristic of traditional fire alarm system in using sensor away from fire source is hard to detect the fire early, conceives of a method of camera pattern recognition, and according to using fisheye camera which brought to top as the visual sensor and in a larger scope in the early detection of fire, proposes a frame difference method based on the cross-cutting and polymerization of flame flash point's location, which achieved flame object segmentation. By the execution of two consecutive frames' difference in live video streaming, this method rules out the interference of moving objects according to the crossing of the flash point's location, and then rules out part of dissociative flash point which is interference according to the polymerization of the flash point's location, and get suspected flame area finally. By thresholding method, after getting contour and drawing contour, we get the outline of the flame. This method can effectively segment the flame from the images, and so as to provide efficient recognition method for the early warning of the fire prevention.	contour line;fisheye;interference (communication);pattern recognition;streaming media;thresholding (image processing)	Chen Ning;Ding Fei	2012	2012 Third International Conference on Digital Manufacturing & Automation	10.1109/ICDMA.2012.101	computer vision;simulation;engineering;computer graphics (images)	Robotics	44.14727625837998	-44.18105854682118	3565
7bc174117d9a701b8d4f899cf1b4176a6a15466d	a bayesian approach for protein classification	bayesian classifier;protein family;bayesian approach;protein sequence;data mining;protein classification	In this work, we propose a new approach for protein classification based on Bayesian classifiers. Our goal is to predict the functional family of novel protein sequences based on their motif composition. For this purpose, datasets extracted from Prosite, a curated protein family database, are used as training datasets. In the conducted experiments, the performance of our classifier is compared to other known data mining approaches. The computational results have shown that the proposed method outperforms the other ones and looks very promising for problems with characteristics similar to the problem addressed here.	bayesian network;computation;data mining;experiment;motif;naive bayes classifier;prosite;peptide sequence;protein family	Luiz Henrique de Campos Merschmann;Alexandre Plastino	2006		10.1145/1141277.1141322	naive bayes classifier;bayesian probability;computer science;bioinformatics;protein sequencing;pattern recognition;data mining;protein family	ML	7.909471536791044	-49.54672778741908	3570
3c5aaba161cabe920b624513d4bfcb6c2e31e99f	an extended support vector machine forecasting framework for customer churn in e-commerce	customer behavior;decision tree;e commerce;kernel function;selective sampling;large scale;support vector machine;data warehouse;customer churn;extract transform load;artificial neural network	In order to accurately forecast and prevent customer churn in e-commerce, a customer churn forecasting framework is established through four steps. First, customer behavior data is collected and converted into data warehouse by extract transform load (ETL). Second, the subject of data warehouse is established and some samples are extracted as train objects. Third, alternative predication algorithms are chosen to train selected samples. Finally, selected predication algorithm with extension is used to forecast other customers. For the imbalance and nonlinear of customer churn, an extended support vector machine (ESVM) is proposed by introducing parameters to tell the impact of churner, non-churner and nonlinear. Artificial neural network (ANN), decision tree, SVM and ESVM are considered as alternative predication algorithms to forecast customer churn with the innovative framework. Result shows that ESVM performs best among them in the aspect of accuracy, hit rate, coverage rate, lift coefficient and treatment time. This novel ESVM can process large scale and imbalanced data effectively based on the framework. 2010 Published by Elsevier Ltd.	approximation algorithm;artificial neural network;decision tree;e-commerce;email;matthews correlation coefficient;nonlinear system;package tracking;support vector machine	Xiaobing Yu;Shunsheng Guo;Jun Guo;Xiaorong Huang	2011	Expert Syst. Appl.	10.1016/j.eswa.2010.07.049	kernel;support vector machine;computer science;data science;machine learning;decision tree;data warehouse;data mining;artificial neural network	AI	5.258412863337668	-20.30820635007609	3571
12dbd65a70b46355be7e40d8290670898ec9323c	a novel approach to haptic tele-operation of aerial robot vehicles	aerial robots;closed loop system;robot sensing systems;control systems;aircraft control;telerobotics aerospace robotics aircraft control closed loop systems surveillance;surveillance task;surveillance;closed loop systems;aerial robot vehicles;haptic feedbacks;inspection task;inspection;communications links haptic tele operation aerial robot vehicles haptic feedback inspection task surveillance task network theory port hamiltonian system closed loop system;force;time delay;aerial robotic vehicles;conference paper;safe operation;hamiltonian system;feedback;3d environment;potential time;aerospace robotics;robots;port hamiltonian system;sensor data;local environments;communications links;keywords 3 d environments;ro;telerobotics;haptic feedback;hamiltonians;port hamiltonian systems;network theory;inspection and surveillance;vehicles;couplings;haptic interfaces;communication system control;master slave;haptic interfaces robots vehicles inspection control systems communication system control master slave mechanical variables control robotics and automation surveillance;robotics and automation;vehicle dynamics;haptic tele operation;tele operations;mechanical variables control	We present a novel, simple and effective approach for tele-operation of aerial robotic vehicles with haptic feedback. Such feedback provides the remote pilot with an intuitive feel of the robot's state and perceived local environment that will ensure simple and safe operation in cluttered 3D environments common in inspection and surveillance tasks. Our approach is based on energetic considerations and uses the concepts of network theory and port-Hamiltonian systems. We provide a general framework for addressing problems such as mapping the limited stroke of a ‘master’ joystick to the infinite stroke of a ‘slave’ vehicle, while preserving passivity of the closed-loop system in the face of potential time delays in communications links and limited sensor data.	aerial photography;aerobot;algorithm;haptic technology;joystick;network theory;robot;simulation;television;vii	Stefano Stramigioli;Robert E. Mahony;Peter I. Corke	2010	2010 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2010.5509591	network theory;hamiltonian system;telerobotics;robot;control engineering;embedded system;master/slave;vehicle dynamics;simulation;inspection;computer science;engineering;control system;artificial intelligence;feedback;haptic technology;coupling;force	Robotics	63.67209319648827	-19.065518067855816	3581
b2d12fbb2f5e45687e7aa5b4d9f134557ebf305f	an architecture for massive parallelization of the compact genetic algorithm	parallel genetic algorithm;fault tolerant;probabilistic model;genetic algorithm;compact genetic algorithm;evolutionary computing	This paper presents an architecture which is suitable for a massive parallelization of the compact genetic algorithm. The resulting scheme has three major advantages. First, it has low synchronization costs. Second, it is fault tolerant, and third, it is scalable. The paper argues that the benefits that can be obtained with the proposed approach is potentially higher than those obtained with traditional parallel genetic algorithms. In addition, the ideas suggested in the paper may also be relevant towards parallelizing more complex probabilistic model building genetic algorithms.	automatic parallelization;central processing unit;computer;distributed computing;experiment;fault tolerance;genetic algorithm;parallel computing;scalability;search for extraterrestrial intelligence;software release life cycle;speedup;statistical model	Fernando G. Lobo;Cláudio F. Lima;Hugo Martires	2004		10.1007/978-3-540-24855-2_49	statistical model;fault tolerance;meta-optimization;genetic algorithm;cultural algorithm;computer science;artificial intelligence;theoretical computer science;genetic operator;machine learning;genetic representation;distributed computing;evolutionary computation;population-based incremental learning	AI	26.201892615720347	-0.49194740140146226	3583
b8a48fe9ff1f773a8c74232aacd4661be2e3f696	ozone sensing based on palladium decorated carbon nanotubes	health research;uk clinical guidelines;biological patents;europe pubmed central;citation search;multiwall carbon nanotubes;gas sensor;uk phd theses thesis;life sciences;uk research reports;medical journals;ozone;europe pmc;biomedical research;pd nanoparticles;bioinformatics	Multiwall carbon nanotubes (MWCNTs) were easily and efficiently decorated with Pd nanoparticles through a vapor-phase impregnation-decomposition method starting from palladium acetylacetonates. The sensor device consisted on a film of sensitive material (MWCNTs-Pd) deposited by drop coating on platinum interdigitated electrodes on a SiO₂ substrate. The sensor exhibited a resistance change to ozone (O₃) with a response time of 60 s at different temperatures and the capability of detecting concentrations up to 20 ppb. The sensor shows the best response when exposed to O3 at 120 °C. The device shows a very reproducible sensor performance, with high repeatability, full recovery and efficient response.	carbon tetrachloride;coating excipient;fertilization;nanotubes;nanotubes, carbon;ozone;palladium;palladium:scnt:pt:hair:qn;parkinson disease;platinum;repeatability;response time (technology);sensor;walls of a building;electrode;vapor	Selene Capula Colindres;Khalifa Aguir;Felipe Cervantes Sodi;Luis Villa-Vargas;José A. Moncayo Salazar;Vicente Garibay Febles	2014		10.3390/s140406806	ozone;bioinformatics;electrical engineering;nanotechnology;physics	Mobile	94.57549272260201	-17.768092397395428	3595
8a3d7b261a9e1138f15316bc3487cb4f7f6fef38	a method of remote sensing image auto classification based on interval type-2 fuzzy c-means	remote sensing fuzzy set theory geophysical image processing image classification pattern clustering;uncertainty remote sensing clustering algorithms vegetation mapping partitioning algorithms matrix converters clustering methods;unsupervised classification remote sensing image auto classification interval type 2 fuzzy c means imperfect expressions pattern sets clustering algorithms pattern recognition algorithms fuzzy c means algorithm soft clustering methods hard clustering methods type 2 fuzzy set type 1 fuzzy set;remote sensing classification type 2 fuzzy sets uncertainty fuzzy c means fuzzy clustering it2fcm	The pattern set of a remote sensing image contains many kinds of uncertainties. Uncertain information can create imperfect expressions for pattern sets in various pattern recognition algorithms, such as clustering algorithms. Methods based the fuzzy c-means algorithm can manage some uncertainties. As soft clustering methods, They are known to perform better on auto classification of remote sensing images than hard clustering methods. However, if the clusters in a pattern set are of different density and high order uncertainty, performance of FCM may significantly vary depending on the choice of fuzzifiers. Thus, we cannot obtain satisfactory results by using type-1 fuzzy set. Type-2 fuzzy sets permit us to model various uncertainties which cannot be appropriately managed by type-1 fuzzy sets. This paper introduces the theory of interval type-2 fuzzy set into the unsupervised classification of remote sensing images and proposes the automatic remote sensing image classification method based on the interval type-2 fuzzy c-means. Experimental results indicate that our method can obtain more coherent clusters and more accurate boundaries from the data with density difference. Our type-2 fuzzy model can manage the uncertainties of remote sensing images more appropriately and get a more desirable result.	algorithm;cluster analysis;coherence (physics);computer vision;fuzzy clustering;fuzzy cognitive map;fuzzy set;interference (communication);k-means clustering;norm (social);pattern recognition;set theory;type-2 fuzzy sets and systems;unsupervised learning	Xianchuan Yu;Wei Zhou;Hui He	2014	2014 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2014.6891759	correlation clustering;fuzzy clustering;flame clustering;fuzzy classification;canopy clustering algorithm;neuro-fuzzy;machine learning;pattern recognition;data mining;mathematics;cluster analysis;fuzzy set operations	Robotics	2.5700631331299566	-39.61622384992407	3597
25e955612dbbdd24cea2fea2db477332d6abe6e7	prediction and analysis of mature microrna with flexible neural tree model		miRNA is a class of small non-coding RNA molecules, length of about 20–24 nucleotides. It combines with mRNA by the principle of complementary base pairing to achieve the objective of cracking or suppressing mRNA, which has the function of gene regulation. Therefore, study on the prediction of miRNA is always the hot topic in bioinformatics. In this paper, we drew on a new method of feature extraction and combined the flexible neural tree (FNT) to predict miRNA. For comparison, we adopted XUE dataset, used the training dataset to train the classifier, and then used the classifier to test on testing dataset. The final average accuracy rate of our experiment that is 93.7% is higher than the prediction method of XUE triple-SVM. So our method achieves a better classification effect.	bioinformatics;feature extraction;password cracking	Rongbin Xu;Huijie Shang;Gaoqiang Yu;Yunguang Lin	2017		10.1007/978-3-319-63312-1_74	machine learning;pattern recognition;regulation of gene expression;artificial intelligence;complementarity (molecular biology);feature extraction;computer science;decision tree model;microrna	NLP	9.834734477454278	-54.66304230115757	3598
0b51479fceb9306d8fb59425016fa80fc1a3425b	detection of potato storage disease via gas analysis: a pilot study using field asymmetric ion mobility spectrometry	potato storage disease;plant diseases;early disease detection;mass spectrometry;solanum tuberosum;faims;pectobacterium carotovorum;volatile organic compounds;plant tubers;biological markers;soft rot;sb plant culture	Soft rot is a commonly occurring potato tuber disease that each year causes substantial losses to the food industry. Here, we explore the possibility of early detection of the disease via gas/vapor analysis, in a laboratory environment, using a recent technology known as FAIMS (Field Asymmetric Ion Mobility Spectrometry). In this work, tubers were inoculated with a bacterium causing the infection, Pectobacterium carotovorum, and stored within set environmental conditions in order to manage disease progression. They were compared with controls stored in the same conditions. Three different inoculation time courses were employed in order to obtain diseased potatoes showing clear signs of advanced infection (for standard detection) and diseased potatoes with no apparent evidence of infection (for early detection). A total of 156 samples were processed by PCA (Principal Component Analysis) and k-means clustering. Results show a clear discrimination between controls and diseased potatoes for all experiments with no difference among observations from standard and early detection. Further analysis was carried out by means of a statistical model based on LDA (Linear Discriminant Analysis) that showed a high classification accuracy of 92.1% on the test set, obtained via a LOOCV (leave-one out cross-validation).	cluster analysis;color gradient;cross reactions;cross-validation (statistics);early diagnosis;endocrine system diseases;experiment;infection;ions;k-means clustering;linear discriminant analysis;microsoft outlook for mac;pectobacterium atrosepticum;principal component analysis;progressive disease;software rot;statistical model;test set;unsupervised learning;statistical cluster;vapor	Massimo Rutolo;James A. Covington;John P. Clarkson;Daciana Iliescu	2014		10.3390/s140915939	chemistry;mass spectrometry	ML	8.97760679232404	-54.16454643423616	3599
d09344d03bc54de004e4715933209949801ea64d	an extraction method of lip movement images from successive image frames in the speech activity extraction process	image frames;image processing;acoustic noises;acoustic noise;lip movement image;speech image processing system;speech recognition;extraction method	In this paper, we propose an extraction method of lip movement images from successive image frames and present the possibility to utilize lip movement images in the speech activity extraction process of speech recognition phase. The image frames are acquired from the PC image camera with the assumption that facial movement is limited during talking. First of all, one new lip movement image frame is generated with comparing two successive image frames each other. Second, the fine image noises are removed. Each fitness rate is calculated by comparing the lip feature data as objectly separated images. It is analyzed whether or not there is the lip movement image through verification to the objects and three images which have higher rates in their fitnesses. As a result of linking the speech & image processing system, the interworking rate shows 99.3% even in the various illumination environments. It was visually confirmed that lip movement images are tracked and can be utilized in speech activity extraction process.	activity tracker;feature data;formal verification;frame language;image processing;speech recognition	Eung-Kyeu Kim;Soo-Jong Lee;Nohpill Park	2010		10.1007/978-3-642-15399-0_33	computer vision;feature detection;speech recognition;image processing;computer science;communication	Vision	40.23970953436427	-48.61410936552465	3607
927894c8fbdd43a11905b60d9929b338a61fd67c	a model predictive combined planning and control approach for guidance of automated vehicles	vehicles trajectory vehicle dynamics planning tires wheels predictive models;trajectory;vehicle dynamics collision avoidance nonlinear control systems optimal control predictive control trajectory control;traffic scenarios model predictive combined planning and control approach automated vehicle guidance combined trajectory planning and control optimal control theory active safety system automated driving collision avoidance critical situations cpc algorithm timed elastic band approach teb approach vehicle dynamics model stable vehicle guidance trajectory generation nonlinear model predictive control method nmpc method;predictive models;planning;tires;vehicles;vehicle dynamics;wheels	A novel approach for combined trajectory planning and control is presented in this contribution. The developed method integrates optimal control theory and trajectory planning and leads to an active safety system, which is applicable for automated driving and can avoid collisions in critical situations. The Combined Planning and Control (CPC) algorithm extends the Timed Elastic Band (TEB) approach by a suitable vehicle dynamics model, which facilitates stable vehicle guidance. The problem of trajectory generation is reformulated such that a nonlinear model predictive control (NMPC) method can be applied. The analysis of different traffic scenarios shows the functionality of the developed concept.	algorithm;automated planning and scheduling;autonomous car;cartesian perceptual compression;computation;control theory;lateral thinking;loss function;mathematical optimization;nonlinear system;optimal control;optimization problem;real-time clock;tebibyte	Christian Gotte;Martin Keller;Carsten Hass;Karl-Heinz Glander;Alois Seewald;Torsten Bertram	2015	2015 IEEE International Conference on Vehicular Electronics and Safety (ICVES)	10.1109/ICVES.2015.7396896	control engineering;simulation;engineering;control theory	Robotics	61.046302726449895	-17.211023234221692	3609
2a404fb7906f505818d6f5744de20e8c537004e1	synchronization control for arrays of coupled discrete-time delayed cohen-grossberg neural networks	coupled systems;cohen grossberg neural network;discrete time cohen grossberg neural networks;discrete time;exponential synchronization;coupled system;lmi approach;linear matrix inequality;neural network model;time varying delay;lyapunov krasovskii functional	This paper investigates the global exponential synchronization for an array of coupled discrete-time Cohen–Grossberg neural networks (CGNNs) with time-varying delay, in which both the constant coupling and delayed one are considered. Through constructing an improved Lyapunov–Krasovskii functional, the delay-dependent sufficient condition is obtained to guarantee the global synchronizaits feasibility can be easily checked by resorting to Matlab LMI Toolbox. Moreover, the addressed system can include some famous neural network models as its special cases, which can help extend those present results. Finally, the effectiveness of the proposed method can be further illustrated with the help of two numerical examples. & 2010 Elsevier B.V. All rights reserved.	artificial neural network;linear matrix inequality;lyapunov fractal;matlab;numerical analysis;numerical method;time complexity	Tao Li;Aiguo Song;Shumin Fei	2010	Neurocomputing	10.1016/j.neucom.2010.02.018	discrete time and continuous time;linear matrix inequality;computer science;artificial intelligence;machine learning;control theory;mathematics;artificial neural network	AI	72.63475424339524	1.9551744240302085	3617
1c305332dfad26e7e837d119416a83ec87a8bc6d	fuzzy control in real-time for vision guided autonomous mobile robots	fuzzy controller;image processing;autonomous vehicle;real time;fuzzy control;real time simulation;autonomous mobile robot;development environment;camera calibration	To refine lateral control in an autonomous vehicle, a number of fuzzy controllers were developed in real-time simulation trials using real-world video images. The controller with the best results from the simulation trials was then mounted on the experimental vehicle ATHENE and shown to be functionally sound under the real-world circumstances of letting it drive through a corridor. The results were recorded and evaluated. One particular advantage of the fuzzy controller became obvious: input of the required quality could be derived relatively easily from the video images by means of simple image processing without extensive camera calibration. The development environment for the simulation and image processing are described in this paper, and the results are presented and explained.	autonomous robot;fuzzy control system;real-time transcription	Bernhard Blöchl	1993		10.1007/3-540-56920-0_13	control engineering;mobile robot;computer vision;simulation	Robotics	58.49161473800859	-29.33053576288294	3619
7d48a6ab76b671f138c788718c58647955ed9c0e	3d facial feature extraction and recognition : an investigation of 3d face recognition : correction and normalisation of the facial data, extraction of facial features and classification using machine learning techniques			facial recognition system;feature extraction;machine learning;three-dimensional face recognition	Sokyna M. S. Al-Qatawneh	2010				AI	31.130338602326585	-58.285801326798506	3620
8bf05cdd542fe63e86be08dd02bbc39839342a98	lipid clustering correlates with membrane curvature as revealed by molecular simulations of complex lipid bilayers	health research;uk clinical guidelines;lipids;biochemical simulations;biological patents;simulation and modeling;europe pubmed central;protein lipid interactions;citation search;uk phd theses thesis;life sciences;cholesterol;uk research reports;medical journals;europe pmc;cell membranes;biomedical research;protein interactions;membrane proteins;bioinformatics	Cell membranes are complex multicomponent systems, which are highly heterogeneous in the lipid distribution and composition. To date, most molecular simulations have focussed on relatively simple lipid compositions, helping to inform our understanding of in vitro experimental studies. Here we describe on simulations of complex asymmetric plasma membrane model, which contains seven different lipids species including the glycolipid GM3 in the outer leaflet and the anionic lipid, phosphatidylinositol 4,5-bisphophate (PIP2), in the inner leaflet. Plasma membrane models consisting of 1500 lipids and resembling the in vivo composition were constructed and simulations were run for 5 µs. In these simulations the most striking feature was the formation of nano-clusters of GM3 within the outer leaflet. In simulations of protein interactions within a plasma membrane model, GM3, PIP2, and cholesterol all formed favorable interactions with the model α-helical protein. A larger scale simulation of a model plasma membrane containing 6000 lipid molecules revealed correlations between curvature of the bilayer surface and clustering of lipid molecules. In particular, the concave (when viewed from the extracellular side) regions of the bilayer surface were locally enriched in GM3. In summary, these simulations explore the nanoscale dynamics of model bilayers which mimic the in vivo lipid composition of mammalian plasma membranes, revealing emergent nanoscale membrane organization which may be coupled both to fluctuations in local membrane geometry and to interactions with proteins.	cholesterol;cluster analysis;composition;computer simulation;concave function;emergence;gnu nano;genetic heterogeneity;glycolipids;interaction;large;leaflet device component;lipid bilayers;mammals;molecular modelling;phosphatidylinositol 4,5-diphosphate;plasma active;plasma membrane;tissue membrane;video-in video-out;membrane organization;statistical cluster	Heidi Koldsø;David Shorthouse;Jean Hélie;Mark S. P. Sansom	2014		10.1371/journal.pcbi.1003911	protein–protein interaction;polar membrane;biology;biochemistry;model lipid bilayer;membrane fluidity;lipid bilayer;lipid bilayer fusion;cell biology;elasticity of cell membranes;bioinformatics;lipid bilayer phase behavior;membrane biophysics;membrane protein;orientations of proteins in membranes database;biological membrane	Comp.	8.638279659707838	-64.24007983440029	3622
b20169e599c38cdbfed323a1ffb04fa2df1d0416	nabla-net: a deep dag-like convolutional architecture for biomedical image segmentation		Biomedical image segmentation requires both voxel-level information and global context. We report on a deep convolutional architecture which combines a fully-convolutional network for local features and an encoder-decoder network in which convolutional layers and maxpooling compute high-level features, which are then upsampled to the resolution of the initial image using further convolutional layers and tied unpooling. We apply the method to segmenting multiple sclerosis lesions and gliomas.	directed acyclic graph;image segmentation	Richard McKinley;Rik Wepfer;Tom Gundersen;Franca Wagner;Andrew Chan;Roland Wiest;Mauricio Reyes	2016		10.1007/978-3-319-55524-9_12	market segmentation;architecture;convolutional neural network;image segmentation;scale-space segmentation;nabla symbol;computer vision;artificial intelligence;computer science;pattern recognition	Vision	30.599774844553515	-74.40016881295347	3626
f21b8e774dc106959121214c538b87adfd270d43	constructing accurate fuzzy classification systems: a new approach using weighted fuzzy rules	fuzzy classification;weighted fuzzy rule;design process;fuzzy rules;rule based;ruleweight;classification;fuzzy set theory;rule weight specification fuzzy classification system weighted fuzzy rule descriptive classifier category accurate classifier category knowledge representation;fuzzy classification system;accurate classifier category;fuzzy rule base;classification system;prediction accuracy;pattern classification;generalization accuracy fuzzy systems classification ruleweight;pattern classification fuzzy set theory fuzzy systems knowledge based systems knowledge representation;fuzzy systems fuzzy sets design engineering learning systems knowledge representation knowledge based systems computer science process design computer simulation accuracy;descriptive classifier category;knowledge representation;generalization accuracy;computer simulation;fuzzy systems;knowledge based systems;fuzzy system;rule weight specification	Different approaches to design fuzzy rule-based classification systems can be grouped into two main categories: descriptive and accurate. In the descriptive category, the emphasis is on the interpretability of the resulting classifier. The classifier is usually represented by a set of short fuzzy rules (i.e., with a few number of antecedent conditions) that make it a suitable tool for knowledge representation. In the accurate category, the generalization ability of the classifier is the main target in the design process and no attempt is made to use understandable fuzzy rules in constructing the rule base. In this paper, we propose a simple and efficient method to construct an accurate fuzzy classification system. We use rule-weight as a simple mechanism to tune the classifier and propose a new method of rule-weight specification for this purpose. Through computer simulations on some data sets from UCI repository, we show that the proposed scheme achieves better prediction accuracy compared with other fuzzy and non- fuzzy rule-based classification systems proposed in the past.	computer simulation;fuzzy classification;fuzzy rule;knowledge representation and reasoning;logic programming;rule-based system	Seyed Mostafa Fakhrahmad;Mansoor Zolghadri Jahromi	2007	Computer Graphics, Imaging and Visualisation (CGIV 2007)	10.1109/CGIV.2007.31	fuzzy logic;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy classification;fuzzy number;neuro-fuzzy;machine learning;pattern recognition;data mining;mathematics;fuzzy associative matrix;fuzzy set operations	Web+IR	4.539561805127291	-27.78398498606484	3628
4e15297ced27fdd83ee0ab20bacda80b2483d4ce	model of the signal processing and decision making process of hexapods with static-stable walking	acyclic gait signal processing model decision making process hexapods static stable walking cognitive control system class insecta six legged insect six legged robot design six legged arthropod locomotion reinforcement learning principle learning methods;acyclic gait;stability cognitive systems decision making gait analysis learning artificial intelligence legged locomotion signal processing;cognitive systems;learning;legged locomotion;reinforcement learning;service robots;static stable walking;hexapod;stability;control system;signal processing;legged locomotion learning robot kinematics mathematical model insects service robots;gait analysis;mathematical model;learning artificial intelligence;insects;static stable walking acyclic gait control system hexapod reinforcement learning;robot kinematics	The aim of this paper is to describe a model of the cognitive control system which allows the hexapod to static stable walk on terrain with obstacles. Therefore, the main aim is to model the decision making process of the insect. Hexapods are six-legged arthropods of the class Insecta. The nervous system of six-legged insect controls six legs. Model of the control system of the static stability of hexapods could help in the design of six-legged robots. Our control system assumes and ensures the static stable walking because the concept of static stability of walking is the most used in six-legged arthropod locomotion. The control system is based on the reinforcement learning principle because it is assumed in the real organism. The model of control system enables to walk on flat surface with small obstacles. We also describe the verification of algorithms and application of learning methods for cyclic and acyclic gait.	algorithm;control system;directed acyclic graph;reinforcement learning;robot;signal processing	Patrik Kutilek;Slavka Viteckova;Jan Hejda	2012	2012 35th International Conference on Telecommunications and Signal Processing (TSP)	10.1109/TSP.2012.6256302	simulation;gait analysis;stability;computer science;control system;signal processing;mathematical model;hexapod;reinforcement learning;robot kinematics	Robotics	63.66843515562745	-25.016022179793858	3629
a009dc9e38dcb214896087f2dfa760b4356e7129	data visualization using kernel density estimation to examine patterns of physician practice	kernel density estimation;open heart surgery;data mining;patient care;data analysis;cardiopulmonary bypass;kernel density estimate;data visualization;coronary artery;healthcare outcomes	Studies have shown that examinations of physician practices and variation can improve patient care while reducing costs. However, most of the studies rely more on physician consensus than on data analysis. The existence of large hospital databases allows for the examination of the relationship between practice and outcome. Because it is generally not known beforehand which practice optimizes outcome, particularly given the variability in patient risk, data mining techniques are ideal to examine the relationship. In particular, this paper uses the data visualization technique of kernel density estimation to explore and define physician practices. Three physician practices were examined at a hospital during 2001--2002. The first physician practice identified was the administering of heparin to 100 patients as an anti-coagulant during angioplasty to open blocked coronary arteries. The second practice examined was two different open-heart procedures: one done on a cardiopulmonary bypass pump, the other done off bypass pump. The third physician practice examined the prescribing of antibiotics for patients undergoing open-heart surgery.	data mining;data visualization;database;heart rate variability;kernel density estimation	Patricia B. Cerrito	2003		10.1145/952532.952588	kernel density estimation;computer science;data visualization;statistics	HCI	2.969708561359096	-76.50945915013372	3632
9b19f9d4a6c1cef78dfae7f7cd274500fe9f6998	dynamic regions of interest for interactive flow exploration	categories and subject descriptors according to acm ccs i 3 1 computer graphics parallel processing i 3 7 computer graphics virtual reality i 6 6 simulation and modelling simulation output analysis;region of interest	Virtual Reality (VR) provides a useful tool for understanding complex, unsteady flow phenomena. The user can directly interact with the data and therefore benefits from a spatial coherence of action and result. However, visualization in virtual environments imposes very high demands on interactivity in order to maintain this coherence. Exploration of large, unsteady datasets in VR requires efficient visualization or data reduction algorithms to produce results within acceptable waiting times. We propose a technique for reducing required data especially suited for direct interaction in virtual environments. We use a distributed system to parallely extract a dynamic region of interest (DROI) from the simulation data. This DROI is adapted according to the user's interaction behavior and allows for the analysis of local flow features. With this reduction we provide interactive extraction of local features from large, time-varying datasets.	algorithm;coherence (physics);distributed computing;interactivity;region of interest;simulation;virtual reality	Marc Wolter;Christian H. Bischof;Torsten Kuhlen	2007		10.2312/EGPGV/EGPGV07/053-060	simulation;computer science;operating system;multimedia;computer graphics (images);region of interest	Visualization	70.04244262200278	-50.09841155891991	3640
a6f4659229740bd4a33916ca713e447fb84151cc	template adaptation based fingerprint verification	bayesian estimation;fingerprint recognition fingers image matching authentication feature extraction image segmentation bayesian methods sensor systems iterative closest point algorithm image sensors;query fingerprint;image matching;bayes methods;fingerprint matching;minutiae based template adaptation;fingerprint authentication;adaptive algorithm;minutiae credibility;minutiae credibility fingerprint verification minutiae based template adaptation fingerprint authentication query fingerprint fingerprint matching bayesian estimation;bayesian estimator;image matching bayes methods fingerprint identification;fingerprint identification;false accept rate;fingerprint verification	This paper proposes a minutiae-based template adaptation algorithm which can be applied after the fingerprint authentication process. The algorithm updates a template by using a query fingerprint, which is successfully verified by the fingerprint matcher as a high quality genuine input. This algorithm generates an updated minutiae set by using not only the minutiae but also local fingerprint quality information and utilizes a successive Bayesian estimation to evaluate the credibility of minutiae and their types. The proposed algorithm updates fingerprint minutiae information in the template as well as appends new minutiae from the query fingerprint. Preliminary experiments show an average 32.7% EER reduction and an even higher matching accuracy improvement at low false accept rates	algorithm;authentication;display resolution;enhanced entity–relationship model;experiment;feature extraction;fingerprint recognition;minutiae;sensor	Choonwoo Ryu;Hakil Kim;Anil K. Jain	2006	18th International Conference on Pattern Recognition (ICPR'06)	10.1109/ICPR.2006.1105	fingerprint verification competition;fingerprint;computer vision;bayes estimator;computer science;pattern recognition;data mining;fingerprint recognition	Vision	32.952098640293016	-62.4745489848719	3643
53d241926c860efa0d8ed712ed999cdf3012259f	color me noisy: example-based rendering of hand-colored animations with temporal noise control	i 3 3 computer graphics picture image generation bitmap and framebuffer operations;paper;i 3 4 computer graphics graphics utilities paint systems;cuda;i 3 m computer graphics miscellaneous visual arts;image generation;nvidia;algorithms;computer science;3d graphics and realism;categories and subject descriptors according to acm ccs	We present an example-based approach to rendering hand-colored animations which delivers visual richness comparable to real artwork while enabling control over the amount of perceived temporal noise. This is important both for artistic purposes and viewing comfort, but is tedious or even intractable to achieve manually. We analyse typical features of real hand-colored animations and propose an algorithm that tries to mimic them using only static examples of drawing media. We apply the algorithm to various animations using different drawing media and compare the quality of synthetic results with real artwork. To verify our method perceptually, we conducted experiments confirming that our method delivers distinguishable noise levels and reduces eye strain. Finally, we demonstrate the capabilities of our method to mask imperfections such as shower-door artifacts.	algorithm;cg artist;coherence (physics);computer graphics;experiment;lively kernel;low-pass filter;synthetic intelligence;visual artifact;visual computing	Jakub Fiser;Michal Lukác;Ondrej Jamriska;Martin Cadík;Yotam I. Gingold;Paul Asente;Daniel Sýkora	2014	Comput. Graph. Forum	10.1111/cgf.12407	computer vision;2d computer graphics;computer science;artificial intelligence;operating system;real-time computer graphics;multimedia;graphics software;programming language;computer graphics;algorithm;3d computer graphics;computer graphics (images)	Graphics	63.372742945125	-50.35644804413785	3649
3462d6cb9bc7d1634934efe6eb64ec06fd38f5b3	last- und systemregelung im bs 2000		This article shows the potential use of a systematic management of storm risk in forestry with a focus on the potentials and drawbacks of insurance to cope with future storm events. First, the risk management process which consists of risk analysis, risk treatment and monitoring is described and discussed in the context of the relevant characteristics of central European forestry (long production periods, low profitability). The main methods for analysing storm risks in forestry such as expert systems, regression models and mechanistic models and their characteristics are presented. Finally, we focus on risk transfer through insurance: after discussing the demandand supply-related reasons for poor insurance performance, we propose an insurance model for forest stands. The components of the calculated gross premiums are a net premium (representing the pure risk which is the loss potential multiplied by the probability of a storm) and a risk premium (representing the insurers' risk depending on the size of the insured forest area). The model is demonstrated using a practical example.	expert system;it risk management;loss function	G. Dedié	1983		10.1007/978-3-642-69034-1_4		ML	4.8281095524464535	-8.671744925993767	3657
f540612902765dd4e675bc02784e190228551bd2	gpu accelerated normalized mutual information and b-spline transformation	normalized mutual information;categories and subject descriptors according to acm ccs i 4 3 image processing and computer vision registration	Visualization of multimodal images in medicine and other application areas requires correct and efficient registration. Optimally, the alignment operation is made an integral part of the rendering process. Voxel based approaches using mutual information ensure high quality similarity measurement. As a limiting factor, high computational load is caused since for every iteration of the optimization procedure one volume is transformed into the coordinate system of the other, a 2D histogram is generated and mutual information is computed. The expensive trilinear interpolation operations are well supported by 3D texture mapping hardware. However, existing strategies compute the histogram and mutual information on the CPU and thus require a cost intensive data transfer. Overcoming this considerable bottleneck, we introduce a new approach that efficiently supports all computations on modern graphics cards. This makes expensive data transfers from GPU to main memory dispensable. Due to its modularity, the presented approach can be integrated into general frameworks. As a major result, the speed improvement over existing GPU-CPU strategies amounts to a factor of 4 and over pure conventional CPU techniques to more than a factor of 10. Overall, the suggested strategy contributes considerably to integration of multimodal registration directly into interactive volume visualization.		Matthias Tessmann;Christian Eisenacher;Frank Enders;Marc Stamminger;Peter Hastreiter	2008		10.2312/VCBM/VCBM08/117-124	computer vision;computer science;theoretical computer science;machine learning	ML	69.93635565989578	-52.60276563472642	3658
e8f3131bb1e3df4c969992ce8ab0947e0c14183b	three-dimensional empirical mode decomposition based hyperspectral band selection method		Hyperspectral technology is a huge leap of remote sensing technology, however, how to make full use of its rich information is a difficult problem. A novel hyperspectral band selection method based on 3D empirical mode decomposition (3D-EMD) is proposed by constructing the upper approximation and the lower approximation of the whole image. The hyperspectral image (HSI) is decomposed into a set of tridimensional intrinsic mode functions (TIMFs) and bands are selected from the HSI using each TIMF based on optimum index factor (OIF). These bands are combined into a band combination. Experimental results demonstrate that the proposed method yields improved decomposition performance on the HSI and increases classification accuracy.	approximation;hilbert–huang transform;horizontal situation indicator;optical internetworking forum	Miao Zhang;Wenbo Yu;Yi Shen	2018	IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2018.8518164	hilbert–huang transform;interpolation;computer vision;computer science;artificial intelligence;hyperspectral imaging	Robotics	30.293457954925167	-43.850464688723626	3666
15aaf9082b1f8bc0ed0efa148b484eb05943d699	an embedded structure of model reference adaptive system	lyapunov methods;recursive least square;lyapunov stability;least squares approximations;model following embedded structure model reference adaptive system closed loop subsystem lyapunov stability recursive least square algorithm convergence rate trajectory tracking;convergence;model reference adaptive system;transfer functions;lyapunov function;closed loop systems;probability density function;reference model;convergence rate;data mining;stability;model reference adaptive control systems;adaptation model;trajectory;pi controller model reference adaptive system lyapunov function recursive least square algorithm;adaptive systems;control system synthesis;recursive least square algorithm;mathematical model;pi controller;trajectory tracking;stability closed loop systems control system synthesis convergence least squares approximations lyapunov methods model reference adaptive control systems;adaptive systems adaptive control programmable control lyapunov method open loop systems error correction robotics and automation resonance light scattering trajectory control systems	The traditional model reference adaptive system (MRAS) is of parallel configuration. By representing the reference model as an equivalent closed-loop subsystem, these two subsystems can share a controller virtually. An embedded structure of MRAS is thereby presented. In this paper, two approaches are developed for designing the adaptive laws. The adaptive law based on Lyapunov stability theory is designed for low-order systems. The adaptive law based on the recursive least-square (RLS) algorithm is suitable for both low- and high-order systems. The RLS-based adaptive law provides a quick convergence rate for the controller adaption. MRAS with RLS-based adaptation law performs better in trajectory tracking and model following. Simulation results show the effectiveness of the proposed method.	adaptive system;algorithm;embedded system;lyapunov fractal;moore's law;rate of convergence;recursion;recursive least squares filter;reference model;simulation	Qiwen Yang;Yuncan Xue;Simon X. Yang;Qiuye Li;Rong Li;Max Q.-H. Meng	2008	2008 10th International Conference on Control, Automation, Robotics and Vision	10.1109/ICARCV.2008.4795528	pid controller;control engineering;mathematical optimization;probability density function;reference model;convergence;stability;adaptive control;lyapunov function;computer science;trajectory;adaptive system;mathematical model;control theory;mathematics;transfer function;rate of convergence;statistics	Robotics	65.27546629160629	-9.165669267575556	3669
d7341a306bb606ba91b9e8dc16a72ea2a05b888b	dcdg-ea: dynamic convergence-diversity guided evolutionary algorithm for many-objective optimization		Abstract Maintaining a good balance between the convergence and the diversity is particularly crucial for the performance of the evolutionary algorithms (EAs). However, the traditional multi-objective evolutionary algorithms, which have shown their competitive performance with a variety of practical problems involving two or three objectives, face significant challenges in case of problems with more than three objectives, namely many-objective optimization problems (MaOPs). This paper proposes a dynamic convergence–diversity guided evolutionary algorithm, namely (DCDG-EA) for MaOPs by employing the decomposition technique. Besides, the objective space of MaOPs is divided into K subspaces by a set of uniformly distributed reference vectors. Each subspace has its own subpopulation and evolves in parallel with the other subspaces. In DCDG-EA, the balance between the convergence and the diversity is achieved through the convergence–diversity based operator selection (CDOS) strategy and convergence–diversity based individual selection (CDIS) strategy. In CDOS, for each operator of the set of operators, a selection probability is assigned which is related to its convergence and diversity capabilities. Based on the attributed selection probabilities, an appropriate operator is selected to generate the offsprings. Furthermore, CDIS is used which allows to greatly overcome the inefficiency of the Pareto dominance approaches. It updates each subpopulation by using two independent distance measures that represent the convergence and the control diversity, respectively. The experimental results on DTLZ and WFG benchmark problems with up to 15 objectives show that our algorithm is highly competitive comparing with the four state-of-the-art evolutionary algorithms in terms of convergence and diversity.		Zhiyong Li;Ke Lin;Mourad Nouioua;Shilong Jiang;Yu Gu	2019	Expert Syst. Appl.	10.1016/j.eswa.2018.09.025	operator (computer programming);machine learning;linear subspace;artificial intelligence;evolutionary algorithm;inefficiency;subspace topology;pareto principle;convergence (routing);computer science;optimization problem	Theory	24.159611860047857	-3.9835379262320876	3671
cc2ddaa991ba94494c4da17a85382334731ba527	pseudo almost-periodic solution of shunting inhibitory cellular neural networks with delay	lyapunov function;almost periodic solution;cellular neural network;global exponential stability;existence and uniqueness	Shunting inhibitory cellular neural networks are studied. Some sufficient criteria are obtained for the existence and uniqueness of pseudo almost-periodic solution of this system. Our results improve and generalize those of the previous studies. This is the first paper considering the pseudo almost-periodic SICNNs. Furthermore, several methods are applied to establish sufficient criteria for the globally exponential stability of this system. The approaches are based on constructing suitable Lyapunov functionals and the well-known Banach contraction mapping principle.		Haihui Wu	2011	J. Applied Mathematics	10.1155/2011/510789	mathematical optimization;mathematical analysis;cellular neural network;lyapunov function;control theory;mathematics	ML	73.70250785308544	2.765923693673894	3674
ba466a9e6a84de73f94f30f2f9cd8b0e971abd3f	multilayer complex network descriptors for color-texture characterization		A new method based on complex networks is proposed for color-texture analysis. The proposal consists on modeling the image as a multilayer complex network where each color channel is a layer, and each pixel (in each color channel) is represented as a network vertex. The network dynamic evolution is accessed using a set of modeling parameters (radii and thresholds), and new characterization techniques are introduced to capt information regarding within and between color channel spatial interaction. An automatic and adaptive approach for threshold selection is also proposed. We conduct classification experiments on 5 well-known datasets: Vistex, Usptex, Outex13, CURet and MBT. Results among various literature methods are compared, including deep convolutional neural networks with pre-trained architectures. The proposed method presented the highest overall performance over the 5 datasets, with 97.7 of mean accuracy against 97.0 achieved by the ResNet convolutional neural network with 50 layers.	artificial neural network;channel (digital image);color space;complex network;convolutional neural network;experiment;linear discriminant analysis;model-based testing;multilayer perceptron;pixel;relevance;vertex (graph theory)	Leonardo F. S. Scabini;Rayner H. Montes Condori;Wesley Nunes Gonçalves;Odemir Martinez Bruno	2018	CoRR		machine learning;complex network;convolutional neural network;mathematics;pixel;vertex (geometry);residual neural network;artificial intelligence;channel (digital image);pattern recognition	Vision	29.20492607058723	-55.7648526799511	3680
65f470762f70407b6ae71d026aeaec0e4ccc50b3	a physical piano model for music performance	music performance;arte	The implementation of a complete physical model of the piano is demonstrated. It includes 88 lossy and dispersive strings (with full polyphony), hammers and dampers, and a soundboard load/radiation.	dispersive partial differential equation;lossy compression	Gianpaolo Borin;Davide Rocchesso;Francesco Scalcon	1997			speech recognition;acoustics;music	DB	47.03037840880159	-24.66318111416999	3681
31a00defacf5813e1871ec186e6e113cae54a03a	optical waveguides with compound multiperiodic grating nanostructures for refractive index sensing	aperiodic;refractive index sensing;compound grating;photonic crystal;nanostructure;optical sensing	The spectral characteristics and refractive index sensitivity of compound multiperiodic grating waveguides are investigated in theory and experiment. Compound gratings are formed by superposition of two or more monoperiodic gratings. Compared to monoperiodic photonic crystal waveguides, compound grating waveguides offer more degrees of design freedom by choice of component grating periods and duty cycles. Refractive index sensing is achieved by evaluating the wavelength or intensity of guided mode resonances in the reflection spectrum. We designed, fabricated, and characterized 24 different compound multiperiodic nanostructured waveguides for refractive index sensing. Simulations are carried out with the Rigorous Coupled Wave Algorithm (RCWA).The resulting spectra, resonance sensitivities, and quality factors are compared tomonoperiodic as well as to three selected aperiodic nanostructures (Rudin-Shapiro, Fibonacci, and Thue-Morse). The refractive index sensitivity of the TE resonances is similar for all types of investigated nanostructures. For the TM resonances the compound multiperiodic nanostructures exhibit higher sensitivity values compared to the monoperiodic nanostructure and similar values as the aperiodic nanostructures. No significant influence of the compound grating duty cycles on the sensitivity is observed.		Lars Thorben Neustock;Sabrina Jahns;Jost Adam;Martina Gerken	2016	J. Sensors	10.1155/2016/6174527	photonic crystal;electronic engineering;nanostructure;optoelectronics;nanotechnology;optics;physics	HCI	93.51949482281665	-14.847197127749748	3682
2ece5984f863fdd07932f1cc5dd4cf4fb4fdf5b3	disparity-compensated picture prediction for multi-view video coding	algorithme rapide;analisis contenido;compression algorithm;multi view video coding;organisation internationale;sistema experto;image coding;norme iso;multimedia;motion compensation;normalisation;securite informatique;norma iso;disparity;base connaissance;classification;iso standard;disparidad;motion compensated;multiple view;computer security;compensation mouvement;codage image;video coding;content analysis;codage video;telecomunicacion;seguridad informatica;fast algorithm;telecommunication;international standards organization;detection algorithm;normalizacion;international organization;vue multiple;international telecommunication union;base conocimiento;systeme expert;analyse contenu;organizacion internacional;disparite;algoritmo rapido;clasificacion;standardization;vista multiple;knowledge base;expert system	Multi-view video coding (MVC) is currently being standardized by International Standardization Organization (ISO) and International Telecommunication Union (ITU). Although translation-based motion compensation can be applied to the picture prediction between different cameras, a better prediction exists if the camera parameters are known. This paper analyses the rules between pictures taken at the parallel or arc camera arrangements where the object is facing to an arbitrary direction. Based on the derived rules, block-width, block-slant and block-height compensations are proposed for the accurate picture prediction. A fast disparity vector detection algorithm and an efficient disparity vector compression algorithm are also discussed.	binocular disparity;data compression	Takanori Senoh;Terumasa Aoki;Hiroshi Yasuda;Takuyo Kogure	2006		10.1007/11848035_92	data compression;knowledge base;simulation;content analysis;telecommunications;biological classification;computer science;artificial intelligence;motion compensation;expert system;standardization	AI	46.568449108015166	-14.031152522209391	3687
71b87998119e2055cd22ba5b82716b844b286654	shape from pairwise silhouettes for plan-view map generation	color processing;shape from silhouettes;plan view maps;condensation algorithm;particle filtering;person tracking	A plan-view map is a representation mechanism especially appropriated for people detection and tracking. It allows to represent volumetric information in a very compact and efficient way so that very effective detection and tracking algorithms can be applied using it. In spite of their advantages, plan-view maps have mainly been used with stereo cameras but not with monocular ones. The reason why is that many of the three-dimensional reconstruction algorithms using monocular vision impose strong conditions (such as controlled lightning, lack of occlusion, etc.) thus impeding their applicability in realistic environments. This paper presents two main novelties. First, a three-dimensional reconstruction algorithm that is especially appropriate for people detection and tracking because of its robustness to errors in the background subtraction. Second, a new technique for creating plan-view maps from volumetric reconstructions that reduces the amount of false positives in the occluded regions. We show experimentally that these techniques produce plan-view maps that can be reliably employed for tracking purposes. The proposed methods are compared with the traditional SfS obtaining relevant improvements.		Rafael Muñoz-Salinas;Enrique Yeguas-Bolivar;Luis Díaz-Más;Rafael Medina Carnicer	2012	Image Vision Comput.	10.1016/j.imavis.2012.02.005	computer vision;simulation;particle filter;computer science;mathematics;statistics;computer graphics (images)	Vision	52.27080377474029	-45.608307489823346	3689
80611d5425a64d39ea3d9e5641e866a2a299e83e	multi-sensor data fusion using bayesian programming : an automotive application	bayesian theory;probability;advanced driver assistance system;bayes methods;project engineering road vehicles computerised navigation uncertainty handling bayes methods inference mechanisms probability sensor fusion;inference mechanisms;laser radar;uncertainty handling;information content;multi sensor data fusion;bayesian programming carsense project advanced driver assistance systems multisensor data fusion probabilistic reasoning uncertainty handling road vehicles probability bayesian inference;robot control;project engineering;sensor fusion;bayesian methods automotive applications laser radar sensor fusion sensor systems laser fusion robustness control systems vehicles image sensors;probabilistic reasoning;road vehicles;computerised navigation	HAL is a multi-disciplinary open access archive for the deposit and dissemination of scientific research documents, whether they are published or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers. L’archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la diffusion de documents scientifiques de niveau recherche, publiés ou non, émanant des établissements d’enseignement et de recherche français ou étrangers, des laboratoires publics ou privés. Multi-Sensor Data Fusion Using Bayesian Programming : an Automotive Application Christophe Coué, Thierry Fraichard, Pierre Bessiere, Emmanuel Mazer	archive;bayesian programming;comefrom;hal;linear algebra;thierry coquand	Christophe Coué;Thierry Fraichard;Pierre Bessière;Emmanuel Mazer	2002		10.1109/IRDS.2002.1041379	lidar;computer vision;self-information;bayesian probability;computer science;engineering;machine learning;probability;data mining;sensor fusion;robot control;probabilistic logic;statistics	Vision	51.10725445927975	-33.55932892739443	3690
f7a760c66efccd1065214b1518bad6c9d0025f24	model-based tracking: temporal conditional random fields	probability;potential function visual tracking discriminative models conditional random fields;motion compensation;temporal conditional random fields;training;kalman filters;model based tracking;kalman filter;conditional random fields;constant motion;computational modeling;optical imaging;crf based predictor;probabilistic framework;object tracking;image sequence;tracking image sequences motion compensation probability;conditional random field;object motion modeling;graphical model;optical flow;computational modeling kalman filters training target tracking data models optical imaging;target tracking;constant motion model based tracking temporal conditional random fields probabilistic framework object motion modeling image sequence crf based predictor;potential function;visual tracking;template matching;discriminative model;discriminative models;tracking;data models;image sequences	We present Temporal Conditional Random Fields, a probabilistic framework for modeling object motion. The state-of-the-art discriminative approach for tracking is known as dynamic conditional random fields. This method models an event based on spatial and temporal relation between pixels in an image sequence without any prediction. To facilitate such a powerful graphical model with prediction and come up with a CRF-based predictor, we propose a set of new temporal relations for object tracking, with feature functions such as optical flow (calculated among consequent frames) and line filed features. We validate our proposed method with real data sequences and will show that the TCRF prediction is nearly equivalent with result of template matching. Experimental results indicate that our TCRF can predict future state of any maneuvering target with nearly zero error during its constant motion. Not only the proposed TCRF has a simple and easy to implement structure, but also it outperforms the state-of-the-art predictors such as Kalman filter.	conditional random field;graphical model;graphical user interface;kalman filter;kerrison predictor;optical flow;pixel;template matching	Mohammad Javad Shafiee;Zohreh Azimifar;Paul W. Fieguth	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5653823	kalman filter;computer vision;computer science;machine learning;pattern recognition;mathematics;conditional random field;discriminative model	Vision	45.04007300504789	-47.20426267372373	3698
371e4bd2f7d1b6fffdb551b55ac681011308b729	multi-domain manifold learning for drug-target interaction prediction			nonlinear dimensionality reduction	Ruichu Cai;Zhenjie Zhang;Srinivasan Parthasarathy;Anthony K. H. Tung;Zhifeng Hao;Wen Zhang	2016		10.1137/1.9781611974348.3	manifold alignment	ML	12.04672068769129	-54.583114884779434	3700
48cfc0498f04030b559f4dfa48ff45183d2bcf85	iocd: intensity order curve descriptor	journal;iocd descriptor;curve matching;intensity order	Curve matching plays an important role in pattern recognition, computer vision and image understanding. In several past years, this problem has been studied mainly based on the curve contour, while only little progress has been made using the texture feature of the curve's neighborhood. This paper develops a novel texture-based curve matching method called IOCD, which consists of three steps: (1) Curve support region (CSR) without assigning a dominant orientation is ̄rst determined; (2) CSR is equally partitioned into several order bins according to the overall intensity order; (3) The feature vector is computed based on the local intensity order mapping. Experiments prove that the proposed IOCD performs robust to image rotation, viewpoint change, illumination change, blur, noise and JPEG compress. The application of image mosaic further identi ̄es IOCD can achieve good matching performance.	computer vision;experiment;feature vector;gaussian blur;jpeg;ncsa mosaic;nl (complexity);numerical aperture;pattern recognition	Hongmin Liu;Shanshan Zhi;Zhiheng Wang	2013	IJPRAI	10.1142/S0218001413550112	computer vision;mathematical optimization;mathematics;geometry	Vision	37.35733788776355	-59.52826697652184	3701
53a9ca4500c15938afadd1228636e0e8731ababa	cmp antenna array gpr and signal-to-clutter ratio improvement	clutter;velocity estimation common midpoint cmp antenna array ground penetrating radar gpr landmine detection signal to clutter ratio scr;cmp antenna array gpr;cmp multifold stacking;landmine detection;antenna arrays;signal to clutter ratio;surface roughness;data processing;spectrum;rough surfaces;remote sensing by radar clutter ground penetrating radar landmine detection radar antennas radar signal processing;remote sensing by radar;ground penetrating radar gpr;antenna arrays ground penetrating radar landmine detection rough surfaces surface roughness radar detection clutter signal processing data processing stacking;ground penetrating radar;stacking;radar antennas;signal processing;time gating;common midpoint antenna array;radar detection;rough ground surface;antenna array;velocity estimation;data processing technique;common midpoint cmp antenna array;signal to clutter ratio scr;buried landmine detection;velocity spectrum;radar signal processing;surface clutter;background averaging;rough ground surface cmp antenna array gpr signal to clutter ratio ground penetrating radar buried landmine detection surface clutter time gating background averaging common midpoint antenna array data processing technique velocity spectrum cmp multifold stacking	Ground-penetrating radar (GPR) is recognized as a promising sensor for detecting buried landmines. In this case, the GPR antenna(s) must be elevated above the ground. However, this requirement results in heavy surface clutter. It is therefore necessary to overcome the effect. A commonly used procedure of time gating and background averaging cannot suit to small shallow nonmetallic landmine beneath a rough ground surface. In this letter, we proposed techniques to enhance the target signal through common midpoint (CMP) antenna array and data processing techniques, including velocity spectrum and CMP multifold stacking. The method has been tested using experiment data over a rough ground under which small plastic antipersonnel landmines is shallowly buried. The result shows the signal-to-clutter ratio was dramatically improved.	clutter;kriging;radar;sensor;smart antenna;stacking;velocity (software development)	Xuan Feng;Motoyuki Sato;Yan Zhang;Cai Liu;Fusheng Shi;Yonghui Zhao	2009	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2008.2006634	spectrum;ground-penetrating radar;surface roughness;data processing;computer science;stacking;signal processing;clutter;optics;antenna array;physics;remote sensing	Robotics	77.79162163867475	-67.6979313322786	3708
6c16077c28c30932c201067dd3c55da61c8f18ee	a particle system for interactive visualization of 3d flows	stream ribbon particle system interactive visualization steady 3d flow field uniform grids particle integration rendering graphics accelerator graphics processing unit graphics memory frame buffer interactive streaming real world experiment image texture splat visual flow analysis sorting network visibility sorting visualization geometry article lines;high resolution;paper;data visualization graphics computational modeling rendering computer graphics displays sorting geometry acceleration particle accelerators streaming media;glsl;interactive visualization;virtual reality;ati;ati radeon x800;visibility sorting;algorithms computer simulation computer systems data display environment imaging three dimensional models theoretical online systems rheology user computer interface;indexing terms;index terms flow visualization;image texture;particle tracing;data visualisation;sorting network;flow field;visualization geometry index terms flow visualization particle tracing programmable graphics hardware visibility sorting;visual cues;visualization geometry;particle system;nvidia;graphic processing unit;nvidia geforce 6800 ultra;programmable graphics hardware;opengl;computer science;fluid simulation;rendering computer graphics;image texture virtual reality interactive systems data visualisation rendering computer graphics real time systems;interactive systems;flow visualization;flow analysis;real time systems	We present a particle system for interactive visualization of steady 3D flow fields on uniform grids. For the amount of particles we target, particle integration needs to be accelerated and the transfer of these sets for rendering must be avoided. To fulfill these requirements, we exploit features of recent graphics accelerators to advect particles in the graphics processing unit (GPU), saving particle positions in graphics memory, and then sending these positions through the GPU again to obtain images in the frame buffer. This approach allows for interactive streaming and rendering of millions of particles and it enables virtual exploration of high resolution fields in a way similar to real-world experiments. The ability to display the dynamics of large particle sets using visualization options like shaded points or oriented texture splats provides an effective means for visual flow analysis that is far beyond existing solutions. For each particle, flow quantities like vorticity magnitude and A2 are computed and displayed. Built upon a previously published GPU implementation of a sorting network, visibility sorting of transparent particles is implemented. To provide additional visual cues, the GPU constructs and displays visualization geometry like particle lines and stream ribbons.	buffers;computer graphics;data-flow analysis;experiment;flow;framebuffer;graphics processing unit;image resolution;imagery;interactive visualization;particle system;quantity;requirement;scientific publication;shading;solutions;sorting network;subatomic particle	Jens H. Krüger;Peter Kipfer;Polina Kondratieva;Rüdiger Westermann	2005	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2005.87	fluid simulation;image texture;computer vision;particle tracking velocimetry;index term;image resolution;interactive visualization;sensory cue;flow visualization;sorting network;computer science;data-flow analysis;particle system;virtual reality;multimedia;data visualization;computer graphics (images)	Visualization	71.55492391097475	-50.53929883117306	3709
8d5d7f46b94cb0aa2a0634d04c3ba8568869952d	fine-grained bird species recognition via hierarchical subset learning	trees mathematics image classification learning artificial intelligence neural nets;deep convolutional neural network fine grained bird species recognition hierarchical subset learning fine grained bird species classification similarity tree;subset clustering fine grained classification;birds vegetation visualization feature extraction support vector machines neural networks training	We propose a novel method to improve fine-grained bird species classification based on hierarchical subset learning. We first form a similarity tree where classes with strong visual correlations are grouped into subsets. An expert local classifier with strong discriminative power to distinguish visually similar classes is then learnt for each subset. On the challenging Caltech200-2011 bird dataset we show that using the hierarchical approach with features derived from a deep convolutional neural network leads to the average accuracy improving from 64.5% to 72.7%, a relative improvement of 12.7%.	artificial neural network;convolutional neural network	ZongYuan Ge;Chris McCool;Conrad Sanderson;Alex Bewley;Zetao Chen;Peter I. Corke	2015	2015 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2015.7350861	computer science;machine learning;pattern recognition;data mining	Vision	29.04255774497233	-45.66809661575109	3710
358aa402bd0ad5d425700d743a72087a0f0c986f	gaze tracking system at a distance for controlling iptv	cameras tv reflection cornea calibration transforms monitoring;remote gaze tracking camera;gaze tracking;television cameras calibration iptv object tracking position control screens display;position detection;television cameras;tv control gaze tracking system iptv position detection calibrations camera tv screen geometric transform near infrared passing filter nir;near infrared;specular reflection;near infrared passing filter;geometric transform;monitoring;gaze tracking large screen tv remote gaze tracking camera geometric transform;position control;object tracking;tv screen;transforms;nir;large screen tv;gaze tracking system;tv;tv control;visible light;calibration;cornea;reflection;iptv;cameras;screens display;camera;calibrations	Gaze tracking is used for detecting the position that a user is looking at. In this research, a new gaze-tracking system and method are proposed to control a large-screen TV at a distance. This research is novel in the following four ways as compared to previous work: First, this is the first system for gaze tracking on a large-screen TV at a distance. Second, in order to increase convenience, the user's eye is captured by a remote gaze-tracking camera not requiring a user to wear any device. Third, without the complicated calibrations among the screen, the camera and the eye coordinates, the gaze position on the TV screen is obtained by using a simple 2D method based on a geometric transform with pupil center and four cornea specular reflections. Fourth, by using a near-infrared (NIR) passing filter on the camera and NIR illuminators, the pupil region becomes distinctive in the input image irrespective of the change in the environmental visible light. Experimental results showed that the proposed system could be used as a new interface for controlling a TV with a 60-inch-wide screen (16:9).	eye tracking;iptv;pixel;reflection (computer graphics);remote control;sensor;tracking system	Hyeon Chang Lee;Duc Thien Luong;Chul Woo Cho;Eui Chul Lee;Kang Ryoung Park	2010	IEEE Transactions on Consumer Electronics	10.1109/TCE.2010.5681143	near-infrared spectroscopy;computer vision;calibration;computer graphics (images)	Mobile	48.601585270170816	-41.98641621404554	3713
fc1ae33d5ba4b0bb5b54194df5265ab802967bd3	stream-based wandering monitoring system for elderly people with dementia	senior citizens global positioning system monitoring real time systems dementia tracking loops history;data stream;handicapped aids global positioning system;gps and geographic information;wandering detection;gps and geographic information wandering detection dementia stream processing data stream;dementia;stream processing;wandering monitoring system stream based wandering monitoring system elderly people dementia united nations elderly population aging population mental ability wandering behavior repetitive locomotion dementia movement elderly movement stream processing technique gps technology	Statistics reported by the United Nations indicate that the elderly population has grown rapidly and continuously. These indicators illustrate the fact that we are entering into the era of aging population. The elderly are usually faced with many problems resulting from the deterioration of health with increasing age. One of the major problems comes in the form of a decline in mental ability, commonly called dementia. This problem leads to wandering behavior - aimless movement, repetitive locomotion with no identifiable goal and disorientation to time and place. The wandering behavior often results in negative consequences such as getting lost and serious injuries or eventually death. To control and reduce negative consequences associated with wandering, we propose a novel wandering monitoring system that timely detect wandering patterns in the elderly with dementia movement. Our system applies a stream processing technique together with GPS technology to identify location and detect wandering behavior of the elderly in real-time. The experimental results show that our wandering monitoring system offers low-latency processing and can provide real-time and instantaneous responses. The average latency about 1 ms per additional elderly patient was observed with reasonable processing overheads.	blueprint;global positioning system;real-time clock;stream processing;tire-pressure monitoring system	Watsawee Sansrimahachai	2015	2015 15th International Symposium on Communications and Information Technologies (ISCIT)	10.1109/ISCIT.2015.7458292	psychology;simulation;developmental psychology;communication	Arch	7.779897834539991	-83.63002169778954	3716
61a3c45c9f802f9d5fa8d94fee811e203bac6487	a customized sparse representation model with mixed norm for undersampled face recognition	databases;mixed norm undersampled face recognition generic learning variational information customized sparse representation;linear regression;probes;learning systems;face face recognition dictionaries linear regression learning systems probes databases;face recognition;dictionaries;face;ar databases customized sparse representation model undersampled face recognition extended sparse representation based classification model face databases lfw databases georgia databases cmu pie databases;image representation face recognition image classification	In this paper, a customized sparse representation model is proposed to take advantage of the variational information for undersampled face recognition. The proposed model with the mixed norm is a generalization of the extended sparse representation-based classification model. This model guarantees the sparsity of representation coefficient and the robustness for the variational information from generic data set. The mixed norm well fits the distribution of variational information (such as illumination, expression, poses, and occlusion) and the interference information (somewhat face-specific in generic data set) simultaneously. We compare the proposed method with the related methods on several popular face databases, including AR, CMU-PIE, Georgia, and LFW databases. The experimental results show that the proposed method outperforms several popular face recognition methods.	ar (unix);calculus of variations;coefficient;cyclic redundancy check;database;dictionary;experiment;fits;facial recognition system;feature engineering;general mn-type image filter;hidden surface determination;interference (communication);numerical analysis;semiconductor research corporation;sparse approximation;sparse matrix;variational principle	Zhi-Ming Li;Zheng-Hai Huang;Kun Shang	2016	IEEE Transactions on Information Forensics and Security	10.1109/TIFS.2016.2567318	facial recognition system;face;computer vision;computer science;linear regression;machine learning;pattern recognition	Vision	25.20752987040544	-43.34388393498423	3720
8c0f46c529033da815d945c4ae509d593ffe0579	closed-loop control in an autonomous bio-hybrid robot system based on binocular neuronal input	motion vision;blowfly;closed loop control;autonomous;brain machine interface	In this paper, we describe the implementation of a closed-loop control architecture on a bio-hybrid robotic system. The control loop uses the spiking activity from two motion-sensitive H1-cells recorded in both halves of the blowfly's brain as visual feedback signals that are sent to an ARM processor, programmed to establish a brain machine interface. The resulting output controls the movements of the robot which, in turn, generates optic flow that modifies the activity of the H1-cells. Instead of being inhibited by front-to-back optic flow would the robot move forward in a straight line, the closed-loop system autonomously produces an oscillatory trajectory, alternatingly stimulating both H1-cells with back-to-front optic flow. The spike rate information of each cell is then used to control the speed of each robot wheel, on average driving the robot in the forward direction. Our extracellular recordings from the two cells show similar spike rate oscillation frequencies and amplitude, but opposite phases. From our experiments we derive parameters relevant for the future implementation of collision avoidance capabilities. Finally, we discuss a control algorithm that combines positive and negative feedback to drive the robot.		Jiaqi V. Huang;Holger G. Krapp	2015		10.1007/978-3-319-22979-9_17	control engineering;simulation;engineering;control theory	Robotics	66.10026240431999	-24.90459242608157	3721
9cbc0d7df57faec821a304811bbc974382619355	image segmentation via ant colony algorithm and loopy belief propagation algorithm	image segmentation;label pruning;iterative methods;adaptive label pruning scheme image segmentation ant colony algorithm loopy belief propagation algorithm lbp pairwise connected markov random fields mrf discrete label space aca local optimal label search convergence iteration;label pruning image segmentation loopy belief propagation ant colony algorithm local optimal;ant colony algorithm;local optimal;search problems;belief maintenance;markov processes;image segmentation inference algorithms algorithm design and analysis optimization belief propagation equations mathematical model;loopy belief propagation;search problems belief maintenance image segmentation iterative methods markov processes	Loopy belief propagation (LBP) algorithm over Pairwise-connected Markov random fields (MRF) has become widely used for low-level vision problems. However, Pairwise MRFs are often insufficient to capture more expressive priors, and LBP is still extremely slow for application on MRFs with large discrete label space. To solve these problems, a new segmentation algorithm combining ant colony and loopy belief propagation is proposed in this paper. Based on Pairwise MRF, a local interaction region MRF model is constructed. Then ant colony algorithm (ACA) is used to search local optimal label in every local region and to prune the label space for each pixel. Finally the loopy belief propagation algorithm is applied to transfer the local optimal result to adjacent region. This process is iterated until convergence. Compared with some previous algorithms, the proposed algorithm generates more accurate segmentation results and also more speed, because the proposed algorithm utilizes the local optimal result as the propagated messages between nodes in MRF, and uses adaptive label pruning scheme to reduce the number of labels for each pixel, Experimental results on a wide variety of images have verified the effectiveness of the proposed algorithm.	algorithm;ant colony optimization algorithms;belief propagation;casio loopy;image segmentation;software propagation	Shengjun Xu;Guanghui Liu;Xin Liu	2012		10.1109/IJCNN.2012.6252573	mathematical optimization;ant colony optimization algorithms;computer science;machine learning;pattern recognition;mathematics;iterative method;image segmentation;markov process;belief propagation	ML	44.912411387832584	-50.70658422655248	3723
c1cf9abc9082bcb5a14d1b585e09d4b7d6b445e1	a three degree-of-freedom optical orientation measurement method for spherical actuator applications	lasers;moment of inertia;measurement by laser beam;rigid body;rotors actuators displacement measurement lasers;rotation measurement;sensors;degree of freedom;orientation measurements;actuators;rotation measure;rotor;sensors actuators orientation measurements;laser based noncontact high precision spherical displacement measurement;laser based angular displacement measurement method degree of freedom optical orientation measurement method spherical actuator application laser detector laser based noncontact high precision spherical displacement measurement rotor moment of inertia;degree of freedom optical orientation measurement method;displacement measurement;rotors;position measurement;laser detector;laser based angular displacement measurement method;motion measurement;multiple degree of freedom;high sensitivity;spherical actuator application;measurement by laser beam sensors position measurement rotation measurement rotors motion measurement spinning;spinning	The advance of robotics, actuators and manufacturing technology motivates the research on measurement of multiple degree-of-freedom (DOF) rotational motions. A novel laser-based noncontact high-precision spherical displacement measurement methodology has been proposed for spherical actuator applications in this paper. The laser detector is utilized to measure the distance from the target to the detector on several light spots, and thus to calculate the rotation angle of the rigid body in three directions. As there is no physical contact between the laser detector and the moving body, additional mass/moment of inertia and friction on the rotor are avoided, and thus the working efficiency of moving body can be improved. The algorithm of orientation angles has been derived. Experimental apparatuses have been developed to evaluate the working performance of the measurement method. Comparison between experimental and analytical results shows that the proposed method can achieve high-precision measurement for multi-DOF rotational motions. Precision of the laser-based angular displacement measurement method can be improved further by increasing the slot density on the rigid body or using new models of laser detectors.	algorithm;angularjs;diode;displacement mapping;dots per inch;measurement in quantum mechanics;r.o.t.o.r.;robotics;sensor	Liang Yan;I-Ming Chen;Zhongwei Guo;Yan Lang;Yunhua Li	2011	IEEE Transactions on Automation Science and Engineering	10.1109/TASE.2010.2089981	control engineering;rigid body;laser;rotor;spinning;sensor;moment of inertia;degrees of freedom;actuator	Robotics	76.56974281529348	-25.339774487122106	3727
f2f4b09ccec3d0f10ca3dc6b5c40f74f261595f0	reconciling estimates of cell proliferation from stable isotope labeling experiments	sensitivity and specificity;deuterium;granulocytes;radioisotope dilution technique;deuterium oxide;monocytes;thymocytes;lymphocyte count;journal article;glucose;isotope labeling;blood;reproducibility of results;cell proliferation;t cells;algorithms;humans;lymphocytes;radiopharmaceuticals;blood plasma	Stable isotope labeling is the state of the art technique for in vivo quantification of lymphocyte kinetics in humans. It has been central to a number of seminal studies, particularly in the context of HIV-1 and leukemia. However, there is a significant discrepancy between lymphocyte proliferation rates estimated in different studies. Notably, deuterated (2)H2-glucose (D2-glucose) labeling studies consistently yield higher estimates of proliferation than deuterated water (D2O) labeling studies. This hampers our understanding of immune function and undermines our confidence in this important technique. Whether these differences are caused by fundamental biochemical differences between the two compounds and/or by methodological differences in the studies is unknown. D2-glucose and D2O labeling experiments have never been performed by the same group under the same experimental conditions; consequently a direct comparison of these two techniques has not been possible. We sought to address this problem. We performed both in vitro and murine in vivo labeling experiments using identical protocols with both D2-glucose and D2O. This showed that intrinsic differences between the two compounds do not cause differences in the proliferation rate estimates, but that estimates made using D2-glucose in vivo were susceptible to difficulties in normalization due to highly variable blood glucose enrichment. Analysis of three published human studies made using D2-glucose and D2O confirmed this problem, particularly in the case of short term D2-glucose labeling. Correcting for these inaccuracies in normalization decreased proliferation rate estimates made using D2-glucose and slightly increased estimates made using D2O; thus bringing the estimates from the two methods significantly closer and highlighting the importance of reliable normalization when using this technique.	blood glucose;discrepancy function;estimated;experiment;gene ontology term enrichment;hiv infections;in situ nick-end labeling;isotope labeling;isotopes;kinetics (discipline);kinetics internet protocol;protocols documentation;quantitation;scientific publication;sequence labeling;stable angina;video-in video-out;leukemia;lymphocyte proliferation	Raya Ahmed;Liset Westera;Julia Drylewicz;Marjet Elemans;Yan Zhang;Elizabeth Kelly;Rajko Reljic;Kiki Tesselaar;Rob J. De Boer;Derek C. Macallan;José A. M. Borghans;Becca Asquith	2015	PLoS computational biology	10.1371/journal.pcbi.1004355	biology;biochemistry;immunology;radiopharmacology;cell growth;genetics;deuterium	Web+IR	6.624139495642351	-64.46087393988866	3733
39d284ef4aa8f626321ad7c2b6a433cf3dbde531	modelling and trading the london, new york and frankfurt stock exchanges with a new gene expression programming trader tool	transaction;gene expression;trading;genetic algorithm	Intell Sys Acc Fin Mgmt. 2017;24:3–11. Summary The scope of this manuscript is to present a new short‐term financial forecasting and trading tool: the Gene Expression Programming (GEP) Trader Tool. It is based on the gene expression programming algorithm. This algorithm is based on a genetic programming approach, and provides supreme statistical and trading performance when used for modelling and trading financial time series. The GEP Trader Tool is offered through a user‐friendly standalone Java interface. This paper applies the GEP Trader Tool to the task of forecasting and trading the future contracts of FTSE100, DAX30 and S&P500 daily closing prices from 2000 to 2015. It is the first time that gene expression programming has been used in such massive datasets. The model's performance is benchmarked against linear and nonlinear models such as random walk model, a moving‐ average convergence divergence model, an autoregressive moving average model, a genetic programming algorithm, a multilayer perceptron neural network, a recurrent neural network a higher order neural network. To gauge the accuracy of all models, both statistical and trading performances are measured. Experimental results indicate that the proposed approach outperforms all the others in the in‐sample and out‐of‐sample periods by producing superior empirical results. Furthermore, the trading performances are improved further when trading strategies are imposed on each of the models.	acc (programming language);algorithm;artificial neural network;autoregressive model;benchmark (computing);closing (morphology);gene expression programming;genetic programming;interface (java);java;moving-average model;multilayer perceptron;nonlinear system;performance;recurrent neural network;time series;trader media east	Andreas S. Karathanasopoulos	2017	Int. Syst. in Accounting, Finance and Management	10.1002/isaf.1401	simulation;gene expression;genetic algorithm;economics;computer science;artificial intelligence;pairs trade;operations management;trading strategy;machine learning	ML	7.433342914586667	-19.92581059929817	3741
4be063d27087e4d0ef7fbb0e985405bd38c4e913	a study on pre-adoption of a self-management application by parkinson™s disease patients				Mevludin Memedi;Joakim Lindqvist;Tobias Tunedal;Axel Duvåker	2018				ML	10.36513017591416	-80.86999458723365	3742
754df32fcdd8b887896eec8be253d0e2a48c22aa	selecting informative genes from microarray data for cancer classification with genetic programming classifier using k-means clustering and snr ranking	microarray data;biology computing;genetic program;cancer;cancer genetic programming clustering algorithms testing organisms data analysis dna partitioning algorithms information technology data engineering;genetics biology computing cancer;genetics;genetic programming classifier;snr ranking;selecting informative genes;cancer classification;bioinformatics selecting informative genes microarray data cancer classification genetic programming classifier k means clustering snr ranking;k means clustering;bioinformatics	This paper presents a method for selecting informative features using K-Means clustering and SNR ranking. The performance of the proposed method was tested on cancer classification problems. Genetic Programming is employed as a classifier. The experimental results indicate that the proposed method yields higher accuracy than using the SNR ranking alone and higher than using all of the genes in classification. The clustering step assures that the selected genes have low redundancy, hence the classifier can exploit these features to obtain better performance.	cluster analysis;genetic programming;information;k-means clustering;microarray;signal-to-noise ratio;statistical classification	Supoj Hengpraprohm;Prabhas Chongstitvatana	2007	2007 Frontiers in the Convergence of Bioscience and Information Technologies	10.1109/FBIT.2007.84	computer science;bioinformatics;pattern recognition;data mining	ML	8.002929118335308	-47.55012557664092	3747
44ca8b4f5b1627cb36c559fc547d5189575964bf	an eeg-based study on coherence and brain networks in mild depression cognitive process	eeg coherence;small world network;gta	Depression is a common mental disorder, and in recent years, there has been an increasing trend of mild to moderate depression among college students. Additionally, effective detection of mild depression at an earlier stage remains an urgent problem that must be solved. In this study, electroencephalography (EEG) activities were recorded from 37 participants during processing of facial expression stimuli. With both high-gamma and low-gamma bands, the coherence in the right hemisphere of normal controls was greater than that of mildly depressive subjects, especially for electrodes P8, TP8, C4, FC4, and F8. In the low gamma band, the clustering coefficients of healthy controls in the prefrontal lobe (AF4, AFz, AF3, FC5, F4, and F6) and the parietal lobe (PO3, PO4, and P2) were significantly higher than those of mildly depressive subjects. The ratio of the characteristic path length between the functional network of the mildly depressed group and the small-world network was greater than 1. For the normal group, the ratio was near 1. This research contributes to the study of the cognitive process of mild depression. In our study, the results show closer cooperation in the brain areas of right hemisphere in normal controls during the cognitive process compared with the mildly depressed group, while the activity of the prefrontal and parietal regions in mild depression was significantly lower than that of normal controls. At the same time, in terms of the characteristic path length, the functional network of the mildly depressed group deviates from the small-world network.	acoustic lobing;cluster analysis;coefficient;cognition;electroencephalography;gamma correction	Xiaowei Li;Zhuang Jing;Bin Hu;Shuting Sun	2016	2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)	10.1109/BIBM.2016.7822702	computer science;small-world network	HCI	18.12738379268152	-79.78679738139154	3748
66155e02e48c3540d8000088cf0bd1076c966eb2	linearization by prolongations: new bounds on the number of integrators	multi input systems;differential geometric methods;teoria sistema;differential geometry;nonlinear control;systems theory;commande non lineaire;integrator;theorie systeme;linearisation par retour etat;integrador;mathematical systems theory;geometrie differentielle;non linear control;feedback linearization;geometria diferencial;linearizacion por retroaccion;control no lineal;integrateur	This paper gives a bound on the number of integrators needed to linearize a control system with an arbitrary number of inputs. Although some work has been done in this direction, our bound improves the existing results for systems with four or more inputs. The bound for two input systems is the same as the one that appeared in the literature. The bound for three input systems has been improved further in a previous paper.	control system;input/output	Jaume Franch;Enric Fossas	2005	Eur. J. Control	10.3166/ejc.11.171-179	control engineering;integrator;nonlinear control;engineering;calculus;control theory;mathematics;feedback linearization;systems theory	Theory	71.76572079497377	-3.769486302648081	3752
99bf3f7cab685ca2db90cb04d7879038888edc52	robust self-calibrating ncpmg acquisition: application to body diffusion-weighted imaging		This paper demonstrates a robust diffusion-weighted single-shot fast spin echo (SS-FSE) sequence in the presence of significant off-resonance, which includes a variable-density acquisition and a self-calibrated reconstruction as improvements. A non-Carr–Purcell–Meiboom–Gill (nCPMG) SS-FSE acquisition stabilizes both the main and parasitic echo families for each echo. This preserves both the in-phase and quadrature components of the magnetization throughout the echo train. However, nCPMG SS-FSE also promotes aliasing of the quadrature component, which complicates reconstruction. A new acquisition and reconstruction approach is presented here, where the field-of-view is effectively doubled, but a partial k-space and variable density sampling is used to improve scan efficiency. The technique is presented in phantom scans to validate SNR and robustness against rapidly varying object phase. In vivo healthy volunteer examples and the clinical cases are demonstrated in abdominal imaging. This new approach provides comparable SNR to previous nCPMG acquisition techniques as well as providing more uniform apparent diffusion coefficient maps in phantom scans. In vivo scans suggest that this method is more robust against motion than previous approaches. The proposed reconstruction is an improvement to the nCPMG sequence as it is auto-calibrating and is justified to accurately treat the signal model for the nCPMG SS-FSE sequence.	aliasing;ct scan;carr–benkler wager;coefficient;computational human phantom;fast software encryption;imaging phantom;in-phase and quadrature components;map;parasites;phantoms, imaging;resonance;sampling (signal processing);scanning;signal-to-noise ratio;spin echo;video-in video-out;fetal scalp electrode;non-t, non-b, calla negative childhood acute lymphoblastic leukemia;synovial sarcoma	Eric K. Gibbons;Patrick Le Roux;Shreyas S. Vasanawala;John M. Pauly;Adam B. Kerr	2018	IEEE Transactions on Medical Imaging	10.1109/TMI.2017.2741421	robustness (computer science);mathematics;computer vision;iterative reconstruction;artificial intelligence;imaging phantom;quadrature (mathematics);diffusion mri;aliasing;signal-to-noise ratio;spin echo	Vision	45.885119161381	-82.90379570825883	3754
b539dd1a45f70ac6574a11b26cfe736dd73aa0a8	automated visual analysis in large scale sensor networks	cameras target tracking pixel streaming media image resolution boats visualization;high resolution;image resolution;video signal processing;user interface;florida;real time;real time sensors;environmental conditions;video analysis;geo registration;automated visual analysis;video processing;data fusion;heterogeneous sensors;image sensors;sensor network;geo registration automated visual analysis large scale sensor networks modern automated video analysis systems heterogeneous sensors real time sensors video processing errors high resolution sensors data fusion wide area video analysis system intuitive user interfaces video data visualization florida;data visualisation;sensor data fusion;large scale;visualization;intuitive user interfaces;high resolution sensors;monitoring;streaming media;visual sensor networks;geophysical signal processing;image registration;pixel;monitoring visual sensor networks geo registration video analysis;video data visualization;visual analysis;wide area video analysis system;video signal processing data visualisation geophysical signal processing image registration image sensors real time systems sensor fusion user interfaces;modern automated video analysis systems;large scale sensor networks;sensor fusion;target tracking;user interfaces;cameras;video processing errors;real time systems;boats	Modern automated video analysis systems consist of large networks of heterogeneous sensors. These systems must extract, integrate and present relevant information from the sensors in real-time. This paper addresses some of the major challenges such systems face: efficient video processing for high-resolution sensors; data fusion across multiple modalities; robustness to changing environmental conditions and video processing errors; and intuitive user interfaces for visualization and analysis. The paper discusses enabling technologies to overcome these challenges and presents a case study of a wide area video analysis system deployed at a port in the state of Florida, USA. The components of the system are also detailed and justified using quantitative and qualitative results.	image resolution;real-time clock;real-time computing;sensor;user interface;video content analysis;video processing	Zeeshan Rasheed;Xiaochun Cao;Khurram Shafique;Haiying Liu;Li Yu;Mun Wai Lee;Krishnan Ramnath;Tae Eun Choe;Omar Javed;Niels Haering	2008	2008 Second ACM/IEEE International Conference on Distributed Smart Cameras	10.1109/ICDSC.2008.4635678	computer vision;visual analytics;image resolution;computer science;sensor fusion;multimedia;user interface;data visualization;computer graphics (images)	Visualization	54.27925012165735	-44.009785820351844	3755
fe8afc82b0099ca4a16bb2f32096bf190681f062	extrinsic calibration of a laser galvanometric setup and a range camera	non-perspective-n-point problem;extrinsic calibration;galvanometer;range camera;scanning laser doppler vibrometer	Currently, galvanometric scanning systems (like the one used in a scanning laser Doppler vibrometer) rely on a planar calibration procedure between a two-dimensional (2D) camera and the laser galvanometric scanning system to automatically aim a laser beam at a particular point on an object. In the case of nonplanar or moving objects, this calibration is not sufficiently accurate anymore. In this work, a three-dimensional (3D) calibration procedure that uses a 3D range sensor is proposed. The 3D calibration is valid for all types of objects and retains its accuracy when objects are moved between subsequent measurement campaigns. The proposed 3D calibration uses a Non-Perspective-n-Point (NPnP) problem solution. The 3D range sensor is used to calculate the position of the object under test relative to the laser galvanometric system. With this extrinsic calibration, the laser galvanometric scanning system can automatically aim a laser beam to this object. In experiments, the mean accuracy of aiming the laser beam on an object is below 10 mm for 95% of the measurements. This achieved accuracy is mainly determined by the accuracy and resolution of the 3D range sensor. The new calibration method is significantly better than the original 2D calibration method, which in our setup achieves errors below 68 mm for 95% of the measurements.	angioplasty, balloon, laser-assisted;calibration;checking (action);condition number;conflict (psychology);design of experiments;exogenous intraocular fluid;experiment;hessian;kinect;national fund for scientific research;overfitting;perspective-n-point;physical object;preparation;radiation;range imaging;relocation of home or business;root mean square;sap business one;sampling - surgical action;well-posed problem;workspace	Seppe Sels;Boris Bogaerts;Steve Vanlanduit;Rudi Penne	2018		10.3390/s18051478	electronic engineering;galvanometer;beam (structure);calibration;laser doppler vibrometer;engineering;scanning laser doppler;laser;optics;planar	Robotics	58.20682290391011	-40.203751430447014	3761
151a2f9a95e96694c764f77fec767b2f8e2bdded	multi-robot manipulation with no communication using only local measurements	robot sensing systems;force;acceleration;robot kinematics force acceleration robot sensing systems force measurement velocity measurement;multirobot manipulation grand piano lyapunov stability arguments follower robots leader robot leader following process acceleration measurements velocity measurements cooperative manipulation task manipulation forces local measurements;force measurement;velocity measurement;velocity control acceleration control lyapunov methods mobile robots multi robot systems stability trajectory control;robot kinematics	This paper presents a novel approach to coordinate the manipulation forces of a group of robots without explicit communication during a cooperative manipulation task. Robots use the measurements of the motion of the object as the only information to reach a consensus on their forces. It is proven that the consensus can be reached even if all the robots have different velocity and acceleration measurements since they take measurements at different attachment points around the object while the object is rotating and translating. The convergence of the leader-following process where a leader robot actively steers the forces of all follower robots to navigate the object along a desired trajectory is also proven with Lyapunov stability arguments. We verify our method in both numerical simulations and a physics simulator, where we transport a grand piano with 1001 robots.	attachments;lyapunov fractal;no-communication theorem;numerical analysis;robot;simulation;velocity (software development)	Zijian Wang;Mac Schwager	2015	2015 54th IEEE Conference on Decision and Control (CDC)	10.1109/CDC.2015.7402230	acceleration;control engineering;simulation;engineering;control theory;robot control;force;robot kinematics	Robotics	64.55434509211307	-18.460754350842834	3765
3c852d9b793fbfaa67b30f273211084feaedaea4	a genetic algorithm approach to color image enhancement	performance measure;simulation ordinateur;fitness;algorithm performance;image processing;color space;uniform distribution constraint;etude experimentale;enhancement mode;procesamiento imagen;algoritmo genetico;traitement image;objective function;optimization problem;red green blue;reconstruction image;image enhancement;mode enrichissement;reconstruccion imagen;nonlinear transformation;generalized transform;resultado algoritmo;human visual system;image reconstruction;image quality;color image enhancement;performance algorithme;algorithme genetique;genetic algorithm;modo enriquecimiento;simulacion computadora;adecuacion;imagen color;computer simulation;estudio experimental;image couleur;human visual perception;fitness function;color image;power measurement;uniform distribution;genetic algorithm ga	Image enhancement techniques are used to improve image quality or extract the fine details in the degraded images. Most existing color image enhancement techniques usually have three weaknesses: (1) color image enhancement applied in the RGB (red, green, blue) color space is inappropriate for the human visual system; (2) the uniform distribution constraint employed is not suitable for human visual perception; (3) they are not robust, i.e., one technique is usually suitable for one type of degradations only.#R##N##R##N#In this study, a genetic algorithm (GA) approach to color image enhancement is proposed, in which color image enhancement is formulated as an optimization problem. In the proposed approach, a set of generalized transforms for color image enhancement is formed by linearly weighted combining four types of nonlinear transforms. The fitness (objective) function for GAs is formed by four performance measures, namely, the AC power measure, the compactness measure, the Brenner’s measure, and the information–noise change measure. Then GAs are used to determine the “optimal” set of generalized transforms with the largest fitness function value.#R##N##R##N#Based on the experimental results obtained in this study, the enhanced color images by the proposed approach are better than that by any of the three existing approaches for comparison. This shows the feasibility of the proposed approach.	color image;genetic algorithm;image editing	Ming-Suen Shyu;Jin-Jang Leou	1998	Pattern Recognition	10.1016/S0031-3203(97)00073-3	iterative reconstruction;computer simulation;image quality;demosaicing;color histogram;optimization problem;rgb color model;computer vision;color quantization;genetic algorithm;color image;binary image;image processing;computer science;artificial intelligence;mathematics;color balance;uniform distribution;color space;human visual system model;fitness function	Vision	59.2127730496618	-63.316462889133376	3768
2350de9ef8b670e873b7431ddb516af048eafd00	discriminative subnetworks with regularized spectral learning for global-state network data		Data mining practitioners are facing challenges from data with network structures. In this paper, we address a specific class of global-state networks that comprise of a set of network instances sharing a similar structure yet having different values at local nodes. Each instance is associated with a global state that indicates the occurrence of an event. The objective is to uncover a small set of discriminative subnetworks that can optimally classify global network values. Unlike most existing studies that explore an exponential subnetwork space, we address this difficult problem by adopting a space transformation approach. Specifically, we present an algorithm that optimizes a constrained dual-objective function to learn a low-dimensional subspace that is capable of discriminating networks labeled by different global states, while reconciling with a common network topology sharing across instances. Our algorithm is based on the spectral graph learning and we show that the globally optimum solution can be obtained via matrix eigen-decomposition.	advanced configuration and power interface;algorithm;data mining;discriminative model;eigen (c++ library);global network;global serializability;ibm notes;list of algorithms;local variable;loss function;network topology;optimization problem;relevance;sampling (signal processing);subnetwork;synthetic data;time complexity	Xuan-Hong Dang;Ambuj K. Singh;Petko Bogdanov;Hongyuan You;Bayyuan Hsu	2014		10.1007/978-3-662-44848-9_19	machine learning;pattern recognition;data mining;mathematics	ML	22.970989616374474	-42.78189312537872	3770
6d0d5d905b4e6fef60ce9565f63a786f50a7d131	local relevance weighted maximum margin criterion for text classification	feature extraction;kernel;local relevance weighted maximum margin criterion;semi-supervised learning;tensor;text classification	Text classification is a very important task in information retrieval and data mining. In vector space model (VSM), document is represented as a high dimensional vector, and a feature extraction phase is usually needed to reduce the dimensionality of the document. In this paper, we propose a feature extraction method, named Local Relevance Weighted Maximum Margin Criterion (LRWMMC). It aims to learn a subspace in which the documents in the same class are as near as possible while the documents in the different classes are as far as possible in the local region of each document. Furthermore, the relevance is taken into account as a weight to determine the extent to which the documents will be projected. LRWMMC is able to find the low dimensional manifold embedded in the high dimensional ambient space. In addition, We generalize LRWMMC to Reproducing Kernel Hilbert Space (RKHS), which can resolve the nonlinearity of the input space. We also generalize LRWMMC to tensor space which is suitable for a new document representation, named tensor space model (TSM). On the other hand, in order to utilize the large amount of unlabeled documents, we also present a Semi-Supervised LRWMMC, which aims to find a projection inferred from the labeled samples, as well as the unlabeled samples. Finally, we present a fast algorithm based on QR-decomposition to make the methods proposed in this paper apply for large scale data set. Encouraging experimental results on benchmark text classification data sets indicate that the proposed methods outperform many existing feature extraction methods for text classification.	algorithm;benchmark (computing);data mining;document classification;embedded system;feature extraction;hilbert space;ibm spectrum protect (tivoli storage manager);information retrieval;nonlinear system;qr decomposition;relevance;semi-supervised learning;statistical classification;viable system model	Quanquan Gu;Jie Zhou	2009		10.1137/1.9781611972795.97	semi-supervised learning;machine learning;artificial intelligence;tensor;hilbert space;pattern recognition;curse of dimensionality;feature extraction;ambient space;vector space model;mathematics;reproducing kernel hilbert space	ML	24.259971005206634	-41.95270056160489	3778
d50fc28de18f1461d62bb4378e558cce9cc132b3	a novel video coding framework by perceptual representation and macroblock-based matching pursuit algorithm	estensibilidad;texture;largeur bande;optimisation;image coding;multimedia;optimizacion;algorithme glouton;low frequency;fotografia rapida;visual perceptual;codage image;video coding;haute frequence;high speed photography;senal video;signal video;codage video;anchura banda;filter based decompose;textura;basse frequence;video representation;greedy algorithm;video signal;bandwidth;matching pursuit;algoritmo gloton;optimization;baja frecuencia;extensibilite;scalability;alta frecuencia;human perception;high frequency;hybrid video coding;photographie rapide	This paper presents a novel hybrid video coding framework by perceptual representation and macroblock-based matching pursuit algorithm (PRMBMP), which uses a set of filters to extract the perceptual parts of each video frame. The original video frame is separated into low-frequency image and high-frequency image. The low-frequency image has low sensitivity to human perception and few complex texture details, which can be handled efficiently by traditional H.264 video coding. The high-frequency image is the perceptual representation, which includes more texture details and edges. The proposed macroblock-based matching pursuit algorithm is used to compress the high-frequency image, which speeds up the conventional matching pursuit algorithm efficiently by estimating the local optimization. The experiments show that the proposed framework can achieve 2 to 3 dB improvements compared with the conventional H.264 video coding framework. The proposed framework also has the definition scalability, which can be widely used in bandwidth-variation video applications.	algorithm;macroblock;matching pursuit	Jianning Zhang;Lifeng Sun;Yuzhuo Zhong	2007		10.1007/978-3-540-69423-6_32	computer vision;greedy algorithm;scalability;visual perception;computer science;video tracking;high frequency;coding tree unit;block-matching algorithm;multimedia;low frequency;texture;high-speed photography;perception;h.261;bandwidth;matching pursuit;multiview video coding;computer graphics (images)	Vision	46.32240646824126	-14.954654411265196	3789
c4ccea4b6f68b42a26b80cec41963dcd494b6c66	layered image resizing in compression domain	estensibilidad;traitement signal;dimensionnement;two layer;image coding;capa acumulacion;image processing;data compression;block dct;longueur mot;couche accumulation;video signal processing;low frequency;transformation cosinus discrete;canal transmision;erreur quadratique moyenne;procesamiento imagen;dimensioning;image resizing;traitement image;discrete cosine transform;frequency response;codage image;video coding;haute frequence;reponse frequence;respuesta frecuencia;enhancement layer;compression image;scalable codec;image compression;codage video;word length;mmse;canal transmission;transmission channel;discrete cosine transforms;mean square error;signal processing;longitud palabra;basse frequence;traitement signal video;codec;baja frecuencia;extensibilite;scalability;compresion dato;error medio cuadratico;dimensionamiento;alta frecuencia;procesamiento senal;high frequency;compression donnee;compresion imagen	As an efficient unitary transform, the discrete cosine transform (DCT) has been widely adopted in compression standards. Most compressed images and videos are stored in the DCT format, and from time to time, they need to be resized for various transmission channels and consumer terminals. In this paper, we investigate existing resizing schemes, focusing on the difference of short and long basis vector truncation. A layered resizing scheme is then proposed based on the above analysis, where compressed images are divided into lowand high-frequency layers. The DCT vectors of these two layers are truncated with different word lengths, and then form the elementary layer (EL) and the enhancement layer (EH) of the downsampled image. The EL and EH can be transmitted together or separately according to the bandwidth available. An upsampling scheme is also provided in this paper to recover visual details. Experimental results show improvements of the proposed approach over existing resizing schemes, which can be explained since its frequency response is closer to the ideal downsample filter. The new approach can cater for user terminals’ display limitation and channel bandwidth constraints, with additional scalability. r 2007 Elsevier B.V. All rights reserved.		Ci Wang;Ping Xue;Weisi Lin	2008	Sig. Proc.: Image Comm.	10.1016/j.image.2007.10.002	data compression;frequency response;codec;scalability;telecommunications;image processing;image compression;computer science;signal processing;discrete cosine transform;high frequency;mathematics;mean squared error;low frequency;dimensioning;algorithm;statistics;computer graphics (images)	AI	45.96642752845429	-14.965282918660169	3792
eb426a1c0dbd16ce198bc3f3d76654105daa7c10	perceptual object tracking	perception laws;color histogram perceptual object tracking improved kernel based target tracking target appearance visual perception whole tracking algorithm reduced dimension perceptual feature space video sequences complete occlusion luminance histogram;video signal processing;kernel based tracking;computer graphics;object tracking;target tracking histograms kernel computational efficiency feature extraction;kernel based tracking nonrigid object tracking perception laws target localization and representation;target tracking;nonrigid object tracking;target localization and representation;image sequences;video signal processing computer graphics image sequences object tracking target tracking	This paper presents an improved kernel-based target tracking that uses new and effective features able to describe the target appearance. The key idea consists of adopting features that are related to the visual perception of the target in place of its color histogram. The change of the feature space is twofold advantageous. It allows us to faithfully follow the target and to considerably reduce the computational cost of the whole tracking algorithm thanks to the reduced dimension of the perceptual feature space. Preliminary tests on some video sequences are encouraging. The proposed tracker is able to follow the target even in case of complete occlusion for some consecutive frames. Moreover, it is more stable than the algorithm that uses just the luminance histogram as target feature. Finally, it is robust to the presence of other moving targets.	algorithm;color histogram;computation;computational complexity theory;feature vector;luminance hdr	Vittoria Bruni;Elisa Rossi;Domenico Vitulano	2012	2012 IEEE Workshop on Biometric Measurements and Systems for Security and Medical Applications (BIOMS) Proceedings	10.1109/BIOMS.2012.6345774	computer vision;speech recognition;video tracking;pattern recognition;mathematics	Vision	37.62110309398936	-51.05825736445575	3793
c4f3b549693a7e4fba9cf937d13b9e0bd0e39be8	a 3d active surface model for the accurate segmentation of drosophila schneider cell nuclei and nucleoli	gradient vector flow;force field;surface model	We present an active surface model designed for the segmentation of Drosophila Schneider cell nuclei and nucleoli from wide-field microscopic data. The imaging technique as well as the biological application impose some major challenges to the segmentation. On the one hand, we have to deal with strong blurring of the 3D data, especially in z-direction. On the other hand, concerning the biological application, we have to deal with non-closed object boundaries and touching objects. To cope with these problems, we have designed a fully 3D active surface model. Our model prefers roundish object shapes and especially imposes roughly spherical surfaces where there is little gradient information. We have adapted an external force field for this model, which is based on gradient vector flow (GVF) and has a much larger capture range than standard GVF force fields.	force field (chemistry);gradient descent;subdivision surface	Margret Keuper;Jan Padeken;Patrick Heun;Hans Burkhardt;Olaf Ronneberger	2009		10.1007/978-3-642-10331-5_80	computer vision;force field	Vision	46.81711861865323	-73.63046551419333	3794
b4d381cfffcbccb228745dc53cff4c2a60957875	comparison of curve fitting method for hyperspectral data classification with nonlinear based feature extraction methods		Hyperspectral imagery is one of the most important tools in remote sensing. Increasing the number of bands, lack of training samples, correlation of spectral samples and redundancy of data make conventional image classification methods without reducing the dimensions of the feature vector, not applicable. Dimension reducing as a preprocessing step could be done in two approach: feature selection, and feature extraction. In this paper, recently rational function curve fitting-feature extraction (RFCF-FE) method is analyzed and compared with some nonlinear non-kernel based feature extraction methods such as locally linear embedding (LLE), piecewise constant function approximations (PCFA) and the proposed feature extraction Based on breakpoints (BPB). The maximum likelihood (ML) classification results demonstrate that RFCF-FE provides better classification accuracies compared to competing methods. Also, in this paper, we propose a method for determining the separability of data classes based on specific breakpoints of the spectral response curve (SRC) of pixels.	approximation;arc diagram;bios parameter block;breakpoint;coefficient;computer vision;constant function;curve fitting;feature extraction;feature selection;feature vector;kernel (operating system);least squares;linear separability;nonlinear dimensionality reduction;nonlinear system;pixel;polynomial;preprocessor;sample rate conversion;statistical classification	M. Sadighi Tehrani;S. Abolfazl Hosseini;Anthony LoGalbo	2018	2018 11th International Symposium on Communication Systems, Networks & Digital Signal Processing (CSNDSP)	10.1109/CSNDSP.2018.8471794	piecewise;data classification;feature selection;dimensionality reduction;feature extraction;feature vector;contextual image classification;artificial intelligence;pattern recognition;mathematics;curve fitting	EDA	29.769836014993935	-42.33920511975646	3795
0b700bd1f413cd1d5fc9f27fd79cca58ee1268fc	comparative study on engine torque modelling using different neural networks	selection model;modelizacion;model based reasoning;raisonnement base sur modele;base donnee;soupape;sistema temporizado;engine control;engine test;valve;multilayer perceptrons;real time;data collection;fonction base radiale;spark ignition;modele lineaire;timed system;database;ensayo motor;base dato;motor explosion interna;multilayer perceptron;motor torque;modelo lineal;model order selection;perceptron multicouche;modelisation;red multinivel;radial basis function;moteur combustion interne;internal combustion engine;couple moteur;temps reel;network model;linear model;valvula;comparative study;systeme temporise;tiempo real;modele donnee;model test;multilayer network;essai moteur;variable valve timing;reseau multicouche;reseau neuronal;funcion radial base;modeling;par motor;red neuronal;data models;neural network	In this paper, modelling the torque of a variable valve timing (VVT) spark ignition (SI) engine is studied using multilayer Perceptron (MLP), radial basis function (RBF) and local linear model tree (LOLIMOT) networks. Real data collection, model order selection, model test are discussed. The obtained model is finally tested in real time on-line mode by an engine test bench. The model output is compared with the process output and compared among different network models. Selected model performs well and can be used in the model-based engine control.	neural networks	Dingli Yu;Michael Beham	2005		10.1007/11427469_137	data modeling;valve;radial basis function;internal combustion engine;simulation;systems modeling;computer science;artificial intelligence;model-based reasoning;network model;machine learning;comparative research;linear model;multilayer perceptron;artificial neural network;data collection	NLP	9.415161901660747	-24.700849795185075	3800
d401ec889361a907a5c1734415c82b13bfa1deeb	real-time volumetric lighting in participating media		Simulating light scattering in participating media, such as dust, fog or smoke, can greatly improve the overall realism of the images. This volumetric effect has been well studied in the context of off-line rendering but is still challenging for interactive applications. In this paper we present a GPU-based algorithm to compute volumetric light-shafts generated by single scattering in participating media. The proposed method uses shadow maps to account for shadowing and interleaved sampling to maintain high frame rates without sacrificing image quality.	algorithm;graphics processing unit;image quality;map;online and offline;real-time transcription;sampling (signal processing);shadow mapping;volume ray casting;volumetric lighting	Balázs Tóth;Tamás Umenhoffer	2009		10.2312/egs.20091048	geography;multimedia;optics;computer graphics (images)	Graphics	64.59018280867905	-51.50684383544331	3801
2e7755ad4dc5b5f28b26c98d153a09ce1ad28a53	non-stationary t-distribution prior for image source separation from blurred observations	image differential;image source separation;image source;image separation problem;non-stationary t-distribution;astrophysical image mixture;non-stationary spatial image model;bayesian formulation;prior image model;order image differential;stationary model;first order;sampling methods	image differential;image source separation;image source;image separation problem;non-stationary t-distribution;astrophysical image mixture;non-stationary spatial image model;bayesian formulation;prior image model;order image differential;stationary model;first order;sampling methods	source separation;stationary process	Koray Kayabol;Ercan Engin Kuruoglu	2010		10.1007/978-3-642-15995-4_63	computer vision;econometrics;feature detection;mathematics;anisotropic diffusion;statistics	Vision	60.703795264217696	-71.14016261900585	3803
49359479feb7ad2f1e2df0304f068011f0200869	fuzzy discriminant analysis with kernel methods	nonlinear mapping;high dimensionality;kernel methods;euclidean distance;feature space;discriminant analysis;theoretical analysis;kernel method;fuzzy discriminant analysis;kernel fuzzy discriminant analysis	A novel fuzzy nonlinear classifier, called kernel fuzzy discriminant analysis (KFDA), is proposed to deal with linear non-separable problem. With kernel methods KFDA can perform efficient classification in kernel feature space. Through some nonlinear mapping the input data can be mapped implicitly into a high-dimensional kernel feature space where nonlinear pattern now appears linear. Different from fuzzy discriminant analysis (FDA) which is based on Euclidean distance, KFDA uses kernel-induced distance. Theoretical analysis and experimental results show that the proposed classifier compares favorably with FDA.	kernel method;linear discriminant analysis	Xiao-Hong Wu;Jian-Jiang Zhou	2006	Pattern Recognition	10.1016/j.patcog.2006.05.004	kernel;principal component regression;kernel method;mathematical optimization;kernel fisher discriminant analysis;kernel embedding of distributions;radial basis function kernel;kernel principal component analysis;computer science;machine learning;pattern recognition;optimal discriminant analysis;mathematics;linear discriminant analysis;variable kernel density estimation;polynomial kernel;multiple discriminant analysis	Vision	23.335478207531988	-40.19408812446081	3804
987dfc388a0e8a93cd7c2a90965835fcc5d8c9cf	estimation of parameters sensitivity for scientific workflows	neural networks workflows parameter significance;scientific workflow significant parameter estimation;neural networks;neural nets;patients blood glucose calculation parameters sensitivity estimation scientific workflow significant parameter estimation noninvasive glucose measurement workflow neural network model;workflow management software medical computing neural nets parameter estimation;scientific workflow;noninvasive glucose measurement workflow;data mining;medical computing;parameter estimation neural networks sugar parallel processing scientific computing computer science blood distributed computing grid computing;artificial neural networks;computational modeling;parameter significance;workflows;blood;parameters sensitivity estimation;empirical validation;time use;mathematical model;datavetenskap datalogi;workflow management software;computer science;parameter estimation;sugar;neural network model;quality of service;neural network;patients blood glucose calculation	Usually workflow activities in the scientific domain depend on a collection of parameters. These parameters determine the output of the activity, and consequently the output of the whole workflow. In the scientific domain, workflows have exploratory nature and are used to understand a scientific phenomenon or answer scientific questions. In the process of a scientific experiment a workflow is executed multiple times using various values of the parameters of activities. It is relevant to identify (1) which parameter strongly affects the overall result of the workflow and (2) for which combination of parameter values we obtain the expected result. Foreseeing these issues, in this paper we present our methodology to estimate the significance of all scientific workflow parameters as well as to estimate the most significant parameter to the workflow. The estimation of parameter significance will enable the scientist to fine tune, and optimize his results efficiently. Furthermore, we empirically validate our methodology on Non-Invasive Glucose Measurement workflow and discuss our results. The NIGM workflow uses the neural network model to calculate the glucose level in patient blood. The neural network model has a set of parameters, which affect the result of the workflow significantly. But, unfortunately the impact significance of these parameters is commonly unknown to the user. We present our approach for estimating and quantifying impact significance of neural network parameters.	artificial neural network;experiment;network model;quality of results	Fakhri Alam Khan;Yuzhang Han;Sabri Pllana;Peter Brezany	2009	2009 International Conference on Parallel Processing Workshops	10.1109/ICPPW.2009.9	workflow;quality of service;computer science;bioinformatics;data science;mathematical model;data mining;estimation theory;computational model;artificial neural network	HPC	8.589419575595374	-72.15528564362718	3806
8ab523fec8619f08137ebfbe73591e30b855a413	infrared target tracking using naïve-bayes-nearest-neighbor		Robust yet efficient techniques for detecting and tracking targets in infrared (IR) images are a significant component of automatic target recognition (ATR) systems. In our previous works, we have proposed infrared target detection and tracking systems based on sparse representation method. The proposed infrared target detection and tracking algorithms are based on sparse representation and Bayesian probabilistic techniques, respectively. In this paper, we adopt Naı̈ve Bayes Nearest Neighbor (NBNN) that is an extremely simple, efficient algorithm that requires no training phase. State-of-the-art image classification techniques need a comprehensive learning and training step (e.g., using Boosting, SVM, etc.) In contrast, non-parametric Nearest Neighbor based image classifiers need no training time and they also have other more advantageous properties. Results of tracking in infrared sequences demonstrated that our algorithm is robust to illumination changes, and the tracking algorithm is found to be suitable for real-time tracking of a moving target in infrared sequences and its performance was quite good. key words: target detection, Naı̈ve Bayes Nearest Neighbor, infrared sequences, night vision	automatic target recognition;boosting (machine learning);computer vision;k-nearest neighbors algorithm;naive bayes classifier;real-time clock;sensor;sparse approximation;sparse matrix;tracking system	Shujuan Gao;Insuk Kim;Seong Tae Jhang	2015	IEICE Transactions		computer vision	Vision	40.77527771959236	-49.55186424198755	3808
64be5e1fedb1b1f352881dfc3e5deedcbe53f4cb	transient performance improvement in tracking control for a class of nonlinear systems with input saturation		This paper addresses the composite nonlinear feedback (CNF) control for a class of singleinput single-output nonlinear systems with input saturation to track a time varying reference target with good transient performance. The CNF control law consists of a tracking control law and a performance compensator. The tracking control law is designed to drive the output of the system to track the time varying reference target rapidly, while the performance compensator is used to reduce the overshoot caused by the tracking control law. The stability of the closed-loop system is established. The design procedure and the improvement of transient performance of the closed-loop system are illustrated with a numerical example and the controlled Van del Pol oscillator.	nonlinear system	Tao Lu;Weiyao Lan;Zhibin Li	2018	J. Systems Science & Complexity	10.1007/s11424-018-7362-y	control theory;performance improvement;saturation (chemistry);mathematics;oscillation;overshoot (signal);nonlinear system	Robotics	67.4550374565125	-6.461374085463798	3809
2e96d089361fc5a2c131e899296e916336d77bb6	using novel particle swarm optimization scheme to solve resource-constrained scheduling problem in psplib	optimal solution;critical path method;particle swarm optimizer;bidirectional scheduling;scheduling;critical path;particle swarm optimization;scheduling problem;local search;delay local search	This investigation proposes an improved particle swam optimization (PSO) approach to solve the resource-constrained scheduling problem. Two proposed rules named delay local search rule and bidirectional scheduling rule for PSO to solve scheduling problem are proposed and evaluated. These two suggested rules applied in proposed PSO facilitate finding global minimum (minimum makespan). The delay local search enables some activities delayed and altering the decided start processing time, and being capable of escaping from local minimum. The bidirectional scheduling rule which combines forward and backward scheduling to expand the searching area in the solution space for obtaining potential optimal solution. Moreover, to speed up the production of feasible solution, a critical path is adopted in this study. The critical path method is used to generate heuristic value in scheduling process. The simulation results reveal that the proposed approach in this investigation is novel and efficient for resource-constrained class scheduling problem.	mathematical optimization;particle swarm optimization;scheduling (computing)	Ruey-Maw Chen;Chung-Lun Wu;Chuin-Mu Wang;Shih-Tang Lo	2010	Expert Syst. Appl.	10.1016/j.eswa.2009.07.024	fair-share scheduling;nurse scheduling problem;job shop scheduling;mathematical optimization;real-time computing;flow shop scheduling;dynamic priority scheduling;computer science;rate-monotonic scheduling;genetic algorithm scheduling;critical path method;mathematics;distributed computing;lottery scheduling;round-robin scheduling	DB	22.151983887476206	-0.865826362018121	3810
b68c981a011ab681b5083c9615bcb42a9de00c60	landing motion of a legged robot with impact force reduction and joint torque minimization	legged locomotion;joints force legged locomotion robot kinematics foot collision avoidance;foot;motion planning singular configuration multiobjective optimization;joints;force;singular configuration;motion planning;multiobjective optimization;collision avoidance;robot kinematics	This paper deals with an optimal landing motion of a 4-link legged robot that minimizes the impact force at the contact point and the joint torques necessary during the motion. The cost function for optimization is given as the weighted sum of the one for impact force and the one for joint torques. While the configuration where the leg is bent is advantageous in reducing the impact force, the configuration that is close to a singular configuration is advantageous in minimizing the joint torques. It is shown by numerical optimization results with different weights for the cost function.	loss function;mathematical optimization;weight function	Xianglong Wan;Takateru Urakubo;Yukio Tada	2013	2013 Second International Conference on Robot, Vision and Signal Processing	10.1109/RVSP.2013.65	control engineering;simulation;engineering;control theory	Robotics	63.62676268478662	-21.139029625088078	3813
d687a2e8648e7dfe294909bec3439f9c30d3079c	optimizing pushback decisions to valuate airport surface surveillance information	optimisation;surveillance aerospace safety airports decision making optimisation stochastic processes;environmental impacts;stochastic process;surface surveillance;surveillance;airport departure operations;airports;collaborative decision making cdm;surface surveillance airport departure operations collaborative decision making cdm information value optimization;aerospace safety;airport surface traffic control;collaborative decision making;aerospace control;vectors;stochastic processes;ground handling;information value;arrivals and departures;optimization;atmospheric modeling;laguardia airport;aircraft;aircraft atmospheric modeling airports surveillance aerospace control stochastic processes vectors;intelligent operator pushback decision optimization airport surface surveillance information aircraft ground position information future surface surveillance system stochastic model surface operation single ramp surface operation laguardia airport departure surface operation taxiing aircraft runway utilization rate simple threshold policy safety benefit gate holding policy	As airport surface surveillance technologies develop, aircraft ground position information becomes more easily available and accurate. This paper provides a better understanding of the value of future surface surveillance systems where departures, and more specifically pushback times, will be optimized. It analytically quantifies the potential benefits yielded by providing surveillance information to the agent or system that is entrusted with tactically optimizing pushback clearances under nominal conditions. A stochastic model of surface operations is developed for single-ramp surface operations and calibrated to emulate departure surface operations at LaGuardia Airport. Two levels of information are examined within a tactically optimized collaborative decision-making framework. For each level, emissions, number of taxiing aircraft, and runway utilization rate are analyzed and compared with a simple threshold policy to evaluate surface surveillance information. Safety benefits, however, are not considered in this paper. It is estimated that optimally controlling pushback clearances from a single-ramp area using detailed surface surveillance information does not provide significant benefits when compared with controlling pushback clearances using a gate-holding policy based on the number of aircraft currently taxiing. However, when the runway is functioning at intermediate capacity (50%-72% runway utilization rates), e.g., under adverse weather conditions, surveillance information may improve optimization of departure operations. In such case, emissions and the number of taxiing aircraft are reduced by up to 6% when compared with the gate-holding policy and by up to 3% when compared with the performance of an intelligent operator with limited information.	benchmark (computing);computation;downstream (software development);enriques–kodaira classification;entry point;image resolution;intelligent agent;mathematical optimization;media dispatch protocol;observable;optimizing compiler;ramp simulation software for modelling reliability, availability and maintainability;requirement;throughput;trionic;value (ethics)	Pierrick Burgain;Olivia J. Pinon;Eric Feron;John-Paul Clarke;Dimitri N. Mavris	2012	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2011.2166388	stochastic process;atmospheric model;simulation;group decision-making;engineering;value of information;mathematics;transport engineering;statistics	Vision	6.561441215047164	-7.888176623738308	3815
3f5c8c844a0bea7922d54359c6d2f31919f3f72a	adaptive controller design-based ablf for a class of nonlinear time-varying state constraint systems	time-varying systems;nonlinear systems;adaptive control;backstepping;control systems	In this paper, we address an adaptive control problem for a class of nonlinear strict-feedback systems with uncertain parameter. The full states of the systems are constrained in the bounded sets and the boundaries of sets are compelled in the asymmetric time-varying regions, i.e., the full state time-varying constraints are considered here. This is for the first time to control such a class of systems. To prevent that the constraints are overstepped, the time-varying asymmetric barrier Lyapunov functions (TABLFs) are employed in each step of the backsstepping design and we also establish a novel control TABLF scheme to ensure the asymptotic output tracking performance. The performances of the adaptive TABLF-based control are verified by a simulation example.	lyapunov fractal;nonlinear system;performance;simulation	Yanjun Liu;Shumin Lu;Dongjuan Li;Shaocheng Tong	2017	IEEE Transactions on Systems, Man, and Cybernetics: Systems	10.1109/TSMC.2016.2633007	control theory	Embedded	67.32600686988448	-3.7494232203543487	3817
41eec7709a3d360dcb02ef40f2a60a954a54fb9e	image processing algorithms and measures for the analysis of biomedical imaging systems applications	biological patents;biomedical journals;text mining;europe pubmed central;citation search;biomedical imaging;citation networks;research articles;abstracts;open access;life sciences;clinical guidelines;algorithms;full text;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	The medical profession has changed dramatically in the past decade due to the advances in engineering innovations. Traditionally, medical doctors relied solely on their training and experience for diagnosis and interpretation of medical conditions captured through a variety of imaging techniques. Typically, these images are produced from CAT scan machines, MRI, or X-rays. Today, computer aided techniques are designed to enhance the visual quality of biomedical images to better help humans discriminate regions of interest, such as the presence of cancer. Since these enhanced images are intended to assist the medical professional in making a diagnosis or to track the progress of a treatment, it is important that abnormalities presented in the images are actual objects and not artifacts resulting from the enhancement process that could lead to an inaccurate diagnosis. Too much information presented can be distracting for the viewer and too little information presented could be disastrous for the patient, should a life-threatening abnormality be missed or filtered out. Therefore, the dilemma lies in answering the question: “What is the best image to use?” The answer has always been subjective due to the perspective of the human observer and what constitutes the desired target object to be detected. For instance, some doctors looking at a scan may be interested in masses; another doctor may want to focus on calcifications or, even perhaps, the blood vessels. In this special issue, articles are presented that address this question for several different biomedical image processing applications. Several novel quantifiable evaluation methods that can rank the quality of a processed image in accordance with human visual perception are introduced. These metrics are rigorously compared to evaluations obtained fromhuman evaluators using the “Mean Opinion Score” (MOS), along with other standard benchmarking techniques. The innovative approaches presented in this issue are sure to lead the biomedical image processing field beyond one of an “assistive technology” to one that creates a reliable, accurate autonomous technology for use in distance and robot vision applications. Achieving these goals will forge new groundbreaking paths that will reach into and serve remote populations of the world.	assistive technology;autonomous robot;blood vessel;ct scan;congenital abnormality;diagnostic radiologic examination;evaluation;forge;image processing;imaging techniques;medical imaging;morphologic artifacts;neoplasms;patients;physical object;physiologic calcification;population;radiation;region of interest;x-ray computed tomography;algorithm	Karen Panetta;Sos S. Agaian;Jean-Charles Pinoli;Yicong Zhou	2015		10.1155/2015/926921	text mining;medical research;medicine;computer science;bioinformatics;data science;data mining	Graphics	36.40314018143716	-84.18249285751622	3818
27b4fc9a04965b1aad5bb215e7024df8b01f0766	kinematics and workspace analysis of a three-axis parallel manipulator: the orthoglide	workspace isotropy;parallel manipulator;parallel mechanism;singularity analysis;orthoglide;parallel manipulators;singularity;inverse and direct kinematics	The paper addresses kinematic and geometrical aspects of the Orthoglide, a three-DOF parallel mechanism. This machine consists of three fixed linear joints, which are mounted orthogonally, three identical legs and a mobile platform, which moves in the Cartesian x-y-z space with fixed orientation. New solutions to solve inverse/direct kinematics are proposed and we perform a detailed workspace and singularity analysis, taking into account specific joint limit constraints.	algorithm;apache axis;cartesian closed category;existential quantification;hyperlink;inverse kinematics;mathematical optimization;mobile device;parallel manipulator;real life;serial port;technological singularity;workspace	Anatoly Pashkevich;Damien Chablat;Philippe Wenger	2006	Robotica	10.1017/S0263574704000347	singularity;parallel manipulator;simulation;control theory;mathematics;geometry	Robotics	69.82533670304925	-21.05252801913886	3819
bbab62f2a5d2d902614c27ac039a71cfc35096cf	modeling land change using one or two time points based calibration - a comparison of factors		One of land change model parameters in calibration step relates to how changes over time and space are considered in the model. A land change model can be calibrated with the state at one time point or with the difference between two time points. The purpose is describing land use and cover (LUC) state patterns, i.e. one time point calibration, and LUC transition patterns, i.e. two time points. For a case study in Spain we obtained the collections of factors for two calibration periods at one time point (dates 2000 and 2006) and the collections of factors for two calibration periods between two time points (periods 1990-2000 and 20002006). Evidence likelihood is used to transform the explanatory variables into factors. The objective of this paper is to compare these four collections of factors to show how the choice of reference maps influences the factors and how these factors highlight the change patterns in two different calibration periods and in the calibration of two models. As a following step the detailed results for the different factors and LUC categories are analysed.	map;nonlinear system;simulation;state pattern;state transition table	María Teresa Camacho Olmedo	2017		10.5220/0006384503410349	econometrics;geography;remote sensing	SE	24.632342248971906	-21.67461720260272	3820
1c0fb3f9b1cb29f15a02d148942fc93fc3928f1e	trifocal transfer on commodity graphics hardware	tensile stress;estimation;three dimensional displays;graphics processing units;signal processing algorithms;rendering computer graphics;cameras	We present an algorithm and its implementation for the interactive view synthesis of 3D video objects. The algorithm is based on the theory of trifocal transfer using dense depth information of two cameras. In order to implement the algorithm and its post processing stages at interactive rates we employ commodity graphics hardware. We present issues that arise when using the pipeline architecture of the GPU for non-computer graphics algorithms and introduce approaches how to solve these problems with focus on our application. Finally, we show that the employment of a GPU can speed up the synthesis significantly.	algorithm;binocular disparity;computer graphics;graphics hardware;graphics processing unit;mathematical optimization;pipeline (computing);pixel;software framework;speedup;trifocal tensor;view synthesis;yang	Christian Weigel;Peter Schubel	2007	2007 15th European Signal Processing Conference		computer vision;vector graphics;graphics pipeline;2d computer graphics;computer science;theoretical computer science;texture mapping unit;real-time computer graphics;computer graphics;general-purpose computing on graphics processing units;software rendering;3d computer graphics;computer graphics (images)	Graphics	66.94409320801967	-52.913543013423435	3821
5d15edb2487513ea156cec82b2fce847d4f682a6	a customized python module for cfd flow analysis within vistrails	complex fluid flows customized python module cfd flow analysis vistrails standard workflow;high level languages;vistrails;visualization corner;computation fluid dynamics;data mining;cfd flow analysis;computational fluid dynamics;physics;python;visualization;binary star;computational modeling;streaming media;three dimensional displays;complex fluid;quantitative analysis;computational fluid dynamics streaming media displays layout assembly visualization joining processes;visualization corner vistrails astrophysics computational fluid dynamics python;astrophysics;standard workflow;customized python module;switches;complex fluid flows;flow analysis;high level languages computational fluid dynamics	A relatively simple, customized Python module that plugs smoothly into an otherwise standard workflow within VisTrails facilitates a quantitative analysis of complex fluid flows in simulations of merging binary stars.	python;simulation;smoothing;vistrails	Joel E. Tohline;Jinghya Ge;Wesley Even;Erik Anderson	2009	Computing in Science & Engineering	10.1109/MCSE.2009.44	computational science;visualization;python;binary star;computational fluid dynamics;computer science;bioinformatics;quantitative analysis;theoretical computer science;complex fluid;programming language	PL	71.75851522953529	-49.78120524147664	3824
c1470d1b6302f7a5bb0911ac40179d940e10b232	lumi: a pipeline for processing illumina microarray	bioinformatique;microreseau;microarreglo;microarray;bioinformatica;bioinformatics	UNLABELLED Illumina microarray is becoming a popular microarray platform. The BeadArray technology from Illumina makes its preprocessing and quality control different from other microarray technologies. Unfortunately, most other analyses have not taken advantage of the unique properties of the BeadArray system, and have just incorporated preprocessing methods originally designed for Affymetrix microarrays. lumi is a Bioconductor package especially designed to process the Illumina microarray data. It includes data input, quality control, variance stabilization, normalization and gene annotation portions. In specific, the lumi package includes a variance-stabilizing transformation (VST) algorithm that takes advantage of the technical replicates available on every Illumina microarray. Different normalization method options and multiple quality control plots are provided in the package. To better annotate the Illumina data, a vendor independent nucleotide universal identifier (nuID) was devised to identify the probes of Illumina microarray. The nuID annotation packages and output of lumi processed results can be easily integrated with other Bioconductor packages to construct a statistical data analysis pipeline for Illumina data.   AVAILABILITY The lumi Bioconductor package, www.bioconductor.org	affymetrix;bioconductor;gene annotation;microarray;nucleotide universal identifier;nucleotides;pipeline (computing);preprocessor;sample variance;universally unique identifier;algorithm	Pan Du;Warren A. Kibbe;Simon M. Lin	2008	Bioinformatics	10.1093/bioinformatics/btn224	biology;computer science;bioinformatics;data science;microarray;data mining;microarray databases	Comp.	-1.9686105878972733	-57.207808945629736	3825
cd457aa6ece2ccbf5a76ec37a67bdb04e3a33c79	nf-bypass: analysis of blood flow inside a bypass graft to artery connection using adaptive neuro-fuzzy and computational fluid dynamics technique	neuro fuzzy;blood flow		computation;computational fluid dynamics;grafting (decision trees);neuro-fuzzy	N. Arslan;Alexander Nikov;F. Karaca;O. Beyca	2004			neuro-fuzzy;blood flow;computational fluid dynamics;artery;mathematics;control theory	HPC	31.288095671825086	-86.5581298753694	3827
0e0aebfaf15b73dea486ce4af9aa4bfc19d72e2d	signal processing for noise and online force modeling detection for a robot hand based on ethercat communication	robot hand;signal processing;ethercat;noise;force detection	This study proposes three efficient algorithms for online computing of fingertip force modeling and noise involved in 1-D, 2-D and 3-D models. Fast and effective real-time detection for fingertip force modeling is extremely important for successful manipulation on an object by a robot hand in a real situation. Meanwhile, signal processing for a desired force signal is necessary if unwanted signals and noise cannot be ignored in practical applications. Butterworth low-pass filters designed in this study can be effectively used for filtering noise and obtaining flatter and smoother force modeling detection signals. An EtherCAT slave circuit is designed and force detection models using Beckhoff TwinCAT 3.1 provide feedback signals. The calculation is made during several real-time experiments.	algorithm;butterworth filter;experiment;low-pass filter;real-time clock;robot;signal processing	Mingxin Hou;Li Jiang;Hong Liu;Zhaopeng Chen	2016	Intelligent Automation & Soft Computing	10.1080/10798587.2015.1025481	embedded system;computer vision;ethercat;computer science;noise;signal processing	Robotics	75.86060730322156	-12.84101338717125	3828
902c69a3100882cec4e7d5e4c18af2bb7a5eacf9	design and implementation of a new sliding mode controller on an underactuated wheeled inverted pendulum		In this paper, a sliding mode controller (SMC) is proposed for control of a wheeled inverted pendulum (WIP) system, which consists of a pendulum and two wheels in parallel. The control objective is to use only one actuator to perform setpoint control of the wheels while balance the pendulum around the upright position, which is an unstable equilibrium. When designing the SMC for the WIP system, various uncertainties are taken into consideration, including matched uncertainties such as the joint friction, and unmatched uncertainties such as the ground friction, payload variation, or road slope. The SMC proposed is capable of handling system uncertainties and applicable to general underactuated systems with or without input coupling. For switching surface design, the selection of the switching surface coefficients is in general a sophisticated design issue because those coefficients are nonaffine in the sliding manifold. In this work, the switching surface design is transformed into a linear controller design, which is simple and systematic. By virtue of the systematic design, various linear control techniques, such as linear quadratic regulator (LQR) or linear matrix inequality (LMI), can be incorporated in the switching surface design to achieve optimality or robustness for the sliding manifold. To further improve the WIP responses, the design of reference signals is addressed. The reference position for the pendulum is adjusted according to the actual equilibrium of the pendulum, which depends on the size of the friction and slope angle of the traveling surface. A smooth reference trajectory for the setpoint of the wheel is applied to avoid abrupt jumps in the system responses, meanwhile the reaching time of the switching surface can be reduced. The effectiveness of the SMC is validated using intensive simulations and experiment testings.	inverted pendulum;underactuation	Zhao-Qin Guo;Jian-Xin Xu;Tong Heng Lee	2014	J. Franklin Institute	10.1016/j.jfranklin.2013.02.002	control engineering;inverted pendulum;simulation;engineering;double pendulum;control theory	Robotics	65.67904622282754	-14.204530105835826	3829
b5306321846329ef3b3ea8895ed3e1cea7a72b95	global robust stability and stabilization of boolean network with disturbances	boolean networks;stability;stabilization;disturbances;semi-tensor product of matrices	Based on semi-tensor product of matrices, global robust stability and stabilization of Boolean network (BN) with disturbances are investigated. Firstly, the definition of global robust stability for BN with disturbances is presented. By using the algebraic state space representation of disturbed BN, some necessary and sufficient criteria are obtained to ensure the global robust stability with respect to (w.r.t.) a fixed point or w.r.t. a limit cycle. In contrast, if a given disturbed BN is not globally robust stable w.r.t. a fixed point or w.r.t. a limit cycle, system can achieve stability by a matrix transformation technique. However, it is a difficult task to find such a suitable matrix transformation. In this paper, a pinning state feedback control design is proposed to find a suitable transformation. The obtained results are well illustrated by numerical example.	boolean network	Jie Zhong;Daniel W. C. Ho;Jianquan Lu;Wenying Xu	2017	Automatica	10.1016/j.automatica.2017.07.013	limit cycle;mathematical optimization;algebraic number;boolean network;control theory;mathematics;state-space representation;matrix (mathematics);fixed point;transformation matrix	Robotics	68.6884734547701	-0.2377977799101126	3832
9fa2af44c9a1ef4ba2eef1747780636d3555d2ed	radar imaging through cinderblock walls: achievable performance by a model-corrected linear inverse scattering approach	image resolution;singular value decomposition finite difference time domain analysis light scattering radar imaging;through wall imaging twi finite difference time domain fdtd linear inverse scattering multipath;scattering;truncated singular value decomposition algorithm radar imaging cinderblock walls model corrected linear inverse scattering approach born approximation integral equation finite difference time domain method;time domain analysis;image reconstruction;imaging;imaging inverse problems image reconstruction cutoff frequency scattering time domain analysis image resolution;cutoff frequency;inverse problems	We address the problem of imaging targets located behind an inhomogeneous wall made with cinderblocks. The problem, which has relevance in through-wall-imaging applications, is characterized by the presence of multipath propagation phenomena usually producing artifacts and distortions in the retrieved images, if not suitably accounted for in the scattering model. The strategy here adopted to mitigate this issue is to employ a linearized scattering model based on the Born approximation, where the kernel of the relevant integral equation is evaluated numerically by means of the finite-difference time-domain method. In this way, the complexity of the background scenario is accurately taken into account. The inversion is successfully performed by the truncated singular value decomposition algorithm so as to regularize the inverse problem. The achievable imaging capabilities are analyzed in terms of resolution limits, and most notably, resolution can be effectively enhanced, owing to multipath exploitation. Numerical tests based on synthetic data are reported to assess the reconstruction performance in the case of canonical objects.	algorithm;approximation;distortion;finite-difference time-domain method;inter-protocol exploitation;multipath propagation;numerical analysis;relevance;resolution (logic);singular value decomposition;software propagation;synthetic data	Gianluca Gennarelli;Francesco Soldovieri	2014	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2014.2301851	iterative reconstruction;medical imaging;scattering theory;mathematical optimization;image resolution;inverse problem;inverse scattering problem;calculus;cutoff frequency;mathematics;optics;scattering;physics	Vision	74.27882159387657	-68.99390426351205	3834
36c6b47b14108c1fec654b66dc80889312ec8a47	all-pass filtering in iterative learning control	learning process;computacion informatica;iterative learning control;grupo de excelencia;robot manipulator;phase shifter;ciencias basicas y experimentales;phase lead compensators;robotic manipulators;all pass filters	In iterative learning control (ILC), it is highly desirable to have a learning compensator with a unitgain for all frequencies, in order to avoid noise amplification and learning speed degradation during the learning process. In this paper, we show that the realization of a unit-gain compensator is straightforward in ILC, using both forward and backward filtering. As an illustrative example, a unit-gain derivative is proposed to overcome the drawbacks of the conventional derivative. The proposed scheme is equivalent to an all-pass unit-gain phase shifter; the forward filtering uses a 0.5-order derivative and the backward filtering employs a 0.5-order integral. The all-pass phase shifter is deployed in a unit-gain D-type ILC. The advantages of the unit-gain feature are demonstrated by some experimental results on a robot manipulator. © 2008 Elsevier Ltd. All rights reserved.	all-pass filter;elegant degradation;iterative method;robot;system camera	Yongqiang Ye;Abdelhamid Tayebi;Peter Xiaoping Liu	2009	Automatica	10.1016/j.automatica.2008.07.011	control engineering;electronic engineering;engineering;phase shift module;control theory;iterative learning control	AI	67.61184058960279	-6.101231796469421	3835
52278fa6b7e6b01eb6dbd78b578d33f64bc057ce	proposal of a new structural thermal vacuum sensor with diode-thermistors combined with a micro-air-bridge heater	temperature ambiante;estructura suspendida;resistencia termica;soi;temperatura ambiente;suspended structure;thermal resistance;temperature control;structure suspendue;termistencia;union p n;captador presion;vacuum sensor;medida presion;thermal detector;thermal sensor;systeme asservi;temperature sensor;transfert chaleur;pressure sensor;sensor temperatura;pressure measurement;capteur temperature;temperature sensor microheater;heat transfer;heat exchanger;low power electronics;detecteur thermique;transferencia termica;thermal conductivity;detector termico;servomecanismo;resistance thermique;pn junction;silicon on insulator technology;technologie silicium sur isolant;microheater;0707d;power consumption;feedback system;ambient temperature;consommation energie electrique;mesure pression;jonction p n;room temperature;conductividad termica;capteur pression;commande temperature;thermistor;electronique faible puissance;high sensitivity;low power consumption;feedback control;conductivite thermique;diode jonction;thermistance;tecnologia silicio sobre aislante;junction diodes;control temperatura;p n junction	A new structure thermal vacuum sensor with two pn junction diodes, Th-A and Th-B, as a high sensitive temperature sensor working like a thermistor and a micro heater formed on a micro air‒bridge is proposed. The micro air‒bridge is separated into two regions of A and B. The Th-A measures the temperature of the micro-heater formed on the region A and the Th-B does that of the region B which is connected to the micro-heater with thermal resistance. The diode thermistor, Th-C, formed on the SOI substrate is provided to measure the ambient temperature Tc. Vacuum pressure is measured based on heat dissipation change from the heated micro air‒ bridge to ambient gas. It is demonstrated that this sensor will expand the measuring vacuum range into about two orders or more wider range (2x10-1x10 Pa) than the traditional thermal vacuum sensor, such as Pirani vacuum sensor, and has very fast response and low power consumption.	ambient occlusion;diode;p–n junction;sensor web;silicon on insulator;thermal management (electronics);thermal resistance	Mitsuteru Kimura;Fumitoshi Sakurai;Hirao Ohta;Tomoyuki Terada	2007	Microelectronics Journal	10.1016/j.mejo.2006.09.018	electronic engineering;telecommunications;electrical engineering;feedback;room temperature;physics;quantum mechanics;p–n junction	Robotics	92.77907683474663	-10.564356949216979	3836
a53e00fe275ed6bffffecd35269c7de3e847b62c	evaluation of data mining techniques for suspicious network activity classification using honeypots data	extraction information;0705k;cluster algorithm;networks;decision tree;neural networks;0705m;information extraction;reseau ordinateur;data processing;traitement donnee;intrusion detection;service telecommunication;data mining;remote operation;computer networks;data analysis;artificial neural networks;fouille donnee;servicio de red;network traffic;teleaccion;algorithme genetique;service reseau;telecommunication services;algorithms;analyse donnee;genetic algorithm;k nearest neighbor;genetic algorithms;classification automatique;reseau neuronal;network services;automatic classification;clasificacion automatica;network service;extraccion informacion;artificial neural network;teleoperation;neural network	As the amount and types of remote network services increase, the analysis of their logs has become a very difficult and time consuming task. There are several ways to filter relevant information and provide a reduced log set for analysis, such as whitelisting and intrusion detection tools, but all of them require too much finetuning work and human expertise. Nowadays, researchers are evaluating data mining approaches for intrusion detection in network logs, using techniques such as genetic algorithms, neural networks, clustering algorithms, etc. Some of those techniques yield good results, yet requiring a very large number of attributes gathered by network traffic to detect useful information. In this work we apply and evaluate some data mining techniques (KNearest Neighbors, Artificial Neural Networks and Decision Trees) in a reduced number of attributes on some log data sets acquired from a real network and a honeypot, in order to classify traffic logs as normal or suspicious. The results obtained allow us to identify unlabeled logs and to describe which attributes were used for the decision. This approach provides a very reduced amount of logs to the network administrator, improving the analysis task and aiding in discovering new kinds of attacks against their networks.	artificial neural network;cluster analysis;data mining;decision tree learning;expert system;genetic algorithm;honeypot (computing);intrusion detection system;k-nearest neighbors algorithm;machine learning;network security;network traffic control;neural networks;numerical analysis;requirement;unsupervised learning;whitelist	André Ricardo Abed Grégio;Rafael D. C. Santos;Antonio Montes	2007		10.1117/12.719023	genetic algorithm;computer science;artificial intelligence;machine learning;data mining;artificial neural network	ML	6.570199589438343	-33.703908687130436	3837
3408f023886200a763646a4e8485d694990e17c5	pairwise boosted audio fingerprint	audio signal processing;audio fingerprinting;distance measure;classification error reduction pairwise boosted audio fingerprint binary audio fingerprint spectral centroids filtering spectral centroids quantization coined pairwise boosting feature selection algorithm query audio clip binary classification problem;binary classification problem;classification error reduction;spectral centroids quantization;binary audio fingerprint;coined pairwise boosting;audio recording;indexing terms;query audio clip;quantisation signal;iterative methods;boosting;feature selection algorithm;fingerprint recognition;quantisation signal audio signal processing filtering theory iterative methods pattern classification;spectral centroids filtering;classification algorithms;pattern classification;classification error;matched filters;robustness;feature selection;fingerprint recognition filters lead iterative algorithms robustness machine learning algorithms boosting laboratories casting electronic mail;algorithm design and analysis;filtering theory;content based audio identification;pairwise boosted audio fingerprint;content based audio identification audio fingerprinting boosting	A novel binary audio fingerprint obtained by filtering and then quantizing the spectral centroids is proposed. A feature selection algorithm, coined pairwise boosting (PB), is used to determine the filters and quantizers by casting the fingerprinting problem of identifying a query audio clip into a binary classification problem. The PB algorithm selects the filters and quantizers which lead to accurate classification of matching and nonmatching audio pairs: a matching pair is an audio pair that should be classified as being identical, and a nonmatching pair is a pair that should be classified as being different. By iteratively reducing the classification error of both matching and nonmatching pairs, the PB algorithm improves both the robustness and discriminating ability. In our experiments, the proposed fingerprint outperformed previously reported binary fingerprints in terms of robustness and discriminating ability. In the experiment, we compared the performances of a number of distance measures.	acoustic fingerprint;adaboost;audio signal processing;binary classification;experiment;feature selection;fingerprint (computing);pairwise summation;performance;quantization (signal processing);selection algorithm	Dalwon Jang;Chang Dong Yoo;Sunil Lee;Sungwoong Kim;Ton Kalker	2009	IEEE Transactions on Information Forensics and Security	10.1109/TIFS.2009.2034452	algorithm design;speech recognition;index term;audio signal processing;computer science;machine learning;pattern recognition;iterative method;matched filter;feature selection;fingerprint recognition;boosting;robustness	Vision	30.968317127328756	-64.7374437219481	3839
aba119b674b74af108c81df093e800ad60c52336	a numerical test for the closure properties of 3-d grasps	optimisation sous contrainte;constrained optimization;contacto mecanico;manipulators;modelo 3 dimensiones;convex programming;numerical method;modele 3 dimensions;multifingered grasp;contact ponctuel;prension;convex constrained optimization;three dimensional model;test rango;rank test;programmation convexe;testing;robotics;gripping;multifingered grasps;orbital robotics;three dimensional;contacto puntual;optimizacion con restriccion;dexterous manipulators;single linear program;frotamiento deslizamiento;mechanical contact;contact mecanique;numerical analysis;point contact;parallel robots;form closure;metodo numerico;linearisation;linearizacion;relative;cone;force closure;performance analysis;linear programming numerical analysis dexterous manipulators;linear programming;numerical test;robotica;linear program;linearization;parallel machines;prehension;robotique;testing parallel robots robotics and automation jacobian matrices orbital robotics manipulators robot kinematics performance analysis parallel machines design methodology;jacobian matrices;constrained optimization problem;frottement glissement;test rang;robotics and automation;methode numerique;3d frictionless grasps;single linear program numerical test 3d frictionless grasps force closure form closure multifingered grasps convex constrained optimization;robot kinematics;sliding friction;design methodology;cono;programacion convexa	This paper presents a numerical test for the closure properties (force closure and form closure) of multifingered grasps. For three-dimensional (3-D) grasps with frictional point contacts or soft contacts, the numerical test is formulated as a convex constrained optimization problem without linearization of the friction cone. For 3-D frictionless grasps, it can be calculated by solving a single linear program. The proposed numerical test (along with the rank of the grasp matrix) provides an efficient tool for the analysis of the force-closure property and the relative force-closure property.	constrained optimization;constraint (mathematics);linear programming;mathematical optimization;numerical analysis;optimization problem	Xiangyang Zhu;Han Ding;Michael Yu Wang	2004	IEEE Transactions on Robotics and Automation	10.1109/TRA.2004.825514	control engineering;mathematical optimization;simulation;numerical analysis;computer science;linear programming;mathematics	Robotics	69.9378838668701	-19.629308030878196	3841
20281e61838af5dcd7b7bcff195cd8843996ee14	an integrated replenishment model with quantity discounts, reentry and downward substitution for control wafers	control wafers;mixed integer programming;replenishment;quantity discounts;dynamic lot sizing	This paper considers control wafers replenishment problem in wafer fabrication factories. A dynamic lot-sizing replenishment problem with reentry and downward substitution is examined in a pulling control production environment. The objective is to set the inventory level so as to minimize the total cost of control wafers, where the costs include order cost, purchase cost, setup cost, production cost and holding cost, while maintaining the same level of production throughput. In addition, purchase quantity discounts and precise inventory level are considered in the replenishment model. The control wafers replenishment problem is first constructed as a network, and is then transformed into a mixed integer programming model. Lastly, an efficient heuristic algorithm is proposed for solving large-scale problems. A numerical example is given to illustrate the practicality for empirical investigation. The results demonstrate that the proposed mixed integer programming model and the heuristic algorithm are effective tools for determining the inventory level of control wafers for multi-grades in multi-periods.	algorithm;deployment environment;dynamic lot-size model;heuristic (computer science);integer programming;inventory;linear programming;national supercomputer centre in sweden;numerical analysis;programming model;semiconductor fabrication plant;take-grant protection model;throughput;wafer (electronics);wafer fabrication	He-Yau Kang;Amy Hsin-I Lee;Chun-Mei Lai	2012	J. Intelligent Manufacturing	10.1007/s10845-010-0479-z	mathematical optimization;integer programming;computer science;engineering;engineering drawing	Robotics	10.771122789492457	-2.8751771602923086	3843
dc27bdd7dec42c70d7d734b220baf74645ef7273	towards a dynamic modeling of the predator prey problem	predator prey model;dynamic optimization problems;evolutionary algorithms	This article addresses a new dynamic optimization problem (DOP) based on the Predator-Prey (PP) relationship in nature. Indeed, a PP system involves two adversary species where the predator’s objective is to hunt the prey while the prey’s objective is to escape from its predator. By analogy to dynamic optimization, a DOP can be seen as a predation between potential solution(s) in the search space, which represents the predator, and the moving optimum, as the prey. Therefore we define the dynamic predator-prey problem (DPP) whose objective is to keep track of the moving prey, so as to minimize the distance to the optimum. To solve this problem, a dynamic approach that continuously adapts to the changing environment is required. Accordingly, we propose a new evolutionary approach based on three main techniques for DOPs, namely: multi-population scheme, random immigrants, and memory of past solutions. This hybridization of methods aims at improving the evolutionary approaches ability to deal with DOPs and to restrain as much as possible their drawbacks. Our computational experiments show that the proposed approach achieves high performance for DPP and and competes with state of the art approaches.	adversary (cryptography);computation;digital photo professional (dpp);dynamic programming;experiment;iterative and incremental development;lotka–volterra equations;mathematical optimization;optimization problem;prey	Hajer Ben-Romdhane;Enrique Alba;Saoussen Krichen	2015	Applied Intelligence	10.1007/s10489-015-0727-1	simulation;computer science;artificial intelligence;machine learning;evolutionary algorithm	AI	25.922476736253696	-4.694275259470585	3846
d0999fdbff5dfe4a1ed92599db90ff6fddfeeeb5	tied factors analysis for high-dimensional image feature extraction and recognition application		Feature extraction from images, which are typical of high dimensionality, is crucial to the recognition performance. To explore the discriminative information while depressing the intra-class variations due to variable illumination and view conditions, we propose a factor analysis framework for separate “content” from “style,” identifying a familiar face seen under unfamiliar viewing conditions, classifying familiar poses presented in an unfamiliar face, estimating age across unfamiliar faces. The framework applies efficient algorithms derived from objective factor separating functions and space mapping functions, which can produce sufficiently expressive representations of feature extraction and dimensionality reduction. We report promising results on three different tasks in the high-dimensional image perceptual domains: face identification with two benchmark face databases, facial pose classification with a benchmark facial pose database, extrapolation of age to unseen facial image. Experimental results show that our approach produced higher classification performance when compared to classical LDA, WLDA, LPP, MFA, and DLA algorithms.	algorithm;benchmark (computing);database;dimensionality reduction;drive letter assignment;extrapolation;facial recognition system;factor analysis;feature (computer vision);feature extraction;heuristic;information theory;kernelization;local-density approximation;space mapping	Haibin Liao;Youbin Chen;Wenhua Dai;Ruolin Ruan	2016	Pattern Analysis and Applications	10.1007/s10044-016-0572-9	computer vision;feature extraction;computer science;machine learning;pattern recognition	Vision	26.73313041693695	-44.05402221149004	3851
1375accccd52697fc1948e05a3bc91c4a3e30a4d	classification of the heart auscultation signals		Listening to the internal body sounds (auscultation) is one of the oldest techniques in medicine to diagnose heart and lung diseases. The digital heart auscultation signals are obtained with digital electronic stethoscope and can be processed automatically to obtain some coarse indications about the heart or lung condition. There are many ways of how to process the auscultation signals and quite some were published in the last years. In this paper we present one possible set of methods to reach the goal of heart murmur recognition up to the level to distinguish between the pathological murmurs from the physiological ones. The special attention was devoted to signal feature selection and extraction where we used the distribution of signal power over frequencies as the key difference between the normal and the pathological murmurs. The whole procedure including the signal processing, the feature extraction and the comparison of four machine learning classification methods is adequately described. It was tested on a balanced and on an unbalanced dataset with the best achieved classification accuracy of 87.5%.	feature extraction;feature selection;machine learning;murmurhash;signal processing;unbalanced circuit	Primoz Kocuvan;Drago Torkar	2015			heart auscultation;computer science;feature extraction;electronic stethoscope;feature selection;signal processing;statistical classification;speech recognition;heart murmur;auscultation	Web+IR	16.39834014157178	-89.23455722547102	3852
35e005e76cb2d7fb43a05c158c52f9751f113899	a hierarchical neural model in short-term load forecasting	context information;neural model;multilayer perceptron;load forecasting;short term load forecasting;data extraction;self organizing map;self organized map;electric utilities;neural network	This paper proposes a novel neural model to the problem of short-term load forecasting (STLF). The neural model is made up of two self-organizing map (SOM) nets—one on top of the other. It has been successfully applied to domains in which the context information given by former events plays a primary role. The model was trained on load data extracted from a Brazilian electric utility, and compared to a multilayer perceptron (MLP) load forecaster. It was required to predict once every hour the electric load during the next 24 h. The paper presents the results, the conclusions, and points out some directions for future work.		Otávio Augusto S. Carpinteiro;Agnaldo J. R. Reis;Alexandre P. Alves da Silva	2004	Appl. Soft Comput.	10.1016/j.asoc.2004.02.005	self-organizing map;computer science;artificial intelligence;machine learning;data mining;multilayer perceptron;artificial neural network	NLP	9.226500597911194	-18.236397271369974	3858
8230fbcb51d3b5c5f14b28acc709ac44918202c6	an investigation into improved non-contact adhesion mechanism suitable for wall climbing robotic applications	stratified flow end effectors grippers intelligent sensors plastic flow;electric shock;computation fluid dynamics;stratified flow;force;adhesives;computational fluid dynamics;grippers;adhesives force grippers computational fluid dynamics atmospheric modeling electric shock;low pressure;adhesion force;atmospheric modeling;viscous flow entrainment wall climbing robot non contact end effectors pneumatic pick and place adhesion force bernoulli principle;plastic flow;intelligent sensors;end effectors	Pneumatic, non-contact end effectors for robotic pick-and-place applications use compressed air to provide an adhesion force based on the Bernoulli principle, which creates a low pressure region near the surface of the end effector. However, consideration of the influence of the Bernoulli principle alone gives an inadequate account of the performance of some end effector designs. In fact, end effector geometry can be constructed in such a way that the Bernoulli principle provides only a small contribution to the overall adhesion force. This paper investigates the role of viscous flow entrainment in improving adhesion efficiency. Both simulation and experimental results show a significant improvement of adhesion force by nearly five times over a commercially available Bernoulli gripper, which makes the improved adhesion mechanism suitable for more demanding wall climbing robotic applications.	bernoulli polynomials;brainwave entrainment;euler–bernoulli beam theory;hill climbing;navier–stokes equations;newman's lemma;robot end effector;smt placement equipment;simulation;velocity (software development)	Matthew Journee;Xiaoqi Chen;James Robertson;Mark Jermy;Mathieu Sellier	2011	2011 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2011.5979842	control engineering;atmospheric model;plasticity;robot end effector;simulation;computational fluid dynamics;engineering;low-pressure area;adhesive;force;intelligent sensor;mechanical engineering	Robotics	75.05317837505773	-23.285249130570367	3860
0d899231131e715903c0dd14a291d5e50dcd2f3a	empirical investigation of fault predictors in context of class membership probability estimation	probability;chidamber and kemerer metrics;performance;weka;accuracy;estimation;classifiers	In the domain of software fault prediction, class membership probability of a selected classifier and the factors related to its estimation can be considered as necessary information for tester to take informed decisions about software quality issues. The objective of this study is to empirically investigate the class membership probability estimation capability of 15 classifiers/fault predictors on 12 datasets of open source projects retrieved from PROMISE repository. We empirically validate the effect of dataset characteristics and set of metrics on the performance of classifiers in estimating the class membership probability. We used Receiver Operating Characteristics-Area under Curve (ROC-AUC) value and overall accuracy as benchmarks to evaluate and compare the performance of classifiers. We apply Friedman's, post-hoc Nemenyi and Analysis of Means (ANOM) test to compare the significant performance of classifiers. We conclude that ADTree and RandomForest outperform, while ZeroR classifier cannot show significant performance for estimation of class membership probability.	alternating decision tree;hoc (programming language);open-source software;software quality;statistical classification	Shahid Hussain;Arif Ali Khan;Kwabena Ebo Bennin	2016		10.1145/2851613.2851973	estimation;performance;computer science;machine learning;pattern recognition;probability;data mining;accuracy and precision;statistics	SE	6.913426200358981	-40.49948349104245	3863
308f384d9136e95fec22acb3b8a58f2ae0d10d7d	a hierarchical deep neural network design for stock returns prediction		Abstract We present in this paper a hierarchical Deep Neural Network for stock returns prediction. This DNN is trained in a high frequency context, we use 5 minutes returns of TUNINDEX stocks in a period of 4 years. The designed network aims to predict the next 5 minutes return of a given stock. The predictive power of our network is improved by the hierarchical design and stocks classification while the training process is simplified by dimensionality reduction techniques. Experimental study shows an accuracy up to 71% and a considerable improvement comparing to recent related works.		Oussama Lachiheb;Mohamed Salah Gouider	2018		10.1016/j.procs.2018.07.260	machine learning;stock (geology);dimensionality reduction;artificial neural network;artificial intelligence;predictive power;computer science	EDA	7.282643203293683	-20.103367071475702	3866
52d79c3bfea98ab72c12e2151464d224e05229d7	noe ts fuzzy modelling of nonlinear dynamic systems with uncertainties using symbolic interval-valued data	symbolic interval valued clustering;nonlinear system identification;fuzzy modelling;interval valued linear regression;uncertainty modelling	An approach to Nonlinear Output Error (NOE) modelling using Takagi-Sugeno (TS) fuzzy model for a class of nonlinear dynamic systems having variability in their outputs is presented. Furthermore, the appro ch is compared and graphically illustrated with other alternate approaches on the basis of interval data and interval membership functions. Assuming the identification method can be repeated offline a number of times under similar conditions, multiple inputoutput time series can be obtained from the underlying system. These time series are pre-processed using the techniques of statistics and probability theory to generate the envelopes of response (curves outlining the upper and lower extremes of response) at each time instant. Two types of envelopes are described in this research: the max-min envelopes and the envelopes based on the confidence intervals provided by extended Chebyshev’s inequality. By incorporating interval data in fuzzy modelling and using the theory of symbolic interval-valued data, a TS fuzzy model with interval antecedent and consequent parameters is obtained. This algorithm provides a model for predicting the expected response as well as envelopes. In order to validate the presented model, a simulation case study is devised in this paper. Moreover, it is demonstrated on the real data obtained from an electro-mechanical throttle valve. ∗Corresponding author Email addresses: salman.zaidi@mrt.uni-kassel.de (Salman Zaidi), andreas.kroll@mrt.uni-kassel.de (Andreas Kroll) Preprint submitted to Elsevier March 12, 2017 Final Menuscript Click here to view linked References	algorithm;benchmark (computing);cisco ios;cluster analysis;complex systems;computational intelligence;dynamical system;email;fm broadcasting;fuzzy logic;heart rate variability;intelligent control;machine learning;mathematical optimization;maxima and minima;microsoft outlook for mac;multimodal interaction;nonlinear system;online and offline;probabilistic analysis of algorithms;simulation;social inequality;spatial variability;stochastic process;time series;ultrasparc t1	Salman Zaidi;Andreas Kroll	2017	Appl. Soft Comput.	10.1016/j.asoc.2017.04.004	mathematical optimization;computer science;artificial intelligence;machine learning;mathematics;nonlinear system identification;statistics	ML	6.759912631780495	-21.01776519433624	3871
018b9936111e5eb2510c8644d22e149452b4c9dc	using correlated parameters for improved ranking of protein-protein docking decoys	benchmark;interaction;supercomputer education research centre;scoring;ranking;docking;protein protein docking;quaternary structure;prediction;method;bioinformatics centre	A successful protein-protein docking study culminates in identification of decoys at top ranks with near-native quaternary structures. However, this task remains enigmatic because no generalized scoring functions exist that effectively infer decoys according to the similarity to near-native quaternary structures. Difficulties arise because of the highly irregular nature of the protein surface and the significant variation of the nonbonding and solvation energies based on the chemical composition of the protein-protein interface. In this work, we describe a novel method combining an interface-size filter, a regression model for geometric compatibility (based on two correlated surface and packing parameters), and normalized interaction energy (calculated from correlated nonbonded and solvation energies), to effectively rank decoys from a set of 10,000 decoys. Tests on 30 unbound binary protein-protein complexes show that in 16 cases we can identify at least one decoy in top three ranks having ≤10 Å backbone root mean square deviation from true binding geometry. Comparisons with other state-of-art methods confirm the improved ranking power of our method without the use of any experiment-guided restraints, evolutionary information, statistical propensities, or modified interaction energy equations. Tests on 118 less-difficult bound binary protein-protein complexes with ≤35% sequence redundancy at the interface showed that in 77% cases, at least 1 in 10,000 decoys were identified with ≤5Å backbone root mean square deviation from true geometry at first rank. The work will promote the use of new concepts where correlations among parameters provide more robust scoring models. It will facilitate studies involving molecular interactions, including modeling of large macromolecular assemblies and protein structure prediction.	boat dock;docking (molecular);energy, physics;inference;interaction energy;interface device component;internet backbone;macromolecular docking;mean squared error;medical device incompatibility problem;membrane proteins;physical restraint equipment (device);plant roots;protein structure prediction;score;scoring functions for docking;set packing;vertebral column	Pralay Mitra;Debnath Pal	2011	Journal of computational chemistry	10.1002/jcc.21657	protein quaternary structure;docking;method;interaction;chemistry;benchmark;prediction;ranking;computational chemistry	Comp.	11.761001721080989	-58.82912210936383	3875
b1c6156f5e1afa2bfa5bc4512873f384ed588151	the planning-ahead smo algorithm		The sequential minimal optimization (SMO) algorithm and variants thereof are the de facto standard method for solving large quadratic programs for support vector machine (SVM) training. In this paper we propose a simple yet powerful modification. The main emphasis is on an algorithm improving the SMO step size by planning-ahead. The theoretical analysis ensures its convergence to the optimum. Experiments involving a large number of datasets were carried out to demonstrate the superiority of the new algorithm.	algorithm;experiment;greedy algorithm;hybrid algorithm;iteration;mathematical optimization;newton;pa-risc;sequential minimal optimization;support vector machine;vergence	Tobias Glasmachers	2013	CoRR		mathematical optimization;machine learning;mathematics;sequential minimal optimization;algorithm	ML	21.214743056983988	-38.08874572007917	3876
2ad7397e319b6734bf8ae6dd0d98ca02aa5ed211	3d building roof reconstruction from point clouds via generative models	statistical approach;airborne laser scanning;model selection;high resolution;bottom up;building;generic model;top down;low resolution;statistical method;statistical model;rjmcmc;reversible jump markov chain monte carlo;laser scanning;point cloud;statistical modeling;building model;3d reconstruction;lidar	This paper presents a generative statistical approach to 3D building roof reconstruction from airborne laser scanning point clouds. In previous works bottom-up methods, e.g., points clustering, plane detection, and contour extraction, are widely used. Since the laser scanning data of urban scenes often contain extra structures and artefacts due to tree clutter, reflection from windows, water features, etc., bottom-up reconstructions may result in a number of incomplete or irregular roof parts.  We propose a new top-down statistical method for roof reconstruction, in which the bottom-up efforts mentioned above are no more required. Based on a predefined primitive library we conduct a generative modeling to construct the target roof that fit the data. Allowing overlapping, primitives are assembled and, if necessary, merged to present the entire roof. The selection of roof primitives, as well as the sampling of their parameters, is driven by the Reversible Jump Markov Chain Monte Carlo technique. Experiments are performed on both low-resolution (1m) and high-resolution (0.18m) data-sets. For high-resolution data we also show the possibility to reconstruct smaller roof features, such as chimneys and dormers. The results show robustness despite the clutter and flaws in the data points and plausibility in reconstruction.	airborne ranger;bottom-up parsing;bottom-up proteomics;cluster analysis;clutter;data point;generative modelling language;generative model;image resolution;microsoft windows;monte carlo method;plausibility structure;point cloud;reversible-jump markov chain monte carlo;sampling (signal processing);top-down and bottom-up design	Hai Huang;Claus Brenner;Monika Sester	2011		10.1145/2093973.2093977	statistical model;computer vision;simulation;image resolution;computer science;machine learning;top-down and bottom-up design;mathematics;cartography;statistics;remote sensing	Vision	58.03419765402478	-45.44852116654179	3882
f28fc9551aed8905844feae98b4697301b9e983b	generating roc curves for artificial neural networks	image recognition feedforward neural nets backpropagation medical image processing;image recognition;receiver operator characteristic;backpropagation;feature space;medical image;artificial neural networks biomedical imaging image analysis backpropagation image segmentation computer science biomedical engineering performance analysis cost benefit analysis neural networks;medical image processing;roc analysis;classification system;detection rate;true positive;roc curve;feedforward neural nets;false positive;artificial neural network;backpropagation neural nets roc curves artificial neural networks receiver operating characteristic analysis diagnostic performance medical imaging classification system feature space bias unit value hidden layer nodes	"""Receiver operating characteristic (ROC) analysis is an established method of measuring diagnostic performance in medical imaging studies. Traditionally, artificial neural networks (ANN's) have been applied as a classifier to find one """"best"""" detection rate. Recently researchers have begun to report ROC curve results for ANN classifiers. The current standard method of generating ROC curves for an ANN is to vary the output node threshold for classification. Here, the authors propose a different technique for generating ROC curves for a two class ANN classifier. They show that this new technique generates better ROC curves in the sense of having greater area under the ROC curve (AUC), and in the sense of being composed of a better distribution of operating points."""	anatomic node;artificial neural network;hypothalamic area, lateral;medical imaging;roc curve;receiver operating characteristic;receiver operator characteristics	Kevin S. Woods;Kevin W. Bowyer	1994	IEEE Transactions on Medical Imaging	10.1109/CBMS.1994.316012	computer science;artificial intelligence;machine learning;pattern recognition;receiver operating characteristic;artificial neural network	ML	34.0739096532333	-77.10437309826986	3883
87eeb312cfc5e0a601940e2a67f41237ae6cbd66	decentralized receding horizon control and coordination of autonomous vehicle formations	optimisation sous contrainte;modelizacion;constrained optimization;utilisation information;robot movil;graph theory;esquiva colision;topology;autonomous vehicle formation;predictive control;optimisation;uso informacion;conflict free trajectory decentralized receding horizon control autonomous vehicle formation high fidelity model organic air vehicle graph structure communication topology collision avoidance emergency maneuver invariant set theory optimisation formation flight;invariant set;formacion;autonomous vehicle;information use;autonomous system;emergency maneuver;teoria conjunto;topologie;level control;decentralized receding horizon control;conjunto invariante;theorie ensemble;control modelo predicativo;mobile robots;set theory;set invariance constrained optimization formation flight hierarchical decomposition receding horizon control;commande niveau;formation flight;receding horizon;sistema autonomo;hierarchical decomposition;invariant set theory;set invariance;formation;topologia;receding horizon control;model predictive control;feasibility;optimizacion con restriccion;modelisation;graph structure;aerospace control;decentralised control;robot mobile;commande mpc;high fidelity model;commande decentralisee;conflict free trajectory;systeme autonome;decentralized control;control decentralizado;ensemble invariant;coordinacion;set theory aerospace control collision avoidance decentralised control graph theory mobile robots optimisation predictive control;horizon fuyant;collision avoidance;organic air vehicle;esquive collision;modeling;open ended horizon;article;practicabilidad;faisabilite;moving robot;horizonte huidizo;remotely operated vehicles mobile robots unmanned aerial vehicles communication system control laboratories distributed control collision avoidance land vehicles underwater vehicles monitoring;coordination;communication topology	This paper describes the application of a novel methodology for high-level control and coordination of autonomous vehicle teams and its demonstration on high-fidelity models of the organic air vehicle developed at Honeywell Laboratories. The scheme employs decentralized receding horizon controllers that reside on each vehicle to achieve coordination among team members. An appropriate graph structure describes the underlying communication topology between the vehicles. On each vehicle, information about neighbors is used to predict their behavior and plan conflict-free trajectories that maintain coordination and achieve team objectives. When feasibility of the decentralized control is lost, collision avoidance is ensured by invoking emergency maneuvers that are computed via invariant set theory.	autonomous car;autonomous robot;distributed control system;high- and low-level;set theory	Tamás Keviczky;Francesco Borrelli;Kingsley Fregene;Datta N. Godbole;Gary J. Balas	2008	IEEE Transactions on Control Systems Technology	10.1109/TCST.2007.903066	control engineering;constrained optimization;simulation;engineering;graph theory;control theory;mathematics;model predictive control	Robotics	59.53264446329203	-18.18405318630191	3884
e3923ba48a09e27141df413a50e144a908f6029b	anatomically guided neuronavigation. sample applications of the sulcuseditor in a clinical setting	magnetic resonance image	The clinical use of the SulcusEditor (SE) on an everyday basis for preoperative analysis of 3D MRI data sets of neurosurgical patients is reported. The SE is applied to MRI data sets of patients with tumors in the central and the temporal region of the brain in order to trace the individual sulcal pattern. This leads to a better understanding of the topographical relationships between the tumor and the surrounding tissue. Hence the operating neurosurgeon is provided preoperatively with detailed knowledge concerning the area of interest, eventually leading to a reduction of the operating trauma. Three cases of patients with tumors in the central region are shown examplarily.	topography	Volker Arnd Coenen;Klaus Niemann;Uwe Spetzger;Armin Thron	1998				ML	36.448353271837384	-83.64298942787407	3886
69283521732f20012386a2d00e265203e46d5729	robust 1-d barcode recognition on camera phones and mobile product information display	recognition algorithm;camera phone;camera movement;present result;1-d barcode recognition;matlab implementation;present mobilepid;robust algorithm;mobile product information display;1-d barcodes;proposed algorithm;web-enabled camera phone	In this paper we present a robust algorithm for the recognition of 1-D barcodes using camera phones. The recognition algorithm is highly robust regarding the typical image distortions and was tested on a database of barcode images, which covers typical distortions, such as inhomogeneous illumination, reflections, or blurs due to camera movement. We present results from experiments with over 1,000 images from this database using a MATLAB implementation of our algorithm, as well as experiments on the go, where a Symbian C++ implementation running on a camera phone is used to recognize barcodes in daily life situations. The proposed algorithm shows a close to 100% accuracy in real life situations and yields a very good resolution dependent performance on our database, ranging from 90.5% (640 ×480) up to 99.2% (2592 ×1944). The database is freely available for other researchers. Further we shortly present MobilePID, an application for mobile product information display on web-enabled camera phones. MobilePID uses product information services on the internet or locally stored on-device data.	barcode	Steffen Wachenfeld;Sebastian Terlunen;Xiaoyi Jiang	2008		10.1007/978-3-642-12349-8_4	computer vision;camera auto-calibration;simulation;computer science;computer graphics (images)	HCI	26.754217745671028	-59.764521389670975	3889
c2e026ae0af989da5a2e96359e49ab19bd47e9e3	miniature humanoid mh-1 for wearable telecommunicator	stereo camera system wearable telecommunicator personal telerobot system wearable telexistence system wearable miniature humanoid robot mh 1 4 dof dual arm 3 dof head;humanoid robot;manipulators;concept design;sensors;acoustics;prototypes;wires;joints;robot vision;humanoid robots;robots;stereo image processing;telerobotics;joints wires sensors robots prototypes acoustics calibration;telerobotics humanoid robots manipulators robot vision stereo image processing;calibration	In our previous work, a personal telerobot system named “telecommunicator” has been introduced. The telecommunicator achieves communication with a remote person through a teleoperated robot. The telecommunicator can be considered as a wearable telexistence system. Until now, three prototypes of the wearable telecommunicator, T1, T2 and T3, have been developed as research platforms. Through these robots, the operator can communicate with a person in the remote site with realistic sensation. However, it is difficult for the person in the remote site to feel presence of the operator, because of their typical robot appearance. In this paper, to tackle this problem, a wearable miniature humanoid robot MH-1 is introduced as the telecommunicator. MH-1 equips with 4-dof dual arms and a 3-dof head with a stereo camera system. In this paper, its concept, design and a developed prototype model and results of fundamental experiments are described.	coat of arms;experiment;humanoid robot;prototype;stereo camera;telerobotics;telexistence;wearable computer;wearable technology	Yuichi Tsumaki;Nobuyuki Inoue;Yutaka Satoh;Riichiro Tadakuma	2011	2011 IEEE International Conference on Robotics and Biomimetics	10.1109/ROBIO.2011.6181289	control engineering;computer vision;simulation;computer science;engineering;humanoid robot;artificial intelligence	Robotics	62.35400941165544	-30.94703668501474	3892
d4107fbc5ca47b2b73fb6b92b2d31f34af19229a	performance analysis and validation of a paracatadioptric omnistereo system	map building;stereo configuration;map building application;error analysis;omnidirectional camera;vectors;image representation;vector representation;stereo image processing;performance analysis;paracatadioptric omnistereo system;vectors error analysis image representation stereo image processing;autonomous surface vehicle;paracatadioptric omnidirectional camera;performance analysis cameras uncertainty layout navigation remotely operated vehicles mobile robots humans estimation error assembly;map building application performance analysis paracatadioptric omnistereo system vector representation stereo configuration autonomous surface vehicle paracatadioptric omnidirectional camera	In this paper we present a vector-based 3D localization formula for a paracatadioptric omnistereo system. Based on vector representation, the performance of this stereo system is analyzed numerically, including the maximum detectable range and the uncertainty of 3D localization, with respect to the flexible stereo configuration of the system, positions of scene points, as well as errors in correspondence matching and errors in stereo configuration. The results of performance analysis are used to guide the trajectory of an autonomous surface vehicle (ASV), which is equipped with a paracatadioptric omnidirectional camera, in a map building application.	autonomous robot;numerical analysis;omnidirectional camera;profiling (computer programming)	Xiaojin Gong;Anbumani Subramanian;Christopher L. Wyatt;Daniel J. Stilwell	2007	2007 IEEE 11th International Conference on Computer Vision	10.1109/ICCV.2007.4409201	computer vision;simulation;vector space;mathematics	Vision	53.56494095174361	-37.29919275365232	3899
d825ea7021c46f14b4c40151338813c75e25c424	regional target control of the wave equation	wave equation	Taylor & Francis makes every effort to ensure the accuracy of all the information (the “Content”) contained in the publications on our platform. However, Taylor & Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor & Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content.	francis;numerical analysis;optimal control;primary source;simulation	El Hassan Zerrik;R. Larhrissi	2001	Int. J. Systems Science	10.1080/00207720110035264	mathematical optimization;wave equation;mathematical analysis;calculus;mathematics	Robotics	49.79430362792215	-3.0375496362968657	3903
72030303ea0ac0ffbd8fc4773df4ccfd641bf641	high power led package with vertical structure		A LED package structure with vertical geometry of W5II is proposed for high power optoelectronic semiconductor devices. To provide an efficient and easy method of assembling high-power LEDs to sockets of the fixtures, a vertical design referenced from the typical Edison screw base found in US-based lamp systems is used in the structure. The thermal simulation of the structure, fabrication processes and thermal characteristic are evaluated. Thermal resistance measurements are performed to characterize the thermal performance of W5II, and the optical and electrical characteristics of W5II are also verified. The good heat dissipation of W5II could be utilized to enhance the reliability and thermal fatigue capability of high-power LED packages.		Ming-Te Lin;Shang-Ping Ying;Ming-Yao Lin;Kuang-Yu Tai;Jyh-Chen Chen	2012	Microelectronics Reliability	10.1016/j.microrel.2011.07.064	embedded system;electronic engineering;optoelectronics	EDA	89.39246682155766	-13.362926382052885	3904
ffcd5fb223f9b4f6393114504f671b465e31cfb8	robust radiometric calibration for dynamic scenes in the wild	histograms;cameras histograms robustness heuristic algorithms radiometry calibration optimization;high dynamic range imaging robust radiometric calibration dynamic scenes camera response function crf pixel intensities linear irradiance computational imaging applications feature matching motion estimation intensity mapping functions pixel value mapping pixel correspondences rank minimization algorithm;radiometry calibration image matching image resolution minimisation motion estimation;radiometry;heuristic algorithms;robustness;optimization;calibration;cameras	The camera response function (CRF) that maps linear irradiance to pixel intensities must be known for computational imaging applications that match features in images with different exposures. This function is scene dependent and is difficult to estimate in scenes with significant motion. In this paper, we present a novel algorithm for radiometric calibration from multiple exposure images of a dynamic scene. Our approach is based on two key ideas from the literature: (1) intensity mapping functions which map pixel values in one image to the other without the need for pixel correspondences, and (2) a rank minimization algorithm for radiometric calibration. Although each method has its problems, we show how to combine them in aformulation that leverages their benefits. Our algorithm recovers the CRFs for dynamic scenes better than previous methods, and we show how it can be applied to existing algorithms such as those for high-dynamic range imaging to improve their results.	algorithm;conditional random field;dynamic range;experiment;frequency response;high-dynamic-range imaging;metric;mathematical optimization;pixel;preprocessor;range imaging;synthetic intelligence	Abhishek Badki;Nima Khademi Kalantari;Pradeep Sen	2015	2015 IEEE International Conference on Computational Photography (ICCP)	10.1109/ICCPHOT.2015.7168373	computer vision;geography;optics;remote sensing	Vision	55.15765830102857	-55.096155641716365	3908
4bfc18d893b830a6fa34b50a2364e4124dc75866	feature-distributed sparse regression: a screen-and-clean approach		Most existing approaches to distributed sparse regression assume the data is partitioned by samples. However, for high-dimensional data (D N ), it is more natural to partition the data by features. We propose an algorithm to distributed sparse regression when the data is partitioned by features rather than samples. Our approach allows the user to tailor our general method to various distributed computing platforms by trading-off the total amount of data (in bits) sent over the communication network and the number of rounds of communication. We show that an implementation of our approach is capable of solving `1-regularized `2 regression problems with millions of features in minutes.	algorithm;coefficient;data compression;distributed computing;sparse matrix;telecommunications network	Jiyan Yang;Michael W. Mahoney;Michael A. Saunders;Yuekai Sun	2016			computer science;telecommunications network;machine learning;artificial intelligence;partition (number theory);theoretical computer science	ML	24.569489914808436	-36.13220931849614	3910
4f98567b9f448dcd8bb899275e51c00383a589ec	sparse mimo array forward-looking gpr imaging based on compressed sensing in clutter environment	compressed sensing;image coding;data compression;interference suppression;mimo radar;ground penetrating radar;image reconstruction;vehicle mounted stepped frequency radar sparse mimo array forward looking gpr imaging compressed sensing multiple input multiple output array ground penetrating radar cs data acquisition azimuth clutter suppression preprocessing method trihedral reflector real buried land mine experimental data forward looking ground penetrating virtual aperture radar national university of defense technology basis pursuit declutter;radar imaging;stepped frequency compressed sensing cs ground penetrating radar gpr land mine detection multiple input and multiple output mimo array;radar imaging compressed sensing data acquisition data compression ground penetrating radar image coding image reconstruction interference suppression mimo radar radar clutter;radar clutter;data acquisition;arrays ground penetrating radar clutter image reconstruction mimo transmitters	This paper presents a sparse multiple-input and multiple-output (MIMO) array and sparse frequency ground-penetrating radar (GPR) imaging scheme based on compressed sensing (CS). Since the targets of interest for GPR are usually sparse, the number of the MIMO array elements and frequencies can be reduced using CS theory. Thus, the system complexity and data acquisition time can be reduced accordingly. Considering the serious clutter in forward-looking GPR, we propose two methods for the CS reconstruction in clutter environment. The first one is a clutter suppression preprocessing method, which can effectively suppress the azimuth clutter and short range clutter outside the reconstruction region and significantly improve the reconstruction result. The second one is to determine the regularization parameter for the CS reconstruction in clutter environment. We refer to this reconstruction process as basis pursuit declutter. The proposed imaging scheme can produce pointlike and less cluttered images of sparse targets using fewer array elements and frequencies. Results from simulated data, trihedral reflector, and real buried land mine experimental data are presented to show the validity of the proposed methods. The experimental data are acquired by the vehicle-mounted stepped-frequency forward-looking ground-penetrating virtual aperture radar, which is designed and developed by the National University of Defense Technology.	adaptive grammar;basis pursuit;clutter;compressed sensing;data acquisition;kriging;mimo;matrix regularization;preprocessor;simulation;sparse matrix;zero suppression	Jungang Yang;Tian Jin;Xiaotao Huang;John Thompson;Zhimin Zhou	2014	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2013.2282308	data compression;iterative reconstruction;early-warning radar;man-portable radar;computer vision;continuous-wave radar;radar engineering details;radar lock-on;ground-penetrating radar;fire-control radar;radar horizon;bistatic radar;envelope;pulse-doppler radar;clutter;3d radar;data acquisition;radar imaging;compressed sensing;physics;radar;remote sensing	Visualization	76.35695340982107	-66.47918274591044	3912
47788dfaec8fb6b339a9c94be968ea9adc7c3231	a spatially adaptive gradient-projection algorithm to remove coding artifacts of h.264	human visual system	In this paper, we propose a spatially adaptive gradient-projection algorithm for the H.264 video coding standard to remove coding artifacts using local statistics. A hybrid method combining a new weighted constrained least squares (WCLS) approach and the projection onto convex sets (POCS) approach is introduced, where weighting components are determined on the basis of the human visual system (HVS) and projection set is defined by the difference between adjacent pixels and the quantization index (QI). A new visual function is defined to determine the weighting matrices controlling the degree of global smoothness, and a projection set is used to obtain a solution satisfying local smoothing constraints, so that the coding artifacts such as blocking and ringing artifacts can be simultaneously removed. The experimental results show the capability and efficiency of the proposed algorithm.	algorithm;gradient;h.264/mpeg-4 avc	Kwon Yul Choi;Min-Cheol Hong	2011	IEICE Transactions		quantum hall effect;computer vision;mathematical optimization;computer science;gradient method;weighting;mathematics;convex set;human visual system model;least squares;algorithm;statistics	ML	58.622736536588114	-66.17971338578701	3924
7f4e810cf38199c31a39d1937ba5c30600e1b609	a new lip-reading approach for human computer interaction	human computer interaction	Today, Human-Machine interaction represents a certain potential for autonomy especially of dependant people. Automatic Lip-reading system is one of the different assistive technologies for hearing impaired or elderly people. The need for an automatic lip-reading system is ever increasing. Extraction and reliable analysis of facial movements make up an important part in many multimedia systems such as videoconference, low communication systems, lip-reading systems. We can imagine, for example, a dependent person ordering a machine with an easy lip movement or by a simple visemes (visual phoneme) pronunciation. We present in this paper a new approach for lip localization and feature extraction in a speaker’s face. The extracted visual information is then classified in order to recognize the uttered viseme. We have developed our Automatic Lip Feature Extraction prototype (ALiFE). ALiFE prototype is evaluated with a multiple speakers under natural conditions. Experiments include a group of French visemes by different speakers. Results revealed that our system recognizes 92.50 % of French visemes.	assistive technology;autonomy;coefficient;experiment;feature extraction;human computer;human–computer interaction;internationalization and localization;prototype;spatial variability;speech recognition;text corpus	Salah Werda;Walid Mahdi;Abdelmajid Ben Hamadou	2007				HCI	-2.7656644221779283	-82.5920857546114	3926
b572e480e71641fd7d05dd3d39c26f5583fc228e	using an octree-based rag in hyper-irregular pyramid segmentation of texture volume	spatial dependence;texture features;three dimensional;region adjacency graph	Octree Splitting We extend the irregular pyramid segmentation scheme to three dimensional (3D) digital space and we call it hyper-irregular pyramid segmentation. Based on this segmentation scheme, a 3D texture image is split into partitions recursively based on octree structure. The testure features are calculated based on 3D gray level spatial dependency measurement. The octree is subsequently converted into a 3D region adjacency graph (RAG). Each vertex of the graph consists of a texture feature vector of the corresponding partition and each edge represents the neighborhood relationship between two partitions. The hyper-irregular pyramid process is then applied to the graph and finally a segmentation of the three dimensional image is obtained.	feature vector;grayscale;image segmentation;octree;recursion	Horace Ho-Shing Ip;Stephen Wang-Cheung Lam	1994			image texture;three-dimensional space;computer vision;spatial dependence;pattern recognition;mathematics;geometry;image segmentation;scale-space segmentation	Vision	47.37276542900703	-66.11585348491221	3928
d6127837199a496f1e67f649c21dcb8ec7441e7b	robust visual knowledge transfer via extreme learning machine-based domain adaptation		We address the problem of visual knowledge adaptation by leveraging labeled patterns from source domain and a very limited number of labeled instances in target domain to learn a robust classifier for visual categorization. This paper proposes a new extreme learning machine (ELM)-based cross-domain network learning framework, that is called ELM-based Domain Adaptation (EDA). It allows us to learn a category transformation and an ELM classifier with random projection by minimizing the ℓ2,1-norm of the network output weights and the learning error simultaneously. The unlabeled target data, as useful knowledge, is also integrated as a fidelity term to guarantee the stability during cross-domain learning. It minimizes the matching error between the learned classifier and a base classifier, such that many existing classifiers can be readily incorporated as the base classifiers. The network output weights cannot only be analytically determined, but also transferrable. In addition, a manifold regularization with Laplacian graph is incorporated, such that it is beneficial to semisupervised learning. Extensively, we also propose a model of multiple views, referred as MvEDA. Experiments on benchmark visual datasets for video event recognition and object recognition demonstrate that our EDA methods outperform the existing cross-domain learning methods.	acclimatization;benchmark (computing);categories;categorization;class;computer vision;domain adaptation;graph - visual representation;learning disorders;matching;manifold regularization;outline of object recognition;random projection;semi-supervised learning;source data;statistical classification;tracer;weight	Lei Zhang;David Zhang	2016	IEEE Transactions on Image Processing	10.1109/TIP.2016.2598679	computer vision;knowledge management;machine learning	ML	23.711508120514864	-46.237781789849066	3932
34d85ecb8507184aa7d4ea26a93a08e0e06dbe27	near-minimax recursive density estimation on the binary hypercube	recursive estimation;satisfiability;density estimation;computational complexity;mean square error;polynomial time	This paper describes a recursive estimation procedure for m ultivariate binary densities using orthogonal expansions. For d covariates, there are 2 basis coefficients to estimate, which renders conventional approaches comput ationally prohibitive whend is large. However, for a wide class of densities that satisfy a certain sparsity condition, our estimator runs in probabilistic polyno mial time and adapts to the unknown sparsity of the underlying density in two key way s: (1) it attains near-minimax mean-squared error, and (2) the computationa l c mplexity is lower for sparser densities. Our method also allows for flexible co ntrol of the trade-off between mean-squared error and computational complexity.	coefficient;computational complexity theory;mean squared error;minimax;recursion;rendering (computer graphics);sparse matrix	Maxim Raginsky;Svetlana Lazebnik;Rebecca M Willett;Jorge G. Silva	2008			time complexity;mathematical optimization;combinatorics;density estimation;computer science;mathematics;mean squared error;computational complexity theory;statistics;satisfiability	ML	28.47815912006359	-27.337878555665252	3934
b329b3a56270a453ded2732fb1e3c97b9461b354	opacity peeling for direct volume rendering	light transport;participating media;texture mapping;multi dimensional;transfer function;optical properties;real time implementation;direct volume rendering;volume data	The most important technique to visualize 3D scalar data, as they arise e.g. in medicine from tomographic measurement, is direct volume rendering. A transfer function maps the scalar values to optical properties which are used to solve the integral of light transport in participating media. Many medical data sets, especially MRI data, however, are difficult to visualize due to different tissue types being represented by the same scalar value. The main problem is that interesting structures will be occluded by less important structures because they share the same range of data values. Occlusion, however, is a view-dependent problem and cannot be solved easily by transfer function design. This paper proposes a new method to display different entities inside the volume data in a single rendering pass. The proposed opacity peeling technique reveals structures in the data set that cannot be visualized directly by oneor multi-dimensional transfer functions without explicit segmentation. We also demonstrate real-time implementations using texture mapping and multiple render targets.	autonomous robot;computer scientist;entity;glossary of computer graphics;map;multiple render targets;non-photorealistic rendering;real-time clock;requirement;scientific visualization;texture mapping;transfer function;unbiased rendering;volume rendering	Christof Rezk-Salama;Andreas Kolb	2006	Comput. Graph. Forum	10.1111/j.1467-8659.2006.00979.x	texture mapping;computer vision;simulation;rendering;computer science;geometry;transfer function;computer graphics (images)	Visualization	69.96205145124247	-49.17171256418148	3939
ac20a41a35764ef8404a6e929800c966a518ed1b	inference of moving forms via belief propagation	motion labelling moving form inference moving object tracking bayesian framework belief propagation;bayesian framework;moving object;belief networks;image motion analysis;belief propagation motion analysis labeling motion estimation layout tracking bayesian methods performance analysis shape information analysis;bayes methods;moving object tracking;motion labelling;belief propagation;moving form inference;image motion analysis bayes methods belief networks	In this paper we address the issue of how form and motion can be integrated in order to provide suitable information to attentively track multiple moving objects. Such integration is designed in a Bayesian framework, and a belief propagation technique is exploited to perform coherent form/motion labelling of regions of the observed scene	belief propagation;coherence (physics);software propagation	Giuseppe Boccignone;Angelo Marcelli;Paolo Napoletano	2006	18th International Conference on Pattern Recognition (ICPR'06)	10.1109/ICPR.2006.710	computer vision;computer science;machine learning;pattern recognition;mathematics;belief propagation	Robotics	46.68869799122408	-48.42578420228944	3943
c4d786e23108ac7e7b8fade59d3c88662174b25d	new sample complexity bounds for phylogenetic inference from multiple loci	trees mathematics biology computing evolution biological genetics;phylogenetic inference evolutionary history phylogeny individual genes gene trees phylogenetic analysis molecular sequences data requirement analysis species tree reconstruction method estimation errors;history frequency locked loops silicon data structures boolean functions	We consider the problem of estimating the evolutionary history of a set of species (phylogeny or species tree) from several genes. It has been known however that the evolutionary history of individual genes (gene trees) might be topologically distinct from each other and from the underlying species tree, possibly confounding phylogenetic analysis. A further complication in practice is that one has to estimate gene trees from molecular sequences of finite length. We provide the first full data-requirement analysis of a species tree reconstruction method that takes into account estimation errors at the gene level. Under that criterion, we also devise a novel algorithm that provably improves over all previous methods in a regime of interest.	algorithm;computational phylogenetics;requirements analysis;sample complexity	Gautam Dasarathy;Robert D. Nowak;Sébastien Roch	2014	2014 IEEE International Symposium on Information Theory	10.1109/ISIT.2014.6875191	phylogenetic tree;bioinformatics;computational phylogenetics;tree rearrangement;phylogenetic network;phylogenetics	Theory	1.96014170444127	-51.926724981889826	3945
0fe097a6a35a6b4ab03425c87d63c558f82126c9	deep reward shaping from demonstrations		Deep reinforcement learning is rapidly gaining attention due to recent successes in a variety of problems. The combination of deep learning and reinforcement learning allows for a generic learning process that does not consider specific knowledge of the task. However, learning from scratch becomes more difficult when tasks involve long trajectories with delayed rewards. The chances of finding the rewards using trial and error become much smaller compared to tasks where the agent continuously interacts with the environment. This is the case in many real life applications which poses a limitation to current methods. In this paper we propose a novel method for combining learning from demonstrations and experience to expedite and improve deep reinforcement learning. Demonstrations from a teacher are used to shape a potential reward function by training a deep supervised convolutional neural network. The shaped function is added to the reward function used in deep-Q-learning (DQN) to perform off-policy training through trial and error. The proposed method is demonstrated on navigation tasks that are learned from raw pixels without utilizing any knowledge of the problem. Navigation tasks represent a typical AI problem that is relevant to many real applications and where only delayed rewards (usually terminal) are available to the agent. The results show that using the proposed shaped rewards significantly improves the performance of the agent over standard DQN. This improvement is more pronounced the sparser the rewards are.		Ahmed Hussein;Eyad Elyan;Mohamed Medhat Gaber;Chrisina Jayne	2017	2017 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2017.7965896	q-learning;machine learning;pattern recognition;convolutional neural network;intelligent agent;reinforcement learning;artificial neural network;scratch;deep learning;computer science;artificial intelligence;trial and error	AI	19.96056229512997	-20.649436204887447	3947
d438c946aba7004459fe9958761eca3ceabfd084	leveraging pre-trained 3d object detection models for fast ground truth generation		Training 3D object detectors for autonomous driving has been limited to small datasets due to the effort required to generate annotations. Reducing both task complexity and the amount of task switching done by annotators is key to reducing the effort and time required to generate 3D bounding box annotations. This paper introduces a novel ground truth generation method that combines human supervision with pre-trained neural networks to generate per-instance 3D point cloud segmentation, 3D bounding boxes, and class annotations. The annotators provide object anchor clicks which behave as a seed to generate instance segmentation results in 3D. The points belonging to each instance are then used to regress object centroids, bounding box dimensions, and object orientation. Our proposed annotation scheme requires 30x lower human annotation time. We use the KITTI 3D object detection dataset [1] to evaluate the efficiency and the quality of our annotation scheme. We also test the the proposed scheme on previously unseen data from the Autonomoose self-driving vehicle to demonstrate generalization capabilities of the network.		Jungwook Lee;Sean Walsh;Ali Harakeh;Steven Lake Waslander	2018	2018 21st International Conference on Intelligent Transportation Systems (ITSC)	10.1109/ITSC.2018.8569793	artificial neural network;point cloud;artificial intelligence;mathematics;machine learning;ground truth;object detection;task switching;annotation;minimum bounding box;object-orientation	Robotics	27.93072647952574	-49.985458174474005	3952
29d36ec053bee16d292d7ceafdf2bea5971542de	saliency-guided perceptual grouping using motion cues in region-based artificial visual attention		Region-based artificial attention constitutes a framework for bio-inspired attentional processes on an intermediate abstraction level for the use in computer vision and mobile robotics. Segmentation algorithms produce regions of coherently colored pixels. These serve as proto-objects on which the attentional processes determine image portions of relevance. A single region—which not necessarily represents a full object—constitutes the focus of attention. For many post-attentional tasks, however, such as identifying or tracking objects, single segments are not sufficient. Here, we present a saliency-guided approach that groups regions that potentially belong to the same object based on proximity and similarity of motion. We compare our results to object selection by thresholding saliency maps and a further attention-guided strategy.	abstraction layer;algorithm;british informatics olympiad;computer vision;koch snowflake;map;mathematical optimization;microsoft outlook for mac;mobile robot;object detection;object-based language;pixel;relevance;robotics;thresholding (image processing)	Jan Tünnermann;Dieter Enns;Bärbel Mertsching	2013	CoRR		computer vision;machine learning	Robotics	30.174696778800193	-56.28006110863806	3954
caa42bfb7c2ca51b45f7ab90c3e23851899d9cff	the mixed multi-attribute group decision making method with preference information		The aim of this study was to solve the problems of mixed multiattribute group decision, which have the interval-valued, linguistic and interval-valued intuitionistic fuzzy number. As a solution to this problem, we propose a decision-making method that the decision maker weight was unknown, however, the attribute weight was known as well as the score function with transformation parameters. Firstly, conversion formula was used to make the mixed evaluation information unified transformed to the interval-valued intuitionistic fuzzy number. Furthermore, the decision maker and attribute weight were determined by the interval-valued intuitionistic fuzzy numbers distance and expectation values. Under the instruction of intuitionistic fuzzy weighted averaging operator, decision maker weight and attribute weight, we aggregated the individual decision making matrix and scheme different attribute successively. Additionally, we also made the dynamic scheduling analysis by the score function with transformation parameter. Finally, this method feasibility was testified by the example solving.	fuzzy number;scheduling (computing)	Xiaofei Bian;Haiwei Pan	2017		10.1145/3127404.3127456	fuzzy logic;operator (computer programming);group decision-making;dynamic priority scheduling;matrix (mathematics);expectation value;computer science;machine learning;score;artificial intelligence;fuzzy number	AI	-2.841843474759343	-20.175968917932853	3959
cb0867c0a4feb879b2901c7e067761598e93509d	automation and monitoring of the distribution network in presence of distributed generations by using petri nets		Considering the fact that the main part of electrical energy losses is related to the distribution level; therefore, controlling and monitoring of this part of the network is necessary to reduce energy losses which is made possible through distribution automation. Distribution automation systems need decision making regarding the responsibilities being performed by operators. On the other hand, there is the possibility of incorrect decision making by operators due to the large volume of information and decision making time shortage during automation steps which can be created by incorrect information received from the network and consequently causes the incorrect control command sending to distribution network devices. Also, introducing the Distributed Generations (DGs) in the protection system of distribution network, it is possible for the protective devices to have an incorrect operation which affects the operators’ decisions performed by the automation system. In this paper, a new Petri-Net modeling method is used to make the distribution network monitoring and automation possible and also to help the operator in correct decision making. In addition, the protective devices’ operation is determined more reliably using the attained data from the automation system.	automation;bsd;petri net	Ahmad Ashouri;Abolfazl Jalilvand	2015	Journal of Intelligent and Fuzzy Systems	10.3233/IFS-151602	real-time computing;artificial intelligence	Robotics	12.234873533860563	-13.06430062665326	3961
6d5af8d9b4bfedc96e5b5011413d5b1119978356	data mining analysis of an urban tunnel pressure drop based on cfd data	08 information and computing sciences;080108 neural evolutionary and fuzzy computation	An accurate estimation of pressure drop due to vehicles inside an urban tunnel plays a pivotal role in tunnel ventilation issue. The main aim of the present study is to utilize computational intelligence technique for predicting pressure drop due to cars in traffic congestion in urban tunnels. A supervised feed forward back propagation neural network is utilized to estimate this pressure drop. The performance of the proposed network structure is examined on the dataset achieved from Computational Fluid Dynamic (CFD) simulation. The input data includes 2 variables, tunnel velocity and tunnel length, which are to be imported to the corresponding algorithm in order to predict presure drop. 10-fold Cross validation technique is utilized for three data mining methods, namely: multi-layer perceptron algorithm, support vector machine regression, and linear regression. A comparison is to be made to show the most accurate results. Simulation results illustrate that the Multi-layer perceptron algorithm is able to accurately estimate the pressure drop.	data mining	Esmaeel Eftekharian;Seyed Amin Khatami;Abbas Khosravi;Saeid Nahavandi	2015		10.1007/978-3-319-26561-2_16	simulation;computer science;machine learning;data mining;operations research	HCI	9.765829529446712	-20.302945786283527	3969
4ffa3b7eb2e65c0353a4ce3a02596243223d53a3	fine-mapping of dna damage and repair in specific genomic segments	dna;genes;immunoglobuline;recepteur membranaire;low density lipoprotein receptor;membrane receptor;immunoglobulin;ultraviolet rays;gene igvdj;reaction chaine polymerase;oxides;inmunoglobulina;genes ras;irradiacion uv;hombre;mutagenese;immunoglobulins;palatine tonsil;genetic mapping;genes immunoglobulin;lesion;ultraviolet irradiation;mutacion;joule unit of energy;polymerase chain reaction;region variable;time factors;cells cultured;lymphocyte b;variable region;human;linfocito b;carte genetique;reparation;receptors ldl;proto oncogenes;regression analysis;immunoglobulin variable region;humans;mapa genetico;dna repair;molecular sequence data;b lymphocyte;base sequence;reparacion;irradiation uv;kinetics;b lymphocytes;mutagenesis;in vivo;dna damage;amplification;mutation;immunoglobulin heavy chains;repair;receptor membrana;homme	The susceptibility of various genomic regions to DNA damage and repair is heterogeneous. While this can be related to factors such as primary sequence, physical conformation, and functional status, the exact mechanisms involved remain unclear. To more precisely define the key features of a genomic region targeted for these processes, a useful tool would be a method for fine-mapping gene-specific DNA damage and repair in vivo. Here, a polymerase chain reaction-based assay is described for measuring DNA damage and repair in small (less than 500 bp) genomic segments of three transcriptionally active but functionally distinct loci (rearranged immunoglobulin heavy chain variable region [Ig VDJ], low-density lipoprotein receptor gene, and N-ras proto-oncogene) in human tonsillar B lymphocytes. Analysis of ultraviolet (254 nm)-induced DNA damage revealed single-hit kinetics and a similar level of sensitivity (D50% approximately 6000 joule/m2) in all three regions, indicating that a single photoproduct was sufficient to fully block PCR amplification. A similar time period per unit length was required for repair of this DNA damage (average t1/2 per fragment length = 23.5 seconds per bp). DNA damage and repair was also detectable with the base adducting agent, 4-nitroquinoline-1-oxide. However, in this case IgVDJ differed from segments within the other two loci by its relative inaccessibility to alkylation. This assay thus permits high-resolution mapping of DNA damage and repair activity.	4-nitroquinoline-1-oxide;dna damage;genetic heterogeneity;image resolution;immunoglobulin heavy chains;immunoglobulins;joule;kinetics internet protocol;ldlr protein, human;license;lipoprotein receptor;lipoproteins;oncogenes;polymerase chain reaction;proto-oncogenes;slc3a2 gene;thrombocytopenia;video-in video-out;virtual dj	Herman L. Govan;Yadira Valles-Ayoub;Jonathan Braun	1990	Nucleic acids research	10.1093/nar/18.13.3823	biology;biochemistry;molecular biology;antibody;genetics	Comp.	5.159560673289141	-63.67235112677902	3972
3bff180b4224912cc7045faa88d3a74a71161bde	ultra-fast 3d filtered backprojection on commodity graphics hardware	computer graphics;computerised tomography;image reconstruction;medical image processing;surgery;commodity graphics hardware;cone-beam scanner;fast reconstruction algorithm;image-guided surgical interventions;interactive scanning;real-time diagnosis;time-varying data;ultra-fast 3d filtered backprojection;hardware;graphics hardware;arithmetic;application software;real time;workstations;acceleration;graphics	Recent efforts in cone-beam scanner technology have focused on developing interactive scanning capabilities, for example, to enable image-guided surgical interventions or real-time diagnosis with time-varying data. However, apart from a fast scanner these applications also require a fast reconstruction algorithm to match. The filtered backprojection algorithm devised by Feldkamp, Davis, and Kress is the most widely used algorithm for 3D reconstruction from cone-beam projections, and it is the algorithm with the lowest complexity. Yet, pure software implementations have difficulties to process the data at the speeds required for real-time scanning. One option is to utilize expensive and rare custom boards for this purpose. We describe an alternative solution, which is inexpensive, uses readily available PC graphics hardware boards, and provides the desired performance at the quality required.	3d reconstruction;algorithm;graphics hardware;real-time clock;real-time web	Klaus Mueller;Fang Xu	2004	2004 2nd IEEE International Symposium on Biomedical Imaging: Nano to Macro (IEEE Cat No. 04EX821)		iterative reconstruction;acceleration;computer vision;application software;simulation;workstation;computer science;graphics;computer graphics;graphics hardware;computer graphics (images)	Visualization	41.94011291942886	-88.12195792960843	3973
f99ae7d002872fd2f4505a41e364134410570e6d	an improved multi-objective particle swarm optimization algorithm based on adaptive local search		This paper demonstrates a novel local search approach based on an adaptive time variant search space index improving the exploration ability as well as diversity in multi-objective Particle Swarm Optimization. The novel strategy searches for the neighbourhood particles in a range which gradually increases with iterations. Particles get updated according to the rules of basic PSO and the non-dominated particles are subjected to Evolutionary update archiving. To improve the diversity, the archive is truncated based on crowding distance parameter. The leader is chosen among the candidates in the archive based on another local search. From the simulation results, it is clear that the implementation of the new scheme results in better convergence and diversity as compared to NSGA-II, CMPSO, and SMPSO reported in literature. Finally, the proposed algorithm is used to solve machine design based engineering problems from literature and compared with existing algorithms.	algorithm;local search (optimization);particle swarm optimization	Swapnil Prakash Kapse;Shankar Krishnapillai	2017	IJAEC	10.4018/IJAEC.2017040101	multi-swarm optimization;meta-optimization;tabu search;derivative-free optimization;local search;hill climbing;particle swarm optimization;metaheuristic;guided local search	Robotics	26.874697449794574	-3.9419433061225075	3977
f18731d30f2dd01a368f81ed950d298df0cd7f6c	multi-agent reinforcement learning based on multi-channel art networks		3-channel fuzzy ART networkFALCON is a good solution to combine reinforcement learning with state segmentation, where it learns the relations among percepts, actions and rewards. FALCON, however, does not have a mechanism to predict behavior of other agents, and thus it is difficult for FALCON to learn the optimal agent’s behavior in a multi-agent circumstance. In this paper, an action prediction module based on 2-channel fuzzy ART network is proposed, and FALCON is modified in order to be able to register the output of the action prediction module. The modified FALCON is called FALCONAP. Moreover,FALCONERthat estimates the expected value of rewards and selects an action according to the value is proposed. Through experiments in which FALCON, FALCONAP and FALCONER are applied to a card game Hearts, it is shown that FALCONER receives less penalty points and learns better rules.	experiment;logic programming;multi-agent system;reinforcement learning	Hitomi Morishita;Hiroaki Ueda;Kenichi Takahashi	2013			error-driven learning;learning classifier system	AI	18.13549973601676	-20.202144012947564	3981
33d01e49de52c9b4f1d060beb84eda24c8213970	image analysis using space-filling curves and 1d wavelet bases	wavelet analysis;analisis imagen;scale function;mathematical morphology;image processing;data compression;edge detection;analisis forma;procesamiento imagen;1d wavelet analysis;peano space filling;traitement image;deteccion contorno;hilbert space;detection contour;pattern detection;image analysis;large patterns detection;pattern analysis;space filling curve;compresion dato;analyse image;analyse forme;compression donnee	Abstract   Images are transformed into signals using a Peano-Hilbert space-filling curve: this continuous mapping captures local informations when Hilbert's curve is wandering inside the image. Then the obtained signal is analysed using 1 D (one-dimensional) wavelet orthogonal bases. The spaces of details of smaller and smaller scales are considered. They are built from a scaling function. An image can be computed by selecting wavelet coefficients corresponding to one or several scales and using the inverse Peano-Hilbert procedure. Due to the good localization of wavelet analysis local and global patterns can be detected and global ones can be reinforced. So the edge detection of global edges is prepared and easier with mathematical morphology.	image analysis;space-filling curve;wavelet	Claude-Henri Lamarque;F. Robert	1996	Pattern Recognition	10.1016/0031-3203(95)00157-3	data compression;wavelet;computer vision;image analysis;mathematical morphology;edge detection;topology;image processing;computer science;cascade algorithm;mathematics;geometry;wavelet packet decomposition;stationary wavelet transform;wavelet transform;hilbert space	Vision	49.764990032452154	-62.60115947768361	3989
ad8f2b3c61a705c9cdcb0969c3098258980d7120	shadow algorithms for computer graphics	computer graphics;shadows;computer graphic;hidden surface removal;shadow algorithm;raster displays;shading	Shadows are advocated for improved comprehension and enhanced realism in computer-synthesized images. A classification of shadow algorithms delineates three approaches: shadow computation during scanout; division of object surfaces into shadowed and unshadowed areas prior to removal of hidden surfaces; and inclusion of shadow volumes in the object data. The classes are related to existing shadow algorithms and implementations within each class are sketched. A brief comparison of the three approaches suggests that the last approach has the most appealing characteristics.	algorithm;computation;computer graphics;shadow volume	Franklin C. Crow	1977		10.1145/563858.563901	drop shadow;computer vision;shadow;shading;simulation;hidden surface determination;computer science;shadow mapping;shadow volume;computer graphics;computer graphics (images)	Graphics	64.9528638288341	-50.40853038970647	3991
47b3e1795ec3da0c43f26d78f16706776b7179ea	transport infrastructure surveillance and monitoring by electromagnetic sensing: the istimes project	vagor och signaler;electromagnetic phenomena;electromagnetic sensing techniques;elektroteknik och elektronik;transport infrastructure non invasive monitoring;spectrum analysis;waves and signals;architecture as topic;signalbehandling;models biological;engineering and technology;electrodes;teknik och teknologier;optical fibers;geographic information systems;signal processing;transportation;italy;humans;remote sensing technology;wireless technology;system architecture;systems integration;radar;environmental monitoring	"""The ISTIMES project, funded by the European Commission in the frame of a joint Call """"ICT and Security"""" of the Seventh Framework Programme, is presented and preliminary research results are discussed. The main objective of the ISTIMES project is to design, assess and promote an Information and Communication Technologies (ICT)-based system, exploiting distributed and local sensors, for non-destructive electromagnetic monitoring of critical transport infrastructures. The integration of electromagnetic technologies with new ICT information and telecommunications systems enables remotely controlled monitoring and surveillance and real time data imaging of the critical transport infrastructures. The project exploits different non-invasive imaging technologies based on electromagnetic sensing (optic fiber sensors, Synthetic Aperture Radar satellite platform based, hyperspectral spectroscopy, Infrared thermography, Ground Penetrating Radar-, low-frequency geophysical techniques, Ground based systems for displacement monitoring). In this paper, we show the preliminary results arising from the GPR and infrared thermographic measurements carried out on the Musmeci bridge in Potenza, located in a highly seismic area of the Apennine chain (Southern Italy) and representing one of the test beds of the project."""	beds;displacement mapping;embedded system;embedding;enumerator polynomial;fiber optic technology;gram-positive rods (bacteria);imaging technology;kriging;multi-model database;numerous;optical fiber;psychologic displacement;remote control;spectroscopy, near-infrared;sensor (device);thermography	Monica Proto;Massimo Bavusi;Romeo Bernini;Lorenzo Bigagli;Marie Bost;Frédrèric Bourquin;Louis-Marie Cottineau;Vincenzo Cuomo;Pietro Della Vecchia;Mauro Dolce;Jean Dumoulin;Lev Eppelbaum;Gianfranco Fornaro;Mats Gustafsson;Johannes Hugenschmidt;Peter Kaspersen;Hyunwook Kim;Vincenzo Lapenna	2010		10.3390/s101210620	embedded system;transport;spectrum analyzer;simulation;telecommunications;engineering;electrical engineering;electrode;optical fiber;signal processing;environmental monitoring;radar;remote sensing;system integration	EDA	86.71227244842304	-45.48042292037568	3993
fac8370632f8184afecc869c43e59d3c70e168d5	cokriging for cross-attribute fusion in sensor networks	coregionalisation model;submodularity;cokriging;wireless sensor networks;cross attribute fusion	Optimisation of the number of required measurement points and their location is an important research topic in sensor networks. Finding the optimal positions increases spatial coverage and reduces deployment costs. This paper presents an approach for the case that two attributes have to be measured with a different number of available sensors. The proposed Cokriging method performs cross-attribute fusion in sensor networks by being based on the analysis of multi-variable spatial correlations. To the best of our knowledge, this scientific work is the first one considering kriging and cokriging interpolations as IF methods. The single-variable ordinary kriging and bi-variable methods were applied to experimental data. The combination of humidity and temperature data in a refrigerated container is used as exemplary case, humidity measurements are considered to be the expensive attribute to measure. The average estimation error for intermediate points was estimated as a function of the number of humidity sensors. When variability is high, data fusion using the bivariable method produced results as accurate as the single-variable one, without the necessity of deploying a large number of humidity measuring points, by complementing the estimation with temperature measurements. © 2014 Elsevier Science. All rights reserved	heart rate variability;kriging;mathematical optimization;mean squared error;scientific literature;sensor;software deployment	Javier Palafox-Albarrán;Reiner Jedermann;Bonghee Hong;Walter Lang	2015	Information Fusion	10.1016/j.inffus.2014.09.007	econometrics;wireless sensor network;computer science;data mining	Robotics	47.49675627126359	2.963209383416193	3995
f6111e3a280ca2d0005082726e9e55ba619cff12	a wavelet multiresolution technique for polarimetric texture analysis and segmentation of sar images	forestry;image segmentation;wavelet analysis image resolution image analysis image texture analysis image segmentation polarization feature extraction wavelet coefficients backscatter vegetation mapping;image texture;wavelet transforms;l band dlr e sar sensor wavelet multiresolution technique polarimetric texture analysis segmentation polarimetric sar synthesized image multiscale wavelet decomposition wavelet frame coefficient texturally optimal polarization state radar backscatter intensity mda transform multiple discriminant analysis monte carlo simulation fragmented forest simulation real sar data;radar polarimetry;radar imaging;forestry synthetic aperture radar radar polarimetry image texture monte carlo methods wavelet transforms radar imaging image segmentation geophysical techniques;monte carlo methods;geophysical techniques;synthetic aperture radar	A technique is presented for multiscale texture analysis and segmentation of polarimetric SAR images. Textural features are extracted using a multiscale wavelet decomposition based on a wavelet frame. The feature vector is composed of local variance estimates of the smooth image and of the wavelet coefficients. The decomposition is performed at two scales and using images derived by polarimetric power synthesis at a set of polarization configurations. This set is chosen based on a priori-knowledge of the texturally optimal polarization states. Alternatively a complete and nonredundant representation of the full polarimetric information consisting of nine backscatter intensities is used. Feature reduction is achieved by an approximate solution of the Multiple Discriminant Analysis (MDA) transform. A set of controlled experiments, based on Monte Carlo simulations, is set up to assess the performance of the technique with respect to texture segmentation problems. One case is reported concerning the simulation of a fragmented forest, where two vegetation classes with different structural characteristics are mixed. Finally, as an example of the application of the technique to real SAR data, texture segmentation of a high resolution image acquired by the DLR E-SAR sensor at L-band is illustrated.	approximation algorithm;coefficient;dynamic language runtime;experiment;feature vector;framing (world wide web);image resolution;information extraction;l band;linear discriminant analysis;monte carlo method;multiple discriminant analysis;polarimetry;polarization (waves);random forest;simulation;wavelet	Gianfranco De Grandi;Dirk H. Hoekman;Jong-Sen Lee;Dale L. Schuler;Thomas L. Ainsworth	2004	IGARSS 2004. 2004 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2004.1369129	image texture;computer vision;synthetic aperture radar;computer science;image segmentation;optics;scale-space segmentation;radar imaging;remote sensing;monte carlo method;wavelet transform	Robotics	72.54882666036903	-62.04191965141076	3997
5dbac9fc2d33f32ea5e7b185dbd5ccb7fd71d14d	on the convergence rates of some adaptive markov chain monte carlo algorithms	mixing time;total variation distance	This paper studies the mixing time of certain adaptive Markov Chain Monte Carlo algorithms. Under some regularity conditions, we show that the convergence rate of Importance Resampling MCMC algorithm, measured in terms of the total variation distance is O(n−1). By means of an example, we establish that in general, this algorithm does not converge at a faster rate. We also study the interacting tempering algorithm, a simplified version of Equi-Energy sampler, and establish that its mixing time is of order O(n−1/2).	algorithm;converge;interaction;markov chain monte carlo;monte carlo method;rate of convergence;sampling (signal processing)	Yves F. Atchadé;Yizao Wang	2015	J. Applied Probability	10.1017/S0021900200113452	econometrics;markov chain;mathematical optimization;dynamic monte carlo method;hybrid monte carlo;markov chain monte carlo;monte carlo molecular modeling;mathematics;kinetic monte carlo;markov chain mixing time;markov model;parallel tempering;total variation;statistics;monte carlo method	ML	32.994815928801415	-16.02101454100908	4003
3cee815fc422fb6dbe4e9a4e3acd2c4b7b782233	identifying patterns of correspondence between modeled flow directions and field evidence: an automated flow direction analysis	scandinavie;computadora;tratamiento datos;computers;europa;scandinavia;sweden;time dependent;modele numerique;systeme information geographique;ordinateur;direccion;data processing;glace;traitement donnee;lineation;ice sheets;methode nouvelle;ice sheet model;suede;suecia;residuals;statistical analysis;gis;numerical model;direction;geographic information systems;lineacion;analyse statistique;new methods;inlandsis;automated flow direction analysis;analyse automatisee;europe;numerical models;natural phenomena;metodo nuevo;hielo;flow pattern;ice;masa de hielo;flow regime;automated analysis	Comparison of numerical model output that predicts spatial flow patterns against field observations is a necessity within several areas of the geosciences. However in many cases these comparisons are qualitative or relative in nature. Automated flow direction analysis (AFDA) is a new method designed to provide a systematic comparison between modeled flow patterns and field observations, with particular focus on two-dimensional linear features representing flow directions of natural phenomena. By subtracting vector output of time-dependent models from field-observed directions, the resultant mean residual and variance of the offset between these data sets can be used to identify patterns of correspondence and variation between model-predicted directions and field observations. The technique is demonstrated by comparison of modeled basal ice flow directions of the Fennoscandian Ice Sheet with observed lineations mapped in Northern Sweden. In this example, the analysis provides an effective means to quantitatively validate the modeled basal thermal and flow regime with observed glacial lineations. The technique has potential applications in a wide range of flow vector direction comparisons in the geosciences, for example lava flow, landslides, aeolian and fluvial processes. r 2006 Elsevier Ltd. All rights reserved.	basal (phylogenetics);mathematical model;numerical analysis;resultant	Yingkui Li;Jacob Napieralski;Jon Harbor;Alun Hubbard	2007	Computers & Geosciences	10.1016/j.cageo.2006.06.016	lineation;data processing;geology;ice sheet;operations research;statistics	SE	88.85715681380219	-57.13053993640096	4006
8a2fb54abe380201ad0ee17e9b5b0d72f3255f7e	ranking classification algorithms and combinations of methods	classification algorithm		algorithm;statistical classification	Pavel Brazdil	2001			computer science;pattern recognition;ranking svm	ML	9.723214568924238	-40.0782180413762	4012
8c890c726cb951eed48d4523e233a99729d63dee	improved relay auto-tuning method for unstable tito systems	transfer functions;tuning;limit cycles;decentralized control;relays;mimo;harmonic analysis	In this paper, the relay auto-tuning method is improved by modifying the Zeigler-Nichols method to obtain the improved value of the controller gains of a TITO unstable system. The effect of the higher order harmonic terms are also taken into consideration while designing the controllers. The proposed method is simulated on two 2×2 unstable transfer function matrices. The implementation of the method does not need the knowledge of the transfer function matrix. The performance of the proposed method is compared with the conventional method of relay tuning. The present method gives improved response and reduced interactions. The present method is also robust.	control theory;interaction;nichols plot;performance tuning;relay;robustness (computer science);self-tuning;transfer function	Saxena Nikita;M. Chidambaram	2016	2016 IEEE 21st International Conference on Emerging Technologies and Factory Automation (ETFA)	10.1109/ETFA.2016.7733714	control engineering;electronic engineering;decentralised system;engineering;artificial intelligence;harmonic analysis;control theory;transfer function;statistics;mimo	Robotics	67.23969806232031	-7.217801911416049	4013
48ac4f16578ba5d3e30126c8025fb5a5cffd0728	spatiotemporal coupling with the 3d+t motion laplacian	spatial relationship;motion editing;character animation;motion retargeting	Motion editing requires the preservation of spatial and temporal information of the motion. During editing, this information should be preserved at best. We propose a new representation of the motion based on the Laplacian expression of a 3D+t graph: the set of connected graphs given by the skeleton over time. Through this Laplacian representation of the motion, we propose an application which allows an easy and interactive editing, correction or retargeting of a motion. The new created motion is the result of the combination of two minimizations, linear and non-linear: the first penalizes the difference of energy between the Laplacian coordinates from an animation to the desired one. The other one preserves the length of segments. Using several examples, we demonstrate the benefits of our method and in particularly the preservation of the spatiotemporal properties of the motion in an interactive context.	loose coupling;nonlinear system;retargeting	Thibaut Le Naour;Nicolas Courty;Sylvie Gibet	2013	Journal of Visualization and Computer Animation	10.1002/cav.1518	spatial relation;character animation;computer vision;computer science;motion estimation;multimedia;motion field;computer graphics (images)	Graphics	66.24986880351791	-46.03590945416456	4016
09bc88c10c3d10a1a3488a6c64fefabbe86dc019	compensation of periodic disturbances in continuous processing plants by means of a neural controller	neural controller;periodic disturbances;self-learning controller;compensation of periodic disturbances;continuous processing plants.;adaptive control;continuous processing plants;neural networks;neural network;oscillations;radial basis function	In this paper, a new approach for the compensation of unknown periodic disturbances by means of a neural network is presented. The neural controller supports the conventional controller by suppressing periodic disturbances. This is done by online learning in order to adapt to different operating conditions and to time varying unknown disturbances. The neural network learns an optimal compensation signal, such that the effect of the disturbance becomes zero in the considered output signal. With this method, there is no need to redesign existing control loops. Exemplified by the compensation of eccentricities of the unwinder of a continuous processing plant, the neural controller is explained and simulation results are shown. An extension to the basic method is to consider an additional input dimension in the neural network, which represents the current operating point. The information about the optimal compensation signal of a specific operating point is stored in the network weights of a multidimensional Radial Basis Function Network. For pre-trained operation ranges, this guarantees an optimal compensation result even if the operating point changes. The main benefit of the presented method in industrial applications is the capability to augment the production speed and to improve the product quality, by reducing tension force oscillations caused by eccentricities of rollers or unwinders.	artificial neural network;control flow;control system;event loop;game controller;interpolation;online machine learning;operating point;radial basis function network;rejection sampling;simulation;simulation interoperability standards organization	Martin Rau;Dierk Schröder	2002			control engineering;engineering;machine learning;control theory	ML	65.48455747560742	-6.902105884808252	4018
920ccc9e6bd2789a55f0e27cc9e82725b785ad2a	solving workshop layout by hybridizing invasive weed optimization with simulated annealing	harmonic coefficient;quadratic programming;evolutionary computation;logistics system;effective algorithms;metropolis criterion;sa;会议论文;layout;quadratic assignment problems;simulated annealing;facilities layout;np hard problem;computational results;harmonic coefficient workshop layout problem invasive weed optimization simulated annealing quadratic assignment problem qap np hard problem logistics system sa offspring generation rule random keys encoding;computational complexity;workshop layout problem;random keys encoding;quadratic assignment problem;simulated annealing computational complexity evolutionary computation facilities layout quadratic programming;invasive weed optimization;education simulated annealing encoding europe layout;europe;harmonic coefficients;encoding;offspring generation rule;qap;metropolis criterion invasive weed optimization simulated annealing qap	We herein model workshop layout problem as quadratic assignment problem (QAP), which is an important NP-hard problem in logistics system. Moreover, we proposed an effective algorithm hybridizing invasive weed optimization (IWO for short) with the simulated annealing (SA) for solving this problem. Our basic idea is to employ IWO for providing diversity to explore solution, and use metropolis criterion of SA to provide a better direction. In our algorithm, we employed an offspring generation rule with disturbance, and used random-keys encoding to produce new solution for solving QAP. We also designed a harmonic coefficient to improve the fluctuation problem effectively. The computational results from equipment layout problems validated our algorithm.	capability maturity model;coefficient;combinatorial optimization;computation;experiment;logistics;mathematical optimization;metropolis;metropolis–hastings algorithm;np-hardness;quadratic assignment problem;quantum fluctuation;simulated annealing	Yanjun Shi;Luyang Hou;Xiaojun Zheng	2015	2015 IEEE 19th International Conference on Computer Supported Cooperative Work in Design (CSCWD)	10.1109/CSCWD.2015.7231007	layout;mathematical optimization;simulation;simulated annealing;computer science;machine learning;np-hard;computational complexity theory;quadratic programming;encoding;quadratic assignment problem;evolutionary computation	Vision	25.08371004669383	-1.0864242100152608	4022
9ce35827fecf3d3d491d6248b4dddd9fb3d3844c	cnc tool path in terms of b-spline curves	forma libre;concepcion asistida;free surface;interpolador;surface libre;molienda;computer aided design;distance function;field emitter;modele geometrique;bola;broyeur boulet;cnc machine tool;tool path;machine outil;free form;milling cutter;grinding comminution;interpolateur;catodo emision campo;condicion estacionaria;courbure;condition stationnaire;ball;interpolator;surface lisse;b spline curve;pick feed;cathode emission champ;smooth surface;computer digital control;herramienta corte;outil coupe;forme libre;aproximacion esplin;surface plane;forme coquille;scallop height;spline approximation;approximation spline;fraise outil;free form curve interpolator;superficie libre;stationary condition;fresa herramienta;conception assistee;curvatura;end milling;maquina herramienta;bille;curvature;b spline;esplin cubico;spline cubique;broyage;molino bolas;machine tool;superficie lisa;offset surface;cutting tool;plane surface;control numerico computador;b splin;superficie plana;geometrical model;ball mill;cubic spline;commande numerique calculateur;modelo geometrico	We present an accurate and efficient method to generate a CNC tool path for a smooth free-form surface in terms of planar cubic B-spline curves which will be fed into a free-form curve interpolator. We assume the use of a three-axis CNC machine tool with a ball end-mill cutter. We first interpolate break points, which are generated by computing the  offset surface – driving plane  intersection curve reflecting the curvature, by a planar cubic B-spline curve. We then evaluate the maximum scallop height along a scallop curve by computing the stationary points of the distance function between the scallop curve and the design surface. Furthermore, we compute the maximum pick feed such that the maximum scallop height along a scallop curve coincides with the prescribed tolerance. Illustrative examples show the substantial improvements this method achieves over conventional methods where the tool path consists of linear or circular paths.	b-spline;spline (mathematics)	Claire Lartigue;François Thiebaut;Takashi Maekawa	2001	Computer-Aided Design	10.1016/S0010-4485(00)00090-7	b-spline;spline;simulation;metric;engineering;machine tool;computer aided design;mathematics;geometry;ball mill;curvature;ball;free surface;engineering drawing;mechanical engineering	EDA	68.99793153960026	-37.97800867134017	4023
48f8dbef6f163ccc77ec24eb6fd4264a1013e377	palmprint recognition using fastica algorithm and radial basis probabilistic neural network	radial basis probabilistic neural network rbpnn;fastica algorithm;independent component analysis;fixed point;orthogonal least square;recognition rate;palmprint recognition;probabilistic neural network;仿生感知与控制	This paper proposes a novel and successful method for recognizing palmprint based on radial basis probabilistic neural network (RBPNN) proposed by us. The RBPNN is trained by the orthogonal least square (OLS) algorithm and its structure is optimized by the recursive OLS algorithm (ROLSA). The Hong Kong Polytechnic University (PolyU) palmprint database, which is pre-processed by a fast fixed-point algorithm for independent component analysis (FastICA), is exploited to test our approach. The experimental results show that the RBPNN achieves higher recognition rate and better classification efficiency than other usual classifiers. r 2006 Elsevier B.V. All rights reserved.	algorithm;artificial neural network;fastica;fingerprint;fixed-point iteration;independent component analysis;ordinary least squares;probabilistic neural network;radial (radio);recursion	Li Shang;De-shuang Huang;Ji-Xiang Du;Qing Yan	2006	Neurocomputing	10.1016/j.neucom.2005.11.004	independent component analysis;probabilistic neural network;speech recognition;computer science;machine learning;pattern recognition;mathematics;fixed point	AI	22.562817279712597	-39.9403633213008	4024
8375f18920a1182b612b3fab99fc5351df226fa9	on the problem of comparing ordered ordinary fuzzy multisets		In this work we deal with a particular type of hesitant fuzzy set, in the case where membership values can appear multiple times and are ordered. They are called ordered ordinary fuzzy multisets. Some operations between them are introduced by means of an extension principle. In particular, the divergence measures between two of these multisets are defined and we have studied in detail the local family of divergences. Finally, these measures are related to the ones given for ordinary fuzzy sets.		Ángel Riesgo;Pedro Alonso;Irene Díaz;Vladimír Janis;Vladimír Kobza;Susana Montes	2018		10.1007/978-3-319-91476-3_29	fuzzy logic;discrete mathematics;divergence;fuzzy set;mathematics	AI	-0.8068657546878585	-22.496041770224654	4026
6a972faeee1a0b988b9af81a62442f75724ba378	location of exons in dna sequences using digital filters	dna;signal processing band pass filters bioelectric potentials biological techniques biology computing digital filters discrete fourier transforms dna genomics low pass filters molecular biophysics proteins;passband;biology computing;numerical sequence;genomics;complete genome;bioelectric potentials;fourier transform;band pass filters;resonant recognition model rrm;computational techniques;narrowband bandpass digital filters dna exons period 3 property resonant recognition model rrm electron ion interaction potential eiip;exons location;nucleotides;dna sequences;nucleotide location;data mining;digital filter;electron ion interaction potential eiip;genomes;hot spot;electron ion interaction potential;proteins;dna sequences digital filters band pass filters filtering proteins charge carrier processes narrowband passband frequency conversion;short time discrete fourier transform;signal processing;protein hot spot;digital filters;molecular biophysics;narrowband bandpass digital filters;lowpass filter;low pass filters;bandpass filter;period 3 property;lowpass filter dna sequences filtering technique protein hot spot exons location electron ion interaction potential numerical sequence narrowband bandpass digital filter nucleotide location short time discrete fourier transform genomes bandpass filtered signal;biological techniques;dna sequence;narrowband bandpass digital filter;discrete fourier transforms;bandpass filtered signal;narrowband;time discretization;exons;filtering technique;bioinformatics	A filtering technique for the location of hot spots in proteins proposed recently is applied for the location of exons in DNA sequences. The technique involves conversion of a DNA character sequence into a numerical sequence using the electron-ion interaction potential values and then filtering the numerical sequence using a narrowband bandpass digital filter whose passband is centered at the period-3 frequency, i.e., 2π/3. The strength of the bandpass-filtered signal as a function of nucleotide location is then detected using a lowpass filter. A plot of the signal power versus location reveals the presence of exons as distinct peaks. Simulations have shown that the technique leads to more accurate exon locations than another computational technique based on the short-time discrete Fourier transform. Furthermore, the amount of computation required is reduced by as much as 97 percent thereby rendering the technique suitable for the processing of long DNA sequences, even complete genomes.	computation;computer simulation;digital filter;discrete fourier transform;electron;low-pass filter;numerical analysis;proteomics;rendering (computer graphics);short-time fourier transform	Parameswaran Ramachandran;Wu-Sheng Lu;Andreas Antoniou	2009	2009 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2009.5118268	computer vision;dna sequencing;genomics;electronic engineering;digital filter;low-pass filter;telecommunications;computer science;signal processing;band-pass filter;molecular biophysics	Embedded	97.9904285166483	-17.677737292871896	4028
64daee82586e55f42c1913225c362f5569b72de0	performance evaluation of fuzzy cluster validity indexes for optimal data clustering	gustafson kessel clustering;optimal clusters fuzzy c means gustafson kessel clustering validity index;magnetic resonance imaging;fuzzy c means;fuzzy c means clustering method fuzzy cluster validity indexes optimal data clustering;pattern clustering data mining fuzzy set theory;validity index;optimal clusters	In this paper, the problem of detection of optimal fuzzy clusters for data clustering is addressed. A comparative study involving different validity indexes is achieved based on simulated data. To evaluate the performance of each validity concept, two fuzzy clustering methods are considered. Based on simulation results, general deductions and interpretations are made about the efficiency of validity indexes in terms of their ability to achieve optimal clustering with respect to the selected clustering method.	cluster analysis;fuzzy clustering;performance evaluation;simulation	Ouzala Mahd;Habbi Hacene	2013	13th International Conference on Hybrid Intelligent Systems (HIS 2013)	10.1109/HIS.2013.6920501	correlation clustering;k-medians clustering;fuzzy clustering;flame clustering;fuzzy classification;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;mathematics;hierarchical clustering;cluster analysis	Robotics	2.0998015329715436	-39.590976044336266	4029
3ddac1ac00848a3fbd1e728b0f0e411ca602f5da	reliability approach to the tensile strength of unidirectional cfrp composites by monte-carlo simulation	tensile strength;monte carlo simulation		monte carlo method;simulation	K. Goda	1993			materials science;structural engineering;tensile testing;forensic engineering	ML	87.57908585032928	-15.058248393147029	4031
ff0a891b69bfe4646074c8a00e5ee52bf2cc5046	bias estimation for practical distributed multiradar-multitarget tracking systems	radar tracking;estimation vectors gain measurement;fusion registration bias estimation tracklet distributed;tracking filters;local filter gains practical distributed multiradar multitarget tracking systems sensor registration global tracks multisensor multitarget tracking local measurement centralized system covariances opportunity targets sampling time distributed multisensor tracking systems locals tracks state estimates fusion center nonconsecutive sampling instant irregular track transmission rates exact bias estimation algorithm;covariance matrices;sensor fusion;target tracking;tracking filters covariance matrices radar signal processing radar tracking sensor fusion target tracking;radar signal processing	Bias estimation or sensor registration is an essential step in ensuring the accuracy of global tracks in multisensor-multitarget tracking. Most algorithms previously proposed for bias estimation rely on local measurements in centralized systems or tracks in distributed systems, along with additional information like covariances, filter gains or targets of opportunity. In addition, it is generally assumed that such data are made available to the fusion center at every sampling time. In real distributed multisensor tracking systems, where each platform sends locals tracks to the fusion center, only state estimates and, perhaps, their covariances are sent to the fusion center at non-consecutive sampling instants or scans. That is, not all the information required for exact bias estimation, at the fusion center is available in practical distributed tracking systems. In this paper, a new algorithm that is capable of accurate bias estimation even in the absence of filter gain information from local platforms, is proposed for distributed tracking systems with irregular track transmission rates. It is shown that the performance of the new algorithm, which uses the tracklet idea and does not require track transmission at every sampling time or exchange of filter gains, can approach the performance of the exact bias estimation algorithm that requires local filter gains.	algorithm;centralized computing;distributed computing;sampling (signal processing);selection bias;sensor;simulation;small-bias sample space;stacking;tracking system	Ehsan Taghavi;Ratnasingham Tharmarasa;Thia Kirubarajan;Yaakov Bar-Shalom	2013	Proceedings of the 16th International Conference on Information Fusion		computer vision;simulation;tracking system;geography;control theory	Robotics	53.22946916163603	3.0542731324595924	4039
f9c387a8cd16d6258b1c301058867569011d9a9b	distinguishing computer-generated graphics from natural images based on sensor pattern noise and deep learning	computer-generated graphics;convolutional neural network;image forensics;natural images;sensor pattern noise	Computer-generated graphics (CGs) are images generated by computer software. The rapid development of computer graphics technologies has made it easier to generate photorealistic computer graphics, and these graphics are quite difficult to distinguish from natural images (NIs) with the naked eye. In this paper, we propose a method based on sensor pattern noise (SPN) and deep learning to distinguish CGs from NIs. Before being fed into our convolutional neural network (CNN)-based model, these images—CGs and NIs—are clipped into image patches. Furthermore, three high-pass filters (HPFs) are used to remove low-frequency signals, which represent the image content. These filters are also used to reveal the residual signal as well as SPN introduced by the digital camera device. Different from the traditional methods of distinguishing CGs from NIs, the proposed method utilizes a five-layer CNN to classify the input image patches. Based on the classification results of the image patches, we deploy a majority vote scheme to obtain the classification results for the full-size images. The experiments have demonstrated that (1) the proposed method with three HPFs can achieve better results than that with only one HPF or no HPF and that (2) the proposed method with three HPFs achieves 100% accuracy, although the NIs undergo a JPEG compression with a quality factor of 75.		Ye Yao;Weitong Hu;Wei Zhang;Ting Wu;Yunqing Shi	2018		10.3390/s18041296	convolutional neural network;computer vision;residual;electronic engineering;digital camera;deep learning;software;engineering;computer graphics;graphics;jpeg;artificial intelligence	Graphics	55.862943567456185	-62.22860089968959	4045
189ef439bc19593d7f45d261ec64cde5dba1fa9f	optimal, distributed decision-making: the case of no communication	distributed system;optimisation;systeme reparti;combinatorics;optimizacion;combinatoria;informacion incompleta;combinatoire;prise decision;equation polynomiale;polynomial equation;condicion optimalidad;politope;incomplete information;condition optimalite;sistema repartido;ecuacion polinomial;information incomplete;optimization;algoritmo optimo;toma decision;algorithme optimal;optimal algorithm;optimality condition;polytope	We present a combinatorial framework for the study of a natural class of distributed optimization problems that involve decisionmaking by a collection of n distributed agents in the presence of incomplete information; such problems were originally considered in a load balancing setting by Papadimitriou and Yannakakis (Proceedings of the 10th Annual ACM Symposium on Principles of Distributed Computing, pp. 61–64, August 1991). For any given decision protocol and assuming no communication among the agents, our framework allows to obtain a combinatorial inclusion-exclusion expression for the probability that no “overflow” occurs, called the winning probability, in terms of the volume of some simple combinatorial polytope. Within our general framework, we offer a complete resolution to the special cases of oblivious algorithms, for which agents do not “look at” their inputs, and non-oblivious algorithms, for which they do, of the general optimization problem. In either case, we derive optimality conditions in the form of combinatorial polynomial equations. For oblivious algorithms, we explicitly solve these equations to show that the optimal algorithm is simple and uniform, in the sense that agents need not “know” n. Most interestingly, we show that optimal non-oblivious algorithms must be non-uniform: we demonstrate that the optimality conditions admit different solutions for particular, different “small” values of n; however, these solutions improve in terms of the winning probability over the optimal, oblivious algorithm. Our results demonstrate an interesting tradeoff between the amount of knowledge used by agents and uniformity for optimal, distributed decision-making with no communication. G. Ciobanu and G. Păun (Eds.): FCT’99, LNCS 1684, pp. 293–303, 1999. c © Springer-Verlag Berlin Heidelberg 1999 294 S. Georgiades, M. Mavronicolas, and P. Spirakis	algebraic equation;approximation algorithm;circuit complexity;distributed computing;lecture notes in computer science;load balancing (computing);mathematical optimization;no-communication theorem;non-deterministic turing machine;optimization problem;podc;springer (tank)	Stavros Georgiades;Marios Mavronicolas;Paul G. Spirakis	1999		10.1007/3-540-48321-7_24	polytope;mathematical optimization;combinatorics;computer science;artificial intelligence;machine learning;database;mathematics;distributed computing;complete information;algorithm;statistics	Theory	-2.3753126327539147	0.5076114736157629	4052
b78f1236938ea2383e961c004c7553e49e2fb8eb	imaging of electrode movement and conductivity change in electrical impedance tomography	medical image processing electric impedance imaging image reconstruction matrix algebra;matrix algebra;electric impedance imaging;regularization;inverse problem;conductivity tomography image reconstruction biomedical electrodes energy measurement impedance measurement electric variables measurement medical services biomedical equipment motion measurement;image reconstruction;medical image processing;regularization electrical impedance tomography inverse problems;electrical impedance tomography;medical application;image reconstruction matrix regularization electrode movement imaging conductivity change imaging electrical impedance tomography medical application;inverse problems	Electrical impedance tomography (EIT) applies and measures electrical energy on the boundary of a medium to produce an image of its internal impedance distribution. In many medical applications, such as imaging of the chest, the electrodes move during measurements, introducing artefacts into the calculated images. This paper proposes a new algorithm that compensates for electrode movement during difference EIT measurements. The reconstructed image is calculated by regularizing the image reconstruction matrix based on the sensitivity and smoothness of the conductivity and movement data. A comparison of the standard and proposed EIT reconstruction methods is made. Images are reconstructed from 2D and 3D simulations exhibiting conductivity changes and electrode movements. Results show the proposed method yields a good estimation of electrode movement and a significant improvement in conductivity image reconstructions	algorithm;characteristic impedance;electromagnetically induced transparency;iterative reconstruction;nominal impedance;output impedance;simulation;tomography	Camille Gomez-Laberge;Andy Adler	2006	2006 Canadian Conference on Electrical and Computer Engineering	10.1109/CCECE.2006.277846	computer vision;electronic engineering;electrical resistivity tomography;inverse problem;biological engineering	Vision	47.60738650879193	-83.23549402446116	4053
881030b6048cb4a422dedb9855ed7ae859a3634f	shedding light on stereoscopic segmentation	what happens when the assumptions that allow estab-;convergence;minimisation;image reconstruction;computational geometry;local minima;shape from shading;cross correlation;image segmentation;albedo;least square;point to point;texture mapping;cost function	We propose a variational algorithm to jointly estimate the shape, albedo, and light configuration of a Lambertian scene from a collection of images taken from different vantage points. Our work can be thought of as extending classical multi-view stereo to cases where point correspondence cannot be established, or extending classical shape from shading to the case of multiple views with unknown light sources. We show that a first naive formalization of this problem yields algorithms that are numerically unstable, no matter how close the initialization is to the true geometry. We then propose a computational scheme to overcome this problem, resulting in provably stable algorithms that converge to (local) minima of the cost functional. Although we restrict our attention to Lambertian objects with uniform albedo, extensions of our framework are conceivable.	algorithm;calculus of variations;computer vision;control theory;converge;fundamental interaction;ibm notes;lambertian reflectance;mathematical optimization;maxima and minima;numerical analysis;numerical stability;pattern recognition;photometric stereo;shading;stereoscopy;variational principle	Hailin Jin;Daniel Cremers;Anthony J. Yezzi;Stefano Soatto	2004	Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.	10.1109/CVPR.2004.233	iterative reconstruction;texture mapping;computer vision;minimisation;photometric stereo;convergence;computational geometry;point-to-point;computer science;cross-correlation;maxima and minima;mathematics;geometry;image segmentation;least squares;statistics;albedo	Vision	55.205883679459426	-51.88604708624624	4054
032ad04aba4081c85015b84bac1a8f1f8a8aec82	multi-objective parallel test-sheet composition using enhanced particle swarm optimization	particle swarm optimization;computer-assisted testing;genetic algorithm;test-sheet composition;evaluation methods;internet;comparative analysis	For large-scale tests, such as certification tests or entrance examinations, the composed test sheets must meet multiple assessment criteria. Furthermore, to fairly compare the knowledge levels of the persons who receive tests at different times owing to the insufficiency of available examination halls or the occurrence of certain unexpected situations, a set of parallel test sheets needs to be composed, which is almost impossible to accomplish manually. To cope with this problem, an enhanced particle swarm optimization approach is proposed to efficiently compose parallel test sheets from very large item banks, while simultaneously meeting multiple assessment criteria. Moreover, a computer-assisted testing system was developed, and a series of experiments were performed to evaluate the comparative performance of the proposed approach and a genetic algorithm counterpart.	convergence insufficiency;experiment;genetic algorithm;mathematical optimization;particle swarm optimization	Tsu-Feng Ho;Peng-Yeng Yin;Gwo-Jen Hwang;Shyong Jian Shyu;Ya-Nan Yean	2009	Educational Technology & Society			AI	4.144664875055972	-35.18918543310668	4056
0b57a35ad2a86d14c5531b508f6b17e9fb236ef9	estimating contrast agent motion from ultrasound images using an anisotropic diffusion-based optical flow technique	tv l1 approximation;ultrasound image;image recomposition;optical flow;contrast agent;anisotropic diffusion model	The purpose of this study was to suggest a novel optical flow method to estimate the motion patterns of contrast agents from an ultrasound image. We first recomposed the original image with relative structural and textural parts. We embedded an anisotropic diffusion model into a slightly non-convex total variation-L1 approximation scheme to provide more reliable estimates. An intermediate bilateral filter was adopted after each computation step to prevent over-smoothing effects. Ultrasound data were acquired during continuous injection of contrast agent into a tissue mimicking phantom. The results showed that our method provided robust performance for estimating contrast agent flow patterns.		Ju Hwan Lee;Sung Min Kim	2013	Computers in biology and medicine	10.1016/j.compbiomed.2013.09.004	computer vision;simulation;computer science;optical flow;anisotropic diffusion	Vision	44.47409879582505	-82.2272505261924	4057
98e16380f5b718811009fdef6606f22fca671f1d	convergence of some asynchronous nonlinear multisplitting methods	local convergence;block method;nonlinear system	Frommer's nonlinear multisplitting methods for solving nonlinear systems of equations are extended to the asynchronous setting. Block methods are extended to include overlap as well. Several specific cases are discussed. Sufficient conditions to guarantee their local convergence are given. A numerical example is presented illustrating the performance of the new approach.	local convergence;nonlinear system;numerical analysis	Daniel B. Szyld;Jian-Jun Xu	2000	Numerical Algorithms	10.1023/A:1016617309072	local convergence;mathematical optimization;discrete mathematics;nonlinear system;control theory;mathematics;algebra	Theory	71.09299706061617	1.3142534370136973	4062
ea178c254b0374558ae03a9448604828cba68a43	skeleton path based approach for nonrigid 3d shape analysis and retrieval	retrieval;shape analysis;skeleton path;nonrigid 3d shapes;shape retrieval;high efficiency	In this paper, we propose a skeleton path based approach to analyze and retrieve nonrigid 3D shapes. The main idea is to match skeleton graphs by comparing the geodesic paths between skeleton endpoints. Our approach is motivated by the fact that the path feature is stable in the presence of articulation of components. The experimental results demonstrate the performance of our proposed method in terms of robustness to symmetry, discrimination against different graph structures, and high efficiency in nonrigid shape retrieval.	biconnected component;sensor;shape analysis (digital geometry)	Chunyuan Li;A. Ben Hamza	2011		10.1007/978-3-642-21073-0_10	computer vision;computer science;pattern recognition;shape analysis;mathematics;geometry;topological skeleton	AI	41.54521694044301	-57.75761347634278	4067
e9f40d53f9ffaa26cf3c4e7f9f588fdf80bc842d	brain-wave bio potentials based mobile robot control: wavelet-neural network pattern recognition approach	bioelectric potentials;brain wave biopotentials wavelet eog state classification robot pattern recognition cyberlink neural network fuzzy signals signal processing brain computer interface artificial neural networks eog patterns mobile robot control;brain computer interface;neural nets;mobile robot;mobile robots robot control electrooculography electromyography pattern recognition signal processing control systems humans wavelet transforms fuzzy neural networks;wavelet neural network;mobile robots;wavelet transforms;wavelet transform;pattern recognition;wavelet transforms neural nets bioelectric potentials mobile robots pattern recognition;human computer interface;neural network	We show how a recently developed wavelet methodology could be useful for EOG state classification. The work focuses on using EOG and EMG signals to drive a robot and our approach is characterized by our emphasis on wavelet pattern recognition methods rather than on the experimenter's feedback training. In our experimental system, we used a control device activated by human EOG and EMG signals from Cyberlink/sup TM/ to manipulate a robot's direction of movement. The EOG signal was processed by wavelet transform technique to facilitate the detection of EMG artifacts and to produce fuzzy signals for a neural network. We were successful in controlling the robot direction without the use of the experimenter's hands. The current implementation requires great concentration by the experimenter, and fatigue easily affects the desired performance of the system. To overcome these limitations, we propose a new kind of pattern recognition technique. The approach produces adequate classification when applied to EOG continuous data. Besides classification, this approach gives us valuable information about the relationship between EOG and EMG signals. This can be used to effectively detect, separate and remove EOG artifacts from even contaminated EMG signals variants.		Choi Kyoung Ho;Minoru Sasaki	2001		10.1109/ICSMC.2001.969832	mobile robot;computer vision;computer science;artificial intelligence;machine learning;artificial neural network;wavelet transform	Robotics	15.421231433659994	-91.86099529788733	4073
acf6afe7b56c4dbd8e92480711e649a8166b6be9	three dimensional visualisation of microscope imaging to improve understanding of human embryo development		The analysis of processes on a cellular and sub-cellular level plays a crucial role in life sciences. Commonly microscopic assays make use of stains and cellular markers in order to enhance image contrast, but in many cases, cell imaging requires the sample to be undisturbed during the imaging process, making staining, dying and fixing impractical. Non-destructive techniques are especially useful in long term imaging or in the study of sensitive cell types, such as stem cells, embryos or nerve cells. Novel advances in computation, imaging and incubator technology have recently made it possible to prolong the imaging time, reduced the cost of storing data and opened a door to the development of new computer aided analytical tools based on microscopic image data. Here we illustrate how Hoffman Modulation Contrast imaging and Confocal Microscopy can be combined with visual computing and present results from determination of cell number, volume, spatial location and blastomere connectivity, using examples from embryos grown for in vitro fertilisation. We give examples of how knowledge of the imaging technique can be used to further improve the computer analysis and also how visually guided tools may aid in the diagnostic interpretation of image data and improve the result. Finally we discuss how the use of microscopic data as a basis for embryo modelling may help in both research and educational purposes. The aim of this chapter is to give an example of how microscopic imaging can be combined with standard computer vision techniques to aid in the interpretation of microscopic data, and demonstrate how visual computing techniques can make an essential difference in terms of scientific output and understanding.		Anna Leida Mölder;Sarah Drury;Nicholas Costen;Geraldine Hartshorne;Silvester Czanner	2016		10.1007/978-3-319-24523-2_11	computer vision;cell biology;anatomy	Robotics	35.178136098138374	-85.10046195768862	4074
73b209552e101bb36de0b88b435453425dc1fb27	a new fuzzy approach to mammographic breast mass segmentation		A suitable support device for newborns including premature babies comprises a doughnut-shaped structure having a gel-filled GORE-TEX casing of about five to six inches outer diameter with a central aperture of about two to three inches in diameter. The structure provides an annular tube having about a 1 to 2 inch diameter. The tube is preferably circular in cross-section at the rear or head region and preferably flattened to provide a generally oval cross-section at the front or neck region. The case is filled with a cohesive gel mass such as silicone gel or silicone elastomers with sufficiently cross-linked polysiloxane networks to substantially retain a selected shape despite the force of a limited incident weight.	fuzzy logic	Farhang Sahba;Anastasios N. Venetsanopoulos	2009			casing;biomedical engineering;silicone elastomers;aperture;mammographic breast mass;silicone;segmentation;materials science	Vision	85.57112023269745	-19.257567120978635	4075
34ec7d5d844171ed08b161f7f866189e75535fce	bayesian data integration and enrichment analysis for predicting gene function in malaria	hierarchical structure;data integrity;sequence similarity;gene function prediction;machine learning;malaria;plasmodium falciparum;gene set enrichment analysis;bayes classifier;gene function;biological process;gene ontology;heterogeneous data sources	Malaria is one of the world's most deadly diseases and is caused by the parasite  Plasmodium falciparum  . Sixty percent of  P. falciparum  genes have no known function and therefore new methods of gene function prediction are needed. To address this problem, we train a naive Bayes classifier on multiple sources of data and subsequently apply a modified version of the Gene Set Enrichment Analysis Algorithm to predict gene function in  P. falciparum  . To define gene function, we exploit the hierarchical structure of the Gene Ontology, specifically using the Biological Process category. We demonstrate the value of integrating multiple data sources by achieving accurate predictions on genes that cannot be annotated using simple sequence similarity based methods.	gene ontology term enrichment	Philip M. R. Tedder;James R. Bradford;Chris J. Needham;Glenn A. McConkey;Andrew J. Bulpitt;David R. Westhead	2009		10.1007/978-3-642-03073-4_47	bayes classifier;computer science;bioinformatics;machine learning;data integrity;data mining;biological process	Logic	6.950006312465445	-55.46035997156431	4080
8dbe9c4ac830c23639a951217d19ec0c9cedb781	robust surface normal estimation via greedy sparse regression	signal image and speech processing;biometrics;pattern recognition;image processing and computer vision	Photometric Stereo (PST) is a widely used technique of estimating surface normals from an image set. However, it often produces inaccurate results for non-Lambertian surface reflectance. In this study, PST is reformulated as a sparse recovery problem where nonLambertian errors are explicitly identified and corrected. We show that such a problem can be accurately solved via a greedy algorithm called Orthogonal Matching Pursuit (OMP). Furthermore, we introduce a smoothness constraint by expanding the pixel-wise sparse PST into a joint sparse recovery problem where several adjacent pixels are processed simultaneously, and employ a Sequential Compressive Multiple Signal Classification (SeqCS-MUSIC) algorithm based on Simultaneous Orthogonal Matching Pursuit (S-OMP) to reach a robust solution. The performance of OMP and SeqCS-MUSIC is evaluated on synthesized and real-world datasets, and we found that these greedy algorithms are overall more robust to non-Lambertian errors than other state-of-the-art sparse approaches with little loss of efficiency.		Mingjing Zhang;Mark S. Drew	2015	EURASIP J. Image and Video Processing	10.1186/s13640-015-0098-x	mathematical optimization;computer science;archaeology;machine learning;pattern recognition;sparse approximation;biometrics	Vision	32.98472114004233	-46.35963300089448	4081
1506605d42fec7dfd9acedc4eddd7cea95f6bb31	comparing svm ensembles for imbalanced datasets	imbalanced dataset handling;kernel;classification algorithm;parten;supervised learning;support vector machines;roc space measure svm ensemble supervised learning high quality classifier imbalanced training data set machine learning imbalanced dataset handling parten umjc lfm f measure;training;class imbalance svm ensembles classification supervised learning;imbalanced training data set;svm ensemble;class imbalance;classification;support vector machines data handling learning artificial intelligence pattern classification;boosting;lfm;ensembles;machine learning;intelligent systems;pattern classification;f measure;svm;training support vector machines astronomy intelligent systems boosting kernel;astronomy;data handling;learning artificial intelligence;roc space measure;umjc;high quality classifier	Real life datasets often suffer from the problem of class imbalance, which thwarts supervised learning process. In such data sets examples of positive (minority) class are significantly less than those of negative (majority) class leading to severe class imbalance. Constructing high quality classifiers for such imbalanced training data sets is one of the major challenges in machine learning, since traditional classification algorithms tend to get biased towards majority class. In this paper, we compare three ensemble based approaches for handling imbalanced datasets. All the three approaches aim to overcome the under representation of minority class by learning from each of the minority class samples and a subset of majority class samples. The three approaches namely, PARTEN, UMjC and LFM were evaluated on several public datasets. Precision, recall, F-measure, g-mean and ROC space measures were used for comparison. Thread-bare discussion of the results is presented in the paper. Subsequently, we present an astronomy application, where the three methods are compared for prediction of class II, IIn and IIp supernovae.	algorithm;display resolution;f1 score;machine learning;precision and recall;real life;supervised learning;thread pool	Vasudha Bhatnagar;Manju Bhardwaj;Ashish Mahabal	2010	2010 10th International Conference on Intelligent Systems Design and Applications	10.1109/ISDA.2010.5687191	support vector machine;computer science;machine learning;pattern recognition;data mining;supervised learning	AI	14.32152020539307	-41.76613119449772	4088
e1d742c77c8a38c29e460f5cabbd04a1668a2bde	characterization of the structure and reactivity of monocopper-oxygen complexes supported by -diketiminate and anilido-imine ligands	oxygen activation;density functional theory;ch bond activation;density function theory;biomimetic copper oxygen complexes;copper;c h bond activation	Copper-oxygen complexes supported by beta-diketiminate and anilido-imine ligands have recently been reported (Aboelella et al., J Am Chem Soc 2004, 126, 16896; Reynolds et al., Inorg Chem 2005, 44, 6989) as potential biomimetic models for dopamine beta-monooxygenase (DbetaM) and peptidylglycine alpha-hydroxylating monooxygenase (PHM). However, in contrast to the enzymatic systems, these complexes fail to exhibit C--H hydroxylation activity (Reynolds et al., Chem Commun 2005, 2014). Quantum chemical characterization of the 1:1 Cu-O(2) model adducts and related species (Cu(III)-hydroperoxide, Cu(III)-oxo, and Cu(III)-hydroxide) indicates that the 1:1 Cu-O(2) adducts are unreactive toward substrates because of the weakness of the O--H bond that would be formed upon hydrogen-atom abstraction. This in turn is ascribed to the 1:1 adducts having both low reduction potentials and basicities. Cu(III)-oxo species on the other hand, determined to be intermediate between Cu(III)-oxo and Cu(II)-oxyl in character, are shown to be far more reactive toward substrates. Based on these results, design strategies for new DbetaM and PHM biomimetic ligands are proposed: new ligands should be made less electron rich so as to favor end-on dioxygen coordination in the 1:1 Cu-O(2) adducts. Comparison of the relative reactivities of the various copper-oxygen complexes as hydroxylating agents provides support for a Cu(II)-superoxide species as the intermediate responsible for substrate hydroxylation in DbetaM and PHM, and suggests that a Cu(III)-oxo intermediate would be competent in this process as well.	1:1 pixel mapping;2-(methylamino)-5-chlorobenzophenone imine;biomimetics;chorionic gonadotropin, beta subunit, human;copper;dec alpha;dioxygen;dopamine;electron;hydrogen;hydroxylation;imines;ligands;pam gene;quantum;adduct;alpha-mannosidosis;cyclosidimine;negative regulation of reactive oxygen species biosynthetic process	Benjamin F. Gherman;William B. Tolman;Christopher J. Cramer	2006	Journal of computational chemistry	10.1002/jcc.20502	chemistry;organic chemistry;computational chemistry;inorganic chemistry;density functional theory;physics;quantum mechanics	Crypto	97.7110308686253	-6.94687526568445	4089
8cac9489d4095e79710c1d9a900f94684fdafca0	modelling condition monitoring intervals: a hybrid of simulation and analytical approaches	forecasting;reliability;project management;information systems;monitoring interval;stochastic filtering;maintenance;soft or;information technology;simulation;packing;residual time;operations research;location;subjects outside of the university themes;investment;journal;journal of the operational research society;inventory;purchasing;history of or;logistics;condition monitoring;marketing;scheduling;production;communications technology;computer science;delay time;qa275 mathematical statistics;operational research;ha statistics;applications of operational research;or society;jors;management science;infrastructure	This paper reports on a study of modelling condition monitoring intervals. The model is formulated based upon two important concepts. One is the failure delay time concept, which is used to divide the failure process of the item into two periods, namely a normal working period followed by a failure delay time period from a defect being first identified to the actual failure. The other is the conditional residual time concept, which assumes that the residual time also depends on the history condition information obtained. Stochastic filtering theory is used to predict the residual time distribution given all monitored information obtained to date over the failure delay time period. The solution procedure is carried out in two stages. We first propose a static model that is used to determine a fixed condition monitoring interval over the item life. Once the monitored information indicates a possible abnormality of the item concerned, that is the start of the failure delay time, a dynamic approach is employed to determine the next monitoring time at the current monitoring point given that the item is not scheduled for a preventive replacement before that time. This implies that the dynamic model overrides the static model over the failure delay time since more frequent monitoring might be needed to keep the item in close attention before an appropriate replacement is made prior to failure. Two key problems are addressed in the paper. The first is which criterion function we should use in determining the monitoring check interval, and the second is the optimization process for both models, which can be solved neither analytically nor numerically since they depend on two unknown quantities, namely, the available condition information and a decision of the time to replace the item over the failure delay time. For the first problem, we propose five appealingly good criterion functions, and test them using simulations to see which one performs best. The second problem was solved using a hybrid of simulation and analytical solution procedures. We finally present a numerical example to demonstrate the modelling methodology.	simulation	Weiwei Wang	2003	JORS	10.1057/palgrave.jors.2601508	project management;logistics;simulation;inventory;economics;forecasting;investment;computer science;residual time;marketing;operations management;reliability;location;management;operations research;information technology;scheduling;statistics	EDA	7.105495864405916	-1.3487855936085928	4090
d2dcd0070fe4194e6c373103c217f54bff35edac	efunn ensembles construction using a clustering method and a coevolutionary multi-objective genetic algorithm	distributed system;multiobjective programming;programmation multiobjectif;analyse amas;systeme reparti;fuzzy neural nets;execution time;neural network ensemble;reseau neuronal flou;time series;algoritmo genetico;classification;coevolution;dynamical system;multi objective genetic algorithm;systeme dynamique;sistema repartido;cluster analysis;clustering method;serie temporelle;cone;serie temporal;algorithme genetique;temps execution;genetic algorithm;analisis cluster;sistema dinamico;reseau neuronal;evolving fuzzy neural network;tiempo ejecucion;coevolucion;clasificacion;red neuronal;time series prediction;neural network;programacion multiobjetivo;cono	This paper presents the experiments which where made with the Clustering and Coevolution to Construct Neural Network Ensemble (CONE) approach on two classification problems and two time series prediction problems. This approach was used to create a particular type of Evolving Fuzzy Neural Network (EFuNN) ensemble and optimize its parameters using a Coevolutionary Multi-objective Genetic Algorithm. The results of the experiments reinforce some previous results which have shown that the approach is able to generate EFuNN ensembles with accuracy either better or equal to the accuracy of single EFuNNs generated without using coevolution. Besides, the execution time of CONE to generate EFuNN ensembles is lower than the execution time to produce single EFuNNs without coevolution.	artificial neural network;cluster analysis;experiment;genetic algorithm;neural ensemble;run time (program lifecycle phase);software release life cycle;time series	Fernanda L. Minku;Teresa Bernarda Ludermir	2006		10.1007/11893295_97	computer science;artificial intelligence;machine learning;time series;artificial neural network;algorithm;statistics	AI	9.965755355230907	-31.37611365284835	4091
0364e1f3980cc1a20edb210270e236f69ffcffc7	translation initiation sites prediction with mixture gaussian models	forecasting;numerical sequence;agua abajo;global feature;melange loi probabilite;translation initiation site;suite numerique;mixture gaussian model;sucesion numerica;bioinformatique;mixed distribution;mixtures;translation languages;cdna sequence;numerical features;open reading frame;numerical methods;research efforts;global measures;artificial intelligence;mezcla ley probabilidad;aval;downstream;teoria mezcla;bioinformatica;mixture theory;theorie melange;bioinformatics	Translation initiation sites (TIS) are important signals in cDNA se- quences. Many research efforts have tried to predict TIS in cDNA sequences. In this paper, we propose using mixture Gaussian models to predict TIS in cDNA sequences. Some new global measures are used to generate numerical features from cDNA sequences, such as the length of the open reading frame down- stream from ATG, the number of other ATGs upstream and downstream from the current ATGs, etc. With these global features, the proposed method predicts TIS with sensitivity 98% and specificity 92%. The sensitivity is much better than that from other methods. We attribute the improvement in sensitivity to the nature of the global features and the mixture Gaussian models.		Guoliang Li;Tze-Yun Leong;Louxin Zhang	2004		10.1007/978-3-540-30219-3_29	open reading frame;biology;downstream;simulation;speech recognition;forecasting;numerical analysis;computer science;bioinformatics;mathematics;mixture	ML	-3.5091293262737158	-54.442268364241755	4093
f4d81a7e9d4c03a014b112028ad5fd817ca3d7e0	hipaa compliant wireless sensing smartwatch application for the self-management of pediatric asthma	wireless sensor networks biomedical communication cloud computing cryptography diseases medical computing mobile computing mobile radio patient monitoring watches;sensors wireless sensor networks wireless communication real time systems encryption pediatrics;health insurance portability and accountability act self management asthma program pediatric asthma individuals asthma attack risk prediction system cryptography framework restful api representational state transfer application program interfaces environmental wireless sensors physiological wireless sensors mhealth system mobile health system student absenteeism chronic disease wireless sensing smartwatch application hipaa	Asthma is the most prevalent chronic disease among pediatrics, as it is the leading cause of student absenteeism and hospitalization for those under the age of 15. To address the significant need to manage this disease in children, the authors present a mobile health (mHealth) system that determines the risk of an asthma attack through physiological and environmental wireless sensors and representational state transfer application program interfaces (RESTful APIs). The data is sent from wireless sensors to a smartwatch application (app) via a Health Insurance Portability and Accountability Act (HIPAA) compliant cryptography framework, which then sends data to a cloud for real-time analytics. The asthma risk is then sent to the smartwatch and provided to the user via simple graphics for easy interpretation by children. After testing the safety and feasibility of the system in an adult with moderate asthma prior to testing in children, it was found that the analytics model is able to determine the overall asthma risk (high, medium, or low risk) with an accuracy of 80.10±14.13%. Furthermore, the features most important for assessing the risk of an asthma attack were multifaceted, highlighting the importance of continuously monitoring different wireless sensors and RESTful APIs. Future testing this asthma attack risk prediction system in pediatric asthma individuals may lead to an effective self-management asthma program.	application programming interface;chronic disease;compliance behavior;cryptography;end-to-end principle;graphics;health insurance portability and accountability act;hospitalization;institute for operations research and the management sciences;mhealth;mechatronics;mobile health;real-time transcription;representation (action);representational state transfer;self-management (computer science);smartwatch;user interface device component;sensor (device)	Anahita Hosseini;Chris M. Buonocore;Sepideh Hashemzadeh;Hannaneh Hojaiji;Haik Kalantarian;Costas Sideris;Alex A. T. Bui;Christine E. King;Majid Sarrafzadeh	2016	2016 IEEE 13th International Conference on Wearable and Implantable Body Sensor Networks (BSN)	10.1109/BSN.2016.7516231	embedded system;simulation;operating system;computer security	Mobile	6.100116411740002	-89.87370130930022	4096
c4adb510a9f76f50c739c0b96b361682fc65c861	design and implementation of fdm support automatic generation algorithm based on matlab		In the process of rapid prototyping, the melt extrusion technique requires the addition of a support structure to the three-dimensional model. An automatic generation algorithm for 3D model support is designed through Matlab application program in this paper. Two angles were compared in this algorithm to identify the support area, one angle is the included angle between the normal direction of triangular facet in STL (Stereo Lithography) model and the direction of processing, the other is the printer critical angle. The supporting structure generated is maintained at a reasonable range and need to be optimized. The algorithm is validated by the program developed by Matlab, and the results show that the model can be accurately and effectively supported.	3d modeling;algorithm;c++;finite difference method;matlab;normal (geometry);opengl;printer (computing);printing;rapid prototyping;sampling (signal processing)	Tingqiang Song;Yalin Liu;Min Zhang;XingLu Ma	2017	2017 International Conference on Security, Pattern Analysis, and Cybernetics (SPAC)	10.1109/SPAC.2017.8304333	frequency-division multiplexing;normal;matlab;facet (geometry);total internal reflection;solid modeling;algorithm;data modeling;rapid prototyping;computer science	Robotics	73.88668816680581	-42.974026632960836	4098
b51dd5e7ea95f1b504cd1242ddf9aa176722d250	an explicit-solvent hybrid qm and mm approach for predicting pka of small molecules in sampl6 challenge	sampl6;hybrid qm and mm;explicit solvent;pka prediction	In this work we have developed a hybrid QM and MM approach to predict pKa of small drug-like molecules in explicit solvent. The gas phase free energy of deprotonation is calculated using the M06-2X density functional theory level with Pople basis sets. The solvation free energy difference of the acid and its conjugate base is calculated at MD level using thermodynamic integration. We applied this method to the 24 drug-like molecules in the SAMPL6 blind pKa prediction challenge. We achieved an overall RMSE of 2.4 pKa units in our prediction. Our results show that further optimization of the protocol needs to be done before this method can be used as an alternative approach to the well established approaches of a full quantum level or empirical pKa prediction methods.		Samarjeet Prasad;Jing Huang;Qiao Zeng;Bernard R. Brooks	2018	Journal of computer-aided molecular design	10.1007/s10822-018-0167-1	small molecule;conjugate;chemistry;thermodynamic integration;computational chemistry;deprotonation;molecule;solvation;density functional theory;solvent	Comp.	97.13152799076343	-4.415802821380534	4101
962f9cb03dfe3c7871b20357c8d2ffd515968991	binding of pollutant aromatics on carbon nanotubes and graphite	transfert electron;nanotubo de carbono;devenir polluant;carbon nanotubes;etude experimentale;electron transfer;medio acuoso;energia interaccion;chimie quantique;methode pm3;biomolecules;interaction energy;graphite;complexes;carbon nanotube;energie interaction;graphene;amino substituted aromatics;full range;pollutant behavior;noncovalent interactions;level;evolucion contaminante;compose aromatique;milieu aqueux;aromatics;pm3 calculations;aqueous medium;dispersion;quantum chemistry;nanotube carbone;functionals;adsorption	The semiempirical PM3 method with dispersive corrections (PM3-D) is used to predict the interaction energy of a number of aromatic pollutants with a graphene surface and with carbon nanotubes. It is found that the dispersive interactions are dominant in determining the magnitude of the interaction and that electron transfer between the adsorbate and the surface is small. Good agreement is found between the calculated interaction energies and the experimental affinities measured in an aqueous environment.	aromatics;carbon dioxide;dispersive partial differential equation;electron transport;energy, physics;graphene;interaction energy;nanotubes;nanotubes, carbon;natural graphite;semi-empirical quantum chemistry method	Anitha Ramraj;Ian H. Hillier	2010	Journal of chemical information and modeling	10.1021/ci1000604	carbon nanotube;chemistry;organic chemistry;quantum chemistry	Graphics	96.26593558218777	-5.721252631540295	4104
b28d6f608d6894fc9c26008eb65c394464156f65	axml: a fast program for sequential and parallel phylogenetic tree calculations based on the maximum likelihood method	dna;phylogeny topology supercomputers computer science databases acceleration computer architecture biology computing concurrent computing bioinformatics;processor architecture;biology computing;topology;evaluation function;optimisation;search space;parallel programming;parallel programming axml phylogenetic tree maximum likelihood method np complete problem rrna sequences optimization functions tree topology search space fastdnaml;phylogenetic tree;computational complexity;high performance computer;maximum likelihood sequence estimation;maximum likelihood sequence estimation dna biology computing parallel programming topology computational complexity tree searching optimisation;tree searching;maximum likelihood method;np complete problem	Heuristics for the NP-complete problem of calculating the optimal phylogenetic tree for a set of aligned rRNA sequences based on the maximum likelihood method are computationally expensive. In most existing algorithms the tree evaluation and branch length optimization functions, calculating the likelihood value for each tree topology examined in the search space, account for the greatest part of overall computation time. This paper introduces AxML, a program derived from fastDNAml, incorporating a fast topology evaluation function. The algorithmic optimizations introduced, represent a general approach for accelerating this function and are applicable to both sequential and parallel phylogeny programs, irrespective of their search space strategy. Therefore, their integration into three existing phylogeny programs rendered encouraging results. Experimental results on conventional processor architectures show a global run time improvement of 35% up to 47% for the various test sets and program versions we used.	algorithm;alignment;analysis of algorithms;anatomy, regional;architecture as topic;computation;evaluation function;heuristics;mathematical optimization;np-completeness;phylogenetic tree;run time (program lifecycle phase);time complexity;tree network;version	Alexandros Stamatakis;Thomas Ludwig;Harald Meier;Marty J. Wolf	2002	Proceedings. IEEE Computer Society Bioinformatics Conference	10.1109/CSB.2002.1039325	mathematical optimization;combinatorics;phylogenetic tree;np-complete;microarchitecture;computer science;bioinformatics;theoretical computer science;machine learning;tree rearrangement;evaluation function;mathematics;maximum likelihood;maximum likelihood sequence estimation;computational complexity theory;genetics;dna	Comp.	-1.3963083619955814	-50.699122040234904	4108
23607e6997d6977006b203324e18cdd7f9b314b0	the k-means-u* algorithm: non-local jumps and greedy retries improve k-means++ clustering		We present a new clustering algorithm called k-means-u* which in many cases is able to significantly improve the clusterings found by k-means++, the current de-facto standard for clustering in Euclidean spaces. First we introduce the k-means-u algorithm which starts from a result of k-means++ and attempts to improve it with a sequence of non-local “jumps” alternated by runs of standard k-means. Each jump transfers the “least useful” center towards the center with the largest local error, offset by a small random vector. This is continued as long as the error decreases and often leads to an improved solution. Occasionally k-means-u terminates despite obvious remaining optimization possibilities. By allowing a limited number of retries for the last jump it is frequently possible to reach better local minima. The resulting algorithm is called k-means-u* and dominates k-means++ wrt. solution quality which is demonstrated empirically using various data sets. By construction the logarithmic quality bound established for k-means++ holds for k-means-u* as well.	approximation error;cluster analysis;computer simulation;experiment;greedy algorithm;k-means clustering;k-means++;machine learning;mathematical optimization;maxima and minima;np-completeness;overhead (computing);python;randomized algorithm;retry;monotone	Bernd Fritzke	2017	CoRR		machine learning;mathematical optimization;artificial intelligence;multivariate random variable;cluster analysis;logarithm;k-means clustering;mathematics;maxima and minima;data set;offset (computer science);algorithm;jump	ML	19.8472989044394	-36.90663539551264	4109
f480c9678509d3c0f3b8be78970307028cd21422	a real-time image stabilization system based on fourier-mellin transform	software platform;real time;image stabilization;mellin transform	The paper presents a robust real-time image stabil iz tion system based on the Fourier-Mellin transform. The system i s capable of performing image capture-stabilization-display at a rate of st andard video on a general Pentium III at 800 MHz without any specialized hardwar e and the use of any particular software platforms. This paper describes th e theoretical basis of the image matching used and the practical aspects conside red to increase its robustness and accuracy as well as the optimizations carr ied out for its real-time implementation. The system has been submitted to exte nsive practical experimentation in several applications showing high robustn e s.	carr–benkler wager;ibm system i;image registration;real-time clock;real-time transcription	J. Ramiro Martinez de Dios;Aníbal Ollero	2004		10.1007/978-3-540-30125-7_47	computer science;mellin transform;image stabilization	Vision	43.95546110966556	-37.34371950496405	4112
f134e9b23311a28ff44c82c83b8a9f12d7604026	spatio-temporal interest points chain (stipc) for activity recognition	object recognition;image motion analysis;spatiotemporal phenomena feature extraction image motion analysis image representation image sequences object recognition;interest points;space time;computer vision;accuracy;trajectory;image representation;feature extraction;image sequence;spatiotemporal phenomena;pattern recognition;humans;space time feature extraction activity recognition spatiotemporal interest points chain stipc activity representation discriminative motion information point trajectory extraction image sequences continuous motion points discontinuous motion points;trajectory feature extraction tracking computer vision humans pattern recognition accuracy;tracking;image sequences;activity recognition	We present a novel feature, named Spatio-Temporal Interest Points Chain (STIPC), for activity representation and recognition. This new feature consists of a set of trackable spatio-temporal interest points, which correspond to a series of discontinuous motion among a long-term motion of an object or its part. By this chain feature, we can not only capture the discriminative motion information which space-time interest point-like feature try to pursue, but also build the connection between them. Specifically, we first extract the point trajectories from the image sequences, then partition the points on each trajectory into two kinds of different yet close related points: discontinuous motion points and continuous motion points. We extract local space-time features around discontinuous motion points and use a chain model to represent them. Furthermore, we introduce a chain descriptor to encode the temporal relationships between these interdependent local space-time features. The experimental results on challenging datasets show that our STIPC features improves local space-time features and achieve state-of-the-art results.	activity recognition;encode;experiment;interdependence;interest point detection;motion compensation	Fei Yuan;Gui-Song Xia;Hichem Sahbi;Véronique Prinet	2011	The First Asian Conference on Pattern Recognition	10.1109/ACPR.2011.6166581	computer vision;machine learning;pattern recognition;mathematics;motion field	Vision	36.65177632027503	-50.01301166620596	4114
31a1286d2376a1d9899a8368254b41637eada45a	global and local statistical regularities control visual attention to object sequences	complexity theory;psychology;sensitivity;visualization;statistical learning;monitoring;tracking	Many previous studies have shown that both infants and adults are skilled statistical learners. Because statistical learning is affected by attention, learners' ability to manage their attention can play a large role in what they learn. However, it is still unclear how learners allocate their attention in order to gain information in a visual environment containing multiple objects, especially how prior visual experience (i.e., familiarly of objects) influences where people look. To answer these questions, we collected eye movement data from adults exploring multiple novel objects while manipulating object familiarity with global (frequencies) and local (repetitions) regularities. We found that participants are sensitive to both global and local statistics embedded in their visual environment and they dynamically shift their attention to prioritize some objects over others as they gain knowledge of the objects and their distributions within the task.	embedded system;machine learning	Alexa R. Romberg;Yayun Zhang;Benjamin Newman;Jochen Triesch;Chen Yu	2016	2016 Joint IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob)	10.1109/DEVLRN.2016.7846829	computer vision;computer science;machine learning;communication	Robotics	15.177142520988584	-76.38973201677187	4120
1e167703bd15c0e423ac24030aa9770309c92e01	data mining and knowledge discovery for monitoring and intelligent control of a wastewater treatment plant	intelligent control;data mining;knowledge discovery	Intelligent control of medium-scale industrial processes has been applied with success but, as a method of advanced control, can be further improved. Since intelligent control makes use of knowledge-based techniques (such as expert systems, fuzzy logic, neural networks, etc.), a data mining and knowledge discovery subsystem embedded in a control system can support an intelligent controller to achieve a more reliable and robust operation of the controlled process. This paper proposes a combined intelligent control and data mining scheme for monitoring and mainly for controlling a wastewater treatment plant. The intelligent control system is implemented in a programmable logic controller, while the data mining and knowledge discovery system in a personal computer. The entire control system is basically a knowledgebased system which improves drastically the behavior of the wastewater treatment plant.	advanced process control;artificial neural network;control flow;control system;data mining and knowledge discovery;discovery system;embedded system;expert system;fuzzy logic;intelligent control;mined;personal computer	Stamatis Manesis;Vasileios Deligiannis;M. Koutri	2008			photolithography;engineering;anti-reflective coating;field emission display;transistor;data mining;substrate (electronics);anode;fabrication;electrical conductor	Robotics	56.708763430010606	-9.456302665456414	4122
8bbbd7c93d8aa994de73f2704686f41b57adc87e	optimal time-complexity speed planning for robot manipulators		We consider the speed planning problem for a robotic manipulator. In particular, we present an algorithm for finding the time-optimal speed law along an assigned path that satisfies velocity and acceleration constraints and respects the maximum forces and torques allowed by the actuators. The addressed optimization problem is a finite dimensional reformulation of the continuous-time speed optimization problem, obtained by discretizing the speed profile with N points. The proposed algorithm has linear complexity with respect to N and to the number of degrees of freedom. Such complexity is the best possible for this problem. Numerical tests show that the proposed algorithm is significantly faster than algorithms already existing in literature.	algorithm;mathematical optimization;numerical method;optimization problem;robot;time complexity;velocity (software development)	Luca Consolini;Marco Locatelli;Andrea Minari;Akos Nagy;István Vajk	2018	CoRR		time complexity;acceleration;torque;discretization;manipulator;actuator;control theory;robot;mathematics;optimization problem	Robotics	62.98551358422377	-20.753997336664526	4134
317999329265a680857e93b5478be6fb15368b8c	on the strong convergence of the optimal linear shrinkage estimator for large dimensional covariance matrix	62g20;large dimensional asymptotics;covariance matrix estimation;60b20;62g30;random matrix theory;62h12	In this work we construct an optimal linear shrinkage estimator for the covariance matrix in high dimensions. The recent results from the random matrix theory allow us to find the asymptotic deterministic equivalents of the optimal shrinkage intensities and estimate them consistently. The developed distribution-free estimators obey almost surely the smallest Frobenius loss over all linear shrinkage estimators for the covariance matrix. The case we consider includes the number of variables p ? ∞ and the sample size n ? ∞ so that p / n ? c ? ( 0 , + ∞ ) . Additionally, we prove that the Frobenius norm of the sample covariance matrix tends almost surely to a deterministic quantity which can be consistently estimated.		Taras Bodnar;Arjun K. Gupta;Nestor Parolya	2014	J. Multivariate Analysis	10.1016/j.jmva.2014.08.006	covariance mapping;scatter matrix;estimation of covariance matrices;econometrics;covariance matrix;mathematical optimization;law of total covariance;covariance;random matrix;mathematics;state-transition matrix;statistics;covariance function	Vision	30.71936460507938	-23.480352071916244	4136
1fed73d47de8a7330b361307b70bd07cd6167874	antialiasing the hough transform	hough transform	Abstract   The discretization of the Hough transform parameter plane is considered. It is shown that the popular accumulator method implies sampling of a nonbandlimited signal. The resultant aliasing accounts for several familiar difficulties in the algorithm. Bandlimiting the parameter plane would allow Nyquist sampling, thus aliasing could be avoided. An effectively alias-free Hough algorithm is presented and analyzed. The uncertainty principle of signal representation induces a compromise between image-space localization and parameter-space sampling density, as well as an upper bound on the performance of the algorithm. These results contribute to the development of a design methodology for hierarchical “coarse to fine” Hough algorithms.	hough transform;spatial anti-aliasing	Nahum Kiryati;Alfred M. Bruckstein	1991	CVGIP: Graphical Model and Image Processing	10.1016/1049-9652(91)90043-J	hough transform;computer vision;mathematical optimization;computer science;theoretical computer science;mathematics	Vision	64.50070701853394	-54.2574118970053	4140
8c86acb65dd544d1501f205df79513e8f010ddf1	precise real-time outlier removal from motion vector fields for 3d reconstruction	image motion analysis;real time;computer vision motion vector field 3d reconstruction image sequence feature extraction block matching real time hardware real time outlier removal;motion vector field;computer vision;conference paper;motion vector;feature extraction;image reconstruction;image sequence;multidimensional signal processing;multidimensional signal processing image sequences image motion analysis image reconstruction feature extraction computer vision;image reconstruction layout cameras hardware image sequences geometry matrix decomposition surface reconstruction educational institutions computer vision;block matching;3d structure;3d reconstruction;image sequences	Finding the correct correspondences in an image sequence is a significant task for deriving 3D structure from motion. Most research has concentrated on extracting and matching salient feature points for correspondence. Block-matching has largely been disregarded due to its significant number of correspondence-outliers and its complexity. However, nowadays real-time hardware is available to obtain blockmotion vectors. We present a fast method to filter out more than99.7% of all outliers and show that the obtained correspondences can be used to derive the 3D scene depth of real image sequences.	3d reconstruction;real-time clock;structure from motion	Andreas Dante;Mike Brookes	2003		10.1109/ICIP.2003.1246981	3d reconstruction;iterative reconstruction;multidimensional signal processing;computer vision;feature extraction;computer science;pattern recognition;mathematics;computer graphics (images)	Vision	52.54662513463602	-50.605409969212836	4149
7d6118a95ce081f0acb4656e0aad3a7d4988c9d1	identifying diseases-related metabolites using random walk	infdissim;misim;metabolites;random walk;similarity of diseases;similarity of metabolites	Metabolites disrupted by abnormal state of human body are deemed as the effect of diseases. In comparison with the cause of diseases like genes, these markers are easier to be captured for the prevention and diagnosis of metabolic diseases. Currently, a large number of metabolic markers of diseases need to be explored, which drive us to do this work. The existing metabolite-disease associations were extracted from Human Metabolome Database (HMDB) using a text mining tool NCBO annotator as priori knowledge. Next we calculated the similarity of a pair-wise metabolites based on the similarity of disease sets of them. Then, all the similarities of metabolite pairs were utilized for constructing a weighted metabolite association network (WMAN). Subsequently, the network was utilized for predicting novel metabolic markers of diseases using random walk. Totally, 604 metabolites and 228 diseases were extracted from HMDB. From 604 metabolites, 453 metabolites are selected to construct the WMAN, where each metabolite is deemed as a node, and the similarity of two metabolites as the weight of the edge linking them. The performance of the network is validated using the leave one out method. As a result, the high area under the receiver operating characteristic curve (AUC) (0.7048) is achieved. The further case studies for identifying novel metabolites of diabetes mellitus were validated in the recent studies. In this paper, we presented a novel method for prioritizing metabolite-disease pairs. The superior performance validates its reliability for exploring novel metabolic markers of diseases.	area under curve;diabetes mellitus;extraction;human metabolome database;mental association;metabolic diseases;metabolic process, cellular;metabolite;national center for biomedical ontology;node - plant part;other toxicity studies: metabolites;paget's disease, mammary;parkinson disease;receiver operating characteristic;text mining	Yang Hu;Tianyi Zhao;Ningyi Zhang;Tianyi Zang;Jun Zhang;Liang Cheng	2018		10.1186/s12859-018-2098-1	random walk;receiver operating characteristic;metabolite;human metabolome database;bioinformatics;biology;text mining	Comp.	5.540905621458038	-56.36024997856207	4154
218ffa31162b4162e25d3b5eb8360f7a95efc342	regularizing adaboost with validation sets of increasing size	standards;neural networks;training;noise measurement;iterative methods;training data;bars	AdaBoost is an iterative algorithm to construct classifier ensembles. It quickly achieves high accuracy by focusing on objects that are difficult to classify. Because of this, AdaBoost tends to overfit when subjected to noisy datasets. We observe that this can be partially prevented with the use of validation sets, taken from the same noisy training set. But using less than the full dataset for training hurts the performance of the final classifier ensemble. We introduce ValidBoost, a regularization of AdaBoost that takes validation sets from the dataset, increasing in size with each iteration. ValidBoost achieves performance similar to AdaBoost on noise-free datasets and improved performance on noisy datasets, as it performs similar at first, but does not start to overfit when AdaBoost does.	adaboost;algorithm;iteration;iterative method;linear classifier;overfitting;test set	Dirk W. J. Meijer;David M. J. Tax	2016	2016 23rd International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2016.7899632	adaboost;training set;computer science;noise measurement;machine learning;pattern recognition;data mining;iterative method;artificial neural network	Vision	15.445021687383973	-40.70971802577238	4157
f126cab6b92aa954d10ba17f9aeab02da52d16b4	catching falling knives: speculating on liquidity shocks	arbitrage	Many market participants invest resources to acquire information about liquidity rather than fundamentals. I show that agents using such information can reduce the magnitude of short-lived pricing errors by trading against liquidity shocks. However, the short-run stabilizing effect of this behavior also makes it more difficult to identify liquidity shocks, a signal-jamming effect that slows down price discovery in the long-run. As more agents invest in non-fundamental information, market prices become more resilient to liquidity shocks, but also recover more slowly from temporary price deviations.		Jean-Edouard Colliard	2017	Management Science	10.1287/mnsc.2016.2440	financial economics;liquidity crisis;accounting liquidity;liquidity risk;high-frequency trading;economics;market impact;finance;microeconomics;market liquidity;arbitrage;monetary economics	Theory	-1.3384846069985152	-8.495989471968537	4160
34d1c0b4455f3efd62cf8d5169fcce8bad7da794	many-objective optimization based on information separation and neighbor punishment selection	期刊论文	Many-objective optimization refers to optimizing a multi-objective optimization problem (MOP) where the number of objectives is more than 3. Most classical evolutionary multi-objective optimization (EMO) algorithms use the Pareto dominance relation to guide the search, which usually perform poorly in the many-objective optimization scenario. This paper proposes an EMO algorithm based on information separation and neighbor punishment selection (ISNPS) to deal with many-objective optimization problems. ISNPS separates individual’s behavior in the population into convergence information and distribution information by rotating the original coordinates in the objective space. Specifically, the proposed algorithm employs one coordinate to reflect individual’s convergence and the remaining \(m-1\) coordinates to reflect individual’s distribution, where m is the number of objectives in a given MOP. In addition, a neighborhood punishment strategy is developed to prevent individuals from being crowded. From a series of experiments on 42 test instances with 3–10 objectives, ISNPS has been found to be very competitive against six representative algorithms in the EMO area.	mathematical optimization	Ruimin Shen;Jinhua Zheng;Miqing Li;Juan Zou	2017	Soft Comput.	10.1007/s00500-015-1842-y	mathematical optimization;computer science;artificial intelligence;multi-objective optimization;machine learning;mathematics;statistics	EDA	24.371825441508346	-3.9804499153533053	4163
5b4c7d70f2633f90825811e9aedd203214a6894d	event-driven video adaptation: a powerful tool for industrial video supervision	computer vision tools;objects tracking;adaptive scalable video adaptation	Efficient video content adaptation requires techniques for content analysis and understanding as well as the development of appropriate mechanisms for content scaling in terms of the network properties, terminal devices characteristics and users’ preferences. This is particularly evident in industrial surveillance applications, due to the huge amount of data needed to be stored, delivered and handled. In this paper, we address both issues by incorporating (a) computer vision tools that allows efficient tracking of salient visual objects for long time regardless of the dynamics of the visual environment –via a self initialized tracking algorithm—and (b) an adaptive optimal rate distortion scheme able to allocate different priorities for each detected video object with respect to users’ needs, network platforms capabilities and terminal characteristics. The self initialized tracker firstly appropriately describes visual content, secondly incorporates adaptive mechanisms for automatically update the tracker to adjust to the current conditions and thirdly includes an efficient decision mechanism that estimates the time instances in which adaptation should be activated. For the rate distortion algorithm, an optimal adaptive framework is adopted which is capable of allocating the desired quality to objects of users’ interest without violating the target bit rate of the sequence. The Wavelet Packet Transform (WPT) is adopted towards this purpose. The advantage of the WPT is that it localizes the frequency components of each video object and therefore it offers additionally content adaptability according to video object texture coding. The WPT tree is transmitted only at the first frame of each shot and thus dew bits are required for its encoding. Experimental results and comparisons with other approaches are presented to illustrate the good performance of the proposed architecture. The results cover real-world and complex industrial environments.	adaptive optimization;algorithm;computer vision;content adaptation;digital video;distortion;emoticon;high- and low-level;image scaling;mathematical optimization;norm (social);particle filter;perturbation theory;rate–distortion theory;real life;scalability;transmitter;unsupervised learning;visual objects;wavelet packet decomposition	Anastasios D. Doulamis	2012	Multimedia Tools and Applications	10.1007/s11042-012-0992-5	computer vision;simulation;telecommunications;computer science;operating system;machine learning;video tracking;multimedia;world wide web;computer security	Robotics	50.78101371542572	-17.249785520203876	4164
859f47fb7e37966d0d1227d309df3463770a2f39	novel concepts for a planetary surface exploration rover	encapsulation;robot movil;movilidad;caracteristica funcionamiento;modele geometrique;mobility;aplicacion espacial;encapsulacion;robotics;mobilite;motion;endnotes;robot mobile;rough terrain mobility;caracteristique fonctionnement;robotica;pubications;space technology;robotique;rough terrain;vehiculo todo terreno;crosscountry vehicle;performance characteristic;centre of mass;application spatiale;moving robot;space application;geometrical model;design methodology;vehicule tout terrain;modelo geometrico	Purpose - This paper aims to address some of the needs of present and upcoming rover designs, and introduces novel concepts incorporated in a planetary surface exploration rover design that is currently under development. Design/methodology/approach - The Multitasking Rover (MTR) is a highly re-configurable system that aims to demonstrate functionality that will cover many of the current and future needs such as rough-terrain mobility, modularity and upgradeability. It comprises a surface mobility platform which is highly re-configurable, which offers centre of mass re-allocation and rough terrain stability, and also a set of science/tool packs - individual subsystems encapsulated in packs which the rover picks up, transports and deploys. Findings - Early testing of the suspension system suggests exceptional performance characteristics. Originality/value - Principles employed in the design of the MTR can be used in future rover systems to reduce associated mission costs and at the same time provide multiples the functionality.	planetary scanner;rover (the prisoner)	Antonios K. Bouloubasis;Gerard T. McKee;Peter Tolson	2007	Industrial Robot	10.1108/01439910710727450	center of mass;simulation;encapsulation;design methods;computer science;engineering;motion;space technology;robotics	Robotics	64.54116808315368	-32.71334099956518	4166
d397ee21bad924d7acc6df1a3f18f241604f20db	improved method for determining the shear strength of chip component solder joints	resistance electrique;metalizacion;electronic circuit;resistencia electrica componente;microelectronic fabrication;desviacion tipica;fabricacion microelectrica;resistencia cizallamiento;soldeo con refusion;capacitor;montage surface composant;packaging electronico;condensador;brasage avec refusion;resistance cisaillement;alignment defect;standard deviation;shear strength;circuito electronico;surface mount technology;chip;metallizing;packaging electronique;integrated circuit bonding;defaut alignement;defecto alineacion;reflow soldering;assemblage circuit integre;electronic packaging;defaillance;condensateur;ecart type;montaje superficie componente;failures;assemblage brasage tendre;resistor;junta soldada;circuit electronique;fallo;soldered joint;metallisation;fabrication microelectronique	In this paper an improved method has been presented to determine the solder joint shear strength of passive discrete surface mounted (SMD) chip components (like resistors and capacitors). To calculate the stress in a solder joint in the case of shear loading, the force applied should be measured and the amount of joined surface (wetted area of the component metallization) calculated. Using the method we suggest, we first measured the exact position of the chip component after soldering according to the guidelines set out in standard IPC 9850 (Institute for Interconnecting and Packaging Electronic Circuits). To determine the accurate value of the joined surface, a 3D profile calculation was carried out taking into account the exact position of chip components after soldering. The calculation of the profile was based on the principle of minimum energy. Then, the next stage was to determine the maximum force experimentally that the solder joint was able to withstand before failure in shear. The evaluation of the shear load results verified that the standard deviation coefficient of the results was lower when the shear strength of the solder is characterized by the maximum stress instead of maximum force. It was proved by our experiments and by simulations that the shear strength of misaligned components solder joints depends on the degree of component misalignment after reflow soldering.	coefficient;electronic circuit;experiment;maximum force;reflow soldering;simulation;surface-mount technology	Olivér Krammer;Bálint Sinkovics	2010	Microelectronics Reliability	10.1016/j.microrel.2009.10.016	resistor;chip;electronic circuit;electronic engineering;metallizing;capacitor;engineering;electrical engineering;electronic packaging;forensic engineering;standard deviation;engineering drawing;shear strength	Robotics	90.52096049110973	-10.355527195233659	4167
ae8a9c2063e91e531e7bf07371e7a180bef8ff22	a generic compliance modeling method for two-axis elliptical-arc-filleted flexure hinges	compliance modeling;elliptical arc filleted;flexure hinge;flexure segment;two-axis	As a kind of important flexible joint, two-axis flexure hinges can realize in-plane and out-of-plane motions and can be used for constructing flexure-based spatial compliant mechanisms. The paper introduces a common two-axis elliptical-arc-filleted flexure hinge that is generated by two different elliptical-arc-filleted cutout profiles and that provides some new hinge types. The analytical compliance equations of both half-segments of the two-axis elliptical-arc flexure hinges are firstly formulated, and then, based on a generic compliance modeling method of a flexure serial chain, the closed-form compliance and precision matrices of two-axis elliptical-arc-filleted flexure hinges are established and validated by the finite element method. Some numerical simulations are conducted to compare the effect of different design geometric parameters on the performance of the two-axis flexure hinges.	aibo;apache axis;axis vertebra;castigliano's method;checking (action);compliance behavior;computer simulation;conflict (psychology);correctness (computer science);f9 embryonic antigen gene;federal enterprise architecture;finite element method;flexure;generic drugs;genus axis;hinge (physical object);hinge device component;iscador;lithium;manuscripts;motion;numerical analysis;optic axis of a crystal;sheng-yu-tang	Lijian Li;Dan Zhang;Sheng Guo;Haibo Qu	2017		10.3390/s17092154	engineering;arc (geometry);finite element method;hinge;compliant mechanism;structural engineering;matrix (mathematics)	Robotics	70.42591286497488	-20.579559120411126	4170
d96cae370e08b48f6a89db3a44f97b13684d95e1	texture-based segmentation and finite element mesh generation for heterogeneous biological image data	dissertation	Title of dissertation: TEXTURE-BASED SEGMENTATION AND FINITE ELEMENT MESH GENERATION FOR HETEROGENEOUS BIOLOGICAL IMAGE DATA Prabhakar Reddy, Gudla, Ph.D., 2005 Dissertation directed by: Dr. Hubert J. Montas Biological Resources Engineering Department The design, analysis, and control of bio-systems remain an engineering challenge. This is mainly due to the material heterogeneity, boundary irregularity, and nonlinear dynamics associated with these systems. The recent developments in imaging techniques and stochastic upscaling methods provides a window of opportunity to more accurately assess these bio-systems than ever before. However, the use of image data directly in upscaled stochastic framework can only be realized by the development of certain intermediate steps. The goal of the research presented in this dissertation is to develop a texture-segmentation method and a unstructured mesh generation for heterogeneous image data. The following two new techniques are described and evaluated in this dissertation. 1. A new texture-based segmentation method, using the stochastic continuum concepts and wavelet multi-resolution analysis, is developed for characterization of heterogeneous materials in image data. The feature descriptors are developed to efficiently capture the micro-scale heterogeneity of macro-scale entities. The materials are then segmented at a representative elementary scale at which the statistics of the feature descriptor stabilize. 2. A new unstructured mesh generation technique for image data is developed using a hierarchical data structure. This representation allows for generating quality guaranteed finite element meshes. The framework for both the methods presented in this dissertation, as such, allows them for extending to higher dimensions. The experimental results using these methods conclude them to be promising tools for unifying data processing concepts within the upscaled stochastic framework across biological systems. These are targeted for inclusion in decision support systems where biological image data, simulation techniques and artificial intelligence will be used conjunctively and uniformly to assess bio-system quality and design effective and appropriate treatments that restore system health. TEXTURE-BASED SEGMENTATION AND FINITE ELEMENT MESH GENERATION FOR HETEROGENEOUS BIOLOGICAL IMAGE DATA by Prabhakar Reddy, Gudla Dissertation submitted to the Faculty of the Graduate School of the University of Maryland, College Park in partial fulfillment of the requirements for the degree of Doctor of Philosophy 2005 Advisory Commmittee: Hubert J. Montas, Ph.D., Chair/Advisor Adel Shirmohammadi, Ph.D., Co-Advisor Yang Tao, Ph.D. Dennis Healy, Ph.D. Andrew S. Kane, Ph.D. c ©Copyright by Prabhakar Reddy, Gudla 2005	algorithm;algorithmic efficiency;artificial intelligence;best practice;bi-directional text;biological system;british informatics olympiad;british undergraduate degree classification;computation;computational complexity theory;computational resource;conformity;data structure;decision support system;delaunay triangulation;develop;dictionary;dyadic transformation;elemental;emoticon;entity;feature vector;finite element method;geist;haar wavelet;hierarchical database model;image resolution;image scaling;image segmentation;image sensor;java development kit (jdk);library (computing);lookup table;mesh generation;message passing interface;mobile agent;multiresolution analysis;numerical analysis;numerical partial differential equations;parallel virtual machine;parallel computing;pixel;planar straight-line graph;pointer (computer programming);polygon mesh;population dynamics;quadtree;quicksort;refinement (computing);relevance;requirement;reversible computing;soap with attachments;simulation;software development kit;supercomputer;synthetic intelligence;tree (data structure);tree traversal;triune continuum paradigm;unstructured grid;video post-processing;visual descriptor;window of opportunity;yang	Prabhakar R. Gudla	2005			computational science;mesh generation;computer science;theoretical computer science;segmentation-based object categorization;scale-space segmentation;engineering drawing	Robotics	47.81093595770521	-72.69681132008208	4177
95799c76e54524ed180e5c5ae5f23aba30148be1	a gradient descent technique coupled with a dynamic simulation to determine the near optimum of floor plan designs			gradient descent;simulation	Eugénio Rodrigues;Adélio Rodrigues Gaspar;Álvaro Gomes	2013	CoRR			EDA	17.38186335518281	-24.49256959820115	4179
f11d21e78ee50c8d69e0e0d8b2ccd707a6f76898	enhanced stability analysis for networked control systems under random and malicious packet losses	numerical stability;networked control systems;packet loss;asymptotic stability;stability analysis;linear programming	We investigate the effects of random and malicious packet losses on the stability of a networked control system. Specifically, we explore the networked control problem for the case where the plant and the controller exchange state and control input packets over a communication channel that may face random transmission failures as well as malicious attacks by an intelligent agent. We obtain a sufficient condition for the stability of the networked system and show that this condition can be assessed by examining the solutions to linear programming problems. The coefficients of the constraints in these problems depend on an asymptotic upper-bound for the average number of transmission failures that we use for characterizing random packet losses and malicious attacks. We illustrate the efficacy of our results with a numerical example.	channel (communications);coefficient;computational complexity theory;control system;intelligent agent;linear programming;malware;network packet;numerical analysis	Ahmet Cetinkaya;Hideaki Ishii;Tomohisa Hayakawa	2016	2016 IEEE 55th Conference on Decision and Control (CDC)	10.1109/CDC.2016.7798673	control engineering;von neumann stability analysis;networked control system;linear programming;control theory;mathematics;distributed computing;packet loss;numerical stability	Embedded	65.97525427450974	2.057544179098815	4180
f29c21fe516531a2c1e04d4827c2dd44679b1016	an incremental clustering pattern sequence-based short-term load prediction for cloud computing		Short-term load prediction is a significant cost-optimal resource allocation and energy saving approach for a cloud computing environment. Traditional linear or nonlinear prediction models that forecast future load directly from historical information appear less effective. Load classification before prediction is necessary to improve prediction accuracy. In this paper, a novel clustering algorithm and prediction approach is proposed to forecast future load for cloud computing data centres. First, an incremental kernel k-means clustering based data clustering method is adopted to classify the continuously coming cloud load. Secondly, Hausdorff distance based similarity computation method is then used to identify the most appropriate cluster that possesses the maximum likelihood for current load. With the data from this cluster, a fast neural network is used to forecast future load. Experimental results show that our approach is more efficient and outperforms other approaches reported in previous works.	cloud computing;cluster analysis	Dayu Xu;Xuyao Zhang	2016	IJGUC	10.1504/IJGUC.2016.10001961	machine learning;pattern recognition;data mining	HPC	9.078384510414773	-20.941655149024825	4181
a6d32c1706db5079560245695b23ffbe88934a1b	stokes flow patterns induced by a single cardiac cell	cardiomyocyte;e c coupling;flow signature;stokes flow	"""Stokes flow motions induced by a beating single cardiac cell (cardiomyocyte) are obtained numerically using the method of fundamental solutions (MFS). A two-dimensional meshfree-Stokeslets computational framework is used to solve the Stokes governing equations around an isolated cardiomyocyte. An approximate beating kinematical model is derived and used to approximate the cell-length shortening over a complete cardiac cycle. The induced flow patterns have been found to be characterized by the presence of counter-rotating vortices at both cell's edges. These vortical flow structures are clearly shown by rendering the velocity streamlines. The static pressure contours are also calculated at different time snapshots during both contraction and relaxation phases of the beating motion. The pressure signal is calculated at a point in the neighborhood of cell surface to capture the induced normal stress (traction) by the cell morphological motions to the surrounding fluid medium. The presented results have shown that, cells with a slightly different shortening/beating profile can induce different flow field. This implies that, each cell is characterized by a unique flow pattern """"signature"""", which potentially can be correlated to the sub-cellular excitation-contraction processes of cardiac cells."""	approximation algorithm;conflict (psychology);excitation;flow network;heart cell;hydrodynamics;linear programming relaxation;marfan syndrome;meshfree methods;method of fundamental solutions;moose file system;morphing;motion;myocytes, cardiac;navier–stokes equations;numerical analysis;pressure signal device component;traction teampage;velocity (software development);vortex	Yasser Aboelkassem	2017	Computers in biology and medicine	10.1016/j.compbiomed.2017.05.001	mathematics;geometry;stokes flow	Graphics	28.6553209087682	-86.91918635744629	4183
cb05b1c0597470a70248a8cb7eb056e53cfb362d	learning with sample dependent hypothesis spaces	sample size;learning algorithm;approximation error;sample dependent hypothesis spaces;mathematical analysis;error analysis;kernel method;regularization scheme;learning theory;linear space	Many learning algorithms use hypothesis spaces which are trained from samples, but little theoretical work has been devoted to the study of these algorithms. In this paper we show that mathematical analysis for these algorithms is essentially different from that for algorithms with hypothesis spaces independent of the sample or depending only on the sample size. The difficulty lies in the lack of a proper characterization of approximation error. To overcome this difficulty, we propose an idea of using a larger function class (not necessarily linear space) containing the union of all possible hypothesis spaces (varying with the sample) to measure the approximation ability of the algorithm. We show how this idea provides error analysis for two particular classes of learning algorithms in kernel methods: learning the kernel via regularization and coefficient based regularization. We demonstrate the power of this approach by its wide applicability. © 2008 Elsevier Ltd. All rights reserved.	algorithm;approximation error;coefficient;cross-validation (statistics);error analysis (mathematics);kernel method;loss function;machine learning;matrix regularization	Qiang Wu;Ding-Xuan Zhou	2008	Computers & Mathematics with Applications	10.1016/j.camwa.2008.09.014	sample size determination;regularization perspectives on support vector machines;kernel method;econometrics;mathematical optimization;approximation error;mathematical analysis;learning theory;mathematics;linear space;statistics;generalization error	ML	20.686563824876178	-33.48330650329666	4188
8f4a46bcaaee8b4bc4879e7a12726534308c3995	rough set algorithm for crack category determination of reinforced concrete structures	reinforced concrete;concrete structure;rough set theory;mathematical methods;crack characteristics;data analysis;cracking;algorithms;concrete structures;decision algorithm;rough set;decision table;environmental factor	Rough set theory is a new mathematical approach to imprecision, vagueness and uncertainty in data analysis. This paper presents a new approach to rough set theory in determining the category of crack causes for insufficient and imprecise crack characteristics observed in regular inspection of concrete structures. The categories of crack causes are classified into four classes, that is, (1) concrete material, (2) construction work, (3) service and environmental factors, and (4) structure and applied loads. The crack characteristics include time of formation, shape, regularity, cause of concrete deformation, and range. The decision table was constructed considering crack characteristics as condition attributes and the categories of crack causes as decision attributes. A minimal decision algorithm for this decision table was generated on the basis of rough set theory; the algorithm is equivalent to the original decision table, but requires minimum subsets of condition attributes. It turned out in determining the category of crack causes that, ‘‘time of formation” had the most important influence among crack characteristics, and ‘‘shape” could be omitted with the least influence on diagnosis. 2008 Elsevier Ltd. All rights reserved.	algorithm;convergence insufficiency;decision table;fuzzy concept;rough set;set theory;vagueness	Yeong Min Kim;Chee Kyeong Kim;Jae Cheol Lee	2009	Advances in Engineering Software	10.1016/j.advengsoft.2008.04.002	structural engineering;discrete mathematics;rough set;computer science;machine learning;mathematics;engineering drawing	AI	-2.730313182339597	-26.197985624875678	4196
64eec989f917ce08700a41bb31dcab4bfe34412c	cardiac deformation recovery via incompressible transformation decomposition	heart;three dimensions;cardiac muscle;satisfiability;mr imaging;short axis;right ventricular;physical properties	This paper presents a method for automated deformation recovery of the left and right ventricular wall from a time sequence of anatomical images of the heart. The deformation is recovered within the heart wall, i.e. it is not limited only to the epicardium and endocardium. Most of the suggested methods either ignore or approximately model incompressibility of the heart wall. This physical property of the cardiac muscle is mathematically guaranteed to be satisfied by the proposed method. A scheme for decomposition of a complex incompressible geometric transformation into simpler components and its application to cardiac deformation recovery is presented. A general case as well as an application specific solution is discussed. Furthermore, the manipulation of the constructed incompressible transformations, including the computation of the inverse transformation, is computationally inexpensive. The presented method is mathematically guaranteed to generate incompressible transformations which are experimentally shown to be a very good approximation of actual cardiac deformations. The transformation representation has a relatively small number of parameters which leads to a fast deformation recovery. The approach was tested on six sequences of two-dimensional short-axis cardiac MR images. The cardiac deformation was recovered with an average error of 1.1 pixel. The method is directly extendable to three dimensions and to the entire heart.© (2005) COPYRIGHT SPIE--The International Society for Optical Engineering. Downloading of the abstract is permitted for personal use only.		Oskar M. Skrinjar;Arnaud Bistoquet	2005		10.1117/12.596168	mathematical optimization;mathematics;geometry;engineering drawing	Vision	69.8188485720397	-45.476605318937544	4198
b5251a89d72f8c207e54a723e28e8b722dccd01e	exceptional object analysis for finding rare environmental events from water quality datasets	08 information and computing sciences;respubid25438;anomaly detection;0801 artificial intelligence and image processing;09 engineering;journal;080109 pattern recognition and data mining;0502 environmental science and management;exceptional object analysis;college of science and engineering;clustering;eoa;svm;rare environmental event;support vector machine;rare environmental events	This paper provides a novel Exceptional Object Analysis for Finding Rare Environmental Events (EOAFREE). The major contribution of our EOAFREE method is that it proposes a general Improved Exceptional Object Analysis based on Noises (IEOAN) algorithm to efficiently detect and rank exceptional objects. Our IEOAN algorithm is more general than already known outlier detection algorithms to find exceptional objects that may be not on the border; and experimental study shows that our IEOAN algorithm is far more efficient than directly recursively using already known clustering algorithms that may not force every data instance to belong to a cluster to detect rare events. Another contribution is that it provides an approach to preprocess heterogeneous real world data through exploring domain knowledge, based on which it defines changes instead of the water data value itself as the input of the IEOAN algorithm to remove the geographical differences between any two sites and the temporal differences between any two years. The effectiveness of our EOAFREE method is demonstrated by a real world application - that is, to detect water pollution events from the water quality datasets of 93 sites distributed in 10 river basins in Victoria, Australia between 1975 and 2010.		Jing He;Yanchun Zhang;Guangyan Huang	2012	Neurocomputing	10.1016/j.neucom.2011.08.036	support vector machine;anomaly detection;computer science;data science;machine learning;data mining	ML	0.07442767035308423	-35.19889365400335	4200
cd59c14bb6c94d658956cb8b0520ba15a8f13ea0	multi-criteria robust design of a jit-based cross-docking distribution center for an auto parts supply chain	cross docking;response surface methodology;robust optimization;journal;latin hypercube sampling;supply chain simulation;bootstrapping	We present a solution framework based on discrete-event simulation and enhanced robust design technique to address a multi-response optimization problem inherent in logistics management. The objective is to design a robust configuration for a cross-docking distribution center so that the system is insensitive to the disturbances of supply uncertainty, and provides steady parts supply to downstream assembly plants. In the proposed approach, we first construct a simulation model using factorial design and central composite design (CCD), and then identify the models that best describe the relationship between the simulation responses and system factors. We employ the response surface methodology (RSM) to identify factor levels that would maximize system potential.	algorithm;best, worst and average case;bootstrapping (compilers);docking (molecular);dynamical system;etsi satellite digital radio;extrapolation;global optimization;heart rate variability;interpolation;just-in-time compilation;kriging;mathematical optimization;netware file system;numerical analysis;response surface methodology;robust optimization;simulation;star catalogue;taguchi methods;windows nt 4.0	Wen Shi;Zhixue Liu;Jennifer Shang;Yujia Cui	2013	European Journal of Operational Research	10.1016/j.ejor.2013.03.013	mathematical optimization;robust optimization;response surface methodology;simulation;latin hypercube sampling;computer science;operations management;mathematics;bootstrapping	Robotics	11.172930603087952	-3.258606859843736	4201
e1a593dfbdcdb2f347d241ec5794dac44c4ca5fc	on competitive sequential location in a network with a decreasing demand intensity	graph node;graphe non oriente;fixed cost;politica optima;location problem;optimisation;probleme localisation;game theory;non directed graph;costo fijo;nudo grafo;optimizacion;competitividad;test hotelling;cout fixe;localization;location game theory;teoria juego;localizacion;theorie jeu;location;optimal policy;profit;graphe pondere;localisation;grafo pondero;beneficio;grafo no orientado;edge graph;competitiveness;benefice;arete graphe;optimization;profitability;problema localizacion;hotelling test;weighted graph;politique optimale;competitivite;noeud graphe;arista grafico	We introduce and analyze a Hotelling like game wherein players can locate in a city, at a fixed cost, according to an exogenously given order. Demand intensity is assumed to be strictly decreasing in distance and players locate in the city as long as it is profitable for them to do so. For a linear city (i) we explicitly determine the number of players who will locate in equilibrium, (ii) we fully characterize and compute the unique family of equilibrium locations, and (iii) we show that players' equilibrium expected profits decline in their position in the order. Our results are then extended to a city represented by an undirected weighted graph whose edge lengths are not too small and co-location on nodes of the graph is not permitted. Further, we compare the equilibrium outcomes with the optimal policy of a monopolist who faces an identical problem and who needs to decide upon the number of stores to open and their locations in the city so as to maximize total profit.		Daniel Granot;Frieda Granot;Tal Raviv	2010	European Journal of Operational Research	10.1016/j.ejor.2009.12.021	game theory;mathematical optimization;profit;internationalization and localization;economics;operations management;mathematics;mathematical economics;location;welfare economics;fixed cost;profitability index	Vision	0.6445347871255895	-4.647041076888086	4202
9cc0711700bf5113404ffaea780372c961a9f4fa	adept, a dynamic next generation sequencing data error-detection program with trimming	computational biology bioinformatics;illumina error prediction;algorithms;basic biological sciences;combinatorial libraries;next generation sequencing;computer appl in life sciences;position specific quality;basic biological sciences next generation sequencing;local quality scores;microarrays;bioinformatics	Illumina is the most widely used next generation sequencing technology and produces millions of short reads that contain errors. These sequencing errors constitute a major problem in applications such as de novo genome assembly, metagenomics analysis and single nucleotide polymorphism discovery. In this study, we present ADEPT, a dynamic error detection method, based on the quality scores of each nucleotide and its neighboring nucleotides, together with their positions within the read and compares this to the position-specific quality score distribution of all bases within the sequencing run. This method greatly improves upon other available methods in terms of the true positive rate of error discovery without affecting the false positive rate, particularly within the middle of reads. ADEPT is the only tool to date that dynamically assesses errors within reads by comparing position-specific and neighboring base quality scores with the distribution of quality scores for the dataset being analyzed. The result is a method that is less prone to position-dependent under-prediction, which is one of the most prominent issues in error prediction. The outcome is that ADEPT improves upon prior efforts in identifying true errors, primarily within the middle of reads, while reducing the false positive rate.	antibody-directed enzyme prodrug therapy;base;biopolymer sequencing;de novo transcriptome assembly;error detection and correction;error message;genome assembly sequence;massively-parallel sequencing;metagenomics;nucleotides;reading (activity);sensitivity and specificity;silo (dataset);single nucleotide polymorphism	Shihai Feng;Chien-Chi Lo;Po-E Li;Patrick S. G. Chain	2016		10.1186/s12859-016-0967-z	computational biology;biology;dna sequencing;dna microarray;computer science;bioinformatics;dna sequencing theory;hybrid genome assembly	Comp.	0.9198169592154422	-54.99307612267988	4207
2ce5782a9bbd399d314919d1b2c77a23a5079e69	a homography formulation to the 3pt plus a common direction relative pose problem		In this paper we present an alternative formulation for the minimal solution to the 3pt plus a common direction relative pose problem. Instead of the commonly used epipolar constraint we use the homography constraint to derive a novel formulation for the 3pt problem. This formulation allows the computation of the normal vector of the plane defined by the three input points without any additional computation in addition to the standard motion parameters of the camera. We show the working of the method on synthetic and real data sets and compare it to the standard 3pt method and the 5pt method for relative pose estimation. In addition we analyze the degenerate conditions for the proposed method.	ambiguous name resolution;computation;epipolar geometry;experiment;homography (computer vision);normal (geometry);synthetic data;visual odometry	Olivier Saurer;Pascal Vasseur;Cédric Demonceaux;Friedrich Fraundorfer	2014		10.1007/978-3-319-16808-1_20	computer vision;mathematical optimization	Vision	53.19409003806634	-49.20338513023551	4208
2473f6571b09b139386a65c36ee47837480c553b	coordination mechanisms	congestion games;price of anarchy;selfish task allocation;mechanism;game theory	We introduce the notion of coordination mechanisms to improve the performance in systems with independent selfish agents. The quality of a coordination mechanism is measured by its price of anarchy—the worst-case performance of a Nash equilibrium over the (centrally controlled) social optimum. We give upper and lower bounds for the price of anarchy for selfish resource allocation and analyze the case of selfish routing on a simple network.	anarchy;best, worst and average case;nash equilibrium;routing	George Christodoulou;Elias Koutsoupias;Akash Nanavati	2004		10.1007/978-3-540-27836-8_31	price of stability;mathematical economics;price of anarchy	ECom	-4.1834197482588396	0.7066292449797528	4215
7ac5d61088f6c421830823ba526f0361f92ac3d1	dissociating error-based and reinforcement-based loss functions during sensorimotor learning		It has been proposed that the sensorimotor system uses a loss (cost) function to evaluate potential movements in the presence of random noise. Here we test this idea in the context of both error-based and reinforcement-based learning. In a reaching task, we laterally shifted a cursor relative to true hand position using a skewed probability distribution. This skewed probability distribution had its mean and mode separated, allowing us to dissociate the optimal predictions of an error-based loss function (corresponding to the mean of the lateral shifts) and a reinforcement-based loss function (corresponding to the mode). We then examined how the sensorimotor system uses error feedback and reinforcement feedback, in isolation and combination, when deciding where to aim the hand during a reach. We found that participants compensated differently to the same skewed lateral shift distribution depending on the form of feedback they received. When provided with error feedback, participants compensated based on the mean of the skewed noise. When provided with reinforcement feedback, participants compensated based on the mode. Participants receiving both error and reinforcement feedback continued to compensate based on the mean while repeatedly missing the target, despite receiving auditory, visual and monetary reinforcement feedback that rewarded hitting the target. Our work shows that reinforcement-based and error-based learning are separable and can occur independently. Further, when error and reinforcement feedback are in conflict, the sensorimotor system heavily weights error feedback over reinforcement feedback.	cns disorder;cursor (databases);feedback;lateral computing;lateral thinking;loss function;money;movement;noise (electronics);shift jis	Joshua G. A. Cashaback;Heather R. McGregor;Ayman Mohatarem;Paul L. Gribble	2017		10.1371/journal.pcbi.1005623	cursor (user interface);biology;probability distribution;bioinformatics;simulation;mode (statistics);reinforcement;control theory	HCI	15.33982021279543	-74.28751581172706	4216
4d994974f19fcffd31ca6dcc1159f1f978174f4a	learning high-order task relationships in multi-task learning	multitask learning;task relationship;new formulation;new probabilistic model;recent methods model;new model;multi-task learning;high-order task relationship;model parameter;different task;simultaneous learning	Multi-task learning is a way of bringing inductive transfer studied in human learning to the machine learning community. A central issue in multi-task learning is to model the relationships between tasks appropriately and exploit them to aid the simultaneous learning of multiple tasks effectively. While some recent methods model and learn the task relationships from data automatically, only pairwise relationships can be represented by them. In this paper, we propose a new model, called MultiTask High-Order relationship Learning (MTHOL), which extends in a novel way the use of pairwise task relationships to high-order task relationships. We first propose an alternative formulation of an existing multi-task learning method. Based on the new formulation, we propose a high-order generalization leading to a new prior for the model parameters of different tasks. We then propose a new probabilistic model for multi-task learning and validate it empirically on some benchmark datasets.	benchmark (computing);computer multitasking;floor and ceiling functions;machine learning;model selection;multi-task learning;norm (social);statistical model	Yu Zhang;Dit-Yan Yeung	2013			semi-supervised learning;unsupervised learning;robot learning;feature learning;proactive learning;multi-task learning;instance-based learning;error-driven learning;algorithmic learning theory;sequence learning;computer science;artificial intelligence;data science;online machine learning;machine learning;inductive transfer;learning classifier system;stability;competitive learning;computational learning theory;active learning;generalization error	AI	22.17286980424008	-43.915806942093276	4220
28d518e1f51eaaa4653a6b1f42358fb76e96b63d	online isotonic regression		We consider the online version of the isotonic regression problem. Given a set of linearly ordered points (e.g., on the real line), the learner must predict labels sequentially at adversarially chosen positions and is evaluated by her total squared loss compared against the best isotonic (nondecreasing) function in hindsight. We survey several standard online learning algorithms and show that none of them achieve the optimal regret exponent; in fact, most of them (including Online Gradient Descent, Follow the Leader and Exponential Weights) incur linear regret. We then prove that the Exponential Weights algorithm played over a covering net of isotonic functions has a regret bounded by O ( T 1/3 log(T ) ) and present a matching Ω(T ) lower bound on regret. We provide a computationally efficient version of this algorithm. We also analyze the noise-free case, in which the revealed labels are isotonic, and show that the bound can be improved to O(log T ) or even to O(1) (when the labels are revealed in isotonic order). Finally, we extend the analysis beyond squared loss and give bounds for entropic loss and absolute loss.	algorithm;algorithmic efficiency;gradient descent;isotonic regression;machine learning;mean squared error;ordered pair;regret (decision theory)	Wojciech Kotlowski;Wouter M. Koolen;Alan Malek	2016			mathematical optimization;isotonic regression;machine learning;mathematics;statistics	ML	21.390345854196905	-30.726088957774387	4221
e37c041ad2ab6b5a9b2c4795e99f9e6dc0376983	moving horizon estimation: error dynamics and bounding error sets for robust control	computacion informatica;grupo de excelencia;moving horizon estimation;robust mpc;ciencias basicas y experimentales;mhe;error dynamics	In this work we present a method for the derivation of the estimation error dynamics, the bounding set of the estimation error, and the state estimate dynamic equations of the constrained Moving Horizon Estimator (MHE).	robust control	Anna Voelker;Konstantinos I. Kouramas;Efstratios N. Pistikopoulos	2013	Automatica	10.1016/j.automatica.2013.01.008	control engineering;control theory;mathematics;moving horizon estimation;statistics	ML	63.95175249108913	-1.5091734329959432	4223
fcae086918ba1993d7bb3772afbe2dc875d64789	task allocation algorithm based on immune system for autonomously cooperative multi-robot system1	artificial immune system;the multi robot system;autonomous cooperation;task allocation	In many cases, tasks are unknown for the multi-robot system in advance. Therefore, robots are required to work cooperatively during the proceeding of tasks. Such cooperation is called autonomous cooperation. To realize autonomous cooperation in the multi-robot system, an appropriate task allocation algorithm is quite important for the efficiency of the system. Taking advantage of the interactions of antibodies and antigen stimulus of the artificial immune system, this paper proposes an immune based task allocation algorithm for the autonomously cooperative robots. Simulating the mechanism of the immune system, the immune-based task allocation algorithm is designed utilizing the interactions among the antibodies from robots. By considering the antibodies from interand intra-robot simultaneously, the algorithm is different from other allocation algorithms which are greedy in task allocation. Besides the interactions among the antibodies, the task allocation algorithm is developed by self-reinforcement learning of the antigen stimulus. The autonomous cooperation among robots is realized by adjusting the antigen stimulus. The allocation algorithm for the autonomously cooperative robots is demonstrated and analyzed in the simulation of box fixing, in which a group of robots must work cooperatively to fix boxes with unknown difficulty in advance.	artificial immune system;autonomous robot;greedy algorithm;interaction;memory management;positive feedback;reinforcement learning;simulation	Yunyuan Gao	2009	Intelligent Automation & Soft Computing	10.1080/10798587.2009.10643030	simulation;computer science;artificial intelligence;machine learning;artificial immune system	Robotics	17.659678816092917	-18.63690527714448	4226
0e27c2912420800fce575bedd83d4389ff3425d5	shape and light directions from shading and polarization	refractive index light sources shape cameras three dimensional displays surface reconstruction dielectrics;stereo image processing estimation theory light polarisation refractive index;refractive index estimation shape direction light direction shading dielectric object polarization image single optimization scheme polarization based method complementary ability surface orientation zenith angle observing angle surface point polarization effect photometric stereo method multiple light source surface normal	We introduce a method to recover the shape of a smooth dielectric object from polarization images taken with a light source from different directions. We present two constraints on shading and polarization and use both in a single optimization scheme. This integration is motivated by the fact that photometric stereo and polarization-based methods have complementary abilities. The polarization-based method can give strong cues for the surface orientation and refractive index, which are independent of the light direction. However, it has ambiguities in selecting between two ambiguous choices of the surface orientation, in the relationship between refractive index and zenith angle (observing angle), and limited performance for surface points with small zenith angles, where the polarization effect is weak. In contrast, photometric stereo method with multiple light sources can disambiguate the surface orientation and give a strong relationship between the surface normals and light directions. However, it has limited performance for large zenith angles, refractive index estimation, and faces the ambiguity in case the light direction is unknown. Taking their advantages, our proposed method can recover the surface normals for both small and large zenith angles, the light directions, and the refractive indexes of the object. The proposed method is successfully evaluated by simulation and real-world experiments.	algorithm;experiment;mathematical optimization;normal (geometry);photometric stereo;polarization (waves);polarizer;shading;simulation;specularity	Trung Ngo Thanh;Hajime Nagahara;Rin-ichiro Taniguchi	2015	2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2015.7298844	computer vision;photometric stereo	Vision	59.27014072214922	-52.21290232414509	4227
2aa1f218c66486054b3f2678189ff5324300a788	learning decision trees with flexible constraints and objectives using integer optimization		We encode the problem of learning the optimal decision tree of a given depth as an integer optimization problem. We show experimentally that our method (DTIP) can be used to learn good trees up to depth 5 from data sets of size up to 1000. In addition to being efficient, our new formulation allows for a lot of flexibility. Experiments show that we can use the trees learned from any existing decision tree algorithms as starting solutions and improve the trees using DTIP. Moreover, the proposed formulation allows us to easily create decision trees with different optimization objectives instead of accuracy and error, and constraints can be added explicitly during the tree construction phase. We show how this flexibility can be used to learn discrimination-aware classification trees, to improve learning from imbalanced data, and to learn trees that minimise false positive/negative errors.	decision tree;program optimization	Sicco Verwer;Yingqian Zhang	2017		10.1007/978-3-319-59776-8_8	mathematical optimization;constraint (mathematics);decision tree learning;computer science;optimal decision;machine learning;data set;decision tree;optimization problem;artificial intelligence;integer	Logic	18.496981864373314	-36.81640394354905	4228
6b1bc6a504d3489644d5c20fff22f06bea725901	an efficient technique for automatic segmentation of fingerprint roi from digital slap image	minutiae;biometrics;slap scanner;segmentation;fingerprint	Slap fingerprint scanner can simultaneously acquire fingerprint of four fingers of a human hand in a single image. Subsequently, the image is required to be segmented to get fingerprints of individual fingers. The paper proposes a novel and efficient technique to extract individual fingerprints. It applies a divide and conquer strategy by using force field and heuristics. The technique has been tested on two slap image databases viz. IITK-4Slap-Rural and IITK-4Slap-Student and has achieved high segmentation accuracy.	fingerprint;region of interest	Kamlesh Tiwari;Phalguni Gupta	2015	Neurocomputing	10.1016/j.neucom.2014.04.086	minutiae;fingerprint;computer vision;speech recognition;segmentation;biometrics;computer graphics (images)	Vision	34.45141244616438	-61.944017809153664	4230
2097efe5482d160df1265bb93e65151c90e74ba7	how frugal is mother nature with haplotypes?	databases genetic;heterozygote;bioinformatik berakningsbiologi;humans;haplotypes;bioinformatics computational biology	MOTIVATION Inference of haplotypes from genotype data is crucial and challenging for many vitally important studies. The first, and most critical step, is the ascertainment of a biologically sound model to be optimized. Many models that have been proposed rely partially or entirely on reducing the number of unique haplotypes in the solution.   RESULTS This article examines the parsimony of haplotypes using known haplotypes as well as genotypes from the HapMap project. Our study reveals that there are relatively few unique haplotypes, but not always the least possible, for the datasets with known solutions. Furthermore, we show that there are frequently very large numbers of parsimonious solutions, and the number increases exponentially with increasing cardinality. Moreover, these solutions are quite varied, most of which are not consistent with the true solutions. These results quantify the limitations of the Pure Parsimony model and demonstrate the imperative need to consider additional properties for haplotype inference models. At a higher level, and with broad applicability, this article illustrates the power of combinatorial methods to tease out imperfections in a given biological model.	cardinality;genotype;haplotypes;imperative programming;inference;maximum parsimony (phylogenetics);occam's razor;solutions	Sharlee Climer;Gerold Jäger;Alan R. Templeton;Weixiong Zhang	2009	Bioinformatics	10.1093/bioinformatics/btn572	heterozygote advantage;biology;haplotype;bioinformatics;genetics	Comp.	2.1580377791312246	-52.4748926010686	4233
18ef4586d4133754920a3db88f2a3eb6562d330b	invariant codes for similar transformation and its application to shape matching	similarity transformation;shape matching	In this paper, we propose a new method for the measurement of shape similarity. Our proposed method encodes the contour of an object by using the curvature of the object. If one objects are similar (under translation, rotation, and scaling) in shape to the other, these codes themselves or their cyclic shift have the same values. We compare our method with other methods such as CSS (curvature scale space), and shape context. We show that the recognition rate of our method is 100 % and 90.40 % for the rotation and scaling robustness test using MPEG7-CE-Shape1 and 81.82 % and 95.14 % for the similarity-based retrieval test and the occlusion test using Kimia's silhouette. In particular, the value of the occlusion test is approximately 25 % higher than those of CSS, SC. Moreover, we show that the computational cost of our method is not so large by comparison our method with above methods.	algorithmic efficiency;cascading style sheets;circular shift;code;computation;experiment;fast fourier transform;image scaling;scale space;shape context	Eiji Yoshida;Seiichi Mita	2008			matrix similarity;computer science;pattern recognition	Vision	40.755205446840286	-57.50107363216873	4243
7f6f0b956deccf41af5e0601d5da3c8cf2db5616	boundary detection in scintigraphic images		S OF PAPERS ACCEPTED FOR PUBLICATION 93 Rank filters operating on images assign the k th value of the gray levels from the window consisting of M pixels arranged according to their value to the center point of the window. The special cases k = I, k = M (MIN and MAX filter) and k = (M + I)/2 (median filter), which have already been applied in image processing, are investigated in systematic connection with all rank filters. Some of their properties can be formulated analytically. They commute with monotonic transforms of the gray scale. In the one-dimensional case-also valid for line-like structures in images-the output functions of monotonicinput functions can be calculated directly. The alternating application of MIN and MAX filters leads, if repeated more than once, to the same result as a single application. The application of the rank filters to a set of test images shows that there is no simple way to describe their action on the spectrum by means of a transfer or autocorrelation function, In particular the smoothing of the median filter cannot be described in terms of a low-pass filter, but rather by the reduction of the mean local variance. As shown on real and statistical model images, rank filters smooth less than linear filters, but preserve edges. Boundary Detectron rn Scintigruphic Images. SIEGFRIED J. P~PPL AND G~NTER HERRMANN. Institute for Medical Informatics and Health Services Res., GSF, Department of Signal Processing and Pattern Recognition, D-8000 Munich 81, Arabellastrasse 4, West Germany. Received August 19, 1981 Automatic boundary detection in image signals obtained from an Anger scintillation camera requires special methods. On the one hand the mapped object is defocussed by the limited resolution of the camera and the radioactive process; on the other hand the true boundaries of the mapped organs for the comparison of different algorithms are not very easily obtainable. Therefore, different filtering and boundary detection algorithms have been applied to phantom scintigrams from different objects, i.e., hollow spheres. A Two-Puss Filling Algorithm for Raster Graphics. A. DISTANTE AND N. VENEZIANI. Istituto di Fisica, GNPPO Nazionale Cibemetica e Biofisica, Universita di Bari, Italy. Received August 24, 1981. An algorithm for filling irregular polygons is presented. It is designed to operate in raster mode and uses the refresh memory of a graphic display as working space. The algorithm has been implemented in a host computer but it can be implemented easily within the microprocessor controlling the display. All areas contoured by closed lines are shaded at the same time according to a preassigned color code. False Contour Removal by Random Blurring. SEIICHI NISHIHARA AND KATSUO IKEDA. Institute of Information Sciences and Electronics, University of Tsukuba, Sakura-mura, Niibari-gun, Ibaraki 305, Japan. Received September 18, 1981; revised December I, 1981; accepted March I, 1982. A method is presented to eliminate spurious brightness discontinuities, or false contours, in areas in which brightness varies gradually, while preserving edge sharpness. The basic idea of the method is randomly blurring points only in the neighborhood of each border on which those spurious brightness discontinuities may occur. The method is used for producing natural appearance of a picture for the human eye, not for picture analysis. Chord Distributions for Shape Matching. STEPHEN P. SMITH AND ANIL K. JAIN. Department of Computer Science, Michigan State University, East Lansing, Michigan 48824. Received September 21, 1981; revised February 5, 1982. A shape-matching method based on concepts tied to integral geometry is studied. The method consists of computing the distribution of random chords over the figures to be matched and comparing them by means of the Kolmogorov-Smimov test. A simplified form of the procedure for matching figures to a circle yields a test for circularity. It is generalized to obtain a statistical test for shape matching and then	algorithm;autocorrelation;computer science;contour line;gamma camera;generic sensor format;grayscale;host (network);image processing;imaging phantom;informatics;low-pass filter;max;median filter;microprocessor;modified uniformly redundant array;particle filter;pattern recognition;pixel;randomness;raster graphics;shading;shape context;signal processing;smoothing;statistical model;workspace	Siegfried J. Pöppl;Günter Herrmann	1982	Computer Graphics and Image Processing	10.1016/0146-664X(82)90160-5	computer vision;computer science;mathematics;geometry	Vision	49.16785954508218	-65.55536794606786	4244
5754bfd6728c7671ac1df67a59a4286b4b02a4a2	"""experiments on a representation-independent """"top-down and prune"""" induction scheme"""	systeme intelligent;decision tree;procesamiento informacion;systeme apprentissage;disjunctive normal form;top down;sistema inteligente;learning systems;information processing;intelligent system;information system;traitement information;systeme information;sistema informacion	Recently, some methods for the induction of Decision Trees have received much theoretical attention. While some of these works focused on efficient top-down induction algorithms, others investigated the pruning of large trees to obtain small and accurate formulae. This paper discusses the practical possibility of combining and generalizing both approaches, to use them on various classes of concept representations, not strictly restricted to decision trees or formulae built from decision trees. The algorithm, WIREi, is able to produce decision trees, decision lists, simple rules, disjunctive normal form formulae, a variant of multilinear polynomials, and more. This shifting ability allows to reduce the risk of deviating from valuable concepts during the induction. As an example, in a previously used simulated noisy dataset, the algorithm managed to find systematically the target concept itself, when using an adequate concept representation. Further experiments on twenty-two readily available datasets show the ability of WIREi to build small and accurate concept representations, which lets the user choose his formalism to best suit his interpretation needs, in particular for mining purposes.	experiment	Richard Nock;Marc Sebban;Pascal Jappy	1999		10.1007/978-3-540-48247-5_24	information processing;computer science;artificial intelligence;machine learning;decision tree;top-down and bottom-up design;database;mathematics;disjunctive normal form;information system;algorithm	Crypto	7.825286217140476	-32.58622394019447	4250
9ff8f0141816ef32206576c825354ad52932788b	face representation method using pixel-to-vertex map (pvm) for 3d model based face recognition	reconnaissance visage;modelizacion;parallelisme;virtual machine;image tridimensionnelle;interfase usuario;vision ordenador;coreano;image processing;illumination;facies;image databank;user interface;vision estereoscopica;luminance;vision stereoscopique;procesamiento imagen;machine virtuelle;traitement image;korean;computer vision;modelisation;parallelism;3d model;face recognition;paralelismo;coreen;banco imagen;banque image;face modeling;pattern recognition;tridimensional image;interface utilisateur;vision ordinateur;reconnaissance forme;reconocimiento patron;stereopsis;maquina virtual;modeling;eclairement;imagen tridimensional;alumbrado;luminancia	3D model based approach for face recognition has been spotlighted as a robust solution under variant conditions of pose and illumination. Since a generative 3D face model consists of a large number of vertices, a 3D model based face recognition system is generally inefficient in computation time. In this paper, we propose a novel 3D face representation algorithm to reduce the number of vertices and optimize its computation time. Finally, we evaluate the performance of proposed algorithm with the Korean face database collected using a stereo-camera based 3D face capturing device.	3d modeling;facial recognition system;parallel virtual machine;pixel	Taehwa Hong;Hagbae Kim;Hyeonjoon Moon;Yong-Guk Kim;Jongweon Lee;Seungbin Moon	2006		10.1007/11754336_3	facial recognition system;computer vision;facies;image processing;computer science;virtual machine;stereopsis;artificial intelligence;luminance;user interface;korean;computer graphics (images)	Vision	47.132249185903504	-57.6691045772581	4255
d4ef4446b0881817c71c7b5dac73e5fa93e5f7e7	distributed evolutionary algorithms and their models: a survey of the state-of-the-art	qa75 electronic computers computer science;distributed evolutionary computation;coevolutionary computation;evolutionary algorithms;multiobjective optimization;global optimization	The increasing complexity of real-world optimization problems raises new challenges to evolutionary computation. Responding to these challenges, distributed evolutionary computation has received considerable attention over the past decade. This article provides a comprehensive survey of the state-of-the-art distributed evolutionary algorithms and models, which have been classified into two groups according to their task division mechanism. Population-distributed models are presented with master-slave, island, cellular, hierarchical, and pool architectures, which parallelize an evolution task at population, individual, or operation levels. Dimension-distributed models include coevolution and multi-agent models, which focus on dimension reduction. Insights into the models, such as synchronization, homogeneity, communication, topology, speedup, advantages and disadvantages are also presented and discussed. The study lobal optimization ultiobjective optimization of these models helps guide future development of different and/or improved algorithms. Also highlighted are recent hotspots in this area, including the cloud and MapReduce-based implementations, GPU and CUDA-based implementations, distributed evolutionary multiobjective optimization, and real-world applications. Further, a number of future research directions have been discussed, with a conclusion that the development of distributed evolutionary computation will continue to flourish. © 2015 Published by Elsevier B.V.		Yue-jiao Gong;Wei-neng Chen;Zhi-hui Zhan;Jun Zhang;Yun Li;Qingfu Zhang;Jing-Jing Li	2015	Appl. Soft Comput.	10.1016/j.asoc.2015.04.061	evolutionary programming;mathematical optimization;interactive evolutionary computation;human-based evolutionary computation;java evolutionary computation toolkit;computer science;artificial intelligence;theoretical computer science;multi-objective optimization;machine learning;evolutionary algorithm;evolution strategy;global optimization;evolutionary computation	ML	22.28942987289022	-5.512704684026931	4259
ebc64fb61dd86f5f8147a6abe3b70ba17eb3e9d4	the use of one-class classifiers for differentiating healthy from epileptic eeg segments		Epilepsy is the fourth most frequent neurological disorder. Epileptic seizures are the result of temporary electrical disturbances in the brain. This disorder can be diagnosed by electroencephalograms (EEG). Accordingly, data mining supported by machine learning (ML) methods can be used to find patterns in EEG and to build classifiers. However, the presence of physiological abnormalities is considered rare in medical data. Also, rhythmic activities in an EEG of a healthy person varies in different situations, such as with closed or opened eyes. Other issue is that normal and interictal epileptic EEG can contain similar patterns. In this work, we developed classifiers in order to differentiate EEG of healthy volunteers, with opened and closed eyes, from epileptic EEG. This approach was applied in an EEG set with 500 segments, in which cross-correlation and power spectrum methods were performed for feature extraction. Subsequently, predictive models based on one-class classification were built using the one-class support vector machine (OSVM) technique in addition to traditional ML methods, such as nearest-neighbor, artificial neural network, and two-class support vector machine (TSVM). The polynomial and radial basis function (RBF) kernels were applied in OSVM and TSVM. To enable the use of traditional ML methods, the OCC was converted to two-class classification problem through assignment of negative (abnormal) class to each EEG segment that does not belong to normal (positive) class. In the evaluation, it was found that the OSVM classifier with RBF kernel reached the best values for all confusion matrix parameters, such as accuracy, positive predictive value, sensitivity, and false positive rate.	artificial neural network;confusion matrix;cross-correlation;data mining;electroencephalography;feature extraction;machine learning;nearest-neighbor interpolation;one-class classification;optimistic concurrency control;polynomial;predictive modelling;radial (radio);radial basis function kernel;spectral density;support vector machine	Jefferson Tales Oliva;João Luís Garcia Rosa	2017	2017 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2017.7966222	artificial intelligence;support vector machine;false positive rate;machine learning;pattern recognition;ictal;electroencephalography;feature extraction;artificial neural network;speech recognition;computer science;radial basis function kernel;confusion matrix	ML	17.046242479247795	-90.17499354360827	4262
696d40ed3d418ddc0a485345cf3d065bc7006335	automatic identification peaks in chromatographic fingerprints based on fuzzy matching	medical computing chromatography fuzzy set theory;fuzzy set theory;retention time;medical computing;relative retention factor;fuzzy matching;herbal medicine;fingerprint recognition feature extraction standards educational institutions hamming distance reactive power euclidean distance;matching degree;chromatographic fingerprint;matching degree chromatographic fingerprint identification peaks fuzzy matching relative retention factor;chromatography;exocarpium citrus grandis automatic identification peaks chromatographic fingerprints fuzzy matching retention time shift herbal medicine extracted features relative retention factor maximal matching degree beyond threshold value;identification peaks;fingerprint identification	Aiming at the problem of retention time shift in chromatographic fingerprints of herbal medicine, a method for automatic identification peaks based on fuzzy matching was proposed, which extracted features according to relative retention factor, introduced the principles of “maximal matching degree” and “beyond threshold value”. The method was successfully applied in the experiment on the 28 chromatographic fingerprints of different batches of Exocarpium Citrus Grandis. The results showed that the scheme could avoid the shortcomings of previous algorithms, such as requirement for expertise and complex operations, and provided a new idea for rapid, effective identification common peaks and extraction common pattern without manual supervision.	algorithm;automatic identification and data capture;fingerprint;matching (graph theory);maximal set;timeshift	Hang Wei;Li Lin;Honglai Zhang;Qinqun Chen	2012	2012 9th International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2012.6234247	fingerprint;approximate string matching;computer science;machine learning;pattern recognition;mathematics;fuzzy set	Robotics	27.963702806333437	-72.77333540980649	4264
ebeb436349526214999c1c3ef8ff05a995b5418e	efficient weighted lattice rules with applications to finance	calcul scientifique;analisis numerico;65d30;metodo monte carlo;finance;65c05;fonction poids;stochastic method;pricing;methode monte carlo;option pricing;lattice system;fijacion precios;integration;methode quasi monte carlo;analyse numerique;algorithme;algorithm;computacion cientifica;systeme reticule;numerical analysis;estimation erreur;error estimation;integracion;monte carlo method;estimacion error;quasi monte carlo methods;lattice rules;funcion peso;methode stochastique;weight function;sistema reticulado;scientific computation;multivariate integration;fixation prix;65d32;finanzas;algoritmo;metodo estocastico	Good lattice rules are an important type of quasi-Monte Carlo algorithms. They are known to have good theoretical properties, in the sense that they can achieve an error bound (or optimal error bound) that is independent of the dimension for weighted spaces with suitably decaying weights. To use the theory of weighted function spaces for practical applications, one has to determine what weights should be used. After explaining that lattice rules based on figures of merit with classical weights (classical lattice rules) may not give good results for highdimensional problems, we propose a “matching strategy”, which chooses the weights in such a way that the typical functions of a weighted Korobov space are similar (in the sense of similar relative sensitivity indices) to those of the given function. The matching strategy relates the weights of the spaces to the sensitivity indices of the given function. We apply the Korobov construction and the component-bycomponent construction of lattice rules with the suitably chosen weights to the valuation of high-dimensional financial derivative securities, and find that such weighted lattice rules improve dramatically on the results for the classical lattice rules; moreover, they are competitive with other types of low discrepancy sequences. We also demonstrate that lattice rules combined with variance reduction techniques and dimension reduction techniques increase the efficiency significantly. We show that an acceptance-rejection approach to the construction of lattice rules can reduce the construction cost with almost no loss of accuracy. 2000 Mathematics Subject Classification: 65C05, 65D30, 65D32.	algorithm;dimensionality reduction;discrepancy function;low-discrepancy sequence;mathematics subject classification;monte carlo method;quasi-monte carlo method;rejection sampling;value (ethics);variance reduction	Xiaoqun Wang;Ian H. Sloan	2006	SIAM J. Scientific Computing	10.1137/S1064827502418197	pricing;econometrics;weight function;numerical analysis;valuation of options;calculus;mathematics;mathematical economics;algorithm;statistics;monte carlo method	ML	34.09990681015684	-19.415933984466054	4266
746183dc6baa72273c62d855de962858f8e54662	spectral mixture analysis: linear and semi-parametric full and iterated partial unmixing in multi- and hyperspectral image data	orthogonal transformation;mixture model;matched filter;additive model;least square;linear model;generalized eigenvalue problem;least squares regression;ordinary least square;spectrum;eigenvalues	As a supplement or an alternative to classification of hyperspectral image data linear and semi-parametric mixture models are considered in order to obtain estimates of abundance of each class or end-member in pixels with mixed membership. Full unmixing based on both ordinary least squares (OLS) and non-negative least squares (NNLS), and the partial unmixing methods orthogonal subspace projection (OSP), constrained energy minimization (CEM) and an eigenvalue formulation alternative are dealt with. The solution to the eigenvalue formulation alternative proves to be identical to the CEM solution. The matrix inversion involved in CEM can be avoided by working on (a subset of) orthogonally transformed data such as signal maximum autocorrelation factors, MAFs, or signal minimum noise fractions, MNFs. This will also cause the partial unmixing result to be independent of the noise isolated in the MAF/MNFs not included in the analysis. CEM and the eigenvalue formulation alternative enable us to perform partial unmixing when we know one desired end-member spectrum only and not the full set of end-member spectra. This is an advantage over full unmixing and OSP. The eigenvalue formulation of CEM inspires us to suggest an iterated CEM scheme. Also the target constrained interference minimized filter (TCIMF) is described. Spectral angle mapping (SAM) is briefly described. Finally, semi-parametric unmixing (SPU) based on a combined linear and additive model with a non-linear, smooth function to represent end-member spectra unaccounted for is introduced. An example with two generated bands shows that both full unmixing, the CEM, the iterated CEM and TCIMF methods perform well. A case study with a 30 bands subset of AVIRIS data shows the utility of full unmixing, SAM, CEM and iterated CEM to more realistic data. Iterated CEM seems to suppress noise better than CEM. A study with AVIRIS spectra generated from real spectra shows (1) that ordinary least squares in this case with one unknown spectrum performs better than non-negative least squares, and (2) that although not fully satisfactory the semi-parametric model gives better estimates of end-member abundances than the linear model.	additive model;autocorrelation;cem kaner;energy minimization;interference (communication);iterated function;iteration;iterative method;least mean squares filter;linear model;mixture model;non-negative least squares;nonlinear system;ordinary least squares;parametric model;pixel;semi-thue system;semiconductor industry;semiparametric model;the matrix	Allan Aasbjerg Nielsen	2001	Journal of Mathematical Imaging and Vision	10.1023/A:1011269530293	mathematical optimization;combinatorics;mathematics;least squares;statistics	ML	77.79654946141268	-37.762193713114264	4271
2475a4c406795a534b5564c69ecdd57850c30c92	robust image segmentation in low depth of field images		In photography, low depth of field (DOF) is an important technique to emphasize the object of interest (OOI) within an image. Thus, low DOF images are widely used in the application area of macro, portrait or sports photography. When viewing a low DOF image, the viewer implicitly concentrates on the regions that are sharper regions of the image and thus segments the image into regions of interest and non regions of interest which has a major impact on the perception of the image. Thus, a robust algorithm for the fully automatic detection of the OOI in low DOF images provides valuable information for subsequent image processing and image retrieval. In this paper we propose a robust and parameterless algorithm for the fully automatic segmentation of low DOF images. We compare our method with three similar methods and show the superior robustness even though our algorithm does not require any parameters to be set by hand. The experiments are conducted on a real world data set with high and low DOF images.	algorithm;experiment;image processing;image retrieval;image segmentation;ocean observatories initiative;region of interest	Franz Graf;Hans-Peter Kriegel;Michael Weiler	2013	CoRR			Vision	41.10960352310418	-69.36833430202951	4273
3f574f7c93c8fd9880138ae729edfb40b4638042	npp estimation using time-series gf-1 data in sparse vegetation area		Net primary productivity (NPP) is an important ecological indicator to evaluate ecosystem, and it is useful for land degradation assessment and monitoring. However, owing to dryland's particularity, retrieving vegetation properties from satellite remote sensing presents some significant challenges in sparse vegetation area. In this study, based on the wildly used time-series GF-1 data, the NPP estimation in sparse vegetation area was analyzed. Results showed that GF-1 data have high spatial and high temporal resolution characteristics, it is useful to distinguish land cover types in semi-arid areas based on NDVI time series data, the accuracy was 83.37% and Kappa coefficient was 0.79. Some key parameters of grassland were simulated and optimized based on CASA model. Compared with the measured data, the result was R2 with 0.71, and results indicated that NPP estimation by GF-1 data based on the new parameters in semi-arid area is feasible.	computational auditory scene analysis;ecological indicator;ecosystem;elegant degradation;kernel density estimation;matthews correlation coefficient;semiconductor industry;sparse matrix;time series	Bin Sun;Zengyuan Li;Zhihai Gao;Wentao Gao;Yuanyuan Zhang;Xiangyuan Ding;Changlong Li	2018	IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2018.8518912	time series;ecological indicator;remote sensing;vegetation;land cover;normalized difference vegetation index;land degradation;temporal resolution;data modeling;computer science	EDA	83.47762943503248	-58.362509088169006	4275
