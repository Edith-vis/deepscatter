id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
38828baf936cc3c3cb48d5e57c1681987aa56182	hierarchical multi-resolution finite element model for soft body simulation	biomechanics;finite element;soft body simulation;boundary condition;finite element model;interactive simulation;geometric model;multi resolution;physical simulation	The complexity of most surgical models has not allowed interactive simulations on standard computers. We propose a new framework to finely control the resolution of the models. This allows us to dynamically concentrate the computational force where it is most needed. Given the segmented scan of an object to simulate, we first compute a bounding box and then recursively subdivide it where needed. The cells of this octree structure are labelled with mechanical properties based on material parameters and fill rate. An efficient physical simulation is then performed using hierarchical hexaedral finite elements. The object surface can be used for rendering and to apply boundary conditions. Compared with traditional finite element approaches, our method dramatically simplifies the task of volume meshing in order to facilitate the using of patient specific models, and increases the propagation of the deformations.	boundary case;computation;computer;dynamical simulation;finite element method;minimum bounding box;multiresolution analysis;octree;physics engine;recursion;rendering (computer graphics);segmented scan;software propagation;time complexity	Matthieu Nesme;François Faure;Yohan Payan	2006		10.1007/11790273_5	simulation;extended finite element method;computer science;biomechanics;theoretical computer science;finite element method;mixed finite element method	Graphics	70.01318074943558	-46.90024033605355	64234
8fab0c7a6cf9951e1eb93efdbb0f899951afce6f	incremental polygonization of implicit surfaces	implicit surface;complex objects;topology;octree;interactive modeling;polygonization;potential field;implicit surfaces;interaction model;incremental algorithm;lipschitz condition;adjacency graph	This paper describes an incremental polygonization technique for implicit surfaces built from skeletal elements. Our method is dedicated to fast previewing in an interactive modeling system environment. We rely on an octree decomposition of space combined with Lipschitz conditions to recursively subdivide cells until a given level of precision is reached and converge to the implicit surface. We use a trilinear interpolation approximation of the field function to create a topologically consistent tessellation characterized by an adjacency graph. Our algorithm aims at updating the mesh locally in regions of space where changes in the potential field occurred. Therefore, we propose an octree inflating and deflating strategy to preserve the octree structure as much as possible and to avoid useless or redundant computations. Timings show that our incremental algorithm dramatically speeds up the overall polygonization process for complex objects.		Eric Galin;Samir Akkouche	2000	Graphical Models	10.1006/gmod.1999.0514	mathematical optimization;discrete mathematics;mathematics;geometry;lipschitz continuity;octree	HCI	69.92123437757726	-44.3463547630264	64778
d25cbc73b320511e91f33435d14c3e063a25c137	triangulation of molecular surfaces using an isosurface continuation algorithm	implicit surface;biocomputing;van der waals model;triangulation molecular surface blobby molecule implicit surface;solid modelling biocomputing macromolecules molecular biophysics quasimolecules;mesh quality;isosurfaces molecular biophysics computational modeling probes visualization solvents synthetic aperture sonar proteins shape solid modeling;three dimensional;isosurface continuation algorithm;blobby molecule;van der waals;quasimolecules;g protein;molecular biophysics;marching cubes algorithm;macromolecules;molecular surfaces triangulation;triangulation;molecular surface;marching cube;biomolecules visualization;solid modelling;marching cubes algorithm molecular surfaces triangulation isosurface continuation algorithm biomolecules visualization van der waals model blobby molecule	There are several computational models and algorithms for the visualization of biomolecules (e.g. proteins and DNA), usually as three-dimensional surfaces called molecular surfaces. Based on the Van-der-Waals model, which represents a molecule as a set of spheres (i.e. atoms), a major approach in modeling and visualization of molecular surfaces, called blobby molecules, represents an implicitly-defined molecular surface as the result of the sum of implicit functions, where each function describes a Van-der-Walls sphere (i.e. the geometry of an atom). In general, these surfaces are polygonized using the well known marching cubes algorithm or similar space-partitioning method. In contrast, this paper presents a very accurate continuation algorithm to polygonize blobby molecules with a very high mesh quality, smoothness and scalability.	accessible surface area;algorithm;atom;computation;computational model;continuation;implicit surface;isosurface;iteration;marching cubes;newton;newton's method;numerical analysis;numerical continuation;olap cube;sampling (signal processing);scalability;space partitioning;time complexity;whole earth 'lectronic link	Adriano N. Raposo;João A. Queiroz;Abel João Padrão Gomes	2009	2009 International Conference on Computational Science and Its Applications	10.1109/ICCSA.2009.16	mathematics;geometry;marching cubes;molecular biophysics;computer graphics (images)	Visualization	73.3930104695261	-46.99633019204775	64907
ca17f1e976e49c868b211e4bd65a6f6b361e382a	interactive seismic interpretation with piecewise global energy minimization	minimization;solid modelling energy consumption hydrocarbon reservoirs interactive systems oil drilling;feature detection;well drillings;visual analysis interactive 3d seismic interpretation piecewise global energy minimization world wide energy consumption oil depletion oil reservoirs reservoir valorization well drillings piecewise optimal horizon surface;oil depletion;oil drilling;hydrocarbon reservoirs;three dimensional displays surface treatment minimization face wire geology feature extraction;data capture;world wide energy consumption;indexing terms;three dimensional;computer graphic;i 4 6 image processing and computer vision segmentation edge and feature detection i 3 8 computing methodologies computer graphics applications;computer vision;wire;surface treatment;i 4 6 image processing and computer vision segmentation edge and feature detection;geology;piecewise global energy minimization;oil reservoirs;energy consumption;three dimensional displays;i 3 8 computing methodologies computer graphics applications;feature extraction;visual analysis;piecewise optimal horizon surface;face;energy minimization;interactive 3d seismic interpretation;interactive systems;hours of work;reservoir valorization;solid modelling	Increasing demands in world-wide energy consumption and oil depletion of large reservoirs have resulted in the need for exploring smaller and more complex oil reservoirs. Planning of the reservoir valorization usually starts with creating a model of the subsurface structures, including seismic faults and horizons. However, seismic interpretation and horizon tracing is a difficult and error-prone task, often resulting in hours of work needing to be manually repeated. In this paper, we propose a novel, interactive workflow for horizon interpretation based on well positions, which include additional geological and geophysical data captured by actual drillings. Instead of interpreting the volume slice-by-slice in 2D, we propose 3D seismic interpretation based on well positions. We introduce a combination of 2D and 3D minimal cost path and minimal cost surface tracing for extracting horizons with very little user input. By processing the volume based on well positions rather than slice-based, we are able to create a piecewise optimal horizon surface at interactive rates. We have integrated our system into a visual analysis platform which supports multiple linked views for fast verification, exploration and analysis of the extracted horizons. The system is currently being evaluated by our collaborating domain experts.	buncefield fire;cognitive dimensions of notations;computation;coupling (computer programming);depletion region;energy minimization;fundamental fysiks group;horizon effect;reservoir computing;scalability	Thomas Höllt;Johanna Beyer;Fritz Gschwantner;Philipp Muigg;Helmut Doleisch;Gabor Heinemann;Markus Hadwiger	2011	2011 IEEE Pacific Visualization Symposium	10.1109/PACIFICVIS.2011.5742373	computer vision;mathematical optimization;simulation;engineering	Visualization	71.87099920033235	-50.10416903376494	65157
3b4f9afbb72a914409944041ae8918e3e066d8d9	interpolation methods for spatio-temporal geographic data	approximation lineaire;interpolation;base donnee;color rendering;systeme information geographique;spatio temporal interpolation;geographic information system;real estate;metodo reduccion;krigeage;revue bibliographique;estudio comparativo;revista bibliografica;interpolacion;temporal data;coaccion;contrainte;database;linear approximation;fatiga color;base dato;finite element method;shape function;shape functions;experimental result;etude comparative;funcion forma;constraint;spatio temporal data;fonction forme;interpolation method;rendu couleur;space time correlation;bibliographic review;aproximacion lineal;comparative study;resultado experimental;house prices;ejemplo;constraint databases;methode reduction;extension;inverse distance weighted;correlacion espacio tiempo;resultat experimental;kriging;correlation spatiotemporelle;example;reduction method;sistema informacion geografica;exemple	We consider spatio-temporal interpolation of geographic data using both the reduction method, which treats time as an independent dimension, and the extension method, which treats time as equivalent to a spatial dimension. We adopt both 2-D and 3-D shape functions from finite element methods for the spatio-temporal interpolation of 2-D spatial and 1-D temporal data sets. We also develop new 4-D shape functions and use them for the spatio-temporal interpolation of 3-D spatial and 1-D temporal data sets. Using an actual real estate data set with house prices, we compare these methods with other spatio-temporal interpolation methods based on inverse distance weighting and kriging. The comparison criteria include interpolation accuracy, error-proneness to time aggregation, invariance to scaling on the coordinate axes, and the type of constraints used in the representation of the interpolated data. Our experimental results show that the extension method based on shape functions is the most accurate and the overall best spatio-temporal interpolation method. New color rendering algorithms are also developed for the visualization of time slices of the interpolated spatio-temporal data. We show some visualization results of the real estate data set including the vertical profile of house prices.	algorithm;cognitive dimensions of notations;extension method;finite element method;image scaling;interpolation;kriging	Lixin Li;Peter Z. Revesz	2004	Computers, Environment and Urban Systems	10.1016/S0198-9715(03)00018-8	spline interpolation;bilinear interpolation;interpolation;stairstep interpolation;calculus;bicubic interpolation;mathematics;geometry;geographic information system;nearest-neighbor interpolation;multivariate interpolation;cartography;statistics;trilinear interpolation	Visualization	68.6299681637676	-39.23167010143771	65377
c8ff5b6924ea34c91ac0ad79c46f728fc0e95041	particle-based simulation and visualization of fluid flows through porous media	relative position;fluid flow;particle based modeling;boundary condition;incompressible flow;porous media;smooth particle hydrodynamics;low reynolds number;smoothed particle hydrodynamics;fluid simulation;physically based modeling	We propose a method of fluid simulation where boundary conditions are designed in such a way that fluid flow through porous media, pipes, and chokes can be realistically simulated. Such flows are known to be low Reynolds number incompressible flows and occur in many real life situations. To obtain a high quality fluid surface, we include a scalar value in isofunction. The scalar value indicates the relative position of each particle with respect to the fluid surface.	central processing unit;circuit complexity;computation;display resolution;fluid animation;graphics processing unit;hash function;isosurface;marching cubes;multi-core processor;overhead (computing);particle filter;programming paradigm;real life;search algorithm;simulation;smoothed-particle hydrodynamics;stationary process	Serkan Bayraktar;Ugur Güdükbay;Bülent Özgüç	2010	J. Visualization	10.1007/s12650-010-0041-2	classical mechanics;statistical physics;mathematical optimization;smoothed-particle hydrodynamics;fluid parcel;reynolds stress;thermodynamics;physics;fluid mechanics;fluid dynamics	Visualization	70.95244683056505	-48.56349496742496	65699
19e03b5f9aad393e2a86310d96ad2cc1571a8091	optimized pattern-based adaptive mesh refinement using gpu	paper;silhouette refinement optimized pattern adaptive triangular mesh refinement graphical processing unit;mesh simplification;mesh generation computational geometry computer graphics;adaptive mesh refinement;computer graphics;computational geometry;triangular mesh;adaptive mesh refinement computer graphics indexing optimization methods parallel processing performance gain solid modeling image processing application software high performance computing;indexation;nvidia;algorithms;profitability;nvidia geforce 9600 gt;opengl;triangular meshes;computer science;mesh generation;high performance;3d graphics and realism	The high performance of GPUs and the increasing use of their programming mechanisms have fostered the development of graphics applications that better exploit the raw power of these devices to achieve higher levels of realism. Silhouette refinement, as one of the techniques that help to improve realism, has profited from GPUs' advances in recent years. In this paper, we present a method for triangular mesh refinement which alleviates the problem of rugged silhouettes. We demonstrate that, through a clever indexing scheme, our method is able to use adaptive patterns in an optimized way, taking full advantage of the GPU's parallelism. The ideas we used were adapted from distinct previous works, but our method presents astounding performance gains. Also, our method works very well with existing meshes, therefore, it can improve the visual appearance of existing models without mesh redesign.	adaptive mesh refinement;graphics processing unit;parallel computing;polygon mesh;rugged computer	Ricardo Lenz;Joaquim B. Cavalcante Neto;Creto Augusto Vidal	2009	2009 XXII Brazilian Symposium on Computer Graphics and Image Processing	10.1109/SIBGRAPI.2009.37	computational science;computer science;theoretical computer science;computer graphics (images)	Graphics	69.46324034789006	-51.10440028600938	65970
5dd983f858c6a9c9a80e67f194e7f2b8b849ce7b	phonon tracing for auralization and visualization of sound	frequency dependence;raytracing;low frequency;acoustics;front propagation;acoustic signal processing;finite element;color coded blobs phonon tracing sound auralization sound visualization particle tracing approach photon mapping finite response filter accounting frequency dependent absorption coefficients realistic aural impression wave front propagation;acoustic wave propagation phonons data visualisation realistic images ray tracing acoustic signal processing;particle tracing;phonons visualization computational modeling frequency layout acoustic propagation absorption computer simulation acoustic applications computer graphics;data visualisation;phonons;auralization;ray tracing;realistic images;absorption coefficient;photon mapping;high frequency;physical simulation;nite element;acoustic wave propagation	We present a new particle tracing approach for the simulation of mid- and high-frequency sound. Inspired by the photorealism obtained by methods like photon mapping, we develop a similar method for the physical simulation of sound within rooms. For given source and listener positions, our method computes a finite-response filter accounting for the different reflections at various surfaces with frequency-dependent absorption coefficients. Convoluting this filter with an anechoic input signal reproduces a realistic aural impression of the simulated room. We do not consider diffraction effects due to low frequencies, since these can be better computed by finite elements. Our method allows the visualization of a wave front propagation using color-coded blobs traversing the paths of individual phonons.	coefficient;dynamical simulation;finite element method;phonon;photon mapping;reflection (computer graphics);software propagation;sonification	Martin Bertram;Eduard Deines;Jan Mohring;Jevgenijs Jegorovs;Hans Hagen	2005	VIS 05. IEEE Visualization, 2005.	10.1109/VIS.2005.78	ray tracing;data visualization	Visualization	72.63591461850687	-48.999200141960735	66067
32b3b8fe0846f4c8723c22453ebf91bfb884fdbe	string kernels for complex time-series: counting targets from sensed movement	pulsed doppler radar string kernels complex time series sensed movement complex signals symbolic approach complex sax representation cstrings gaussian process regression framework fourier transforms complex kernels;time series fourier transforms gaussian processes regression analysis signal processing;kernel discrete fourier transforms time series analysis radar computed tomography vectors approximation methods;tk electrical engineering electronics nuclear engineering	Complex (imaginary) signals arise commonly in the field of communications in the form of time series in the complex space. In this work we propose a symbolic approach for such signals based on string kernels derived from a complex SAX representation and apply it to a challenging counting problem. Our approach, that we call cStrings, is within a Gaussian process regression framework and outperforms established Fourier transforms and complex kernels, achieving a correlation coefficient of 0.985 when predicting the number of targets sensed by a pulsed Doppler radar.	coefficient;counting problem (complexity);imaginary time;kriging;time series	Theodoros Damoulas;Jin He;Richard Bernstein;Carla P. Gomes;Anish Arora	2014	2014 22nd International Conference on Pattern Recognition	10.1109/ICPR.2014.758	mathematical optimization;speech recognition;machine learning;mathematics	ML	79.73494891778707	-40.166505236845396	66398
db1d3dc0dc68be67af70b05ed17bd9b9d75963a1	vibration detection method for optical fibre pre-warning system	intrusion signals;optical fibre vibration measurement;constant false alarm rate;rayleigh distribution;optical fibre prewarning system;cfar method;false alarm rate;vibration signals;interference;bha cfar;bha cfar method;go so cfar;vibration detection method;instrument;da gang oilfield;phase sensitive optical time domain reflectometer	The measurement of optical fibre vibration is a key part of optic fibre pre-warning system, which has gradually focused on phase-sensitive optical time-domain reflectometer. However, for this instrument, false alarm rate is very high and some unstable intrusion signals cannot be detected by using its fixed threshold method in the actual application. It needs to develop new vibration detection method to overcome the above defect. The vibration signals normally consist of three parts, that is, noise, interference and intrusion signals. After a large number of data analysis, the authors find that the system noise is time varying and follows the Rayleigh distribution. Hence, the authors innovatively use the constant false alarm rate (CFAR) method to detect this type of intrusion. Considering interference is also time varying and diverse, a good detection performance cannot be obtained only by using the conventional CFAR. For this reason, a background homogeneity adaptive CFAR (BHA-CFAR) method is further proposed to detect the vibration signals in this study. The BHA-CFAR consists of two detectors, cell averaging CFAR (CA-CFAR) detector and greatest-of/smallest-of CFAR (GO/SO-CFAR) detector. A parameter, homogeneity of background, is estimated first to classify the surrounding. Then CA-CFAR and GO/SO-CFAR are optionally used according to the surrounding is homogeneous or heterogeneous, respectively. This new detection method can adapt to any background surrounding and has a good detection performance. In order to check the feasibility and validity of the BHA-CFAR method, several experiments were carried out in Da Gang oilfield. The detection results show that the proposed method can provide a good tradeoff between the detection performance and computation time.	optical fiber	Hongquan Qu;Tong Zheng;Fukun Bi;Liping Pang	2016	IET Signal Processing	10.1049/iet-spr.2015.0562	electronic engineering;telecommunications;engineering;constant false alarm rate;mathematics;forensic engineering;statistics	ML	82.04677732184815	-41.758088869036534	66744
09dde72c3d0a86b18eb061c44bda7ac0fef3e139	signed area of sectors between spline curves and the origin	spline;computational geometry;b spline curve;splines mathematics;deformation;deformation computational geometry splines mathematics;area preserving deformations signed area spline curves sectors bezier curves b spline curve segments control points basis functions	Formulas for representing the signed area of sectors between B ezier and B-spline curve segments and the origin are presented. The area is expressed by using the coordinates of the control points and coe cients calculated with the basis functions of the spline curves. Area-preserving deformations of the spline curves by moving the control points are also investigated.	b-spline;basis function;control point (mathematics);origin;polynomial;spline (mathematics)	Kanji Ueda	1999		10.1109/IV.1999.781575	spline interpolation;spline;mathematical optimization;mathematical analysis;perfect spline;smoothing spline;cubic hermite spline;hermite spline;mathematics;geometry;thin plate spline;polyharmonic spline;flat spline;m-spline	Graphics	69.06684145055769	-40.51236233832841	66867
58136de0e74e896330e8b4b74a8cafdcc6ce2f66	as-killing-as-possible vector fields for planar deformation	i 3 5 computer graphics computational geometry and object modeling geometric algorithms three dimensional graphics and realism;i 3 7 animation	Cartoon animation, image warping, and several other tasks in two-dimensional computer graphics reduce to the formulation of a reasonable model for planar deformation. A deformation is a map from a given shape to a new one, and its quality is determined by the type of distortion it introduces. In many applications, a desirable map is as isometric as possible. Finding such deformations, however, is a nonlinear problem, and most of the existing solutions approach it by minimizing a nonlinear energy. Such methods are not guaranteed to converge to a global optimum and often suffer from robustness issues. We propose a new approach based on approximate Killing vector fields (AKVFs), first introduced in shape processing. AKVFs generate near-isometric deformations, which can be motivated as direction fields minimizing an “as-rigid-as-possible” (ARAP) energy to first order. We first solve for an AKVF on the domain given user constraints via a linear optimization problem and then use this AKVF as the initial velocity field of the deformation. In this way, we transfer the inherent nonlinearity of the deformation problem to finding trajectories for each point of the domain having the given initial velocities. We show that a specific class of trajectories — the set of logarithmic spirals — is especially suited for this task both in practice and through its relationship to linear holomorphic vector fields. We demonstrate the effectiveness of our method for planar deformation by comparing it with existing state-of-the-art deformation methods.	approximation algorithm;computer graphics;converge;distortion;global optimization;image warping;isometric projection;linear programming;map;mathematical optimization;nonlinear system;optimization problem;robustness (computer science);velocity (software development)	Justin Solomon;Mirela Ben-Chen;Adrian Butscher;Leonidas J. Guibas	2011	Comput. Graph. Forum	10.1111/j.1467-8659.2011.02028.x	computer vision;mathematical optimization;topology;computer science;mathematics;geometry;algorithm;computer graphics (images)	Graphics	69.08697366760984	-45.0132537014401	67512
a8b542992e685fdfcfba81e4760a0d5d6b5ba19b	stream surface parametrization by flow-orthogonal front lines	solid;curve;and object representations;i 3 5 computer graphics;i 3 5 computer graphics computational geometry and object modeling curve;computational geometry and object modeling;surface	The generation of discrete stream surfaces is an important and challenging task in scientific visualization, which can be considered a particular instance of geometric modeling. The quality of numerically integrated stream surfaces depends on a number of parameters that can be controlled locally, such as time step or distance of adjacent vertices on the front line. In addition there is a parameter that cannot be controlled locally: stream surface meshes tend to show high quality, well-shaped elements only if the current front line is “globally” approximately perpendicular to the flow direction. We analyze the impact of this geometric property and present a novel solution – a stream surface integrator that forces the front line to be perpendicular to the flow and that generates quaddominant meshes with well-shaped and well-aligned elements. It is based on the integration of a scaled version of the flow field, and requires repeated minimization of an error functional along the current front line. We show that this leads to computing the 1-dimensional kernel of a bidiagonal matrix: a linear problem that can be solved efficiently. We compare our method with existing stream surface integrators and apply it to a number of synthetic and real world data sets.	bidiagonal matrix;cylinder seal;discretization;display resolution;dynamic language runtime;geometric modeling;linear programming;neighbourhood (graph theory);numerical analysis;polygon mesh;robustness (computer science);sampling (signal processing);scientific visualization;streamsurface;synthetic intelligence	Maik Schulze;Tobias Germer;Christian Rössl;Holger Theisel	2012	Comput. Graph. Forum	10.1111/j.1467-8659.2012.03177.x	computer vision;computer science;artificial intelligence;theoretical computer science;mathematics;geometry;curve;solid;surface;algorithm;computer graphics (images)	Graphics	69.41559428803427	-44.530110225893885	68167
70cfd60cfddb5d82b9ff3c683d13f2f0f836bccf	radial basis function based level set interpolation and evolution for deformable modelling	numerical stability;partial differential equation;distance function;active contour;high dimensionality;ordinary differential equation;level set approach;level set;finite difference method;radial basis function;re initialisation free;surface model;deformable model;active contour model	a r t i c l e i n f o Keywords: Deformable model Level set Radial basis function Re-initialisation free We present a study in level set representation and evolution using radial basis functions (RBFs) for active contour and active surface models. It builds on recent works by others who introduced RBFs into level sets for structural topology optimisation. Here, we introduce the concept into deformable models and present a new level set formulation able to handle more complex topological changes, in particular perturbation away from the evolving front. In the conventional level set technique, the initial active contour/surface is implicitly represented by a signed distance function and periodically re-initialised to maintain numerical stability. We interpolate the initial distance function using RBFs on a much coarser grid, which provides great potential in modelling in high dimensional space. Its deformation is considered as an updating of the RBF interpolants, an ordinary differential equation (ODE) problem, instead of a partial differential equation (PDE) problem, and hence it becomes much easier to solve. Re-initialisation is found no longer necessary, in contrast to conventional finite difference method (FDM) based level set approaches. The proposed level set updating scheme is efficient and does not suffer from self-flattening while evolving, hence it avoids large numerical errors. Further, more complex topological changes are readily achievable and the initial contour or surface can be placed arbitrarily in the image. These properties are extensively demonstrated on both synthetic and real 2D and 3D data. We also present a novel active contour model, implemented with this level set scheme, based on multiscale learning and fusion of image primitives from vector-valued data, e.g. colour images, without channel separation or decomposition. Ever since Kass et al. [1] introduced the active contour or snake model, there has been a multitude of works on the development of active contour models, some theoretical and some tuned to certain applications. Traditional snakes suffer from several issues, such as limited capture range and difficulties in reaching concavities. The application of the level set method [2] to the active contour model has enabled the latter to adapt to complex topologies. It avoids the need to reparameterise the curve and the contours are able to split or merge in order to capture an unknown number of objects without resorting to dedicated contour tracking. However, the original level set based active contour [3] has proved to be of limited …	active contour model;algorithmic efficiency;color image;contour line;emoticon;finite difference method;grid computing;interpolation;mathematical optimization;numerical analysis;numerical stability;perturbation theory;radial (radio);radial basis function network;reaching definition;synthetic intelligence;topology optimization	Xianghua Xie;Majid Mirmehdi	2011	Image Vision Comput.	10.1016/j.imavis.2010.08.011	computer vision;mathematical optimization;mathematical analysis;computer science;machine learning;active contour model;mathematics;geometry;level set method	Vision	69.03167788212674	-45.5937831571382	68194
bcf0c5ca1f4def758caf91127f37e1841db39db3	hexahedral mesh generation for the simulation of the human mandible	fem simu- lation;quadrilateral mesh generation;hexahedral mesh generation;human mandible;optimized mesh smoothing;condition number;mesh generation;jacobian matrix	A combinatorial approach for the generation of hexahedral meshes by means of successive dual cycle elimination has been proposed by the second author in previous work. We provide a case study for the applicability of our hexahedral mesh generation approach to the simulation of physiological stress scenarios of the human mandible. Due to its complex and very detailed free-form geometry, the mandible model is very demanding. This test case is used as a running example to report on the progress and recent advances of the cycle elimination scheme. The given input data, a surface triangulation, requires a substantia l mesh reduction and a suitable conversion into a quadrilateral surface mesh as a first step, for which we use mesh clustering andb-matching techniques. Several strategies for improved cycle elimination orders are proposed. They lead to a significant reduction in the mesh size and a better structural quality. Based on the resulting combinatorial meshes, gradientbased optimized smoothing with the condition number of the Jacobian matrix as objective together with mesh untangling techniques yielded embeddings of a satisfactory quality. We tested our hexahedral meshes for the mandible model in an FEM simulation under the scenario of a bite on a “hard nut.” Our simulation results are in good agreement with observations from biomechanical experiments.	adaptive mesh refinement;algorithm;cluster analysis;condition number;display resolution;experiment;finite element method;gradient;hexahedron;jacobian matrix and determinant;line search;maximal set;mesh generation;newton;polygon mesh;quasi-newton method;refinement (computing);sane;simulation;smoothing;surface triangulation;test case;types of mesh	Cornelia Kober;Matthias Müller-Hannemann	2000			hexahedron;mathematical optimization;jacobian matrix and determinant;mesh generation;mathematics;condition number	Graphics	70.23757253269486	-44.995333658594525	68712
0dc0132b1dc961c7128165640a9c3070c1531880	a study of algorithms for detecting pulsed sinusoidal interference in microwave radiometry	gaussian noise;detectors;spectral kurtosis;gaussian field;detection probability;passive microwave;interference amplitude;detection algorithms;signal detection;short pulsed sinusoidal interference;random variables;interference;digital filter;microwave radiometry;radiofrequency interference;statistical properties;radiometry;monte carlo procedures;interference duration;expected value;radio frequency interference;geophysical signal processing;remote sensing;duty cycle;detection algorithm;narrowband interference;interference detection algorithm;interference phase;interference arrival time;interference frequency;radio astronomy;microwave radiometry radiofrequency interference frequency pulse measurements detection algorithms narrowband random variables extraterrestrial measurements fourier transforms algorithm design and analysis;microwave theory and techniques;monte carlo simulation;monte carlo procedures interference detection algorithm short pulsed sinusoidal interference microwave radiometry interference frequency interference phase interference amplitude interference arrival time interference duration detection probability;gaussian distribution;monte carlo methods;geophysical techniques;signal detection geophysical signal processing geophysical techniques monte carlo methods radiofrequency interference radiometry remote sensing	The performance of four algorithms for detecting the presence of short pulsed sinusoidal interference in microwave radiometry is compared. The pulsed sinusoidal interference sources considered have unknown frequency, initial phase, amplitude, arrival time, and duration. Statistical properties of three of the algorithms can be determined analytically, although numerical integrations are required in some cases in order to compute the obtained probabilities of detection. The performance of the fourth algorithm was evaluated using Monte Carlo procedures. Results show that three of the algorithms have a performance that is roughly comparable for the cases considered, while the fourth yields reduced sensitivity.	algorithm;interference (communication);microwave;monte carlo method;numerical analysis;sensor;statistical interference;time of arrival	Joel T. Johnson;Lee C. Potter	2008	IGARSS 2008 - 2008 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2008.4778952	radio astronomy;telecommunications;optics;physics;statistics;monte carlo method	Arch	80.84800321736856	-43.38353609612191	69142
a04191cc08810e4ba2a69e1a4c5679f2a5890b1c	real-time high-quality surface rendering for large scale particle-based fluids	particle splatting;normal estimation;real time rendering;ray casting;large scale fluids	Particle-based methods like Smoothed Particle Hydrodynamics (SPH) are increasingly adopted for large scale fluid simulation in interactive computer graphics. However, surface rendering for such dynamic particle sets is challenging: current methods either produce coarse results, or time consuming. We introduce a novel approach to render high-quality fluid surface in screen space by an efficient combination of particle splatting, ray-casting and surface normal estimation techniques. We apply particle splatting to accelerate ray-casting process, and estimate surface normal using Principal Component Analysis (PCA). We adopt GPU technique to further accelerate our method. Our method can produce high-quality smooth surface while preserving thin and sharp details of large scale fluids. The computation and memory cost of our rendering step only depend on the image resolution. These advantages make our method very suitable for previewing or rendering hundreds of millions particles interactively. We demonstrate the efficiency and effectiveness of our method by rendering various fluid scenarios with different-sized particle sets.	computation;fluid animation;glossary of computer graphics;graphics processing unit;human–computer interaction;image resolution;interactivity;normal (geometry);particle system;principal component analysis;ray casting;real-time clock;rendering (computer graphics);simulation;smoothed-particle hydrodynamics;smoothing	Xiangyun Xiao;Shuai Zhang;Xubo Yang	2017		10.1145/3023368.3023377	computer vision;simulation;3d rendering;rendering;computer science;ray casting;real-time rendering;computer graphics (images)	Graphics	69.41463787814533	-50.700981189126075	69285
b18aad6896892fb6c56c4129a9b78e6f85d445d2	fast simulation of detailed layered deformable objects in contact	deformable objects;expressive imagery;non realistic modeling	We present an efficient algorithm for simulating contacts between deformable bodies with high-resolution surface geometry using dynamic deformation textures , which reformulate the 3D elastoplastic deformation and collision handling on a 2D parametric atlas to reduce the extremely high number of degrees of freedom arising from large contact regions and high-resolution geometry. Such computationally challenging dynamic contact scenarios arise when objects with rich surface geometry are rubbed against each other while they bounce, roll or slide through the scene, as shown in Figure 1.	algorithm;image resolution;simulation	Nico Galoppo;Miguel A. Otaduy;Paul Mecklenburg;Markus H. Gross;Ming C. Lin	2006		10.1145/1179849.1179998	computer vision;simulation;computer graphics (images)	Robotics	70.23150386311818	-47.63420849262466	69583
e7166e15d778bf8ad1bab2871e5314e22895368a	on the smooth convergence of subdivision and degree elevation for bézier curves	concepcion asistida;computer aided design;piecewise linear;curva bezier;piecewise linear approximation;ajustamiento curva;approximation;degree elevation;courbe bezier;linearisation morceau;divided difference;derivee;linearizacion trozo;conception assistee;ajustement courbe;subdivision;curve fitting;derivada;piecewise linearization;derivative;bezier curve	Bezier subdivision and degree elevation algorithms generate piecewise linear approximations of Bezier curves that converge to the original Bezier curve. Discrete derivatives of arbitrary order can be associated with these piecewise linear functions via divided differences. Here we establish the convergence of these discrete derivatives to the corresponding continuous derivatives of the initial Bezier curve. Thus, we show that the control polygons generated by subdivision and degree elevation provide not only an approximation to a Bezier curve, but also approximations of its derivatives of arbitrary order.	bézier curve;subdivision surface	Géraldine Morin;Ron Goldman	2001	Computer Aided Geometric Design	10.1016/S0167-8396(01)00059-0	mathematical optimization;topology;piecewise linear function;derivative;computer aided design;subdivision;approximation;bézier curve;mathematics;geometry;divided differences;statistics;curve fitting	Theory	68.80476377285905	-40.304558353241426	69811
0acb69e3bc6c57b7c6b11abed95d64a087a8bdf7	interactive virtual materials	elasticity;large deformations;real time;spectrum;finite element method;physically based animation;finite element;plasticity;fracture;linear elasticity;multi resolution;stiffness warping;tetrahedral mesh	In this paper we present a fast and robust approach for simulating elasto-plastic materials and fracture in real time. Our method extends the warped stiffness finite element approach for linear elasticity and combines it with a strain-state-based plasticity model. The internal principal stress components provided by the finite element computation are used to determine fracture locations and orientations. We also present a method to consistently animate and fracture a detailed surface mesh along with the underlying volumetric tetrahedral mesh. This multi-resolution strategy produces realistic animations of a wide spectrum of materials at interactive rates that have typically been simulated off-line thus far.	algorithm;computation;elasticity (cloud computing);elasticity (data store);finite element method;image resolution;online and offline;real-time clock;simulation;stiffness matrix;switzerland	Matthias Müller;Markus H. Gross	2004			simulation;extended finite element method;computer science;finite element method;mixed finite element method	Graphics	69.16872908107386	-47.70850733834777	69939
d6b9a39349de5288f9a08e596b3a3b359b4ef99b	hilbert spectral analysis of vowels using intrinsic mode functions	spectrogram speech wideband time frequency analysis harmonic analysis visualization speech analysis;signal analysis;speech analysis;speech analysis hilbert space signal analysis;pathological speakers vowels intrinsic mode functions mathematical theory time frequency analysis nonstationary signals complex am fm components instantaneous amplitude instantaneous frequency hilbert spectral analysis approach hsa approach signal structure wideband spectrograms narrowband spectrograms intraglottal pulse phenomena fine scale analysis speech based medical diagnosis automatic speech recognition asr;time frequency analysis amplitude modulation frequency modulation hilbert spaces natural language processing spectral analysis speech recognition;hilbert space	In recent work, we presented mathematical theory and algorithms for time-frequency analysis of non-stationary signals. In that work, we generalized the definition of the Hilbert spectrum by using a superposition of complex AM-FM components parameterized by the Instantaneous Amplitude (IA) and Instantaneous Frequency (IF). Using our Hilbert Spectral Analysis (HSA) approach, the IA and IF estimates can be far more accurate at revealing underlying signal structure than prior approaches to time-frequency analysis. In this paper, we have applied HSA to speech and compared to both narrowband and wideband spectrograms. We demonstrate how the AM-FM components, assumed to be intrinsic mode functions, align well with the energy concentrations of the spectrograms and highlight fine structure present in the Hilbert spectrum. As an example, we show never before seen intra-glottal pulse phenomena that are not readily apparent in other analyses. Such fine-scale analyses may have application in speech-based medical diagnosis and automatic speech recognition (ASR) for pathological speakers.	algorithm;align (company);fm broadcasting;frequency analysis;heterogeneous system architecture;hilbert spectral analysis;hilbert spectrum;instantaneous phase;spectrogram;speech recognition;stationary process;time–frequency analysis	Steven Sandoval;Phillip L. De Leon;Julie M. Liss	2015	2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)	10.1109/ASRU.2015.7404846	speech recognition;computer science;hilbert–huang transform;signal processing;mathematics;hilbert spectral analysis;hilbert space	ML	79.79554884689595	-39.56858012735802	70499
3d4644368366e5e72b4b7e078b1119aea71bf852	adaptive structured recovery of compressive sensing via piecewise autoregressive modeling	matching pursuit algorithms;inverse problem compressive sensing adaptive modeling autoregressive process;biomedical measurements;compressed sensing;spatial domain;locally adaptive signal dependent spaces;image coding image reconstruction discrete cosine transforms inverse problems matching pursuit algorithms counting circuits statistics autoregressive processes adaptive signal processing extraterrestrial measurements;reconstruction quality;cs recovery;adaptive structured recovery;cs solution space;autoregressive process;adaptive modeling;autoregressive model;higher order statistics;signal reconstruction autoregressive processes higher order statistics;local adaptation;sparse space;cs measurements;adaptation model;inverse problem;autoregressive processes;compressive sensing;model guided adaptive recovery;marx framework;pixel;signal structures;reconstruction quality adaptive structured recovery compressive sensing piecewise autoregressive modeling natural signals sparse space time domain spatial domain cs recovery locally adaptive signal dependent spaces cs measurements signal structures model guided adaptive recovery marx framework second order statistics structured sparsities cs solution space cs acquired images;time domain;signal reconstruction;tv;natural signals;second order statistics;piecewise autoregressive modeling;cs acquired images;structured sparsities;inverse problems	In compressive sensing (CS) a challenge is to find a space in which the signal is sparse and hence recoverable faithfully and efficiently. Given the nonstationarity of many natural signals such as images, the sparse space varies in time/spatial domain. As such, CS recovery should be conducted in locally adaptive, signal-dependent spaces to counter the fact that the CS measurements are global and irrespective of signal structures. On the contrary most CS methods seek for a fixed set of bases (e.g., wavelets, DCT, and gradient spaces) for the entirety of a signal. To rectify this problem we propose a new framework for model-guided adaptive recovery of compressive sensing (MARX), and show how a piecewise autoregressive model can be integrated into the MARX framework to adapt to changing second order statistics of a signal in CS recovery. In addition, MARX offers a powerful mechanism of characterizing and exploiting structured sparsities of a signal, greatly restricting the CS solution space. A case study on CS-acquired images shows that the proposed MARX technique can increase the reconstruction quality by up to 8 dB over existing methods.	autoregressive model;compressed sensing;decibel;discrete cosine transform;exploit (computer security);feasible region;schedule (computer science);sparse matrix;wavelet	Xiaolin Wu;Xiangjun Zhang	2010	2010 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2010.5495811	computer vision;mathematical optimization;speech recognition;computer science;inverse problem;machine learning;mathematics;autoregressive model;compressed sensing;statistics	Robotics	78.77976380334489	-42.22655798935569	70563
821a17bd4c1f723cb5a66358a99dce895f19917f	generation of tree movement sound effects	moving trees;sound modelling;computer animation;natural phenomena;sound rendering	Abstract#R##N##R##N#This paper presents a method for automatically generating sound effects for an animation of branches and leaves moving in the wind. Each tree is divided into branches and leaves, and an independent sound effect generation process is employed for each element. The individual results are then compounded into one sound effect. For the branches, we employ an approach based on the frequencies of experimentally obtained Karman vortex streets. For the leaves, we use the leaf blade state as the input and assume a virtual musical instrument that uses wave tables as the sound source. All computations can be performed independently for each frame step. Therefore, each frame step can be executed on completion of the animation step. The results of the implementation of the approach are presented and it is shown that the process offers the possibility of real-time operation through the use of parallel computing techniques. Copyright © 2005 John Wiley & Sons, Ltd.		Katsutsugu Matsuyama;Tadahiro Fujimoto;Kazunobu Muraoka;Norishige Chiba	2005	Journal of Visualization and Computer Animation	10.1002/cav.58	physical modelling synthesis;simulation;speech recognition;computer science;artificial intelligence;computer animation;computer graphics (images)	Visualization	72.56547606084567	-48.924526155988346	71018
750c67ea00a880cfe5678aa38186698b7fca6d38	optimal multi-degree reduction of bézier curves with g2-continuity	minimisation;modelizacion;utilisation information;concepcion asistida;minimization;computer aided design;uso informacion;degree reduction;tecnologia electronica telecomunicaciones;curva bezier;singularite;computacion informatica;information use;bezier curves;aproximacion optima;minimizacion;courbure;fonction objectif;objective function;modelisation;optimal approximation;approximation optimale;courbe bezier;mathematical programming;ciencias basicas y experimentales;l 2 norm;singularidad;conception assistee;curvatura;funcion objetivo;curvature;tecnologias;grupo a;g2 continuity;modeling;programmation mathematique;l2 norm;programacion matematica;singularity;bezier curve	This paper presents a novel approach to consider optimal multi-degree reduction of Bézier curve with G-continuity. By minimizing the distances between corresponding control points of the two curves through degree raising, optimal approximation is achieved. In contrast to traditional methods, which typically consider the components of the curve separately, we use geometric information on the curve to generate the degree reduction. So positions and tangents are preserved at the two endpoints. For satisfying the solvability condition, we propose another improved algorithm based on regularization terms. Finally, numerical examples demonstrate the effectiveness of our algorithms.	algorithm;approximation error;bézier curve;communication endpoint;degree (graph theory);euclidean distance;geometric median;loss function;numerical analysis;numerical method;optimization problem;scott continuity	Lizheng Lu;Guozhao Wang	2006	Computer Aided Geometric Design	10.1016/j.cagd.2006.09.002	topology;computer aided design;calculus;bézier curve;mathematics;geometry	EDA	69.18759112019491	-39.80448010077322	71294
eb2d7d55cda4b612d241de10ada3465cd6bc9fa9	a constrained resampling strategy for mesh improvement		In many geometry processing applications, it is required to improve an initial mesh in terms of multiple quality objectives. Despite the availability of several mesh generation algorithms with provable guarantees, such generated meshes may only satisfy a subset of the objectives. The conflicting nature of such objectives makes it challenging to establish similar guarantees for each combination, e.g., angle bounds and vertex count. In this paper, we describe a versatile strategy for mesh improvement by interpreting quality objectives as spatial constraints on resampling and develop a toolbox of local operators to improve the mesh while preserving desirable properties. Our strategy judiciously combines smoothing and transformation techniques allowing increased flexibility to practically achieve multiple objectives simultaneously. We apply our strategy to both planar and surface meshes demonstrating how to simplify Delaunay meshes while preserving element quality, eliminate all obtuse angles in a complex mesh, and maximize the shortest edge length in a Voronoi tessellation far better than the state-of-the-art.		Ahmed Abdelkader;Ahmed H. Mahmoud;Ahmad A. Rushdi;Scott A. Mitchell;John D. Owens;Mohamed S. Ebeida	2017	Comput. Graph. Forum	10.1111/cgf.13256	theoretical computer science;computer science;voronoi diagram;smoothing;computational geometry;polygon mesh;mesh generation;mathematical optimization;delaunay triangulation;geometry processing;t-vertices	Graphics	68.72386494486823	-44.61913699404992	72025
27dc3ccb7e38dc1b2df03af690e4da57b50c7b60	cubic spline approximation of a circle with maximal smoothness and accuracy	high accuracy approximation of circles;geometric smoothness;cubic spline;bezier curve	Abstract We construct cubic spline approximations of a circle which are four times continuously differentiable and converge with order six.	approximation;converge;cubic hermite spline;cubic function;maximal set;spline (mathematics)	Christian Apprich;A. Dieterich;Klaus Höllig;Esfandiar Nava Yazdani	2017	Computer Aided Geometric Design	10.1016/j.cagd.2017.05.001	spline interpolation;spline;mathematical optimization;mathematical analysis;perfect spline;cubic form;smoothing spline;monotone cubic interpolation;cubic hermite spline;hermite spline;bézier curve;mathematics;geometry;thin plate spline;flat spline;mechanical engineering	Theory	69.51983636527912	-40.378321808023294	72232
fdbad4db85c3ce8c2ac4af071c4f8f985452f88c	design and testing of an active 190-ghz millimeter-wave imager	extremely high frequency;imaging system;field of view;antennas;millimeter wave;ionizing radiation;imaging systems	The design and testing of an active 190-GHz imaging system is presented. The system features two beam-scanning antennas, one of which transmits a vertical fan beam, and the other which receives a horizontal fan beam. By correlating the transmitted and received signals, an output is obtained that is proportional to the millimeter-wave reflectivity at the intersection of the two fan beams. Beam scanning is obtained by rotating a small subreflector within each antenna, allowing rapid scanning. The system has an angular resolution of 0.3 deg, a field of view of 14×14 deg, and operates at a standoff distance of 5 m. © 2010 SPIE and IS&T. [DOI: 10.1117/1.3514744]	angularjs;degree (graph theory);image sensor	Greg P. Timms;Michael L. Brothers;John D. Bunton;John W. Archer;Grahame C. Rosolen;Yue Li;Andrew D. Hellicar;Juan Y. Tello;Stuart G. Hay	2010	J. Electronic Imaging	10.1117/1.3514744	telecommunications;extremely high frequency	Robotics	77.8678343548948	-48.801294628775906	72691
9442ec3f9f7b1a1ee10cc20b8ea3a66c95e79055	solid modeling of polyhedral objects by layered depth-normal images on the gpu	complex objects;parallel algorithm;solid modeler;gpu;hardware accelerator;layered depth normal images;boolean operation;graphics hardware;polygonal meshes;solid modeling;feature preservation;parallel computer;graphic processing unit;finite element analysis	We introduce a novel solid modeling framework taking advantage of the architecture of parallel computing onmodern graphics hardware. Solidmodels in this framework are represented by an extension of the ray representation — Layered Depth-Normal Images (LDNI), which inherits the good properties of Boolean simplicity, localization and domain decoupling. The defect of ray representation in computational intensity has been overcome by the newly developed parallel algorithms running on the graphics hardware equipped with Graphics Processing Unit (GPU). The LDNI for a solid model whose boundary is representedby a closedpolygonalmesh canbe generated efficientlywith thehelp of hardware accelerated sampling. The parallel algorithm for computing Boolean operations on two LDNI solids runs well on modern graphics hardware. A parallel algorithm is also introduced in this paper to convert LDNI solids to sharp-feature preserved polygonal mesh surfaces, which can be used in downstream applications (e.g., finite element analysis). Different from those GPU-based techniques for rendering CSG-tree of solid models Hable and Rossignac (2007, 2005) [1,2], we compute and store the shape of objects in solid modeling completely on graphics hardware. This greatly eliminates the communication bottleneck between the graphics memory and the main memory. © 2010 Elsevier Ltd. All rights reserved.	3d acis modeler;cgal;computer data storage;conditional (computer programming);constructive solid geometry;coupling (computer programming);downstream (software development);finite element method;graphics hardware;graphics processing unit;open-source software;parallel algorithm;parallel computing;polygon mesh;polyhedron;prototype;pseudocode;sampling (signal processing);software bug;solid modeling;traffic collision avoidance system	Charlie C. L. Wang;Yuen-Shan Leung;Yong Chen	2010	Computer-Aided Design	10.1016/j.cad.2010.02.001	graphics pipeline;parallel computing;hardware acceleration;computer science;theoretical computer science;polygonal modeling;finite element method;real-time computer graphics;parallel algorithm;solid modeling;computer graphics;graphics hardware;constructive solid geometry;general-purpose computing on graphics processing units;computer graphics (images)	Graphics	68.72175036978858	-49.763043212086195	73173
3befd4bc536c15b4c96d96b0d6efbc969369c5b6	efficient algorithms for non-rational and rational bézier curves	nonrational curves;nonrational drawing rational bezier curves linear computational complexity nonrational curves;complexity theory;rational curve;computer graphics;efficient algorithm;linear complexity;curve;computational complexity curve;visualization;computational modeling;computational complexity;imaging;linear computational complexity;rational functions;rational functions computational complexity;complexity theory algorithm design and analysis computational modeling visualization imaging computer graphics conferences;rational bezier curves;algorithm design and analysis;nonrational drawing;conferences;bezier curve	In this paper, a new basis with linear computational complexity has been introduced and used to form non-rational and rational curves. Two algorithms for computing points on non-rational and rational proposed curves are expressed with their linear complexity. Moreover, the relationships between these proposed curves and the Bezier curves, for both non-rational and rational forms, are given by using polar form and homogeneous coordinate approaches. Consequently, two efficient algorithms with linear complexity have been introduced to be used in drawing non-rational and rational Bezier curves.	algorithm;bézier curve;computational complexity theory;linear programming	Natasha Dejdumrong	2008	2008 Fifth International Conference on Computer Graphics, Imaging and Visualisation	10.1109/CGIV.2008.62	mathematical optimization;combinatorics;discrete mathematics;geometric design;non-uniform rational b-spline;mathematics;family of curves	Vision	71.02935206832522	-41.40545444395416	74366
9a229dc82dadcd0123858ad8c6a1260a61328c9b	model selection for parametric surfaces approximating 3d point clouds for deformation analysis		Deformation monitoring of structures is a common application and one of the major tasks of engineering surveying. Terrestrial laser scanning (TLS) has become a popular method for detecting deformations due to high precision and spatial resolution in capturing a number of three-dimensional point clouds. Surface-based methodology plays a prominent role in rigorous deformation analysis. Consequently, it is of great importance to select an appropriate regression model that reflects the geometrical features of each state or epoch. This paper aims at providing the practitioner some guidance in this regard. Different from standard model selection procedures for surface models based on information criteria, we adopted the hypothesis tests from D.R. Cox and Q.H. Vuong to discriminate statistically between parametric models. The methodology was instantiated in two numerical examples by discriminating between widely used polynomial and B-spline surfaces as models of given TLS point clouds. According to the test decisions, the B-spline surface model showed a slight advantage when both surface types had few parameters in the first example, while it performed significantly better for larger numbers of parameters. Within B-spline surface models, the optimal one for the specific segment was fixed by Vuong’s test whose result was quite consistent with the judgment of widely used Bayesian information criterion. The numerical instabilities of B-spline models due to data gap were clearly reflected by the model selection tests, which rejected inadequate B-spline models in another numerical example.	3d scanner;b-spline;bayesian information criterion;epoch (reference date);model selection;numerical analysis;point cloud;polynomial;sensor;terrestrial television	Xin Zhao;Boris Kargoll;Mohammad Omidalizarandi;Xiangyang Xu;Hamza Alkhatib	2018	Remote Sensing	10.3390/rs10040634	parametric model;computer vision;point cloud;geology;regression analysis;artificial intelligence;mathematical optimization;statistical hypothesis testing;model selection;b-spline;deformation monitoring;bayesian information criterion	Vision	71.68760211626211	-38.19634175496376	74588
3124a045585edb1ac50faae128540260f773505b	a beamforming particle filter for eeg dipole source localization	dipole source localization;band pass filters;array signal processing particle filters electroencephalography psychology multiple signal classification filtering brain modeling spatial resolution magnetic resonance imaging head;source localization;beamforming particle filtering;array signal processing;optimum linear data independent filter beamforming particle filtering eeg dipole source localization electroencephalogram noise elimination;particle filtering numerical methods array signal processing electroencephalography interference suppression medical signal processing;particle filter beamforming dipole source localization eeg;interference suppression;brain modeling;noise elimination;particle filter;eeg;beamforming;electroencephalography;signal to noise ratio;electroencephalogram;eeg dipole source localization;medical signal processing;optimum linear data independent filter;particle filtering numerical methods;noise	Recently we have developed a method for electroencephalogram (EEG) dipole source localization based on particle filtering (PF). In this study the method is combined with beamforming to eliminate the noise which is spatially uncorrelated with the desired signal and accordingly to improve its performance. The proposed beamforming is an optimum, linear and data independent filter which can be applied to stationary as well as non-stationary data. Simulation and real data results have been provided to show its better performance over PF and beamforming approaches for dipole source localization.	beamforming;electroencephalography;particle filter;simulation;stationary process	Hamid Reza Mohseni;Foad Ghaderi;Edward L. Wilding;Saeid Sanei	2009	2009 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2009.4959589	speech recognition;electroencephalography;computer science;wsdma	Robotics	81.91015340499897	-38.84218972712833	74665
b4c1d8e3046500b982fc9f923d9d69d82529c5af	efficient subdivision of finite-element datasets into consistent tetrahedra	efficient subdivision;mesh subdivision;isosurfaces metrics;consistent tetrahedron;volume rendering;tetrahedralization;flow visualization;irregular grids;finite-element datasets;hexahedra;finite element analysis;finite element methods;finite element;pyramids;tetrahedra;prisms	The paper discusses the problem of subdividing unstructured mesh topologies containing hexahedra, prisms, pyramids and tetrahedra into a consistent set of only tetrahedra, while preserving the overall mesh topology. Efficient algorithms for volume rendering, iso-contouring and particle advection exist for mesh topologies comprised solely of tetrahedra. General finite-element simulations however, consist mainly of hexahedra, and possibly prisms, pyramids and tetrahedra. Arbitrary subdivision of these mesh topologies into tetrahedra can lead to discontinuous behaviour across element faces. This will show up as visible artifacts in the iso-contouring and volume rendering algorithms, and lead to impossible face adjacency graphs for many algorithms. The authors present various properties of tetrahedral subdivisions, and an algorithm SOP determining a consistent subdivision containing a minimal set of tetrahedra.	algorithm;hexahedron;mesh networking;pyramid (geometry);simulation;subdivision surface;volume rendering	Guy Albertelli;Roger Crawfis	1997	Proceedings. Visualization '97 (Cat. No. 97CB36155)	10.1145/266989.267062	mesh generation;mathematical optimization;finite element method;mathematics;geometry;triangulation	Visualization	69.41749703174985	-44.25017764076038	75219
0961546825d95fa33b8606f399f4e9a17fb54830	polarimetric detection and range estimation of a point-like target	detectors;clutter;radar polarimetry covariance matrices gaussian processes object detection radar clutter radar detection;estimation;covariance matrices;radar polarimetry;radar detection;radar detector point like target estimation polarimetric diversity detection gaussian clutter covariance matrix adaptive detector generalized likelihood ratio test wald test receiver constant false alarm rate property;detectors clutter covariance matrices estimation radar detection radar polarimetry	In this paper, we deal with the problem of polarimetric diversity detection for point-like targets in the presence of Gaussian clutter with unknown covariance matrix. To this end, we jointly exploit the polarization diversity and the spillover of target energy to consecutive range samples to improve the performances of detection and range localization. For estimation purposes, we assume that a set of secondary data (free of signal components) is available with the same covariance matrix as the clutter in the cells under test. Because the uniformly most powerful test does not exist for this problem, we derive two adaptive detectors: the generalized likelihood ratio test and the Wald test. Interestingly, these new receivers ensure the constant false alarm rate property with respect to covariance matrix of the clutter. The performance assessments conducted on both simulated data and real recorded dataset reveal that the proposed detectors outperform, in both detection and localization, the traditional state-of-the-art counterparts that ignore either the polarimetry or the spillover.	clutter;constant false alarm rate;knowledge spillover;performance;polarimetry;sensor	Chengpeng Hao;Saeed Gazor;Xiaochuan Ma;Shefeng Yan;Chaohuan Hou;Danilo Orlando	2016	IEEE Transactions on Aerospace and Electronic Systems	10.1109/TAES.2015.140657	stationary target indication;estimation;detector;continuous-wave radar;electronic engineering;constant false alarm rate;mathematics;clutter;statistics;remote sensing	Vision	79.3184876282783	-48.05610183905362	75308
29d8b461fd6f5ef2aa828152e39fa695f6642217	enhancing the visualization of characteristic structures in dynamical systems	fixed point;phase space;dynamic system	We present a thread of streamlets as a new technique to visualize dynamical systems in three-space. A trade-oo is made between solely visualizing a mathematical abstraction through lower-dimensional manifolds, i.e., characteristic structures such as xed point, separatrices, etc., and directly encoding the ow through stream lines or stream surfaces. Bundlers of streamlets are selectively placed near characteristic trajectories. An overpopulation of phase space with occlusion problems as a consequence is omitted. On the other hand, information loss is minimized since characteristic structures of the ow are still illustrated in the visualization.	dynamical system	Helwig Löffelmann;Eduard Gröller	1998		10.2312/vissym19981005	discrete mathematics;topology;mathematics;geometry	Visualization	72.20764482742668	-44.951708133270074	75519
cdbd827b14db5c1e301bb2864414233a09357a79	interference detection for direct tool path generation from measured data points	interference detection;nurbs;industrial application;end milling;tool path generation;reverse engineering	One of the main issues of the reverse engineering (RE) is the duplication of an existing physical part whose geometric information is partially or completely unavailable in measured form. In some industrial applications, physical parts are duplicated using three-axis CNC machines and ball-end mills. Many researches studied the problem of direct tool path generation from measured data point. However, up to the present, it appears that there is no reported study on interference detection in paths generated directly from measured data points. Interference detection is a curial problem in direct tool path generation from measured data points. This paper discusses the problem of local and global interference detection for three-axis machining in RE and proposes algorithms for local and global interference detection. With these algorithms, the measured data points captured from a physical part are analyzed and classified according to the shapes of the part. The method has been tested with several industrial parts, and it is shown to be robust and efficient especially for the part with free-form surfaces.	algorithm;apache axis;concave function;cutter expansive classification;data point;freeform surface modelling;interference (communication);optic axis of a crystal;reverse engineering;robustness (computer science)	D. Y. Li;Y. H. Peng;Zenshan Yin	2006	Engineering with Computers	10.1007/s00366-006-0027-9	simulation;non-uniform rational b-spline;computer science;engineering;engineering drawing;reverse engineering	Robotics	70.39436754171078	-38.3271914234805	75732
b77082a931c01291084ec00ceb2632e906fcc74d	shape modification of b-spline curves via constrained optimization for multi-target points	constrained optimization;splines mathematics computational geometry curve fitting optimisation;spline;optimisation;spline constraint optimization shape control optimal control surface topography surface reconstruction computer graphics computational geometry weight control interpolation;computer graphics;computational geometry;shape modification;surface reconstruction;satisfiability;data mining;b spline curve;surface topography;splines mathematics;constrained optimization computer graphics shape modification b spline curve;computer graphic;shape;cagd;optimization;multitarget point b spline curve computer graphics computational geometry cagd shape modification constrained optimization;curve fitting;geometric constraints;multitarget point	The B-spline curve is one of the most famous curves in computer graphics, computational geometry and CAGD. Developing more convenient techniques for designing and modifying B-spline curves is an important problem, and is also an important research issue. In this study, the shape modification of B-spline curves by geometric constraints is presented. A new method based on changing the control points of the curves via constrained optimization is proposed. To satisfy the given constraints, the shape of the curves is optimally modified with adjusting control points of the original B-spline curves. Finally, numerical examples are also given.	b-spline;computational geometry;computer graphics;computer-aided design;constrained optimization;control point (mathematics);mathematical optimization;numerical analysis;numerical method;shadow volume;spline (mathematics)	Qingbiao Wu;Shuyi Tao	2009	2009 WRI World Congress on Computer Science and Information Engineering	10.1109/CSIE.2009.668	spline;mathematical optimization;constrained optimization;combinatorics;geometric design;surface reconstruction;computational geometry;shape;mathematics;geometry;family of curves;computer graphics;curve fitting;satisfiability	Graphics	70.11263871720323	-40.84328411268254	75778
d9a9b453af85cf9100c6d9a43cfc62a08266a2ec	mass density laplace-spectra for image recognition	graph theory;image recognition;feature detection;incrementally modular abstraction hierarchy;abstraction hierarchy;three dimensions;handle loops;abstract data types;computational geometry;data processing;cellular data system;graphs;closed surface;application function;data analysis;cyberworlds;equivalence relation;graphs handle loops tunnel loops closed surface;tunnel loops;shape joining processes topology embedded computing sun character generation computer vision algorithm design and analysis greedy algorithms geometry;graph theory computational geometry;process algebra;unit calculation algebra	Modern multimedia applications generate vast amounts of image data. With the availability of cheap photo hardware and affordable rendering software even more such data is being collected. In order to manage huge collections of image data one needs short representations of the data sets, or to be more precise invariant features being appropriate to identify a specific voxel data set using just a few numbers. This paper describes a variation of a method introduced by Reuter, Wolter and Peinecke based on the computation of the spectrum of the Laplace operator for the image for generating an invariant feature vector - a fingerprint. Oppose to previous techniques interpreting the image as a height function we make use of the representation of the image as a density function. We discuss the use of the spectrum of eigenvalues of the Laplace mass density operator as a fingerprint and show the usability of this approach in several cases. Instead of using the discrete Laplace-Kirchhoff operator the approach presented in this paper is based on the continuous Laplace operator allowing better results in comparing the resulting spectra and deeper insights into the problems arising when comparing two spectra generated using discrete Laplacians.	computation;computer vision;density matrix;feature vector;fingerprint;kirchhoff's theorem;list of 3d rendering software;rca spectra 70;usability;voxel	Niklas Peinecke;Franz-Erich Wolter	2007	2007 International Conference on Cyberworlds (CW'07)	10.1109/CW.2007.49	three-dimensional space;combinatorics;process calculus;discrete mathematics;data processing;computational geometry;computer science;graph theory;theoretical computer science;machine learning;feature detection;mathematics;geometry;graph;equivalence relation;data analysis;surface;abstract data type	Visualization	75.14078221194904	-44.158467301022085	76078
12c81e9cdb268d22ac2bf2c906b4fed876aaf6a6	synchronous enhancement of periodic transients on polar diagram for machine fault diagnosis	polar diagram;synchronous enhancement;wavelet transform;fault diagnosis	A periodic transient detection and representation method, termed synchronous enhancement of periodic feature (SEPF), based on the Continuous Wavelet Transform (CWT) is proposed. This method first represents a given signal in the time-scale domain through CWT and then maps the resulted wavelet coefficients into the polar diagram using the given potential periods. Transient features would thus be synchronously enhanced at a certain area if the mapping time period matches that of the transient vibrations. The simulation study and the applications in the gearbox and bearing fault diagnosis verify the effectiveness of the proposed method in detecting the periodical transient vibrations.	coefficient;complex wavelet transform;continuous wavelet;diagram;fault detection and isolation;map;rotary system;rotary woofer;sensor;simulation;synchronization (computer science)	Zhongkui Zhu;Zhiyong He;Anzhu Wang;Shibin Wang	2009	IJWMIP	10.1142/S0219691309003008	mathematical analysis;real-time computing;mathematics;wavelet transform	EDA	77.98370220883224	-38.71310651388437	76348
eb324cf57bf5b01d21208bfac2b7c52ae9a56b18	wiener splines	spline;interpolation;identity based encryption;computer graphics;matrix decomposition;signal processing;wiener filter;computer science;signal processing algorithms;read only memory	"""We describe an alternative way of constructing interpolating B-spline curves, surfaces or volumes in Fourier space which can be used for visualization. In our approach the interpolation problem is considered from a signal processing point of view and is reduced to finding an inverse B-spline filter sequence. The Fourier approach encompasses some advantageous features, such as successive approximation, compression, fast convolution and hardware support. In addition, optimal Wiener filtering can be applied to remove noise and distortions from the initial data points and to compute a smooth, least-squares fitting """"lq Wiener spline"""". Unlike traditional fitting methods, the described algorithm is simple and easy to implement. The performance of the presented method is illustrated by some examples showing the restoration of surfaces corrupted by various types of distortions."""	algorithm;approximation;b-spline;circuit restoration;convolution;data point;distortion;fast fourier transform;inverse filter;least squares;minimum phase;signal processing;spectral density estimation;spline (mathematics);spline interpolation;wiener filter;wiener–khinchin theorem	Markus H. Gross;David Kleiner	1997	Scientific Visualization Conference (dagstuhl '97)		mathematical optimization;discrete mathematics;deconvolution;theoretical computer science;mathematics;wiener filter;wiener deconvolution	Visualization	74.04563816615047	-40.677384416070524	76599
6c6a63dcefa5571f4d5efb6da8e7355ea1abf851	autonomous surveying boat		The fresh water reservoirs are one of the main power resources of Pakistan. These water reservoirs are in the form of Tarbela Dam, Mangla Dam, Bhasha Dam, Warsak Dam etc.[1]. To estimate the current power capability of the Dams, the statistical information about the water in the dam has to be clear. For the purpose of water management monthly or yearly survey of the dams required. One of the important parameter is to find the water level of the water, which can help us in finding the pressure and flow of water in dams. The existing surveying systems have some problems, i.e., risky, errors in measurement and sometimesexpensive. Our project has tried a lot to overcome these flaws and to develop more economical, safe and accurate system for finding depth values of dams and ponds. The key purpose of Our Project “Autonomous Surveying Boat” is to have it log water depths along a predefined set of waypoints. The Autonomous Surveying Boat float in water according to pre-defined path, getting the coordinates from GPS Sensor and direction is controlled by using Magnetometer (compass) Sensor. It stores its data on SD card as a text file for later readings. The boat can also be used to find the average capacity of the dam. The average depth is calculated from the measured depth values at different set points of the dam. The actual length of the dam is determined by the magnetometer. The numbers of surveys over the time can help us in finding the silting ratio in dams. For square dams the length and width of the damare measured and the average depth, then using these three parameters we can estimate the average capacity of the dam [2]. The boat is scalable for furthered modification if needed.	global positioning system;glossary of computer graphics;scalability;secure digital;waypoint	Sajid Ullah	2014	CoRR		engineering;civil engineering;geotechnical engineering	Mobile	81.09433375570028	-51.775765445354985	77221
3858fc5d167e61c7e6d55d80062868836dec8352	combining topological simplification and topology preserving compression for 2d vector fields	topology skeleton switches data visualization smoothing methods computer graphics application software piecewise linear techniques vectors;image coding;critical point;topological complexity;image coding solid modelling vector quantisation;compression ratio;topology preservation;vector field;vector quantisation;2d visualization topological simplification topology preserving compression 2d vector fields vector field critical points separatrices algorithm flow data set compression ratio;solid modelling	Topological simplification techniques and topology preserving compression approaches for 2D vector fields have been developed quite independently of each other. In this paper we propose a combination of both approaches: a vector field should be compressed in such a way that its important topological features (both critical points and separatrices) are preserved while its unimportant features are allowed to collapse and disappear. To do so, a number of new solutions and modifications of pre-existing algorithms are presented. We apply the approach to a flow data set which, is both large and topologically complex, and achieve significant compression ratios there.	algorithm;level of detail;text simplification;topological derivative	Holger Theisel;Christian Rössl;Hans-Peter Seidel	2003		10.1109/PCCGA.2003.1238287	combinatorics;discrete mathematics;vector field;topology;subspace topology;compression ratio;mathematics;geometry;weak topology;extension topology;critical point;general topology;vector quantization;digital topology	Visualization	71.65766740519994	-44.782838135210255	77364
3172ecf33c19a625b317b8fb429dfaa1a83c8ed8	untangling triangulations through local explorations	deformable mesh;local surgery;mesh repair;triangulations	"""The problem of maintaining a valid mesh (triangulation) within a certain domain that deforms over time arises in many applications. During a period for which the underlying mesh topology remains unchanged, the deformation moves vertices of the mesh and thus potentially turns a mesh invalid, or as we call it, tangled. We introduce the notion of locally removable regions, which are certain tangled regions in the mesh that allow for local removal and re-meshing. We present an algorithm that is able to quickly compute, through local explorations, a minimum locally removable region containing a """"seed"""" tangled region in an invalid mesh. By re-meshing within this area, the """"seed"""" tangled region can then be removed from the mesh without introducing any new tangled region. The algorithm is output-sensitive in the sense that it never explores outside the output region."""	delaunay triangulation;mesh networking;output-sensitive algorithm;removable media;time complexity	Pankaj K. Agarwal;Bardia Sadri;Hai Yu	2008		10.1145/1377676.1377726	topology;mathematics;geometry;laplacian smoothing;t-vertices	Graphics	69.43844316637426	-43.66734821965988	77398
91f78f873b8c6e6c87cc3f87a70f5acf83a7ef56	vessel segmentation and blood flow simulation using level-sets and embedded boundary methods	fast-marching;blood-flow;navier-stokes equations;segmentation;embedded boundary methods;navier–stokes equations;level-sets;level set method;level set;simulation;level sets;fast marching;blood flow;radiology;finite element;construction;boundary representation;incompressible fluid	In this article we address the problem of blood flow simulation in realistic vascular objects. The anatomical surfaces are extracted by means of Level-Sets methods that accurately model the complex and varying surfaces of pathological objects such as aneurysms and stenoses. The surfaces obtained are defined at the sub-pixel level where they intersect the Cartesian grid of the image domain. It is therefore straightforward to construct embedded boundary representations of these objects on the same grid, for which recent work has enabled discretization of the Navier– Stokes equations for incompressible fluids. While most classical techniques require construction of a structured mesh that approximates the surface in order to extrapolate a 3D finite-element gridding of the whole volume, our method directly simulates the blood flow inside the extracted surface without losing any complicated details and without building additional grids. D 2004 CARS and Elsevier B.V. All rights reserved.	adaptive mesh refinement;discretization;embedded system;extrapolation;finite volume method;interaction;navier–stokes equations;pixel;refinement (computing);regular grid;simulation;unstructured grid	Thomas Deschamps;Peter O. Schwartz;David Trebotich;Phillip Colella;D. Saloner;Ravi Malladi	2004			biology;mathematical optimization;radiology;medicine;computer science;level set;theoretical computer science;mathematics;geometry	Robotics	69.82360562188424	-46.24753693802221	77867
b5181bd9f5c53b3aa846543cefc040f4d9f368a0	scale-space for union of 3d balls	implicit surface;smoothing methods computer graphics geometry particle filtering numerical methods radial basis function networks;computer graphics;bismuth;geometry;data mining;computer graphic;radial basis function networks;planar curvature motions 3d balls union shape discretization computer graphics geometric modeling implicit surface reconstruction radial basis functions molecular atomic models fluid simulation particle systems particle deformation tracking particle filters scale space properties bidimensional curvature motion;shape measurement smoothing methods deformable models computer graphics solid modeling surface reconstruction atomic measurements computational modeling particle tracking particle filters;smoothing methods;computational modeling;scale space;radial basis function;shape;curvature motion union of balls scale spaces;particle filter;three dimensional displays;numerical computation;particle system;union of balls;geometric model;approximation methods;fluid simulation;curvature motion;scale spaces;particle filtering numerical methods	Shape discretization through union of weighted points or balls appears as a common representation in different fields of computer graphics and geometric modeling. Among others, it has been very successful for implicit surface reconstruction with radial basis functions, molecular atomic models, fluid simulation from particle systems and deformation tracking with particle filters. These representations are commonly generated from real measurements or numerical computations, which may require filtering and smoothing operations.This work proposes a smoothing mechanism for union of balls that tries to inherit from the scale-space properties of bi-dimensional curvature motion: it avoids disconnecting the shape, prevents self-intersection, regularly decreases the area and convexifies the shape. The smoothing is computed iteratively by moving each ball of the union according to a combination of projected planar curvature motions. Experiments exhibits nice properties of this scale-space.	computation;computer graphics;discretization;experiment;fluid animation;geometric modeling;implicit surface;numerical analysis;particle filter;particle system;radial (radio);radial basis function;scale space;simulation;smoothing	Alex Laier Bordignon;Betina Vath;Thales Vieira;Marcos Craizer;Thomas Lewiner;Cynthia O. L. Ferreira	2009	2009 XXII Brazilian Symposium on Computer Graphics and Image Processing	10.1109/SIBGRAPI.2009.9	classical mechanics;mathematical optimization;mathematics;geometry	Graphics	70.27435602531735	-47.04423685502454	77900
83f0397a30900eec0d1aa44bd9c75414625e5794	modeling of bézier curves using a combination of linear and circular arc approximations	linear arc approximations;software built in line;pure circular arc approximation;approximation algorithms;linear approximation;recursive method;monomial form approach;de casteljau algorithm;polynomials;splines mathematics;approximation;de casteljau algorithm bezier curves rendering linear arc approximations recursive method pure linear approximation pure circular arc approximation user defined tolerance computational time software built in line arc functions monomial form approach;user defined tolerance;linear approximation approximation algorithms algorithm design and analysis software algorithms splines mathematics polynomials;bezier curves rendering;rendering computer graphics polynomial approximation;software algorithms;computational time;arc functions;rendering computer graphics;algorithm design and analysis;pure linear approximation;polynomial approximation;bezier curve;approximation bezier curve	This paper presents the new approach of the Bézier curve rendering using linear and circular arc approximations, instead of a recursive method that calculates all points on the curve pixel by pixel. The new developed algorithm also works better than a pure-linear approximation which generates rough output, and a pure-circular-arc approximation which involves with too many parameters. The purposed algorithm subdivides the curve into a series of lines and arcs satisfying the user-defined tolerance. This new method can reduce the number of points involved with the calculation and also decrease the computational time since the software built-in line and arc functions calculate faster than the recursive method. This algorithm is based on the monomial form approach which can provides more efficiency and flexibility, compare with traditional methods that are mostly based on a polynomial basis or the de Casteljau's algorithm.	bézier curve;canonical account;computation;de casteljau's algorithm;linear approximation;monomial basis;pixel;polynomial basis;recursion;time complexity	Pongrapee Kaewsaiha;Natasha Dejdumrong	2012	2012 Ninth International Conference on Computer Graphics, Imaging and Visualization	10.1109/CGIV.2012.20	mathematical optimization;combinatorics;de casteljau's algorithm;discrete mathematics;mathematics	Visualization	70.17437324142689	-40.585235961226545	78147
100fbd72b96afae706d0f55054f13990fbfe0736	a new unified model of univariate and bivariate bases for curves, rectangular surfaces and triangular surfaces	matrix representation;rectangular basis functions polynomial curve modeling linear computation blending functions control points recursive algorithm coefficient matrix representation bezier curve triangular basis functions;computer graphics;probability density function;wang ball curve;computational geometry;polynomials computational complexity partitioning algorithms shape computer graphics visualization interpolation solid modeling;partition of unity;matrix algebra;data mining;linear complexity;polynomials;dp curve;unified model;computational modeling;rectangular surfaces;shape;computational complexity;triangular surfaces;triangular basis functions;rectangular basis functions;linear computation;triangular surfaces bezier curve said ball curve wang ball curve dp curve dejdumrong curve linear complexity rectangular surfaces;control points;said ball curve;dejdumrong curve;recursive algorithm;convex hull;coefficient matrix representation;polynomial curve modeling;blending functions;convex combination;polynomials computational complexity computational geometry computer graphics matrix algebra;bezier curve	In this paper, a new basis for polynomial curve modeling is presented with its linear computation. This new proposed curve can be formed by the convex combination of its blending functions and related control points. Moreover, several important geometric properties for this curve are identified, for examples, a partition of unity, convex hull property and symmetry. Later the recursive algorithm, coefficient matrix representation, the derivatives and the relationships between Bezier curve and this proposed curve are defined. Finally, a new proposed rectangular and triangular basis functions are also presented with their surface definitions.	algorithm;alpha compositing;basis function;bivariate data;bézier curve;coefficient;computation;control point (mathematics);convex hull;matrix representation;polynomial;recursion (computer science);unified model	Jaratpong Jangchai;Natasha Dejdumrong	2009	2009 Sixth International Conference on Computer Graphics, Imaging and Visualization	10.1109/CGIV.2009.75	mathematical optimization;combinatorics;discrete mathematics;mathematics;convex curve;parallel curve;stable curve;curve orientation;curve fitting	Visualization	70.88433588562968	-41.40534987807666	78904
b63df056adbbf047156132a2d7de5215c0d30b1e	curve fitting with conic splines	optical character recognition;optimal location;higher order;curve fitting;interactive graphics;interaction design;cubic spline	Conic splines are formed by arcs of conics, each defined by its endpoints and the tangents at them plus an intermediate point. Instead of the common general equation that depends on five parameters, an equation with a single parameter is used, thus simplifying significantly the curve fitting problem. The resulting guided conics resemble Bezier polynomials and for parabolas are identical to them. Such splines can be used conveniently both for interactive design and for automatic curve fitting. They allow circular, elliptical, and hyperbolic arcs to be included in the spline family, while the common forms using a B-spline basis allow the inclusion of parabolic arcs only. Conic splines are described either in a rational parametric or in algebraic form f(x, y) = 0. A simple estimate for the distance of a point from such a curve is given and is used to test the quality of approximations. The data to be fitted are first approximated by a polygon, and then simple heuristics are used to decide which sequences of vertices should be approximated by conics. The conics found by the applications of the heuristics are usually close approximations of the data and need no further adjustments. When adjustments are needed, the interval is split and a conic is fitted on each part. It is shown theoretically that exact knot placement at the optimal locations is less important for higher order splines than for polygons. Examples of application of the method to the fitting of font and other contours are given. Comparisons with other methods suggest that conic splines require no more knots than cubic splines for similar quality of approximation.	approximation algorithm;b-spline;cubic function;curve fitting;heuristic (computer science);interactive design;linear algebra;parabolic antenna;placement (eda);polynomial;spline (mathematics)	Theodosios Pavlidis	1983	ACM Trans. Graph.	10.1145/357314.357315	spline;computer vision;econometrics;mathematical optimization;higher-order logic;smoothing spline;computer science;interaction design;mathematics;optical character recognition;flat spline;statistics;curve fitting	Graphics	69.53323628983415	-39.48269001984113	78975
1d131088a4236663b0328e2d57651255b57343fe	quad/triangle subdivision	object representation;visual quality;computer graphic;subdivision scheme;triangle mesh	In this paper we introduce a new subdivision operator that unifies triangular and quadrilateral subdivision schemes. Designers often want the added flexibility of having both quads and triangles in their models. It is also well known that triangle meshes generate poor limit surfaces when using a quad scheme, while quad-only meshes behave poorly with triangular schemes. Our new scheme is a generalization of the well known CatmullClark and Loop subdivision algorithms. We show that our surfaces are C1 everywhere and provide a proof that it is impossible to construct a C2 scheme at the quad/triangle boundary. However, we provide rules that produce surfaces with bounded curvature at the regular quad/triangle boundary and provide optimal masks that minimize the curvature divergence elsewhere. We demonstrate the visual quality of our surfaces with several examples.	algorithm;catmull–clark subdivision surface;formal proof;stationary process;subdivision surface;triangle mesh;variable shadowing;whole earth 'lectronic link	Jos Stam;Charles T. Loop	2003	Comput. Graph. Forum	10.1111/1467-8659.t01-2-00647	discrete mathematics;topology;computer science;triangle mesh;mathematics;geometry;computer graphics (images)	Graphics	69.11845739849673	-43.13496464067641	79849
04c5df91e3da637e0f843e908bb4ce3886d2e96f	texturing fluids	haptics;visual prosthetics;sensory substitution	"""We present a novel technique for synthesizing textures over dynamically changing fluid surfaces. We use both image textures, as well as bump maps as example inputs. Image textures can enhance the rendering of the fluid by either imparting a realistic appearance to it or by stylizing it, whereas bump maps enable the generation of complex microstructures on the surface of the fluid that may be very difficult to synthesize using simulation. To generate temporally coherent textures over a fluid sequence, we transport texture information, that is, color and local orientation, between free surfaces of the fluid from one time step to the next. This is accomplished by extending the texture information from the first fluid surface to the 3D fluid domain, advecting this information within the fluid domain along the fluid velocity field for one time step and interpolating it back onto the second surface-this operation, in part, uses a novel vector advection technique for transporting orientation vectors. We then refine the transported texture by performing texture synthesis over the second surface using our """"surface texture optimization"""" algorithm, which keeps the synthesized texture visually similar to the input texture and temporally coherent with the transported one. We demonstrate our novel algorithm for texture synthesis on dynamically evolving fluid surfaces in several challenging scenarios."""	algorithm;bump mapping;coherence (physics);computational fluid dynamics;image texture;interpolation;liquid substance;map;mathematical optimization;simulation;texture mapping;texture synthesis;thermal copper pillar bump;velocity (software development);one time	Vivek Kwatra;David Adalsteinsson;Nipun Kwatra;Mark Carlson;Ming C. Lin	2006	IEEE Transactions on Visualization and Computer Graphics	10.1145/1179849.1179928	sensory substitution;computer science;haptic technology	Visualization	70.97449707374682	-48.3584934994186	80725
5990b547c3d4b610491d1aaf52a5d5b6b43ed877	synchrosqueezed wave packet transform for 2d mode decomposition	local wavevector;42a99;journal article;synchrosqueezing;65t99;phase space representation;clustering;wave packet transform;empirical mode decomposition	This paper introduces the synchrosqueezed wave packet transform as a method for analyzing twodimensional images. This transform is a combination of wave packet transforms of a certain geometric scaling, a reallocation technique for sharpening phase space representations, and clustering algorithms for modal decomposition. For a function that is a superposition of several wave-like components with a highly oscillatory pattern satisfying certain separation conditions, we prove that the synchrosqueezed wave packet transform identifies these components and estimates their local wavevectors. A discrete version of this transform is discussed in detail, and numerical results are given to demonstrate the properties of the proposed transform.	algorithm;cluster analysis;curvelet;frequency analysis;image noise;image scaling;landau–yang theorem;lexical analysis;modal logic;network packet;nonlinear system;normal mode;numerical analysis;numerical method;quantum superposition;reflections of signals on conducting lines;stationary process;time–frequency analysis;wave packet;yang	Haizhao Yang;Lexing Ying	2013	SIAM J. Imaging Sciences	10.1137/120891113	mathematical optimization;mathematical analysis;wave packet;s transform;computer science;theoretical computer science;fractional fourier transform;hilbert–huang transform;machine learning;mathematics;cluster analysis	Networks	80.04439801621696	-38.72796164890683	80758
17590afae5f4f22e186c16ad8b0e82b78bd78b06	an amplitude-preserved time–frequency peak filtering based on empirical mode decomposition for seismic random noise reduction	time frequency plane amplitude preserved time frequency peak filtering empirical mode decomposition seismic random noise reduction classical filtering method time frequency domain wigner ville distribution analytical signal instantaneous frequency short window length signal amplitude bad random noise reduction effective random noise reduction;seismology;window length empirical mode decomposition emd random noise reduction signal amplitude preservation time frequency peak filtering tfpf;random noise;noise measurement noise reduction signal to noise ratio time frequency analysis attenuation frequency modulation;seismology geophysical techniques random noise;geophysical techniques	Time-frequency peak filtering (TFPF) is a classical filtering method in time-frequency domain. It applies Wigner-Ville distribution to estimate the instantaneous frequency of an analytical signal. There is a pair of contradiction in this method, i.e., selecting a short window length may lead to good preservation for signal amplitude but bad random noise reduction whereas selecting a long window length may lead to serious attenuation for signal amplitude but effective random noise reduction. In order to make a good tradeoff between valid signal amplitude preservation and random noise reduction, we adopt empirical mode decomposition (EMD) to improve the TFPF results. The new idea is to utilize the decomposition characteristic of EMD which decomposes a signal to several modes from high to low frequency and to take advantage of the time-frequency filtering characteristic of TFPF which can recognize the valid signal component in the time-frequency plane in order to achieve effective random noise reduction together with good amplitude preservation. Through some experiments on synthetic seismic models and field seismic records, we show the better performance of the new method compared with the conventional TFPF.	experiment;hilbert–huang transform;instantaneous phase;noise (electronics);noise reduction;subject reduction;synthetic intelligence;time–frequency analysis;wigner quasiprobability distribution	Yanping Liu;Yue Li;Hongbo Lin;Haitao Ma	2014	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2013.2281202	gradient noise;median filter;noise spectral density;speech recognition;acoustics;value noise;noise temperature;geology;noise measurement;quantum noise;noise;multiplicative noise;noise;mathematics;white noise;noise figure;noise floor;phase noise;stochastic resonance;salt-and-pepper noise	HPC	81.00773881588194	-41.26825161477068	80923
13e4a2d2c4a46a85186ccb925a3d20bc9e0fbb9c	flexible and rapid animation of brittle fracture using the smoothed particle hydrodynamics formulation	smoothed particles hydrodynamics sph;brittle fracture;animation	This paper presents a hybrid animation approach to the flexible and rapid crack simulation of brittle material. At the physical level, the local stress tensors induced by collision are analyzed by using the smoothed particle hydrodynamics (SPH) formulation. Specifically, in order to determine the internal stress when rigid bodies collide with each other or neighboring environments, we treat all of them as completely rigid body that has infinite stiffness and then evaluate virtual displacement for colliding particles. At the geometric level, in order to faithfully maintain the fracture interface during the crack simulation, we utilize an efficient shape representation of solid based on the tetrahedral decomposition of the original solid geometry. This novel hybrid approach resorts to local particle models, whose goal is to avoid heavy computational burden during crack interface updating and topological changing, and meanwhile, it facilitates the user-initiated interactive control during the crack generation and propagation. Our animation experiments demonstrate the effectiveness of our novel particle-based method to simulate the crack of brittle material. Copyright © 2013 John Wiley & Sons, Ltd.	collision detection;computation;displacement mapping;experiment;geometry processing;gradient;graphics processing unit;internet of things;john d. wiley;particle filter;simulation;smoothed-particle hydrodynamics;smoothing;software propagation;trustworthy computing	Feibin Chen;Changbo Wang;Buying Xie;Hong Qin	2013	Journal of Visualization and Computer Animation	10.1002/cav.1514	anime;simulation;computer science;artificial intelligence	Graphics	70.91623116582598	-47.167872047710766	81259
c49ab66d99025e330c30251f86d48e4c951804f6	underwater trajectory estimation based on grid localization and smoothing		Underwater trajectory estimation is a challenging problem and established technologies developed for terrestrial systems cannot be used underwater. Known trajectory estimation techniques suffer from high cost in the setting up the infrastructures. In this paper, we propose and investigate a receiver trajectory estimation technique based on grid map localization and data smoothing approach. In the localization the receiver estimates the Channel State Information and compares it with a CSI pre-computed on a grid of points covering the area of interest; the best match indicates the estimated location. For a dynamic receiver moving in the area of interest, with estimated locations of sample points along the receiver trajectory, smoothing approach based on P-splines is applied to recover the trajectory. Numerical results show that an accurate estimated trajectory can be achieved.	acoustic cryptanalysis;channel state information;coefficient;expectation propagation;gps navigation device;global positioning system;motion estimation;numerical analysis;numerical method;precomputation;sid meier's alpha centauri;smoothing;spline (mathematics);terrestrial television	Li Liao;Benjamin Henson;Yuriy Zakharov	2018	2018 IEEE 10th Sensor Array and Multichannel Signal Processing Workshop (SAM)	10.1109/SAM.2018.8448487	channel state information;grid;mathematical optimization;computer science;estimation;smoothing;grid reference;trajectory;underwater	Robotics	79.46020914359113	-45.68994905687021	81562
401104c5d2ae8ffaf3c6e514a77a136f5d466d54	incremental deformation subspace reconstruction		Recalculating the subspace basis of a deformable body is a mandatory procedure for subspace simulation, after the body gets modified by interactive applications. However, using linear modal analysis to calculate the basis from scratch is known to be computationally expensive. In the paper, we show that the subspace of a modified body can be efficiently obtained from the subspace of its original version, if mesh changes are small. Our basic idea is to approximate the stiffness matrix by its lowfrequency component, so we can calculate new linear deformation modes by solving an incremental eigenvalue decomposition problem. To further handle nonlinear deformations in the subspace, we present a hybrid approach to calculate modal derivatives from both new and original linear modes. Finally, we demonstrate that the cubature samples trained for the original mesh can be reused in fast reduced force and stiffness matrix evaluation, and we explore the use of our techniques in various simulation problems. Our experiment shows that the updated subspace basis still allows a simulator to generate visual plausible deformation effects. The whole system is efficient and it is compatible with other subspace construction approaches.	analysis of algorithms;approximation algorithm;computer graphics;eurographics;graphics processing unit;image stitching;john d. wiley;modal logic;nonlinear system;numerical integration;simulation;stiffness matrix	Rupam Mukherjee;Xin Wu;Huaqing Wang	2016	Comput. Graph. Forum	10.1111/cgf.13014	3d reconstruction;anime;computer vision;vector graphics;scientific visualization;2d computer graphics;computer science;theoretical computer science;real-time computer graphics;computer animation;computer graphics;3d computer graphics;computer graphics (images)	Graphics	69.65964684039508	-46.99418732589952	81679
e33dfd8d47b95b4b9d046904b6aa413b23c0a27c	dynamic meshes for accurate polygonization of implicit surfaces with sharp features	implicit surface;smoothing methods isosurfaces anisotropic magnetoresistance surface fitting sampling methods joining processes shape binary trees signal sampling adaptive signal processing;computational geometry;surface fitting;solid modelling computational geometry mesh generation surface fitting;mesh generation;solid modeling dynamic meshes implicit surface polygonization sharp features mesh evolution mesh vertex positions mesh normals;solid modelling	The paper presents a novel approach for accurate polygonization of implicit surfaces with sharp features. The approach is based on mesh evolution towards a given implicit surface with simultaneous control of the mesh vertex positions and mesh normals.	implicit surface	Yutaka Ohtake;Alexander G. Belyaev;Alexander A. Pasko	2001		10.1109/SMA.2001.923377	mesh generation;mathematical optimization;computational geometry;mathematics;geometry;laplacian smoothing;t-vertices	Graphics	69.75783266102198	-43.45008120488178	81767
cfc62aa3afd0ecb2e5e4fa47022a84c1b71565cf	establishment of a pair of concentric circles with the minimum radial separation for assessing roundness error	tolerance geometrique;concepcion asistida;organigramme;computer aided design;metodologia;flowchart;metrologia;circularidad;interpretation geometrique;computational geometry;metrologie;metrology;rounding errors;methodologie;geometric tolerance;conception assistee;methodology;organigrama;automatic part inspection;roundness;circularite	Abstract   The paper presents a computational-geometry-based method of determining the roundness error of a measured workpiece. A set of   n   points (obtained from the measured workpiece) in a plane being given, it is required that the center and the radii of a pair of concentric circles be found such that no point is exterior to the space bounded by the circles, with the condition that the radial separation between the circles is minimum. The paper addresses the mathematical formalization of the problem. The properties of convex-hull and Voronoi diagrams have been exploited to develop a faster algorithm for establishing the circles. The methodology has been implemented, and the results have been presented to validate the computational effectiveness of the approach.	radial (radio)	Utpal Roy;Xuzeng Zhang	1992	Computer-Aided Design	10.1016/0010-4485(92)90035-9	simulation;flowchart;computational geometry;geometric dimensioning and tolerancing;computer science;engineering;computer aided design;methodology;roundness;mathematics;geometry;engineering drawing;metrology	EDA	68.66685210097603	-38.37949649836968	82901
3767b1b44c9cdd5bdc4df3dccc859007728cc9c7	modifying free-formed nurbs curves and surfaces for offsetting without local self-intersection	tecnologia electronica telecomunicaciones;computacion informatica;grupo de excelencia;ciencias basicas y experimentales;tecnologias;curves and surfaces	This paper presents an algorithm of modifying free-formed NURBS curve/surface for offsetting without local self-intersecting. The method consists of (1) sampling a number of points from a progenitor curve/surface based on second derivatives; (2) checking the curvature or maximum curvature of the progenitor curve/surface at the sampled points; (3) inserting corresponding knots of sampled points; (4) repositioning control points till the curvature/maximum curvature of the curve/surface everywhere are less than the reciprocal of offset distance. The method is efficient and is able to obtain better offsetting results.	non-uniform rational b-spline	Haonan Liu;Takashi Maekawa;Nicholas M. Patrikalakis;Emanuel M. Sachs;Wonjoon Cho	2004	Computer-Aided Design	10.1016/j.cad.2003.11.002	topology;mathematics;geometry;engineering drawing	EDA	69.27552719831121	-39.376000641269265	83120
d016d6277db16a9265acad1c1cb91308809349e9	implementation of cl points preprocessing methodology with nurbs curve fitting technique for high-speed machining	nurbs fitting;cl point preprocessing;defect point;cnc machining;bisection method	Due to the limitations of linear/circular interpolation, parametric interpolation typically represented by NURBS curves, has played a key role in the computer control of machine tools. To achieve the highest quality parts, generated trajectories must describe the desired toolpath accurately. However, a NURBS toolpath cannot be generated directly from a CAD model, due to certain limitations. CL points preprocessing methodology with NURBS curve fitting technique for high-speed machining is proposed in this paper. CL points are initially preprocessed, and unsuitable CL points can be grouped into three problematic cases: short distance, unequal adjacent distance and sharp points. Each case is identified and corrected, which can improve the fitted NURBS curve quality. CL points corner and distance rules are then used to extract CL points for each segmentation. Using the bisection method, a NURBS curve is efficiently fitted using CL points. If there is no advantage to using a curve in this case, CL points will be output directly. The simulation performed on CL points for a wave surface demonstrates that the proposed approach reduces the NC blocks to about 3.3%, while staying within the tolerance of deviation. The machining experiment shows that the proposed approach can increase the machining quality while reducing the machining time by about 19%. 2	bisection method;computer control company;computer-aided design;curve fitting;nc (complexity);non-uniform rational b-spline;preprocessor;simulation;whittaker–shannon interpolation formula	Jichun Wu;Huicheng Zhou;Xiaoqi Tang;Jihong Chen	2015	Computers & Industrial Engineering	10.1016/j.cie.2014.12.018	bisection method;mathematical optimization;mathematics;geometry;numerical control;engineering drawing	Robotics	69.28265664392924	-38.186680728903625	83571
f803fb6ea354d3d0b8f721feddadf8c2477f70b2	lidar data reduction using vertex decimation and processing with gpgpu and multicore cpu technology	gpu;multicore cpu;tin;lidar	Airborne light detection and ranging (LiDAR) topographic data provide highly accurate representations of the earth's surface. However, large data volumes pose computing issues when disseminating and processing the data. The main goals of this paper are to evaluate a vertex decimation algorithm used to reduce the size of the LiDAR data and to test parallel computation frameworks, particularly multicore CPU and GPU, in processing the data. In this paper we use a vertex decimation technique to reduce the number of vertices available in a triangulated irregular network (TIN) representation of LiDAR data. In order to validate and verify the algorithm, the authors have used last returns only (LRO) and all returns (AR) of points from four tiles of LiDAR data taken from flat and undulating terrains. The results for flat terrain data showed decimation rates of roughly 95% for last returns only and 55% for all returns without significant loss of accuracy in terrain representation. Accordingly, file sizes were reduced by about 96.5% and 60.5%. The processing speed greatly benefited from parallel programming using the multicore CPU framework. The GPU usage demonstrated an additional impediment caused by noncomputational overhead. Nonetheless, tremendous acceleration was demonstrated by the GPU environment in the computational part alone.		Dossay Oryspayev;Ramanathan Sugumaran;John DeGroote;Paul Gray	2012	Computers & Geosciences	10.1016/j.cageo.2011.09.013	lidar;parallel computing;geology;tin;computer science;theoretical computer science;computer graphics (images)	HPC	68.79201033084532	-51.997575705588496	84022
bff10a3b5cef06d2e116387b1d124edea13479d5	on the generation of spiral-like paths within planar shapes		Abstract We simplify and extend prior work by Held and Spielberger [CAD 2009, CADu0026A 2014] to obtain spiral-like paths inside of planar shapes bounded by straight-line segments and circular arcs: We use a linearization to derive a simple algorithm that computes a continuous spiral-like path which (1) consists of straight-line segments, (2) has no self-intersections, (3) respects a user-specified maximum step-over distance, and (4) starts in the interior and ends at the boundary of the shape. Then we extend this basic algorithm to double-spiral paths that start and end at the boundary, and show how these double spirals can be used to cover complicated planar shapes by composite spiral paths. We also discuss how to improve the smoothness and reduce the curvature variation of our paths, and how to boost them to higher levels of continuity.		Martin Held;Stefan de Lorenzo	2018	J. Computational Design and Engineering	10.1016/j.jcde.2017.11.011	arc (geometry);mathematical optimization;curvature;smoothness;engineering;simple algorithm;bounded function;spiral;linearization;planar	Theory	68.48426103527446	-43.473732562144164	84230
946cc147fcc283d4673d0f06377179d4f85f8bcd	scale-invariant minimum-cost curves: fair and robust design implements	scale invariance;robust design	Four functionals for the computation of minimum cost curves are compared. Minimization of these functionals result in the widely studied Minimum Energy Curve (MEC), the recently introduced Minimum Variation Curve (MVC), and their scale-invariant counterparts, (SI-MEC, SI-MVC). We compare the stability and fairness of these curves using a variety of simple interpolation problems. Previously, we have shown MVC to possess superior fairness. In this paper we show that while MVC have fairness and stability superior to MEC they are still not stable in all configurations. We introduce the SI-MVC as a stable alternative to the MVC. Like the MVC, circular and helical arcs are optimal shapes for the SI-MVC. Additionally, the application of scale invariance to functional design allows us to investigate locally optimal curves whose shapes are dictated solely by their topology, free of any external interpolation or arc length constraints.	computation;fairness measure;functional design;interpolation;local optimum;model–view–controller;serial digital video out	Henry P. Moreton;Carlo H. Séquin	1993	Comput. Graph. Forum	10.1111/1467-8659.1230473	mathematical optimization;combinatorics;discrete mathematics;topology;computer science;scale invariance;mathematics;geometry;statistics	Robotics	71.91972227942985	-42.25894816355397	84263
1781df90e208892b571beb7fdfddd120a1664583	spherical projective displacement mesh	personal computer;data visualisation;large scale;cameras data visualization rendering computer graphics arrays graphics processing unit tiles mathematical model;rendering computer graphics data visualisation microcomputers;very fast rendering rate spherical projective displacement mesh algorithm large scale spherical terrain visualization mesh vertices height field atlas texture atlas personal computer high quality images;spherical terrain;terrain visualization;rendering computer graphics;microcomputers;spherical terrain terrain visualization	This paper presents a novel algorithm (We call it Spherical Projective Displacement Mesh algorithm)for large scale spherical terrain visualization. The algorithm generates terrain meshes by rotating, displaces mesh vertices from height field atlas, and then projects a texture atlas onto the mesh vertices to create images. We successfully implement the spherical terrain visualization on a personal computer, obtain high quality images and reach a very fast rendering-rate.	2.5d;algorithm;displacement mapping;display resolution;download;experiment;frustum;glossary of computer graphics;heightmap;hidden surface determination;personal computer;pixel;ray casting;terrain rendering;texture atlas;vertex (geometry)	Jianxin Luo;Guiqiang Ni;Guyu Hu;Jinsong Jiang;Yifeng Duan	2011	2011 12th International Conference on Computer-Aided Design and Computer Graphics	10.1109/CAD/Graphics.2011.11	terrain rendering;computer vision;scientific visualization;simulation;computer science;microcomputer;data visualization;computer graphics (images)	Visualization	68.94325106744397	-51.44638738849085	84524
46d7117c3e48dd3d9423f99fe8a94ad3316ead71	optimal parameterization for cubic splines	concepcion asistida;computer aided design;optimisation;interpolation;optimizacion;interpolacion;aproximacion;parametrization;parametrizacion;approximation;conception assistee;optimization;esplin cubico;spline cubique;parametrisation;cubic spline	Abstract   A global cubic C 2  spline curve is derived from a minimum-energy principle. The parameterization is chosen such that a high-order circle approximation is achieved. Methods for preserving the shape of given data are considered.	spline (mathematics)	Utz Wever	1991	Computer-Aided Design	10.1016/0010-4485(91)90041-T	parametrization;mathematical optimization;combinatorics;cubic form;smoothing spline;monotone cubic interpolation;interpolation;computer aided design;mathematics;geometry;box spline;mechanical engineering	EDA	68.9192836795658	-40.192185972092425	85366
4a4ccc498702ef99f2fe217a4c3bd0f60ee15d16	landscapes synthesis achieved through erosion and deposition process simulation	natural textures;geological phenomena simulation;artificial landscapes modelling;process simulation;landscapes rendering	Abstract#R##N##R##N#This paper describes an original approach to terrain evolution in landscapes synthesis. In order to create some realistic landforms, we simulate geologically contrasted terrains and apply to them deterministic erosion processes. This allows us to relate the erosion on any point of the landsurface to local geological parameters. Any height field may be chosen as an initial topographic surface. Small perturbations may be introduced to avoid unpleasant regularities. A 3D model defines the geological parameters of each point according to its elevation. Our method is iterative: at each step, rock removal and possible alluvial deposition are computed at each point of the landsurface. The available erosion laws simulate mechanical erosion, chemical dissolution and alluvial deposition. At the end of each iteration, a new landsurface and the corresponding river network are created. Landsurfaces can be visualized at the final stage by two rendering algorithms including natural textures mapping. The stream network and the ridges may also be visualized.	chemical vapor deposition;erosion (morphology);simulation	Pascale Roudier;Bernard Péroche;M. Perrin	1993	Comput. Graph. Forum	10.1111/1467-8659.1230375	process simulation;computer science	Logic	71.78404440894782	-48.379525696753774	85478
eaa8fac211da721d1aba92f4175efb81a79382a4	vectorization of visualization algorithms - a practical example	parallel algorithm;volume rendering;scientific visualization;image generation	In scientific visualization, interactive image generation times are needed. Yet typical algorithms are computationally expensive, e.g. image generation algorithms for volume rendering, which is an important and widely used technique. Since vector computers are still among the most powerful machines available, we propose a vectorized variant of a volume rendering algorithm. Vectorization has a long research tradition. Many of the obstacles encountered in transforming loops of sequential programs into vector operations can be handled automatically by state of the art compilers. However, experiences in the course of our work revealed that there are still some shortcomings. It turned out that even for relatively simple optimization transformations it can be necessary to resort to assembler level programming. This is exemplified by a detailed analysis of the proposed algorithm. Results of the final implementation on a mini supercomputer conclude the presentation.	algorithm;automatic vectorization	Alfred Spalt;Siegfried Grabner;Jens Volkert	1996		10.1007/3-540-61695-0_16	computational science;scientific visualization;information visualization;computer science;theoretical computer science;parallel rendering;computer graphics (images)	Visualization	70.03246730227696	-51.762951319088884	86219
7878afc9835b102e1ab967237edd83f40a059d29	two-dimensional diffusion model for diffuse ink painting	colloidal liquid;computer animation;multidimensional diffusion processes;diffusion model	In our previous work [1] the multidimensional diffusion model for computer animation of diffuse ink painting was suggested. The model, which we proposed, provided the intensity distributions very similar to those in real images. In [1] only few calculations in the case of a circle as an initial zone were presented. Now we modify the model and present the results of more accurate calculations for an initial zone of arbitrary shape.	computer animation	Tosiyasu L. Kunii;Gleb V. Nosovskij;Vladimir L. Vecherinin	2001	International Journal of Shape Modeling	10.1142/S0218654301000047	simulation;diffusion;computer animation;computer graphics (images)	Vision	72.51694706077298	-48.9511232021539	86838
bdce10ccdb7b1949dedc730100b3c15513a0d43d	active visualization in a multidisplay immersive environment	tiled display;parallel rendering;user interface;distributed networks;real time;large data sets;immersive environment;graphics system;image generation;interactive rendering;metabuffer;progressive image composition;multiresolution;image compositing;foveated vision;large data	Building a system to actively visualize extremely large data sets on large tiled displays in a real-time immersive environment involves a number of challenges. First, the system must be completely scalable to support the rendering of large data sets. Second, it must provide fast, constant frame rates regardless of user viewpoint or model orientation. Third, it must output the highest resolution imagery where it is needed. Fourth, it must have a flexible user interface to control interaction with the display. This paper presents the prototype for a system which meets all four of these criteria. It details the design of a wireless user interface in conjunction with two different multiresolution techniques—foveated vision and progressive image composition—to generate images on a tiled display wall. The system emphasizes the parallel, multidisplay, and multiresolution features of the Metabuffer image composition hardware architecture to produce interactive renderings of large data streams with fast, constant frame rates.	immersion (virtual reality)	William J. Blanke;Chandrajit L. Bajaj	2003	Computers & Graphics	10.1016/S0097-8493(03)00141-9	computer vision;computer science;parallel rendering;multimedia;user interface;computer graphics (images)	Visualization	68.50613292522598	-51.781518655068226	86910
a9cbf8ce358ab8a96e6938b7d7d990dafed1da0f	free-form deformation of constructive shell models	constructive shell representation;deformable objects;polyhedral model;boolean operation;algebraic surfaces;solid modeling;free form deformation;shell model;coordinate system;freeform deformation	Free-form deformation (FFD) is known to be a powerful technique for deforming an object independent of its representation. A point on a solid is deformed by specifying the point relative to a coordinate system defined with a lattice of control points. Adjusting the control points of the lattice deforms the object. The deformed object can be visualized by sampling points on the object surfaces, or by approximating the object with a polyhedral model. However, a conversion of the polyhedral model to the underlining solid representation is required if further solid operations (e.g. Boolean operation) is to be applied. This paper presents a technique for applying FFD on constructive shell models (CSR). A CSR object is constructed by subtracting a set of depression (negative) trunctets from the union of a set of outer (positive) trunctets and a polyhedron core. FFD is applied to the polyhedron core and the trunctets of the model. A trunctet is deformed by applying FFD to a set of selected surface points on the trunctet. The deformed trunctet is obtained by interpolating a surface through the deformed surface points. In the deformation of a trunctet, an outer trunctet may become a depression trunctet (or vice versa). By using the concept of mating trunctet, and an approach for classifying the shape of a trunctet, shape changes of a trunctet in a deformation can be determined. Deformation of a trunctet may also result in an invalid trunctet that bulges out of its enclosing tetrahedron. Besides, the degree of the algebraic surface used determines the size of a trunctet relative to the distance between adjacent lattice control points. A subdivision of a trunctet may have to be performed to maintain the validity of a deformation.	free-form deformation	K. C. Hui	2003	Computer-Aided Design	10.1016/S0010-4485(03)00038-1	topology;coordinate system;mathematics;geometry;solid modeling;algebraic surface;engineering drawing	EDA	68.39753658192897	-42.3156855218072	87396
fa2411a6501fa25a07696ee38bd7172678892623	inverse procedural modelling of trees	journal_article;biological modeling;i 3 5 computer graphics computational geometry and object modelling;natural phenomena;mesh generation;i 3 6 computer graphics methodology and techniques interaction techniques i 6 8 simulation and modelling types of simulation visual	Procedural tree models have been popular in computer graphics for their ability to generate a variety of output trees from a set of input parameters and to simulate plant interaction with the environment for a realistic placement of trees in virtual scenes. However, defining such models and their parameters is a difficult task. We propose an inverse modelling approach for stochastic trees that takes polygonal tree models as input and estimates the parameters of a procedural model so that it produces trees similar to the input. Our framework is based on a novel parametric model for tree generation and uses Monte Carlo Markov Chains to find the optimal set of parameters. We demonstrate our approach on a variety of input models obtained from different sources, such as interactive modelling systems, reconstructed scans of real trees and developmental models.	computer graphics;markov chain monte carlo;monte carlo method;parametric model;simulation	Ondrej Stava;Sören Pirk;Julian Kratt;Baoquan Chen;Radomír Mech;Oliver Deussen;Bedrich Benes	2014	Comput. Graph. Forum	10.1111/cgf.12282	computational science;mesh generation;computer science;theoretical computer science;mathematics	Graphics	68.98388456058235	-47.93302889067997	87505
52d085b34ad1b1e6eebabe0b575ab9e72d1d5e7f	emd via memd: multivariate noise-aided computation of standard emd	brain computer interface;electroencephalogram;empirical mode decomposition;multivariate empirical mode decomposition	A noise-assisted approach in conjunction with multivariate empirical mode decomposition (MEMD) algorithm is proposed for the computation of empirical mode decomposition (EMD), in order to produce localized frequency estimates at the accuracy level of instantaneous frequency. Despite many advantages of EMD, such as its data driven nature, a compact decomposition, and its inherent ability to process nonstationary data, it only caters for signals with a sufficient number of local extrema. In addition, EMD is prone to mode-mixing and is designed for univariate data. We show that the noise-assisted MEMD (NA-MEMD) approach, which utilizes the dyadic filter bank property of MEMD, provides a solution to the above problems when used to calculate standard EMD. The method is also shown to alleviate the effects of noise interference in univari-ate noise-assisted EMD algorithms which directly add noise to the data. The efficacy of N. ur Rehman et al. the proposed method, in terms of improved frequency localization and reduced mode-mixing, is demonstrated via simulations on electroencephalogram (EEG) data sets, over two paradigms in brain-computer interface (BCI).	algorithm;brain–computer interface;computation;dyadic transformation;electroencephalography;filter bank;hilbert–huang transform;instantaneous phase;interference (communication);maxima and minima;simulation	Naveed ur Rehman;Cheolsoo Park;Norden E. Huang;Danilo P. Mandic	2013	Advances in Adaptive Data Analysis	10.1142/S1793536913500076	brain–computer interface;econometrics;speech recognition;hilbert–huang transform;mathematics;statistics	ML	80.59455110703713	-38.27247583263517	87737
cceebec165d94a32f2a51669e22bb0a4b2915ad2	a data-driven approach for real-time clothes simulation	computacion informatica;i 3 5 computer graphics physically based modeling;real time;grupo de excelencia;real time processing;cloth simulation;collision detection;linear interpolation;ciencias basicas y experimentales;physically based simulation;cloth wrinkles;geometric deformation;mesh deformation;data driven approach	Abstract#R##N##R##N#A data-driven approach for the real-time processing of clothes, particularly suitable for simulating dresses worn by virtual characters, is proposed. It starts, prior to real-time simulation, by analyzing cloth behavior in relation to the underlying skeleton movement from a presimulated sequence of the cloth obtained using any high-quality off-line simulators. The idea is to use this analysis to find an optimal combination of physics-based simulation and geometric approximation of the simulator; potentially colliding regions are defined on the cloth such that they will hold true for the skeleton movement that closely matches that of presimulated sequence. At runtime, using these analyses, our simulation process provides both visually pleasing results and performance, as long as the motion of the character remains sufficiently close to the original sequence used for the precomputation.#R##N##R##N##R##N##R##N#The key contributions of this paper are (1) efficient collision handling that prunes out potentially colliding objects by using the off-line simulation sequence as examples; (2) data-driven fix-up process for the coarse mesh simulation that deduces the gross behavior of the cloth; and (3) geometric approximation of the fine mesh deformation, responsible for details in the shape of the cloth such as wrinkles.	real-time transcription;simulation	Frederic Cordier;Nadia Magnenat-Thalmann	2005	Comput. Graph. Forum	10.1111/j.1467-8659.2005.00841.x	simulation;computer science;artificial intelligence;linear interpolation;collision detection;computer graphics (images)	Embedded	68.59531885185923	-47.282768073758085	88037
462d381f9e749a262d31e0a2bd2d83909530619b	compression of dense and regular point clouds	range scanning;point clouds;spanning tree;compression	We present a simple technique for single-rate compression of point clouds sampled from a surface, based on a spanning tree of the points. Unlike previous methods, we predict future vertices using both a linear predictor, which uses the previous edge as a predictor for the current edge, and lateral predictors that rotate the previous edge 90° left or right about an estimated normal.By careful construction of the spanning tree and choice of prediction rules, our method improves upon existing compression rates when applied to regularly sampled point sets, such as those produced by laser range scanning or uniform tesselation of higher-order surfaces. For less regular sets of points, the compression rate is still generally within 1.5 bits per point of other compression algorithms.	algorithm;data compression;kerrison predictor;lateral computing;lateral thinking;point cloud;spanning tree	Bruce Merry;Patrick Marais;James E. Gain	2006		10.1145/1108590.1108593	mathematical optimization;spanning tree;point cloud;compression	Vision	70.9095648659519	-43.91925367183149	88183
0a62fe7aa1c01d7ca4e7c1679e3fe8afcfc5f759	view-dependent rendering for large polygonal models over networks	level of detail rendering;computer graphics;surface simplification;multiresolution hierarchies	In this paper we are presenting a novel approach that enables the rendering of large-shared datasets at interactive rates using inexpensive workstations. Our algorithm is based on view-dependent rendering and client-server technology — servers host large datasets and manage the selection of the various levels of detail, while clients receive blocks of update operations which are used to generate the appropriate level of detail in an incremental manner. We assume that servers are capable machines in terms of storage capacity and computational power and clients are inexpensive workstations which have limited 3D rendering capabilities. For optimization purposes we have developed two similar approaches — one for local area networks and the other for wide area networks. For the second approach we have performed several changes to adapt to the limitation of the wide area networks. To avoid network latency we have developed two powerful mechanisms that cache the adapt operation blocks on the clients' side and predict the future view-parameters of clients based on their recent behavior. Our approach dramatically reduces the amount of memory used by each client and the entire computing system since the dataset is stored only once in the local memory of the server. In addition, it decreases the load on the network as a result of the incremental update contributed by view-dependent rendering.		Jihad El-Sana;Neta Sokolovsky	2003	Int. J. Image Graphics	10.1142/S0219467803001007	real-time computing;simulation;rendering;computer science;computer graphics;computer graphics (images)	Graphics	68.69406239405222	-51.583279795653276	88220
e021d44987f4dac1501f8e3ab005c4453fdc1d27	breast mr image fusion by deformable implicit polynomial (dip)	breast mr image;non rigid image registration;implicit polynomial	MR Image fusion is desired in various image-guide breast surgeries. However it often suffers from the difficulty on dealing with large deformation of breast. This paper presents a novel method for efficiently modeling and inferring the physical parameters, including gravity, Young’s modulus, Poisson’s ratio, etc, which are important elements for handling the biomechanical deformations of breast with finite element model. Our method consists of two major steps: 1) deformation modeling and 2) non-rigid registration. The former builds a deformable implicit polynomial (DIP) model to encode the physical parameters according to deformation. The latter fast registers the prior DIP to the online breast image such that the image fusion can be achieved. Experimental results demonstrate the good performance of our method.	encode;finite element method;image fusion;modulus robot;polynomial	Bo Zheng;Ryoichi Ishikawa;Jun Takamatsu;Katsushi Ikeuchi;Takaaki Endo;Kiyohide Sato;Takayuki Ueno;Tomoharu Sugie;Masakazu Toi;Shotaro Kanao;Kaori Togashi	2013	IPSJ Trans. Computer Vision and Applications	10.2197/ipsjtcva.5.99	computer vision;control theory	Vision	71.78380055707493	-46.59127731125063	88622
d80d0194d40587639370a09206796b1283db237e	impact of oscillator noise in bistatic and multistatic sar	oscillations;theoretical model;phase noise;phase modulation;low frequency;institut fur hochfrequenztechnik und radarsysteme;frequency phase noise phase modulation microwave oscillators synthetic aperture radar spaceborne radar stochastic processes 1f noise stability demodulation;satelliten sar systeme;stability;microwave oscillators;demodulation;stochastic processes;impulse response;1f noise;second order statistics;frequency;spaceborne radar;synthetic aperture radar	This letter addresses the impact of limited oscillator stability in bistatic and multistatic synthetic aperture radars (SARs). Oscillator noise deserves special attention in distributed SAR systems since there is no cancellation of low-frequency phase errors as in a monostatic SAR, where the same oscillator signal is used for modulation and demodulation. It is shown that the uncompensated phase noise may cause a time-variant shift, spurious sidelobes, and a broadening of the impulse response, as well as a low-frequency phase modulation of the focused SAR signal. Quantitative estimates are derived analytically for each of these errors based on a system-theoretic model taking into account the second-order statistics of the oscillator phase noise	modulation;oscillator phase noise;radar;synthetic intelligence;theory	Gerhard Krieger;Marc Rodriguez-Cassola;Marwan Younis;Robert Metzig	2005	IEEE Geoscience and Remote Sensing Letters	10.1109/IGARSS.2005.1525293	stochastic process;synthetic aperture radar;stability;telecommunications;impulse response;bistatic radar;frequency;phase modulation;low frequency;demodulation;oscillation;phase noise;physics;remote sensing	EDA	80.36020050664153	-42.793719988476454	88958
f8a0e2b10874e8252312caebaee321a60f2d1a81	radar pulse completion and high-resolution imaging with sas based on reweighted anm			image resolution;radar	Xingyu He;Ningning Tong;Xiaowei Hu;Weike Feng	2018	IET Signal Processing	10.1049/iet-spr.2017.0366	mathematical optimization;mathematics;radar;electronic engineering;pulse (signal processing)	Graphics	81.12885053675147	-47.10178423674559	89236
50b18938f86b1bc451ea7eb74feb9958f2925c93	time-domain analysis and research on blasting vibration signals based on fourier transform	detonation waves;mechanical engineering computing;vibrations;fourier transform;time frequency;vibrations detonation waves fourier transforms mechanical engineering computing signal reconstruction time domain analysis;time domain analysis;time frequency analysis vibrations continuous wavelet transforms wavelet analysis discrete wavelet transforms;signal reconstruction time domain analysis blasting vibration signals fourier transforms blasting earthquake waves energy distribution;energy distribution;fourier transforms;characteristic analysis fourier transform energy distribution time frequency characteristics short time instability;signal reconstruction	The blasting earthquake waves characteristics is studied by the Fourier transform in this paper, the energy distribution of blasting vibration signals with short-time and non-stationary characteristics is analyzed by short-time Fourier transform. According to time-frequency properties of Fourier transform, the blasting vibration time signal is scanned with stratified reconstruction signal, the relative energy distribution of blasting vibration and time variation law of the vibration intensity with different frequencies are obtaapined by these signals. The measured results of blasting vibration signal show that the analysis of energy distribution characteristics based on Fourier transform can accurately gives the information of blasting vibration signal. The results provide much more feasible and efficient way of analyzing the structural security of blasting vibration.	blast;domain analysis	Baoquan Geng	2011		10.1109/EMEIT.2011.6022920	structural engineering;wavelet;fourier transform;constant q transform;discrete-time fourier transform;electronic engineering;z-transform;harmonic wavelet transform;acoustics;short-time fourier transform;continuous wavelet transform;pulse sequence;engineering;discrete fourier transform;mathematics;spectral density estimation;fourier analysis;non-uniform discrete fourier transform;spectral density;frequency domain	Theory	79.58187965264862	-39.42989485869229	89283
f11a88d8c3e44d9fee961e1dace86ce961317f89	increasing the number and volume of hexahedral and prism elements in a hex-dominant mesh by topological transformations	hex-dominant mesh;topological transformation;satisfiability;finite element analysis	This paper describes a new method for increasing the number and the volume of hexahedral and prism elements in a hexdominant mesh by topological transformations. The method takes as input a hex-dominant mesh consisting of hexahedrons, prisms, pyramids and tetrahedrons and modifies the mesh to increase the number and the volume of hexahedrons and prisms while maintaining the relaxed conformity criteria, which allows a connection from two tetrahedrons to a quadrilateral face of a hexahedron or a prism. If a hex-dominant mesh satisfies the relaxed conformity criteria, it can be used in the finite element analysis by applying an error reduction scheme on non-conforming faces [1-3], inserting pyramids on non-conforming faces [4], or converting the mesh to an all-hex mesh by a template method [5, 6]. With more hexahedrons and prisms in a hex-dominant mesh, a more accurate finite element solution can be obtained in a shorter time. Hence the proposed method increases the practical value of a hex-dominant mesh. Several experiments showed the number of hexahedrons increased by about 10% to 20%, yielding hex-dominant meshes with 70% to 90% hexahedron volume ratio.	conformity;cube;experiment;finite element method;hex;hexahedron;pony island;template method pattern	Soji Yamakawa;Kenji Shimada	2003			hexahedron;prism;polygon mesh;mixed finite element method;finite element method;quadrilateral;extended finite element method;finite element limit analysis;topology;mathematics	Visualization	68.76212376348522	-43.08117582356092	90374
044923bff862f4c697bd2e6aacee916fa4dcbf94	construction of 2-d parametric surfaces bounded with four irregular curves	interpolation;coons surface;mathematical modelling;2d surface	Conventional methods of boundary-conformed 2D surface generation including algebraic interpolation method (like the linear Coons method) usually yield some problems. This paper proposes a new method namely boundary transformation method. This new method applies a mechanism of simultaneous displacement of the interpolated curves from the opposite boundaries to generate the intermediate curves. The geometric properties considered for displacements include weighted combination of geometric and linear displacement vectors of the two opposite generating curves. The algorithm has one adjustable parameter that controls the characteristics of transformation of one set of curves from their parents. Case studies show that this algorithm gives reasonably smooth transformation of the boundaries and manipulation of the surface constructed is quite user friendly. The new algorithm is capable enough to resolve the 2D boundary-conformed parameterisation problems.		Subhajit Sarkar;Partha Pratim Dey	2012	IJCAET	10.1504/IJCAET.2012.048843	mathematical optimization;discrete mathematics;geometric design;interpolation;mathematical model;coons patch;mathematics;geometry;family of curves;statistics	HCI	68.81132010339557	-42.27642104238153	90548
0873d42ad2017b22f91981a83f528f1e8487ef26	normal meshes	surface parameterization;multiresolution;meshes;subdivision;irregular connectivity;wavelets	Normal meshes are new fundamental surface descriptions inspired by differential geometry. A normal mesh is a multiresolution mesh where each level can be written as a normal offset from a coarser version. Hence the mesh can be stored with a single float per vertex. We present an algorithm to approximate any surface arbitrarily closely with a normal semi-regular mesh. Normal meshes can be useful in numerous applications such as compression, filtering, rendering, texturing, and modeling.	approximation algorithm;data compression;filter (signal processing);normal (geometry);polygon mesh;rendering (computer graphics);semiconductor industry;texture mapping	Igor Guskov;Kiril Vidimce;Wim Sweldens;Peter Schröder	2000		10.1145/344779.344831	polygon mesh;wavelet;mathematical optimization;topology;static mesh;computer science;volume mesh;subdivision;mathematics;geometry;statistics;computer graphics (images)	Graphics	68.54482770488904	-44.892700259898305	90717
6f5e51dea55c4222877da7c2f047142cf6eeba41	a novel constrained topographic independent component analysis for separation of epileptic seizure signals	health research;uk clinical guidelines;biological patents;europe pubmed central;epileptic seizure;citation search;independent component analysis;uk phd theses thesis;life sciences;uk research reports;medical journals;europe pmc;biomedical research;tk electrical engineering electronics nuclear engineering;bioinformatics	Blind separation of the electroencephalogram signals (EEGs) using topographic independent component analysis (TICA) is an effective tool to group the geometrically nearby source signals. The TICA algorithm further improves the results if the desired signal sources have particular properties which can be exploited in the separation process as constraints. Here, the spatial-frequency information of the seizure signals is used to design a constrained TICA for the separation of epileptic seizure signal sources from the multichannel EEGs. The performance is compared with those from the TICA and other conventional ICA algorithms. The superiority of the new constrained TICA has been validated in terms of signal-to-interference ratio and correlation measurement.	algorithm;antibody to islet cells of pancreas measurement;bandlimiting;blind signal separation;electroencephalography;epilepsy;independent computing architecture;independent component analysis;interference (communication);seizures;topography;visually impaired persons	Min Jing;Saeid Sanei	2007	Computational Intelligence and Neuroscience	10.1155/2007/21315	independent component analysis;speech recognition;computer science;bioinformatics;artificial intelligence;data mining	ML	82.84471220809533	-39.12401964380136	90805
8c63c6858915e89679a337fdcb1a40650f6bfc8c	reparameterization of piecewise rational bezier curves and its applications	ruled surface;three dimensions;b spline curve;general methods;bezier curve	. Although the curve segments are C 1 continuous in three dimensions, they may be C 0 continuous in four dimensions. In this case, the multiplicity of each interior knot cannot be reduced and the B-spline basis function becomes C 0 continuous. Using a surface generation method, such as skinning these kinds of rational B-spline curves to construct an interpolatory surface, may generate surfaces with C 0 continuity. This paper presents a reparameterization method for reducing the multiplicity of each interior knot to make the curve segments C 1 continuous in four dimensions. The reparameterized rational B-spline curve has the same shape and degree as before and also has a standard form. Some applications in skinned surface and ruled surface generation based on the reparameterized curves are shown.		Yoshimasa Tokuyama;Kouichi Konno	2001	The Visual Computer	10.1007/s003710100110330	three-dimensional space;mathematical analysis;topology;ruled surface;bézier curve;mathematics;geometry	Graphics	68.7093782211868	-41.23585195136404	90904
15a93f86587251e72366f5ffff10dace7304d94f	massively parallel software rendering for visualizing large-scale data sets	distributed memory;distributed memory systems;shared memory systems parallel programming rendering computer graphics data visualisation distributed memory systems;shared memory;volume rendering;large scale computing massively parallel software rendering large scale data sets data visualization unstructured grid volume data isosurfaces distributed memory parallel architectures shared memory parallel architectures direct rendering pc clusters;parallel programming;data visualisation;large scale;shared memory systems;data visualization large scale systems rendering computer graphics computational modeling partitioning algorithms hardware concurrent computing shape displays isosurfaces;unstructured grid;massively parallel computer;parallel architecture;rendering computer graphics;volume data	or some time, researchers have done production visualization almost exclusively using high-end graphics workstations. They routinely archived and analyzed the outputs of simulations running on massively parallel super-computers. Generally, a feature extraction step and a geometric modeling step to significantly reduce the data's size preceded the actual data rendering. Researchers also used this procedure to visualize large-scale data produced by high-resolution sensors and scanners. While the graphics workstation allowed interactive visual-ization of the extracted data, looking only at a lower resolution and polygonal representation of the data defeated the original purpose of performing the high-resolution simulation or scanning. To look at the data more closely, researchers could run batch-mode software rendering of the data at the highest possible resolution on a parallel supercomput-er using the rendering parameters suggested by the interactive viewing. However, researchers frequently didn't do this for several reasons. First, a supercomput-er is a precious resource. Scientists wanted to reserve their limited computer time on the supercomputers for running simulations rather than visualization calculations. Second, many of the parallel-rendering algorithms don't scale well, so the large number of massively parallel computer processors couldn't be fully and efficiently used. Third, most of the parallel-rendering algorithms were developed for meeting research curiosity rather than for production use. As a result, large and complex data couldn't be rendered cost effectively. However, the current technology trend of cheaper, more powerful computers prompted us to revisit the option of using parallel software rendering (and in some cases, discarding hardware rendering totally). Most graphics cards were mainly optimized for polygon rendering and texture mapping. Scientists can now model physical phenomena with greater accuracy and complexity. Analyzing the resulting data demands advanced rendering features that weren't generally offered by commercial graphics workstations. In addition, the short lifespan, limited resolution, and high cost of graphics workstations constrain what scientists can do. However, the decreasing cost and rapidly increasing performance of commodity PC and network technologies have let us build powerful PC clusters for large-scale computing. Supercomputing is no longer a shared resource. Scientists can build cluster systems dedicated to their own research. They can also build such systems incrementally to solve problems with increasing complexity and scale. More importantly, they can now afford to use the same machine for visualization calculations either for runtime visual monitoring of the simulation or postprocessing visualization calculations. Therefore, parallel software rendering is becoming a viable solution for visualizing large-scale data …	algorithm;archive;batch processing;central processing unit;clustered file system;complexity;computer cluster;feature extraction;geometric modeling;graphics;image resolution;parallel computing;sensor;simulation;software rendering;supercomputer;texture mapping;video card;workstation	Kwan-Liu Ma;Steven G. Parker	2001	IEEE Computer Graphics and Applications	10.1109/38.933526	shared memory;computer architecture;parallel computing;unstructured grid;distributed memory;computer science;operating system;volume rendering;data visualization;computer graphics (images)	Visualization	68.83209364876443	-51.554044870278744	91402
b33478e05ce94e23e41634b1f9e831a78bd31d88	self-organizing feature map (sofm) based deformable cad models	time varying;computational geometry;shape deformation;three dimensional;model behavior self organizing feature map deformable cad model adaptive modeling deformable hexahedral mesh interactive geometric modeling 3d sofm discrete point mass solid object resultant mass spring mesh haptic tool haptic interface time varying mesh topology virtual object;self organising feature maps;self organized feature map;self organising feature maps computational geometry mesh generation;geometric model;deformable models solid modeling shape surface fitting equations mathematical model geometry haptic interfaces libraries topology;topology preservation;mesh generation;material properties	An adaptive modeling approach that uses a self-organizing feature map (SOFM) to create deformable hexahedral meshes for interactive geometric modeling is presented in this paper. The technique uses the nodes of a three-dimensional SOFM to represent discrete point masses that comprise a solid object. Although the geometry of the resultant mass-spring mesh will change under the influence of inputs applied through a haptic tool and interface, the relative connectivity of neighboring nodes in the time-varying mesh are maintained under the external and internal forces. The initial mesh can either be retrieved from a library of primitive shapes, or created by automatically fitting the topology preserving SOFM to selected surface points. The designer reshapes the virtual object by applying external forces and pressure to the initial mesh. The accuracy of the system depends on the mathematical equations used in formulating the model behavior. The model behavior can be altered by changing the material properties in the underlying mathematical equation. Examples of shape deformation are provided to illustrate the concepts introduced.	algorithm;computer-aided design;displacement mapping;feature model;geometric modeling;graphical user interface;haptic technology;hexahedron;mesh networking;organizing (structure);resultant;self-organization;self-organizing map;time-varying mesh	Philip C. Igwe;George K. Knopf	2006	The 2006 IEEE International Joint Conference on Neural Network Proceedings	10.1109/IJCNN.2006.246873	material properties;three-dimensional space;mesh generation;computer vision;simulation;computational geometry;computer science;geometric modeling	Robotics	69.06945293625257	-45.81440402860847	91951
829d7550ae01449f22d04b2f2135cbf7516b49fe	a framework for volume segmentation and visualization using augmented reality	direct manipulation system;real time modification;control systems;motion control;image segmentation;motion tracking cube;ct mri data;direct manipulation;real time;data mining;motion tracking;data visualisation;transfer function;medical image processing;magnetic resonance imaging;data visualization;computerised tomography;volume visualization;cross section;shape control;humans;medical image processing augmented reality biomedical mri computerised tomography data visualisation image segmentation;volume segmentation;augmented reality;real time modification volume segmentation volume visualization augmented reality direct manipulation system ct mri data motion tracking cube;direct volume rendering;needles;augmented reality data visualization data mining magnetic resonance imaging tracking humans motion control control systems shape control needles;tracking;biomedical mri;volume data	We propose a two-handed direct manipulation system to achieve complex volume segmentation of CT/MRI data in Augmented Reality with a remote controller attached to a motion tracking cube. At the same time segmented data is displayed by direct volume rendering using a programmable GPU. Our system achieves visualization of real time modification of volume data with complex shading including transparency control by changing transfer functions, displaying any cross section, and rendering multi materials using a local illumination model. Our goal is to build a system that facilitates direct manipulation of volumetric CT/MRI data for segmentation in Augmented Reality. Volume segmentation is a challenging problem and segmented data has an important role for visualization and analysis.	augmented reality;cross section (geometry);direct manipulation interface;graphics processing unit;list of common shading algorithms;remote control;volume rendering	Takehiro Tawara;Kenji Ono	2010	2010 IEEE Symposium on 3D User Interfaces (3DUI)	10.1109/3DUI.2010.5444707	computer vision;simulation;computer science;scale-space segmentation;computer graphics (images)	Visualization	69.89125436275287	-49.7449565113608	92131
4fbfab7a0b299b27f10a3b1c358b7040519f5d53	parallel unstructured mesh generation by an advancing front method	advancing front method;unstructured mesh generation;dynamic load balancing;high density;large scale;parallel;mpi;distributed work;short period;mesh generation	Mesh generation is a critical step in high fidelity computational simulations. High-quality and high-density meshes are required to accurately capture the complex physical phenomena. A robust approach for a parallel framework has been developed to generate large-scale meshes in a short period of time. A coarse tetrahedral mesh is generated first to provide the basis of block interfaces and then is partitioned into a number of sub-domains using METIS partitioning algorithms. A volume mesh is generated on each sub-domain in parallel using an advancing front method. Dynamic load balancing is achieved by evenly distributing work among the processors. All the sub-domains are combined to create a single volume mesh. The combined volume mesh can be smoothed to remove the artifacts in the interfaces between sub-domains. A void region is defined inside each sub-domain to reduce the data points during the smoothing operation. The scalability of the parallel mesh generation is evaluated to quantify the improvement on sharedand distributed-memory computer systems.	algorithm;central processing unit;computer simulation;data point;delaunay triangulation;distributed memory;domain decomposition methods;interpolation;load balancing (computing);metis;parallel mesh generation;polygon mesh;scalability;shared memory;smoothing;unstructured grid;volume mesh	Yasushi Ito;Alan M. Shih;Anil K. Erukala;Bharat K. Soni;Andrey N. Chernikov;Nikos Chrisochoides;Kazuhiro Nakahashi	2007	Mathematics and Computers in Simulation	10.1016/j.matcom.2006.12.008	switched mesh;mesh generation;parallel computing;computer science;message passing interface;shared mesh;parallel;mathematics;distributed computing;t-vertices	HPC	70.13017219950622	-50.3946214176486	92529
64c81c82bd486958bc25f1d3161a36e6cefc3c4c	framewise heterodyne chirp analysis of birdsong	biology computing;biocommunications;chirp birds probes histograms frequency modulation dictionaries bandwidth;time series;signal classification;sinusoidal representation framewise chirp analysis harmonic birdsong standard fft representations chirplet techniques bird vocalisations single scale chirp analysis ordinary time series;fast fourier transforms;time series biocommunications biology computing fast fourier transforms signal classification	Harmonic birdsong is often highly nonstationary, which suggests that standard FFT representations may be of limited suitability. Wavelet and chirplet techniques exist in the literature, but are not often applied to signals such as bird vocalisations, perhaps due to analysis complexity. In this paper we develop a single-scale chirp analysis (computationally accelerated using FFT) which can be treated as an ordinary time-series. We then study a sinusoidal representation simply derived from the peak bins of this time-series. We show that it can lead to improved species classification from birdsong.	chirp;chirplet transform;fast fourier transform;heterodyne;time series;wavelet	Dan Stowell;Mark D. Plumbley	2012	2012 Proceedings of the 20th European Signal Processing Conference (EUSIPCO)		electronic engineering;speech recognition;acoustics;computer science;chirp	ML	79.97912143360602	-39.90244735097784	93038
ca1979d0afa828b96c9537e329f65193b123920a	the performance comparison of wigner-based radar target classification methods for resonance region targets	radar signal to noise ratio radar antennas conferences;radar antennas;signal to noise ratio;wigner distribution radar target classification electromagnetic resonance scattering region;conferences;radar	This study includes the performance comparison of the target classification methods based on Wigner distribution in the resonance scattering region where the dimensions of the target close to wavelengths. In the suggested method, important optimum late-time intervals of the scattered signals are theoretically determined by using the Wigner energy maps of the signals. Then, targets' feature vectors are determined for each target by using the Wigner distributions over the selected late-time region at several different reference aspects and Principal Component Analysis (PCA). These vectors are used for classification in test stage. When it is compared with the methods in literature, the suggested method has important features such as not being required to find pole numbers and values with high sensitivity, and being independent from aspect angles. In this study, tests are realized with lossless dielectric spheres, which are geometrically simple but complex targets in terms of scattering mechanism, and the method containing target-specific optimum late-time intervals is especially found as more successful.	feature vector;high-κ dielectric;lossless compression;map;principal component analysis;radar;resonance;wigner quasiprobability distribution	Salih Poyraz;Mustafa Secmen	2014	2014 22nd Signal Processing and Communications Applications Conference (SIU)	10.1109/SIU.2014.6830583	continuous-wave radar;computer science;electrical engineering;pulse-doppler radar;signal-to-noise ratio;radar	Visualization	82.03863076866612	-40.939953542890606	93198
a5fc46e15a939b0d4f4160a67c0cdb8c10d3685d	curvature-varying hyperbolic trace tfpf for seismic random noise attenuation	seismology geophysical techniques;radon noise time frequency analysis transforms attenuation noise measurement noise reduction;attenuation;noise measurement;noise reduction;transforms;radon;time frequency analysis;noise attenuation curvature varying hyperbolic trace tfpf seismic random noise attenuation time frequency peak filtering seismic records seismic record inverse radon transform radon data denoised signal seismic reflection event preservation;noise;time frequency peak filtering tfpf curvature varying hyperbolic trace radon transform reflection event preservation seismic random noise attenuation	Time-frequency peak filtering (TFPF) is effective in suppressing random noise in seismic records. However, the signal may be attenuated at the same time, especially for the high-frequency signal. In this letter, we propose a curvature-varying hyperbolic trace TFPF to reduce the error. We sample the seismic record along the time-distance curve of the event. For the event area, the sampled signal in each sampling trace is approximately linear (low frequency) due to the correlation of the signal along the time-distance curve, which reduces the bias of the seismic wave estimation brought by TFPF. However, the curvatures of seismic events are various, so adopting traces with a single curvature to sample the whole seismic record is not reasonable. As the events with different curvatures are located in different areas in Radon domain, the events with almost the same curvature can be separated out by an inverse Radon transform of only part of the Radon data. The optimal hyperbolic sampling traces are chosen for each separated subrecord, and we filter the data along the traces by TFPF. The denoised signal is reconstructed by summing all of the processed subrecords. Compared with TFPF, our method gets a good performance in both seismic reflection event preservation and noise attenuation.	aliasing;noise (electronics);sampling (signal processing);tracing (software)	Guanghai Zhuang;Yue Li;Ning Wu;Yanan Tian	2015	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2015.2464233	attenuation;seismology;time–frequency analysis;noise measurement;noise;noise reduction;mathematics;optics;radon;physics	ML	81.08833679734681	-41.58240607635498	93293
defa64a5ba951ea4e197cd19a0f1394dc62486f6	interactive design and debugging of gpu-based volume visualizations	computer and information science;volume visualization;data och informationsvetenskap;interaction design	There is a growing need for custom visualization applications to deal with the rising amounts of volume data to be analyzed in fields like medicine, seismology, and meteorology. Visual programming techniques have been used in visualization and other fields to analyze and visualize data in an intuitive manner. However, this additional step of abstraction often results in a performance penalty during the actual rendering. In order to prevent this impact, a careful modularization of the required processing steps is necessary, which provides flexibility and good performance at the same time. In this paper, we will describe the technical foundations as well as the possible applications of such a modularization for GPU-based volume raycasting, which can be considered the state-of-the-art technique for interactive volume rendering. Based on the proposed modularization on a functional level, we will show how to integrate GPU-based volume ray-casting in a visual programming environment in such a way that a high degree of flexibility is achieved without any performance impact.	abstraction layer;blue (queue management algorithm);central processing unit;debugging;german research centre for artificial intelligence;graphics processing unit;integrated development environment;interactive design;interactive visualization;layout engine;programming paradigm;rapid prototyping;visual programming language;volume ray casting;volume rendering;voreen;z1 (computer)	Jennis Meyer-Spradow;Timo Ropinski;Jörg Mensmann;Klaus H. Hinrichs	2010			visual analytics;information visualization;human–computer interaction;computer science;data science;interaction design;computer graphics (images)	Visualization	70.32177193589659	-52.067098437918695	93366
fb775008a87392e60767d50bfbd0e0d5bcd664bc	nielson-type transfinite triangular interpolants by means of quadratic energy functional optimizations		We generalize the transfinite triangular interpolant of (Nielson, 1987) in order to generate visually smooth (not necessarily polynomial) local interpolating quasi-optimal triangular spline surfaces. Given as input a triangular mesh stored in a half-edge data structure, at first we produce a local interpolating network of curves by optimizing quadratic energy functionals described along the arcs as weighted combinations of squared length variations of first and higher order derivatives, then by optimizing weighted combinations of first and higher order quadratic thin-platespline-like energies we locally interpolate each curvilinear face of the previous curve network with triangular patches that are usually only C continuous along their common boundaries. In a following step, these local interpolating optimal triangular surface patches are used to construct quasi-optimal continuous vector fields of averaged unit normals along the joints, and finally we extend the G continuous transfinite triangular interpolation scheme of (Nielson, 1987) by imposing further optimality constraints concerning the isoparametric lines of those groups of three side-vertex interpolants that have to be convexly blended in order to generate the final visually smooth local interpolating quasi-optimal triangular spline surface. While we describe the problem in a general context, we present examples in special polynomial, trigonometric, hyperbolic and algebraic-trigonometric vector spaces of functions that may be useful both in computer-aided geometric design and in computer graphics. Mathematics Subject Classification (2000) 65D17 · 65D18 · 68U05 · 68U07	computer graphics;computer-aided design;data structure;doubly connected edge list;geometric design;linear algebra;mathematics subject classification;polygon mesh;polynomial;spline (mathematics);transfinite interpolation	Ágoston Róth	2016	CoRR		mathematical optimization;mathematical analysis;discrete mathematics;mathematics;geometry;algebra	Graphics	69.05774469578516	-41.53939616562877	93415
d34d3f6ea27697d918664d31cdbcc00af31dced4	discrete geometric modeling of thick pelvic organs with a medial axis	discrete mesh;offset;parametric surface;medial axis	Modeling of soft pelvic organs and their thicknesses is a difficult task, especially when inputs are noisy and scattered. In order to define the geometric step for a global pelvic surgery simulator, we define a new method based only on geometry while considering the problem of error transfer between outer and inner organ surfaces. We compare this approach with a parametric formulation and a mass-spring system.	geometric modeling;medial graph;optic axis of a crystal	Thierry Bay;Romain Raffin;Marc Daniel	2012		10.1007/978-3-642-33564-8_2	medial axis;computer science;parametric surface;mathematics;geometry;offset	Robotics	70.01012654850103	-43.348502265073236	93619
6ae2442df770d34176e5d332a05c967f23a9836e	rbf dipole surface evolution	implicit surface;surface fitting image resolution radial basis function networks solid modelling;image resolution;embossing;rbf dipole surface evolution;level set;surface fitting;isosurfaces;surface texture;surface reconstruction;surface fitting level set surface morphology isosurfaces shape control surface texture smoothing methods embossing noise reduction solid modeling;surface morphology;radial basis function networks;surface treatment;smoothing methods;adaptation model;shape modeling operations;radial basis function;shape;surface fitting implicit surface level set radial basis function particle system;surface fitting rbf dipole surface evolution level set method shape modeling operations voxel isosurface fixed resolution grid radial basis function dipole pairs surface propagation arbitrary resolution model;noise reduction;solid modeling;particle system;fixed resolution grid;voxel isosurface;radial basis function dipole pairs;shape control;shape modeling;surface propagation;arbitrary resolution model;level set method;solid modelling	The level set method can implement a wide variety of shape modeling operations (e.g. offsetting, skeletonization, morphing, blending, smoothing, sharpening, embossing, denoising, sculpting, growing, texturing and fitting) simply by specifying a corresponding speed function that controls the growth of an evolving voxel isosurface. The problem is that the basic level set method is implemented on a fixed resolution grid, which limits the utility of these shape modeling operations. We instead represent surfaces with a collection of radial basis function dipole pairs, and derive the motion of these dipoles to implement a surface propagation similar to the level set method but on a smooth, arbitrary resoluton model. We demonstrate the utility of this approach with new level set methods for surface fitting, blending and center redistribution for RBF dipole models.	algorithm;alpha compositing;approximation;coefficient;consistency model;image embossing;implicit surface;isosurface;linear system;mathematical optimization;morphing;multivariate interpolation;noise reduction;polygon mesh;radial (radio);radial basis function;smoothing;software propagation;texture mapping;unsharp masking;voxel	Yuntao Jia;Xinlai Ni;Eric Lorimer;Michael Mullan;Ross T. Whitaker;John C. Hart	2010	2010 Shape Modeling International Conference	10.1109/SMI.2010.41	computer vision;mathematics;geometry;engineering drawing	Vision	69.72629412832428	-44.58798145301847	93774
0ffccf98d0a8d825bba1f0e1c76dd45f0bb20fba	time domain classification of transient radio frequency interference	kernel;kernel principal components analysis radio frequency interference time domain classification principal components analysis;standards;cluster separation time domain classification transient radio frequency interference planned radio telescopes meerkat ska radio frequency interference events astronomical observations principal components analysis kernel pca transient rfi sources;transient analysis;indexes;principal component analysis;principal component analysis transient analysis kernel standards indexes relays switches;radiotelescopes principal component analysis radiofrequency interference;relays;switches	Short, transient radio-frequency interference (RFI) events could threaten the quality of astronomical observations made by new and planned radio telescopes such as MeerKAT, the SKA and HERA in the radio quiet reserve in South Africa. Because they are so short, often of the order of microseconds long, these events are difficult to detect and identify in the time-frequency plots typically produced by RFI monitoring systems. In this paper, we record and analyse a dataset of the time domain RFI signals of nine typical transient RFI sources. We show that it is possible to classify such transient signals in the time domain according to their source using Principal Components Analysis (PCA) and Kernel PCA. Using an adapted measure of cluster separation, we show that Kernel PCA is significantly better than standard PCA at distinguishing transient RFI sources from one another.	interference (communication);kernel principal component analysis;radio frequency	Daniel Czech;Amit Kumar Mishra;Michael R. Inggs	2016	2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)	10.1109/IGARSS.2016.7729071	database index;kernel;speech recognition;telecommunications;network switch;computer science;machine learning;principal component analysis	Embedded	82.20544115606798	-41.943846764306535	94548
37bcd462948471294b37ce4cd3ebeb3702a84fa3	prototyping of interactive satellite image analysis tools using a real-time data-flow computer	real time data;satellite image;parallel architecture;data flow	Tools for computer-aided satellite image analysis require inter-activity, i.e. the capability to modify some parameters and see instantaneously the result of the processing, for eecient work. Due to the amount of data to process that interactivity can only be achieved by parallel ar-chitectures. In this paper, we show how a data-ow computer developed in our laboratory can be used to prototype tools for satellite image analysis. Thanks to its high computational power it was possible to implement two complex algorithms without the loss of real-time interactivity : visu-alization by means of an anamorphosis and image contrast enhancement. Both tools allow to pan across the image and provide smooth and interactive parameters adjustement. The visualization tool enables the image analyst to preserve global information when zooming on a region. Based on human vision characteristics, the contrast enhancement tool lets the image analyst interactively adjust frequency band gains to optimize target perception.	algorithm;frequency band;image analysis;interactive media;interactivity;prototype;real-time clock;real-time data;real-time transcription	Stéphane Praud;Pierre Germain;Justin Plantier	1995		10.1007/3-540-60298-4_332	embedded system;data flow diagram;computer vision;real-time data;computer science;computer graphics (images)	Visualization	70.99488174319909	-51.77658984853122	94618
3b408cd7bf721bb5e42f4fe8a791f473603d54fd	an exact general remeshing scheme applied to physically conservative voxelization	dark matter;mathematics and computing;conservative;astro comp grqc;remesh;voxelization;plasma;rasterization;vlasov;remap;hydrodynamics;poisson	We present an exact general remeshing scheme to compute analytic integrals of polynomial functions over the intersections between convex polyhedral cells of old and new meshes. In physics applications this allows one to ensure global mass, momentum, and energy conservation while applying higher-order polynomial interpolation. We elaborate on applications of our algorithm arising in the analysis of cosmological N-body data, computer graphics, and continuum mechanics problems. We focus on the particular case of remeshing tetrahedral cells onto a Cartesian grid such that the volume integral of the polynomial density function given on the input mesh is guaranteed to equal the corresponding integral over the output mesh. We refer to this as “physically conservative voxelization”. At the core of our method is an algorithm for intersecting two convex polyhedra by successively clipping one against the faces of the other. This algorithm is an implementation of the ideas presented abstractly by Sugihara (1994), who suggests using the planar graph representations of convex polyhedra to ensure topological consistency of the output. This makes our implementation robust to geometric degeneracy in the input. We employ a simplicial decomposition to calculate moment integrals up to quadratic order over the resulting intersection domain. We also address practical issues arising in a software implementation, including numerical stability in geometric calculations, management of cancellation errors, and extension to two dimensions. In a comparison to recent work, we show substantial performance gains. We provide a C implementation intended to be a fast, accurate, and robust tool for geometric calculations on polyhedral mesh elements.	algorithm;clipping (computer graphics);computation;computational physics;computer graphics (computer science);convolution;degeneracy (graph theory);depth-first search;george sugihara;graph (abstract data type);graph traversal;hoc (programming language);input/output;n-body problem;nichols plot;numerical stability;on the fly;pat hanrahan;planar graph;polygon mesh;polyhedron;polynomial interpolation;simplicial complex;simulation;singular value decomposition;tree traversal;triune continuum paradigm;unified framework;yet another	Devon Powell;Tom Abel	2015	J. Comput. Physics	10.1016/j.jcp.2015.05.022	plasma;rasterisation;mathematical optimization;combinatorics;dark matter;mathematics;geometry;poisson distribution;physics;algorithm;quantum mechanics;fluid dynamics	Graphics	68.66641867689148	-46.156078397389685	95303
8e8f298a1ed60dd9c4601bac70d474bfb7cbe28f	adaptive voids		Additive manufacturing processes have the potential to change the way we produce everyday objects. Design for additive manufacturing focuses on dealing with the characteristics and constraints of a given additive process. These constraints include both geometric and material constraints which have a major impact on the feasibility, quality and cost of the printed object. When designing for additive manufacturing, one of the desirable objectives is to reduce the amount of material while maximising the strength of the printed part. For this, the inclusion of cellular structures in the design has been an efficient way to address these constraints while supporting other application-specific requirements. These structures, which are commonly inspired by shapes found in nature, provide high strength while maintaining a low mass. In this paper we propose the adaptive voids algorithm, an automatic approach to generate, given a volume boundary, a parameterised adaptive infill primal and/or dual cellular structure for additive manufacturing. The produced output can potentially be applied in various applications, including design and engineering, architecture, clothing and protective equipment, furniture and biomedical applications.	3d printing;additive model;algorithm;data structure;polyhedron;requirement;skin (computing);thickness (graph theory);utility functions on indivisible goods	Asla Medeiros Sá;Vinícius Moreira Mello;Karina Rodriguez-Echavarria;Derek Covill	2015	The Visual Computer	10.1007/s00371-015-1109-8	mechanical engineering	Graphics	74.65768612203398	-38.582713414928996	97203
9d961a1607dbf74a1908e57871d62da1148e9d5d	the use of image processing techniques in rendering maps with deterministic chaos	image processing;visualizacion;chaos;computer graphics;caos;procesamiento imagen;dynamic system;traitement image;visualization;visualisation;image processing techniques;grafico computadora;infographie;deterministic chaos;chaos theory	Chaos theory involves the study of how complicated behavior can arise in systems which are based on simple rules, and how minute changes in the input of a system can lead to large differences in the output. In this paper, bifurcation maps of the equationX t+1=λX t [1+X t ]−β are presented, and they reveal a visually striking and intricate class of patterns ranging from stable points, to a bifurcating hierarchy of stable cycles, to apparently random fluctuations.	bifurcation theory;chaos theory;image processing;map	Clifford A. Pickover	1988	The Visual Computer	10.1007/BF01901282	computer vision;simulation;visualization;image processing;computer science;artificial intelligence;theoretical computer science;algorithm;computer graphics (images)	Graphics	73.35582306481369	-45.50414267295082	97464
838d688c7ab202daa30fd8f6e961cff9b826200c	using marching cubes on small machines	ombre;visualizacion;order;extraccion;algorithme marching cubes;sintesis imagen;algorithme;image synthesis;algorithm;visualization;sombra;isosurface;visualisation;shadow;smoothing;alisamiento;ordre;synthese image;rendu;marching cube;lissage;extraction;orden;algoritmo	Abstract   A simple technique to visualize the isosurfaces extracted from a cell-based volumetric dataset using the Marching Cubes algorithm is proposed. The technique exploits the intrinsic ordering of the triangles produced by the surface extraction algorithm by adopting a Back-to-Front visualization technique. The use of the technique together with the adoption of a simple shading algorithm permits the rendering of high resolution volumetric datasets in computational environments with limited capabilities in terms of memory and graphics hardware.	marching cubes	Claudio Montani;Roberto Scopigno	1994	CVGIP: Graphical Model and Image Processing	10.1006/cgip.1994.1017	asymptotic decider;marching tetrahedra;visualization;computer science;isosurface;marching cubes;algorithm;computer graphics (images)	Robotics	68.47213604931791	-50.48314754779666	97597
b9f16da9a9b9e834d02ec0b5f5dab4028f737a46	a review of two approaches for joining 3d meshes	local approxima tion;local approximation rbf local interpolation;mesh connection smoothness;b splines wavelets;b spline wavelets;smoothness;triangular meshes;mesh connection;rbf local interpolation	The construction of smooth surfaces of 3D complex objects is an im- portant problem in many graphical applications. Unfortunately, cracks or holes may appear on their surfaces caused by the limitation of scanners or the differ- ence in resolution levels and subdivision schemes between adjacent faces. In this paper, we introduce two approaches for joining 3D meshes of different resolu- tions to remove the cracks or holes. These approaches use a wavelet transform and a RBF local interpolation or a tangent plane local approximation. They guar- antee that the discrete continuity between meshes is preserved and the connecting mesh can change gradually in resolution between coarse and fine mesh areas.	3d printing	Anh-Cang Phan;Romain Raffin;Marc Daniel	2016		10.1007/978-3-319-46909-6_9	mathematical optimization;topology;volume mesh;mathematics;geometry	HCI	68.63851715762819	-42.22863584935676	98044
4ebb0f1d0048069bd6d0e3de70de56a3a847878b	interactive deformation of structurally complex heart models constructed from medical images	i 3 8 computer graphics;applications	We present a data structure for interactive deformation of complicated organ models, such as hearts, and a technique for automatically constructing the data structure from given medical images. The data structure is a dual model comprising of a graph structure for elastic simulation and a surface mesh for visualization. The system maps the simulation results to the mesh using a skinning technique. First, the system generates a dense graph and mesh from input medical images; then, it independently reduces them. Finally, the system establishes correspondence between the reduced graph and mesh by backtracking the reduction process. We also present an interactive browser for exploring heart shapes, and report initial feedback from target users.	backtracking;data structure;map;medical imaging;polygon mesh;simulation;skin (computing)	Kazutaka Nakashima;Yuki Koyama;Takeo Igarashi;Takashi Ijiri;Shin Inada;Kazuo Nakazawa	2016		10.2312/egsh.20161012	computer vision;computer science;multimedia;3d computer graphics;computer graphics (images)	Graphics	68.75951101013244	-45.98145315655997	98385
438d0218517d5ee956e5ed61fc69015b237cb99b	a subdivision scheme for poisson curves and surfaces	producto tensorial;ley poisson;curva bezier;polyedre;sous division stationnaire;fonction analytique;poliedro;produit tensoriel;de casteljau algorithm;condicion estacionaria;polyhedron;condition stationnaire;blending;forma geometrica;tensor product;point controle;coupage;courbe bezier;geometrical shape;control point;stationary condition;subdivision scheme;funcion analitica;forme geometrique;punto control;algorithme de casteljau;loi poisson;stationary subdivision;analytical function;curves and surfaces;analytic function;poisson distribution;mezcla;bezier curve	The de Casteljau evaluation algorithm applied to a finite sequence of control points defines a Bézier curve. This evaluation procedure also generates a subdivision algorithm and the limit of the subdivision process is this same Bézier curve. Extending the de Casteljau subdivision algorithm to an infinite sequence of control points defines a new family of curves. Here, limits of this stationary non-uniform subdivision process are shown to be equivalent to curves whose control points are the original data points and whose blending functions are given by the Poisson distribution. Thus this approach generalizes standard subdivision techniques from polynomials to arbitrary analytic functions. Extensions of this new subdivision scheme from curves to tensor product surfaces are also discussed.	algorithm;alpha compositing;bézier curve;control point (mathematics);data point;polynomial;stationary process;subdivision surface	Géraldine Morin;Ron Goldman	2000	Computer Aided Geometric Design	10.1016/S0167-8396(00)00028-5	de casteljau's algorithm;finite subdivision rule;topology;analytic function;calculus;mathematics;geometry	Graphics	68.40891387234173	-40.133658221682516	98454
7b2d14eadd45af3ba8ea8925430a69495987120e	bounding proxies for shape approximation		Many computer graphics applications use simpler yet faithful approximations of complex shapes to conduct reliably part of their computations. Some tasks, such as physical simulation, collision detection, occlusion queries or free-form deformation, require the simpler proxy to strictly enclose the input shape. While there are algorithms that can output such bounding proxies on simple input shapes, most of them fail at generating a proper coarse approximant on real-world complex shapes, which may contain multiple components and have a high genus. We advocate that, before reducing the number of primitives to describe a shape, one needs to regularize it while maintaining the strict enclosing property, to avoid any geometric aliasing that makes the decimation unreliable. Depending on the scale of the desired approximation, the topology of the shape itself may indeed have to be first simplified, to let the subsequent geometric optimization be free from topological locks.  We propose a new bounding shape approximation algorithm which takes as input an arbitrary surface mesh, with potentially complex multi-component structures, and generates automatically a bounding proxy which is tightened on the input and can match even the coarsest levels of approximation. To sustain the nonlinear approximation process that may eventually abstract both geometry and topology, we propose to use an intermediate regularized representation in the form of a shape closing, computed in real time using a new fast morphological framework designed for efficient parallel execution. Once the desired level of approximation is reached in the shape closing, a coarse, tight and bounding polygonization of the proxy geometry is extracted using an adaptive meshing scheme. Our underlying representation is both geometry- and topology-adaptive and can be optionally controlled accurately by a user, through sizing and orientation fields, yielding an intuitive brush metaphor within an interactive proxy design environment. We provide extensive experiments on various kinds of input meshes and illustrate the potential applications of our method in scenarios that benefit greatly from coarse, tight bounding substitutes to the actual high resolution geometry of the original 3D model, including freeform deformation, physical simulation and level of detail generation for rendering.	3d modeling;aliasing;approximation algorithm;closing (morphology);collision detection;computation;decimation (signal processing);dynamical simulation;experiment;free-form deformation;genus (mathematics);glossary of computer graphics;image resolution;level of detail;lock (computer science);mathematical optimization;nonlinear system	Stéphane Calderon;Tamy Boubekeur	2017	ACM Trans. Graph.	10.1145/3072959.3073714	bounding interval hierarchy;mathematical optimization;mathematics;rendering (computer graphics);polygon mesh;bounding volume hierarchy;collision detection;approximation algorithm;level of detail;bounding volume	Graphics	68.39385196977325	-46.348150874086116	99060
894a0082754c1e8406bd96db0d59c0a4a6d55216	metaballs-based physical modeling and deformation of organs for virtual surgery	skinning;metaballs;organ;deformation;期刊论文;optimization	Prior research on metaballs-based modeling solely focuses on shape geometry and its processing for organic objects. This paper takes a different approach by exploring a new metaballs-based physical modeling method for digital organs that are imperative to support virtual surgery. We propose a novel hybrid physical model comprising both surface mesh and the metaballs which occupy organs’ interior. The finer surface mesh with high-precision geometric information and texture is necessary to represent the boundary structure of organs. Through the use of metaballs, the organ interior is geometrically simplified via a set of overlapping spheres with different radii. This work’s novelty hinges upon the integration of metaballs and position-based dynamics (PBD) which enables metaballs-based organs to serve as physical models and participate in dynamic simulation. For the metaballs construction, we develop an adaptive approach based on Voronoi Diagram for model initialization. Using global optimization, an electrostatic attraction model is proposed to drive the metaballs to best match with the organ’s boundary. Using PBD, we devise a novel metaballs-based deformation algorithm, which preserves two local shape properties via constraints on Laplacian coordinates and local volume. To retain the organ’s smooth deformation, we propose a new skinning method based on distance field, and it is employed to build the mapping between the metaballs and organ boundary. This metaballs-based deformation technique has already been integrated into a VR-based laparoscopic surgery simulator.	algorithm;distance transform;dynamic simulation;global optimization;imperative programming;mathematical optimization;metaballs;programming by demonstration;skin (computing);surgery simulator;voronoi diagram	JunJun Pan;Chengkai Zhao;Xin Zhao;Aimin Hao;Hong Qin	2015	The Visual Computer	10.1007/s00371-015-1106-y	computer vision;simulation;computer science;geometry;metaballs;deformation;computer graphics (images)	Vision	68.94768853121796	-46.23659954356851	99081
d96a8e8cd927f4d8283a32ec2c56e140c5aa67bf	a hybrid mesh deformation algorithm using anisotropic pdes and multiobjective mesh optimization	anisotropy;mesh optimization;mesh warping;finite element method;deforming domain;moving mesh	We propose a hybrid mesh deformation algorithm which uses the direction of the boundary deformation to determine the positions of the interior mesh vertices in the deformed mesh. Our goal is to produce meshes on deformed domains which maintain mesh ‘similar’ element shape and possess no inverted elements. The hybrid mesh deformation algorithm consists of two steps, anisotropic finite element-based mesh warping (FEMWARP) followed by multiobjective mesh optimization. The first step estimates the interior vertex positions on the deformed mesh using the boundary deformation to choose appropriate partial differential equation (PDE) coefficients in the anisotropic FEMWARP method. As a second step, we find the local optimal mesh with no inverted elements on the deformed domain by employing multiobjective mesh optimization with one term controlling element shape and a second term designed to untangle inverted elements. Numerical results show that our hybrid algorithm outperforms existing mesh deformation algorithms in terms of mesh quality and number of inverted elements and is able to preserve ‘similar’ element shape on the deformed domain while eliminating inverted elements on the deformed domain.	coefficient;experiment;finite element method;heuristic (computer science);hybrid algorithm;ibm notes;linear system;mathematical optimization;multi-objective optimization;numerical analysis;numerical method;star trek:	Jibum Kim;Brian J. Miller;Suzanne M. Shontz	2015	Computers & Mathematics with Applications	10.1016/j.camwa.2015.08.008	mesh generation;mathematical optimization;finite element method;mathematics;geometry;laplacian smoothing;anisotropy	Visualization	69.08353193959302	-43.797880521631264	99566
243c1e2f38b2e864bef334c41d45173274d45464	animating gases with hybrid meshes	construccion arquitectura tecnologia ambiental;computacion informatica;computation fluid dynamics;grupo de excelencia;physically based animation;computational fluid dynamics;staggered grid;physics based animation;ciencias basicas y experimentales;tecnologias;natural phenomena;tetrahedral mesh	This paper presents a method for animating gases on unstructured tetrahedral meshes to efficiently model the interaction of fluids with irregularly shaped obstacles. Because our discretization scheme parallels that of the standard staggered grid mesh. we are able to combine tetrahedral cells with regular hexahedral cells in a single mesh. This hybrid mesh offers both accuracy near obstacles and efficiency in open regions.		Bryan E. Feldman;James F. O'Brien;Bryan Matthew Klingner	2005	ACM Trans. Graph.	10.1145/1073204.1073281	physically based animation;simulation;computational fluid dynamics;computer science;computer graphics (images)	Graphics	70.96467459864863	-48.233881704195	99600
3299ae58c1cc2e0d0030a8a4ef1a06e162de7c7d	application and research of somatic data visualization	graphics processing technology;somatic data modeling;computer graphics;poser 7 0 development environments;reconstruction system;edge collapse somatic data visualization abstract somatic data computer graphics graphics processing technology somatic data modeling program flow data structure opengl c poser 7 0 development environments human body triangular facet grid algorithm;biological system modeling;edge collapse;somatic data;somatic data visualization;triangular facet;rendering computer graphics data flow analysis data models data visualisation;computer graphic;data model;grid simplified;data visualisation;visualization;computational modeling;development environment;program flow;three dimensional displays;human body;data visualization;visualization somatic data reconstruction system triangular facet grid simplified;data flow analysis;data visualization humans biological system modeling image reconstruction solid modeling computer graphics neck genetics computer applications computer displays;opengl;abstract somatic data;rendering computer graphics;c;visual system;data structure;solids;data models;grid algorithm	Somatic data visualization is a theory and technology that change abstraat somatic data to graph or image displayed on screen by using computer graphics and graphics processing technology. This paper studies somatic data modeling and visualization, analyses the program flow of reconstruction system, divides the system into modules, and put forward how to build system data structure. Finally, this paper introduces a somatic visualization system based on OpenGL by using C# and Poser7.0 development environment. As the number of reconstruction system of human body and triangular facet increses, which badly slows down rendering speed, this paper introduces a system which uses simplified grid algorithm based on edge collapse to record, manage and operate the reconstruction system.	algorithm;build automation;computer graphics;control flow;data modeling;data structure;data visualization;opengl	Feng Wei	2009	2009 Third International Conference on Genetic and Evolutionary Computing	10.1109/WGEC.2009.14	data modeling;computer vision;human body;simulation;visualization;visual system;data model;computer science;theoretical computer science;data-flow analysis;solid;development environment;computer graphics;computational model;data visualization;computer graphics (images)	Visualization	71.45764647221723	-49.737166948003114	99939
7ed5a4986d5f58af0f9c765c4b5980178d7c614b	techniques for interactive design using the pde method	partial differential equation;closed form solution;cad;real time;boundary value problem;pde method;interactive design;boundary condition;partial differential equations;interaction design	Interactive design of practical surfaces using the partial differential equation (PDE) method is considered. The PDE method treats surface design as a boundary value problem (ensuring that surfaces can be defined using a small set of design parameters). Owing to the elliptic nature of the PDE operator, the boundary conditions imposed around the edges of the surface control the internal shape of the surface. Moreover, surfaces obtained in this manner tend to be smooth and fair. The PDE chosen has a closed form solution allowing the interactive manipulation of the surfaces in real time. Thus we present efficient techniques by which we show how surfaces of practical significance can be constructed interactively in real time.	interactive design;interactivity	Hassan Ugail;Malcolm I. G. Bloor;Michael J. Wilson	1999	ACM Trans. Graph.	10.1145/318009.318078	mathematical optimization;mathematical analysis;boundary value problem;computer science;mathematics;geometry;pde surface;partial differential equation	Graphics	69.20996993109559	-45.5724079085523	100118
63de7b850dd01dd4789e39585be2330df91900f0	real-time simulation of brittle fracture using modal analysis	stress;damping;geometric surface;fracture surface real time simulation brittle fracture modal analysis physically based interactive application fracture initiation energy based fracture propagation algorithm contact duration contact force stiff bodies damped deformation wave damping property contact property fracture pipeline fragment geometric surface;energy based fracture propagation algorithm;damping property;modal analysis physical simulation brittle fracture;real time simulation;modal analysis brittle fracture damping mechanical contact;fracture surface;fracture pipeline;materials;stiff bodies;force;brittle fracture;mechanical contact;computational modeling;contact force;contact duration;fracture initiation;contact property;physically based interactive application;modal analysis;damped deformation wave;surface cracks;physical simulation;surface cracks computational modeling real time systems force materials stress modal analysis;real time systems;fragment	We present a novel physically based approach for simulating realistic brittle fracture of impacting bodies in real time. Our method is mainly composed of two novel parts: 1) a fracture initiation method based on modal analysis, and 2) a fast energy-based fracture propagation algorithm. We propose a way to compute the contact durations and the contact forces between stiff bodies to simulate the damped deformation wave that is responsible for fracture initiation. As a consequence, our method naturally takes into account the damping properties of the bodies as well as the contact properties to simulate the fracture. To obtain a complete fracture pipeline, we present an efficient way to generate the fragments and their geometric surfaces. These surfaces are sampled on the edges of the physical mesh, to visually represent the actual fracture surface computed. As shown in our results, the computation time performances and realism of our method are well suited for physically based interactive applications.	algorithm;cognitive function: initiation;computation;human body;modal logic;musculoskeletal diseases;pathological fracture;performance;pipeline pilot;plausibility structure;real-time clock;real-time computing;sampling (signal processing);sampling - surgical action;simulation;software propagation;time complexity	Loeïz Glondu;Maud Marchal;Georges Dumont	2013	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2012.121	damping;modal analysis;contact force;stress;computational model;force	Visualization	71.47890871912283	-47.31009302013062	100316
030dadf91c104cb73406c75bedceb7ae3764067c	a computational model for periodic pattern perception based on frieze and wallpaper groups	sensitivity and specificity;animals;image coding;frieze group;data compression;application software;wallpaper group;computer graphics;dogs;texture synthesis;computer model;gait;algorithms animals artificial intelligence biological clocks computer graphics dogs form perception gait humans image enhancement image interpretation computer assisted information storage and retrieval models statistical numerical analysis computer assisted pattern recognition automated periodicity reproducibility of results sensitivity and specificity signal processing computer assisted subtraction technique user computer interface;mathematical theory;periodic pattern;indexing terms;group theory;gait analysis computational model periodic pattern perception wallpaper groups mathematical theory crystallographic groups euclidean space frieze groups monochrome patterns computer algorithms pattern indexing texture synthesis image compression;motifs;pattern indexing;signal processing computer assisted;periodic pattern perception;periodicity;image enhancement;numerical analysis computer assisted;computational modeling;image compression;image interpretation computer assisted;periodic structures;symmetry group;indexation;solid modeling;biological clocks;reproducibility of results;monochrome patterns;wallpaper groups;pattern classification;frieze groups;gait analysis;mathematical model;group theory pattern classification data compression;models statistical;euclidean space;artificial intelligence;algorithms;pattern recognition automated;humans;tiles;computational modeling mathematical model crystallography periodic structures solid modeling application software image coding;subtraction technique;computer algorithms;user computer interface;form perception;crystallography;computational model;information storage and retrieval;crystallographic groups;lattice	"""We present a computational model for periodic pattern perception based on the mathematical theory of crystallographic groups. In each N-dimensional Euclidean space, a finite number of symmetry groups can characterize the structures of an infinite variety of periodic patterns. In 2D space, there are seven frieze groups describing monochrome patterns that repeat along one direction and 17 wallpaper groups for patterns that repeat along two linearly independent directions to tile the plane. We develop a set of computer algorithms that """"understand"""" a given periodic pattern by automatically finding its underlying lattice, identifying its symmetry group, and extracting its representative motifs. We also extend this computational model for near-periodic patterns using geometric AIC. Applications of such a computational model include pattern indexing, texture synthesis, image compression, and gait analysis."""	aic gene;akaike information criterion;algorithm;computation (action);computational model;gait analysis;image compression;indexes;mathematics;monochrome;texture synthesis	Yanxi Liu;Robert T. Collins;Yanghai Tsin	2003	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2004.1262332	computer vision;computer science;wallpaper group;theoretical computer science;machine learning;mathematics;geometry;group theory;frieze group;computational model	Vision	74.20972362251204	-46.10090076268949	100489
9d2c074751c068a288e0e39d80d00f94fd6bc446	area collapse and road centerlines based on straight skeletons	effondrement;tratamiento datos;geographic information science;straight skeleton;road centerlines;polygone;data processing;traitement donnee;cartographie;computer programs;cartografia;route;roads;polygons;carretera;cartography;skeletonization;collapse;programa computador;generalization;programme ordinateur	Skeletonization of polygons is a technique, which is often applied to problems of cartography and geographic information science. Especially it is needed for generalization tasks such as the collapse of small or narrow areas, which are negligible for a certain scale. Different skeleton operators can be used for such tasks. One of them is the straight skeleton, which was rediscovered by computer scientists several years ago after decades of neglect. Its full range of practicability and its benefits for cartographic applications have not been revealed yet. Based on the straight skeleton an area collapse that preserves topological constraints as well as a partial area collapse can be performed. An automatic method for the derivation of road centerlines from a cadastral dataset, which uses special characteristics of the straight skeleton, is shown.	algorithm;cartography;computer scientist;geographic information science;interactivity;medial graph;norm (social);optic axis of a crystal;requirement;straight skeleton;wave function collapse	Jan-Henrik Haunert;Monika Sester	2008	GeoInformatica	10.1007/s10707-007-0028-x	data processing;geography;polygon;mathematics;geometry;algorithm;cartography	Vision	71.99324953730027	-41.1665113673004	100762
0f41499db4b812e0660b3881bcbebf4893bdbe0b	eeg extended source localization: tensor-based vs. conventional methods	space time wave vector analysis;space time frequency analysis;tensor decomposition;eeg;distributed source localization	The localization of brain sources based on EEG measurements is a topic that has attracted a lot of attention in the last decades and many different source localization algorithms have been proposed. However, their performance is limited in the case of several simultaneously active brain regions and low signal-to-noise ratios. To overcome these problems, tensor-based preprocessing can be applied, which consists in constructing a space-time-frequency (STF) or space-time-wave-vector (STWV) tensor and decomposing it using the Canonical Polyadic (CP) decomposition. In this paper, we present a new algorithm for the accurate localization of extended sources based on the results of the tensor decomposition. Furthermore, we conduct a detailed study of the tensor-based preprocessing methods, including an analysis of their theoretical foundation, their computational complexity, and their performance for realistic simulated data in comparison to conventional source localization algorithms such as sLORETA, cortical LORETA (cLORETA), and 4-ExSo-MUSIC. Our objective consists, on the one hand, in demonstrating the gain in performance that can be achieved by tensor-based preprocessing, and, on the other hand, in pointing out the limits and drawbacks of this method. Finally, we validate the STF and STWV techniques on real measurements to demonstrate their usefulness for practical applications.		Hanna Becker;Laurent Albera;Pierre Comon;Martin Haardt;Gwénaël Birot;Fabrice Wendling;Martine Gavaret;Christian Bénar;Isabelle Merlet	2014	NeuroImage	10.1016/j.neuroimage.2014.03.043	psychology;mathematical optimization;neuroscience;radiology;electroencephalography;theoretical computer science;machine learning;mathematics	AI	81.73097136637601	-38.921315251467306	101388
5de7f5d5c27f8c2724f9f841d7c7218f87e81724	discrete shells origami	selected works;collision detection;bepress;shell model;physical properties	We introduce a way of simulating the creation of simple Origami (paper folding). The Origami is created in a thin shell simulation that realistically models the behavior and physical properties of paper. We demonstrate how to fold and crease the simulated paper wherever the user desires. This work employs cuttingedge advances in the field of discrete shell modeling to meet the challenge of simulating Origami. We found that the discrete shell model is capable of creating simple Origami that does not involve paper to paper collisions. For more advanced origami, however, some kind of collision detection and resolution scheme is required. Further research is necessary to implement collision handling while maintaining a practical simulation speed.	collision detection;gnome;simulation	Rob Burgoon;Zoë J. Wood;Eitan Grinspun	2006			computer science;collision detection;physical property	Robotics	70.78938981983109	-47.65434970742072	101640
501b3d8117a954a156312c695f37c6a4280c5503	interpolation with minimal-energy splines	nonlinear spline;interpolation;energie minimale;computer graphics;interpolacion;spline non lineaire;smoothing;alisamiento;energia minima;grafico computadora;infographie;lissage;minimum energy	The problem of interpolating a sequence of points in the plane with a nonlinear spline curve of minimal energy and prescribed tangents in the endpoints is addressed. The method presented is based on the idea of representing the interpolant as a curve of a piecewise polynomial curvature function. The algorithm to determine the interpolant involves a table lookup to speed up the computation of the relevant parameters, and an optimization to minimize curvature discontinuities in the curve.	algorithm;computation;interpolation;lookup table;mathematical optimization;nonlinear system;polynomial;spline (mathematics)	Guido Brunnett;Johannes Kiefer	1994	Computer-Aided Design	10.1016/0010-4485(94)90034-5	spline interpolation;spline;mathematical optimization;mathematical analysis;smoothing spline;interpolation;hermite spline;mathematics;geometry;thin plate spline;statistics	EDA	69.22289663205129	-40.36482002081314	101823
456d94d358c5af984d5789d0060cec2929eded39	four-point wavelets and their applications	scale function;four point subdivision;numerical analysis;level of detail;image compression;linear time;signal compression;subdivision scheme;multiresolution;multiresolution analysis;wavelet;physical simulation	Multiresolution analysis (MRA) and wavelets provide useful and efficient tools for representing functions at multiple levels of details. Wavelet representations have been used in a broad range of applications, including image compression, physical simulation and numerical analysis. In this paper, the authors construct a new class of wavelets, calledfour-point wavelets, based on an interpolatory four-point subdivision scheme. They are of local support, symmetric and stable. The analysis and synthesis algorithms have linear time complexity. Depending on different weight parametersw, the scaling functions and wavelets generated by the four-point subdivision scheme are of different degrees of smoothness. Therefore the user can select better wavelets relevant to the practice among the classes of wavelets. The authors apply the four-point wavelets in signal compression. The results show that the four-point wavelets behave much better than B-spline wavelets in many situations.	algorithm;b-spline;dynamical simulation;image compression;image scaling;interpolation;multiresolution analysis;numerical analysis;signal compression;spline wavelet;subdivision surface;time complexity	Guofu Wei;Falai Chen	2002	Journal of Computer Science and Technology	10.1007/BF02943287	multiresolution analysis;time complexity;wavelet;computer vision;mathematical optimization;numerical analysis;image compression;computer science;legendre wavelet;level of detail;fast wavelet transform;gabor wavelet;algorithm	Graphics	73.00506916212917	-42.2256683491686	101865
a408828fbfa5769d7dc3be64bd32a852aff4cf71	eyelet particle tracing - steady visualization of unsteady flow	time-varying data vi- sualization;vector/tensor visualization;flow visualization;3d vector field visualization;computer animation;flow visualisation;computational fluid dynamics;particle tracing;data visualisation;vector field;computational geometry	"""It is a challenging task to visualize the behavior of time-dependent 3D vector fields. Most of the time an overview of unsteady fields is provided via animations, but, unfortunately, animations provide only transient impressions of momentary flow. In this paper we present two approaches to visualize time varying fields with fixed geometry. Path lines and streak lines represent such a steady visualization of unsteady vector fields, but because of occlusion and visual clutter it is useless to draw them all over the spatial domain. A selection is needed. We show how bundles of streak lines and path lines, running at different times through one point in space, like through an eyelet, yield an insightful visualization of flow structure (""""eyelet lines""""). To provide a more intuitive and appealing visualization we also explain how to construct a surface from these lines. As second approach, we use a simple measurement of local changes of a field over time to determine regions with strong changes. We visualize these regions with isosurfaces to give an overview of the activity in the dataset. Finally we use the regions as a guide for placing eyelets."""	clutter;contour line;isosurface	Alexander Wiebel;Gerik Scheuermann	2005	VIS 05. IEEE Visualization, 2005.	10.1109/VIS.2005.38	computer vision;simulation;flow visualization;computational geometry;computer science;mathematics;geometry;data visualization;computer graphics (images)	Visualization	72.5003436953491	-45.51301585550149	101866
2e046f6e265c5781fd625f46cab53a8675d38434	target detection of non-stationary radar signal and riemannian geometry	long ar model;non-stationary radar signal;riemannian geometry;smoothness constraint;target detection	Smooth prior long AR model (SLAR) and Riemannian geometry (RG) method have been combined in this paper to realize target detection of non-stationary radar signal. Firstly, SLAR is used for the parameterization of non-stationary signal. Then, the signal is mapped to a parameter vector space which can be described as a complex Riemannian manifold. Each point of this manifold is identified by a vector of AR coefficients. Numeric experiments and real radar target detection within sea clutter are given to demonstrate the effectiveness of our proposed target detection method. © 2013 Springer-Verlag.	radar;stationary process	Haiyan Fan;Yongmei Jiang;Gangyao Kuang	2013		10.1007/978-3-642-40020-9_92	geometry	EDA	80.90508760837716	-39.024480631915345	101900
2e7cfe9cc36425954c905c0b280574a83ab15b46	isotopic approximations and interval solids	tecnologia electronica telecomunicaciones;computacion informatica;grupo de excelencia;surface reconstruction;ciencias basicas y experimentales;interval solids;offsets and deformations;tecnologias;ambient isotopy;computational topology;reverse engineering;surface analysis	Given a nonsingular compact two-manifold F without boundary, we present methods for establishing a family of surfaces which can approximate F so that each approximant is ambient isotopic to F: The methods presented here offer broad theoretical guidance for a rich class of ambient isotopic approximations, for applications in graphics, animation and surface reconstruction. They are also used to establish sufficient conditions for an interval solid to be ambient isotopic to the solid it is approximating. Furthermore, the normals of the approximant are compared to the normals of the original surface, as these approximating normals play prominent roles in many graphics algorithms. The methods are based on global theoretical considerations and are compared to existing local methods. Practical implications of these methods are also presented. For the global case, a differential surface analysis is performed to find a positive number r so that the offsets Foð^rÞ of F at distances ^r are nonsingular. In doing so, a normal tubular neighborhood, FðrÞ; of F is constructed. Then, each approximant of F lies inside FðrÞ: Comparisons between these global and local constraints are given. q 2004 Elsevier Ltd. All rights reserved.	ambient calculus;approximation algorithm;graphics	Takis Sakkalis;Thomas J. Peters;Justin Bisceglio	2004	Computer-Aided Design	10.1016/j.cad.2004.01.008	computational topology;surface reconstruction;computer science;surface weather analysis;calculus;mathematics;geometry;algorithm;reverse engineering	Graphics	69.7539962521535	-40.99932858761588	102022
6eb483eb4b952f37d8f35f2237c89377405d4e75	domain decomposition techniques for parallel generation of tetrahedral meshes	geometric decomposition;domain decomposition;anisotropic metric;riemannian metric;unstructured mesh;mesh generation;tetrahedral mesh	We present solutions for dealing with the problem of parallel generation of unstructured meshes for 3D domains. The selected approach is based on a geometric decomposition of the domain where the input data is given in the form of a surface mesh. The difference between the two presented decomposition techniques lies in the step where the actual partitioning takes place. In the first method the partitioning is obtained using solely the surface mesh while in the latter one a coarse tetrahedral mesh is used. After the decomposition and creation of an interface mesh (which is a surface mesh in both approaches) the final volume meshes are generated independently for each subdomain. The interface mesh and the subdomain meshes are constructed using a Riemannian metric combined with a control space structure which allows to generate meshes with varied density and anisotropy [1].	domain decomposition methods	Barbara Glut;Tomasz Jurczyk	2008		10.1007/978-3-540-69384-0_69	mesh generation;mathematical optimization;topology;computer science;volume mesh;mathematics;geometry;laplacian smoothing;domain decomposition methods;t-vertices	HPC	69.56485202880668	-44.83836094231176	102372
a8a377e5992bde348ad97684a4c166fd2704f4be	two adaptive detectors for range-spread targets in non-gaussian clutter	detection probability;generalized likelihood ratio test;constant false alarm rate;journal;degeneration;generalized likeli hood ratio test;range spread target detection;false alarm probability;spherically invariant random vector;high resolution radar;covariance matrix	This paper addresses adaptive detection of range-spread target in spherically invariant random vector clutter. Based on the nonadaptive detectors of NSDD-GLRT and SDD-GLRT, two adaptive detectors named ANSDD-GLRT and ASDD-GLRT are devised by replacing the unknown normalized clutter covariance matrix with the sample covariance matrix based on the secondary data. The formulas of detection probability and false alarm probability are deduced. Moreover, the constant false alarm rate properties of both adaptive detectors are assessed and the influence of mismatched steering vector on detection probability is analyzed. The experimental results show that the adaptive detectors have an acceptable loss with respect to the nonadaptive counterparts. Furthermore, the ANSDD-GLRT converges much faster than the ASDD-GLRT, however, the latter rejects the misaligned signals more effectively. Both detectors perform well for the uniformly-distributed-energy target, but degenerate for the fluctuating targets. In addition, the correlation between target scatterers results in an additional gain for low signal-to-clutter power ratio.	clutter;constant false alarm rate;sensor	You He;Tao Jian;Feng Su;Dianfa Ping	2010	Science China Information Sciences	10.1007/s11432-010-4164-9	covariance matrix;speech recognition;likelihood-ratio test;pattern recognition;constant false alarm rate;mathematics;statistics	Vision	79.31915075091325	-48.020094408543486	102999
c9979f322a3e1c5a94c73f0d62e695cc59d47512	parameter estimation and reconstruction of digital conics in normal positions	parameter estimation	Reconstruction of the original curve (and the estimation of its parameters) from its digitization is a challenging problem as quantization always causes some loss of information. So we often estimate at least one (or aU) continuous curve(s) which is (are) isomorphic to the original one under discretization. Some work has already been done in this respect on straight lines, circles, squares, etc. In this paper, we have attempted this problem for a specialized class of tonics which are said to be in normal positions. In normal position the center of the conic is situated at a grid point and its axes are parallel to the coordinate axes. For circles and parabolas, we can directly formulate the domain, i.e., the entire set of continuous curves which produces the same digitization. For ellipses (and this can be extended to hyperbolas too), we first compute the smallest rectangle containing the domain of the given digitization, and then estimate the domain itself. The major contribution of this paper lies in the development of a new method of analysis (via the iterative refinement of parameter bounds) which can be easily extended to other lor Z-parameter piecewise monotonic shapes such as straight lines or circles with known radius.	discretization;estimation theory;iterative method;iterative refinement;quantization (signal processing);refinement (computing);situated	S. Chattopadhyay;Partha Pratim Das	1992	CVGIP: Graphical Model and Image Processing	10.1016/1049-9652(92)90023-Q	mathematical optimization;combinatorics;computer science;mathematics;geometry;estimation theory	Vision	70.8249164763112	-41.513005713488546	103428
87bf24326fb259c27fe149bc5125bb0551fc5a2c	a realistic and real-time watercolour painting engine using domain decomposition scheme	pigment mixing;non photorealistic rendering;art;domain decomposition;drying;painting systems;colour modelling;lattice boltzmann method;lbm;fluid simulation;watercolour painting;diffusion	We present an efficient watercolour painting system, which can work interactively on low-powered computing machines, even mobile devices. A subdomain based lattice Boltzmann method (LBM) is applied to guarantee realism of the simulated watercolour painting output and to enhance the computational efficiency. Our system reflects essential features of watercolour painting such as diffusion, pigment mixing, and drying. We provide an improved colour model, which is adequate for handling subtractive pigment mixing. Results from experiments show that our system can perform interactively, and can even be utilised even for professional art work.	computer;domain decomposition methods;experiment;interactivity;lattice boltzmann methods;mixing (mathematics);mobile device;pigment;real-time clock	Junkyu Oh;Yongho Seo;Taesoo Kwon;Jinho Park	2015	IJSNet	10.1504/IJSNET.2015.072877	fluid simulation;simulation;computer science;lattice boltzmann methods;drying;non-photorealistic rendering;domain decomposition methods;diffusion;computer graphics (images)		69.90411425775157	-49.91752432034025	103812
6ef827c346fe1ca5d92515ae4e259c744b2ca1a8	physically based baking animations with smoothed particle hydrodynamics	fluid-solid phase transition;baking process;peculiar type;mechanical property;particle hydrodynamics;new model;physically-based animation;adaptive field function;volume expansion;surface browning	In this paper, we propose a new model for creating physically-based animations of the baking process. Our model is capable of reproducing the fluid-solid phase transition, volume expansion, and surface browning that take place during the baking process. Furthermore, an adaptive field function is presented that is able to reconstruct the surface of the baked good as its volume expands. The model is very flexible in that it can reproduce the mechanical properties of a wide array of fluids from thin fluids to semi-solids. The sequences presented show that the proposed model can produce animations of different and peculiar types of bread.	smoothed-particle hydrodynamics;smoothing	Omar Rodriguez-Arenas;Yee-Hong Yang	2011			simulation	EDA	71.35153019866803	-47.99503610881619	103964
76721242b7795524d2a28208dc21e38419638ee7	real-time crying simulation	real time;real time simulation;physical characteristic;smooth particle hydrodynamics;facial animation;on the fly;fluid simulation	Displaying facial motions such as crying or laughing is difficult to achieve in real-time simulations and games. Not only because of the complicated simulation of the physical characteristics such as muscle motions or fluid simulations, but also because one needs to know how to control these motions on a higher level. In this paper, we propose a method that uses the MPEG-4 Facial Animation standard to control a realistic crying face in real-time. The tear simulation is based on the Smoothed Particle Hydrodynamics technique, which we optimized for real-time tear generation and control. Through simple parameters, a wide range of expressions and tears can be generated on the fly. Additionally, our method works independently of the graphics and physics engines that are used.	dialog system;embodied agent;face animation parameter;fluid animation;gate;graphics;on the fly;physics engine;real-time clock;real-time transcription;simulation;skeletal animation;smoothed-particle hydrodynamics;smoothing;while	Wijnand van Tol;Arjan Egges	2009		10.1007/978-3-642-04380-2_25	fluid simulation;smoothed-particle hydrodynamics;simulation;computer facial animation;computer science;communication;computer graphics (images)	Graphics	70.6403720281733	-48.42849591886964	104291
8e32f976692f51625638aefbda460198f5d56b33	efficient mesh motion using radial basis functions with volume grid points reduction algorithm		Abstract As one of the most robust mesh deformation technique available, the radial basis function (RBF) mesh deformation has been accepted widely. However, for volume mesh deformation driven by surface motion, the RBF system may become impractical for large meshes due to the large number of both surface (control) points and volume points. Surface points selection procedure based on the greedy algorithm results in an efficient implementation of the RBF-based mesh deformation procedure. The greedy algorithm could reduce the number of surface points involved in the RBF interpolation while acquire an acceptable accuracy as shown in literature. To improve the efficiency of the RBF method furthermore, an issue that how to reduce the number of the volume points needed to be moved is addressed. In this paper, we propose an algorithm for volume points reduction based on a wall distance based restricting function which is added to the formulation of the RBF based interpolation. This restricting function is firstly introduced by the current article. To support large deformation, a multi-level subspace interpolation is essentially needed, although this technique was used to improve the efficiency of the surface points selection procedure in the existed literature. The key point of this technique is setting the error of previous interpolation step as the object of current step, and restricting interpolation region gradually. Because the tolerance of the error is decreased hierarchically, the number of the surface points is increased but the number of the volume points needed to be moved is reduced gradually. Therefore, the CPU cost of updating the mesh motion could be reduced eventually since it scales with the product of these two numbers. The computational requirement of the proposed procedure is reduced evidently compared with the standard procedure as proved by some examples.	algorithm;algorithmic efficiency;bridging (networking);computation;interpolation;onera;radial (radio);radial basis function	Liang Xie;Hong Liu	2017	J. Comput. Physics	10.1016/j.jcp.2017.07.042	mathematical optimization;interpolation;mathematics;deformation (mechanics);grid;volume mesh;polygon mesh;algorithm;subspace topology;radial basis function;greedy algorithm	Visualization	70.70387396096416	-43.98150333330139	104392
452f36566201a55f7ca6d81cc005b67964aef3ec	fluid dynamic visualisations of cuttings-bleeding for virtual reality heart beating surgery simulation	heart;virtual reality;gpu;fluid dynamics;surgery simulation	Visualisations and computations of a real time fluid dynamic rendering have always been a fascinating research topic in the field of computer graphics. However most existing virtual realities surgery simulators tend to avoid the fluid dynamic model involvement in their system. This is due to the fact that real time fluid dynamic visualisation is computationally expensive. The calculation may slow down the rendering time and reduce force feedback interactions during simulations. With the availability of high performance graphic hardware, the improvement of visual quality and computational accelerations become easier to achieve. In this paper, we propose fast and efficient fluid dynamic visualisations for a heart surgery simulation. The algorithm utilizes the GPUs capable streaming computations to generate physically-based computational fluid dynamic for bleeding in real time. Our approach is based on the Navier-Stokes Equations that is implemented on twodimensional data structure which stores height fields, blood quantity, and dissolving blood velocities. We use the cubic interpolated propagation as a fluid solver of blood movement. In this paper, the blood flowing on the surface of a beating heart when the surgical knife cut the heart muscle surface in a certain thickness. The blood will move from the sources and follow the height map of the heart surface. The comparison of the frame rates of surgery simulation with and without fluid inclusion is presented and proves that our approaches are effective enough for evolving fluid dynamic visualisation during surgical simulations. Abstract	algorithm;analysis of algorithms;computation;computational fluid dynamics;computer graphics;cubic function;data structure;graphics processing unit;haptic technology;heightmap;interaction;interpolation;mathematical model;navier–stokes equations;simulation;software propagation;solver;surgery simulator;thickness (graph theory);virtual reality	Sugeng Rianto	2010			simulation;computer science;multimedia;computer graphics (images)	Graphics	70.39650369760014	-48.29924228643765	104718
6f9a3e71b8eac55500fd1e9535f60c3ad88662cc	staircase-aware smoothing of medical surface meshes	solid;categories and subject descriptors according to acm ccs i 3 5 computer graphics computational geometry and object modeling curve;and object representations;surface	The evaluation of spatial relationships between anatomic structures is a major task in surgical planning. Surface models generated from medical image data (intensity, binary) are often used for visualization and 3D measurement of extents and distances between neighboring structures. In applications for intervention or radiation treatment planning, the surface models should exhibit a natural look (referring to smoothness of the surface), but also be accurate. Smoothing algorithms allow to reduce artifacts from mesh generation, but often degrade accuracy. In particular, relevant features may be removed and distances between adjacent structures get changed. Thus, we present a modification to common mesh smoothing algorithms, which allows to focus the smoothing effect directly to previously identified staircase artifacts. This allows to preserve non-artifact features. The approach has been applied to various data to demonstrate the suitability for different anatomical shapes. The results are compared to the ones of standard uniform mesh smoothing algorithms and are evaluated regarding smoothness and accuracy with respect to the application within surgical planning.	algorithm;automated planning and scheduling;medical imaging;mesh generation;polygon mesh;smoothing	Tobias Mönch;Simon Adler;Bernhard Preim	2010		10.2312/VCBM/VCBM10/083-090	computer vision;computer science;theoretical computer science;engineering drawing;smoothing	Visualization	68.68073679098862	-45.33838170691304	104842
27c6404f30ba535777d72e441cb6727645d2ce7a	surface remeshing in arbitrary codimensions	high dimensionality;remeshing;geometric algorithms;approximation;geometric algorithm;surface	We present a method for remeshing surfaces that is both general and efficient. Existing efficient methods are restrictive in the type of remeshings they produce, while methods that are able to produce general types of remeshings are generally based on iteration, which prevents them from producing remeshes at interactive rates. In our method, the input surface is directly mapped to an arbitrary (possibly high-dimensional) range space, and uniformly remeshed in this space. Because the mesh is uniform in the range space, all the quantities encoded in the mapping are bounded, resulting in a mesh that is simultaneously adapted to all criteria encoded in the map, and thus we can obtain remeshings of arbitrary characteristics. Because the core operation is a uniform remeshing of a surface embedded in range space, and this operation is direct and local, this remeshing is efficient and can run at interactive rates.	algorithm;approximation;computer graphics (computer science);correctness (computer science);embedded system;hilbert space;iteration;sampling (signal processing);theory	Guillermo D. Cañas;Steven J. Gortler	2006	The Visual Computer	10.1007/s00371-006-0073-8	mathematical optimization;combinatorics;approximation;mathematics;geometry;surface	Graphics	68.85669705182333	-44.771105208576635	105102
513cd9d8c47336d432eef06e9ffd501fe8095829	revisiting homomorphic wavelet estimation and phase unwrapping	spectrum;phase unwrapping	Surface-consistent deconvolution is a standard processing technique in land data to uniformize the wavelet across all sources and receivers. The required wavelet estimation step is generally done in the homomorphic domain since this is a convenient way to separate the phase and the amplitude spectrum in a linear fashion. Unfortunately all surface-consistent deconvolutions make a minimum-phase assumption which is likely to be sub-optimal. Recent developments in statistical wavelet estimation demonstrate that nonminimum wavelets can be estimated directly from seismic data, thus offering promise to create a nonminimum phase surface-consistent deconvolution approach. Unfortunately the major impediment turns out to be phase unwrapping. In this paper we review several existing phase unwrapping techniques and discuss their advantages and inconveniences.	deconvolution;instantaneous phase;minimum phase;spectral density;wavelet	Roberto Henry Herrera;Mirko van der Baan	2011	CoRR		spectrum;mathematical optimization;telecommunications;mathematics;statistics	ML	80.60850827584015	-40.84534102927226	105442
9bc8c581ccfc1698fa3de08a81d8390b8e9de3de	interpolatory, solid subdivision of unstructured hexahedral meshes	engineering design;approximate algorithm;multiresolution modeling;subdivision surface;finite element mesh;solid modeling;subdivision algorithms;multiresolution models;geometric and topological representations;subdivision scheme;volumetric meshes	This paper presents a new, volumetric subdivision scheme for interpolation of arbitrary hexahedral meshes. To date, nearly every existing volumetric subdivision scheme is approximating, i.e., with each application of the subdivision algorithm, the geometry shrinks away from its control mesh. Often, an approximating algorithm is undesirable and inappropriate, producing unsatisfactory results for certain applications in solid modeling and engineering design (e.g., finite element meshing). We address this lack of smooth, interpolatory subdivision algorithms by devising a new scheme founded upon the concept of tri-cubic Lagrange interpolating polynomials. We show that our algorithm is a natural generalization of the butterfly subdivision surface scheme to a tri-variate, volumetric setting.	approximation algorithm;color mapping;cubic function;data (computing);data structure;engineering design process;finite element method;hexahedron;ibm notes;imperative programming;interpolation;lagrange polynomial;patch (computing);polygon mesh;polynomial;provable prime;refinement (computing);scientific visualization;scott continuity;simulation;smoothing;solid modeling;subdivision surface;triangular function;whole earth 'lectronic link;xfig	Kevin T. McDonnell;Yu-Sung Chang;Hong Qin	2004	The Visual Computer	10.1007/s00371-004-0246-2	mathematical optimization;mathematics;geometry;solid modeling;subdivision surface;engineering design process;mechanical engineering	Graphics	69.6406226173577	-43.023946153800104	106580
c573f7704fbd0ca9bdf6c589fe98f18d1b229711	linfo - a visual basic program for lineament density, frequency and intersection density analysis		This paper describes the development of visual basic program called ‘Linfo’ which can be used for calculation of spatial properties (orientation, length, density, frequency and intersection density) of lineaments. The program allows the user to analyze the orientation of lineaments easily and shows the results in the form of rose diagram. Linfo can handle large number of lineaments at a time and calculates the results faster than any other software. Linfo generates regular square grids over lineament data and calculates the number, length and intersections of lineaments that fall within each cell. The program is validated with lineament data of Kerala state, India and prepared the spatial maps using Inverse Distance Weighted (IDW) interpolation method. The experimental results show that Linfo is useful for generating spatial maps using any interpolation method. Potential application of the program includes demarcation of groundwater potential zones, landslide risk assessment etc.	visual basic	A. C. Dinesh;Vipin Joseph Markose;K. S. Jayappa	2014	Earth Science Informatics	10.1007/s12145-013-0134-2	computer vision	Logic	80.8710284942028	-51.90191461006799	106594
2c2d88952d3620de45b9ea1679b5f54a50ac6262	improving surface meshing from discrete data by feature recognition	discrete data;feature recognition;geometric model;mesh generation;reverse engineering	In this paper, we propose a method to identify, on a mesh, geometric primitives commonly used in mechanical parts (plane, sphere, cylinder, torus, cone) in order to improve the quality of the surface remeshing. We have already presented techniques to adapt an existing surface mesh based on a mesh-free technique denoted as diffuse interpolation. In this approach, a secondary local geometrical model is built from the mesh. From this model, principal curvatures are calculated and the type of surface can be determined from the computation of the curvatures. Some of the concepts presented here are original while others have been adapted from techniques used in reverse engineering. Our approach is not limited to feature recognition on meshes but has been extended to a set of points.	computation;cylinder seal;discrete mathematics;feature recognition;interpolation;reverse engineering	C. Chappuis;Alain Rassineux;Piotr Breitkopf;Pierre Villon	2004	Engineering with Computers	10.1007/s00366-004-0288-0	feature recognition;mesh generation;computer vision;engineering;geometric modeling;mathematics;geometry;engineering drawing;reverse engineering;mechanical engineering	Graphics	68.33795738093517	-42.054271429569624	106866
a9bb1427bd70e99f52c1779ef5517773998a4d3e	approximate conversion of a rational boundary gregory patch to a nonuniform b-spline surface	curva bezier;image processing;geometrie algorithmique;computational geometry;procesamiento imagen;c continuity;traitement image;rational boundary gregory patch;b spline surface;least squares;gregory patch;courbe bezier;least square;geometria computacional;b spline;shape modeling;nonuniform b spline surface;conversion;b splin;bezier curve	A rational boundary Gregory patch is characterized by the facts that anyn-sided loop can be smoothly interpolated and that it can be smoothly connected to an adjacent patch. Thus, it is well-suited to interpolate complicated wire frames in shape modeling. Although a rational boundary Gregory patch can be exactly converted to a rational Bézier patch to enable the exchange of data, problems of high degree and singularity tend to arise as a result of conversion. This paper presents an algorithm that can approximately convert a rational boundary Gregory patch to a bicubic nonuniform B-spline surface. The approximating surface hasC 1 continuity between its inner patches.	algorithm;b-spline;bicubic interpolation;bézier curve;scott continuity;smoothing	Yoshimasa Tokuyama;Kouichi Konno	1995	The Visual Computer	10.1007/BF01909876	topology;image processing;computational geometry;computer science;calculus;mathematics;geometry;least squares;statistics	Graphics	68.56820560899757	-41.029769828084966	107203
476843f423934486e6ca686837a0c8776e40ba33	an analytical representation of conformal mapping for genus-zero implicit surfaces and its application to surface shape similarity assessment	implicit surface;analytical representation;conformal mapping;surface matching;期刊论文;shape similarity assessment	This paper develops an analytical representation of conformal mapping for genus-zero implicit surfaces based on algebraic polynomial functions, and its application to surface shape similarity assessment. Generally, the conformalmapping oftenworks as a tool of planar or spherical parameterization for triangle mesh surfaces. It is further exploited for implicit surface matching in this study. The method begins with discretizing one implicit surface by triangle mesh, where a discrete harmonic energy model related to both the mesh and the other implicit surface is established based on a polynomial-function mapping. Then both the zero-center constraint and the landmark constraints are added to the model to ensure the uniqueness of mapping result with the Möbius transformation. By searching optimal polynomial coefficients with the Lagrange–Newton method, the analytical representation of conformal mapping is obtained, which reveals all global and continuous one-to-one correspondent point pairs between two implicit surfaces. Finally, a shape similarity assessment index for (two) implicit surfaces is proposed through calculating the differences of all the shape index values among those corresponding points. The proposed analytical representation method of conformal mapping and the shape assessment index are both verified by the simulation cases for the closed genus-zero implicit surfaces. Experimental results show that the method is effective for genus-zero implicit surfaces, which will offer a new way for object retrieval and manufactured surface inspection. © 2015 Elsevier Ltd. All rights reserved.	algebraic equation;coefficient;implicit surface;linear algebra;newton's method;one-to-one (data model);polynomial;simulation;triangle mesh	Shunzhou Huang;Hao Wang;Yong Zhao;Zhongqin Lin	2015	Computer-Aided Design	10.1016/j.cad.2015.02.002	conformal map;combinatorics;topology;mathematics;geometry	Graphics	69.46887240521593	-39.83083989651967	107230
1a797880dac57b4efab0453c2156f4fcc7c6a23c	fast optimization-based elasticity parameter estimation using reduced models	finite element method;computer animation;physically based modeling	Elasticity parameters are central to physically-based animation and medical image analysis. We present an accelerated method to automatically estimate these parameters for a deformation simulator using an iterative optimization framework, given the desired (target) output surface/shape. During the optimization, the input model is deformed by the simulator, and the distance between the deformed surface and the target surface is minimized numerically. To accelerate the optimization process, we introduce a dimension reduction technique to allow a trade-off between the computational efficiency and desired accuracy. The reduced model is constructed using statistical training with a set of example deformations. To demonstrate this approach, we apply the computational framework to 2D animations of elastic bodies simulated with a linear finite element method. We also present a 3D elastography example, which is simulated with a reduced-dimension finite element model to improve the performance of the optimizer.	computation;dimensionality reduction;distance transform;elasticity (data store);elastography;entity–relationship model;estimation theory;finite element method;image analysis;iterative method;landmark point;mathematical optimization;medical image computing;medical imaging;modulus robot;nonlinear system;numerical analysis;physically based animation;ron sun;simulation;surgery simulator;teaching method	Huai-Ping Lee;Ming C. Lin	2012	The Visual Computer	10.1007/s00371-012-0686-z	mathematical optimization;simulation;computer science;finite element method;computer animation;mixed finite element method	Vision	71.02027903106689	-46.372000022174575	107354
e45f844ad22c66eaba0588735c6befd16d20cb99	maximum likelihood estimation of object location in diffraction tomography	diffracted wavefield;rytov approximation;phase measurement;diffraction;maximum likelihood;phase noise;noisy data;noisy measurements;object location;filtered backpropagation algorithm;filters;complex phase;least squares approximation;frequency measurement;backpropagation;maximum likelihood estimation;additive zero mean gaussian white noise;gaussian white noise;generalized projection;maximum likelihood estimate;position measurement computerised picture processing computerised tomography;filter function;backpropagation algorithm;least square;position measurement;maximum likelihood estimation diffraction tomography filters context modeling least squares approximation frequency measurement phase measurement phase noise additive white noise;computerised tomography;computerised picture processing;additive white noise;estimation error;cramer rao bound;log likelihood function;computer simulation maximum likelihood estimation noisy measurements computerised tomography object location diffraction tomography complex phase diffracted wavefield rytov approximation log likelihood function additive zero mean gaussian white noise filtered backpropagation algorithm filter function generalized projection cramer rao bound estimation error;context modeling;computer simulation;tomography;likelihood function;diffraction tomography	The problem of estimating the location of a known object from a set of noisy measurements of the wavefields diffracted from the object in a suite of tomographic experiments is addressed using maximum likelihood estimation theory. The problem is formulated within the context of diffraction tomography where the complex phase of the diffracted wavefield is modeled using the Rytov approximation and the measurements consist of noisy renditions of this complex phase at a single frequency. The log likelihood function is computed for the case of additive zero mean Gaussian white noise and shown to be expressible in the form of the filtered backpropagation algorithm of diffraction tomography where, however, the filter function is no longer the wellknown “rho filter’’ appropriate to least squares reconstruction but is now the generalized projection (propagation) of the object (centered at the origin) onto the line(s) parallel to the measurement line(s), but passing through the origin. This result allows the estimation problem to be solved via a diffraction tomographic imaging procedure where the noisy data is filtered and backpropagated in a first step and the point of maximum value of the resulting image is then the maximum likelihood (ML) estimate of the object’s location. The paper includes a calculation of the Cramer-Rao bound for the estimation error and a computer simulation study illustrating the estimation procedure.	algorithm;approximation;backpropagation;bregman divergence;ct scan;computer simulation;estimation theory;experiment;least squares;signal-to-noise ratio;software propagation;tomography;utility functions on indivisible goods;white noise	Anthony J. Devaney;George A. Tsihrintzis	1991	IEEE Trans. Signal Processing	10.1109/78.80886	computer simulation;econometrics;mathematical optimization;mathematics;maximum likelihood;tomography;maximum likelihood sequence estimation;statistics	ML	81.03686591229446	-44.76271327004405	107518
d7d50bfb0caa4395b3f7b2f0baf063b1d1b45067	noise robust surface reconstruction by combining pu and graph-cut		We present a novel method of reconstructing surfaces from 3D scattered points by combining Partition of Unity (PU) and a Graph-cut approach. PU is a local approximation technique, meaning that the surfaces obtained have high accuracy but are sensitive to noise. Graph-cut, on the other hand, is a global algorithm that is robust to noise but produces low-accuracy results because it is a discrete binary operation. Our algorithm combines these two methods to achieve robust, high accuracy surface reconstruction. First, a PU implicit function is constructed by covering a space containing a point cloud with spherical supports of linear polynomials. Graph-cut is then performed to separate the covered domain into inside and outside areas of the object to be reconstructed. Finally, we extract the zero-level of PU using the marching tetrahedra approach.	algorithm;approximation;cut (graph theory);ibm systems network architecture;marching tetrahedra;one-class classification;point cloud;polynomial	Yukie Nagai;Yutaka Ohtake;Hiromasa Suzuki	2009		10.2312/egs.20091052	mathematical optimization;cut;surface reconstruction;mathematics	Vision	69.40112290319789	-42.53817950744001	107812
146dabb84b8f6ef6441bdc749f0c939be525bde5	layered tetrahedral meshing of thin-walled solids for plastic injection molding fem	injection molding;triangular mesh;finite element method;plastic injection molding;finite element analyses;wall thickness;finite element analysis;tetrahedral mesh	This paper describes a method for creating a well-shaped, layered tetrahedral mesh of a thin-walled solid by adapting the surface triangle sizes to the estimated wall thickness. The primary target application of the method is the finite element analysis of plastic injection molding, in which a layered mesh improves the accuracy of the solution. The edge lengths of the surface triangles must be proportional to the thickness of the domain to create well-shaped tetrahedrons; when the edge lengths are too short or too long, the shape of the tetrahedron tends to become thin or flat. The proposed method creates such a layered tetrahedral mesh in three steps: (1) create a preliminary tetrahedral mesh of the target geometric domain and estimate thickness distribution over the domain; (2) create a non-uniform surface triangular mesh with edge length adapted to the estimated thickness, then create a single-layer tetrahedral mesh using the surface triangular mesh; and (3) subdivide tetrahedrons of the single-layer mesh into multiple layers by applying a subdivision template. The effectiveness of the layered tetrahedral mesh is verified by running some experimental finite element analyses of plastic injection molding.	air traffic control radar beacon system;finite element method;subdivision surface;thickness (graph theory)	Soji Yamakawa;Charles Shaw;Kenji Shimada	2005		10.1145/1060244.1060272	mesh generation;finite element method;mathematics	Visualization	70.76251737665073	-45.12408543704192	108016
e2353c447a44c26a4d44e156e40e25e09a44707e	properties of g1 continuity conditions between two b-spline surfaces	systeme degenere;curva bezier;computer graphics;fonction polynomiale;fonction speciale;degenerate system;linear functionals;funcion especial;b spline surface;sistema degenerado;degeneration;courbe bezier;special function;spline function;b spline;funcion polinomial;polynomial function;grafico computadora;infographie;b splin;bezier curve	This paper addresses some properties of G1 continuity conditions between two B-spline surfaces with arbitrary degrees and generally structured knots. Key issues addressed in the paper include necessary G1 continuity conditions between two B-spline surfaces, general connecting functions, continuities of the general connecting functions, and intrinsic conditions of the general connecting functions along the common boundary. In general, one may use piecewise polynomial functions, i.e. B-spline functions, as connecting functions for G1 connection of two B-spline surfaces. Based on the work reported in this paper, some recent results in literature using linear connecting functions are special cases of the general connecting functions reported in this paper. In case that the connecting functions are global linear functions along the common boundary commonly used in literature, the common boundary degenerates as a Bezier curve for proper G1 connection. Several examples for connecting two uniform biquadratic B-spline surfaces with G1 continuity are also presented to demonstrate the results.	b-spline;scott continuity	Nailiang Zhao;Weiyin Ma	2006		10.1007/11784203_73	b-spline;spline;mathematical analysis;topology;computer science;bézier curve;mathematics;geometry;computer graphics	Robotics	68.43026628339106	-40.1394169193616	108778
22ecf061b3f2aa5271fc1c4bc8cf9801e20eed13	fast generation of spherical slicing surfaces for irregular volume rendering	proyeccion;tranchage;image processing;generic algorithm;volume rendering;efficient algorithm;procesamiento imagen;cell;seed cells;tetrahedral shape;algoritmo genetico;forme spherique;traitement image;irregular volume;spherical shape;slicing;isosurface;forme tetraedrique;projection;forma esferica;algorithme genetique;chapeado;superficie;genetic algorithm;surface;cellule;celula;forma tetraedrica	An efficient algorithm for generating a set of concentric spherical slicing surfaces for volume rendering of irregular volume datasets is presented. Our original algorithm, which approximates volume rendering by accumulating concentric spherical slicing surfaces from back to front, generates these surfaces by means of a conventional isosurface generation algorithm. However, this causes a performance bottleneck. To solve the problem, we propose a proliferous generation of slicing surfaces from seed cells, which are automatically determined according to the extremum points of the values of distances from a viewing point. A benchmark test shows that this approach can improve the performance significantly. In addition, we compare this algorithm with a raycasting algorithm that we proposed previously, and discuss a criterion for selecting which one to use for maximizing the performance	algorithm;benchmark (computing);isosurface;maxima and minima;ray casting;volume rendering	Koji Koyamada;Takayuki Ito	1995	The Visual Computer	10.1007/BF01898602	genetic algorithm;image processing;computer science;artificial intelligence;mathematics;geometry;computer graphics (images)	Visualization	70.8702255404738	-50.71291717772013	108846
6a8d32408ef2854edc706a8575065999a896f3f3	a vlsi architecture for video object motion estimation using a 2d hierarchical mesh model	video object;building block;motion estimation;progressive mesh;motion vector;video object plane;performance analysis;code size;mobile application;vlsi architecture	This paper proposes a novel hierarchical mesh-based video object model and a motion estimation architecture that generates a contentbased video object representation. The 2D mesh-based video object is represented using two layers: an alpha plane and a texture. The alpha plane consists of two layers: (1) a mesh layer and (2) a binary layer that defines the object boundary. The texture defines the object’s colors. A new hierarchical adaptive structured mesh represents the mesh layer. The proposed mesh is a coarse-to-fine hierarchical 2D mesh that is formed by recursive triangulation of the initial coarse mesh geometry. The proposed technique reduces the mesh code size and captures the mesh dynamics. The proposed motion estimation architecture generates a progressive mesh code and the motion vectors of the mesh nodes. The performance analysis for the proposed video object representation and the proposed motion estimation architecture shows that they are suitable for very low bit rate online mobile applications and the motion estimation architecture can be used as a building block for MPEG4 codec. q 2003 Elsevier Science B.V. All rights reserved.	algorithm;cmos;codec;color;low-power broadcasting;mesh networking;mobile app;motion estimation;progressive meshes;prototype;recursion;texture mapping;unstructured grid;very-large-scale integration	Wael M. Badawy	2003	Microprocessors and Microsystems	10.1016/S0141-9331(02)00106-0	computer vision;quarter-pixel motion;computer science;theoretical computer science;motion estimation;laplacian smoothing;t-vertices;computer graphics (images)	Robotics	73.18013897780244	-44.07854481395764	109050
326b02a6dff063bf5bfcd335a5d165805c9ef5c5	anisotropic mesh adaptation for evolving triangulated surfaces	free surface;dynamic surfaces;fluid solid interaction;geometric feature;metric tensor;anisotropic meshes;adaptive algorithm;mesh adaptation;anisotropic meshing;feature preservation;anisotropic mesh adaptation;multiphase flow;aspect ratio;surface analysis	Dynamic surfaces arise in many applications, such as free surfaces in multiphase flows and moving interfaces in fluid–solid interaction. In many engineering applications, an explicit surface triangulation is often used to represent dynamic surfaces, posing significant challenges in adapting their meshes, especially if large curvatures and sharp features may dynamically emerge or vanish as the surfaces evolve. In this paper, we present an anisotropic mesh adaptation technique to meet these challenges. Our technique strives for optimal aspect ratios of the triangulation to reduce positional errors and to capture geometric features of dynamic surfaces based on a novel extension of the quadrics. Our adaptation algorithm combines the operations of vertex redistribution, edge flipping, edge contraction, and edge splitting. Experimental results demonstrate the effectiveness of our anisotropic adaptation technique for static and dynamic surfaces.	algorithm;approximation theory;benchmark (computing);edge contraction;numerical analysis;polygon mesh;simulation;smoothing;surface triangulation;unified framework;vanish (computer science)	Xiangmin Jiao;Andrew Colombi;Xinlai Ni;John Hart	2006	Engineering with Computers	10.1007/s00366-009-0170-1	mathematical optimization;metric tensor;aspect ratio;surface weather analysis;mathematics;geometry;free surface;engineering drawing	Visualization	69.07984560408075	-45.45328659066287	109620
26b488f466de86583a5acdbdca3802ef9b857867	modeling with cubic a-patches	three dimensions;algebraic surfaces;computer aided geometric design	We present a sufficient criterion for the Bernstein Bezier (BB) form of a trivariate polynomial within a tetrahedron, such that the real zero contour of the polynomial defines a smooth and single-sheeted algebraic surface patch, We call this an A-patch. We present algorithms to build a mesh of cubic A-patches to interpolate a given set of scattered point data in three dimensions, respecting tbe topology of any surface triangulation T of the given point set. In these algorithms we first specify “normals” an the data points, then build a simplicial hull consisting of tetrahedral surrounding the surface triangulation 2’, and finally construct cubic A-patches within each tetrahedron. The resulting surface constructed is C’ (tangent plane) continuous and single sheeted in each of the tetrahedral. We also show how to adjust the free parameters of the A-patches to achieve both local and global shape control.	algebraic equation;algorithm;bernstein polynomial;cubic function;data point;eisenstein's criterion;interpolation;phil bernstein;surface triangulation	Chandrajit L. Bajaj;Jindong Chen;Guoliang Xu	1995	ACM Trans. Graph.	10.1145/221659.221662	three-dimensional space;computer vision;theoretical computer science;mathematics;geometry;algebraic surface	Graphics	69.15542472396457	-41.85169396837639	110018
deb9239d0d5a3c682e1ffde2784f702bb0b43d85	streaming mesh optimization for cad	mesh optimization;mesh quality;scientific visualization;global optimization;tetrahedral mesh;computer simulation	Computational simulation of physical phenomena plays a central role in many important applications, including scientific visualization and the generation visual effects for entertainment. Typically, these simulations rely on high-quality meshes to model physical objects. Meshes with badly shaped elements degrade both the accuracy and efficiency of the simulation. Traditionally, mesh optimization has relied on global algorithms which are ill-suited to the massive meshes demanded by many modern applications. In this paper, we describe a streaming framework for tetrahedral mesh optimization. We provide empirical results demonstrating that streaming is faster and more memory efficient than global optimization while resulting in essentially identical mesh quality. We also describe a novel streaming method for optimizing the surface of a tetrahedral mesh that is efficient, preserves features, and significantly increases the tetrahedral mesh quality.	algorithm;computation;computer-aided design;experiment;global optimization;mathematical optimization;nelder–mead method;scalability;scientific visualization;simulation;visual effects;volume mesh	Tian Xia;Eric Shaffer	2008		10.1007/978-3-540-89646-3_102	computer simulation;mesh generation;scientific visualization;simulation;computer science;theoretical computer science;t-vertices;global optimization	Visualization	70.06334574960366	-50.40272230753707	110156
86e65feadcca4abfce72144d7e39260d6bf75912	computing discrete minimal surfaces using a nonlinear spring model	minimisation;fixed boundary;surface fitting minimisation solid modelling;degenerated triangles discrete minimal surface nonlinear spring model mean curvature normal;mean curvature normal;algorithms mathematical model equations computational modeling surface treatment nonlinear systems;spider web animation;computational geometry;surface fitting;linear elastic springs;degenerated triangles;nonlinear systems;surface treatment;computational modeling;discrete minimal surface;degeneration;linear elasticity;mathematical model;algorithms;minimal surface;discrete minimal surface computing;mean curvature;nonlinear spring model;mesh generation;spider web animation nonlinear spring model discrete minimal surface computing fixed boundary linear elastic springs;solid modelling	A new algorithm can derive one or more minimal surfaces from an initial arbitrary surface with a fixed boundary. A discrete surface is a mesh represented as a set of vertices in 3D. The method extends earlier work, which used linear elastic springs to simulate a spider web animated in 3D.	3d computer graphics;algorithm;simulation;vertex (graph theory)	Yongquan Jiang;Li Chen;Qishu Chen;Qiang Peng;Jim X. Chen	2010	Computing in Science & Engineering	10.1109/MCSE.2010.127	mesh generation;minimisation;mathematical optimization;combinatorics;computational geometry;mean curvature;mathematical model;mathematics;geometry;linear elasticity;computational model;minimal surface;statistics	Graphics	69.78896876417721	-42.127588178786496	110607
ed534457810489c0638160916cb0dfb8482b1971	the arm-approach based local modelling of the gravitational field	fourier series;statistical potential;statistical modelling;mathematical model	Gravimetrical Measurements revealing particularities of the gravitational field, play an important role in a research after oil thanks to their capacity to reduce the cost of reconnaissance work. Graphic generalization of the field is given by gravimetrical maps of the field anomalies, drawn out with the corresponding software on the basis of one or other version of interpolation formula. Mathematical models of the trend forming regional reference-surface, can extend the capabilities of graphic generalization of measurements. With these models and corresponding contour maps, the local particularities of the gravitational field reveal themselves more clearly.#R##N##R##N#The use of models in the manner of truncated decompositions in Fourier series on spherical surfaces is offered for practical geophysical tasks. An algorithm of adaptive statistical modelling which allows optimum mathematical models to be built is considered.#R##N##R##N#In the last section we describe the developed software that allows us to build the statistical potential field models. The corresponding field maps of residual anomalies are described.		Sultan Valeev;Konstantin Samokhvalov	2003		10.1007/3-540-44862-4_50	statistical model;econometrics;computer science;electrical engineering;statistical potential;artificial intelligence;machine learning;calculus;mathematical model;mathematics;algorithm;quantum mechanics;fourier series;statistics	Robotics	76.39781576236705	-43.918788115625134	111075
5319be0618016ad586dfcca16d5a4313a696fc18	multi-degree reduction of triangular bézier surfaces with boundary constraints	degree reduction;tecnologia electronica telecomunicaciones;matrix representation;computacion informatica;grupo de excelencia;approximation;triangular bezier surfaces;constrained least square;degree raising;ciencias basicas y experimentales;boundary constraints;tecnologias;error estimate	Given a triangular Bézier surface of degree n, the problem of multi-degree reduction by a triangular Bézier surface of degree m with boundary constraints is investigated. This paper considers the continuity of triangular Bézier surfaces at the three corners, so that the boundary curves preserve endpoints continuity of any order α. The l2and L2-norm combined with the constrained least-squares method are used to get the matrix representations for the control points of the degree reduced surfaces. Both methods can be applied to piecewise continuous triangular patches or to only a triangular patch with the combination of surface subdivision. And the resulting piecewise approximating patches are globally C0 continuous. Finally, error estimation is given and numerical examples demonstrate the effectiveness of our methods. c © 2006 Elsevier Ltd. All rights reserved.	approximation algorithm;bézier curve;least squares;mean squared error;numerical analysis;patch (computing);scott continuity;subdivision surface;the matrix	Lizheng Lu;Guozhao Wang	2006	Computer-Aided Design	10.1016/j.cad.2006.07.004	combinatorics;topology;matrix representation;approximation;mathematics;geometry	Robotics	69.3777151445325	-40.35975735268276	111912
871b996ca56b8785348e57a00509cb9d5b8dc484	a generalized distance transform: theory and applications to weather analysis and forecasting	forecasting;measurement;indexes;morphology;transforms;measurement robustness meteorology transforms indexes forecasting morphology;robustness;shape processing distance transform dt hausdorff distance level set methods lsms meteorological applications metobject object based distance;meteorology	The distance transform (DT) (also known as distance map or distance field) is a fundamental tool of mathematical morphology. We introduce a generalized DT (GDT) that is smoother than the classical DT. This transform can be used to define a generalized Hausdorff metric that is shown to be more robust to noise while preserving all metric properties. It is also shown to lead to smoother level sets, allowing contour evolution without having to solve a partial differential equation. Two applications in weather analysis and forecasting demonstrate the usefulness of this proposed GDT. In particular, the dilation of sets according to the GDT allows the simplification of numerical weather forecasts and analysis into geometric objects, called MetObjects, and the generalized Hausdorff distance can be used as a forecast verification metric.	dilation (morphology);distance transform;hausdorff dimension;mathematical morphology;numerical analysis;numerical weather prediction;symbolic computation	Dominique Brunet;David Sills	2017	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2016.2632042	database index;mathematical optimization;morphology;forecasting;machine learning;mathematics;physics;measurement;statistics;robustness	Vision	77.22962999866076	-45.31160995195645	113062
322f49c23216a90c11c374a0170625ee326ccb1e	filling arbitrary holes in finite element models		We present a new approach for filling arbitrary holes in Finite Element (FE) models composed of independent meshes which are commonly used in Computer Aided Engineering (CAE). During the preprocessing of an FE model for fluid structure simulation a closed structure hull is required in order to subsequently generate a volume mesh representing the fluid. Today’s complex CAE models include thousands of holes that need to be effectively processed. The widespread algorithms mostly fail the special requirements in this context as they can only process holes within a single mesh or smooth surfaces without feature lines. Extending these features into the filling mesh is important for both the visual quality as well as the simulation results. The presented approach handles these cases and automatically detects ordinary holes, e.g. with topological defined boundary. Additionally, holes with more complex boundaries, e.g. defined by several independent meshes, can be specified interactively by the user. Our hole quantification enables the user to automatically close the vast majority of holes and substantially accelerates this preprocessing step. Furthermore, the presented meshing algorithm offers extensive control to gain satisfying results for the described use case.	algorithm;finite element method;interactivity;mind;preprocessor;requirement;simulation;volume mesh;x/open	Andreas Schilling;Katrin Bidmon;Ove Sommer;Thomas Ertl	2008			extended finite element method;mixed finite element method;finite element limit analysis	Graphics	69.48846117140526	-45.4324117011694	113078
97ac51712369d6f2dc789518fa0ebff0f49c6763	accurate 3-d parameterised modelling of complex milling cutter with fir-slots for rotor-groove machining		Milling cutters with fir-slot (fir tree cutters) are widely used in the high speed machining of blade-root and rotor-groove. The complicated structures and multitudinous specification sizes make it difficult to achieve an accurate three-dimensional (3-D) parameterised modelling of the cutter. In this paper, the conical helix and radial cross-sectional curves are expressed mathematically based on geometric curve and surface theory. The radial geometric angles are calculated from the normal angles based on the analysis of the relationship between radial section and normal section. Finally, rapid computer aided engineering (CAE) software are realised. The results show that the established mathematical model can efficiently express the fir tree cutters, which improves the efficiency and accuracy of cutter design. Copyright © 2016 Inderscience Enterprises Ltd.	cutter expansive classification;finite impulse response;r.o.t.o.r.	Xiang Su;Gang Wang;Jianfeng Li;Yiming Rong	2016	IJMR	10.1504/IJMR.2016.10003304	engineering drawing	Robotics	69.58437190868877	-39.14635656003107	113461
671f5f57d79e2102e4185cc1b82cde5f95be25a3	vibrational error extraction method based on wavelet technique		A key factor in developing and assessing any vibration attenuation technique for elastic systems is the measure that quantifies the occurring vibrations. In this paper, we propose a general and instantaneous vibration measure which allows for more subtle methods of localized vibration attenuation techniques. This measure is based on extracting the vibrational part from the conventional tracking error signal using wavelet technique. The paper also provides a method for constructing a wavelet function based on the system impulse response. This wavelet outperforms the existing ones in representing the system behavior while guaranteeing admissibility and providing sufficient smoothness and rate of decay in both time and frequency domains.	wavelet	Loay Alkafafi;Carsten Hamm;Tomas Sauer	2012		10.1007/978-3-642-54382-1_1	analytical chemistry;wavelet packet decomposition	EDA	77.85412930398668	-38.81018302145937	113699
ea1bdb6c8aec12706dbfc4d9d53f7e23937718c1	biorthogonal wavelet construction for hybrid quad/triangle meshes	wavelet analysis;biorthogonal wavelet;data compression;shape approximation;biorthogonal wavelets;lifting scheme;triangular mesh;quad triangle subdivision;progressive transmission;hybrid mesh;linear time;subdivision scheme;triangle mesh	Ever since its introduction by Stam and Loop, the quad/triangle subdivision scheme, which is a generalization of the well-known Catmull–Clark subdivision and Loop subdivision, has attracted a great deal of interest due to its flexibility of allowing both quads and triangles in the same model. In this paper, we present a novel biorthogonal wavelet—constructed through the lifting scheme—that accommodates the quad/triangle subdivision. The introduced wavelet smoothly unifies the Catmull–Clark subdivision wavelet (for quadrilateral meshes) and the Loop subdivision wavelet (for triangular meshes) in a single framework. It can be used to flexibly and efficiently process any complicated semi-regular hybrid meshes containing both quadrilateral and triangular regions. Because the analysis and synthesis algorithms of the wavelet are composed of only local lifting operations allowing fully in-place calculations, they can be performed in linear time. The experiments demonstrate sufficient stability and fine fitting quality of the presented wavelet, which are similar to those of the Catmull–Clark subdivision wavelet and the Loop subdivision wavelet. The wavelet analysis can be used in various applications, such as shape approximation, progressive transmission, data compression and multiresolution edit of complex models.	approximation;biorthogonal wavelet;catmull–clark subdivision surface;data compression;experiment;in-place algorithm;lambda lifting;lifting scheme;semiconductor industry;smoothing;time complexity;triangulated irregular network;whole earth 'lectronic link	Huawei Wang;Kai Tang	2008	The Visual Computer	10.1007/s00371-008-0300-6	mathematical optimization;topology;second-generation wavelet transform;computer science;triangle mesh;mathematics;geometry;wavelet packet decomposition;lifting scheme;statistics	Graphics	68.41437528528158	-44.833734013769885	113917
d2b8ec36bd7507d53c9e0bf41193d1177908c693	detecting precursor patterns for frequency fluctuation in an electrical grid		Precursor pattern identification addresses the problem of detecting warning signals in data that herald an impending event of extraordinary interest. In the context of electrical power systems, identifying precursors to fluctuations in power generation in advance would enable engineers to put in place measures that mitigate against the effects of such fluctuations. In this research we use the Morlet wavelet to transform a time series defined on electrical power generation frequency which was sampled at intervals of 30 seconds to identify potential precursor patterns. The power spectrum that results is then used to select high coefficient regions that capture a large faction of the energy in the spectrum. We then subjected the high coefficient regions together with a contrasting low coefficient region to a non-parametric ANOVA test and our results indicate that one high coefficient region dominates by predicting an overwhelming percentage of the variation that occurs during the subsequent fluctuation event. These results suggest that the wavelet is an effective mechanism to identify precursor activity in electricity time series data.	coefficient;ibm power systems;morlet wavelet;quantum fluctuation;sensor;spectral density;time series	Md. Shahidul Islam;Russel Pears;Boris Bacic	2017	2017 18th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)	10.1109/SNPD.2017.8022725	real-time computing;time series;morlet wavelet;wavelet transform;artificial intelligence;machine learning;electric power system;electricity generation;wavelet;statistics;computer science;electrical grid;spectral density	SE	78.86661788757608	-39.769145735950666	114113
70368a622d583a546f969e7eb085d2441554b5ff	sliding deformation: shape preserving per-vertex displacement		We present a novel algorithm for deforming a locally smooth polygonal mesh by sliding its vertices over the surface. This sliding deformation creates the visual appearance of texture animation without requiring an explicit global surface parameterization or the overhead of storing texture coordinates. The proposed deformation algorithm can also be employed to slide vertices over the surface to increase or decrease resolution in desired regions. To produce the sliding displacement, we define a deformation space that couples the precision of a local parameterization with the advantages of a global parameterization, needed for consistency of displacements over the affected region. On demand, we establish a set of local parameterization spaces that minimize distortion error around each displaced vertex. To propagate the displacement direction across the set of spaces while ensuring coherency, we calculate a representation of global direction in each local space. To map a displaced vertex from the deformation space back to the surface with minimal distortion, we use the local parameterization space of the given vertex. Our method is inherently parallelizable, works on arbitrary topology, and provides a user-friendly, intuitive interface.	algorithm;brent's method;displacement mapping;distortion;dylan;gustavus simmons;overhead (computing);polygon mesh;texture mapping;usability;vertex (geometry)	Dmitriy V. Pinskiy	2010		10.2312/egsh.20101033	mathematical optimization;topology;mathematics;geometry	Graphics	68.46361178636205	-44.85896072140505	114414
49ca35f87c7ecabf424bebc084491df1f5296df2	blind extraction algorithm of the harmonic signal based on the steady-state point capture in lorenz energy accumulation area	gaussian processes;chaos;blind source separation;independent component analysis;chaotic background blind extraction algorithm harmonic signal extraction steady state point capture algorithm lorenz energy accumulation area chaotic signal low frequency characteristics wide spectrum characteristics high energy characteristics independent component analysis three dimensional chaotic dynamic system lyapunov exponent blind source separation multifrequency harmonic signal gaussian signal noise variance computer simulation spectrum aliasing;harmonic analysis chaotic communication steady state signal processing algorithms noise blind source separation;independent component analysis blind source separation chaos gaussian processes;similarity coefficient matrix lyapunov exponent fast ica jade	The chaotic signal has the characteristics of the low-frequency, wide spectrum and high energy, which brings great difficulty for extracting the harmonic signal in its energy accumulation area. We take independent component analysis as a framework in this paper. According to the three-dimensional chaotic dynamics system of Lorenz, we propose the steady-state point capture based on the Lyapunov exponent, and then separate blind source for Lorenz energy accumulation area of multi-frequency harmonic signal and Gaussian signal. In different separation algorithms, we respectively compare the improvement of separation performance from steady-state point capture algorithm, and also analyzes the influence of the amplitude, frequency interval of harmonic signal and noise variance on the extraction performance. Computer simulation verified that blind extraction algorithm based on the steady-state point capture can accurately extract the harmonic signal in the case of spectrum aliasing. It provides a new idea for the harmonic signal extraction from chaotic background.	algorithm;aliasing;chaos theory;computer simulation;independent component analysis;lyapunov fractal;steady state;tree accumulation	Erfu Wang;Dongqing Wang;Qun Ding	2012	2012 IEEE 2nd International Conference on Cloud Computing and Intelligence Systems	10.1109/CCIS.2012.6664297	independent component analysis;computer science;blind equalization;gaussian process;blind signal separation	EDA	80.04503798953985	-39.37818638771817	115186
567a8be5bec41a9cb6a7a5a22296d42512ffa7b5	finding best-fitted rectangle for regions using a bisection method	rectangular fit;centroid;least square method;segmentation;orientation;minimum bounding box;major axis;shape features;minor axis	In this paper, we have presented a new method for computing the best-fitted rectangle for closed regions using their boundary points. The vertices of the best-fitted rectangle are computed using a bisection method starting with the upper-estimated rectangle and the under-estimated rectangle. The vertices of the upper- and under-estimated rectangles are directly computed using closed-form solutions by solving for pairs of straight lines. Starting with these two rectangles, we solve for the best-fitted rectangle iteratively using a bisection method. The algorithm stops when the areas of the fitted rectangles remain unchanged during consecutive iterations. Extensive evaluation of our algorithm demonstrates its effectiveness.	algorithm;bisection method;correctness (computer science);direct method in the calculus of variations;iteration;vertex (geometry)	D. Chaudhuri;N. K. Kushwaha;I. Sharif;A. Samal	2011	Machine Vision and Applications	10.1007/s00138-011-0348-6	mathematical optimization;combinatorics;centroid;minimum bounding box;largest empty rectangle;mathematics;geometry;orientation;rectangle method;segmentation;least squares;semi-major axis	Vision	70.24885711277818	-38.33424813639327	115202
c93623d5e41b47dbf10efb27c5cd3d9fece5c43e	volume data numerical integration and differentiation using cuda	research needs;ground motion;volume rendering;earthquake engineering;parallel programming;cuda;data analysis;large scale;parallel architectures;graphics processing unit earthquakes acceleration rendering computer graphics casting computational modeling three dimensional displays;numerical integration;volume rendering cuda;on the fly;field data;parallel programming data analysis earthquake engineering parallel architectures;cpu volume data numerical integration volume data numerical differentiation cuda earthquake simulations large scale ground motion velocity wave field data sets;data exploration;numerical quadrature;ray casting;volume data	Earthquake simulations generate large-scale ground-motion velocity wave-field data sets. One way to look at the data is to make it volume rendered so that researchers can examine the data efficiently. However, the volume data is usually quite large and a commodity graphics device can store one set of volume data. When the researcher need to look at the acceleration or displacement wave-field. The velocity data has to be integrated into displacement wave-field. Sometimes, the velocity data has to be differentiated into displacement wave-field. In this study, we used CUDA to compute the ground acceleration and displacement from velocity data. Numerical quadrature and centred difference method were implemented using CUDA for on-the-fly data exploration. We used CUDA to speed up ray casting method, trapezoidal quadrature, and centered difference method. We achieved a speed up of $100$ times faster than using a CPU.	baseline (configuration management);chi;cuda;central processing unit;displacement mapping;display device;finite difference;graphics processing unit;numerical integration;numerical linear algebra;particle system;ray casting;rendering (computer graphics);simulation;transverse wave;trapezoidal rule;velocity (software development);volume rendering	Ming-Da Chen;Tung-Ju Hsieh;Yang-Lang Chang	2011	2011 IEEE 17th International Conference on Parallel and Distributed Systems	10.1109/ICPADS.2011.148	computational science;parallel computing;earthquake engineering;numerical integration;computer science;computer graphics (images)	Visualization	72.20128688863218	-50.87577398550409	115345
6eec0a85de5e16f3f40ef7886a6bdaa151117ef6	chirp signal detection using the duffing oscillator	oscillations;choi williams distribution chirp signal detection duffing oscillator chaotic oscillator harmonic signals signal to noise ratio frequency modulated signals time frequency analysis periodic signal frequency detection chaotic properties frequency shift short time fourier transform wigner ville distribution;oscillators time frequency analysis chirp signal to noise ratio frequency estimation;frequency modulation;chaos;oscillators;signal analysis;time frequency;signal detection;short time fourier transform;frequency modulated;fourier transforms;wigner ville distribution;chirp modulation;signal to noise ratio;time frequency analysis chaos chirp modulation fourier transforms frequency modulation oscillators signal detection;time frequency analysis	The chaotic oscillator has become an important tool in the analysis of harmonic signals with low signal to noise ratio. On the other hand, traditionally, frequency modulated (FM) signals have always been studied through conventional techniques of time-frequency analysis. Recently, the Duffing oscillator has shown to be a powerful tool to detect frequencies for periodic signals because of its chaotic properties. It is for this reason that this paper presents a chirp signal analysis in the presence of noise using a Duffing oscillator. The oscillator's chaotic characteristic provides information on how the frequency changes in a signal therefore, for the chirp case it is also convenient to apply time-frequency techniques to obtain the frequency shift for such signals. Here, it is found that is possible to use the Duffing oscillator for the analysis of chirp signals with -15 dB of SNR. The time-frequency techniques used in this paper are the Short Time Fourier Transform, the Wigner-Ville Distribution, its corresponding Choi-Williams and end with a proposed array of Duffing oscillators.	chirp;crystal oscillator;detection theory;electronic oscillator;fm broadcasting;frequency analysis;modulation;short-time fourier transform;signal processing;signal-to-noise ratio;time–frequency analysis;wigner quasiprobability distribution	Carlos R. Bermudez-Gómez;Rogerio A. Enríquez-Caldera;Jorge Martínez-Carballido	2012	CONIELECOMP 2012, 22nd International Conference on Electrical Communications and Computers	10.1109/CONIELECOMP.2012.6189936	electronic engineering;time–frequency analysis;telecommunications;signal processing;variable-frequency oscillator;mathematics;oscillation;chirp;quantum mechanics	EDA	80.0496718332257	-39.43485126160646	115680
a079013cc63e6728b0f8cfccb83ef55527c33a1f	detection of simple radially symmetric targets: further evidence for the matched filter processing scheme in human pattern detection	bessel function;spectrum;pattern detection;transfer function;matched filter;spatial frequency	The detection of small radially symmetric targets was studied using a subthreshold summation paradigm. Small disc and disc-like patterns with diameters up to 0.6∘ were used for superposition on Bessel functions of zero order, subthreshold contrast and various spatial frequencies. Contrast interrelation functions prove linear over the whole range of contrasts used for the Bessel functions while their slopes show systematic variation with spatial frequency. An extrapolation of sensitivity from the slopes reveals that sensitivity can be predicted by a simple model assuming detection to be mediated by a transfer function made up as a cascade of an even bandpass function and the disc pattern spectrum, as has been found previously using one dimensional luminance distributions. Problems concerning the formation of pattern-specific radial symmetric filters are discussed.	arabic numeral 0;bessel filter;cascade device component;extrapolation;granulometry (morphology);hearing loss, high-frequency;matched filter;pattern recognition;programming paradigm;quantum superposition;radial (radio);radial basis function;transfer function;slope	U. Mortensen;G. Meinhardt	2001	Biological Cybernetics	10.1007/s004220170005	spectrum;electronic engineering;mathematical analysis;bessel filter;bessel function;mathematics;spatial frequency;transfer function;optics;matched filter;physics	Vision	81.14854591976702	-44.43020644431384	115727
c037ab65a09bc84f000ce8521910db16affeab3a	an algorithm for multi-resolution analysis of nurbs surfaces	multi resolution analysis;t mesh;surface;cae	A new algorithm is proposed for the multi-resolution representation of NURBS (NonUniform Rational B-Spline) surfaces with boundary consistency constraints. The kernel idea is to employ T-meshes (T-spline control meshes) to construct the multi-resolution representation for surfaces. To this end, the original B-mesh (B-spline control mesh) is first divided into a boundary part and an inner one; then, the multi-resolution representation of the inner part is achieved by wavelet transform; at last, the topology of the boundary part is added onto the obtained multi-resolution meshes to construct the final T-spline based multi-resolution representation result. The boundary shapes keep unchanged in the whole process of the treatment, because the boundary consistency constraints are always contained in the boundary part. This advantage of the new algorithm is demonstrated by three examples. © 2016 Elsevier Ltd. All rights reserved.	algorithm;multiresolution analysis;non-uniform rational b-spline;spline (mathematics);t-spline;wavelet transform	Aizeng Wang;Gang Zhao	2016	Computers & Mathematics with Applications	10.1016/j.camwa.2016.05.018	t-spline;topology;mathematics;geometry;surface	Graphics	68.54421829193973	-42.43803343028956	115783
5309e5b314769bd13b4a41ecdb46e425839eae52	adaptive border sampling for hardware texture-based volume visualization	artefacto;0705k;servidor proxy;texture;sampling rate;echantillonnage;serveur proxy;razon muestreo;artefact;sampling;data analysis;visualization;visualisation;taux echantillonnage;volume visualization;analyse donnee;computer hardware;muestreo;proxy server	This paper introduces a technique to properly sample volume boundaries in hardware texture-based Volume Visualization. Prior techniques render a volume with a set of uniformly-spaced proxy geometries that sample (and represent) a set of uniform-depth slices. While this is sufficient for the core of a volume, it does not consider a sample's partial overlap at the boundaries of a volume, and this failing can lead to significant artifacts at these boundaries. Increasing the sampling rate doesn't solve the problem - but the proper calculation will. While these artifacts might not be easily visible with large datasets, this paper expands on the fundamentals of visualization by presenting a correct handling of sampling at boundaries - which is missing from previous literature. Our technique computes the non-unit depth contributions of the volume at the boundaries. We use fragment programs to perform this adaptive border sampling to compute the partial sample contributions and to match sampling-planes at the volume boundaries with the sampling geometry in the core of the volume.	sampling (signal processing);scientific visualization	Eric LaMar	2006		10.1117/12.643269	computer vision;visualization;computer science;data mining;computer graphics (images)	Visualization	68.80648694020772	-50.764192356188616	116167
60f6af6da79152ee08b25a580577c09eba261315	a time-dependent vector field topology based on streak surfaces	eigenvalues and eigenfunctions;topology;cfd time dependent vector field topology streak surfaces 2d vector field topology concept stationary vector fields streak lines 3d vector field topology separatrices 0d seeds critical points 1d seeding 2d generalization intersection curves codimension 1 ridges forward and reverse finite time lyapunov exponent fields ftle ridges streak manifolds lagrangian coherent structures lcs visual quality computational cost time series computational fluid dynamics;streak lines;manifolds;algorithms computer graphics image enhancement image interpretation computer assisted imaging three dimensional reproducibility of results rheology sensitivity and specificity user computer interface;time dependent vector fields;time series;vectors manifolds topology three dimensional displays eigenvalues and eigenfunctions trajectory surface reconstruction;surface reconstruction;lagrangian coherent structures;data visualisation;trajectory;vectors;three dimensional displays;time series data visualisation;time dependent vector fields vector field topology lagrangian coherent structures streak lines;vector field topology	It was shown recently how the 2D vector field topology concept, directly applicable to stationary vector fields only, can be generalized to time-dependent vector fields by replacing the role of stream lines by streak lines [1]. The present paper extends this concept to 3D vector fields. In traditional 3D vector field topology separatrices can be obtained by integrating stream lines from 0D seeds corresponding to critical points. We show that in our new concept, in contrast, 1D seeding constructs are required for computing streak-based separatrices. In analogy to the 2D generalization we show that invariant manifolds can be obtained by seeding streak surfaces along distinguished path surfaces emanating from intersection curves between codimension-1 ridges in the forward and reverse finite-time Lyapunov exponent (FTLE) fields. These path surfaces represent a time-dependent generalization of critical points and convey further structure in time-dependent topology of vector fields. Compared to the traditional approach based on FTLE ridges, the resulting streak manifolds ease the analysis of Lagrangian coherent structures (LCS) with respect to visual quality and computational cost, especially when time series of LCS are computed. We exemplify validity and utility of the new approach using both synthetic examples and computational fluid dynamics results.	algorithmic efficiency;anatomy, regional;coherence (physics);computation (action);computational fluid dynamics;exemplification;generalization (psychology);hydrodynamics;intersection of set of elements;lagrangian coherent structure;lyapunov fractal;plant seeds;seeding;stationary process;synthetic intelligence;time series	Markus Üffinger;Filip Sadlo;Thomas Ertl	2013	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2012.131	lagrangian coherent structures;discrete mathematics;topology;surface reconstruction;manifold;trajectory;time series;mathematics;geometry;data visualization;statistics	Visualization	72.02407006740226	-44.923807516793346	117017
fb43559ce71453c22c68c17cc07cd793f3ebb5d0	surface approximation of a cloud of 3d points	minimisation;curva;minimization;formation image tridimensionnelle;convergence;3d imaging;geometrie algorithmique;computational geometry;ajustement;courbe;minimizacion;curve;fitting;convergencia;pattern recognition;superficie;surface;geometria computacional;reconnaissance forme;reconocimiento patron;ajuste;formacion imagen tridimensional;algoritmo optimo;algorithme optimal;optimal algorithm;surface approximation	"""We present an implementation of deformable models to approximate a 3-0 surface given by a cloud of 3 0 points. It is an extension of ow previous work on """"Bsnakes"""" c2J and [12], which approximates curves and surfaces using B-splines. The user (or the system itsel@ provides an initial simple swfme, such as a closed cylindeti which is subject to internal forces (describing impkit continuity properties such as smoothness) and external forces which attract it toward the data points. The problem is cast in terms of energy minimization. We solve this non-convex optimization problem by using the well known Powell algorithm which guarantees convergence and does not require gradient information. The variables are the positions of the control points. The number of control points processed by Powell at one time is controlled. This methodology leads to a reasonable complex& robustness, and good numerical stability. We keep the time and space complexities in check through a coarse to fine approach and a partitioning scheme. We handle closed surfmes by decomposing an object into two caps and an open cylinder, smoothly connected. The process is controlled by two parameters onlx which are constant for all our experiments. We show results on real range images to illustrate the applicability of our approach. The advantages of this approach are that it provides a compact representation of the approximated dala, and lends itserf to applications such as non-rigid motion tracking and ob'ect recognition. Currently, our algorithm gives only a cd continuous analytical description of the data, but because the output of our algorithm is in rectangular mesh format, a c' or c"""" sdace can be constructed easily by existing algorithms."""	approximation algorithm;b-spline;cloud computing;control point (mathematics);convex optimization;cylinder seal;data point;energy minimization;experiment;gradient descent;mathematical optimization;numerical stability;optimization problem;powell's method;scott continuity;smoothing;whole earth 'lectronic link	Chia-Wei Liao;Gérard G. Medioni	1995	CVGIP: Graphical Model and Image Processing	10.1006/gmip.1995.1007	stereoscopy;minimisation;convergence;computational geometry;computer science;calculus;mathematics;geometry;curve;surface;algorithm;statistics	Vision	69.9399627723046	-41.62605563886998	117244
275879459716a0e8a35a8df2a477e0bd80df1361	planar pixelations and image recognition	image recognition;differential geometry;computational geometry;topological invariant;pattern recognition;morse theory;betti number	Any subset of the plane can be approximated by a set of square pixels. This transition from a shape to its pixelation is rather brutal since it destroys geometric and topological information about the shape. Using a technique inspired by Morse Theory, we algorithmically produce a PL approximation of the original shape using only information from its pixelation. This approximation converges to the original shape in a very strong sense: as the size of the pixels goes to zero we can recover important geometric and topological invariants of the original shape such as Betti numbers, area, perimeter and curvature measures.	approximation algorithm;betti number;computer vision;perimeter;pixel aspect ratio;pixelation	Brandon Rowekamp	2011	CoRR		betti number;differential geometry;combinatorics;topology;computational geometry;mathematics;geometry;morse theory	Vision	70.31879547587016	-42.91087596665319	117465
12dcdb38df3ca6a84888c62484e194cecb171707	medial axis of a planar region by offset self-intersections	medial axis transform;interpolation;offset self intersection;medial axis;voronoi diagram	The medial axis (MA) of a planar region is the locus of those maximal disks contained within its boundary. This entity has many CAD/CAM applications. Approximations based on the Voronoi diagram are efficient for linear-arc boundaries, but such constructions are more difficult if the boundary is free. This paper proposes an algorithm for free-form boundaries that uses the relation between MA and offsets. It takes the curvature information from the boundary in order to find the self-intersections of successive offset curves. These self-intersection points belong to the MA and can be interpolated to obtain an approximation in Bezier form. This method also approximates the medial axis transform by using the offset distance to each self-intersection.	medial graph;optic axis of a crystal	Rubén Dorado	2009	Computer-Aided Design	10.1016/j.cad.2009.08.005	topology;voronoi diagram;medial axis;interpolation;straight skeleton;mathematics;geometry;engineering drawing	EDA	69.10915560770407	-39.713430020042864	118016
d36ca2620ab4bd798439fdedd4629896c29553be	research issues in volume visualization	image segmentation;volume graphics;volume rendering;computer graphic equipment;transform coding;scattered data;data visualization computer graphics buffer storage geophysics computing rendering computer graphics computer science solid modeling layout mirrors information technology;data visualisation;data analysis;special purpose hardware volume visualization volumetric data sets interactive graphics imaging representation rendering 3d regular grid volume elements voxels volume buffer cubic frame buffer volume graphics volume rendering transform coding segmentation real time rendering parallelism;volume visualization;computer graphic equipment rendering computer graphics spatial data structures data visualisation data analysis image segmentation real time systems;spatial data structures;interactive graphics;rendering computer graphics;real time rendering;real time systems;volume data	Volume visualization is a method of extracting meaningful information from volumetric data sets through the use of interactive graphics and imaging. It addresses the representation, manipulation, and rendering of volumetric data sets, providing mechanisms for peering into structures and understanding their complexity and dynamics. Typically, the data set is represented as a 3D regular grid of volume elements (voxels) and stored in a volume buffer (also called a cubic frame buffer), which is a large 3D array of voxels. However, data is often defined at scattered or irregular locations that require using alternative representations and rendering algorithms. There are eight major research issues in volume visualization: volume graphics, volume rendering, transform coding of volume data, scattered data, enriching volumes with knowledge, segmentation, real-time rendering and parallelism, and special purpose hardware.<<ETX>>	algorithm;cubic function;framebuffer;graphics;parallel computing;peering;real-time clock;regular grid;scientific visualization;transform coding;volume rendering;voxel	Arie E. Kaufman;Karl Heinz Höhne;Wolfgang Krüger;Lawrence J. Rosenblum;Peter Schröder	1994	IEEE Computer Graphics and Applications	10.1109/38.267473	computer vision;scientific visualization;transform coding;rendering;computer science;theoretical computer science;parallel rendering;image segmentation;data analysis;real-time rendering;volume rendering;data visualization;statistics;computer graphics (images)	Visualization	68.49569047084645	-51.11210413974995	118364
070ff6b3bec25f98e4fb7d5d63cd5bffb01db867	volume-enclosing surface extraction	computed tomography;hydrodynamic models;3d imaging;computational techniques;computed tomographic;surfaces and interfaces;computational geometry;three dimensional;heavy ion collision;relativistic models;hydrodynamic model;image analysis;marching cube;numerical simulation;x rays	In this paper we present a new method, which allows for the construction of triangular isosurfaces from three-dimensional data sets, such as 3D image data and/or numerical simulation data that are based on regularly shaped, cubic lattices. This novel volume-enclosing surface extraction technique, which has been named VESTA, can produce up to six different results due to the nature of the discretized 3D space under consideration. VESTA is neither template-based nor it is necessarily required to operate on 2 × 2 × 2 voxel cell neighborhoods only. The surface tiles are determined with a very fast and robust construction technique while potential ambiguities are detected and resolved. Here, we provide an in-depth comparison between VESTA and various versions of the well-known and very popular Marching Cubes algorithm for the very first time. In an application section, we demonstrate the extraction of VESTA isosurfaces for various data sets ranging from computer tomographic scan data to simulation data of relativistic hydrodynamic fireball expansions.	algorithm;ct scan;computer simulation;cubic function;discretization;isosurface;marching cubes;numerical analysis;surface-mount technology;tomography;vesta (software configuration management);voxel;whole earth 'lectronic link;x-ray microtomography	B. R. Schlei	2012	Computers & Graphics	10.1016/j.cag.2011.12.008	stereoscopy;three-dimensional space;image analysis;computational geometry;computer science;mathematics;geometry;marching cubes;computer graphics (images)	Visualization	71.16402039270596	-49.91172431132404	118471
6538a7fc52453b3e3ab554897741f0fb1db0ec1d	contributions to performance verification and uncertainty determination of industrial computed tomography for dimensional metrology (bijdrage tot prestatieanalyse en onzekerheidsbepaling van industriële computer tomografie voor dimensionele kwaliteitscontrole)			ct scan;tomography	Kim Kiekens	2017				Robotics	75.93396785026167	-42.96176633466342	118500
9a51d7253ccd8a56f9eb0712a5273ea9889e3789	puppeteering 2.5d models	two dimensional displays;joints;computational modeling;bones;three dimensional displays;solid modeling;animation	A laborious task for animators is the redrawing of 2D models for each new required view or pose. As a consequence, several applications have been proposed to make this task easier. A successful approach is the Cartoon 2.5D Models. Its goal is the automatic computation of new views - by the simulation of 3D global rotation - from user-provided 2D models. However, previous work of 2.5D Models does not have support to calculate new poses efficiently, i.e., the user redraws the input views again in the new pose. We present a novel approach that allows the user to produce both new views and new poses easily for the 2.5D Models, thus puppeteering the 2.5D Models. It makes use of a hierarchical bone structure that explores the methodology of the 2.5D Models, ensuring coherence between the bone structure and the model. The usability of the present approach is intuitive for users acquainted with previous 2.5D Modeling tools.	2.5d;coherence (physics);computation;computer animation;displacement mapping;interpolation;pose (computer vision);puppeteer;simulation;usability;yaws	Joao Coutinho;Bruno A. D. Marques;Joao Paulo Gois	2016	2016 29th SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)	10.1109/SIBGRAPI.2016.010	computer vision;simulation;computer science;computer graphics (images)	Graphics	69.02511676003226	-47.03384647274226	118842
e403d87fe475c8fb2501f7e43ffcac562c71c1bf	lung-heart sound separation using noise assisted multivariate empirical mode decomposition	bioacoustics;cardiology;nam emd based ls extraction lung heart sound separation noise assisted multivariate empirical mode decomposition lung sound separation breath sound recording emd based algorithm mode mixing standard emd lung sound extraction algorithm emd extensions ensemble emd eemd multivariate emd noise assisted m emd heart sound segmentation segments reconstruction auditory analysis numerical analysis;lung;numerical analysis;noise assisted multivariate empirical mode decomposition breath sound recordings lung sound extraction empirical mode decomposition ensemble empirical mode decomposition multivariate empirical mode decomposition;feature extraction;signal reconstruction;empirical mode decomposition lungs heart white noise standards algorithm design and analysis;pneumodynamics;source separation;medical signal processing;source separation bioacoustics cardiology feature extraction lung medical signal processing numerical analysis pneumodynamics signal reconstruction	Separating lung sound (LS) from breath sound (BS) recording has been of interest to doctors and researchers in the last two decades. Many algorithms have been developed to solve this question, one of them is based on the empirical mode decomposition (EMD). Due to the notorious mode mixing issue in the standard EMD, this paper surveys LS extraction based on EMD extensions, including ensemble EMD (EEMD), multivariate EMD (M-EMD), and noise assisted M-EMD (NAM-EMD). In this study, the algorithm for LS extraction is composed of heart sound (HS) segmentation, LS separation, and segments reconstruction. The performance evaluation by auditory and numerical analyses reveals that NAM-EMD based LS extraction is superior to the standard EMD and its extensions.	algorithm;basis function;ensemble kalman filter;hilbert–huang transform;image noise;least squares;nam;numerical analysis;performance evaluation;signal processing;white noise	ChingShun Lin;Wisena A. Tanumihardja;HongHui Shih	2013	2013 International Symposium on Intelligent Signal Processing and Communication Systems	10.1109/ISPACS.2013.6704645	speech recognition;acoustics;engineering;pattern recognition	PL	81.17910542625665	-38.60480647010632	118996
66e14b13b5e80743b4cd9fb77c909666e432d6ec	scci-hybrid methods for 2d curve tracing	hybrid method;curve tracing;contouring;interval analysis	A hybrid method for plotting 2-dimensional curves, defined implicitly by equations of the form f(x,y) = 0 is presented. The method is extremely robust and reliable and consists of Space Covering techniques, Continuation principles and Interval analysis (i.e. SCCI). The space covering, based on iterated subdivision, guarantees that no curve branches or isolated curve parts or even points are lost (which can happen if grid methods are used). The continuation method is initiated in a subarea as soon as it is proven that the subarea contains only one smooth curve. Such a subarea does not need to be subdivided further so that the computation is accelerated as far as possible with respect to the subdivision process. The novelty of the SCCI-hybrid method is the intense use of the implicit function theorem for controlling the steps of the method. Although the implicit function theorem has a rather local nature, it is empowered with global properties by evaluating it in an interval environment. This means that the theorem can provide global information about the curve in a subarea such as existence, non-existence, uniqueness of the curve or even the presence of singular points. The information gained allows the above-mentioned control of the subarea and the decision of its further processing, i.e. deleting it, subdividing it, switching to the continuation method or preparing the plotting of the curve in this subarea. The curves can be processed mathematically in such a manner, that the derivation of the plotted curve from the exact curve is as small as desired (modulo the screen resolution).		Helmut Ratschek;Jon G. Rokne	2005	Int. J. Image Graphics	10.1142/S0219467805001859	mathematical optimization;discrete mathematics;vertical line test;mathematics;geometry;interval arithmetic;moore curve;curve fitting	Robotics	71.53243121019992	-41.34856857413477	119648
1a8757db84057bdbc9e051de262a5ace88eb08cb	flow reduction marching cubes algorithm	ct;marching cubes;human tissue model;discrete data;reduction;mr;3d	The article focuses on modification of traditional Marching cubes algorithm for medical applications. Main disadvantage of the traditional algorithm is size of models. A reduction process has been added to the MC to produce much smaller models. The reduction process is applied during the MC, after processing of each input data layer. Resulting models are 4-5 times smaller in only doubled time.	algorithm;marching cubes;olap cube	Premysl Krsek	2004		10.1007/1-4020-4179-9_131	marching tetrahedra;marching squares;isosurface	Visualization	68.8626996517695	-50.57693959134593	120003
5babfbb944f41d496f8528a77e37b0ce8bdc4751	cad/tolerancing integration: mechanical assembly with form defects		Abstract Geometric deviations affect the assemblability and functional compliance of products, since small part variations accumulate through large-scale assemblies and lead to malfunctions. The Digital Muck Up (DMU) upgrade requires a tolerance consideration in CAD models. The improvement of tolerancing leads to industrial success. Therefore, improving the CAD model to be closer to the realistic model is a necessity to verify and validate the mechanical system assemblability. In previous work, an approach to consider the dimensional, positional and orientation tolerances in CAD models was developed. In this paper, the above approach is improved to take into account form defects in CAD models. To model the component with form defects, the toleranced face is modeled by gird vertices. According to form tolerance values, a White Gaussian Noise (WGN) of gird vertices is computed. The realistic face is obtained by an interpolation based on the tessellation using Thin Plate Surface (TPS) modeling. The realistic assembly configurations were performed by updating the mating constraints. In fact, in realistic modeling, a new method to redefine constraints, while respecting the Objective Function of the Assembly (OFA), is established. In the case of a planar joint, a sub-algorithm based on Oriented Bounding Box (OBB) and the matrix transformation is developed. Relative part displacements are simulated with or without guaranteeing contact. Tolerance impacts on the realistic assembly motion are quantified. The realistic cylindrical joint is performed using an optimization method: the minimum cylinder inside a realistic hole and the maximum cylinder outside a realistic pin. Finally, in the case of a revolute joint, a sub-algorithm to redefine the mating constraints between two realistic parts is performed. This paper proposes a new approach to incorporate tolerances on CAD models in the case of planar and cylindrical faces by determining configurations with positional, orientation and form defects. This approach provides an assembly result closer to the real assembly of the mechanical system. Integrating tolerances in CAD allows the simulation and visualization of the mechanical assemblies’ behavior in their real configurations.		I. Jbira;M. Tlija;B. Louhichi;Antoine Tahan	2017	Advances in Engineering Software	10.1016/j.advengsoft.2017.07.010	fold (higher-order function);interpolation;mathematical optimization;cylinder;computer science;mechanical system;vertex (geometry);revolute joint;minimum bounding box;cylindrical joint	SE	70.16135174089622	-38.09755029853512	120286
d0281f39674e987a2696a937c2b8dec0e5f49cf1	body wave separation in the time-frequency domain	seismic signals body wave separation time frequency domain s wave arrival surface waves p wave arrival high resolution time frequency transform microseismic events hydraulic fracturing treatment synchrosqueezing transform high resolution time frequency decompositions short term fourier transforms;seismic waves geophysical techniques;time frequency analysis time domain analysis geophysics continuous wavelet transforms educational institutions;wave separation signal reconstruction spectral decomposition synchrosqueezing transform sst	Separation of a seismogram into its individual constitutive phases (Pand S-wave arrivals, surface waves, etc.) is a long-standing problem. In this letter, we use a high-resolution time-frequency transform to achieve this and reconstruct their individual waveforms in the time domain. The procedure is illustrated using microseismic events recorded during a hydraulic fracturing treatment. The synchrosqueezing transform is an extension of the continuous wavelet transform combined with frequency reassignment. Its high-resolution time-frequency decompositions allow for separation and identification of Pand S-waves with subtly different frequency contents that would not be recoverable using short-term Fourier transforms due to its smearing in the frequency domain. It is an invertible transform, thus allowing for signal reconstruction in the time domain after signal separation. The same approach is applicable to other seismic signals such as resonance frequencies and long-period events and offers promising new possibilities for enhanced signal interpretation in terms of underlying physical processes.	continuous wavelet;image resolution;list of fourier-related transforms;resonance;schedule (computer science);signal reconstruction;surface wave;wavelet transform	Roberto Henry Herrera;Jean-Baptiste Tary;Mirko van der Baan;David W. Eaton	2015	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2014.2342033	seismology;constant q transform;speech recognition;s transform;harmonic wavelet transform;acoustics;short-time fourier transform;continuous wavelet transform;spectral density estimation;frequency domain;discrete frequency domain	Visualization	79.83651188667865	-39.807453637941656	120400
225e7fff6f43aec27cafc3c39882e268fe75b458	rig-space physics	animation control;physically based simulation	We present a method that brings the benefits of physics-based simulations to traditional animation pipelines. We formulate the equations of motions in the subspace of deformations defined by an animator's rig. Our framework fits seamlessly into the workflow typically employed by artists, as our output consists of animation curves that are identical in nature to the result of manual keyframing. Artists can therefore explore the full spectrum between handcrafted animation and unrestricted physical simulation. To enhance the artist's control, we provide a method that transforms stiffness values defined on rig parameters to a non-homogeneous distribution of material parameters for the underlying FEM model. In addition, we use automatically extracted high-level rig parameters to intuitively edit the results of our simulations, and also to speed up computation. To demonstrate the effectiveness of our method, we create compelling results by adding rich physical motions to coarse input animations. In the absence of artist input, we create realistic passive motion directly in rig space.	computation;computer animation;dynamical simulation;fits;finite element method;high- and low-level;key frame;pipeline (computing)	Fabian Hahn;Sebastian Martin;Bernhard Thomaszewski;Robert W. Sumner;Stelian Coros;Markus H. Gross	2012	ACM Trans. Graph.	10.1145/2185520.2185568	computer vision;simulation;computer science;multimedia;algorithm;computer graphics (images)	Graphics	69.15670027555927	-47.28143001837917	121129
018ea097b44c7dbc40f50c87e25593f9ad1c3e71	efficient representation and extraction of 2-manifold isosurfaces using kd-trees	feature extraction;graphical user interfaces;image representation;mesh generation;octrees;solid modelling;trees (mathematics);2-manifold meshes;computational efficiency;computer graphics;dual contouring approach;feature preservation;implicit object representation;isosurface extraction algorithm;kd-tree hierarchy;modeling;multiple surface components;object surface;octree;quadric error metrics;solid object;thin object structure;topology;kd tree	In this paper, we propose the utilization of a kd-tree based hierarchy as an implicit object representation. Compared to an octree, the kd-tree based hierarchy is superior in terms of adaptation to the object surface. In consequence, we obtain considerably more compact implicit representations especially in case of thin object structures. We describe a new isosurface extraction algorithm for this kind of implicit representation. In contrast to related algorithms for octrees, it generates 2-manifold meshes even for kd-trees with cells containing multiple surface components. The algorithm retains all the good properties of the Dual Contouring approach [1] like feature preservation, computational efficiency, etc. In addition, we present a simplification framework for the surfaces represented by the kd-tree based on quadric error metrics. We adapt this framework to quantify the influence of topological changes, thereby allowing controlled topological simplification of the object. The advantages of the new algorithm are demonstrated by several examples.	algorithm;computation;isosurface;level of detail;octree	Alexander Greß;Reinhard Klein	2003		10.1109/PCCGA.2003.1238278	mesh generation;computer vision;feature extraction;computer science;theoretical computer science;k-d tree;graphical user interface;programming language;computer graphics (images)	Visualization	68.41675621792248	-45.78964223472305	121458
aab27824afa6319692d9a8424111d6c3b77dd84a	real-time rendering of trimmed surfaces	real time;tensor product;graphics system;nonlinear equation;hermite polynomial;real time rendering	Rational tensor product surfaces, (B&eacute;zier, NURBS, Hermite, polynomial, etc.) are rendered in real-time by uniform faceting. The described methods are modular and can be balanced for optimal implementation on different hardware platforms. Discretization anomalies such as angularities, Mach banding, cracking etc. are avoided by tessellating the surface patches and segmenting the trimming curves based on the view.	colour banding;discretization;faceting;hermite polynomials;non-uniform rational b-spline;password cracking;polynomial;real-time clock;real-time transcription	Alyn P. Rockwood;Kurt Heaton;Tom Davis	1989		10.1145/74333.74344	tensor product;mathematical optimization;discrete mathematics;hermite polynomials;nonlinear system;computer science;mathematics;geometry;real-time rendering;quantum mechanics	Graphics	72.04933082147396	-42.78909787535879	121978
f7365bc0464e335ba919ee2b7e35a287f1eca545	mesh coding: application to texture and scalable 3d scene coding	cosine transformation;scalability;three dimensional representation;discrete transformation.;texture;image coding;triangular shape;topology;predictive coding;mesh;three dimensional;triangular mesh;cosine transform	This article examines the problem of 3D scalable mesh and texture coding. The triangular mesh topology is encoded by an edge-based conquest strategy, while positions are encoded using a multiple prediction method associated with an adaptive arithmetic coder. Moreover, scalability is obtained on the positions via a bitplane coding technique, adapted to f loat numbers. Then, texture is approximated by a hybrid technique mixing affine interpolation and discrete cosinus transform applied to the triangles of a hierarchical nested mesh.	approximation algorithm;arithmetic coding;conquest;interpolation;mesh networking;scalability;texture mapping	Pierre Alliez;Nathalie Laurent;Patrick Lechat	2000	Annales des Télécommunications	10.1007/BF03001905		Vision	73.04872679240607	-44.08003556707957	122653
959b471d0927adcc4e2090304d37ffee946054c6	topological generators and cut-graphs of arbitrary triangle meshes	graph search;graph theory;boundary components topological generators arbitrary triangle meshes cut graphs adaptive sampling global parameterization problem;boundary components;arbitrary triangle meshes cut graphs;signal sampling;geometry;topological generators;surface texture;stability;shape;solid modeling;adaptive sampling;scalar field;sampling methods;mesh generation;triangle mesh;sampling methods mesh generation shape geometry stability optical devices surface texture solid modeling;global parameterization problem;optical devices;signal sampling graph theory	Recent advances in the parameterization and adaptive sampling of disc-like surfaces have brought a renewed interest on the global parameterization problem and, more specifically, on the cut-graph search. This paper focuses on the calculation of a family of generators and cut-graphs for the global parameterization of arbitrary triangle meshes. This result is achieved by combining the construction of harmonic scalar fields f : M rarr R of known maxima and minima with the quasi Morse-Smale complex of(M, f). The proposed technique has a simple implementation and outperforms previous work in terms of smoothness of the cut-graphs, stability with respect to the surface sampling, tessellation, topological noise (e.g., tiny handles), and capability of handling boundary components. Since we generate a family of cut-graphs, we also provide a comparison between the parameterizations of M induced by two cut-graphs.	adaptive sampling;global optimization;graph traversal;maxima and minima;sampling (signal processing);triangle mesh	Giuseppe Patanè;Michela Spagnuolo;Bianca Falcidieno	2007	IEEE International Conference on Shape Modeling and Applications 2007 (SMI '07)	10.1109/SMI.2007.37	combinatorics;topology;mathematics;geometry	Robotics	69.18324752074935	-43.914972597893026	122863
51ecffc5501f9023b7b4612e2e644032d8b25f80	gpu based multi-histogram volume navigation for virtual bronchoscopy	rendering computer graphics navigation graphics processing units three dimensional displays geometry bronchoscopy games;high performance multihistogram volume rendering gpu based multihistogram volume navigation virtual bronchoscopy interactive navigation system;virtual reality biomedical optical imaging endoscopes graphics processing units medical image processing rendering computer graphics	An interactive navigation system for virtual bronchoscopy is presented, which is based solely on GPU based high performance multi-histogram volume rendering.	blood vessel;bronchi;bronchoscopy;capsule endoscopy;colon classification;game engine;graphics processing unit;histogram;motion planning;navigation;numerous;prototype;semiconductor industry;simulation;traditional game;video card;virtual camera system;volume rendering;biologic segmentation	Ruida Cheng;Sheng Xu;Alexandra Bokinsky;Evan S. McCreedy;William Gandler;Bradford J. Wood;Matthew J. McAuliffe	2014	2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/EMBC.2014.6944330	computer vision;tiled rendering;image-based modeling and rendering;3d rendering;rendering;computer science;parallel rendering;real-time computer graphics;multimedia;real-time rendering;computer graphics;alternate frame rendering;volume rendering;software rendering;3d computer graphics;computer graphics (images)	Visualization	69.67305411394481	-49.70963558781302	122875
a211162c91902f36cfd3b4e4b299a05fab666bf2	decomposition of the instantaneous spectrum of a random system	traitement signal;random medium;spectrum analysis;analyse spectre;analisis espectro;reponse transitoire;signal analysis;time frequency;analisis de senal;wigner spectrum;spectrum;distribution wigner;decomposition systeme;analyse frequence temps;transient response;respuesta transitoria;medio aleatorio;wigner distribution;signal processing;system decomposition;descomposicion sistema;procesamiento senal;milieu aleatoire;time frequency analysis;random systems;analyse signal;transient	We characterize the nature of the instantaneous spectrum of a random system. We define the instantaneous spectrum as the Wigner spectrum of the state of the system, and we apply a method which transforms the system in the domain of the Wigner spectrum. By using this approach we prove that the instantaneous spectrum is made by three components. Two components decay with time, while a third stationary component is reached for large times. & 2009 Elsevier B.V. All rights reserved.	frequency analysis;initial condition;spectral density;spectrum analyzer;stationary process;stochastic process;transient (computer programming);world-system	Lorenzo Galleani	2010	Signal Processing	10.1016/j.sigpro.2009.09.003	instantaneous phase;time–frequency analysis;telecommunications;computer science;electrical engineering;calculus;signal processing;mathematics	Embedded	79.42117683703027	-38.85862917258606	122937
e7e05e4f291d531d5bbd93fdc190b4524daf9ea4	virtual clothes, hair and skin for beautiful top models	virtual reality computer animation;hair skin biological system modeling humans shape deformable models plastics animation cameras tv;synthetic actor;virtual reality;virtual human;computer animation;zdf television program virtual clothes virtual hair virtual skin beautiful top models realistic looking humans marilyn monroe humphrey bogart plaster model animation	"""Since l986, we have led extensive research on simulating realistic looking humans. We have created Marilyn Monroe and Humphrey Bogart that met in a Cafe in Montreal. At that time, they did not wear any dress as such. Humphrey's body was made out of a plaster model that has the shape of a suit. Colors on Marilyn's body looked like a dress. Hairs were simulated as a global shape and skin was a color. Since then, we have developed extensive research to simulate real virtual deformable clothes wearing by virtual humans. We also needed to have appropriate simulation of a skin and recently, we have developed new research on skin in order to decrease the plastic color of our synthetic actors. Also there was a need to simulate hair in an efficient way. New methods have been developed, both for design and animation purpose that are compatible with the clothes module. In this paper, we like to introduce our most recent research results on these topics. We are now able to simulate top models that start to look like real ones. Our latest work, that shows Marilyn receiving a golden camera Award in Berlin, Germany , demonstrates the results of our research. This sequence has been shown in a ZDF television program that was seen by more than l5 millions of viewers. I Cloth modeling and simulation State of art in cloth synthesis and animation The evolution of cloth synthesis has gone though several phases. The first one, and the most important, has been to simulate efficiently fabric motion and deformations using mechanical computer simulation. These techniques have been pioneered by Weil [C23], Terzopoulos et al. (1987) and Haumann and Parent (1988), using different kinds of mechanical models based on triangular discretisation of the simulated surface. More recently, several other techniques have been explored, such as polynomial surfaces (Witkin and Welch [C25], Baraff and Witkin [C3]) and particle systems (Breen et al. [C4]). The second phase was to focus on some specific behaviors of cloth deformation, such as wrinkle generation. Kunii and Godota [C10] and Aono [C1] produced wrinkles by combining geometric and physical aspects of the deformations. The final aspect of cloth generation was the design and animation of garments on virtual actors. This aspect includes several issues such as garment modeling, building, and also precise handling of the interaction between the cloth and the body that wears it, using collision detection and response. The first studies have been performed by Lafleur et al. [C11], Carignan et al. [C6] and Yang and Magnenat Thalmann [C27] where complex garments were generated by assembling panels around a synthetic actor, which then was animated. More than just pieces of fabric hanging in the air or draping around a table, realistic cloth simulation provide a variety of new problems and challenges, such as being able to design any shape of cloth, such as a real tailor would cut into fabric pieces and assemble them without being constrained by limitations due for example to the data structure (regular triangle meshes, square panels). Then, the cloth behavior is strongly determined by the collisions between the fabric and the body, and also the self-collision within the cloth itself, usually in wrinkles in crumpling situations. Collision detection should be very efficiently managed, particularly in the case of selfcollision detection. The mechanical model should also be very efficient for simulating high deformations, such as those occurring during wrinkling. This requires good modelisation of the non linear mechanical behavior of the fabric [C7], as well as an efficient calculation algorithm which gives acceptable results even if the deformations gets big compared to the discretisation size of the surface, and then ensures stability even with high extension and curvature. Figure 1: A sequence involving animation of synthetic cloth Concerning the design itself, a good cloth software should provide efficient tools for designing a cloth, and fitting its shape exactly to the synthetic actor. Simulating cloth with high deformations One of the major difficulties of cloth simulation is to cope with all the high deformations and wrinkle situations that real cloth may have when worn by a body. Typically, the model should be robust enough to ensure realistic animation even in cases where high deformations have to be considered compared to the discretisation size of the surface, and despite the lot of collisions created by crumpling, wrinkling, or simply by contact with the body. The main idea of the mechanical model is to integrate Newton's motion equation in a direct way to keep quickly evaluated time steps small and maintain very frequent collision detection [C20]. More sophisticated and time consuming models based on global minimizations or Lagrangian dynamics formulations allowing a higher time step would be a waste of time. Thus, discontinuous responses such as collisions will be handled in an accurate way. Furthermore, this direct formulation allows us easy and precise inclusion of any nonlinear mechanical behavior. With such a model, we can also act directly on the position and speed of the elements, and thus avoid handling collisions through strong repulsive forces that perturb the simulation. The animated deformable object is represented as a particle system by sets of vertices forming irregular triangles, thus allowing surfaces of any shape to be easily modeled and simulated. Using the material mechanical behavior parameters of the considered material, strains are computed within the considered triangles and the resulting forces are applied on the vertices. The object is considered to be isotropic and of constant thickness. Elastic properties of the object are mainly described by the standard parameters such as the Young modulus, the Poisson coefficient, the density and the thickness. Rough discretisation, however, alters the behavior of the surface. In particular, heterogeneous triangulations are """"rigidifying"""" the whole surface, preventing easy buckling. These effects have to be corrected through tuning and adjustments of the mechanical parameters. In particular, textile easily buckles into double curvature, but buckle formation requires a change of area that increases with the size of the discretised elements. To facilitate buckle formation on roughly discretised objects without loosing textile stretching stiffness, we use a variable Young modulus for reducing the stretching stiffness for compression and small extension. Collision management Collisions are widely spread within all the simulated cloth, either being in contact with the body, or through contact between the wrinkles of the cloth. Thus, an efficient way of handling these numerous collisions is required. Instead of considering collisions as being dynamic """"potential walls"""" repelling the considered objects using high and discontinuous forces, thus requiring very small time steps for precise computation, collisions are rather taken into account in a separate step as a geometrical and cinematical constraint resolution independent from the mechanical computation resulting from """"continuous"""" forces, such as elasticity, and gravity and wind, as described above. The computation time step is in this way not altered by collisions, and the computation remains efficient despite a huge number of collisions. As soon as two elements collide, momentum transfer is performed according to the mechanical conservation laws, and taking into account bouncing and friction effects. All the collisions are processed independently in this way, during one single and common time step. Whenever elements are involved in several collisions, such as multilayer cloth, collision response is performed iteratively until all the collision constraints are resolved. This technique also allows propagation of the collision effect through the different layers of a cloth stack. Compressible viscoplasticity has been added to the collision response model, not only for simulating accurately multilayer compressibility, but also for ensuring good stability of the model in these situations. Figure 2: Dressed actors and multilayer cloth. Collision detection Collision and particularly self-collision detection is often the bottleneck of simulation applications in terms of calculation time, because of the scene complexity that involves a huge number of geometrical tests for determining which elements are colliding. Depending of the way the cloth object is represented, different techniques have been developped for solving efficiently the collision detection problem, using methods based on space subdivision such as voxelisation or octree [C11][C26] or hierarchisation [C22], rasterisation [C16], shortest distance tracking [C12], or mathematical techniques suited to curved parametrical surfaces [C2][C8][C17][C18]. In our case, the problem is complicated further because we are handling discretised surfaces that may contain thousands of polygons. Considering the clothing problem where garments are widely in contact with the body, collisions are not sparse at all and should be detected efficiently and accurately. Furthermore, because of all wrinkles and possible contacts between different parts of the same cloth, we have to efficiently detect self-collisions within the surfaces. This prevents the use of standard bounding box algorithms because potentially colliding regions of a surface are always touching each other by adjacency. We have developed a very efficient algorithm for handling this situation. This algorithm is based on hierarchisation and takes advantage of the adjacency which, combined with a surface curvature criteria, lets us skip large regular regions from the self-collision detection. We then get a collision evaluation time that is roughly proportional to the number of colliding elements, and independent of the total number of elements that compose our deforming surfaces. Figure 3: Hie"""	algorithm;buckling;c11 (c standard revision);cloth modeling;coefficient;collision detection;color;computation;computer simulation;data structure;discretization;elastic matching;elasticity (data store);humans;mechanical computer;minimum bounding box;modulus robot;monty newborn;multilayer perceptron;newton;nonlinear system;octree;particle system;performance tuning;perturbation theory;polynomial;rasterisation;rough set;software propagation;sparse matrix;subdivision surface;synthetic data;synthetic intelligence;thickness (graph theory);time complexity;triangle mesh;vertex (graph theory);virtual actor;watts humphrey;weil conjectures;welch's method;word lists by frequency;yang	Nadia Magnenat-Thalmann;Stéphane Carion;Martin Courchesne;Pascal Volino;Yin Wu	1996		10.1109/CGI.1996.511795	computer vision;simulation;computer science;artificial intelligence;virtual reality;computer animation;computer graphics (images)	Graphics	70.71937835242962	-47.32855765309679	122991
848c2d8e8c112ae89d3dc671c09c4d4d8f5d60f2	implicit multibody penalty-baseddistributed contact	torque;computer graphics;kinematics and dynamics computer graphics physically based modeling animation;force;computational modeling;singular value decomposition computer animation gaussian processes mechanical contact pi control real time systems;dynamics;animation;mathematical model;kinematics and dynamics;computer graphics implicit multibody penalty stability problems time varying distributed geometrically complex contact semiimplicit integration exact analytical contact gradients svd solver symbolic gaussian elimination stable penalty based frictional contact time varying contact areas articulated rigid objects complex conforming contact self contact implicit proportional derivative control forces real time control articulated structures computer animation;friction;force friction computational modeling torque equations mathematical model dynamics;physically based modeling	The penalty method is a simple and popular approach to resolving contact in computer graphics and robotics. Penalty-based contact, however, suffers from stability problems due to the highly variable and unpredictable net stiffness, and this is particularly pronounced in simulations with time-varying distributed geometrically complex contact. We employ semi-implicit integration, exact analytical contact gradients, symbolic Gaussian elimination and a SVD solver to simulate stable penalty-based frictional contact with large, time-varying contact areas, involving many rigid objects and articulated rigid objects in complex conforming contact and self-contact. We also derive implicit proportional-derivative control forces for real-time control of articulated structures with loops. We present challenging contact scenarios such as screwing a hexbolt into a hole, bowls stacked in perfectly conforming configurations, and manipulating many objects using actively controlled articulated mechanisms in real time.	cluster analysis;coherence (physics);computation;computer graphics;conformity;ephrin type-b receptor 1, human;excretory function;friction;gaussian elimination;gradient;maximal set;mental suffering;muscle rigidity;normal statistical distribution;pc game;parkinson disease;penalty method;personality character;physical object;real-time clock;real-time computing;robotics;semiconductor industry;servo;simulation;singular value decomposition;solver;stacking;stiffness matrix;time complexity;statistical cluster	Hongyi Xu;Yili Zhao;Jernej Barbic	2014	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2014.2312013	anime;dynamics;simulation;computer science;friction;mathematical model;torque;computer graphics;computational model;force;computer graphics (images)	Robotics	70.89541555180388	-47.18508981984356	123052
359cc7b6010c5f1d02da9d425f2a5d0ff07090a3	spotting structure in complex time dependent flow	time dependent;mathematics;high performance computing;climate model;large data sets;noise generators;direct numerical simulation;computational modeling;streaming media;flow field;displays;data visualization;high performance computer;computer science;climate modeling;interactive scientific visualization;high performance computing interactive scientific visualization flow visualization climate modeling direct numerical simulation;read only memory;flow visualization;data visualization read only memory numerical simulation mathematics computer science computational modeling displays high performance computing streaming media noise generators;numerical simulation	Analyzing structure in complex time dependent flow fields is a challenging problem. This paper describes the use of spot noise for the visualization of this type of fields. Spot noise is a technique which utilizes texture for the visualization of flow fields. In this paper, two extensions of spot noise are discussed. These extensions allow spot noise to be used for detailed analysis of time dependent flow fields. In addition, two cases are discussed: a large data set resulting from a direct numerical simulation of turbulence and a data set resulting from a global climate measurements.	direct numerical simulation;image resolution;numerical analysis;numerical weather prediction;turbulence	Wim C. de Leeuw;Robert van Liere	1997	Scientific Visualization Conference (dagstuhl '97)	10.1109/DAGSTUHL.1997.10015	simulation;computer science;theoretical computer science;computer graphics (images)	HPC	71.89721840356343	-50.74591803666881	123065
eceb2341fbb37d076ee8cf353ff5b820e75551c4	stream processors texture generation model for 3d virtual worlds: learning tools in vacademia	user evaluation stream processors texture generation model 3d virtual worlds learning tools three dimensional virtual worlds computational resources network resources educational content image processing tasks load reduction cpu collaborative work negative learning curve effect 2d graphical content vacademia virtual world;groupware;image processing;generic model;computer aided instruction;3d virtual world;virtual reality;three dimensional displays image color analysis program processors discrete wavelet transforms streaming media mathematical model;image texture;3d virtual worlds;vacademia;vacademia 3d virtual worlds image processing stream processors educational content;educational content;stream processors;virtual reality computer aided instruction groupware image texture microprocessor chips;microprocessor chips	In this paper, we address the challenges of applying three-dimensional virtual worlds for learning. Despite the numerous positive conclusions, this technology is far from becoming mainstream in education. The most common problems with applying it in everyday teaching and learning are steep learning curve and demand for computational and network resources. In order to address these problems, we developed a stream processors texture generation model for displaying educational content in 3D virtual worlds. The model suggests conducting image-processing tasks on stream processors in order to reduce the load on CPU. It allows designing convenient and sophisticated tools for collaborative work with graphics inside a 3D environment. Such tools simplify the use of a 3D virtual environment, and therefore, improve the negative learning curve effect. We present the methods of generating images based on the suggested model, the design and implementation of a set of tools for collaborative work with 2D graphical content in vAcademia virtual world. In addition, we provide the evaluation of the suggested model based on a series of tests which we applied to the whole system and specific algorithms. We also present the initial result of user evaluation.	2d computer graphics;3d computer graphics;algorithm;central processing unit;computation;discrete wavelet transform;filter (signal processing);graphical user interface;image processing;raster graphics;rasterisation;stream processing;virtual reality;virtual world	Andrey Smorkalov;Mikhail Fominykh;Mikhail Morozov	2013	2013 IEEE International Symposium on Multimedia	10.1109/ISM.2013.13	image texture;computer vision;simulation;stream processing;image processing;computer science;instructional simulation;operating system;machine learning;database;virtual reality;multimedia;world wide web;computer graphics (images)	Visualization	69.98304909372163	-50.36882636523789	123445
8343b6cb5a866226a437a9154d9a136ab9fc2a6e	the elements of probabilistic time geography	computadora;tratamiento datos;computers;analisis cualitativo;probability;ordinateur;cones;analisis cuantitativo;data processing;qualitative analysis;traitement donnee;space time;accuracy;modelo;a priori knowledge;precision;analyse qualitative;analyse quantitative;echantillon reference;probability distribution;probabilidad;cone;probabilite;quantitative analysis;space time cone;modele;geografia;mobile agent;time geography;geographie;topological relations;models;standard samples;roca patron;geography	Time geography uses space-time volumes to represent the possible locations of a mobile agent over time in a x-y-t space. A volume is a qualitative representation of the fact that the agent is at a particular time ti inside of the volume’s base at ti. Spacetime volumes enable qualitative analysis such as potential encounters between agents. In this paper the qualitative statements of time geography will be quantified. For this purpose an agent’s possible locations are modeled from a stochastic perspective. It is shown that probability is not equally distributed in a space-time volume, i.e., a quantitative analysis cannot be based simply on proportions of intersections. The actual probability distribution depends on the degree of a priori knowledge about the agent’s behavior. This paper starts with the standard assumption of time geography (no further knowledge), and develops the appropriate probability distribution by three equivalent approaches. With such a model any analysis of the location of an agent, or relations between the locations of two agents, can be improved in expressiveness as well as accuracy.	mobile agent;time geography	Stephan Winter;Zhang-Cai Yin	2011	GeoInformatica	10.1007/s10707-010-0108-1	cone;time geography;data processing;geography;artificial intelligence;mathematics;accuracy and precision;cartography;statistics	AI	77.40011626441013	-50.53122776064489	124334
c3e52fb6b09927b3be0a85bddde7e58ec770d2c5	smooth bi-3 spline surfaces with fewest knots	catmull rom splines;bicubic;spline surfaces;tensor product;construction;geometry continuity	Converting a quadrilateral input mesh into a C surface with one bi-3 tensorproduct spline patch per facet is a classical challenge. We give explicit local averaging formulas for the spline control points. Where the quadrilateral mesh is not regular, the patches have two internal double knots, the least number and multiplicity to always allow for an unbiased G construction.	b-spline;bicubic interpolation;catmull–clark subdivision surface;coefficient;polynomial;spline (mathematics);vieta's formulas	Jianhua Fan;Jörg Peters	2011	Computer-Aided Design	10.1016/j.cad.2010.11.002	spline interpolation;tensor product;mathematical analysis;perfect spline;topology;construction;smoothing spline;engineering;cubic hermite spline;hermite spline;bicubic interpolation;mathematics;geometry;thin plate spline;flat spline	EDA	69.08555597763265	-41.19146077003816	124455
383df4b75d87081f7cc4b3af134e2406d08d51e2	stable real-time deformations	elasticity;large deformations;strain measurement;mass spring system;real time;reference frame;physically based animation;finite element;computer graphic;deformable objects;physics based animation;simulation technique;real time animation;linear model;finite element model;finite elements;large deformation;stiffness warping	The linear strain measures that are commonly used in real-time animations of deformable objects yield fast and stable simulations. However, they are not suitable for large deformations. Recently, more realistic results have been achieved in computer graphics by using Green's non-linear strain tensor, but the non-linearity makes the simulation more costly and introduces numerical problems.In this paper, we present a new simulation technique that is stable and fast like linear models, but without the disturbing artifacts that occur with large deformations. As a precomputation step, a linear stiffness matrix is computed for the system. At every time step of the simulation, we compute a tensor field that describes the local rotations of all the vertices in the mesh. This field allows us to compute the elastic forces in a non-rotated reference frame while using the precomputed stiffness matrix. The method can be applied to both finite element models and mass-spring systems. Our approach provides robustness, speed, and a realistic appearance in the simulation of large deformations.	computer graphics;finite element method;linear model;nonlinear system;numerical analysis;precomputation;real-time clock;reference frame (video);robustness (computer science);simulation;stiffness matrix	Matthias Müller;Julie Dorsey;Leonard McMillan;Robert Jagnow;Barbara Cutler	2002		10.1145/545261.545269	simulation;computer science;finite element method;geometry;computer graphics (images)	Graphics	69.88874646308778	-47.14644789606009	124799
86d7457b6278c112edc7dfefd664bb8d252c409f	a new time delay estimation in subbands for resolving multiple specular reflections	transformation ondelette;discrete wavelet transforms;traitement signal;deteccion blanco;acoustique sous marine;transformacion discreta;discrete wavelet transform;adaptive filtering;filtrado adaptable;underwater sound;time delay estimation;acoustic wave reflection;backscatter;indexing terms;analyse multiresolution;delay effects delay estimation signal processing discrete wavelet transforms optical reflection object detection clutter matched filters amplitude estimation cities and towns;time delay;sonar detection;detection cible;specular reflection;adaptive filters;signal decorrelation time delay estimation multiple specular reflections multiresolution analysis discrete wavelet transform dwt signal decomposition mean squared error primary signal reference signal mse curves underwater target detection acoustic backscattered data iterative estimation sonar subband adaptive filtering decimation filter bank snr;mean square error;signal processing;adaptive signal detection;discrete transformation;mean square error methods;signal resolution;temps retard;filtrage adaptatif;transformacion ondita;delay time;mean square error methods acoustic wave reflection underwater sound delay estimation backscatter sonar detection discrete wavelet transforms adaptive signal detection adaptive filters filtering theory signal resolution;multiresolution analysis;procesamiento senal;target detection;tiempo retardo;transformation discrete;underwater acoustics;adaptive filter;filtering theory;delay estimation;wavelet transformation;analisis multiresolucion;acustica submarina;sonar	In this correspondence, a new time delay estimation procedure is proposed using the multiresolution analysis framework through a discrete wavelet transform (DWT). Once the signals are decomposed, the time delays are estimated iteratively in each sub-band using two different adaptation mechanisms that minimize the mean squared error (MSE) between the reference and primary signals in the corresponding sub-band and level. The localization of the minima of the MSE curves at different levels and subbands is used in order to arrive at the time delay estimates. The proposed scheme is then applied to a real-life problem of underwater target detection from the acoustic backscttered data.	acoustic cryptanalysis;broadcast delay;discrete wavelet transform;maxima and minima;mean squared error;multiresolution analysis;real life;reflection (computer graphics)	Mahmood R. Azimi-Sadjadi;S. Charleston;JoEllen Wilbur;Gerald J. Dobeck	1998	IEEE Trans. Signal Processing	10.1109/78.735312	adaptive filter;computer vision;speech recognition;computer science;signal processing;mathematics	Metrics	82.56579436607512	-39.36162660261551	124810
afbc49c588d4d4814166b3b15a2e1ac51d8c6040	volumetric modeling with implicit functions: a cloud is born	implicit function;volumetric modeling	This sketch describes a new, flexible, natural, intuitive, volumetric modeling and animation technique that combines implicit functions with turbulence-based procedural techniques. A cloud is modeled to demonstrate its advantages.	cloud computing;turbulence	David S. Ebert	1997		10.1145/259081.259233	computer graphics (images);procedural approach;implicit function;computer graphics;cloud computing;computer science	Graphics	68.96879017239837	-47.822623416153654	124868
ab7265bac0d46699fa7580fc06e0f147243ed16a	adaptive radial basis function mesh deformation using data reduction	fluid structure interaction;adaptive selection;greedy selection;radial basis function mesh deformation;boundary correction	Radial Basis Function (RBF) mesh deformation is one of the most robust mesh deformation methods available. Using the greedy (data reduction) method in combination with an explicit boundary correction, results in an efficient method as shown in literature. However, to ensure the method remains robust, two issues are addressed: 1) how to ensure that the set of control points remains an accurate representation of the geometry in time and 2) how to use/automate the explicit boundary correction, while ensuring a high mesh quality. In this paper, we propose an adaptive RBF mesh deformation method, which ensures the set of control points always represents the geometry/displacement up to a certain (user-specified) criteria, by keeping track of the boundary error throughout the simulation and re-selecting when needed. Opposed to the unit displacement and prescribed displacement selection methods, the adaptive method is more robust, user-independent and efficient, for the cases considered. Secondly, the analysis of a single high aspect ratio cell is used to formulate an equation for the correction radius needed, depending on the characteristics of the correction function used, maximum aspect ratio, minimum first cell height and boundary error. Based on the analysis two new radial basis correction functions are derived and proposed. This proposed automated procedure is verified while varying the correction function, Reynolds number (and thus first cell height and aspect ratio) and boundary error. Finally, the parallel efficiency is studied for the two adaptive methods, unit displacement and prescribed displacement for both the CPU as well as the memory formulation with a 2D oscillating and translating airfoil with oscillating flap, a 3D flexible locally deforming tube and deforming wind turbine blade. Generally, the memory formulation requires less work (due to the large amount of work required for evaluating RBF's), but the parallel efficiency reduces due to the limited bandwidth available between CPU and memory. In terms of parallel efficiency/scaling the different studied methods perform similarly, with the greedy algorithm being the bottleneck. In terms of absolute computational work the adaptive methods are better for the cases studied due to their more efficient selection of the control points. By automating most of the RBF mesh deformation, a robust, efficient and almost user-independent mesh deformation method is presented.	radial (radio);radial basis function	T. Gillebaart;David Blom;Alexander van Zuijlen;Hester Bijl	2016	J. Comput. Physics	10.1016/j.jcp.2016.05.036	mathematical optimization;mathematics;geometry	Theory	70.39703710257484	-44.72413378361591	125098
fff4dd376acd61a8139304f51d397c9bb15d9b1a	optimal parameter values for approximating conic sections by the quartic bézier curves	quartic bezier curves;hausdorff distance;conic section;optimal parameters	The previous approximation curves of conic section by quartic Bezier curves interpolate the conic section at the specified parameter values. In this paper, by solving the minimax problem, we present the optimal parameter values for approximating conic sections by the quartic Bezier curves. The upper bound on the Hausdorff distance between the conic section and the approximation curves is minimized. The method of solving the minimax problem is changed to solve a quartic algebraic equation by delicate reasoning.	bézier curve;quartic function	Xuli Han;Xiao Guo	2017	J. Computational Applied Mathematics	10.1016/j.cam.2017.03.029	hausdorff distance;mathematical optimization;mathematical analysis;mathematics;geometry;family of curves;conic section;quartic surface;conic constant	ML	69.68585527745442	-40.27371938356739	125418
7847876de9e194cfcc741b7bc73e4deeed2faa5c	rzsweep: a new hardware-assisted volume-rendering technique for regular datasets	data interpretation;volume rendering;space complexity rzsweep algorithm hardware assisted volume rendering technique regular datasets scientific datasets data interpretation plane sweep paradigm direct volume rendering back to front projection algorithm image plane hardware rendering pipeline;computer graphic equipment;data visualisation;computational complexity;rendering computer graphics transfer functions hardware data visualization magnetic resonance imaging computed tomography positron emission tomography lattices laboratories scientific computing;space complexity;computational complexity rendering computer graphics computer graphic equipment data visualisation;near real time;rendering computer graphics;direct volume rendering;off the shelf	A long-standing challenge in the field of volume rendering is to obtain high quality images in near real-time. This is particularly important for scientific datasets where highly precise results are required to ensure accurate data interpretation. We present RZSweep as a new hardware-assisted technique for volume rendering of regular datasets based on the plane-sweep paradigm. Although some research has been done in the past for irregular datasets, we present the first attempt to explore the capabilities of the plane sweep paradigm for regular datasets. RZSweep is an exact, object order, direct volume rendering (DVR), back-to-front projection algorithm wherein a plane sweeps through the volume in depth order, projecting all the data elements within the user specified threshold onto the image plane. The algorithm uses a hardware rendering pipeline to composite the final image. The space complexity of the algorithm is bound to the number of vertices in the largest diagonal plane of the dataset. A prototype of the algorithm renders a dataset of size 256/sup 3/ in near real-time of 0.77 seconds on a single off-the-shelf commodity hardware PC. Every data element within the specified threshold contributes to the final image in its correct spatial order that is guaranteed by the use of a heap.	volume rendering	Gautam Chaudhary;Lakshmy Ramaswamy;Ricardo C. Farias	2003		10.1109/SIBGRA.2003.1240997	computer vision;tiled rendering;scientific visualization;image-based modeling and rendering;3d rendering;rendering;computer science;theoretical computer science;parallel rendering;real-time rendering;texture memory;alternate frame rendering;volume rendering;software rendering;computer graphics (images)	EDA	68.5386680918188	-51.4211687476189	125541
b953d26ae5e8832b9462844e530850ef2cfdd2ef	surface remeshing with robust high-order reconstruction	high order methods;mesh adaptation;accuracy and stability;mesh generation;curves and surfaces	Remeshing is an important problem in variety of applications, such as finite element methods and geometry processing. Surface remeshing poses some unique challenges, as it must deliver not only good mesh quality but also good geometric accuracy. For applications such as finite elements with high-order elements (quadratic or cubic elements), the geometry must be preserved to high-order (third-order or higher) accuracy, since low-order accuracy may undermine the convergence of numerical computations. The problem is particularly challenging if the CAD model is not available for the underlying geometry, and is even more so if the surface meshes contain some inverted elements. We describe remeshing strategies that can simultaneously produce high-quality triangular meshes, untangling mildly folded triangles and preserve the geometry to high-order of accuracy. Our approach extends our earlier works on high-order surface reconstruction and mesh optimization by enhancing its robustness with a geometric limiter for under-resolved geometries. We also integrate high-order surface reconstruction with surface mesh adaptation techniques, which alter the number of triangles and nodes. We demonstrate the utilization of our method to meshes for high-order finite elements, biomedical image-based surface meshes, and complex interface meshes in fluid simulations.	algorithm;computation;computer graphics (computer science);computer-aided design;converge;cubic function;finite element method;geometry processing;limiter;mathematical optimization;numerical analysis;order of accuracy;polygon mesh;quadratic function;simulation;triangle mesh;triangulated irregular network	Navamita Ray;Tristan Delaney;Daniel R. Einstein;Xiangmin Jiao	2014	Engineering with Computers	10.1007/s00366-014-0359-9	mesh generation;mathematical optimization;computer science;volume mesh;engineering;mathematics;geometry;engineering drawing	Graphics	69.05637214234089	-44.81805048515025	125992
15f8b70b9e67bcaf91b515eef4c4e3b7f5f8af21	facial hexahedral mesh transferring by volumetric mapping based on harmonic fields	facial soft tissue;complex objects;soft tissue modeling;harmonic field;soft tissue;hexahedral mesh;tetrahedral mesh	Hexahedral mesh has obvious mechanical advantages over tetrahedral mesh, but it is no trivial task to generate hexahedral mesh for complex object shapes such as individual faces. This paper presents a novel method to generate patient-specific hexahedral meshes of facial soft tissue models, based on a volumetric cross-parameterization mapping from a standard hexahedral mesh to the individual model. The volumetric parameterization is constructed based on triple of the volumetric harmonic fields, which are adapted to be as close to mutually orthogonal as possible, to achieve some quasi-conformal effect. In addition, some piecewise constraints on the harmonic fields are added to ensure anatomical feature correspondence. Experimental results show that our approach works efficiently for facial soft tissue modeling, avoids element flipping and preserves mesh element angles to a significant extent.	hexahedron	Meng-fei Li;Shenghui Liao;Ruofeng Tong	2011	Computers & Graphics	10.1016/j.cag.2010.11.013	simulation;geometry;soft tissue	Visualization	68.71648753300397	-45.316586519481994	126106
4b8635043b766167c9f7371527334d85eb9aeade	large-scale terrain visualization system in a cluster environment	graph theory;gpu terrain visualization lod scene graph;terrain mapping data visualisation graph theory rendering computer graphics;data visualisation;large scale terrain visualization system lod gpu optimized terrain rendering algorithms data thread view frustum culling scene graph structure single machine environment level of detail building cluster environment;terrain mapping;rendering computer graphics;rendering computer graphics graphics processing units instruction sets tiles servers data visualization acceleration	In this paper we propose an efficient Large-scale terrain visualization system in a cluster environment. Firstly, we implement level-of-detail (LOD) building in parallel, with an obvious time saving compared with a single-machine environment. The LODs are then stored in the server. While rendering, clients get LODs on demand. LODs are organized with a scene graph structure, thus view frustum culling can be performed efficiently. A data thread is introduced to particularly manage data exchanged with the scene graph, cooperating with the rendering thread. In addition, we use GPU optimized terrain rendering algorithms to achieve high triangles throughput. To summarize, with our system, original large-scale terrain data can be built into LODs in a relatively short time and LODs will be rendered interactively.	algorithm;frustum;glossary of computer graphics;graphics processing unit;hidden surface determination;interactivity;level of detail;scene graph;server (computing);terrain rendering;throughput	Xiao Guo;Lei Liu;Yong Gao;Hao Yu;Haoran Li	2013	2013 21st International Conference on Geoinformatics	10.1109/Geoinformatics.2013.6626125	terrain rendering;computer vision;simulation;image-based modeling and rendering;rendering;computer science;parallel rendering;software rendering;computer graphics (images)	Visualization	69.1371932622935	-51.61852321915046	126134
0abb9fd244ddf3f2af1135309a9fbe94d245f1d7	using the cw-complex to represent the topological structure of implicit surfaces and solids	implicit surface;particle systems;topology;critical point;interactive modeling;critical points;polygonization;catastrophe theory;implicit surfaces;surface model;morse theory;data structure;interval analysis	We investigate the CW-complex as a data structure for visualizing and controlling the topology of implicit surfaces. Previous methods for contolling the blending of implicit surfaces redefined the contribution of a metaball or unioned blended components. Morse theory provides new insight into the topology of the surface a function implicitly defines by studying the critical points of the function. These critical points are organized by a separatrix structure into a CW-complex. This CW-complex forms a topological skeleton of the object, indicating connectedness and the possibility of connectedness at various locations in the surface model. Definitions, algorithms and applications for the CW-complex of an implicit surface and the solid it bounds are given as a preliminary step toward direct control of the topology of an implicit surface.	algorithm;alpha compositing;data structure;definition;implicit surface;metaballs;topological skeleton	John C. Hart	2005		10.1145/1198555.1198643	discrete mathematics;topology;data structure;catastrophe theory;particle system;mathematics;geometry;critical point;morse theory;critical point	Graphics	71.22021627515211	-44.337625731819784	126211
e21d3b0bd8f72a6bcb4aa0a4a45544ab6eee8b53	simulating 2d tearing phenomena for interactive medical surgery simulators	real time;computational geometry;virtual reality;virtual surgery;computational geometry deformation fracture surgery computer animation virtual reality medical computing;medical computing;deformable objects;medical simulation minimally invasive surgery solid modeling biological system modeling deformable models humans computer graphics computational modeling biological tissues accidents;fracture;deformation;surgery;surgery simulation;virtual environment;computer animation;physical simulation;physical simulation elements 2d tearing phenomena interactive medical surgery simulators fracture phenomenon realistic tearing simulations 2d deformable objects real time surgery simulators human tissue virtual environment	This paper introduces a methodology to simulate a particular kind of the fracture phenomenon : the tearing one. The objective of the method is to provide realistic tearing simulations, occuring on 2D deformable objects, and working in real-time. This method can find applications in surgery simulators, where tearing human tissue is an important feature to provide. It can also be applied, in a more general scope, in virtual environment applications. The central idea in this scheme is to “separate” the physical simulation elements, instead of destroying or dividing the m. This avoids the increase of physical elements that need to be simulated (which occurs when they are divided) and gives a more accurate simulation than when they are destroyed.	algorithm;dynamical simulation;feedback;mesh networking;opengl;real-time clock;real-time locating system;steam rupture;surgery simulator;virtual reality	François Boux de Casson;Christian Laugier	2000		10.1109/CA.2000.889020	computer vision;simulation;engineering;computer graphics (images)	Graphics	71.40708266991835	-47.29019185810332	126926
32269b800369e179d27b35264272463872782a22	gregory-type patches bounded by low degree intergral curves for g2 continuity	forma libre;concepcion asistida;computer aided design;continuity;curva bezier;analyse surface;free form;nurbs;forme libre;courbe bezier;analisis superficie;blending function;courbe niveau;conception assistee;be zier patch;curva nivel;g 2 continuity;gregory type patch;contour line;bezier curve;surface analysis	Copyright (c) 1996 Elsevier Science B.V. All rights reserved. G 2  continuity of free-form surfaces is sometimes very important in engineering applications. The conditions for G 2  continuity to connect two Bezier patches were studied and methods have been developed to ensure it. However, they have some restrictions on the shapes of patches of the Bezier-patch formulation. Gregory patch is a kind of free-form surface patch which is extended from Bezier patch so that four first derivatives on its boundary curves can be specified without restrictions of the compatibility condition. Several types of Gregory patches have been developed for integral, rational, and NURBS boundary curves. In this paper, we propose some integral boundary Gregory-type patches bounded by cubic and quartic curves for G 2  continuity.	business continuity;scott continuity	Kenjiro Takai Miura;Kuo-King Wang	1996	Computer Aided Geometric Design	10.1016/0167-8396(95)00037-2	topology;non-uniform rational b-spline;computer aided design;surface weather analysis;calculus;bézier curve;mathematics;geometry;contour line;mechanical engineering	Theory	68.57754188820064	-40.55572309016587	127827
ce567ab186c731a83d6d4c1963a6343aff78faa5	chaotic attractors with symmetries of the triangle groups	x ray diffraction;chaos fractals visualization mathematics art x ray diffraction crystallography mathematical model computer graphics orbits;fractals;art;iterated function system;mathematics;probability;image processing;chaos;computer graphics;computational geometry;symmetry;iterative methods;data visualisation;orbits;visualization;fine structure;probabilistic iterated function systems;chaotic attractors;symmetry group;affine transformation;hyperbolic plane;mathematical model;tiling patterns chaotic attractors triangle groups hyperbolic plane probabilistic iterated function systems;tiling patterns;iterative methods computational geometry data visualisation image processing chaos fractals symmetry probability;crystallography;triangle groups	The chaotic attractors with symmetries of the triangle groups are visualized in the hyperbolic plane. Using probabilistic iterated function systems that involve both affine transformations of the plane and isometries of triangle groups, we can create intriguing and exotic attractor's images. The derived images are of self-similarity and of infinitely fine structure; they contain symmetries associated with the triangle groups as well. The method provides a novel approach for devising tiling patterns with symmetries.	iterated function system;iteration;self-similarity;tiling array;tiling window manager;triangle group	Ruisong Ye	2005	International 2005 Computer Graphics	10.1109/CGI.2005.1500382	triangle group;combinatorics;discrete mathematics;symmetry group;visualization;fractal;image processing;computational geometry;fine structure;hyperbolic geometry;probability;mathematical model;affine transformation;mathematics;geometry;iterative method;symmetry;computer graphics;iterated function system	Visualization	74.09289294299407	-46.078318556421465	127914
ec4a61c5efb2e20089b93a55d64540c240e8deb5	establishing a balanced neighborhood of discrete points for local quadric surface fitting	quadric surface fitting;balanced point neighborhood;local geometry;fitting bias;point cloud	This paper presents a novel algorithm to establish a balanced neighborhood of points for reliable local quadric surface fitting, a common task in point cloud data processing. The underlying smooth surface geometry of a point cloud in the vicinity of a point can be locally approximated by the best fitted quadric surface at the point. The quality of the fitted surface considerably depends on what neighboring points are selected for the fitting. Specifically, if the selected neighboring points carry a biased distribution, the fitted geometry becomes biased, resulting in loss of accuracy in the fitting. The presented algorithm in this paper is able to reliably select neighboring points considering measures of both distance and direction. The main feature is the development of a geometric relationship, named as Territory Claiming, between the selected and the candidate neighboring points. The fundamental principle is for the selected point set to cover the whole neighborhood domain without redundancy. The selection procedure starts with a distance-based sequence of neighboring points with the territory claiming relationship functioning as a filter to establish a well-balanced neighborhood. The neighborhood can be expanded to incorporate sufficient number of points for the quadric surface fitting while maintaining the balance of the overall neighborhood. The implementation results have demonstrated that the presented method is robust and selects local neighboring points with superior fitting performance in comparison with the distance-based neighbors, mesh neighbors, and elliptic Gabriel graph neighbors.	approximation algorithm;multivariate interpolation;point cloud	Farbod Khameneifar;Hsi-Yung Feng	2017	Computer-Aided Design	10.1016/j.cad.2016.12.001	mathematical optimization;topology;engineering;point cloud;mathematics;geometry;mechanical engineering	Vision	70.56616355771007	-43.32222468102526	128290
9652dceade832afbe15d01a596a83996104a7888	geometric conditions of non-self-intersecting nurbs surfaces		NURBS surface is very useful in geometric modeling, animation, image morphing and deformation. Constructing non-self-intersecting (injective) NURBS surfaces is an important process in surface and solid modeling. In this paper, the injective conditions of tensor product NURBS surface are studied, based on the geometric positions of control points, which are equivalent to the surface to be non-self-intersecting for all positive weights. Finally, some representative examples are provided.	non-uniform rational b-spline	Xuan-Yi Zhao;Chun-Gang Zhu;Han Wang	2017	Applied Mathematics and Computation	10.1016/j.amc.2017.04.016	t-spline;topology;mathematics;geometry	Robotics	68.46874497943332	-41.83952981469699	128648
ddee859ddd0eea0d9c6f81d8a93fb6d7f89d512a	neurotessmesh: a tool for the generation and visualization of neuron meshes and adaptive on-the-fly refinement	gpus and multi-core architectures;bioinformatics visualization;compression techniques;geometry-based techniques;multiresolution techniques	Gaining a better understanding of the human brain continues to be one of the greatest challenges for science, largely because of the overwhelming complexity of the brain and the difficulty of analyzing the features and behavior of dense neural networks. Regarding analysis, 3D visualization has proven to be a useful tool for the evaluation of complex systems. However, the large number of neurons in non-trivial circuits, together with their intricate geometry, makes the visualization of a neuronal scenario an extremely challenging computational problem. Previous work in this area dealt with the generation of 3D polygonal meshes that approximated the cells' overall anatomy but did not attempt to deal with the extremely high storage and computational cost required to manage a complex scene. This paper presents NeuroTessMesh, a tool specifically designed to cope with many of the problems associated with the visualization of neural circuits that are comprised of large numbers of cells. In addition, this method facilitates the recovery and visualization of the 3D geometry of cells included in databases, such as NeuroMorpho, and provides the tools needed to approximate missing information such as the soma's morphology. This method takes as its only input the available compact, yet incomplete, morphological tracings of the cells as acquired by neuroscientists. It uses a multiresolution approach that combines an initial, coarse mesh generation with subsequent on-the-fly adaptive mesh refinement stages using tessellation shaders. For the coarse mesh generation, a novel approach, based on the Finite Element Method, allows approximation of the 3D shape of the soma from its incomplete description. Subsequently, the adaptive refinement process performed in the graphic card generates meshes that provide good visual quality geometries at a reasonable computational cost, both in terms of memory and rendering time. All the described techniques have been integrated into NeuroTessMesh, available to the scientific community, to generate, visualize, and save the adaptive resolution meshes.		Juan Jose Garcia-Cantero;Juan P. Brito;Susana Mata;Sofía Bayona Beriso;Luis Pastor	2017		10.3389/fninf.2017.00038	computational problem;theoretical computer science;rendering (computer graphics);machine learning;artificial intelligence;computer science;visualization;artificial neural network;polygon mesh;shader;mesh generation;adaptive mesh refinement	Visualization	70.14262061556951	-51.011045073651445	128745
a4734f1f9f5ef65c85ef50c1cf45a2ea7afad4dc	individual time-stepping for rigid-fluid coupling of particle based fluids	physically basedsimulation;rigid fluid coupling;individual time stepping	We propose an efficient rigid-fluid coupling method using individual time-stepping for particle-based fluid simulation. It updates neighbors and forces only for active particles. It allocates computing resources to complex regions and has an apparent speedup. We realize our method with several scenarios for rigid-fluid interaction. The experimental results demonstrate that our method is capable to implement interaction of rigid body and fluids while it also improves the efficiency.	algorithm;computational fluid dynamics;fluid animation;simulation;speedup;stepping level	Ruixiang Li;Xiaokun Wang	2016	2016 International Conference on Cyberworlds (CW)	10.1109/CW.2016.48	simulation	Robotics	71.98620658594814	-47.55728321305962	129499
205b0711be40875a7ea9a6ac21c8d434b7830943	hexahedral meshing with varying element sizes.		Hexahedral (or Hex-) meshes are preferred in a number of scientific and engineering simulations and analyses due to their desired numerical properties. Recent state-of-the-art techniques can generate high quality hex-meshes. However, they typically produce hex-meshes with uniform element sizes and thus may fail to preserve small scale features on the boundary surface. In this work, we present a new framework that enables users to generate hex-meshes with varying element sizes so that small features will be filled with smaller and denser elements, while the transition from smaller elements to larger ones is smooth, compared to the octree-based approach. This is achieved by first detecting regions of interest (ROI) of small scale features. These ROIs are then magnified using the as-rigid-as-possible (ARAP) deformation with either an automatically determined or a user-specified scale factor. A hex-mesh is then generated from the deformed mesh using existing approaches that produce hex-meshes with uniform-sized elements. This initial hex-mesh is then mapped back to the original volume before magnification to adjust the element sizes in those ROIs. We have applied this framework to a variety of man-made and natural models to demonstrate its effectiveness.	3d modeling;computer graphics;display resolution;finite element method;hausdorff dimension;hex;hexahedron;ibm notes;image scaling;image-based meshing;jacobian matrix and determinant;limbo;microsoft developer network;minimum bounding box;numerical analysis;octree;polygon mesh;pony island;region of interest;resultant;sensor;simulation;triangular function;volume mesh;yang	Kaoji Xu;Xifeng Gao;Zhigang Deng;Guoning Chen	2017	Comput. Graph. Forum	10.1111/cgf.13100	hexahedron;computer vision;artificial intelligence;computer science;mesh generation	Graphics	68.79947156830697	-44.92655403895803	129543
73456d391ef721d0944413278fe553a8b319d027	visualizing collision effects between penetrating and non-penetrating objects	real time;poisson disk;virtual reality;distance field;blue noise;sampling;deformable objects;surgery simulation	"""We present a novel method for visualizing collisions effects between penetrating or non-penetrating objects. The interaction of 3D objects has been traditionally divided into two stages: the detection of a collision, and the response or effects after the collision. In this work, we concern ourselves with the latter. The rendering of collision effects is useful for virtual sculpting, virtual reality, surgery simulation and games. The interaction between two colliding objects has been used for virtual sculpting, where one object (virtual """"tool""""), generates indentations in the other object (virtual """"clay""""). This has been implemented in real-time with the aid of a distance field representation of the virtual tool. One of the assumptions of virtual sculpting, reminiscent of real sculpting materials (e.g., clay), is that objects are non-penetrating. In this paper, we generalize the notion of collision effects by allowing penetrating """"tools"""", where the object is cut as the tool penetrates the object. The collision effect is such that the object is still deformed, but a break appears where the tool is. In this work, we use an implicit representation of the deformed object (virtual """"clay""""), as opposed to traditional mesh-based approaches. The ability to deform objects implicitly was exploited by our previous work [Correa et al. 2006a; Correa et al. 2006b], but they were concerned with simple primitive deformations such as twists or peels."""	distance transform;real-time computing;real-time locating system;simulation;virtual reality	Carlos D. Correa;Deborah Silver	2007		10.1145/1278780.1278837	sampling;computer vision;simulation;colors of noise;computer science;geometry;virtual reality;distance transform;statistics;computer graphics (images)	Visualization	68.76668935949812	-47.76675047844537	130019
6fda2d552876d5947ff3bf0b25b48b60eec64e72	weitek solids modeling engine	solid modeling	"""INTRODUCTION Computer graphics technology has evolved to where the solid modeling of three-dimensional (3D) objects is technically feasible. However, it is only recently that solid modeling has become commercially viable. Especially in the mechanical computer-aided design and manufacturing (CAD/CAM) applications, solid modeling application software and systems are becoming increasingly popular. Likewise, other applications, such as architectural design, piping design, medical diagnostic imaging and molecular modeling, are using solid modeling and shaded graphic display techniques. In the mechanical CAD/CAM environment, a solid model is designed through interaction with a constructive solid geometry or boundary representation software system resident on a host computer. As the model ,evolves, the system builds and maintains a working representation of the solid object for display purposes. This working representation takes the form of polyhedral approximations of the object or, in some cases, may be represented by complex parametric cubic surface patches. The Weitek Solids Modeling Engine"""" performs high-speed 3D rotation, translation, scaling, clipping, perspective division, tesselation and light-source modeling through a high-performance, 32-bit floating point transformation processor. The processed graphics data are transferred to a tiling engine, which performs hidden surface removal and Gouraud shading. Additionally, the Weitek processor performs sectioning of the solid objects, or makes objects translucent, so they can be seen through, locally with no load to the host processor. Individual, shaded pixels are then sent directly to high-speed frame buffers for display on monitors. This entire process from graphics transformation through shading takes place in under 10 sec, even for very complex solid objects."""	32-bit;approximation;boundary representation;clipping (computer graphics);computational chemistry;computer graphics;computer-aided design;constructive solid geometry;cubic function;framebuffer;gouraud shading;hidden surface determination;host (network);image scaling;mechanical computer;medical imaging;molecular modelling;pixel;polyhedron;single-precision floating-point format;software system;solid modeling;tiling window manager;transformation matrix	Weitek	1984	Computers & Graphics	10.1016/0097-8493(84)90042-6	computer science;solid modeling	Graphics	68.7308181339188	-49.495939967562514	130148
82b232c500340ae44fdccb3baa49cdbe9243f68d	designing and evaluating a mesh simplification algorithm for virtual reality		With the increasing accessibility of the mobile head-mounted displays (HMDs), mobile virtual reality (VR) systems are finding applications in various areas. However, mobile HMDs are highly constrained with limited graphics processing units (GPUs) and low processing power and onboard memory. Hence, VR developers must be cognizant of the number of polygons contained within their virtual environments to avoid rendering at low frame rates and inducing simulator sickness. The most robust and rapid approach to keeping the overall number of polygons low is to use mesh simplification algorithms to create low-poly versions of pre-existing, high-poly models. Unfortunately, most existing mesh simplification algorithms cannot adequately handle meshes with lots of boundaries or nonmanifold meshes, which are common attributes of many 3D models.  In this article, we present QEM4VR, a high-fidelity mesh simplification algorithm specifically designed for VR. This algorithm addresses the deficiencies of prior quadric error metric (QEM) approaches by leveraging the insight that the most relevant boundary edges lie along curvatures while linear boundary edges can be collapsed. Additionally, our algorithm preserves key surface properties, such as normals, texture coordinates, colors, and materials, as it preprocesses 3D models and generates their low-poly approximations offline.  We evaluated the effectiveness of our QEM4VR algorithm by comparing its simplified-mesh results to those of prior QEM variations in terms of geometric approximation error, texture error, progressive approximation errors, frame rate impact, and perceptual quality measures. We found that QEM4VR consistently yielded simplified meshes with less geometric approximation error and texture error than the prior QEM variations. It afforded better frame rates than QEM variations with boundary preservation constraints that create unnecessary lower bounds on overall polygon count reduction. Our evaluation revealed that QEM4VR did not fair well in terms of existing perceptual distance measurements, but human-based inspections demonstrate that these algorithmic measurements are not suitable substitutes for actual human perception. In turn, we present a user-based methodology for evaluating the perceptual qualities of mesh simplification algorithms.	3d modeling;accessibility;algorithm;approximation error;color;computer graphics;experiment;futures studies;graphics processing unit;head-mounted display;level of detail;low poly;multimedia pc;normal (geometry);online and offline;polygon (computer graphics);polygon mesh;simulation;software quality assurance;text simplification;texture mapping;virtual reality	Kanchan Bahirat;Chengyuan Lai;Ryan P. McMahan;B. Prabhakaran	2018	TOMCCAP	10.1145/3209661	rendering (computer graphics);frame rate;quadric;polygon;polygon (computer graphics);simulator sickness;polygon mesh;approximation error;algorithm;computer science	Visualization	68.68854373010467	-45.21368206721221	130532
5bbf7e2c86429f2ef6fd53c8f4859d1d535e0ba9	optical density visualization and abel reconstruction of vortex rings using background-oriented schlieren	optical density;background oriented schlieren;vortex ring	These figures represent a sequence of visualizations of CO2-loaded vortex rings generated at the orifice opening of a piston-cylinder apparatus (diameter of orifice opening is 7 cm; Re = 36’000; ratio of the piston stroke length to diameter is 0.5 and field of view is 15 x 20 cm). Qualitative Schlieren visualizations are obtained using a Background Oriented Schlieren (BOS) technique (Fig. 1) and a vector map of the gradients of the refractive index is extracted using a PIV algorithm (Fig. 2). The projected density field (Fig. 3) is then obtained by integrating the measured gradient field. Finally, an Abel inverse transform is implemented to reconstruct the true radial vortex ring profiles for enhanced visualization of flow structures such as the recirculating spiral roll-ups and trailing wakes (Fig. 4). Fig. 1. Schlieren visualization: absolute difference between phase and reference are displayed. Fig. 2. Vector map of the refractive index gradient (scale in arbitrary units).	abel transform;algorithm;cylinder seal;gradient;music visualization;radial (radio);taylor–green vortex;vector map;xfig	Josué Sznitman;Thomas Rösgen	2007	J. Visualization	10.1007/BF03181789	schlieren photography;optics;nuclear magnetic resonance;thermodynamics;physics;synthetic schlieren;vortex ring;absorbance	Visualization	73.88103432619329	-48.835493256391935	130781
8a39a897257effef34def186d94120907ed45c48	improved subdivision scheme for the root computation of univariate polynomial equations	polygonal bound;bernstein basis;univariate polynomial equation;subdivision;root computation	In this paper, a subdivision method of computing the roots of a univariate polynomial equation is proposed using novel bounding methods. The equation is converted into a Bézier curve, and the root computation problem is transformed into the geometric problem of intersection computation between the curve and an axis, which can be solved using a bound of the curve and subdivision. Four different bounding schemes are compared, and new hybrid bounding schemes are proposed for use in the root computation. In particular, the convex hull and the quasi-interpolating bound are combined to produce a smaller polygonal bound, which is then used for the root computation of the input equation. The new bounding scheme provides improved robustness and performance compared to the existing convex hull-based method, e.g., the Projected Polyhedron algorithm. Examples are provided to demonstrate the performance of the proposed method, which shows that the proposed approach is superior to the existing one. 2013 Elsevier Inc. All rights reserved.	algebraic equation;algorithm;apache axis;bézier curve;computation;computational problem;convex hull;interpolation;polyhedron;polynomial;subdivision surface	K. H. Ko;K. Kim	2013	Applied Mathematics and Computation	10.1016/j.amc.2013.01.032	mathematical optimization;combinatorics;discrete mathematics;subdivision;mathematics	Robotics	69.81329115469451	-40.426380603148104	131328
e5f87dd02e719a0fd87728f99f6a9d8cb3eeb026	sparse time-frequency representation of gravitational-wave signals in unions of wilson bases		We investigate the question of obtaining a reduced time-frequency description of a chirp type signal that can be used as reference pattern in time-frequency searches. This is particularly relevant for searches of transient gravitational waves from astrophysical sources such as the mergers of neutron stars and/or black holes, the main area of this study. Sparse approximation algorithms that allow constraints on the approximation error do not perform well when the decomposition bases are redundant. This study puts in evidence some of the shortcomings of sparse approximation algorithms when dealing with unions of highly correlated bases, a case that currently lacks of a comprehensive mathematical analysis, and proposes solutions to mitigate them. We propose a variation of the matching pursuit algorithm that improves its robustness in the context of gravitational waves patterns construction. We also compare this algorithm to standard sparse approximation methods.	approximation algorithm;approximation error;base;chirp;matching pursuit;mathematics;neutrons;solutions;sparse approximation;sparse matrix;stars, celestial;time–frequency representation	Quentin Bammey;Philippe Bacon;Eric Chassande- Mottin;Aurélia Fraysse;Stéphane Jaffard	2018	2018 26th European Signal Processing Conference (EUSIPCO)	10.23919/EUSIPCO.2018.8553079		ML	80.30069377401612	-40.53147639343884	131338
da68de59deb9c864b3d688a619045201a7cbd18c	quality tetrahedral mesh smoothing via boundary-optimized delaunay triangulation	feature preserving;health research;uk clinical guidelines;biological patents;volume preserving;europe pubmed central;citation search;mesh quality;uk phd theses thesis;optimal delaunay triangulation;life sciences;tetrahedral mesh smoothing;uk research reports;medical journals;europe pmc;biomedical research;bioinformatics	"""Despite its great success in improving the quality of a tetrahedral mesh, the original optimal Delaunay triangulation (ODT) is designed to move only inner vertices and thus cannot handle input meshes containing """"bad"""" triangles on boundaries. In the current work, we present an integrated approach called boundary-optimized Delaunay triangulation (B-ODT) to smooth (improve) a tetrahedral mesh. In our method, both inner and boundary vertices are repositioned by analytically minimizing the error between a paraboloid function and its piecewise linear interpolation over the neighborhood of each vertex. In addition to the guaranteed volume-preserving property, the proposed algorithm can be readily adapted to preserve sharp features in the original mesh. A number of experiments are included to demonstrate the performance of our method."""	algorithm;bell test experiments;delaunay triangulation;experiment;interpolation imputation technique;linear interpolation;measure-preserving dynamical system;on-die termination;piecewise linear continuation;repositioning (procedure);smoothing (statistical technique);vertex (geometry)	Zhanheng Gao;Zeyun Yu;Michael J. Holst	2012	Computer aided geometric design	10.1016/j.cagd.2012.07.001	minimum-weight triangulation;medical research;delaunay triangulation;ruppert's algorithm;theoretical computer science;pitteway triangulation;point set triangulation;data mining;constrained delaunay triangulation;chew's second algorithm;surface triangulation;bowyer–watson algorithm	Visualization	68.51151232530661	-44.29025740945663	131531
d9f826910ba3b4ccab0694240fb2f533d415257e	detection of weak signals based on empirical mode decomposition and singular spectrum analysis	fault component;singular spectrum analysis;signal detection;characteristic component detection;gearbox;empirical mode decomposition	A novel method for the detection of weak signals embedded in non-stationary backgrounds is derived based on empirical mode decomposition and singular spectrum analysis in the present study. Simulated example reveals that the new method performs well in the detection of the characteristic components and especially the weak signals. Finally, the method is applied to the experimental signals of gearbox and the useful weak fault components can be exactly captured, which shows that the method presented in this study provides an effective means to the detection of weak signals.	hilbert–huang transform	Rui Ma;Yushu Chen;Huagang Sun	2013	IET Signal Processing	10.1049/iet-spr.2011.0215	electronic engineering;speech recognition;transmission;hilbert–huang transform;mathematics;singular spectrum analysis;statistics;detection theory	ML	79.24540177058454	-38.18464822430375	132311
0529d5d16d4fd7330bfb3780f4ea154449fc4354	genbrdf: discovering new analytic brdfs with genetic programming	isotropic;genetic programming;analytic;brdf	We present a framework for learning new analytic BRDF models through Genetic Programming that we call genBRDF. This approach to reflectance modeling can be seen as an extension of traditional methods that rely either on a phenomenological or empirical process. Our technique augments the human effort involved in deriving mathematical expressions that accurately characterize complex high-dimensional reflectance functions through a large-scale optimization. We present a number of analysis tools and data visualization techniques that are crucial to sifting through the large result sets produced by genBRDF in order to identify fruitful expressions. Additionally, we highlight several new models found by genBRDF that have not previously appeared in the BRDF literature. These new BRDF models are compact and more accurate than current state-of-the-art alternatives.	bidirectional reflectance distribution function;data visualization;genetic programming;mathematical optimization	Adam Brady;Jason Lawrence;Pieter Peers;Westley Weimer	2014	ACM Trans. Graph.	10.1145/2601097.2601193	genetic programming;mathematical optimization;computer science;artificial intelligence;theoretical computer science;machine learning;bidirectional reflectance distribution function;isotropy;quantum mechanics	Graphics	76.22820404096014	-47.25566962962557	132640
05acdc2dabfe8cfbf25f42b2ffad60d648107e3b	implant sprays: compression of progressive tetrahedral mesh connectivity	compression ratio;interpolation;mesh generation;computational geometry;data compression;compression	<italic>Irregular tetrahedral meshes, which are popular in many engineering and scientific applications, often contain a large number of vertices. A mesh of V vertices and T tetrahedra requires 48·V bits or less to store the vertex coordinates, 4·Vlog<subscrpt>2</subscrpt> bits to store the tetrahedra-vertex incidence relations, also called connectivity information, and k·V bits to store the k-bit value samples associated with the vertices. Given that T is 5 to 7 times larger than V and that V often exceeds 32<supscrpt>3</supscrpt>, the storage space required for the connectivity is larger than 300·V bits and thus dominates the overall storage cost. Our “implants spray” compression approach introduced in this paper reduces this cost to about 30·V bits or less — a 10:1 compression ratio. Furthermore, implant spray supports the progressive refinement of a crude model through a series of vertex-splits operations.</italic>	incidence matrix;progressive refinement;refinement (computing);vertex (geometry);vertex (graph theory)	Renato Pajarola;Jarek Rossignac;Andrzej Szymczak	1999	Proceedings Visualization '99 (Cat. No.99CB37067)		data compression;mesh generation;computational geometry;interpolation;computer science;theoretical computer science;compression ratio;mathematics;geometry;compression;statistics	Visualization	68.89076806506182	-50.16198708641866	132677
9bf18c7f2cda2fb729ab0232c4a316763f55ae7b	constructing g1 quadratic bézier curves with arbitrary endpoint tangent vectors	computers;design automation;biarcs;polynomials educational technology laboratories application software computer numerical control information systems computer security information security national security computer science education;computational geometry;geometric continuity;performance comparison;endpoint condition;arbitrary endpoint unit tangent vector;satisfiability;data mining;quadratic bezier curve;g 1 quadratic bezier curve;polynomials;computational geometry g 1 quadratic bezier curve arbitrary endpoint unit tangent vector composite geometric hermite curve biarcs;shape;smoothness;mathematical model;curve fitting;smoothness quadratic bezier curve geometric continuity endpoint condition;composite geometric hermite curve;curve fitting computational geometry;arbitrary units;bezier curve	Quadratic Bézier curves are important geometric entities in many applications. However, it was often ignored by the literature the fact that a single segment of a quadratic Bézier curve may fail to fit arbitrary endpoint unit tangent vectors. The purpose of this paper is to provide a solution to this problem, i.e., constructing G1 quadratic Bézier curves satisfying given endpoint (positions and arbitrary unit tangent vectors) conditions. Examples are given to illustrate the new solution and to perform comparison between the G1 quadratic Bézier cures and other curve schemes such as the composite geometric Hermite curves and the biarcs.	biarc;bézier curve;communication endpoint;cubic hermite spline;entity	He-Jin Gu;Jun-Hai Yong;Jean-Claude Paul;Fuhua Cheng	2009	2009 11th IEEE International Conference on Computer-Aided Design and Computer Graphics	10.1109/CADCG.2009.5246892	smoothness;mathematical optimization;tangent vector;discrete mathematics;arbitrary unit;electronic design automation;computational geometry;shape;bézier curve;mathematical model;mathematics;geometry;statistics;polynomial;curve fitting;satisfiability	EDA	70.05315016997507	-41.04854937871466	132926
fde3b0adba41805d1c28e28f2417eb690aa56fcf	visualizing directional stresses in a stress tensor field		Tensors represent a natural means for describing many physical phenomena. Therefore, there is a need to develop methods that will assist in analyzing such data. Despite some calls for generality, we concentrate on quite a specific application because general methods designed for visualizing information contained in second order tensor fields may fail to provide users with some of the features that could be of high importance for them. In the following paragraphs, our approach focused on second order tensor field datasets describing the state of stress in material will be discussed. It is based on the visualization of directional stresses using glyphs as well as surface extraction.		Tomás Jirka	2004		10.1007/1-4020-4179-9_69	computational physics;tensor field;tensor;generality;visualization;mathematics;cauchy stress tensor	HCI	75.9836728641905	-46.92140716058431	132960
01ffad2b724e4052e0cd9d54da683a7fd4b178c5	a set of features for classification of intrapulse modulations	histograms;radar signal to noise ratio histograms feature extraction frequency modulation;frequency modulation;entropy electronic warfare intrapulse modulation amplitude histograms statistical moments;signal classification electronic warfare feature extraction pulse compression pulse modulation radar resolution;feature extraction;signal to noise ratio;radar signals intrapulse modulations electronic warfare environments radar emitters intrapulse parameters pulse repetition interval modulation pulse compression techniques range resolution automatic recognition feature extraction amplitude histograms;radar	In modern electronic warfare environments radar emitters intentionally modify their pulse repetition intervals and intra-pulse parameters in specific patterns called modulation. While pulse repetition interval modulation serves for a hidden functional purpose of the emitter, intra-pulse modulation also known as pulse compression techniques makes it possible to simultaneously maximize the target range and the range resolution of a radar. In this study, new features due to automatic recognition of intra-pulse modulations are proposed. These features are extracted from the amplitude histograms of radar signals. Experimental work shows that common modulation types can be highly separated by the utilization of the proposed feature set.	lambert's cosine law;pulse compression;pulse-width modulation;radar	Kenan Gencol	2016	2016 24th Signal Processing and Communication Application Conference (SIU)	10.1109/SIU.2016.7496189	computer vision;pulse compression;electronic engineering;speech recognition;engineering;pulse repetition frequency;pulse-doppler radar	Robotics	82.0148674477978	-40.91008142522437	133067
8217f03b097923e6a63edd34344706890e697f55	leveraging feature generalization and decomposition to compute a well-connected midsurface		Computer-aided design (CAD) models of thin-walled parts, such as sheet metal or plastic parts, are often represented by their corresponding midsurfaces for computer-aided engineering (CAE) analysis. The reason being, 2D surface elements, such as “shell” elements, which need to be placed on the midsurface, provide fairly accurate results, while requiring far lesser computational resources time compared to the analysis using 3D solid elements. Existing approachesof midsurface computation are not reliable and robust. They result in ill-connected midsurfaces having missing patches, gaps, overlaps, etc. These errors need to be corrected, mostly by a manual and time-consuming process, requiring from hours to even days. Thus, an automatic and robust technique for computation of a well-connected midsurface is the need of the hour. This paper proposes an approachwhich, instead of working on the complex final solid shape, typically represented by B-rep (boundary representation), leverages feature information available in the modern CAD models for techniques such as defeaturing, generalization, and decomposition. Here, first, the model is defeaturedby removing irrelevant features, generating a simplified shape called “gross shape”. The remaining features are then generalizedto their corresponding generic loft-feature equivalents. The model is then decomposed into sub-volumes, called “cells” having respective owner loft features. A graph is populated, with the cells at the graph nodes. The nodes are classified into midsurface patch-generating nodes (called ‘solid cells’ or sCells) and interaction-resolving nodes (called ‘interface cells’ or iCells). Using owner loft feature’s parameters, sCells compute their own midsurface patches. Using a generic logic, the patches then get connected appropriately in the iCells, resulting in a well-connected midsurface. The efficacy of the approach is demonstrated by computing well-connected midsurfaces of various real-life sheet metal parts.	algorithm;battle of midway;boundary representation;computation;computational resource;computer-aided design;corner case;hard coding;mike lesser;norm (social);population;real life;relevance	Yogesh H. Kulkarni;Anil Sahasrabudhe;Mukund Kale	2016	Engineering with Computers	10.1007/s00366-016-0466-x	mathematical optimization;simulation;computer science;engineering;artificial intelligence;machine learning;database;mathematics;geometry;engineering drawing;algorithm;statistics;mechanical engineering	AI	72.5646300154726	-43.45344569988465	133480
2a2c1e8fb5322cd907c3678aea9c776ed27b8640	procedural authoring of solid models	computer graphic;electrical engineering and computer science;thesis;surface model;solid modeling	This thesis investigates the creation, representation, and manipulation of volumetric geometry suitable for computer graphics applications. In order to capture and reproduce the appearance and behavior of many objects, it is necessary to model the internal structures and materials, and how they change over time. However, producing real-world effects with standard surface modeling techniques can be extremely challenging. My key contribution is a concise procedural approach for authoring layered, solid models. Using a simple scripting language, a complete volumetric representation of an object, including its internal structure, can be created from one or more input surfaces, such as scanned polygonal meshes, CAD models or implicit surfaces. Furthermore, the resulting model can be easily modified using sculpting and simulation tools, such as the Finite Element Method or particle systems, which are embedded as operators in the language. Simulation is treated as a modeling tool rather than merely a device for animation, which provides a novel level of abstraction for interacting with simulation environments. I present an implementation of the language using a flexible tetrahedral representation, which I chose because of its advantages for simulation tasks. The language and implementation are demonstrated on a variety of complex examples that were inspired by real-world objects. Thesis Supervisor: Julie Dorsey Title: Professor of Computer Science, Yale University Thesis Supervisor: Leonard McMillan Title: Professor of Computer Science, University of North Carolina, Chapel Hill		Barbara Cutler	2003			simulation;computer science;computer graphics (images)	Graphics	69.96821363870981	-47.80356163101528	133824
964deb435a054087b9f9901239c60663a58b51b8	kurtosis-based constrained independent component analysis and its application on source contribution quantitative estimation	vibrations;vibrations noise integrated circuits vibration measurement noise measurement estimation algorithm design and analysis;noise measurement;estimation;vibration measurement;source contribution constraint independent component analysis cica kurtosis reference signal;integrated circuits;algorithm design and analysis;noise	Aiming at finding the major vibration and noise sources of vehicles, a quantitative estimation method for source contribution using the kurtosis-based constrained independent component analysis (cICA) algorithm is proposed. First, the similarity between the ICs and the reference signals with given characteristics is described by a concise and effective closeness measurement function. Meanwhile, how to choose the reference signals and the choice of some other closeness measurements is discussed. Then, a widely used contrast function, namely, kurtosis, is modified by the closeness measurement to obtain an enhanced contrast function. The fixed-point iteration and deflation approach are employed to train the separating matrix. Then, the enhanced contrast function is therefore maximized and the kurtosis-based cICA algorithm is obtained. After that, the source contribution is quantitatively calculated by the reduced energy of the mixed signals in each extraction: the reduction of the energy in mixed signals corresponds to the contribution of the extracted IC. The correspondence relationship between the ICs and source signals can be obtained by prior knowledge. Finally, the effectiveness of the proposed algorithm is verified by numerical simulation and experiments. The results show that the proposed method has high accuracy in separating sources and quantitatively calculating the source contribution.	algorithm;centrality;circuit restoration;computer simulation;experiment;fixed-point iteration;independent computing architecture;independent component analysis;newton's method;test bench	Jie Zhang;Zhousuo Zhang;Wei Cheng;Xiang Li;Binqiang Chen;Zhibo Yang;Zhengjia He	2014	IEEE Transactions on Instrumentation and Measurement	10.1109/TIM.2013.2293236	algorithm design;econometrics;estimation;electronic engineering;noise measurement;noise;vibration;mathematics;physics;statistics	Metrics	80.61146726106294	-39.49563250301038	134147
7ea100477348cf21013bec9240abd2289a6074fb	an accurate error measure for adaptive subdivision surfaces	subdivision surface;surface fitting;linear approximation;error analysis;mesh accurate error measure adaptive subdivision surface tight estimate linear approximation;mesh generation;face linear approximation spline sampling methods testing shape control mesh generation power generation graphics yield estimation;error analysis mesh generation surface fitting	A tight estimate on the maximum distance between a subdivision surface and its linear approximation is introduced to guide adaptive subdivision with guaranteed accuracy.	algorithm;catmull–clark subdivision surface;computation;interference (communication);linear approximation;pov-ray;ray tracing (graphics);web search engine	Xiaobin Wu;Jörg Peters	2005	International Conference on Shape Modeling and Applications 2005 (SMI' 05)	10.1109/SMI.2005.4	mathematical optimization;approximation error;mathematics;geometry;subdivision surface;statistics	Robotics	70.14926754722104	-43.356333570106	134313
0ec8fd799d34b4cdd114505a10d002575b876138	the calculation of target poles with wavelet transform for electromagnetic target discrimination	electromagnetic scattering;oscillations;object recognition;antennas wavelet transforms radar resonant frequency electromagnetic scattering;morlet wavelet transform electromagnetic target discrimination target poles extraction target recognition mention method;wavelet transforms;wavelet transform;target recognition;resonant frequency;antennas;radar;wavelet transforms object recognition	In this paper, a method oriented to the extraction of target poles is presented for target recognition purpose. By applying wavelet transform to whole signal, the mention method obtains target poles with a map extracted in time and frequency axes simultaneously. For this purpose, with complex Morlet wavelet transform, a map having sufficient resolutions in both axes is obtained by multiplying two maps, one using narrowband and another using wideband mother wavelet. The peaks in the frequency axis of the resulting map give the oscillation coefficients of the poles, and the attenuations in time axis give the damping coefficients of the poles. In this study, the method is initially applied for a dielectric sphere whose pole values can be analytically calculated and it is observed that the calculated pole results are highly consistent with theoretical values especially with respect to oscillation coefficients. Afterwards, for target discrimination purpose, the method is applied to two small-scale airplane targets whose pole values are theoretically unknown and it is shown that although the dimensions of airplanes are very close, their pole values are considerably different.	apache axis;coefficient;map;morlet wavelet;optic axis of a crystal;wavelet transform	Mustafa Secmen;Nalan Özkurt	2012	2012 20th Signal Processing and Communications Applications Conference (SIU)	10.1109/SIU.2012.6204498	wavelet;computer vision;constant q transform;harmonic wavelet transform;second-generation wavelet transform;continuous wavelet transform;cascade algorithm;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;lifting scheme;wavelet transform	Robotics	82.21890662230547	-41.081736030260544	134556
98a0e84793d89866f184203487405d72a8183dd6	optimal multi-degree reduction of bézier curves with constraints of endpoints continuity	metodo cuadrado menor;concepcion asistida;methode moindre carre;computer aided design;degree reduction;curva bezier;modele geometrique;least squares method;approximation error;ajustamiento curva;numerical method;approximation method;least squares approximation;endpoint continuity;optimal approximation;metodo numerico;courbe bezier;conception assistee;ajustement courbe;curve fitting;reduction method;methode numerique;geometrical model;bezier curve;modelo geometrico	Given a Bézier curve of degree n, the problem of optimal multi-degree reduction (degree reduction of more than one degree) by a Bézier curve of degree m (m< n−1)with constraints of endpoints continuity is investigated. With respect toL2 norm, this paper presents one approximate method (MDR by L2) that gives an explicit solution to deal with it. The method has good properties of endpoints interpolation: continuity of any r, s (r, s 0) orders can be preserved at two endpoints respectively. The method in the paper performs multi-degree reduction at one time and does not need the stepwise computing. When applied to the multi-degree reduction with endpoints continuity of any orders, the MDR byL2 obtains the best least squares approximation. Comparison with another method of multi-degree reduction (MDR by L∞), which achieves the nearly best uniform approximation with respect to L∞ norm, is also given. The approximate effect of the MDR by L2 is better than that of the MDR by L∞. Explicit approximate error analysis of the multi-degree reduction methods is presented.  2002 Published by Elsevier Science B.V.	approximation algorithm;basis function;bézier curve;error analysis (mathematics);error-tolerant design;jacobi method;least squares;memory data register;polynomial basis;scott continuity;stepwise regression;subdivision surface;whittaker–shannon interpolation formula	Guodong Chen;Guo-Jin Wang	2002	Computer Aided Geometric Design	10.1016/S0167-8396(02)00093-6	mathematical optimization;computer aided design;calculus;mathematics;geometry;least squares;statistics	AI	69.06382952033073	-39.9288700335438	134559
610b11525ee6d4a4d18f07e7547dedf58a057603	a new model for the recovery of cylindrical structures from medical image data	second order;blood vessel;finite element;medical image;cross section;surface area;analytical model	We introduce a novel analytic model formulation for recovering cylindrical structures (e.g., blood vessels) from segmented 3-D medical image data. Unlike all previous formulations, our model is capable of describing a cylinder with an arbitrary spine (a space curve based on cubic B-splines) and arbitrary cross section which is guaranteed to be orthogonal to the spine. Given this expressiveness, we are able to provide a second order continuous approximation to the centerline of nearly any tubular object. This information may be used for such tasks as a reformatting of the original image data in order to visually detect stenoses or aneurysms. In addition, the cross-section parameter values of our model may aid in automatically isolating these regions. We maintain a relatively simple cross-section function to make this detection straightforward (note that any cross-section function is possible). To describe ne detail in the data, we employ local nite element deformations from the model surface. Thus we are able to recover gross geometric approximations as well as quantify characteristics of the object such as its surface area. We apply our model to the recovery of both a healthy and diseased aorta from segmented CT acquisitions.	approximation;b-spline;cross section (geometry);cubic function;cylinder seal;glossary of computer graphics	Thomas O'Donnell;Alok Gupta;Terrance E. Boult	1997		10.1007/BFb0029241	structural engineering;engineering;biological engineering;engineering drawing	ML	69.88251467165361	-45.587380183644136	134657
1190e8df3a1b01aafccf0c444826f00850753390	gpu-based inverse rendering with multi-objective particle swarm optimization	paper;disease simulation;glsl;visualization;in situ visualization;exascale;particle swarm optimization;parallelization;algorithms;computer science;rendering	"""We present a novel, GPU-accelerated per-pixel inverse rendering (IR) optimization algorithm based on Particle Swarm Optimization (PSO), IRPSO. IRPSO estimates the per-pixel scene attributes including reflectance properties of a 3D model, and is fast enough to do in situ visualization of the optimization in real-time. We utilize the GPU framebuffer as a computational domain, where each pixel is treated as an independent computational """"swarm"""". We propose a parallel framework that recovers the reflectance at each pixel location in a massively parallel fashion. The algorithm's high parallel efficiency is demonstrated through our GPU/GLSL shader implementation of the method. IRPSO is validated experimentally on examples including simulated ground truth images."""	algorithm;experiment;framebuffer;graphics processing unit;ground truth;mathematical optimization;opengl shading language;particle swarm optimization;pixel;real-time clock;shader;speedup	Koki Nagano;Thomas Joseph Collins;Chi-An Chen;Aiichiro Nakano	2015		10.1145/2818517.2818523	simulation;visualization;rendering;computer science;theoretical computer science;parallel rendering;particle swarm optimization;computer graphics (images)	Visualization	69.46939531519718	-50.5993741557492	134965
acfbe4320a13e60e0c079e65e64ee24bf8f0ec88	gpu-based dynamic flow visualization for climate research applications	time dependent;climate model;interactive visualization;fluid flow;visualization technique;data visualization;graphic processing unit;interactive data exploration;flow visualization;data grid;free software	Climate models simulate the most important processes of the Earth System, including the circulation of the atmosphere and the ocean. For the the visualization of the resulting large time dependent data sets, many different techniques are used. In order to analyze the fluid flow, interactive flow visualization techniques are important. Various commercial and free software packages used for climate data visualization have rather limited support for the interactive exploration of large flow data sets. For example, some of them are designed to generate pre-calculated animations only. On the other hand, recent developments in visualization techniques exploiting programmable features of current graphics processing unit (GPU) have proven to be very powerful. This is especially true for particle-based techniques. However, there is still a functional gap between these particle-based flow visualization techniques utilizing GPU processing power and the requirements in the context of geophysical fluid flows, which is the focus of this paper. We present a complete GPU-based framework for interactive visualization of timedependent climate flow data. The framework includes a proper data work flow from the simulation to the visualization, the handling of non-uniform data grids and, finally, a proper support for the interactive data-exploration feasible to climate researchers. We demonstrate the framework using the flow data of a simulation of a typhoon. This work is still in progress and the framework’s design is open for future incorporation of more particle-based visualization techniques. ∗{nicolas.cuntz,andreas.kolb,christof.rezk}@uni-siegen.de †leidl@tk.informatik.tu-darmstadt.de ‡boettinger@dkrz.de Figure 1: Interactive, particle-based flow visualization of a typhoon (Data courtesy of MaxPlanck-Institute for Meteorology), incorporating shadows casted onto a height map of the respective geographical region. The left image shows a color-coding of flow magnitude, the right image uses additive blending in order to obtain smooth visual effects.	alpha compositing;blend modes;climate model;color-coding;computer graphics;data visualization;earth system science;graphics processing unit;heightmap;interactive visualization;particle filter;requirement;simulation;typhoon;utility functions on indivisible goods;visual effects	Nicolas Cuntz;Andreas Kolb;Martin Leidl;Christof Rezk-Salama;Michael Böttinger	2007			software visualization;scientific visualization;simulation;information visualization;flow visualization;computer science;theoretical computer science;computer graphics (images)	Visualization	71.933226230993	-51.07527269526535	136197
e0d3caa65c980421bac22c23f512748263413602	double-step generation of ellipses	iterative method;curva;concepcion asistida;computer aided design;modele geometrique;personal communication networks;technological innovation;incrementation;application software;computer graphics;ellipse;discrete plane;computational geometry;courbe;curve;methode calcul;metodo iterativo;metodo calculo;calculating method;iterative methods;double step incremental generation;shape;methode iterative;displays;workstations;ellipses;double step algorithm;scan conversion computer graphics computational geometry iterative methods ellipses double step incremental generation nonparametric curves discrete plane double step algorithm;animation;nonparametric curves;conception assistee;arithmetic;curve fitting;iterative methods computational geometry computer graphics curve fitting;scan conversion;pathology;computer graphics shape application software displays pathology animation personal communication networks workstations technological innovation arithmetic;geometrical model;modelo geometrico	The principle of double-step incremental generation of nonparametric curves on a discrete plane is used to develop a double-step algorithm for scan converting ellipses in sixteen separate segments. The algorithm iterates only half as many times as current algorithms, while each iteration demands the same number of operations or slightly fewer operations than M. Pitteway's (1967, 1985) algorithm (previously the fastest one). This marked gain in the speed of scan conversion is due to the fact that the middle pixel in a two-step increment is obtained without computation.<<ETX>>	algorithm;fastest;iteration;pixel;scan conversion	Xiaolin Wu;Jon G. Rokne	1989	IEEE Computer Graphics and Applications	10.1109/38.28111	simulation;computational geometry;computer science;artificial intelligence;mathematics;geometry;iterative method;algorithm;ellipse;statistics;computer graphics (images)	Visualization	68.43404316595043	-48.92229502286364	136410
41a25abb4c7fafec1ddb1907356f26cfd298e904	triangular springs for modeling nonlinear membranes	finite element methods;elasticity;membrane;physics based modeling;tensile strain;st venant kirchhoff;computer graphics;finite element discretization;young s modulus elasticity finite element analysis poisson ratio springs mechanical;one dimensional elasticity;two dimensional elasticity;triangular mesh;deformable models;indexing terms;finite element;biomembranes;cloth simulation;springs mechanical;springs;computational modeling;stretching energy;tensile biquadratic springs;young s modulus;isotropic deformations;trbs;spring mass;springs biomembranes computational modeling deformable models elasticity computer graphics computer simulation finite element methods tensile strain mesh generation;animation;animation animation physically based modeling;planar curves;trqs;unstructured mesh;poisson ratio;finite element analysis;poisson ratios continuum triangular springs nonlinear membranes continuum mechanics two dimensional elasticity one dimensional elasticity finite element discretization stretching energy planar curves tensile biquadratic springs isotropic deformations young modulus;nonlinear membranes;poisson ratios;continuum triangular springs;mesh generation;biquadratic springs;computer simulation;continuum mechanics;physically based modeling;young modulus	This paper provides a formal connection between springs and continuum mechanics in the context of one-dimensional and two-dimensional elasticity. In the first stage, the equivalence between tensile springs and the finite element discretization of stretching energy of planar curves is established. Furthermore, when the strain is a quadratic function of stretch, this energy can be described with a new type of springs called tensile biquadratic springs. In the second stage, we extend this equivalence to nonlinear membranes (St Venant-Kirchhoff materials) on triangular meshes leading to triangular biquadratic and quadratic springs. Those tensile and angular springs produce isotropic deformations parameterized by Young modulus and Poisson ratios on unstructured meshes in an efficient and simple way. For a specific choice of the Poisson ratio, 1/3, we show that regular spring-mass models may be used realistically to simulate a membrane behavior. Finally, the different spring formulations are tested in pure traction and cloth simulation experiments.	angularjs;appendix;approximation algorithm;basic;bloc1s3 gene;choice behavior;circumscribe (action);cloth modeling;compression;computation;computer graphics;diabetes insipidus;discretization;elastic modulus;elasticity (data store);emoticon;experiment;finite element method;geometry processing;graph - visual representation;heron;hexahedron;isotropic position;kirchhoff's theorem;map3k8 protein, human;mechanics;modulus of continuity;natural springs;new type;nonlinear system;normal (geometry);polygon mesh;quadratic function;quartic function;rp (complexity);relative change and difference;short interspersed nucleotide elements;simulation;stage level 1;stage level 2;sudden infant death syndrome;tissue membrane;traction teampage;triangle mesh;triangulated irregular network;triune continuum paradigm;turing completeness;vertex;xfig;septation initiation signaling	Hervé Delingette	2008	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2007.70431	computer simulation;computer science;finite element method;calculus;geometry	Visualization	70.82676490670892	-47.2600833355083	136871
87ea0a028f024786c20c0862fa2ed3e89ac005f2	a combined ray tracing method for improving the precision of the usbl positioning system in smart ocean	acoustic positioning;constant sound speed ray tracing;equal gradient ray tracing;sound speed profile;ultra-short baseline (usbl)	The ultra-short baseline positioning system (USBL) has the advantages of flexible application and easy installation, and it plays an extremely important role in the underwater positioning and communication. The error of the USBL in underwater positioning is mainly caused by a ranging error due to ray tracing, a phase difference error of the USBL, and acoustic noise in the underwater communication. Most of these errors are related to the changes in the sound speed during its propagation through the ocean. Therefore, when using the USBL for underwater detection, it is necessary to correct the sound speed profile in the detection area and optimize the ray tracing. Taking into account the actual conditions, this paper aims at correcting the model of underwater sound speed propagation and improving the tracking method of sound lines when the marine environment in the shallow sea area changes. This paper proposes a combined ray tracing method that can adaptively determine whether to use the constant sound speed ray tracing method or the equal gradient ray tracing method. The theoretical analysis and simulation results show that the proposed method can effectively reduce the error of slant distance in USBL compared with the traditional acoustic tracking method and the constant sound speed ray tracing method. The proposed sound ray correction algorithm solves the contradiction between the number of iterations and the reduction of positioning error and has engineering application value.	acoustic cryptanalysis;algorithm;baseline (configuration management);covox speech thing;decompression sickness;end-of-file;experiment;gradient;iteration;natural science disciplines;positioning system;ray tracing (graphics);real-time transcription;simulation;software propagation;stationary process;transponder device component;velocity (software development);sensor (device)	Jian Li;Qi Gu;Ying Chen;Guiqing Sun;Haocai Huang	2018		10.3390/s18103586	engineering;positioning system;electronic engineering;ray tracing (graphics)	Robotics	79.63750304814866	-45.81459478742471	137548
213aeb4bdab7009908dab7c6b681075e6c2ee314	filtering in the time-frequency domain for the detection of compact objects	gaussian noise;wigner distribution;correlation methods;filtering theory;signal detection;wigner-ville transform;compact object detection;cross-term reduction;local maxima identification;signal enhancement;source detection;time-frequency wigner-ville distribution;time-frequency data distribution;time-frequency domain filtering;time-frequency profile;two-dimensional correlator filter	In this work we propose a detection method for compact sources in the Time-Frequency domain. By exploring the joint Time-Frequency distribution of data and by calculating the time-frequency profile of the compact sources it is possible to improve the detectability of faint point sources. We propose the detection of sources by identification of local maxima (peaks) in the time-frequency Wigner-Ville distribution of the data, previously filtered with a two-dimensional correlator filter. The filter serves the double purpose of enhancing the signal and of reducing cross-terms derived from the Wigner-Ville transform. We test our method and compare it with a the case of the detection in the time domain (filtered with the appropriate one-dimensional correlator), for a situation in which we have faint sources embedded in stationary white Gaussian noise. The detection in the time-frequency domain gives us better significance levels of faint sources (signal-to-noise ratio ~ 1) than the detection in just the time domain.	cross-correlation;embedded system;fast fourier transform;maxima and minima;signal-to-noise ratio;stationary process;time–frequency analysis;time–frequency representation;wigner quasiprobability distribution	Diego Herranz;José Luis Sanz;Ercan Engin Kuruoglu	2009	2009 17th European Signal Processing Conference		gaussian noise;electronic engineering;mathematics;optics;statistics	Vision	81.73212994049119	-40.64440014104402	137655
7eebc2639634614b4e829862a226368107229f87	local deformations of digital curves	topology preservation	"""A new concept of a """"digitally continuous"""" deformation of a digital curve is introduced. Such a deformation, in general, need not be topology-preserving; but a deformation under which the regions inside and outside the curve remain disjoint is digitally continuous and does preserve topology. © 1997 Elsevier Science B.V."""		Azriel Rosenfeld;Akira Nakamura	1997	Pattern Recognition Letters	10.1016/S0167-8655(97)00038-X	discrete mathematics;topology;computer science;mathematics;geometry	Vision	69.24285820267804	-41.63792861540626	138086
0fe300326d050647253b32a48aed122b58242d47	a new nonparametric method for testing stationarity based on trend analysis in the time marginal distribution	trends stationarity test time frequency analysis time marginals empirical mode decomposition;time frequency analysis signal detection signal representation;market research time frequency analysis testing time series analysis signal processing speech indexes;signal detection time marginal distribution nonparametric test empirical mode decomposition time frequency representation slowly varying nonstationarity detection	In this manuscript, we propose a novel nonparametric test for nonstationarities that are seen as a trend or an evolution in the local energy of the signal. The idea of the proposed technique consists in applying empirical mode decomposition for estimating and further quantifying the trend in the time marginal of the estimated time-frequency representation. Such methodology allows for the detection of slowly-varying nonstationarities of first and second-order.	hilbert–huang transform;marginal model;stationary process;time–frequency representation	Douglas David Baptista de Souza;Jocelyn Chanussot;Anne-Catherine Favre;Pierre Borgnat	2014	2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2014.6853610	econometrics;speech recognition;time domain;computer science;singular spectrum analysis;statistics	Robotics	79.188957378951	-39.412586622236425	138376
f90f75db8439166da034605154a5524cb0f40550	real-time simulation of granular materials using graphics hardware	paper;fluids;particle based simulation;gpu distinct element method;computer graphic equipment;gpu;real time simulation;physics computing;force;acceleration;cuda;distinct element method;distance measurement;graphics processing unit real time simulation granular material graphics hardware particle based simulation distinct element method;computational modeling;graphics hardware;real time graphics;particle simulation;real time systems computer graphic equipment digital simulation friction granular materials physics computing;computational modeling force friction fluids distance measurement acceleration graphics;nvidia;nvidia geforce 8800 ultra;computer science;granular material;graphics processing unit;granular materials;friction;data structure;graphics;digital simulation;real time systems	We present a method to compute friction in a particle-based simulation of granular materials on GPUs and its data structure. We use Distinct Element Method to compute the force between particles. There has been a method to accelerate Distinct Element Method using GPUs, but the method does not compute friction. We implemented friction into the DEM simulation on GPUs and this leads to the real-time simulation of granular materials.	data structure;graphics hardware;graphics processing unit;real-time clock;simulation	Ren Yasuda;Takahiro Harada;Yoichiro Kawaguchi	2008	2008 Fifth International Conference on Computer Graphics, Imaging and Visualisation	10.1109/CGIV.2008.45	computational science;computer hardware;computer science;computer graphics (images)	Visualization	71.42891292802501	-48.213921413714154	138595
8452f0c97809c638c88050275e540e016eaa07d2	comparative visualization of two-dimensional flow data using moment invariants	time dependent;structural change;flow field;wind speed;similarity measure;moment invariant	The analysis of time-dependent data is often guided by the question of how dominant structures develop over time. It is important to understand how patterns or structures identified for one time step evolve over time, by changing or moving in the domain. To gain insight into such evolving structural change it is crucial to effectively compare different time steps. This paper proposes a comparison method for twodimensional flow fields. The method is based on a feature description using invariant moments. The specific strength of these moments is their invariance under scaling and rotation, thus facilitating an identification of features even if they occur at other positions, with changed orientation, and variation in size. In addition the moments themselves can be used to define a similarity measure. To evaluate the significance of this concept it has been applied to wind speed data from meteorological simulations.	computer graphics;don sannella;hurricane weather research and forecasting model;ibm notes;image moment;image scaling;similarity measure;simulation	Michael Schlemmer;Ingrid Hotz;Bernd Hamann;Hans Hagen	2009			wind speed;simulation;structural change;data mining;mathematics;statistics	Visualization	75.96089294958706	-45.62180609189911	138803
b7e67455f99d8107db55253573b334c18a421aab	filling holes with shape preserving conditions	shape preserving;filling;approximation;minimal energy;powell sabin finite element	Recently, several techniques have been developed to fill polygonal holes in a given surface by using C 1 -spline patches. Such techniques are based on the minimization of an energy functional which controls the fairness of the patch as well as its closeness to the original surface where it is known, that is, outside the hole. Nevertheless, the filling patch obtained tends to be flat due to the definition of the energy functional, so the used technique does not work properly in certain cases. Here we propose to generalize the filling method previously developed in other works in order to fill holes with some 'shape' conditions, i.e., in such a way that the filling patch 'inherits' as much as possible the shape of the original surface where it is known.		Miguel A. Fortes;Pedro González;Antonio Palomares;Miguel Pasadas	2015	Mathematics and Computers in Simulation	10.1016/j.matcom.2014.12.008	topology;approximation;mathematics;geometry	Vision	69.23439393175813	-42.22679282900362	139558
29ab141e0cf496bd72b49b2afc8e9dd8d88366c2	point-based rigid shape interpolation	finite element method analysis;strain measurement;user interface;shape interpolation;force	Nearly rigid interpolation has been shown to generate good interpolation results for both 2D and 3D shape morphing problems [Alexa et al. 2000]. The key to success of most proposed techniques like that of Alexa, et al. is generating a high-quality compatible triangulations of the source and target shapes in 2D and compatible tetrahedralizations in 3D. A number of good methods are available for the 2D case, however, it is more difficult in 3D. The 3D rigid morphing example demonstrated by Alexa et al. [2000] has, by construction, a fairly trivial compatible tetrahedralization; however, for real-world cases, where the inputs are typically 3D meshes without a priori tetrahedralizations, the task is much more difficult.	3d printing;interpolation;morphing	William Baxter	2006		10.1145/1179622.1179728	mathematical optimization;bilinear interpolation;topology;computer science;operating system;bicubic interpolation;geometry;user interface;force;quantum mechanics;trilinear interpolation	Graphics	68.37800435792647	-45.19286069095257	139807
91f6fbe3718c85f0ecc989937c3e2dea561abac3	improved blind source separation method based on independent component analysis and empirical mode decomposition	probability;probability density;source signal blind source separation independent component analysis empirical mode decomposition signal processing probability density time frequency feature;blind source separation;time frequency;independent component analysis;probability density blind source separation independent component analysis empirical mode decomposition non stationary processes;signal processing;blind source separation algorithm design and analysis independent component analysis signal processing algorithms sensors educational institutions;time frequency analysis blind source separation independent component analysis medical signal processing probability;time frequency analysis;algorithm design;medical signal processing;stationary process;empirical mode decomposition	Blind source separation (BSS) has recently received a great deal of attention in signal processing. In order to improve the limited separation performance of the conventional BSS method by the influence of the probability density of the source signal, based on the Independent Component Analysis and Empirical Mode Decomposition theories, an improved blind separation method is proposed. The method is demonstrated by some examples. Simulation results show the improved separation performance of the proposed method, and the time-frequency feature of the source signal has a better reflection.		Jie Zhang;Jianhui Lin	2011		10.1109/BMEI.2011.6098718	independent component analysis;speech recognition;time–frequency analysis;computer science;blind equalization;signal processing;pattern recognition;mathematics;blind signal separation;statistics	SE	81.89329807330718	-38.13757312708139	140709
122a74058e559bd30f2609e4ce1c8697bcac98a1	surface deformation with differential geometric structures	constrained optimization;differential geometric structure;mean curvature vector;laplacian;deformation energy;energy use;surface deformation;smooth connection;mean curvature;geometric structure	This paper considers the deformation of a given surface to a surface that smoothly connects to previously designed surfaces while reflecting the overall shape of the initial surface. We introduce deformation energy using a Laplacian-based functional, which is defined by the global differential geometric structures of the initial surface. It is shown that the proposed deformation energy does not depend on representations of the initial surface, and relates to the mean curvature vector, a geometric quantity correlated to overall surface shape, and also has a good computational property. An example is presented to demonstrate the effectiveness of our method.		Masahiro Kimura;Takafumi Saito;Mikio Shinya	1996	Computer Aided Geometric Design	10.1016/0167-8396(95)00025-9	constrained optimization;laplace operator;mathematical analysis;topology;mean curvature flow;mean curvature;mathematics;geometry;geometric shape	Theory	70.38039278332631	-42.650817064575456	141063
ba53eb300e87eddcc9415cfea4e78d28d1c5e726	flow visualization based on a derived rotation field		We identify and investigate the Φ field – a derived flow attribute field whose value at a given spatial location is determined by the integral curve initiated at the point. Specifically, we integrate the angle difference between the velocity vectors at two consecutive points along the integral curve to get the Φ field value. Important properties of the Φ field and its gradient magnitude |∇Φ| field are studied. In particular, we show that the patterns in the derived Φ field are generally aligned with the flow direction based on an inequality property. In addition, we compare the Φ field with some other attribute fields and discuss its relation with a number of flow features, such as LCS and cusp-like seeding structures. Furthermore, we introduce a unified framework for the computation of the Φ field and its gradient field, ∇Φ, and employ the Φ field and |∇Φ| field to a number of flow visualization and exploration tasks, including integral curve filtering, seeds generation and flow domain segmentation. We show that these tasks can be conducted more efficiently based on the information encoded in the Φ field.	apache axis;computation;gradient;information theory;numerical analysis;numerical integration;social inequality;synthetic data;unified framework;velocity (software development)	Guoning Chen;Robert S. Laramee;David Thompson;Adrian Sescu	2016			computer vision;mathematics;artificial intelligence;flow visualization	Visualization	75.83461328278311	-45.51394688450216	143561
c849af014175e3c6ad3ce4193e1e586dbe1ee5e2	a positivity-preserving pyramid scheme for anisotropic diffusion problems on general hexahedral meshes with nonplanar cell faces		Abstract In this paper, a new cell-centered positivity-preserving pyramid scheme (called P 3 -scheme) is proposed for anisotropic diffusion problems. It can be regarded as a development of the O -scheme [31] for general hexahedral meshes with nonplanar cell-faces. In the P 3 -scheme, the flux on the nonplanar cell-face is approximated by the so called effective directional flux. Compared with the O -scheme, the P 3 -scheme is much more robust with respect to the distortion of the meshes, and has lower cost in computation and storage. Being different from the P -scheme [32] , the P 3 -scheme is positivity-preserving and can be applied to anisotropic diffusion problems. Numerical results are presented to show the performance of P 3 -scheme on various kinds of distorted meshes for problems with continuous and discontinuous diffusion coefficients.	anisotropic diffusion;hexahedron	Shuai Wang;Xudeng Hang;Guangwei Yuan	2018	J. Comput. Physics	10.1016/j.jcp.2018.05.026	mathematical optimization;flux;hexahedron;mathematics;pyramid;anisotropic diffusion;polygon mesh;distortion	Theory	71.91627339831655	-43.43119307433413	144009
56ca6ddd7d31d2cb0912380f570b0eee1da84238	curvature-based adaptive remeshing for wavelet-based multiresolution 3d meshes	image sampling;wavelet analysis;image sampling adaptive signal processing wavelet transforms image resolution;image resolution;geometry laboratories telematics wavelet analysis sampling methods error correction programmable control adaptive control computer errors solid modeling;wavelet transforms;adaptive signal processing;subdivision scheme;connectivity optimization curvature based adaptive remeshing wavelet based multiresolution 3d meshes	Most of the meshes coming from a variety of source are over sampled and exhibit highly irregular connectivity that prevent efficient wavelet analysis. Remeshing comes as a solution to sampling and connectivity optimization with respect to the geometry. The main drawback of existing subdivision remeshing approaches is the lack of control either on the number of triangles or on the geometrical error. We propose a new remeshing scheme that directly produces wavelet-based multiresolution meshes which does not suffer from the previous limitations. We provide a finer control on the resulting number of triangles by using an adaptive subdivision scheme. We also show a lowering of the local resulting geometrical error by using a new curvature-based subdivision criterion.	3d computer graphics;computer graphics (computer science);mathematical optimization;sampling (signal processing);subdivision surface;wavelet	Alexandre Gouaillard;Arnaud Gelas;Sébastien Valette;Eric Boix;Rémy Prost	2005	IEEE International Conference on Image Processing 2005	10.1109/ICIP.2005.1529930	adaptive filter;wavelet;computer vision;mathematical optimization;image resolution;second-generation wavelet transform;computer science;cascade algorithm;mathematics;geometry;wavelet packet decomposition;statistics;wavelet transform	Visualization	69.9070140935993	-43.7880132791834	144043
25c244e8d4903f895ab00c582795410b7b40850f	a generic and scalable pipeline for gpu tetrahedral grid rendering	feed forward;pipelines rendering computer graphics hardware personal communication networks sampling methods computational geometry computer graphics visualization feedforward systems plastics;paper;computational geometry;computer graphic equipment;geometry construction gpu tetrahedral grid rendering graphics hardware feed forward pipeline;adolescent adolescent psychology child child psychology curriculum great britain health services needs and demand humans interdisciplinary communication mental disorders mental health services psychosocial deprivation teaching;cell projection;programmable graphics hardware direct volume rendering unstructured grids;unstructured grid;graphics hardware;feed forward pipeline;nvidia geforce 7900 gtx;nvidia;programmable graphics hardware;geometry construction;gpu tetrahedral grid rendering;computer science;rendering computer graphics;unstructured grids;direct volume rendering;rendering computer graphics computational geometry computer graphic equipment grid computing pipeline processing;grid computing;3d graphics and realism;pipeline processing;rendering	Recent advances in algorithms and graphics hardware have opened the possibility to render tetrahedral grids at interactive rates on commodity PCs. This paper extends on this work in that it presents a direct volume rendering method for such grids which supports both current and upcoming graphics hardware architectures, large and deformable grids, as well as different rendering options. At the core of our method is the idea to perform the sampling of tetrahedral elements along the view rays entirely in local barycentric coordinates. Then, sampling requires minimum GPU memory and texture access operations, and it maps efficiently onto a feed-forward pipeline of multiple stages performing computation and geometry construction. We propose to spawn rendered elements from one single vertex. This makes the method amenable to upcoming Direct3D 10 graphics hardware which allows to create geometry on the GPU. By only modifying the algorithm slightly it can be used to render per-pixel iso-surfaces and to perform tetrahedral cell projection. As our method neither requires any pre-processing nor an intermediate grid representation it can efficiently deal with dynamic and large 3D meshes	algorithm;architecture as topic;barycentric subdivision;beye;cell (microprocessor);computation;const (computer programming);direct3d;fits;generic drugs;geodesic grid;graphics hardware;graphics pipeline;graphics processing unit;hl7publishingsubsection <operations>;initial volume of distribution;isosurface;lithium;map;pixel;preprocessor;radiation;rasterisation;sampling (signal processing);sampling - surgical action;scalability;seizures;spawn (computing);verification of theories;vertex;volume ray casting;volume rendering;zellweger syndrome;cell projection	Joachim Georgii;Rüdiger Westermann	2006	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2006.110	computer vision;tiled rendering;graphics pipeline;unstructured grid;computer hardware;rendering;computational geometry;computer science;theoretical computer science;texture memory;graphics hardware;feed forward;grid computing;computer graphics (images)	Visualization	69.13775629461665	-49.989499746386386	144187
0880733727a5489a95ebeb18797607bc40fa49e3	geodesic binding for degenerate character geometry using sparse voxelization	windings;iterative methods computational geometry computer animation;skinning;sparse voxelization;skin;distance computation geodesic binding degenerate character geometry sparse voxelization fully automatic method influence weights closed form skinning methods linear blend dual quaternion skinning production meshes nonmanifold geometry intersecting triangles multiple connected components character rest pose mesh skeleton hierarchy binding weights nonexterior voxels interactive rates iteration parameters weight assignment decoupling;geometry;computational modeling;bones;deformation;geometry bones octrees windings skin computational modeling;character animation;octrees	We propose a fully automatic method for specifying influence weights for closed-form skinning methods, such as linear blend or dual quaternion skinning. Our method is designed to work with production meshes that may contain non-manifold geometry, be non-watertight, have intersecting triangles, or be comprised of multiple connected components. Starting from a character rest pose mesh and skeleton hierarchy, we first voxelize the input geometry. The resulting sparse voxelization is then used to calculate binding weights, based on the geodesic distance between each voxel lying on a skeleton “bone” and all non-exterior voxels. This yields smooth weights at interactive rates, without time-constants, iteration parameters, or costly optimization at bind or pose time. By decoupling weight assignment from distance computation we make it possible to modify weights interactively, at pose time, without additional pre-processing or computation. This allows artists to assess impact of weight selection in the context in which they are used.	asp.net;abnormal degeneration;algorithm;bone tissue;buffers;collision detection;computation (action);connected component (graph theory);constructive solid geometry;coupling (computer programming);distance (graph theory);distance transform;dual;energy minimization;explicit and implicit methods;fluid animation;framebuffer;generalized valence bond;interactivity;isosurface;iteration;level of detail;mathematical optimization;preprocessor;rendering (computer graphics);simulation;skeleton;sparse matrix;stencil buffer;telling untruths;voxel;weight;manifold	Olivier Dionne;Martin de Lasa	2014	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2014.2321563	character animation;combinatorics;mathematics;geometry;skin;electromagnetic coil;computational model;deformation	Visualization	69.1032603745105	-46.195467270851985	144291
59782cd676ec09e11fdfd384a58c987b6d046141	bone glow: an improved method for the assignment of weights for mesh deformation	skeletal animation;skinning;bone heat;real time;bone glow;vertex weighting;animation;mesh deformation;realtime	Many real-time algorithms for mesh deformation driven by animation of an underlying skeleton make use of a set of per-bone weights associated with each vertex. There are few unguided algorithms for the assignment of these weights with a recent proposed solution being bone heat [1]. In this paper, we briefly discuss bone heat and provide examples where it performs poorly. We then develop a refinement of bone heat, termed bone glow, which, in our validation, performed as well as bone heat in simple cases while not suffering from bone heat’s observed weaknesses.	algorithm;approximation algorithm;computation;glow;numerical analysis;real-time clock;refinement (computing);stepping level;tux	Rich Wareham;Joan Lasenby	2008		10.1007/978-3-540-70517-8_7	anime;skeletal animation;computer science	Graphics	70.89068270063014	-46.90065765415979	145312
cb588ca53461c28bfd6655c4ed12f09d6f356445	affine arc length polylines and curvature continuous uniform b-splines	quadratic b spline;geometric continuity;discrete affine differential geometry;curve approximation	We study the recently introduced notion of polylines that form a discrete version of planar curves in affine arc length parametrization, showing that they match the control polylines of curvature continuous uniform quadratic B-splines (with analogous results in Rn). It is demonstrated how inflection-free planar curves may be approximated by such affine arc length polylines in a way that the polyline is close to an affinely equidistant discretization of the curve and allows good approximations of the smooth affine curvature.	approximation algorithm;b-spline;discrete mathematics;discretization	Florian Käferböck	2014	Computer Aided Geometric Design	10.1016/j.cagd.2014.05.003	affine geometry;mathematical optimization;topology;affine coordinate system;affine plane;affine geometry of curves;affine hull;affine transformation;mathematics;geometry;affine shape adaptation;affine combination;affine group	Theory	69.28750093251648	-40.73412967565564	145573
a8ec9bf695091ed0e498298840a54e867947d419	subregion graph: a path planning acceleration structure for characters with various motion types in very large environments	path planning acceleration very large environments motion types abstraction	Modern computer graphics applications commonly feature very large virtual environments and diverse characters which perform different kinds of motions. To accelerate path planning in such a scenario, we propose the subregion graph data structure. It consists of subregions, which are clusters of locally connected waypoints inside a region, as well as subregion connectivities. We also present a fast algorithm to automatically generate a subregion graph from an enhanced waypoint graph map representation, which also supports various motion types and can be created from large virtual environments. Nevertheless, a subregion graph can be generated from any graphbased map representation. Our experiments show that a subregion graph is very compact relative to the input waypoint graph. By firstly planning a subregion path, and then limiting waypoint-level planning to this subregion path, over 8 times average speedup can be achieved, while average length ratios remain as low as 102.5%.	algorithm;automated planning and scheduling;average path length;chart;computer graphics;data structure;digital media;experiment;fast fourier transform;graph (abstract data type);marina;motion planning;octree;real-time clock;speedup;subdivision surface;virtual reality;waypoint	Nicholas Mario Wardhana;Henry Johan;Seah Hock Soon	2015	Computational Visual Media	10.1007/s41095-015-0018-0	any-angle path planning;computer science	Graphics	68.71024115083475	-49.838772185621494	145576
e74f370847e5b8ec7838bd58a32edc13e46f4402	voxel modelling for rapid manufacturing	thesis or dissertation	Rapid Manufacturing techniques create objects by adding material under computer control. The possibility of varying the material being added allows these processes to create Functionally Graded Materials. There are several research efforts that have succeeded in the creation of this type of objects but there are no established methods to model them in a CAD environment, since standard modelling applications presuppose a homogeneous object. This research explores the voxel modelling technique as a method to support Rapid Manufacturing where variable material composition will be possible. Rapid Manufacturing processes are reviewed as well as applications of FGM objects, the decomposition model through voxels and the general CAGD modelling techniques. Alternative representation methods currently in research were reviewed and the representation of an FGM using an FEA application was considered. Visualisation techniques for the exploration of a voxel model are examined, including volume rendering. Visualisation software available for these operations is identified. A system is developed based on the Visual%zation Toolkit (VTK), an open source, freely available visualisation library. Methods of generation of a voxel model, its visualisation and transfer to a Rapid Manufacturing machine are created. An example part was built based on a two-material model. The toolkit is extended to include the octree decomposition of graded material voxel models and the method is tested as a compression scheme, showing poor performance due to the overhead of pointers. Despite its large memory requirements at high resolution, the voxel model seems suitable at the resolutions available through prospective creation methods.		Ronaldo Mercado	2001			computer science;artificial intelligence;operations management	Robotics	68.70354192976357	-48.25598152006344	145661
383490053d02175dd2e9daa0edb0b7d386ce5255	material interface reconstruction	material boundaries;isosurface extraction;finite element methods surface reconstruction isosurfaces surface cracks surface treatment error analysis hydrodynamics equations mesh generation;nonmanifold boundary surfaces material interface reconstruction data sets fractional material information reconstruction problem dual grid barycentric coordinate tuple tetrahedron voronoi cells euclidean coordinates triangulation;computational geometry;barycentric coordinates;barycentric coordinate;error analysis;volume fraction;finite element analysis computational geometry;material interfaces;finite elements;finite element analysis;eulerian flow;nite element	This paper presents an algorithm for material interface reconstruction for data sets where fractional material information is given as a percentage for each element of the underlying grid. The reconstruction problem is transformed to a problem that analyzes a dual grid, where each vertex in the dual grid has an associated barycentric coordinate tuple that represents the fraction of each material present. Material boundaries are constructed by analyzing the barycentric coordinate tuples of a tetrahedron in material space and calculating intersections with Voronoi cells that represent the regions where one material dominates. These intersections are used to calculate intersections in the Euclidean coordinates of the tetrahedron. By triangulating these intersection points, one creates the material boundary. The algorithm can treat data sets containing any number of materials. The algorithm can also create nonmanifold boundary surfaces if necessary. By clipping the generated material boundaries against the original cells, one can examine the error in the algorithm. Error analysis shows that the algorithm preserves volume fractions within an error range of 0.5 percent per material.	algorithm;approximation;barycentric subdivision;clipping (computer graphics);reconstruction conjecture;subdivision surface;voronoi diagram	Kathleen S. Bonnell;Mark A. Duchaineau;Daniel R. Schikore;Bernd Hamann;Kenneth I. Joy	2003	IEEE Trans. Vis. Comput. Graph.	10.1109/TVCG.2003.1260744	combinatorics;topology;computational geometry;finite element method;mathematics;geometry	Visualization	69.4680160091492	-43.7949514061406	145741
9c4f9187e208da9032d777c6ae0fa9d500867d43	one-to-one sweeping based on harmonic s-t mappings of facet meshes and their cages	hexahedral;harmonic;one to one;sweeping;mesh generation	The sweeping algorithm is one of the most robust techniques to generate hexahedral meshes. During one-to-one sweeping, the most difficult thing is to map an all-quad source surface mesh onto its target surface. In this paper, a harmonic function is used to map meshes from a source surface to its target surface. The result shows that it can generate an all-quad mesh on the target surface with good mesh quality for the convex, concave or multiply-connected surface and thus avoid expensive smoothing algorithm (untangling). Meanwhile, the cage-based deformation method is used to locate interior nodes between the source and target surface during sweeping. Finally, examples are provided and the execution time for our proposed algorithm is discussed.	algorithm;concave function;graphical user interface;hex;hexahedron;one-to-one (data model);polygon mesh;run time (program lifecycle phase);smoothing;time complexity;tree (data structure);triangle mesh;triangulated irregular network;twisted	Shengyong Cai;Timothy J. Tautges	2014	Engineering with Computers	10.1007/s00366-014-0374-x	mesh generation;mathematical optimization;volume mesh;harmonic;mathematics;geometry;hexahedron;engineering drawing	Graphics	68.79142333087061	-43.46205043889667	145860
c71c7140651ceaba299981f62aeae9e523f1b047	interference suppression algorithm for sar based on time–frequency transform	wavelet coefficients interference suppression filter time frequency transform wideband interference narrowband interference synthetic aperture radar nonparametric method nbi wbi false alarm rate algorithm short time fourier transform sar echo stft domain instantaneous frequency spectrum;nonparametric method;time varying;wideband interference wbi;short time fourier transform stft;frequency modulation;nonparametric statistics;frequency domain analysis;constant false alarm rate;time frequency;subband decomposition;instantaneous frequency;spectrum;indexing terms;short time fourier transform;wavelet transforms;interference suppression;wavelet transform;synthetic aperture radar sar;frequency modulated;wavelet transforms echo suppression fourier transforms nonparametric statistics radar cross sections synthetic aperture radar time frequency analysis;fourier transforms;wavelet transform wt;narrowband interference;frequency domain analysis interference suppression wavelet domain frequency modulation algorithm design and analysis wavelet transforms;echo suppression;wideband interference wbi interference suppression narrowband interference nbi short time fourier transform stft subband decomposition synthetic aperture radar sar wavelet transform wt;radar cross sections;wavelet domain;time frequency analysis;algorithm design;algorithm design and analysis;narrowband interference nbi;synthetic aperture radar	The goal of this paper is to suppress the narrowband interference (NBI) and wideband interference (WBI) in synthetic aperture radar (SAR) by using a nonparametric method. The method is based on the analysis of time-frequency characteristic of NBI and WBI from which an interference suppression filter combined with the constant false alarm rate algorithm is designed. In this approach, the short-time Fourier transform (STFT) is used to estimate the instantaneous frequency of the SAR echo data with interference. In the STFT domain, the instantaneous frequency spectrum is represented by wavelet, and then, the designed filter filters the corresponding wavelet coefficients of the interference components. In addition, this algorithm is robust to time-varying NBI and WBI. The performance of the proposed approach is evaluated by the simulated and measured data, and the effectiveness is demonstrated.	algorithm;coefficient;constant false alarm rate;instantaneous phase;interference (communication);list of fourier-related transforms;short-time fourier transform;simulation;spectral density;statistical interference;synthetic data;wavelet;zero suppression	Shuang-xi Zhang;Meng-dao Xing;Rui Guo;Lei Zhang;Zheng Bao	2011	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2011.2164409	algorithm design;speech recognition;time–frequency analysis;telecommunications;mathematics;physics;quantum mechanics;wavelet transform	Visualization	81.50626756774514	-39.70119317691521	145975
21d555b6451cfbf2b72dcaca3587a3b8fbd50e72	simulating wood using a voxel approach	voxels;knots;soft wood;solid texture	In this paper we present a technique for generating three-dimensional wood textures using a regular texture array. Currently three-dimensional wood textures are generated using procedural textures. Procedural textures are flexible and require little memory, however the modeling of local artifacts such as knots is difficult using the procedural approach. By representing the wood as a texture array and growing the wood in this array we can easily simulate local phenomena such as knots. Our growth model is an approximation to the biological model and assumes that there are several similar wood cells per array element. This means that we can model artifacts that are defined by groups of similar cells. In particular our model is well suited for the modeling of soft-woods.	voxel	John W. Buchanan	1998	Comput. Graph. Forum	10.1111/1467-8659.00258	softwood;computer science;procedural texture;voxel;knot	NLP	68.69477622661127	-48.17375963070718	146119
a96bdd2650b0aafe3edca675fc48c91afab6136f	interpolation and the chirp transform: dsp meets optics	digital signal processing;interpolation;optics;frequency modulation;fourier transform;bandlimited analog signal chirp transform dsp optics signal interpolation uniformly spaced grid magnification optical systems chirp fourier transform analog schemes interpolation algorithms digital signal processing chirp z transform performance parameters identification linear fm chirp signals;z transforms;first principle;signal processing;fourier transforms;bandlimited signals;bandlimited signals interpolation chirp modulation frequency modulation z transforms optics signal processing fourier transforms;chirp modulation;interpolation chirp digital signal processing optical signal processing optical filters finite impulse response filter signal processing algorithms grid computing fourier transforms optical imaging	This paper considers the problem of interpolating a signal from one uniformly-spaced grid to another, where the grid spacings may be related by an arbitrary, irrational factor. Noting that interpolation is the digital equivalent of magnification, we begin by reviewing optical systems for magnification and “computation” of the chirp Fourier transform. This route suggests several analog schemes for magnification, which can be discretized to produce algorithms for interpolation. We then derive one of these algorithms from first principles, using a digital-signal-processing perspective. The result is an important, but forgotten, algorithm for interpolation first suggested as an application of the chirp-z transform by Rabiner, Schafer, and Rader. Unlike the earlier derivation, our approach is direct – we do not make use of Bluestein’s trick of completing the square. In addition, our approach identifies parameters under user control that can be optimized for best performance.	chirp z-transform;computation;digital signal processing;discretization;interpolation;lawrence rabiner;rader's fft algorithm;user interface	David C. Munson;Orhan Arikan	1999		10.1109/ICASSP.1999.758347	multidimensional signal processing;analog signal processing;fourier transform;constant q transform;decimation;speech recognition;computer science;fractional fourier transform;digital signal processing;discrete fourier transform;signal processing;mathematics;fourier analysis;chirp spread spectrum;non-uniform discrete fourier transform;chirp	Theory	74.18690489318314	-40.62437013124715	146199
6d6e3862a041a23c7c45cd875025fb89290d939b	parametric blending using fanout surfaces	parametric surface	A method for blending parametric surfaces is presented. The blend is designed by interpolating positional and tangential data along the two contact curues in which the blend meets the two giuen prim itiue surfaces, The prim itiue surfaces can be uny parametric surfaces that are continuously differentiable. The user is free to chalk out any curve on the prim itiue surface to act as a contact curue, The correspondence between the opposite contact curues is defined through another space curve called guiding trajectory. This helps in controlling the end-tangent magnitudes, and hence the shape, of the Mterpolant curue, The concept of the fanout surface, based on the prim itiue surface and the contact curve on it, k introduced and the guiding trajectory is obtained as the intersection of the two fanout surfaces. This is essential in remouing the arbitrariness in the correspondence between the contact curues which otherwise affects the blend shape in an undesirable manner.	alpha compositing;fan-out;interpolation;prim's algorithm;subdivision surface	Pramod Koparkar	1991		10.1145/112515.112557	parametric surface;mathematics	Robotics	68.7784949832088	-42.725127109734395	146220
1b69649f2b297198ac088aa47719a3c852a36727	a practical framework for generating volumetric meshes of subject-specific soft tissue	finite element simulation;computational geometry;subject specific modeling;virtual human;scientific;musculoskeletal simulation	Studying human motion using musculoskeletal models is a common practice in the field of biomechanics. By using such models, recorded subject’s motions can be analyzed in successive steps from kinematics and dynamics to muscle control. However simulating muscle deformation and interaction is not possible, but other methods such as a finite element (FE) simulation are very well suited to simulate deformation and interaction of objects. In this paper we present a practical framework for the automatic generation of FE ready meshes based on subject-specific segmented MRI data. The proposed method resolves several types of data inconsistencies: noise, an incomplete data set and self-intersections. This paper shows the different steps of the method, such as solving overlaps in the segmented surfaces, generating the volume mesh and the connection to a musculoskeletal simulation.	finite element method;interaction;kinesiology;simulation;volume mesh	Pieter Peeters;Nicolas Pronost	2013	The Visual Computer	10.1007/s00371-013-0788-2	simulation;virtual actor;computational geometry;computer science;theoretical computer science;mathematics;geometry	Visualization	70.44149769254489	-46.41623517670053	146432
de85fb990824bfa0f8534a5baeb35aa43d924dcb	adaptive polygonisation of non-manifold implicit surfaces	implicit surface;adaptive polygonisation;line stitching implicit surfaces non manifold octrees intervals polygonisation;nonmanifold implicit surface;computer graphics;information technology;computational geometry;surface fitting;non manifold;line stitching algorithm;polygonisation;polynomials;books;line stitching;adaptive octree subdivision;line stitching algorithm adaptive polygonisation nonmanifold implicit surface rendering adaptive octree subdivision interval arithmetic surface exclusion;intervals;implicit surfaces;surface cracks robustness australia arithmetic sampling methods informatics information technology polynomials books computer graphics;arithmetic;robustness;informatics;interval arithmetic;sampling methods;rendering computer graphics;surface exclusion;surface fitting rendering computer graphics octrees computational geometry;conference proceeding;octrees;surface cracks;australia;rendering	We discuss the polygonisation and rendering of non-manifold implicit surfaces using adaptive octree subdivision and interval arithmetic for surface exclusion in octree nodes. We present a new algorithm that polygonises some surfaces that self intersect, or have other non-manifold features such as separate sections that meet at points. Gradient information is used to resolve ambiguous polygonisations in plotting nodes. A line-stitching algorithm is discussed that allows for multiple polygons to be in a plotting node. We illustrate the algorithm with a number of surfaces that demonstrate its capabilities and limitations.	algorithm;gradient descent;implicit surface;interval arithmetic;octree;subdivision surface;vertex (geometry);vertex (graph theory)	Ronald J. Balsys;Kevin G. Suffern	2005	International Conference on Computer Graphics, Imaging and Visualization (CGIV'05)	10.1109/CGIV.2005.13	computer vision;computer science;theoretical computer science;computer graphics (images)	Visualization	69.80219089703179	-43.023844197861884	146555
0e79e9a64e9705bb3ed5c76bd163fcb0daa7d716	geometric texturing using level sets	implicit surface;topology;geometric modeling geometric texture mapping parameterization implicit surfaces volume texturing;polygonal mesh;interpolation;high resolution;computer graphics;texture mapping;level set;geometry;computational geometry;bridges;parameterization;surface texture;fast particle advection;computer graphics computational geometry;geometric texture mapping;engineering and technology;teknik och teknologier;volume texturing;implicit surfaces;computational complexity;mesh to level set scan conversion;data structures;polygonal meshes;solid modeling;level set surface texture solid modeling geometry robustness interpolation data structures bridges topology computational complexity;high resolution level sets;geometric texturing;smooth transition;high quality surfaces;geometric modeling;topologically connected surfaces;robustness;geometric model;high quality surfaces geometric texturing high resolution level sets polygonal mesh topologically connected surfaces mesh to level set scan conversion fast particle advection	We present techniques for warping and blending (or subtracting) geometric textures onto surfaces represented by high-resolution level sets. The geometric texture itself can be represented either explicitly as a polygonal mesh or implicitly as a level set. Unlike previous approaches, we can produce topologically connected surfaces with smooth blending and low distortion. Specifically, we offer two different solutions to the problem of adding fine-scale geometric detail to surfaces. Both solutions assume a level set representation of the base surface, which is easily achieved by means of a mesh-to-level-set scan conversion. To facilitate our mapping, we parameterize the embedding space of the base level set surface using fast particle advection. We can then warp explicit texture meshes onto this surface at nearly interactive speeds or blend level set representations of the texture to produce high-quality surfaces with smooth transitions.	algorithm;alpha compositing;discrete subaortic stenosis;distortion;embedding;geometric median;image resolution;image warping;intersection of set of elements;level set (data structures);map;merge;polygon mesh;radial (radio);radial basis function;scan conversion;semantic parameterization;smoothing (statistical technique);solutions;speed (motion);time complexity	Anders Brodersen;Ken Museth;Serban D. Porumbescu;Brian Budge	2008	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2007.70408	computer vision;topology;computational geometry;computer science;geometric modeling;mathematics;geometry;level set method	Visualization	68.56419051449821	-44.80007318934403	147025
d578a4f0500d81bc44247f5d1482d9cb02750752	trimming bézier surfaces on bézier surfaces via blossoming	triangular bezier patch;trimming;rectangular bezier patch;blossoming;reparametrization	The problem of trimming Bezier surfaces on Bezier surfaces contains many cases, such as the subdivision, conversion and conjoining. Different methods have been given for some special cases. In this paper, by means of blossoming and parameter transformation, a united approach is given for this problem. The approach can be extended to trim Bezier patches on any polynomial or rational surfaces naturally.	bézier curve	Lian-Qiang Yang;Xiao-Ming Zeng	2008		10.1007/978-3-540-79246-8_50	discrete mathematics;computer science;trimming;mathematics;engineering drawing;computer graphics (images)	Robotics	68.49443568988642	-41.7958999823852	147144
ddabc4f44edebb8cce2efda5b5e4902990b11f6b	continuous skeleton computation by voronoi diagram	voronoi diagram	Abstract The skeleton of a continuous shape can be approximated from the Voronoi diagram of points sampled along the shape boundary. To bound the error of this approximation, one must relate the spatial complexity of the shape to the boundary sampling density. The regular set model of mathematical morphology provides a practical basis to establish such a relationship. Given a binary image shape, we exhibit a corresponding continuous, regular shape such that the sequence of points describing its boundary constitutes a sufficiently dense sampling for an accurate skeleton approximation. Additionally, we bound the regeneration error from the sampling density and the regularity parameter. This approach opens significant new possibilities for shape analysis by the exact, Euclidean skeleton. As a simple example, we describe how the skeleton can be refined by pruning, without introducing significant error in the regenerated image.	computation;voronoi diagram	Jonathan W. Brandt;V. Ralph Algazi	1992	CVGIP: Image Understanding	10.1016/1049-9660(92)90030-7	mathematical optimization;combinatorics;mathematics;geometry;topological skeleton	Robotics	70.84609420268569	-42.47274958737782	147356
45552ce6d31e609412b58bc9d6e0cf3a8b187193	mean-value laplacian coordinates for triangular meshes	linear systems;quadratic programming;quadratic energy function;laplace operator;quadratic energy function mean value laplacian coordinates triangular mesh laplace operator mean value weights quadratic optimization problem sparse linear system;computer graphics;boundary conditions;laplacian mesh editing;geometry;computational geometry;triangular mesh;skeleton;energy function;laplace equations;bones;solid modeling;quadratic optimization problem;animation;mean value laplacian coordinates;laplacian mesh editing mean value coordinates;mean value weights;quadratic optimization;quadratic programming computational geometry computer animation mesh generation;computer animation;mesh generation;laplace equations skeleton computer graphics geometry bones automation linear systems solid modeling animation boundary conditions;mean value coordinates;sparse linear system;automation	This paper presents an effective approach for triangular mesh editing, based on mean-value Laplacian coordinates for triangular meshes. We discretize the Laplace operator using mean value weights instead of uniform weights for fine approximation qualities. The results are obtained by solving a quadratic optimization problem, which can be efficiently minimized by solving a sparse linear system. Moreover, the quadratic energy function is assigned to each triangle rather than each vertex, which is more convenient to add control items. The result shows that our method is effective enough for common applications	approximation;discretization;emoticon;linear system;mathematical optimization;numerical analysis;optimization problem;polygon mesh;region of interest;sparse matrix;triangle mesh;triangular matrix;triangulated irregular network	Huai-Yu Wu;Chunhong Pan;Jia Pan;Songde Ma	2006	International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)	10.1109/CGIV.2006.64	mathematical optimization;combinatorics;mathematics;geometry	Visualization	69.85031965882207	-42.28875725530653	147411
b22fc744d4515ccc8e063d24684000816088a207	incorporating biomechanics into architectural tree models	environmental influence;computer graphics;tree growth;tree architecture;gravity;biomechanics;deformable models;layout;null;wood products;tree graphs;computational modeling;shape;biomechanics shape tree graphs computational modeling gravity layout production computer graphics deformable models animation;animation;production;cross section;natural scenes	We present a method for creating tree models with realistically curved branches, useful in the portrayal of natural scenes. Instead of attempting to replicate a tree’s final shape by observation, we obtain this shape as nature does — by considering the tree’s development in the context of its environment. The final shape of the branches results from their growth in length, girth, weight and rigidity under the influence of gravity and tropisms. Using the framework of L-systems, we extend Jirasek’s biomechanical simulation of a plant axis to correctly represent an entire tree. Our model also simulates the reaction wood which actively re-orients a leaning branch by differentiating the wood production in angular portions of the branch cross-section. To obtain realistic and controllable tree architectures, we regulate growth elements in the model using functions based on botanical findings. We create a multi-year simulation of tree growth under environmental influences, obtaining a realistic tree shape at every stage of its development.	angularjs;apache axis;girth (graph theory);l-system;self-replicating machine;simulation	Julia Taylor-Hell	2005	XVIII Brazilian Symposium on Computer Graphics and Image Processing (SIBGRAPI'05)	10.1109/SIBGRAPI.2005.32	simulation;computer science;engineering drawing;computer graphics (images)	Graphics	68.71383959052456	-48.04140835400496	147556
89ab2cde291973dabfe74dea03e28768f8afd990	"""identification and """"reverse engineering"""" of pythagorean-hodograph curves"""	pythagorean hodograph curves;arc length;bezier control points;numerical quadrature;parametric speed;reverse engineering	Methods are developed to identify whether or not a given polynomial curve, specified by Bezier control points, is a Pythagorean-hodograph (PH) curve - and, if so, to reconstruct the internal algebraic structure that allows one to exploit the advantageous properties of PH curves. Two approaches to identification of PH curves are proposed. The first is based on the satisfaction of a system of algebraic constraints by the control-polygon legs, and the second uses the fact that numerical quadrature rules that are exact for polynomials of a certain maximum degree generate arc length estimates for PH curves exhibiting a sharp saturation as the number of sample points is increased. These methods are equally applicable to planar and spatial PH curves, and are fully elaborated for cubic and quintic PH curves. The reverse engineering problem involves computing the complex or quaternion coefficients of the pre-image polynomials generating planar or spatial Pythagorean hodographs, respectively, from prescribed Bezier control points. In the planar case, a simple closed-form solution is possible, but for spatial PH curves the reverse engineering problem is much more involved. Methods to identify whether or not given control points define a Pythagorean-hodograph (PH) curve are formulated.The methods are based on the satisfaction of control-point constraints or saturation of quadrature arc-length estimates, and apply equally to planar and spatial PH curves.For identified PH curves, algorithms to reconstruct the complex or quaternion pre-image polynomials are developed.The proposed methods allow existing CAD systems to fully exploit the advantageous properties of PH curves within the context of prevailing CAD geometry representations.	reverse engineering	Rida T. Farouki;Carlotta Giannelli;Alessandra Sestini	2015	Computer Aided Geometric Design	10.1016/j.cagd.2015.04.001	mathematical optimization;discrete mathematics;mathematics;geometry;family of curves;arc length;reverse engineering	Theory	70.16618053889646	-39.59099504919202	147647
13e1e972ac069fb67ad6157d0dd547485d25d309	immersive 4d visualization of complex dynamics	complex holomorphic dynamics;complex dynamics;holomorphic dynamics;dynamic system;visualization technique;three dimensional graphics and realism;4 d visualization	A visualization technique for dynamical systems in two complex variables is introduced. With this new technique, interesting properties and structures previously undetected with other methods have been observed. CR Categories: 1.3.7 [Three-Dimensional Graphics and Realism]: Fractals	complex dynamics;dynamical system;fractal;graphics	Estela A. Gavosto;James R. Miller;John Sheu	1998		10.1145/324332.324346	computer vision;simulation;computer science;computer graphics (images)	Visualization	73.44540973717604	-45.583127785722205	147943
8c47228423d2603c7c0040b05cc430d662610b1e	toward interactive-rate simulation of fluids with moving obstacles using navier-stokes equations	simulation ordinateur;modelizacion;representation graphique;condiciones limites;physics based modeling;comportement;condition aux limites;computer graphics;representacion grafica;three dimensional shape;computation fluid dynamics;fluid flow;fluid model;forma tridimensional;dinamica fluido;computer graphic;modelisation;forme tridimensionnelle;conducta;campo flujo;boundary condition;flow field;equation navier stokes;champ ecoulement;reynolds number;fluid dynamics;simulacion computadora;fluid simulation;behavior;dynamique fluide;modeling;grafico computadora;computer simulation;infographie;graphics;navier stokes equation;ecuacion navier stokes	Abstract   We present a new method for physically based modeling and interactive-rate simulation of 3D fluids in computer graphics. By solving the 2D Navier-Stokes equations using a computational fluid dynamics method, we map the surface into 3D using the corresponding pressures in the fluid flow field. The method achieves realistic interactive-rate fluid simulation by solving the physical governing laws of fluids but avoiding the extensive 3D fluid dynamics computation. Unlike previous computer graphics fluid models, our approach can simulate many different fluid behaviors by changing the internal or external boundary conditions. It can model different kinds of fluids by varying the Reynolds number. It can also simulate objects moving or floating in fluids. In addition, we can visualize the animation of the fluid flow field, the streakline of a flow field, and the blending of fluids of different colors. Our model can serve as a testbed to simulate many other fluid phenomena which have never been successfully modeled previously in computer graphics.	interactivity;navier–stokes equations;simulation	Jim X. Chen;Niels da Vitoria Lobo	1995	CVGIP: Graphical Model and Image Processing	10.1006/gmip.1995.1012	fluid simulation;simulation;flow visualization;computer science;graphics;reynolds number;calculus;mathematics;geometry;computer graphics;cfd-dem;fluid mechanics;behavior;fluid dynamics;immersed boundary method	Robotics	71.53977014874145	-48.418961518488345	147967
90c36c12e880b0250df0d7c69a4fd180a0f65b26	univariate cubic l1 interpolating splines based on the first derivative and on 5-point windows: analysis, algorithm and shape-preserving properties	interpolation;locally calculated;cubic l 1 spline;cubic l1 spline;first derivative based;shape preservation	In this paper, univariate cubic L 1 interpolating splines based on the first derivative and on 5-point windows are introduced. Analytical results for minimizing the local spline functional on 5-point windows are presented and, based on these results, an efficient algorithm for calculating the spline coefficients is set up. It is shown that cubic L 1 splines based on the first derivative and on 5-point windows preserve linearity of the original data and avoid extraneous oscillation. Computational examples, including comparison with first-derivative-based cubic L 1 splines calculated by a primal affine algorithm and with second-derivative-based cubic L 1 splines, show the advantages of the first-derivative-based cubic L 1 splines calculated by the new algorithm.	algorithm;cubic function;interpolation;microsoft windows;spline (mathematics)	Qingwei Jin;Lu Yu;John E. Lavery;Shu-Cherng Fang	2012	Comp. Opt. and Appl.	10.1007/s10589-011-9426-y	spline interpolation;mathematical optimization;combinatorics;smoothing spline;monotone cubic interpolation;interpolation;bicubic interpolation;mathematics;geometry;box spline	Theory	70.33703682888569	-40.38501560942486	148622
a3af7a7bcf042f5b2e7b3bc958183eb03ef42c24	animating physically based explosions in real-time	particle systems;paper;thermal expansion;real time;gpu;physically based animation;visual quality;physics based animation;real time graphics;real time animation;animation;particle system;explosions;fluid dynamics;computer science;natural phenomena;3d graphics and realism;turbulence model	We present a framework for real-time animation of explosions that runs completely on the GPU. The simulation allows for arbitrary internal boundaries and is governed by a combustion process, a Stable Fluid solver, which includes thermal expansion, and turbulence modeling. The simulation results are visualised by two particle systems rendered using animated textures. The results are physically based, non-repeating, and dynamic real-time explosions with high visual quality.	graphics processing unit;particle system;real-time clock;simulation;solver;turbulence modeling	Lars Andreas Ek;Rune Vistnes;Odd Erik Gundersen	2007		10.1145/1294685.1294696	simulation;computer science;particle system;multimedia;computer graphics (images);fluid dynamics	Graphics	70.6538408432805	-48.54906105884717	148623
1d50b7a52ac85f1b4e23156cf5563256d26e7104	contour forests: fast multi-threaded augmented contour trees		This paper presents a new algorithm for the fast, shared memory multi-threaded computation of contour trees on tetrahedral meshes. In contrast to previous multi-threaded algorithms, our technique computes the augmented contour tree. Such an augmentation is required to enable the full extent of contour tree based applications, including for instance data segmentation. Our approach relies on a range-driven domain partitioning. We show how to exploit such a partitioning to rapidly compute contour forests. We also show how such forests can be efficiently turned into the output contour tree. We report performance numbers that compare our approach to a reference sequential implementation for the computation of augmented contour trees. These experiments demonstrate the run-time efficiency of our approach. We demonstrate the utility of our approach with several data segmentation tasks. We also provide a lightweight VTK-based C++ implementation of our approach for reproduction purposes.	c++;computation;contour line;distributed computing;experiment;field (computer science);interactivity;level of detail;memory bound function;openmp;out-of-core algorithm;parallel computing;persistence (computer science);reference implementation;sas;shared memory;simulation;thread (computing);vtk	Charles Gueunet;Pierre Fortin;Julien Jomier	2016	2016 IEEE 6th Symposium on Large Data Analysis and Visualization (LDAV)	10.1109/LDAV.2016.7874333	computer vision;computer science;theoretical computer science;machine learning	Visualization	68.67190757464134	-51.04137752349995	148670
e92a249726423e99598045fad2cea7127713a5b9	fast collision detection for skeletally deformablemodels	collision detection	We present a new method of collision detection for models deformed by linear blend skinning. The linear blend skinning (also known as skeleton-subspace deformation, vertex-blending, or enveloping) is a popular method to animate believable organic models. We consider an exact collision detection based on a hierarchy of bounding spheres. The main problem with this approach is the update of bounding volumes – they must follow the current deformation of the model. We introduce a new fast method to refit the bounding spheres, which can be executed on spheres in any order. Thanks to this on-demand refitting operation we obtain a collision detection algorithm with speed comparable to the standard rigid body collision detection. The algorithm was tested on a variety of practical situations, including an animated crowd. According to these experiments, the proposed approach is considerably faster than the previous method.	algorithm;alpha compositing;bounding sphere;bounding volume;collision detection;experiment;refit;skin (computing)	Ladislav Kavan;Jirí Zára	2005	Comput. Graph. Forum	10.1111/j.1467-8659.2005.00861.x	simulation;computer science;mathematics;geometry;collision detection;computer graphics (images)	Vision	69.11187577404193	-47.59240869444882	148888
c15c3e61e67d12b091fa77ed948f1f256185ff4e	wavelet analysis of a microbarograph network	transformation ondelette;wavelet analysis;tratamiento datos;iterative method;traitement signal;oscillations;gravity wave;long period;horizontal speed;onda gravedad;cross correlation function;non linear filter;adaptive filtering;mesosphere;wavelet based algorithm;filtrado adaptable;microbarograph network;time frequency;gravity;fixed time;data processing;weather forecasting;nonlinear filter;traitement donnee;spectrum;indexing terms;data mining;cross correlation functions;medida presion;long period gravity waves;thermosphere;methode calcul;metodo calculo;wavelet transforms;atmospheric movements;pressure measurement;wavelet analysis gravity weather forecasting data mining atmospheric waves data processing adaptive filters wavelet coefficients time frequency analysis wavelet transforms;adaptive filters;wavelet transform;adaptive signal processing;geophysical signal processing;signal processing;selective reconstruction;atmospheric pressure;barograph network atmosphere meteorology measurement technique signal processing wavelet analysis microbarograph network atmospheric pressure wavelet based algorithm gravity wave atmospheric pressure trace data processing nonlinear adaptive filter selective reconstruction wavelet transform iterative method long period gravity waves horizontal speed propagation direction cross correlation functions;atmospheric waves;gravity waves;propagation direction;filtro no lineal;atmospheric techniques;adaptive filters atmospheric pressure atmospheric techniques atmospheric movements meteorology mesosphere thermosphere gravity waves geophysical signal processing wavelet transforms adaptive signal processing;filtrage adaptatif;transformacion ondita;methode domaine temps frequence;mesure pression;onde gravite;atmosphere;procesamiento senal;barograph network;measurement technique;time frequency analysis;meteorology;metodo dominio tiempo frecuencia;adaptive filter;nonlinear adaptive filter;wavelet coefficients;computing method	This paper presents a wavelet-based algorithm for the detection, identification, and extraction of gravity waves from atmospheric pressure traces. The main data processing tool is a nonlinear adaptive filter based on the selective reconstruction of a waveform from its wavelet coefficients. The time-frequency localization of the wavelet transform provides an ideal framework for the decomposition of long-period gravity waves (30 min–6 h), which are characterized by a generally broad spectrum and few oscillation cycles. The procedure is iterative and allows the exhaustive processing of all the events present in a fixed time period. The waveform of each disturbance is reconstructed with high accuracy. This minimizes the influence of the data-processing technique on the estimate of horizontal speed and direction of propagation, obtained by maximization of the cross-correlation functions between the reconstructed waveforms at the different stations. The introduction of coherency criteria through the network of seven stations allows us to separate the events into two classes. The first includes the events that propagate with very small distortion through the network, while the second includes less coherent but still highly energetic events. The size of the network and the algorithm developed for the analysis is well suited for the identification and the extraction of those mesoscale disturbances that have a particularly strong influence on the weather as well as on the forecast.	adaptive filter;coefficient;coherence (physics);cross-correlation;distortion;expectation–maximization algorithm;iterative method;nonlinear system;software propagation;tracing (software);waveform;wavelet transform	Stefano Grivet-Talocia;Franco Einaudi	1998	IEEE Trans. Geoscience and Remote Sensing	10.1109/36.662727	adaptive filter;meteorology;data processing;telecommunications;signal processing;stationary wavelet transform;physics;quantum mechanics;remote sensing	Mobile	81.16098609509238	-42.06274623178685	149429
00f737bff72c2ef69d3d374962df73a60d6a9727	grid applications - high-performance dynamic graphics streaming for scalable adaptive graphics environment	video streaming data visualisation high definition video image resolution middleware rendering computer graphics video signal processing;graphics rendering computer graphics streaming media data visualization computer displays chromium collaboration pixel wide area networks drives;desktop window manager high performance dynamic graphics streaming scalable adaptive graphics environment middleware high definition video image resolution distributed rendering	The scalable adaptive graphics environment (SAGE) is specialized middleware for enabling data, high-definition video and extremely high-resolution graphics to be streamed in real-time from remotely distributed rendering and storage clusters to scalable display walls over ultra high-speed networks. In this paper, we present the SAGE architecture, focusing on its dynamic graphics streaming capability. In the SAGE framework, multiple visualization applications can be streamed to large tiled displays and viewed at the same time. The application windows can be moved, resized and overlapped like any standard desktop window manager. Every window movement or resize operation requires dynamic and non-trivial reconfiguration of the involved graphics streams. This approach has been successfully shown to scale to support streaming on the LambdaVision 100 megapixel display wall. SAGE is now being extended to support distance collaboration with multiple endpoints by streaming visualization to all the participants	graphics;scalability	Byungil Jeong;Luc Renambot;Ratko Jagodic;Rajvikram Singh;Julieta Aguilera;Andrew E. Johnson;Jason Leigh	2006		10.1109/SC.2006.35	graphics pipeline;scientific visualization;image-based modeling and rendering;computer hardware;rendering;computer science;real-time computer graphics;multimedia;graphics software;real-time rendering;texture memory;computer graphics;alternate frame rendering;general-purpose computing on graphics processing units;software rendering;3d computer graphics;computer graphics (images)	HPC	68.50485746753007	-51.76651548821281	149694
4a22edcc8eb39fae5975660fe05a3f016fc6b230	topology-guided downsampling	critical point;data compression;scalar field;volume visualization;topology preservation	Wepresentanew downsamplingmethodfor structuredvolumegrids, whichpreservesmuchmoreof thetopologyof ascalarfield thanexistingdownsamplingmethodsby preferablyselectingscalarvaluesof critical points.In particular, many critical pointscanbepreservedwhicharelostby traditionaldownsamplingmethods.Our methodis named“topology-guideddownsampling”as topology-preservingdownsamplingis impossiblein general.However, we show that even an approximatepreservation of topologyis highly desirableif isosurfacesareextractedfrom the downsampledvolumegrid, e.g.for interacti ve previewing, becausemany topologicalfeaturesof the isosurfaces,e.g.the number of components, tunnels,andholes,are preserved. We illustrate the benefitsof our methodwith examplesfrom medicalandtechnicalapplicationsof volume visualization.	decimation (signal processing);scientific visualization	Martin Kraus;Thomas Ertl	2001		10.2312/VG/VG01/223-235	discrete mathematics;theoretical computer science;mathematics;geometry	Visualization	71.61220925878112	-44.92161401685275	150132
47a662f84a2163972e1307fc0cec13984161f490	statics aware grid shells	picture image generation;line and curve generation;i 3 3 computer graphics picture image generation line and curve generation;i 3 3 computer graphics;categories and subject descriptors according to acm ccs	We introduce a framework for the generation of polygonal grid-shell architectural structures, whose topology is designed in order to excel in static performances. We start from the analysis of stress on the input surface and we use the resulting tensor field to induce an anisotropic non-Euclidean metric over it. This metric is derived by studying the relation between the stress tensor over a continuous shell and the optimal shape of polygons in a corresponding grid-shell. Polygonal meshes with uniform density and isotropic cells under this metric exhibit variable density and anisotropy in Euclidean space, thus achieving a better distribution of the strain energy over their elements. Meshes are further optimized taking into account symmetry and regularity of cells to improve aesthetics. We experiment with quad meshes and hex-dominant meshes, demonstrating that our grid-shells achieve better static performances than state-of-the-art grid-shells.	bures metric;euclidean distance;hex;performance;polygon mesh	Nico Pietroni;Davide Tonelli;Enrico Puppo;Maurizio Froli;Roberto Scopigno;Paolo Cignoni	2015	Comput. Graph. Forum	10.1111/cgf.12590	combinatorics;topology;mathematics;geometry;algorithm;computer graphics (images)	Visualization	68.87387878956952	-45.488667006597964	150354
1ba8864ebda27491f64df0fa6bd3cbc3f19b167f	visualising spins and clusters in regular and small-world ising models with gpus	nvidia geforce gtx 260;paper;model system;compute unified device architecture;small world;cuda;physics;visualization;interactive system;ising model;nvidia;graphic processing unit;opengl;monte carlo;lattice model;computer simulation	Visualising computational simulation models of solid state physical systems is a hard problem for dense lattice models. Fly throughs and cutaways can aid viewer understanding of a simulated system. Interactive time model parameter updates and overlaying of measurements and graticules, cluster colour labelling and other visual highlighting cues can also enhance user intuition of the model’s meaning. We present some graphical and simulation optimisation techniques and various graphical rendering and explanatory techniques for computational simulation models such as the Ising model in 2 and 3 dimensions. In addition to aiding understanding of conventional algorithms such as Metropolis Monte Carlo, we try to visualise cluster updates to the system using algorithms like that of Wolff. We also explore ways to visualise pathlength shortening and other changes to the Ising system when small-world link rewiring is applied to the system. We use a combination of OpenGL visualisation software and General Purpose Computing on Graphics Processing Units (GPGPU) with the Compute Unified Device Architecture (CUDA) and consider ways to accelerate both the simulation itself as well as the graphical rendering to make an interactive system of model system sizes that are large enough to be challenging and visually interesting.	cuda;color;complex systems;computer cluster;display device;general-purpose computing on graphics processing units;graphical user interface;graphics processing unit;interactivity;interoperability;ising model;lattice model (physics);mathematical optimization;metropolis;metropolis–hastings algorithm;monte carlo method;opengl;programming model;real-time clock;scientific visualization;simulation;small-world experiment;solid-state drive;vertex buffer object	Arno Leist;Daniel Peter Playne;Kenneth A. Hawick	2010		10.1016/j.procs.2010.04.191	computer simulation;ising model;simulation;lattice model;visualization;computer science;theoretical computer science;operating system;statistics;monte carlo method;computer graphics (images)	ML	69.69912355870446	-50.69441779440668	150790
12d12737392094d031b1e760894655117684603a	comparative flow visualization	vortices;comparative visualization;image resolution;indexing terms;vortices flow visualisation rendering computer graphics data visualisation feature extraction image resolution;65;data visualisation;flow visualisation;feature extraction;visual features;feature extraction index terms streamline streamribbon comparative visualization;index terms streamline;streamline;vector data;streamribbon;rendering computer graphics;flow visualization;data visualization geometry feature extraction computational modeling power generation graphics interpolation degradation numerical models;stream ribbons comparative flow visualization tools vector data set vortex cores polylines feature extraction streamlines	There are many situations where one needs to compare two or more data sets. It may be to compare different models, different resolutions, differences in algorithms, different experimental results, etc. There is therefore a need for comparative visualization tools to help analyze the differences. This paper focuses on comparative visualization tools for analyzing flow or vector data sets. The techniques presented allow one to compare individual streamlines and stream ribbons as well as a dense field of streamlines. These comparison methods can also be used to study differences in vortex cores that are represented as polylines.	algorithm;imagery;vortex	Vivek Verma;Alex T. Pang	2004	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2004.39	computer vision;flow visualization;computer science;theoretical computer science;data visualization;computer graphics (images)	Visualization	72.34816955412693	-51.46596333721359	151273
ae3a1994834cc1457808cd83120d6bd42aa58aa5	new spectral decomposition for 3d polygonal meshes and its application for watermarking	spectral decomposition;3d polygonal meshe;watermarking;singular spectrum analysis ssa;time series;data analysis techniques;similarity transformation;singular spectrum analysis	This paper present a generalization of a data analysis technique called a singular spectrum analysis (SSA). The original SSA is a tool for analyzing one-dimensional data such as time series, whereas our generalization is suitable for multi-dimensional data such as 3D polygonal meshes. One of applications of the proposed generalization are also shown. The application of the generalized SSA is a new robust watermarking method that adds a watermark to a 3D polygonal mesh. Watermarks embedded by our method are resistant to similarity transformations and random noises. Our method has the advantage in that it requires smaller calculation cost than other methods with nearly equal performance.	3d computer graphics;digital watermarking;embedded system;polygon mesh;time series;watermark (data file)	Kohei Murotani;Kokichi Sugihara	2005				EDA	73.13962688024901	-42.024405439490415	152528
12e7b18d8550fa95f98f5879a37b0625d7ec0852	direct raytracing of particle-based fluid surfaces using anisotropic kernels		Particle-based simulation models have assumed a significant role in the numerical computation of high-fidelity transient flow and continuum mechanical problems. However, direct visualization of surfaces from particle data without intermediary discrete triangulation remains a challenging task. We demonstrate a novel direct raytracing scheme for free surface intersection based on anisotropic smoothing kernels. Our approach efficiently reduces the number of candidate kernels evaluated to converge to the surface threshold, thereby running in image space rather than object space complexity. We conduct comprehensive benchmarks with respect to data set size, scene complexity, visual fidelity and hardware setup. Our versatile system is suitable for both high quality and interactive desktop rendering, scales reasonably well even with trivial parallelization and renders up to 170 million particles on 32 distributed compute nodes at close to interactive frame rates at 4K resolution with ambient occlusion.	4k resolution;ambient occlusion;anisotropic diffusion;benchmark (computing);computation;computer cluster;converge;dspace;desktop computer;display resolution;emoticon;eurographics;graphics processing unit;image scaling;interpolation;moving least squares;nearest neighbor search;numerical analysis;parallel computing;path tracing;preprocessor;ray tracing (graphics);rendering (computer graphics);simulation;smoothing;triune continuum paradigm;tuple space	Tim Biedert;Jan-Tobias Sohns;Simon Schröder;Jefferson Amstutz;Ingo Wald;Christoph Garth	2018		10.2312/pgv.20181090	parallel computing;rendering (computer graphics);computational science;smoothing;computation;visualization;triangulation (social science);computer science;frame rate;ray tracing (graphics);ambient occlusion	Graphics	68.94940472065298	-50.80178353004972	152756
bc3e8d21faf9221abcce1d45e037e75fe4ddb99a	curve reconstruction: connecting dots with good reason	curve reconstruction;sampling;pattern recognition;geometric modeling;geometric model;curve modeling;curve fitting;reconstruction algorithm	Abstract   Curve reconstruction algorithms are supposed to reconstruct curves from point samples. Recent papers present algorithms that come with a guarantee: Given a sufficiently dense sample of a closed smooth curve, the algorithms construct the correct polygonal reconstruction. Nothing is claimed about the output of the algorithms, if the input is not a dense sample of a closed smooth curve, e.g., a sample of a curve with endpoints. We present an algorithm that comes with a guarantee for any set   P   of input points. The algorithm constructs a polygonal reconstruction   G   and a smooth curve   Γ   that justifies   G   as the reconstruction from   P  .		Tamal K. Dey;Kurt Mehlhorn;Edgar A. Ramos	2000	Comput. Geom.	10.1016/S0925-7721(99)00051-6	curve sketching;econometrics;mathematical optimization;singular point of a curve;tripling-oriented doche–icart–kohel curve;geometric modeling;blancmange curve;polygonal chain;mathematics;geometry;parallel curve;moore curve;curve orientation;curve fitting	Theory	70.47502630020576	-41.37777726316258	152855
cb987baf3ab1ac5ce28a8e81b5531ff091fd66d5	approximate parameterization by planar rational curves	approximate parameterization;rational interpolation;rational curve;optimization technique;algebraic spline curves;objective function;weight function;nonlinear optimization;region growing;algebraic curve	We describe a method for approximate parameterization of a planar algebraic curve by a rational Bézier (spline) curve. After briefly discussing exact methods for parameterization and methods for rational interpolation, we describe a new technique for rational parameterization. Our approach is based on the minimization of a suitable--nonlinear objective function, which takes both the distance from the curve and the positivity of the weight function (i.e., the numerator of the rational parametric representation) into account. The solution is computed by using an SQP-type optimization technique. In addition, we use a region--growing--type approach in order to obtain a good initial solution, which is crucial for the convergence of the nonlinear optimization procedure.	approximation algorithm;bézier curve;interpolation;linear algebra;loss function;mathematical optimization;nonlinear programming;nonlinear system;optimization problem;sequential quadratic programming;spline (mathematics);weight function	Bert Jüttler;Pavel Chalmovianský	2004		10.1145/1037210.1037215	rational normal curve;rational function;polynomial and rational function modeling;mathematical optimization;mathematical analysis;discrete mathematics;weight function;nonlinear programming;mathematics;region growing;algebraic curve;stable curve	ML	69.54642394828811	-40.323033037881494	153295
f74bf1d0d1cc97fe877ec986f94349e878c1d935	sweep-surface reconstruction from three-dimensional measured data	non linear effect;metodo cuadrado menor;methode moindre carre;computer aided design;nonlinear least squares;modelo 3 dimensiones;least squares method;modele 3 dimensions;surface fitting;retroingenierie;three dimensional model;efecto no lineal;least square method;surface reconstruction;sweep surface fitting;three dimensional;reconstruction surface;aproximacion esplin;spline approximation;approximation spline;surface model;conception assistee;ajustement courbe;measurements;mesure;reconstruccion superficie;effet non lineaire;curve fitting;ingeniera inversa;reverse engineering	The purpose of this work was to present a surface fitting algorithm for sweep surface reconstruction from three-dimensional measured data. The sweeping rule considered in this work was essentially translational sweeping in which the generators traverse about the directors to form the desired sweep surface. The sweep-surface fitting was formulated as a nonlinear least-squares minimization problem for which an error expression was minimized which yields the optimized generators, the directors and the parameter values corresponding to each measured data. An algorithm was presented also to convert the sweep surface model into a composite spline surface which could be imported into most CAD/CAM systems. Effective experimental results were provided to illustrate the feasibility of the proposed strategy. q 1998 Elsevier Science Ltd. All rights reserved.	algorithm;computer-aided design;multivariate interpolation;non-linear least squares;nonlinear system;spline (mathematics);traverse	Wen-Der Ueng;Jiing-Yih Lai;Ji-Liang Doong	1998	Computer-Aided Design	10.1016/S0010-4485(98)00037-2	sweep line algorithm;computer aided design;calculus;mathematics;geometry;least squares;statistics	EDA	68.87258593090316	-39.642887833687034	153661
74b5d097c3b592d1d0fed8f001a67a8e10e01d21	developable strip approximation of parametric surfaces with global error bounds	automatic control;application software;computer graphics;brushes statistics automatic control asia image color analysis statistical analysis computer graphics application software internet design optimization;design optimization;internet;statistical analysis;image color analysis;transfer operator;statistics;global optimization;asia;brushes	Developable surfaces have many desired properties in manufacturing process. Since most existing CAD systems utilize parametric surfaces as the design primitive, there is a great demand in industry to convert a parametric surface within a prescribed global error bound into developable patches. In this work we propose a simple and efficient solution to approximate a general parametric surface with a minimum set of C0-joint developable strips. The key contribution of the proposed algorithm is that, several global optimization problems are elegantly solved in a sequence that offers a controllable global error bound on the developable surface approximation. Experimental results are presented to demonstrate the effectiveness and stability of the proposed algorithm.	approximation algorithm;approximation error;computer-aided design;contour line;global optimization;mathematical optimization;strips;triangle strip	Yong-Jin Liu;Yu-Kun Lai;Shi-Min Hu	2007	15th Pacific Conference on Computer Graphics and Applications (PG'07)	10.1109/PG.2007.13	computer vision;application software;the internet;multidisciplinary design optimization;simulation;color quantization;computer science;automatic control;mathematics;computer graphics;statistics;global optimization;computer graphics (images)	Vision	70.22010405269111	-40.97683379886318	153678
bfe0f021014fc66742738bec7079c24ab6bffbb5	using multiple frequency bins for stabilization of fd-ica algorithms	frequency domain independent component analysis;audio sources separation;multiple bins	"""In the frequency domain independent component analysis approaches for audio sources separation, the convolutive mixing problem is replaced by the solution of several instantaneous mixing problems, one for each frequency bin of the short time Fourier transform. This methodology yields good results but requires the solution of the permutation ambiguity. Moreover, the performance of the separation algorithms for each bin is not guaranteed to be equivalent, thus some bins can have worse results than others. In this paper a technique based on data from multiple bins is proposed to address these issues. The use of multiple bin information produces a coupling of the separation, resulting in more stable separation matrices and reducing the occurrence of permutations, but increasing in computational cost. This can be mitigated by a sub sampling of the multiple bins information. The results show that both approaches are beneficial for the frequency domain ICA approach, producing better separation in terms of objective quality measures. ∗Corresponding Author. Tel. +54 342 4575233 ext. 192 Email addresses: ldipersia@sinc.unl.edu.ar (Leandro E. Di Persia), dmilone@sinc.unl.edu.ar (Diego H. Milone) Preprint submitted to Signal Processing July 16, 2015 *Manuscript Click here to view linked References si nc (i ) R es ea rc h C en te r fo r Si gn al s, S ys te m s an d C om pu ta tio na l I nt el lig en ce ( fi ch .u nl .e du .a r/ si nc ) L . D i P er si a & D . H . M ilo ne ; """" U si ng m ul tip le f re qu en cy b in s fo r st ab ili za tio n of F D -I C A a lg or ith m s. """" Si gn al P ro ce ss in g. , V ol . 1 19 , p p. 1 62 -1 68 , 2 01 6. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65"""	ab initio quantum chemistry methods;algorithm;algorithmic efficiency;ambiguity function;architecture of windows nt;chroma subsampling;computation;computer-aided industrial design;cylinder-head-sector;digraphs and trigraphs;email;experiment;ext js javascript framework;fo (complexity);independent computing architecture;independent component analysis;lateral thinking;mixing (mathematics);nl (complexity);numerical aperture;sampling (signal processing);short-time fourier transform;signal processing;time complexity	Leandro E. Di Persia;Diego H. Milone	2016	Signal Processing	10.1016/j.sigpro.2015.07.025	econometrics;mathematical optimization;mathematics;statistics	ML	82.8924470101994	-38.196374305566515	153816
3308e0dcce70b520b40771940213f8499fd6683d	basilar-membrane responses to broadband noise modeled using linear filters with rational transfer functions	chinchilla;modelizacion;fonction rationnelle;gaussian noise;hilbert transforms;nonlinear filters;laser velocimetry;phase minimum;animals;signal to noise ratio nonlinear filters transfer functions kernel gaussian noise laser velocimetry frequency noise level discrete transforms phase measurement;phase measurement;appareil auditif;broadband noise;wide band noise;kernel;minimum phase;hilbert transformation;phase behavior;linear filter;cross correlation;genie biomedical;noise stimuli;wiener kernels;transfer functions;methode noyau;minimum phase discrete filters;oido interno;transformation hilbert;cochlea;nonlinear filter;arma model;models biological;ruido banda ancha;cochlear processing basilar membrane responses broadband noise rational transfer functions white gaussian noise laser velocimetry basal sites chinchilla cochlea first order wiener kernels noise stimuli minimum phase discrete filters hilbert transforms;bruit large bande;transformacion hilbert;cochlear processing;characteristic frequency;filtro lineal;linear filtering;modelo arma;signal processing computer assisted;modelisation;basilar membrane responses;hilbert transform;moving average;basilar membrane bm;cochlee;first order;rational transfer functions;ear;noise level;biomedical engineering;inner ear;autoregressive moving average arma modeling;oreille interne;filtre lineaire;discrete transforms;transfer function;neurophysiology acoustic noise ear gaussian noise hearing;funcion traspaso;acoustic noise;algorithms animals basilar membrane chinchilla linear models models biological regression analysis signal processing computer assisted;metodo nucleo;chinchilla cochlea;basal sites;modele arma;organ of hearing;algorithms;kernel method;wiener kernels autoregressive moving average arma modeling basilar membrane bm cochlea hilbert transform minimum phase;fonction transfert;regression analysis;ingenieria biomedica;white gaussian noise;neurophysiology;funcion racional;function prediction;signal to noise ratio	Basilar-membrane responses to white Gaussian noise were recorded using laser velocimetry at basal sites of the chinchilla cochlea with characteristic frequencies near 10 kHz and first-order Wiener kernels were computed by cross correlation of the stimuli and the responses. The presence or absence of minimum-phase behavior was explored by fitting the kernels with discrete linear filters with rational transfer functions. Excellent fits to the kernels were obtained with filters with transfer functions including zeroes located outside the unit circle, implying nonminimum-phase behavior. These filters accurately predicted basilar-membrane responses to other noise stimuli presented at the same level as the stimulus for the kernel computation. Fits with all-pole and other minimum-phase discrete filters were inferior to fits with nonminimum-phase filters. Minimum-phase functions predicted from the amplitude functions of the Wiener kernels by Hilbert transforms were different from the measured phase curves. These results, which suggest that basilar-membrane responses do not have the minimum-phase property, challenge the validity of models of cochlear processing, which incorporate minimum-phase behavior.	basal (phylogenetics);chinchilla<genus>;cochlear implant;cochlear structure;computation (action);cross-correlation;fits;first-order predicate;hilbert transform;kernel;kilohertz;minimum phase;normal statistical distribution;tissue membrane;transfer function;wiener–khinchin theorem	Alberto Recio-Spinoso;Yun-Hui Fan;Mario A. Ruggero	2011	IEEE Transactions on Biomedical Engineering	10.1109/TBME.2010.2052254	electronic engineering;speech recognition;acoustics;linear filter;mathematics;transfer function;neurophysiology;statistics	ML	81.12157496051277	-44.546404097103995	153919
263e0b5ded948135aa1b1d4fc10f947dd7397063	interactive rendering and efficient querying for large multivariate seismic volumes on consumer level pcs	out of core methods;multivariate 3d seismic dataset volume visualization method interactive rendering large multivariate seismic volume data consumer level pcs volume rendering pipeline virtual memory structure out of core multivariate multiresolution data gpu based ray caster interactive multivariate transfer function design gaussian mixture model representation nearly interactive querying gaussian functions;query processing;seismology;gaussian processes;transfer functions;data visualization transfer functions graphics processing units runtime educational institutions rendering computer graphics three dimensional displays;virtual storage data structures gaussian processes geophysics computing graphics processing units interactive systems pipeline processing query processing rendering computer graphics seismology transfer functions;geophysics computing;data structures;graphics processing units;rendering computer graphics;interactive systems;transfer functions multivariate volume out of core methods;virtual storage;multivariate volume;pipeline processing	We present a volume visualization method that allows interactive rendering and efficient querying of large multivariate seismic volume data on consumer level PCs. The volume rendering pipeline utilizes a virtual memory structure that supports out-of-core multivariate multi-resolution data and a GPU-based ray caster that allows interactive multivariate transfer function design. A Gaussian mixture model representation is precomputed and nearly interactive querying is achieved by testing the Gaussian functions against user defined transfer functions on the GPU in the runtime. Finally, the method has been tested on a multivariate 3D seismic dataset which is larger than the size of the main memory of the testing machine.	computer data storage;graphics pipeline;graphics processing unit;mixture model;out-of-core algorithm;personal computer;precomputation;rendering (computer graphics);scientific visualization;transfer function;volume rendering	Liang Zhou;Charles D. Hansen	2013	2013 IEEE Symposium on Large-Scale Data Analysis and Visualization (LDAV)	10.1109/LDAV.2013.6675167	computer science;data science;theoretical computer science;computer graphics (images)	Visualization	69.20982464570088	-51.495690486124346	154592
0c653a3153d869f804bcdc65dfc1ac8096267f45	efficient surgical cutting with position-based dynamics	visualization;computational modeling;heuristic algorithms;surgery;mathematical model;finite element analysis;rendering computer graphics	Simulations of cuts on deformable bodies have been an active research subject for more than two decades. However, previous works based on finite element methods and mass spring meshes cannot scale to complex surgical scenarios. This article presents a novel method that uses position-based dynamics (PBD) for mesh-free cutting simulation. The proposed solutions include a method to efficiently render force feedback while cutting, an efficient heat diffusion model to simulate electrocautery, and a novel adaptive skinning scheme based on oriented particles.https://extras.computer.org/extra/mcg2017030024s1.mp4	computer simulation;cutting-plane method;electrocoagulation;finite element method;greater than;haptic technology;human body;incised wound;programming by demonstration;solutions	Iago U. Berndt;Rafael P. Torchelsen;Anderson Maciel	2017	IEEE Computer Graphics and Applications	10.1109/MCG.2017.45	simulation;visualization;computer science;finite element method;mathematical model;computational model;statistics;computer graphics (images)	Visualization	71.00181448679372	-47.20418099663677	154824
754005b5519bb1679282e3fd6831ded2328aa3ba	texture pattern generation using clonal mosaic	pattern generation;model simulation	In this paper, an effective system for synthesizing animal skin patterns on arbitrary polygonal surfaces is developed. To accomplish the task, a system inspired by the Clonal Mosaic (CM) model is proposed. The CM model simulates cells’ reactions on arbitrary surface. By controlling the division, mutation and repulsion of cells, a regulated spatial arrangement of cells is formed. This arrangement of cells shows appealing result, which is comparable with those natural patterns observed from animal skin. However, a typical CM simulation process incurs high computational cost, where the distances among cells across a polygonal surface are measured and the movements of cells are constrained on the surface. In this framework, an approach is proposed to transform each of the original 3D geometrical planes of the surface into its Canonical Reference Plane Structure. This structure helps to simplify a 3D computational problem into a more manageable 2D problem. Furthermore, the concept of Local Relaxation is developed to optimally enhance the relaxation process for a typical CM simulation. The performances of the proposed solution methods have been verified with extensive experimental results.	algorithmic efficiency;computational problem;linear programming relaxation;ncsa mosaic;patterns in nature;performance;relaxation (approximation);simulation	How Teo;Kok Wong	2006	Journal of Computer Science and Technology	10.1007/s11390-006-0173-y	mathematical optimization;simulation;computer science;artificial intelligence;algorithm	Robotics	68.93024454644906	-48.57528557537229	154998
a9a44be1c284e6b83fda634db46958dd2d0c4631	a framework for open surgery simulation	surgery simulation	In this thesis we present a framework for the simulation of surgery procedures, like they occur during operations in open surgery. While various algorithms for simulating the deformation oforgans have been developed in the past, few tried to modify the underlying geometry. The geometric modification, however, is a very important component in every type of surgical simulation, since it is the modification in the tissue structures that constitutes the success of a surgical operation. This thesis focuses on an accurate simulation of tissue intersections ofarbitrary trajectories, as they are performed by a surgical scalpel. To provide a realistic simulation environment, further components, like physically-based tissue deformation and realistic haptic feedback are also integrated in our framework, as well as some surgical hooks that allow to open the resulting incisions. The representation ofour models by tetrahedral meshes allows to simulate volumetric effects, while still providing the basis for an efficient calculation of the tissue's deformation. The algorithm that ensures the consisteney of the tetrahedral mesh during cutring is based on a state machine which describes the topological cut pattern of each tetrahedron. This state machine concept enables an incremental construction of the tetrahedral subdivisions by applying state transitions, efficiently represented by preprocessed lookup table entries, A recursive continuation of this state machine makes it possible to even track very complex scalpel trajectories by a local refinement of the mesh structures. For the efficient registration of collisions berween surgical tools and the tissue structures, a collision detection algorithm determines possible intersections between the involved objects. Hierarchical and local recursive algorithms have been developed that keep the computational burden of the collision detection low and guarantee a consistent detection of all mesh inrersections even within heavily moving tissue. Two models represent the physical behavior of the tissue. An explicit finite .element method correctly simulates incornpressibiliry, whereas a simpler but faster damped mass spring systern handles large deformations more realistically. Both models are solved with implicit numerical methods bya parallelized solver that allows adaptive time steps for each individual node. Realistic haptic feedback ofour surgical tools is achieved by building a hierarchy ofsfmulation loops running at different update rates, and by physically modeling the various occurring friction forces. We demonstrated several surgical tasks with the resulting surgery simulation framework. The simulation shows very realistic seenarios and reacrs interactively for substantial mesh sizes while supporting real-time frame rates. In particular, the cut algorithm captivates with its geometrical accuraey, and its flexibility concerning topological modifications.		Daniel Bielser	2003		10.3929/ethz-a-004467520	computer-assisted surgery;simulation;geometric modeling;surgery;computer science	Robotics	69.85407914290572	-46.66554501693935	155802
b44ad0a4465efd062db77472febd81f5b14f3cbc	frequency tracking of resonant-like sounds from audio recordings of arterio-venous fistula stenosis	data recording;nonlinear filters;frequency modulation;kalman filters;audio recording;nonlinear filters audio recording blood vessels data recording diseases frequency modulation kalman filters medical signal processing;frequency modulation chirp power harmonic filters harmonic analysis blood flow vibrations mathematical model;resonant like sounds mean frequencies venous stenosis ekf frequency tracker data base recordings extended kalman filter time varying frequency signals synthetic bruits containing chirps time frequency plots frequency modulated signals severe stenosis tea kettle sounds time frequency domain arteriovenous fistula stenosis audio recordings;diseases;medical signal processing;blood vessels	"""In this work we study in time-frequency domain what has been termed """"tea-kettle"""" sounds found in audio recordings of arteriovenous fistulas with severe stenosis. These sounds appears as frequency modulated signals (chirps) in the time-frequency plots. We first developed a simple model for generating synthetic bruits containing chirps. Then we performed tracking of the time-varying frequency signals using an Extended Kaiman filter (EKF). Finally we analyzed a data base of recordings from 5 patients suffering from venous stenosis EKF frequency tracker, the results shows the chirps mean frequencies (fundamental) can vary in a rather wide range of frequencies from about 200 Hz to 600 Hz and that up to two harmonics can also be found in the considered cases."""	chirp;database;extended kalman filter;extended precision;medical ultrasound;modulation;synthetic data;time–frequency analysis	Pablo Vasquez Obando;Bengt Mandersson	2012	2012 IEEE International Conference on Bioinformatics and Biomedicine Workshops	10.1109/BIBMW.2012.6470237	frequency modulation;kalman filter;speech recognition;computer science	Robotics	80.5515876858225	-39.37749164684661	156050
0b023e6a8822d41e42bbbd56bbc4ae7605b94444	interactive pc texture-based volume rendering for large datasets	interactive visualization interactive pc volume rendering large datasets graphics hardware 3d texture mapping memory consumption;large datasets;large dataset;volume rendering;memory consumption;interactive visualization;computer graphic equipment;interactive pc;image texture;very large databases computer graphic equipment data visualisation image texture interactive systems rendering computer graphics;data visualisation;large scale;graphics hardware;rendering computer graphics hardware graphics decoding data visualization large scale systems data engineering prototypes computational modeling grid computing;on the fly;very large databases;rendering computer graphics;interactive systems;3d texture mapping	We present a novel technique for rendering large-scale volume datasets interactively on general purpose PC hardware. To circumvent the limited texture memory for texture based volume rendering, the dataset is partitioned into the bricks with reasonable size. The bricks are loaded to the graphics hardware dynamically and rendered using 3D texture mapping. During the rendering only one brick resides on the texture memory. Additionally, the sophisticated PC graphics hardware functionality is utilized to estimate the gradient on the fly avoiding the huge memory consumption in previous approaches. Using a prototype implementation of the algorithm, we are able to perform fast data loading and interactive visualization for the large datasets on a single standard PC	algorithm;gradient;graphics hardware;interactive visualization;interactivity;on the fly;prototype;texture mapping;texture memory;volume rendering	Jie Zheng;Hongbing Ji;Wanhai Yang	2006	First International Conference on Innovative Computing, Information and Control - Volume I (ICICIC'06)	10.1109/ICICIC.2006.302	image texture;computer vision;interactive visualization;3d rendering;computer hardware;rendering;computer science;texture atlas;real-time rendering;texture memory;graphics hardware;volume rendering;software rendering;computer graphics (images)	Visualization	69.02209025072358	-51.25515019235287	156098
1f552314f37bf2b1210bedc14df40589c2db2e48	controlling the opacity of a building envelope by a triangular two-color two-dimensional cellular automaton		This paper presents the system based on cellular automata (CA) for controlling the average opacity of any triangulated surface, in particular serving as a building envelope. The concept is based on the emergent and modular qualities of CAs and is proposed for a practical application in the field of architecture. The concept of triangular cellular automata (TCA) is explained, followed by application of totalistic (tTCA) and semi-totalistic (stTCA) TCA on an imperfect test mesh, that is a mesh with voids and nodes of various degrees. Preliminary analysis of the behavior of these TCAs at various types of initial conditions in the context of shading purposes is presented.	cellular automaton	Machi Zawidzki;Katsuhiro Nishinari	2012		10.1007/978-3-642-33350-7_20	simulation;mathematics;geometry	HCI	72.5023644520084	-40.80475347504407	156460
0131cc11616ca63049a36086977006ec536fc08d	dynamic implicit surface tesselation	implicit surface;medical simulation;bsp trees;real time;culling;virtual reality walkthrough;clipping;polygonal approximation;graphics;pipeline	This paper presents a method for quickly computing polygonal approximations of dynamic implicit surfaces: when the shape of a surface is changed, its polygonal approximation is updated in real-time. The algorithm relies on the fact that moving a control point only affects the part of the surface within that point’s radius of influence. A real-time environment for our algorithm is provided by a medical simulator where human organs modelled using implicit surfaces are constantly deformed and have to be displayed at an interactive frame rate.	algorithm;approximation;control point (mathematics);implicit surface;real-time clock;simulation	Max Froumentin;Eric Varlet	1997		10.1145/261135.261151	medical simulation;binary space partitioning;computer vision;simulation;computer science;clipping;graphics;culling;pipeline;computer graphics (images)	Robotics	68.71804947134109	-49.101311390772224	157272
a42ef64ddea5c0f8b5eb1343db4993355184e8c2	handle-aware isolines for scalable shape editing	low frequency;rigidity aware;handle aware;shape deformation;isolines;nonlinear problem;scalar field;mesh deformation;scalable shape editing;harmonic fields;structural similarity	Handle-based mesh deformation is essentially a nonlinear problem. To allow scalability, the original deformation problem can be approximately represented by a compact set of control variables. We show the direct relation between the locations of handles on the mesh and the local rigidity under deformation, and introduce the notion of  handle-aware rigidity . Then, we present a reduced model whose control variables are intelligently distributed across the surface, respecting the rigidity information and the geometry. Specifically, for each handle, the control variables are the transformations of the isolines of a harmonic scalar field representing the deformation propagation from that handle. The isolines constitute a virtual skeletal structure similar to the bones in skinning deformation, thus correctly capturing the low-frequency shape deformation. To interpolate the transformations from the isolines to the original mesh, we design a method which is local, linear and geometry-dependent. This novel interpolation scheme and the transformation-based reduced domain allow each iteration of the nonlinear solver to be fully computed over the reduced domain. This makes the per-iteration cost dependent on only the number of isolines and enables compelling deformation of highly detailed shapes at interactive rates. In addition, we show how the handle-driven isolines provide an efficient means for deformation transfer without full shape correspondence.	contour line;scalability	Oscar Kin-Chung Au;Hongbo Fu;Chiew-Lan Tai;Daniel Cohen-Or	2007	ACM Trans. Graph.	10.1145/1276377.1276481	scalar field;topology;structural similarity;mathematics;geometry;low frequency;contour line;quantum mechanics	Graphics	68.599643213558	-45.53821622334158	157328
9fedd35d5e68f0dba2b2736b74d458ace919c0d9	real-time virtual pipes simulation and modeling for small-scale shallow water		We propose an approach for real-time shallow water simulation, building upon the virtual pipes model with multi-layered heightmaps. Our approach introduces the use of extended pipes which resolve flow through fully-flooded passages, which is not possible using current multi-layered techniques. We extend the virtual pipe method with a physically-based viscosity model that is both fast and stable. Our viscosity model is integrated implicitly without the expense of solving a large linear system. The liquid is rendered as a triangular mesh surface built from a heightmap. We propose a novel surface optimization approach that prevents interpenetrations of the liquid surface with the underlying terrain geometry. To improve the realism of small-scale scenarios, we present a meniscus shading approach that adjusts the liquid surface normals based on a distance field. Our approach runs in real time on various scenarios of roughly 10 x 10 cm at a resolution of 0.5 mm, with up to five layers. CCS Concepts •Computing methodologies → Physical simulation;	distance transform;dynamical simulation;heightmap;linear system;mathematical optimization;normal (geometry);real-time computing;real-time locating system;real-time transcription;shading	François Dagenais;Julián Guzman;Valentin Vervondel;Alexander Hay;Sébastien Delorme;David Mould;Eric Paquette	2018		10.2312/vriphys.20181067	computing methodologies;waves and shallow water;geology;marine engineering	Visualization	70.3910481397419	-47.90412107835858	157733
5671bd5d98ed7f32d4e0cf6d52aad3b8f4f8f396	smooth patching of refined triangulations	box splines;mesh generation	This paper presents a simple algorithm for associating a smooth, low-degree polynomial surface with triangulations whose extraordinary mesh nodes are separated by sufficiently many ordinary, 6-valent mesh nodes. Output surfaces are at least tangent continuous and are C2 sufficiently far away from extraordinary mesh nodes; they consist of three-sided Bézier patches of degree 4. In particular, the algorithm can be used to skin a mesh generated by a few steps of Loop's generalization of three-direction box-spline subdivision.	algorithm;box spline;bézier curve;polynomial;subdivision surface	Jörg Peters	2001	ACM Trans. Graph.	10.1145/383745.383746	mesh generation;mathematical optimization;topology;computer science;mathematics;geometry;laplacian smoothing;t-vertices	Graphics	69.43033673975357	-42.34275385421182	157806
a7f547bbdf768738b49cc670fbdefef02e4bac13	blind deconvolution: a matter of norm	minimisation;spectroscopy;computer aided analysis;homework;blind deconvolution;spectroscopy computing;spectrometers;spectrum;matrix algebra;error analysis;deconvolution;mathematical;signal reconstruction;error analysis deconvolution matrix algebra spectroscopy;error matrix blind deconvolution spectroscopy problem true spectrum reconstruction;mathematical spectrometers homework;deconvolution least squares approximation spectroscopy differential equations vectors minimization methods linear systems least squares methods size control;toeplitz matrices	We continue the spectroscopy problem from the last issue, trying to reconstruct a true spectrum from an observed one. Again, we'll use blind deconvolution, but this time we'll impose some constraints on the error matrix E, leading to a more difficult problem to solve but often a more useful reconstruction.	blind deconvolution	Dianne P. O'Leary	2005	Computing in Science and Engineering	10.1109/MCSE.2005.27	signal reconstruction;spectrum;minimisation;mathematical optimization;spectroscopy;computer science;deconvolution;spectrometer;mathematics;blind deconvolution;quantum mechanics;statistics	DB	82.88678150044068	-44.86408887044115	158064
23b2c0e86fdec3039bb2b09062aa82a60d8a3f68	adaptive sampling in three dimensions for volume rendering on gpus	image sampling;scientific visualisation;data decompression;image resolution;data compression;three dimensions;volume rendering;computer graphic equipment;large volumetric data set direct volume rendering;large data sets;graphical programming;data visualisation;adaptive gpu based data sampling;graphics hardware;compact representation;image representation;image quality;gpu based volume rendering algorithm;adaptive sampling;volume visualization;programmable graphics hardware;sampling methods image sampling rendering computer graphics data visualization hardware bandwidth computer graphics data structures costs grid computing;graphics memory;volumetric data compressed representation;rendering computer graphics;rendering computer graphics computer graphic equipment data compression data visualisation image representation image resolution image sampling;direct volume rendering;graphics programming unit;volume visualization gpu based volume rendering algorithm large volumetric data set direct volume rendering programmable graphics hardware graphics memory main memory data transfer graphics programming unit data decompression image quality volumetric data compressed representation adaptive gpu based data sampling scientific visualisation;main memory;data transfer	Direct volume rendering of large volumetric data sets on programmable graphics hardware is often limited by the amount of available graphics memory and the bandwidth from main memory to graphics memory. Therefore, several approaches to volume rendering from compact representations of volumetric data have been published that avoid most of the data transfer between main memory and the graphics programming unit (GPU) at the cost of additional data decompression by the GPU. To reduce this performance cost, adaptive sampling techniques were proposed; which are, however, usually restricted to the sampling in view direction. In this work, we present a GPU-based volume rendering algorithm with adaptive sampling in all three spatial directions; i.e., not only in view direction but also in the two perpendicular directions of the image plane. This approach allows us to reduce the number of samples dramatically without compromising image quality; thus, it is particularly well suited for many compressed representations of volumetric data that require a computational expensive GPU-based sampling of data.	3d projection;adaptive sampling;algorithm;application programming interface;computation;computer data storage;data compression;data structure;glossary of computer graphics;grammar-based code;graphics hardware;graphics processing unit;image plane;image quality;image resolution;opengl;oracle database;oracle machine;oversampling;pixel;random access;sampling (signal processing);viewing frustum;volume rendering;z-buffering	Martin Kraus;Magnus Strengert;Thomas Klein;Thomas Ertl	2007	2007 6th International Asia-Pacific Symposium on Visualization	10.1109/APVIS.2007.329285	cuda pinned memory;computer vision;tiled rendering;computer hardware;rendering;computer science;real-time computer graphics;texture memory;volume rendering;software rendering;computer graphics (images)	Visualization	68.54722084396022	-51.43274525632347	158640
2a55fe1741211046779078ab935127cd12667146	normality and correlation coefficient in estimation of insulators’ spectral signature	histograms;frequency 1 ghz insulator spectral signature contamination state statistical signal processing radio frequency signals corona effect histograms correlation coefficient statistical methods normality gaussianity test pollution state voltage 8 kv;pollution measurement;signal processing glass insulator contamination;insulators correlation coefficient histograms transmission line measurements correlation glass pollution measurement;glass;transmission line measurements;insulators;correlation;correlation coefficient;normality test correlation coefficient high voltage histograms insulators	This work aims to classify the contamination state in insulators using statistical signal processing approaches. When subjected to high voltage, insulators can radiate radio frequency signals (corona effect). The histograms, normality, and correlation coefficient statistical methods are used to estimate the pollution state in glass insulators when subjected to 8 kV. It is shown that in particular situations the histograms can be used to distinguish clean and dirty insulators. The histogram limitation analysis can be improved using the correlation coefficient and the normality or Gaussianity test. Indeed, it is shown that using these parameters into an analysis per sub-bands, it is possible to estimate the pollution state of the insulators. That is, the analysis using these tools checks if the insulators spectra under test are noticeably different from the clean one, used as reference. It is achieved eliminating the fast variation of the correlation coefficient based on the amplitude and width of the peaks. The tests were done up to the frequency of 1 GHz using measured data.	algorithm;coefficient;dirty data;glass;radiate;radio frequency;relevance;statistical signal processing;topological insulator	Glauco Fontgalland;Haslan J. G. Pedro	2015	IEEE Signal Processing Letters	10.1109/LSP.2015.2390638	insulator;histogram;mathematics;glass;correlation;statistics	ML	82.05186827273556	-42.44988401522773	158714
049292fbfe721a00a4b16325ed3c801c434cb766	meshing non-uniformly sampled and incomplete data based on displaced t-spline level sets	bilateral filtering;level sets;maps;topology;spline;piecewise smooth mesh;nonuniformly sampled data;t spline;mesh reconstruction piecewise smooth mesh nonuniformly sampled data incomplete data displaced t spline level sets unorganized data points topology marching triangulation method feature preserving bilateral filters;piecewise smooth;mesh reconstruction;level set;level set surface reconstruction image reconstruction topology spline surface cracks clouds scattering computer vision shape;level sets mesh reconstruction point cloud displacement maps t spline;displacement maps;scattering;surface reconstruction;splines mathematics;computer vision;displaced t spline level sets;incomplete data;shape;image reconstruction;clouds;spline function;feature preservation;marching triangulation method;displacement;feature preserving bilateral filters;point cloud;topology image reconstruction mesh generation splines mathematics;mesh generation;surface cracks;unorganized data points	We propose a new method for constructing a piecewise smooth mesh from a set of unorganized data points, which may be non-uniformly sampled, noisy, and even containing holes. The method is based on the construction of an implicit representation of the surface, by using smooth (C2 in our case) T-spline scalar functions. We first generate the T- spline control grid, and use an evolution process such that the resulting T-spline level sets capture the topology and outline of the object to be reconstructed. The initial mesh with high quality is obtained from the implicit T-spline function through the marching triangulation method. Then we project each data point to the initial mesh, and get a scalar displacement field. Detailed features will be captured by the displaced mesh. We also propose an additional evolution process, which combines data-driven velocities and feature- preserving bilateral filters, in order to reproduce sharp features.	bilateral filter;data point;displacement mapping;display resolution;marching cubes;spline (mathematics);t-spline	Huaiping Yang;Bert Jüttler	2007	IEEE International Conference on Shape Modeling and Applications 2007 (SMI '07)	10.1109/SMI.2007.26	computer vision;mathematical optimization;mathematics;geometry	Visualization	69.87387159074056	-44.53099106383008	158831
d06c101aed3ecfdf8e03f823db8441b541703288	a fast simulation method using overlapping grids for interactions between smoke and rigid objects	types of simulation animation;i 6 3 simulation and modeling;simulation methods;i 3 7 computer graphics;i 6 8 simulation and modeling;three dimensional graphics and realism;three dimensional graphics and realism animation;applications	Recently, many techniques using computational fluid dynamics have been proposed for the simulation of natural phenomena such as smoke and fire. Traditionally, a single grid is used for computing the motion of fluids. When an object interacts with a fluid, the resolution of the grid must be sufficiently high because the shape of the object is represented by a shape sampled at the grid points. This increases the number of grid points that are required, and hence the computational cost is increased. To address this problem, we propose a method using multiple grids that overlap with each other. In addition to a large single grid (a global grid) that covers the whole of the simulation space, separate grids (local grids) are generated that surround each object. The resolution of a local grid is higher than that of the global grid. The local grids move according to the motion of the objects. Therefore, the process of resampling the shape of the object is unnecessary when the object moves. To accelerate the computation, appropriate resolutions are adaptively-determined for the local grids according to their distance from the viewpoint. Furthermore, since we use regular (orthogonal) lattices for the grids, the method is suitable for GPU implementation. This realizes the real-time simulation of interactions between objects and smoke.	communications of the acm;computation;computational complexity theory;computational fluid dynamics;glossary of sudoku;graphics processing unit;interaction;overlap–add method;real-time clock;resampling (statistics);sampling (signal processing);simulation	Yoshinori Dobashi;Yasuhiro Matsuda;Tsuyoshi Yamamoto;Tomoyuki Nishita	2008	Comput. Graph. Forum	10.1111/j.1467-8659.2008.01145.x	computer vision;grid file;simulation;computer science;alpha-numeric grid;theoretical computer science;grid;information technology;computer graphics (images)	Robotics	69.49927226951566	-49.511835390472534	158891
f567132e4e08afc1250740f9c492330845d5539d	a fast and complete convex-hull algorithm architecture based on ellipse and elastic ellipse methods	eight directions;computational geometry;extreme points;ellipse method;convex hull	The number of inner points excluded in an initial convex hull (ICH) is vital to the e±ciency getting the convex hull (CH) in a planar point set. The maximum inscribed circle method proposed recently is e®ective to remove inner points in ICH. However, limited by density distribution of a planar point set, it does not always work well. Although the a±ne transformation method can be used, it is still hard to have a better performance. Furthermore, the algorithm mentioned above fails to deal with the exceptional distribution: the gravity centroid (GC) of a planar point set is outside or on the edge formed by the extreme points in ICH. This paper considers how to remove more inner points in ICH when GC is inside of ICH and completely process the case which mentioned above. Further, we presented a complete algorithm architecture: (1) using the ellipse and elasticity ellipse methods (EM and EEM) to remove more inner points in ICH and process the cases: GC is inside or outside of ICH. (2) Using the traditional	algorithm;approximation;convex hull;elasticity (data store);gravity pipe	Xue Gang Wu;Bin Fang;Yuan Yan Tang;Patrick Shen-Pei Wang	2013	IJPRAI	10.1142/S0218001413540086	extreme point;mathematical optimization;combinatorics;computational geometry;convex hull;mathematics;geometry	Robotics	70.48690822914759	-38.4826463064411	159009
712fee4e7fc5ff7021ea4cf9c7d7448d4ac03fd1	functional curve fitting algorithm via multi-heterogeneous data curve	measurement uncertainty;hilbert space;r tree lebesgue measure hilbert space mildew index curve fitting;environmental measurement parameters functional curve fitting algorithm multiheterogeneous data curve multiheterogeneous parameters linear relations nonlinear relations infinite dimensional hilbert space autonomous learning base temporal spatial microscale mildew indices temperature factor humidity factor wind force factor water vapor pressure reference coordinate base multiscale functional fitting tree r tree lebesgue similarity measure algorithm tree nodes environmental subspace parent node environmental characteristic mildew curve son node mildew curves lebesgue multidimensional matrix envelop curve algorithm;temperature measurement;curve fitting;wireless sensor networks;measurement uncertainty temperature measurement wireless sensor networks curve fitting hilbert space;tree data structures curve fitting environmental science computing hilbert spaces learning artificial intelligence	Complex computing based on multi-heterogeneous parameters can utilize the Lebesgue measure to measure their linear and non-linear relations in infinite dimensional Hilbert space, which can characterize those different dimensional characteristics in a set of autonomous learning base. The temporal-spatial micro-scale mildew indices can be fitted by using four heterogeneous parameters, including temperature, humidity, wind force, and water vapor pressure. This paper proposes a fitting process of heterogeneous parameter curves, which is based on a common reference coordinate base. The multi-scale functional fitting tree is constructed by R*-tree and the Lebesgue similarity measure algorithm. When the similarity of two tree nodes representative of similar environmental sub-space exceeds a given threshold, these nodes can be combined into a parent node, which represents the environmental characteristic of combined bigger-space and its mildew curve is the fitted result of all the son-node mildew curves. The curve similarity measurement based on the Lebesgue multi-dimensional matrix and the functional curve generating process of multi-heterogeneous data curves are proposed in the micro-space mildew index fitting example. The boundary finding of fitting curves can be realized by envelop curve algorithm. Based on two years environmental measurement parameters of ancient dwellings, the mildew index comparison of indoor and outdoor can be obtained. The experimental results show that the multi-heterogeneous curve fitting algorithm is effective and the mildew indices of indoor and outdoor have cyclical differences.	algorithm;autonomous robot;complexity index;curve fitting;hilbert space;micro-space;nonlinear system;similarity measure;tree (data structure)	Laixin Shen;Fan Yang;Xuefei Wang;Paul Comerford	2017	IEEE Access	10.1109/ACCESS.2016.2643679	mathematical optimization;mathematical analysis;wireless sensor network;hilbert r-tree;temperature measurement;mathematics;geometry;statistics;measurement uncertainty;curve fitting;hilbert space	Robotics	77.34743798458977	-45.330953211455565	159378
05123b6bfb882fa1a356b00f91459d4b646a2837	high fidelity haptic rendering of stick-slip frictional contact with deformable objects in virtual environments using multi-rate simulation	frictional contact;time scale;haptic interfacing;human computer interaction;stick slip frictional contact;stick slip friction high fidelity haptic rendering stick slip frictional contact virtual environment multirate simulation human computer interaction haptic interfacing graphical deformable object simulations force feedback collision detection virtual tool rolling contact;medical simulation;stick slip friction;virtual reality;virtual reality force feedback haptic interfaces human computer interaction medical computing rendering computer graphics rolling friction;haptic interfaces virtual environment deformable models computational modeling medical simulation force feedback usa councils friction surgery robotics and automation;deformable models;usa councils;high fidelity haptic rendering;humancomputer interaction;medical computing;deformable objects;graphical deformable object simulations;force feedback;computational modeling;collision detection;haptic rendering;surgery;multirate simulation;virtual environment;haptic interfaces;rendering computer graphics;friction;user interaction;robotics and automation;rolling contact;rolling friction;virtual tool;haptic interface	An increasingly common new modality in human-computer interaction is haptic interfacing, especially in the field of medical simulation. The order-of-magnitude difference in update rates between graphical deformable object simulations and haptic interfaces can be bridged using local low-order approximations. However, providing force feedback using local models complicates collision detection and response with the virtual tool, since the user interacts with lower-order proxies rather than the full simulated objects. A novel approach focusing on rolling contact with stick-slip friction is presented where all collision detection and response with the virtual tool is performed at the local level at the haptic time-scale, utilizing linearized low-order local models that approximate the behavior of the full model for the short time steps and small deformations involved.	approximation algorithm;collision detection;graphical user interface;haptic technology;human–computer interaction;modality (human–computer interaction);simulation;virtual reality	Paul Jacobs;Murat Cenk Cavusoglu	2007	Proceedings 2007 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2007.363774	medical simulation;computer vision;simulation;computer science;engineering;artificial intelligence;virtual reality;haptic technology;computer graphics (images)	Robotics	70.19152356998643	-47.186088415657636	160958
660865327a297ca92e5fec22849ad695a686e1c2	advanced hierarchical spherical parameterizations		Computing spherical parameterizations for genus-zero closed surfaces is a fundamental task for geometric processing and computer graphics. Existing methods usually suffer from a lack of practical robustness or poor quality. In this paper, we present a practically robust method to compute high-quality spherical parameterizations with bijection and low isometric distortion. Our method is based on the hierarchical scheme containing mesh decimation and parameterization refinement. The practical robustness of our method relies on two novel techniques. The first one is a flat-to-extrusive decimation strategy, which contains two decimation error metrics to alleviate the difficulty of further mesh refinement. The second is a flexible group refinement technique that consists of flexible vertex insertion and efficient volumetric distortion minimization to control the maximum distortion. We convert the task of volumetric distortion minimization to one of tetrahedral mesh improvement to make the vertices distribute uniformly for efficient refinement. Compared with state-of-the-art methods, our method is more practically robust and possesses better mapping qualities. We demonstrate the efficacy of our method in spherical parameterization computations on a data set containing over five thousand complex models.	academy;adaptive mesh refinement;algorithm;align (company);alignment;arabic numeral 0;clinical act of insertion;computation;computer graphics;decimation (signal processing);deep learning;distortion;exhibits as topic;genus;genus (mathematics);grammar-based code;incised wound;insertion mutation;isometric projection;natural science disciplines;refinement (computing);review [publication type];robustness (computer science);vertex;anatomical layer;biologic segmentation	Xin Hu;Xiao-Ming Fu;Ligang Liu	2018	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2017.2704119	theoretical computer science;mathematical optimization;robustness (computer science);decimation;distortion;vertex (geometry);parametrization;nonlinear distortion;computer science;mesh generation;computer graphics	Visualization	68.76007234206652	-44.99125520919133	161061
7fdce41dd04e43ec76ce5e0e05f03ac13b4f23e6	interactive visualization of vector data in unstructured volumes	vitesse;velocity;trajectoire;champ vectoriel;particule;ecoulement;particles;computer graphics;interactive visualization;simulacion numerica;flujo;tracing;dinamica fluido;trajectories;systeme conversationnel;algorithme;visualization;vector fields;visualisation;interactive system;simulation numerique;tracage;sistema conversacional;algorithms;fluid dynamics;vector data;dynamique fluide;infographie;trazado;flow fluid;numerical simulation	Particle-based flow visualization techniques are used for the investigation of huge, complex data sets from computational fluid dynamics (CFD). Particle tracing requires the interpolation of local velocity inside a discretized data set and the numerical integration, to find succeeding spatial positions of the moving particles. More and more often, CFD-data is computed on unstructured Finite Element grids, which makes interpolation and integration difficult. One strategy is to resample the unstructured volume data to a regular one. In contrast to this, this article describes how particle tracing can be performed directly inside unstructured grids, thereby preserving the quality and size of the original data for visualization: The basic idea is to perform the integration in the computational space of the individual cells of the grid. This leads to a faster computation of the particle's new position and an easier determination of the corresponding new cell into which the particle has moved. The proposed algorithms allow an interactive investigation with real-time computation of particle trajectories of huge data sets that are organized on unstructured grids.		Thomas Frühauf	1994	Computers & Graphics	10.1016/0097-8493(94)90117-1	simulation;visualization;interactive visualization;computer science;theoretical computer science;computer graphics (images);fluid dynamics	Visualization	71.13731037871814	-50.40412148614737	161110
d185bd2348c3df33e5c4335de12b31d76d0c5353	comparison of measures of time-frequency distribution optimization	quantitative measures;gabor transform;choi williams distribution;short time fourier transform;jones parks measure;s method;wigner ville distribution;time frequency representations;renyi entropy;mean squared error	This article describes the problems of time-frequency representation optimization. The goal is to achieve compromise between good resolution and reduced cross terms. The quantitative measures used for this optimization are described. For the case of a simulated signal a measure based on mean squared error is proposed. The proposed method compares an optimized result to the ideal representation obtained as sum of Wigner-Ville distributions of mono-component signals. Three simulated signals are used to compare optimization of Choi-Williams Distribution, Gabor spectrogram, Short-time Fourier transform and S-method. The optimization is realized on the basis of four quantitative measures: Rényi entropy, Jones-Parks measure, Stanković measure and proposed method. Results show problematic use of Jones-Parks measure for optimization and comparable optimization of all tested time-frequency distributions by means of three usable measures. For practical use with real signal, S-method with Rényi or Stanković measure is recommended.	chirp;gabor wavelet;jones calculus;lennard-jones potential;mathematical optimization;mean squared error;rényi entropy;short-time fourier transform;simulation;spectrogram;time–frequency representation	Stanislav Pikula;Petr Benes	2016	2016 8th International Congress on Ultra Modern Telecommunications and Control Systems and Workshops (ICUMT)	10.1109/ICUMT.2016.7765378	gabor transform;mathematical analysis;rényi entropy;short-time fourier transform;calculus;mathematics;mean squared error;statistics	Robotics	78.35935542247174	-39.66076464222866	161157
0bac20a0740a55277036a5fc73453a11c2b0d6c7	edge-sharpener: recovering sharp features in triangulations of non-adaptively re-meshed surfaces	new edge-sharpener filter;non-adaptively re-meshed surface;additional information;wrapper9 remeshing-based compressor;new vertex;incident triangle;sharp feature;chamfer edge;l2 error;sharp edge;significant error;triangle mesh	3D scanners, iso-surface extraction procedures, and several recent geometric compression schemes sample surfaces of 3D shapes in a regular fashion, without any attempt to align the samples with the sharp edges and corners of the original shape. Consequently, the interpolating triangle meshes chamfer these sharp features and thus exhibit significant errors. The new Edge-Sharpener filter introduced here identifies the chamfer edges and subdivides them and their incident triangles by inserting new vertices and by forcing these vertices to lie on intersections of planes that locally approximate the smooth surfaces that meet at these sharp features. This post-processing significantly reduces the error produced by the initial sampling process. For example, we have observed that the L error introduced by the SwingWrapper remeshing-based compressor can be reduced down to a fifth by executing Edge-Sharpener after decompression, with no additional information.	adaptive sampling;align (company);approximation algorithm;bilateral filter;chamfer;computer graphics (computer science);data compression;distortion;extrapolation;ibm notes;interpolation;isosurface;marching cubes;marco dorigo;olap cube;reverse engineering;sampling (signal processing);sierpinski triangle;vertex (geometry);video post-processing	Marco Attene;Bianca Falcidieno;Michela Spagnuolo;Jarek Rossignac	2003			sampling;combinatorics;computer science;technical report;triangle mesh;mathematics;geometry	Graphics	68.53040618141331	-44.35363130504969	161495
164c00ad9958264d545dc9b0cb6fa40ff768cde7	real-time particle-based simulation on gpus	real time;computer graphic;physically based simulation;real time application	As physical laws govern the motion of objects around us, a physically-based simulation plays an important role in computer graphics. For instance, the motion of a fluid, which is difficult to generate by hand, can be produced by solving the governing equations. Acceleration of a simulation is one of the most important research themes because the speed and stability of a simulation are essential for real-time applications.		Takahiro Harada;Masayuki Tanaka;Seiichi Koshizuka;Yoichiro Kawaguchi	2007		10.1145/1280720.1280778	physically based animation;simulation;computer science;theoretical computer science;computer graphics (images)	Graphics	70.9985061358938	-48.49309443599809	161923
229fd8391a64521064bc3db7476b9fae0ad869ec	piecewise regular meshes: construction and compression	compression algorithm;triangle mesh	We present an algorithm which splits a 3D surface into reliefs, relatively flat regions that have smooth boundaries. The surface is then resampled in a regular manner within each of the reliefs. As a result, we obtain a piecewise regular mesh (PRM), having a regular structure on large regions. Experimental results show that we are able to approximate the input surface with the mean square error of about 0.01− 0.02 per cent of the diameter of the bounding box without increasing the number of vertices. We introduce a compression scheme tailored to work with our remeshed models and show that it is able to compress them losslessly (after quantizing the vertex locations) without significantly increasing the approximation error using about 4 bits per vertex of the resampled model.	approximation algorithm;approximation error;lossless compression;mean squared error;minimum bounding box;regular expression;statistical relational learning	Andrzej Szymczak;Jarek Rossignac;Davis King	2002	Graphical Models	10.1006/gmod.2002.0577	data compression;mathematical optimization;combinatorics;discrete mathematics;computer science;triangle mesh;mathematics;geometry	ML	70.67592754989514	-43.95864265258406	162070
fc3fdeb6f106fde780ec7ce17c86a966f1d779d7	validating nonlinear mixing models: benchmark datasets from vegetated areas		Our understanding of nonlinear mixing events in vegetated areas is currently hampered by a pertinent lack of well-validated datasets. Most quantification and modeling efforts are based on theoretical assumptions or indirect empirical observations. In this study, a physically based ray tracer was used to create simulated hyperspectral datasets of vegetative systems. This model incorporates multiple scattering effects, and nonlinear mixing behavior can be observed in the rendered data. The main benefit of the ray-tracer is that we were able to demonstrate with in situ measurements that both the nature and the intensity of the nonlinear mixing events are realistically modeled. Different ray-tracer datasets will be made available to the wider scientific community as a benchmark dataset to test and validate new and existing unmixing methodologies. In this contribution, we would like to present the structure of these datasets, and show how they can be used to evaluate nonlinear mixing models. In addition, and maybe even more important, we would like to draw the attention to the limitations of the data, as well point out the assumptions made in the construction of the data.	benchmark (computing);consistency model;nonlinear system identification;persistent vegetative state;ray tracing (graphics);relevance	Laurent Tits;Ben Somers;Jan Stuckens;Pol Coppin	2014	2014 6th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)	10.1109/WHISPERS.2014.8077645	econometrics;mathematical optimization	ML	76.3394829815976	-47.499285040351154	162275
e765aa50247589b5e513021293d30194316d4865	interactive rendering for large-scale mesh based on mapreduce	cloud computing large scale mesh visualization parallel rendering mapreduce;rendering computer graphics cloud computing computational modeling data visualization parallel processing;web services big data cloud computing data visualisation interactive systems online front ends parallel programming rendering computer graphics;big data authoring mapreduce cloud computing platform adaptive parallel rasterization method data format enhanced layered depth image eldi triangle based strategy pixel based strategy pipeline processing data transfer reduction web service interactive rendering large scale interactive mesh visualization browsers mobile devices work stations complex mesh big data exploration big data analysis;computational modeling;data visualization;rendering computer graphics;parallel processing;cloud computing	We present an interactive rendering solution that aims at visualizing large-scale mesh with hundred millions of triangles up to extremely high resolution on cloud computing platform. Firstly, we propose a novel adaptive parallel rasterization method based on MapReduce, whose results are stored in a data format called enhanced layered depth image (ELDI). In order to fully apply the powerful capabilities of cloud computing, our proposed method integrates pixel and triangle based strategies in one processing pipeline so as to significantly reduce data transfer between Map and Reduce steps. Consequently, a light-weight Web service for interactive rendering is also proposed to couple with the rasterization step, which enables user to interactively visualize large-scale mesh via browser on both mobile devices and work stations. According to demonstrated examples in the paper, once the rasterization results are obtained, users can freely adjust visualizing effects of complex mesh. Our rendering approach is promising for big data exploration, analysis or authoring.	big data;browsing;cloud computing;disk image;image resolution;interactivity;intermediate representation;interpolation;mapreduce;mobile device;pixel;rasterisation;rendering (computer graphics);texture mapping;web service	Hongxin Zhang;Biao Zhu;Wei Chen	2013	2013 International Conference on Computer-Aided Design and Computer Graphics	10.1109/CADGraphics.2013.52	parallel processing;cloud computing;rendering;computer science;theoretical computer science;data-intensive computing;parallel rendering;real-time rendering;computational model;world wide web;data visualization;software rendering;computer graphics (images)	Visualization	69.22101715396128	-51.58087754972153	162456
9ffa0ae506503e1a794effc3b40b00ff0f840b69	radial hermite operators for scattered point cloud data with normal vectors and applications to implicitizing polygon mesh surfaces for generalized csg operations and smoothing	scattering smoothing methods isosurfaces surface fitting solid modeling data visualization computer graphics three dimensional displays surface reconstruction interpolation;interpolation;isosurface generation scattered point cloud data volume modeling polygon mesh surfaces boolean operation solid geometry smoothing algorithm filtering algorithm radial hermite operator normal vectors surface reconstruction;computer graphics;surface fitting;scattering;isosurfaces;surface reconstruction;smoothing methods;boolean operation;point clouds;vectors;three dimensional displays;polygonal meshes;solid modeling;data visualization;constructive solid geometry;polygon mesh;point cloud;mesh generation;volume modeling;solid modelling;polygon mesh surface reconstruction point clouds isosurfaces;interpolation mesh generation solid modelling surface reconstruction vectors surface fitting	We describe a new technique for fitting scattered point cloud data. Given a scattered point cloud of 3D data points and associated normal vectors, our new method produces an implicit volume model whose zero level isosurface interpolates the given points and associated normal vectors. In this paper, we concentrate on certain application of these new volume modeling techniques. We take existing polygon mesh surfaces and use the present methods to construct implicit volume models for these surfaces. Implicit models allow for the application of Boolean operations on these surfaces through the techniques of constructive solid geometry. Also, standard wavelet and filter operators can be applied to the implicit volume model leading to effective smoothing and filtering algorithms which are simple to implement.	algorithm;constructive solid geometry;data point;interpolation;isosurface;normal (geometry);point cloud;polygon mesh;radial (radio);radial basis function;smoothing;wavelet	Gregory M. Nielson	2004	IEEE Visualization 2004	10.1109/VISUAL.2004.87	mathematical optimization;computer science;point cloud;mathematics;geometry;constructive solid geometry;data visualization;statistics;computer graphics (images)	Visualization	70.02826897000575	-44.521794134925564	162457
67d7e131531f48466eac71585288d4175fa688fa	shape-preserving, multiscale fitting of univariate data by cubic l1 smoothing splines	spacing;espacement;fonction propre;eigenfunction;interpolation;forma;espaciamiento;ajustamiento curva;methode echelle multiple;interpolacion;funcion propia;arbitrary shaped body;metodo escala multiple;polynomial;scattered data;tensor product;shape;cubic smoothing spline;smoothing;weighted sums;polinomio;alisamiento;preservation;ajustement courbe;multiscale method;esplin cubico;spline cubique;corps forme arbitraire;curve fitting;cuerpo forma arbitraria;multiscale;polynome;forme;data fitting;preservacion;lissage;smoothing spline;shape preservation;cubic spline	Bivariate cubic L1 smoothing splines are introduced. The coefficients of a cubic L1 smoothing spline are calculated by minimizing the weighted sum of the L1 norms of second derivatives of the spline and the 1 norm of the residuals of the data-fitting equations. Cubic L1 smoothing splines are compared with conventional cubic smoothing splines based on the L2 and 2 norms. Computational results for fitting a challenging data set consisting of discontinuously connected flat and quadratic areas by Csmooth Sibson-element splines on a tensor-product grid are presented. In these computational results, the cubic L1 smoothing splines preserve the shape of the data while cubic L2 smoothing splines do not.	bivariate data;coefficient;computation;cubic hermite spline;cubic function;curve fitting;smoothing spline;spline (mathematics);taxicab geometry;weight function	John E. Lavery	2000	Computer Aided Geometric Design	10.1016/S0167-8396(00)00025-X	mathematical optimization;smoothing spline;interpolation;mathematics;geometry;statistics;curve fitting;box spline	Graphics	69.10135494569911	-40.69477134415909	162794
0c58574a112ad7d5e65aa52b29c6c61ae8292b34	virtual rheoscopic fluids	nvidia geforce gtx 280;complex dynamics;nvidia geforce 8800 gtx;rheoscopic particles;paper;raytracing;shear flow;tensile stress;computer graphics 0 0 1 0 1 computer simulation 0 0 1 0 1 imaging three dimensional 0 0 1 0 1 models theoretical 0 0 1 0 1 rheology 0 0 1 0 1 user computer interface 0 0 1 0 1;reflectivity;application software;rheoscopic fluid;real time;particle reflectance;velocity field;computational fluid dynamics;3d simulation;visualization;flow visualisation;computational modeling;simulated fluid dynamics data;visualization technique;ellipsoidal particle dynamics;animation;data visualization;ray tracing;ellipsoidal particle dynamics rheoscopic fluid flow visualization tensor field visualization;nvidia;fluid dynamics computational modeling data visualization reflectivity tensile stress ray tracing computational fluid dynamics application software computer science animation;fluid dynamics;ray tracing virtual rheoscopic fluid simulated fluid dynamics data velocity field gradient visualization rheoscopic particles shear flow 3d velocity fields particle reflectance;3d velocity fields;virtual rheoscopic fluid;computer science;flow simulation;shear flow computational fluid dynamics flow simulation flow visualisation ray tracing reflectivity;velocity field gradient visualization;tensor field visualization;flow visualization	We present a visualization technique for simulated fluid dynamics data that visualizes the gradient of the velocity field in an intuitive way. Our work is inspired by rheoscopic particles, which are small, flat particles that, when suspended in fluid, align themselves with the shear of the flow. We adopt the physical principles of real rheoscopic particles and apply them, in model form, to 3D velocity fields. By simulating the behavior and reflectance of these particles, we are able to render 3D simulations in a way that gives insight into the dynamics of the system. The results can be rendered in real time, allowing the user to inspect the simulation from all perspectives. We achieve this by a combination of precomputations and fast ray tracing on the GPU. We demonstrate our method on several different simulations, showing their complex dynamics in the process.	aggregate data;align (company);birefringence;complex dynamics;drude particle;embedding;experiment;flow;glyph;gradient;graphics processing unit;hydrodynamics;imagery;inspiration function;large;liquid substance;precomputation;ray tracing (graphics);real-time computing;real-time transcription;shading;simulation;velocity (software development)	Florian Hecht;Peter J. Mucha;Greg Turk	2010	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2009.46	computational science;ray tracing;simulation;flow visualization;computer science;data visualization;computer graphics (images);fluid dynamics	Visualization	71.02405302316424	-49.13240542186948	163132
264ac113e97dd328acde2513c3de308e51992b24	extending neural networks for b-spline surface reconstruction	ajustement surface;curva bezier;modele geometrique;ajustamiento curva;groupe lie;surface fitting;surface reconstruction;b spline surface;tensor product;reconstruction surface;aproximacion esplin;estimation erreur;courbe bezier;error estimation;spline approximation;grupo lie;approximation spline;surface model;estimacion error;ajustement courbe;lie group;b spline;reconstruccion superficie;reseau neuronal;curve fitting;red neuronal;b splin;geometrical model;neural network;bezier curve;modelo geometrico	Recently, a new extension of the standard neural networks, the so-called functional networks, has been described [5]. This approach has been successfully applied to the reconstruction of a surface from a given set of 3D data points assumed to lie on unknown Bézier [17] and B-spline tensor-product surfaces [18]. In both cases the sets of data were fitted using Bézier surfaces. However, in general, the Bézier scheme is no longer used for practical applications. In this paper, the use of B-spline surfaces (by far, the most common family of surfaces in surface modeling and industry) for the surface reconstruction problem is proposed instead. The performance of this method is discussed by means of several illustrative examples. A careful analysis of the errors makes it possible to determine the number of B-spline surface fitting control points that best fit the data points. This analysis also includes the use of two sets of data (the training and the testing data) to check for overfitting, which does not occur here.	b-spline;bézier curve;control point (mathematics);curve fitting;data point;fits;multivariate interpolation;neural networks;overfitting;reconstruction conjecture	G. Echevarría;Andrés Iglesias;Akemi Gálvez	2002		10.1007/3-540-46080-2_32	tensor product;b-spline;topology;surface reconstruction;calculus;bézier curve;mathematics;geometry;lie group;artificial neural network;statistics;curve fitting	Vision	70.52986913511093	-41.12486504801971	163421
5048257b7480f548db22d194252ae60f07ab25c9	an annotated bibliography of scientific visualization. part 1		structure to the data that would suggest a more customized method or that would lead to degenerate cases. The author discusses and references point schemes (Shepard’s formula, radial interpolants, Hardy’s multiquadrics, thin plate splines), natural neighbour interpolation, tetrahedral schemes (polynomial and rational, including Boolean sums and convex combinations), simplicia1 schemes and multivariate splines. Some s ecial to ics	interpolation;polynomial;radial (radio);scientific visualization;thin plate spline	Gregory M. Nielson;Keith Voegele	1990	Journal of Visualization and Computer Animation	10.1002/vis.4340010206		Visualization	69.42149093915758	-41.72556911328364	163460
c124198d6fb605bd29a9a656b6f96f79eb87664d	point and tangent computation of tensor product rational bézier surfaces	producto tensorial;spline;interpolation;curva bezier;condiciones limites;algorithm performance;condition aux limites;geometrie algorithmique;esplin;interpolacion;computational geometry;produit tensoriel;convexite;parametrization;convexidad;parametrizacion;algorithme;tensor product;algorithm;aproximacion polinomial;courbe bezier;boundary condition;resultado algoritmo;approximation polynomiale;non uniform degree;performance algorithme;superficie;convexity preserving interpolant;surface;geometria computacional;evaluation;functional data;polynomial splines of nonuniform degree;evaluacion;convexity;semi local and global increase of degrees;local increase of degree;parametrisation;polynomial splines;polynomial approximation;bezier curve;algoritmo;c p interpolant	Abstract   In this paper we present a simple automatic algorithm for constructing  C  2  continuous planar parametric interpolants, which preserve the local-convexity information contained in the interpolation point set and the imposed boundary conditions. The algorithm exploits the shape-preserving capabilities of a special family of polynomial splines of non-uniform degree, introduced, for the case of functional data, in (Kaklis and Pandelis, 1990). The numerical performance of the algorithm is discussed for three data sets and various parametrizations.	bézier curve;computation	Thomas W. Sederberg	1995	Computer Aided Geometric Design	10.1016/0167-8396(94)00029-R	parametrization;mathematical optimization;topology;computational geometry;interpolation;mathematics;geometry	Theory	68.4846894247213	-40.22785029042462	163824
49736157d79684da18482e57318df4831cb4b936	nc milling tool path generation for arbitrary pockets defined by sculptured surfaces	machining;controle;fraisage;control proceso;usinage;computer aid;process control;asistencia ordenador;control;mecanizado;fresado;tool path generation;milling;assistance ordinateur;commande processus;check	In machining die cavities or mechanical parts, it is often necessary to remove material within a given boundary. Although this pocket cutting capability is implemented in many numerical control packages, most of them can handle only convex shaped pockets bounded by curves of limited types and numbers. A procedure has been developed to machine a pocket with a convex or concave free surface bounded by lines, circular arcs and free curves. The cutter location data are computed directly with better computational efficiency than normal, without using an iterative method.	concave function;convex function;cutter expansive classification;iterative method;numerical analysis	Yong Seok Suh;Kunwoo Lee	1990	Computer-Aided Design	10.1016/0010-4485(90)90092-Q	check;simulation;machining;engineering;process control;engineering drawing;scientific control;mechanical engineering	Robotics	68.70620993529543	-38.40825141992643	163848
6b823e6f8362a6aa890f1f2496ffa4eebc42b829	a comparison of two hilbert spectral analyses of heart rate variability	pulse frequency modulation;hilbert huang transform;heart rate variability;instantaneous frequency;spectrum;wavelet packet decomposition;integral pulse frequency modulation;hilbert huang spectrum;arrhythmia;time frequency analysis;wavelet transformation;empirical mode decomposition	The present paper compares the performance of two Hilbert spectral analyses when applied to a synthetic RR series from a nonstationary integral pulse frequency modulation model and to real RR series from a dataset of normal sinus arrhythmia. The Hilbert–Huang transformation based on empirical mode decomposition is compared to the presently introduced Hilbert–Olhede–Walden transformation based on stationary wavelet packet decomposition. The comparison gives consistent results pointing to a superior performance of the Hilbert–Olhede–Walden transformation showing 33–163 times smaller deviations when estimating the instantaneous frequency traces of the synthetic RR series. Artificial fluctuations caused by mode mixing in the Hilbert–Huang spectrum are seen in both the synthetic and real RR series. It can be concluded that the instantaneous frequencies and amplitudes estimated by the Hilbert–Huang transformation should be interpreted with caution when investigating heart rate variability.	confidence intervals;estimated;heart rate variability;hilbert transform;hilbert–huang transform;hoc (programming language);instantaneous phase;modulation;natural deduction;network packet;nonlinear system;rapid refresh;round-robin scheduling;silo (dataset);sinus - general anatomical term;small;stationary process;synthetic intelligence;tracing (software);wavelet packet decomposition;omacetaxine mepesuccinate	Espen Alexander Fürst Ihlen	2009	Medical & Biological Engineering & Computing	10.1007/s11517-009-0500-x	mathematical analysis;neuroscience;speech recognition;hilbert–huang transform;calculus;mathematics;hilbert spectral analysis;physics;quantum mechanics	HPC	79.56571098166816	-39.578516514165614	164037
19dfabd8b719ddb4d7047207fd8f3a83b3de78d4	research on application of the minimum error average fitting method in computer color matching	minimum error average fitting;pattern matching colour dyeing error statistics fabrics newton method;newton iteration method;color matching;helium;newton iteration;dyeing;helium data models;data model;newton iteration method color matching minimum error average fitting;newton iteration method minimum error average fitting method computer color matching dyeing concentration fabric color mathematical model rgb value;pattern matching;fabrics;error statistics;newton method;data models;colour	In order to get the prediction of dyeing concentration on the value of fabric color, this paper establishes the mathematical model of RGB value and dyeing concentration based on the measured data. And then the minimum error average fitting method is used to calculate the function to get the prediction. The Newton iteration method is also used in solving the equation after the measured RGB data is substituted. And finally the dyeing concentration we need is found.	iteration;mathematical model;newton's method	Bingsen Zhang;Yaping Shi;Houjun Yang;Xiaojie Liu;Anling Zhang	2011	Proceedings of 2011 IEEE International Conference on Intelligence and Security Informatics	10.1109/ISI.2011.5984100	mathematical optimization;computer science;newton's method;statistics	Robotics	82.50645687614981	-48.548152699302875	164196
d1ba1c6ed8f1db677433eadbdfabf3ddb5721abc	voxel-based fabrication through material property mapping: a design method for bitmap printing	digital fabrication;3d printing;voxels;prosthetics;bitmap printing;additive manufacturing	Wepresent a bitmap printingmethod and digital workflow usingmulti-material high resolution Additive Manufacturing (AM). Material composition is defined based on voxel resolution and used to fabricate a design object with locally varying material stiffness, aiming to satisfy the design objective. In this workflowvoxel resolution is set by theprinter’s native resolution, eliminating theneed for slicing andpath planning. Controlling geometry and material property variation at the resolution of the printer provides significantly greater control over structure–property–function relationships. To demonstrate the utility of the bitmap printing approach we apply it to the design of a customized prosthetic socket. Pressuresensing elements are concurrently fabricated with the socket, providing possibilities for evaluation of the socket’s fit. The level of control demonstrated in this study cannot be achieved using traditional CAD tools and volume-based AM workflows, implying that new CAD workflows must be developed in order to enable designers to harvest the capabilities of AM. © 2014 Elsevier Ltd. All rights reserved.	3d printing;additive model;bitmap;computer-aided design;image resolution;native resolution;printer (computing);voxel	Eugeni L. Doubrovski;E. Y. Tsai;D. Dikovsky;Jo M. P. Geraedts;Hugh Herr;Neri Oxman	2015	Computer-Aided Design	10.1016/j.cad.2014.05.010	3d printing;engineering;biological engineering;engineering drawing;computer graphics (images);mechanical engineering	HCI	74.64261768497187	-38.604820007999	164511
43de354f0d93f1aacbbb7c985db7399fb8a6a35e	multiple-reflection noise attenuation using adaptive randomized-order empirical mode decomposition	mos devices;spatial coherence;data processing;attenuation;petroleum;smoothing methods;noise reduction	We propose a novel approach for removing noise from multiple reflections based on an adaptive randomized-order empirical mode decomposition (EMD) framework. We first flatten the primary reflections in common midpoint gather using the automatically picked normal moveout velocities that correspond to the primary reflections and then randomly permutate all the traces. Next, we remove the spatially distributed random spikes that correspond to the multiple reflections using the EMD-based smoothing approach that is implemented in the  $f-x$  domain. The trace randomization approach can make the spatially coherent multiple reflections random along the space direction and can decrease the coherency of near-offset multiple reflections. The EMD-based smoothing method is superior to median filter and prediction error filter in that it can help preserve the flattened signals better, without the need of exact flattening, and can preserve the amplitude variation much better. In addition, EMD is a fully adaptive algorithm and the parameterization for EMD-based smoothing can be very convenient.	adaptive algorithm;amiga reflections;coherence (physics);hilbert–huang transform;median filter;noise (electronics);noise reduction;randomized algorithm;randomness;reflection (computer graphics);smoothing;tracing (software)	Wei Chen;Jianyong Xie;Shaohuan Zu;Shuwei Gan;Yangkang Chen	2017	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2016.2622918	attenuation;computer vision;mathematical optimization;data processing;computer science;noise reduction;mathematics;petroleum;statistics	Vision	81.17219964488228	-41.66111781143569	164634
e0d10b5c48a39cea7de65d25fddc1ef7c45c776a	generation of fractal curves and surfaces using ternary 4-point interpolatory subdivision scheme	generation of fractal curves and surfaces;ternary 4 point scheme;fractal properties;fractal mountains;interpolatory subdivision scheme	In this paper, the generation of fractal curves and surfaces along with their properties, using ternary 4-point interpolatory subdivision scheme with one parameter, are analyzed. The relationship between the tension parameter and the fractal behavior of the limiting curve is demonstrated through different examples. The specific range of the tension parameter has also been depicted, which provides a clear way to generate fractal curves. Since the fractal scheme introduces, in the paper, have more number of control points therefore it gives more degree of freedom to control the shape of the fractal curve.	fractal;interpolation;subdivision surface	Shahid S. Siddiqi;Usama Idrees;Kashif Rehan	2014	Applied Mathematics and Computation	10.1016/j.amc.2014.07.078	t-square;multifractal system;mathematical analysis;topology;fractal derivative;self-affinity;fractal dimension on networks;fractal analysis;fractal landscape;mathematics;geometry;fractal dimension	Graphics	71.30690697008359	-39.92006171318959	164650
4630669e3d176ab4f615512aad37a6fe10413eaa	parameterization of samples for modeling of laser burning - increasing the lifelikeness of synthetically generated samples		This paper describes methods for the generation of samples in modeling burning by a laser beam. In the first part, it briefly informs about real data set and the way of approximation of real samples by mathematically described smooth surfaces. In the main part, the paper focuses on methods which can be used for increasing the lifelikeness of the result of the sample generation process for simulation purposes. Finally, the results are summarized and the future plans are outlined.	approximation;institute for operations research and the management sciences;simulation	Jana Hajková;Pavel Herout	2009			computer science;data mining;geometry;parametrization;regular polygon;rod;laser;rigid frame	Robotics	76.42583426078062	-43.836599007803436	165322
6e0b4e75ba2360a17a96161449e59584a0d7107d	bspline approximation of circle arc and straight line for pocket machining	oscillations;machine tool behaviour;high speed machining;bspline approximation;machine tool;pocket machining	This article proposes a new method of 2D curve interpolation using non-uniform cubic B-splines particularly adapted to the interpolation of sequences of straight lines and circle arcs. The purpose of this method is to calculate C2 continuous curves adapted to high feedrate pocket machining. Industrially machined pockets usually present simple forms. Generally, the tool path is defined by circle arcs and line segments that introduce slowdowns during machining. Thus, a method for approximating a sequence of line segments and circle arcs using Bspline curves is proposed. The proposed method ensures exact line interpolation, to approach the tool path precisely, to reduce the number of control points and to avoid thickening and oscillation at the connections between line segments and circle arcs. Various applications are presented and numerous tests onmachine tools allow the advantages of thismethod to be illustrated. © 2010 Elsevier Ltd. All rights reserved.	approximation;b-spline;control point (mathematics);cubic function;expectation–maximization algorithm;interpolation	Vincent Pateloup;Emmanuel Duc;Pascal Ray	2010	Computer-Aided Design	10.1016/j.cad.2010.05.003	engineering;machine tool;geometry;oscillation;engineering drawing;mechanical engineering	Robotics	69.3334484878338	-38.39843948337393	165342
97f73e8904afe901ec0597165f1150b9369a0355	atrioventricular blood flow simulation based on patient-specific data	mitral valve;phase contrast;momentum transfer;wall motion;finite difference;left ventricle;left atrium;functional model;geometric model;blood flow;navier stokes equation	We propose a new framework for simulating blood flow inside the heart, usable with geometric models of the heart from patientspecific data. The method is geared toward realistic simulation of blood flow, taking into account not only heart wall motion but also valve motion. The simulator uses finite differences to discretize a domain that includes a functional model of the left ventricle and the left atrium of the heart. Navier-Stokes equations are robustly solved throughout the simulation domain, such that one-way momentum transfer between the heart model and the blood is enforced. The valve motion was timed to correspond to valve motions obtained from MRI. The final simulation results are qualitatively consistent to those from MR phase contrast velocity mapping, including high-velocity flow to the aorta during systole and toroidal vortex formation past the mitral valve during diastole.	computer simulation;depth perception;discretization;finite difference;function model;navier–stokes equations;one-way function;taylor–green vortex;toroidal graph;velocity (software development)	Viorel Mihalef;Dimitris N. Metaxas;Mark Sussman;Vassilios Hurmusiadis;Leon Axel	2009		10.1007/978-3-642-01932-6_42	classical mechanics;mathematics;geometry;cardiology	Graphics	70.67723468937014	-46.3584450684687	165841
23cc543d45601a17c958db3e075af8283d587085	multi-scale and adaptive cs-rbfs for shape reconstruction from clouds of points		We describe a multi-scale approach for interpolation and approximation of a point set surface by compactly supported radial basis functions. Given a set of points scattered over a surface, we first use down-sampling to construct a point set hierarchy. Then starting from the coarsest level, for each level of the hierarchy, we use compactly supported RBFs to approximate the set of points at the level as an offset of the RBF approximation computed at the previous level. A simple RBF centre reduction scheme combined with the multi-scale approach accelerates the latter and allows us to achieve high quality approximations using relatively small number of RBF centres. We also develop an adaptive RBF fitting procedure for which the RBF centres are randomly chosen from the set of points of the level. The randomness is controlled by the density of points and geometric characteristic of the set. The support size of the RBF we use to approximate the point set at a vicinity of a point depends on the local density of the set at that point. Thus parts with complex geometry are approximated by dense RBFs with small supports. Numerical experiments demonstrate high speed and good performance of the proposed methods in processing irregularly sampled and/or incomplete data.	3d scanner;adaptive filter;approximation algorithm;cs-cipher;computer-aided design;display resolution;experiment;interpolation;multivariate interpolation;numerical method;radial (radio);radial basis function;randomness;sampling (signal processing)	Yutaka Ohtake;Alexander G. Belyaev;Hans-Peter Seidel	2005		10.1007/3-540-26808-1_8	mathematical optimization;interpolation;hierarchy;cloud computing;radial basis function;offset (computer science);small number;mathematics	Vision	71.00536029849071	-43.32624767452089	165883
0c42ed5818386e0edfd7f7ced14a98cf6d9c2385	improved multi-dimensional hough transform as a track-before-detect method	score based track confirmation track before detect hough transform multidimensional hough transform;radar tracking;transforms;target tracking;signal to noise ratio;signal processing algorithms;transforms radar tracking signal to noise ratio target tracking signal processing algorithms conferences;conferences	This study proposes an improved Multi-Dimensional Hough Transform technique for the detection of low SNR targets (dim targets) in radar data. The proposed Track-Before-Detect technique improves the Multi-Dimensional Hough Transform by limiting the target's maximum velocity and incorporating the SNR values of the targets in the algorithm. In addition, the performance is enhanced by confirming the Hough Transform results with a score-based confirmation algorithm.	algorithm;hough transform;signal-to-noise ratio;track-before-detect;velocity (software development)	Gozde Sahin;Mübeccel Demirekler	2014	2014 22nd Signal Processing and Communications Applications Conference (SIU)	10.1109/SIU.2014.6830390	hough transform;computer vision;radar tracker;speech recognition;computer science;pattern recognition;scale-invariant feature transform;signal-to-noise ratio	Visualization	79.0813833853899	-44.134522276669635	166162
75053877e2e56e216bf72166bbe8c8889a2069f9	system for monitoring and controlling a climbing and walking robot for landslide consolidation		This paper describes the application of space robotics control technology applied to the slope/landslide consolidation sector. The description is made from the point of view of monitoring and controlling a walking/climbing machine used in the Roboclimber project.	mobile robot;semiconductor consolidation	Leif Steinicke;David Dal Zot;Tanguy Fautré	2005		10.1007/3-540-26415-9_110	geomorphology;geography;hydrology;geotechnical engineering	Robotics	81.86493918684288	-51.620121840098996	166384
26ad2316c1a4d06cccf026c4987e8fd471c74cca	multiresolution statistical modeling with application to modeling groundwater flow	laboratory for information and decision systems;electrical engineering and computer science;thesis	The development of accurate mathematical models describing the flow of groundwater is an important problem due to the prevalence of contaminants in or near groundwater supplies. An important parameter of these models is hydraulic conductivity, which describes the ability of the subsurface geology to conduct water flow. Because hydraulic conductivity is a function of the earth's subsurface, direct measurements can be made only at a relatively small number of locations. Instead, one must rely on indirect measurement sources, which supply observations of conductivity at different locations and resolutions. An important problem is to estimate the hydraulic conductivity function from all available data, and to characterize the remaining uncertainty. The class of multiscale processes introduced in [Chou et al., 19941 appears to be well-suited to this problem, since these processes can be estimated efficiently at every scale at which they are modeled; the multiscale estimator also provides an uncertainty measure for the estimates. However, this multiscale framework has some limitations that must be overcome before it can be applied to general data fusion problems. First, because all of the previous applications have focused on the measurement and estimation of the finest-scale process, arbitrary nonlocal properties of interest (e.g., coarse-resolution measurements of hydraulic conductivity) have not been represented within the multiscale framework. Second, the class of stochastic processes that is well modeled by low-order tree models has not been fully characterized. To address the first limitation, this thesis (1) extends multiscale realization theory so that coarse-scale variables can represent particular non-local properties to be measured or estimated and (2) applies these realization algorithms to the estimation of hydraulic conductivity from sparse measurements made at different resolutions. To partially address the second limitation, the multiscale processes are shown to provide natural approximations of fractional Brownian motion. These approximations are based on the observation that the multiscale realization problem can be considerably simplified when modeling random processes that are statistically self-similar and/or have stationary increments.	algorithm;approximation;brownian motion;mathematical model;multiresolution analysis;quantum nonlocality;realization (systems);self-similarity;sparse matrix;stationary process;statistical model;stochastic process	Michael M. Daniel	1997			computer science;engineering;data science	Vision	76.79151976850821	-47.17646213390712	166492
bc6b7392c8896b2b3a03480f5ca8a6dbaf2edc03	bezier developable surfaces	bezier;edge of regression;blossom;developable surface	In this paper we address the issue of designing developable surfaces with Bézier patches. We show that developable surfaces with a polynomial edge of regression are the set of developable surfaces which can be constructed with Aumann’s algorithm. We also obtain the set of polynomial developable surfaces which can be constructed using general polynomial curves. The conclusions can be extended to spline surfaces as well.	algorithm;bézier curve;polynomial;spline (mathematics)	L. Fernández-Jambrina	2017	Computer Aided Geometric Design	10.1016/j.cagd.2017.02.001	topology;developable surface;bézier curve;mathematics;geometry;tangential developable	Graphics	68.95258131636328	-41.34757412930015	167171
d0bbd17c4efb0300d74eeeb814166f40f246265b	smoothing of xdawn spatial filters for robust extraction of event-related potentials	subspace constraints;smoothing methods;electrodes;robustness;correlation;electroencephalography;algorithm design and analysis	The xDAWN algorithm is well-known as a method for designing spatial filters to improve signal-to-noise ratio and to reduce the dimension of observed EEG signals. This paper proposes a method for spatially smoothing xDAWN spatial filters to give a robustness against small sample problem. The proposed method gives a subspace constraint to the parameter space of the spatial filters. This subspace is given as a basis of the graph Fourier transform on a graph representing geometric structure of the electrodes. The spatial filters found in the subspace are smooth in the spatial domain. The proposed method is evaluated by experiments with artificial signals and BCI datasets.	algorithm;basis (linear algebra);brain–computer interface;erp;electroencephalography;experiment;signal-to-noise ratio;simulation;smoothing;spatial anti-aliasing	Hiroshi Higashi;Tomasz M. Rutkowski;Toshihisa Tanaka;Yuichi Tanaka	2016	2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)	10.1109/APSIPA.2016.7820750	mathematical optimization;machine learning;pattern recognition;mathematics	Vision	81.31595595485541	-38.260686804096785	167185
866a5f19c946f9212ddb0c9278d7b8e25682817c	c2 hermite interpolation by minkowski pythagorean hodograph curves and medial axis transform approximation	concepcion asistida;medial axis transform;computer aided design;rational interpolation;hermite interpolation;ajustamiento curva;hodographe pythagorien;surface parametrique;aproximacion optima;interpolation hermite;metrique minkowski;superficie parametrica;polynomial interpolation;esqueltizacion;parameterization;metrico minkowski;pythagorean hodograph curve;parametrizacion;squelettisation;interpolacion racional;interpolacion hermite;optimal approximation;approximation optimale;pythagorean hodograph;conception assistee;ajustement courbe;skeletonization;minkowski metric;interpolacion polinomial;curve fitting;hodografo pythagor;parametric surface;minkowski space;parametrisation;interpolation polynomiale;interpolation rationnelle	We describe and fully analyze an algorithm for C Hermite interpolation by Pythagorean hodograph curves of d egree 9 in Minkowski spaceR. We show that for any data there exists a four-parameter syst em of interpolants and we identify the one which preserves symmetry and planarity of t he input data and which has the optimal approximation degree. The new algorithm is applied to an efficient approxim ation of segments of the medial axis transform of a planar domain leading to rational parameterizations of the offsets of the domain boundaries with a high order of approximation.	algorithm;apache axis;hermite interpolation;medial graph;minkowski functional;order of approximation;planar graph;semantics (computer science)	Jirí Kosinka;Zbynek Sír	2010	Computer Aided Geometric Design	10.1016/j.cagd.2010.04.005	mathematical analysis;minkowski space;topology;computer aided design;mathematics;geometry;algebra	Theory	68.45576748936614	-40.255396454952574	167346
6dc7ad663152b52570171fce9fdbd3246aae8ffe	isotropic kernels for two-dimensional image interpolation	interpolation;approximation;radial basis functions;finite elements method;splines;bessel functions;spatial spectrum of an image	The paper proposes a technique of the composition of two-dimensional interpolation kernels possessing the approximately isotropic spectral characteristics. The application of these kernels makes it possible to weaken many artifacts that appear at interpolation procedures implemented by the traditional techniques. The paper presents the results of the mathematical simulation that confirm the advantages of the proposed technique.	computer simulation;interpolation	N. S. Kulberg;T. V. Yakovleva	2012	Journal of Mathematical Imaging and Vision	10.1007/s10851-012-0334-2	spline interpolation;mathematical optimization;discrete mathematics;bilinear interpolation;interpolation;stairstep interpolation;mathematics;geometry;nearest-neighbor interpolation;trilinear interpolation	Robotics	72.81463112179645	-42.124223622562	167782
d5203fddbdce0df48d619528751f15528bf5072a	motion field modeling and estimation using motion transform	affine projection;time frequency;motion estimation;discrete cosine transforms;image representation;dft motion field modeling motion estimation motion transform image representation transform domain motion discontinuities time frequency components simulation results dct;motion estimation discrete cosine transforms discrete fourier transforms discrete wavelet transforms pixel discrete transforms fourier transforms time frequency analysis optical surface waves image motion analysis;time frequency analysis image representation image sequences motion estimation discrete cosine transforms discrete fourier transforms;discrete fourier transforms;time frequency analysis;image sequences	This paper presents the concept of using a motion transform for finding the motion field between two images. A motion transform is a representation for modeling the motion field in the transform domain. Compared to other parametric motion models, e.g., affine, projective, etc., a motion transform offers a considerable advantage by its capability to model any motion field, including one with motion discontinuities. It also offers the flexibility of dynamically choosing the significant time-frequency components used to model the underlying motion. Simulation results on motion estimation using the DCT/DFT for motion modeling are presented. These results are comparable to the results from a wavelet-based approach.	discrete cosine transform;motion estimation;motion field;simulation;wavelet	Yun-Ting Lin;Siu-Leong Iu	1999		10.1109/ICIP.1999.822972	sine and cosine transforms;computer vision;mathematical optimization;time–frequency analysis;harmonic wavelet transform;modified discrete cosine transform;discrete sine transform;discrete fourier transform;discrete cosine transform;motion estimation;mathematics;geometry;discrete fourier transform;motion field;non-uniform discrete fourier transform;motion compensation;discrete frequency domain	Vision	75.56015339702986	-40.57315930745979	167912
fb470e9f926323c232a60abe655c4de283fff6f1	the use of stochastic matched filter in active sonar	correlation;simulation;sonar;matched filters;reverberation;matched filter;stochastic processes;active filter;shallow water;signal to noise ratio	"""In this present study we propose to evaluate a kind of matched filter, called """"stochastic matched filter”, first described by J-F Cavassilas and originated to detect short stationary stochastic “pulses” in stationary noise. We can show in this paper that this filter, with minor changes in its definition, can be applied on frequency time-varying signals, such as wide band modulated sonar signal propagated in shallow water. The stochastic matched filter is able to take into account uncertainties and variations of the propagation channel, thus enhancing the detection efficiency. The main characteristics of this treatment is shown up with simulated data."""	matched filter;modulation;sonar (symantec);software propagation;stationary process	J.-L. Mori;P. Gounon	2000	2000 10th European Signal Processing Conference		raised-cosine filter;electronic engineering;kernel adaptive filter;telecommunications;engineering;root-raised-cosine filter;filter design;matched filter;cartography	Vision	80.43239644624846	-42.7186275347212	168134
ed48064110e2df9f8a684b81a35ab853d666d4d4	ecg signal enhancement with serial cascade owa filter		Noises which appear during recording of biomedical signal are seldom characterized by the Gaussian distribution, whereas a noise can have an impulsive nature. An application of traditional filtering method at such assumption can lead to introducing a distortion to a filtered signal. In this paper, an overview of the robust filtering problem is provided. The new serial cascade robust filter with application of the ordered weighted aggregation (OWA) operator is proposed. The OWA operator is described applying a weighted average of ordered signal samples in a moving window. By introducing the nonlinear sorting operation and assigning to the sorted samples appropriate values of weights the improvement of filtering in impulsive environment is achieved. The structure of the proposed serial cascade OWA filter consists of two successive OWA filters. The proposed filter is tested with the help of real signal corrupted with an artificial as well as a real noise. The indicator of the filter's performance quality is the root mean square error (RMSE). For testing purpose the electrocardiographic signal is used. The obtained results show that in the field of impulsive noise suppression, the proposed serial cascaded OWA filter lead to better results compared to the reference filters.	distortion;mean squared error;nonlinear system;ordered weighted averaging aggregation operator;signal-to-noise ratio;sorting;zero suppression	Tomasz Pander;Tomasz Przybyla	2018	"""2018 25th International Conference """"Mixed Design of Integrated Circuits and System"""" (MIXDES)"""	10.23919/MIXDES.2018.8436900	control engineering;computer science;operator (computer programming);robustness (computer science);filter (signal processing);distortion;control theory;sorting;filtering problem;mean squared error;weighted arithmetic mean	Robotics	81.75436393391507	-39.201891507258075	168227
303d644528102fc01950a439fe484622b5bb7425	application of hilbert-huang transform to engine knock detection	hilbert transforms;ignition;combustion process hilbert huang transform engine knock detection multicomponent nonstationary signal decomposition;engines resonant frequency transforms combustion time frequency analysis frequency modulation signal to noise ratio;engines;automotive components;ignition automotive components engines hilbert transforms;knocking combustion signal processing hilbert huang transform time frequency analysis automotive	Knocking combustion in spark ignition (SI) engines is an undesirable mode of combustion. It causes many effects such as a rapid increase in pressure and temperature in the chamber and vibrations of the engine block. This phenomenon limits performance, durability and fuel economy and can lead to serious engine damage. The paper presents a knock detection method based on Hilbert-Huang transform (HHT). This relatively new tool allows for decomposition of a multicomponent nonstationary signal into individual components. Applying this method to the pressure signal, we are able to obtain precise information about the combustion process.	durability (database systems);hilbert space;hilbert–huang transform;nonlinear system;port knocking;resonance	Jerzy Fiolka	2013	Proceedings of the 20th International Conference Mixed Design of Integrated Circuits and Systems - MIXDES 2013		electronic engineering;engineering;automotive engineering;forensic engineering	EDA	79.42062457229376	-39.38243392240435	168441
118be297e473ebcd632683c839870c181c181ecf	clustering of em radiation source based on eigenvector	terrestrial electricity geophysical techniques data analysis earthquakes;223 hz geophysical measurement technique geoelectric method terrestrial electricity earthquake prediction precursor em radiation eigenvector clustering anomalous source principal component analysis covariance matrices data analysis em source seismic activity em wave japan;earthquake prediction;earthquakes;near field;data analysis;covariance matrices;principal component analysis;terrestrial electricity;human activity;electromagnetic radiation seismic measurements lightning earthquakes electromagnetic measurements noise measurement humans principal component analysis covariance matrix radar;geophysical techniques;eigenvectors	Our goal is to locate anomalous electromagnetic (EM) sources caused by seismic activity for earthquake prediction. For the purpose, we have measured the EM wave of 223Hz at 35 sites in Japan. The measured signal contains much noise caused by lightning around the equator, human activity, near field lightning and so on. EM noises from far sources can be eliminated by using correlation in multi-point observation. But near-field noise is not easy to eliminate. This paper shows that the principal component analysis methods for covariance matrices of measured signals are effective for clustering signals and separating each source. It is shown that a result to estimate thunder lightning location in near field agrees with real thundercloud observed by radar system. And results of estimation of sources of anomalous EM radiation occurred before and after earthquake are also shown.		Ichi Takumi;Shuhei Murakami;Akio Shimura;Masayasu Hata;Hiroshi Yasukawa	2002		10.1109/IGARSS.2002.1026718	geophysics;seismology;eigenvalues and eigenvectors;near and far field;data analysis;earthquake prediction;physics;statistics;remote sensing;principal component analysis	NLP	82.22973381033198	-42.53907828348536	168444
2c3e3312c3c518a3d0e9a86a00b2ee0c5035a8aa	surface models of tube trees	polygonal representation;tube trees;method isnot artifact-free;constructsa topologically-correct surface mesh;new method;maximal-disc interpolation method;well-known algorithmsfor surface;surface meshserves;parallel cross-section;proposed method extractsa;surface models;data representation;graphics;real time;geometric modeling;surface modeling;data mining;mesh generation;tree data structures;acceleration;cross section;tree graphs;convolution;virtual reality;interpolation	"""This paper describes a new method for generating surfaces of branching tubular structures with given center-lines and radii. As the centerlines are not straight lines, the cross-sections are not parallel and well-known algorithms for surface tiling from parallel cross-sections cannot be used. Nonparallel cross-sections can be tiled by means of the maximal-disc interpolation method; special methods for branching-structures modeling by means of convolution surfaces produce excellent results, but these methods are more complex than our approach. The proposed method tiles nonparallel circular cross-sections and constructs a topologically-correct surface mesh. The method is not artifact-free, but it is fast and simple. The surface mesh serves as a data representation of a vessel tree suitable for real-time virtual reality operation planning and operation support within a medical application. Proposed method extracts a """"classical"""" polygonal representation, which can be used in common surface-oriented graphic accelerators"""	adaptive sampling;algorithm;augmented reality;catmull–clark subdivision surface;convolution;data (computing);interpolation;maximal set;polygon mesh;precise point positioning;real-time locating system;recursion;sampling (signal processing);tiling window manager;virtual reality	Petr Felkel;Rainer Wegenkittl;Katja Bühler	2004	Proceedings Computer Graphics International, 2004.	10.1109/CGI.2004.1309194	freeform surface modelling;mesh generation;interpolation;computer science;theoretical computer science;geometric modeling;cross section;geometry;external data representation;virtual reality;tree;programming language;computer graphics (images)	Visualization	68.46533400070054	-50.0848506436503	168489
1f5679d7bf705508da2b04803aff089dd0cb91e3	topology verification for isosurface extraction	software;isosurface extraction;ciencias exatas e da terra;topology;interpolation;topology verifiable visualization isosurface;manifolds;isosurface coding mistakes;ciencia da computacao;topology computational geometry data visualisation;digital topology;level set;computational geometry;isosurfaces;verifiable visualization;algorithmic implementations;topological properties;data visualisation;isosurface;topological invariant;topology verification;face;morse theory;topological invariant verification;article;stratified morse theory;isosurfaces topology face level set software interpolation manifolds;topology verification isosurface extraction verifiable visualization algorithmic implementations stratified morse theory digital topology topological invariant verification isosurface coding mistakes	The broad goals of verifiable visualization rely on correct algorithmic implementations. We extend a framework for verification of isosurfacing implementations to check topological properties. Specifically, we use stratified Morse theory and digital topology to design algorithms which verify topological invariants. Our extended framework reveals unexpected behavior and coding mistakes in popular publicly available isosurface codes.	adobe streamline;algorithm;anatomy, regional;code;computation;digital topology;floor and ceiling functions;formal verification;imagery;isosurface;level of detail;mathematics;occur (action);scientific visualization;solutions;verification of theories;volume rendering	Tiago Etiene;Luis Gustavo Nonato;Carlos Eduardo Scheidegger;Julien Tierny;Thomas J. Peters;Valerio Pascucci;Robert Michael Kirby;Cláudio T. Silva	2012	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2011.109	face;discrete mathematics;topology;manifold;computational geometry;interpolation;level set;theoretical computer science;isosurface;mathematics;geometry;data visualization;morse theory;digital topology	Visualization	71.63834392442631	-44.79632663528973	168609
2f458fb29bfe7cebc8fb28037c4894758285407c	a point-based method for animating elastoplastic solids	based simulation;animation point;constitutive model;best approximation;finite deformation;physics;i 3 7 computer graphics;types of simulation;affine transformation;animation;i 6 8 simulation and modeling;multiple model;three dimensional graphics and realism;natural phenomena;based animation	In this paper we describe a point-based approach for animating elastoplastic materials. Our primary contribution is a simple method for computing the deformation gradient for each particle in the simulation. The deformation gradient is computed for each particle by finding the affine transformation that best approximates the motion of neighboring particles over a single timestep. These transformations are then composed to compute the total deformation gradient that describes the deformation around a particle over the course of the simulation. Given the deformation gradient we can apply arbitrary constitutive models and compute the resulting elastic forces. Our method has two primary advantages: we do not store or compare to an initial rest configuration and we work directly with the deformation gradient. The first advantage avoids poor numerical conditioning and the second naturally leads to a multiplicative model of deformation appropriate for finite deformations. We demonstrate our approach on a number of examples that exhibit a wide range of material behaviors.	gradient;numerical analysis;simulation	Dan Gerszewski;Haimasree Bhattacharya;Adam W. Bargteil	2009		10.1145/1599470.1599488	anime;simulation;affine transformation;mathematics;geometry;constitutive equation;computer graphics (images)	Vision	69.95557996624403	-47.33958134010025	168782
a3c4abe52fba2b7e90b49d74ef75188a49a434d2	approximation of a variable density cloud of points by shrinking a discrete membrane	i 3 5 computational geometry and object modeling;computacion informatica;scattered data points;grupo de excelencia;ciencias basicas y experimentales;discrete geometry;voxelization;external research report;3d data point;article;surface approximation	This paper describes a method to obtain a closed surface that approximates a general 3D data point set with non-uniform density. Aside from the positions of the initial data points, no other information is used. Particularly, neither the topological relations between the points nor the normal to the surface at the data points are needed. The reconstructed surface does not exactly interpolate the initial data points, but approximates them with a bounded maximum distance. The method allows to reconstruct closed surfaces with arbitrary genus and closed surfaces with disconnected shells. ACM CSS: I.3.5 Computational Geometry and Object Modeling.	algebraic equation;approximation;approximation algorithm;approximation error;cascading style sheets;computation;computational geometry;data point;genus (mathematics);interpolation;octree;point cloud;polygon mesh;sensor;voxel	Jordi Esteve;Pere Brunet;Alvar Vinacua	2005	Comput. Graph. Forum	10.1111/j.1467-8659.2005.00902.x	discrete geometry;combinatorics;topology;mathematics;geometry	Graphics	70.00826096087121	-41.78453511409719	170539
c54a7b1befc3af7ddde055e38c1152b3bdf2c49e	a hybrid approach to facial rigging	real time;hybrid approach;emotion;interface;facial animation	In production environments, facial rigging is commonly done by either geometric deformations or blendshapes. Geometric deformations are driven by simulated muscle actions, which are loosely based upon the dynamics of facial tissue [Magnenat-Thalmann et al. 1988]. Blendshapes interpolate a large number of sculpted shapes [Bergeron and Lachapelle 1985]. The former approach is intuitive, yet slow and less precise. The latter is fast, yet memory intensive and sensitive to model changes. Conventional implementations of both approaches are difficult to generalize in order to build rigs quickly and retarget animation efficiently.	geometric median;geometric modeling;interpolation	David Komorowski;Vinod Melapudi;Darren Mortillaro;Gene S. Lee	2010		10.1145/1899950.1899992	computer vision;simulation;emotion;computer science;interface;programming language;computer graphics (images)	Graphics	69.16466113176698	-47.496493189246856	170730
22320f0c3c21f821aee394d50c0be8a0e178d227	univariate cubic l1 interpolating splines: spline functional, window size and analysis-based algorithm	second derivative;interpolation;locally calculated;5 point window;function value;univariate;spline function;cubic l1 spline;antiderivative;first derivative;global	We compare univariate L1 interpolating splines calculated on 5-point windows, on 7-point windows and on global data sets using four different spline functionals, namely, ones based on the second derivative, the first derivative, the function value and the antiderivative. Computational results indicate that second-derivative-based 5-point-window L1 splines preserve shape as well as or better than the other types of L1 splines. To calculate second-derivative-based 5-point-window L1 splines, we introduce an analysis-based, parallelizable algorithm. This algorithm is orders of magnitude faster than the previously widely used primal affine algorithm.	algorithmic efficiency;approximation algorithm;bicubic interpolation;bivariate data;cpu cache;computation;cubic hermite spline;cubic function;fits;fitts's law;ibm notes;microsoft windows;philippe kruchten;smoothing spline;spline (mathematics);spline wavelet;eric	Lu Yu;Qingwei Jin;John E. Lavery;Shu-Cherng Fang	2010	Algorithms	10.3390/a3030311	spline interpolation;antiderivative;spline;mathematical optimization;combinatorics;mathematical analysis;perfect spline;smoothing spline;derivative;interpolation;second derivative;mathematics;thin plate spline;univariate;polyharmonic spline;box spline	ML	70.76132057854853	-40.167908955458195	170946
1bf27b32220e10144244fc2a7240f2a096447a29	statistical analysis of the phase-locking value	phase locking;gaussian noise;frequency modulation;synchronization coherence time multicomponent signal phase locking value;probability density function;uniform noise;faculty of science environment engineering and technology;instantaneous frequency;multicomponent signal statistical analysis phase locking value gaussian noise uniform noise laplace noise instantaneous frequency coherence time;frequency spectrum;computational modeling;statistical analysis;laplace noise;synchronization;230113;coherence time;random processes;mathematical model;statistical analysis electroencephalography;brain activation;statistical analysis optical noise phase noise electric variables measurement phase measurement frequency synchronization brain narrowband gaussian noise analysis of variance;electroencephalography;phase locking value;multicomponent signal;pre2009 dynamical systems;noise	The phase-locking value (PLV) is an important measure of synchronization when studying biosignals and especially electrical brain activities. Because the physical interpretation of the instantaneous phase of broadband signals leads to difficulties, most applications using the PLV deal with narrowband signals. The paper presents a statistical analysis of the PLV under the influence of Gaussian, Uniform and Laplace noise when added to the instantaneous phases and frequencies of mono- and two-component signals. We derive analytical expressions for the PLV values for these conditions and show that the PLV is rapidly dropping with increasing noise variance: broadening the frequency spectrum. We show that the noise influence on the instantaneous frequency is much more pronounced than on the instantaneous phase. We also introduce the concept of PLV-coherence time by analogy to optical coherence. Finally, we show that two signals with identical factitious instantaneous frequency as found in multicomponent signals can be phase locked leading to possible wrong interpretations of the PLV.	cache coherence;instantaneous phase;lock (computer science);spectral density;two-phase locking	Patrick Celka	2007	IEEE Signal Processing Letters	10.1109/LSP.2007.896142	frequency modulation;gaussian noise;instantaneous phase;stochastic process;synchronization;frequency spectrum;probability density function;electroencephalography;telecommunications;computer science;noise;mathematical model;mathematics;computational model;statistics;coherence time	Metrics	79.41117928047488	-39.16136841846715	171349
388c4ddca7aaeaede73b2b5e5801dca92df399eb	nonstationary signals analysis by teager-huang transform (tht)		In this paper a new method called Teager-Huang Transform (THT) for analysis of nonstationary signals is introduced. The THT estimates the Instantaneous frequency (IF) and the Instantaneous amplitude (IA) of a signal. The method is based on the Empirical mode decomposition (EMD) algorithm of Huang et al. [1] and the Teager energy operator (TEO) [2]. Both EMD and TEO deal with non-stationary signals [1],[3]. The signal is first band pass filtered using the EMD into zero-mean AM-FM components called Intrinsic mode functions (IMFs) with well defined IF. Then TEO tracks the modulation energy of each IMF and estimates the corresponding IF and IA. The final presentation of the IF and the IA results is an energy Time frequency representation (TFR) of the signal. Based on the EMD, the THT is flexible enough to analyze any data (linear or nonlinear) and stationary or nonstationary signals. Furthermore, the THT is free of interferences. To show the effectiveness of the THT, TFRs of five synthetic signals are presented and results compared to those of the Hilbert-Huang transform (HHT) [1], the spectrogram, the Smoothed pseudo Wigner-Ville distribution (SPWVD), the scalogram and the Choi-Williams distribution (CWD).	algorithm;energy operator;fm broadcasting;hilbert space;hilbert–huang transform;instantaneous phase;modulation;nonlinear system;scaleogram;smoothing;spectrogram;stationary process;synthetic intelligence;through-hole technology;wigner distribution function;wigner quasiprobability distribution	Jean-Christophe Cexus;Abdel-Ouahab Boudraa	2006	2006 14th European Signal Processing Conference		electronic engineering;speech recognition;hilbert–huang transform;calculus;mathematics	EDA	80.12208258618567	-38.59381725737107	171411
f7b2493be39a32e10de7da4684164732d35bfee0	wavetool: an integrated software for wavelet and multirate signal processing	digital image processing;filter bank;image processing;data compression;band pass filters;seismology;software prototyping;texture classification;interactive tree building;image classification;image restoration;wavelet based algorithms;wavetool;integrated software environment;image texture;wavelet transforms;filter design;rapid prototyping;geophysical signal processing;signal processing;seismic data compression;seismology band pass filters integrated software software prototyping software tools wavelet transforms image texture image classification image restoration geophysical signal processing;integrated software;multirate signal processing;image analysis;software tools;image restoration multirate signal processing integrated software wavetool multirate filter banks digital image processing image analysis one dimensional signal processing integrated software environment rapid prototyping wavelet based algorithms online multirate filter design interactive tree building image processing texture classification seismic data compression;filter bank signal processing algorithms application software wavelet analysis digital images electronics packaging image texture analysis digital signal processing software prototyping software algorithms;multirate filter banks;one dimensional signal processing;online multirate filter design	Wavelets and multi-rate filter banks are increa.singly important tools for various digital image processing (DIP) aad image analysis tasks in addit,ion to their traditional applica.tions in one dimensional signal processing. WaveTool' . is an integrated soft,ware environment for rapid prototyping of wavelet-based algorithms. I t is part,icularly attractive for signal processing because of it,s rich collection of filter bank choices, online multi-rate filter design, a.nd interactive tree building. This paper presents a general overview of WziveTool with emphasis on its image processing capabilit,ies. Example applications to texture classification, seismic data compression, and image restoration are presented, and the fut,ure direction of WaveTool is discussed.	algorithm;circuit restoration;data compression;digital image processing;filter bank;filter design;image analysis;image restoration;integrated software;rapid prototyping;signal processing;warez;wavelet	H. Singh;Peter N. Heller	1995		10.1109/ICIP.1995.529045	data compression;multidimensional signal processing;image texture;image restoration;computer vision;contextual image classification;pyramid;image analysis;image processing;computer science;theoretical computer science;digital signal processing;signal processing;digital image processing;filter bank;band-pass filter;filter design;wavelet transform	Graphics	75.85650093249608	-50.18032723579198	171925
547464a050ad1521a1a9b8e9b8a96785c4a07b5f	cloth animation with adaptively refined meshes	cloth animation;bottom up;adaptive mesh;refinement and simplification;triangular mesh;adaptive refinement;cloth simulation;mesh adaptation;mesh refinement;discrete differential geometry;dynamic adaptation	Cloth animation is a very expensive process in terms of computational cost, due to the flexible nature of cloth objects. Since wrinkles and smooth areas co-exist commonly in cloth, it is tempting to reduce computational cost by avoiding redundant tessellation at the smooth areas. In this paper we present a method for dynamic adaptation of triangular meshes suitable for cloth simulation. A bottom-up approach is used for mesh refinement, which does not require precomputation and storage of multiresolution hierarchy. The hierarchy is constructed in runtime and allows reverting of the refinement locally. Local mesh refinement and simplification are triggered by curvature-induced criterion, where the curvature is estimated using methods of discrete differential geometry. The results presented are the realistic animation of garment worn by a walking mannequin generated with Baraff-Witkin type cloth solver enhanced with the mesh adaptation scheme.	adaptive mesh refinement;algorithmic efficiency;bottom-up parsing;cloth modeling;computer animation;level of detail;precomputation;refinement (computing);run time (program lifecycle phase);simulation;solver;triangulated irregular network	Vasily Volkov	2005			simulation;computer science;engineering drawing;t-vertices;computer graphics (images)	Graphics	68.80741690041833	-46.71871738435007	172582
2af011d19385b1748f520f7fa50b9547823e8817	imaging and detection of cracks in metallic structures with eddy current sensors	inversion scheme;new developed technique;metallic structure;industrial application;eddy current sensor;non destructive evaluations;imaging crack;points source;ships building;sensors;modeling;metals	In this paper we propose an original analytical modeling applied to electromagnetic (EM) systems, in the aim ofrnperforming an inversion scheme. The purpose consists in imaging cracks through eddy current sensors, the image being constituted by an estimation of the conductivity of the piece of metal under test, voxel by voxel. There are plenty of industrial applications in Non Destructive Evaluations for monitoring of cracks in structures: Aeronautics, Metallurgy, ships building and so on. For modeling, a relatively new developed technique, called ‘Distributed Points Source Method’ (DPSM) has been used. This original modeling was invented at Ecole Normale Superieure of Cachan in 2000, and since developed and patented in many kind of applications (Ultrasonics, Electrostatics…).	sensor	Dominique Placko;Thierry Bore;Pierre-Yves Joubert	2011			systems modeling;telecommunications;sensor;optics	Logic	78.92165781360588	-49.82008297163782	172692
6c3c831fda67a40676b0b5fa845bd4713dd12fb0	a comparison of mesh morphing methods for 3d shape optimization	shape optimization	The ability to automatically morph an existing mesh to conform to geometry modifications is a necessary capability to enable rapid prototyping of design variations. This paper compares six methods for morphing hexahedral and tetrahedral meshes, including the previously published FEMWARP and LBWARP methods as well as four new methods. Element quality and performance results show that different methods are superior on different models. We recommend that designers of applications that use mesh morphing consider both the FEMWARP and a linear simplex based method.	hexahedron;morphing;rapid prototyping;shadow volume;shape optimization	Matthew L. Staten;Steven J. Owen;Suzanne M. Shontz;Andrew G. Salinger;Todd S. Coffey	2011			shape optimization;computer vision;polygon mesh;morphing;engineering drawing;rapid prototyping;artificial intelligence;computer science;tetrahedron	Graphics	69.6740498198309	-44.254718097471326	172842
865c28d8ed986fcf11e83bb8b0853501c8aa9b14	principal stream surfaces	principal stream surface;velocity points;vector visualization;physics computing;principal stream surface algorithm;volume rendering;fluid field velocity direction;flow topology;principal stream function;visualization;normal vectors;starting point placement;filtering;scalar field;streamlines;flow field;irrotational flow;mechanical engineering;stream function	The use of stream surfaces and streamlines is well established in vector visualization. However, the proper placement of starting points is critical for these constructs to clearly illustrate the flow topology. In this paper, we present the principal stream surface algorithm, which automatically generates stream surfaces that properly depict the topology of an irrotational flow. For each velocity point in the fluid field, we construct the normal to the principal stream surface through the point. The set of all such normal vectors is used to construct the principal stream function, which is a scalar field describing the direction of velocity in the fluid field. Volume rendering can then be used to visualize the principal stream function, which is directly related to the flow topology. Thus, topology in a fluid field can be easily modeled and rendered.	algorithm;streamsurface;velocity (software development);volume rendering	Wenli Cai;Pheng-Ann Heng	1997	Proceedings. Visualization '97 (Cat. No. 97CB36155)	10.1145/266989.267027	filter;scalar field;mathematical optimization;visualization;streamlines, streaklines, and pathlines;theoretical computer science;conservative vector field;mathematics;geometry;stream function;volume rendering	Visualization	70.6761436732044	-45.48158731997414	173262
0948b9e4d6b21e62e2d5b0750abf2a2d2334e0d9	techniques for non-linear magnification transformations	piecewise linear;normal views;expressiveness;smooth interpolation;efficiency;piecewise linear methods;data visualisation;nonlinear magnification transformations;domain constraint;multiple transformations combination;expressiveness nonlinear magnification transformations magnifications combination domain constraint multiple transformations combination smooth interpolation normal views piecewise linear methods efficiency;image resolution spatial resolution visualization computer science piecewise linear techniques computer displays computer applications application software computer graphics layout;magnifications combination	This paper presents efficient methods for implementing general non-linear magnification transformations. Techniques are provided for: combining linear and non-linear magnifications, constraining the domain of magnifications, combining multiple transformations, and smoothly interpolating between magnified and normal views. In addition piecewise linear methods are introduced which allow greater efficiency and expressiveness than their continuous counterparts.	aggregate data;aggregate function;approximation algorithm;categorization;interpolation;nonlinear system;piecewise linear continuation;smoothing	Alan Keahey;Edward L. Robertson	1996		10.1109/INFVIS.1996.559214	mathematical optimization;discrete mathematics;piecewise linear function;theoretical computer science;expressivity;mathematics;efficiency;data visualization	EDA	70.10317290527661	-44.93447171245123	173341
5e0f5ae1a0ca4e3140e0aca760a3fe57c6ae79a6	a complete ensemble empirical mode decomposition with adaptive noise	mode mixing problem;gaussian noise;oscillations;heart rate variability;adaptive noise ensemble empirical mode decomposition mode averaging gaussian white noise mode mixing problem discrete dirac delta function electrocardiogram signal spectral separation sifting iterations;discrete dirac delta function;sifting iterations;oscillators;adaptive noise;white noise electrocardiography signal to noise ratio computational efficiency oscillators signal processing algorithms;electrocardiogram signal;spectral separation;ensemble empirical mode decomposition;gaussian white noise;electrocardiography;adaptive signal processing;heart rate variability empirical mode decomposition biomedical signal processing;signal processing;biomedical signal processing;mode averaging;white noise adaptive signal processing gaussian noise;signal to noise ratio;signal processing algorithms;computational efficiency;white noise;electrocardiogram;empirical mode decomposition	In this paper an algorithm based on the ensemble empirical mode decomposition (EEMD) is presented. The key idea on the EEMD relies on averaging the modes obtained by EMD applied to several realizations of Gaussian white noise added to the original signal. The resulting decomposition solves the EMD mode mixing problem, however it introduces new ones. In the method here proposed, a particular noise is added at each stage of the decomposition and a unique residue is computed to obtain each mode. The resulting decomposition is complete, with a numerically negligible error. Two examples are presented: a discrete Dirac delta function and an electrocardiogram signal. The results show that, compared with EEMD, the new method here presented also provides a better spectral separation of the modes and a lesser number of sifting iterations is needed, reducing the computational cost.	algorithm;algorithmic efficiency;computation;dirac delta function;hilbert–huang transform;image noise;iteration;mike lesser;numerical analysis;white noise	María Eugenia Torres;Marcelo A. Colominas;Gastón Schlotthauer;Patrick Flandrin	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5947265	computer vision;speech recognition;computer science;signal processing;mathematics;white noise;oscillation;statistics	Robotics	80.37878605360238	-38.61465712323539	173633
5e9d37067532a932a083638a3cf1a89df9e9470d	visual simulation of smoke using overlapping grids	computational grid;object interaction;computation fluid dynamics;visual simulation;adaptive method;natural phenomena;tetrahedral mesh	Many methods for the visual simulation of natural phenomena, such as smoke and fire, have been proposed. These methods use a technique known as computational fluid dynamics. We propose a new method using overlapping grids for the visual simulation of fluids. Previously, the motion of the fluids was computed by using a single computational grid. When an object interacts with a fluid, the resolution of a grid must be sufficiently high to accurately represent the shape of the object that is voxelized by sampling the shape at the grid points. To address this problem, an adaptive method using a tetrahedral mesh has been proposed to represent the shape of the object accurately [Klinger et al. 2006]. However, this requires restructuring of the mesh when the objects move. The computational algorithm also becomes complex due to the uneven structure of the mesh. Our method addresses these problems by using multiple grids, and provides a practical and efficient way of handling objects interacting with fluids.	algorithm;augmented reality;computation;computational fluid dynamics;glossary of sudoku;grid computing;interaction;sampling (signal processing);simulation	Yoshinori Dobashi;Tsuyoshi Yamamoto;Tomoyuki Nishita	2007		10.1145/1280720.1280780	simulation;computer science;theoretical computer science;computer graphics (images)	Robotics	69.7256198841474	-49.39498265312747	173717
fffa22430ffac7edeb31e1c160cf1e22f41f241b	computational mesh generation for vascular structures with deformable surfaces	surgical planning;image segmentation;finite element method;blood flow;mesh generation;deformable model	Computational blood flow and vessel wall mechanics simulations for vascular structures are becoming an important research tool for patient-specific surgical planning and intervention. An important step in the modelling process for patient-specific simulations is the creation of the computational mesh based on the segmented geometry. Most known solutions either require a large amount of manual processing or lead to a substantial difference between the segmented object and the actual computational domain. We have developed a chain of algorithms that lead to a closely related implementation of image segmentation with deformable models and 3D mesh generation. The resulting processing chain is very robust and leads both to an accurate geometrical representation of the vascular structure as well as high quality computational meshes. The chain of algorithms has been tested on a wide variety of shapes. A benchmark comparison of our mesh generation application with five other available meshing applications clearly indicates that the new approach outperforms the existing methods in the majority of cases.	algorithm;benchmark (computing);computation;computational geometry;display resolution;image segmentation;mesh generation;simulation	S. de Putter;Frans N. van de Vosse;Frans A. Gerritsen;F. Laffargue;Marcel Breeuwer	2006	International Journal of Computer Assisted Radiology and Surgery	10.1007/s11548-006-0004-1	mesh generation;computer vision;simulation;medicine;computer science;blood flow;finite element method;image segmentation;image-based meshing	Visualization	69.83522410059251	-46.03374974410128	174843
7405ebc6df920b7e157dd577651afa7d667dce4d	a robust and fast partitioning algorithm for extended target tracking using a gaussian inverse wishart phd filter	cardinality underestimation problem;gaussian inverse wishart probability hypothesis density filter;fuzzy adaptive resonance theory;extended target tracking;partitioning algorithm	Extended target Gaussian inverse Wishart probability hypothesis density (ET-GIW-PHD) filter is a promising filter. However, the exact filter requires all possible partitions of the current measurement set for updating, which is computationally intractable. In order to limit the number of partitions, we propose a robust and fast partitioning algorithm, called modified Bayesian adaptive resonance theory (MB-ART) partition, based on Bayesian ART neural network architecture. In MB-ART partition, the alternative partitions approximating all possible partitions of the measurement set are generated by the different vigilance parameters, and these parameters are obtained by the bisection method. In addition, MB-ART partition can also solve the cardinality underestimation problem caused by the separating tracks scenario which was investigated by Granström et al. [1], since it takes into account the shape information of the different sized extended targets by iteratively updating variance. Simulation results show that our proposed partitioning algorithm can well handle the cardinality underestimation problem caused by the separating tracks scenario and reduce computational burden without losing tracking performance. For a four-target tracking scenario, the ET-GIW-PHD filter using MB-ART partition only requires 8.391 s on average for one Monte Carlo run, while the ET-GIW-PHD filter using combination partition requires 14.834 s. It implies that the proposed MB-ART partition has good application prospects for the real-time extended target tracking (ETT) system. © 2015 Elsevier B.V. All rights reserved.	adaptive resonance theory;artificial neural network;automatic control;bisection method;categorization;cluster analysis;computation;computational complexity theory;electron tomography;emoticon;euro-vo;execution unit;expectation–maximization algorithm;gaussian blur;information retrieval;interval exchange transformation;inventory;k-means clustering;knowledge-based systems;koch snowflake;machine learning;monte carlo method;multidimensional digital pre-distortion;network architecture;neurocomputing;pattern recognition;real-time clock;remote file sharing;signature block;simulation;software propagation;source-to-source compiler;springer (tank);verse protocol;yang	Yongquan Zhang;Hongbing Ji	2016	Knowl.-Based Syst.	10.1016/j.knosys.2015.12.008	mathematical optimization;statistics	AI	76.46181812599644	-38.987276192496324	175304
34d9aee6a0a5bd5ddb4b330f2d75bbb1878acc8d	multi-dimensional space-time-frequency component analysis of event related eeg data using closed-form parafac	wigner distribution electroencephalography medical signal processing multidimensional signal processing neurophysiology time frequency analysis wavelet transforms;time varying;electroencephalography signal analysis neuroscience multidimensional signal processing wavelet analysis time frequency analysis signal processing signal processing algorithms signal resolution brain modeling;event related eeg;space time frequency;tensile stress;closed form parafac algorithm;time frequency;spectrum;parafac;tensor;parallel factor analysis;neuroscience;wavelet transforms;multi dimensional;brain modeling;wavelet transform;event related eeg data;wigner distribution;signal processing;multi dimensional signal processing;space time frequency component analysis;multidimensional signal processing;component analysis;signal resolution;wigner ville distribution space time frequency component analysis event related eeg data closed form parafac algorithm electroencephalography neuroscience multidimensional signal processing wavelet transformation parallel factor analysis;efficiency analysis;wigner ville distribution;neurophysiology;electroencephalography;time frequency analysis;medical signal processing;wavelet transformation;wigner ville distribution tensor multi dimensional signal processing parafac event related eeg;continuous wavelet transforms	The efficient analysis of electroencephalographic (EEG) data is a long standing problem in neuroscience, which has regained new interest due to the possibilities of multidimensional signal processing. We analyze event related multi-channel EEG recordings on the basis of the time-varying spectrum for each channel. It is a common approach to use wavelet transformations for the time-frequency analysis (TFA) of the data. To identify the signal components we decompose the data into time-frequency-space atoms using Parallel Factor (PARAFAC) analysis. In this paper we show that a TFA based on the Wigner-Ville distribution together with the recently developed closed-form PARAFAC algorithm enhance the separability of the signal components. This renders it an attractive approach for processing EEG data. Additionally, we introduce the new concept of component amplitudes, which resolve the scaling ambiguity in the PARAFAC model and can be used to judge the relevance of the individual components.	algorithm;catastrophic interference;electroencephalography;frequency analysis;image scaling;interference (communication);linear separability;multidimensional signal processing;parallel computing;relevance;rendering (computer graphics);time–frequency analysis;wavelet;wigner quasiprobability distribution	Martin Weis;Florian Roemer;Martin Haardt;Dunja Jannek;Peter Husar	2009	2009 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2009.4959592	speech recognition;time–frequency analysis;signal processing;pattern recognition;mathematics;neurophysiology;statistics;wavelet transform	Robotics	79.82946549919086	-39.02910225894014	175326
db2da6949bb59d0a86cf7ec8ae304b0cc11ca95d	mesh simplification method based on reverse interpolation loop subdivision		In this paper, a mesh simplification method based on a reverse interpolation loop subdivision is proposed. Combined with the treatment of extraordinary vertex in the improved butterfly subdivision, the loop subdivision mask is expanded, thereby improving the traditional loop subdivision algorithm into interpolation subdivision. The reverse operation of the interpolation loop subdivision is used to simplify the complex 3D mesh; a progressive mesh is generated by an initial mesh and a series of vertex offsets. The algorithm reduces the regular point relative reverse butterfly subdivision of compensation operation and greatly reduces simplification and reconstruction. Likewise, the reverse butterfly subdivision algorithm reduces the regular point compensation operation and greatly reduces simplification and reconstruction compared with the existing reverse loop subdivision by considering more control vertices. Furthermore, the edge point with respect to the center point is compensated by sacrificing a small amount of time to calculate the smaller vertex offset, this method gives high transmission speed and low offset. In addition, in future, it can be applied to the large-scale point cloud model simplification and other fields.	algorithm;interpolation imputation technique;level of detail;point cloud;progressive meshes;small;subdivision surface;symbolic computation;text simplification;vertex	Zhuo Shi;Yalei An;Songhua Xu;Zhongshuai Wang;Ke Yu;Xiaonan Luo	2017		10.1145/3036331.3036356	point cloud;interpolation;vertex (geometry);polygon mesh;mathematical optimization;subdivision;offset (computer science);subdivision surface;mathematics	Graphics	70.45774525157033	-43.97359111255949	176132
46e22cad56de245a040d47fe4ccf38b5744afac6	feature preserving delaunay mesh generation from 3d multi-material images	solid;computational geometry and object modeling i 3 5 curve;and object representations modeling packages;feature preservation;surface;mesh generation	Generating realistic geometric models from 3D segmented images is an important task in many biomedical applications. Segmented 3D images impose particular challenges for meshing algorithms because they contain multimaterial junctions forming features such as surface patches, edges and corners. The resulting meshes should preserve these features to ensure the visual quality and the mechanical soundness of the models. We present a feature preserving Delaunay refinement algorithm which can be used to generate high-quality tetrahedral meshes from segmented images. The idea is to explicitly sample corners and edges from the input image and to constrain the Delaunay refinement algorithm to preserve these features in addition to the surface patches. Our experimental results on segmented medical images have shown that, within a few seconds, the algorithm outputs a tetrahedral mesh in which each material is represented as a consistent submesh without gaps and overlaps. The optimization property of the Delaunay triangulation makes these meshes suitable for the purpose of realistic visualization or finite element simulations.	approximation algorithm;blackwell (series);ct scan;compiler;delaunay triangulation;eurographics;finite element method;mathematical optimization;medical imaging;mesh generation;refinement (computing);ruppert's algorithm;simulation	Dobrina Boltcheva;Mariette Yvinec;Jean-Daniel Boissonnat	2009	Comput. Graph. Forum	10.1111/j.1467-8659.2009.01522.x	mesh generation;computer vision;topology;ruppert's algorithm;mathematics;geometry;constrained delaunay triangulation;chew's second algorithm;solid;surface;bowyer–watson algorithm	Graphics	68.66810336177305	-45.257224081743026	176474
3dad9816d0e9cfe29dc9e200e77f02e1c332bcb0	multi-wavelet coherence for point processes on the real line	goodman distribution multiwavelet coherence point processes frequency domain stationary random processes linear dependency time scale space signals wavelet spectra orthogonal morse wavelets independent poisson processes;wavelet transforms coherence frequency domain analysis random processes signal processing statistical distributions stochastic processes time series;coherence continuous wavelet transforms density functional theory wavelet analysis spectral analysis;point processes wavelet coherence morse wavelets	Coherence is a well established measure of linear dependency between a pair of stationary random processes in the frequency domain. Wavelet coherence measures the linear dependency between a pair of signals in time-scale space and is therefore more suitable for non-stationary processes. Until now it has only been considered in relation to regularly sampled ordinary time-series. Here, for the first time, it is applied to point processes on the real line. We consider smoothing the individual wavelet spectra by averaging over a set of orthogonal Morse wavelets and show that under the assumption of independent Poisson processes the Goodman distribution is appropriate.	scale space;smoothing;stationary process;stochastic process;time series;wavelet transform	Edward A. K. Cohen	2014	2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2014.6854080	wavelet;combinatorics;mathematical analysis;mathematics;wavelet packet decomposition;discrete wavelet transform;fast wavelet transform;gabor wavelet;coherence;statistics	Robotics	79.0860283327495	-39.27943594242516	176862
526bf170429943666a64db2887c1cd0d629e3a2c	precise deformation of rheology msd model calibrated by randomized algorithm	personal computer;model calibration;shape deformation;randomized algorithm	In this paper, we propose and compare three kinds of mass-spring-damper (MSD) models of rheology object, and experimentally select the best model concerning to shape deformation and volume accuracy. The MSD model requires a few costs to calculate force propagation and shape deformation of rheology object. For this reason, the dynamic animation can be made by a personal computer within the video-frame rate (about twenty milli-seconds). Moreover, in order to maintain deformation precision, we calibrate all coefficients of dampers and springs under many experimental data by the randomized algorithm. Then in the set of simple pushing experimental operations, shape deformation and volume of virtual rheology object based on the best model is similar to these of real rheology object. This is a case study to generate dynamic animation efficiently and precisely by the MSD model.	randomized algorithm	Hiroshi Noborio;Ryo Nogami;Ryo Enoki	2003		10.2312/egs.20031055	computer vision;simulation;computer science;randomized algorithm	Vision	70.26470543927377	-47.28934419417212	177004
1f180210e8045f971e9e2bf55cdce49e0ff29514	image visualization and restoration by curvature motions	curvature scale space;65n99;65d18;topographic map;scientific visualization;53c21;53a04;partial differential equations;image reconstruction;mean curvature motion;curvature scale spaces;35k65;image shape analysis;level lines;affine curvature motion	This paper presents a review, analysis and comparison of numerical methods implementing the curvature motion and the affine curvature motion for 2D images, shapes, and curves. These curvature scale spaces allow, in principle, to compute an accurate multiscale curvature in digital images. The fastest and most invariant of them can be used in a complete image processing chain. This numerical chain simulates the accurate sub-pixel evolution of an image by mean curvature motion or by affine invariant curvature motion. To do so, it lets all the level lines of the image evolve by curvature shortening (of affine shortening), computes the image curvature directly on the smoothed level lines, and reconstructs the evolved image and its curvatures in an intrinsic, grid-independent representation. The paper describes a careful implementation of this chain, and analyzes its effects on many examples. The microscopic visualization of an image curvature map reveals after processing many image details. This image process improves graphic images, gets rid of compression and aliasing effects. It also gives an accurate tool to explore the validity of Attneave’s and Julesz theories on shape perception and texture discrimination. The “curvature microscope” runs on line for any image at http://www.ipol.im/pub/algo/cmmm_image_curvature_microscope/.	aliasing;circuit restoration;data compression;digital image;fastest;image processing;numerical method;pixel;scale space;smoothing;theory	Adina Ciomaga;Pascal Monasse;Jean-Michel Morel	2011	Multiscale Modeling & Simulation	10.1137/100791269	iterative reconstruction;willmore energy;total curvature;computer vision;topographic map;scientific visualization;theorema egregium;topology;fundamental theorem of curves;principal curvature;mean curvature;mathematics;geometry;curvature;partial differential equation	Vision	71.88000033798019	-45.14705938253355	177163
5ba8fc22a32bb8df139452c8c492c343ce1f8154	defeaturing cad models using a geometry-based size field and facet-based reduction operators	topology;computer aided design;defeaturing;boundary conditions;cad simplification;geometry;feature suppression;modifications;mathematical methods and computing;discrete model;mathematical models;boundary condition;facet reduction	We propose a method to automatically defeature a CAD model by detecting irrelevant features using a geometry-based size field and a method to remove the irrelevant features via facet-based operations on a discrete representation. A discrete B-Rep model is first created by obtaining a faceted representation of the CAD entities. The candidate facet entities are then marked for reduction using a geometry-based size field. This is accomplished by estimating local mesh sizes based on geometric criteria. If the field value at a facet entity goes below a user-specified threshold value then it is identified as an irrelevant feature and is marked for reduction. The reduction of marked facet entities is performed using various facet operators. Care is taken to retain a valid geometry and topology of the discrete model throughout the procedure. The original model is not altered as the defeaturing is performed on a separate discrete model. Associativity between the entities of the discrete model and that of original CAD model is maintained in order to decode the attributes and boundary conditions applied on the original CAD entities onto the mesh via the entities of the discrete model. Example models are presented to illustrate the effectiveness of the proposed approach.	3d modeling;algorithm;automatic identification and data capture;boundary representation;computer-aided design;entity;faceted classification;mathematical model;mesh generation;network topology;non-uniform rational b-spline;reduction (complexity);relevance;sensor;simulation;zero suppression	William Roshan Quadros;Steven J. Owen	2009	Engineering with Computers	10.1007/s00366-011-0252-8	combinatorics;boundary value problem;computer aided design;machine learning;mathematics;geometry;engineering drawing	SE	68.94063711116704	-43.56044791630816	177926
eb1a48bdd553258f7bbb24f96094e9421af96f29	fast computation of contact points for robotic simulations based on cad models without tessellation	geometry;computational modeling;image edge detection;solid modeling;robots;numerical models;solids	Computing multiple contact points between geometric models evolved in a virtual 3D environment is central to many robotic simulation applications. While this task can be performed efficiently and robustly between complex polyhedra, using the exact analytic geometric models issued by CAD modelers still suffers from efficiency limitations. Yet models composed of smooth surfaces are required to ensure smooth contact constraints, thus avoiding possible numerical artifacts which may dramatically affect the behavior of the system in the case of functional contacts. This paper builds on the observation that industrial CAD models are mostly composed of simple surfaces to perform an off-line identification of similar features and build a bounding volume hierarchy in order to locate potential contacts. Those are then computed by dedicated analytic methods, or an iterative root-finder, depending on the actual geometric representations of the features. In the context of dynamic simulation of robotic tasks, our method exhibits interactive computation times while naturally providing better result accuracy than existing polyhedron-specific algorithms.	artifact (software development);bounding volume hierarchy;computer-aided design;interactive computation;iterative method;numerical analysis;online and offline;polyhedron;robot;root-finding algorithm;simulation;tessellation (computer graphics)	Sébastien Crozet;Jean-Claude Léon;Xavier Merlhiot	2016	2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2016.7759162	robot;computer vision;simulation;computer science;artificial intelligence;theoretical computer science;solid;solid modeling;computational model	Robotics	69.50476934456175	-46.71231699137629	178553
b3c5da0d793a88e461a695a35b60f684d72c2ae8	managing coherent groups	coherent group;interesting steering behavior;robust algorithm;path planning;dynamic environment;common objective;boundary value problem;john wiley;single path;large group;group map;motion planning	The animation of groups of characters involves the generation of some interesting steering behaviors like following a single path, moving toward a common objective, and moving while keeping a formation. This paper presents a new approach to manage the movement of groups in dynamic environments using a simple and robust algorithm that includes a strategy to keep formations during the displacement of the group. Our method is based on a boundary value problem (BVP) involving Laplace’s equation and has two layers. In the first one, a group map is built to allow for local control of each individual, while in the second one a path planning is performed for each group as a whole. Results show that our technique is robust to several situations and can be implemented on GPU, which results in real-time performance for large groups. Copyright © 2008 John Wiley & Sons, Ltd.	algorithm;coherence (physics);displacement mapping;graphics processing unit;john d. wiley;motion planning;reactive planning;real-time clock	Renato Silveira;Edson Prestes e Silva;Luciana Porcher Nedel	2008	Journal of Visualization and Computer Animation	10.1002/cav.261	games;computer vision;simulation;computer science;artificial intelligence;machine learning;motion planning;computer graphics (images)	Graphics	72.02849506365767	-47.57415117298819	178605
d6fc3d726272df49483ce7b14820e6d55e2dc694	digital back-end for rfi detection and mitigation in microwave radiometers	rfi mitigation;spectrogram analysis;statistical analysis digital back end rfi detection rfi mitigation microwave radiometers radiofrequency interference time domain analysis frequency domain analysis spectrogram analysis wavelet based rfi mitigation technique signal interference denoising capabilities wavelet transform rfi signal estimation rfi mitigated noise signal;wavelet transforms frequency domain analysis geophysical signal processing interference suppression microwave detectors radiofrequency interference radiometry signal denoising statistical analysis time domain analysis;clocks;digital back end;frequency domain analysis;microwave detectors;signal interference;rfi detection;microwave radiometry;radiofrequency interference;time domain analysis;denoising capabilities;wavelet transforms;interference suppression;radiometry;microwave radiometers;microwave radiometry field programmable gate arrays remote sensing microwave theory and techniques radiofrequency interference clocks;wavelet transform;statistical analysis;geophysical signal processing;remote sensing;conference report;wavelet based rfi mitigation technique;rfi signal estimation;rfi mitigated noise signal;field programmable gate arrays;microwave theory and techniques;rfi mitigation wavelet radiometry;wavelet;signal denoising	Radio-frequency interference (RFI) is a serious problem in microwave radiometry. In order to process the received data in real time, a powerful back-end for RFI detection and mitigation system must be implemented. This system includes many techniques against RFI, like time, frequency domain and spectrogram analysis. A wavelet-based RFI-mitigation technique is implemented in hardware. The interfering signal is estimated by using the powerful denoising capabilities of the wavelet transform. Then, this estimate of the RFI signal is subtracted from the total received signal to obtain a RFI-mitigated noise signal. Statistical analysis helps to validate the mentioned methods.	ddos mitigation;digital camera back;download;interference (communication);microwave;noise reduction;spectrogram;wavelet transform	Giuseppe Forte;Adriano Camps;Isaac Ramos-Pérez;Mercè Vall-Llossera	2012	2012 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2012.6350706	telecommunications;statistics;remote sensing;wavelet transform	Embedded	81.72827849166954	-40.964218737369194	178670
cb442f83196b1817cf76783842e1c5995d879e5f	optimal quantization noise allocation and coding gain in transform coding with two-dimensional morphological haar wavelet	two-dimensional morphological haar wavelet;basic nonlinear wavelet;numerical experiment;transform coding;coding gain;cumbersome nonlinear operator;paper analytically;nonlinear wavelet;appropriate approximation;optimal quantization noise allocation;quantization noise	This paper analytically formulates both the optimal quantization noise allocation ratio and the coding gain of the two-dimensional morphological Haar wavelet transform. The two-dimensional morphological Haar wavelet transform has been proposed as a nonlinear wavelet transform. It has been anticipated for application to nonlinear transform coding. To utilize a transformation to transform coding, both the optimal quantization noise allocation ratio and the coding gain of the transformation should be derived beforehand regardless of whether the transformation is linear or nonlinear. The derivation is crucial for progress of nonlinear transform image coding with nonlinear wavelet because the two-dimensional morphological Haar wavelet is the most basic nonlinear wavelet. We derive both the optimal quantization noise allocation ratio and the coding gain of the two-dimensional morphological Haar wavelet transform by introducing appropriate approximations to handle the cumbersome nonlinear operator included in the transformation. Numerical experiments confirmed the validity of formulations. key words: coding gain, morphological Haar wavelet, nonlinear transform coding, nonlinear wavelet, optimal quantization noise allocation	approximation;autoregressive model;coding gain;coefficient;experiment;haar wavelet;nonlinear system;numerical method;pixel;quantization (signal processing);transform coding;wavelet transform	Yasunari Yokota;Xiaoyong Tan	2005	IEICE Transactions		wavelet;constant q transform;mathematical optimization;transform coding;speech recognition;s transform;harmonic wavelet transform;quantization;second-generation wavelet transform;continuous wavelet transform;computer science;coding gain;cascade algorithm;mathematics;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;lifting scheme;statistics;wavelet transform	Vision	75.64613105988927	-40.645160702843974	179065
f7f66d54fffad70da63f5c0fc3f12b021ad6292e	real-time marching cubes on the vertex shader		In this paper we propose a new approach for visualizing volumetric datasets by their isosurfaces. For an interactive isosurface reconstruction an optimized version of the well-known marching cubes algorithm is used. We extend the original algorithm by an additional vertex shader program. Contrary to other hardware-accelerated solutions our program is not based on a tetrahedral algorithm and thus the implementation for structured grids is more effective. Furthermore, surfaces of time-varying datasets at distinguished threshold values can be extracted in real-time.	algorithm;hardware acceleration;isosurface;marching cubes;olap cube;real-time clock;real-time transcription;shader;whole earth 'lectronic link	Frank Goetz;Theodor Junklewitz;Gitta Domik	2005		10.2312/egs.20051010	asymptotic decider;parallel computing;marching tetrahedra;computer science;theoretical computer science;isosurface;marching cubes;vertex;computer graphics (images)	Visualization	68.49133904716248	-50.67340626914882	179334
12d6ea3631bc324295ba795a657e030b2ee04495	improved shape for multi-surface blends	bi quintic;parameterization;multi sided;surface;shape curvature multi surface	For many design applications, where multiple primary surface pieces meet, the distribution of curvature is more important than formally achieving exact curvature continuity. For parametric spline surfaces, when constructing a multi-sided surface cap, we demonstrate a strong link between the uniform variation of the re-parameterization between (boundary) data of the joining pieces and a desirable distribution of curvature. We illustrate this interdependence between parameterization quality and surface quality by developing a G bi-quintic surface cap consisting of n pieces that smoothly fills holes in a piecewise bi-cubic tensor-product spline complex. These bi-5 surface caps have arguably better shape than higher-degree, formally curvature continuous alternatives.	bicubic interpolation;business continuity;cubic function;interdependence;quintic function;scott continuity;smoothing;spline (mathematics)	Kestutis Karciauskas;Jörg Peters	2015	Graphical Models	10.1016/j.gmod.2015.06.006	parametrization;mathematical optimization;topology;principal curvature;mean curvature;mathematics;geometry;curvature;surface	Graphics	68.80983102208951	-41.760830329342035	179364
035c7e000195255f6c8c550f6ff7a89501ee5351	automatic mesh generation and transformation for topology optimization methods	cad fea integration;multiple domains;topology optimization;conforming boundaries;advancing front;b rep;mesh generation	This paper presents automatic tools aimed at the generation and adaptation of unstructured tetrahedral meshes in the context of composite or heterogeneous geometry. These tools are primarily intended for applications in the domain of topology optimization methods but the approach introduced presents great potential in a wider context. Indeed, various fields of application can be foreseen for which meshing heterogeneous geometry is required, such as finite element simulations (in the case of heterogeneous materials and assemblies, for example), animation and visualization (medical imaging, for example). Using B-Rep concepts as well as specific adaptations of advancing front mesh generation algorithms, the mesh generation approach presented guarantees, in a simple and natural way, mesh continuity and conformity across interior boundaries when trying to mesh a composite domain. When applied in the context of topology optimization methods, this approach guarantees that design and non-design sub-domains are meshed so that finite elements are tagged as design and non-design elements and so that continuity and conformity are guaranteed at the interface between design and non-design sub-domains. The paper also presents how mesh transformation and mesh smoothing tools can be successfully used when trying to derive a functional shape from raw topology optimization results.	mathematical optimization;mesh generation;topology optimization	Jean-Christophe Cuillière;Vincent François;Jean-Marc Drouet	2013	Computer-Aided Design	10.1016/j.cad.2013.07.004	mesh generation;mathematical optimization;topology optimization;simulation;computer science;engineering;mathematics;engineering drawing;t-vertices	EDA	69.80596955679631	-45.332723556146085	179640
edeff61e0a5c0ac6f668540c0975ff144f57613c	nonlinear spline generation with curve evolutions driven by curvature	nonlinear spline;spline;image processing;nonlinear spline design;curve evolution;boundary conditions;end points;curve evolutions;computational geometry;shape analysis;spline computer vision cities and towns design methodology boundary conditions equations active shape model image analysis image processing active contours;active contours;satisfiability;computational geometry splines mathematics curve fitting;splines mathematics;computer vision;closed curves;curvature values;boundary condition;discrete analogs;normal direction;polygonal curves;speed function;cities and towns;tangent directions;curvature;image analysis;discrete analogs nonlinear spline generation curve evolutions curvature nonlinear spline design end points boundary conditions curvature values tangent directions normal direction curvature derivatives speed function nonlinear spline closed curves multiscale shape analysis polygonal curves;curvature derivatives;curve fitting;multiscale shape analysis;active shape model;nonlinear spline generation;design methodology	The paper develops a method to design nonlinear splines on a plane via curve evolutions driven by curvature. We consider a curve passing through two given end points and satisfying prescribed boundary conditions at them (for example, curvature values or tangent directions are specified at the end points). Each point of the curve moves in the normal direction with speed equal to a function of the curvature and curvature derivatives at the point. Choosing the speed function properly, the evolving curve converges to a desired nonlinear spline. We also consider evolutions of closed curves for purposes of multiscale shape analysis. Smooth curve evolutions are approximated by evolutions of polygonal curves. Discrete analogs of the curvature and its derivatives are considered.	spline (mathematics)	Alexander G. Belyaev;Elena V. Anoshkina;Shin Yoshizawa	1999		10.1109/SMA.1999.749334	center of curvature;total curvature;mathematical optimization;scalar curvature;topology;fundamental theorem of curves;osculating circle;asymptotic curve;mathematics;geometry;torsion of a curve;curvature;flat spline;curve fitting;radius of curvature	Robotics	70.19432046791206	-40.76351930739932	180089
4f58ed2698770fb0e9f890c9070b41f796cfb331	surface simplification of 3d animation models using robust homogeneous coordinate transformation		The goal of 3D surface simplification is to reduce the storage cost of 3Dmodels. A 3D animation model typically consists of several 3D models. Therefore, to ensure that animation models are realistic, numerous triangles are often required. However, animation models that have a high storage cost have a substantial computational cost. Hence, surface simplification methods are adopted to reduce the number of triangles and computational cost of 3D models. Quadric error metrics (QEM) has recently been identified as one of the most effective methods for simplifying static models. To simplify animation models by using QEM, Mohr and Gleicher summed the QEM of all frames. However, homogeneous coordinate problems cannot be considered completely by using QEM. To resolve this problem, this paper proposes a robust homogeneous coordinate transformation that improves the animation simplification method proposed by Mohr and Gleicher. In this study, the root mean square errors of the proposed method were compared with those of the method proposed by Mohr and Gleicher, and the experimental results indicated that the proposed approach can preserve more contour features than Mohr’s method can at the same simplification ratio.	3d modeling;algorithmic efficiency;computation;computer animation;level of detail;mean squared error;text simplification	Juin-Ling Tseng	2014	J. Applied Mathematics	10.1155/2014/189241	mathematical optimization;mathematics;geometry	Graphics	68.421963268065	-45.722781925558685	180433
957a01d3e7f587f3a68736526e8037d9f8115f9b	streaming compression of tetrahedral volume meshes	object representation;processing;general and miscellaneous mathematics computing and information science;mesh compression;buffers;geometry;large data sets;compressors;batch process;connectivity coding;algorithms;tetrahedral meshes;compression;stream processing;geometry processing;geometry streaming;tetrahedral mesh;random access;99 general and miscellaneous mathematics computing and information science;volume data	Geometry processing algorithms have traditionally assumed that the input data is entirely in main memory and available for random access. This assumption does not scale to large data sets, as exhausting the physical memory typically leads to IO-inefficient thrashing. Recent works advocate processing geometry in a “streaming” manner, where computation and output begin as soon as possible. Streaming is suitable for tasks that require only local neighbor information and batch process an entire data set. We describe a streaming compression scheme for tetrahedral volume meshes that encodes vertices and tetrahedra in the order they are written. To keep the memory footprint low, the compressor is informed when vertices are referenced for the last time (i.e. are finalized). The compression achieved depends on how coherent the input order is and how many tetrahedra are buffered for local reordering. For reasonably coherent orderings and a buffer of 10,000 tetrahedra, we achieve compression rates that are only 25 to 40 percent above the state-of-the-art, while requiring drastically less memory resources and less than half the processing time. CR Categories: I.3.5 [Computational Geometry and Object Modeling]: Curve, surface, solid, and object representations	algorithm;batch processing;benchmark (computing);coherence (physics);compressed sensing;computation;computational geometry;computer data storage;data structure;encoder;geometry processing;hash table;ibm notes;jason;kerrison predictor;memory footprint;pipeline (unix);polygon mesh;random access;requirement;scalability;stream (computing);streaming simd extensions;streaming algorithm;streaming media;surface-mount technology;thrashing (computer science);time complexity;triangle mesh;udo of aachen;vertex (geometry);visual computing	Martin Isenburg;Peter Lindstrom;Stefan Gumhold;Jonathan Richard Shewchuk	2006		10.1145/1143079.1143098	stream processing;gas compressor;computer science;processing;theoretical computer science;machine learning;mathematics;geometry;programming language;compression;algorithm;random access;batch processing;computer graphics (images)	Graphics	68.544837232716	-50.90894504225532	180522
89bf6db3f4c7e27e93fd3fec8dea210893df7cbc	smoke surfaces: an interactive flow visualization technique inspired by real-world flow experiments	surface structures;topology;interactive flow visualization;image motion analysis;smoke visualization index terms 8212 unsteady flow visualization streak surfaces;smoke visualization;unsteady flow visualization;semitransparent streak surface;testing;index terms 8212;indexing terms;smoke rendering;computational fluid dynamics;data visualisation;flow visualisation;semitransparent streak surface smoke rendering interactive flow visualization;gases;interactive application;shape;wool;index terms unsteady flow visualization;unsteady flow;data visualization;smoke;data visualization wool rendering computer graphics shape testing gases surface structures topology image motion analysis;rendering computer graphics;smoke computational fluid dynamics data visualisation flow visualisation interactive systems rendering computer graphics;interactive systems;flow visualization;streak surfaces	Smoke rendering is a standard technique for flow visualization. Most approaches are based on a volumetric, particle based, or image based representation of the smoke. This paper introduces an alternative representation of smoke structures: as semi-transparent streak surfaces. In order to make streak surface integration fast enough for interactive applications, we avoid expensive adaptive retriangulations by coupling the opacity of the triangles to their shapes. This way, the surface shows a smoke-like look even in rather turbulent areas. Furthermore, we show modifications of the approach to mimic smoke nozzles, wool tufts, and time surfaces. The technique is applied to a number of test data sets.	bottle, device;computation;computer data storage;graphics processing unit;imagery;inspiration function;interactive visualization;mammal in fiber production;normal (geometry);out-of-core algorithm;preparation;semiconductor industry;smoke testing (software);test data;time complexity;turbulence	Wolfram von Funck;Tino Weinkauf;Holger Theisel;Hans-Peter Seidel	2008	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2008.163	computer vision;simulation;flow visualization;computer science;mathematics;geometry;data visualization;statistics;computer graphics (images)	Visualization	71.66296027983061	-49.63117367642764	180622
cbd5bdd71e0b347451c502d217e99a8e7cbc24a6	simulating wrinkles and skin aging	real time;facial animation	We describe a methodology and framework to simulate facial animation and skin aging, taking into account skin texture and wrinkle dynamics. We split the facial simulation process into facial surface deformation and wrinkle generation. The first of these is based on a three-layered facial structure (muscle, connective tissue, and skin layers). The layer of B-spline patch muscles provides the relevant contraction forces to enable the skin deformation, while the layer of connective tissues constrains the range of skin movement. The wrinkle generation uses a synthetic texture, and wrinkles are formed dynamically, thanks to a linear plastic model. The wrinkle generation and rendering can be incorporated into real-time facial animation systems.	b-spline;blue moon rendering tools;computer animation;facial recognition system;finite element method;logical connective;mathematical model;polygonal modeling;real-time clock;real-time locating system;real-time transcription;simulation;skin (computing);switzerland;synthetic intelligence;texture mapping;unbiased rendering	Yin Wu;Prem Kumar Kalra;Laurent Moccozet;Nadia Magnenat-Thalmann	1999	The Visual Computer	10.1007/s003710050171	computer facial animation;computer science;computer graphics (images)	Graphics	69.16602222006152	-47.73729236948462	180803
f5d0f5af9c79fdbf2ff9360af7a1e3095b7cdbf6	automatic block decomposition using fuzzy logic analysis	fuzzy logic	A method is presented for the automatic block decom position of complex multi-component closed volumes. Decompositions produced by this method re main unchanged when the geometry undergoes minor parametric changes. This feature is attracti ve in time-varying geometry or design optimization problems. Examples of the application of this meth od to all-hexahedral mesh generation are also prese nt d. Tetrahedral versus hexahedral meshes In most engineering applications, automatic tetrahedral (tet) meshing offers a perfectly viable solution. However, hexahedral (hex) meshing is still in demand due to a host of reasons that are not always technical in nature: 1. Certain applications do not accept tets or cannot properly and accurately use tets 2. There is sometime a lack of symmetry or downright lack of accuracy associated with tets 3. Tet meshes may require up to an order of magnitude more elements than equivalent hex meshes, especially for boundary layers or high aspect ratio parts. 4. All things being equal, in general, applications exact a higher quality from tets than hexes 5. Tet meshes are harder to view and repair. For instance, the vertices of a one millionnode tet mesh, scaled to an XGA screen, will hardly leave any pixels for the cursor, let alone node labels. Additional requirements of variable geometry applications Variable geometry applications such as CFD in combustion chambers and stirred tank reactors, metal forming applications and geometric design optimization may have additional mesh requirements. Indeed, most engineering applications are conducted with mesh densities that are sub-optimal, and it is desirable that during geometry change, mesh adaptation be achieved mostly through stretching (homeomorphic transformation) of the mesh while maintaining element connectivity intact for as long as possible. In this fashion, we can keep the number of rezonings to a minimum, and minimize truncation errors. We will call this a robust mesh. In manual mesh generation, a smart choice of the initial block decomposition relies on a robust block decomposition that enables the user to maintain the same blocking throughout the geometry change, limiting rezoning to the addition or removal of a few element layers here and there. In Delauney, advancing front or “Lego-type” mesh generation methods, there is no guaranty that a small parametric change in the geometry will not initiate a complete reshuffling of the mesh connectivity, thereby masking the changes we are looking for. Mapped meshes are, on the other hand, more robust. Fuzzy logic block decomposition In the context of this presentation, fuzzy logic block decomposition means a systematic investigation and grading of a very large number of admissible block decomposition alternatives, and selection of the one with the highest grade. Takahashi et al [1][2] and Takizawa et al [3] have proposed a fuzzy logic approach to twodimensional geometry recognition, and have successfully applied it to complex mesh generation problems. Here, we approach the problem of block decomposition as the search for the coarsest possible all-hex mesh of a closed volume. We are also looking for a robust (as previously defined) mesh that remains unchanged should the geometry undergo “small” homeomorphic transformations. We will investigate a large number of coarse surface quad meshes, select those that are necessarily the surface mesh of a blockstructured volume mesh, grade each resulting volume mesh based on the quality of its elements, modify the surface quad mesh to achieve a better grade and so on. Automatic surface partition using fuzzy reasoning Step 1: Creating the coarsest block-structured mesh Consider an arbitrary closed triangular surface meshS (Figure 1). Let’s assume that we already have a structured block decomposition of its internal volume. Vertices A, B, C, D, E, F, G and H represent the vertices of the block decomposition. { } Ai is the set of quadrilateral elements constituting the outer surface of the block decomposition (here, { } Ai contains 6 quads, ADHE, BCGF, ABFE, DCGH, ABCD and EFGH). Each quad Ai is a collection of neighboring triangles. { } Ai with [ ] n i , 1 = , defines a partition of S :	abcd schema;blocking (computing);cursor (databases);disk partitioning;fuzzy logic;geometric design;graphics display resolution;hexahedron;logic block;mathematical optimization;mesh generation;pixel;requirement;robustness (computer science);tomotaka takahashi;truncation;volume mesh	Reza Taghavi	2000			logic optimization;fuzzy logic;fuzzy associative matrix;discrete mathematics;fuzzy control system;mathematics	Graphics	69.06480045801202	-43.42850376217756	180862
fee2cb50b808892768cbdc1f5a52b55b3e32aa78	analysis of sunspot time series (1749-2014) by means of 0-1 test for chaos detection	electronic mail;lyapunov spectrum basis sunspot time series analysis ad 1749 01 to 2014 12 binary 0 1 test chaos detection method nonstationary time series chaotic characteristic analysis monthly mean relative sunspot time series;metals;chaos detection;chaos;linear regression;data mining;time series analysis chaos reactive power electronic mail linear regression metals correlation;time series analysis;0 1 test;data mining sunspot numbers time series chaos detection 0 1 test;sunspots astronomical techniques;correlation;sunspot numbers time series;reactive power	We present here an implementation of the novel test called binary 0-1 test which is a chaos detection method and applied to the sunspot time series. This method seems to represent a useful improvement for the non-stationary time series chaotic characteristic analysis and computationally low cost. It is concluded that the monthly mean relative sunspot time series from January 1749 to December 2014 is inherently chaotic based on the outputs of the test. Additionally, a more detailed characterization of the sunspot time series is carried out on Lyapunov spectrum basis.	lyapunov fractal;stationary process;time series	Qingtai Xiao;Shibo Wang;Zhan Zhang;Jian-Xin Xu	2015	2015 11th International Conference on Computational Intelligence and Security (CIS)	10.1109/CIS.2015.60	econometrics;linear regression;calculus;time series;mathematics;ac power;order of integration;correlation;statistics	EDA	78.82475879027496	-40.03055234853311	180940
cd72e635e2a6f9d26afec8145a0bb63792171e03	efficient wideband sonar parameter estimation using a single slice of radon-ambiguity transform	radon transforms;wideband;radon transform;narrowband ambiguity function;working environment noise;time delay estimation;complex white gaussian noise;delay effects;wideband ambiguity function;awgn;radon ambiguity transform;time delay;sonar detection;moving targets;signal processing;overlapping targets;target parameter estimation;wideband sonar parameter estimation;wideband sonar parameter estimation delay effects delay estimation signal processing working environment noise chirp modulation narrowband matched filters;matched filters;doppler stretch estimation;multiple targets;white gaussian noise;sonar signal processing parameter estimation delay estimation sonar detection radon transforms awgn;chirp modulation;parameter estimation;target return signal detection;sonar signal processing;ambiguity function;target return signal detection wideband sonar parameter estimation radon ambiguity transform time delay estimation doppler stretch estimation narrowband ambiguity function wideband ambiguity function radon transform multiple targets moving targets target parameter estimation complex white gaussian noise overlapping targets;narrowband;delay estimation;sonar	A novel efficient technique, based on a Radon-ambiguity transform (RAT), for time delay and Doppler stretch estimation is presented. The proposed approach combines the narrowband ambiguity function (NAF), the wideband ambiguity function (WAF), and the Radon transform (RT) to estimate multiple targets in noisy environments. The main ridges of NAF represent straight lines whose slopes depend on the Doppler rates of the moving targets. These lines could be effectively detected by computing the RT of the NAF for all possible angles. However, the computation of RT for all possible angles is computationally exhaustive. It is shown that, without calculating the entire RAT, it is possible to estimate target parameters using only a single slice of RAT, i.e., using an appropriate projection of the NAF. The resolution issue and the effect of the integration length of RAT in complex white Gaussian noise are also discussed. It is demonstrated that the proposed method can successfully separate overlapping targets.	ambiguity function;broadcast delay;computation;doppler effect;estimation theory;nato architecture framework;sonar (symantec);waf	Md. Raihan Sharif;Saman S. Abeysekera	2005	Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.	10.1109/ICASSP.2005.1416376	additive white gaussian noise;telecommunications;computer science;signal processing;mathematics;statistics	Robotics	82.85600984282907	-39.62186650058503	181030
4e45851e128176bcb7aeb400702fbba3616a6ff9	visualization of connectivity in octree based models	octree;porous media;stochastic modelling;visualization of fluid penetration	An interesting problem in the oil and gas industry is the visualization of the movement of oil and gas in porous media. An example of such a medium is a rock sample with some distribution of holes (pores) connected by channels (pore throats), the solid parts of the rock are called grains. In our work we have simulated the porous medium using a pointer-based octree, representing these pores and grains. This data structure allows us to model the connectivity of the pores and thus visualize fluid penetration within the medium. Whereas earlier models represent a serious simplifications or two dimensional homogeneous layers, our model provides us with a statistically accurate distribution in three dimensions and a more accurate representation of the connectivity. In this paper we present our data structure and the techniques which were used to create models of porous media and their porous networks. Next, we present aigorithms for connectivity in octrees and we show how to apply them to modelling and visualization of fluid penetration in porous media.	data structure;octree;pointer (computer programming)	Leon L. Fedenczuk;Brian Wyvill	1995	Journal of Visualization and Computer Animation	10.1002/vis.4340060306	simulation;computer science;stochastic modelling;theoretical computer science;porous medium;octree;computer graphics (images)	Visualization	71.76427030926598	-48.34518711775736	181240
df86482a0bded46b90b79ab76f8188a9737483b5	geometric rounding and feature separation in meshes		Geometric rounding of a mesh is the task of approximating its vertex coordinates by floating point numbers while preserving mesh structure. Geometric rounding allows algorithms of computational geometry to interface with numerical algorithms. We present a practical geometric rounding algorithm for 3D triangle meshes that preserves the topology of the mesh. The basis of the algorithm is a novel strategy: 1) modify the mesh to achieve a feature separation that prevents topology changes when the coordinates change by the rounding unit; and 2) round each vertex coordinate to the closest floating point number. Feature separation is also useful on its own, for example for satisfying minimum separation rules in CAD models. We demonstrate a robust, accurate implementation.	algorithm;computational geometry;computer-aided design;numerical analysis;rounding	Victor J. Milenkovic;Elisha Sacks	2019	Computer-Aided Design	10.1016/j.cad.2018.10.003	mathematical optimization;triangle mesh;vertex (geometry);computational geometry;floating point;mathematics;polygon mesh;rounding	Graphics	68.66226371294032	-43.67254352445454	181544
4395a6b51cebccd7a5e9b73b87719f294059ba3c	ray casting for collision detection in haptic rendering of volume data	surgical planning;collision response;time complexity;user studies;data type;force feedback;collision detection;haptic rendering;medical training;augmented reality;daylighting design;ray casting;volume data	A haptic exploration adds an additional dimension to working with 3D data: a sense of touch (figure 1). This is especially useful in areas such as medical training and pre-surgical planning, entertainment, CAD and others. Each haptic rendering frame consists of three stages: collision detection, collision response and force-feedback generation. In order to feel the 3D data smoothly, an update rate of at least 1 kHz is required [Brooks Jr. et al. 1990]. Unaddressed practical problems for almost all haptic rendering methods are that no guarantees for collision detection could be given and/or that a special topological structure of objects is required. Here we present an approach which does not have these drawbacks. Furthermore our algorithm has nearly constant time complexity independent of data resolution and does not require any additional precomputed structures. We focus on volumetric voxel data, since that is the direct input from the scanning devices. Other data types could be transformed to this one, if necessary.	algorithm;collision detection;computer-aided design;haptic technology;precomputation;ray casting;smoothing;time complexity;voxel	Roman Vlasov;Karl-Ingo Friese;Franz-Erich Wolter	2012		10.1145/2159616.2159661	time complexity;computer vision;augmented reality;collision response;simulation;data type;computer science;artificial intelligence;ray casting;haptic technology;programming language;collision detection;computer graphics (images)	Robotics	68.42122858497589	-49.60902397544469	181923
d12d879b4319d5d4b424b6782ad2aa6d265b70c1	g2 quasi-developable bezier surface interpolation of two space curves	surface energy;gaussian curvature;developable surfaces;ruled surfaces;garment design	Surface development is used in many manufacturing planning operations, e.g., for garments, ships and automobiles. However, most freeform surfaces used in design are not developable, and therefore the developed patterns are not isometric to the original design surface. In some domains, the CAD model is created by interpolating two given space curves. In this paper, we propose a method to obtain a G^2 quasi-developable Bezier surface interpolating two arbitrary space curves. The given curves are first split into a number of piecewise Bezier curves and elemental Bezier patches each of which passes through four splitting points are constructed. All neighboring elemental patches are G^2 connected and they are assembled optimally in terms of the degree of developability (the integral Gaussian curvature). Experiments show that the final composite Bezier surface is superior to a lofted one which is defined regardless of the final surface developability.	gnutella2;interpolation	Ming Chen;Kai Tang	2013	Computer-Aided Design	10.1016/j.cad.2013.06.009	surface energy;gaussian curvature;mathematical optimization;topology;developable surface;mathematics;geometry	EDA	68.93919185581723	-42.124864216906104	182161
e62d0829f4786685bdd2cc6045f6bdcd7ac06c2d	visualization of scientific video data using kl decomposition	real time visualization;videocommunication;systeme temps reel;videocomunicacion;algorithm performance;image processing;visualizacion;video signal processing;etude experimentale;real time;video analysis;procesamiento imagen;real time scientific visualization scientific video data visualization kl decomposition video data classification best coordinate system temporally ordered image collection separable approximations physically significant visualizations combustion neurobiology relevant information extraction video acquisition process real time feedback video analysis;traitement image;scientific visualization;algorithme;algorithm;data visualisation;visualization;karhunen loeve transforms;col;visualisation;visualization technique;resultado algoritmo;image sequence;performance algorithme;real time system;secuencia imagen;sistema tiempo real;neurophysiology;natural sciences computing;estudio experimental;data visualization combustion laboratories fires fuels physics computing data mining neurofeedback real time systems;combustion;sequence image;coordinate system;real time systems;image sequences;algoritmo;video signal processing data visualisation natural sciences computing combustion neurophysiology image sequences real time systems karhunen loeve transforms	Fast methods are developed for visualizing and classifying certain types of scientific video data. These techniques, which are based on KL decomposition, find a best coordinate system for a data set. When the data set represents a temporally ordered collection of images, the best coordinate system leads to approximations that are separable in time and space. Practical methods for computing this best coordinate system are discussed, and physically significant visualizations for experimental video data are developed. The visualization techniques are applied to two experimental systems — one from combustion and the other from neurobiology to show how relevant information can be quickly extracted from video data. These techniques can be integrated into the video acquisition process to provide real-time feedback to the experimentalist during the operation of an experiment. Keywords—Scientific visualization, real-time visualization, video analysis.	approximation;experiment;experimental system;real-time clock;real-time locating system;scientific visualization;singular value decomposition;temporal logic;video content analysis	Kay A. Robbins	1998	IEEE Trans. Vis. Comput. Graph.	10.1109/2945.765327	computer vision;scientific visualization;simulation;visualization;image processing;computer science;theoretical computer science;neurophysiology;data visualization;computer graphics (images)	Visualization	71.40771368084886	-51.16402985658885	182208
ba789181ace6cf231c08dc0bca7dc632eb7775ba	a novel geometric tolerance modeling inspired by parametric space envelope		Tolerance is an essential part of design and manufacturing. It plays a key role in product quality and manufacturing costs. Understanding and controlling production variations on key geometric features can provide firms with a competitive edge. A model to link production variation to tolerance is highly desirable but difficult to build, especially for deformable parts with complex surfaces. Inspired by an innovative idea of volumetric space envelope (constructed from a base parametric curve), this paper proposes a novel spatial tolerance model. In this proposal, a volumetric envelope is superimposed onto the target manufacturing part, whose deformation and deviation (during manufacturing or assembly) are viewed as spatial variation, and this variation is modeled and linked to movements of envelope’s control points. This unique model design bypasses direct modeling of complex intrapart interactions, which is nonlinear in general and a major source of inaccuracy and low efficiency of many existing methods. The adopted indirect modeling brings many benefits. It can handle complex shapes and surfaces, and is able to take into account form errors. Also, it is capable of modeling both global and local variations observed in many practical cases. The new method is illustrated and verified through an example on a deformable vehicle door hinge plate. The proposed model shows application potential in every major stage of production, and makes possible to build a coherent cross-production life-cycle tolerancing framework from the early stage design, to manufacturing, to postproduction quality inspection. Note to Practitioners—Tolerance is a matter of everyday life to manufacturing and assembly engineers and designers. It directly impacts the product quality and production costs. Driven by the client’s ever-increasing variety needs, more and more deformable parts with complex surfaces have been entering into production in the past decade. However, existing degree of freedom (DOF) concept-based models, surrounding the idea of six DOFs of a rigid body, have difficulty in handling. While the decomposed type of models may help provide useful insight into the geometric variation, they are dependent on reliable measurement data, which may not be readily available. Motivated by the idea of deforming a manufacturing part indirectly through reshaping its surrounding (purposely designed) parametric space envelope (PSE), this paper proposes a novel spatial model for tolerancing. The proposed model is intuitive and is able to provide insight into geometric variations by visualizing them. Due to its novel model design, the proposed method can handle some fairly complex parts and surfaces (be it parametric or implicit) and is able to take into account form errors. The base curve to construct the PSE can be a Bezier curve, which is commonly available in computer-aided design/computer-aided manufacturing systems and is familiar to many practitioners (designers and engineers). This could substantially lower the application hurdle and open up a potentially wide application for the proposed method.	assembly language;automatic number plate recognition;bézier curve;coherence (physics);computation;computer-aided design;control point (mathematics);interaction;nonlinear system	Chen Luo;Pasquale Franciosa;Darek Ceglarek;Zhonghua Ni;Fang Jia	2018	IEEE Transactions on Automation Science and Engineering	10.1109/TASE.2018.2793920	geometric dimensioning and tolerancing;bézier curve;mathematical optimization;computer science;competitive advantage;parametric equation;control engineering;hinge;nonlinear system;spatial variability;parametric statistics	Robotics	70.8560283528966	-38.18247019493551	182331
11b8c5f45da3afc1f3054c32d30a4ee67ae26711	facial expression composition using dynamic displacement mapping	facial expressions;simulation;displacement mapping	Construction and animation of realistic human facial models is still a challenging task in computer graphics. In this paper, we present a practical way to generate facial expressions using dynamic vector displacement mapping. We describe the construction of displacement maps from scanned data, parametric virtual muscles and from feature points with influence. To produce facial expressions our approach dynamically combines multiple displacement maps using image composition and blending techniques. Dynamic skin behavior is simulated using spring--mass system driven by the generated displacement we call an expression map. This approach allows parametric animation of the facial model in real-time and can be entirely implemented on the GPU. Final composition and rendering of our models utilizes photorealistic lighting and shading of the facial skin.	alpha compositing;computer graphics;displacement mapping;graphics processing unit;map;parametric animation;real-time transcription;shading	Peter Drahos	2011		10.1145/2461217.2461223	computer vision;simulation;displacement mapping;computer science;facial expression;computer graphics (images)	Graphics	69.15288441185952	-47.61914342818608	182481
10f70283095e92469212d8317323e7e21de7ed1e	analysis of time delay stability of force reflection telepresence system in elastic environment	reservoirs;project management;management system;spatial data;geographic information;data collection;information technology;water resources;data engineering;embedded system;technology management;seasonality;agricultural engineering;flood control;mixed mode;humans;floods;reservoirs floods water resources humans agricultural engineering data engineering hardware project management information technology technology management;data acquisition;spatial information;hardware	In force reflection telepresence system there exits communication delay and instable feedback. To improve the accuracy of system, it is essential to set up a force reflection telepresence time delay dynamic model in elastic environment. This paper begins with the differential difference equation and analyses a relative simple case in one dimension force reflection telepresence system--suppose that the remote mechanical hand deals with only one elastic object in environment without displacement and that there is only elastic distortion. Furthermore, this paper also analyses the effect caused by model parameters and ascertains the time delay limit which make it possible to maintain the system operation.	broadcast delay;displacement mapping;distortion;mathematical model;recurrence relation	Wensong Hu;Min Zhu;Xiaoyong Lu	2007	Workshop on Intelligent Information Technology Application (IITA 2007)	10.1109/IITA.2007.71	project management;technology management;data mining;spatial analysis;information technology;statistics	Robotics	82.20566645635067	-51.97317965235891	182729
ca5439e1b8c259e133b5dd6b7faf280809cbb239	evaluation of signal space separation via simulation	spherical harmonic;maxwell equation;interference suppression;magnetoencephalography;signal processing;signal reconstruction;computer simulation	Signal space separation (SSS) method is an advanced signal-processing approach that can be used to recover bio-magnetic signal and remove external disturbance in empirical magnetoencephalography (MEG) measurements. SSS is based on the solution of the quasi-static approximation of Maxwell equations (i.e., Laplace’s equation) which can be expressed as linear combinations of spherical harmonic functions. In applying SSS, MEG measurements can be split into two parts: brain signals and external interferences. In this paper, after a brief review of the basics of SSS, we evaluate SSS systematically via computer simulation and real MEG data. In the simulations of this paper, two types of interference sources with magnetic and electric current dipoles are used. The interference suppression effects and the quality of the reconstruction of the interested signal are investigated. Also, the degree of spherical harmonic functions and its relationship with signal reconstruction and interference suppression are studied thoroughly. Finally, we provide objective assessments of the advantages and limitations of the SSS approach, and its practical value in MEG measurements.	approximation;british informatics olympiad;computer simulation;electricity;evaluation procedure;interference (communication);magnetoencephalography;maxwell (microarchitecture);signal processing;signal reconstruction;zero suppression;synovial sarcoma	Tao Song;Kathleen Gaa;Li Cui;Lori Feffer;Roland R. Lee;Mingxiong Huang	2007	Medical & Biological Engineering & Computing	10.1007/s11517-007-0290-y	computer simulation;signal reconstruction;maxwell's equations;electronic engineering;computer science;electrical engineering;signal processing;nuclear magnetic resonance;physics;quantum mechanics;magnetoencephalography;spherical harmonics	ML	80.54890336605482	-39.53916638903415	182860
900c7ff9e32cba6c4ab3e343c7c57ae84674a946	on performance differences of emd and wd in the nonlinear time series analysis	wavelet analysis;hilbert transforms;wd hilbert transformation empirical mode decomposition nonlinear time series analysis intrinsic mode functions instantaneous amplitude instantaneous frequency wavelet decomposition emd;empirical mode decomposition nonlinear time series hilbert transformation;time series;wavelet decomposition;nonlinear time series analysis;nonlinear time series;wavelet transforms;hilbert transform;wavelet transform;time series analysis time frequency analysis wavelet transforms correlation wavelet analysis temperature;time series analysis;intrinsic mode function;wavelet transforms hilbert transforms time series;time frequency analysis;empirical mode decomposition	The procedure of the Hilbert transformation (HT) based on empirical mode decomposition (EMD) is first decomposing a nonlinear time series into its intrinsic mode functions (IMFs), and then taking the HT of each IMF and computing the instantaneous amplitude and frequency. In the context of an ideal time series, merits and defects of EMD and wavelet decomposition (WD) as well as HT and wavelet transformation (WT) in the nonlinear time series analysis are systematically analyzed/compared in this paper, and aiming at their defects, some proposals for possible improvement are also given. Research results show that the combination with the EMD-based analysis method and the WD-based one may more effectively identify the characteristic information of the original time series.	hilbert–huang transform;nonlinear system;time series;wavelet transform	Xiaojuan Wang;Guolin Feng;Aixia Feng	2011	2011 Eighth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)	10.1109/FSKD.2011.6019508	mathematical optimization;mathematical analysis;speech recognition;time series;mathematics;statistics;wavelet transform	DB	78.95022930147425	-39.55092524954554	182961
33f8e6e41692821baff3f794ee0bee409cd13c88	layered rhombus-chain-connected model for real-time haptic rendering	virtual reality;feedback force;deformable objects;layered rhombus chain connected model;haptic rendering	The modeling and simulation of deformable objects is a challenging topic in the field of haptic rendering between human and virtual environment. In this paper, a novel and efficient layered rhombus-chain-connected haptic deformation model based on physics is proposed for an excellent haptic rendering. During the modeling, the accumulation of relative displacements in each chain structure unit in each layer is equal to the deformation on the virtual object surface, and the resultant force of corresponding springs is equivalent to the external force. The layered rhombus-chain-connected model is convenient and fast to calculate, and can satisfy real-time requirement due to its simplicity. Experimental study in both homogenous and non-homogenous virtual human liver and lungs based on the proposed model are conducted, and the results demonstrate that our model provides stable and realistic haptic feeling in real time. Meanwhile, the display result is vivid.	algorithm;coefficient;experiment;feedback;finite element method;haptic technology;interaction;real-time clock;real-time locating system;resultant;simulation;tree accumulation;virtual actor;virtual reality	Xiaorui Zhang;Wei Sun;Aiguo Song	2011	Artificial Intelligence Review	10.1007/s10462-011-9297-8	simulation;computer science;artificial intelligence;virtual reality;multimedia;computer graphics (images)	Robotics	70.61014717445386	-47.08122694361648	182965
d2b4ec2b8a18ef0fb7722851f5ccd3b1f20951fb	fast surface modelling using a 6th order pde	computacion informatica;grupo de excelencia;ciencias basicas y experimentales	Although the control-point based parametric approach is used most widely in free-form surface modelling, complementary techniques co-exist to meet various specialised requirements. The partial differential equation (PDE) based modelling approach is especially suitable for satisfying surface boundary constraints. They are also effective for the generation of families of free-form surfaces, which share a common base and differ in their secondary features. In this paper, we present a fast surface modelling method using a sixth order PDE. This PDE provides enough degrees of freedom not only to accommodate tangent, but also curvature boundary conditions and offers more shape control parameters to serve as user controls for the manipulation of surface shapes. In order to achieve real-time performance, we have constructed a surface function and developed a high-precision approximate solution to the 6th order PDE. Unlike some existing PDE-based techniques, this resolution method can satisfy the boundary conditions exactly, and is able to create free-form surfaces as fast and almost as accurately as the closed-form (analytical) solutions. Due to the fact that it has sufficient degrees of freedom to accommodate the continuity of 3-sided and 4-sided surface patches at their boundaries, this method is able to model complex surfaces consisting of multiple patches. Compared with existing PDE-based modelling methods, this method is both fast and can solve a larger class of surface modelling problems.	approximation algorithm;boundary case;real-time clock;requirement;scott continuity	Jian J. Zhang;Lihua You	2004	Comput. Graph. Forum	10.1111/j.1467-8659.2004.00762.x	mathematical optimization;topology;calculus;mathematics;geometry;pde surface	Graphics	69.38186588515521	-45.54695998083907	183518
4d931d50c149a4575382413485ddad78b4be0ad7	scalar-field-guided adaptive shape deformation and animation	power efficiency;interaction techniques;shape deformation;velocity field;shape deformations;scalar field;scalar fields;shape modeling	In this paper, we propose a novel scalar-field-guided adaptive shape deformation (SFD) technique founded on PDE-based flow constraints and scalar fields of implicit functions. Scalar fields are used as embedding spaces. Upon deformation of the scalar field, a corresponding displacement/velocity field will be generated accordingly, which results in a shape deformation of the embedded object. In our system, the scalar field creation, sketching, and manipulation are both natural and intuitive. The embedded model is further enhanced with self-optimization capability. During the deformation we can also enforce various constraints on embedded models. In addition, this technique can be used to ease the animation design. Our experiments demonstrate that the new SFD technique is powerful, efficient, versatile, and intuitive for shape modeling and animation.	archive;displacement mapping;embedded system;experiment;free-form deformation;geometric design;graphics;haptic technology;ibm notes;interaction;key frame;mathematical optimization;programming paradigm;prototype;requirement;scalar processor;scott continuity;simulation;torsion (gastropod);velocity (software development);visual modeling	Jing Hua;Hong Qin	2003	The Visual Computer	10.1007/s00371-003-0225-z	scalar field;vector field;topology;electrical efficiency;mathematics;geometry;quantum mechanics	Graphics	68.72307867729748	-46.3743171724997	183701
8136da2a9af46da7304b06c697d551905b797992	rendering implicit flow volumes	isocontouring implicit flow volume rendering technique linear interpolation scalar field advection operator slice based three dimensional texture mapping interval volume segmentation tetrahedron projection based renderer implicit stream flows geometric flow volume;image segmentation;high dimensionality;interval volume rendering;volume rendering;image matching;image matching rendering computer graphics computational geometry flow visualisation image texture computational fluid dynamics data visualisation image segmentation;texture mapping;computational geometry;implicit stream flow;graphics hardware interval volume rendering implicit stream flow flow visualization;three dimensional;image texture;computational fluid dynamics;data visualisation;flow visualisation;dynamic texture;graphics hardware;linear interpolation;scalar field;stream flow;visualization rendering computer graphics computer graphics surface texture geometry aerodynamics computer science vectors interpolation manipulator dynamics;vector field;rendering computer graphics;flow visualization;geometric flow	Traditional flow volumes construct an explicit geometrical or parametrical representation from the vector field. The geometry is updated interactively and then rendered using an unstructured volume rendering technique. Unless a detailed refinement of the flow volume is specified for the interior, information inside the underlying flow volume is lost in the linear interpolation. These disadvantages can be avoided and/or alleviated using an implicit flow model. An implicit flow is a scalar field constructed such that any point in the field is associated with a termination surface using an advection operator on the flow. We present two techniques, a slice-based three-dimensional texture mapping and an interval volume segmentation coupled with a tetrahedron projection-based renderer, to render implicit stream flows. In the first method, the implicit flow representation is loaded as a 3D texture and manipulated using a dynamic texture operation that allows the flow to be investigated interactively. In our second method, a geometric flow volume is extracted from the implicit flow using a high dimensional iso-contouring or interval volume routine. This provides a very detailed flow volume or set of flow volumes that can easily change topology, while retaining accurate characteristics within the flow volume. The advantages and disadvantages of these two techniques are compared with traditional explicit flow volumes.	interactivity;linear interpolation;refinement (computing);shadow volume;texture mapping;volume rendering	Daqing Xue;Caixia Zhang;Roger Crawfis	2004	IEEE Visualization 2004	10.1109/VISUAL.2004.90	computer vision;flow visualization;computational geometry;computer science;theoretical computer science;mathematics;geometry;data visualization;computer graphics (images)	Visualization	70.42560806092341	-45.66416569214531	184215
1775cf1737a51400df05e1a118f8edac14b8363f	the fractal nature of an ecological model	comportement chaotique;chaos;computer graphics;caos;modelo;plan complexe;fractal;ecological model;modele;modele ecologique;grafico computadora;infographie;models	Abstract#R##N##R##N#With the help of computer graphics, the chaotic behaviour of an ecological model in the complex plane has been investigated. Convergence maps that show multi-fractal characteristics (science) and beautiful patterns (art) are presented.	ecosystem model;fractal	Wang Bennan;Shi Yin	1992	Comput. Graph. Forum	10.1111/1467-8659.1110061	fractal;computer science;mathematics;geometry;computer graphics;social ecological model	NLP	74.33254581115102	-45.31649472598548	184543
cdb18e30c1634213895810b9ef40d7c77b246a44	using difference intervals for time-varying isosurface visualization	out of core;time varying;data compression;span space;video encoding techniques;point based previewing technique difference intervals out of core time varying isosurface visualization time varying dataset interactive visualization video encoding techniques isosurface information extraction span space extraction techniques isosurface geometry temporal compression;point based rendering;interactive visualization;geometry;computational geometry;surface fitting;point based rendering isosurface time varying span space out of core;isosurfaces;space time;data mining;surface fitting computational geometry data compression data visualisation feature extraction interactive systems rendering computer graphics;time varying dataset interactive visualization;computational fluid dynamics;data visualisation;isosurface geometry;data analysis;computational modeling;point based previewing technique;isosurface;isosurfaces data visualization data mining geometry data analysis encoding computational modeling bandwidth computational fluid dynamics workstations;isosurface information extraction;feature extraction;workstations;out of core time varying isosurface visualization;data visualization;temporal compression;bandwidth;rendering computer graphics;span space extraction techniques;difference intervals;encoding;interactive systems;computer simulation	We present a novel approach to out-of-core time-varying isosurface visualization. We attempt to interactively visualize time-varying datasets which are too large to fit into main memory using a technique which is dramatically different from existing algorithms. Inspired by video encoding techniques, we examine the data differences between time steps to extract isosurface information. We exploit span space extraction techniques to retrieve operations necessary to update isosurface geometry from neighboring time steps. Because only the changes between time steps need to be retrieved from disk, I/O bandwidth requirements are minimized. We apply temporal compression to further reduce disk access and employ a point-based previewing technique that is refined in idle interaction cycles. Our experiments on computational simulation data indicate that this method is an extremely viable solution to large time-varying isosurface visualization. Our work advances the state-of-the-art by enabling all isosurfaces to be represented by a compact set of operations	computer data storage;data compression;experiment;hl7publishingsubsection <operations>;imagery;interactivity;isosurface;out-of-core algorithm;requirement;scientific visualization;simulation;span distance	Kenneth W. Waters;Christopher S. Co;Kenneth I. Joy	2006	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2006.188	data compression;computer vision;out-of-core algorithm;workstation;feature extraction;computational fluid dynamics;computer science;theoretical computer science;isosurface;space time;data mining;data analysis;computational model;data visualization;bandwidth;encoding;statistics;computer graphics (images)	Visualization	71.26101426506008	-51.143035094492106	184636
9c981b40b37b1d737aea332eb4ae7642d4af5efb	algorithm for nc tool paths automatic generation on surfaces based on space-filling curves	automatic generation of tool-path;offsetting curves;space-filling curves;surface	Due to the inadequate residue amount in traditional algorithm, this article further studies the generation principles and algorithms of offsetting curves and space-filling curves, and tool-path generation in parameter field and surfaces. The problem in generating and jointing space-filling curves of different powers is solved by constructing space-filling curves with control vector. By dividing the data of rectangular region into rectangular blocks, and combining the space-filling curves and offsetting curve algorithm, the mixed tool-path is successfully generated. © 2011 Springer-Verlag Berlin Heidelberg.	algorithm	Zhanfang Chen;Xiaoming Zhang;Dongsong Han;Shufang Wu;Wenbo Zhang	2011		10.1007/978-3-642-23756-0_71	combinatorics;engineering drawing;algorithm	NLP	69.3766493112615	-39.53856453625256	185556
553b5e9cb1a6f8081440b0ff0ab2f3dad2027ed9	dna rendering of polyhedral meshes at the nanoscale	self assembly;geociencias medio ambiente;dna origami;ciencias biologicas generalidades;grupo de excelencia;ciencias basicas y experimentales generalidades;ciencias basicas y experimentales;geociencias medio ambiente generalidades;ciencias biologicas;grupo a;graph algorithms;dna nanostructures dna origami graph algorithms self assembly;dna nanostructures	It was suggested more than thirty years ago that Watson–Crick base pairing might be used for the rational design of nanometre-scale structures from nucleic acids. Since then, and especially since the introduction of the origami technique, DNA nanotechnology has enabled increasingly more complex structures. But although general approaches for creating DNA origami polygonal meshes and design software are available, there are still important constraints arising from DNA geometry and sense/antisense pairing, necessitating some manual adjustment during the design process. Here we present a general method of folding arbitrary polygonal digital meshes in DNA that readily produces structures that would be very difficult to realize using previous approaches. The design process is highly automated, using a routeing algorithm based on graph theory and a relaxation simulation that traces scaffold strands through the target structures. Moreover, unlike conventional origami designs built from close-packed helices, our structures have a more open conformation with one helix per edge and are therefore stable under the ionic conditions usually used in biological assays.	algorithm;base pairing;biological assay;graph theory;greater than;ionic;linear programming relaxation;nucleic acids;polygon mesh;polyhedron;simulation;tracing (software);watson (computer);antisense therapy	Erik Benson;Abdulmelik Mohammed;Johan Gardell;Sergej Masich;Eugen Czeizler;Pekka Orponen;Björn Högberg	2015	Nature	10.1038/nature14586	dna origami;nanotechnology;self-assembly	Graphics	73.98501153624026	-43.793418934186846	186299
9809a53b35f705b08dea8a874186bac43267e17f	curve approximation with quadratic b-splines	spline;cad;data points;b spline curve;opening angle plot curve approximation quadratic b splines data points knots;splines mathematics;approximation theory;opening angle plot;knots;quadratic b splines curve approximation;curve fitting;quadratic b splines;curve approximation;cad splines mathematics curve fitting approximation theory	A curve approximation technique using quadratic B-splines is presented in this paper which automatically computes data points to minimize errors. This technique can be useful for efficient storage of geometric shapes in any graphic or CAD applications. The computed data points are the control points and knots of approximating quadratic B-spline curve rather than simple interpolants. Curve approximation is a three step process, involving computation of initial data points from the opening angle plot of given curve, new knot(s) insertion at appropriate location and error minimization by changing knot positions. The algorithm is simple, efficient and robust to any curve model. Demonstrated results show that even higher degree polynomial curves can be approximated with very few data points with reasonable accuracy.	approximation algorithm;b-spline;computation;computer-aided design;control point (mathematics);data point;polynomial;spline (mathematics)	Asif Masood;Muhammad Sarfraz;Shaiq A. Haq	2005	Ninth International Conference on Information Visualisation (IV'05)	10.1109/IV.2005.39	curve sketching;mathematical optimization;combinatorics;osculating curve;tripling-oriented doche–icart–kohel curve;mathematics;geometry;flat spline;bullet-nose curve;butterfly curve;moore curve;stable curve;curve orientation;curve fitting	Robotics	69.77149632750371	-40.73931532446906	186314
dc082d2316939e6c68895a26a5137dca05dc474e	curvature variation factor and its application	cad;curve;curvature;wavelets	Evaluation on curvature variation is a significant problem in the field of curve fairing, and traditional evaluation is generally performed by observing curvature plots qualitatively. By using the theory of B-spline wavelet, the concept of curvature variation factor (CVF) is put forth in this paper to describe curvature variation quantitatively. It is verified that the quantitative evaluation results by the CVF method agree well with the traditional qualitative judgments. Based on CVF, a new curve-fairing algorithm is further put forward, whose validness is verified by an example. The advantage of the CVF-based algorithm is that it can realize different fairing effect to meet different requirements under diverse circumstances only by adjusting the shape error.	algorithm;b-spline;requirement;spline wavelet	Aizeng Wang;Gang Zhao	2017	IJWMIP	10.1142/S0219691317500138	wavelet;topology;cad;mathematics;geometry;curve;curvature	Vision	71.46446807790848	-39.828868249935034	186880
cf0a30f32a718f5f4642037806a998d448dcbdc2	conformal self-organizing map on curved seamless surface	conformal map;texture mapping;manifold learning;surface reconstruction;conformal mapping;morphing;multidimensional scaling;self organizing map;self organized map	This paper presents a new mapping to construct the self-organizing map on the curved seamless surface. This mapping is developed for the planar triangle surface derived from the conformal selforganizing map [C.-Y. Liou, Y.-T. Kuo, Conformal self-organizing map for a genus-zero manifold, Visual Comput. 21(5) (2005) 340–353]. It shows how to construct a seamless surface for the genus-zero manifold. The constructed surface is both seamless and continuous. The mapping between the model surface and the sphere surface is one-to-one and onto. This kind of surface can facilitate many applications of the self-organizing map. We show experiments in surface reconstructions and texture mappings. & 2008 Elsevier B.V. All rights reserved.	experiment;one-to-one (data model);organizing (structure);seamless3d;self-organization;self-organizing map	Cheng-Yuan Liou;Yen-Ting Kuo;Jau-Chi Huang	2008	Neurocomputing	10.1016/j.neucom.2008.04.031	conformal map;combinatorics;topology;displacement mapping;computer science;machine learning;mathematics;geometry	Robotics	68.69736013318001	-42.01149937662751	186891
4d0b9bc77728515786167175d37ac41cb3d58bb0	shape-preserving, multiscale interpolation by univariate curvature-based cubic l1 splines in cartesian and polar coordinates	interpolation;coordenada polar;methode echelle multiple;l 2 spline;metodo escala multiple;aproximacion esplin;computer experiment;spline approximation;approximation spline;univariate;ajustement courbe;curvature;polar coordinate;multiscale method;esplin cubico;spline cubique;coordonnee cartesienne;curve fitting;multiscale;polar coordinates;coordonnee polaire;l 1 spline;cartesian coordinates;coordinate system;shape preservation;cubic spline	We investigateC1-smooth univariate curvature-based cubic L1 interpolating splines in Cartesian and polar coordinates. The coefficients of these splines are calculated by minimizing the L1 norm of curvature. We compare these curvature-based cubic L1 splines with second-derivative-based cubic L1 splines and with cubicL2 splines based on theL2 norm of curvature and of the second derivative. In computational experiments in Cartesian coordinates, cubic L1 splines based on curvature preserve the shape of multiscale data well, as do cubic L1 spl nes based on the second derivative. Cartesian-coordinate cubic L1 splines preserve shape much better than analogous Cartesian-coordinate cubic L2 splines. In computational experiments in polar coordinates, cubic L1 splines based on curvature preserve the shape of multiscale data better than cubic L1 splines based on the second derivative and much better than analogous cubic L2 splines. Extensions to splines in general curvilinear coordinate systems, to bivariate splines in spherical coordinate systems and to nonpolynomial splines are outlined. Published by Elsevier Science B.V.	bivariate data;cartesian closed category;coefficient;cubic hermite spline;cubic function;experiment;geometric modeling;interpolation;spline (mathematics);t-norm;taxicab geometry	John E. Lavery	2002	Computer Aided Geometric Design	10.1016/S0167-8396(02)00087-0	mathematical optimization;polar coordinate system;cubic form;topology;bicubic interpolation;mathematics;geometry;box spline	Graphics	69.15658988615499	-40.79941524510172	186961
2941d3ee7899a540d375cd83c852732061330478	shellsplatting: interactive rendering of anisotropic volumes	interactive rendering;elliptical gaussian;anisotropic volume;yields higher quality volume;performance penalty;shell rendering;efficient data-structures;ubiquitously available commercial graphics;non-precise boundary;volume data;graphics hardware;effective use;data structure;volume rendering	This work presents an extension of shell rendering that is more flexible and yields higher quality volume renderings. Shell rendering consists of efficient data-structures and methods to manipulate and render structures with non-precise boundaries in volume data. We have updated these algorithms by creating an implementation that makes effective use of ubiquitously available commercial graphics hardware. More significantly, we have extended the algorithm to make use of elliptical Gaussian splats instead of straight-forward voxel projection. This dramatically increases the quality of the renderings, especially with anisotropically sampled volumes. The use of the graphics hardware alleviates the performance penalty of using splats.	algorithm;graphics hardware;voxel	Charl P. Botha;Frits H. Post	2003			tiled rendering;scientific visualization;image-based modeling and rendering;3d rendering;computer hardware;rendering;computer science;parallel rendering;real-time rendering;texture memory;alternate frame rendering;volume rendering;engineering drawing;software rendering;computer graphics (images)	Graphics	68.41659089316244	-50.74205520753626	187244
1ff19c2b9ff02c5dc9ec17e824a4d7df6dcdd9dd	smooth reverse loop and catmull-clark subdivision	reverse subdivision;loop subdivision;catmull clark subdivision;least square;multiresolution;energy minimization	In this paper we present a new multiresolution technique for general topology surfaces based on reversing subdivision with energy minimization. We first introduce a general reverse subdivision approach that starts from a trial set of biorthogonal multiresolution filters and refines the resulting coarse points using local masks. The refinement step tries to find a good approximation of the fine points while minimizing the local energy of the coarse points in a least-squares sense. This approach is then used to find smooth reverse of the Loop and Catmull–Clark subdivisions. We discuss the advantages of using this technique in various surface editing and synthesis applications. 2011 Elsevier Inc. All rights reserved.	approximation;b-spline;catmull–clark subdivision surface;cubic function;energy minimization;least squares;mathematical optimization;multiresolution analysis;refinement (computing);reversing: secrets of reverse engineering	Javad Sadeghi;Faramarz F. Samavati	2011	Graphical Models	10.1016/j.gmod.2011.03.004	mathematical optimization;computer science;catmull–clark subdivision surface;mathematics;geometry;energy minimization;least squares;subdivision surface	Graphics	69.41811520531458	-41.33104886603498	187498
a9cba5ecbe5fc88be47d46e0f64f84f082872354	simulating coordinated movement with tendons	human movement;poisson disk;simulation framework;blue noise;sampling;human body	Despite significant advances in the research of human movement [Kry and Pai 2006], simulating the dynamics of the human body, such as compliance coupling between joints, remains challenging. Simulating biomechanics is difficult, in part, due to the complex inter-dependence between tendons and bones. Modeling this interdependence is necessary, however, since much of the coordinated motion in human biomechanics is due to the specific ways in which tendons are arranged with respect to bones. Accurate tendon modeling is also important for visual effects, since tendon motion has a noticeable effect on skin deformation. We address these issues with a novel simulation framework that efficiently and accurately models the coupled dynamics of tendons and bones.	interdependence;simulation;visual effects	Shinjiro Sueda;Dinesh K. Pai	2007		10.1145/1278780.1278829	sampling;human body;simulation;colors of noise	Robotics	71.54321265863678	-47.1882201266667	187755
3e16057ce0599785651c2cd95cc9fe7a7f8b7399	blending face details: synthesizing a face using multiscale face models		Creating realistic 3D face models is a challenging problem in computer graphics because humans are so sensitive to facial abnormalities. The authors propose a method to synthesize a 3D face model using weighted blending of multiscale details from different face models. Using multiscale continuous displacement maps (CDMs), they achieve full correspondences across multiple scales in the parameter space. Their results demonstrate detail transfer across faces with highly different proportions, such as between humans and nonhuman creatures. An artist evaluation also indicated the proposed approach is intuitive and easy to use.	alpha compositing;computer graphics;congenital abnormality;displacement mapping;face;humans;isaacs syndrome;map;population parameter;psychologic displacement	Seung-Hyun Yoon;John P. Lewis;Taehyun Rhee	2017	IEEE Computer Graphics and Applications	10.1109/MCG.2017.4031069	computer vision;parameter space;creatures;displacement mapping;artificial intelligence;parametrization;computer graphics;computer science;solid modeling	Graphics	69.20510832482105	-47.85847782602566	187783
5c0f73cca1e5b8c101914a7f2a0405a738e99828	synthesis of linkages with four points of accuracy using maple-v	experimental design;graphic method;relative position;analisis numerico;matematicas aplicadas;mathematiques appliquees;05bxx;geometrie algorithmique;computer graphics;computational geometry;plan experiencia;center point curve;independent suspension system;65d18;analyse numerique;computer graphic;constant camber;numerical analysis;estimation erreur;62k99;methode graphique;maple v;plan experience;maple software;logiciel maple;error estimation;courbe point cercle;estimacion error;62 09;curva lisa;circle point curve;metodo grafico;geometria computacional;65s05;courbe point central;four point of accuracy;applied mathematics;55p40;grafico computadora;generation mouvement;motion generation;infographie;suspension	A graphical and computational based method for synthesis of linkages with fourpoint of accuracy using the Maple-V software is presented. In our problem an arbitrary moving link of the mechanism should be located in four different attitudinal and translational positions with respect to another moving link. To design of such a mechanism the graphical based method is used but to prevent the lack of accuracy due to the graphical errors the method is implemented by computer graphics with using a mathematical symbolic software namely Maple-V. After definition of four relative positions of two moving links, all poles and imagepoles are determined. Selecting an opposite pole quadrilateral, center point curve and circle point curves are obtained and drawn. By using a few commands in the Maple-V, these two curves can be achieved with the desirable precision and in the desirable plane. A sample problem is solved using the above mentioned method. In the example a six-bar linkage for independent type suspension mechanism of a vehicle is designed with four points of accuracy such that it maintains the camber angle of wheel when it moves over a bump. The method can be easily implemented for motion generation problems such as design of four points of the six-bar as well as Watt s and Stephenson s family mechanisms and can be applied not only in the professional designs but also for educational purposes. 2004 Elsevier Inc. All rights reserved. 0096-3003/$ see front matter 2004 Elsevier Inc. All rights reserved. doi:10.1016/j.amc.2004.04.085 E-mail address: shirazi@cua.ac.ir 732 K.H. Shirazi / Appl. Math. Comput. 164 (2005) 731–755	bump mapping;computation;computer graphics;graphical user interface;linkage (software);maple	Kourosh H. Shirazi	2005	Applied Mathematics and Computation	10.1016/j.amc.2004.04.085	simulation;applied mathematics;numerical analysis;computational geometry;mathematics;suspension;computer graphics;design of experiments;algorithm;statistics	Robotics	68.57995199704693	-38.84550950343096	187861
a6242bc4cf856eb8ba66738109ba8bcd4bc75dad	dynamic modeling of string for robotics application	solid modelling approximation theory control engineering computing digital simulation image recognition nonlinear differential equations partial differential equations robots;real string parameter robotics application computer graphics deformable object modeling mass spring model simulator model string simulation interconnected particle spring element damping element string motion nonlinear partial different equation recognition method image information grayscale image image conversion approximation;image processing computer graphic mass spring model graphical user interface	This paper presents the modeling of deformable objects within the computer graphics. The method to model a physically correct simulator using a mass spring model for string simulation is proposed. The outline of this project is to design a dynamic model of a string. The string t is represented by a model that discretizes the object into a network of interconnected particles, springs and damping elements. The motion of the string is calculated by using nonlinear partial different equations. Besides, a recognition method based on the image information obtained is used to identify the position of string the string particles. The pixel of the image will be recorded as an array in the memory and is converted into grayscale image. After that, several processes such as background elimination and binarization is used to identify the position of each particle. Based on the results obtained, it can be concluded that the mathematical model developed are capable to approximate the real string parameter. Therefore, through modeling, a real string parameter inside a model can be measured and approximated with ease through simulation. Thus, this can be used to design systems for robotics application which have not been manufactured for testing, handling and manipulation.	approximation algorithm;binary image;computer graphics;grayscale;image processing;mathematical model;nonlinear system;opencv;pixel;robotics;simulation;string (computer science)	Khairul Salleh Mohamed Sahari;Chew Han Min;Yew Cheong Hou	2012	The 6th International Conference on Soft Computing and Intelligent Systems, and The 13th International Symposium on Advanced Intelligence Systems	10.1109/SCIS-ISIS.2012.6505346	computer vision;simulation;computer science;theoretical computer science;machine learning	Robotics	70.39637319859862	-47.441709390201254	188276
f15eac51b879e61f19ad77bf552f9b555068e18d	subdivision interpolating implicit surfaces	implicit surface;tessellation;closed form solution;interpolating implicit surface;subdivision surface;newton iteration;triangular mesh;surface reconstruction;radial basis function;surface model;subdivision scheme;multiresolution	Interpolating implicit surfaces using radial basis functions can directly specify surface points and surface normals with closed form solutions, so they are elegantly used in surface reconstruction and shape morphing. This paper presents subdivision interpolating implicit surfaces, a new progressive subdivision tessellation scheme for interpolating implicit surfaces controlled by a triangular mesh with arbitrary topology. We use a recursive polyhedral subdivision scheme to subdivide the control triangular mesh, and the new generated vertices are mapped to the implicit surface using Newton iteration. A multiresolution surface representation is automatically built with the proposed approach. Based on this approach, a newly surface modeling tool with more flexible control is developed by blending the interpolating subdivision surfaces with the subdivision interpolating implicit surfaces. r 2003 Elsevier Ltd. All rights reserved.	alpha compositing;barycentric subdivision;implicit surface;interpolation;iteration;morphing;newton's method;normal (geometry);polygon mesh;polyhedron;radial (radio);radial basis function;recursion;subdivision surface	Xiaogang Jin;Hanqiu Sun;Qunsheng Peng	2003	Computers & Graphics	10.1016/S0097-8493(03)00149-3	closed-form expression;mathematical optimization;radial basis function;topology;surface reconstruction;computer science;triangle mesh;machine learning;mathematics;geometry;newton's method;tessellation;subdivision surface	Graphics	69.08986929710412	-41.83170967136309	188674
f6cba7400c92bcb8078f657adec887d8670a9da3	3d visualization of the microscopic characteristic in magnetorheological fluids	focal plane;magnetorheology flow visualisation holography magnetohydrodynamics;会议论文;focal plane magnetorheological mr fluids digital micro holography;flow visualisation;magnetorheology;digital micro holography;holography;fluids holography magnetic fields image reconstruction microscopy image segmentation magnetic liquids;magnetohydrodynamics;chain structure 3d visualization microscopic characteristic magnetorheological fluids 3d microstructure digital microholographic system rheological mechanism mr fluids focal plane overall sharpness method focal plane determination image segmentation method two threshold method particle size;magnetorheological mr fluids	The 3D microstructure and the rheology of Magnetorheological (MR) fluids are investigated using digital micro holography. A digital micro holographic system which can measures the rheological mechanism of MR fluids is presented. To accurately locate the focal plane of the particles in MR Fluids in digital holography, the overall-sharpness method is introduced which can effectively eliminate the noise on the focal plane determination. The two-threshold and image segmentation methods were used to obtain high quality binary images from which we can get satisfied measurement results of particle size and chain structure. The size distributions and the 3D visualization of microscopic characteristic of MR fluids are effectively measured. The experiment results show the digital holography is a well tool for measurement of the behaviors of MR fluids.	binary image;digital holography;display resolution;focal (programming language);image segmentation;visualization (graphics);volume rendering	Yan Yang;Jin Huang	2013	The 8th Annual IEEE International Conference on Nano/Micro Engineered and Molecular Systems	10.1109/NEMS.2013.6559894	classical mechanics;magnetohydrodynamics;cardinal point;optics;holography;physics	Visualization	73.02078568240388	-49.15650350568827	189598
99e9d10aacb6cbe7b2cebe9dc5d05ee7ad80b2e0	preserving the volume of fluid using multi-phase flow approach	fluid volume preservation;viscosity;marker and cell grid;mathematics;viscosity animation navier stokes equations computational modeling computational fluid dynamics computer graphics solid modeling mathematics physics informatics;multi phase flow;computer graphics;navier stokes equations;computational fluid dynamics fluid volume preservation multiphase flow multiphase fluid animation fluid mixtures navier stokes equations fluid motion marker and cell grid volume of fluid representation fluid density viscosity flow simulation;fluid mixtures;volume of fluid;computational fluid dynamics;physics;flow visualisation;computational modeling;solid modeling;animation;fluid density;marker and cell;informatics;flow simulation;navier stokes equations computational fluid dynamics computer animation flow simulation flow visualisation multiphase flow;computer animation;volume of fluid representation;multiphase flow;navier stokes equation;multiphase fluid animation;fluid motion	Methods to animate multiphase fluids are rarely researched, although such phenomena are common in our daily lives. The main focus of this research is to develop a suitable method to create realistic animation of multiphase flows, i.e. flow of fluid mixtures. Our method is based on Navier-Stokes equations, a set of physical equations that describe motion of fluid. A marker-and-cell (MAC) grid is used to solve the equations on discrete computational domain. To simulate several fluids, we propose the modification of volume-of-fluid (VOF) representation of fluid and integrate it into the multiphase-fluid approach. A mixture of fluids is treated as a single fluid having variable density and viscosity. This scheme allows two or more fluids having different densities and viscosities to be simulated simultaneously	computational fluid dynamics;interaction;marker-and-cell method;navier–stokes equations;simulation;volume of fluid method	Roman Durikovic;Katsuhiro Numata	2006	Tenth International Conference on Information Visualisation (IV'06)	10.1109/IV.2006.87	classical mechanics;fluid simulation;statistical physics;computer science;computational physics;cfd-dem	Visualization	71.288678744337	-47.76043863012454	189687
458dcf7bf61d12311fe4e187fec180378219790c	the study of a new parametrized spline algorithm	error analysis;cubic spline	The method applied most frequently in drawing a curve to interpolate a set of given points without spoiling the aim for “shape preserving” is parametrized cubic spline method; however, possibility of producing a bad result on “shape preserving” exists. CATIA system which uses “quintic spline” is not a perfect one. We derive a “quintic spline” from a “cubic spline” and has it modified. Except for the above, we also do the work of error analysis and execute a few examples for comparison.	algorithm;catia;cubic hermite spline;cubic function;error analysis (mathematics);interpolation;quintic function;spline (mathematics)	Bai-Jiun Tu;Yu-Tang Fei	1986		10.1145/800239.807170	spline interpolation;mathematical optimization;discrete mathematics;perfect spline;smoothing spline;monotone cubic interpolation;cubic hermite spline;hermite spline;mathematics;geometry;thin plate spline;flat spline;m-spline	Graphics	69.52887907761703	-40.54572903895303	190037
23fdbf1a950d66cdb566e47e4a5fb7a0ca1a5599	generation of unstructured meshes in 2-d, 3-d, and spherical geometries with embedded high resolution sub-regions		We present 2-D, 3-D, and spherical mesh generators for the Finite Element Method (FEM) using triangular and tetrahedral elements. The mesh nodes are treated as if they were linked by virtual springs that obey Hooke’s law. Given the desired length for the springs, the FEM is used to solve for the optimal nodal positions for the static equilibrium of this spring system. A ’guide-mesh’ approach allows the user to create embedded high resolution sub-regions within a coarser mesh. The method converges rapidly. For example, in 3-D, the algorithm is able to refine a specific region within an unstructured tetrahedral spherical shell so that the edge-length factor l0r/l0c = 1/33 within a few iterations, where l0r and l0c are the desired spring length for elements inside the refined and coarse regions respectively. One use for this type of mesh is to model regional problems as a fine region within a global mesh that has no fictitious boundaries, at only a small additional computational cost. The algorithm also includes routines to locally improve the quality of the mesh and to avoid badly shaped ’slivers-like’ tetrahedra.		J. M. Taramón;Jason P. Morgan;C. Shi;Jörg Hasenclever	2017	CoRR		topology;finite element method;spring system;mathematics;polygon mesh;mechanical equilibrium;spherical shell;tetrahedron	Visualization	69.76260116954089	-44.24478808051616	190051
c9efe66812ccdff6c99f6f83413a45739826badd	research on weak fault extraction method for alleviating the mode mixing of lmd		Compared with the strong background noise, the energy entropy of early fault signals of bearings are weak under actual working conditions. Therefore, extracting the bearings’ early fault features has always been a major difficulty in fault diagnosis of rotating machinery. Based on the above problems, the masking method is introduced into the Local Mean Decomposition (LMD) decomposition process, and a weak fault extraction method based on LMD and mask signal (MS) is proposed. Due to the mode mixing of the product function (PF) components decomposed by LMD in the noisy background, it is difficult to distinguish the authenticity of the fault frequency. Therefore, the MS method is introduced to deal with the PF components that are decomposed by the LMD and have strong correlation with the original signal, so as to suppress the modal aliasing phenomenon and extract the fault frequencies. In this paper, the actual fault signal of the rolling bearing is analyzed. By combining the MS method with the LMD method, the fault signal mixed with the noise is processed. The kurtosis value at the fault frequency is increased by eight-fold, and the signal-to-noise ratio (SNR) is increased by 19.1%. The fault signal is successfully extracted by the proposed composite method.	aliasing;distortion;experiment;life model decoy;modal logic;noise reduction;signal-to-noise ratio	Lin Zhang;Zhijian Wang;Long Quan	2018	Entropy	10.3390/e20050387	mathematical optimization;mathematics;masking (art);aliasing;mode (statistics);kurtosis;background noise;control theory;bearing (mechanical);phenomenon	Robotics	78.92150092351257	-38.52653842382685	190186
841d746954742b7b4829fa060c859255ab3a64dc	bend-it: design and fabrication of kinetic wire characters		Elastically deforming wire structures are lightweight, durable, and can be bent within minutes using CNC bending machines. We present a computational technique for the design of kinetic wire characters, tailored for fabrication on consumer-grade hardware. Our technique takes as input a network of curves or a skeletal animation, then estimates a cable-driven, compliant wire structure which matches user-selected targets or keyframes as closely as possible. To enable large localized deformations, we shape wire into functional spring-like entities at a discrete set of locations. We first detect regions where changes to local stiffness properties are needed, then insert bendable entities of varying shape and size. To avoid a discrete optimization, we first optimize stiffness properties of generic, non-fabricable entities which capture well the behavior of our bendable designs. To co-optimize stiffness properties and cable forces, we formulate an equilibrium-constrained minimization problem, safeguarding against inelastic deformations. We demonstrate our method on six fabricated examples, showcasing rich behavior including large deformations and complex, spatial motion.	discrete optimization;entity;key frame;mathematical optimization;rollable display;semiconductor device fabrication;skeletal animation;turbulence kinetic energy	Hongyi Xu;Espen Knoop;Stelian Coros;Moritz Bächer	2018	ACM Trans. Graph.	10.1145/3272127.3275089		Graphics	68.37799892314574	-46.858255934824825	190465
0d699ee6bbc935278f73c1af9e030af8447bbab5	simulation and visualization of knee joint contact using deformable model	contact area;medical simulation;knee joint contact;data visualisation;visualization;geometric modeling knee joint contact deformable model femur menisci contact human knee joint computer generated 3d models clinical problems knee biomechanical analysis graphical visualization virtual medical simulation knee models motion data contact area information physics based algorithm menisci deformation knee menisci contact gait cycles;3d model;biomedical engineering;animation;deformable models knee joints humans data visualization computational modeling cadaver medical simulation solid modeling muscles;geometric modeling;geometric model;knee joint;deformable model;new physics;3d graphics;graphics;digital simulation;solid modelling;digital simulation biomedical engineering data visualisation solid modelling	We introduce a new method for simulating and visualizing femur-menisci contact in human knee joint using computer generated 3D models. Knee joint contact analysis is important for understanding or solving clinical problems. Traditional knee biomechanical analysis is mostly conducted on a cadaver or based on mathematical descriptions. The lack of 3D graphical visualization makes it difficult to understand the analysis and integrate it into virtual medical simulation systems. Our method addresses this problem by using patient specific knee models and motion data to obtain the contact area information. Our method employs a new physics-based algorithm for menisci deformation and can calculate and visualize knee-menisci contact areas during gait cycles.	3d modeling;algorithm;cadaver;contact analysis (cryptanalysis);graphical user interface;simulation	Ying Zhu;Jim X. Chen	2004	The Fourth International Conference onComputer and Information Technology, 2004. CIT '04.	10.1109/CIT.2004.1357278	medical simulation;computer vision;simulation;computer science;geometric modeling	Robotics	70.55664839018111	-46.644139933094216	190739
6b0fb8320951d80826da28f3cd24d3395d50e4fc	phase retrieval: an overview of recent developments		In many physical measurement systems, one can only measure the power spectral density, i.e., the magnitudesquare of the Fourier transform of the underlying signal. For example, in an optical setting, detection devices like CCD cameras and photosensitive films cannot measure the phase of a light wave and instead measure the photon flux. In addition, at a large enough distance from the imaging plane the field is given by the Fourier transform of the image (up to a known phase factor). Thus, in the far field, optical devices essentially measure the Fourier transform magnitude. Since the phase encodes a lot of the structural content of the image, important information is lost. The problem of reconstructing a signal from its Fourier magnitude is known as phase retrieval [1,2]. This reconstruction problem is one with a rich history and arises in many areas of engineering and applied physics, including optics [3], X-ray crystallography [4], astronomical imaging [5], speech processing [6], computational biology [7], blind deconvolution [8] and more. Reconstructing a signal from its Fourier magnitude alone is generally a very difficult task. It is well known that Fourier phase is quite often more important than Fourier magnitude in reconstructing a signal from its Fourier transform [9]. To demonstrate this fact, a synthetic example, courtesy of [10], is provided in Figure 1. The figure shows the result of the following numerical simulation: Two images are Fourier transformed, their Fourier phases are swapped and then they are inverse Fourier transformed. The result clearly demonstrates the importance of Fourier phase. Therefore, simply ignoring the phase and performing an inverse Fourier transform does not lead to satisfactory recovery. Instead, algorithmic phase retrieval can be used, offering a means for recovering the phase from the given magnitude measurements and possibly additional prior knowledge, providing an alternative to sophisticated measurement setups as in holography which attempt to directly measure the phase by requiring interference with another known field. To set up the phase retrieval problem mathematically, we focus on the discretized one-dimensional (1D) setting. Let x = (x[0], x[1], . . . , x[N − 1]) be a signal of length N such that it has non-zero values only within the interval [0, N−1]. Denote by y = (y[0], y[1], . . . , y[N−1])T its N point discrete Fourier transform (DFT) and let z = (z[0], z[1], . . . , z[N − 1]) be the Fourier magnitude-square measurements z[m] = ∣∣y[m]∣∣2. Phase retrieval can be mathematically stated as:	algorithm;autocorrelation;blind deconvolution;charge-coupled device;computation;computational biology;computer simulation;detection theory;discrete fourier transform;discretization;fast fourier transform;holography;interference (communication);phase factor;phase retrieval;radar;reconstruction conjecture;short-time fourier transform;sparse matrix;spectral density;speech processing;synthetic intelligence;system of measurement;variable shadowing	Kishore Jaganathan;Yonina C. Eldar;Babak Hassibi	2015	CoRR		computer vision;mathematical optimization;mathematics;algorithm	Theory	80.37835119143837	-41.07525298339975	190836
37313f2547241cd3a990560d4053a43081618105	efficient simplification of large vector maps rendered onto 3d landscapes	geoscience and remote sensing;topology;simplification;vector map;graphics and multimedia vector map simplification level of detail digital elevation models overlay computer graphics;computer graphics;graphics and multimedia;computational geometry;virtual reality;large scale systems surface topography rendering computer graphics topology geoscience and remote sensing three dimensional displays geodesy environmental management;overlay;large scale system;digital elevation model;surface topography;three dimensional;computer graphic;level of detail;three dimensional displays;remote sensing;digital elevation models;terrain mapping;virtual environment 3d landscape real time rendering large scale vector maps terrain surfaces polylines polygons;virtual reality computational geometry rendering computer graphics terrain mapping;rendering computer graphics;environmental management;large scale systems;geodesy	Real-time rendering of large scale vector maps over terrain surfaces requires displaying substantial numbers of polylines and polygons. The proposed approach simplifies such maps, permitting more efficient rendering and reducing latency in the display and manipulation of a virtual environment.	map;microtubule-associated proteins;real-time locating system;text simplification;virtual reality	Ling Yang;Liqiang Zhang;Jingtao Ma;Zhizhong Kang;Lixin Zhang;Jonathan Li	2011	IEEE Computer Graphics and Applications	10.1109/MCG.2010.63	computer vision;simulation;digital elevation model;computational geometry;computer science;virtual reality;computer graphics (images)	Visualization	69.0556283733242	-51.6422983469383	191040
e189b9340c42c66cef27eea435f01e932370f428	stochastic simulation by image quilting of process-based geological models	shannon entropy;multiple point statistics;fft;tau model;gpgpu;relaxation;voxel reuse	Process-based modeling offers a way to represent realistic geological heterogeneity in subsurface models. The main limitation lies in conditioning such models to data. Multiple-point geostatistics can use these process-based models as training images and address the data conditioning problem. In this work, we further develop image quilting as a method for 3D stochastic simulation capable of mimicking the realism of process-based geological models with minimal modeling effort (i.e. parameter tuning) and at the same time condition them to a variety of data. In particular, we develop a new probabilistic data aggregation method for image quilting that bypasses traditional ad-hoc weighting of auxiliary variables. In addition, we propose a novel criterion for template design in image quilting that generalizes the entropy plot for continuous training images. The criterion is based on the new concept of voxel reuse—a stochastic and quilting-aware function of the training image. We compare our proposed method with other established simulation methods on a set of process-based training images of varying complexity, including a real-case example of stochastic simulation of the buried-valley groundwater system in Denmark.	simulation	Júlio Hoffimann;Céline Scheidt;Adrian Barfod;Jef Caers	2017	Computers & Geosciences	10.1016/j.cageo.2017.05.012	quilting;entropy (information theory);voxel;computer science;statistics;probabilistic logic;stochastic simulation;artificial intelligence;machine learning;conditioning;geostatistics;abstract process	Vision	77.04569068347381	-47.04943357467615	191491
c0a9b33cdfbaf8a6947d56183aa5e669e96d8084	all-hexahedral meshing and remeshing for multi-object manufacturing applications	multi object;remeshing;manufacturing;adaptive core mesh;all hexahedral meshing	All‐hexahedral meshing and remeshing algorithms giving support to finite element modeling of manufacturing processes require continuous development for improving its overall robustness and applicability. This paper draws from a previous all‐hexahedral algorithm presented by the authors and proposes new developments related to the construction of adaptive core meshes and processing of multi‐ objects that are typical of manufacturing applications. Along with the aforementioned improvements there are other developments that will also be presented due to their effectiveness in increasing the robustness of all‐hexahedral algorithms. These include identification and simplification of boundary features, reconstruction of vertices and edges with minimum element distortion, smoothing of nodal points along edges and topology based mesh repairment procedures to ensure completeness of edge representation. The presentation is enriched with examples taken from pure geometry and metal forming applications, and a resistance projection welding industrial test case consisting of four different objects is included to show the capabilities of selective remeshing of objects while maintaining contact conditions and local geometrical details that are critical for electro‐thermo‐mechanical numerical simulations.	approximation algorithm;computer graphics (computer science);distortion;finite element method;hexahedron;image-based meshing;level of detail;numerical analysis;robustness (computer science);simulation;smoothing;test case;vertex (graph theory)	C. V. Nielsen;J. L. M. Fernandes;P. A. F. Martins	2013	Computer-Aided Design	10.1016/j.cad.2013.01.002	structural engineering;simulation;engineering;manufacturing;engineering drawing	Graphics	69.13262509322554	-44.62597645598831	191590
e096a311fc3e1284d795e767aac4269e53b17c2a	deformable haptic model generation through manual exploration	haptic interfaces finite element analysis geometry force mathematical model three dimensional displays data models;computational geometry;mesh generation computational geometry haptic interfaces;position readings virtual deformable model haptic contexts surface geometry elastic parametrization force reading operator assisted exploration 3d mesh model surface contact points internal elastic modulus 3d finite element method deformable homogeneous silicone samples geometry reconstruction 1d model parametrization contact location elastic modulus reconstruction global model parameterization;haptic interfaces;mesh generation	Interaction with virtual deformable models is common in several haptic contexts, such as in medical training simulators. This paper presents a methodological procedure for the creation of such virtual models from their real-life counterparts. Both the surface geometry and the elastic parametrization of an object are reconstructed from position/force readings during an operator-assisted exploration of the object. A 3D mesh model is then generated from the surface contact points. The internal elastic modulus is found using the 3D finite element method. This modeling method is compared with two common 1D elastic models, namely Kelvin-Voigt and Hunt-Crossley. Results using three deformable homogeneous silicone samples show successful geometry reconstruction. 1D model parameterizations exhibit high variation dependent on geometry and contact location. In contrast, elastic modulus reconstruction yields a global model parameterization independent of geometry. Elastic moduli estimated in experiments correlated with their known values, and were shown to be reproducible among samples with different geometries.	3d modeling;computer graphics (computer science);elastic matching;elasticity (data store);experiment;finite element method;haptic technology;java platform, micro edition;modulus of continuity;modulus robot;real life;simulation	Orcun Goksel;Seokhee Jeon;Matthias Harders;Gábor Székely	2013	2013 World Haptics Conference (WHC)	10.1109/WHC.2013.6548466	mesh generation;simulation;computational geometry;mathematics;geometry;engineering drawing	Vision	71.63227801023324	-46.34868455158688	191935
0ebda1c26db02db7d5657252a2428a96c18675e3	an image-space morse decomposition for 2d vector fields		Morse decompositions have been proposed to compute and represent the topological structure of steady vector fields. Compared to the conventional differential topology, Morse decomposition and the resulting Morse Connection Graph (MCG) is numerically stable. However, the granularity of the original Morse decomposition is constrained by the resolution of the underlying spatial discretization, which typically results in non-smooth representation. In this work, an Image-Space Morse decomposition (ISMD) framework is proposed to address this issue. Compared to the original method, ISMD first projects the original vector field onto an image plane, then computes the Morse decomposition based on the projected field with pixels as the smallest elements. Thus, pixel-level accuracy can be achieved. This ISMD framework has been applied to a number of synthetic and real-world steady vector fields to demonstrate its utility. The performance of the ISMD is carefully studied and reported. Finally, with ISMD an ensemble Morse decomposition can be studied and visualized, which is shown useful for visualizing the stability of the Morse sets with respect to the error introduced in the numerical computation and the perturbation to the input vector fields.	code;computation;directed graph;discretization;image plane;numerical analysis;numerical integration;numerical stability;pixel;sampling (signal processing);singular value decomposition;synthetic data;synthetic intelligence	Guoning Chen;Shuyu Xu	2015		10.1117/12.2080196	vector decomposition;discrete morse theory;morse theory	Visualization	72.03395400152564	-44.87089948639626	192249
0749b2a9066ffdf3eb1a7f3d7839b2444dd450d4	rain simulation in dynamic scenes	gpu;collision detection;compute unified device architecture cuda;rain;simulation real time rendering	Rain is a complex phenomenon and its simulation is usually very costly. In this article, the authors propose a fully-GPU rain simulation based on the utilization of particle systems. The flexibility of CUDA allows the authors to include, aside from the rainfall simulation, a system for the detection and handling of the collisions of particles against the scenario. This detection system allows for the simulation of splashes at the same time. This system obtains a very high performance because of the hardware programming capabilities of CUDA.	cuda;graphics processing unit;particle system;simulation	Anna Puig-Centelles;Nicolau Sunyer;Oscar Ripolles;Miguel Chover;Mateu Sbert	2011	IJCICG	10.4018/jcicg.2011070102	real-time computing;simulation;computer science;artificial intelligence;collision detection;computer graphics (images)	HPC	72.17464807401835	-50.75142814439672	192680
8a900c4521df1de31418b01d1c90eeb30db31985	a morphological interpolation approach - geodesic set definition in case of empty intersection	binary interpolation;geodesic distance;polynomial interpolation;mathematical morphology;3d smooth visualisation.	This article presents a morphological approach for the generation of intermediate 2D objects, using others 2D objects, as initial ones. The approach is useful for a smooth realistic 3D object’s visualisation, in case of shortage input information. The study is based on mathematical morphology concepts, such as geodesic distance and geodesic set definition, dealing with the case of empty intersection between the objects, in a orthogonal projection over a plane. A classical approach is used to define spatial polynomial curves, interpolating the extreme left and right sets of visible essential border points of the initial objects. Further, the arc of each curve bordered between a couple of initial 2D objects is orthogonal projected over the lower plane together with the upper 2D object. The projections and the object, all over the same plane, are used to create the geodesic set. Then, a set of intermediate sections, between each couple of initial objects, is defined by the application of a morphological linear interpolation. Experiments were performed in order to validate the theory. Real data obtained by well logs performed in Vale de Milhaços (Setúbal - Portugal) was used for this propose, and the results are given in the article.	interpolation	Isabel Granado;N. Sirakov;Fernando Muge	2000		10.1007/0-306-47025-X_9	intersection	Vision	68.81113148894798	-41.88978779996583	192799
8278336e39fe3ca7b544ab8801de497aab3407a4	toward quality surface meshing	curvature adaptation.;surface mesh generation;size map;triangular;parameter space;mesh generation	This paper presents recent progress and extensions to TriQuaMesh (TQM) [1], targeted at providing good quality surface meshes: Increased robustness of the 1D mesh generator to handle highly non linear size variations; interior node generation driven by a size variation interpolation domain; improved mesh distortion reduction between the parameter space and the physical space. The concepts of Size Control, Size Map and Triangle Map are introduced to increase the flexibility and the control on the final mesh. These concepts are general and apply to any meshing algorithm, although they will be illustrated with TQM.	algorithm;best practice;discretization;distortion;interpolation;mesh generation;polygon mesh;sampling (signal processing);smoothing;tree (data structure)	Jean Cabello	2003			mathematical optimization;parameter space;robustness (computer science);interpolation;mesh generation;polygon mesh;distortion;nonlinear system;mathematics;engineering drawing	Graphics	69.33624875495902	-44.335764478020465	192873
041eb3a1229c3ac7fff1e50bf09b7613290c9f88	corotated sph for deformable solids	deformation field;deformable solid;corotated sph;sph scheme;computer graphics;linear strain tensor;early sph approach;collinear particle data set;smoothed particle hydrodynamics;natural phenomenon;novel corotational sph formulation	Smoothed Particle Hydrodynamics (SPH) is a powerful technique for the animation of natural phenomena. While early SPH approaches in Computer Graphics have mainly been concerned with liquids or gases, recent research also focuses on the dynamics of deformable solids using SPH. In this paper, we present a novel corotational SPH formulation for deformable solids. The rigid body modes are extracted from the deformation field which allows to use a linear strain tensor. In contrast to previous rotationally invariant meshless approaches, we show examples using coplanar and collinear particle data sets. The presented approach further allows for a unified meshfree representation of deformable solids and fluids. This enables the animation of sophisticated phenomena, such as phase transitions. The versatility and the efficiency of the presented SPH scheme for deformable solids is illustrated in various experiments.	computer graphics;eurographics;experiment;isometric projection;nonlinear system;particle filter;simulation;smoothing;tensor operator;transformation matrix;upwind scheme	Markus Becker;Markus Ihmsen;Matthias Teschner	2009		10.2312/EG/DL/conf/EG2009/nph/027-034	classical mechanics;simulation;physics;computer graphics (images)	Graphics	70.54721370447476	-47.432552476977364	193010
c362a9852c31e6868500da800fa79e4086753251	graphical analysis and visualization of 3-d properties of molecules and solids	graph theory;electron charge distribution;teoria grafo;modelo 3 dimensiones;etude theorique;modele 3 dimensions;fonction onde;molecula;three dimensional model;theorie graphe;crystals;distribution charge electronique;molecules;distribucion carga electronica;estudio teorico;cristal;funcion onda;wave function;molecule;theoretical study	A simple approach is presented for visualizing three-dimensional properties of molecular and crystalline systems by using PC-based molecular drawing software. This allows a scientist with lower-cost PC-graphics capability to carry out analysis of data, such as wave functions and electron density, which would otherwise require a dedicated graphics workstation and expensive specialized software		Mark D. Jackson	1991	Journal of Chemical Information and Computer Sciences	10.1021/ci00001a023	molecule;telecommunications;graph theory;algorithm;quantum mechanics	Visualization	74.49146675549257	-47.129434185668195	193027
d0fff6c0c56078d2c98d8a3b1cf2189b861eb940	changeable degree spline basis functions	computacion informatica;b spline curve;b spline basis function;ciencias basicas y experimentales;matematicas;free form shapes;grupo a;piecewise polynomial function;parametric curve;piecewise polynomial curve	A B-spline basis function is a piecewise function of polynomials of equal degree on its support interval. This paper extends B-spline basis functions to changeable degree spline (CD-spline for short) basis functions, each of which may consist of polynomials of different degrees on its support interval. The CD-spline basis functions possess many B-spline-like properties and include the B-spline basis functions as subcases. Their corresponding parametric curves, called CD-spline curves, are like B-spline curves and also have many good properties. If we use the CD-spline basis functions to design a curve made up of polynomial segments of different degrees, the number of control points may be decreased.	basis function;spline (mathematics)	Wanqiang Shen;Guozhao Wang	2010	J. Computational Applied Mathematics	10.1016/j.cam.2010.03.015	spline interpolation;b-spline;spline;mathematical optimization;mathematical analysis;perfect spline;geometric design;smoothing spline;parametric equation;basis function;hermite spline;mathematics;geometry;flat spline;piecewise	Theory	69.24384065951396	-40.49050414527711	193087
1fc916c71bbf8e53901dd2c07f8412a7bc9d57dc	seismic multiple removal with a primal-dual proximal algorithm	seismology;variational techniques;perturbation techniques;signal restoration optimization methods wavelet transforms adaptive filters geophysical signal processing;wavelet transforms;statistical analysis;geophysical signal processing;wavelet frame seismic multiple removal primal dual proximal algorithm random perturbations structured perturbations seismic data geophysical information seismic multiples wave field bouncing time varying filtering process amplitude inaccuracies time shift average frequency filters estimation convex variational formulation noise statistics filter time variations;random processes;wavelet transforms geophysical signal processing perturbation techniques random processes seismology statistical analysis time varying filters variational techniques;time varying filters;noise signal processing algorithms indexes seismic waves estimation geophysics adaptation models	Both random and structured perturbations affect seismic data. Their removal, to unveil meaningful geophysical information, requires additional priors. Seismic multiples are one form of structured perturbations related to wave-field bouncing. In this paper, we model these undesired signals through a time-varying filtering process accounting for inaccuracies in amplitude, time-shift and average frequency of available templates. We recast the problem of jointly estimating the filters and the signal of interest (primary) in a new convex variational formulation, allowing the incorporation of knowledge about the noise statistics. By making some physically plausible assumptions about the slow time variations of the filters, and by adopting a potential promoting the sparsity of the primary in a wavelet frame, we design a primal-dual algorithm which yields good performance in the provided simulation examples.	diffusing update algorithm;simulation;sparse matrix;variational principle;wavelet	Mai Quyen Pham;Caroline Chaux;Laurent Duval;Jean-Christophe Pesquet	2013	2013 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2013.6638056	stochastic process;mathematical optimization;mathematics;statistics;wavelet transform	Robotics	78.78348304756027	-42.22102889830378	193311
2064ee91e98030f38d66a5d4f22ade2aa0729848	isa and ibfvs: image space-based visualization of flow on surfaces	temporal correlation;time dependent;texture mapping index terms unsteady flow visualization computational fluid dynamics cfd surface representation surface rendering;computer graphic equipment flow visualisation surface fitting correlation methods mesh generation image texture computational fluid dynamics rendering computer graphics computational geometry;instruction sets visualization surface texture geometry computational fluid dynamics rendering computer graphics fluid flow computer society functional analysis image analysis;surface representation;texture mapping;computation fluid dynamics;unsteady flow visualization;computational geometry;computer graphic equipment;surface fitting;surface rendering;triangular mesh;correlation methods;indexing terms;65;image texture;computational fluid dynamics;flow visualisation;graphics hardware;computational fluid dynamics cfd;index terms unsteady flow visualization;unsteady flow;cfd;vector field;rendering computer graphics;texture mapping image space advection image space based flow visualization curved surfaces time dependent vector field spatio temporal correlation 3d vector fields triangular meshes texture properties graphics hardware unsteady flow visualization computational fluid dynamics surface representation surface rendering;mesh generation;flow visualization	We present a side-by-side analysis of two recent image space approaches for the visualization of vector fields on surfaces. The two methods, image space advection (ISA) and image-based flow visualization for curved surfaces (IBFVS) generate dense representations of time-dependent vector fields with high spatio-temporal correlation. While the 3D vector fields are associated with arbitrary surfaces represented by triangular meshes, the generation and advection of texture properties is confined to image space. Fast frame rates are achieved by exploiting frame-to-frame coherency and graphics hardware. In our comparison of ISA and IBFVS, we point out the strengths and weaknesses of each approach and give recommendations as to when and where they are best applied.	body dysmorphic disorders;graphics hardware;image-based flow visualization;imagery;triangulated irregular network;weakness	Robert S. Laramee;Jarke J. van Wijk;Bruno Jobard;Helwig Hauser	2004	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2004.47	computer vision;computational fluid dynamics;computational geometry;computer science;theoretical computer science;geometry;computer graphics (images)	Visualization	72.294244965161	-51.32866723802976	193494
be53d323c4de1e500d30e60f84749482174dd3b0	interactive volume ray tracing	implicit kd tree;pluecker coordinates;graphik;plucker koordinaten;visualisierung;visualization;stahlverfolgungs algorithmus;impliziter kd baum;rasterisierungsansatz	Volume rendering is one of the most demanding and interesting topics among scientific visualization. Applications include medical examinations, simulation of physical processes, and visual art. Most of these applications demand interactivity with respect to the viewing and visualization parameters. The ray tracing algorithm, although inherently simulating light interaction with participating media, was always considered too slow. Instead, most researchers followed object-order algorithms better suited for graphics adapters, although such approaches often suffer either from low quality or lack of flexibility. Another alternative is to speed up the ray tracing algorithm to make it competitive for volumetric visualization tasks. Since the advent of modern graphic adapters, research in this area had somehow ceased, although some limitations of GPUs, e.g. limited graphics board memory and tedious programming model, are still a problem. The two methods discussed in this thesis are therefore purely software-based since it is believed that software implementations allow for a far better optimization process before porting algorithms to hardware. The first method is called implicit kd-tree, which is a hierarchical spatial acceleration structure originally developed for iso-surface rendering of regular data sets that now supports semi-transparent rendering, time-dependent data visualization, and is even used in non volume-rendering applications. The second algorithm uses so-called Plücker coordinates, providing a fast incremental traversal for data sets consisting of tetrahedral or hexahedral primitives. Both algorithms are highly optimized to support interactive rendering of volumetric data sets and are therefore major contributions towards a flexible and interactive volume ray tracing framework.		Gerd Marmitt	2009			visual arts;computer science;performance art	Visualization	69.76084398294924	-51.21787188301756	193576
80b9df54d0c2a56325186ca88e01ba830ec93b74	mesh repair with user-friendly topology control	topology;topology control;closing;morphological operation;morphology;discrete model;polygonal meshes;2 manifold;topological defect;triangle mesh;article;large classes;opening	Limitations of current 3D acquisition technology often lead to polygonal meshes exhibiting a number of geometrical and topological defects which prevent them from widespread use. In this paper we present a new method for model repair which takes as input an arbitrary polygonal mesh and outputs a valid two-manifold triangle mesh. Unlike previous work, our method allows users to quickly identify areas with potential topological errors and to choose how to fix them in a userfriendly manner. Key steps of our algorithm include the conversion of the input model into a set of voxels, the use of morphological operators to allow the user to modify the topology of the discrete model, and the conversion of the corrected voxel set back into a two-manifold triangle mesh. Our experiments demonstrate that the proposed algorithm is suitable for repairing meshes of a large class of shapes.	algorithm;experiment;mathematical morphology;polygon mesh;topology control;triangle mesh;usability;voxel	Franck Hétroy;Stéphanie Rey;Carlos Andújar;Pere Brunet;Alvar Vinacua	2011	Computer-Aided Design	10.1016/j.cad.2010.09.012	topology;morphology;computer science;volume mesh;triangle mesh;closing;mathematics;geometry;extension topology;topological defect;opening;engineering drawing	Graphics	68.91990956944579	-44.074518847002814	194826
956c9ef3cba826b4efc8749337330d21dc4ffb03	wiener crosses borders: interpolation based on second order models	second order;image interpolation;linear filtering;missing data;point of view	Interpolation of signals (arbitrary dimension, here: 2D images) with missing data points is addressed from a statistical point of view. We present a general framework for which a Wiener-style MMSE estimator can be seamlessly adapted to deal with problems such as image interpolation (inpainting), reconstruction from sparse samples, and image extrapolation. The proposed method gives a precise answer on a) how arbitrary can linear filters can be applied to initially incomplete signals and b) shows the definite way to extend images beyond theirs borders such that no size reduction occurs if a linear filter/operator is to be applied to the image.	acf;data point;emoticon;extrapolation;fast fourier transform;inpainting;interpolation;linear logic;missing data;randomness;requirement;sparse matrix;wiener filter	Alvaro Guevara;Rudolf Mester	2011		10.1117/12.871198	mathematical optimization;bilinear interpolation;missing data;interpolation;stairstep interpolation;linear filter;linear interpolation;nearest-neighbor interpolation;multivariate interpolation;second-order logic;trilinear interpolation;image scaling	Vision	73.96456469577902	-40.72939458881337	195064
eba5810bbe37e400be82c50659a22edbbe2b66d1	geometry and morphology of the cosmic web: analyzing spatial patterns in the universe	scalespace analysis;topology;topology astronomy computing clusters of galaxies computational geometry cosmology feature extraction image representation mathematical morphology mesh generation;delaunay tessellation field estimator;mathematical morphology;interpolation;watershed voidfinder;multiscale morphology filter;density measurement;methods;particle measurements;geometry morphology pattern analysis discrete transforms density measurement volume measurement particle measurements interpolation anisotropic magnetoresistance filters;weblike spatial arrangement;continuous volume filling density fields;clusters of galaxies;geometry;computational geometry;filters;filamentary spine;adaptive grid;anisotropic morphology;universe;websearch;data mining;alphashapes;spineweb analysis;discrete watershed transform;hep;dense compact galaxy clusters;cosmic web;large scale;artificial neural networks;morphology;simulation particle distribution;universe cosmic web voronoi tessellations weblike spatial arrangement delaunay tessellation field estimator adaptive grid simulation particle distribution continuous volume filling density fields linear interpolation anisotropic morphology cosmic matter distribution topology multiscale morphology filter scalespace analysis watershed voidfinder discrete watershed transform spineweb analysis morse theory filamentary spine alphashapes dense compact galaxy clusters near empty void regions;watershed transform;astronomy computing;spatial pattern;shape;discrete transforms;cosmic matter distribution topology;linear interpolation;near empty void regions;delaunay tessellation;image representation;feature extraction;theory;anisotropic magnetoresistance;cosmology;voronoi tessellation;volume measurement;pattern analysis;numerical;extraterrestrial measurements;mesh generation;morse theory;surveys cosmology theory large scale structure of universe methods numerical;voronoi tessellations;surveys;structure of universe	We review the analysis of the Cosmic Web by means of an extensive toolset based on the use of Delaunay and Voronoi tessellations. The Cosmic Web is the salientand pervasive foamlike pattern in which matter has organized itself on scales of a few up to more than a hundred Megaparsec. The weblike spatial arrangementof galaxies and mass into elongated filaments,sheetlike walls and dense compact clusters, the existence of large near-empty void regions and the hierarchical nature of this mass distribution are three major characteristics of the comsic matter distribution.First, we describe the Delaunay Tessellation Field Estimator.Using the unique adaptive qualities of Voronoi and Delaunay tessellations, DTFE infers the densityfield from the (contiguous) Voronoi tessellation of a sampled galaxy or simulation particle distribution and uses the Delaunay tessellation as adaptive grid for defining continuous volume-filling fields of density and other measured quantities through linear interpolation. The resulting DTFE formalism is shown to recover the hierarchical nature and the anisotropic morphologyof the cosmic matter distribution. The Multiscale Morphology Filter (MMF) uses the DTFE density field to extract the diverse morphological elements - filaments, sheets and clusters - on the basis of a ScaleSpace analysis which searches for these morphologies over a range of scales. Subsequently, we discuss the Watershed Voidfinder (WVF), which invokes the discrete watershed transform to identify voids in the cosmic matter distribution. The WVF is able todetermine the location, size and shape of the voids. The watershed transform is also a key element in the SpineWeb analysis of the cosmic matter distribution. Finding its mathematical foundation in Morse theory, it allowsthe determination of the filamentary spine and connected walls in the cosmic matter density field through the identification of the singularities and corresponding separatrices. The first results of a directimplementation on the Delaunay tessellation itself are presented. Finally, we describe the concept of Alphashapes for assessing the topology of the cosmic matter distribution.	adaptive mesh refinement;anisotropic diffusion;cosmic;centroidal voronoi tessellation;delaunay tessellation field estimator;delaunay triangulation;dendritic spine;incremental funding methodology;linear interpolation;mathematical morphology;semantics (computer science);simulation;star wars galaxies;voronoi diagram;watershed (image processing)	Rien van de Weygaert;Miguel A. Aragon-Calvo;Bernard J. T. Jones;Erwin Platen	2009	2009 Sixth International Symposium on Voronoi Diagrams	10.1109/ISVD.2009.36	classical mechanics;voronoi diagram;morphology;computational geometry;cosmology;physics	Visualization	76.17561164510816	-44.075547578620785	195960
525d44a28f99d14f1b048d5e308d0ed744ebca67	geometric construction for setback vertex blending	computer aided design;modele geometrique;geometrie algorithmique;surface parametrique;computational geometry;polynomial interpolation;courbure;surface lisse;satisfiability;smooth surface;smooth transition;conception assistee;curvatura;curvature;interpolacion polinomial;superficie lisa;parametric surface;interpolation polynomiale;geometrical model;modelo geometrico	Setback vertex blends merge edge blends by broadening them at certain distances from the vertex. In this way an overall smooth transition is provided. The necessity of this type of vertex blend is illustrated by examples. We suggest a scheme where vertex blends are represented by 2n-sided patches, though special cases may also arise with an odd number of sides. Standard polynomial patches are combined according to the so-called setback split, which provides a natural structure to define vertex blends and offers free parameters to adjust the interior shape. The steps to create setback vertex blends and the basic mathematical constraints to be satisfied are illustrated by the control frame construction, which follows a repeated chamfering strategy. The control frame approximates the vertex blend and indirectly determines most of the surface qualities. A few special vertex blends are analysed in the last part of the paper.	alpha compositing	Tamás Várady;Alyn P. Rockwood	1997	Computer-Aided Design	10.1016/S0010-4485(96)00070-X	combinatorics;topology;vertex normal;computational geometry;polynomial interpolation;engineering;computer aided design;parametric surface;mathematics;geometry;vertex;curvature;satisfiability	EDA	68.53736638447451	-41.44060304449727	195997
13b9336c170f089a7fc33ec1fb7cd9d5545a210e	view-dependent culling of dynamic systems in virtual environments	moving object;dynamic system;virtual environment	Culling of Dynamic Systems in Virtual Environments Stephen Chenney David Forsyth* University of California at Berkeley Scalable rendering of virtual environments requires culling objects that have no etTect on the view. This paper explores culling moving objects by not solving the equations of motion of objects that don’t tiect the view. While this approach could be scalable for many kinds of environments, it raises two problems: consistency ensuring that objects that come back into view do so in the right state and completeness ensuring that objects that would have entered the view volume as a result of their motions, do so. Solutions to these problems lie in studying the statistics of the motion of objects. We show strategies for addressing the problem of consistency, with a number of examples that illustrate both the difficulties involved in, and the potential gains to be obtained by, formulating a comprehensive approach. CR Descriptors: 1.3.7 [Computer Graphics]: ThreeDimensional Graphi@ and Realism Virtual reality 1.6.5 [Simulation and Modeling]: Model Development Modeling methodologies 1.6.8 [Simulation and Modeling]: Types of Simulation Animation	computer graphics;data descriptor;dynamical system;forsyth–edwards notation;regular expression;scalability;simulation;viewing frustum;virtual reality	Stephen Chenney;David A. Forsyth	1997		10.1145/253284.253307	computer science;virtual machine;dynamical system	Graphics	70.07725250631293	-48.15707224419455	196081
74ab6f555c42c578c03ed87d1f93f5002bd985d6	performance modeling of vl3 volume rendering on gpu-based clusters	distributed network graphics;graphics systems;i 3 2 computer graphics	This paper presents an analytical model for parallel volume rendering of large datasets using GPU-based clusters. The model is focused on the parallel volume rendering and compositing stages and predicts their performance requiring only a few input parameters. We also present vl3, a novel parallel volume rendering framework for visualization of large datasets. Its performance is evaluated on a GPU-based cluster, weak and strong scaling are studied, and model predictions are validated with experimental results on up to 128 GPUs.	graphics processing unit;volume rendering	Silvio Rizzi;Mark Hereld;Joseph A. Insley;Michael E. Papka;Thomas D. Uram;Venkatram Vishwanath	2014		10.2312/pgv.20141086	computational science;tiled rendering;parallel computing;3d rendering;rendering;computer science;parallel rendering;real-time computer graphics;texture memory;computer graphics;alternate frame rendering;volume rendering;software rendering;computer graphics (images)	Visualization	69.69433372591683	-50.983220929785375	196303
bd1ad8bfe0821127b51f0f607272e857d27612a2	constructing curvature-continuous surfaces by blending	arbitrary control;flat spot;curvature singularity;simple modification;overall shape;extraordinary vertex;loop subdivision surface;curvature-continuous surface;existing subdivision code;subdivision surface	In this paper we describe an approach to the construction of curvature-continuous surfaces with arbitrary control meshes using subdivision. Using a simple modification of the widely used Loop subdivision algorithm we obtain perturbed surfaces which retain the overall shape and appearance of Loop subdivision surfaces but no longer have flat spots or curvature singularities at extraordinary vertices. Our method is computationally efficient and can be easily added to any existing subdivision code.	algorithm;algorithmic efficiency;alpha compositing;basis function;catmull–clark subdivision surface;eurographics;finite subdivision rule;ibm notes;loop subdivision surface;non-uniform rational b-spline;vertex (geometry)	Denis Zorin	2006			finite subdivision rule;topology;mathematics;geometry;subdivision surface	Graphics	69.14358156487152	-43.22206960693765	196620
4f71ca241d1f6efee5355153c377d495d079fa58	an adaptive model for particle fluid surface reconstruction	adaptive model;particle-based fluid;scalar field;surface reconstruction			Fengquan Zhang;Xukun Shen;Xiang Long	2013	IEICE Transactions		scalar field;surface reconstruction	Vision	70.13077016435822	-48.807053429604956	197197
5545101c10564780f8d79e90f270af8e5da175d2	constructing rheologically deformable virtual objects	finite element method rheologically deformable virtual objects rheological objects modeling biological tissues deformation;virtual reality;biological tissue;rheology;finite element analysis virtual reality rheology deformation solid modelling;deformation;rheology deformable models shape elasticity plastics viscosity solids biological system modeling biological tissues systematics;finite element analysis;physical model;deformable model;solid modelling	A physical modeling of rheological objects is presented. Objects showing rheological nature involve foods and biological tissues yet no systematic approach to build their virtual objects is not established. In this article, we will construct 2D/3D virtual rheological objects.	smart objects	Masafumi Kimura;Yuuta Sugiyama;Seiji Tomokuni;Shinichi Hirai	2003		10.1109/ROBOT.2003.1242170	rheology;simulation;physical model;computer science;finite element method;virtual reality;deformation;mechanical engineering	Visualization	71.52936234164903	-46.59341876565988	197635
c76f3807b797f0b7003a223df4b9cb1bb93d138e	the normalform of a space curve and its application to surface design	plane curve	)=0 with ∥▿h∥=1. The normalform function h is (unlike the latter cases) not differentiable at curve points. Despite of this disadvantage the normalform is a suitable tool for designing surfaces which can be treated as common implicit surfaces. Many examples (bisector surfaces, constant distance sum/product surfaces, metamorphoses, blending surfaces, smooth approximation surfaces) demonstrate applications of the normalform to surface design.		Erich Hartmann	2001	The Visual Computer	10.1007/s003710100117446	plane curve;mathematical analysis;topology;computer science;mathematics;geometry	HCI	69.65674815881636	-41.133617320061965	197862
1b1f2cc2e8754f95aaf71d87563206e0d5882da7	repairing non-manifold triangle meshes using simulated annealing	optimization technique;simulated annealing;cad repair;repairing triangulations;triangle mesh;reverse engineering	In the field of reverse engineering one often faces the problem of repairing triangulations with holes, intersecting triangles, Möbius-band-like structures or other artifacts. In this paper we present a novel approach for generating manifold triangle meshes from such incomplete or imperfect triangulations. Even for heavily damaged triangulations, representing closed surfaces with arbitrary genus, our algorithm results in correct manifold triangle meshes. The algorithm is based on a randomized optimization technique from probability calculus called simulated annealing.	genus (mathematics);manifold alignment;mathematical optimization;point cloud;random optimization;randomized algorithm;randomness;reverse engineering;sampling (signal processing);simulated annealing;simulation;triangulated irregular network	Marc Wagner;Ulf Labsik;Günther Greiner	2003	International Journal of Shape Modeling	10.1142/S0218654303000085	combinatorics;simulated annealing;computer science;triangle mesh;mathematics;geometry;engineering drawing;reverse engineering	Graphics	68.85542369546826	-43.18572802207835	198038
1c623358764f2021ac20bf7bb7c4b54a1bb3cb9a	real-time volumetric haptic and visual burrhole simulation	haptic interfaces bones drilling medical simulation surgery rendering computer graphics force feedback computational modeling brain modeling computer graphics;3d texture based volume rendering volumetric haptic burrhole simulation visual burrhole simulation virtual reality craniotomy surgical simulator bone cutting tools voxel based bone surface boundary detection force feedback calculation voxmap point shell algorithm haptics rendering loop bone erosion method;paper;surgical simulation;haptic device;volume rendering;real time;virtual reality;nvidia geforce 7800 gtx;virtual reality bone force feedback haptic interfaces medical image processing rendering computer graphics surgery;force feedback;3d model;haptic rendering;medical image processing;bone;surgery;nvidia;algorithms;opengl;i 3 6 computer graphics methodology and techniques interaction techniques volumetric haptics volume rendering surgical simulation bone drilling i 3 6 computer graphics three dimensional graphics and realism virtual reality;computer science;haptic interfaces;boundary detection;rendering computer graphics;i 3 6 computer graphics three dimensional graphics and realismvirtual reality;3d graphics and realism;volumetric haptics;bone drilling;i 3 6 computer graphics methodology and techniquesinteraction techniques;haptic interaction;rendering	"""This paper describes real-time volumetric haptic and visual algorithms developed to simulate burrhole creation for a virtual reality-based craniotomy surgical simulator. A modified Voxmap point-shell algorithm (McNeely et al., 1999), (Renz et al., 2001) is created to simulate haptic interactions between bone cutting tools and voxel-based bone. New surface boundary detection and force feedback calculation methods help reduce """"force discontinuities"""" of the original Voxmap point-shell algorithm. To maintain stable haptic update rates, new forces are calculated outside the haptics rendering loop. A multi-rate haptic solution (Cavusoglu and Tendick, 2000) is used to introduce calculated forces into the haptics loop and to interpolate forces between updates. A bone erosion method is also created to simulate bone drilling capabilities of different tools. 3D texture-based volume rendering is used to display the bone and to visually remove bone material due to drilling in real-time. Volumetric shading is computed by the GPU of the video card. The algorithms described make it possible to simulate several tools typically used for a craniotomy. Realistic 3D models are also created from real surgical tools and controlled by the haptic device"""	3d modeling;algorithm;graphics processing unit;haptic technology;interaction;interpolation;real-time clock;real-time transcription;shading;simulation;video card;virtual reality;volume rendering;volumetric display;voxel	Eric Acosta;Alan Liu	2007	2007 IEEE Virtual Reality Conference	10.1109/VR.2007.352492	computer vision;simulation;computer science;artificial intelligence;virtual reality;haptic technology;computer graphics (images)	Visualization	70.0790255205998	-48.53307412175637	198154
f828e83bef577404a1354045adcae99cbe4f1a29	estimating material properties of deformable objects by considering global object behavior in video streams	computer vision;physically based simulation;material property estimation;global deformation;data driven animation	One of the crucial components in improving simulation quality in physics-based animation of deformable object is finding proper material properties that define the movement upon external excitation. Most work in the estimation of material properties for highly deformable objects involves applying localized force to a point on the object’s surface with mechanical devices and measuring the displacement of the surface at the contact point and surrounding points. While understanding this localized behavior provides a step towards accurately simulating objects with known material properties, an understanding of the global behavior of the object undergoing deformation is more important for many practical applications. This paper describes both the computer vision based techniques for tracking global position information of moving deformable objects from a video stream and the optimization routine for estimating the elasticity parameters of a mass-spring simulation. The collected data is the object’s surface node position of object over time which is used to a data-driven simulation of that object to match the behavior of a virtual object to the corresponding real one. This paper demonstrates that estimating material properties of highly elastic objects by matching the global behavior of the object in a video is possible with the proposed method and the experimental results show that the captured and simulated motions are well matched each other.	3d modeling;algorithm;computer vision;displacement mapping;display resolution;elasticity (data store);experiment;image registration;mathematical optimization;real life;simulation;streaming media;traffic enforcement camera	Min-Hyung Choi;Steven C. Wilber;Min Hong	2014	Multimedia Tools and Applications	10.1007/s11042-014-1995-1	computer vision;simulation;computer science;object-oriented design;computer graphics (images)	Robotics	69.45354687617456	-47.35305641350333	198234
3ac2e74c4913adb3a8f8a5abc01307f006ffe689	eigenmodes of surface energies for shape analysis	surface energy;resting state;shape analysis;physical model	In this work, we study the spectra and eigenmodes of the Hessian of various discrete surface energies and discuss applications to shape analysis. In particular, we consider a physical model that describes the vibration modes and frequencies of a surface through the eigenfunctions and eigenvalues of the Hessian of a deformation energy, and we derive a closed form representation for the Hessian (at the rest state of the energy) for a general class of deformation energies. Furthermore, we design a quadratic energy, such that the eigenmodes of the Hessian of this energy are sensitive to the extrinsic curvature of the surface. Based on these spectra and eigenmodes, we derive two shape signatures. One that measures the similarity of points on a surface, and another that can be used to identify features of the surface. In addition, we discuss a spectral quadrangulation scheme for surfaces.	hessian;normal mode;shape analysis (digital geometry);squaregraph;type signature	Klaus Hildebrandt;Christian Schulz;Christoph von Tycowicz;Konrad Polthier	2010		10.1007/978-3-642-13411-1_20	surface energy;computer vision;topology;physical model;shape analysis;mathematics;geometry;resting state fmri	Vision	74.40193607380486	-42.20768124899357	198409
2ebb09e0248fe47950729c7ff93445c75ae93554	morphing of triangular meshes in shape space	computational geometry;triangular mesh;morphing;linear interpolation;shape space;geometry processing	We present a novel approach to morph between two isometric poses of the same non-rigid object given as triangular meshes. We model the morphs as linear interpolations in a suitable shape space S. For triangulated 3D polygons, we prove that interpolating linearly in this shape space corresponds to the most isometric morph in R. We then extend this shape space to arbitrary triangulations in 3D using a heuristic approach and show the practical use of the approach using experiments. Furthermore, we discuss a modified shape space that is useful for isometric skeleton morphing. All of the newly presented approaches solve the morphing problem without the need to solve a minimization problem.	energy minimization;experiment;heuristic;interpolation;isometric projection;linear programming;mathematical optimization;morphing;nonlinear programming;nonlinear system;optimization problem;time complexity;triangle mesh;triangulated irregular network	Stefanie Wuhrer;Prosenjit Bose;Chang Shu;Joseph O'Rourke;Alan Brunton	2010	International Journal of Shape Modeling	10.1142/S0218654310001341	combinatorics;topology;computational geometry;computer science;triangle mesh;mathematics;geometry;linear interpolation;morphing	Graphics	69.55064008928933	-41.85766817463264	198699
9c293e6f808ff0e4b61b8b8a16b0e9de8350b4ba	target curvature driven fairing algorithm for planar cubic b-spline curves	constrained optimization;tecnologia electronica telecomunicaciones;computacion informatica;b spline curve;objective function;ciencias basicas y experimentales;target curvature plots;energy minimization;tecnologias;grupo a;fairing;b spline curves	This paper presents a new algorithm for fairing planar cubic B-spline curves. Target curvature plots prescribed by designers according to design intent are used to identify bad points and bad curve segments. The corresponding control points are then modified using local constrained optimization. The objective function is a weighed combination of two components, which are associated with the fairness in the sense of energy minimization and the coherence to the original design, respectively. Hence, designers have more control over the fairing process by using appropriate weights for the two components. Several numerical examples are provided to demonstrate the effectiveness of the algorithm.	algorithm;b-spline;cubic function;spline (mathematics)	Weishi Li;Shuhong Xu;Jianmin Zheng;Gang Zhao	2004	Computer Aided Geometric Design	10.1016/j.cagd.2004.03.004	mathematical optimization;constrained optimization;mathematics;geometry;energy minimization	EDA	69.7658428435207	-38.59573385803934	199139
131c4d86db1342400a928d902f9e60ac04dc82ef	on the signal interference structure generated by modified wigner-ville distribution	time measurement;frequency control;time frequency analysis frequency control time measurement;wavelet packet decomposition;matching pursuit;wigner ville distribution;time frequency analysis;time frequency distribution	In last years, the time-frequency distributions issued by wavelet packet decomposition or matching pursuit have been more and more used, due to the possibility to extract the information about local signal singularity. The main problem is the choice of decomposition parameters, such as decomposition level or wavelet family.	interference (communication);matching pursuit;network packet;wavelet packet decomposition;wigner quasiprobability distribution	Cornel Ioana;André Quinquis	2002	2002 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2002.5745826	speech recognition;time–frequency analysis;automatic frequency control;mathematics;wavelet packet decomposition;statistics;matching pursuit;time	Robotics	79.67804667268382	-38.96517981501774	199242
2fcea7b0c17c51172cdd3f6afef7f6ce65638ab4	curvature estimation scheme for triangle meshes using biquadratic bézier patches	piecewise linear;tecnologia electronica telecomunicaciones;computacion informatica;feature recognition;surface fitting;shape analysis;grupo de excelencia;segmentation;difference scheme;ciencias basicas y experimentales;triangle meshes;tecnologias;triangle mesh;curvature estimation	When dealing with triangle meshes, it is often important to compute curvature information for the purposes of feature recognition, segmentation, or shape analysis. Since a triangle mesh is a piecewise linear surface, curvature has to be estimated. Several different schemes have been proposed, both discrete and continuous i.e. based on fitting surfaces locally. This paper compares commonly used discrete and continuous curvature estimation schemes. We also present a novel method which uses biquadratic Bézier patches as a local surface fitting technique.	bézier curve;coefficient;feature recognition;multivariate interpolation;piecewise linear continuation;quartic function;shape analysis (digital geometry);smoothing;triangle mesh;triangulation (geometry)	Anshuman Razdan;MyungSoo Bae	2005	Computer-Aided Design	10.1016/j.cad.2005.03.003	feature recognition;combinatorics;bézier triangle;topology;piecewise linear function;computer science;engineering;triangle mesh;shape analysis;mathematics;geometry;segmentation;mechanical engineering	Vision	69.17524734009281	-40.88125040794194	199345
efbd41e590bfa23c043e30a36986392c62b61f64	active contours using a constraint-based implicit representation	libraries;topology;implicit active contours;active contour;image segmentation;medical images;high performance computing;on curve constraint;active contours topology image segmentation biomedical imaging computer science computer vision robustness high performance computing libraries level set;level set;computational geometry;medical images constraint based implicit representation parametric active contours implicit active contours level set methods contour representation on curve constraint off curve constraint radial basis functions synthetic images photographs;biomedical imaging;active contours;computational geometry active vision radial basis function networks;finite element;contour representation;computer vision;radial basis function networks;multiple objectives;photographs;medical image;off curve constraint;synthetic images;level set methods;robustness;parametric active contours;computer science;constraint based implicit representation;level set method;radial basis functions;active vision	We present a new constraint-based implicit active contour, which shares desirable properties of both parametric and implicit active contours. Like parametric approaches, their representation is compact and can be manipulated interactively. Like other implicit approaches, they can naturally adapt to nonsimple topologies. Unlike implicit approaches using level-set methods, representation of the contour does not require a dense mesh. Instead, it is based on specified on-curve and off-curve constraints, which are interpolated using radial basis functions. These constraints are evolved according to specified forces drawn from the relevant literature of both parametric and implicit approaches. This new type of active contour is demonstrated through synthetic images, photographs, and medical images with both simple and nonsimple topologies. For complex input, this approach produces results comparable to those of level set or parameterized finite-element active models, but with a compact analytic representation. As with other active contours they can also be used for tracking, especially for multiple objects that split or merge.		Bryan S. Morse;Weiming Liu;Terry S. Yoo;Kalpathi R. Subramanian	2005		10.1109/CVPR.2005.59	computer vision;mathematical optimization;radial basis function;active vision;computational geometry;computer science;level set;finite element method;active contour model;mathematics;geometry;image segmentation;level set method;robustness	NLP	68.34400853494135	-45.70431546128857	199613
a2fd2a2b7c054b035737f58779249c5793ef8da7	real-time subspace integration for example-based elastic material	i 3 7 computer graphics;types of simulation;animation;i 6 8 simulation and modeling;i 6 8 simulation and modeling types of simulation animation;three dimensional graphics and realism;i 3 7 computer graphics three dimensional graphics and realism animation;categories and subject descriptors according to acm ccs	Example-based material allows simulating complex material behaviors in an art-directed way. This paper presents a method for fast subspace integration for example-based elastic material, which is suitable for real-time simulation in computer graphics. At the core of the method is the formulation of a new potential using example-based Green strain tensors. By using this potential, the deformation can be attracted towards the example-based deformation feature space, the example weights can be explicitly obtained and the internal force can be decomposed into the conventional one and an additional one induced by the examples. The real-time subspace integration is then developed with subspace integration costs independent of geometric complexity, and both the reduced conventional internal force and additional one being cubic polynomials in reduced coordinates. Experiments demonstrate that our method can achieve real-time simulation while providing comparable quality with the prior art.	computer graphics;cubic function;eurographics;feature vector;internal market information system;john d. wiley;kirchhoff's theorem;linear equation;linear system;ph (complexity);polynomial;real-time clock;real-time computing;real-time transcription;simulation;system of linear equations;zhi-li zhang	Wenjing Zhang;Jianmin Zheng;Nadia Magnenat-Thalmann	2015	Comput. Graph. Forum	10.1111/cgf.12569	anime;computer vision;simulation;computer science;artificial intelligence;theoretical computer science;real-time computer graphics;geometry;computer graphics;algorithm;3d computer graphics;computer graphics (images)	Graphics	69.88396037299995	-46.97903648688278	199696
fc11995a3cb0286d01575f5bdaff3cf988d16350	boundary-conformed tool path generation based on global reparametrization		In this paper, a boundary-conformed tool path generation method for either compound or trimmed surfaces are proposed, based on two powerful reparametrization schemes- the discretized harmonic mapping and the convex combination mapping. By globally mapping a 3D surface onto a 2D unit square and then planning an iso-parametric curve in the 2D domain, the corresponding Cutter Contact (CC) curve on the original 3D surface is easily generated which conforms with the boundary of the surface. Based on this CC curve generation strategy, a CC curve expansion algorithm for covering the entire surface is designed which takes into account the machining accuracy requirement, i.e., The specified maximum cusp height. Tool paths generated in this way for compound or trimmed surfaces are boundary-conformed, smooth, and guarantee the required machining accuracy.	algorithm;cutter expansive classification;discretization	Pengcheng Hu;Lufeng Chen;Jiarui Wang;Kai Tang	2015	2015 14th International Conference on Computer-Aided Design and Computer Graphics (CAD/Graphics)	10.1109/CADGRAPHICS.2015.28	computer science;mathematical optimization;machining;harmonic;algorithm;discretization;unit square;convex combination	EDA	69.37260851849557	-41.58213532505961	199854
786d7d1d3716537324e12a7eb30f38967711ca88	structure analysis with 2d quadrilateral meshes generated by a label-driven subdivision	label driven subdivision;hexahedral element;structure analysis		subdivision surface	Bo Liu;Kenjiro T. Miura;Shin Usuki	2016	IJAT	10.20965/ijat.2016.p0187		Robotics	68.6677649440782	-43.33574178529511	199948
