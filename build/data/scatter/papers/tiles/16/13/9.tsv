id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
3623da852b741185dba3512cef7401e144f189a9	optimality conditions for problems over symmetric cones and a simple augmented lagrangian method		In this work we are interested in nonlinear symmetric cone problems (NSCPs), which contain as special cases nonlinear semidefinite programming, nonlinear second order cone programming and the classical nonlinear programming problems. We explore the possibility of reformulating NSCPs as common nonlinear programs (NLPs), with the aid of squared slack variables. Through this connection, we show how to obtain second order optimality conditions for NSCPs in an easy manner, thus bypassing a number of difficulties associated to the usual variational analytical approach. We then discuss several aspects of this connection. In particular, we show a “sharp” criterion for membership in a symmetric cone that also encodes rank information. Also, we discuss the possibility of importing convergence results from nonlinear programming to NSCPs, which we illustrate by discussing a simple augmented Lagrangian method for nonlinear symmetric cones. We show that, employing the slack variable approach, we can use the results for NLPs to prove convergence results, thus extending a special case (i.e., the case with strict complementarity) of an earlier result by Sun, Sun and Zhang for nonlinear semidefinite programs.	analytical engine;augmented lagrangian method;calculus of variations;complementarity theory;cone-rod dystrophies;convergence (action);nonlinear programming;nonlinear system;retinal cone;second-order cone programming;semidefinite programming;slack variable	Bruno F. Lourenço;Ellen H. Fukuda;Masao Fukushima	2018	Math. Oper. Res.	10.1287/moor.2017.0901	mathematical optimization;combinatorics;mathematical analysis;mathematics;algorithm	ML	73.5074005384205	21.907240469650006	65691
5848b079694cc2be29e4ec2c5d9c73803003bbdd	global convergence of a closed-loop regularized newton method for solving monotone inclusions in hilbert spaces		We analyze the global convergence properties of some variants of regularized continuous Newton methods for convex optimization and monotone inclusions in Hilbert spaces. The regularization term is of LevenbergMarquardt type and acts in an open-loop or closed-loop form. In the open-loop case the regularization term may be of bounded variation.	bounded variation;convex optimization;emoticon;hilbert space;levenberg–marquardt algorithm;local convergence;mathematical optimization;matrix regularization;monotone polygon;newton;newton's method;nonlinear system;program optimization;spaces;tensor operator	Hédy Attouch;Patrick Redont;Benar Fux Svaiter	2013	J. Optimization Theory and Applications	10.1007/s10957-012-0222-3	mathematical optimization;mathematical analysis;calculus;mathematics	ML	72.72663375604934	21.080607876716495	65862
c375158a719ba0bd8c5c7027cba4064779d5837b	approximation of common fixed points of two quasi-nonexpansive multi-valued maps in banach spaces	strong convergence;quasi nonexpansive multi valued map;banach space;common fixed point;uniformly convex banach space;article	In this paper, we introduce and study two new iterative procedures with errors for two quasi-nonexpansive multi-valued maps in Banach spaces. Strong convergence theorems of the proposed iterations to a common fixed point of two quasi-nonexpansive multi-valued maps in uniformly convex Banach spaces are established.	approximation;fixed point (mathematics);map	Watcharaporn Cholamjiak;Suthep Suantai	2011	Computers & Mathematics with Applications	10.1016/j.camwa.2010.12.042	functional analysis;eberlein–šmulian theorem;mathematical optimization;banach manifold;mathematical analysis;topology;finite-rank operator;uniformly convex space;modes of convergence;reflexive space;infinite-dimensional vector function;interpolation space;mathematics;unconditional convergence;approximation property;banach space;lp space;c0-semigroup;algebra	Theory	73.93662740881418	18.958938705843895	66063
431537ba65040454eae29146c93c9eec65612219	an alternating trust region algorithm for distributed linearly constrained nonlinear programs, application to the optimal power flow problem	nonconvex optimisation;trust region methods;distributed optimisation;coordinate gradient descent	A novel trust region method for solving linearly constrained nonlinear programs is presented. The proposed technique is amenable to a distributed implementation, as its salient ingredient is an alternating projected gradient sweep in place of the Cauchy point computation. It is proven that the algorithm yields a sequence that globally converges to a critical point. As a result of some changes to the standard trust region method, namely a proximal regularisation of the trust region subproblem, it is shown that the local convergence rate is linear with an arbitrarily small ratio. Thus, convergence is locally almost superlinear, under standard regularity assumptions. The proposed method is successfully applied to compute local solutions to alternating current optimal power flow problems in transmission and distribution networks.Moreover, the new mechanism for computing a Cauchy point compares favourably against the standard projected search, as for its activity detection properties.	algorithm;computation;critical point (network science);distributed algorithm;gradient;local convergence;newton's method;nonlinear programming;nonlinear system;rate of convergence;trust region	Jean-Hubert Hours;Colin Neil Jones	2017	J. Optimization Theory and Applications	10.1007/s10957-015-0853-2	mathematical optimization;combinatorics;discrete mathematics;mathematics;trust region	ML	75.29080582268344	25.311288074195875	66110
4191516cceeab0a84ec54316beff3e179b25c8d4	multiobjective fractional programming involving generalized semilocally v-type i-preinvex and related functions		We study a nonlinear multiple objective fractional programming with inequality constraints where each component of functions occurring in the problem is considered semidifferentiable along its own direction instead of the same direction. New Fritz John type necessary and Karush-Kuhn-Tucker type necessary and sufficient efficiency conditions are obtained for a feasible point to be weakly efficient or efficient. Furthermore, a general Mond-Weir dual is formulated and weak and strong duality results are proved using concepts of generalized semilocally V-type I-preinvex functions. This contribution extends earlier results of Preda (2003), Mishra et al. (2005), Niculescu (2007), and Mishra and Rautela (2009), and generalizes results obtained in the literature on this topic.		Hachem Slimani;Shashi Kant Mishra	2014	Int. J. Math. Mathematical Sciences	10.1155/2014/496149	mathematical optimization;mathematical analysis;calculus;mathematics	AI	71.780784493885	22.18761130036137	66204
a65dcf57126d9cfbb8afe686bf995898a12206a0	an affine scaling projective reduced hessian algorithm for minimum optimization with nonlinear equality and linear inequality constraints	second order;optimisation sous contrainte;metodo cuadrado menor;constrained optimization;methode moindre carre;nonmonotonic technique;line search;matematicas aplicadas;least squares method;mathematiques appliquees;constrenimiento igualdad;interior point;relacion convergencia;merit function;contrainte inegalite;inequality constraint;search strategy;optimization method;methode point interieur;taux convergence;global convergence;local convergence;convergence rate;metodo optimizacion;reduced projective;algorithme;optimizacion con restriccion;acceleration convergence;equality constraint;algorithm;descomposicion matricial;trust region;metodo punto interior;trust region method;decomposition matricielle;funcion penalidad;mathematical programming;constrenimiento desigualdad;matrix decomposition;least square;numero de condicionamiento;algebra lineal numerica;strategie recherche;algebre lineaire numerique;backtracking;methode optimisation;aceleracion convergencia;condition number;echelle affine;backtracking step;convergence locale;affine scaling;numerical linear algebra;qr decomposition;fonction penalite;applied mathematics;interior point method;programmation mathematique;programacion matematica;indice conditionnement;contrainte egalite;penalty function;estrategia investigacion;algoritmo;convergence acceleration	Abstract   In this paper we propose a nonmonotonic interior point backtracking strategy to modify the reduced projective affine scaling trust region algorithm for solving minimum optimization subject to both nonlinear equality and linear inequality constraints. The general full trust region subproblem for solving the minimum optimization is decomposed to a pair of trust region subproblems in horizontal and vertical subspaces of linearize equality constraints and extended affine scaling equality constraints by QR decomposition of an affine scaling matrix and an orthonormal basis on the null subspace. The horizontal subproblem in the proposed algorithm is defined by minimizing a quadratic projective reduced Hessian function subject only to an ellipsoidal trust region constraint, while the vertical subproblem is also defined by the least squares subproblem subject only to an ellipsoidal trust region constraint. Combining trust region strategy with line search technique will switch to strictly feasible interior point step generated by a component direction of the two trust region subproblems. By adopting the  l  1  penalty function as the merit function, the global convergence and fast local convergence rate of the proposed algorithm are established under some reasonable conditions. The second-order correction step and a nonmonotonic criterion are used to overcome Maratos effect and speed up the convergence progress in some ill-conditioned cases, respectively.	affine scaling;algorithm;hessian;image scaling;linear inequality;linear programming;mathematical optimization;nonlinear system;social inequality	Detong Zhu	2005	Applied Mathematics and Computation	10.1016/j.amc.2004.04.056	mathematical optimization;constrained optimization;mathematical analysis;interior point method;calculus;mathematics;geometry;trust region;least squares	EDA	75.67278035166653	22.56904843299655	66248
02c1208de9e9325f83f0751787f5bd8347fc1010	the effects of loss of orthogonality on large scale numerical computations		Many useful large sparse matrix algorithms are based on orthogonality, but for efficiency this orthogonality is often obtained via short term recurrences. This can lead to both loss of orthogonality and loss of linear independence of computed vectors, yet with well designed algorithms high accuracy can still be obtained. Here we discuss a nice theoretical indicator of loss of orthogonality and linear independence and show how it leads to a related higher dimensional orthogonality that can be used to analyze and prove the effectiveness of such algorithms. We illustrate advantages and shortcomings of such algorithms with Cornelius Lanczos’ Hermitian matrix tridiagonalization process. The paper is reasonably expository, keeping simple by avoiding some detailed analyses.	computation	Christopher C. Paige	2018		10.1007/978-3-319-95168-3_29	mathematical optimization;linear independence;lanczos resampling;hermitian matrix;sparse matrix;computer science;nice;computation;orthogonality	HPC	82.69258396827713	24.194021863978648	66259
8d67c134b466eb3a8afa05ddf56d6297d561029d	bundle-level type methods uniformly optimal for smooth and nonsmooth convex optimization	convex programming;complexity;optimal methods;62l20;90c25;90c15;68q25;bundle level	The main goal of this paper is to develop uniformly optimal first-order methods for convex programming (CP). By uniform optimality we mean that the first-order methods themselves do not require the input of any problem parameters, but can still achieve the best possible iteration complexity bounds. By incorporating a multi-step acceleration scheme into the well-known bundle-level method, we develop an accelerated bundle-level (ABL) method, and show that it can achieve the optimal complexity for solving a general class of black-box CP problems without requiring the input of any smoothness information, such as, whether the problem is smooth, nonsmooth or weakly smooth, as well as the specific values of Lipschitz constant and smoothness level. We then develop a more practical, restricted memory version of this method, namely the accelerated prox-level (APL) method. We investigate the generalization of the APL method for solving certain composite CP problems and an important class of saddle-point problems recently studied by Nesterov [Mathematical Programming, 103 (2005), pp 127-152]. We present promising numerical results for these new bundle-level methods applied to solve certain classes of semidefinite programming (SDP) and stochastic programming (SP) problems.	apl;black box;convex optimization;first-order predicate;first-order reduction;iteration;mathematical optimization;numerical analysis;openedge advanced business language (abl);semidefinite programming;stochastic programming;whole earth 'lectronic link	Guanghui Lan	2015	Math. Program.	10.1007/s10107-013-0737-x	mathematical optimization;combinatorics;discrete mathematics;complexity;convex optimization;random coordinate descent;mathematics	Theory	73.40638416967798	24.406694775492415	66521
5a73281c6fc20605d90a6328f77f8f7c63c10da7	snopt: an sqp algorithm for large-scale constrained optimization	constrained optimization;49j20;quadratic program;nonlinear inequality constraints;nonlinear programming;65f05;degree of freedom;merit function;inequality constraint;90c30;49d37;sequential quadratic programming;large scale;65k05;limited memory methods;quasi newton methods;quasi newton method;49j15;constrained optimization problem;augmented lagrangian;test collection;large scale optimization;49m37	Sequential quadratic programming (SQP) methods have proved highly effective for solving constrained optimization problems with smooth nonlinear functions in the objective and constraints. Here we consider problems with general inequality constraints (linear and nonlinear). We assume that first derivatives are available and that the constraint gradients are sparse. We discuss an SQP algorithm that uses a smooth augmented Lagrangian merit function and makes explicit provision for infeasibility in the original problem and the QP subproblems. SNOPT is a particular implementation that makes use of a semidefinite QP solver. It is based on a limited-memory quasi-Newton approximation to the Hessian of the Lagrangian and uses a reduced-Hessian algorithm (SQOPT) for solving the QP subproblems. It is designed for problems with many thousands of constraints and variables but a moderate number of degrees of freedom (say, up to 2000). An important application is to trajectory optimization in the aerospace industry. Numerical results are given for most problems in the CUTE and COPS test collections (about 900 examples).	algorithm;approximation;augmented lagrangian method;constrained optimization;constraint programming;gradient;hessian;mathematical optimization;newton;nonlinear system;numerical method;snopt;sequential quadratic programming;social inequality;solver;sparse matrix;trajectory optimization	Philip E. Gill;Walter Murray;Michael A. Saunders	2002	SIAM Journal on Optimization	10.1137/S1052623499350013	mathematical optimization;constrained optimization;combinatorics;discrete mathematics;quasi-newton method;augmented lagrangian method;nonlinear programming;mathematics;sequential quadratic programming;degrees of freedom	ML	77.30068668083186	24.612216586791767	66707
cd3eba11b0ba8dcca0ab6f297ebdf6b541135436	an extension of yuan's lemma and its applications in optimization	quadratic forms;second-order optimality conditions;global convergence;90c30;90c46	We prove an extension of Yuan’s Lemma to more than two matrices, as long as the set of matrices has rank at most 2. This is used to generalize the main result of [A. Baccari and A. Trad. On the classical necessary second-order optimality conditions in the presence of equality and inequality constraints. SIAM J. Opt., 15(2):394–408, 2005], where the classical necessary second-order optimality condition is proved under the assumption that the set of Lagrange multipliers is a bounded line segment. We prove the result under the more general assumption that the Hessian of the Lagrangian evaluated at the vertices of the Lagrange multiplier set is a matrix set with at most rank 2. We apply the results to prove the classical second-order optimality condition to problems with quadratic constraints and without constant rank of the Jacobian matrix.	algorithm;hessian;jacobian matrix and determinant;lagrange multiplier;local convergence;mathematical optimization;social inequality	Gabriel Haeser	2017	J. Optimization Theory and Applications	10.1007/s10957-017-1123-2		Theory	73.90062046601705	22.16993476208123	66805
883840741821244a260adb5bca782ad4a457328c	d-orientation sequences for continuous functions and nonlinear complementarity problems	continuous function;condition kuhn tucker;d orientation sequence;teorema existencia;condicion kuhn tucker;convex programming;existence theorem;fonction continue;programmation convexe;orientation;condition suffisante;suite d orientation fonction;condicion suficiente;slater qualification;d orientation sequence of a function;p order generalized coercivity;orientacion;complementarity problems;probleme complementarite;problema complementariedad;qualification slater;complementarity problem;sufficient condition;karamardian condition;condition karamardian;theoreme existence;kuhn tucker condition;p mapping;nonlinear complementarity problem;programacion convexa	We introduce the new concept of d-orientation sequence for continuous functions. It is shown that if there does not exist a d-orientation sequence for a continuous function, then the corresponding complementarity problem (CP) has a solution. We believe that such a result characterizes an intrinsic property of CPs. As the concept of ``exceptional family of elements'', the notion of ``d-orientation sequence of a function'' is also a powerful tool for investigating the existence theorems of CPs. We use this new tool to establish, among other things, a new existence result for a class of P -mapping CPs. Ó 1999 Published by Elsevier Science Inc. All rights reserved.	complementarity theory;mixed complementarity problem;nonlinear system	Yun-Bin Zhao	1999	Applied Mathematics and Computation	10.1016/S0096-3003(98)10125-X	continuous function;mathematical optimization;mathematical analysis;convex optimization;calculus;mathematics;orientation	Logic	71.85854303512508	19.299844934941156	66863
968cad1c4e89739ffb180670d2fba30f1e888af6	stabilized column generation for highly degenerate multiple-depot vehicle scheduling problems	depot multiple;generation colonne;systeme degenere;degeneracy;generacion columna;multiple depot;methode echelle multiple;deposito multiple;stabilization;metodo escala multiple;multiple depot vehicle scheduling problem;degenerate system;linear relaxation;large scale;sistema degenerado;programacion lineal;relajacion lineal;estabilizacion;degeneration;scheduling;linear programming relaxation;linear programming;programmation lineaire;scheduling problem;linear program;relaxation lineaire;multiscale method;stabilisation;escala grande;ordonnancement;column generation;reglamento;echelle grande	Column generation has proven to be efficient in solving the linear programming relaxation of large scale instances of the multiple-depot vehicle scheduling problem (MDVSP). However difficulties arise when the instances are highly degenerate. Recent research has been devoted to accelerate column generation while remaining within the linear programming framework. This paper presents an efficient approach to solve the linear relaxation of the MDVSP. It combines column generation, preprocessing variable fixing, and stabilization. The outcome shows the great potential of such an approach for degenerate instances. 2005 Published by Elsevier Ltd.	column generation;linear programming relaxation;preprocessor;scheduling (computing)	Amar Oukil;Hatem Ben Amor;Jacques Desrosiers;Hicham El Gueddari	2007	Computers & OR	10.1016/j.cor.2005.05.011	column generation;mathematical optimization;computer science;linear programming;linear programming relaxation;calculus;mathematics;scheduling;algorithm;degeneracy	AI	69.66840386583202	26.117509541539057	66910
6a20e4b17945339b2eaaf343f1690fcdd2e29ae5	a hybrid approach combining chebyshev filter and conjugate gradient for solving linear systems with multiple right-hand sides	distribution;sistema lineal;calcul matriciel;linear algebra;linear systems;preconditionnement;iterative method;systeme positif symetrique;filtering;sistema multiple;filtrage;analisis numerico;convergence;47a10;standards;65f05;polinomio chebychev;fonction repartition;numerical method;62m20;ley gran numero;matrice coefficient;performance;filtrado;loi grand nombre;resolution math;proceedings;multiple system;preconditioning;conjugate gradient method;matriz simetrica;chebyshev polynomial;linear system;eigenvector;conjugate gradient algorithm;ponencia congreso;analyse numerique;symmetric matrix;metodo iterativo;eigenvalue;algorithme;polynome tchebychev;vector propio;law of large numbers;iterative methods;algorithm;target;funcion distribucion;hybrid approach;convergencia;distribution function;conjugate gradient;14c20;matrice creuse;numerical analysis;multiple right hand sides;matrice definie positive;metodo numerico;positive definite matrix;filter;metodo gradiente conjugado;methode iterative;algebre lineaire;valor propio;65f08;norma;chebyshev filtering polynomials;blanco;filtre;resolucion matematica;algebra lineal;cible;matrice symetrique;polynomial preconditioning;precondicionamiento;valeur propre;60g35;symmetric positive definite matrices;matrix calculus;sparse matrix;rendimiento;methode gradient conjugue;65f50;systeme lineaire;matriz definida positiva;actes congres;65h10;distribucion;60e05;solving;65f15;filtro;norme;65h17;65f10;vecteur propre;methode numerique;calculo de matrices;matriz dispersa;algoritmo;systeme multiple;65n22	One of the most powerful iterative schemes for solving symmetric, positive definite linear systems is the conjugate gradient algorithm of Hestenes and Stiefel [J. Res. Nat. Bur. Standards, 49 (1952), pp. 409–435], especially when it is combined with preconditioning (cf. [P. Concus, G.H. Golub, and D.P. O’Leary, in Proceedings of the Symposium on Sparse Matrix Computations, Argonne National Laboratory, 1975, Academic, New York, 1976]). In many applications, the solution of a sequence of equations with the same coefficient matrix is required. We propose an approach based on a combination of the conjugate gradient method with Chebyshev filtering polynomials, applied only to a part of the spectrum of the coefficient matrix, as preconditioners that target some specific convergence properties of the conjugate gradient method. We show that our preconditioner puts a large number of eigenvalues near one and do not degrade the distribution of the smallest ones. This procedure enables us to construct a lower dimensional Krylov basis that is very rich with respect to the smallest eigenvalues and associated eigenvectors. A major benefit of our method is that this information can then be exploited in a straightforward way to solve sequences of systems with little extra work. We illustrate the performance of our method through numerical experiments on a set of linear systems.	algorithm;amortized analysis;anisotropic diffusion;chebyshev filter;chebyshev polynomials;coefficient;command & conquer:yuri's revenge;computation;conjugate gradient method;discretization;distributed memory;dual ec drbg;experiment;finite element method;intrinsic dimension;iteration;iterative method;krylov subspace;linear system;matlab;nl (complexity);network address translation;nonlinear system;numerical analysis;numerical aperture;parallel computing;polynomial;precomputation;preconditioner;sparse matrix;ut-vpn	Gene H. Golub;Daniel Ruiz;Ahmed Touhami	2007	SIAM J. Matrix Analysis Applications	10.1137/060649458	combinatorics;conjugate residual method;numerical analysis;eigenvalues and eigenvectors;linear algebra;calculus;derivation of the conjugate gradient method;mathematics;geometry;iterative method;conjugate gradient method;nonlinear conjugate gradient method;linear system;biconjugate gradient method;algebra	HPC	82.7091828875361	20.991697605776107	66926
7c3076f8341d4a9294600ac18b2bed265bb6d9dc	a cyclic block coordinate descent method with generalized gradient projections	constrained optimization;nonconvex optimization;90c30;alternating algorithms;65k05;gradient projection methods	The aim of this paper is to present the convergence analysis of a very general class of gradient projection methods for smooth, constrained, possibly nonconvex, optimization. The key features of these methods are the Armijo linesearch along a suitable descent direction and the non Euclidean metric employed to compute the gradient projection. We develop a very general framework from the point of view of block-coordinate descent methods, which are useful when the constraints are separable. In our numerical experiments we consider a large scale image restoration problem to illustrate the impact of the metric choice on the practical performances of the corresponding algorithm. © 2016 Elsevier Inc. All rights reserved.	algorithm;bregman divergence;bures metric;circuit restoration;coordinate descent;descent direction;euclidean distance;experiment;gradient;gradient descent;image restoration;iterative reconstruction;line search;mathematical optimization;numerical analysis;performance;proximal gradient methods for learning;stationary process	Silvia Bonettini;Marco Prato;Simone Rebegoldi	2016	Applied Mathematics and Computation	10.1016/j.amc.2016.04.031	gradient descent;mathematical optimization;constrained optimization;combinatorics;gradient method;backpropagation;neighbourhood components analysis;descent direction;mathematics;stochastic gradient descent;geometry;proximal gradient methods;nonlinear conjugate gradient method	ML	74.43273366330281	24.326948891546905	67143
e317022363c819fcf9667ff2eb685fad1f493b93	breakdown of equivalence between the minimal l1-norm solution and the sparsest solution	sistema lineal;traitement signal;matrice aleatoire;empirical study;random signs matrix ensemble;partial fourier matrix ensemble;random matrix;linear system;underdetermined systems of linear equations;hadamard matrix;programacion lineal;fourier transformation;breakdown point;signal processing;partial hadamard matrix ensemble;transformation fourier;linear programming;representacion parsimoniosa;programmation lineaire;linear program;upper and lower bounds;basis pursuit;overcomplete systems;linear equations;systeme lineaire;random matrix theory;sparse representation;matriz aleatoria;procesamiento senal;matriz hadamard;sparse representations;matrice hadamard;transformacion fourier;representation parcimonieuse	Finding the sparsest solution to a set of underdetermined linear equations is NP-hard in general. However, recent research has shown that for certain systems of linear equations, the sparsest solution (i.e. the solution with the smallest number of nonzeros), is also the solution with minimal ‘ norm, and so can be found by a computationally tractable method. For a given n by m matrix F defining a system y 1⁄4 Fa, with nom making the system underdetermined, this phenomenon holds whenever there exists a ‘sufficiently sparse’ solution a0. We quantify the ‘sufficient sparsity’ condition, defining an equivalence breakdown point (EBP): the degree of sparsity of a required to guarantee equivalence to hold; this threshold depends on the matrix F. In this paper we study the size of the EBP for ‘typical’ matrices with unit norm columns (the uniform spherical ensemble (USE)); Donoho showed that for such matrices F, the EBP is at least proportional to n. We distinguish three notions of breakdown point—global, local, and individual—and describe a semi-empirical heuristic for predicting the local EBP at this ensemble. Our heuristic identifies a configuration which can cause breakdown, and predicts the level of sparsity required to avoid that situation. In experiments, our heuristic provides upper and lower bounds bracketing the EBP for ‘typical’ matrices in the USE. For instance, for an n m matrix Fn;m with m 1⁄4 2n, our heuristic predicts breakdown of local equivalence when the coefficient vector a has about 30% nonzeros (relative to the reduced dimension n). This figure reliably describes the observed empirical behavior. A rough approximation to the observed breakdown point is provided by the simple formula 0:44 n= logð2m=nÞ. There are many matrix ensembles of interest outside the USE; our heuristic may be useful in speeding up empirical studies of breakdown point at such ensembles. Rather than solving numerous linear programming problems per n;m combination, at least several for each degree of sparsity, the heuristic suggests to conduct a few experiments to measure the driving term of the heuristic and derive predictive bounds. We tested the applicability of this heuristic to three special ensembles of matrices, including the partial Hadamard ensemble and the partial Fourier ensemble, and found e front matter r 2005 Elsevier B.V. All rights reserved. pro.2005.05.028 ng author. Tel.: +1650 723 3350; fax: +1 650 725 8977. ss: donoho@stat.stanford.edu (D.L. Donoho).	approximation;cobham's thesis;coefficient;column (database);experiment;fax;hadamard transform;heuristic;like button;linear equation;linear programming;np-hardness;semiconductor industry;sparse matrix;system of linear equations;taxicab geometry;the matrix;turing completeness;x86	Yaakov Tsaig;David L. Donoho	2006	Signal Processing	10.1016/j.sigpro.2005.05.028	mathematical optimization;combinatorics;discrete mathematics;linear programming;random matrix;machine learning;mathematics;algorithm;statistics	AI	80.09463023536141	27.98803982331403	67223
4817fb0ae41adb27088d5137b1a3818e49c98570	explicit univariate global optimization with piecewise linear support functions		Piecewise linear convex and concave support functions combined with Pijavskii’s method are proposed to be used for solving global optimization problems. Rules for constructing support functions are introduced.	concave function;global optimization;linear programming;mathematical optimization	Oleg Khamisov	2016			discrete mathematics;global optimization;mathematical optimization;piecewise linear function;mathematics;univariate	EDA	71.63077989259924	23.42282921626638	67621
795c8627d5ed79f4fc70de1b2cfa86baf28688ef	on computing with the hilbert spline transform	hilbert spline transform;41a15;divided difference representation;65d15;hilbert transform;fast algorithm;b splines;44a15	We develop a fast algorithm for computing the Hilbert transform of a function from a data set consisting of n function values and prove that the complexity of the proposed algorithm is O(n log n). Our point of view is fundamentally based on a B-spline series approximation constructed from available data. In this regard, we obtain new formulas for the Hilbert transform of a B-spline as a divided difference. For theoretical simplicity and computational efficiency we give a detailed description of our algorithm, as well as provided optimal approximation order, only in the case of quadratic splines. However, if higher accuracy is required, extensions of our method to Communicated by Zhongying Chen. C. A. Micchelli Department of Mathematics, City University of Hong Kong, Tat Chee Avenue, Kowloon, Hong Kong, China C. A. Micchelli Department of Mathematics and Statistics, State University of New York, The University at Albany, Albany, NY 12222, USA Y. Xu Department of Mathematics, Syracuse University, Syracuse, NY 13244, USA Y. Xu Guangdong Province Key Lab of Computational Science, Sun Yat-sen University, Guangzhou 510275, People’s Republic of China B. Yu (B) Department of Mathematics, College of Science, China Three Gorges University, Yichang 443002, China e-mail: yubo2003@amss.ac.cn 624 C.A. Micchelli et al. spline approximation of any prescribed degree readily follows the pattern of the quadratic case. Numerical experiments have confirmed that our algorithm has superior performance than previously available methods which we briefly survey in Section 2.	algorithm;b-spline;computation;computational complexity theory;computational science;divided differences;email;entity–relationship model;experiment;hilbert transform;hsinchun chen;numerical analysis;numerical method;order of approximation;quadratic function;spline (mathematics)	Charles A. Micchelli;Yuesheng Xu;Bo Yu	2013	Adv. Comput. Math.	10.1007/s10444-011-9252-x	b-spline;mathematical optimization;mathematical analysis;discrete mathematics;hilbert transform;hilbert r-tree;hilbert–huang transform;mathematics;geometry	ML	77.80322939980086	20.72881755781004	67857
97c4eac42e212023bf6efb032c15570532fac780	accuracy of suboptimal solutions to kernel principal component analysis	optimal solution;kernel principal component analysis;kernel methods;kernel function;dual problem;regularized optimization problems;upper bound;optimization problem;qa75 electronic computers computer science;principal component analysis pca;reproducing kernel hilbert space;principal component analysis;suboptimal solutions;primal and dual problems;kernel method;lagrangian	For Principal Component Analysis in Reproducing Kernel Hilbert Spaces (KPCA), optimization over sets containing only linear combinations of all n-tuples of kernel functions is investigated, where n is a positive integer smaller than the number of data. Upper bounds on the accuracy in approximating the optimal solution, achievable without restrictions on the number of kernel functions, are derived. The rates of decrease of the upper bounds for increasing number n of kernel functions are given by the summation of two terms, one proportional to n−1/2 and the other to n−1, and depend on the maximum eigenvalue of the Gram matrix of the kernel with respect to the data. Primal and dual formulations of KPCA are considered. The estimates provide insights into the effectiveness of sparse KPCA techniques, aimed at reducing the computational costs of expansions in terms of kernel units.		Giorgio Gnecco;Marcello Sanguineti	2009	Comp. Opt. and Appl.	10.1007/s10589-007-9108-y	kernel;principal component regression;kernel method;mathematical optimization;combinatorics;mathematical analysis;kernel embedding of distributions;radial basis function kernel;kernel principal component analysis;reproducing kernel hilbert space;mathematics;variable kernel density estimation;polynomial kernel;kernel smoother	ML	74.65922212227376	26.50565514653851	68013
f914d3987d918a17ad58744b900bdf15313fd291	common fixed points of an infinite family of nonexpansive mappings in uniformly convex metric spaces	convex metric spaces	Abstract   We generalize the notion of   K  -mapping due to Kangtunyakarn and Suantai (2009)  [1]  for an infinite family of nonexpansive mappings on a convex metric space and study some of its useful properties. We also establish strong convergence of a Mann type iterative sequence to a common fixed point of an infinite family of nonexpansive mappings on a uniformly convex metric space.	fixed point (mathematics);uniformly convex space	Withun Phuengrattana;Suthep Suantai	2013	Mathematical and Computer Modelling	10.1016/j.mcm.2012.01.008	convex metric space;mathematical optimization;mathematical analysis;topology;mathematics;metric map	Theory	72.49206189512597	19.385769512525734	68028
0cf560ceb30421cd26030031b49b7b36526f9923	using generalized cayley transformations within an inexact rational krylov sequence method	linear systems;generalized eigenvalue problem;general and miscellaneous mathematics computing and information science;eigenvalue problems;transformations;grupo de excelencia;sparse;spectral transformation;eigenvalues;linear system;arnoldi method;65g05;iterative methods;algorithm;davidson method;matrices;cayley transform;ciencias basicas y experimentales;rightmost eigenvalues;matematicas;rational krylov sequence;cayley transformation;iteration;tecnologias generalidades;matrix pencil;65f50;tecnologias;iteration method;65n25;65f15;computation	The rational Krylov sequence (RKS) method is a generalization of Arnoldi's method. It constructs an orthogonal reduction of a matrix pencil into an upper Hessenberg pencil. The RKS method is useful when the matrix pencil may be eeciently factored. This article considers approximately solving the resulting linear systems with iterative methods. We show that a Cayley transformation leads to a more eecient and robust eigensolver than the usual shift-invert transformation when the linear systems are solved inexactly within the RKS method. A relationship with the recently introduced Jacobi{Davidson method is also established.	arnoldi iteration;eigenvalue algorithm;iterative method;karl hessenberg;krylov subspace;linear system;magma;the matrix	Richard B. Lehoucq;Karl Meerbergen	1998	SIAM J. Matrix Analysis Applications	10.1137/S0895479896311220	mathematical optimization;combinatorics;mathematics;iterative method;linear system;algebra	ML	81.48150744446762	21.964024088657702	68973
a27f1185836899c60a1c80f3e74edc5019f5361f	sub-exponential graph coloring algorithm for stencil-based jacobian computations	stencil discretization;grid coloring algorithm;lipton tarjan separator;65d25;05c50;90c27;divide and conquer approach;05c85;65f50;sparsity exploitation;derivative computation;05c15	Partial differential equations can be discretized using a regular Cartesian grid and a stencil-based method to approximate the partial derivatives. The computational effort for determining the associated Jacobian matrix can be reduced. This reduction can be modeled as a (grid) coloring problem. Currently, this problem is solved by using a heuristic approach for general graphs or by developing a formula for every single stencil. We introduce a sub-exponential algorithm using the Lipton–Tarjan separator in a divide-and-conquer approach to compute an optimal coloring. The practical relevance of the algorithm is evaluated when compared with an exponential algorithm and a greedy heuristic.	algorithm;computation;graph coloring;jacobian matrix and determinant;time complexity	Michael Lülfesmann;Ken-ichi Kawarabayashi	2014	J. Comput. Science	10.1016/j.jocs.2013.06.002	mathematical optimization;combinatorics;greedy algorithm;stencil code;theoretical computer science;mathematics;five-point stencil;greedy coloring	Theory	80.34909628361028	24.791429126650453	69269
ef023c4ba412839690466bd64096ce8fd4a931a0	google pageranking problem: the model and the analysis	equation non lineaire;15a18;pageranking;ecuacion no lineal;equation differentielle;analisis numerico;google matrix;principle of biorthogonality;matriz cuadrada;multistep method;ecuacion trascendente;computacion informatica;matematicas aplicadas;mathematiques appliquees;extrapolation formulae;loi probabilite;ley probabilidad;matrice reelle;complex matrix;canonical form;forme canonique;ecuacion algebraica;differential equation;extrapolation;equation transcendante;methode multipas;methode runge kutta;brauer s theorem;eigenvalues;metodo runge kutta;power method;eigenvector;matrice carree;analyse numerique;real matrix;ecuacion diferencial;eigenvalue;algorithme;square matrix;acceleration convergence;vector propio;metodo multipaso;algorithm;methode matricielle;matrice complexe;jordan canonical form;15a51;numerical analysis;65l06;30xx;ciencias basicas y experimentales;transcendental equation;fonction variable complexe;probability distribution;matematicas;valor propio;numero de condicionamiento;matrix method;algebra lineal numerica;algebre lineaire numerique;funcion variable compleja;aceleracion convergencia;condition number;65f35;surfing model;forma canonica;metodo matriz;15a12;equation algebrique;valeur propre;15a21;numerical linear algebra;40c05;65b05;extrapolacion;grupo a;non linear equation;applied mathematics;65y20;algebraic equation;65f15;stochastic matrices;65h17;65f10;indice conditionnement;vecteur propre;rank one perturbation;runge kutta method;complex variable function;matriz real;eigenvectors;matriz compleja;algoritmo;convergence acceleration	Let A be a given n-by-n complex matrix with eigenvalues λ, λ2, . . . , λn. Suppose there are nonzero vectors x, y ∈ Cn such that Ax = λx, y∗A = λy∗, and y∗x = 1. Let v ∈ Cn be such that v∗x = 1, let c ∈ C, and assume that λ 6= cλj for each j = 2, . . . , n. Define A(c) := cA+(1− c)λxv∗. The eigenvalues of A(c) are λ, cλ2, . . . , cλn. Every left eigenvector of A(c) corresponding to λ is a scalar multiple of y − z(c), in which the vector z(c) is an explicit rational function of c. If a standard form such as the Jordan canonical form or the Schur triangular form is known for A, we show how to obtain the corresponding standard form of A(c). The web hyper-link matrix G(c) used by Google for computing the PageRank is a special case in which A is real, nonnegative, and row stochastic (taking into consideration the dangling nodes), c ∈ (0, 1), x is the vector of all ones, and v is a positive probability vector. The PageRank vector (the normalized dominant left eigenvector of G(c)) is therefore an explicit rational function of c. Extrapolation procedures on the complex field may give a practical and efficient way to compute the PageRank vector when c is close to 1. A discussion on the model, on its adherence to reality, and on possible variations is also considered.	carrier-to-noise ratio;extrapolation;hyperlink;pagerank;triangular matrix;world wide web	Stefano Serra Capizzano	2007	J. Computational Applied Mathematics	10.1016/j.cam.2010.02.005	google matrix;eigenvalues and eigenvectors;calculus;mathematics;geometry;algebra	Theory	79.30588859817466	18.663207984230446	69501
d1cbe62bd41526757666274a575c95ae7a1d76b5	new accelerated conjugate gradient algorithms as a modification of dai-yuan's computational scheme for unconstrained optimization	preconditionnement;analisis numerico;49m07;90c06;line search;computacion informatica;matematicas aplicadas;algoritmo busqueda;mathematiques appliquees;algorithme recherche;search algorithm;optimization method;preconditioning;conjugate gradient method;satisfiability;metodo optimizacion;analyse numerique;numerical comparisons;condition suffisante;sufficient descent condition;conjugate gradient;65k05;numerical analysis;condicion suficiente;ciencias basicas y experimentales;metodo gradiente conjugado;matematicas;methode optimisation;unconstrained optimization;precondicionamiento;optimizacion sin restriccion;sufficient condition;methode gradient conjugue;grupo a;49m10;applied mathematics;optimisation sans contrainte;newton direction;conjugacy condition	New accelerated nonlinear conjugate gradient algorithms which are mainly modifications of the Dai and Yuan’s for unconstrained optimization are proposed. Using the exact line search, the algorithm reduces to the Dai and Yuan conjugate gradient computational scheme. For inexact line search the algorithm satisfies the sufficient descent condition. Since the step lengths in conjugate gradient algorithms may differ from 1 by two order of magnitude and tend to vary in a very unpredictable manner, the algorithms are equipped with an acceleration scheme able to improve the efficiency of the algorithms. Computational results for a set consisting of 750 unconstrained optimization test problems show that these new conjugate gradient algorithms substantially outperform the Dai-Yuan conjugate gradient algorithm and its hybrid variants, Hestenes-Stiefel, Polak-Ribière-Polyak, CONMIN conjugate gradient algorithms, limited quasi-Newton algorithm LBFGS and compare favourable with CG_DESCENT.	algorithm;computation;limited-memory bfgs;line search;local convergence;mathematical optimization;newton;nonlinear conjugate gradient method;nonlinear system;quasi-newton method	Neculai Andrei	2010	J. Computational Applied Mathematics	10.1016/j.cam.2010.05.002	gradient descent;mathematical optimization;conjugate residual method;gradient method;calculus;derivation of the conjugate gradient method;mathematics;geometry;conjugate gradient method;nonlinear conjugate gradient method;biconjugate gradient method;algebra	ML	77.11388752144735	23.298212385164067	69672
8400f88837518f509b541dabbb7adb8d474794c1	numerical range of a continuant matrix	matriz cuadrada;fraction continue;portee numerique;matrice carree;square matrix;numerical range;continued fraction;continued fractions	Abstract   Let  A  be an  n -by- n  complex matrix, the numerical range of  A  is the set   W(A) = {x     ∗    Ax : x ∈   C     n   , x     ∗   x = 1}  . The numerical range of a continuant matrix is explicitly described as an elliptic disc.	numerical linear algebra	Mao-Ting Chien;Jun-Ming Huang	2001	Appl. Math. Lett.	10.1016/S0893-9659(00)00138-5	continued fraction;calculus;mathematics;geometry;algebra	Theory	78.78829180512425	19.22895863143924	69683
76e8732e8aa2b2b53547ef72e5214f2a08773f4a	a new deflation method for verifying the isolated singular zeros of polynomial systems		In this paper, we develop a new deflation technique for refining or verifying the isolated singular zeros of polynomial systems. Starting from a polynomial system with an isolated singular zero, by computing the derivatives of the input polynomials directly or the linear combinations of the related polynomials, we construct a new system, which can be used to refine or verify the isolated singular zero of the input system. In order to preserve the accuracy in numerical computation as much as possible, new variables are introduced to represent the coefficients of the linear combinations of the related polynomials. To our knowledge, it is the first time that considering the deflation problem of polynomial systems from the perspective of the linear combination. Some acceleration strategies are proposed to reduce the scale of the final system. We also give some further analysis of the tolerances we use, which can help us have a better understanding of our method.The experiments show that our method is effective and efficient. Especially, it works well for zeros with high multiplicities of large systems. It also works for isolated singular zeros of non-polynomial systems.		Jin-San Cheng;Xiaojie Dou;Junyi Wen	2018	CoRR			Logic	80.07847280455195	19.550136496454726	69886
4abfa1fa0f56c53023586cbc5c36245366bfc94f	an explicit form of the inverse of a particular circulant matrix	circulant matrix	A particular binary circulant matrix is considered and an explicit form of its inverse is given. Such a matrix was originated in studying a particular discrete optimization problem.	circulant matrix	A. Cambini	1984	Discrete Mathematics	10.1016/0012-365X(84)90192-4	companion matrix;matrix function;hollow matrix;combinatorics;discrete mathematics;nonnegative matrix;single-entry matrix;matrix of ones;circulant matrix;square matrix;mathematics;logical matrix;state-transition matrix;block matrix;symmetric matrix;algebra;involutory matrix	ML	78.82724747992235	20.592732139785188	69928
7d85d54eb5dfa70f8a432f5663eb9bf088d9a6da	a full row-rank system matrix generated by the strip-based projection model in discrete tomography	sistema lineal;metodo directo;iterative method;analisis numerico;matematicas aplicadas;mathematiques appliquees;65f05;bepress selected works;discrete tomography;matrix inversion;inversion matriz;linear system;analyse numerique;metodo iterativo;reconstruction image;14c20;full row rank system;numerical analysis;reconstruccion imagen;methode iterative;image reconstruction;indexation;underdetermined linear system strip based projection model discrete tomography;tomographie;algebra lineal numerica;algebre lineaire numerique;inversion matrice;strip based projection;numerical linear algebra;computer tomography ct strip based projection model;systeme lineaire;tomografia;applied mathematics;methode directe;tomography;65f10;direct method;strip based projection full row rank system	Let  C  u  =  k  be an underdetermined linear system generated by the strip-based projection model in discrete tomography, where  C  is row-rank deficient. In the case of one scanning direction the linear dependency of the rows of  C  is studied in this paper. An index set  H  is specified such that if all rows of  C  with row indices in  H  are deleted then the rows of resultant matrix  F  are maximum linearly independent rows of  C   . Therefore, the corresponding system             F  u  =    k    ˜           is equivalent to  C  u  =  k  and consequently, the cost of an image reconstruction from             F  u  =    k    ˜           is reduced.	discrete tomography	Jiehua Zhu;Xiezhang Li	2010	Applied Mathematics and Computation	10.1016/j.amc.2010.04.073	iterative reconstruction;direct method;numerical analysis;calculus;mathematics;geometry;iterative method;tomography;numerical linear algebra;linear system;algorithm	Theory	82.51507901105256	20.995289578919344	70027
28a0ffc96509cab2335cf7c001ce5d9b899772ce	constrained global optimization by constraint partitioning and simulated annealing	constrained optimization;constrained global optimization;constrained simulated annealing;continuous constrained benchmarks global optimization constraint partitioning constraint partitioned simulated annealing constrained simulated annealing constrained optimization extended saddle points global constraints asymptotic convergence discrete space;constraint partitioned simulated annealing;discrete space;global constraint;simulated annealing;constraint optimization simulated annealing electrostatic precipitators neodymium partitioning algorithms stochastic processes convergence programming profession engineering profession;continuous constrained benchmarks;computational complexity;extended saddle points;constraint theory;global optimization;constraint partitioning;asymptotic convergence;simulated annealing computational complexity constraint theory;global constraints;saddle point	In this paper, we present constraint-partitioned simulated annealing (CPSA), an algorithm that extends our previous constrained simulated annealing (CSA) for constrained optimization. The algorithm is based on the theory of extended saddle points (ESPs). By decomposing the ESP condition into multiple necessary conditions, CPSA partitions a problem by its constraints into subproblems, solves each independently using CSA, and resolves those violated global constraints across the subproblems. Because each subproblem is exponentially simpler and the number of global constraints is very small, the complexity of solving the original problem is significantly reduced. We state without proof the asymptotic convergence of CPSA with probability one to a constrained global minimum in discrete space. Last, we evaluate CPSA on some continuous constrained benchmarks	algorithm;benchmark (computing);constrained optimization;global optimization;locality of reference;mathematical optimization;maxima and minima;nonlinear programming;nonlinear system;simulated annealing	Benjamin W. Wah;Yixin Chen;Andrew Wan	2006	2006 18th IEEE International Conference on Tools with Artificial Intelligence (ICTAI'06)	10.1109/ICTAI.2006.47	mathematical optimization;discrete space;constrained optimization;combinatorics;simulated annealing;machine learning;mathematics;saddle point;computational complexity theory;global optimization	Robotics	70.31383141047942	25.753595949427332	70095
5c76986f98aefc1ee1024b017f1734c529a5ea8d	"""""""optimal"""" choice of the step length of the projection and contraction methods for solving the split feasibility problem"""	split feasibility problem;cq method;projection and contraction method;modified projection and contraction method;inverse strongly monotone;47h05;47h07;47h10;54h25			Q. L. Dong;Y. C. Tang;Y. J. Cho;Themistocles M. Rassias	2018	J. Global Optimization	10.1007/s10898-018-0628-z		Theory	75.54830374418916	20.426139682528053	70336
7a4bf600f4948f3b9670869f05a2009dc2fb5d9b	spectral projected gradient method for stochastic optimization	spectral projected gradient;constrained stochastic problems;sample average approximation;variable sample size	We consider the Spectral Projected Gradient method for solving constrained optimization porblems with the objective function in the form of mathematical expectation. It is assumed that the feasible set is convex, closed and easy to project on. The objective function is approximated by a sequence of Sample Average Approximation functions with different sample sizes. The sample size update is based on two error estimates SAA error and approximate solution error. The Spectral Projected Gradient method combined with a nonmonotone line search is used. The almost sure convergence results are achieved without imposing explicit sample growth condition. Numerical results show the efficiency of the proposed method.	active set method;approximation algorithm;assumed;computation;constrained optimization;convergence (action);convex function;courant–friedrichs–lewy condition;estimated;feasible region;gradient method;heuristic (computer science);heuristics;ift122 gene;iteration;line search;loss function;mathematical optimization;mathematics;noise (electronics);numerical method;optimization problem;physical inactivity;point of view (computer hardware company);projections and predictions;real life;sample size;scheduling (computing);scheduling - hl7 publishing domain;serum amyloid a protein;solutions;stochastic optimization;tree accumulation;spirogermanium	Natasa Krejic;Natasa Krklec Jerinkic	2019	J. Global Optimization	10.1007/s10898-018-0682-6	gradient method;mathematical optimization;line search;mathematics;stochastic optimization;convergence of random variables;expected value;constrained optimization;sample size determination;feasible region	ML	74.04008117687835	24.949789570790127	70524
ac2bb31f14a1274fd56f52f926fe19439a16ade5	sequential linear-quadratic method for differential games with air combat applications	riccati equation;numerical method;system dynamics;saddle points;differential game;differential games;riccati equations;linear quadratic;air combats;nonlinear system;saddle point	We present a numerical method for computing a local Nash (saddle-point) solution to a zero-sum differential game for a nonlinear system. Given a solution estimate to the game, we define a subproblem, which is obtained from the original problem by linearizing its system dynamics around the solution estimate and expanding its payoff function to quadratic terms around the same solution estimate. We then apply the standard Riccati equation method to the linear-quadratic subproblem and compute its saddle solution. We then update the current solution estimate by adding the computed saddle solution of the subproblem multiplied by a small positive constant (a step size) to the current solution estimate for the original game. We repeat this process and successively generate better solution estimates. Our applications of this sequential method to air combat simulations demonstrate experimentally that the solution estimates converge to a local Nash (saddle) solution of the original game.		H. Mukai;A. Tanikawa;I. Tunay;Alpay Özcan;I. N. Katz;Heinz Schättler;P. Rinaldi;G. J. Wang;Lechang Yang;Y. Sawada	2003	Comp. Opt. and Appl.	10.1023/A:1022957123924	mathematical optimization;nonlinear system;control theory;mathematics;saddle point;mathematical economics	Crypto	76.53607649465516	26.55742722136759	70540
b7595e9d1f023b3c300ad7a2b8e6368eaa0144a5	a filter trust-region algorithm for unconstrained optimization with strong global convergence properties	second order;trust region algorithms;filter methods;global convergence;trust region;first order;unconstrained optimization;nonlinear optimization	We present a new filter trust-region approach for solving unconstrained nonlinear optimization problems making use of the filter technique introduced by Fletcher and Leyffer to generate non-monotone iterations. We also use the concept of a multidimensional filter used by Gould et al. (SIAM J. Optim. 15(1):17–38, 2004) and introduce a new filter criterion showing good properties. Moreover, we introduce a new technique for reducing the size of the filter. For the algorithm, we present two different convergence analyses. First, we show that at least one of the limit points of the sequence of the iterates is first-order critical. Second, we prove the stronger property that all the limit points are first-order critical for a modified version of our algorithm. We also show that, under suitable conditions, all the limit points are secondorder critical. Finally, we compare our algorithm with a natural trust-region algorithm and the filter trust-region algorithm of Gould et al. on the CUTEr unconstrained test problems Gould et al. in ACM Trans. Math. Softw. 29(4):373–394, 2003. Numerical results demonstrate the efficiency and robustness of our proposed algorithms.	algorithm;cuter;critical point (network science);first-order predicate;fletcher's checksum;iteration;local convergence;matlab;mathematical optimization;misiurewicz point;nonlinear programming;nonlinear system;numerical linear algebra;numerical method;trust region;monotone	Masoud Fatemi;Nezam Mahdavi-Amiri	2012	Comp. Opt. and Appl.	10.1007/s10589-011-9411-5	adaptive filter;mathematical optimization;combinatorics;kernel adaptive filter;nonlinear programming;first-order logic;mathematics;trust region;second-order logic;algorithm	ML	76.52655509387769	23.673510459388446	70589
d8d47426534330050903bfa450ca953e28e5be85	optimization of functions with many minima	optimisation;simulated annealing temperature optimization methods minimization methods energy states computational modeling data structures context modeling;mathematics computing;optimizacion;numerical method;optimum global;minimizacion funcion;simulated annealing;global optimum;recuit simule;metodo numerico;function minimization;simulated annealing mathematics computing numerical methods;natural language;quasi natural language input functions optimisation minima numerical method global minimum nonconvex functions simulated annealing continuously valued variables interopt;numerical methods;recocido simulado;optimization;optimisation continue;optimo global;methode numerique;minimisation fonction	A numerical method for finding the global minimum of nonconvex functions is presented. The method is based on the principles of simulated annealing, but handles continuously valued variables in a natural way. The method is completely general, and optimizes functions of up to 30 variables. Several examples are presented. A general-purpose program, INTEROPT, is described, which finds the minimum of arbitrary functions, with user-friendly, quasi-natural-language input. >	maxima and minima	Griff L. Bilbro;Wesley E. Snyder	1991	IEEE Trans. Systems, Man, and Cybernetics	10.1109/21.108301	mathematical optimization;numerical analysis;computer science;machine learning;mathematics;algorithm	EDA	76.62977826967307	25.707675513961608	70598
7d34ba2d8281c90af0829278678bb8ab8e63e393	approximate gcds of polynomials and sparse sos relaxations	greatest common divisor;semidefinite programming;global minimization;optimization problem;sum of squares;sums of squares;semideflnite programming;rational function	The problem of computing approximate GCDs of several polynomials with real or complex coefficients can be formulated as computing the minimal perturbation such that the perturbed polynomials have an exact GCD of given degree. We present algorithms based on SOS (Sums Of Squares) relaxations for solving the involved polynomial or rational function optimization problems with or without constraints.	approximation algorithm;coefficient;computation;experiment;factorization of polynomials;lagrangian relaxation;linear programming relaxation;mathematical optimization;maxima and minima;non-linear least squares;numerical linear algebra;optimization problem;polynomial;post–turing machine;quartic function;semidefinite programming;sockets direct protocol;sparse matrix;yang	Bin Li;Jiawang Nie;Lihong Zhi	2008	Theor. Comput. Sci.	10.1016/j.tcs.2008.09.003	optimization problem;rational function;mathematical optimization;combinatorics;discrete mathematics;mathematics;explained sum of squares;greatest common divisor;semidefinite programming	Theory	72.47630468779265	24.969279393377324	70738
dfe0bc99201196c1ce1d7a82aabf751a1e7b6a27	convergence analysis of modified newton-hss method for solving systems of nonlinear equations	convergence analysis;newton hss method;hermitian and skew hermitian splitting;positive definite jacobian matrices;nonlinear equations;large sparse systems	Hermitian and skew-Hermitian splitting(HSS) method has been proved quite successfully in solving large sparse non-Hermitian positive definite systems of linear equations. Recently, by making use of HSS method as inner iteration, Newton-HSS method for solving the systems of nonlinear equations with non-Hermitian positive definite Jacobian matrices has been proposed by Bai and Guo. It has shown that the Newton-HSS method outperforms the Newton-USOR and the Newton-GMRES iteration methods. In this paper, a class of modified Newton-HSS methods for solving large systems of nonlinear equations is discussed. In our method, the modified Newton method with R-order of convergence three at least is used to solve the nonlinear equations, and the HSS method is applied to approximately solve the Newton equations. For this class of inexact Newton methods, local and semilocal convergence theorems are proved under suitable conditions. Moreover, a globally convergent modified Newton-HSS method is introduced and a basic global convergence theorem is proved. Numerical results are given to confirm the effectiveness of our method.	algorithm;backtracking;central processing unit;generalized minimal residual method;high-speed serial interface;iteration;iterative method;jacobian matrix and determinant;linear equation;local convergence;newton;newton's method;nonlinear system;numerical method;rate of convergence;sparse matrix;system of linear equations	Qingbiao Wu;Minhong Chen	2012	Numerical Algorithms	10.1007/s11075-012-9684-5	local convergence;mathematical optimization;mathematical analysis;nonlinear system;calculus;control theory;mathematics	Robotics	80.57244638837186	22.296450033751096	70935
b0541d01e4b70c0b3106c4db794b3b4525deec51	a fixed point approach to certain convex programs with applications in stochastic programming	convex program;convex programming;fixed point;fixed point problem;stochastic programming;simplicial algorithm	This paper characterizes a certain convex program, which often arises in connection with stochastic programming as a fixed point problem, and explores the possibility of solving it by applying simplicial algorithms for finding fixed points of point-to-set mappings. We first establish equivalence between the convex program and the fixed point problem for some point-to-set mapping. As this mapping may have unfavorable properties from a computational viewpoint, we then modify it to reconstruct a point-to-set mapping and consider the application of a simplicial fixed point algorithm to the latter mapping. It is shown that the algorithm converges to a fixed point of the mapping and yields an optimal solution to the original convex program under certain conditions.	convex optimization;fixed point (mathematics);stochastic programming	Masao Fukushima	1983	Math. Oper. Res.	10.1287/moor.8.4.517	stochastic programming;convex analysis;subderivative;mathematical optimization;combinatorics;discrete mathematics;convex optimization;convex polytope;kakutani fixed-point theorem;schauder fixed point theorem;convex combination;second-order cone programming;linear matrix inequality;convex hull;mathematics;fixed point;convex set;fixed-point property;proper convex function;least fixed point	Theory	71.82689618218156	21.970648343558747	71047
aa7d2825b6c6a9891c44f3fe6ecbb89cdfacc133	nonsmooth semi-infinite programming problem using limiting subdifferentials	duality;sufficient optimality conditions;dual problem;semi infinite programming;nonsmooth optimization;generalized convexity;optimality condition	In this paper, we establish necessary and sufficient optimality conditions for nonsmooth semi-infinite programming problem using the powerful tool of limiting subdifferentials. We also formulate Wolfe and Mond-Weir type duals for nonsmooth semi-infinite programming problem and establish weak, strong and strict converse duality theorems for semi-infinite programming problem and the corresponding dual problems.	semi-infinite programming;semiconductor industry	Shashi Kant Mishra;Monika Jaiswal;Le Thi Hoai An	2012	J. Global Optimization	10.1007/s10898-011-9690-5	mathematical optimization;mathematical analysis;discrete mathematics;duality;mathematics;wolfe duality	PL	71.9579865772507	21.63035080890703	71115
4d98e168e4af9cf619b4df06cecea4e24b18675a	on computing certain elements of the inverse of a sparse matrix	elements of inverse;triangular factorization;tridiagonal matrix;sensitivities;recursive algorithm;sparse matrix;lu factorization;sparse matrices	A recursive algorithm for computing the inverse of a matrix from the LU factors based on relationships in Takahashi, et al., is examined. The formulas for the algorithm are given; the dependency relationships are derived; the computational costs are developed; and some general comments on application and stability are made.	algorithm;computation;recursion (computer science);sparse matrix;tomotaka takahashi	A. M. Erisman;W. F. Tinney	1975	Commun. ACM	10.1145/360680.360704	cuthill–mckee algorithm;mathematical optimization;hollow matrix;combinatorics;sparse matrix;incomplete lu factorization;lu decomposition;nonnegative matrix;single-entry matrix;band matrix;invertible matrix;mathematics;matrix decomposition;block matrix;algebra	AI	81.46720037343903	22.669474644461122	71291
b21aefbfb4bdc0ffe1e51c4c58a92c8bc73a34d0	conditional optimization problems: fractional order case		In this manuscript, we introduce a new formulation for the constrained optimization problems in which the objective function is considered in the fractional integral form. The constraints are applied in two separate cases, namely, fractional differential and fractional isoperimetric constraints. In both cases, by using the extended Euler–Lagrange equations and the Lagrange multiplier method, the necessary conditions are obtained. An example is given in order to illustrate the effectiveness of the reported results.		Abolhassan Razminia;Dumitru Baleanu;Vahid Johari Majd	2013	J. Optimization Theory and Applications	10.1007/s10957-012-0211-6	fractional programming;mathematical optimization;mathematical analysis;fractional calculus;calculus;mathematics	Theory	71.08499437602103	22.339427060619276	71509
0a33f0490dd8fe5cafffaa9e524e486221a88751	a multiscale framework for challenging discrete optimization		Current state-of-the-art discrete optimization methods struggle behind when it comes to challenging contrast-enhancing discrete energies (i.e., favoring different labels for neighboring variables). This work suggests a multiscale approach for these challenging problems. Deriving an algebraic representation allows us to coarsen any pair-wise energy using any interpolation in a principled algebraic manner. Furthermore, we propose an energy-aware interpolation operator that efficiently exposes the multiscale landscape of the energy yielding an effective coarse-to-fine optimization scheme. Results on challenging contrast-enhancing energies show significant improvement over state-of-the-art methods.	discrete optimization;interpolation;linear algebra;mathematical optimization;program optimization	Shai Bagon;Meirav Galun	2012	CoRR		mathematical optimization;combinatorics;discrete mathematics;mathematics	Vision	77.74576145223718	27.895333007021996	71950
370bec7452697abd2b6c8927e5c2e4e7c9637a59	some remarks on the paper “strong convergence of a self-adaptive method for the spilt feasibility problem”	strong convergence;split feasibility problem;self adaptive method;variational inequality;contraction	In this paper, we present several remarks on the paper by Yao et al. (citeyearcite.nine). The results presented in the present paper are interesting improvements on the main results of Yao et al.	yao graph	Haiyun Zhou;Peiyuan Wang	2014	Numerical Algorithms	10.1007/s11075-014-9949-2	mathematical optimization;variational inequality;calculus;contraction;mathematics;mathematical economics	NLP	73.60592359784059	21.91861425232744	71985
c7545f8ed207129b556c28f07dda53875958eea9	on a theorem due to crouzeix and ferland	second order;quasiconvex programming;90c26;nonlinear programming;kt pseudoconvex problems;inequality constraint;nonsmooth analysis;first order;26b25;fj pseudoconvex problems;nonsmooth optimization;generalized convexity;49j52	In this article we introduce the notions of Kuhn-Tucker and Fritz John pseudoconvex nonlinear programming problems with inequality constraints. We derive several properties of these problems. We prove that the problem with quasiconvex data is (second-order) Kuhn-Tucker pseudoconvex if and only if every (second-order) Kuhn-Tucker stationary point is a global minimizer. We obtain respective results for Fritz John pseudoconvex problems. For the first-order case we consider Frechet differentiable functions and locally Lipschitz ones, for the second-order case Frechet and twice directionally differentiable functions.		Vsevolod I. Ivanov	2010	J. Global Optimization	10.1007/s10898-009-9407-1	mathematical optimization;mathematical analysis;pseudoconvex function;nonlinear programming;first-order logic;mathematics;mathematical economics;second-order logic	EDA	72.18428645082712	21.403813352414996	72082
62fa810c0730a77572b6d4fcd15d000b06b9618c	on the numerical analysis of oblique projectors	contraste;linear algebra;erreur calcul;proyeccion;representation;65j05;orthogonality;analisis numerico;representation theory;espace orthogonal;computation error;mise a jour;comportement;perturbation theory;15b99;65jxx;xqry form;theorie representation;46c07;analyse numerique;updating algorithms;complementacion;algorithme;actualizacion;orthogonal space;oblique projector;algorithm;b orthogonality;65g50;contrast;numerical analysis;conducta;algebre lineaire;projection;65f25;algebra lineal;error calculo;error redondear;51a50;behavior;theorie perturbation;complementation;teoria representacion;orthogonalite;updating;rounding error;teoria perturbacion;representacion;ortogonalidad;erreur arrondi;algoritmo	An oblique projector is an idempotent matrix whose null space is oblique to its range, in contrast to an orthogonal projector, whose null space is orthogonal to its range. Oblique projectors arise naturally in many applications and have a substantial literature. Missing from that literature, however, are systematic expositions of their numerical properties, including their perturbation theory, their various representations, their behavior in the presence of rounding error, the computation of complementary projections, and updating algorithms. This article is intended to make a start at filling this gap. The first part of the article is devoted to the first four of the above topics, with particular attention given to complementation. In the second part, stable algorithms are derived for updating an XQRY representation of projectors, which was introduced in the first part.	numerical analysis;oblique projection;video projector	G. W. Stewart	2011	SIAM J. Matrix Analysis Applications	10.1137/100792093	representation theory;orthogonality;projection;contrast;numerical analysis;linear algebra;calculus;perturbation theory;mathematics;geometry;representation;complementation;algebra;behavior	Theory	82.38916707066308	20.57280863191754	72342
3a23baa83b19e2930df26e9ae407b98b35c323b3	modification and implementation of two-phase simplex method	programming language;simplex algorithm;simplex method;visual basic;basic feasible solution;linear programming;linear program;90c05	We investigate the problem of finding the initial basic feasible solution in the simplex algorithm. Two modifications of the two-phase simplex method are presented. Implementations of the two-phase simplex method and its modifications in the programming package MATHEMATICA and the programming language Visual Basic are written. We report computational results on numerical examples from the Netlib test set. AMS Subj. Class.: 90C05	ams-latex;iteration;netlib;numerical analysis;programming language;simplex algorithm;test set;two-phase commit protocol;two-phase locking;vhdl-ams;visual basic	Nebojsa V. Stojkovic;Predrag S. Stanimirovic;Marko D. Petkovic	2009	Int. J. Comput. Math.	10.1080/00207160701818992	mathematical optimization;basic solution;linear-fractional programming;computer science;linear programming;theoretical computer science;mathematics;revised simplex method;simplex algorithm;algorithm	Theory	76.46552631028823	25.8576554428588	72475
69d6cac585f86312fc4518e3ade42119be823895	a new iterative algorithm to solve periodic riccati differential equations with sign indefinite quadratic terms	rate of convergence;interpolation;negative semi definite;convergence;complexity theory;periodic riccati differential equations;negative semidefinite quadratic term iterative algorithm periodic riccati differential equations indefinite quadratic term;negative semidefinite quadratic term;convergence of numerical methods;periodic riccati differential equations prde;iterative algorithms riccati equations differential equations control systems time varying systems hydrogen government australia council circuits;global convergence;indexing terms;iterative algorithm;journal article;numerical example;riccati equations differential equations iterative methods;symmetric matrices;riccati differential equation;approximation theory;iterative methods;accuracy;periodic control;riccati equations differential equations iterative methods periodic control;convergence iterative algorithm periodic riccati differential equation sign indefinite quadratic term;institut fur systemdynamik und regelungstechnik;satellites;accuracy convergence equations symmetric matrices interpolation satellites;riccati equations;mathematical model;algorithms;differential equations;quadratic rates;sign indefinite quadratic term;riccati equ periodic riccati differential equations prde;indefinite quadratic term;algorithm design and analysis;keywords existing method;periodic riccati differential equation	An iterative algorithm to solve periodic Riccati differential equations (PRDE) with an indefinite quadratic term is proposed. In our algorithm, we replace the problem of solving a PRDE with an indefinite quadratic term by the problem of solving a sequence of PRDEs with a negative semidefinite quadratic term which can be solved by existing methods. The global convergence and the local quadratic rate of convergence are both established. A numerical example is given to illustrate our algorithm.	algorithm;approximation;local convergence;numerical analysis;quadratic function;rate of convergence;simulation	Yantao Feng;András Varga;Brian D. O. Anderson;Marco Lovera	2011	IEEE Transactions on Automatic Control	10.1109/TAC.2010.2101710	mathematical optimization;mathematical analysis;discrete mathematics;quadratic residuosity problem;solving quadratic equations with continued fractions;mathematics;iterative method	Vision	81.67639934144303	19.068350509350005	72480
85b8dd8ed881fdd2a16f1cf77bc0eede6b913078	incomplete hyperbolic gram-schmidt-based preconditioners for the solution of large indefinite least squares problems	computacion informatica;incomplete hyperbolic classical modified gram schmidt methods;preconditioner;ciencias basicas y experimentales;indefinite least squares problems;matematicas;cgils;grupo a	We propose to precondition the CGILS method based on the incomplete hyperbolic Gram–Schmidt methods for the solution of the large and sparse indefinite least squares (ILS) problem, which requires minimization of an indefinite quadratic form. Numerical experiments show that the incomplete hyperbolic classical/modified Gram–Schmidt (IHCGS/IHMGS) preconditioners can generally greatly reduce the number of iterations, and IHMGS has a better quality for ill-conditioned problems but may be expensive in computation cost.	least squares;preconditioner;schmidt decomposition	Qiaohua Liu;Fudao Zhang	2013	J. Computational Applied Mathematics	10.1016/j.cam.2013.02.016	mathematical optimization;mathematical analysis;calculus;mathematics;preconditioner;algebra	Theory	82.50583640687923	21.537546244116477	72516
5fbae42b5e1648915f4cd2ea02a46c48cf2ef2ab	two methods for solving optimization problems arising in electronic measurements and electrical engineering	90c30;objective function;optimization problem;numerical analysis;initial condition;numerical algorithms;electronic measurements;94a12;global optimization;numerical experiment;electrical engineering;adaptive estimation	In this paper we introduce a common problem in electronic measurements and electrical engineering: finding the first root from the left of an equation in the presence of some initial conditions. We present examples of electrotechnical devices (analog signal filtering), where it is necessary to solve it. Two new methods for solving this problem, based on global optimization ideas, are introduced. The first uses the exact a priori given global Lipschitz constant for the first derivative. The second method adaptively estimates local Lipschitz constants during the search. Both algorithms either find the first root from the left or determine the global minimizers (in the case when the objective function has no roots). Sufficient conditions for convergence of the new methods to the desired solution are established in both cases. The results of numerical experiments for real problems and a set of test functions are also presented.	algorithm;analog signal;distribution (mathematics);electrical engineering;experiment;global optimization;initial condition;mathematical optimization;numerical analysis;optimization problem	Yaroslav D. Sergeyev;Pasquale Daponte;Domenico Grimaldi;Anna Molinaro	1999	SIAM Journal on Optimization	10.1137/S1052623496312393	optimization problem;mathematical optimization;mathematical analysis;numerical analysis;mathematics;initial value problem;global optimization	Theory	75.62822546365904	24.009067983929977	72856
50a98f3509f3e80ad9c0c0ac91385df1b9269694	a modified wei-yao-liu conjugate gradient method for unconstrained optimization	global convergence;conjugate gradient method;sufficient descent condition;unconstrained optimization	In this paper, we give a modified Wei-Yao-Liu conjugate gradient method (Wei et al., 2006 [18]), which will reduce to the original Wei-Yao-Liu method, and possess the sufficient descent property without any line search. Furthermore, we prove that the presented method is globally convergent for nonconvex functions with the weak Wolfe-Powell line search. In a similar way, we also extend these results to the modified Liu-Storey method. Preliminary numerical results show that the proposed methods are effective for the given test problems.	conjugate gradient method;mathematical optimization;yao graph	Hai Huang;Suihua Lin	2014	Applied Mathematics and Computation	10.1016/j.amc.2014.01.012	gradient descent;mathematical optimization;mathematical analysis;conjugate residual method;gradient method;calculus;derivation of the conjugate gradient method;descent direction;mathematics;conjugate gradient method;nonlinear conjugate gradient method;biconjugate gradient method;line search;algebra	Vision	76.7620938752551	23.05488889135225	73029
b98a3fcbf54680bf6e51f9eb23735b4de9ded25b	maximal inner boxes in parametric ae-solution sets with linear shape	dependent interval parameters;ae solution set;65g40;15a06;tolerable solution set;linear equations;inner estimation	We consider linear systems of equations A(p)x = b(p), where the parameters p are linearly dependent and come from prescribed boxes, and the sets of solutions (defined in various ways) which have linear boundary. One fundamental problem is to compute a box being inside a parametric solution set. We first consider parametric tolerable solution sets (being convex polyhedrons). For such solution sets we prove that finding a maximal inner box is an NP-hard problem. This justifies our exponential linear programming methods for computing maximal inner boxes. We also propose a polynomial heuristic that yields a large, but not necessarily the maximal, inner box. Next, we discuss how to apply the presented linear programming methods for finding large inner estimations of general parametric AE-solution sets with linear shape. Numerical examples illustrate the properties of the methods and their application.	approximation algorithm;computation;heuristic;interval arithmetic;linear programming;linear system;maximal set;np-hardness;numerical method;polyhedron;polynomial;recurrence relation;time complexity	Milan Hladík;Evgenija D. Popova	2015	Applied Mathematics and Computation	10.1016/j.amc.2015.08.003	mathematical optimization;combinatorics;mathematical analysis;discrete mathematics;mathematics;linear equation;algebra	Theory	70.2529639435833	23.50867236921792	73305
23bbba845b534b55c5cdc3da918f852b5fed30c9	on convergence of the symmetric modified successive overrelaxation method for 2-cyclic coefficient matrices	iterative method;analisis numerico;convergence;matematicas aplicadas;msor method;mathematiques appliquees;methode modifiee;sor method;analyse numerique;metodo iterativo;convergencia;numerical analysis;methode iterative;overrelaxation method;symmetric msor method;metodo sobrerelajion;iteration method;applied mathematics;methode surrelaxation;methode msor	In this paper, we study the convergence of the symmetric modified successive overrelaxation (SMSOR) iterative method for real symmetric 2-cyclic and consistently ordered coefficient matrices. We proposed some theorems, which they obtain results better than similar works. By some numerical examples we show the goodness of our results.	coefficient;successive over-relaxation	M. T. Darvishi;P. Hessari	2006	Applied Mathematics and Computation	10.1016/j.amc.2006.05.121	mathematical optimization;mathematical analysis;calculus;mathematics;iterative method;algebra	ML	81.91364252039409	18.416522792997206	73562
d82b34acae6dec2db99140ffe8735934501324c6	generalized linear complementarity problems	generalized linear complementarity problem;interior point algorithms;canonical form;representation fonction;forme canonique;convex quadratic programming;etude methode;estudio metodo;programacion lineal;function representation;representacion funcion;linear programming;forma canonica;programmation lineaire;probleme complementarite;problema complementariedad;complementarity problem;method study;linear complementarity problem	We introduce the concept of the generalized (monotone) linear complementarity problem (GLCP) in order to unify LP, convex QP, monotone LCP, and mixed monotone LCP. We establish the basic properties of GLCP and develop canonical forms for its representation. We show that the GLCP reduces to a monotone LCP in the same variables. This implies that many results which hold true for monotone LCP extend to GLCP. In particular, any interior point algorithm for monotone LCP extends to an interior point algorithm for GLCP.	complementarity (physics);complementarity theory	Osman Güler	1995	Math. Oper. Res.	10.1287/moor.20.2.441	canonical form;mathematical optimization;mathematical analysis;linear programming;calculus;mathematics;function representation;linear complementarity problem	Theory	73.58582854764471	20.946016817502468	73771
690d12135cbbe6030f157df531f901aa63e04a74	a convergent decomposition method for box-constrained optimization problems	degree of freedom;global convergence;decomposition method;decomposition methods;gauss southwell method;constrained optimization problem;optimality condition	In this work we consider the problem of minimizing a continuously differentiable function over a feasible set defined by box constraints. We present a decomposition method based on the solution of a sequence of subproblems. In particular, we state conditions on the rule for selecting the subproblem variables sufficient to ensure the global convergence of the generated sequence without convexity assumptions. The conditions require to select suitable variables (related to the violation of the optimality conditions) to guarantee theoretical convergence properties, and leave the degree of freedom of selecting any other group of variables to accelerate the convergence.	constrained optimization;mathematical optimization	Andrea Cassioli;Marco Sciandrone	2009	Optimization Letters	10.1007/s11590-009-0119-8	mathematical optimization;combinatorics;discrete mathematics;decomposition method;mathematics;degrees of freedom	ML	74.22299255247567	23.590466357255817	73858
3688dc2e263f644bdb1f05786f0ac3b77e69e0f4	a piecewise linear dual phase-1 algorithm for the simplex method	phase 1;piecewise linear;piecewise linear functions;simplex method;computer experiment;linear programming;dual simplex method;dual phase	A dual phase-1 algorithm for the simplex method that handles all types of variables is presented. In each iteration it maximizes a piecewise linear function of dual infeasibilities in order to make the largest possible step towards dual feasibility with a selected outgoing variable. The new method can be viewed as a generalization of traditional phase-1 procedures. It is based on the multiple use of the expensively computed pivot row. By small amount of extra work per iteration, the progress it can make is equivalent to many iterations of the traditional method. In addition to this main achievement it has some further important and favorable features, namely, it is very efficient in coping with degeneracy and numerical difficulties. Both theoretical and computational issues are addressed. Examples are also given that demonstrate the power and flexibility of the method.	breakpoint;computation;degeneracy (graph theory);dilution of precision (navigation);dual polyhedron;iteration;lagrange multiplier;linear function;mathematical optimization;numerical analysis;piecewise linear continuation;row (database);simplex algorithm	István Maros	2003	Comp. Opt. and Appl.	10.1023/A:1025102305440	big m method;mathematical optimization;combinatorics;discrete mathematics;piecewise linear function;linear programming;mathematics;revised simplex method	EDA	76.27220833299077	24.37967281360777	73912
072c53e77ebb5bdf4def5a8160d17968ab7065a9	incremental aggregated proximal and augmented lagrangian algorithms		We consider minimization of the sum of a large number of convex functions, and we propose an incremental aggregated version of the proximal algorithm, which bears similarity to the incremental aggregated gradient and subgradient methods that have received a lot of recent attention. Under cost function differentiability and strong convexity assumptions, we show linear convergence for a sufficiently small constant stepsize. This result also applies to distributed asynchronous variants of the method, involving bounded interprocessor communication delays.	algorithm;augmented lagrangian method;convex function;gradient;inter-process communication;loss function;rate of convergence;subgradient method	Dimitri P. Bertsekas	2015	CoRR		mathematical optimization;combinatorics;discrete mathematics;augmented lagrangian method;mathematics	ML	73.84247534874012	25.60166411161891	74020
72b0c269c7ab3d8a9feda53a2c531ba8ca188201	computation of numerical padé-hermite and simultaneous padé systems ii: a weakly stable algorithm	power series;numerical stability;65f05;pade hermite approximants;pade approximation;look ahead;simultaneous pade approximants;65g05;numerical computation;numerical algorithm;pad e approximants;striped sylvester inverses;mosaic sylvester inverses;41a21	For k + 1 power series a0(z), . . . , ak(z), we present a new iterative, look-ahead algorithm for numerically computing Padé-Hermite systems and simultaneous Padé systems along a diagonal of the associated Padé tables. The algorithm computes the systems at all those points along the diagonal at which the associated striped Sylvester and mosaic Sylvester matrices are wellconditioned. The operation and the stability of the algorithm is controlled by a single parameter τ which serves as a threshold in deciding if the Sylvester matrices at a point are sufficiently wellconditioned. We show that the algorithm is weakly stable, and provide bounds for the error in the computed solutions as a function of τ . Experimental results are given which show that the bounds reflect the actual behavior of the error. The algorithm requires O(‖n‖2+s3‖n‖) operations, to compute Padé-Hermite and simultaneous Padé systems of type n = [n0, . . . , nk], where ‖n‖ = n0 + · · ·+nk and s is the largest step-size taken along the diagonal. An additional application of the algorithm is the stable inversion of striped and mosaic Sylvester matrices.	algorithm;computation;gauss–hermite quadrature;iterative method;numerical analysis;padé approximant;padé table;sylvester matrix	Stanley Cabay;Anthony R. Jones;George Labahn	1996	SIAM J. Matrix Analysis Applications	10.1137/S0895479894268695	padé approximant;mathematical optimization;combinatorics;calculus;mathematics;power series;numerical stability	Theory	81.9796442918452	22.45124807439918	74329
9a3665624f22264b169792c5016eae32686605ce	a novel algorithm for determinant calculation of n×n matrix	mathematics computing;mathematical algorithms matrix determinant algebra mathematical computation;matrix algebra;matrix algebra mathematics computing;mathematical proof n n square matrix determinant calculation tabe algorithm;polynomials matrix decomposition informatics electrical engineering educational institutions presses	Evaluation of the determinant of a square matrix is an important issue in science and engineering for different types of applications. Several methods have been already proposed, but there are some limitation and also drawbacks using each method in particular applications. In this paper we present a novel method for determining the determinant of a square matrix which can be especially faster, and more useful when the order of the matrix increases. The paper is presented in five sections. I. Introduction. II. Review III. The Novel Algorithm (TaBe) where we introduce our new method. IV. Mathematical Proof, here we illustrate the mathematical proof. V. Advantages VI. Conclusion, and finally References.	algorithm;machine code;the matrix	Sayed Mostafa Taheri;Jafar Boostanpour;Bahareh Mohammadi	2013	2013 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2013.6637270	gaussian elimination;eigendecomposition of a matrix;determinant;single-entry matrix;computer science;pure mathematics;invertible matrix;trace;mathematics;exchange matrix;state-transition matrix;matrix decomposition;block matrix;matrix;algebra;minor	Robotics	77.98829033910009	20.046113884049408	74398
679c2fd6bb2e4144143ee0bd2638481799bff42f	polynomial root-finding via structured matrices	computational cost;polynomial root-finding;structured matrix;memory requirement;use eigensolvers;univariate polynomial;popular iterative;root-finding technique;companion matrix;machine precision	We study the problem of approximating the zeros of an univariate polynomial (up to machine precision). Some popular iterative root-finding methods construct companion matrices (Frobenius, Lagrange) associated with the given polynomial and use eigensolvers to find the eigenvalues of such matrices. Our goal is to study this root-finding technique, exploiting the structure (e.g., diagonal plus rank one) of companion matrices to obtain a decrease of computational cost and memory requirements.	algorithmic efficiency;approximation algorithm;iterative method;machine epsilon;matrix multiplication;polynomial;requirement;root-finding algorithm	Esteban Segura Ugalde	2012	ACM Comm. Computer Algebra	10.1145/2429135.2429162	companion matrix;matrix analysis;mathematical optimization;combinatorics;discrete mathematics;mathematics;matrix polynomial	Theory	80.37012878130635	23.208397375167447	74725
c90dc59be4ecfb77f7f8fbf360490468dd03ff7e	stage value predictors and efficient newton iterations in implicit runge-kutta methods	prediccion;implicit runge kutta;equation differentielle;erreur;matrice jacobi;implicit method;convergence;control strategies;methode newton;metodo control;relacion convergencia;newton iteration;differential equation;arbre butcher;forme hessenberg;taux convergence;convergence rate;65l05;butcher tree;predictors;methode runge kutta;metodo runge kutta;costo;methode controle;ecuacion diferencial;methode implicite;jacobi matrix;convergencia;matriz jacobi;metodo newton;newton method;implicit runge kutta method;error;hessenberg form;prediction;control method;runge kutta method;cout	The prediction of stage values in implicit Runge--Kutta methods is important both for overall efficiency as well as for the design of suitable control strategies for the method. The purpose of this paper is to construct good stage value predictors for implicit methods and to verify their behavior in practical computations. We show that for stiffly accurate methods of low stage order it is necessary to use several predictors. In other words, a continuous extension for the method will not yield the best results. We also investigate how to gain additional efficiency in the Newton iterations used to correct the prediction error. This leads to new control strategies with respect to refactorization of Jacobians that seek to globally minimize total work per unit time of integration.	iteration;newton;runge–kutta methods;value (computer science)	Hans Olsson;Gustaf Söderlind	1998	SIAM J. Scientific Computing	10.1137/S1064827596306963	mathematical optimization;calculus;mathematics;newton's method;algorithm	HPC	82.51106020805761	18.383576928082743	74915
70a686a39304d5af0d9c1b86c8a81c15df2f6499	convergence analysis for finite family of relatively quasi nonexpansive mappings and systems of equilibrium problems	sistema lineal;strong convergence;iterative method;convergence analysis;convergence forte;analisis numerico;projection operator;matematicas aplicadas;uniformly convex;convergencia fuerte;mathematiques appliquees;espacio banach;relatively quasi nonexpansive mappings;generalized f projection operator;37c25;banach space;punto fijo;uniformly convex and uniformly smooth;linear system;analyse numerique;equilibrium problem;metodo iterativo;numerical analysis;hybrid method;methode iterative;point fixe;real banach spaces;65f08;common fixed point;algebra lineal numerica;algebre lineaire numerique;operador proyeccion;nonexpansive mapping;46bxx;numerical linear algebra;systeme lineaire;applied mathematics;fix point;65f10;espace banach;operateur projection;analyse convergence;equilibrium problems	In this paper, a new iterative scheme by hybrid method is constructed. Strong convergence of the scheme to a common element of the set of common fixed points of finite family of relatively quasi-nonexpansive mappings and set of common solutions of a system of equilibrium problems in a uniformly convex and uniformly smooth real Banach space is proved using the properties of generalized f-projection operator. Our results extend important recent results.		Eric Uwadiegwu Ofoedu;Yekini Shehu	2011	Applied Mathematics and Computation	10.1016/j.amc.2011.03.147	mathematical optimization;mathematical analysis;topology;projection;numerical analysis;mathematics;iterative method;numerical linear algebra;linear system;banach space;algebra	Theory	81.10227418365564	19.317653106087995	75149
7e587e921ccba106c1f0c6c2d554dd6729ab27ce	spherical minimax location problem using the euclidean norm: formulation and optimization	location problem;distance measure;kkt optimality conditions;minimax spherical location;factored secant update;karush kuhn tucker;finite difference;euclidean distance;linear programming;nonlinear equation;linear program;necessary optimality condition;optimality condition;coordinate system	The minimax spherical location problem is formulated in the Cartesian coordinate system using the Euclidean norm, instead of the spherical coordinate system using spherical arc distance measures. It is shown that minimizing the maximum of the spherical arc distances between the facility point and the demand points on a sphere is equivalent to minimizing the maximum of the corresponding Euclidean distances. The problem formulation in this manner helps to reduce Karush-Kuhn-Tucker necessary optimality conditions into the form of a set of coupled nonlinear equations, which is solved numerically by using a method of factored secant update with a finite difference approximation to the Jacobian. For a special case when the set of demand points is on a hemisphere and one or more point-antipodal point pair(s) are included in the demand points, a simplified approach gives a minimax point in a closed form.	approximation;euclidean distance;finite difference;jacobian matrix and determinant;karush–kuhn–tucker conditions;mathematical optimization;minimax;nonlinear system;numerical analysis;secant method;tucker decomposition	Minnie H. Patel	1995	Comp. Opt. and Appl.	10.1007/BF01299160	mathematical optimization;finite difference;combinatorics;mathematical analysis;linear programming;coordinate system;euclidean distance;mathematics;euclidean distance matrix;karush–kuhn–tucker conditions;1-center problem	Vision	70.90714442719683	24.004198372444602	75202
24b1e41465076c90ec4e83694662c08622c76f1b	reachability analysis of polynomial systems using linear programming relaxations	different way;reachability analysis;equivalent problem;linear program;affine lower-bound function;accurate approximation;piecewise affine lower-bound function;new method;polynomial system;discrete-time polynomial dynamical system;linear programming relaxation;bernstein form	In this paper we propose a new method for reachability analysis of the class of discrete-time polynomial dynamical systems. Our work is based on the approach combining the use of template polyhedra and optimization [1, 2]. These problems are non-convex and are therefore generally difficult to solve exactly. Using the Bernstein form of polynomials, we define a set of equivalent problems which can be relaxed to linear programs. Unlike using affine lower-bound functions in [2], in this work we use piecewise affine lower-bound functions, which allows us to obtain more accurate approximations. In addition, we show that these bounds can be improved by increasing artificially the degree of the polynomials. This new method allows us to compute more accurately guaranteed over-approximations of the reachable sets of discrete-time polynomial dynamical systems. We also show different ways to choose suitable polyhedral templates. Finally, we show the merits of our approach on several examples.	approximation;approximation algorithm;bernstein polynomial;convex function;dynamical system;linear programming;mathematical optimization;optimization problem;polyhedron;reachability;variadic template	Mohamed Amin Ben Sassi;Romain Testylier;Thao Dang;Antoine Girard	2012		10.1007/978-3-642-33386-6_12	mathematical optimization;combinatorics;discrete mathematics;mathematics	Logic	69.30771809469853	23.32405333852443	75303
dcdfbc5161c6c332fdc37e188f43432fc022e5b0	extreme point programming with nonlinear constraints	extreme point	This paper summarizes previous results obtained by the authors on methods of solving extreme point mathematical programming problems with linear constraints. It is also shown how these results can be extended to yield an algorithm for solving extreme point mathematical programming problems with nonlinear constraints. Numerical examples to illustrate the algorithms are included.	nonlinear system	M. J. L. Kirby;H. R. Love;Kanti Swarup	1973	Discrete Mathematics	10.1016/0012-365X(73)90128-3	extreme point;mathematical optimization;combinatorics;criss-cross algorithm;mathematics;algorithm	Vision	70.94139054947314	22.896926075523194	75637
01c90338d12d4647bb9034d9275d61c74432d8aa	converged algorithms for orthogonal nonnegative matrix factorizations	orthogonal nonnegative matrix factorizations;clus- tering methods;converged algorithms	Abstract: This paper proposes uni-orthogonal and bi-orthogonal nonnegative matrix factorization algorithms with robust convergence proofs. We design the algorithms based on the work of Lee and Seung [1], and derive the converged versions by utilizing ideas from the work of Lin [2]. The experimental results confirm the theoretical guarantees of the convergences.	algorithm;cluster analysis;computation;coupled cluster;iteration;karush–kuhn–tucker conditions;least squares;linear programming;non-negative matrix factorization;numerical analysis;performance;sparse matrix;stationary process;utility functions on indivisible goods	Andri Mirzal	2010	CoRR		mathematical optimization;combinatorics;discrete mathematics;mathematics	AI	80.319772660277	22.168271332196014	75937
03e6a4c7e0d13c373e82ede937433335255c9f35	global minimization using an augmented lagrangian method with variable lower-level constraints	minimisation;optimisation sous contrainte;constrained optimization;iterative method;ciencias exatas e da terra;minimization;global solution;non linear programming;numerical experiments;lagrangiano aumentado;deterministic global optimization;nonlinear programming;ciencia da computacao;programacion no lineal;optimum global;90c30;minimizacion;global convergence;programmation non lineaire;global optimum;convergence numerique;metodo iterativo;optimizacion con restriccion;numerical convergence;augmented lagrangian method;65k05;lagrangien augmente;mathematical programming;methode iterative;methode lagrange;metodo lagrange;lagrangian method;algorithms;global optimization;solution globale;augmented lagrangians;numerical experiment;nonlinear optimization;programmation mathematique;augmented lagrangian;optimo global;convergencia numerica;solucion global;programacion matematica	A novel global optimization method based on an Augmented Lagrangian framework is introduced for continuous constrained nonlinear optimization problems. At each outer iteration k the method requires the εk-global minimization of the Augmented Lagrangian with simple constraints, where εk → ε. Global convergence to an ε-global minimizer of the original problem is proved. The subproblems are solved using the αBB method. Numerical experiments are presented.	augmented lagrangian method;experiment;global optimization;iteration;mathematical optimization;monte carlo method;nonlinear programming;nonlinear system;numerical method	Ernesto G. Birgin;Christodoulos A. Floudas;José Mario Martínez	2010	Math. Program.	10.1007/s10107-009-0264-y	mathematical optimization;augmented lagrangian method;nonlinear programming;calculus;mathematics;geometry	EDA	75.17271832229692	22.91137561388861	76102
edd818f2aadc9a4c949c56a77c999f91289b5f5b	nonlinear optimization exclusion tests for finding all solutions of nonlinear equations	optimisation sous contrainte;equation non lineaire;constrained optimization;ecuacion no lineal;analisis numerico;test exclusion;matematicas aplicadas;mathematiques appliquees;solutions of nonlinear equations;complexite calcul;analisis intervalo;calcul erreur;optimization method;taylor expansion;complexity;cero de funcion;metodo optimizacion;fonction objectif;analyse numerique;algorithme;optimizacion con restriccion;objective function;algorithm;error analysis;complejidad computacion;numerical analysis;programacion lineal;mathematical programming;computational complexity;zero of function;methode optimisation;linear programming;nonlinear equation;programmation lineaire;linear program;calculo error;funcion objetivo;exclusion test;solution equation;optimisation non lineaire;non linear equation;applied mathematics;nonlinear optimization;programmation mathematique;analyse intervalle;programacion matematica;zero de fonction;interval analysis;algoritmo;exclusion tests	Exclusion tests are a well known tool in the area of interval analysis for finding the zeros of a function over a compact domain. Recently, K. Georg developed linear programming exclusion tests based on Taylor expansions. In this paper, we modify his approach by choosing another objective function and using nonlinear constraints to make the new algorithm converges faster than the algorithm in [K. Georg, A new exclusion test, J. Comput. Appl. Math. 152 (2003) 147–160]. In this way, we reduce the number of subinterval in each level. The computational complexity for the new tests are investigated. Also, numerical results and comparisons will be presented.	mathematical optimization;nonlinear programming;nonlinear system	Muhammed I. Syam	2005	Applied Mathematics and Computation	10.1016/j.amc.2005.01.002	mathematical optimization;complexity;numerical analysis;linear programming;taylor series;calculus;mathematics;computational complexity theory;algorithm	Robotics	76.37175323270891	22.286168395565337	76109
7c04466dc9a21ac7457f3d77bb1a8c1c6c5f61c7	g1 continuity conics for curve fitting using particle swarm optimization	tangent continuity;particle swarm optimization optimization least squares approximation search problems aerospace electronics equations mathematical model;curve fitting problems;particle swarm optimisation curve fitting;least squares approximation;pso;particle swarm optimizer;particle swarm optimization;aerospace electronics;mathematical model;particle swarm optimization curve fitting;optimization;tangent continuity particle swarm optimization curve fitting problems pso g 1 continuity conics;search problems;curve fitting;g 1 continuity conics;particle swarm optimisation	We solve curve fitting problems using Particle Swarm Optimization (PSO). PSO is used to optimize control points and weights of two conic curves to a set of data points. PSO is used to find the best middle control point and weight for both conic curves to provide piecewise conics that preserve tangent continuity. We present the numerical result and an application using our proposed curve fitting technique.	control point (mathematics);curve fitting;data point;numerical analysis;particle swarm optimization;program optimization;scott continuity	Zainor Ridzuan Yahya;Abd. Rahni Mt. Piah;Ahmad Abdul Majid	2011	2011 15th International Conference on Information Visualisation	10.1109/IV.2011.27	mathematical optimization;calculus;mathematics;geometry;curve fitting	Robotics	70.27786612167736	24.70832212430583	76183
f8ae74b8694d1f0cdc3605c0c8c407f9d06d063a	monotone iterates for solving coupled systems of nonlinear parabolic equations	equation derivee partielle;sistema lineal;calcul scientifique;equation non lineaire;iterative method;convergence analysis;65b99;ecuacion no lineal;systeme equation;problema valor limite;partial differential equation;ecuacion derivada parcial;analisis numerico;decomposition domaine;teorema existencia;convergence;ecuacion trascendente;domain decomposition;schema differences;upper and lower solution;numerical solution;schwarz method;theoreme unicite;parabolic equation;metodo descomposicion;methode schwarz;boundary value problem;monotone system;ecuacion algebraica;methode decomposition;multigrille;existence theorem;descomposicion dominio;monotone scheme;equation transcendante;65m55;convergence rate;linear system;coupled system;ecuacion parabolica;uniqueness theorem;systeme monotone;analyse numerique;initial value problem;monotone schemes;metodo iterativo;algorithme;acceleration convergence;difference scheme;algorithm;decomposition method;convergencia;computacion cientifica;65n55;sistema ecuacion;equation parabolique;numerical analysis;35kxx;methode iterative;transcendental equation;esquema diferencias;equation system;domain decomposition method;multigrid;65f08;algebra lineal numerica;algebre lineaire numerique;multigrilla;aceleracion convergencia;problema valor inicial;equation algebrique;metodo schwarz;interaction model;numerical linear algebra;probleme valeur initiale;systeme lineaire;scientific computation;nonlinear parabolic equation;non linear equation;iteration method;monotone convergence;65h10;probleme valeur limite;teorema unicidad;theoreme existence;algebraic equation;65f10;65m12;solution numerique;parabolic systems;analyse convergence;algoritmo;convergence acceleration	This paper deals with numerical solutions of coupled nonlinear parabolic equations. Using the method of upper and lower solutions, monotone sequences are constructed for difference schemes which approximate coupled systems of nonlinear parabolic equations. This monotone convergence leads to existence-uniqueness theorems. An analysis of convergence rates of the monotone iterative method is given. A monotone domain decomposition algorithm which combines the monotone approach and an iterative domain decomposition method based on the Schwarz alternating is proposed. A convergence analysis of the monotone domain decomposition algorithm is presented. An application to a gas–liquid interaction model is given.	alternating turing machine;approximation algorithm;domain decomposition methods;iterative method;navier–stokes equations;nonlinear programming;nonlinear system;numerical analysis;parabolic antenna;monotone	Igor Boglaev	2010	Computing	10.1007/s00607-010-0132-x	bernstein's theorem on monotone functions;mathematical optimization;mathematical analysis;calculus;mathematics;iterative method;algorithm;algebra	Robotics	82.53323781820792	18.813630070804724	76281
b45645bde59835886fd74f099f9015509e58c0d6	adjusting the rayleigh quotient in semiorthogonal lanczos methods	lanczos algorithm;symmetric matrix;lanczos method;partial reorthogonalization;large eigenproblem;65f25;ritz value;technical report;65f50;adjusted rayleigh quotient;rayleigh quotient;65f15	In a semiorthogonal Lanczos algorithm, the orthogonality of the Lanczos vectors is allowed to deteriorate to roughly the square root of the rounding unit, after which the current vectors are reorthogonalized. A theorem of Simon 4] shows that the Rayleigh quotient | i.e., the tridiagonal matrix produced by the Lanczos recursion | contains fully accurate approximations to the Ritz values in spite of the lack of orthogonality. Unfortunately, the same lack of orthogonality can cause the Ritz vectors to fail to converge. It also makes the classical estimate for the residual norm misleadingly small. In this note we show how to adjust the Rayleigh quotient to overcome this problem. Abstract In a semiorthogonal Lanczos algorithm, the orthogonality of the Lanczos vectors is allowed to deteriorate to roughly the square root of the rounding unit, after which the current vectors are reorthogonalized. A theorem of Simon 4] shows that the Rayleigh quotient | i.e., the tridiagonal matrix produced by the Lanczos recursion | contains fully accurate approximations to the Ritz values in spite of the lack of orthogonality. Unfortunately, the same lack of orthogonality can cause the Ritz vectors to fail to converge. It also makes the classical estimate for the residual norm misleadingly small. In this note we show how to adjust the Rayleigh quotient to overcome this problem.	approximation;converge;lanczos algorithm;levinson recursion;rayleigh–ritz method;rounding	G. W. Stewart	2002	SIAM J. Scientific Computing	10.1137/S1064827501388984	mathematical optimization;mathematical analysis;lanczos algorithm;lanczos approximation;technical report;calculus;mathematics;rayleigh quotient iteration;symmetric matrix;algebra	ML	82.4402756738055	23.578460364532457	76688
cc823bcccb6dcee16a14c173edd8a44d72627801	variants of the uzawa method for saddle point problem	gsor method;iterative method;saddle point problem;uzawa sor method;uzawa method	In this paper, we first propose three variants of the Uzawa method for solving the saddle point problem, and then we provide convergence results for the three proposed methods. Numerical experiments show that our proposed methods with three parameters perform about twice as fast as the GSOR (Generalized SOR) method with two parameters since the proposed methods have less workload per iteration than the GSOR.		Jae Heon Yun	2013	Computers & Mathematics with Applications	10.1016/j.camwa.2013.01.037	mathematical optimization;calculus;mathematics;iterative method;mathematical economics;algebra	Theory	77.82851474742246	22.864626641115	76715
10eaa3e385e95545a07e10cd89698c7045c2a276	ipiasco: inertial proximal algorithm for strongly convex optimization	convergence analysis;strongly convex optimization;heavy ball method;inertial proximal method	In this paper, we present a forward–backward splitting algorithm with additional inertial term for solving a strongly convex optimization problem of a certain type. The strongly convex objective function is assumed to be a sum of a non-smooth convex and a smooth convex function. This additional knowledge is used for deriving a worst-case convergence rate for the proposed algorithm. It is proved to be an optimal algorithm with linear rate of convergence. For certain problems this linear rate of convergence is better than the provably optimal worst-case rate of convergence for smooth strongly convex functions. We demonstrate the efficiency of the proposed algorithm in numerical experiments and examples from image processing.	adaptive stepsize;algorithm;best, worst and average case;convex function;convex optimization;experiment;flat rate;gradient;image processing;mathematical optimization;monomial;numerical analysis;numerical method;optimization problem;rate of convergence;relevance	Peter Ochs;Thomas Brox;Thomas Pock	2015	Journal of Mathematical Imaging and Vision	10.1007/s10851-015-0565-0	convex analysis;subderivative;support function;frank–wolfe algorithm;mathematical optimization;conic optimization;proximal gradient methods for learning;combinatorics;mathematical analysis;convex optimization;convex combination;ellipsoid method;linear matrix inequality;convex conjugate;convex hull;absolutely convex set;random coordinate descent;mathematics;proximal gradient methods;convex set;logarithmically convex function;effective domain;proper convex function	ML	73.2823001430037	24.30595460832717	76841
42e3655e5c815892c542b69919fcc1893546157f	computational issues for a new class of preconditioners	weighted linear least squares;linear programming;cholesky factor;sherman morrison-woodbury formula;infeasible interior po int algorithms;linear program;cholesky factorization;iteration method;direct method	In this paper we consider solving a sequence of weighted linear least squares problems where the only changes from one problem to the next are the weights and the right hand side (or data). We alternate between iterative and direct methods to solve the normal equations for the least squares problems. The direct method is the Cholesky factorization. For the iterative method we discuss a class of preconditioners based on a low rank correction of a Cholesky factorization of a weighted normal equation coefficient matrix. Different ways to compute the preconditioner are given. Further, a sparse algorithm for modifying the Cholesky factors by a low rank matrix is derived.	algorithm;cholesky decomposition;coefficient;computation;direct method in the calculus of variations;iterative method;linear least squares (mathematics);ordinary least squares;preconditioner;sparse matrix	Venansius Baryamureeba;Trond Steihaug	1999			iterative method;non-linear least squares;cholesky decomposition;linear programming;mathematical optimization;incomplete cholesky factorization;least squares;mathematics	AI	79.56708377367258	22.57556137748133	76881
e722bd49352c5a6631e9bc4a50911cdcf806089c	new variant of the hss iteration method for weighted toeplitz regularized least-squares problems from image restoration	weighted toeplitz matrices;preconditioning;image restoration;least squares problems;hss iteration method	For fast solving weighted Toeplitz least-squares problems from image restoration, Ng and Pan (2014) studied a new Hermitian and skew-Hermitian splitting (NHSS) preconditioner. In this paper, a generalization of the NHSS preconditioner and the corresponding iterative method are presented. Convergence for the new iteration method is studied and optimal choice of the parameters is discussed. Bounds on the eigenvalues and the corresponding eigenvector distributions are proposed. The degree of the minimal polynomial of the preconditioned matrix is obtained. Numerical experiments arising from image restoration are provided, which show the proposed iteration method is effective and confirm our theoretical results are correct.	circuit restoration;high-speed serial interface;image restoration;iteration;least squares;toeplitz hash algorithm	Li-Dan Liao;Guo-Feng Zhang	2017	Computers & Mathematics with Applications	10.1016/j.camwa.2017.03.027	image restoration;mathematical optimization;mathematical analysis;discrete mathematics;mathematics;preconditioner	Vision	81.53289324520084	21.859859930934956	77006
46c64a52448e75e2bc83767bcd97df51c34e37d8	convex stochastic optimization for random fields on graphs: a method of constructing lagrange multipliers	random graph;52a41;optimisation;1991 mathematics subject classification 90c15;optimizacion;graph method;convex programming;random fields on countable directed graphs;grafo aleatorio;duality;programmation stochastique;stochastic lagrange multipliers;93e20;programmation convexe;graphe aleatoire;lagrange multiplier;metodo grafo;linear constraint;methode graphe;dualite;stochastic optimization;linear operator;directed graph;90c25;graphe oriente;multiplicateur lagrange;93e99;multiplicador lagrange;dualidad;grafo orientado;optimization;90a16;convex stochastic optimization;stochastic programming;biting lemma;programacion estocastica;key words convex stochastic optimization;random field;programacion convexa	The paper analyzes stochastic optimization problems involving random fields on infinite directed graphs. The primary focus is on a problem of maximizing a concave functional of the field subject to a system of convex and linear constraints. The latter are specified in terms of linear operators acting in the space L ∞ . We examine conditions under which these constraints can be relaxed by using dual variables in L 1 – stochastic Lagrange multipliers. We develop a method for constructing the Lagrange multipliers. In contrast to the conventional methods employed for such purposes (relying on the Yosida-Hewitt theorem), our technique is based on an elementary measure-theoretic fact, the “biting lemma”. Copyright Springer-Verlag Berlin Heidelberg 2001	lagrange multiplier;mathematical optimization;stochastic optimization	Igor V. Evstigneev;Michael I. Taksar	2001	Math. Meth. of OR	10.1007/s001860100148	stochastic programming;constraint algorithm;random graph;mathematical optimization;combinatorics;discrete mathematics;random field;duality;directed graph;stochastic optimization;mathematics;linear map;lagrange multiplier;karush–kuhn–tucker conditions	Vision	71.2873825436208	21.30702613851035	77063
55082fa0112978e33c4fbbef1687c50a911c177e	a lagrangian relaxation view of linear and semidefinite hierarchies	90c26;approximation algorithms;semidefinite relaxations;semidefinite and lp hierarchies;lagrangian relaxations;0 1 optimization;global optimization;90c22;linear relaxations;90c05	We consider the general polynomial optimization problem $\mathbf{P}: f^*=\min\{f(\mathbf{x}): \mathbf{x}\in\mathbf{K}\}$, where $\mathbf{K}$ is a compact basic semialgebraic set. We first show that the standard Lagrangian relaxation yields a lower bound as close as desired to the global optimum $f^*$, provided that it is applied to a problem $\tilde{\mathbf{P}}$ equivalent to $\mathbf{P}$, in which sufficiently many redundant constraints (products of the initial ones) are added to the initial description of $\mathbf{P}$. Next we show that the standard hierarchy of linear program (LP-)relaxations of $\mathbf{P}$ (in the spirit of the Sherali--Adams reformulation-linearization technique) can be interpreted as a brute force simplification of the above Lagrangian relaxation in which a nonnegative polynomial (with coefficients to be determined) is replaced with a constant polynomial equal to zero. Inspired by this interpretation, we provide a systematic improvement of the LP-hierarchy by doing a much less brut...	lagrangian relaxation;relaxation (approximation);semidefinite programming	Jean B. Lasserre	2013	SIAM Journal on Optimization	10.1137/130908841	mathematical optimization;combinatorics;topology;mathematics;global optimization	Theory	72.14606977594565	23.46903682706566	77069
b2e4ddade04363c23a0768e21bc08a0e6da72d8b	efficient derivative computations in neutron scattering via interface contraction	local preaccumulation;numerical differentiation;adifor;objective function;forward mode;lines of code;local structure;neutron scattering;interface contraction;automatic differentiation;truncation error;fortran	For the solution of a minimization problem, a neutron scattering simulation needs accurate and efficient derivatives of an objective function in the form of a Fortran 77 program with about 3,500 lines of code. We use the Adifor system implementing the technology of automatic differentiation to transform the given computer code into another program capable of evaluating the objective function and its derivatives. Compared to numerical differentiation, the derivatives obtained from applying automatic differentiation in this black-box fashion are free from truncation error and, in this application, their computation requires less time. To increase the efficiency of automatic differentiation further, a technique called interface contraction is used. The idea of interface contraction is to exploit the local structure of a given code by temporarily reducing the number of derivatives propagated through the code. By reporting performance results, we show the significance of interface contraction in the neutron scattering application. We also demonstrate the simplicity of the approach and argue that interface contraction should be incorporated into future automatic differentiation tools.	automatic differentiation;black box;computation;fortran;loss function;numerical analysis;numerical differentiation;optimization problem;simulation;source lines of code;truncation error	H. Martin Bücker;Arno Rasch	2002		10.1145/508791.508830	automatic differentiation;mathematical optimization;numerical differentiation;computer science;theoretical computer science;neutron scattering;truncation error;source lines of code;algorithm	HPC	77.60938519292095	26.867421673318283	77100
ad93d866d28fedb6ff3710ea71d41f91f8aa7a13	efficient first order superlinear algorithms		Substantial number of problems in artificial intelligence requires optimization. Increasing complexity of the problems imposes several challenges on optimization algorithms. The algorithms must be fast, computationally efficient, and scalable. Balance between convergence speed and computational complexity is of central importance. Typical example is the task of training neural networks. Superlinear algorithms are highly regarded for their speed-complexity ratio. With superlinear convergence rates and linear computational complexity they are often the primary choice. Two first order superlinear algorithms are introduced. The algorithms are computationally efficient, convergent, and theoretically justified. They are applied to several neural network training tasks, practically evaluated, and compared to the relevant first order optimization techniques. Results indicate superior performance.	algorithm;algorithmic efficiency;analysis of algorithms;artificial intelligence;artificial neural network;computational complexity theory;conjugate gradient method;gradient descent;instrumental convergence;iteration;line search;mathematical optimization;rate of convergence;requirement;scalability;simulation	Peter Géczy;Shotaro Akaho;Shiro Usui	2006			scalability;artificial neural network;computational complexity theory;algorithm;convergence (routing);computer science	AI	77.46413128107767	25.53413517409967	77238
6a34fb0ec1dd18101ef29f06f8ea54787495bb92	an adaptive linear approximation algorithm for copositive programs	quadratic programming;quadratic program;maximum clique problem;approximation algorithms;linear approximation;combinatorial problems;quadratic optimization problems;05c69;copositive cone;copositive programming;objective function;90c20;15a48;graph;approximation scheme;linear optimization;ams subject classification;90c05;numerical experiment;semidefinite;stability number;15a63	We study linear optimization problems over the cone of copositive matrices. These problems appear in nonconvex quadratic and binary optimization; for instance, the maximum clique problem and other combinatorial problems can be reformulated as such problems. We present new polyhedral inner and outer approximations of the copositive cone which we show to be exact in the limit. In contrast to previous approximation schemes, our approximation is not necessarily uniform for the whole cone but can be guided adaptively through the objective function, yielding a good approximation in those parts of the cone that are relevant for the optimization and only a coarse approximation in those parts that are not. Using these approximations, we derive an adaptive linear approximation algorithm for copositive programs. Numerical experiments show that our algorithm gives very good results for certain nonconvex quadratic problems.	approximation algorithm;clique (graph theory);clique problem;computation;cone (formal languages);convex cone;experiment;linear approximation;linear programming;mathematical optimization;numerical analysis;numerical method;optimization problem;partition type;point of interest;polyhedron;quadratic assignment problem;quadratic function;quadratic programming;sparse matrix	Stefan Bundfuss;Mirjam Dür	2009	SIAM Journal on Optimization	10.1137/070711815	mathematical optimization;combinatorics;discrete mathematics;mathematics;graph;quadratic programming;approximation algorithm;linear approximation	Theory	72.33607103935817	24.19504829393861	77344
c4c1f757a7de67460f654dec8f80366a8d1ced98	application of scaled nonlinear conjugate-gradient algorithms to the inverse natural convection problem	partial differential equation;time varying;line search;splitting method;broyden fletcher goldfarb shanno;optimization technique;gradient method;quasi newton;adjoint variable method;scaled conjugate gradient;test bed;conjugate gradient method;porous medium;mixed finite element;objective function;f4 3;conjugate gradient;least square;natural convection;nonlinear problem;inverse natural convection problem;consistent splitting method;scaled conjugate gradient method;mixed finite element method;f1 1;mixed finite elements;constrained optimization problem	The inverse natural convection problem (INCP) in a porous medium is a highly non-linear problem because of the nonlinear convection and Forchheimer terms. The INCP can be converted into the minimization of a least-squares discrepancy between the observed and the modelled data. It has been solved using different classical optimization strategies that require a monotone descent of the objective function for recovering the unknown profile of the timevarying heat source function. In this investigation we use this PDE-constrained optimization problem as a demanding testbed to compare the performance of several state-of-the-art variants of the conjugate gradients approach. We propose solving the INCP using the scaled nonlinear conjugate gradient (SCG) method: a low-cost and low-storage optimization technique. The method presented here uses the gradient direction with a particular spectral step length and the quasi-Newton Broyden-Fletcher-Goldfarb-Shanno (BFGS) updating formula without any matrix evaluations. Two adaptive line search approaches are numerically studied in which there is no need for solving the sensitivity problem to obtain the step length directly, and are compared to an exact line search approach. We combine the proposed optimization scheme for INCP with a consistent splitting scheme for solving systems of momentum, continuity and energy equations and a mixed finite element method. We show a number of computational tests which demonstrate that the proposed method performs better than the classical gradient method by improving the number of iterations required and reducing the computational time. We also discuss some practical issues related to the implementation of the different methods.	broyden–fletcher–goldfarb–shanno algorithm;computation;constrained optimization;constraint (mathematics);discrepancy function;finite element method;fletcher's checksum;iteration;least squares;line search;linear programming;mathematical optimization;newton;nonlinear conjugate gradient method;nonlinear system;numerical analysis;optimization problem;scott continuity;stochastic gradient descent;testbed;time complexity;monotone	Jeff C.-F. Wong;Bartosz Protas	2013	Optimization Methods and Software	10.1080/10556788.2011.626778	mathematical optimization;mathematical analysis;calculus;mathematics;conjugate gradient method;nonlinear conjugate gradient method	AI	78.52278976254239	26.90175605709428	77630
8a553478defc56add30376b3d18803d0f418ee5b	structure computation and discrete logarithms in finite abelian p-groups	finite group;sistema lineal;metodo directo;calcul scientifique;20b05;abelian group;analisis numerico;g structure;metodo monte carlo;matrice h;65c05;generic algorithm;65f05;stochastic method;grupo abeliano;methode monte carlo;matrix inversion;discrete logarithm;group theory;inversion matriz;linear system;analyse numerique;finite abelian group;algorithme;number theory;algorithm;computacion cientifica;numerical analysis;monte carlo method;20kxx;monte carlo algorithm;algebra lineal numerica;algebre lineaire numerique;groupe fini;inversion matrice;methode stochastique;numerical linear algebra;matriz h;groupe abelien;systeme lineaire;scientific computation;methode directe;grupo finito;direct method;h matrix;algoritmo;metodo estocastico	We present a generic algorithm for computing discrete logarithms in a finite abelian p-group H, improving the Pohlig–Hellman algorithm and its generalization to noncyclic groups by Teske. We then give a direct method to compute a basis for H without using a relation matrix. The problem of computing a basis for some or all of the Sylow p-subgroups of an arbitrary finite abelian group G is addressed, yielding a Monte Carlo algorithm to compute the structure of G using O(|G|1/2) group operations. These results also improve generic algorithms for extracting pth roots in G.	basis (linear algebra);computation;direct method in the calculus of variations;discrete logarithm;generic programming;monte carlo algorithm;monte carlo method;pohlig–hellman algorithm	Andrew V. Sutherland	2011	Math. Comput.	10.1090/S0025-5718-10-02356-2	direct method;rank of an abelian group;discrete logarithm;combinatorics;number theory;genetic algorithm;numerical analysis;calculus;elementary abelian group;mathematics;abelian group;numerical linear algebra;linear system;group theory;monte carlo algorithm;monte carlo method;algebra	Crypto	81.22235125577693	20.44905050503853	77648
f729f3682b15aa439dc8fd83a5c6b3afff7cfcd4	the fast fourier transform method and ill-conditioned matrices	sistema lineal;linear algebra;funcion periodica;systeme equation;singular value decomposition method;decomposition valeur singuliere;qr method;numerical solution;conditionnement;ecuacion algebraica;singular value decomposition;simulation;periodic function;simulacion;fft;conditioning;linear system;fast fourier transform;methode qr;sistema ecuacion;ill conditioned matrix;fourier transformation;equation system;fonction periodique;transformation fourier;gaussian elimination;equation algebrique;decomposicion valor singular;acondicionamiento;error redondear;fast fourier transform method;systeme lineaire;computer simulation;algebraic equation;solution numerique;rounding error;transformacion fourier;erreur arrondi	We study the problem of ®nding numerical solutions of the linear algebraic equation, a x  b, where a denotes an N  ́ N ill-conditioned coecient matrix. It is well-known that Gaussian elimination methods coupled with pivoting strategies are ineective in this setting due to round-o error. We propose a new and simple application of the fast Fourier transform (FFT) method. Other viable methods, such as the QR method (QRM) or the singular value decomposition method (SVDM), have been proposed in the literature. The goal of this paper is to investigate the performance of the proposed method and compare it to other popular methods. The comparison is illustrated by computer simulation results using MATLAB. Ó 2001 Elsevier Science Inc. All rights reserved.	algebraic equation;artificial intelligence;computation;computer simulation;condition number;experiment;fast fourier transform;gaussian elimination;linear algebra;linear system;matlab;ncr cram;numerical analysis;qr algorithm;round-off error;singular value decomposition;time complexity	Boon Yi Soon;Paul W. Eloe;David Kammler	2001	Applied Mathematics and Computation	10.1016/S0096-3003(99)00171-X	computer simulation;fast fourier transform;mathematical analysis;pseudo-spectral method;linear algebra;calculus;discrete fourier transform;mathematics;geometry;algorithm;algebra	AI	81.87325928482123	20.639155921930154	77656
d64ebe49fa03fe516deea82e0b162f89be93b70e	a modified implementation of minres to monitor residual subvector norms for block systems		Saddle-point systems, i.e., structured linear systems with symmetric matrices are considered. A modified implementation of (preconditioned) MINRES is derived which allows subvectors of the residual to be monitored individually. Compared to the implementation from the textbook of [Elman, Silvester and Wathen, Oxford University Press, 2014], our method requires one extra vector of storage and no additional applications of the preconditioner. Numerical experiments are included.	experiment;generalized minimal residual method;linear system;preconditioner	Roland Herzog;Kirk M. Soodhalter	2017	SIAM J. Scientific Computing	10.1137/16M1093021	mathematical optimization;computer science;calculus;algorithm	HPC	82.41899346973197	22.316348858363952	77836
d3a3e8a87dcb78fa56294bace970244611724ce1	a generalized inexact uzawa method for stable principal component pursuit problem with nonnegative constraints	stable principal component pursuit;generalized inexact uzawa method;distributed processing;closed-form	The problem of recovering the low-rank and sparse components of a matrix is known as the stable principal component pursuit (SPCP) problem. It has found many applications in compressed sensing, image processing, and web data ranking. This paper proposes a generalized inexact Uzawa method for SPCP with nonnegative constraints. The main advantage of our method is that the resulting subproblems all have closed-form solutions and can be executed in distributed manners. Global convergence of the method is proved from variational inequalities perspective. Numerical experiments show that our algorithm converges to the optimal solution as other distributed methods, with better performances.	algorithm;compressed sensing;experiment;image processing;local convergence;loss function;numerical analysis;numerical method;optimization problem;performance;preconditioner;principal component analysis;pursuit-evasion;sparse matrix;variational inequality;variational principle	Kaizhan Huai;Mingfang Ni;Zhanke Yu;Xiang Zheng;Feng Ma	2017	Numerical Algorithms	10.1007/s11075-017-0333-x	mathematical optimization;combinatorics;mathematics;mathematical economics	ML	79.85575200609155	25.490030136545034	77837
9ea79d8ad61d8b04c0cf0a1a3f850494722f8489	parallel algorithms for certain matrix computations	control theory;eigenvalue problem;parallel algorithm;linear system;matrix computation;matrix equation	The complexity of performing matrix computations, such as solving a linear system, inverting a nonsingular matrix or computing its rank, has received a lot of attention by both the theory and the scientific computing communities. In this paper we address some “nonclassical” matrix problems that find extensive applications, notably in control theory. More precisely, we study the matrix equations Ax +XAT = C and Ax XB = C, the “inverse” of the eigenvalue problem (called pole assignment), and the problem of testing whether the matrix [B AB Anp’B] has full row rank. For these problems we show two kinds of PRAM algorithms: on one side very fast, i.e. polylog time, algorithms and on the other side almost linear time and processor efficient algorithms. In the latter case, the algorithms rely on basic matrix computations that can be performed efficiently also on realistic machine models.	computation;computational science;control theory;linear system;numerical stability;parallel algorithm;parallel computing;the matrix;time complexity	Bruno Codenotti;Biswa N. Datta;Karabi Datta;Mauro Leoncini	1997	Theor. Comput. Sci.	10.1016/S0304-3975(97)83810-8	matrix splitting;mathematical optimization;gaussian elimination;combinatorics;eigendecomposition of a matrix;sparse matrix;single-entry matrix;centrosymmetric matrix;computer science;generator matrix;theoretical computer science;band matrix;matrix of ones;convergent matrix;invertible matrix;mathematics;parallel algorithm;pascal matrix;numerical linear algebra;linear system;state-transition matrix;augmented matrix;matrix decomposition;block matrix;eigenvalue algorithm;algorithm;matrix;symmetric matrix;algebra	Theory	81.40468507683471	23.57035455500921	77940
aa26a8e4db6cb1cb329b080dd47d7f03558111d0	a monotone iterative technique for stationary and time dependent problems in banach spaces	sistema lineal;47hxx;solution maximale;monotone operator;iterative method;monotone operators;monotone iterative technique;analisis numerico;time dependent;computacion informatica;matematicas aplicadas;mathematiques appliquees;espacio banach;probleme cauchy;probleme non lineaire;banach space;non linear operator;operator method;nonlinear problems;linear system;analyse numerique;metodo iterativo;problema cauchy;numerical analysis;maximal solution;ciencias basicas y experimentales;methode iterative;partially ordered spaces;matematicas;65f08;algebra lineal numerica;algebre lineaire numerique;nonlinear problem;47j05;operateur non lineaire;operador no lineal;46bxx;numerical linear algebra;systeme lineaire;methode operateur;grupo a;iteration method;applied mathematics;46n10;cauchy problem;existence;47j25;65f10;41a65;espace banach;nonlinear operators;partial order	An abstract monotone iterative method is developed for operators between partially ordered Banach spaces for the nonlinear problem Lu=Nu and the nonlinear time dependent problem u^'=(L+N)u. Under appropriate assumptions on L and N we obtain maximal and minimal solutions as limits of monotone sequences of solutions of linear problems. The results are illustrated by means of concrete examples.	iterative method;stationary process;monotone	Mohamed A. El-Gebeily;Donal O'Regan;Juan J. Nieto	2010	J. Computational Applied Mathematics	10.1016/j.cam.2009.10.024	mathematical optimization;mathematical analysis;strongly monotone;calculus;mathematics;iterative method;algebra	Theory	81.32097248083893	19.170600456292043	77962
994d2814f1952c14b80609c1b017756f5cc04ac5	c-stationarity for optimal control of static plasticity with linear kinematic hardening	static plasticity;optimality conditions;mathematical programs with complementarity constraints;optimal control;49j40;90c33;74p10	An optimal control problem for the static problem of infinitesimal elastoplasticity with linear kinematic hardening is considered. The variational inequality arising on the lower-level is regularized using a Yosida-type approach, and an optimal control problem for the so-called viscoplastic model is obtained. Existence of a global optimizer is proved for both the regularized and original problems, and strong convergence of the solutions is established.	calculus of variations;mathematical optimization;optimal control;social inequality;stationary process;variational inequality	Roland Herzog;Christian Meyer;Gerd Wachsmuth	2012	SIAM J. Control and Optimization	10.1137/100809325	mathematical optimization;optimal control;calculus;control theory;mathematics;mathematical economics	Theory	72.6303869155818	20.99092671606605	78060
5b7fe4e7c0ff955a76edf992cf7ac3593afecbc6	a factorization approach to smoothing of hidden reciprocal models		Acausal signals are ubiquitous in science and engineering. These processes are usually indexed by space, instead of time. Similarly to Markov processes, reciprocal processes (RPs) are defined in terms of conditional independence relations, which imply a rich sparsity structure for this class of models. In particular, the smoothing problem for Gaussian RPs can be traced back to the problem of solving a linear system with a cyclic block tridiagonal matrix as coefficient matrix. In this paper we propose two factorization techniques for the solution of the smoothing problem for Gaussian hidden reciprocal models (HRMs). The first method relies on a clever split of the problem in two subsystems where the matrices to be inverted are positive definite block tridiagonal matrices. We can thus rely on the rich literature for this kind of sparse matrices to devise an iterative procedure for the solution of the problem. The second approach, applies to scalar valued stationary reciprocal processes, in which case the coefficient matrix becomes circulant tridiagonal (and symmetric), and is based on the direct factorization of the coefficient matrix into the product of a circulant lower bidiagonal matrix and a circulant upper bidiagonal matrix. The computational complexity of both algorithms scales linearly with the length of the observation interval.		Francesca P. Carli;Anna Caterina Carli	2018	2018 26th European Signal Processing Conference (EUSIPCO)	10.23919/EUSIPCO.2018.8553436		AI	81.5692859071904	23.16189969362168	78453
f48d20c08af2e9ae56e2ec2702cd7ef6b774ca56	improved eigenvalue shrinkage using weighted chebyshev polynomial approximation		We propose an eigenvalue shrinkage method with a modified Chebyshev polynomial approximation (CPA). The eigenvalue shrinkage has been used in many fields of signal and image processing. However, the shrinkage takes enormous computation time especially in the case that a matrix constructed from a signal or image becomes very large, i.e., eigendecomposition can hardly be performed. The CPA is an approximation method of the shrinkage function that avoids the eigendecomposition of the matrix. Unfortunately, it is known that the CPA generates Gibbs phenomenon around points of discontinuity for approximating an ideal response. The Chebyshev-Jackson polynomial approximation (CJPA) will alleviate the problem, but the transition bandwidth becomes wide, which is an undesired characteristic for some applications. In this paper, we propose an eigenvalue shrinkage method with the reduced Gibbs phenomenon by modifying the CPA using the weighted least squares approach. Our method can reduce the error as well as the CJPA. Furthermore, it yields the narrow transition band. Some experimental results on spectral clustering validate the effectiveness of the method.	approximation algorithm;chebyshev polynomials;cluster analysis;computation;image processing;jackson;least squares;polynomial;reflections of signals on conducting lines;spectral clustering;the matrix;time complexity	Masaki Onuki;Yuichi Tanaka;Masahiro Okuda	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7953016	mathematical optimization;spectral clustering;approximation theory;chebyshev polynomials;eigendecomposition of a matrix;eigenvalues and eigenvectors;gibbs phenomenon;polynomial;mathematical analysis;shrinkage;mathematics	Robotics	82.53363539297742	23.969281396161428	78473
599067fcb4b18dc0fc4af35b8024e91a7fcddb0b	on large-scale diagonalization techniques for the anderson model of localization	maximum weighted matching 65f15;sistema lineal;calcul scientifique;lanczos algorithm;preconditionnement;jacobi davidson algorithm;symmetric indefinite matrix;eigenvalue problem;decalaje;matematicas aplicadas;diagonalisation;maximo;factorisation incomplete;mathematiques appliquees;physique;65f05;valeur propre interieure;implementation;qc physics;qa mathematics;localization;diagonalizacion;grupo de excelencia;cullum willoughby implementation;probleme valeur propre;preconditioning;fisica;maximum;localizacion;maximum weight matching;diagonalization;82b44;matriz simetrica;decalage;linear system;eigenvector;metodo lanczos;68wxx;valor propio interior;symmetric matrix;eigenvalue;algorithme;vector propio;physics;algorithm;accuracy;large scale;computacion cientifica;14c20;numerical analysis;precision;localisation;lanczos method;quantum physics;ciencias basicas y experimentales;large scale eigenvalue problem;methode jacobi;matematicas;valor propio;methode lanczos;anderson model of localization;05c85;metodo jacobi;anderson model;matrice symetrique;precondicionamiento;symmetric system;valeur propre;shift;65f50;systeme lineaire;scientific computation;implementacion;applied mathematics;interior eigenvalue;65f15;systeme symetrique;multilevel preconditioning;65f10;vecteur propre;sistema simetrico;eigenvectors;jacobi method;problema valor propio;maximum weighted matching;algoritmo	We propose efficient preconditioning algorithms for an eigenvalue problem arising in quantum physics, namely, the computation of a few interior eigenvalues and their associated eigenvectors for large-scale sparse real and symmetric indefinite matrices of the Anderson model of localization. We compare the Lanczos algorithm in the 1987 implementation by Cullum and Willoughby with the shift-and-invert techniques in the implicitly restarted Lanczos method and in the Jacobi-Davidson method. Our preconditioning approaches for the shift-and-invert symmetric indefinite linear system are based on maximum weighted matchings and algebraic multilevel incomplete $LDL^T$ factorizations. These techniques can be seen as a complement to the alternative idea of using more complete pivoting techniques for the highly ill-conditioned symmetric indefinite Anderson matrices. We demonstrate the effectiveness and the numerical accuracy of these algorithms. Our numerical examples reveal that recent algebraic multilevel preconditioning solvers can accelerate the computation of a large-scale eigenvalue problem corresponding to the Anderson model of localization by several orders of magnitude.		Olaf Schenk;Matthias Bollhöfer;Rudolf A. Römer	2008	SIAM Review	10.1137/070707002	mathematical optimization;combinatorics;mathematical analysis;eigenvalues and eigenvectors;calculus;mathematics;geometry;algebra	Theory	82.89554672839655	21.458220620409765	78642
4e14f59b6374e7a2512082b8e7c412c0696bee21	constraint propagation on quadratic constraints	constrained optimization;quadratic programming;quadratic program;constraint propagation;rounding errors;rounding error control;continuous constraints;constraint programming;constraint satisfaction problem;quadratic constraint satisfaction problems;verified computation	This paper considers constraint propagation methods for continuous constraint satisfaction problems consisting of linear and quadratic constraints. All methods can be applied after suitable preprocessing to arbitrary algebraic constraints. The basic new techniques consist in eliminating bilinear entries from a quadratic constraint, and solving the resulting separable quadratic constraints by means of a sequence of univariate quadratic problems. Care is taken to ensure that all methods correctly account for rounding errors in the computations. Various tests and examples illustrate the advantage of the presented method.	bilinear filtering;computation;constraint logic programming;constraint satisfaction problem;linear algebra;local consistency;preprocessor;quadratic programming;quadratically constrained quadratic program;round-off error;rounding;software propagation	Ferenc Domes;Arnold Neumaier	2009	Constraints	10.1007/s10601-009-9076-1	constraint logic programming;mathematical optimization;constraint programming;combinatorics;discrete mathematics;quadratic residuosity problem;binary constraint;constraint satisfaction;second-order cone programming;constraint learning;computer science;constraint graph;quadratically constrained quadratic program;constraint satisfaction dual problem;mathematics;complexity of constraint satisfaction;constraint;constraint satisfaction problem;quadratic programming;hybrid algorithm;local consistency;backtracking	Vision	77.3719725596702	26.121857277057575	78702
d4ccedc6607ba730e43683981debfd03984d1d82	parallel multi-block admm with o(1 / k) convergence		This paper introduces a parallel and distributed extension to the alternating direction method of multipliers (ADMM) for solving convex problem: minimize f1(x1) + · · ·+ fN (xN ) subject to A1x1 + · · ·+ANxN = c, x1 ∈ X1, . . . , xN ∈ XN . The algorithm decomposes the original problem into N smaller subproblems and solves them in parallel at each iteration. This Jacobi-type algorithm is well suited for distributed computing and is particularly attractive for solving certain large-scale problems. This paper introduces a few novel results. Firstly, it shows that extending ADMM straightforwardly from the classic Gauss-Seidel setting to the Jacobi setting, from 2 blocks to N blocks, will preserve convergence if matrices Ai are mutually near-orthogonal and have full column-rank. Secondly, for general matrices Ai, this paper proposes to add proximal terms of different kinds to the N subproblems so that the subproblems can be solved in flexible and efficient ways, and the algorithm converges globally at a rate of o(1/k). Thirdly, a simple technique is introduced to improve some existing convergence rates from O(1/k) to o(1/k). In practice, some conditions in our convergence theorems are conservative. Therefore, we introduce a strategy for dynamically tuning the parameters in the algorithm, leading to substantial acceleration of the convergence in practice. Numerical results are presented to demonstrate the efficiency of the proposed method in comparison with several existing parallel algorithms. We implemented our algorithm on Amazon EC2, an on-demand public computing cloud, and report its performance on very large-scale basis pursuit problems with distributed data.	amazon elastic compute cloud (ec2);augmented lagrangian method;basis pursuit;cloud computing;convergence;convex optimization;distributed computing;gauss–seidel method;iteration;jacobian matrix and determinant;numerical method;parallel algorithm;performance tuning	Wei Deng;Ming-Jun Lai;Zhimin Peng;Wotao Yin	2017	J. Sci. Comput.	10.1007/s10915-016-0318-2	mathematical optimization;calculus;mathematics;geometry	ML	80.32802937679085	23.64106243234471	78856
4774ea825b6f3d03523bd72adc80232a52918a10	ranks of least squares solutions of the matrix equation axb=c	solvability condition;maximal rank;matrix rank method;matrix equation;necessary and sufficient condition;minimal rank;least square solution;least squares solution;generalized inverse;common solution	For a complex matrix equation AX B = C , we solve the following two problems: (1) the maximal and minimal ranks of least square solution X to AX B = C , and (2) the maximal and minimal ranks of two real matrices X0 and X1 in least square solution X = X0 + iX1 to AX B = C . We also give a necessary and sufficient condition for matrix equations Ai Xi Bi = Ci (i = 1, 2) to have a common least square solution. c © 2008 Elsevier Ltd. All rights reserved.	least squares;maximal set;microsoft dynamics ax;regular expression;the matrix	Yong Hui Liu	2008	Computers & Mathematics with Applications	10.1016/j.camwa.2007.06.023	mathematical optimization;combinatorics;mathematical analysis;generalized inverse;mathematics;matrix;algebra	AI	79.94122616318562	21.16055738098882	78984
03bcc75b017e9f4d25b18e3a65196878d44ae0b9	the long step rule in the bounded-variable dual simplex method: numerical experiments	metodo simplejo;numerical method;duality;simplex algorithm;simplex method;key words linear programming;optimisation combinatoire;dualite;programacion lineal;metodo numerico;mathematical programming;linear programming;programmation lineaire;linear program;dualidad;dual simplex method;numerical experiment;combinatorial optimization;programmation mathematique;methode simplexe;programacion matematica;methode numerique;optimizacion combinatoria	The dual simplex algorithm is the method of choice when linear programs have to be reoptimized after adding constraints or fixing variables. In this paper we discuss a modication of the standard dual simplex which allows for taking longer steps when proceeding from one dual feasible solution to the other. We describe this long step rule and present computational results on NETLIB and MIPLIB problems. Copyright Springer-Verlag Berlin Heidelberg 2002	duplex (telecommunications);experiment;numerical method;simplex algorithm	Ekaterina A. Kostina	2002	Math. Meth. of OR	10.1007/s001860200188	mathematical optimization;linear programming;calculus;mathematics;simplex algorithm;algorithm	ML	75.7392498851674	22.147851529809817	79012
f69ddae55801b8e5c7ff9d4b7ca5ebe5cf244093	approximating positive solutions of pbvps of nonlinear first order ordinary quadratic differential equations		Abstract   In this paper the authors prove the existence as well as approximations of the positive solutions for a periodic boundary value problem of first order ordinary nonlinear quadratic differential equations. An algorithm for the solutions is developed and it is shown that the sequence of successive approximations converges monotonically to the positive solution of related quadratic differential equations under some suitable mixed hybrid conditions. Our results rely on the Dhage iteration principle embodied in a recent hybrid fixed point theorem of Dhage (2014) in partially ordered normed linear spaces. A numerical example is also provided to illustrate the abstract theory developed in the paper.	nonlinear system	Bapurao C. Dhage;Shyam B. Dhage	2015	Appl. Math. Lett.	10.1016/j.aml.2015.02.023	mathematical optimization;mathematical analysis;discrete mathematics;mathematics	Vision	74.73126294966393	18.67505170147681	79053
68e06a2e97909a83a183d95b779a344fd4585a13	non-convex rank/sparsity regularization and local minima		This paper considers the problem of recovering either a low rank matrix or a sparse vector from observations of linear combinations of the vector or matrix elements. Recent methods replace the non-convex regularization with ℓ1 or nuclear norm relaxations. It is well known that this approach recovers near optimal solutions if a so called restricted isometry property (RIP) holds. On the other hand it also has a shrinking bias which can degrade the solution. In this paper we study an alternative non-convex regularization term that does not suffer from this bias. Our main theoretical results show that if a RIP holds then the stationary points are often well separated, in the sense that their differences must be of high cardinality/rank. Thus, with a suitable initial solution the approach is unlikely to fall into a bad local minimum. Our numerical tests show that the approach is likely to converge to a better solution than standard ℓ1/nuclear-norm relaxation even when starting from trivial initializations. In many cases our results can also be used to verify global optimality of our method.	converge;lagrangian relaxation;linear programming relaxation;matrix regularization;maxima and minima;numerical analysis;restricted isometry property;sparse matrix;stationary process	Carl Olsson;Marcus Carlsson;Fredrik Andersson;Viktor Larsson	2017	2017 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2017.44	mathematical optimization;restricted isometry property;artificial intelligence;low-rank approximation;linear combination;sparse matrix;matrix norm;pattern recognition;computer science;matrix (mathematics);regularization (mathematics);convex function	Vision	74.92836614416885	25.487533091327172	79067
b3b08cd7af16c4189aeba4ea7453e6095f74631d	substructure preconditioners for a class of structured linear systems of equations	metodo regularizacion;sistema lineal;metodo directo;equation non lineaire;preconditionnement;iterative method;computer aided analysis;optimal convergence;gmres;ecuacion no lineal;systeme equation;analisis numerico;problema mal planteado;regularisation;47a10;ecuacion trascendente;matematicas aplicadas;analyse assistee;modele mathematique;mathematiques appliquees;linear system of equations;65f05;fonction repartition;aproximacion optima;regularization method;ecuacion algebraica;probleme mal pose;methode regularisation;equation transcendante;ecuacion lineal;preconditioning;spectrum;modelo matematico;matrix inversion;eigenvalues;inversion matriz;linear system;eigenvector;analyse numerique;65j20;regularization;metodo iterativo;eigenvalue;vector propio;funcion distribucion;distribution function;14c20;sistema ecuacion;numerical analysis;optimal approximation;preconditioner;approximation optimale;methode iterative;transcendental equation;equation system;ill posed problem;valor propio;65f08;algebra lineal numerica;algebre lineaire numerique;convergence optimale;mathematical model;inversion matrice;65f22;analisis asistido;equation algebrique;precondicionamiento;valeur propre;regularizacion;numerical linear algebra;systeme lineaire;linear equation;non linear equation;applied mathematics;65h10;methode directe;spectrum distribution;algebraic equation;65f15;65h17;65f10;vecteur propre;direct method;equation lineaire	We proposed a substructure preconditioner for a class of structured linear system of equations. We show that a preconditioner with half of the constraint terms is able to make the preconditioned matrix have only three distinct eigenvalues. For some practical applications, a regularized variant is formulated, and the influence of the regularization parameter is analyzed. Numerical results show that the regularized variant is as efficient and is able to produce nearly optimal convergence behavior with a wide range of parameters. © 2010 Elsevier Ltd. All rights reserved.	linear system;matrix regularization;numerical linear algebra;preconditioner;system of linear equations	Jituan Zhou;Qiang Niu	2010	Mathematical and Computer Modelling	10.1016/j.mcm.2010.06.019	mathematical analysis;eigenvalues and eigenvectors;calculus;mathematics;preconditioner;geometry;algebra	AI	81.64535600419968	20.25718362582764	79424
054b616f6b94c494696e423373c70018f0241910	strict feasibility for generalized mixed variational inequality in reflexive banach spaces		The purpose of this paper is to investigate the nonemptiness and boundedness of solution set for a generalized mixed variational inequality with strict feasibility in reflexive Banach spaces. A concept of strict feasibility for the generalized mixed variational inequality is introduced, which recovers the existing concepts of strict feasibility for variational inequalities and complementarity problems. By using the equivalence characterization of nonemptiness and boundedness of the solution set for the generalized mixed variational inequality due to Zhong and Huang (J. Optim. Theory Appl. 147:454–472, 2010), it is proved that the generalized mixed variational inequality problem has a nonempty bounded solution set is equivalent to its strict feasibility.	social inequality;spaces;variational inequality;variational principle	Ren-you Zhong;Nan-jing Huang	2012	J. Optimization Theory and Applications	10.1007/s10957-011-9914-3	mathematical optimization;mathematical analysis;variational inequality;topology;mathematics	Logic	72.44149558840242	20.645352310351523	79660
c585c63d5e4ec70e139eb6dd164d183c239be85a	the spectrum of circulant-like preconditioners for some general linear multistep formulas for linear boundary value problems	linear system of time step integrator;15a18;trigonometric preconditioning;problema valor limite;analisis numerico;formule multipas;general linear multistep formulas in boundary value form;circulant matrix;boundary value problem;ecuacion algebraica;matrice non symetrique;ecuacion lineal;spectrum;matriz toeplitz;eigenvalues;linear system;multistep formula;analyse numerique;matrice circulante;eigenvalue;numerical analysis;trigonometric preconditioners;15a48;systeme lineaire integrateur pas temporel;valor propio;matrice toeplitz;algebra lineal numerica;matrice hermitienne;algebre lineaire numerique;linear systems of time step integrators;equation algebrique;valeur propre;boundary value problems;numerical linear algebra;matriz hermitiana;preconditionnement trigonometrique;linear equations;linear equation;nonsymmetric matrix;toeplitz matrix;probleme valeur limite;algebraic equation;65f15;nonsymmetric toeplitz matrices;hermitian matrix;65f10;equation lineaire;matriz circulante;toeplitz matrices;65n22	The spectrum of the eigenvalues, the conditioning, and other related properties of circulant-like matrices used to build up block preconditioners for the nonsymmetric algebraic linear equations of time-step integrators for linear boundary value problems are analyzed. Moreover, results concerning the entries of a class of Toeplitz matrices related to the latter are proposed. Generalizations of implicit linear multistep formulas in boundary value form are considered in more detail. It is proven that there exists a new class of approximations which are well conditioned and whose eigenvalues have positive and bounded real and bounded imaginary part. Moreover, it is observed that preconditioners based on other circulant-like approximations, which are well suited for Hermitian linear systems, can be severely ill conditioned even if the matrices of the nonpreconditioned system are well conditioned.	approximation;circulant matrix;imaginary time;linear algebra;linear equation;linear logic;linear system;preconditioner;toeplitz hash algorithm;type system	Daniele Bertaccini	2002	SIAM J. Numerical Analysis	10.1137/S0036142901397447	mathematical analysis;eigenvalues and eigenvectors;boundary value problem;calculus;mathematics;linear equation;algebra	Theory	80.52447328060404	20.034821977556298	80076
642fd054e1b0ccc4f6a4187a6e6814c1edc5dbbf	improved newton iteration for nonlinear matrix equations on quadratic lie groups	lie algebra;equation non lineaire;ecuacion no lineal;algebre groupe;algebra lie;equation quadratique;numerical method;methode newton;groupe lie;equation matricielle;newton iteration;accelerated test;variedad matematica;group algebra;algebre lie;convergence numerique;algorithme;numerical convergence;nonlinear matrix equations;algorithm;application exponentielle;ensayo acelerado;iteraccion;matrix equation;metodo numerico;grupo lie;ecuacion matricial;iteration;nonlinear equation;nonlinear matrix equation;lie group;metodo newton;newton method;algebra grupo;essai accelere;non linear equation;convergencia numerica;quadratic equation;methode type newton;methode numerique;ecuacion segundo grado;numerical method on manifold;variete mathematique;exponential map;algoritmo;manifold	In this paper we consider Newton iteration methods for solving nonlinear equations on matrix Lie groups. Recently, Owren and Welfert have proposed a method where the original nonlinear equation is transformed into a nonlinear equation on the Lie algebra of the group via the exponential map, thus Newton iteration methods may be applied. Based on this we suggest two improved variants of Newton iteration algorithm. One is that the exponential map would be approximated by Cayley map and give a Cayley version Newton iteration method for solving nonlinear equations on quadratic Lie groups, then we show that, the proposed method converges quadratically; Another is a variant of Newton type method with accelerated convergence and the numerical tests reported seem to support that it converges with cubically. 2006 Elsevier Inc. All rights reserved.	approximation algorithm;backward euler method;cubic function;iteration;iterative method;magma;newton;nonlinear system;numerical analysis;numerical integration;series acceleration;structural analysis;time complexity;vergence	Wencheng Li;Zichen Deng;Suying Zhang	2006	Applied Mathematics and Computation	10.1016/j.amc.2006.05.089	local convergence;lie algebra;mathematical optimization;steffensen's method;mathematical analysis;iteration;nonlinear system;newton fractal;calculus;mathematics;geometry;newton's method;algebra	AI	81.77290352204632	18.94517669919664	80078
927dae8fb34a88ae0d0da15c15318257bad78061	an improved bound for the nystrom method for large eigengap		We develop an improved bound for the approximation error of the Nyström method under the assumption that there is a large eigengap in the spectrum of kernel matrix. This is based on the empirical observation that the eigengap has a significant impact on the approximation error of the Nyström method. Our approach is based on the concentration inequality of integral operator and the theory of matrix perturbation. Our analysis shows that when there is a large eigengap, we can improve the approximation error of the Nyström method from O(N/m) to O(N/m) when measured in Frobenius norm, where N is the size of the kernel matrix, and m is the number of sampled columns.	approximation error;column (database);matrix multiplication;nyström method;social inequality	Mehrdad Mahdavi;Tianbao Yang;Rong Jin	2012	CoRR		mathematical optimization;combinatorics;mathematical analysis;mathematics	ML	77.76264177223258	18.50069568842528	80097
2107b4551870be844e2fee16099dc1f695f12db4	a reduced hessian method for large-scale constrained optimization	constrained optimization;superlinear convergence;quadratic programming;general and miscellaneous mathematics computing and information science;convergence;numerical solution;quasi newton;degree of freedom;mathematical logic;nonlinear problems;65;finite difference method;numerical solution 990200 mathematics computers;optimization problem;iterative methods;factorization;large scale;matrices;calculation methods;nonlinear problem;algorithms;quasi newton method;optimization;newton method;sparse matrix;49;iteration method;successive quadratic programming;reduced hessian methods;mathematical optimization;large scale optimization	We propose a quasi-Newton algorithm for solving large optimization problems with nonlinear equality constraints. It is designed for problems with few degrees of freedom, and is motivated by the need to use sparse matrix factorizations. The algorithm incorporates a correction vector that approximates the cross term Z[sup T]WY[sub PY] in order to estimate the curvature in both the range and null spaces of the constraints. The algorithm can be considered to be, in some sense, a practical implementation of an algorithm of Coleman and Conn. We give conditions under which local and superlinear convergence is obtained.	constrained optimization;hessian;program optimization	Lorenz T. Biegler;Jorge Nocedal;Claudia Schmid	1995	SIAM Journal on Optimization	10.1137/0805017	mathematical optimization;constrained optimization;combinatorics;mathematical analysis;mathematics;iterative method;quadratic programming	ML	79.26832930108472	23.998920169152573	80673
01bf3dad4cb2e9ece114adcf19193eae60128fc1	power system restoration with transient stability	conference paper	We address the problem of power system restoration after a significant blackout. Prior work focus on optimization methods for finding high-quality restoration plans. Optimal solutions consist in a sequence of grid repairs and corresponding steady states. However, such approaches lack formal guarantees on the transient stability of restoration actions, a key property to avoid additional grid damage and cascading failures. In this paper, we show how to integrate transient stability in the optimization procedure by capturing the rotor dynamics of power generators. Our approach reasons about the differential equations describing the dynamics and their underlying transient states. The key contribution lies in modeling and solving optimization problems that return stable generators dispatch minimizing the difference with respect to steady states solutions. Computational efficiency is increased using preprocessing procedures along with traditional reduction techniques. Experimental results on existing benchmarks confirm the feasibility of the new approach.	benchmark (computing);cascading failure;circuit restoration;computation;dynamic dispatch;experiment;ian cullimore;mathematical optimization;nonlinear system;numerical linear algebra;preprocessor;r.o.t.o.r.;reduction (complexity);scalability;steady state;the australian	Hassan L. Hijazi;Terrence W. K. Mak;Pascal Van Hentenryck	2015			simulation;computer science	AI	72.19943821955682	29.17443368809865	80715
637259dfd3db240e3aece80f97c1897d4305f80b	fibonacci polynomials and sylvester determinant of tridiagonal matrix	equation non lineaire;ecuacion no lineal;analisis numerico;fibonacci polynomials;ecuacion trascendente;matematicas aplicadas;mathematiques appliquees;ecuacion algebraica;equation transcendante;polynomial;eigenvector;analyse numerique;tridiagonal matrix;eigenvalue;vector propio;65f40;numerical analysis;transcendental equation;polinomio;determinante;valor propio;matriz tridiagonal;algebra lineal numerica;algebre lineaire numerique;determinant;equation algebrique;valeur propre;numerical linear algebra;sylvester determinant;non linear equation;applied mathematics;polynome;algebraic equation;65f15;matrice tridiagonale;65h17;vecteur propre;eigenvectors	By means of left eigenvector method, we evaluate the determinant of a tridiagonal matrix, which extends the determinant due to Sylvester [5].	fibonacci polynomials;polynomial;sylvester matrix	Wenchang Chu	2010	Applied Mathematics and Computation	10.1016/j.amc.2010.01.089	jacobian matrix and determinant;sylvester matrix;tridiagonal matrix;combinatorics;mathematical analysis;determinant;eigenvalues and eigenvectors;sylvester's law of inertia;sylvester equation;mathematics;hadamard's maximal determinant problem;tridiagonal matrix algorithm;algebra	Theory	78.8358184631403	19.389972797282727	80845
675ef78a48627f994a90200afee063dcf15706c1	weak and strong solutions for fuzzy linear programming problems		This paper discusses some feasibility conditions for fuzzy linear programming problems. The selection of different membership functions in a fuzzy linear programming problem can lead to different solutions, including unbounded and infeasible solutions, so in this paper we generalize concepts of weak and strong solutions for this kind of problems. An application example is provided to illustrate our results.	linear programming	Juan Carlos Figueroa-García;Germán Hernández-Pérez	2017		10.1007/978-3-319-67137-6_42	machine learning;fuzzy logic;artificial intelligence;computer science;linear-fractional programming;mathematical optimization;linear programming	Theory	71.5622121571676	21.951296736274617	80848
24c3dc3e00b31725b7a38748ea548a64fb23f0d5	global optimal solution to discrete value selection problem with inequality constraints		This paper presents a canonical dual method for solving a quadratic discrete value selection problem subjected to inequality constraints. The problem is first transformed into a problem with quadratic objective and 0-1 integer variables. The dual problem of the 0-1 programming problem is thus constructed by using the canonical duality theory. Under appropriate conditions, this dual problem is a maximization problem of a concave function over a convex continuous space. Numerical simulation studies, including some large scale problems, are carried out so as to demonstrate the effectiveness and efficiency of the method proposed.	concave function;convex optimization;dual polyhedron;duality (optimization);duality gap;entropy maximization;expectation–maximization algorithm;loss function;mathematical optimization;np-hardness;quadratic function;selection algorithm;simulation;social inequality;the australian	Ning Ruan;David Yang Gao	2012	CoRR		optimization problem;mathematical optimization;combinatorics;mathematical analysis;function problem;counting problem;duality;nonlinear programming;generalized assignment problem;cutting stock problem;constraint satisfaction dual problem;mathematics;quadratic programming	ML	71.34446831879877	22.58460305067973	81211
545f18e63076c61250bf3e250dacb8c855d02932	l1 solution of overdetermined systems of linear equations	overdetermined system;simplex algorithm;simplex method;linear program;linear equations	In a previous paper by the author, an algorithm for calculating the LI solution of an overdeterrmned system of hnear equations was given. This algorithm applies a dual simplex method to the linear programming formulation of the L1 approximation problem. For the purpose of numermal stability, the present paper uses a triangular decomposition to the basis matrix. This decomposltmn method is a new one and is particularly suitable to this algorithm. Numerical results show that the present method compares favorably with the best existmg algorithms.	approximation;duplex (telecommunications);linear equation;linear programming formulation;numerical method;numerical stability;simplex algorithm;system of linear equations;triangular decomposition	Nabih N. Abdelmalek	1980	ACM Trans. Math. Softw.	10.1145/355887.355894	system of linear equations;mathematical optimization;mathematical analysis;linear programming;coefficient matrix;calculus;mathematics;revised simplex method;simplex algorithm;overdetermined system	Graphics	77.47911618221957	22.26258066182447	81215
b2753f4b03b9ae12a9aeaa99a553f9c03a90c1a6	function of a square matrix with repeated eigenvalues	linear algebra;15xx;analisis numerico;companion matrix;matriz cuadrada;similarity matrix transformation;matematicas aplicadas;krylov matrix;mathematiques appliquees;fonction analytique;jordan matrix;fundamental formula;funcion arbitraria;eigenvalues;matrice carree;analyse numerique;eigenvalue;square matrix;constituent matrices;numerical analysis;algebre lineaire;vandermonde matrix;valor propio;funcion matricial;matrix function;algebra lineal;fonction matricielle;funcion analitica;15a15;valeur propre;arbitrary function;functions of matrices;applied mathematics;fonction arbitraire;analytical function;13h15;analytic function	"""An analytical function f(A) of an arbitrary nxn constant matrix A is determined and expressed by the ''fundamental formula'', the linear combination of constituent matrices. The constituent matrices Z""""k""""h, which depend on A but not on the function f(s), are computed from the given matrix A, that may have repeated eigenvalues. The associated companion matrix C and Jordan matrix J are then expressed when all the eigenvalues with multiplicities are known. Several other related matrices, such as Vandermonde matrix V, modal matrix W, Krylov matrix K and their inverses, are also derived and depicted as in a 2-D or 3-D mapping diagram. The constituent matrices Z""""k""""h of A are thus obtained by these matrices through similarity matrix transformations. Alternatively, efficient and direct approaches for Z""""k""""h can be found by the linear combination of matrices, that may be further simplified by writing them in ''super column matrix'' forms. Finally, a typical example is provided to show the merit of several approaches for the constituent matrices of a given matrix A."""		Feng Cheng Chang	2005	Applied Mathematics and Computation	10.1016/j.amc.2003.12.003	logarithm of a matrix;matrix splitting;matrix analysis;matrix function;combinatorics;eigendecomposition of a matrix;identity matrix;nonnegative matrix;eigenvalues and eigenvectors;matrix multiplication;single-entry matrix;centrosymmetric matrix;analytic function;linear algebra;calculus;square matrix;mathematics;diagonalizable matrix;exchange matrix;state-transition matrix;augmented matrix;square root of a 2 by 2 matrix;block matrix;matrix;integer matrix;algebra;involutory matrix	Theory	78.59484147049763	20.689898253209407	81241
bc741dd681cf469cddad8c645c7b58076599b862	an efficient sparse optimization method for unfinished magic squares		Magic square is an old and interesting mathematical problem, which has the same value of all the sums of the elements in each row, column and diagonal. An unfinished magic square denotes that it gives us some clues and we need to fill the empty cells. In order to solve the unfinished magic squares more efficiently, we propose a solution based on sparse optimization. Using the properties of magic square, we establish a model of constraint programming. Then we transform the constraints into sparse linear constraints, meanwhile use l 0 norm minimization as the objective function to ensure the sparsity of the solution. Moreover, we use l 1 norm to approximate l 0 norm on the basis of RIP and KGG condition. This paper uses the primal-dual interior point method of linear programming, the branch and bound algorithm of binary programming and dual simplex method of integer linear programming to solve the magic square problems. The experimental results show that dual simplex method of integer linear programming can reach almost 100 % success rate. In addition, we propose a kind of special magic square problem and we apply this idea to construct and solve this problem, and obtain the good results.	sparse	Yali Liang;Xiao-hua Xu;Zheng Liao;Ping He	2016		10.1007/978-3-319-46257-8_4	mathematical optimization;computer science;theoretical computer science;sparse approximation;algorithm	Vision	76.56228346010872	25.21210037767407	81242
671d49f523cd42b55bb646faed2f65837c819f62	finding a strict feasible solution of a linear semidefinite program	analisis numerico;matematicas aplicadas;semidefinite programming;mathematiques appliquees;projective interior point method;methode point interieur;analyse numerique;temps calcul;numerical analysis;programacion lineal;metodo punto interior;linear programming;programmation lineaire;linear program;numerical experiment;tiempo computacion;computation time;applied mathematics;interior point method;semidefinite program	This study deals with the performance of projective interior point methods for linear semidefinite program. We propose a modification in the initialization phases of the method in order to reduce the computation time.#R##N##R##N#This purpose is confirmed by numerical experiments showing the efficiency which are presented in the last section of the paper.	semidefinite programming	Djamel Benterki;Abdelkrim Keraghel	2011	Applied Mathematics and Computation	10.1016/j.amc.2010.12.083	mathematical optimization;numerical analysis;linear programming;interior point method;calculus;quadratically constrained quadratic program;mathematics;geometry;semidefinite embedding;semidefinite programming	Theory	75.83488287389264	22.317667865510277	81315
74aed53d8bcb1b2cc478213d9792edd8a72c3181	interior-point methods for cartesian p *(κ)-linear complementarity problems over symmetric cones based on the eligible kernel functions	cartesian p k property;cartesian p κ property;selected works;bepress selected works;euclidean jordan algebra;kernel function;euclidean jordan algebras and symmetric cones;polynomial complexity;barrier and kernel functions;linear complementarity problem euclidean jordan algebras and symmetric cones cartesian p k property barrier functions kernal functions interior point method polynomial complexity;optimization problem;90c51;90c33;bepress;kernal functions;linear complementarity problem;interior point method;barrier functions;large classes;barrier function	An interior-point method IPM for Cartesian  P *κ-linear complementarity problems over symmetric cones SCLCP is analysed and the complexity results are presented. The Cartesian  P *κ-SCLCPs have been recently introduced as the generalization of the more commonly known and more widely used monotone-SCLCPs. The IPM is based on the barrier functions that are defined by a large class of univariate functions called eligible kernel functions, which have recently been successfully used to design new IPMs for various optimization problems. Eligible barrier kernel functions are used in calculating the Nesterov–Todd search directions and the default step-size which lead to very good complexity results for the method. For some specific eligible kernel functions, we match the best-known iteration bound for the long-step methods while for the short-step methods the best iteration bound is matched for all cases.	cartesian closed category;complementarity theory	Goran Lesaja;Guo-Qiang Wang;D. T. Zhu	2012	Optimization Methods and Software	10.1080/10556788.2012.670858	kernel;optimization problem;mathematical optimization;combinatorics;mathematical analysis;barrier function;interior point method;mathematics;linear complementarity problem	Logic	74.23704934822463	22.678993239021914	81375
cd467a4e3d7dc40504699984c5e1a27c6a2c3f6b	conic systems and sublinear mappings: equivalent approaches	90c31;transfer scheme;data perturbations;sublinear mappings;90c25;65f35;conic systems;15a12;distance to ill posedness	It is known that linear conic systems are a special case of setvalued sublinear mappings. Hence the latter subsumes the former. In this note we observe that linear conic systems also contain set-valued sublinear mappings as a special case. Consequently, the former also subsumes the latter.	time complexity	Javier Peña	2004	Oper. Res. Lett.	10.1016/j.orl.2003.11.006	mathematical optimization;mathematical analysis;discrete mathematics;mathematics;sublinear function	DB	72.69877740995442	18.636080163284987	81616
93de1c5eb989e399a6ce2c6d41a03f2db922e0ae	relaxed forms of bbk algorithm and fbp algorithm for symmetric indefinite linear systems	symmetric indefinite matrices;symmetric pivoting;linear system;fbp;direct solvers;bbk	In this paper, we study the direct solvers for the linear system Ax=b, where A is symmetric and indefinite. We discuss the so-called BBK algorithm and FBP algorithm and propose relaxed forms of them which provide options for fast pivot selection. We also present some numerical tests to show the efficiency of our algorithms.	algorithm;flow-based programming;lagrangian relaxation;linear system;serializability	Ting-Zhu Huang;Liang Li	2008	Computers & Mathematics with Applications	10.1016/j.camwa.2007.04.030	mathematical optimization;mathematical analysis;discrete mathematics;control theory;mathematics;linear system	EDA	80.47335875457274	22.91688307867401	81633
3932337c79468d69008f9b7ba881e4cc2178435c	tensor completion via a multi-linear low-n-rank factorization model	tensor completion;singular value decomposition;journal;nonlinear gauss seidal method;期刊论文;multi linear low n rank factorization	The tensor completion problem is to recover a low-n-rank tensor from a subset of its entries. The main solution strategy has been based on the extensions of trace norm for the minimization of tensor rank via convex optimization. This strategy bears the computational cost required by the singular value decomposition (SVD) which becomes increasingly expensive as the size of the underlying tensor increase. In order to reduce the computational cost, we propose a multi-linear low-n-rank factorization model and apply the nonlinear Gauss–Seidal method that only requires solving a linear least squares problem per iteration to solve this model. Numerical results show that the proposed algorithm can reliably solve a wide range of problems at least several times faster than the trace norm minimization algorithm. & 2014 The Authors. Published by Elsevier B.V. All rights reserved.	algorithm;algorithmic efficiency;computation;computational complexity theory;convex optimization;experiment;iteration;linear least squares (mathematics);mathematical optimization;nonlinear system;numerical method;singular value decomposition	Huachun Tan;Bin Cheng;Wuhong Wang;Yu-Jin Zhang;Bin Ran	2014	Neurocomputing	10.1016/j.neucom.2013.11.020	mathematical optimization;combinatorics;discrete mathematics;rank;tensor;mathematics;singular value decomposition	AI	80.11961368494333	23.636992650389164	81777
d85673110ed6a63a48dba5f014cf120587a9f368	an iterative algorithm for the generalized center symmetric solutions of a class of linear matrix equation and its optimal approximation	constraint matrix equation;generalized centrosymmetric solution;iterative algorithm;optimal approximation	For any symmetric orthogonal matrix P, i.e., PT=P, PTP =I, the matrix X is said to be a generalized centrosymmetric matrix if PXP = X for any matrix X. The conjugate gradient iteration algorithm is presented to find the generalized centrosymmetric solution and its optimal approximation of the constraint matrix equation AXB + CXD = F. By this method, the solvability of the equation can be determined automatically. If the matrix equation AXB + CXD = F is consistent, then its generalized centrosymmetric solution can be obtained within finite iteration steps in the absence of round off errors for any initial symmetric matrix X1 and generalized centrosymmetric solution with the least norm can be derived by choosing a proper initial matrix. In addition, the optimal approximation solution for a given matrix of the matrix equation AXB + CXD = F can be obtained by choosing the generalized centrosymmetric solution with the least norm of a new matrix equation AX̃B + CX̃D = F̃. © Springer-Verlag Berlin Heidelberg 2013.	algorithm;approximation;iterative method	Jie Liu;Qingchun Li	2013		10.1007/978-3-642-37502-6_19	cuthill–mckee algorithm;mathematical optimization;combinatorics;mathematical analysis;eigenvalues and eigenvectors;mass matrix;mathematics;generalized linear array model	Theory	80.1686587737886	21.77896416118482	81879
3199578db41c1470f27cfd4b934bc2c2e4a3471e	interval methods for solving underdetermined nonlinear systems		The problem of enclosing all solutions of an underdetermined system of equations is considered. A few variants of the algorithm to solve this problem are compared – some of the features come from the literature and some are original. The paper discusses both implementational and theoretical issues of the problem, including a useful theorem that is proved. Shared-memory parallelization, using OpenMP is also considered and numerical results for proper test problems are presented.	algorithm;interval arithmetic;nonlinear system;numerical analysis;openmp;parallel computing;shared memory	Bartlomiej Jacek Kubica	2011	Reliable Computing		discrete mathematics;mathematical optimization;mathematics;nonlinear system;underdetermined system	HPC	76.97844475264685	20.522597058838752	81947
ad2b9b3a73b40615c66f913683e680e5a11a595d	block gmres method with inexact breakdowns and deflated restarting	block inexact rank deficiency;块gmres方法;15a06;期刊论文;抑制重启技术;block gmres;65f10;deflated restarting;块不精确秩亏损;65n22	We consider the solution of large linear systems with multiple right-hand sides using a block GMRES approach. We introduce a new algorithm that effectively handles the situation of almost rank deficient block generated by the block Arnoldi procedure and that enables the recycling of spectral information at restart. The first feature is inherited from an algorithm introduced by Robbé and Sadkane [M. Robbé and M. Sadkane. Exact and inexact breakdowns in the block GMRES method. Linear Algebra and its Applications, 419: 265-285, 2006.], while the second one is obtained by extending the deflated restarting strategy proposed by Morgan [R. B. Morgan. Restarted block GMRES with deflation of eigenvalues. Applied Numerical Mathematics, 54(2): 222-236, 2005.]. Through numerical experiments, we show that the new algorithm combines efficiently the attractive numerical features of its two parents that it outperforms. Key-words: Block GMRES, multiple right-hand sides, numerical rank deficiency, subspace augmentation, harmonic Ritz values ∗ Inria, CNRS (LaBRI UMR 5800) and Université de Bordeaux † School of Mathematical Sciences/Institute of Computational Science, University of Electronic Science and Technology of China, Chengdu, Sichuan, 611731, P. R. China (yanfeijing@uestc.edu.cn or 00jyfvictory@163.com). This author was supported by INRIA fund, NSFC (61170311, 11201055), Chinese Universities Specialized Research Fund for the Doctoral Program (20120185120026), the State Scholarship Fund, the Fundamental Research Funds for the Central Universities. GMRES par bloc avec inexact breakdowns et deflation au restart Résumé : Nous considérons la résolution de systèmes linéaires avec second-membres multiplques par une approche GMRES par bloc. Nous introduisons un nouvel algorithme qui gère efficacement d’une part la situation de perte de rang numérique dans les blocs générés par la méthode d’Arnoldi d’autre part le recyclage d’information spectrale au redemarrage via une technique d’augmentation d’espace. La première propriété est hérité de l’algorithme introduit par Robbé and Sadkane [M. Robbé and M. Sadkane. Exact and inexact breakdowns in the block GMRES method. Linear Algebra and its Applications, 419: 265-285, 2006.], et la seconde est obtenue en étendant la stratégie de redémmarage proposée par Morgan [R. B. Morgan. Restarted block GMRES with deflation of eigenvalues. Applied Numerical Mathematics, 54(2): 222-236, 2005.]. Via des expérimentations numériques, nous montrons que ce nouvel algorithme combine efficacement les deux propriétés de ces deux parents dont il améliore les performances. Mots-clés : bloc GMRES, second-membres multiples, perte de rang numérique, augmentation de sous-espace, valeurs de Ritz harmoniques Block GMRES-DR method with inexact breakdowns 3	algorithm;angular defect;arnoldi iteration;computation;computational science;espace;experiment;generalized minimal residual method;linear algebra;linear system;nokia 5800 xpressmusic;numerical analysis;performance	Emmanuel Agullo;Luc Giraud;Yan-Fei Jing	2014	SIAM J. Matrix Analysis Applications	10.1137/140961912	mathematical optimization;combinatorics;mathematics;algorithm	Logic	82.57133521454936	21.707044102102945	82017
1267fe36b5ece49a9d8f913eb67716a040bbcced	on the limited memory bfgs method for large scale optimization	optimisation;systeme grande taille;uniformly convex;optimizacion;funcion no lineal;combined cycle;implementation;performance;non linear function;global convergence;large scale system;conjugate gradient method;metodo cuasi newton;ejecucion;large scale;methode quasi newton;metodo gradiente conjugado;fonction non lineaire;quasi newton method;optimization;rendimiento;methode gradient conjugue;nonlinear optimization;sistema gran escala;large scale optimization	We study the numerical performance of a limited memory quasi Newton method for large scale optimization which we call the L BFGS method We compare its performance with that of the method developed by Buckley and LeNir which combines cyles of BFGS steps and conjugate direction steps Our numerical tests indicate that the L BFGS method is faster than the method of Buckley and LeNir and is better able to use additional storage to accelerate convergence We show that the L BFGS method can be greatly accelerated by means of a simple scaling We then compare the L BFGSmethod with the partitioned quasi Newton method of Griewank and Toint a The results show that for some problems the partitioned quasi Newton method is clearly superior to the L BFGS method However we nd that for other problems the L BFGS method is very competitive due to its low iteration cost We also study the convergence properties of the L BFGS method and prove global convergence on uniformly convex problems	broyden–fletcher–goldfarb–shanno algorithm;image scaling;iteration;limited-memory bfgs;local convergence;mathematical optimization;newton;newton's method;numerical analysis;uniformly convex space	Dong C. Liu;Jorge Nocedal	1989	Math. Program.	10.1007/BF01589116	mathematical optimization;combined cycle;quasi-newton method;performance;nonlinear programming;calculus;mathematics;conjugate gradient method;implementation;algorithm	AI	77.18386203410846	23.4974533010622	82152
c623327660cc6bcf675a8cfb5fee2af1d318beb1	analysis of a novel preconditioner for a class of p-level lower rank extracted systems	preconditioned conjugate gradient method;integral equation;partial dierential equation;convolution;integral equations;circulant matrix;spectrum;conjugate gradient method;linear system;preconditioner;inverse problem;image reconstruction;time series data;numerical approximation;non rectangular domains;toeplitz matrix;preconditioned conjugate gradient	This paper proposes and studies the performance of a preconditioner suitable for solving a class of symmetric positive de nite systems, Âx = b, which we call p-level lower rank extracted systems (p-level LRES), by the preconditioned conjugate gradient method. The study of these systems is motivated by the numerical approximation of integral equations with convolution kernels de ned on arbitrary p-dimensional domains. This is in contrast to p-level Toeplitz systems which only apply to rectangular domains. The coe cient matrix, Â, is a principal submatrix of a p-level Toeplitz matrix, A, and the preconditioner for the preconditioned conjugate gradient algorithm is provided in terms of the inverse of a p-level circulant matrix constructed from the elements of A. The preconditioner is shown to yield clustering in the spectrum of the preconditioned matrix which leads to a substantial reduction in the computational cost of solving LRE systems. Copyright ? 2006 John Wiley & Sons, Ltd.	algorithmic efficiency;approximation;circulant matrix;cluster analysis;conjugate gradient method;convolution;john d. wiley;numerical analysis;preconditioner;toeplitz hash algorithm	Srinivasa M. Salapaka;A. Peirce	2006	Numerical Lin. Alg. with Applic.	10.1002/nla.468	mathematical optimization;mathematical analysis;calculus;mathematics;integral equation;algebra	HPC	82.72238747586981	21.944438231695536	82166
04dd40baf85a9271fb815171d990de77f1180fc3	solvability of newton equations in smoothing-type algorithms for the soccp	jacobian matrix;equation non lineaire;ecuacion no lineal;systeme equation;analisis numerico;smoothing type algorithm;matrice jacobi;ecuacion trascendente;computacion informatica;matematicas aplicadas;90c26;mathematiques appliquees;equation ordre 2;ecuacion algebraica;equation transcendante;90c30;optimization method;second order equation;metodo optimizacion;solvability of newton equations;analyse numerique;second order cone complementarity problem;algorithme;resolucion problema;optimization problem;algorithm;jacobi matrix;smoothing methods;sistema ecuacion;numerical analysis;ciencias basicas y experimentales;transcendental equation;smoothing;matriz jacobi;methode lissage;equation system;matematicas;funcion matricial;matrix function;invertibility;methode optimisation;alisamiento;90c33;fonction matricielle;equation algebrique;15a15;ecuacion orden 2;65f60;grupo a;non linear equation;applied mathematics;65h10;algebraic equation;lissage;invertibilite;problem solving;resolution probleme;algoritmo	"""In this paper, we first investigate the invertibility of a class of matrices. Based on the obtained results, we then discuss the solvability of Newton equations appearing in the smoothing-type algorithm for solving the second-order cone complementarity problem (SOCCP). A condition ensuring the solvability of such a system of Newton equations is given. In addition, our results also show that the assumption that the Jacobian matrix of the function involved in the SOCCP is a P""""0-matrix is not enough for ensuring the solvability of such a system of Newton equations, which is different from the one of smoothing-type algorithms for solving many traditional optimization problems in @?^n."""	algorithm;newton;smoothing	Nan Lu;Zheng-Hai Huang	2011	J. Computational Applied Mathematics	10.1016/j.cam.2010.10.025	jacobian matrix and determinant;mathematical optimization;mathematical analysis;calculus;mathematics;algorithm;algebra	ML	81.6667332302366	18.922856642267707	82356
a8ec890a1077b810072974d10b7cca1c430dfdad	indefinite trust region subproblems and nonsymmetric eigenvalue perturbations	indefinite inner product;indefinite trust region subproblems;hard case;90c30;65k10;eigenvalues;trust region;65k05;nonsymmetric eigenvalue perturbation theory;matrix pencils;existence of solution;numerical solutions;existence and optimality conditions;matrix pencil;upper and lower bounds;49m37	1 This paper extends the theory of trust region subproblems in two ways: (i) it allows indeenite inner products in the quadratic constraint and (ii) it uses a two sided (upper and lower bound) quadratic constraint. Characterizations of optimality are presented, which have no gap between necessity and suuciency. Conditions for the existence of solutions are given in terms of the deeniteness of a matrix pencil. A simple dual program is introduced which involves the maximiza-tion of a strictly concave function on an interval. This dual program simpliies the theory and algorithms for trust region subproblems. It also illustrates that the trust region subproblems are implicit convex programming problems, and thus explains why they are so tractable. The duality theory also provides connections to eigenvalue perturbation theory. Trust region subproblems with zero linear term in the objective function correspond to eigenvalue problems, and adding a linear term in the objective function is seen to correspond to a perturbed eigenvalue problem. Some eigenvalue interlacing results are presented.	algorithm;cobham's thesis;concave function;convex optimization;eigenvalue perturbation;interlacing (bitmaps);loss function;optimization problem;perturbation theory;quadratically constrained quadratic program;term (logic);trust region	Ronald J. Stern;Henry Wolkowicz	1995	SIAM Journal on Optimization	10.1137/0805016	mathematical optimization;combinatorics;mathematical analysis;overlapping subproblems;matrix pencil;eigenvalues and eigenvectors;mathematics;trust region;upper and lower bounds	ML	71.70063398543613	22.06887322454091	82414
d8ab30e5aa45d9c9a3a94784734104e33f0ce4bf	extending the gauss-huard method for the solution of lyapunov matrix equations and matrix inversion	linear systems;cpu gpu computing;gauss huard method;matrix inversion;lr adi solver;lyapunov matrix equations		lyapunov fractal	Peter Benner;Pablo Ezzatti;Enrique S. Quintana-Ortí;Alfredo Remón	2017	Concurrency and Computation: Practice and Experience	10.1002/cpe.4076	matrix splitting;matrix function;mathematical optimization;gaussian elimination;eigendecomposition of a matrix;sparse matrix;theoretical computer science;band matrix;coefficient matrix;convergent matrix;linear system;state-transition matrix;augmented matrix;matrix decomposition;matrix;symmetric matrix	HPC	81.16640637027375	23.37076618776462	82418
7d748fad52c1f782170f6f0c345127703916a8a2	dual convergence of the proximal point method with bregman distances for linear programming	bregman distance;convergence rate;linear constraint;linear program;proximal point method	In this article, we consider the proximal point method with Bregman distance applied to linear programming problems, and study the dual sequence obtained from the optimal multipliers of the linear constraints of each subproblem. We establish the convergence of this dual sequence, as well as convergence rate results for the primal sequence, for a suitable family of Bregman distances. These results are obtained by studying first the limiting behavior of a certain perturbed dual path and then the behavior of the dual and primal paths.	bregman divergence;linear programming;rate of convergence	João X. da Cruz Neto;Orizon Pereira Ferreira;Alfredo N. Iusem;Renato D. C. Monteiro	2007	Optimization Methods and Software	10.1080/10556780600565083	mathematical optimization;combinatorics;mathematical analysis;linear programming;mathematics;rate of convergence;bregman divergence	ML	73.80013926381183	23.46573391212797	82737
27c35344c2f110d779cd6c6851ce6c47ba8d07fe	numerical solution for the fredholm integral equation of the second kind with toeplitz kernels by using preconditioners	preconditionnement;matriz bloque;analisis numerico;integral primera;quadrature;integral equation;equation integrale fredholm;fredholm integral equations;numerical solution;approximation numerique;integracion numerica;methode noyau;matrice coefficient;preconditioning;positive definite;matriz toeplitz;eigenvalues;gran sistema;equation fredholm;aproximacion numerica;analyse numerique;preconditioners;eigenvalue;numerical analysis;matrice definie positive;ecuacion fredholm;positive definite matrix;matrice bloc;large system;numerical integration;metodo nucleo;valor propio;matrice toeplitz;numero de condicionamiento;equation integrale;condition number;kernel method;block matrix;precondicionamiento;ecuacion integral;cubicacion;valeur propre;integrale premiere;formula cuadratura;numerical approximation;quadrature formula;toeplitz;matriz definida positiva;first integral;toeplitz matrix;integration numerique;formule quadrature;cubature;cg;indice conditionnement;solution numerique;block toeplitz;grand systeme;fredholm integral equation;fredholm equation	In this paper, first we consider the Fredholm integral equations of the second kind with Toeplitz kernel for examples 0096-3 doi:10 * Co E-m kðs; tÞ 1⁄4 1 2js tj ; cosðs tÞ 1þ js tj ; sin js tj 1þ js tj ; 1 1þ js tj ; 1 ð1þ js tjÞ ; . . . Then by using quadrature method the integral equations reduce to a Block Toeplitz system. The condition number of the coefficient matrix of the system is large. We use preconditioners to decrease the condition number of the new system. In the case that the coefficient matrix of above system is positive definite, we can use CG method for solving the system. 2006 Published by Elsevier Inc.	coefficient;condition number;conjugate gradient method;numerical partial differential equations;preconditioner;toeplitz hash algorithm	Khosrow Maleknejad;Mohsen Rabbani	2006	Applied Mathematics and Computation	10.1016/j.amc.2005.11.142	mathematical analysis;eigenvalues and eigenvectors;calculus;toeplitz matrix;mathematics;geometry;positive-definite matrix;fredholm integral equation;algebra	Robotics	82.4687836040506	20.129647427047352	82740
f93da50b09f88fce098b1dc58409255ae820b2f6	abnormal equality-constrained optimization problems: sensitivity theory	second order;optimisation sous contrainte;constrained optimization;regularite;fonction perturbation;analisis sensibilidad;fonction valeur;rate of change;regularidad;constrenimiento igualdad;regularity;optimizacion con restriccion;equality constraint;sensitivity analysis;asymptotic behavior of solutions;equality constrained problem;analyse sensibilite;funcion perturbacion;value function;upper and lower bounds;second order sufficient conditions;perturbation function;parametric optimization;optimal value function;abnormal point;constrained optimization problem;optimisation parametrique;contrainte egalite	For the equality-constrained optimization problem, we consider the case when the customary regularity of constraints can be violated. Under the assumptions substantially weaker than those previously used in the literature, we develop a reasonably complete local sensitivity theory for this class of problems, including upper and lower bounds for the rate of change of the optimal value function subject to parametric perturbations, as well as the estimates and the description of asymptotic behavior of solutions of the perturbed problems.	bellman equation;constrained optimization;constraint (mathematics);mathematical optimization;optimization problem	Aram V. Arutyunov;Alexey F. Izmailov	2004	Math. Program.	10.1007/s10107-003-0494-3	perturbation function;mathematical optimization;constrained optimization;combinatorics;calculus;mathematics;bellman equation;upper and lower bounds;sensitivity analysis;second-order logic	Theory	70.41281664864448	19.804883915016916	82882
6da6a0e13d6c3849d08e541933308a0ebb062f84	extension of quasi-newton methods to mathematical programs with complementarity constraints	superlinear convergence;mathematical program with complementarity constraints;mpcc;local convergence;piecewise sequential quadratic programming;sequential quadratic programming;mathematical program with equilibrium constraints;mpec;quasi newton method;complementarity problem;mathematical programs with equilibrium constraints;complementarity constraints	Quasi-Newton methods in conjunction with the piecewise sequential quadratic programming are investigated for solving mathematical programming with equilibrium constraints, in particular for problems with complementarity constraints. Local convergence as well as superlinear convergence of these quasi-Newton methods can be established under suitable assumptions. In particular, several well-known quasi-Newton methods such as BFGS and DFP are proved to exhibit the local and superlinear convergence.	complementarity theory;newton;quasi-newton method	Houyuan Jiang;Daniel Ralph	2003	Comp. Opt. and Appl.	10.1023/A:1022945316191	local convergence;mathematical optimization;mathematical analysis;quasi-newton method;mathematics;mixed complementarity problem;sequential quadratic programming;mathematical economics;complementarity theory	AI	73.20271064861669	21.951235346985357	82982
3d22f61317bd7127c150b8cfb3527f49e7c2ce58	"""reply to a note on the paper """"duality theory for optimization problems with interval-valued objective function"""""""	90c26;interval valued lagrangian function;closed intervals;65g30;interval valued lagrangian dual function	"""The author replies to a note on the paper """"Duality Theory for Optimization Problems with Interval-Valued Objective Function"""" by saying that the note is invalid."""	loss function	Hsien-Chung Wu	2016	J. Optimization Theory and Applications	10.1007/s10957-016-0906-1	perturbation function;mathematical optimization;mathematical analysis;discrete mathematics;duality;duality gap;mathematics	Theory	70.708037735504	21.58762648552918	83100
be6bf61eff836133e013cf4c52a17fae860f80b9	two hadamard numbers for matrices	condition numbers;eigenvalues;linear system;error analysis;matrices;condition number;norms;gram schmidt orthogonalization	A discussion is given of two functions of the entries of a square matrix, both related to Hadamard's determinant theorem, which have some merits as alternatives to norm-bound “condition numbers.” One (for linear systems) is known; the other (for eigensystems) seems to be new.	condition number;linear system	Garrett Birkhoff	1975	Commun. ACM	10.1145/360569.360655	matrix analysis;combinatorics;discrete mathematics;eigenvalues and eigenvectors;condition number;mathematics;complex hadamard matrix;linear system;hadamard matrix;hadamard's inequality;hadamard's maximal determinant problem;matrix;gram–schmidt process;norm;algebra	Theory	78.92747404468318	20.43014714367981	83359
62d925528782895fbac9681479c6813b15fbcf7a	lagrangian bounds in multiextremal polynomial and discrete optimization problems	discrete optimization;discrete optimization problems on graphs;nonlinear programming;eigenvalues;symmetric matrices;superfluous constraints;convex function;nondifferentiable optimization;matrix function;lagrangian bounds;branch and bound;quadratic type problems	Many polynomial and discrete optimization problems can be reduced to multiextremal quadratic type models of nonlinear programming. For solving these problems one may use Lagrangian bounds in combination with branch and bound techniques. The Lagrangian bounds may be improved for some important examples by adding in a model the so-called superfluous quadratic constraints which modify Lagrangian bounds. Problems of finding Lagrangian bounds as a rule can be reduced to minimization of nonsmooth convex functions and may be successively solved by modern methods of nondifferentiable optimization. This approach is illustrated by examples of solving polynomial-type problems and some discrete optimization problems on graphs.	discrete optimization;mathematical optimization;polynomial	Naum Z. Shor;Petro I. Stetsyuk	2002	J. Global Optimization	10.1023/A:1014004625997	convex function;matrix function;discrete optimization;mathematical optimization;combinatorics;mathematical analysis;eigenvalues and eigenvectors;nonlinear programming;mathematics;l-reduction;quadratic programming;branch and bound;symmetric matrix	Theory	72.35779411880839	23.609747018000366	83581
96bc460e750e7baaa8c40eb29ae86f896007c46e	two-step iterative methods for nonlinear equations	equation non lineaire;iterative method;methode a pas;ecuacion no lineal;analisis numerico;one step method;convergence;matematicas aplicadas;mathematiques appliquees;metodo predictor corrector;methode newton;metodo un paso;efficiency;metodo descomposicion;methode decomposition;predictor corrector method;methode predicteur correcteur;refinement;three step method;analyse numerique;metodo iterativo;decomposition method;afinamiento;eficacia;step method;numerical analysis;decomposition methods;methode iterative;two step methods;methode un pas;efficacite;nonlinear equation;methode deux pas;predictor corrector methods;affinement;nonlinear equations;metodo newton;newton method;numerical examples;methode 3 pas;non linear equation;iteration method;applied mathematics;two step method;three step methods;one step methods;ordre convergence;metodo a paso	In recent years several iterative methods have been suggested and analyzed for solving nonlinear equation f(x)=0. All these methods can be classified as one-step and two-step methods. In this paper, we consider and analyze two-step iterative methods for solving nonlinear equations. It is shown that two-step iterative methods are more efficient than one-step methods including Newton method. Several numerical examples are given to illustrate this comparison. Our results can be viewed as important refinement and improvement of the previously known results.	iterative method;nonlinear system	Muhammad Aslam Noor;Faizan Ahmad;Shumaila Javeed	2006	Applied Mathematics and Computation	10.1016/j.amc.2006.01.065	local convergence;mathematical optimization;nonlinear system;calculus;mathematics;iterative method;matrix-free methods;relaxation;algorithm;algebra	Robotics	82.23676790269232	18.546528592470036	83632
bbeeaea24c8ec2b3e7851bf2b3a631180af7a5be	multistep matrix splitting iteration preconditioning for singular linear systems	preconditioning;inner-outer iteration;gmres method;flexible gmres method;matrix splitting iterations;singular linear system;65f08;65f10;65f20;65f50	Multistep matrix splitting iterations serve as preconditioning for Krylov subspace methods for solving singular linear systems. The preconditioner is applied to the generalized minimal residual (GMRES) method and the flexible GMRES (FGMRES) method. We present theoretical and practical justifications for using this approach. Numerical experiments show that the multistep generalized shifted splitting (GSS) and Hermitian and skew-Hermitian splitting (HSS) iteration preconditioning are more robust and efficient compared to standard preconditioners for some test problems of large sparse singular linear systems.	condition number;experiment;generalized minimal residual method;high-speed serial interface;iteration;krylov subspace;linear system;matrix splitting;numerical linear algebra;numerical method;preconditioner;sparse matrix;spectral density estimation	Keiichi Morikuni	2017	Numerical Algorithms	10.1007/s11075-017-0330-0	mathematical optimization;mathematical analysis;calculus;generalized minimal residual method;mathematics	ML	81.65385751171632	21.853239656636237	83850
f442555a47f4abcc4ce29048963dfbfbc4e43a74	convex risk measures for portfolio optimization and concepts of flexibility	administracion financiera;analyse risque;52a41;non convex programming;approximate algorithm;financial management;90c31;risk analysis;flexibilidad;financial risk management;approximation algorithm;convex risk measure;gestion risque;risk management;programmation non convexe;portfolio optimization;analisis riesgo;programacion no convexa;absorptive capacity;mathematical programming;optimisation portefeuille;90c25;algoritmo aproximacion;portfolio management;flexibilite;gestion cartera;gestion riesgo;convex analysis;optimizacion cartera;gestion portefeuille;gestion financiere;algorithme approximation;programmation mathematique;90a09;programacion matematica;flexibility;strong duality	Due to their axiomatic foundation and their favorable computational properties convex risk measures are becoming a powerful tool in financial risk management. In this paper we will review the fundamental structural concepts of convex risk measures within the framework of convex analysis. Then we will exploit it for deriving strong duality relations in a generic portfolio optimization context. In particular, the duality relationship can be used for designing new, efficient approximation algorithms based on Nesterov’s smoothing techniques for non-smooth convex optimization. Furthermore, the presented concepts enable us to formalize the notion of flexibility as the (marginal) risk absorption capacity of a technology or (available) resources.	approximation algorithm;closed convex function;coherent risk measure;computation;constant term;convex analysis;convex optimization;marginal model;mathematical optimization;numerical analysis;numerical method;operational definition;risk management;risk measure;smoothing;strong duality;value (ethics)	Hans-Jakob Lüthi;Jörg Doege	2005	Math. Program.	10.1007/s10107-005-0628-x	convex analysis;mathematical optimization;risk analysis;risk management;convexity in economics;portfolio optimization;mathematics;strong duality;mathematical economics;approximation algorithm;financial risk management	ML	70.60759336454412	21.31219777598327	83892
199a4ed89703494eb3f324010972e89d4fa72182	computing the structured pseudospectrum of a toeplitz matrix and its extreme points	spectral radius;pseudospectrum;eigenvalue;65l07;toeplitz structure;spectral abscissa;65f15;structured pseudospectrum	The computation of the structured pseudospectral abscissa and radius (with respect to the Frobenius norm) of a Toeplitz matrix is discussed and two algorithms based on a low-rank property to construct extremal perturbations are presented. The algorithms are inspired by those considered in [N. Guglielmi and M. Overton, SIAM J. Matrix Anal. Appl., 32 (2011), pp. 1166–1192] for the unstructured case, but their extension to structured pseudospectra and analysis presents several difficulties. Natural generalizations of the algorithms, allowing us to draw significant sections of the structured pseudospectra in proximity of extremal points, are also discussed. Since no algorithms are available in the literature to draw such structured pseudospectra, the approach we present seems promising to extend existing software tools (Eigtool [T. G. Wright, Eigtool: A Graphical Tool for Nonsymmetric Eigenproblems, http://www.comlab.ox.ac.uk/pseudospectra/eigtool (2002)], Seigtool [M. Karow, E. Kokiopoulou, and D. Kressner, Systems Control Lett., 59 (2010), pp. 122–129] to structured pseudospectra representation for Toeplitz matrices. We discuss local convergence properties of the algorithms and show some applications to a few illustrative examples.	computation;comstock–needham system;graphical user interface;iscb overton prize;local convergence;low-rank approximation;matrix multiplication;pseudospectrum;toeplitz hash algorithm	Paolo Buttà;Nicola Guglielmi;Silvia Noschese	2012	SIAM J. Matrix Analysis Applications	10.1137/120864349	mathematical optimization;combinatorics;discrete mathematics;eigenvalues and eigenvectors;mathematics;spectral radius;algebra	Theory	80.74836139160908	23.944851363464828	83903
33b60ec851542aca533c9a1a0c7a4e23ee1ed805	verified error bounds for singular solutions of nonlinear systems	verification;deflations;border systems;11a05;error bounds;singular solutions;13p05	We discuss the verification for isolated singular solutions and non-isolated singular solutions with maximum rank deficiency of general nonlinear systems which have all the relevant partial derivatives. Using the border system technique, we present a deflation algorithm to compute verified error bound for such a singular solution of the given system or for a solution of a slight perturbed system, where the solution of the slight perturbed system is near the above singular solution of the given system. By adding certain components of the solution of certain linear system, our algorithm decreases the dimension of the deflation system in each step. Numerical experiments show the performance of our algorithm.	algorithm;angular defect;experiment;linear system;nonlinear system;numerical method	Zhe Li;Haifeng Sang	2014	Numerical Algorithms	10.1007/s11075-014-9948-3	mathematical optimization;mathematical analysis;discrete mathematics;verification;singular solution;mathematics	EDA	80.22686814964695	19.52917140024636	83958
055e76cc5fb479f8429e74eed3c75837a6e488fb	a kind of nonlinear and non-convex optimization problems under mixed fuzzy relational equations constraints with max-min and max-average composition	concave programming;nonlinear programming;fuzzy set theory;minimax techniques;polynomial time algorithm nonlinear optimization nonconvex optimization mixed fuzzy relational equations constraints max min composition max average composition;computational complexity;fuzzy relation equations nonlinear programming;nonlinear programming computational complexity concave programming fuzzy set theory minimax techniques;fuzzy relation equations;equations optimization mathematical model bismuth fuzzy sets linear programming programming	In this paper, a kind of nonlinear and non-convex optimization problems under the constraints expressed by a system of mixed fuzzy relation equations with max-min and max-average composition is investigated. First, some properties of this kind of optimization problem are obtained. Then, a polynomial-time algorithm for this optimization problem is given based on these properties. Furthermore, we show that this algorithm is optimal for the considered optimization problem. Finally, numerical examples are provided to illustrate our algorithms.	algorithm;convex optimization;mathematical optimization;maxima and minima;nonlinear programming;nonlinear system;numerical analysis;numerical method;optimization problem;time complexity	Shuang Feng;Yingqiu Ma;Jinquan Li	2012	2012 Eighth International Conference on Computational Intelligence and Security	10.1109/CIS.2012.42	stochastic programming;fractional programming;mathematical optimization;combinatorics;discrete mathematics;robust optimization;fuzzy transportation;nonlinear programming;computer science;fuzzy number;mathematics;active set method;fuzzy set;computational complexity theory;fuzzy set operations	Robotics	70.97872645926175	22.449097617915378	84046
aae79a3d288e07c610a403ded8ab11b789125286	a tensor sum preconditioner for stochastic automata networks	performance evaluation;stochastic automata networks;preconditioning;nearest kronecker product;markovian models	Stochastic automata networks (SANs) have gained great interest within the performance-modeling community because of their natural ability to capture parallel and distributed activities. SANs and the related concept of stochastic process algebra are advantageous because they keep the transition matrix in a compact form called the SAN descriptor. Several iterative and projection methods have been tested for SANs. Some preconditioners for SANs have been developed to speed up the convergence. Recently, Langville and Stewart [Langville, A., W. Stewart. 2004. A Kronecker product approximate preconditioner for SANs. Numer. Linear Algebra Appl.11 723--752] proposed the nearest Kronecker product (NKP) preconditioner for SANs with great success. Encouraged by their work, we propose a new preconditioning method, called the tensor sum preconditioner (TSP), which uses a tensor sum preconditioner rather than a Kronecker product preconditioner. In TSP, we take into account as much as possible the effect of synchronization using term grouping, factorizations, and approximation techniques. We conducted an experimental study to compare our TSP with the NKP preconditioner, and the results show that TSP outperformed NKP on both the number of iterations and the execution time.	automaton;preconditioner	Abderezak Touzene	2008	INFORMS Journal on Computing	10.1287/ijoc.1070.0236	mathematical optimization;combinatorics;discrete mathematics;mathematics;preconditioner;algorithm	Theory	81.35477393985968	24.718993285385686	84131
ec3cf08cb5658a1f094c0a99cbb9c54aa80026ca	optimality conditions and duality for a class of nondifferentiable multi-objective fractional programming problems	optimality conditions;duality;dual problem;fractional programming;function optimization;90c32;generalized convex functions;90c29;necessary and sufficient condition;sublinear functions;90c46;generalized convexity;multi objective fractional programming;optimality condition	A class of multi-objective fractional programming problems (MFP) are considered where the involved functions are locally Lipschitz. In order to deduce our main results, we give the definition of the generalized (F,?,?,d)-convex class about the Clarke's generalized gradient. Under the above generalized convexity assumption, necessary and sufficient conditions for optimality are given. Finally, a dual problem corresponding to (MFP) is formulated, appropriate dual theorems are proved.	fractional programming	Sanming Liu;Enmin Feng	2007	J. Global Optimization	10.1007/s10898-006-9103-3	fractional programming;mathematical optimization;combinatorics;mathematical analysis;duality;mathematics	PL	71.83322459372188	21.678131757980623	84329
8ecf6dff14e3a46a5c7e74284e7da55ec9e56f4e	fixed-point methods for a certain class of operators		We introduce in this paper a new class of nonlinear operators which contains, among others, the class of operators with semimonotone additive inverse and also the class of nonexpansive mappings. We study this class and discuss some of its properties. Then we present iterative procedures for computing fixed points of operators in this class, which allow for inexact solutions of the subproblems and relative error criteria. We prove weak convergence of the generated sequences in the context of Hilbert spaces. Strong convergence is also discussed. keywords: fixed points, approximate solutions, hypomonotone operators, semimonotone operators, proximal point algorithms.	approximation algorithm;approximation error;fixed point (mathematics);hilbert space;iterative method;nonlinear system;utility functions on indivisible goods	Rolando Gárciga Otero;Alfredo N. Iusem	2013	J. Optimization Theory and Applications	10.1007/s10957-012-0146-y	mathematical optimization;operator theory;mathematical analysis;discrete mathematics;mathematics	DB	75.80380790370172	18.667253751781608	84461
46cbd0028b4d0ff4815de749b44867c3522a586c	efficient convex optimization with membership oracles		We consider the problem of minimizing a convex function over a convex set given access only to an evaluation oracle for the function and a membership oracle for the set. We give a simple algorithm which solves this problem with Õ(n) oracle calls and Õ(n) additional arithmetic operations. Using this result, we obtain more efficient reductions among the five basic oracles for convex sets and functions defined by Grötschel et al. (1988).1	algorithm;convex function;convex optimization;convex set;oracle nosql db;oracle machine	Yin Tat Lee;Aaron Sidford;Santosh Vempala	2018			subderivative;convex analysis;proper convex function;combinatorics;oracle;discrete mathematics;mathematical optimization;mathematics;regular polygon;convex optimization;convex function;convex set	Theory	70.11630086068371	23.969529068009482	84623
0826987980fd5d97b29e68a0378eaa9a81e66539	expression of a tensor commutation matrix in terms of the generalized gell-mann matrices	tensor product;high energy physics	We have expressed the tensor commutation matrix n⊗n as linear combination of the tensor products of the generalized Gell-Mann matrices. The tensor commutation matrices 3 ⊗ 2 and 2 ⊗ 3 have been expressed in terms of the classical Gell-Mann matrices and the Pauli matrices.		Christian Rakotonirina	2007	Int. J. Math. Mathematical Sciences	10.1155/2007/20672	tensor product;matrix analysis;symmetric tensor;mathematical analysis;tensor field;tensor;higher-dimensional gamma matrices;exact solutions in general relativity;matrix multiplication;cartesian tensor;tensor;pure mathematics;tensor contraction;tensor product of algebras;cauchy stress tensor;mathematics;einstein tensor;weyl–brauer matrices;kronecker product;tensor density;lanczos tensor;tensor product of hilbert spaces;quantum mechanics;algebra	Vision	78.04524930301145	19.64564225039385	84859
fbe9dc37f774d3c1f6e7110a1a965ed86d9c3c4b	linear inequalities, mathematical programming and matrix theory	matrix theory;mathematical programming	A survey is made of solvability theory for systems of complex linear inequalities.	linear inequality;mathematical optimization	Abraham Berman;Adi Ben-Israel	1971	Math. Program.	10.1007/BF01584093	fractional programming;mathematical optimization;combinatorics;functional calculus;criss-cross algorithm;second-order cone programming;linear-fractional programming;mathematics;matrix	Theory	71.18892913888536	22.64043079866065	85171
8f53b2fd8a5815388553bcbf32d7fe01e0288aaf	solution semicontinuity of parametric generalized vector equilibrium problems	continuity;set valued mapping;equilibrium problem;scalarization;lower semicontinuity;parametric generalized vector equilibrium problems;solution mappings	In this paper, the lower semicontinuity and continuity of the solution mapping to a parametric generalized vector equilibrium problem involving set-valued mappings are established by using a new proof method which is different from the ones used in the literature.	scott continuity;semi-continuity	C. R. Chen;Shunfen Li;Kok Lay Teo	2009	J. Global Optimization	10.1007/s10898-008-9376-9	mathematical optimization;combinatorics;mathematics;mathematical economics	Theory	72.17841594807436	20.925539328837697	85367
33ab5e39453c3b2189955414bc60cc17b6c85b56	real solving of ill-conditioned sine-polynomials equations	ill-conditioned sine-polynomials equation	ill-conditioned sine-polynomials equation	condition number;polynomial	Aude Maignan	1999	ACM SIGSAM Bulletin	10.1145/347127.347263	discrete mathematics;polynomial;mathematics;sine	Graphics	81.23798670734982	19.63257499630263	85473
1a5aaf9be0b142383aea98501168bc0e4d460405	on some ways of approximating inverses of banded matrices in connection with deriving preconditioners based on incomplete block factorizations	preconditionnement;matriz bloque;symmetric positive definite;tridiagonal matrices;inverso;elliptic boundary value problem;incomplete factorization;incomplet;decay rate;aproximacion;schur complement;differential equation;preconditioning;incomplete;matriz banda;finite element;matrice mathematique;tridiagonal matrix;approximation;equation elliptique;incompleto;elliptic equation;factorization;band matrix;mathematical matrix;factorizacion;elemento acabado;matrice bloc;matriz tridiagonal;factorisation;matriz matematica;bloc matrix;element fini;ecuacion eliptica;matrice bande;matrice tridiagonale;inverse;finite element solution	A unified approach of deriving band approximate inverses of band symmetric positive definite matrices is considered. Such band approximations to the inverses of successive Schur complements are required throughout incomplete block factorizations of block-tridiagonal matrices. Such block-tridiagonal matrices arise, for example, in finite element solution of second order elliptic differential equations. A sharp decay rate estimate for inverses of blocktridiagonal symmetric positive definite matrices is given in addition. Numerical tests on a number of model elliptic boundary value problems are presented comparing thus derived preconditioning matrices. Es wird ein einheitlicher Zugang für die Ableitung von approximierten Inversen von symmetrischen, positiv definiten Bandmatrizen beschrieben. Solche Band-Approximationen für die Inversen von aufeinanderfolgenden Schur-Komplementen werden benötigt bei der unvollständigen Blockfaktorisierung von blocktridiagonalen Matrizen, die bei der Finite-Elemente-Lösung von elliptischen Differentialgleichungen zweiter Ordnung entstehen. Eine scharfe Abschätzung für die Abklingrate der Blöcke der Inversen von blocktridiagonalen symmetrischen positiv definiten Matrizen wird zusätzlich gegeben. Numerische Tests für einige elliptische Modelldifferentialgleichungen werden präsentiert, um die abgeleiteten Präkonditionierungsmatrizen miteinander zu vergleichen.	approximation algorithm;eine and zwei;finite element method;numerical linear algebra;preconditioner;unified model	Panayot S. Vassilevski	1990	Computing	10.1007/BF02242922	mathematical analysis;calculus;mathematics;factorization;algebra	Theory	82.59638569080255	19.582305668755556	85528
bcdcb02109bddb0dceb0822b225616e2fb236f57	effectiveness of a geometric programming algorithm for optimization of machining economics models	machining;optimisation;non linear programming;optimizacion;geometric program;programacion no lineal;programmation non lineaire;economic model;algorithme;algorithm;usinage;mathematical programming;programacion geometrica;optimization;mecanizado;linear equations;programmation mathematique;geometric programming;programacion matematica;programmation geometrique;algoritmo	Machining economics problems usually contain highly nonlinear equations which may present difficulties for some nonlinear programming algorithms. An earlier article by Duffuaa et al. [1] compared the performance of several nonlinear programming algorithms, including a geometric programming algorithm, applied to five machining economics problems. Those authors concluded that the Generalized Reduced Gradient (GRG) algorithm is the most suitable method for solving such problems. In this paper, we point out shortcomings in that conclusion and demonstrate the effectiveness of the Geometric Programming technique in such problems compared with the results of GRG which were presented. †Jae Chul Choi is Research Associate at The University of Iowa, where he earlier received his M.S. and Ph.D. degrees in Industrial and Management Engineering. ‡Dennis L. Bricker is Associate Professor of Industrial Engineering at The University of Iowa. He received B.S. and M.S. degrees in Mathematics from the University of Illinois, and M.S. and Ph.D. degrees in Industrial Engineering and Management Sciences from Northwestern University. Please address correspondence to the address above or (e-mail) dennis-bricker@uiowa.edu. January 13, 1996 page 1	algorithm;email;geometric programming;gradient;industrial engineering;management science;mathematical optimization;nonlinear programming;nonlinear system	Jae Chul Choi;Dennis L. Bricker	1996	Computers & OR	10.1016/0305-0548(96)00008-1	mathematical optimization;combinatorics;geometric programming;criss-cross algorithm;machining;computer science;economic model;mathematics;linear equation;algorithm	Robotics	71.52232409371135	26.51593676136398	85859
b80f2548767e397dbe6a301e88b468b23de8caf3	a class of interior proximal-like algorithms for convex second-order cone programming	90c30;second order cone convexity;second order cone;65k05;measure of distance;second order cone program;proximal method	We propose a class of interior proximal-like algorithms for the second-order cone program which is to minimize a closed proper convex function subject to general second-order cone constraints. The class of methods uses a distance measure generated by a twice continuously differentiable strictly convex function on (0, +∞), and includes as a special case the entropy-like proximal algorithm [12] which was originally proposed for minimizing a convex function subject to nonnegative constraints. Particularly, we consider an approximate version of these methods, allowing the inexact solution of subproblems. Like the entropy-like proximal algorithm for convex programming with nonnegative constraints, we under some mild assumptions establish the global convergence expressed in terms of the objective values for the proposed algorithm, and show that the sequence generated is bounded and every accumulation point is a solution of the considered problem. Preliminary numerical results are reported for two approximate entropy-like proximal algorithms, and numerical comparisons are also made with the merit function approach [8], which verify the effectiveness of the proposed method.	approximate entropy;approximation algorithm;condition number;conic optimization;convex optimization;entropy (information theory);local convergence;numerical analysis;numerical method;proper convex function;rate of convergence;second-order cone programming;system on a chip;tree accumulation	Shaohua Pan;Jein-Shan Chen	2008	SIAM Journal on Optimization	10.1137/070685683	mathematical optimization;conic optimization;mathematical analysis;convex cone;dual cone and polar cone;second-order cone programming;mathematics;geometry;logarithmically convex function;proper convex function	ML	72.3918745935302	23.02911522269605	85864
feb780235b8ea17dc465d5bf1a1814cb33ad14e7	block-toeplitz/hankel structured total least squares	reduccion sistema;metodo cuadrado menor;62j12;matriz bloque;methode moindre carre;systems;sample size;methode locale;algorithm complexity;numerical solution;least squares method;complexite calcul;numerical method;reduction modele;complejidad algoritmo;tamano muestra;efficiency;optimal method;model reduction 15a06;system reduction;taille echantillon;grupo de excelencia;optimization method;metodo optimizacion;approximation;identificacion sistema;optimization problem;algorithm;15a06;eficacia;total least square;complejidad computacion;model reduction;complexite algorithme;metodo numerico;system identification;probleme optimisation;reduction systeme;matrice bloc;computational complexity;ciencias basicas y experimentales;structured total least squares;matematicas;total least squares;modele simulation;estimacion parametro;methode optimisation;efficacite;block matrix;tecnologias generalidades;modelo simulacion;parameter estimation;estimation parametre;tecnologias;sista;computational efficiency;simulation model;37m10;identification systeme;solution numerique;methode numerique;methode lineaire	A structured total least squares problem is considered in which the extended data matrix is partitioned into blocks and each of the blocks is block-Toeplitz/Hankel structured, unstructured, or exact. An equivalent optimization problem is derived and its properties are established. The special structure of the equivalent problem enables us to improve the computational efficiency of the numerical solution methods. By exploiting the structure, the computational complexity of the algorithms (local optimization methods) per iteration is linear in the sample size. Application of the method for system identification and for model reduction is illustrated by simulation examples.	algorithmic efficiency;approximation;computable function;computational complexity theory;dynamical system;flops;iteration;loss function;mimo;mathematical optimization;numerical analysis;numerical partial differential equations;optimization problem;simulation;singular value decomposition;system identification;toeplitz hash algorithm;total least squares	Ivan Markovsky;Sabine Van Huffel;Rik Pintelon	2005	SIAM J. Matrix Analysis Applications	10.1137/S0895479803434902	sample size determination;total least squares;optimization problem;system identification;numerical analysis;approximation;calculus;simulation modeling;system;mathematics;geometry;efficiency;estimation theory;computational complexity theory;least squares;block matrix;algorithm	ML	81.655565502509	20.45029094225878	86111
19a493d3b736eff8fb6af455d20c961d70fdecd3	exact bounds for steepest descent algorithms of l-convex function minimization	discrete optimization;discrete convex function;analysis of algorithm;steepest descent algorithm	We analyze minimization algorithms for L\-convex functions in discrete convex analysis, and establish exact bounds for the number of iterations required by the steepest descent algorithm and its variants.	algorithm;convex analysis;convex function;gradient descent;international symposium on algorithms and computation;iteration;polyhedron;shortest path problem	Kazuo Murota;Akiyoshi Shioura	2014	Oper. Res. Lett.	10.1016/j.orl.2014.06.005	gradient descent;discrete optimization;mathematical optimization;combinatorics;discrete mathematics;method of steepest descent;subgradient method;descent direction;random coordinate descent;mathematics;stochastic gradient descent	ML	71.96345490537287	24.036409442425132	86471
0ec23d8b388cecd50aec2809682a45132b410b2a	regularization and preconditioning of kkt systems arising in nonnegative least-squares problems	inexact newton methods;interior point;preconditioning;regularization;kkt systems;least squares problem;condition number;iterative linear solvers;bound constrained linear least squares problems;spectral properties;iteration method	A regularized Newton-like method for solving nonnegative least-squares problems is proposed and analysed in this paper. A preconditioner for KKT systems arising in the method is introduced and spectral properties of the preconditioned matrix are analysed. A bound on the condition number of the preconditioned matrix is provided. The bound does not depend on the interior-point scaling matrix. Preliminary computational results confirm the effectiveness of the preconditioner and fast convergence of the iterative method established by the analysis performed in this paper.	condition number;image scaling;iterative method;karush–kuhn–tucker conditions;matlab;matrix regularization;newton;non-negative least squares;precondition;preconditioner;spectral density estimation;whole earth 'lectronic link	Stefania Bellavia;Jacek Gondzio;Benedetta Morini	2009	Numerical Lin. Alg. with Applic.	10.1002/nla.610	regularization;mathematical optimization;mathematical analysis;condition number;interior point method;calculus;mathematics;preconditioner;iterative method;algebra	ML	80.50107364733861	21.834988306918767	86684
9025b9d437291feea87be91fc3832aaf89b57664	half-quadratic transportation problems		We present a primal–dual memory efficient algorithm for solving a relaxed version of the general transportation problem. Our approach approximates the original cost function with a differentiable one that is solved as a sequence of weighted quadratic transportation problems. The new formulation allows us to solve differentiable, non– convex transportation problems.	algorithm;loss function;transportation theory (mathematics)	Mariano Rivera	2017	CoRR		mathematical optimization;mathematical analysis;mathematics;algorithm	ML	72.53018564062526	23.477952561325402	86690
0b5814c69f7cee6292bdf1df3d5a465f0ee39866	a direct method for solving circulant tridiagonal block systems of linear equations	sistema lineal;metodo directo;equation non lineaire;matriz bloque;ecuacion no lineal;analisis numerico;tridiagonal matrices;circulant matrix;equation matricielle;matrix inversion;matriz toeplitz;inversion matriz;linear system;analyse numerique;tridiagonal matrix;matrice circulante;block sherman morrison woodbury formula;nonlinear matrix equations;numerical analysis;matrix equation;matrice bloc;formule block sherman morrison woodbury;toeplitz and hermitian matrices;matrice toeplitz;matriz tridiagonal;algebra lineal numerica;matrice hermitienne;algebre lineaire numerique;ecuacion matricial;circulant tridiagonal block linear systems;inversion matrice;nonlinear matrix equation;block matrix;numerical linear algebra;matriz hermitiana;linear equations;hermitian matrices;systeme lineaire;non linear equation;toeplitz matrix;methode directe;matrice tridiagonale;hermitian matrix;direct method;matriz circulante	This paper presents a modification of Rojo's algorithm [Comput. Math. Appl. 20 (1990) 61] to solve block circulant tridiagonal systems of linear equations which are Toeplitz and Hermitian. This new approach gives us a general direct algorithm for solving the problem. We will show how to choose a block matrix as a parameter to describe the method. We employ the factorization of block Toeplitz tridiagonal matrices as the product of two block Toeplitz subdiagonal and superdiagonal matrices. The algorithm is based on obtaining the solution of the nonlinear matrix equation A=@C+B*@C^-^1B. Finally, some numerical results will be given.	circulant matrix;direct method in the calculus of variations;linear equation;system of linear equations	Salah M. El-Sayed	2005	Applied Mathematics and Computation	10.1016/j.amc.2004.06.041	hermitian matrix;tridiagonal matrix;combinatorics;calculus;toeplitz matrix;mathematics;block matrix;tridiagonal matrix algorithm;algebra	Logic	81.42532537002012	21.0467655516854	86704
b4856645957426c2234652e9baeba8286cc045e3	minimum norm solution to the absolute value equation in the convex case		In this paper, we give an algorithm to compute the minimum norm solution to the absolute value equation (AVE) in a special case. We show that this solution can be obtained from theorems of the alternative and a useful characterization of solution sets of convex quadratic programs. By using an exterior penalty method, this problem can be reduced to an unconstrained minimization problem with once differentiable convex objective function. Also, we propose a quasi-Newton method for solving unconstrained optimization problem. Computational results show that convergence to high accuracy often occurs in just a few iterations.	algorithm;computation;convex function;iteration;mathematical optimization;newton's method;optimization problem;penalty method;quasi-newton method;singular value decomposition	Saeed Ketabchi;Hossein Moosaei	2012	J. Optimization Theory and Applications	10.1007/s10957-012-0044-3	convex function;convex analysis;subderivative;mathematical optimization;combinatorics;mathematical analysis;convex optimization;mathematics;proper convex function	ML	73.70501210943357	23.632215277846814	86763
80fdd0e61a5bfca35340b5c28313b1c8ffe42b53	elliptic control problems with gradient constraints - variational discrete versus piecewise constant controls	piecewise linear;cost function;finite element;control problem;gradient constraints;error bound;error estimate;elliptic optimal control problem;error estimates;optimal control problem	We consider an elliptic optimal control problem with pointwise bounds on the gradient of the state. To guarantee the required regularity of the state we include the Lr-norm in our cost functional with r > d, (d = 2, 3). We investigate variational discretization of the control problem [6] as well as piecewise constant approximations of the control. In both cases we use standard piecewise linear and continuous finite elements for the discretization of the state. Pointwise bounds on the gradient of the discrete gradient are enforced element-wise. Error bounds for control and state are obtained in two and three space dimensions depending on the value of r . Mathematics Subject Classification (2000): 49J20, 49K20, 35B37	approximation;discretization;finite element method;gradient;lr parser;mathematical optimization;mathematics subject classification;optimal control;piecewise linear continuation;variational principle	Andreas Günther;Michael Hinze	2011	Comp. Opt. and Appl.	10.1007/s10589-009-9308-8	mathematical optimization;mathematical analysis;discrete mathematics;piecewise linear function;finite element method;mathematics	ML	72.2405746501179	19.64367002282882	87287
a16ab9da58a64a2e6e67c3063609d3ce2a846595	beyond convex? global optimization is feasible only for convex objective functions: a theorem	objective function;np hard problem;computational complexity;convex function;generating function;global optimization;non convexity	It is known that there are feasible algorithms for minimizing convex functions, and that for general functions, global minimization is a difficult (NP-hard) problem. It is reasonable to ask whether there exists a class of functions that is larger than the class of all convex functions for which we can still solve the corresponding minimization problems feasibly. In this paper, we prove, in essence, that no such more general class exists. In other words, we prove that global optimization is always feasible only for convex objective functions.	algorithm;convex function;convex optimization;elementary function;global optimization;loss function;mathematical optimization;np-hardness;nonlinear system;optimization problem	Vladik Kreinovich;R. Baker Kearfott	2005	J. Global Optimization	10.1007/s10898-004-2120-1	convex function;convex analysis;subderivative;support function;mathematical optimization;conic optimization;generating function;combinatorics;mathematical analysis;convex optimization;pseudoconvex function;ellipsoid method;danskin's theorem;linear matrix inequality;convex conjugate;nonlinear programming;quasiconvex function;absolutely convex set;convexity in economics;np-hard;mathematics;computational complexity theory;logarithmically convex function;effective domain;proper convex function;global optimization	ML	71.20485825091096	23.192701300727308	87674
c0877a253342b7568b13810a4389bf7c9c0086f5	the cone condition and nonsmoothness in linear generalized nash games	90c31;piecewise linear function;genericity;nikaido isoda function;generalized nash equilibrium problem;91a10;constraint qualification;parametric optimization;91a06	We consider linear generalized Nash games and introduce the socalled cone condition which characterizes the smoothness of the Nikaido-Isoda function under weak assumptions. The latter mapping arises from a reformulation of the generalized Nash equilibrium problem as a possibly nonsmooth optimization problem. Other regularity conditions like LICQ or SMFC(Q) are only sufficient for smoothness, but have the advantage that they can be verified more easily than the cone condition. Therefore, we present special cases where these conditions are not only sufficient, but also necessary for smoothness of the Nikaido-Isoda function. Our main tool in the analysis is a global extension of the NikaidoIsoda function that allows us to avoid technical issues that may appear at the boundary of the domain of the Nikaido-Isoda function.	cone (formal languages);mathematical optimization;nash equilibrium;optimization problem	Oliver Stein;Nathan Sudermann-Merx	2016	J. Optimization Theory and Applications	10.1007/s10957-015-0779-8	epsilon-equilibrium;mathematical optimization;mathematical analysis;piecewise linear function;mathematics;mathematical economics;piecewise	ML	72.04137553621183	20.714210961022708	87727
a3cb124ee5ff90bc9768146432f71bd10afe7744	sparse signal recovery from nonlinear measurements	sparse signal recovery partial sparse simplex methods greedy methods iterative hard thresholding method coordinate wise optimality sparsity constraints general continuously differentiable function nonlinear measurements;compressed sensing;greedy algorithms;vectors compressed sensing signal processing algorithms matching pursuit algorithms linear programming signal processing iterative methods;iterative methods;linear programming;linear programming compressed sensing greedy algorithms iterative methods	We treat the problem of minimizing a general continuously differentiable function subject to sparsity constraints. We present and analyze several different optimality criteria which are based on the notions of stationarity and coordinate-wise optimality. These conditions are then used to derive three numerical algorithms aimed at finding points satisfying the resulting optimality criteria: the iterative hard thresholding method and the greedy and partial sparse-simplex methods. The theoretical convergence of these methods and their relations to the derived optimality conditions are studied.	detection theory;greedy algorithm;iterative method;nonlinear system;numerical analysis;simplex algorithm;sparse matrix;stationary process;thresholding (image processing)	Amir Beck;Yonina C. Eldar	2013	2013 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2013.6638708	mathematical optimization;combinatorics;greedy algorithm;computer science;linear programming;machine learning;mathematics;iterative method;compressed sensing	Robotics	73.92572765801773	25.198013717610156	87968
84bdffe117a129acddf335858ae17136048ff98a	an approach to first principles electronic structure calculation by symbolic-numeric computation	symbolic computation	This article is an introduction to a new approach to first principles electronic structure calculation. The starting point is the Hartree-Fock-Roothaan equation, in which molecular integrals are approximated by polynomials by way of Taylor expansion with respect to atomic coordinates and other variables. It leads to a set of polynomial equations whose solutions are eigenstate, which is designated as algebraic molecular orbital equation. Symbolic computation, especially, Gröbner bases theory, enables us to rewrite the polynomial equations into more trimmed and tractable forms with identical roots, from which we can unravel the relationship between physical parameters (wave function, atomic coordinates, and others) and numerically evaluate them one by one in order. Furthermore, this method is a unified way to solve the electronic structure calculation, the optimization of physical parameters, and the inverse problem as a forward problem.	approximation algorithm;cobham's thesis;electronic structure;fock space;gaussian orbital;gröbner basis;hartree–fock method;linear algebra;mathematical optimization;molecular orbital;numerical analysis;polynomial;quantum state;rewrite (programming);symbolic computation;symbolic-numeric computation	Akihito Kikuchi	2013	CoRR		mathematical analysis;discrete mathematics;symbolic computation;mathematics;quantum mechanics	Theory	77.50559442882712	19.313934402228867	88468
8f99ea7a8b9593acb2937b8a4d947e15dc4b6ea5	computational method for solving a stochastic linear-quadratic control problem given an unsolvable stochastic algebraic riccati equation	generalized convex feasible set;65k15;fixed point optimization algorithm;linear matrix inequality;90c25;stochastic linear quadratic control problem;nonexpansive mapping;49j15;fixed point set;stochastic algebraic riccati equation	We discuss a stochastic linear-quadratic control problem in which a stochastic algebraic Riccati equation derived from the problem is unsolvable. The Riccati equation has no solution when the state and control weighting matrices in the objective function of the problem are indefinite, and the conventional methods cannot solve the problem when the Riccati equation itself is unsolvable. We first show that the optimal value of the problem is finite. Next, we formulate a compromise solution to the stochastic algebraic Riccati equation and show that the problem can be solved via this compromise solution under certain assumptions. Moreover, we propose a novel computational method for finding the compromise solution based on iterative techniques for a convex optimization problem over the fixed point set of a certain nonexpansive mapping. Numerical examples demonstrate the effectiveness of this method.	algebraic riccati equation;algebraic equation;algorithm;authorization;brownian motion;command & conquer:yuri's revenge;computation;convex optimization;fixed point (mathematics);iterative method;letter-quality printer;mathematical optimization;maximal set;nl (complexity);numerical analysis;numerical aperture;numerical method;operational amplifier;optimal control;optimization problem	Hideaki Iiduka;Isao Yamada	2012	SIAM J. Control and Optimization	10.1137/110850542	mathematical optimization;linear-quadratic regulator;mathematical analysis;discrete mathematics;linear matrix inequality;algebraic riccati equation;riccati equation;mathematics	ML	74.21285445560642	19.88728229890038	88778
707ef3bec676c9a99ea5ef3ecfdee3537bca7105	on the stability of lagrange programming neural networks for satisfiability problems of propositional calculus	combinatorial optimization problem;differential equation;simulated annealing;satisfiability;equilibrium point;stability;gradient descent;propositional calculus;high order neural network;boltzmann machine;lagrangian method;local minima;constrained optimization problem;satisfiability problem;neural network;penalty function	The Hopfield type neural networks for solving difficult combinatorial optimization problems have used the gradient descent algorithms to solve constrained optimization problems via penalty functions. However, it is well known that the convergence to local minima is inevitable in these approaches. The Boltzmann Machines have used a simulated annealing technique, and were proven to be able to find global minima theoretically. However they require large computational resources. Recently Lagrange programming neural networks have been proposed. They differ from the gradient descent algorithms by using anti-descent terms in their dynamical differential equations. In this paper we theoretically analyze the stability and the convergence property of one of the Lagrange programming neural networks (LPPH) when it is applied to a satisfiability problem (SAT) of prepositional calculus. We prove that (1) the solutions of the SAT are the equilibrium points of the LPPH and  vice versa , and (2) if the given expression is satisfiable, there is at least one stable equilibrium point of the LPPH.	artificial neural network;propositional calculus	Masahiro Nagamatu;Torao Yanaru	1996	Neurocomputing	10.1016/0925-2312(95)00087-9	gradient descent;boltzmann machine;equilibrium point;mathematical optimization;combinatorics;discrete mathematics;stability;simulated annealing;computer science;machine learning;mathematics;propositional calculus;differential equation;artificial neural network;satisfiability	AI	76.38854289555888	28.423263228339707	88999
78c56e31de11cc99ff4f584f3d000585bc64b827	second-order multiplier update calculations for optimal control problems and related large scale nonlinear programs	second order;90c06;nonlinear programming;65k10;newtonian multiplier updates;banded equilibrium equations;large scale;49m29;efficient solution methods;optimal control problem;newtonian projection methods	A second-order multiplier update rule is applied to K-stage discrete-time optimal control problems with control and state variable constraints. Each update entails the assembly and solution of sparse block-banded equilibrium equations. Several direct elimination solution techniques are considered, and it is shown that the updates can always be calculated in $O( K )$ flops. Part of the analysis applies not only to control problems, but also to other similarly structured large scale nonlinear programs with equality and inequality constraints.	nonlinear programming;optimal control	J. C. Dunn	1993	SIAM Journal on Optimization	10.1137/0803023	mathematical optimization;nonlinear programming;control theory;mathematics;mathematical economics;second-order logic	Theory	76.74358557217434	26.596245268938603	89117
21075c5ee60da4d2339c69acde900e3045996cb5	a new look at the existence of p-optimal policies in dynamic programming	dynamic programming;dynamic program;optimal policy;infinite horizon problem	A new look at a class of existence results in dynamic programming under the expected utility---or pI-optimality---criterion is afforded by embedding the policies in an abstract L∞-space Balder, E. J. 1979. On a useful compactification for optimal control theory. J. Math. Anal. Appl.72 391--398; Balder, E. J. 1981. Relaxed inf-compactness for variational problems by Hilbert cube compactification. J. Math. Anal. Appl.79 1--12; Balder, E. J. 1981. Lower semicontinuity of integral functionals with nonconvex integrands by relaxation-compactification. SIAM J. Control Optim.19 533--542.. The results obtained in this way form a generalization of Schal, M. 1975. On dynamic programming: Compactness of the space of policies. Stochastic Processes Appl.3 345--364, §5.	dynamic programming	Erik J. Balder	1981	Math. Oper. Res.	10.1287/moor.6.4.513	mathematical optimization;mathematical analysis;discrete mathematics;dynamic programming;mathematics	Theory	72.31570161438538	19.06785734277299	89344
811bab5c2eccf6b3c8489034e3122d6693a70be0	convergence of a class of inexact interior-point algorithms for linear programs	sistema lineal;infeasible interior point algorithm;convergence analysis;convergence;interior point;search space;methode point interieur;linear system;feasibility;residual;convergencia;programacion lineal;metodo punto interior;linear programming;programmation lineaire;linear program;infeasible interior point method;systeme lineaire;inexact search direction;interior point method;practicabilidad;interior point algorithm;faisabilite	We present a convergence analysis for a class of inexact infeasible-interior-point methods for solving linear programs. The main feature of inexact methods is that the linear systems defining the search direction at each interior-point iteration need not be solved to high accuracy. More precisely, we allow that these linear systems are only solved to a moderate accuracy in the residual, but no assumptions are made on the accuracy of the search direction in the search space. In particular, our analysis does not require that feasibility is maintained even if the initial iterate happened to be a feasible solution of the linear program. We also present a few numerical examples to illustrate the effect of using inexact search directions on the number of interior-point iterations.	algorithm;linear programming;system of linear equations	Roland W. Freund;Florian Jarre;Shinji Mizuno	1999	Math. Oper. Res.	10.1287/moor.24.1.50	mathematical optimization;linear programming;interior point method;calculus;mathematics;geometry	Theory	76.80256565596792	23.196638530540888	89533
ebc5d67daa1be013d37363a8d73974756d7abfc7	sparse solution of nonnegative least squares problems with applications in the construction of probabilistic boolean networks	least squares;gradient decent method;projection;probabilistic boolean networks pbns	In this paper, we consider finding a sparse solution of nonnegative least squares problems with a linear equality constraint. We propose a projection-based gradient descent method for solving huge size constrained least squares problems. Traditional Newton-based methods require solving a linear system. However, when the matrix is huge, it is neither practical to store it nor possible to solve it in a reasonable time. We therefore propose a matrix-free iterative scheme for solving the minimizer of the captured problem. This iterative scheme can be explained as a projection-based gradient descent method. In each iteration, a projection operation is performed to ensure the solution is feasible. The projection operation is equivalent to a shrinkage operator, which can guarantee the sparseness of the solution obtained. We show that the objective function is decreasing. We then apply the proposed method to the inverse problem of constructing a probabilistic Boolean network. Numerical examples are then given to illustrate both the efficiency and effectiveness of our proposed method. Copyright © 2015 John Wiley & Sons, Ltd.	boolean network;gradient descent;iteration;iterative method;john d. wiley;linear equation;linear least squares (mathematics);linear system;loss function;neural coding;newton;non-negative least squares;numerical method;optimization problem;projection (relational algebra);sparse matrix;the matrix	You-Wei Wen;Man Wang;Zhi-Ying Cao;Xiaoqing Cheng;Wai-Ki Ching;Vassilios S. Vassiliadis	2015	Numerical Lin. Alg. with Applic.	10.1002/nla.2001	mathematical optimization;combinatorics;discrete mathematics;projection;mathematics;least squares;algorithm;algebra	AI	79.55224323969739	23.594031367014473	89633
42b5a75758ebaca8016823436592703451d12f30	screening for learning classification rules via boolean compressed sensing	signal classification boolean functions compressed sensing duality mathematics optimisation;boolean equations boolean compressed sensing convex relaxations sparse representation problems convex formulations screening methods duality theory optimization problem learning sparse classification rules linear program;vectors linear programming training compressed sensing speech speech processing testing;sparse signal approximation linear programming duality rule learning screening	Convex relaxations for sparse representation problems, which aim to find sparse solutions to systems of equations, have enabled a variety of exciting applications in high-dimensional settings. Yet, with dimensions large enough, even these convex formulations become prohibitively expensive. Screening methods attempt to use duality theory to dramatically reduce the size of the optimization problem through easily computable certificates that many of the variables must be zero in the optimal solution. In this paper we consider learning sparse classification rules via Boolean compressed sensing and develop screening procedures that can significantly reduce the size of the resulting linear program. Boolean compressed sensing deals with systems of Boolean equations (instead of linear equations in traditional compressed sensing); we develop screening methods specifically for this setting. We demonstrate the effectiveness of our screening rules on several real-world classification data sets.	boolean algebra;compressed sensing;computable function;linear equation;linear programming;mathematical optimization;optimization problem;sparse approximation;sparse matrix	Sanjeeb Dash;Dmitry M. Malioutov;Kush R. Varshney	2014	2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2014.6854223	mathematical optimization;combinatorics;machine learning;mathematics;binary decision diagram	AI	75.5580883200127	26.901429776431165	89841
d77544ce13dd716378c56d38164aa16df7965948	parallel multisplitting two-stage iterative methods with general weighting matrices for non-symmetric positive definite systems	linear systems;non symmetric matrix;journal;parallel multisplitting;two stage;weighting matrices	In this work, we propose a new parallel multisplitting iterative method for non-symmetric positive definite linear systems. Based on optimization theory, the new method has two great improvements; one is that only one splitting needs to be convergent, and the other is that the weighting matrices are not scalar and nonnegative matrices. The convergence of the new parallel multisplitting iterative method is discussed. Finally, the numerical results show that the new method is effective.		Chuan-Long Wang;Guo-Yan Meng	2013	Appl. Math. Lett.	10.1016/j.aml.2013.05.018	mathematical optimization;combinatorics;discrete mathematics;control theory;mathematics;linear system	Robotics	81.27557835340247	22.112621587538147	89872
ba3de280f91606a39103652af8bbbe6680e6daef	global error bound for convex inclusion problems	convex inclusion;constraint qualification;error bound;convex multifunction	The existence of global error bound for convex inclusion problems is discussed in this paper, including pointwise global error bound and uniform global error bound. The existence of uniform global error bound has been carefully studied in Burke and Tseng (SIAM J. Optim. 6(2), 265–282, 1996) which unifies and extends many existing results. Our results on the uniform global error bound (see Theorem 3.2) generalize Theorem 9 in Burke and Tseng (1996) by weakening the constraint qualification and by widening the varying range of the parameter. As an application, the existence of global error bound for convex multifunctions is also discussed.	karush–kuhn–tucker conditions;truncation error (numerical integration);william l. burke	Yiran He	2007	J. Global Optimization	10.1007/s10898-007-9145-1	mathematical optimization;calculus;mathematics;mathematical economics	Theory	72.53955068867451	19.07012295458243	89911
a9d16a529688608a0b0afc56d11a70b299bbc51e	"""counterexamples to a triality theorem in """"canonical dual least square method"""""""	least square method;optimization problem;triality;counterexample	In this short note we give two counterexamples to the Triality Theorem concerning the optimization problem presented in Ruan et al., Comput. Optim. Appl. (2008). In order to understand the problem first we introduce the framework of this paper and quote the theorem we have in view. Then, while trying to verify this theorem we encounter several difficulties and note their nature. We end by revealing our counterexamples.	dual polyhedron	M. D. Voisei;Constantin Zalinescu	2011	Comp. Opt. and Appl.	10.1007/s10589-010-9320-z	optimization problem;mathematical optimization;combinatorics;discrete mathematics;triality;counterexample;mathematics;least squares	Logic	69.47617122461291	22.18274583165463	89930
de0d0b4fbe191fd9de654d27367d52d52e4b4a06	on the spectral norms of several iterative processes	spectral norm		iterative method	J. W. Sheldon	1959	J. ACM	10.1145/320998.321003	computer science;matrix norm;mathematics;algorithm	Theory	80.04594716717125	21.635622728566034	90023
c59e7b245fc88d23ed2d7299219fad956f15ebf0	analysis of henrici's transformation for singular problems	schur complement;singular value decomposition;convergence rate;asymptotic expansion;nonlinear equation	Henrici's transformation is the underlying scheme that generates, by cycling, Steffensen's method for the approximation of the solution of a nonlinear equation in several variables. The aim of this paper is to analyze the asymptotic behavior of the obtained sequence (s n * ) by applying Henrici's transformation when the initial sequence (s n ) behaves sublinearly. We extend the work done in the regular case by Sadok [17] to vector sequences in the singular case. Under suitable conditions, we show that the slowest convergence rate of (s n * ) is to be expected in a certain subspace N of R p . More precisely, if we write s n * =s n * ,N+s n * ,N⊥, the orthogonal decomposition into N and N ⊥, then the convergence is linear for (s n * ,N) but ( n * ,N⊥) converges to the same limit faster than the initial one. In certain cases, we can have N=R p and the convergence is linear everywhere.	approximation;emoticon;nonlinear system;rate of convergence;shanks transformation;steffensen's method	Mohammed Bellalij	2003	Numerical Algorithms	10.1023/A:1025587215883	mathematical optimization;mathematical analysis;discrete mathematics;nonlinear system;calculus;mathematics;schur complement;rate of convergence;singular value decomposition;asymptotic expansion;algebra	Theory	79.85925969978368	20.636371026917743	90101
7a0b705f3414895c1cadc775174bb0739af8195b	a randomly perturbed infomax algorithm for blind source separation	randomly perturbed infomax method blind source separation unstable equilibria;blind source separation;stochastic processes;convergence blind source separation algorithm design and analysis linear programming eigenvalues and eigenfunctions indexes;gradient methods;stochastic processes blind source separation differential equations gradient methods;differential equations;likelihood functions randomly perturbed infomax algorithm blind source separation natural gradient descent limiting ordinary differential equation saddle points local minima random perturbations iterating sequence randomly perturbed algorithm blind demixing stochastic signals	We present a novel modification to the well-known infomax algorithm of blind source separation. Under natural gradient descent, the infomax algorithm converges to a stationary point of a limiting ordinary differential equation. However, due to the presence of saddle points or local minima of the corresponding likelihood function, the algorithm may be trapped around these “bad” stationary points for a long time, especially if the initial data are near them. To speed up convergence, we propose to add a sequence of random perturbations to the infomax algorithm to “shake” the iterating sequence so that it is “captured” by a path descending to a more stable stationary point. We analyze the convergence of the randomly perturbed algorithm, and illustrate its fast convergence through numerical examples on blind demixing of stochastic signals. The examples have analytical structures so that saddle points or local minima of the likelihood functions are explicit.	algorithm;blind signal separation;gradient descent;infomax;information geometry;maxima and minima;numerical analysis;randomness;shake;source separation;stationary process;whole earth 'lectronic link	Qi He;Jack Xin	2013	2013 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2013.6638252	independent component analysis;stochastic process;mathematical optimization;combinatorics;computer science;machine learning;mathematics;blind signal separation;differential equation;statistics	ML	77.0518317456793	26.094718162369986	90152
d1936588b66b403cca27dbf53356d07ce00261e4	an affine scaling interior algorithm via lanczos path for solving bound-constrained nonlinear systems	iterative method;65b99;analisis numerico;line search;interior points;matematicas aplicadas;algorithm performance;mathematiques appliquees;interior point;relacion convergencia;search strategy;taux convergence;global convergence;local convergence;convergence rate;analyse numerique;metodo iterativo;algorithme;acceleration convergence;objective function;algorithm;nonlinear systems;lanczos path;numerical analysis;resultado algoritmo;methode iterative;linear model;numero de condicionamiento;algebra lineal numerica;systeme non lineaire;strategie recherche;algebre lineaire numerique;performance algorithme;backtracking;aceleracion convergencia;condition number;65f35;convergence locale;affine scaling;numerical linear algebra;nonlinear system;applied mathematics;sistema no lineal;non linear system;indice conditionnement;estrategia investigacion;algoritmo;convergence acceleration	In this paper we propose an affine scaling interior algorithm via Lanczos path for solving nonlinear equality systems subject to bounds on variables. Employing the affine scaling Lanczos path search strategy, we obtain an iterative direction by solving the linearize model. By using the line search backtracking technique, we will find an acceptable trial step length along this direction which is strictly feasible and makes the objective function nonmonotonically decreasing. The global convergence and fast local convergence rate of the proposed algorithm are established under some reasonable conditions. Furthermore, the numerical results of the proposed algorithm indicate to be effective.	affine scaling;algorithm;nonlinear system	Chunxia Jia;Detong Zhu	2008	Applied Mathematics and Computation	10.1016/j.amc.2007.05.066	local convergence;mathematical optimization;lanczos algorithm;nonlinear system;numerical analysis;condition number;interior point method;calculus;linear model;mathematics;geometry;iterative method;numerical linear algebra;rate of convergence;line search;algorithm;backtracking	EDA	77.07418731418674	23.3109015351922	90213
d7fcc929676db1dc1bd57eecd0602c27aab77b31	a new algorithm for the symmetric tridiagonal eigenvalue problem	eigenvalue problem	We apply a novel approach to approximate within ϵ to all the eigenvalues of an n × n symmetric tridiagonal matrix A using at most n2([3 log2(625n6)] + (83n − 34)[log2 (log2((λ1 − λn)/(2ϵ))/log2(25n))]) arithmetic operations where λ1 and λn denote the extremal eigenvalues of A. The algorithm can be modified to compute any fixed numbers of the largest and the smallest eigenvalues of A and may also be applied to the band symmetric matrices without their reduction to the tridiagonal form.		Victor Y. Pan;James Demmel	1993	J. Complexity	10.1006/jcom.1993.1025	divide-and-conquer eigenvalue algorithm;tridiagonal matrix;mathematical optimization;combinatorics;mathematics;tridiagonal matrix algorithm;algebra	Theory	81.65012420903541	22.322670476171076	90422
09acb70b1a33e95104139ead5d0de21baaa0afe6	a finite branch-and-bound algorithm for nonconvex quadratic programming via semidefinite relaxations	quadratic programming;condition kuhn tucker;non convex programming;non linear programming;quadratic program;recursive partitioning;programmation semi definie;semidefinite programming;condicion kuhn tucker;programmation quadratique;generation coupe;lift and project relaxations;programacion no lineal;cut generation;branch and bound algorithm;branching;partitioning;karush kuhn tucker method;optimum global;programmation non convexe;karush kuhn tucker;programmation non lineaire;global optimum;methode karush kuhn tucker;programacion no convexa;nonconvex quadratic programming;branch and bound method;relaxation semidefinie;first order;programacion lineal;metodo branch and bound;mathematical programming;ramificacion;generacion corte;ensemble convexe;linear programming;analyse non convexe;programmation lineaire;linear program;ramification;nonconcave quadratic maximization;programacion cuadratica;global optimization;partitionnement;convex set;subdivision;methode separation et evaluation;non convex analysis;programacion semi definida;relajacion semidefinida;semidefinite programming relaxation;branch and bound;programmation mathematique;lp relaxation;optimo global;programacion matematica;semidefinite relaxation;kuhn tucker condition;conjunto convexo;metodo karush kuhn tucker;semidefinite program;semi definite programming;analisis no convexo	Existing global optimization techniques for nonconvex quadratic programming (QP) branch by recursively partitioning the convex feasible set and thus generate an infinite number of branch-and-bound nodes. An open question of theoretical interest is how to develop a finite branch-and-bound algorithm for nonconvex QP. One idea, which guarantees a finite number of branching decisions, is to enforce the first-order Karush-Kuhn-Tucker (KKT) conditions through branching. In addition, such an approach naturally yields linear programming (LP) relaxations at each node. However, the LP relaxations are unbounded, a fact that precludes their use. In this paper, we propose and study semidefinite programming relaxations, which are bounded and hence suitable for use with finite KKT-branching. Computational results demonstrate the practical effectiveness of the method, with a particular highlight being that only a small number of nodes are required.	algorithm;branch and bound;computation;feasible region;first-order predicate;global optimization;karush–kuhn–tucker conditions;linear programming;mathematical optimization;quadratic programming;recursion;semidefinite programming;tucker decomposition	Samuel Burer;Dieter Vandenbussche	2008	Math. Program.	10.1007/s10107-006-0080-6	mathematical optimization;combinatorics;linear programming;mathematics;mathematical economics;quadratic programming;branch and bound;semidefinite programming	ML	72.18083645847192	23.038430531441296	90690
c26c27aa44044dda18bdc095b9060261a49c4196	a new iteration method for a class of complex symmetric linear systems	complex symmetric linear system;iterative methods;pmhss method;convergence theory;numerical experiment;65f10;15a24	In this paper, a new iteration method is proposed for solving the complex symmetric linear systems. In theory, we show that the convergence factor or the upper bound of the spectral radius of the iteration matrix of the new method are smaller than that of the PMHSS method proposed by Bai, Benzi and Chen (Numer. Algor. (2011)56:297-317). Moreover, we analyze and compare the parameter-free versions and the spectrum distributions of the preconditioned matrix of the new method and the PMHSS method. Finally, we present some numerical experiments on a few model problems to illustrate the theoretical results and show the effectiveness of our new method.	ar (unix);emoticon;entity–relationship model;experiment;iteration;linear system;newton's method;numerical analysis;preconditioner	Teng Wang;Qingqing Zheng;Linzhang Lu	2017	J. Computational Applied Mathematics	10.1016/j.cam.2017.05.002	mathematical optimization;combinatorics;mathematical analysis;discrete mathematics;symbolic convergence theory;mathematics;iterative method;algebra	ML	81.24897153373006	21.75901979052035	90872
a702df1b482d1049cfdc556b051d7e37119eb590	constraint preconditioning for indefinite linear systems	numerical solution;grupo de excelencia;preconditioning;spectrum;krylov subspace methods;linear system;numerical analysis;indefinite matrices;ciencias basicas y experimentales;matematicas;tecnologias generalidades;65f50;tecnologias;65f15;65f10;eigenvectors	The problem of finding good preconditioners for the numerical solution of indefinite linear systems is considered. Special emphasis is put on preconditioners that have a 2 × 2 block structure and that incorporate the (1, 2) and (2, 1) blocks of the original matrix. Results concerning the spectrum and form of the eigenvectors of the preconditioned matrix and its minimum polynomial are given. The consequences of these results are considered for a variety of Krylov subspace methods. Numerical experiments validate these conclusions.		Carsten Keller;Nicholas I. M. Gould;Andrew J. Wathen	2000	SIAM J. Matrix Analysis Applications	10.1137/S0895479899351805	spectrum;mathematical optimization;numerical analysis;eigenvalues and eigenvectors;calculus;mathematics;preconditioner;linear system;algebra	Theory	80.42025103691705	21.270968636758006	90907
b702e8792b217ee6772ff26a7ba3f18f9ac001ef	on duality bound methods for nonconvex global optimization	duality bound methods;global optimization;nonconvex global optimization	A counter-example is given to several recently published results on duality bound methods for nonconvex global optimization.	global optimization;mathematical optimization	Hoang Tuy	2007	J. Global Optimization	10.1007/s10898-006-9055-7	mathematical optimization;combinatorics;mathematical analysis;duality;mathematics;strong duality;wolfe duality;global optimization	Vision	72.48540437547501	22.856514528826914	90978
1980707050d6779e20968131e95d13eab7163272	a modular string averaging procedure for solving the common fixed point problem for quasi-nonexpansive mappings in hilbert space	intermittent control;cyclic control;46n40;subgradient projection;convex feasibility problem;superiorization;47n10;block iterative algorithms;quasi nonexpansive operator;47h10;nonexpansive operator;common fixed point problem;65j99;string averaging;perturbation resilience;47h09;cutter;almost cyclic control;46n10;47j25;65f10;firmly nonexpansive operator	In this paper we consider the common fixed point problem for a finite family of quasi-nonexpansive mappings U i : ℋ → ℋ $U_{i}\colon \mathcal H\rightarrow \mathcal H$ , where i ∈ I := {1,…, M}, M≥1, and ℋ $\mathcal H$ is a real Hilbert space. This problem is defined as follows: find x ∈ ⋂ i ∈ I Fix U i ≠ ∅ $x\in \bigcap _{i\in I}\text {Fix} U_{i}\neq \emptyset $ , where Fix U i : = { z ∈ ℋ ∣ z = U i z } $U_{i}:=\{z\in \mathcal H\mid z=U_{i}z\}$ . We propose the following iterative method: x 0 ∈ ℋ , x k + 1 : = T k x k , $$ x^{0}\in\mathcal H,\quad x^{k+1}:=T_{k} x^{k}, $$ where for each k=0,1,2,…, the operator T k is defined by a certain amalgamation procedure called modular string averaging. The main idea of this procedure is to combine repeatedly and recursively three primal operations: relaxation, convex combination and composition of the given operators U i . The modular string averaging procedure, when combined with the above iterative method, provides a very flexible framework which covers and fills the gap between different algorithmic approaches such as string averaging and block iterative schemes. Moreover, our framework enables us to construct many algorithmic schemes, the convergence of which has not been investigated so far. The aim of this paper is to establish both weak and strong convergence results for the above iterative method. Moreover, in the case of firmly nonexpansive U i ’s, we show that convergence is preserved in the presence of inexact computations. In particular, this implies that the iterative scheme is resilient to bounded perturbations, which is important from the superiorization methodology point of view.	algorithm;computation;fixed point (mathematics);hilbert space;iterative method;linear programming relaxation;recursion	Simeon Reich;Rafal Zalas	2015	Numerical Algorithms	10.1007/s11075-015-0045-z	mathematical optimization;combinatorics;mathematical analysis;discrete mathematics;calculus;mathematics;algebra	Theory	76.33946202094752	19.49462824392192	91169
ab67d2552505c79780acfc231e82b525523b3157	complete solutions and extremality criteria to polynomial optimization problems	nonlinear programming;duality;system theory;optimization problem;np hard problem;global optimization;polynomial optimization problem;critical point theory;polynomial minimization	Abstract. This paper presents a set of complete solutions to a class of polynomial optimization problems. By using the so-called sequential canonical dual transformation developed in the author’s recent book [Gao, D.Y. (2000), Duality Principles in Nonconvex Systems: Theory, Method and Applications, Kluwer Academic Publishers, Dordrecht/Boston/London, xviii + 454 pp], the nonconvex polynomials in R can be converted into an one-dimensional canonical dual optimization problem, which can be solved completely. Therefore, a set of complete solutions to the original problem is obtained. Both global minimizer and local extrema of certain special polynomials can be indentified by Gao-Strang’s gap function and triality theory. For general nonconvex polynomial minimization problems, a sufficient condition is proposed to identify global minimizer. Applications are illustrated by several examples.	dual polyhedron;mathematical optimization;maxima and minima;optimization problem;polynomial;support vector machine;systems theory	David Yang Gao	2006	J. Global Optimization	10.1007/s10898-005-3068-5	optimization problem;mathematical optimization;combinatorics;mathematical analysis;duality;nonlinear programming;np-hard;mathematics;systems theory;global optimization	ML	70.71237293410242	22.691029697214596	91314
0f01c138d152a0be79c8cdb7f45f565e3ec6d04f	a new condition measure, preconditioners, and relations between different measures of conditioning for conic linear systems	convex programming;90c60;complexity analysis;convex optimization;conditioning;data representation;linear system;preconditioners;90c;90c25;condition number;90c05;problem behavior;complexity of convex programming;interior point algorithm	In recent years, a body of research into “condition numbers” for convex optimization has been developed, aimed at capturing the intuitive notion of problem behavior. This research has been shown to be relevant in studying the efficiency of algorithms (including interior-point algorithms) for convex optimization as well as other behavioral characteristics of these problems such as problem geometry, deformation under data perturbation, etc. This paper studies measures of conditioning for a conic linear system of the form (FPd): Ax = b, x ∈ CX , whose data is d = (A, b). We present a new measure of conditioning, denoted μd, and we show implications of μd for problem geometry and algorithm complexity and demonstrate that the value of μ = μd is independent of the specific data representation of (FPd). We then prove certain relations among a variety of condition measures for (FPd), including μd, σd, χ̄d, and C(d). We discuss some drawbacks of using the condition number C(d) as the sole measure of conditioning of a conic linear system, and we introduce the notion of a “preconditioner” for (FPd), which results in an equivalent formulation (FPd̃) of (FPd) with a better condition number C(d̃). We characterize the best such preconditioner and provide an algorithm and complexity analysis for constructing an equivalent data instance d̃ whose condition number C(d̃) is within a known factor of the best possible.	algorithm;analysis of algorithms;condition number;convex optimization;courant–friedrichs–lewy condition;data (computing);emoticon;flat panel detector;interior point method;linear system;mathematical optimization;nikon cx format;preconditioner	Marina Epelman;Robert M. Freund	2002	SIAM Journal on Optimization	10.1137/S1052623400373829	mathematical optimization;combinatorics;discrete mathematics;convex optimization;conditioning;condition number;mathematics;external data representation;linear system	Theory	74.69182452657948	26.923731522316444	91390
0f5ac3436123dd4d365f1a063ff180d0b29b0010	an efficient sixth-order newton-type method for solving nonlinear systems	iterative method;newton s method;nonlinear systems;computational efficiency	In this paper, we present a new sixth-order iterative method for solving nonlinear systems and prove a local convergence result. The new method requires solving five linear systems per iteration. An important feature of the new method is that the LU (lower upper, also called LU factorization) decomposition of the Jacobian matrix is computed only once in each iteration. The computational efficiency index of the new method is compared to that of some known methods. Numerical results are given to show that the convergence behavior of the new method is similar to the existing methods. The new method can be applied to smalland medium-sized nonlinear systems.	computation;iteration;iterative method;jacobian matrix and determinant;lu decomposition;linear system;local convergence;newton;nonlinear system;numerical method	Xiaofeng Wang;Yang Li	2017	Algorithms	10.3390/a10020045	local convergence;mathematical optimization;mathematical analysis;stone method;nonlinear system;calculus;mathematics;iterative method;newton's method	Robotics	82.5959615922461	18.361989007679313	91437
0ccea77e0309bb72e7ecf03bb65537982739cb24	the linear nonconvex generalized gradient and lagrange multipliers	lagrange multipliers;90c30;lagrange multiplier;nonsmooth analysis;generalized gradient;co derivative;convex hull;49j52	A Lagrange multiplierrules that uses small generalized gradients is introduced. It includes both inequality and set constraints. The generalized gradient is the linear generalized gradient. It is smaller than the generalized gradients of Clarke and Mordukhovich but retains much of their nice calculus. Its convex hull is the generalized gradient of Michel and Penot if a function is Lipschitz. The tools used in the proof of this Lagrange multiplier result are a co-derivative, a chain rule and a scalarization formula for this co-derivative. Many smooth and nonsmooth Lagrange multiplier results are corollaries of this result. It is shown that the technique in this paper can be used for the case of equality, inequality and set constraints if one considers the generalized gradient of Mordukhovich. An open question is if a Lagrange multiplier result holds when one has equality constraints and uses the linear generalized gradient.	boris mordukhovich;convex hull;edmund m. clarke;gradient;lagrange multiplier;social inequality	Jay S. Treiman	1995	SIAM Journal on Optimization	10.1137/0805033	constraint algorithm;mathematical optimization;mathematical analysis;calculus;lagrange's theorem;mathematics;lagrange multiplier	Theory	72.4417639940244	21.19152322240674	91826
8aa57d3f71f0ca13c391d7591bcd20b4523db8b9	benson type algorithms for linear vector optimization and applications	multiple objective optimization;duality;91g99;set valued risk measure;outer approximation;90c29;90 08;linear programming;vector optimization;algorithms;transaction costs;90c05	New versions and extensions of Benson’s outer approximation algorithm for solving linear vector optimization problems are presented. Primal and dual variants are provided in which only one scalar linear program has to be solved in each iteration rather than two or three as in previous versions. Extensions are given to problems with arbitrary pointed solid polyhedral ordering cones. Numerical examples are provided, one of them involving a new set-valued risk measure for multivariate positions.	approximation algorithm;iteration;linear programming;mathematical optimization;numerical method;polyhedron;risk measure;vector optimization	Andreas H. Hamel;Andreas Löhne;Birgit Rudloff	2014	J. Global Optimization	10.1007/s10898-013-0098-2	mathematical optimization;combinatorics;transaction cost;discrete mathematics;duality;linear programming;mathematics;vector optimization	Theory	71.826416962934	23.5114158601182	92327
27b91e0210d505d952f3a4dd2c6ad4a04d93c218	exact solutions of some nonconvex quadratic optimization problems via sdp and socp relaxations	optimal solution;nonconvex quadratic optimization problem;exact solution;objective function;sparsity;second order cone program;second order cone programming relaxation;quadratic optimization;semidefinite programming relaxation;semidefinite program	We show that SDP (semidefinite programming) and SOCP (second order cone programming) relaxations provide exact optimal solutions for a class of nonconvex quadratic optimization problems. It is a generalization of the results by S. Zhang for a subclass of quadratic maximization problems that have nonnegative off-diagonal coefficient matrices of quadratic objective functions and diagonal coefficient matrices of quadratic constraint functions. A new SOCP relaxation is proposed for the class of nonconvex quadratic optimization problems by extracting valid quadratic inequalities for positive semidefinite cones. Its effectiveness to obtain optimal values is shown to be the same as the SDP relaxation theoretically. Numerical results are presented to demonstrate that the SOCP relaxation is much more efficient than the SDP relaxation.	coefficient;expectation–maximization algorithm;lagrangian relaxation;linear programming relaxation;mathematical optimization;numerical method;quadratic programming;quadratically constrained quadratic program;second-order cone programming;semidefinite programming;sockets direct protocol	Sunyoung Kim;Masakazu Kojima	2003	Comp. Opt. and Appl.	10.1023/A:1025794313696	mathematical optimization;combinatorics;mathematical analysis;second-order cone programming;quadratically constrained quadratic program;mathematics;sparsity-of-effects principle;quadratic programming;semidefinite programming	ML	73.66710101826871	24.35715007394361	92812
93c723ac621e495e651dd58a516ff4e5fc11172d	improved projected gradient algorithms for singly linearly constrained quadratic programs subject to lower and upper bounds	projection subproblem;quadratic program;lower and upper bound;monotone projected gradient algorithm;support vector machine svm;gradient method;optimization problem;dai yuan step size;support vector machine;numerical experiment;adaptive steepest descent step size;steepest descent	In this paper, we consider the projected gradient algorithms for solving the quadratic program with bound constraints and a single linear equality constraint (SLBQP). We establish the relationship between the Lagrangian multiplier in the projection subproblem and the Lagrangian multiplier in the original optimization problem. Then we give an improved initial estimate of the Lagrangian multiplier in the subproblem based on this relationship. It appears that this initial estimate is very close to the optimal Lagrangian multiplier after several iterations of the outer loop. This will reduce at most 40% of the computing time in the projection subproblem. This initial guess can also be used in all kinds of projected gradient methods for solving the SLBQP problem. The numerical results show that it brings much more improvement in monotone algorithms than in nonmonotone algorithms. We also apply the adaptive steepest descent step-size and the Dai-Yuan step-size which are two monotone step-sizes to the projected gradient method of this SLBQP problem. Our numerical experiments showed that their performance can be better than some other monotone projected gradient methods.	algorithm;gradient	Yun-Shan Fu;Yu-Hong Dai	2010	APJOR	10.1142/S0217595910002594	gradient descent;optimization problem;support vector machine;mathematical optimization;combinatorics;computer science;gradient method;calculus;mathematics;proximal gradient methods;nonlinear conjugate gradient method;quadratic programming	Theory	75.07460157693592	24.23712221403373	92941
2722ffea768b47afe779d46cb7da45cdcf8101e6	simultaneously sparse solutions to linear inverse problems with multiple system matrices and a single observation vector	metodo cuadrado menor;sistema lineal;metodo directo;calcul scientifique;health research;second order cone programming;uk clinical guidelines;iterative method;biological patents;orthogonal matching pursuit;methode moindre carre;15a29;analisis numerico;resonance;programmation;linear inverse problem;least squares method;65f05;algorithme glouton;05bxx;europe pubmed central;magnetic resonance imaging excitation pulse design;aproximacion;resonancia;citation search;sparse set;sparse approximation;matrix inversion;problema inverso;inversion matriz;linear system;magnetic resonance image;analyse numerique;approximation;programacion;metodo iterativo;simultaneous sparse approximation;np hard problem;multiple measurement vectors;computacion cientifica;14c20;radio frequency;numerical analysis;inverse problem;second order cone program;uk phd theses thesis;methode iterative;iteratively reweighted least squares;65j22;algebra lineal numerica;life sciences;algebre lineaire numerique;least squares matching;inversion matrice;greedy algorithm;matching pursuit;algoritmo gloton;iterative shrinkage;94a12;ensemble epars;34a55;numerical linear algebra;convex relaxation;systeme lineaire;35b34;scientific computation;46n10;uk research reports;iterative reweighted least squares;methode directe;medical journals;programming;probleme inverse;65f10;direct method;europe pmc;biomedical research;bioinformatics	A problem that arises in slice-selective magnetic resonance imaging (MRI) radio-frequency (RF) excitation pulse design is abstracted as a novel linear inverse problem with a simultaneous sparsity constraint. Multiple unknown signal vectors are to be determined, where each passes through a different system matrix and the results are added to yield a single observation vector. Given the matrices and lone observation, the objective is to find a simultaneously sparse set of unknown vectors that approximately solves the system. We refer to this as the multiple-system single-output (MSSO) simultaneous sparse approximation problem. This manuscript contrasts the MSSO problem with other simultaneous sparsity problems and conducts an initial exploration of algorithms with which to solve it. Greedy algorithms and techniques based on convex relaxation are derived and compared empirically. Experiments involve sparsity pattern recovery in noiseless and noisy settings and MRI RF pulse design.	algorithm;authorization;behavior;cardiomyopathies;copyright;excitation;experiment;greedy algorithm;high-frequency ventilation;iterative method;iteratively reweighted least squares;linear programming relaxation;loss function;magnetic resonance imaging;manuscripts;omeprazole;openmp;optimization problem;radio frequency;second-order cone programming;sparse approximation;sparse language;sparse matrix;mapped	Adam C. Zelinski;Vivek K. Goyal;Elfar Adalsteinsson	2010	SIAM journal on scientific computing : a publication of the Society for Industrial and Applied Mathematics	10.1137/080730822	mathematical optimization;magnetic resonance imaging;calculus;mathematics;algorithm;statistics;matching pursuit	ML	78.2981771173299	23.107352028782476	93135
b3850edae731b66a4f6b0bcb39868c860ba3b23f	quasi-nonexpansive iterations on the affine hull of orbits: from mann's mean value algorithm to inertial methods		Fixed point iterations play a central role in the design and the analysis of a large number of optimization algorithms. We study a new iterative scheme in which the update is obtained by applying a composition of quasi-nonexpansive operators to a point in the affine hull of the orbit generated up to the current iterate. This investigation unifies several algorithmic constructs, including Mannu0027s mean value method, inertial methods, and multilayer memoryless methods. It also provides a framework for the development of new algorithms, such as those we propose for solving monotone inclusion and minimization problems.	algorithm;iteration	Patrick L. Combettes;Lilian E. Glaudin	2017	SIAM Journal on Optimization	10.1137/17M112806X	discrete mathematics;monotone polygon;operator (computer programming);orbit;mathematical optimization;forward–backward algorithm;inertial frame of reference;affine hull;mathematics;fixed-point iteration;fixed point;algorithm	Vision	73.45380989181827	23.186489037039905	93154
d1e691d193bc9557e7f5fa671df71f14dd3517b5	multi-objective control-structure optimization via homotopy methods	probability one homotopy;optimal curve tracing;efficient solutions;active set;multi objective optimization;73k;bi objective;control structure optimization;homotopy;homotopy method;65k;control structure;historical collection till dec 2001;49b;65f	A recently developed active set algorithm for tracing parametrized optima is adapted to multi-objective optimization. The algorithm traces a path of Kuhn–Tucker points using homotopy curve tracking techniques, and is based on identifying and maintaining the set of active constraints. Second order necessary optimality conditions are used to determine nonoptimal stationary points on the path. In the bi-objective optimization case the algorithm is used to trace the curve of efficient solutions (Pareto optima). As an example, the algorithm is applied to the simultaneous minimization of the weight and control force of a ten-bar truss with two collocated sensors and actuators, with some interesting results.		Joanna Rakowska;Raphael T. Haftka;Layne T. Watson	1993	SIAM Journal on Optimization	10.1137/0803033	mathematical optimization;combinatorics;discrete mathematics;homotopy analysis method;homotopy perturbation method;homotopy;multi-objective optimization;mathematics;control flow	Theory	69.3653983339453	23.159040419475257	93256
ac3a8b631c96005b96cedf26d1e4fa79373ebff9	computing the determinants of matrix padé approximation	power series;analisis numerico;matematicas aplicadas;mathematiques appliquees;pade approximation;schur complement;skew symmetric systems;serie entiere;analyse numerique;1509;numerical analysis;serie potencias;aproximacion pade;arnoldi process;determinante;aproximacion matriz;approximant pade;determinant;aproximante pade;matrix approximation;symmetric system;generalized inverse;pade approximant;approximation pade;41a21;applied mathematics;systeme symetrique;sistema simetrico;approximation matricielle	This paper is devoted to a computational problem of two special determinants which appear in the construction of generalized inverse matrix Pade approximants of type [n/2k] for the given power series with matrix coefficients. The main tools to be used are well-known Schur complement theorem and Arnoldi process for skew-symmetric systems.	approximation;padé approximant	Jindong Shen;Chuanqing Gu	2009	Applied Mathematics and Computation	10.1016/j.amc.2009.04.009	padé approximant;mathematical optimization;mathematical analysis;padé table;calculus;mathematics;schur complement;state-transition matrix;algebra	Theory	79.06165475261122	19.3552131624862	93361
067f212f01449753c3bbd01bf4cd5bac12a209e6	on centroidal voronoi tessellation - energy smoothness and fast computation	constrained cvt;centroidal voronoi tessellation;remeshing;lloyd s method;optimal method;numerical optimization;energy function;computer graphic;computational science and engineering;voronoi tessellation;quasi newton methods;quasi newton method;linear convergence;newton method;convex domain;article	Centroidal Voronoi tessellation (CVT) is a particular type of Voronoi tessellation that has many applications in computational sciences and engineering, including computer graphics. The prevailing method for computing CVT is Lloyd's method, which has linear convergence and is inefficient in practice. We develop new efficient methods for CVT computation and demonstrate the fast convergence of these methods. Specifically, we show that the CVT energy function has 2nd order smoothness for convex domains with smooth density, as well as in most situations encountered in optimization. Due to the 2nd order smoothness, it is possible to minimize the CVT energy functions using Newton-like optimization methods and expect fast convergence. We propose a quasi-Newton method to compute CVT and demonstrate its faster convergence than Lloyd's method with various numerical examples. It is also significantly faster and more robust than the Lloyd-Newton method, a previous attempt to accelerate CVT. We also demonstrate surface remeshing as a possible application.	centroidal voronoi tessellation;computation;computational science;computer graphics;mathematical optimization;newton;newton's method;numerical analysis;quasi-newton method;rate of convergence	Yang Liu;Wenping Wang;Bruno Lévy;Feng Sun;Dong-Ming Yan;Lin Lu;Chenglei Yang	2009	ACM Trans. Graph.	10.1145/1559755.1559758	mathematical optimization;quasi-newton method;voronoi diagram;centroidal voronoi tessellation;computational science and engineering;theoretical computer science;mathematics;geometry;newton's method;rate of convergence	Graphics	76.13285746148779	25.40641009520483	93422
ca525aab984749b010b838be26360501395fce2a	generalized rybicki press algorithm	rybicki press algorithm;fast determinant computation;exponential covariance;carma processes;fast direct solver;semi separable matrices	This article discusses a more general and numerically stable Rybicki Press algorithm, which enables inverting and computing determinants of covariance matrices, whose elements are sums of exponentials. The algorithm is true in exact arithmetic and relies on introducing new variables and corresponding equations, thereby converting the matrix into a banded matrix of larger size. Linear complexity banded algorithms for solving linear systems and computing determinants on the larger matrix enable linear complexity algorithms for the initial semi-separable matrix as well. Benchmarks provided illustrate the linear scaling of the algorithm.	ab initio quantum chemistry methods;benchmark (computing);c++;computation;embedded system;image scaling;linear system;numerical stability;rybicki press algorithm;semiconductor industry;sparse matrix;the matrix	Sivaram Ambikasaran	2015	Numerical Lin. Alg. with Applic.	10.1002/nla.2003	mathematical optimization;combinatorics;theoretical computer science;mathematics;algorithm;algebra	ML	81.7009534446656	22.854987198671058	93452
142477cb71699f5acaed58ff75c099bae7dbc864	an exact penalty global optimization approach for mixed-integer programming problems	003 sistemi classificare qui la ricerca operativa;mixed integer program;i modelli simulazione applicati ai sistemi in modo reale;mixed integer programming;global optimization;exact penalty functions;penalty function;la teoria l analisi la progettazione l ottimizzazione dei sistemi	In this work, we propose a global optimization approach for mixed-integer programming problems. To this aim, we preliminarily define an exact penalty algorithm model for globally solving general problems and we show its convergence properties. Then, we describe a particular version of the algorithm that solves mixed integer problems.	algorithm;global optimization;integer programming;linear programming;mathematical optimization	Stefano Lucidi;Francesco Rinaldi	2013	Optimization Letters	10.1007/s11590-011-0417-9	mathematical optimization;combinatorics;penalty method;mathematics;algorithm;global optimization	AI	74.77930328432048	22.533664199933295	93453
d8123022351d0e2afbabfb4f81e6c7f5d93c1805	entropy based transportation problem use geometric programming method	analisis numerico;entropia;problema transporte;transportation problem;matematicas aplicadas;mathematiques appliquees;constrenimiento igualdad;geometric program;probleme transport;primal dual method;spatial interaction;optimization method;lagrange multiplier;methode primale duale;dual problem;metodo optimizacion;analyse numerique;algorithme;equality constraint;algorithm;numerical analysis;metodo primal dual;mathematical programming;entropie;multiplicateur lagrange;programacion geometrica;methode optimisation;multiplicador lagrange;optimization;entropy;applied mathematics;programmation mathematique;geometric programming;programacion matematica;programmation geometrique;contrainte egalite;algoritmo	In this paper, we have analysed variety entropy models by geometric programming. Entropy models are emerging as valuable tools in the study of various social and engineering problems of spatial interaction. With the development of the modeling has come diversity. Increased flexibility in the model can be obtained by allowing certain constraints to be relaxed from equality to inequality. To provide a better understanding of this entropy based transportation model they are analysed by geometric programming. Finally the algorithm are developed using pair of primal and dual problems and are relatively simple to program. 2006 Published by Elsevier Inc.	algorithm;dual polyhedron;entropy (information theory);geometric programming;lagrangian relaxation;social inequality;transportation theory (mathematics)	Bablu Samanta;Sanat Kumar Majumder	2006	Applied Mathematics and Computation	10.1016/j.amc.2005.11.154	entropy;mathematical optimization;geometric programming;calculus;mathematics;algorithm	Robotics	73.63389856582369	21.342116625049776	93469
30d7ebc450890f1ea49378c66b8d9e1f4aed700b	iteration functions re-visited	computer science and information systems;iteration functions;multiple zeros;polynomial zeros	Two classes of Iteration Functions (IFs) are derived in this paper. The first (one-point IFs) was originally derived by Joseph Traub using a different approach to ours (simultaneous IFs). The second is new and is demonstrably shown to be more informationally efficient than the first. These IFs apply to polynomials with arbitrary complex coefficients and zeros, which can also be multiple.		Michael Farmer;George Loizou;Stephen J. Maybank	2017	J. Computational Applied Mathematics	10.1016/j.cam.2016.08.021	mathematical optimization;mathematical analysis;discrete mathematics;jenkins–traub algorithm;mathematics	Theory	76.82046055120317	19.777708418025725	93686
2ddf0540d48b1f2566910158a4d7c50b6d386056	inexact semismooth newton methods for large-scale complementarity problems	obstacle problem;linear system of equations;inexact newton method;semismooth newton method;newton s method;optimal control problems;large scale;semismooth functions;obstacle problems;complementarity problems;complementarity problem;point of view;system of equations;optimal control problem	The semismooth Newton method is a nonsmooth Newton-type method applied to a suitable reformulation of the complementarity problem as a nonlinear and nonsmooth system of equations. It is one of the standard methods for solving these kind of problems, and it can be implemented in an inexact way so that all linear systems of equations have to be solved only inexactly. However, from a practical point of view, this inexact Newton method seems to have a significantly worse behaviour than its exact counterpart. The aim of this paper is therefore to show that the inexact Newton method can also be used in a reliable and efficient way at least for some classes of problems. We illustrate this statement by some numerical examples with up to one million variables.	complementarity theory;linear system;newton;newton's method;nonlinear system;numerical analysis	Christian Kanzow	2004	Optimization Methods and Software	10.1080/10556780310001636369	system of linear equations;mathematical optimization;mathematical analysis;calculus;mathematics	ML	77.70977495529655	24.448990001976984	93788
ec16c6dd253893257c05412eaaa3abad5bf2d595	on homogeneous and self-dual algorithms for lcp	modelo homogeneo;interior point;infeasible starting algorithm;methode point interieur;homogeneous and self dual model;modele homogene;algorithme;metodo punto interior;mathematical programming;probleme complementarite lineaire;linear programming;programmation lineaire;linear program;algorithms;probleme complementarite;problema complementariedad;complementarity problem;linear complementarity problem;interior point method;programmation mathematique;homogeneous model;modele autodual	"""We present some generalizations of a homogeneous and self-dual linear programming (LP) algorithm to solving the monotone linear complementarity problem (LCP). Again, while it achieves the best known interior-point iteration complexity, the algorithm does not need to use any """"b ig -M"""" number, and it detects LCP infeasibility by generating a certificate. To our knowledge, this is the first interior-point and infeasible-starting algorithm for the LCP with these desired features."""	algorithm;complementarity theory;interior point method;iteration;linear complementarity problem;linear programming;monotone	Yinyu Ye	1996	Math. Program.	10.1007/BF02614384	mathematical optimization;linear programming;interior point method;calculus;mathematics;algorithm	Theory	75.1537564135434	22.09202993017685	94111
71c92a57eb2a5d9b7c6239f30dd1a934514b1f6b	a hybridization of the polak-ribière-polyak and fletcher-reeves conjugate gradient methods	global convergence;conjugate gradient method;65k05;unconstrained optimization;large scale optimization;49m37;90c53	In order to achieve a theoretically effective and numerically efficient method for solving large-scale unconstrained optimization problems, a hybridization of the Fletcher-Reeves and Polak-Ribière-Polyak conjugate gradient methods is proposed. In the method, the hybridization parameter is computed such that the generated search directions approach to the search directions of the efficient three-term conjugate gradient method proposed by Zhang et al., to the extent possible. Under proper conditions, global convergence of the method is established without convexity assumption on the objective function. The method is numerically compared with the three-term conjugate gradient method proposed by Zhang et al. and a modified version of the Polak-Ribière-Polyak method suggested by Gilbert and Nocedal. Comparative testing results demonstrating the efficiency of the proposed method are reported.	broyden–fletcher–goldfarb–shanno algorithm;conjugate gradient method;gilbert cell;local convergence;mathematical optimization;numerical analysis;optimization problem	Saman Babaie-Kafaki;Reza Ghanbari	2014	Numerical Algorithms	10.1007/s11075-014-9856-6	gradient descent;mathematical optimization;mathematical analysis;conjugate residual method;gradient method;calculus;derivation of the conjugate gradient method;newton's method in optimization;mathematics;conjugate gradient method;biconjugate gradient stabilized method;nonlinear conjugate gradient method;biconjugate gradient method;line search;algebra	AI	76.64204388137517	23.541226845983463	94457
117f91911df5a256f9839013baac7cbcc8178282	jacobi method for symmetric 4 × 4 matrices converges for every cyclic pivot strategy	eigenvalues;jacobi method;global convergence;65f15	The paper studies the global convergence of the Jacobi method for symmetric matrices of size 4. We prove global convergence for all 720 cyclic pivot strategies. Precisely, we show that inequality S(A [t+3]) ≤ γ S(A [t]), t ≥ 1, holds with the constant γ < 1 that depends neither on the matrix A nor on the pivot strategy. Here, A [t] stands for the matrix obtained from A after t full cycles of the Jacobi method and S(A) is the off-diagonal norm of A. We show why three consecutive cycles have to be considered. The result has a direct application on the J-Jacobi method.	jacobi method;local convergence;social inequality;the matrix	Erna Begovic Kovac;Vjeran Hari	2017	Numerical Algorithms	10.1007/s11075-017-0396-8	mathematical analysis;mathematics;inequality;mathematical optimization;jacobi eigenvalue algorithm;jacobi operator;jacobi rotation;eigenvalues and eigenvectors;jacobi method;matrix (mathematics);symmetric matrix	ML	79.24205828259669	20.59895658749715	94558
b958e5ea157de6aed6d2654ca5593cc552cde45e	a secret to create a complete market from an incomplete market	continuous time;analisis numerico;matematicas aplicadas;mathematiques appliquees;brownian motion;65kxx;martingale method;duality;temps continu;optimization method;tiempo continuo;metodo optimizacion;analyse numerique;utility maximization;49xx;dualite;numerical analysis;mouvement brownien;mathematical programming;methode optimisation;movimiento browniano;dualidad;applied mathematics;programmation mathematique;complete market;programacion matematica;optimal portfolio;incomplete market	Abstract   The Martingale method has been given increasing attention since it was conducted by Cox and Huang in 1989. Martingale method allows us to solve the problems of utility maximization in a very elegant manner. However, the Martingale method is not omnipotent. When the market is incomplete, traditional Martingale method will be problematic. To overcome the problem of incompleteness, Karatzas/Shreve/Xu [Martingale and duality for utility maximization in an incomplete market, SIAM J. Control and Optimization 29 (1991) 702–730] have developed a way to complete the market by introducing additional  fictitious  stocks and then making them uninteresting to the investor. Nevertheless, to find such fictitious stocks is not straightforward. In particular, when the number of such stocks needed in order to complete the market were very large, it would be very computational, and even may not be possible to be expressed explicitly. To make life easier, we provide an alternative method by directly creating a complete market from the incomplete one such that the dimension of the underlying Brownian motion equals the number of available stocks. Our approach is ready to be used.		Aihua Zhang	2007	Applied Mathematics and Computation	10.1016/j.amc.2007.02.086	mathematical optimization;mathematical analysis;duality;numerical analysis;martingale pricing;brownian motion;mathematics;mathematical economics;incomplete markets;complete market;algorithm;statistics	ECom	75.85382532272978	19.92611698274155	94616
f350f5e8ca485daf2367a08221e1e6c3cad0e78d	a unified approach to the global exactness of penalty and augmented lagrangian functions i: parametric exactness	penalty function;augmented lagrangian function;exactness;localization principle;semidefinite programming;65k05;90c30	In this two-part study, we develop a unified approach to the analysis of the global exactness of various penalty and augmented Lagrangian functions for constrained optimization problems in finite-dimensional spaces. This approach allows one to verify in a simple and straightforward manner whether a given penalty/augmented Lagrangian function is exact, i.e., whether the problem of unconstrained minimization of this function is equivalent (in some sense) to the original constrained problem, provided the penalty parameter is sufficiently large. Our approach is based on the so-called localization principle that reduces the study of global exactness to a local analysis of a chosen merit function near globally optimal solutions. In turn, such local analysis can be performed with the use of optimality conditions and constraint qualifications. In the first paper, we introduce the concept of global parametric exactness and derive the localization principle in the parametric form. With the use of this version of the localization principle, we recover existing simple, necessary, and sufficient conditions for the global exactness of linear penalty functions and for the existence of augmented Lagrange multipliers of Rockafellar–Wets’ augmented Lagrangian. We also present completely new necessary and sufficient conditions for the global exactness of general nonlinear penalty functions and for the global exactness of a continuously differentiable penalty function for nonlinear second-order cone programming problems. We briefly discuss how one can construct a continuously differentiable exact penalty function for nonlinear semidefinite programming problems as well.	augmented lagrangian method	Maxim V. Dolgopolik	2018	J. Optimization Theory and Applications	10.1007/s10957-018-1238-0		PL	74.2591117008505	23.912983938918057	94832
77004f95b306383acf7b6a4b704239594365fddb	variational piecewise constant level set methods for shape optimization of a two-density drum	two phase;lagrange multiplier;topology optimization;satisfiability;eigenvalues;eigenvalue;augmented lagrangian method;shape optimization;level set methods;piecewise constant level set method;variational method;level set method;constrained optimization problem	We apply the piecewise constant level set method to a class of eigenvalue related two-phase shape optimization problems. Based on the augmented Lagrangian method and the Lagrange multiplier approach, we propose three effective variational methods for the constrained optimization problem. The corresponding gradient-type algorithms are detailed. The first Uzawa-type algorithm having applied to shape optimization in the literature is proven to be effective for our model, but it lacks stability and accuracy in satisfying the geometry constraint during the iteration. The two other novel algorithms we propose can overcome this limitation and satisfy the geometry constraint very accurately at each iteration. Moreover, they are both highly initial independent and more robust than the first algorithm. Without penalty parameters, the last projection Lagrangian algorithm has less severe restriction on the time step than the first two algorithms. Numerical results for various instances are presented and compared with those obtained by level set methods. The comparisons show effectiveness, efficiency and robustness of our methods. We expect our promising algorithms to be applied to other shape optimization and multiphase problems.	augmented lagrangian method;calculus of variations;computer simulation;constraint satisfaction;expectation–maximization algorithm;global serializability;gradient descent;hybrid algorithm;iteration;iterative method;jacobi method;lagrange multiplier;mathematical optimization;newton;newton's method;numerical method;pixel;quasi-newton method;reflections of signals on conducting lines;semiconductor industry;shape optimization;topology optimization;two-phase locking;variational principle	Shengfeng Zhu;Qingbiao Wu;Chunxiao Liu	2010	J. Comput. Physics	10.1016/j.jcp.2010.03.026	mathematical optimization;combinatorics;topology optimization;mathematical analysis;augmented lagrangian method;lagrangian relaxation;eigenvalues and eigenvectors;shape optimization;penalty method;mathematics	AI	75.6008554149697	24.48427965082881	94928
d8db3b862cec6a8d3181ad761eed6d46ada16a19	a class of iteration methods based on the generalized preconditioned hermitian and skew-hermitian splitting for weakly nonlinear systems	gphss iteration method;computacion informatica;hss iteration method;ciencias basicas y experimentales;matematicas;65t50;65w05;inner outer iteration scheme;65f50;grupo a;weakly nonlinear systems;65f10	For large sparse systems of weakly nonlinear equations, based on the separable property and strong dominance between the linear and the nonlinear terms, Bai and Yang studied two nonlinear composite iteration schemes called Picard-HSS and nonlinear HSS-like methods (see [Z.-Z. Bai, X. Yang, On HSS-based iteration methods for weakly nonlinear systems, Appl. Numer. Math. 59 (12) (2009) 2923–2936]). In this paper, we generalize these methods and propose a class of generalized nonlinear composite splitting iteration schemes called Picard-GPHSS and nonlinear GPHSS-like iteration methods, to solve the large sparse systems of weakly nonlinear equations. We derive conditions for guaranteeing the local convergence of these iterative methods and derive some special case of iterative methods by choosing different parameters and preconditioned matrices. Numerical experiments are used to demonstrate the feasibility and effectiveness. And an efficient preconditioner is presented for the new methods in actual implementation. The efficiency is effectively testified by some comparisons with numerical results.	iteration;iterative method;nonlinear system;preconditioner	Zhao-Nian Pu;Mu-Zheng Zhu	2013	J. Computational Applied Mathematics	10.1016/j.cam.2013.02.021	fixed-point iteration;mathematical optimization;mathematical analysis;discrete mathematics;power iteration;calculus;mathematics;preconditioner	Robotics	81.09934719397121	21.848439128312748	95305
28877168b0f032ee36432b5360c94930e6e743d9	computing the value of the convex envelope of quadratic forms over polytopes through a semidefinite program	semidefinite programming;quadratic forms;convex envelope	In this paper we show that at any point of a given polytope P , the value and a supporting hyperplane at that point for the convex envelope of any n-dimensional quadratic form over P , can be computed through the solution of a semidefinite problem. The result is also extended to a broader class of functions. © 2013 Elsevier B.V. All rights reserved.	convex hull;semidefinite programming	Marco Locatelli	2013	Oper. Res. Lett.	10.1016/j.orl.2013.04.004	mathematical optimization;conic optimization;combinatorics;discrete mathematics;convex optimization;convex polytope;quadratic form;second-order cone programming;linear matrix inequality;quadratically constrained quadratic program;mathematics;semidefinite embedding;semidefinite programming	Theory	70.56995926281395	21.40960432330211	95578
57715dd14da77a22d2956a3518ec7871fdb9dff8	strong convergence to solutions for a class of variational inequalities in banach spaces by implicit iteration methods		In this paper, in order to solve a variational inequality problem over the set of common fixed points of an infinite family of nonexpansive mappings on a real reflexive and strictly convex Banach space with a uniformly Gâteaux differentiable norm, we introduce two new implicit iteration methods. Their strong convergence is proved, by using new V -mappings instead of W -ones.	algorithm;calculus of variations;convex function;gradient descent;gâteaux derivative;iteration;social inequality;spaces;tomotaka takahashi;variational inequality;variational principle	Nguyen Buong;Nguyen Thi Hong Phuong	2013	J. Optimization Theory and Applications	10.1007/s10957-013-0350-4	mathematical optimization;mathematical analysis;discrete mathematics;uniformly convex space;mathematics	ML	73.07951552915674	21.438689698549915	95666
07b724bcfb149705831b68b631155a25a85d7675	optimal control of uncertain stochastic systems subject to total variation distance uncertainty	49k35;93e20;60h10;minimax problems;total variation;optimal stochastic control	This paper is concerned with optimization of uncertain stochastic systems, in which uncertainty is described by a total variation distance constraint between the measures induced by the uncertain systems and the measure induced by the nominal system, while the pay-off is a linear functional of the uncertain measure. Robustness at the abstract setting is formulated as a minimax game, in which the control seeks to minimize the pay-off over the admissible controls while the uncertainty aims at maximizing it over the total variation distance constraint. By invoking the Hanh-Banach theorem, it is shown that the maximizing measure in the total variation distance constraint exists, while the resulting pay-off is a linear combination of L1 and L∞ norms. Further, the maximizing measure is characterized by a linear combination of a tilted measure and the nominal measure, giving rise to a pay-off which is a non-linear functional on the space of measures to be minimized over the admissible controls. The abstract formulation and results are subsequently applied to continuous-time uncertain stochastic controlled systems, in which the control seeks to minimize the pay-off while the uncertainty aims to maximize it over the total variation distance constraint. The minimization over the admissible controls of the non-linear functional pay-off is addressed by developing a generalized principle of optimality or dynamic programming equation satisfied by the value function. Subsequently, it is proved that the value function satisfies a generalized Hamilton-Jacobi-Bellman (HJB) equation. Finally, it is shown that the value function is also a viscosity solution of the generalized HJB equation. Throughout the paper the formulation and conclusions are related to previous work found in the literature.	bellman equation;calculus of variations;convex function;dynamic programming;hamilton–jacobi–bellman equation;jacobi method;level of measurement;mathematical optimization;maximising measure;minimax;nonlinear system;optimal control;social inequality;spaces;stochastic process;viscosity solution	Farzad Rezaei;Charalambos D. Charalambous;Nasir Uddin Ahmed	2012	SIAM J. Control and Optimization	10.1137/100786381	mathematical optimization;combinatorics;mathematical analysis;discrete mathematics;mathematics;total variation	ML	71.40414741390077	20.14884796582071	96010
6487722a761a967dc8dee62d31fbef4a5463801e	diagonal update method for a quadratic matrix equation	quadratic matrix equation;m matrix;fixed point iteration;bernoulli s method;65h10;diagonal update method	The quadratic matrix equation A X 2 + B X + C = 0 has been studied in many researches. The sufficient condition for the existence of the primary solution is already provided where the equation arose from an overdamped vibrating system with M-matrix coefficients. The Bernoulli's method can not be modified by relaxation in this equation. In this paper, we will introduce the diagonal update method and modify the Bernoulli's method.		Young-Jin Kim;Hyun-Min Kim	2016	Applied Mathematics and Computation	10.1016/j.amc.2016.02.016	matrix difference equation;fixed-point iteration;bernoulli process;mathematical optimization;combinatorics;mathematical analysis;sylvester's law of inertia;band matrix;riccati equation;square matrix;mathematics;m-matrix;diagonal matrix;matrix differential equation;symmetric matrix;algebra	Vision	79.89170313966827	20.29158059559184	96151
4a544fca4aefa70f1f8d6335d83ac8b5f59014bf	an accelerated symmetric sor-like method for augmented systems		Abstract Recently, Njeru and Guo presented an accelerated SOR-like (ASOR) method for solving the large and sparse augmented systems. In this paper, we establish an accelerated symmetric SOR-like (ASSOR) method, which is an extension of the ASOR method. Furthermore, the convergence properties of the ASSOR method for augmented systems are studied under suitable restrictions, and the functional equation between the iteration parameters and the eigenvalues of the relevant iteration matrix is established in detail. Finally, numerical examples show that the ASSOR is an efficient iteration method.		Cheng-Liang Li;Changfeng Ma	2019	Applied Mathematics and Computation	10.1016/j.amc.2018.08.003	iterative method;mathematical analysis;mathematical optimization;functional equation;eigenvalues and eigenvectors;mathematics;matrix (mathematics)	Robotics	81.07198561174593	21.873121993732614	96482
4c1f486821e9ce2e3b7441bf212d17bc8750d952	on duality theory for non-convex semidefinite programming	invex function;semidefinite programming;duality;duality theory;convex like function;nonconvex semidefinite programming;semidefinite program	In this paper, with the help of convex-like function, we discuss the duality theory for nonconvex semidefinite programming. Our contributions are: duality theory for the general nonconvex semidefinite programming when Slater’s condition holds; perfect duality for a special case of the nonconvex semidefinite programming for which Slater’s condition fails. We point out that the results of [2] can be regarded as a special case of our result. Mathematics Subject Classifications: 65K05, 90C22, 90C26,	semidefinite programming	Wenyu Sun;Chengjin Li;Raimundo J. B. de Sampaio	2011	Annals OR	10.1007/s10479-011-0861-z	mathematical optimization;combinatorics;mathematical analysis;duality;duality;duality gap;weak duality;quadratically constrained quadratic program;mathematics;semidefinite embedding;strong duality;slater's condition;semidefinite programming	ML	71.81972098903772	21.698455160094166	96778
5a6eb04c261b7e668a3703bbd402805a63ddb3d8	a general framework for establishing polynomial convergence of long-step methods for semidefinite programming	interior point methods;semidefinite programming;newton directions;satisfiability;symmetric matrix;upper bound;cholesky factorization;path following methods;central path;interior point method;long step methods;path following;semidefinite program	This article considers feasible long-step primal–dual path-following methods for semidefinite programming based on Newton directions associated with central path equations of the form ðPXP ,P SP Þ I 1⁄4 0, where the map and the nonsingular matrix P satisfy several key properties. An iteration-complexity bound for the long-step method is derived in terms of an upper bound on a certain scaled norm of the second derivative of . As a consequence of our general framework, we derive polynomial iteration-complexity bounds for long-step algorithms based on the following four maps: ðX,SÞ 1⁄4 ðXS þ SXÞ=2, ðX,SÞ 1⁄4 Lx SLx, ðX ,SÞ 1⁄4 XSX, and ðX,SÞ 1⁄4WXSW , where Lx is the lower Cholesky factor of X and W is the unique symmetric matrix satisfying S 1⁄4WXW .	algorithm;cholesky decomposition;iteration;map;newton;polynomial;semidefinite programming	Samuel Burer;Renato D. C. Monteiro	2003	Optimization Methods and Software	10.1080/1055678031000111227	mathematical optimization;combinatorics;discrete mathematics;interior point method;mathematics;semidefinite programming	ML	74.28168827189371	22.279856422908857	96894
12269219d57b812e9e2e392c4d9ba584c6809ef1	a unified proof for the convergence of jacobi and gauss-seidel methods	gauss seidel method;methode gauss seidel;65 01;matrice diagonalement dominante;eigenvalues;convergence numerique;numerical convergence;iterative methods;resolucion sistema ecuacion;resolution systeme equation;metodo gauss seidel;methode jacobi;metodo jacobi;equation system solving;linear equations;system of equations;iteration method;gauss seidel;convergencia numerica;65f10;jacobi method;systems of equations	We present a new unified proof for the convergence of both the Jacobi and the Gauss-Seidel methods for solving systems of linear equations under the criterion of either (a) strict diagonal dominance of the matrix, or (b) diagonal dominance and irreducibility of the matrix. These results are well known. The proof for criterion (a) makes use of Gerggorin's theorem, while the proof for criterion (b) uses Taussky's theorem that extends Gersgorin's work. Hence the topic is interesting for teaching purposes.	diagonally dominant matrix;emoticon;gauss–seidel method;irreducibility;jacobi method;linear equation;system of linear equations;the matrix	Roberto Bagnara	1995	SIAM Review	10.1137/1037008	system of linear equations;mathematical optimization;mathematical analysis;calculus;analytic proof;mathematics;geometry;iterative method;gauss–seidel method;algebra	Theory	80.62473582266873	21.407275315674802	96977
641cb4465ab624612b4c00a00a3bc724a3bdbf02	optimal parameter in hermitian and skew-hermitian splitting method for certain two-by-two block matrices	sistema lineal;metodo directo;calcul scientifique;matriz bloque;analisis numerico;splitting;splitting method;optimal estimation;splitting iteration method;g 1 3;matrix inversion;inversion matriz;linear system;analyse numerique;matrice non hermitienne;computacion cientifica;iteraccion;numerical analysis;matrice bloc;non hermitian matrix;algebra lineal numerica;matrice hermitienne;algebre lineaire numerique;iteration;inversion matrice;block matrix;numerical linear algebra;matriz hermitiana;estimation optimale;65f50;systeme lineaire;skew hermitian matrix;scientific computation;iteration method;methode directe;hermitian matrix;65f10;direct method;optimal iteration parameter;estimacion optima	The optimal parameter of the Hermitian/skew-Hermitian splitting (HSS) iteration method for a real 2-by-2 linear system is obtained. The result is used to determine the optimal parameters for linear systems associated with certain 2-by-2 block matrices, and to estimate the optimal parameters of the HSS iteration method for linear systems with n-by-n real coefficient matrices. Numerical examples are given to illustrate the results.	coefficient;high-speed serial interface;iteration;linear system;numerical method;symplectic integrator	Zhong-Zhi Bai;Gene H. Golub;Chi-Kwong Li	2006	SIAM J. Scientific Computing	10.1137/050623644	direct method;hermitian matrix;optimal estimation;combinatorics;skew-hermitian matrix;iteration;jacobi method for complex hermitian matrices;numerical analysis;calculus;mathematics;iterative method;numerical linear algebra;linear system;block matrix;algorithm;splitting;algebra	HPC	81.60739210935391	20.87060011400126	97078
19bba70d656b611f04fb19847064669f8b2fdb2f	subgradient of distance functions with applications to lipschitzian stability	optimisation sous contrainte;constrained optimization;optimisation variationnelle;distance function;parametric programming;analyse variationnelle;optimisation sousgradient;programmation parametrique;espacio banach;variational analysis and optimization;20c20;banach space;sous differentiel;set valued mapping;punto fijo;subdifferential;reference point;optimizacion con restriccion;programacion parametrica;stabilite lipschitz;mathematical programming;point fixe;subgradient optimization;lipschitz continuity;20g40;variational analysis;differenciation generaliree;generalized differentiation;lipschitzian stability;optimizacion subgradiente;dual space;distance functions;programmation mathematique;programacion matematica;fix point;espace banach;subdiferencial	The paper is devoted to studying generalized differential properties of distance functions that play a remarkable role in variational analysis, optimization, and their applications. The main object under consideration is the distance function of two variables in Banach spaces that signifies the distance from a point to a moving set. We derive various relationships between Fréchet-type subgradients and limiting (basic and singular) subgradients of this distance function and corresponding generalized normals to sets and coderivatives of set-valued mappings. These relationships are essentially different depending on whether or not the reference point belongs to the graph of the involved set-valued mapping. Our major results are new even for subdifferentiation of the standard distance function signifying the distance between a point and a fixed set in finite-dimensional spaces. The subdifferential results obtained are applied to deriving efficient dual-space conditions for the local Lipschitz continuity of distance functions generated by set-valued mappings, in particular, by those arising in parametric constrained optimization.	constrained optimization;mathematical optimization;scott continuity;subderivative;subgradient method;variational analysis	Boris S. Mordukhovich;Nguyen Mau Nam	2005	Math. Program.	10.1007/s10107-005-0632-1	subderivative;mathematical optimization;constrained optimization;radial basis function;mathematical analysis;topology;metric;variational analysis;dual space;mathematics;lipschitz continuity;distance;banach space	Theory	71.8363694154689	20.7820796769529	97266
29e1ad50fe375852a30821a89caa443e2af57e25	generalized semi-infinite optimization: a first order optimality condition and examples	generalized semi infinite optimization problem;optimisation;programacion semi infinita;condicion necesaria;first order necessary optimality condition;ordre 1;set valued mapping;optimization problem;condicion optimalidad;fritz john condition;condition optimalite;semi infinite programming;necessary condition;first order;mathematical programming;indexation;optimization;constraint qualification;condition necessaire;programmation semi infinie;condition fritz john;orden 1;necessary optimality condition;optimality condition	We consider a generalized semi-infinite optimization problem (GSIP) of the form (GSIP) min{f(x)‖xeM}, where M={x∈ℝn|hi(x)=0i=l,...m, G(x,y)⩾0, y∈Y(x)} and all appearing functions are continuously differentiable. Furthermore, we assume that the setY(x) is compact for allx under consideration and the set-valued mappingY(.) is upper semi-continuous. The difference with a standard semi-infinite problem lies in thex-dependence of the index setY. We prove a first order necessary optimality condition of Fritz John type without assuming a constraint qualification or any kind of reduction approach. Moreover, we discuss some geometrical properties of the feasible setM.	mathematical optimization;program optimization	Hubertus Th. Jongen;Jan-J. Rückmann;Oliver Stein	1995	Math. Program.	10.1007/BF02680555	optimization problem;mathematical optimization;calculus;first-order logic;mathematics;mathematical economics	Logic	71.78458461869906	21.041030071314648	97328
7e2a14163572acbacb8d421bf06719f98bd982fe	a global optimization using linear relaxation for generalized geometric programming	microordenador;optimal solution;global solution;funcion exponencial;geometric program;fonction exponentielle;generalized geometric programming;exponential function;optimum global;global optimum;microordinateur;microcomputer;linear relaxation;branch and bound method;programacion lineal;metodo branch and bound;programacion geometrica;linear programming;analyse non convexe;programmation lineaire;linear program;global optimization;solution globale;methode separation et evaluation;non convex analysis;branch and bound;geometric programming;optimo global;solucion global;programmation geometrique;analisis no convexo	Many local optimal solution methods have been developed for solving generalized geometric programming (GGP). But up to now, less work has been devoted to solving global optimization of (GGP) problem due to the inherent difficulty. This paper considers the global minimum of (GGP) problems. By utilizing an exponential variable transformation and the inherent property of the exponential function and some other techniques the initial nonlinear and nonconvex (GGP) problem is reduced to a sequence of linear programming problems. The proposed algorithm is proven that it is convergent to the global minimum through the solutions of a series of linear programming problems. Test results indicate that the proposed algorithm is extremely robust and can be used successfully to solve the global minimum of (GGP) on a microcomputer. 2007 Elsevier B.V. All rights reserved.	algorithm;feasible region;general game playing;geometric programming;global optimization;linear programming relaxation;loss function;mathematical optimization;maxima and minima;microcomputer;nonlinear system;numerical analysis;optimization problem;refinement (computing);time complexity	Shao-Jian Qu;Kecun Zhang;Fusheng Wang	2008	European Journal of Operational Research	10.1016/j.ejor.2007.06.034	mathematical optimization;combinatorics;linear programming;calculus;exponential function;mathematics;microcomputer;global optimum;branch and bound	Robotics	74.82999462962948	22.33747557805372	97713
7841cb25c86bc279260035617ff14292728470c4	a constant step stochastic douglas-rachford algorithm with application to non separable regularizations		The Douglas Rachford algorithm is an algorithm that converges to a minimizer of a sum of two convex functions. The algorithm consists in fixed point iterations involving computations of the proximity operators of the two functions separately. The paper investigates a stochastic version of the algorithm where both functions are random and the step size is constant. We establish that the iterates of the algorithm stay close to the set of solution with high probability when the step size is small enough. Application to structured regularization is considered.	algorithm;computation;convex function;fixed point (mathematics);iteration;matrix regularization;with high probability	Adil Salim;Pascal Bianchi;Walid Hachem	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8461469		Robotics	74.57184962519179	24.371882659824703	97943
3d2eea33272e69f7d0708fb3dabcb40bfaecfbe8	"""comment: """"modeling an augmented lagrangian for blackbox constrained optimization"""" by gramacy et al"""	grupo de excelencia;ciencias basicas y experimentales;matematicas	Comment: “Modeling an Augmented Lagrangian for Blackbox Constrained Optimization” by Gramacy et al. Yichen Cheng & Faming Liang To cite this article: Yichen Cheng & Faming Liang (2016) Comment: “Modeling an Augmented Lagrangian for Blackbox Constrained Optimization” by Gramacy et al., Technometrics, 58:1, 15-17, DOI: 10.1080/00401706.2015.1040927 To link to this article: http://dx.doi.org/10.1080/00401706.2015.1040927	augmented lagrangian method;constrained optimization;technometrics	Yichen Cheng;Faming Liang	2016	Technometrics	10.1080/00401706.2015.1040927	mathematical optimization;calculus;mathematics;algorithm	NLP	70.22366460844333	27.313167566365173	98430
05797cacefac0b5ef397af6e06975a5d89c287be	weak and strong convergence theorems for a finite family of generalized asymptotically quasi-nonexpansive mappings	strong convergence;iterative method;banach space;asymptotically nonexpansive mapping;fixed point;generalized asymptotically quasi nonexpansive mapping;common fixed point;uniformly convex banach space;nonexpansive mapping;iteration method	In this paper, we introduce a new iterative scheme for finding a common fixed point of a finite family of generalized asymptotically quasi-nonexpansive mappings in a uniformly convex Banach space. We establish weak and strong convergence theorems. Our main results improve and extend the corresponding ones obtained in Schu (1991) [J. Schu, Iterative construction of fixed points of asymptotically nonexpansive mapping, J. Math. Anal. Appl. 159 (1991) 407–413] and many others. © 2010 Elsevier Ltd. All rights reserved.	asymptote;comstock–needham system;fixed point (mathematics);iterative method;uniformly convex space	Watcharaporn Cholamjiak;Suthep Suantai	2010	Computers & Mathematics with Applications	10.1016/j.camwa.2010.07.025	mathematical optimization;mathematical analysis;topology;mathematics;iterative method;algebra	AI	74.47754945466049	18.65189612238523	98481
65ffeec9cd3982e0a72fe37c7361d81ea91783f7	improved enclosure for some parametric solution sets with linear shape	interval enclosure;interval linear equations;dependent data;solution set	Consider linear algebraic systems where the elements of the matrix and the right-hand side vector depend linearly on a number of interval parameters. We prove some sufficient conditions for the united parametric solution set of such a system to have linear boundary. These conditions imply an equivalent representation of the parametric system where each parameter appears once in a diagonal matrix. The latter representation allows us to expand the scope of applicability of the best known so far interval method, developed by A. Neumaier and A. Pownuk, for enclosing the parametric solution set and to generalize the method for systems where the parameter dependencies connect the matrix and the right-hand side vector. Some examples demonstrate that: parametric solution sets with linear boundary appear in various application domains, the generalized method improves the solution enclosure and the proven sufficient conditions can be helpful for solving various other interval problems.	application domain;discretization;formal verification;interval arithmetic;linear algebra;linear system;problem domain;the matrix	Evgenija D. Popova	2014	Computers & Mathematics with Applications	10.1016/j.camwa.2014.04.005	mathematical optimization;combinatorics;mathematical analysis;discrete mathematics;solution set;mathematics;algebra	Theory	75.68101123012592	18.36234404031855	98641
470e67dfc31dd8dc70d8341cb5d21b222fb37060	using the wpg method for solving integral equations of the second kind	transformation ondelette;sistema lineal;metodo directo;analisis numerico;galerkin method;integral equation;trial space test space;matematicas aplicadas;the wavelet petrov galerkin method;mathematiques appliquees;numerical method;integral equations;ondelette;trial space;matrix inversion;inversion matriz;linear system;equation fredholm;analyse numerique;regular pairs;test space;numerical analysis;galerkin petrov method;ecuacion fredholm;metodo numerico;methode galerkin petrov;numero de condicionamiento;algebra lineal numerica;algebre lineaire numerique;equation integrale;condition number;inversion matrice;ecuacion integral;numerical linear algebra;transformacion ondita;systeme lineaire;applied mathematics;methode directe;wavelets;indice conditionnement;methode numerique;wavelet transformation;direct method;fredholm integral equation;fredholm equation	In this paper, we use the Wavelet Petrov–Galerkin (WPG) method based on discontinuous orthogonal multiwavelets for solving Fredholm integral equations of the second kind that yields linear system that the condition number of its coefficient matrix is bounded and for showing efficiency of method, we use numerical examples.		Khosrow Maleknejad;M. Karami	2005	Applied Mathematics and Computation	10.1016/j.amc.2004.04.109	mathematical analysis;numerical analysis;calculus;mathematics;geometry;fredholm integral equation;integral equation;algebra	Logic	82.45623915856882	19.75011397788915	98679
0f5e84ce6b5c357b33f882efd0376d69ff0be663	an infeasible primal-dual interior-point algorithm for linearly constrained convex optimization based on a parametric kernel function	optimisation;kernel;constraint optimization kernel educational institutions equations testing books prototypes vectors;constraint optimization;prototypes;interior point algorithm linearly constrained convex optimization primal dual methods;primal dual method;kernel function;parametric kernel function infeasible primal dual interior point algorithm linearly constrained convex optimization;convex optimization;testing;infeasible primal dual interior point algorithm;data mining;books;linearly constrained convex optimization;parametric kernel function;vectors;programming profession;optimization;primal dual methods;algorithm design and analysis;interior point algorithm	In this paper we present an infeasible primal-dual interior-point algorithm for linearly constrained convex optimization based on a parametric kernel function, with parameters $p\in[0,1]$ and $q \geq 1$. Numerical test shows that the efficiency of the proposed algorithm and investigates the behavior of the algorithm with different parameters $p,q$ and $\theta$.	algorithm;convex optimization;duality (optimization);interior point method;mathematical optimization;numerical method	Guoqiang Wang;Baocun Wang;Qingduan Fan	2009	2009 International Joint Conference on Computational Sciences and Optimization	10.1109/CSO.2009.156	kernel;algorithm design;mathematical optimization;combinatorics;discrete mathematics;kernel;convex optimization;mathematics;prototype;software testing	ML	72.66880137904324	24.37555758022346	98845
4c778aaced63a596d7bbb1f89332edf505626d4e	a derivative-free algorithm for linearly constrained optimization problems	conjugate gradient method;trust region subproblem;linearly constrained problem;least frobenius norm method;derivative free optimization	Based on the NEWUOA algorithm, a new derivative-free algorithm is developed, named LCOBYQA. The main aim of the algorithm is to find a minimizer $x^{*} \in\mathbb{R}^{n}$ of a non-linear function, whose derivatives are unavailable, subject to linear inequality constraints. The algorithm is based on the model of the given function constructed from a set of interpolation points. LCOBYQA is iterative, at each iteration it constructs a quadratic approximation (model) of the objective function that satisfies interpolation conditions, and leaves some freedom in the model. The remaining freedom is resolved by minimizing the Frobenius norm of the change to the second derivative matrix of the model. The model is then minimized by a trust-region subproblem using the conjugate gradient method for a new iterate. At times the new iterate is found from a model iteration, designed to improve the geometry of the interpolation points. Numerical results are presented which show that LCOBYQA works well and is very competing against available model-based derivative-free algorithms.	algorithm;constrained optimization;linear programming;mathematical optimization	Elzain A. E. Gumma;M. H. A. Hashim;M. Montaz Ali	2014	Comp. Opt. and Appl.	10.1007/s10589-013-9607-y	mathematical optimization;combinatorics;discrete mathematics;newuoa;derivative-free optimization;mathematics;conjugate gradient method	Theory	75.12573954515402	24.314722940068183	98961
44c16f3e6da53c0416cef2438ce9ee6c39552faf	outer solution of linear systems whose elements are affine functions of interval parameters	sistema lineal;conjunto independiente;metodo directo;matrice intervalle;independent set;fonction affine;ecuacion lineal;linear system;methode calcul;systeme algebrique;metodo calculo;resolucion sistema ecuacion;resolution systeme equation;ensemble independant;matriz intervalo;interval matrix;aritmetica intervalo;equation system solving;interval arithmetic;systeme lineaire;linear equation;arithmetique intervalle;methode directe;algebraic system;computing method;direct method;equation lineaire	In this paper, linear systems whose elements are affine functions of a given set of independent intervals are considered. A direct method for computing an outer solution to such systems is suggested.	system of linear equations	Lubomir V. Kolev	2002	Reliable Computing	10.1023/A:1021320711392	direct method;affine space;independent set;affine coordinate system;calculus;affine geometry of curves;affine hull;control theory;affine transformation;mathematics;geometry;linear equation;interval arithmetic;affine shape adaptation;linear system;affine combination;algorithm;algebra	Theory	79.16266590789208	18.924248580036448	99441
96170c586a26adeffa0ec6cc7716c1df1fcf215b	error bound moduli for conic convex systems on banach spaces	convex inequality system;banach space;conic convex function;error bound modulus;error bound	We give two explicit formulas which express the error bound moduli for conic convex systems, one in terms of directional derivative, and the other in terms of the coderivative. As applications, we study error bounds for systems of infinitely many convex inequalities.	spaces	Xi Yin Zheng;Kung Fu Ng	2004	Math. Oper. Res.	10.1287/moor.1030.0088	convex analysis;subderivative;support function;mathematical optimization;conic optimization;mathematical analysis;convex optimization;convex polytope;pseudoconvex function;topology;convex combination;convex body;linear matrix inequality;convex conjugate;convex hull;absolutely convex set;mathematics;convex set;logarithmically convex function;banach space;effective domain;proper convex function;choquet theory	Theory	71.90491247527913	20.13703677054704	99539
24260cd230e92bd9975f48cf5c3d5fc36f0d70d5	a sparse matrix arithmetic based on $\cal h$ -matrices. part i: introduction to ${\cal h}$ -matrices	operador integral;erreur troncature;tridiagonal matrices;producto matriz;hierarchical block partitioning;matrix inversion;linear complexity;inversion matriz;arithmetique;matrice mathematique;tridiagonal matrix;hierarchical matrices;operateur integral;matrice creuse;mathematical matrix;aritmetica;arithmetics;integral operator;matriz tridiagonal;matriz matematica;inversion matrice;truncation error;sparse matrix;produit matrice;error truncamiento;matrice tridiagonale;sparse matrices;matrix product;matriz dispersa	A class of matrices ( $\Cal H$ -matrices) is introduced which have the following properties. (i) They are sparse in the sense that only few data are needed for their representation. (ii) The matrix-vector multiplication is of almost linear complexity. (iii) In general, sums and products of these matrices are no longer in the same set, but their truncations to the $\Cal H$ -matrix format are again of almost linear complexity. (iv) The same statement holds for the inverse of an $\Cal H$ -matrix. This paper is the first of a series and is devoted to the first introduction of the $\Cal H$ -matrix concept. Two concret formats are described. The first one is the simplest possible. Nevertheless, it allows the exact inversion of tridiagonal matrices. The second one is able to approximate discrete integral operators.	approximation algorithm;matrix multiplication;sparse matrix;the matrix;truncation	Wolfgang Hackbusch	1999	Computing	10.1007/s006070050015	combinatorics;sparse matrix;calculus;mathematics;algebra	Theory	81.7611332659008	22.65484569100162	99699
bfd74eb03922b0f8cab77df109b3e702a6324445	semidefinite programming	non-linear;vector programming;semidefinite programming;low-rank matrices	3 Why Use SDP? 5 3.1 Tractable Relaxations of Max-Cut . . . . . . . . . . . . . . . . . . . . . . . . 5 3.1.1 Simple Relaxation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 3.1.2 Trust Region Relaxation . . . . . . . . . . . . . . . . . . . . . . . . . 6 3.1.3 Box Constraint Relaxation . . . . . . . . . . . . . . . . . . . . . . . . 6 3.1.4 Eigenvalue Bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 3.1.5 SDP Bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 3.1.6 Summary and Lagrangian Relaxation . . . . . . . . . . . . . . . . . . 8 3.2 Recipe for SDP relaxations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 3.3 SDP Relaxation for the Quadratic Assignment Problem, QAP . . . . . . . . 9	lagrangian relaxation;linear model;linear programming relaxation;maximum cut;np-hardness;numerical analysis;quadratic assignment problem;semidefinite programming;sockets direct protocol;trust region	Michael L. Overton;Henry Wolkowicz	1997	Math. Program.	10.1007/BF02614431	second-order cone programming;quadratically constrained quadratic program;semidefinite programming	ML	73.5644719038616	27.256339030751725	99799
5986fe9eb0a77f8e471fd2ed2e97db0815679323	comparison of duality models in fractional linear programming	linear program	This paper deals with the duality models in fractional linear programming presented in the last years bySwarup, Kaska, Sharma andSwarup and other authors.	linear programming	Jaromir Abrham;S. Luthra	1977	Math. Meth. of OR	10.1007/BF01919768	fractional programming;mathematical optimization;combinatorics;duality;linear-fractional programming;linear programming;calculus;mathematics	Theory	70.91361137171727	22.607866904961305	99962
23c4e2dff297b37dd160b5b7691674c64e79ebc1	a non-monotone memory gradient method for unconstrained optimization	memory gradient method;convergence;numerical tests nonmonotone memory gradient method unconstrained optimization large scale problems memorygradient direction r linear convergence nonmonotone line search strategy;r linear convergence;search problems approximation theory convergence gradient methods;approximation theory;iterative methods;nonmonotone line search;r linear convergence unconstrained optimization memory gradient method nonmonotone line search;gradient methods;unconstrained optimization;search problems;gradient methods convergence search problems iterative methods educational institutions	The memory gradient method is used for unconstrained optimization, especially large scale problems. In this paper, we develop a nonmonotone memory gradient method for unconstrained optimization, where a class of memory gradient direction is combined efficiently. The global and Rlinear convergence is obtained by using a nonmonotone line search strategy and the numerical tests are also given to show the efficiency of the proposed algorithm.	algorithm;gradient method;line search;mathematical optimization;numerical analysis;monotone	Shenghua Gui;Hua Wang	2012	2012 Fifth International Joint Conference on Computational Sciences and Optimization	10.1109/CSO.2012.92	gradient descent;mathematical optimization;combinatorics;convergence;gradient method;machine learning;mathematics;iterative method;nonlinear conjugate gradient method;line search;approximation theory	Vision	76.69919121797068	23.589924884383077	100055
b4b3838c34258ec910ecf1940a226254aead88a8	an efficient and accurate method to compute the fiedler vector based on householder deflation and inverse power iteration	parallel computing;conjugate gradient iteration;eigenvalue problem;computacion informatica;68w30;preconditioner;ciencias basicas y experimentales;matematicas;fiedler vector;grupo a;sparse linear system	The Fiedler vector of a graph plays a vital role in many applications. But it is usually very expensive in that it involves the solution of an eigenvalue problem. In this paper, we introduce the inverse power method incorporated with the Householder deflation to compute the Fiedler Vector. In the inverse power iterations, the coefficient matrix is formed implicitly, to take advantage of the sparsity. The linear systems encountered at each iteration must be symmetric positive definite, thus the conjugate gradient method is used. In addition, preconditioning techniques are introduced to reduce the computation cost. Any kind of preconditioning techniques with dropping can be used. For the graphs related to some of the sparse matrices downloaded from the UF Sparse Matrix Collection, the experiments are compared to the known novel schemes. The results show that the provided method is more accurate. While it is slower than MC73 sequentially, it has good parallel efficiency compared with TraceMin. In addition, it is insensitive to the selection of parameters, which is superior to the other two methods.	algebraic connectivity;inverse iteration;power iteration	Jian-Ping Wu;Jun-qiang Song;Wei-min Zhang	2014	J. Computational Applied Mathematics	10.1016/j.cam.2014.03.018	algebraic connectivity;mathematical optimization;combinatorics;mathematical analysis;calculus;inverse iteration;mathematics;preconditioner;algebra	HPC	82.36084761286605	22.814474913792715	100320
330fa0bf59d2d554624daee36d66e55cfd4ae209	error bounds and convergence analysis of feasible descent methods: a general approach	rate of convergence;convergence analysis;linear convergence;error bound	We survey and extend a general approach to analyzing the convergence and the rate of convergence of feasible descent methods that does not require any nondegeneracy assumption on the problem. This approach is based on a certain error bound for estimating the distance to the solution set and is applicable to a broad class of methods.	rate of convergence	Zhi-Quan Luo;Paul Tseng	1993	Annals OR	10.1007/BF02096261	mathematical optimization;combinatorics;computer science;modes of convergence;compact convergence;mathematics;convergence tests;rate of convergence;normal convergence;statistics	ML	74.11286052768044	23.461155759900215	100478
a5548c947294483e493a19b76dbfbb76e4f3b314	stabilization of interior-point methods for linear programming	interior point methods;numerical stability;degeneracy;stabilization;ill conditioning;degeneration;gaussian elimination;linear program;interior point method;path following	The paper studies numerical stability problems arising in the application of interior-point methods to primal degenerate linear programs. A stabilization procedure based on Gaussian elimination is proposed and it is shown that it stabilizes all path following methods, original and modified Dikin‘s method, Karmarkar‘s method, etc.	interior point method;linear programming	Vera Kovacevic-Vujcic;Miroslav D. Asic	1999	Comp. Opt. and Appl.	10.1023/A:1026452506999	mathematical optimization;combinatorics;mathematical analysis;linear programming;interior point method;mathematics	Theory	76.79209178402311	22.59760746120475	100481
ff5d577cfa673303d5e5341fa6b39ad120f200f2	an iterative algorithm for l 1-norm approximation in dynamic estimation problems		In this paper, an approach to state estimation in dynamic systems is considered, which consists in solving an l1-norm approximation problem. An algorithm is proposed for the solution of this problem, the so-called weight and time recursion method, which combines the ideas of weighted variational quadratic approximations and smoothing Kalman filtering. For the iterations of the proposed method, estimates of levels of nonoptimality are computed; this is considered as an extension of earlier results obtained by the authors for the classical least absolute deviation method.	algorithm;approximation;iterative method	Pavel A. Akimov;Alexander I. Matasov	2015	Automation and Remote Control	10.1134/S000511791505001X	mathematical optimization;combinatorics;discrete mathematics;mathematics	Robotics	74.51719837802995	25.214323575749052	100754
0fbfd3cae81f561b5c7a1c8fda5e063078226410	spectral finite-element methods for parametric constrained optimization problems	28cxx;optimisation sous contrainte;constrained optimization;parametric problems;metodo espectral;analisis numerico;eigenvalue problem;general and miscellaneous mathematics computing and information science;stochastic finite elements;methode element fini;metodo elemento finito;polinomio ortogonal;methode approchee;level set;orthogonal polynomial;mathematics and computing;spectral approximation;ensembe niveau;metodo aproximado;optimization method;probleme valeur propre;approximate method;finite element method;eigenvalues;metodo optimizacion;polynomials;finite element;analyse numerique;42c05;optimizacion con restriccion;objective function;optimization problem;41a10;aproximacion polinomial;65k05;numerical analysis;roughness;approximation polynomiale;orthogonal polynomials;spectral method;methode optimisation;spectral approximations;methode spectrale;optimization;28bxx;stochastic finite element;polynome orthogonal;constrained optimization problem;33c45;polynomial approximation;problema valor propio;approximations	We present a method to approximate the solution mapping of parametric constrained optimization problems. The approximation, which is of the spectral finite element type, is represented as a linear combination of orthogonal polynomials. Its coefficients are determined by solving an appropriate finite-dimensional constrained optimization problem. We show that, under certain conditions, the latter problem is solvable because it is feasible for a sufficiently large degree of the polynomial approximation and has an objective function with bounded level sets. In addition, the solutions of the finite dimensional problems converge for an increasing degree of the polynomials considered, provided that the solutions exhibit a sufficiently large and uniform degree of smoothness. Our approach solves, in the case of optimization problems with uncertain parameters, the most computationally intensive part of stochastic finite-element approaches. We demonstrate that our framework is applicable to parametric eigenvalue problems. Numerical results in one-dimensional cases indicate that our method is, for those examples, superior in both accuracy and speed to blackbox approaches.	approximation algorithm;artelys knitro;basis function;best, worst and average case;black box;coefficient;constrained optimization;constraint (mathematics);converge;decision problem;finite element method;lagrange multiplier;mathematical optimization;maxima and minima;nonlinear system;numerical method;optimization problem;polynomial;polynomial basis;simulation;social inequality;well-posed problem	Mihai Anitescu	2009	SIAM J. Numerical Analysis	10.1137/060676374	optimization problem;mathematical optimization;constrained optimization;combinatorics;mathematical analysis;cobyla;finite element method;calculus;mathematics;geometry;orthogonal polynomials;l-reduction	Theory	78.74754529665005	24.944036657201988	100867
164be3c0fbe7329c4d33fe5768cef25130ea6d45	advances in matrices, finite and infinite, with applications 2014		Matrix theory either finite or infinite has increasingly proved to be a key element in many different modern scientific fields, far beyond its natural mathematical environment.This volume exposes the growing sophistication of the techniques involving matrices as well as some of many applications. Classification of modular Lie algebras has recently been the subject of many authors. In particular, alteration techniques play a major role. Natural filtration for Cartan types has been shown to be invariant in infinite dimensional case. In this issue, Q. Mu in “Natural filtrations of infinitedimensional modular contact superalgebras” establishes that the natural filtration is invariant under automorphisms in the case of Lie algebras. The author uses ad-nilpotent elements techniques. It is well known that rank of matrix plays an important role in matrix theory and has many applications in other areas. In the paper “Some results on characterizations of matrix partial orderings” by H. Wang and J. Xu, five matrix partial orderings defined in Cm×n are considered and the (left/rightand both sided-) star and sharp partial orderings are characterized by applying rank equalities. Comparison theorems between the spectral radii of different matrices are useful tools for judging the efficiency of preconditioned. In S.-X. Miao and Y. Cao’s paper “On comparison theorems for splittings of different semimonotone matrices,” the authors gave some comparisons for the spectral radii of matrices arising from proper splittings of different semimonotone matrices. In the work reported by G. M. L Gladwell et al. in the paper “A test matrix for an inverse eigenvalue problem,” a simple derivation of a set of explicit expressions is provided for the components of a Jacobi matrix of order n × n. This matrix has the property that its eigenvalues come from the set {0, 2, . . . , 2n − 2} while also satisfying the additional condition that the eigenvalues of the leading principal (n − 1) × (n − 1) submatrix belong to the set {1, 3, . . . , 2n − 3}. As an application, an explicit solution for a spring-mass inverse problem is presented. The combinedmatrix of a nonsingularmatrixA is defined as C(A) = A ∘ (A)T, where ∘means the Hadamard product. Combined matrices appear in the chemical literature, where they represent the relative gain array. Furthermore, the study of combined matrices yields the relation between the eigenvalues and diagonal entries of a diagonalizable matrix. It is well known that the row and column sums of a combined matrix are always equal to one, and, consequently, if C(A) is entrywise nonnegative, then it has interesting properties and applications since it is a doubly stochastic matrix. In the paper “Nonnegative combined matrices” of R. Bru et al., the matrices which have nonnegative combined matrices are investigated. In particular, combined matrices of different classes ofmatrices, such as totally positive and totally negative matrices, and also when A is totally nonnegative and totally nonpositive, are studied.	doubly stochastic model;jacobi method;jacobian matrix and determinant;stochastic matrix;stochastic process	P. N. Shivakumar;Panayiotis J. Psarrakos;K. C. Sivakumar;Yonghui Zhang;Carlos M. da Fonseca	2014	J. Applied Mathematics	10.1155/2014/518487	jacobian matrix and determinant;matrix (mathematics);diagonalizable matrix;rank (linear algebra);mathematical analysis;eigenvalues and eigenvectors;doubly stochastic matrix;hadamard product;invariant (mathematics);mathematics	Theory	78.94559181781258	20.766101502655562	100925
96a6273d15c73e4dfe73af4af4b5567b17d0a1ff	maximizing a monomial geometric objective function subject to bipolar max-product fuzzy relation constraints		In this paper, the problem of maximizing a monomial geometric objective function subject to bipolar max-product fuzzy relation constraints is studied. First of all, it is shown that the bipolar max-product Fuzzy Relation Inequality (FRI) system can equivalently be converted to a bipolar max-product Fuzzy Relation Equation (FRE) system. Hence, the structure of feasible domain of the problem is determined in the case of the bipolar max-product FRE system. It is shown that its solution set is non-convex, in a general case. Some sufficient conditions are proposed for solution existence of its feasible domain. An algorithm is designed to solve the optimization problem with regard to the structure of its feasible domain and the properties of the objective function. Its importance is also illustrated by an application example in the area of economics and covering problem. Some numerical examples are given to illustrate the above points.	algorithm;bittorrent;convex function;covering problems;mathematical optimization;monomial;numerical analysis;optimization problem;peer-to-peer file sharing;social inequality	Samaneh Aliannezhadi;Shadi Shahab Ardalan;Ali Abbasi Molai	2017	Journal of Intelligent and Fuzzy Systems	10.3233/JIFS-151820	mathematical optimization;combinatorics;discrete mathematics;mathematics	Robotics	71.13184656346746	21.489702047594644	101349
9eeed8a495968dcfef6fd7154f7e053c4015af59	products of real matrices with prescribed characteristic polynomials	calcul matriciel;linear algebra;15a18;factorization of matrices;matrix factorization;matrice reelle;eigenvalues;real matrices;factorisation matricielle;real matrix;15a23;eigenvalue;polynome caracteristique;algebre lineaire;valor propio;algebra lineal;valeur propre;matrix calculus;characteristic polynomial;factorizacion matricial;calculo de matrices;polinomio caracteristico;matriz real	Let A be a matrix with entries in the field of real numbers. In this paper we give necessary and sufficient conditions for the existence of real matrices B and C, with prescribed characteristic polynomials, such that A = BC.	polynomial ring	Susana Furtado;Laura Iglésias;Fernando C. Silva	2002	SIAM J. Matrix Analysis Applications	10.1137/S0895479800381690	difference polynomials;matrix analysis;combinatorics;discrete mathematics;eigenvalues and eigenvectors;linear algebra;mathematics;algebra	Theory	78.79688479930951	19.642989260695003	101694
b55c1baf211e70f3e0158852cd03acb8356e1ad2	complexity analysis of successive convex relaxation methods for nonconvex sets	lift and project procedure;semidefinite programming;nonconvex quadratic program;complexity analysis;complexity;computational complexity;global optimization;convex relaxation;sdp relaxation	This paper discusses computational complexity of conceptual successive convex relaxation methods proposed by Kojima and Tunn cel for approximating a convex relaxation of a compact subset F = fx 2 C 0 : p(x) 0 (8p() 2 P F)g of the n-dimensional Euclidean space R n. Here C 0 denotes a nonempty compact convex subset of R n , and P F a set of nitely or innnitely many quadratic functions. We evaluate the number of iterations which the successive convex relaxation methods require to attain a convex relaxation of F with a given accuracy , in terms of , the diameter of C 0 , the diameter of F, and some other quantities characterizing the Lipschitz continuity, the nonlinearity and the nonconvexity of the set P F of quadratic functions.	analysis of algorithms;computational complexity theory;convex optimization;convex set;iteration;linear programming relaxation;nonlinear system;quadratic function;scott continuity	Masakazu Kojima;Akiko Takeda	2001	Math. Oper. Res.	10.1287/moor.26.3.519.10580	convex function;convex geometry;convex analysis;subderivative;mathematical optimization;conic optimization;combinatorics;discrete mathematics;complexity;convex optimization;convex polytope;convex combination;linear matrix inequality;convex hull;absolutely convex set;convexity in economics;mathematics;convex set;computational complexity theory;logarithmically convex function;semidefinite programming;proper convex function;global optimization	ML	71.89357557739052	23.72299823919782	101955
d540c02bcd934bedb699e183f84395cf73e11cd1	an interior-point piecewise linear penalty method for nonlinear programming	optimisation sous contrainte;strong convergence;constrained optimization;iterative method;superlinear convergence;fonction barriere;convergence forte;analisis sensibilidad;equation ordre 1;piecewise linear;global solution;infeasibility;line search;non linear programming;first order equation;convergencia fuerte;ecuacion orden 1;nonlinear programming;piecewise linear techniques;65f05;interior point;programacion no lineal;relacion convergencia;merit function;piecewise linear penalty function;metodo penalidad;90c30;methode point interieur;taux convergence;global convergence;programmation non lineaire;local convergence;convergence rate;convergencia superlineal;convergence numerique;technique lineaire par morceau;metodo iterativo;optimizacion con restriccion;numerical convergence;65k05;penalty method;methode penalite;first order;metodo punto interior;funcion penalidad;linearisation morceau;mathematical programming;fourier transformation;inpractibilidad;sensitivity analysis;methode iterative;transformation fourier;convergence superlineaire;infaisabilite;linearizacion trozo;analyse sensibilite;solution globale;fonction penalite;interior point method;programmation mathematique;convergencia numerica;solucion global;programacion matematica;funcion barrera;piecewise linearization;barrier function;49m37;penalty function;transformacion fourier	We present an interior-point penalty method for nonlinear programming (NLP), where the merit function consists of a piecewise linear penalty function (PLPF) and an `2-penalty function. The PLPF is defined by a set of penalty parameters that correspond to break points of the PLPF and are updated at every iteration. The `2-penalty function, like traditional penalty functions for NLP, is defined by a single penalty parameter. At every iteration the step direction is computed from a regularized Newton system of the first-order equations of the barrier problem proposed in [4]. Iterates are updated using line search. In particular, a trial point is accepted if it provides a sufficient reduction in either the PLPF or the `2-penalty function. We show that the proposed method has the same strong global convergence properties as those established in [4]. Moreover, our method enjoys fast local convergence. Specifically, for each fixed small barrier parameter μ, iterates in a small neighborhood (roughly within o(μ)) of the minimizer of the barrier problem converge Q-quadratically to the minimizer. The overall convergence rate of the iterates to the solution of the nonlinear program is Q-superlinear.	barrier function;circuit restoration;converge;convex function;first-order predicate;fletcher's checksum;ipopt;iteration;line search;local convergence;natural language processing;newton;newton's method;nonlinear programming;nonlinear system;numerical analysis;penalty method;piecewise linear continuation;rate of convergence	Lifeng Chen;Donald Goldfarb	2011	Math. Program.	10.1007/s10107-009-0296-3	mathematical optimization;mathematical analysis;nonlinear programming;interior point method;calculus;penalty method;mathematics	ML	76.22997514200651	23.08198456494601	101965
094d3503a8f8edcb1b3acd3b1725b886f80d62df	vandermonde matrices with nodes in the unit disk and the large sieve		We derive bounds on the extremal singular values and the condition number of N × K, with N > K, Vandermonde matrices with nodes in the unit disk. The mathematical techniques we develop to prove our main results are inspired by a link—first established by Selberg [1] and later extended by Moitra [2]— between the extremal singular values of Vandermonde matrices with nodes on the unit circle and large sieve inequalities. Our main conceptual contribution lies in establishing a connection between the extremal singular values of Vandermonde matrices with nodes in the unit disk and a novel large sieve inequality involving polynomials in z ∈ C with |z| 6 1. Compared to Bazán’s upper bound on the condition number [3], which, to the best of our knowledge, constitutes the only analytical result—available in the literature—on the condition number of Vandermonde matrices with nodes in the unit disk, our bound not only takes a much simpler form, but is also sharper for certain node configurations. Moreover, the bound we obtain can be evaluated consistently in a numerically stable fashion, whereas the evaluation of Bazán’s bound requires the solution of a linear system of equations which has the same condition number as the Vandermonde matrix under consideration and can therefore lead to numerical instability in practice. As a byproduct, our result—when particularized to the case of nodes on the unit circle—slightly improves upon the Selberg–Moitra bound.	bekenstein bound;condition number;general number field sieve;instability;linear system;numerical analysis;numerical stability;polynomial;social inequality;system of linear equations;vandermonde matrix	Céline Aubel;Helmut Bölcskei	2017	CoRR		vandermonde matrix;combinatorics;mathematical analysis;calculus;mathematics;vandermonde polynomial	ML	79.29545923018357	21.62290129369102	102137
0758f3e918f428e8c6e91597c3e798afd1922dd2	the global solver in the lindo api	front end;90c11;90c26;application program interface;65k05;modelling language;mathematical model;global optimization;mixed integer nonlinear programming;nonsmooth optimization;branch and bound;lindo api	The global solver in the LINDO Application Programming Interface (LINDO API) finds guaranteed global optima to nonconvex, nonlinear and integer mathematical models using the branch and bound/relax approach. We describe (a) the class of problems for which it tends to be appropriate; (b) how to access it directly via the LINDO API or via various modelling language front ends; (c) heuristics used for finding good initial solutions; (d) methods for constructing easily solved relaxations; (e) branching rules for splitting a problem into more easily solved subproblems; and (f) some illustrative computational results.	application programming interface;lindo;solver	Youdong Lin;Linus Schrage	2009	Optimization Methods and Software	10.1080/10556780902753221	mathematical optimization;application programming interface;theoretical computer science;front and back ends;mathematical model;mathematics;branch and bound;algorithm;global optimization	EDA	76.35800543233175	25.68417022411623	102330
f5c9a03ffd3c0e8bb530ff96896f3a10da3ab83c	a method for outer interval solution of linear parametric systems	linear system;iteration method	The paper deals with the problem of determining an outer interval solution (interval enclosure of the solution set) of linear systems whose elements are affine functions of interval parameters. An iterative method for finding such a solution is suggested. A numerical example illustrating the new method is solved.	assertion (software development);iteration;iterative method;linear system;numerical analysis	Lubomir V. Kolev	2004	Reliable Computing	10.1023/B:REOM.0000032110.34735.ca	mathematical optimization;mathematical analysis;discrete mathematics;mathematics;iterative method;linear system;algebra	EDA	76.94472882148355	19.20941111620072	102401
6dec49b72d66a88cb92764e0647903b3430e524e	an extension of chubanov's polynomial-time linear programming algorithm to second-order cone programming		In this paper, we extend Chubanovu0027s new polynomial-time algorithm for linear programming to second-order cone programming based on the idea of cutting plane method. The algorithm finds an -dimensional vector x which satisfies , where and is a direct product of n second-order cones and half lines. Like Chubanovu0027s algorithm, one iteration of the proposed algorithm consists of two phases: execution of a basic procedure and scaling. Within calls of the basic procedure, the algorithm either (i) finds an interior feasible solution, (ii) finds a non-zero dual feasible solution, or (iii) verifies that there is no interior feasible solution whose minimum eigenvalue is greater than or equal to ϵ. Each basic procedure requires arithmetic operations, where is the dimension of each second-order cone. If the problem is interior feasible, then the algorithm finds an interior feasible solution in calls of the basic procedure, where is a condition number associated with the system.	algorithm;approximation algorithm;condition number;cone (formal languages);conic optimization;free variables and bound variables;linear programming;maxima and minima;optimization problem;p (complexity);pc bruno;rewriting;second-order cone programming;semidefinite programming;time complexity;universal quantification	Tomonari Kitahara;Takashi Tsuchiya	2018	Optimization Methods and Software	10.1080/10556788.2017.1382495	algorithm design;mathematical optimization;constraint programming;discrete mathematics;basic solution;criss-cross algorithm;second-order cone programming;linear-fractional programming;linear programming;mathematics;simplex algorithm;algorithm;semidefinite programming	Theory	72.4263679597512	23.77846187990251	102565
afcc9f0330bdbb8f990ffaf69ac8d62faa9a68f5	on a generalized conjugate gradient orthogonal residual method	generalized conjugate gradient;rate of convergence;orthogonal residual;article letter to editor;conjugate gradient	To solve a linear system of equations with a generally nonsymmetric matrix, a generalized conjugate gradientorthogonal residual method is presented. The method uses all previous search directions (or a truncated set of them) at each step but, contrary to standard implementations of similar methods, it requires storage of only one set with a linearly growing number of vectors (or the number in the truncated set). Furthermore, there is only one vector (the residual), which must be updated using all the vectors in this set, at each step. In this respect it is similar to the popular GMRES method but it has the additional advantage that it can stop at any stage when the norm of the residual is sufticiently small and no extra computation is needed to compute this norm. Furthermore, the new method can be truncated. The rate of convergence of the method is also discussed.	computation;conjugate gradient method;generalized minimal residual method;linear system;rate of convergence;system of linear equations	Owe Axelsson;M. Makarov	1995	Numerical Lin. Alg. with Applic.	10.1002/nla.1680020507	gradient descent;conjugate residual method;gradient method;derivation of the conjugate gradient method;mathematics;conjugate gradient method;nonlinear conjugate gradient method;rate of convergence;biconjugate gradient method;algorithm	ML	82.20028902110056	23.33769572381292	102622
3655206d4d469885d822ae133e88b850666556c5	sequential quadratic programming methods for large-scale problems	quadratic programming;quadratic program;constrained minimization;sequential quadratic programming;nonlinearly constrained minimization;large scale problem;large scale optimization	Sequential quadratic (SQP) programming methods are the method of choice when solving small or medium-sized problems. Since they are complex methods they are difficult (but not impossible) to adapt to solve large-scale problems. We start by discussing the difficulties that need to be addressed and then describe some general ideas that may be used to resolve these difficulties. A number of SQP codes have been written to solve specific applications and there is a general purposed SQP code called SNOPT, which is intended for general applications of a particular type. These are described briefly together with the ideas on which they are based. Finally we discuss new work on developing SQP methods using explicit second derivatives.	code;explicit substitution;runge–kutta methods;snopt;sequential quadratic programming	Walter Murray	1997	Comp. Opt. and Appl.	10.1023/A:1008671829454	mathematical optimization;combinatorics;discrete mathematics;quadratically constrained quadratic program;mathematics;sequential quadratic programming;quadratic programming	PL	76.75373910350962	24.365805132887633	102677
1f6e4a5e8981888267ba5c2b428316febc0d8fdc	a simple primal-dual feasible interior-point method for nonlinear programming with monotone descent	constrained optimization;convergence analysis;monotone descent;primal dual interior point method;nonlinear programming;local convergence;feasibility;objective function;primal dual interior point methods;interior point method	We propose and analyze a primal-dual interior point method of the “feasible” type, with the additional property that the objective function decreases at each iteration. A distinctive feature of the method is the use of different barrier parameter values for each constraint, with the purpose of better steering the constructed sequence away from non-KKT stationary points. Assets of the proposed scheme include relative simplicity of the algorithm and of the convergence analysis, strong global and local convergence properties, and good performance in preliminary tests. In addition, the initial point is allowed to lie on the boundary of the feasible set. ∗This work was supported in part by the National Science Foundation under Grant DMI9813057.	algorithm;feasible region;gradient descent;interior point method;iteration;lagrange multiplier;local convergence;loss function;nonlinear programming;optimization problem;stationary process;monotone	Sasan Bakhtiari;André L. Tits	2003	Comp. Opt. and Appl.	10.1023/A:1022944802542	local convergence;mathematical optimization;constrained optimization;combinatorics;mathematical analysis;barrier function;nonlinear programming;interior point method;mathematics	ML	74.79221209084278	24.410919205245467	102731
121f7c9b91204acf8c4d9c25d792020547bab89c	the bisection method in higher dimensions	minimisation;minimization;optimisation;convergence;linearity;optimizacion;linearite;optimum global;minimizacion;tiling;global optimisation;global optimum;linearidad;convergencia;bisection;lipschitz continuity;simplex;higher dimensions;bisection method;euclidean space;linear convergence;global optimization;optimization;zonotope;optimo global	Is the familiar bisection method part of some larger scheme? The aim of this paper is to present a natural and useful generalisation of the bisection method to higher dimensions. The algorithm preserves the salient features of the bisection method: it is simple, convergence is assured and linear, and it proceeds via a sequence of brackets whose infinite intersection is the set of points desired. Brackets are unions of similar simplexes. An immediate application is to the global minimisation of a Lipschitz continuous function defined on a compact subset of Euclidean space.	algorithm;bisection method	Graham R. Wood	1992	Math. Program.	10.1007/BF01581205	bisection;bisection method;minimisation;mathematical optimization;combinatorics;topology;convergence;euclidean space;mathematics;linearity;lipschitz continuity;global optimum;rate of convergence;simplex;global optimization	Theory	73.0190995415158	22.86943729318315	103173
3a56348161dc2fd130443180bf04891508db2d09	on the brézis nirenberg stampacchia-type theorems and their applications	c intersectionally upper continuous;kkm map;c topological pseudomonotone;intersectionally closed	In this paper, we obtain several new generalized KKM-type theorems under a new coercivity condition and the condition of intersectionally closedness which improves condition of transfer closedness. As applications, we obtain new versions of equilibrium problem, minimax inequality, coincidence theorem, fixed point theorem and an existence theorem for an 1-person game.	fixed point (mathematics);fixed-point theorem;minimax;social inequality;stampacchia medal	M. Fakhar;M. Lotfipour;Jafar Zafarani	2013	J. Global Optimization	10.1007/s10898-012-9965-5	mathematical optimization;mathematical analysis;topology;mathematics	Theory	71.13318380572476	19.734089744295815	103213
25cafdaf45e11512f5fb67e81a7aaa31bdc69f62	on the superlinear convergence of the variable metric proximal point algorithm using broyden and bfgs matrix secant updating	monotone operator;superlinear convergence;convex programming;quasi newton;global convergence;local convergence;working conditions;hilbert space;proximal point algorithm;linear convergence;maximal monotone operator;proximal point method	In previous work, the authors provided a foundation for the theory of variable metric proximal point algorithms in Hilbert space. In that work conditions are developed for global, linear, and super–linear convergence. This paper focuses attention on two matrix secant updating strategies for the finite dimensional case. These are the Broyden and BFGS updates. The BFGS update is considered for application in the symmetric case, e.g., convex programming applications, while the Broyden update can be applied to general monotone operators. Subject to the linear convergence of the iterates and a quadratic growth condition on the inverse of the operator at the solution, super–linear convergence of the iterates is established for both updates. These results are applied to show that the Chen–Fukushima variable metric proximal point algorithm is super–linearly convergent when implemented with the BFGS update.	broyden–fletcher–goldfarb–shanno algorithm;convex optimization;entity–relationship model;hilbert space;rate of convergence;secant method;monotone	James V. Burke;Maijian Qian	2000	Math. Program.	10.1007/PL00011373	local convergence;mathematical optimization;mathematical analysis;discrete mathematics;convex optimization;quasi-newton method;modes of convergence;compact convergence;mathematics;convergence tests;rate of convergence;hilbert space	ML	73.96199482765863	22.66778685992825	103275
8c30bf88a331d81580583926c05b73d1653c7c96	low-order penalty equations for semidefinite linear complementarity problems	low order penalty equation;cartesian p property;convergence rate;semidefinite linear complementarity problem	We extend the power penalty method for linear complementarity problem(LCP) [Wang and Yang, Operations Research Letters, 36(2): 211-214, 2008] to the semidefinite linear complementarity problem(SDLCP). We establish a family of low-order penalty equations for SDLCPs. Under the assumption that the involved linear transformation possesses the Cartesian P-property, we show that when the penalty parameter tends to infinity, the solution to any equation of this family converges to the solution of the SDLCP exponentially. Numerical experiments verify this convergence result.	cartesian closed category;complementarity (physics);complementarity theory;experiment;numerical method;operations research;penalty method;yang	Chen Zhao;Ziyan Luo;Naihua Xiu	2016	Oper. Res. Lett.	10.1016/j.orl.2016.03.004	mathematical optimization;combinatorics;mathematical analysis;penalty method;mathematics;mixed complementarity problem;rate of convergence;algorithm	ML	77.34649056524968	21.575814922208856	103436
ed75248998fb1ac44026b1013e0d6e6593d2c150	local strong homogeneity of a regularized estimator	26b05;reconstruction;satisfiability;nonsmooth analysis;49q20;regularization;58f10;perturbation analysis;map estimation;soft thresholding;94a12;total variation;proximal point;linear equations;potential function;49j52;62h12;variational methods;inverse problems	This paper deals with regularized pointwise estimation of discrete signals which contain large strongly homogeneous zones, where typically they are constant, or linear, or more generally satisfy a linear equation. The estimate is defined as the minimizer of an objective function combining a quadratic data-fidelity term and a regularization prior term. The latter term is the sum of the values obtained by applying a potential function (PF) to each component, called a difference, of a linear transform of the signal. Minimizers of functions of this form arise in various settings in statistics and optimization. The features exhibited by such an estimate are closely related to the shape of the PF. Our goal is to determine estimators providing solutions which involve large strongly homogeneous zones— where more precisely the differences are null—in spite of the noise corrupting the data. To this end, we require that the strongly homogeneous zones, recovered by the estimator, be insensitive to any variation of the data inside a small open ball. More generally, this requirement is addressed to any local or global minimizer of the objective function whose local behavior with respect to the data gives rise to a locally continuous minimizer function. On the one hand, we show that if the PF is smooth at zero, then all the data, yielding minimizers with large, strongly homogeneous zones, are contained in a closed, negligible set. The chance that noisy data generate such minimizers is null. In contrast, if the PF is nonsmooth at zero, then for almost all data, the strongly homogeneous zones recovered by a minimizer function are preserved constant under any small perturbation of the data. The data domain is thus organized into volumes whose elements yield minimizers which share the same strongly homogeneous zones. This explains why the solutions, obtained using nonsmooth-atzero PFs, exhibit strongly homogeneous zones. These theoretical results are illustrated using a numerical example. Our analysis can be extended to general functions combining smooth and nonsmooth terms.	data domain;julia set;linear equation;loss function;mathematical optimization;numerical analysis;optimization problem;signal-to-noise ratio	Mila Nikolova	2000	SIAM Journal of Applied Mathematics	10.1137/S0036139997327794	regularization;mathematical optimization;combinatorics;mathematical analysis;inverse problem;perturbation theory;mathematics;linear equation;total variation;quantum mechanics;statistics;algebra;satisfiability	ML	75.7995415965675	19.63529250483936	104009
136e2bd3b9d7bada6620ed61e4209b33dfb0ee00	the block cmrh method for solving nonsymmetric linear systems with multiple right-hand sides		Abstract CMRH method (Changing minimal residual with Hessenberg process) is an iterative method for solving nonsymmetric linear systems. This method is similar to QMR method but based on the Hessenberg process instead of the Lanczos process. On dense matrices, the CMRH method is less expensive and requires less storage than other Krylov methods. This paper presents a block version of the CMRH algorithm for solving linear systems with multiple right-hand sides. The new algorithm is based on the block Hessenberg process and the iterates are characterized by a block version of the quasi-minimization property. We analyze its main properties and show that under the condition of full rank of block residual the block CMRH method cannot break down. Finally, some numerical examples are presented to show the efficiency of the new method in comparison with the traditional CMRH method and a comparison with the block GMRES method is also provided.	linear system	S. Amini;Faezeh Toutounian;M. Gachpazan	2018	J. Computational Applied Mathematics	10.1016/j.cam.2018.01.012	residual;iterative method;mathematical optimization;iterated function;lanczos resampling;matrix (mathematics);rank (linear algebra);mathematics;linear system;generalized minimal residual method	NLP	82.33242497460907	22.795712690598073	104140
29504cf6425604e45434910d41bac7679ed7b3a9	a reweighted nuclear norm minimization algorithm for low rank matrix recovery	computacion informatica;low rank matrix minimization;reweighted nuclear norm minimization;90c59;matrix completion problem;15a60;weighted fixed point method;65k05;ciencias basicas y experimentales;matematicas;grupo a	In this paper, we propose a reweighted nuclear norm minimization algorithm based on the weighted fixed point method (RNNM–WFP algorithm) to recover a low rank matrix, which iteratively solves an unconstrained L2–Mp minimization problem introduced as a nonconvex smooth approximation of the low rankmatrixminimization problem.Weprove that any accumulation point of the sequence generated by the RNNM–WFP algorithm is a stationary point of the L2–Mp minimization problem. Numerical experiments on randomly generated matrix completion problems indicate that the proposed algorithm has better recoverability compared to existing iteratively reweighted algorithms. © 2013 Elsevier B.V. All rights reserved.	algorithm;approximation;experiment;fixed-point iteration;numerical analysis;numerical linear algebra;procedural generation;randomness;serializability;stationary process;tree accumulation	Yu-Fan Li;Yan-Jiao Zhang;Zheng-Hai Huang	2014	J. Computational Applied Mathematics	10.1016/j.cam.2013.12.005	iteratively reweighted least squares;mathematical optimization;combinatorics;discrete mathematics;mathematics;low-rank approximation	AI	75.66408952431513	25.066341201245894	104378
593e420e5fe20d38bfb5dd88f1de041e16bc4d80	continuum isomap for manifold learnings	operador integral;embedding;echelonnement multidimensionnel;analisis numerico;escala multidimensional;learning algorithm;eigenvalue problem;learning;60j20;analisis datos;metodo reduccion;45p05;dimension reduction;isometrie;parameterization;probleme valeur propre;variedad matematica;manifold learning;algorithme apprentissage;euclidean distance;continuum isomap;integral;parametrizacion;analyse numerique;geodesic distance;eigenvalue;aprendizaje;reduction dimension;58dxx;geodesique;data analysis;apprentissage;operateur integral;numerical analysis;estimation erreur;isometria;geodesic;error estimation;53c22;nonlinear dimension reduction;geodesico;valor propio;integral operator;plongement;statistical computation;integrale;estimacion error;numero de condicionamiento;multidimensional scaling;calculo estadistico;algebra lineal numerica;algebre lineaire numerique;58e10;domaine convexe;condition number;65f35;perturbation analysis;analyse donnee;valeur propre;methode reduction;calcul statistique;47gxx;numerical linear algebra;inmersion;error bound;isometric embedding;algoritmo aprendizaje;isometry;reduction method;indice conditionnement;parametrisation;problema valor propio;variete mathematique;manifold	Recently, the Isomap algorithm has been proposed for learning a parameterized manifold from a set of unorganized samples from the manifold. It is based on extending the classical multidimensional scaling method for dimension reduction, replacing pairwise Euclidean distances by the geodesic distances on the manifold.A continuous version of Isomap called continuum Isomap is proposed. Manifold learning in the continuous framework is then reduced to an eigenvalue problem of an integral operator. It is shown that the continuum Isomap can perfectly recover the underlying parameterization if the mapping associated with the parameterized manifold is an isometry and its domain is convex. The continuum Isomap also provides a natural way to compute low-dimensional embeddings for out-of-sample data points. Some error bounds are given for the case when the isometry condition is violated. Several illustrative numerical examples are also provided. © 2006 Elsevier B.V. All rights reserved.	algorithm;apache continuum;convex function;data point;distortion;embedded system;image scaling;isomap;isometric projection;multidimensional scaling;nonlinear dimensionality reduction;nonlinear system;numerical analysis;perturbation theory;triune continuum paradigm	Hongyuan Zha;Zhenyue Zhang	2007	Computational Statistics & Data Analysis	10.1016/j.csda.2006.11.027	invariant manifold;geodesic;topology;calculus;mathematics;geometry;nonlinear dimensionality reduction;manifold alignment	ML	75.32762833883501	19.914622989094266	104492
438db56ad05d1ee7067008aecd0b2096b530bfae	gradient based iterative solutions for general linear matrix equations	convergence analysis;iterations;exact solution;sylvester matrix equations;iterative algorithm;least squares;matrix equation;estimation;lyapunov matrix equations;sylvester matrix equation;least squares estimate;condition number;iterative solution	In this paper, we present a gradient based iterative algorithm for solving general linear matrix equations by extending the Jacobi iteration and by applying the hierarchical identification principle. Convergence analysis indicates that the iterative solutions always converge fast to the exact solutions for any initial values and small condition numbers of the associated matrices. Two numerical examples are provided to show that the proposed algorithm is effective.	gradient;iterative method	Li Xie;Jie Ding;Feng Ding	2009	Computers & Mathematics with Applications	10.1016/j.camwa.2009.06.047	matrix splitting;local convergence;mathematical optimization;gaussian elimination;estimation;combinatorics;mathematical analysis;eigendecomposition of a matrix;iteration;non-linear iterative partial least squares;eight-point algorithm;sparse matrix;sylvester's law of inertia;coefficient matrix;convergent matrix;condition number;sylvester equation;stiffness matrix;mathematics;preconditioner;iterative method;conjugate gradient method;state-transition matrix;matrix-free methods;least squares;relaxation;matrix;linear least squares;statistics;algebra	Vision	80.6651800919929	21.921310255704906	104667
504b67d715c0dea2b55db6fcddbbeacb4384ea66	structure in optimization: factorable programming and functions		The purpose of this paper is to explore structures of functions in optimization. We will assume that the functions are composed of user-defined functions and are given as computer programs. Factorable functions and factorable programming problems were developed from 1967 through 1990 and are early examples of structure in nonlinear optimization. We explore the relationship between source code transformation as in algorithmic differentiation (AD) and factorable programming. As an illustration, we consider a classical example due to Jackson and McCormick and show that its Fortran 90 implementation in a source transformation AD yields the desired structure of the second derivative matrix.	automatic differentiation;computer program;fortran;jackson;mathematical optimization;nonlinear programming;nonlinear system;program transformation;source transformation;user-defined function	Trond Steihaug;Shahadat Hossain;Laurent Hascoët	2012		10.1007/978-1-4471-4594-3_46	mathematical optimization;distributed computing;computer science;automatic differentiation;nonlinear programming;unary operation;computation;source code;separable space;optimization problem	PL	71.12820293145234	27.531007297282116	104928
5a3dc97a39700667e06f4115d2790e0dfeaab017	a multi-objective genetic programming approach to uncover explicit and implicit equations from data	complexity theory;mathematical model sociology statistics complexity theory accuracy genetic programming context;genetic programming;gp formulations multiobjective genetic programming approach explicit equations data implicit explicit relationship identification predictor function mathematical expressions implicit equation mining mogpa gp tree maximum depth mean error implicit derivatives;accuracy;statistics;mathematical model;context;sociology;search problems data mining genetic algorithms	Identification of implicit and explicit relationships in a data is a generic problem commonly encountered in many fields of science and engineering. In the case of explicit relations, one is interested in identifying a compact and an accurate predictor function i.e. y = f(x), while in the implicit case, one is interested in identifying an equation of the form f(x) = 0. In both these classes of problems, one would need to search through a space of mathematical expressions, while minimizing some form of error metric. Such expressions are commonly identified using genetic programming (GP). While methods to uncover explicit equations have been studied extensively in the literature, there have been limited attempts to solve implicit cases. Since there are infinite trivial implicit forms that can be generated from a given set of data, the choice of an appropriate error metric is critical in the context of implicit equation mining. In this paper, we introduce a multiobjective genetic programming approach (MOGPA) for the solution of both classes of problems. The maximum depth of a GP-tree is used as the first objective reflecting the complexity/compactness of the expressions, while the mean error, either in the predictor variable or the implicit derivatives is used as the second objective during the course of search. The performance of the approach is illustrated using four examples. The approach delivers expressions of various complexities spanning a range of accuracy levels in a single run, unlike single objective GP formulations. It was able to identify more compact and accurate explicit forms than those from previously reported studies, and the correct, most compact expressions for implicit cases.	algorithmic efficiency;bitwise operation;emoticon;experiment;file spanning;genetic programming;implicit surface;kerrison predictor;logical connective;navier–stokes equations;numerical method;symbolic regression;unary operation	Bing Wang;Hemant Kumar Singh;Tapabrata Ray	2015	2015 IEEE Congress on Evolutionary Computation (CEC)	10.1109/CEC.2015.7257016	explicit and implicit methods;genetic programming;mathematical optimization;combinatorics;discrete mathematics;computer science;machine learning;mathematical model;mathematics;accuracy and precision;statistics	Comp.	68.56688788120886	25.476399203293	105222
3aa92be3727bd402e7baf6cdda8bfc0295cb1abe	regularity and stability for convex multivalued functions	inequalities;optimization;convex analysis	Multivalued functions with convex graphs are shown to exhibit certain desirable regularity properties when their ranges have internal points. These properties are applied to develop a perturbation theory for convex inequalities and to extend results on the continuity of convex functions.		Stephen M. Robinson	1976	Math. Oper. Res.	10.1287/moor.1.2.130	convex analysis;subderivative;support function;mathematical optimization;conic optimization;mathematical analysis;convex cone;convex optimization;convex polytope;topology;convex combination;linear matrix inequality;convex conjugate;quasiconvex function;convex hull;absolutely convex set;convexity in economics;inequality;mathematics;convex set;logarithmically convex function;effective domain;proper convex function;choquet theory	Theory	71.90644107255217	20.0873646986144	105287
1a3f6ac867b9f68e27d5e6a1cf86acf7ab0cc564	accelerated matrix factorisation method for fuzzy clustering		Factorised fuzzy c-means (F-FCM) based on semi nonnegative matrix factorization is a new approach for fuzzy clustering. It does not need the weighting exponent parameter compared with traditional fuzzy c-means, and not sensitive to initial conditions. However, F-FCM does not propose an efficient method to solve the constrained problem, and just suggests to use a lsqlin() function in MATLAB which lead to slow convergence rate and nonconvergence. In this paper, we propose a method to accelerate the convergence rate of F-FCM combining with a non-monotone accelerate proximal gradient (nmAPG) method. We also propose an efficient method to solve the proximal mapping problem when implementing nmAPG. Finally, the experiment results on synthetic and real-world datasets show the performances and feasibility of our method.	fuzzy clustering	Mingjun Zhan;Bo Li	2017		10.1007/978-3-319-70139-4_12	fuzzy logic;pattern recognition;artificial intelligence;matlab;rate of convergence;fuzzy clustering;computer science;factorization;matrix (mathematics);non-negative matrix factorization;weighting	Vision	75.9692942326793	26.06944831241775	105441
382e32b055444c6ac8b072da6a61c90834980dab	effective and robust preconditioning of general spd matrices via structured incomplete factorization		For general symmetric positive definite (SPD) matrices, we present a framework for designing effective and robust black-box preconditioners via structured incomplete factorization. In a scaling-and-compression strategy, off-diagonal blocks are first scaled on both sides (by the inverses of the factors of the corresponding diagonal blocks) and then compressed into low-rank approximations. ULV-type factorizations are then computed. A resulting prototype preconditioner is always positive definite. Generalizations to practical hierarchical multilevel preconditioners are given. Systematic analysis of the approximation error, robustness, and effectiveness is shown for both the prototype preconditioner and the multilevel generalization. In particular, we show how local scaling and compression control the approximation accuracy and robustness, and how aggressive compression leads to efficient preconditioners that can significantly reduce the condition number and improve the eigenvalue clustering. A result is also given to show when the multilevel preconditioner preserves positive definiteness. The costs to apply the multilevel preconditioners are about O(N), where N is the matrix size. Numerical tests on several ill-conditioned problems show the effectiveness and robustness even if the compression uses very small numerical ranks. In addition, significant robustness and effectiveness benefits can be observed as compared with a standard rank structured preconditioner based on direct off-diagonal compression.	approximation error;black box;cluster analysis;condition number;image scaling;numerical analysis;numerical linear algebra;preconditioner;prototype;the matrix;ultra-low-voltage processor	Jianlin Xia;Zi-Xing Xing	2017	SIAM J. Matrix Analysis Applications	10.1137/17M1124152	discrete mathematics;robustness (computer science);mathematical optimization;eigenvalues and eigenvectors;matrix (mathematics);mathematics;incomplete cholesky factorization;factorization;preconditioner;condition number;approximation error	ML	82.83449114102967	22.780444937326518	105477
62df3c46caa1aa3f515b1793ac82f4171512f715	superlinear convergence of a general algorithm for the generalized foley-sammon discriminant analysis	dimensionality reduction;generalized foley-sammon transform;linear discriminant analysis;superlinear convergence;the trace ratio optimization problem	Linear Discriminant Analysis (LDA) is one of the most efficient statistical approaches for feature extraction and dimension reduction. The generalized Foley– Sammon transform and the trace ratio model are very important in LDA and have received increasing interest. An efficient iterative method has been proposed for the resulting trace ratio optimization problem, which, under a mild assumption, is proved to enjoy both the local quadratic convergence and the global convergence to the global optimal solution (Zhang, L.-H., Liao, L.-Z., Ng, M.K.: SIAM J. Matrix Anal. Appl. 31:1584, 2010). The present paper further investigates the convergence behavior of this iterative method under no assumption. In particular, we prove that the iteration converges superlinearly when the mild assumption is removed. All possible limit points are characterized as a special subset of the global optimal solutions. An illustrative numerical example is also presented. The authors would like to thank two anonymous referees and the editor for their helpful comments and suggestions on the earlier version of this paper. Research of the second author was supported in part by FRG grants from Hong Kong Baptist University and the Research Grant Council of Hong Kong. Research of the third author was supported in part by RGC grants 7035/04P, 7035/05P and HKBU FRGs. L.-H. Zhang ( ) Department of Applied Mathematics, Shanghai University of Finance and Economics, 777 Guoding Road, Shanghai 200433, People’s Republic of China e-mail: longzlh@gmail.com L.-Z. Liao · M.K. Ng Department of Mathematics, Hong Kong Baptist University, Kowloon Tong, Kowloon, Hong Kong, People’s Republic of China L.-Z. Liao e-mail: liliao@hkbu.edu.hk M.K. Ng e-mail: mng@math.hkbu.edu.hk 854 J Optim Theory Appl (2013) 157:853–865	algorithm;cohesion (computer science);comstock–needham system;dimensionality reduction;email;feature extraction;functional renormalization group;iteration;iterative method;linear discriminant analysis;local convergence;mathematical optimization;numerical analysis;optimization problem;rate of convergence;sammon mapping	Lei-Hong Zhang;Li-Zhi Liao;Michael K. Ng	2013	J. Optimization Theory and Applications	10.1007/s10957-011-9832-4	mathematical optimization;combinatorics;calculus;mathematics	ML	77.34327416883458	22.123302913249308	105737
0f26ce868d985af1d0567938a03f550755949185	new proof of the gradient-based iterative algorithm for a complex conjugate and transpose matrix equation		Abstract By using the hierarchical identification principle, the gradient-based iterative algorithm is suggested for solving a complex conjugate and transpose matrix equation. With the tools of the real representation of a complex matrix and the vec-operator, a new convergence proof is offered. The necessary and sufficient conditions for the convergence factor is determined to guarantee the convergence of the algorithm for any initial iterative matrix. Also a conjecture proposed by Wu et al. (2011) is solved. Two numerical examples are offered to illustrate the effectiveness of the suggested algorithm and conclusions.	algorithm;gradient;iterative method	Huamin Zhang;Hongcai Yin	2017	J. Franklin Institute	10.1016/j.jfranklin.2017.09.005	discrete mathematics;mathematical optimization;positive-definite matrix;conjugate transpose;iterative method;transpose;mathematics;complex conjugate;real representation;matrix (mathematics);conjugate gradient method	ML	80.47010240331528	21.857964686600653	105748
d4cd9a221840e5dc1c136538121a36699e62340b	a class of linear generalized equations	local lipschitz like property;49j53;normal cone mapping;trust region subproblem;49j40;kkt point set map;linear generalized equation;coderivative;49j52	Solution stability of a class of linear generalized equations in finite dimensional Euclidean spaces is investigated by means of generalized differentiation. Exact formulas for the Fréchet and the Mordukhovich coderivatives of the normal cone mappings of perturbed Euclidean balls are obtained. Necessary and sufficient conditions for the local Lipschitz-like property of the solution maps of such linear generalized equations are derived from these coderivative formulas. Since the trust-region subproblems in nonlinear programming can be regarded as linear generalized equations, these conditions lead to new results on stability of the parametric trust-region subproblems.	boris mordukhovich;convex cone;nonlinear programming;nonlinear system;trust region	Nguyen Thanh Qui;Nguyen Dong Yen	2014	SIAM Journal on Optimization	10.1137/120882329	generalized linear mixed model;mathematical optimization;mathematical analysis;topology;mathematics;generalized linear array model;generalized forces	Theory	72.39341953924541	21.170132869638877	106253
07885461adfb097217dd995853986ad452ed1052	simultaneous state-time approximation of the chemical master equation using tensor product formats	parameter dependent problems;tensor products;chemical master equation;multilinear algebra;alternating iterative methods	We apply the novel tensor product formats (tensor train, quantized TT [QTT], and QTT-Tucker) to the solution of d -dimensional chemical master equations for gene regulating networks (signaling cascades, toggle switches, and phage). For some important cases, for example, signaling cascade models, we prove analytical tensor product representations of the system operator. The quantized tensor representations (QTT, QTT-Tucker) are employed in both state space and time, and the global state-time .d C 1/-dimensional system is solved in the tensor product form by the alternating minimal energy iteration, the ALS-type algorithm. This approach leads to the logarithmic dependence of the computational complexity on the volume of the state space. We investigate the proposed technique numerically and compare it with the direct chemical master equation solution and some previously known approximate schemes, where possible. We observe that the newer tensor methods demonstrate a good potential in simulation of relevant biological systems. Copyright © 2014 John Wiley & Sons, Ltd.	approximation algorithm;biological system;computational complexity theory;iteration;john d. wiley;network switch;numerical analysis;simulation;state space;sysop;tucker decomposition	S. V. Dolgov;Boris N. Khoromskij	2015	Numerical Lin. Alg. with Applic.	10.1002/nla.1942	tensor product;symmetric tensor;mathematical optimization;combinatorics;tensor field;tensor;multilinear algebra;cartesian tensor;tensor;tensor contraction;control theory;mathematics;tensor product of hilbert spaces;algorithm;algebra	ML	80.2764862016737	24.345711538637882	106433
004ed09b20ccd18f2c87e64eb12ecbb8b8f460ae	distributed min-max optimization application to time-optimal consensus: an alternating projection approach		In this paper, we proposed an alternating projection based algorithm to solve a class of distributed MIN-MAX convex optimization problems. We firstly transform this MINMAX problem into the problem of searching for the minimum distance between some hyper-plane and the intersection of the epigraphs of convex functions. The Bregmans alternating method is employed in our algorithm to achieve the distance by iteratively projecting onto the hyper-plane and the intersection. The projection onto the intersection is obtained by cyclic Dykstras projection method. We further apply our algorithm to the minimum time multi-agent consensus problem. The attainable states set for the agent can be transformed into the epigraph of some convex functions, and the search for time-optimal state for consensus satisfies the MIN-MAX problem formulation. Finally, the numerous simulation proves the validity of our algorithm.	algorithm;consensus (computer science);convex function;convex optimization;epigraph (mathematics);mathematical optimization;max;minimax;multi-agent system;simulation	Chunhe Hu;Zongji Chen	2014	CoRR		mathematical optimization	ML	69.91194375663993	24.054280231245826	106459
e3384dd240e5201d83bf68b3e093bcb02c856a10	practical acceleration for computing the hits expertrank vectors	computacion informatica;filter bound;principal eigenvector;hits;ranking;ciencias basicas y experimentales;matematicas;symmetric semidefinite matrix;grupo a;chebyshev filter	A meaningful rank as well as efficient methods for computing such a rank are necessary in many areas of applications. Major methodologies for ranking often exploit principal eigenvectors. Kleinberg’s HITS model is one of such methodologies. The standard approach for computing the HITS rank is the power method. Unlike the PageRank calculations where many acceleration schemes have been proposed, relatively few works on accelerating HITS rank calculation exist. This is mainly because the power method often works quite well in the HITS setting. However, there are cases where power method is ineffective, moreover, a systematic acceleration over the power method is desirable even when power method works well. We propose a practical acceleration scheme for HITS rank calculations based on filtered power method by adaptive Chebyshev polynomials. For cases where the gap-ratio is below 0.85 for which power method works well, our scheme is about twice faster than the power method. For cases where gap-ratio is unfavorable for the power method, our scheme can provide significant speedup. When the ranking problems are of very large scale, even a single matrix-vector product can be expensive, for which accelerations are highly necessary. The scheme we propose is desirable in that it provides consistent reduction in number of matrix-vector products as well as CPU time over the power method, with little memory overhead.	central processing unit;chebyshev polynomials;matrix multiplication;overhead (computing);pagerank;polynomial;power iteration;speedup	Yunkai Zhou	2012	J. Computational Applied Mathematics	10.1016/j.cam.2012.04.006	mathematical optimization;combinatorics;ranking;chebyshev filter;theoretical computer science;mathematics;algorithm;statistics	Web+IR	82.83897242316648	24.47562404220456	106740
2dce8b410dc1d02a364f636e306d74375219b44f	an iterative algorithm to solve periodic riccati differential equations with an indefinite quadratic term	iterative methods;algorithm design and analysis;iterative algorithm;differential equations;mathematical model	An iterative algorithm to solve periodic Riccati differential equations (PRDE) with an indefinite quadratic term is proposed. In our algorithm, we replace the problem of solving a PRDE with an indefinite quadratic term by the problem of solving a sequence of PRDEs with a negative semidefinite quadratic term which can be solved by existing methods. The global convergence is guaranteed and a proof is given.	algorithm;approximation algorithm;iterative method;local convergence;quadratic function;rate of convergence;simulation;theory	Yantao Feng;Brian D. O. Anderson	2008	2008 47th IEEE Conference on Decision and Control	10.1109/CDC.2008.4738777		Vision	80.04292774870058	21.824937486386407	106930
920e240a3da42997f2ba4e6e0a87501e48610e7f	breakdown-free gmres for singular systems	linear system 65f10;sistema lineal;metodo directo;iterative method;analisis numerico;65f20;matriz cuadrada;metodo subespacio krylov;linear system of equations;singular system;krylov subspace method;matrice singuliere;methode sousespace krylov;equation matricielle;grupo de excelencia;solucion aproximada;singular matrix;matrix inversion;inversion matriz;gran sistema;linear system;matrice carree;analyse numerique;metodo iterativo;square matrix;gmres method;numerical analysis;matrix equation;large system;solution approchee;ciencias basicas y experimentales;methode iterative;approximate solution;matematicas;algebra lineal numerica;algebre lineaire numerique;ecuacion matricial;inversion matrice;methode gmres;tecnologias generalidades;ams subject classification;numerical linear algebra;krylov subspace;matriz singular;tecnologias;systeme lineaire;iteration method;methode directe;65f10;direct method;grand systeme;matrice non singuliere	GMRES is a popular iterative method for the solution of large linear systems of equations with a square nonsingular matrix. When the matrix is singular, GMRES may break down before an acceptable approximate solution has been determined. This paper discusses properties of GMRES solutions at breakdown and presents a modification of GMRES to overcome the breakdown.	approximation algorithm;generalized minimal residual method;iterative method;linear system;the matrix	Lothar Reichel;Qiang Ye	2005	SIAM J. Matrix Analysis Applications	10.1137/S0895479803437803	calculus;mathematics;geometry;iterative method;algorithm;algebra	HPC	81.89568261476863	20.58896935670677	106963
21ac8ec76191988b521b138396773a55eb17e845	low frequency tangential filtering decomposition	linear algebra;block decomposition;low frequency;frequency filtering decompositions;iterative methods;preconditioner;partial differ ential equation;krylov method;eigenvectors	For unsymmetric block-tridiagonal systems of linear equations arising from the discretization of partial differential equations, a composite preconditioner is proposed and tested. It combines a classical ILU0 factorization for high frequencies with a tangential filtering preconditioner. The choice of the filtering vector is important: the test-vector is the Ritz eigenvector corresponding to the lowest approximate eigenvalue, obtained after a limited number of iterations of a ILU0 preconditioned Krylov method. Numerical tests are carried out for this method. Keywords— tangential factorization, deflation, convectiondiffusion, linear algebra	approximation algorithm;discretization;iteration;krylov subspace;linear algebra;linear equation;preconditioner;system of linear equations;test vector	Yves Achdou;Frédéric Nataf	2007	Numerical Lin. Alg. with Applic.	10.1002/nla.512	mathematical optimization;combinatorics;discrete mathematics;eigenvalues and eigenvectors;linear algebra;mathematics;preconditioner;iterative method;low frequency;algebra	HPC	82.60451528885159	21.994268271686206	107139
2659c52fd5270b7dc5da7bb649459cef8cf56d68	stabilizing block diagonal preconditioners for complex dense matrices in electromagnetics	singular value decomposition svd;iterative method;mlfma;singular value decomposition;preconditioning;linear system;computational complexity;block diagonalization;iteration method	Preconditioning techniques are widely used to speed up the convergence of iterative methods for solving large linear systems with sparse or dense coefficient matrices. For certain application problems, however, the standard block diagonal preconditioner makes the Krylov iterative methods converge more slowly or even diverge. To handle this problem, we apply diagonal shifting and stabilized singular value decomposition (SVD) to each diagonal block, which is generated from the multilevel fast multiple algorithm (MLFMA), to improve the stability and efficiency of the block diagonal preconditioner. Our experimental results show that the improved block diagonal preconditioner maintains the computational complexity of MLFMA, converges faster and also reduces the CPU cost.	preconditioner;sparse matrix	Xinyu Geng;Yin Wang;Jeonghwa Lee;Jun Zhang	2010	Applied Mathematics and Computation	10.1016/j.amc.2010.06.056	mathematical optimization;combinatorics;discrete mathematics;mathematics;iterative method;algebra	Theory	82.72671719172443	23.254392865071157	107141
1dfee1c99e5ec7718b7ad378f0379095585e6485	gauss-newton method for convex composite optimizations on riemannian manifolds	65h05;65j15;local convergence;satisfiability;riemannian manifolds;l average lipschitz condition;riemannian manifold;value function;lipschitz condition;gauss newton method	A notion of quasi-regularity is extended for the inclusion problem F(p) ∈ C , where F is a differentiable mapping from a Riemannian manifold M to Rn . When C is the set of minimum points of a convex real-valued function h on Rn and DF satisfies the L-average Lipschitz condition, we use the majorizing function technique to establish the semi-local convergence of sequences generated by the Gauss-Newton method (with quasiregular initial points) for the convex composite function h ◦ F on Riemannian manifold. Two applications are provided: one is for the case of regularities on Riemannian manifolds and the other is for the case when C is a cone and DF(p0)(·) − C is surjective. In particular, the results obtained in this paper extend the corresponding one in Wang et al. (Taiwanese J Math 13:633–656, 2009).	direction finding;gauss–newton algorithm;local convergence;newton's method;semiconductor industry	Jinhua Wang;Jen-Chih Yao;Chong Li	2012	J. Global Optimization	10.1007/s10898-010-9638-1	isometry;local convergence;riemannian geometry;mathematical optimization;mathematical analysis;topology;mathematics;lipschitz continuity;bellman equation;satisfiability	Theory	72.87351351075144	20.564062946581757	107173
f15f7d6ed374c0cc237abb4a76699e2bf480ad96	generic well-posedness of optimal control problems without convexity assumptions	complete metric space;90c31;integrand;set valued mapping;49j99;generic property;optimal control problem	The Tonelli existence theorem in the calculus of variations and its subsequent modifications were established for integrands f which satisfy convexity and growth conditions. In A. J. Zaslavski [Nonlinear Anal., to appear], a generic existence and uniqueness result (with respect to variations of the integrand of the integral functional) without the convexity condition was established for a class of optimal control problems satisfying the Cesari growth condition. In this paper we extend the generic existence and uniqueness result in A. J. Zaslavski [Nonlinear Anal., to appear], to a class of optimal control problems in which constraint maps are also subject to variations. The main result of the paper is obtained as a realization of a variational principle extending the variational principle introduced in A. D. Ioffe and A. J. Zaslavski [SIAM J. Control Optim., 38 (2000), pp. 566--581].	optimal control	Alexander J. Zaslavski	2000	SIAM J. Control and Optimization	10.1137/S0363012998345391	mathematical optimization;mathematical analysis;discrete mathematics;integral;generic property;mathematics;complete metric space	Theory	72.46411107740819	19.066957272228382	107240
93a28f01ee5c099fdf66d2fce2aa80b7ed505f68	an algebraic characterization of the optimum of regularized kernel methods	optimisation sous contrainte;modelizacion;analyse risque;constrained optimization;calculo de variaciones;representer theorem;cambio variable;kernel regression;closed form solution;optimum;analisis estadistico;support vector machines;methode noyau;risk analysis;fonction analytique;fonction reguliere;degree of freedom;exact solution;kernel methods;s unbiased risk estimate;kernel function;punto fijo;intelligence artificielle;solucion exacta;variational problem;satisfiability;fonction perte;funcion perdida;fixed point equation;support vector machines degrees of freedom;degrees of freedom;systeme algebrique;fixed point;estimacion insesgada;optimizacion con restriccion;modelisation;analisis riesgo;classification a vaste marge;calcul variationnel;analisis regresion;statistical analysis;convex function;loss function;point fixe;metodo nucleo;optimo;changement variable;funcion nucleo;analyse statistique;fonction noyau;analyse regression;artificial intelligence;funcion analitica;kernel method;representation theorem;regression analysis;funcion regular;inteligencia artificial;support vector machine;unbiased estimation;maquina ejemplo soporte;vector support machine;solution exacte;modeling;algebraic system;fonction convexe;analytical function;fix point;variational calculus;regularization theory;estimation sans biais;variable transformation;smooth function;funcion convexa	The representer theorem for kernel methods states that the solution of the associated variational problem can be expressed as the linear combination of a finite number of kernel functions. However, for non-smooth loss functions, the analytic characterization of the coefficients poses nontrivial problems. Standard approaches resort to constrained optimization reformulations which, in general, lack a closed-form solution. Herein, by a proper change of variable, it is shown that, for any convex loss function, the coefficients satisfy a system of algebraic equations in a fixed-point form, which may be directly obtained from the primal formulation. The algebraic characterization is specialized to regression and classification methods and the fixed-point equations are explicitly characterized for many loss functions of practical interest. The consequences of the main result are then investigated along two directions. First, the existence of an unconstrained smooth reformulation of the original non-smooth problem is proven. Second, in the context of SURE (Stein’s Unbiased Risk Estimation), a general formula for the degrees of freedom of kernel regression methods is derived.	algebraic equation;calculus of variations;coefficient;constrained optimization;fixed point (mathematics);kernel method;linear algebra;loss function;mathematical optimization;representer theorem	Francesco Dinuzzo;Giuseppe De Nicolao	2008	Machine Learning	10.1007/s10994-008-5095-1	support vector machine;kernel method;constrained optimization;combinatorics;kernel embedding of distributions;computer science;machine learning;calculus;mathematics;geometry;degrees of freedom;variable kernel density estimation	ML	74.42250260296377	20.11420158119861	107359
de6c4c6e5936983881b2e0397c27b4e7658d7eb5	"""""""teachers and classes"""" with neural networks"""	neural network	A convenient mapping and an eecient algorithm for solving scheduling problems within the neural network paradigm is presented. It is based on a reduced encoding scheme and a mean eld annealing prescription, which was recently successfully applied to TSP. Most scheduling problems are characterized by a set of hard and soft constraints. The prime target of this work is the hard constraints. In this domain the algorithm persistently nds legal solutions for quite diicult problems. We also make some exploratory investigations by adding soft constraints with very encouraging results. Our numerical studies cover problem sizes up to O(10 5) degrees of freedom with no parameter tuning. We stress the importance of adding self-coupling terms to the energy functions which are redundant from the encoding point of view but beneecial when it comes to ignoring local minima and to stabilizing the good solutions in the annealing process.	algorithm;coupling (physics);line code;maxima and minima;neural networks;numerical analysis;programming paradigm;scheduling (computing);simulated annealing	Lars Gislén;Carsten Peterson;Bo Söderberg	1989	Int. J. Neural Syst.	10.1142/S0129065789000074	mathematical optimization;computer science;machine learning;mathematics;artificial neural network;algorithm	Robotics	76.39697688147024	28.489252332198355	107711
4ed17ee32e1ac9cb4900d0dfed966cbd7566d7ba	a new approach for computing consistent initial values and taylor coefficients for daes using projector-based constrained optimization	dae;differential-algebraic equation;consistent initial value;index;derivative array;projector based analysis;nonlinear constrained optimization;sqp;automatic differentiation;65l05;65l80;34a09;34a34;65d25;90c30;90c55	This article describes a new algorithm for the computation of consistent initial values for differential-algebraic equations (DAEs). The main idea is to formulate the task as a constrained optimization problem in which, for the differentiated components, the computed consistent values are as close as possible to user-given guesses. The generalization to compute Taylor coefficients results immediately, whereas the amount of consistent coefficients will depend on the size of the derivative array and the index of the DAE. The algorithm can be realized using automatic differentiation (AD) and sequential quadratic programming (SQP). The implementation in Python using AlgoPy and SLSQP has been tested successfully for several higher index problems.	algorithm;automatic differentiation;coefficient;computation;constrained optimization;constraint (mathematics);differential algebraic equation;mathematical optimization;optimization problem;python;sequential quadratic programming;video projector	Diana Estévez Schwarz;René Lamour	2017	Numerical Algorithms	10.1007/s11075-017-0379-9	automatic differentiation;mathematics;mathematical analysis;projector;mathematical optimization;python (programming language);computation;constrained optimization;differential algebraic equation;sequential quadratic programming	Comp.	79.15655937491088	22.557738065017134	107791
209f53caa7024140f32891bf0fc11ddda0af780a	the skeleton reduction for finite element substructuring methods		We introduce an abstract concept for decomposing spaces with respect to a substructuring of a bounded domain. In this setting we define weakly conforming finite element approximations of quadratic minimization problems. Within a saddle point approach the reduction to symmetric positive Schur complement systems on the skeleton is analyzed. Applications include weakly conforming variants of least squares and minimal residuals. We consider general weakly conforming substructuring methods and its hybridization for the approximation of linear differential equations on Lipschitz domains Ω ⊂ R. The discretization is based on a decomposition Ωh = ⋃ K∈KK into convex open subdomains K ⊂ Ω with weak continuity constraints on the skeleton Γ = ⋃ ∂K = Ω \ Ωh. Here we present a general concept for the analysis of such discretizations based on corresponding saddle point formulations, and following [7] we consider the reduction to degrees of freedom to the skeleton. For comparison, we also summarize the DPG method [4] in this setting using formal trace mappings arising from integration by parts and quotient spaces replacing trace spaces. 1 Substructuring, trace spaces, and minimization Let L be a linear first-order differential operator with Lv ∈ L2(Ω,R ) for v ∈ C0(Ω,R ), and let L be its adjoint operator with (Lv,w)Ω = (v, L w)Ω , v ∈ C0(Ω,R ) , w ∈ C0(Ω,R ) . We define for L in Ω (and analogously for L and for open subsets of Ω) H(L,Ω) = { v ∈ L2(Ω,R ) : f ∈ L2(Ω,R ) exists with (f, w)Ω = (v, L w)Ω for all w ∈ C0(Ω,R ) } . Then, L extends to this space, and H(L,Ω) is a Hilbert spaces with respect to the graph norm ‖v‖L,Ω = √ ‖v‖Ω + ‖Lv‖Ω . For open subsets K ⊂ Ω we define the bilinear map γK(v, w) = (Lv,w)K − (v, Lw)K , v ∈ H(L,K) , w ∈ H(L,K)	approximation;bilinear filtering;discretization;finite element method;first-order predicate;hilbert space;least squares;quadratic programming;scott continuity	Christian Wieners	2015		10.1007/978-3-319-39929-4_14	skeleton (computer programming);mathematical optimization;finite element method;quadratic equation;least squares;bounded function;schur complement;mathematics;saddle point	AI	71.66163041439034	20.100115748076796	107822
b6b3074ffae9f913ed7aa445c8461250a7937784	existence theorem and algorithm for a general implicit variational inequality in banach space	iterative method;general implicit variational inequality;analisis numerico;projection operator;teorema existencia;convergence;matematicas aplicadas;desigualdad variacional;mathematiques appliquees;espacio banach;inegalite variationnelle;generalized f projection operator;banach space;existence theorem;58e35;65k15;iterative algorithm;journal;analyse numerique;metodo iterativo;algorithme;algorithm;49j40;convergencia;numerical analysis;methode iterative;approximate solution;existencia de solucion;existence of solution;variational inequality;operador proyeccion;46bxx;applied mathematics;theoreme existence;espace banach;existence solution;operateur projection;algoritmo	By using the generalized f-projection operator, the existence theorem of solutions for the general implicit variational inequality GIVI(T-@x,K) is proved without assuming the monotonicity of operators in reflexive and smooth Banach space. An iterative algorithm for approximating solution of the general implicit variational inequality is suggested also, and the convergence for this iterative scheme is shown. These theorems extend the corresponding results of Wu and Huang [K.Q. Wu, N.J. Huang, Comput. Math. Appl. 54 (2007) 399-406], Wu and Huang [K.Q. Wu, N.J. Huang, Bull. Austral. Math. Soc. 73 (2006) 307-317], Zeng and Yao [L.C. Zeng, J.C. Yao, J. Optimiz. Theory Appl. 132 (2) (2007) 321-337] and Li [J. Li, J. Math. Anal. Appl. 306 (2005) 55-71].	algorithm;calculus of variations;social inequality;variational inequality	Qing-bang Zhang;Ru-liang Deng;Liu Liu	2009	Applied Mathematics and Computation	10.1016/j.amc.2009.05.052	mathematical optimization;mathematical analysis;calculus;mathematics;iterative method;algorithm;algebra	ML	81.28223123819564	19.210004970441595	108005
ee01ceb38a5349be75c98c98ccb2c034ed294494	primal-dual first-order methods with o(1/e) iteration-complexity for cone programming	65k10;90c25;90c22;90c05	In this paper we consider the general cone programming problem, and propose primaldual convex (smooth and/or nonsmooth) minimization reformulations for it. We then discuss first-order methods suitable for solving these reformulations, namely, Nesterov’s optimal method [10, 11], Nesterov’s smooth approximation scheme [11], and Nemirovski’s prox-method [9], and propose a variant of Nesterov’s optimal method which has outperformed the latter one in our computational experiments. We also derive iterationcomplexity bounds for these first-order methods applied to the proposed primal-dual reformulations of the cone programming problem. The performance of these methods is then compared using a set of randomly generated linear programming and semidefinite programming (SDP) instances. We also compare the approach based on the variant of Nesterov’s optimal method with the low-rank method proposed by Burer and Monteiro [2, 3] for solving a set of randomly generated SDP instances.	approximation;conic optimization;experiment;first-order predicate;first-order reduction;iteration;linear programming;procedural generation;semidefinite programming;subgradient method	Guanghui Lan;Zhaosong Lu;Renato D. C. Monteiro	2011	Math. Program.	10.1007/s10107-008-0261-6	mathematical optimization;combinatorics;random coordinate descent;mathematics;algorithm	ML	73.06964204740218	24.483146928484313	108052
f32e9923b5404cde7d2b978f0f72e987e060114a	split bregman iteration and infinity laplacian for image decomposition	calculus of variations;split bregman iteration;computacion informatica;space of oscillating functions;viscosity solutions;ciencias basicas y experimentales;matematicas;image decomposition;grupo a;bv functions	In this paper, we address the issue of decomposing a given real-textured image into a cartoon/geometric part and an oscillatory/texture part. The cartoon component is modeled by a function of bounded variation, while, motivated by the works of Meyer [Y. Meyer, Oscillating Patterns in Image Processing and Nonlinear Evolution Equations, vol. 22 of University Lecture Series, AMS, 2001], we propose to model the oscillating component v by a function of the space G of oscillating functions, which is, in some sense, the dual space of BV (Ω). To overcome the issue related to the definition of the G-norm, we introduce auxiliary variables that naturally emerge from the Helmholtz–Hodge decomposition for smooth fields, which yields to the minimization of the L-norm of the gradients of the new unknowns. This constrained minimization problem is transformed into a series of unconstrained problems by means of Bregman Iteration. We prove the existence of minimizers for the involved subproblems. Then a gradient descent method is selected to solve each subproblem, becoming related, in the case of the auxiliary functions, to the infinity Laplacian. Existence/Uniqueness as well as regularity results of the viscosity solutions of the PDE introduced are proved. © 2012 Elsevier B.V. All rights reserved.	bounded variation;bregman divergence;gradient descent;image processing;iteration;vhdl-ams;viscosity solution	Cyrille Bonamy;Carole Le Guyader	2013	J. Computational Applied Mathematics	10.1016/j.cam.2012.07.008	mathematical optimization;mathematical analysis;calculus;mathematics;algorithm;calculus of variations;algebra	AI	76.19477888619288	24.76081601128311	108147
9e48e15532637dfaa0a985eb2f6132cbbe9aca04	recent advances in unconstrained optimization	unconstrained optimization	We survey the development of algorithms and theory for the unconstrained optimization problem during the years 1967–1970. Therefore (except for one remark) the material is taken from papers that have already been published. This exception is an explanation of some numerical difficulties that can occur when using Davidon's (1959) variable metric algorithm.	constrained optimization;mathematical optimization	M. J. D. Powell	1971	Math. Program.	10.1007/BF01584071	mathematical optimization;calculus;mathematics;algorithm	AI	69.01056347187115	22.581984382408162	108244
8e5263d4e8790cee43358908b98dca68dcfdade6	efficient calculation of jacobian matrices by optimized application of the chain rule to computational graphs			jacobian matrix and determinant	Uwe Naumann	1999				HPC	77.36914689862749	19.055974544854365	108298
49d9a7ac7b67a859218e985308107194e77f6eb7	the rayleigh-ritz method, refinement and arnoldi process for periodic matrix pairs	equation non lineaire;ecuacion no lineal;analisis numerico;ecuacion trascendente;computacion informatica;matematicas aplicadas;mathematiques appliquees;rayleigh ritz method;aproximacion;ecuacion algebraica;metodo arnoldi;equation transcendante;refinement;eigenvalues;49m15;eigenvector;ritz values;arnoldi method;analyse numerique;approximation;eigenvalue;vector propio;refinement method;numerical analysis;ciencias basicas y experimentales;transcendental equation;matrice periodique;arnoldi process;periodic matrix;matematicas;valor propio;algebra lineal numerica;algebre lineaire numerique;periodic matrix pairs;ritz value;equation algebrique;valeur propre;numerical linear algebra;methode raffinement;grupo a;non linear equation;applied mathematics;metodo afinamiento;article;algebraic equation;65f15;65h17;vecteur propre;periodic eigenvalues;matriz periodica;methode arnoldi;eigenvectors	We extend the Rayleigh–Ritz method to the eigen-problem of periodic matrix pairs. Assuming that the deviations of the desired periodic eigenvectors from the corresponding periodic subspaces tend to zero, we show that there exist periodic Ritz values that converge to the desired periodic eigenvalues unconditionally, yet the periodic Ritz vectorsmay fail to converge. To overcome this potential problem,weminimize residuals formedwith periodic Ritz values to produce the refined periodic Ritz vectors, which converge under the same assumption. These results generalize the correspondingwell-knownones for Rayleigh–Ritz approximations and their refinement for non-periodic eigen-problems. In addition, we consider a periodic Arnoldi process which is particularly efficient when coupled with the Rayleigh–Ritzmethodwith refinement. Thenumerical results illustrate that the refinement procedure produces excellent approximations to the original periodic eigenvectors. © 2010 Elsevier B.V. All rights reserved.	approximation;arnoldi iteration;converge;eigen (c++ library);existential quantification;final fantasy tactics advance;rayleigh–ritz method;refinement (computing)	Eric King-Wah Chu;Hung-Yuan Fan;Zhongxiao Jia;Tie-xiang Li;Wen-Wei Lin	2011	J. Computational Applied Mathematics	10.1016/j.cam.2010.11.014	combinatorics;mathematical analysis;eigenvalues and eigenvectors;ritz method;calculus;periodic sequence;mathematics;algebra	HPC	81.84568724386895	18.365802897845715	108530
7dc46b4cbb9efada2bce1172f54adc57f46ed340	duality theory and applications to unilateral problems	variational inequalities;elastic plastic torsion;quasi relative interior;strong duality	This paper is concerned with the problem of strong duality between an infinite dimensional convex optimization problem with cone and equality constraints and its Lagrange dual.#R##N##R##N#A necessary and sufficient condition and sufficient conditions, really new, in order that the strong duality holds true are given. As an application, the existence of the Lagrange multiplier associated with the obstacle problem and to an elastic---plastic torsion problem, more general than the ones previously considered, is stated together with a characterization of the elastic---plastic torsion problem. This application is the main result of the paper.#R##N##R##N#It is worth remarking that the usual conditions based on the interior, on the core, on the intrinsic core or on the strong quasi-relative interior cannot be used because they require the nonemptiness of the interior (and of the above mentioned generalized interior concepts) of the ordering cone, which is usually empty.		Patrizia Daniele;Sofia Giuffrè;Antonino Maugeri;Fabio Raciti	2014	J. Optimization Theory and Applications	10.1007/s10957-013-0512-4	mathematical optimization;mathematical analysis;duality;topology;duality gap;weak duality;mathematics;strong duality;slater's condition	Theory	70.84840315480274	20.16160957130422	108723
7c5de952d52679aec71a77375fe6ea54d46347ba	block lu factorizations of m-matrices	matriz bloque;algebraic combinatorics;numerical method;matriz triangular;matrice singuliere;triangular matrix;singular matrix;m matrix;metodo factorizacion;metodo numerico;matrice bloc;factorization method;necessary and sufficient condition;matriz m;matrice triangulaire;block matrix;matriz singular;methode factorisation;matrice m;lu factorization;methode numerique	It is well known that any nonsingular M–matrix admits an LU factorization into M–matrices (with L and U lower and upper triangular respectively) and any singular M–matrix is permutation similar to an M–matrix which admits an LU factorization into M–matrices. Varga and Cai establish necessary and sufficient conditions for a singular M–matrix (without permutation) to allow an LU factorization with L nonsingular. We generalize these results in two directions. First, we find necessary and sufficient conditions for the existence of an LU factorization of a singular M-matrix where L and U are both permitted to be singular. Second, we establish the minimal block structure that a block LU factorization of a singular M–matrix can have when L and U are M–matrices.	incomplete lu factorization;lu decomposition;qr decomposition;triangular matrix	Judith J. McDonald;H. Schneider	1998	Numerische Mathematik	10.1007/s002110050362	combinatorics;incomplete lu factorization;lu decomposition;numerical analysis;calculus;triangular matrix;mathematics;m-matrix;block matrix;algorithm;algebraic combinatorics;algebra	HPC	79.8263406874541	20.480953614724438	109001
5d9f451b0c22b80a556f898934ff8f7aba3cbc8f	the uniqueness and greedy method for quadratic compressive sensing	sparsity;greedy algorithm;quadratic compressive sensing;uniform s regularity;uniqueness	Quadratic compressive sensing, as a nonlinear extension of compressive sensing, has attracted considerable attention in optical image, X-ray crystallography, transmission electron microscopy, etc. We introduce the concept of uniform s-regularity to study the uniqueness in quadratic compressive sensing and propose a greedy algorithm for the corresponding numerical optimization. Moreover, we prove the convergence of the proposed algorithm under the uniform s-regularity condition. Finally, we present numerical results to demonstrate the efficiency of the proposed method.	compressed sensing;electron;greedy algorithm;image;mathematical optimization;nonlinear system;numerical analysis	Jun Fan;Lingchen Kong;Liqun Wang;Naihua Xiu	2016	2016 IEEE International Conference on Data Science and Advanced Analytics (DSAA)	10.1109/DSAA.2016.94	mathematical optimization;combinatorics;mathematical analysis;mathematics	Robotics	76.60046079898636	23.849882116760163	109051
02b2ce3c396ae13ea387a0c0403976f14cc5ba81	sine transform based preconditioners for elliptic problems		We consider applying the preconditioned conjugate gradient (PCG) method to solve linear systems Ax = b where the matrix A comes from the discretization of second-order elliptic operators. Let (L +)) ?1 (L t +) denote the block Cholesky factorization of A with lower block triangular matrix L and diagonal block matrix. We propose a preconditioner M = (^ L +)) ?1 (^ L t +) with block diagonal matrix and lower block triangular matrix ^ L. The diagonal blocks of and the subdiagonal blocks of ^ L are respectively the optimal sine transform approximations to the diagonal blocks of and the subdiagonal blocks of L. We show that for 2-dimensional domains, the construction cost of M and the cost for each iteration of the PCG algorithm are of order O(n 2 log n). Furthermore, for rectangular regions, we show that the condition number of the preconditioned system M ?1 A is of order O(1). In contrast, the system preconditioned by the MILU and MINV methods are of order O(n). We will also show that M can be obtained from A by taking the optimal sine transform approximations of each sub-block of A. Thus the construction of M is similar to that of Level-1 circulant preconditioners. Our numerical results on 2-dimensional square and L-shaped domains show that our method converges faster than the MILU and MINV methods. Extension to higher-dimensional domains will also be discussed.	algorithm;approximation;ccir system m;cholesky decomposition;circulant matrix;condition number;conjugate gradient method;convex conjugate;discretization;iteration;linear system;numerical analysis;preconditioner;the matrix;triangular matrix	Raymond H. Chan;Chiu-Kwong Wong	1997	Numerical Lin. Alg. with Applic.	10.1002/(SICI)1099-1506(199709/10)4:5%3C351::AID-NLA103%3E3.0.CO;2-4	mathematical optimization;mathematical analysis;semi-elliptic operator;mathematics;nonlinear conjugate gradient method;linear system;elliptic partial differential equation;algebra;elliptic operator	AI	81.98560957390131	21.909154568323135	109109
d45a216082e94ca8a047e678803d05bc17b1a218	an algorithm for the solution of linear inequalities	threshold logic;conjugate gradient;linear minimization;function minimization;conjugate gradient consistency function minimization linear inequalities linear minimization pattern classification threshold logic;pattern classification;linear inequalities;computer simulation;consistency	The problem of solving a system of linear inequalities is central to pattern classification where a solution to the system, consistent or not, is required. In this paper, an algorithm is developed using the method of conjugate gradients for function minimization. Specifically, it is shown that the algorithm converges to a solution in both the consistent and inconsistent cases in a finite number of steps: this is the main result. A related criterion function which has significance in pattern classification problems is derived and a variant of the algorithm to minimize the same is given along with computationally convenient modifications. A linear minimization algorithm which makes complete use of the problem structure is given: this is a part of the main algorithm. Computer simulation results for switching problems are presented and the algorithm is compared with Ho–Kashyap and accelerated relaxation algorithms; the results show that the proposed algorithm is faster than the latter algorithms with respect to both the number of iterations and time for convergence.	algorithm;computer simulation;conjugate gradient method;iteration;lagrangian relaxation;linear inequality;linear programming relaxation;loss function	G. Nagaraja;Gopalrao Krishna	1974	IEEE Transactions on Computers	10.1109/T-C.1974.223957	computer simulation;mathematical optimization;combinatorics;discrete mathematics;criss-cross algorithm;linear inequality;mathematics;conjugate gradient method;consistency;quantum algorithm for linear systems of equations;algebra	ML	77.7000995719429	25.063215452191987	109175
a609c2b8ed969ae34c265d645bc4a8cd6a357d70	a line search filter secant method for nonlinear equality constrained optimization	second order;constrained optimization;superlinear convergence;line search;nonlinear programming;global convergence;second order correction;zhujun wang detong zhu 非线性约束优化 等式约束优化 割线法 线搜索 滤子 搜索方向 收敛性分析 超线性收敛 a line search filter secant method for nonlinear equality constrained optimization;secant algorithm;numerical experiment;filter method;li cai detong zhu 非线性等式约束 sqp方法 线搜索 最优化方法 滤子 超线性收敛速度 约束优化问题 搜索技术 a line search filter inexact sqp method for nonlinear equality constrained optimization	This paper formulates and analyzes a line search method for general nonlinear equality constrained optimization based on filter methods for step acceptance and secant methods for search direction. The feature of the new algorithm is that the secant algorithm is used to produce a search direction, a backtracking line search procedure is used to generate step size, some filtered rules are used to determine step acceptance, second order correction technique is used to reduce infeasibility and overcome the Maratos effect. Global convergence properties of this method are analyzed: under mild assumptions it is showed that every limit point of the sequence of iterates generated by the algorithm is feasible, and that there exists at least one limit point that is a stationary point for the problem. Moreover, it is also established that the Maratos effect can be overcome in our new approach by adding second order correction steps so that fast local superlinear convergence to a second order sufficient local solution is achieved. Finally, the results of numerical experiments are reported to show the effectiveness of the line search filter secant method.	algorithm;backtracking line search;constrained optimization;experiment;mathematical optimization;nonlinear system;numerical analysis;rate of convergence;secant method;stationary process	Zhujun Wang;Detong Zhu	2010	J. Systems Science & Complexity	10.1007/s11424-010-8012-1	mathematical optimization;constrained optimization;combinatorics;nonlinear programming;sidi's generalized secant method;calculus;backtracking line search;mathematics;line search;second-order logic;secant method	AI	76.37609083593993	23.54317000606288	109303
7d42229e0c7fa886414973672ddcaea64d9d6635	algorithm 587: two algorithms for the linearly constrained least squares problem	constrained least square	THIS SUBPROGRAM SOLVES A LINEARLY CONSTRAINED LEAST SQUARES PROBLEM. SUPPOSE THERE ARE GIVEN MATRICES E AND A OF RESPECTIVE DIMENSIONS ME BY N AND MA BY N, AND VECTORS F AND B OF RESPECTIVE LENGTHS ME AND MA. THIS SUBROUTINE SOLVES THE PROBLEM EX = F, (EQUATIONS TO BE EXACTLY SATISFIED) AX = B, (EQUATIONS TO BE APPROXIMATELY SATISFIED, IN THE LEAST SQUARES SENSE) SUBJECT TO COMPONENTS L+I,...,N NONNEGATIVE ANY VALUES ME.GE.0, MA.GE.0 AND 0. LE. L .LE.N ARE PERMITTED. THE PROBLEM IS REPOSED AS PROBLEM WNNLS (WT*E)X = (WT*F) ( A) ( B), (LEAST SQUARES) SUBJECT TO COMPONENTS L+I,...,N NONNEGATIVE. THE SUBPROGRAM CHOOSES THE HEAVY WEIGHT (OR PENALTY PARAMETER) WT THE PARAMETERS FOR WNNLS ARE	algorithm;least squares;microsoft dynamics ax;subroutine	Richard J. Hanson;Karen H. Haskell	1982	ACM Trans. Math. Softw.	10.1145/356004.356010	generalized least squares;total least squares;mathematical optimization;non-linear iterative partial least squares;mathematics;non-linear least squares;least squares;recursive least squares filter	Theory	79.02899332075704	22.538695823175875	109624
93744890c3445b3243646ee5f6b0a6f1ad87d9bd	the inverse eigenproblem with a submatrix constraint and the associated approximation problem for (r, s)-symmetric matrices	computacion informatica;ciencias basicas y experimentales;matematicas;grupo a	a Geomathematics Key Laboratory of Sichuan Province, College of Geophysics, Chengdu University of Technology, Chengdu, 610059, PR China b School of Science, Sichuan University of Science and Engineering, Zigong, 643000, PR China c Geomathematics Key Laboratory of Sichuan Province, College of Management Science, Chengdu University of Technology, Chengdu, 610059, PR China d Space Science and Engineering Center, University of Wisconsin-Madison, Madison, 53706, USA	approximation;management science	Feng Yin;Ke Guo;Guang-Xin Huang;Bormin Huang	2014	J. Computational Applied Mathematics	10.1016/j.cam.2014.01.038	mathematical optimization;combinatorics;calculus;mathematics	Theory	77.81743527227138	20.855401668737066	109793
2777c41e8cbb3a607eeecdf324343cca7ff5f15d	the lidskii-mirsky-wielandt theorem - additive and multiplicative versions	linear algebra;analisis numerico;desigualdad;inequality;perturbation theory;singular value;matrice singuliere;inegalite;singular matrix;eigenvector;analyse numerique;eigenvalue;vector propio;descomposicion matricial;numerical analysis;matrix inequality;decomposition matricielle;matrix decomposition;algebre lineaire;valor propio;matrice hermitienne;algebre lineaire numerique;algebra lineal;valeur propre;matriz hermitiana;matriz singular;hermitian matrices;inegalite matricielle;theorie perturbation;hermitian matrix;vecteur propre;teoria perturbacion	We use a simple matrix splitting technique to give an elementary new proof of the Lidskii-Mirsky-Wielandt Theorem and to obtain a multiplicative analog of the Lidskii-Mirsky-Wielandt Theorem, which we argue is the fundamental bound in the study of relative perturbation theory for eigenvalues of Hermitian matrices and singular values of general matrices. We apply our bound to obtain numerous bounds on the matching distance between the eigenvalues and singular values of matrices. Our results strengthen and generalize those in the literature.	matrix splitting;perturbation theory;utility functions on indivisible goods	Chi-Kwong Li;Roy Mathias	1999	Numerische Mathematik	10.1007/s002110050397	hermitian matrix;combinatorics;mathematical analysis;eigenvalues and eigenvectors;linear algebra;calculus;fundamental theorem;mathematics;algebra	Theory	78.52080364617042	19.984921955620518	109855
3c7f4c12799c14c9e8881520f886b271c3a8a7ae	systolic implementation of the adaptive solution to normal equations	linear algebra;parallelisme;red sistolica;traitement signal;adaptive filtering;implementation;filtrado adaptable;real time;ecuacion lineal;ejecucion;parallelism;paralelismo;systolic network;algebre lineaire;signal processing;temps reel;reseau systolique;tiempo real;algebra lineal;filtrage adaptatif;linear equation;procesamiento senal;equation lineaire	We are interested in the systolic computation of projection operators entering digital signal processing, or more precisely, solution of the so-called normal equations involved in adaptive systems. The systolic array proposed achieves a real-time adaptive solution, i.e., updates the left- and right-hand sides of the linear equation and computes its solution at each time step.	linear least squares (mathematics)	Pierre Comon;Yves Robert;Denis Trystram	1990	Computer Vision, Graphics, and Image Processing	10.1016/0734-189X(90)90083-8	adaptive filter;computer vision;discrete mathematics;computer science;linear algebra;signal processing;mathematics;geometry;linear equation;implementation;algorithm	Vision	82.24152454825901	26.712037197835485	109927
c957b01e168eeb9de9e65656fbb6bc368f7d2ecf	a newton-penalty method for a simplified liquid crystal model	65n30;newton s method;35a40;saddle points;65c20;regularized functional;harmonic maps;nonlinear constraint	In this paper we are concerned with the computation of a liquid crystal model defined by a simplified Oseen-Frank energy functional and a (sphere) nonlinear constraint. A particular case of this model defines the well known harmonic maps. We design an new iterative method for solving such a minimization problem with the nonlinear constraint. The main ideas are to linearize the nonlinear constraint by Newton’s method and to define a suitable penalty functional associated with the original minimization problem. It is shown that the solution sequence of the new minimization problems with the linear constraints converges to the desired solutions provided that the penalty parameters are chosen by a suitable rule. Numerical results confirm the efficiency of the new method.	computation;iterative method;map;newton;newton's method;nonlinear system;numerical method;penalty method;whole earth 'lectronic link	Qiya Hu;Long Yuan	2014	Adv. Comput. Math.	10.1007/s10444-013-9305-4	mathematical optimization;mathematical analysis;calculus;harmonic map;mathematics;geometry;saddle point;newton's method	Robotics	76.16328911293758	24.810629641425425	109956
d3e23103d2f579d95405d9ce975dd819737262de	computing proximal points of nonconvex functions	secondary 49m05;critical point;nonconvex optimization;optimization technique;primary;convex optimization;secondary 65k10;natural extension;fixed point;primary 49j52;secondary 49j53;convex function;lipschitz continuity;variational analysis;proximal point method	The proximal point mapping is the basis of many optimization techniques for convex functions. By means of variational analysis, the concept of proximal mapping was recently extended to nonconvex functions that are prox-regular and prox-bounded. In such a setting, the proximal point mapping is locally Lipschitz continuous and its set of fixed points coincide with the critical points of the original function. This suggests that the many uses of proximal points, and their corresponding proximal envelopes (Moreau envelopes), will have a natural extension from Convex Optimization to Nonconvex Optimization. For example, the inexact proximal point methods for convex optimization might be redesigned to work for nonconvex functions. In order to begin the practical implementation of proximal points in a nonconvex setting, a first crucial step would be to design efficient methods of approximating nonconvex proximal points. This would provide a solid foundation on which future design and analysis for nonconvex proximal point methods could flourish. In this paper we present a methodology based on the computation of proximal points of piecewise affine models of the nonconvex function. These models can be built with only the knowledge obtained from a black box providing, for each point, the function value and one subgradient. Convergence of the method is proved for the class of nonconvex functions that are prox-bounded and lower-C and encouraging preliminary numerical testing is reported.	approximation algorithm;black box;computation;convex function;convex optimization;mathematical optimization;numerical analysis;program optimization;subderivative;subgradient method;variational analysis	Warren Hare;Claudia A. Sagastizábal	2009	Math. Program.	10.1007/s10107-007-0124-6	convex function;mathematical optimization;proximal gradient methods for learning;mathematical analysis;convex optimization;topology;variational analysis;mathematics;fixed point;proximal gradient methods;lipschitz continuity;critical point	ML	73.18772210235872	22.828072943624438	110188
f66593ede3c3c28a3502055a314cb4cd26e50fe0	solution algorithm for an optimistic linear stackelberg problem	parametric optimization;bilevel programming;solution algorithm	The optimistic Stackelberg problem is a bilevel programming problem where the constraints in the lower level problem are parameter independent. For linear problems of that type, algorithms for computing local and global optimal solutions are suggested. Their convergence is shown. In the last part, problems with perturbed right-hand side of the lower level constraints are considered, and the behavior of optimal solutions and of the optimal function value is investigated.	algorithm	Stephan Dempe;Susanne Franke	2014	Computers & OR	10.1016/j.cor.2012.09.002	mathematical optimization;combinatorics;computer science;mathematics;mathematical economics;bilevel optimization	AI	70.98617044666851	21.077674127422473	110297
3367332acbc68cd49dc9be665556a7815a09b44b	sparsity constrained nonlinear optimization: optimality conditions and algorithms	90c90;compressed sensing;90c26;optimality conditions;sparsity constrained problems;numerical methods;90c46;stationarity	This paper treats the problem of minimizing a general continuously differentiable function subject to sparsity constraints. We present and analyze several different optimality criteria which are based on the notions of stationarity and coordinate-wise optimality. These conditions are then used to derive three numerical algorithms aimed at finding points satisfying the resulting optimality criteria: the iterative hard thresholding method and the greedy and partial sparse-simplex methods. The first algorithm is essentially a gradient projection method while the remaining two algorithms are of coordinate descent type. The theoretical convergence of these methods and their relations to the derived optimality conditions are studied. The algorithms and results are illustrated by several numerical examples.	coordinate descent;gradient;greedy algorithm;iterative method;nonlinear programming;numerical analysis;numerical method;simplex algorithm;sparse matrix;stationary process;thresholding (image processing)	Amir Beck;Yonina C. Eldar	2013	SIAM Journal on Optimization	10.1137/120869778	stationary process;mathematical optimization;combinatorics;discrete mathematics;numerical analysis;mathematics;compressed sensing	ML	73.98370019112993	25.020353476802832	110388
c1c69f1d6ecebb8cb97b6feb6256dc2bcc16663d	a variant of steffensen-king's type family with accelerated sixth-order convergence and high efficiency index: dynamic study and approach	articulo revista indexada;efficiency index;derivative free;multipoint iterative methods;steffensen s method;jcr;king s family	Keywords: Multipoint iterative methods Steffensen's method King's family Derivative-free Efficiency index a b s t r a c t First, it is attempted to derive an optimal derivative-free Steffensen–King's type family without memory for computing a simple zero of a nonlinear function with efficiency index 4 1=3 % 1:587. Next, since our without memory family includes a parameter in which it is still possible to increase the convergence order without any new function evaluations. Therefore, we extract a new method with memory so that the convergence order rises to six without any new function evaluation and therefore reaches efficiency index 6 1=3 % 1:817. Consequently, derivative-free and high efficiency index would be the substantial contributions of this work as opposed to the classical Steffensen's and King's methods. Finally, we compare some of the convergence planes with different weight functions in order to show which are the best ones. Construction and development of Multipoint methods without memory, for computing a simple or multiple roots of a given nonlinear function, is based on Kung and Traub's conjecture [12], proved in some special cases [22,25,26]. It is supposed that any multipoint method without memory by using n function evaluations per iteration can reach optimal convergence order 2 n. The most well known optimal method is Newton's method [2] but we are interested in two-point methods. There are some legendary optimal two-point methods without memory like King's [10], Ostrowski's [16] and Jarratt's [7,8,14] methods in which they consume three function evaluations per iteration with optimal convergence four. In spite being optimal, they require evaluation of the first derivative at one or two points and hence cannot be used for nonsmooth functions. Needless to say, King's and Ostrowski's type methods have been considerably used to create higher optimal order methods without memory on the other hand, based our best knowledge, there is no higher optimal order of Jarratt's type method. In this work, King's family is modified to avoid using derivative evaluation and consequently can be applied to nonsmooth functions. To this end, its first step is replaced by an Steffensen's type method [19] and its second step is altered by considering the idea of weight function. It is attempted to preserve optimality, too. Until now, this modified family can be called a	frank ostrowski;iteration;iterative method;multipoint ground;newton's method;nonlinear system;rate of convergence;steffensen's method;type family;weight function	Taher Lotfi;Ángel Alberto Magreñán;Katayoun Mahdiani;J. Javier Rainer	2015	Applied Mathematics and Computation	10.1016/j.amc.2014.12.033	mathematical optimization;steffensen's method;mathematical analysis;calculus;control theory;mathematics;mathematical economics;thermodynamics	ML	78.13386963019552	26.07119625801434	110403
f4df8765083ce66417d9a1e211fde5789067eef1	convergence rates of evolutionary algorithms for quadratic convex functions with rank-deficient hessian		The best achievable convergence rates of mutation-based evolutionary algorithms are known for various characteristic test problems. Most results are available for convex quadratic functions with Hessians of full rank. Here, we prove that linear convergence rates are achievable for convex quadratic functions even though the Hessians are rank-deficient. This result has immediate implications for recent convergence results for certain evolutionary algorithms for bi-objective optimization problems.	convex function;evolutionary algorithm;hessian;mathematical optimization;optimization problem;optimizing compiler;quadratic function;quadratic programming;rate of convergence	Günter Rudolph	2013		10.1007/978-3-642-37213-1_16	convex function;convex analysis;subderivative;convex optimization	ML	72.5765030066305	25.254593498791646	110490
4a49bfd3596d422424b6c7552af7e2fed5bf3c48	sequential threshold control in descent splitting methods for decomposable optimization problems	decomposable problems;68w15;90c30;90c25;threshold control;composite optimization;descent splitting methods;coordinate wise steps;68w10;projection methods	We suggest a modification of the descent splitting methods for decomposable composite optimization problems, which maintains the basic convergence properties, but enables one to reduce the computational expenses per iteration and to provide computations in a distributed manner. It consists in making coordinate-wise steps together with a special threshold control.	approximation algorithm;calculus of variations;computation;edmund m. clarke;gradient descent;iteration;john d. wiley;linear programming relaxation;mathematical optimization;maxima and minima;multi-agent system;nonlinear programming;numerical methods for ordinary differential equations;pl/i;prentice hall international series in computer science;relaxation (approximation);relaxation (iterative method);shadow volume;springer (tank);subderivative;variational inequality;variational principle	Igor V. Konnov	2015	Optimization Methods and Software	10.1080/10556788.2015.1030015	mathematical optimization;combinatorics;mathematics;algorithm	ML	77.65031281161893	23.02911973653005	110499
9d4bd3f4b32a8d293b67d1307703e69c1bb7b1be	intensity modulated radiotherapy treatment planning by use of a barrier-penalty multiplier method	treatment planning;equivalent uniform dose;radiation therapy treatment planning;convex programming;biological effect;multiplier methods;theory and method;inequality constraint;global convergence;convex optimization;nonlinear rescaling method;local convergence;spectrum;conjugate gradient method;objective function;optimization problem;cancer treatment;large scale;partial volume;mathematical programming;intensity modulation;barrier penalty functions;radiation therapy;barrier algorithms;nonlinear optimization;constrained optimization problem;barrier function;industrial engineering;intensity modulated radiation therapy;penalty function	The use of nonlinear functions describing biological effects has recently become a major goal in connection with intensity modulated radiotherapy planning models for cancer treatment. In this article, we present a new biological model for this purpose and discuss the solution of the related large-scale nonlinear optimization problems. The model includes equivalent uniform dose and partial volume constraints and employs tumor control probability as the objective. The resulting optimization problems are convex; there are nonconvex constrained optimization problems with several thousands of variables for which gradients of the involved functions are available, but the computation of Hessians is too costly. It is suggested to solve these problems using the barrier-penalty multiplier method by Polyak ([Polyak, R., 1992, Modified barrier functions (theory and methods). Mathematical Programming, 54, 177-222.], [Polyak, R., 2002, Nonlinear rescaling vs. smoothing technique in convex optimization. Mathematical Programming, 92, 197-235.]) and Ben-Tal et al. and Ben-Tal and Zibulevsky ([Ben-Tal, A., Yuzefokich, I. and Zibulevsky, M., 1992, Penalty/barrier multiplier methods for minimax and constrained smooth convex problems. Technical Report 9/92, Optimization Laboratory, Faculty of Industrial Engineering and Management, Technion, Haifa, Israel.], [Ben-Tal, A. and Zibulevsky, M., 1997, Penalty/barrier multiplier methods for convex programming problems. SIAM Journal of Optimization, 7, 347-366.]), where this algorithm is modified according to ideas which are motivated by the related Lagrangian barrier algorithm of Conn et al. ([Conn, A.R., Gould, N.I.M. and Taint, P., 1992, A globally convergent Lagrangian barrier algorithm for optimization with general inequality constraints and simple bounds. Technical Report 92/07, Department of Maths, FUNDP, Namur, Belgium.], [Conn, A.R., Gould, N.I.M. and Taint, P.L., 1992, A globally convergent Lagrangian barrier algorithm for optimization with general inequality constraints and simple bounds. Mathematics of Computation, 66, 261-288.]). In particular, the subproblems in the algorithm are solved by a conjugate gradient method, as the spectrum of the Hessian of the Lagrangian at a solution of such problem indicates fast (local) convergence of the objective function values to a good approximate (locally) optimal value. Some characteristic numbers showing the average numerical performance of the algorithm are tabulated for various types of tumors and for a set of 127 clinical cases in total. Its capabilities and typical behavior also are illustrated explicitly by a computed therapy plan for a difficult clinical case of a larynx tumor.	modulation	M. Alber;Rembert Reemtsen	2007	Optimization Methods and Software	10.1080/10556780600604940	local convergence;optimization problem;spectrum;mathematical optimization;radiation therapy;barrier function;convex optimization;nonlinear programming;intensity modulation;calculus;penalty method;mathematics;conjugate gradient method;mathematical economics;partial volume	Robotics	72.50133931811274	26.094982063022556	110538
a54aa6e79733ee69b8431d821ee6457b3761f6cc	non-negative independent component analysis algorithm based on 2d givens rotations and a newton optimization	newton optimization;non negative ica;givens rotations;complexity calculation	In this paper, we consider the Independent Component Analysis problem when the hidden sources are non-negative (Non-negative ICA). This problem is formulated as a non-linear cost function optimization over the special orthogonal matrix group SO(n). Using Givens rotations and Newton optimization, we developed an effective axis pair rotation method for Non-negative ICA. The performance of the proposed method is compared to those designed by Plumbley and simulations on synthetic data show the efficiency of the proposed algorithm.	algorithm;apache axis;givens rotation;independent computing architecture;independent component analysis;mathematical optimization;newton;nonlinear system;simulation;synthetic data	Wendyam Serge Boris Ouedraogo;Antoine Souloumiac;Christian Jutten	2010		10.1007/978-3-642-15995-4_65	mathematical optimization;mathematics;geometry;algorithm;givens rotation	Robotics	79.23191017753841	22.313737817876763	110964
c0a29917ec3f8c497cbe99607703b6bec7c335e8	hierarchical kronecker tensor-product approximations	integral equations;hierarchical matrices;kronecker products;bem;low-rank matrices;tensors;multi- dimensional matrices;kronecker product;tensor product;differential operators;integral equation	The goal of this work is the presentation of some new formats which are useful for the approximation of (large and dense) matrices related to certain classes of functions and nonlocal (integral, integrodifferential) operators, especially for high-dimensional problems. These new formats elaborate on a sum of few terms of Kronecker products of smaller-sized matrices (cf. [34, 35]). In addition to this we need that the Kronecker factors possess a certain data-sparse structure. Depending on the construction of the Kronecker factors we are led to so-called “profile-low-rank matrices” or hierarchical matrices (cf. [17, 18]). We give a proof for the existence of such formats and expound a gainful combination of the Kronecker-tensor-product structure and the arithmetic for hierarchical matrices. AMS Subject Classification: 65F50, 65F30, 65N38, 65N35, 15A09	approximation;approximation algorithm;computation;distribution (mathematics);emoticon;experiment;fast fourier transform;generalized minimal residual method;interpolation;modulus of smoothness;nonlocal lagrangian;numerical analysis;polynomial;sinc function;sinc numerical methods;singular value decomposition;sparse matrix;technological singularity;truncation;vhdl-ams	Wolfgang Hackbusch;Boris N. Khoromskij;Eugene E. Tyrtyshnikov	2005	J. Num. Math.	10.1515/1569395054012767	tensor product;differential operator;combinatorics;mathematical analysis;discrete mathematics;matrix multiplication;mathematics;kronecker product;integral equation;matrix addition;algebra	Theory	81.74874794552991	22.706443838715337	111003
1cbbe3e1bcaa81e2172ae1bf05fd51c4c231e6f8	a nonmonotone trust region method with new inexact line search for unconstrained optimization	numerical experiments;global convergence;trust region method;unconstrained optimization;inexact line search	In this paper, a new nonmonotone inexact line search rule is proposed and applied to the trust region method for unconstrained optimization problems. In our line search rule, the current nonmonotone term is a convex combination of the previous nonmonotone term and the current objective function value, instead of the current objective function value . We can obtain a larger stepsize in each line search procedure and possess nonmonotonicity when incorporating the nonmonotone term into the trust region method. Unlike the traditional trust region method, the algorithm avoids resolving the subproblem if a trial step is not accepted. Under suitable conditions, global convergence is established. Numerical results show that the new method is effective for solving unconstrained optimization problems.	algorithm;line search;local convergence;mathematical optimization;numerical method;optimization problem;trust region	Jinghui Liu;Changfeng Ma	2012	Numerical Algorithms	10.1007/s11075-012-9652-0	mathematical optimization;calculus;mathematics;trust region;line search	AI	76.13295786593339	23.76435460830271	111171
d840b2a54c6693cb0bddf74046305dbab32f023a	second-order optimality conditions for scalar and vector optimization problems in banach spaces	criterio optimalidad;calculo de variaciones;regularite;multicriteria analysis;multiobjective programming;optimizacion vectorial;programmation multiobjectif;multiplier;non linear programming;metric space;espace metrique;multipliers;regularidad;espacio metrico;espacio banach;teoria unificada;programacion no lineal;regularity;banach space;metric;90c30;programmation non lineaire;second order optimality conditions;condicion optimalidad;calcul variationnel;condition optimalite;multiplicateur;49k27;90c29;optimisation vectorielle;unified theory;vector optimization;metrico;optimality criterion;metric regularity;analisis multicriterio;analyse multicritere;critere optimalite;abstract constraints;theorie unifiee;variational calculus;metrique;espace banach;multiplicador;optimality condition;programacion multiobjetivo	In this paper we present a very general and unified theory of second-order optimality conditions for general optimization problems subject to abstract constraints in Banach spaces. Our results apply both to the scalar case and the multicriteria case. Our approach rests essentially on the use of a signed distance function for characterizing metric regularity of a certain multifunction associated with the problem. We prove variational results which show that, in a certain sense, our results are the best possible that one can obtain by using second-order analysis. We demonstrate how recently devised optimality conditions can be derived from our general framework, how they can be extended under weakened assumptions from the scalar case to the multiobjective case, and even how some new results also can be obtained.	scalar processor;spaces;vector optimization	Helmut Gfrerer	2006	SIAM J. Control and Optimization	10.1137/040612713	mathematical optimization;mathematical analysis;topology;metric;metric space;mathematics;multiplier;vector optimization;unified field theory;banach space;calculus of variations	Theory	71.91807200725363	21.261148317965297	111175
b8e7bc2fbd3ac3433788337c53cbaf594e22429a	on the role of optimality functions in numerical optimal control	optimality conditions;consistent approximations;numerical approximations;optimal control	We present a survey of optimality conditions in optimality function form and discuss their role in establishing that discretized optimal control problems are consistent approximations to the original optimal control problems.	numerical analysis;optimal control	Elijah Polak	2011	Annual Reviews in Control	10.1016/j.arcontrol.2011.10.004	mathematical optimization;optimal control;control theory;mathematics;mathematical economics	Theory	72.82404924899204	21.045393527988868	111309
5435fc6c570cf2393810b4b0a3040ff569e2c05f	on the convergence of conditional epsilon-subgradient methods for convex programs and convex-concave saddle-point problems	non linear programming;mathematics;saddle point problem;game theory;publikationer;convex programming;weighted averaging;subgradient method;konferensbidrag;topology optimization;artiklar;rapporter;linear quadratic;linear program;contact mechanics;generalized convexity;matematik;large scale optimization;saddle point	Abstract   The paper provides two contributions. First, we present new convergence results for conditional  e -subgradient algorithms for general convex programs. The results obtained here extend the classical ones by Polyak [Sov. Math. Doklady 8 (1967) 593; USSR Comput. Math. Math. Phys. 9 (1969) 14; Introduction to Optimization, Optimization Software, New York, 1987] as well as the recent ones in [Math. Program. 62 (1993) 261; Eur. J. Oper. Res. 88 (1996) 382; Math. Program. 81 (1998) 23] to a broader framework. Secondly, we establish the application of this technique to solve non-strictly convex–concave saddle point problems, such as primal-dual formulations of linear programs. Contrary to several previous solution algorithms for such problems, a saddle-point is generated by a very simple scheme in which one component is constructed by means of a conditional  e -subgradient algorithm, while the other is constructed by means of a weighted average of the (inexact) subproblem solutions generated within the subgradient method. The convergence result extends those of [Minimization Methods for Non-Differentiable Functions, Springer-Verlag, Berlin, 1985; Oper. Res. Lett. 19 (1996) 105; Math. Program. 86 (1999) 283] for Lagrangian saddle-point problems in linear and convex programming, and of [Int. J. Numer. Meth. Eng. 40 (1997) 1295] for a linear–quadratic saddle-point problem arising in topology optimization in contact mechanics.	concave function;convex optimization;machine epsilon;subgradient method	Torbjörn Larsson;Michael Patriksson;Ann-Brith Strömberg	2003	European Journal of Operational Research	10.1016/S0377-2217(02)00629-X	game theory;mathematical optimization;combinatorics;topology optimization;linear programming;subgradient method;mathematics;saddle point;mathematical economics;contact mechanics	ML	73.42906024936782	23.40493222565065	111756
bfcbd84af0b1d163ee813a11a7f480ac96e624ae	on uniform global error bounds for convex inequalities	satisfiability;convex inequality;convex function;global error bound;error bound;lower bound;conjugate function	Abstract. This paper studies the existence of a uniform global error bound when a convex inequality g < 0, where g is a closed proper convex function, is perturbed. The perturbation neighborhoods are defined by small arbitrary perturbations of the epigraph of its conjugate function. Under certain conditions, it is shown that for sufficiently small arbitrary perturbations the perturbed system is solvable and there exists a uniform global error bound if and only if g satisfies the Slater condition and the solution set is bounded or its recession function satisfies the Slater condition. The results are used to derive lower bounds on the distance to ill-posedness.	decision problem;epigraph (mathematics);proper convex function;slater's condition;social inequality;well-posed problem	Haowen Hu	2003	J. Global Optimization	10.1023/A:1021942921315	convex function;convex analysis;subderivative;support function;mathematical optimization;combinatorics;mathematical analysis;convex optimization;closed convex function;convex combination;jensen's inequality;linear matrix inequality;convex conjugate;absolutely convex set;convexity in economics;logarithmically concave function;mathematics;convex set;upper and lower bounds;slater's condition;logarithmically convex function;effective domain;proper convex function;satisfiability;concave function	ML	71.78556784064646	19.697683236822776	111881
474eed823c35afe5b3604b7e8d29066a7d72d668	extension of expansion base algorithm for multivariate analytic factorization including the case of singular leading coefficient	expansion point;bivariate polynomial;singular leading coefficient;leading coefficient;analytic factorization;multivariate analytic factorization;specific problem;expansion base algorithm	The expansion base algorithm, which was devised by Abhyankar, Kuo and McCallum is very efficient for analytic factorization of bivariate polynomials. The author had extended it to more than two variables but it was only for polynomials with non-vanishing leading coefficient at the expansion point. We improve it to be able to apply to polynomials including the case of vanishing leading coefficient, that is, singular leading coefficient, which comes to a specific problem only for more than two variables.	algorithm;bivariate data;coefficient;polynomial	Maki Iwami	2005	ACM SIGSAM Bulletin	10.1145/1140378.1140383	difference polynomials;mathematical optimization;mathematical analysis;calculus;mathematics	AI	78.38140209771504	19.136574328392093	111929
a4241522e4c244f5490b35e2484f78d6f4aaac96	systems of structured monotone inclusions: duality, algorithms, and applications	infimal convolution;monotone operator;structured minimization problem;operator splitting;parallel algorithm;primary;coupled system;monotone inclusion;convex minimization;65k05;secondary;90c25;47h05	A general primal-dual splitting algorithm for solving systems of structured coupled monotone inclusions in Hilbert spaces is introduced and its asymptotic behavior is analyzed. Each inclusion in the primal system features compositions with linear operators, parallel sums, and Lipschitzian operators. All the operators involved in this structured model are used separately in the proposed algorithm, most steps of which can be executed in parallel. This provides a flexible solution method applicable to a variety of problems beyond the reach of the state-of-the-art. Several applications are discussed to illustrate this point.	algorithm;hilbert space;monotone	Patrick L. Combettes	2013	SIAM Journal on Optimization	10.1137/130904160	mathematical optimization;combinatorics;discrete mathematics;convex optimization;mathematics;parallel algorithm	Theory	73.70170506273318	24.26931967448654	111982
d7283544582749640054d4fe221184ef9bbd7a20	deterministic global optimization using space-filling curves and multiple estimates of lipschitz and holder constants	classes of test functions;lipschitz functions;direct;deterministic numerical algorithms;space filling curves;global optimization;holder functions	In this paper, the global optimization problem miny∈S F (y) with S being a hyperinterval in R and F (y) satisfying the Lipschitz condition with an unknown Lipschitz constant is considered. It is supposed that the function F (y) can be multiextremal, non-differentiable, and given as a ‘black-box’. To attack the problem, a new global optimization algorithm based on the following two ideas is proposed and studied both theoretically and numerically. First, the new algorithm uses numerical approximations to space-filling curves to reduce the original Lipschitz multi-dimensional problem to a univariate one satisfying the Hölder condition. Second, the algorithm at each iteration applies a new geometric technique working with a number of possible Hölder constants chosen from a set of values varying from zero to infinity showing so that ideas introduced in a popular DIRECT method can be used in the Hölder global optimization. Convergence conditions of the resulting deterministic global optimization method are established. Numerical experiments carried out on several hundreds of test functions show quite a promising performance of the new algorithm in comparison with its direct competitors.	algorithm;approximation;deterministic global optimization;distribution (mathematics);experiment;iteration;mathematical optimization;numerical analysis;numerical method;optimization problem;space-filling curve	Daniela Lera;Yaroslav D. Sergeyev	2014	CoRR	10.1016/j.cnsns.2014.11.015	mathematical optimization;mathematical analysis;discrete mathematics;lipschitz domain;mathematics;lipschitz continuity;statistics;global optimization	EDA	74.83772260253015	23.92030087464886	112198
3de1d0cec205d18d78eef4fd85cf13ab80efcb69	a nonconvex separation property in banach spaces	infinite dimension;optimisation;propiedad geometrica;principio variacional;optimizacion;espacio banach;propriete geometrique;banach space;dimension infinie;sous differentiel;subdifferential;singular multiplier;separation;separacion;nonconvex separation;principe variationnel;geometrical properties;analyse non convexe;optimization;non convex analysis;key words subgradients;variational principle;espace banach;subdiferencial;normal vectors;dimension infinita;ensemble ferme;analisis no convexo	We establish, in innnite dimensional Banach space, a nonconvex separation property for general closed sets that is an extension of Hahn-Banach separation theorem. We provide some consequences in optimization, in particular the existence of singular multipliers and show the relation of our principle with the extremal principle of Mordukhovich.	boris mordukhovich;gabbay's separation theorem;mathematical optimization;singular value decomposition	Jonathan M. Borwein;Alejandro Jofré	1998	Math. Meth. of OR	10.1007/s001860050019	subderivative;mathematical optimization;mathematical analysis;topology;variational principle;mathematics;approximation property;banach space	Logic	72.7109876359214	20.507597799414928	112318
324a8dbf7c668d84a960ede3343bed14f449e3c9	a unified convergence analysis of block successive minimization methods for nonsmooth optimization	90c06;90c26;90c55;bepress selected works;successive inner approximation;block coordinate descent block successive upper bound minimization successive convex approximation successive inner approximation;block successive upper bound minimization;block coordinate descent;successive convex approximation;93e10;94a05	The block coordinate descent (BCD) method is widely used for minimizing a continuous function f of several block variables. At each iteration of this method, a single block of variables is optimized, while the remaining variables are held fixed. To ensure the convergence of the BCD method, the subproblem of each block variable needs to be solved to its unique global optimal. Unfortunately, this requirement is often too restrictive for many practical scenarios. In this paper, we study an alternative inexact BCD approach which updates the variable blocks by successively minimizing a sequence of approximations of f which are either locally tight upper bounds of f or strictly convex local approximations of f . The main contributions of this work include the characterizations of the convergence conditions for a fairly wide class of such methods, especially for the cases where the objective functions are either nondifferentiable or nonconvex. Our results unify and extend the existing convergence results for many classical algorithms such as the BCD method, the difference of convex functions (DC) method, the expectation maximization (EM) algorithm, as well as the block forward-backward splitting algorithm, all of which are popular for large scale optimization problems involving big data.	approximation;big data;binary-coded decimal;convex function;coordinate descent;expectation–maximization algorithm;iteration;mathematical optimization	Meisam Razaviyayn;Mingyi Hong;Zhi-Quan Luo	2013	SIAM Journal on Optimization	10.1137/120891009	mathematical optimization;combinatorics;discrete mathematics;mathematics	ML	74.0459164299694	25.40401653617605	112611
41ebb9e4bc925458dbd03180a0f3f9931e99e907	good lattice rules in weighted korobov spaces with general weights	producto tensorial;pire cas;optimal convergence;analisis numerico;65d30;condicion necesaria;finance;high dimensionality;fonction poids;methode noyau;produit tensoriel;algorithme cbc;methode quasi monte carlo;analyse numerique;condition suffisante;tensor product;enrejado;espace korobov;numerical analysis;necessary condition;estimation erreur;condicion suficiente;treillis;cbc algorithm;error estimation;calcul numerique;numerical computation;metodo nucleo;reproducing kernel;calculo numerico;estimacion error;lattice rules;funcion peso;quasi monte carlo;convergence optimale;algorithme polynomial;quasi monte carlo method;korobov space;kernel method;condition necessaire;weight function;sufficient condition;error bound;multivariate integration;65d32;finanzas;integration multivariable;lattice	We study the problem of multivariate integration and the construction of good lattice rules in weighted Korobov spaces with general weights. These spaces are not necessarily tensor products of spaces of univariate functions. Sufficient conditions for tractability and strong tractability of multivariate integration in such weighted function spaces are found. These conditions are also necessary if the weights are such that the reproducing kernel of the weighted Korobov space is pointwise non-negative. The existence of a lattice rule which achieves the nearly optimal convergence order is proven. A component-by-component (CBC) algorithm that constructs good lattice rules is presented. The resulting lattice rules achieve tractability or strong tractability error bounds and achieve nearly optimal convergence order for suitably decaying weights. We also study special weights such as finite-order and order-dependent weights. For these special weights, the cost of the CBC algorithm is polynomial. Numerical computations show that the lattice rules constructed by the CBC algorithm give much smaller worst-case errors than the mean worst-case errors over all quasiMonte Carlo rules or over all lattice rules, and generally smaller worst-case errors than the best Korobov lattice rules in dimensions up to hundreds. Numerical results are provided to illustrate the efficiency of CBC lattice rules and Korobov lattice rules (with suitably chosen weights), in particular for high-dimensional finance problems.	algorithm;best, worst and average case;computation;hilbert space;polynomial;rate of convergence;spaces	Josef Dick;Ian H. Sloan;Xiaoqun Wang;Henryk Wozniakowski	2006	Numerische Mathematik	10.1007/s00211-005-0674-6	quasi-monte carlo method;combinatorics;mathematical analysis;discrete mathematics;calculus;mathematics;algorithm;statistics;algebra	ML	77.72802729429975	19.192932833665974	112684
5ec089dcfbbd864bf7cbf9bc2f5500854b744505	averaging schemes for variational inequalities and systems of equations	proyeccion;nonexpansive maps;convergence;desigualdad variacional;fixed point theorem;methode plus grande pente;variational inequalities;inegalite variationnelle;fixed point problems;steepest descent method;theoreme point fixe;monotonie;teorema punto fijo;relajacion;convergencia;linearisation;linearizacion;projection;metodo mas grande inclinacion;strongly f monotone maps;monotonicity;methode moyenne;variational inequality;relaxation;linearization;averaging schemes;monotonia;system of equations;averaging method;metodo medio;systems of equations	We study averaging methods for solving variational inequalities whose underlying maps are nonexpansive and for solving systems of asymmetric equations. Our goal is to establish global convergence results using weaker assumptions than are traditional in the literature. We examine averaging schemes for relaxation algorithms and for their specialization as projection and linearization methods and as Cohen's auxiliary framework. For solving systems of equations, we consider averaging for a general class of methods that includes, as a special case, a generalized steepest descent method. We also develop a new interpretation of a norm condition typically used for establishing convergence of relaxation schemes, by associating it with a strong-f-monotonicity condition.	variational inequality;variational principle	Thomas L. Magnanti;Georgia Perakis	1997	Math. Oper. Res.	10.1287/moor.22.3.568	system of linear equations;mathematical optimization;mathematical analysis;variational inequality;method of averaging;calculus;mathematics	Theory	73.83266807075967	20.71744337810292	112792
afd9cee9364e8241d3f21dfd44353622af0285a6	quadratic regularizations in an interior-point method for primal block-angular problems	multicommodity network flows;large scale computational optimization;regularizations;preconditioned conjugate gradient;article;primal block angular problems	One of the most efficient interior-point methods for some cla sses of primal block-angular problems solves the normal equations by a com bination of Cholesky factorizations and preconditioned conjugate gradient for , respectively, the block and linking constraints. Its efficiency depends on the spectral radius—in[0,1)— of a certain matrix in the definition of the preconditioner. Spectra l radius close to 1 degrade the performance of the approach. The purpose of this work is t wofold. First, to show that a separable quadratic regularization term in the objec tive reduces the spectral radius, significantly improving the overall performance in some classes of instances. Second, to consider a regularization term which decreases w ith the barrier function, thus with no need for an extra parameter. Computational expe rience with some primal block-angular problems confirms the efficiency of the reg ularized approach. In particular, for some difficult problems, the solution time i s reduced by a factor of two to ten by the regularization term, outperforming state-ofthe art commercial solvers.	algorithm;angularjs;barrier function;cholesky decomposition;computation;conjugate gradient method;fits;hardware random number generator;interior point method;linear least squares (mathematics);matrix regularization;preconditioner;rca spectra 70;routing	Jordi Castro;Jordi Cuesta	2011	Math. Program.	10.1007/s10107-010-0341-2	mathematical optimization;combinatorics;mathematical analysis;mathematics	ML	75.55568806846755	26.131321820869104	112910
38efca148cff0f188b130fdd64ce03fac648bea8	ellipsoidal approach to box-constrained quadratic problems	global solution;quadratic program;maximum clique problem;nonconvex quadratic programming;global optimization;ellipsoidal constraints	We present a new heuristic for the global solution of box constrained quadratic problems, based on the classical results which hold for the minimization of quadratic problems with ellipsoidal constraints. The approach is tested on several problems randomly generated and on graph instances from the DIMACS challenge, medium size instances of the Maximum Clique Problem. The numerical results seem to suggest some effectiveness of the proposed approach.	quadratic programming	Pasquale L. De Angelis;Immanuel M. Bomze;Gerardo Toraldo	2004	J. Global Optimization	10.1023/B:JOGO.0000006654.34226.fe	mathematical optimization;combinatorics;discrete mathematics;quadratic residuosity problem;quadratically constrained quadratic program;mathematics;quadratic programming;global optimization	Theory	72.73668293664088	23.889158513388217	113083
4e47bdd3fe83a62b7f4d1e68ca253c5d4c1c6300	robust control via concave minimization local and global algorithms	lyapunov methods;concave programming;time varying;concave programming time varying systems robust control matrix algebra optimal control uncertain systems lyapunov methods;uncertain systems;concavity structure robust control local concave minimization global concave minimization linear fractional representation lft uncertain systems time varying parameter uncertainty linear matrix inequality lmi lyapunov variables scalings rank deficiency condition matrix inverse relation rank inequalities bilinear constraints feasible direction algorithm;time varying parameter;time varying systems;robust control;matrix algebra;matrix inversion;indexing terms;optimal control;optimization problem;linear matrix inequality;computer experiment;global optimization;uncertain system;robust control minimization methods linear matrix inequalities constraint theory constraint optimization uncertain systems uncertainty time varying systems linear programming control systems;direct method	This paper is concerned with the robust control problem of LFT (Linear Fractional Representation) uncertain systems depending on a time-varying parameter uncertainty. Our main result exploits an LMI (Linear Matrix Inequality) characterization involving scalings and Lya-punov variables subject to an additional essentially non-convex algebraic constraint. The non-convexity enters the problem in the form of a rank deeciency condition or matrix inverse relation on the scalings only. It is shown that such problems but also more generally rank inequalities and bilinear constraints can be formulated as the minimization of a concave functional subject to Linear Matrix Inequality constraints. First of all, a local Frank and Wolfe feasible direction algorithm is introduced in this context to tackle this hard optimization problem. Exploiting the attractive concavity structure of the problem, several eecient global concave programming methods are then introduced and combined with the local feasible direction method to secure and certify global optimality of the solutions. Convergence and practical implementation details of the algorithms are covered. Stopping criteria are introduced in order to reduce the overall computational overhead. Computational experiments indicate the viability of our algorithms, and that in the worst case they require the solution of a few LMI programs. Power and eeciency of the algorithms are demonstrated through realistic and randomized numerical experiments. Frank and Wolfe algorithms.	approximation algorithm;best, worst and average case;bilinear filtering;computation;concave function;convex optimization;experiment;layer four traceroute;linear matrix inequality;mathematical optimization;numerical analysis;optimization problem;overhead (computing);randomized algorithm;robust control;social inequality	Pierre Apkarian;Hoang Duong Tuan	2000	IEEE Trans. Automat. Contr.	10.1109/9.839953	direct method;robust control;optimization problem;mathematical optimization;discrete mathematics;computer experiment;index term;optimal control;linear matrix inequality;control theory;mathematics;global optimization	Theory	71.64154148215725	23.17383887214409	113403
7e4815447c6dae568fdb90f84abdf1f790ee4a8d	directional differentiability of optimal solutions under slater's condition	second order;optimal solution;optimisation;solution optimale;parametric programming;quadratic program;optimizacion;convex programming;programmation parametrique;solution point differentiability;orden 2;karush kuhn tucker;programmation convexe;satisfiability;convex parametric optimization;directional derivative;condition suffisante;condition optimalite;condicion suficiente;programacion parametrica;slater condition;solucion optima;condicion estado optimo;optimization;contingent derivative;ordre 2;sufficient condition;parametric optimization;differentiability;optimality condition;karush kuhn tucker multiplier;programacion convexa	"""For convex parametric optimization problems it is shown that the optimal solution is directionally ditIerentiable provided that a strong second-order sufficient optimality condition and Slater's condition are satisfied for the unperturbed problem. This directional derivative is equal to the optimal solution of a certain quadratic programming problem. For the construction of this quadratic problem, a preliminary choice of a """"suitable"""" KKT-multiplier is necessary, which under additional assumptions may be taken as a vertex of the set of KKT-multipliers of the unperturbed problem. In the last part of this paper, the contingent derivative of the optimal solution is investigated."""	bellman equation;computability;contingency (philosophy);convex optimization;directional derivative;linear programming relaxation;mathematical optimization;optimization problem;quadratic equation;quadratic programming;slater's condition	Stephan Dempe	1993	Math. Program.	10.1007/BF01581237	mathematical optimization;convex optimization;directional derivative;calculus;differentiable function;mathematics;mathematical economics;slater's condition;quadratic programming;karush–kuhn–tucker conditions;second-order logic;satisfiability	Theory	72.03263935892949	21.22001566572392	113457
a1763c87e7e7412f95bc814e9338a12a5216330f	a new recursive algorithm for inverting hessenberg matrices	symbolic computation;hessenberg matrix;analisis numerico;matematicas aplicadas;mathematiques appliquees;algoritmo recursivo;analyse numerique;matriz hessenberg;numerical analysis;algorithme recursif;determinante;determinant;recursive algorithm;58a25;applied mathematics;matrice hessenberg;determinants;hessenberg matrices	In the current article, the authors present a new recursive symbolic computational Hessenberg matrix algorithm, for inverting general Hessenberg matrices.	algorithm;karl hessenberg;recursion (computer science)	Mohamed Elouafi;Ahmed Driss Aiat Hadj	2009	Applied Mathematics and Computation	10.1016/j.amc.2009.04.017	combinatorics;discrete mathematics;symbolic computation;determinant;mathematics;hessenberg matrix;algebra	Theory	80.93299858137271	20.484888991096128	113489
075cf38011e32bee52351408bb616971b08e2824	a regularized robust design criterion for uncertain data	minimisation;optimisation sous contrainte;metodo cuadrado menor;constrained optimization;minimization;methode moindre carre;regularisation;game theory;filtro kalman;least squares method;cost function;incertidumbre;uncertainty;metodo minimax;filtre kalman;minimax method;90c47;teoria juego;kalman filter;minimizacion;theorie jeu;lagrange multiplier;65k10;funcion coste;regularization;optimizacion con restriccion;15a06;least squares;estimation erreur;error estimation;computational complexity;robustesse;least square;estimacion error;multiplicateur lagrange;methode minimax;multiplicador lagrange;vector optimization;fonction cout;robustness;regularizacion;game problem;incertitude;91a40;uncertain data;min max;large classes;robust design;15a63;robustez	This paper formulates and solves a robust criterion for least-squares designs in the presence of uncertain data. Compared with earlier studies, the proposed criterion incorporates simultaneously both regularization and weighting and applies to a large class of uncertainties. The solution method is based on reducing a vector optimization problem to an equivalent scalar minimization problem of a provably unimodal cost function, thus achieving considerable reduction in computational complexity.	uncertain data	Ali H. Sayed;Vitor H. Nascimento;Flávio A. M. Cipparrone	2002	SIAM J. Matrix Analysis Applications	10.1137/S0895479800380799	game theory;econometrics;mathematical optimization;constrained optimization;mathematics;vector optimization;least squares;statistics	Theory	70.58375472429466	24.885005822502134	113686
c1a1ef12c640a7e2a8b65a7691674c5e722f2552	matrix augmentation and structure preservation in linearly constrained control problems	gradient method;constrained control;simplex method;control problem;large scale;structure preservation;data structure	Matrix augmentation is used for the inversion of bases associated with large linearly constrained control problems. It is shown how an efficient data structure can be maintained by keeping all state variables in the basis, and then nullifying some of them explicitly by using additional constraints. The proposed methodology, together with a basis updating scheme based on augmentation, forms the skeleton for an in-core algorithm using either the revised simplex method or the generalized reduced gradient method.		Johannes Bisschop;Alexander Meeraus	1980	Math. Program.	10.1007/BF01588292	mathematical optimization;combinatorics;data structure;gradient method;control theory;mathematics;simplex algorithm	Theory	75.33791519552044	20.690089773964832	113779
1523ab670d1b8ffdd47185d754d1f586db503910	polynomial preconditioners for collocation matrices			collocation;polynomial;preconditioner	Stephen H. Brill	1999			matrix polynomial;mathematical optimization;matrix (mathematics);orthogonal collocation;polynomial;collocation;mathematics	Theory	82.50167358904233	19.948980062051497	113998
ac0ddadca5dbdb465edb0a20e95c90b169d69c5b	remark on algorithm 662	unconstrained minimization;major revision;quasi-newton method;additional key words and phrases: conjugate gradient;conjugate gradient;quasi newton method	There are several errors in two subroutines, MODULl and MODUL2, of the package that this Remark corrects. MODULl.	algorithm;subroutine	Burton S. Garbow;James N. Lyness	1990	ACM Trans. Math. Softw.	10.1145/98267.98302	theoretical computer science;mathematics;mathematical optimization	Graphics	82.4151480715669	19.409196720707058	114030
fea3282be9c6c8f273c8ddd7856ddd27acba6067	integrated simulation and optimization of distillation-based flowsheets		The present thesis is motivated by chemical engineering: process simulation and optimization with a focus on distillation-based flowsheets. In this context, a process consists of a number of unit operations connected by streams. Such a structure can be represented by a so-called flowsheet. In general, the process should be designed in a way that certain engineering demands are fulfilled, e.g. product purities, product yield, or maximum costs. The design of a process in steady state includes the choice of units in combination with their respective connections and also the choice of values for the specific design variables for each unit. Within the scope of this work, we assume that the layout of the flowsheet is given and we focus on choosing the design variables for each unit. The steady state behavior of a distillation-based flowsheet is modeled by default via a large system of nonlinear equations that reflect conservation and thermodynamic laws, whereas the number of degrees of freedom is small. These large nonlinear systems of equations cannot be solved analytically. Therefore, they are solved numerically with iterative methods after fixing the degrees of freedom, most often by use of Newton-type methods. Convergence failures occur quite often and, in that case, the user has to come up with new guesses for the fixed process variables until convergence is reached. This potentially turns the simulation and optimization of a flowsheet into a time-consuming procedure. In this thesis, we address the problems introduced above and present an approach that facilitates robust, flexible, and simultaneous steady state process simulation and optimization of distillation-based flowsheets. For that purpose, we use the well-understood asymptotic limiting case of distillation columns with an infinite reflux ratio and infinite number of stages, the socalled ∞/∞-model, as starting point and generate ideas that can be transferred to distillation columns with a finite reflux ratio and a finite number of stages. Such distillation columns are then calculated via upward or downward stage-to-stage calculations, where the transition from one stage to the next is formulated as a fixed-point problem. In the limiting case of infinitely large reflux ratio it is immediately possible to guarantee convergence for the corresponding fixed-point iteration. By application of the Banach fixed-point theorem we can also guarantee convergence of stage-to-stage calculations for suitable input variables in case the reflux ratio is finite. Furthermore, process simulation is embedded in an optimization problem with a typically small number of optimization variables and constraints. The input variables for stage-to-stage calculations, which serve as optimization variables, can be adapted in the outer optimization loop such that a feasible or even optimal solution is finally found. In the limiting case of distillation columns with an infinite number of stage, this procedure is similar to applying the shooting method to ordinary differential equations. Using the ∞/∞-model we can generate excellent initial guesses for the optimization variables.	column (database);embedded system;fixed-point iteration;fixed-point theorem;iterative method;mathematical optimization;newton;nonlinear system;numerical analysis;optimization problem;streams;shooting method;simulation;steady state;technological convergence;whole earth 'lectronic link	Anna Hoffmann	2017				EDA	76.4100159102174	26.79916647218558	114054
2236ad81c88e7995fee675f2d74b17c09e442f53	generalized pattern search methods for linearly equality constrained optimization problems	plan recherche;optimisation sous contrainte;search problem;constrained optimization;base positive;analisis numerico;convergence;matematicas aplicadas;algoritmo busqueda;mathematiques appliquees;constrenimiento igualdad;contrainte lineaire egalite;positive basis;algorithme recherche;search algorithm;ensemble positif;search method;optimization method;problema investigacion;metodo optimizacion;generalized pattern search;analyse numerique;optimizacion con restriccion;equality constraint;positive bases;convergencia;numerical analysis;mathematical programming;design pattern;methode optimisation;linearly constrained optimization;unconstrained optimization;optimization;optimizacion sin restriccion;applied mathematics;probleme recherche;optimisation sans contrainte;constrained optimization problem;programmation mathematique;recherche generalisee motif;linear equality constraint;programacion matematica;search design;positive spanning set;contrainte egalite	Torczon and several persons have designed pattern search methods for unconstrained optimization problem, bound constrained, and linearly constrained optimization problems. In this paper, we shall consider linear equality constrained problems as unconstrained ones using generalized pattern search algorithm, except for a feasible starting point. 2006 Elsevier Inc. All rights reserved.	constrained optimization;linear equation;linear programming;mathematical optimization;optimization problem;pattern matching;pattern search (optimization);search algorithm	Liying Liu;Xuesheng Zhang	2006	Applied Mathematics and Computation	10.1016/j.amc.2005.12.065	mathematical optimization;constrained optimization;combinatorics;cobyla;convergence;search problem;numerical analysis;calculus;mathematics;design pattern;algorithm;search algorithm	AI	75.60840943230612	22.40798827952531	114185
810bfff4c8aae7b0500c2f8819ca17d9c02814e2	the affine scaling algorithm fails for stepsize 0.999	degeneracy;convergence;affine scaling algorithm;68c25;73k40;90c05	We present two examples in which the dual affine scaling algorithm converges to a vertex that is not optimal if at each iteration we move 0.999 of the step to the boundary of the feasible region.	affine scaling;algorithm;feasible region;image scaling;iteration	Walter F. Mascarenhas	1997	SIAM Journal on Optimization	10.1137/S1052623493258404	affine space;mathematical optimization;combinatorics;discrete mathematics;convergence;affine coordinate system;affine involution;affine hull;affine transformation;mathematics;affine shape adaptation;affine combination;degeneracy	Theory	74.73387141583792	21.90395523385571	114501
e6f6a1dcdf5a7b1f704fc6d9f9c2080ede62cf40	local optimization method with global multidimensional search	the cutting angle method;optimal method;global optimization;derivative free optimization;numerical experiment;local minima;discrete gradient;lipschitz programming	This paper presents a new method for solving global optimization problems. We use a local technique based on the notion of discrete gradients for finding a cone of descent directions and then we use a global cutting angle algorithm for finding global minimum within the intersection of the cone and the feasible region. We present results of numerical experiments with well-known test problems and with the so-called cluster function. These results confirm that the proposed algorithms allows one to find a global minimizer or at least a deep local minimizer of a function with a huge amount of shallow local minima.		Adil M. Bagirov;Alexander M. Rubinov;Jiapu Zhang	2005	J. Global Optimization	10.1007/s10898-004-2700-0	mathematical optimization;combinatorics;simulated annealing;derivative-free optimization;maxima and minima;mathematics;geometry;line search;global optimization	ML	74.74412993273108	24.13240063553499	114531
80f8d2cf28c90a23695e2beb7773c7656bbc295b	a homotopy method for nonlinear inverse problems	iterative method;homotopie;analisis numerico;regularizacion tikhonov;convergence;matematicas aplicadas;algorithm performance;mathematiques appliquees;homotopia;probleme non lineaire;equation onde;regularisation tikhonov;differential equation;optimization method;punto fijo;global convergence;local convergence;noise suppression;problema inverso;homotopy;metodo optimizacion;onde acoustique;nonlinear problems;analyse numerique;homotopy method;ecuacion onda;nonlinear inverse problems;fixed point;metodo iterativo;optimization problem;wave equation;convergencia;numerical analysis;inverse problem;resultado algoritmo;methode iterative;point fixe;performance algorithme;methode optimisation;acoustic wave equation;acoustic wave;prospeccion geofisica;acoustic waves;tikhonov regularization;iteration method;applied mathematics;nonlinear inverse problem;probleme inverse;prospection geophysique;fix point;geophysical prospecting;onda acustica	We develop a homotopy method for nonlinear inverse problems, where the forward problems are governed by some forms of differential equations. A Tikhonov-style regularization approach yields an optimization problem. Ordinary iterative methods may fail to solve this problem, due to their locally convergent properties. Then the fixed-point homotopy method is introduced to solving the normal equation of the optimization problem, and a new and globally convergent algorithm is constructed, which is highly effective in the aspects of speed of computation, ability of noise suppression and wide region of convergence. As a practical application, the method is used to solve the inverse problem of 2-D acoustic wave equation. We demonstrate the merits and effectiveness of our algorithm on two realistic model problems. 2006 Elsevier Inc. All rights reserved.	acoustic cryptanalysis;algorithm;computation;iterative method;local convergence;mathematical optimization;matrix regularization;nonlinear system;numerical analysis;optimization problem;ordinary least squares;synthetic intelligence;zero suppression	H. S. Fu;B. Han	2006	Applied Mathematics and Computation	10.1016/j.amc.2006.05.139	acoustic wave;mathematical optimization;mathematical analysis;homotopy analysis method;homotopy perturbation method;calculus;mathematics;iterative method;quantum mechanics;algebra	AI	77.81813856239322	22.53213691445231	114638
17984ee75a0c9c7f4f9af1c56b692776f9458692	a conjugate duality scheme generating a new class of differentiable duals	90c30;convex optimization;conjugate duality;90c25;entropic proximal mappings;augmented lagrangians	"""We construct a mechanism to generate a large class of duality schemes for (not necessarily differentiable) convex optimization problems, for which the dual problem is continuously differentiable. We use the conjugate duality framework of Rockafellar; the original primal problem is embedded in a family of perturbed problems. The perturbation function is constructed in terms of two perturbation vectors and a single-variable function $q$. The differentiability is a consequence of the dual objective function's being a kind of ``proximal regularization,"""" but one which is expressed in terms of a {\em nonquadratic} regularizing term associated with the function $q$."""		Aharon Ben-Tal;Marc Teboulle	1996	SIAM Journal on Optimization	10.1137/S1052623493254297	perturbation function;convex analysis;mathematical optimization;mathematical analysis;convex optimization;duality;topology;fenchel's duality theorem;duality gap;weak duality;mathematics;strong duality;wolfe duality	Crypto	72.61099796677513	21.760029614098134	114847
2f0dcf212ce1ad44b1c105ef4d19778697d8b1f4	a method for minimization of quasidifferentiable functions	unconstrained minimization;quasidifferential;continuous approximation;subdifferential;objective function;convex function;numerical experiment;d c function	In this paper, we propose a new method for the unconstrained minimization of a function presented as a difference of two convex functions. This method is based on continuous approximations to the Demyanov-Rubinov quasidifferential. First, a terminating algorithm for the computation of a descent direction of the objective function is described. Then we present a minimization algorithm and study its convergence. An implementable version of this algorithm is discussed. Finally, we report the results of preliminary numerical experiments.	quasi-derivative	Adil M. Bagirov	2002	Optimization Methods and Software	10.1080/10556780290027837	convex function;subderivative;mathematical optimization;mathematical analysis;calculus;mathematics	EDA	74.53459678673	23.571251630454807	114851
5f422472afeca12202138a983c233bfc71e90223	a polak-ribière-polyak method for solving large-scale nonlinear systems of equations and its global convergence	global convergence;nonlinear systems of equations;nonmonotone line search;polak ribiere polyak method	A derivative-free conjugate gradient type method for solving large-scale nonlinear systems of equations is presented. In the iterative method, the search direction is based on the Polak–Ribière–Polyak (PRP) conjugate gradient method, and the steplength is determined by a suitable nonmonotone line search. Under appropriate conditions, the global convergence of the proposed method is established. The method is suitable to large-scale problems for the lower storage requirement. It is shown from the numerical results that the method is practically effective. 2014 Elsevier Inc. All rights reserved.	conjugate gradient method;iterative method;line search;local convergence;nonlinear system;numerical analysis;parallel redundancy protocol	Min Li	2014	Applied Mathematics and Computation	10.1016/j.amc.2014.09.112	local convergence;mathematical optimization;mathematical analysis;conjugate residual method;calculus;derivation of the conjugate gradient method;mathematics;iterative method;conjugate gradient method;nonlinear conjugate gradient method	AI	82.7681640947589	18.466878517515163	114934
a7c745eb01d2f500391f8688a01e65a7b29254c6	levitin-polyak well-posedness of generalized vector quasi-equilibrium problems with functional constraints	approximating solution sequence;90c30;functional constraints;journal;well posedness;set valued map;equilibrium problem;90c29;generalized vector quasi equilibrium problems;期刊论文;generalized vector equilibrium problems;levitin polyak well posedness;gap function	In this paper, we introduce several types of Levitin-Polyak well-posedness for a generalized vector quasi-equilibrium problem with functional constraints and abstract set constraints. Criteria and characterizations of these types of Levitin-Polyak well-posedness with or without gap functions of generalized vector quasi-equilibrium problem are given. The results in this paper unify, generalize and extend some known results in the literature.	well-posed problem	Jian-Wen Peng;Soon-Yi Wu;Yan Wang	2012	J. Global Optimization	10.1007/s10898-011-9711-4	mathematical optimization;combinatorics;mathematics;mathematical economics	PL	71.98899628142074	21.151430071187125	114938
4b8f90530b79db80780120033694cbd586990c73	formulation and numerical solution of nash equilibrium multiobjective elliptic control problems	49j20;nash equilibrium;semismooth newton method;90c30;65n06;global convergence;65k10;optimal control problems;quadratic convergence;multiobjective optimization;elliptic partial differential equations	The formulation and the semismooth Newton solution of Nash equilibria multiobjective elliptic optimal control problems are presented. Existence and uniqueness of a Nash equilibrium is proved. The corresponding solution is characterized by an optimality system that is approximated by second-order finite differences and solved with a semismooth Newton scheme. It is demonstrated that the numerical solution is second-order accurate and that the semismooth Newton iteration is globally and locally quadratically convergent. Results of numerical experiments confirm the theoretical estimates and show the effectiveness of the proposed computational framework.	approximation algorithm;computational complexity theory;control system;experiment;finite difference;iteration;nash equilibrium;newton;newton's method;numerical analysis;numerical partial differential equations;optimal control;rate of convergence	Alfio Borzì;Christian Kanzow	2013	SIAM J. Control and Optimization	10.1137/120864921	mathematical optimization;mathematical analysis;multi-objective optimization;mathematics;mathematical economics;rate of convergence;elliptic partial differential equation;nash equilibrium	ML	73.96857396595794	21.92653905204703	115152
54036a5df6fd78d656a8dceb2dd6d01e8344a5a3	hybrid extragradient-like methods for generalized mixed equilibrium problems, systems of generalized equilibrium problems and optimization problems	strong convergence;general equilibrium;generalized mixed equilibrium problem;iterative algorithm;optimization problems;equilibrium problem;fixed point;optimization problem;hybrid extragradient like iterative scheme;system of generalized equilibrium problems;nonexpansive mapping;fixed points;nonexpansive mappings	In this paper, we introduce and analyze a new hybrid extragradient-like iterative algorithm for finding a common solution of a generalized mixed equilibrium problem, a system of generalized equilibrium problems and a fixed point problem of infinitely many non expansive mappings. Under some mild conditions, we prove the strong convergence of the sequence generated by the proposed algorithm to a common solution of these three problems. Such solution also solves an optimization problem. Several special cases are also discussed. The results presented in this paper are the supplement, extension, improvement and generalization of the previously known results in this area.	mathematical optimization;optimization problem	Lu-Chuan Ceng;Qamrul Hasan Ansari;Siegfried Schaible	2012	J. Global Optimization	10.1007/s10898-011-9703-4	optimization problem;mathematical optimization;mathematical analysis;general equilibrium theory;mathematics;fixed point;mathematical economics	Theory	73.17014604751395	22.546982280494408	115173
9181afc61bd6ae115c5d27d5b8d528cf33641c26	convergence properties of the dependent prp conjugate gradient methods	line search;global convergence;conjugate gradient method;conjugate gradient;shujun lian changyu wang lixia cao 共轭梯度 收敛 线搜索 prp convergence properties of the dependent prp conjugate gradient methods	In this paper, a new region of βk with respect to βkPRP is given. With two Armijo-type line searches, the authors investigate the global convergence properties of the dependent PRP conjugate gradient methods, which extend the global convergence results of PRP conjugate gradient method proved by Grippo and Lucidi (1997) and Dai and Yuan (2002).	conjugate gradient method;parallel redundancy protocol	Shujun Lian;Changyu Wang;Lixia Cao	2006	J. Systems Science & Complexity	10.1007/s11424-006-0288-9	mathematical optimization;mathematical analysis;calculus;mathematics;conjugate gradient method	Theory	77.4803854209083	22.933091744899937	115284
51b87895afbb75b44d7b612140c6bd2a27c121bf	local linear convergence of forward-backward under partial smoothness		In this paper, we consider the Forward–Backward proximal splitting algorithm to minimize the sum of two proper closed convex functions, one of which having a Lipschitz continuous gradient and the other being partly smooth relative to an active manifoldM. We propose a generic framework under which we show that the Forward–Backward (i) correctly identifies the active manifold M in a finite number of iterations, and then (ii) enters a local linear convergence regime that we characterize precisely. This gives a grounded and unified explanation to the typical behaviour that has been observed numerically for many problems encompassed in our framework, including the Lasso, the group Lasso, the fused Lasso and the nuclear norm regularization to name a few. These results may have numerous applications including in signal/image processing processing, sparse recovery and machine learning.	algorithm;compressed sensing;convex function;gradient;image processing;iteration;lasso;machine learning;numerical analysis;rate of convergence;sparse matrix	Jingwei Liang;Mohamed-Jalal Fadili;Gabriel Peyré	2014				ML	72.90694185612394	27.449432508416926	115353
df4a6c0b62697fb1126bb103ef8730edd5c49358	discussion on the existence and uniqueness of solution to nonlinear integro-differential systems	nonlinear integro differential systems;m accretive mapping;maximal monotone operator;strongly accretive mapping	We present an abstract result for the existence and uniqueness of the solution of nonlinear integro-differential systems involving the generalized ( p , q ) -Laplacian. The method used involves result on surjection of the sums of ranges of m -accretive mappings and strongly accretive mappings. The systems and technique discussed in this paper extend and complement some of the previous work.	nonlinear system	Li Wei;Ravi P. Agarwal;Patricia J. Y. Wong	2015	Computers & Mathematics with Applications	10.1016/j.camwa.2014.12.007	mathematical optimization;mathematical analysis;discrete mathematics;mathematics	Theory	72.94313537883284	18.268436469506195	115519
66f9eae23009f0be8a07046aac30772592b56ee8	clustering using max-norm constrained optimization		We suggest using the max-norm as a convex surrogate constraint for clustering. We show how this yields a better exact cluster recovery guarantee than previously suggested nuclearnorm relaxation, and study the effectiveness of our method, and other related convex relaxations, compared to other approaches.	cluster analysis;constrained optimization;lagrangian relaxation;linear programming relaxation	Ali Jalali;Nathan Srebro	2012	CoRR		correlation clustering;constrained clustering;mathematical optimization;combinatorics;cure data clustering algorithm;mathematics;mathematical economics	ML	73.89828793788433	24.37588905434003	115586
5e7f38f5ff2e41c3192b35492062004c367fa0ec	aggregated momentum: stability through passive damping		Momentum is a simple and widely used trick which allows gradient-based optimizers to pick up speed along low curvature directions. Its performance depends crucially on a damping coefficient β. Large β values can potentially deliver much larger speedups, but are prone to oscillations and instability; hence one typically resorts to small values such as 0.5 or 0.9. We propose Aggregated Momentum (AggMo), a variant of momentum which combines multiple velocity vectors with different β parameters. AggMo is trivial to implement, but significantly dampens oscillations, enabling it to remain stable even for aggressive β values such as 0.999. We reinterpret Nesterov’s accelerated gradient descent as a special case of AggMo and analyze rates of convergence for quadratic objectives. Empirically, we find that AggMo is a suitable drop-in replacement for other momentum methods, and frequently delivers faster convergence. In spite of a wide range of modern optimization research, gradient descent with momentum and its variants remain the tool of choice in machine learning. Momentum methods can help the optimizer pick up speed along low curvature directions without becoming unstable in high-curvature directions. The simplest of these methods, classical momentum [23], has an associated damping coefficient, 0 ≤ β < 1, which controls how quickly the momentum vector decays. The choice of β imposes a tradoff between speed and stability: in directions where the gradient is small but consistent, the terminal velocity is proportional to 1/(1 − β), suggesting that β slightly less than 1 could deliver much improved optimization performance. However, large β values are prone to oscillations and instability [21, 4], requiring a smaller learning rate and hence slower convergence. Finding a way to dampen the oscillations while preserving the high terminal velocity of large beta values could dramatically speed up optimization. Sutskever et al. [28] found that Nesterov accelerated gradient descent [19], which they reinterpreted as a momentum method, was more stable than classical momentum for large β values and gave substantial speedups for training neural networks. However, the reasons for the improved performance remain somewhat mysterious. O’Donoghue and Candes [21] proposed to detect oscillations and eliminate them by resetting the velocity vector to zero. But in practice it is difficult to determine an appropriate restart condition. In this work, we introduce Aggregated Momentum (AggMo), a variant of classical momentum which maintains several velocity vectors with different β parameters. AggMo averages the velocity vectors when updating the parameters. We find that this combines the advantages of both small and large β values: the large values allow significant buildup of velocity along low curvature directions, while the small values dampen the oscillations, hence stabilizing the algorithm. AggMo is trivial to implement and incurs almost no computational overhead. We draw inspiration from the physics literature when we refer to our method as a form of passive damping. Resonance occurs when a system is driven at specific frequencies but may be prevented through careful design [5]. Passive damping can address this in structures by making use of different materials with unique resonant frequencies. This prevents any single frequency from producing catastrophic resonance. By combining several momentum velocities together we achieve a similar effect — no single frequency is driving the system and so oscillation is prevented. ar X iv :1 80 4. 00 32 5v 2 [ cs .L G ] 2 3 Ju l 2 01 8 In this paper we analyze rates of convergence on quadratic functions. We also provide theoretical convergence analysis showing that AggMo achieves converging average regret in online convex programming [36]. To evaluate AggMo empirically we compare against other commonly used optimizers on a range of deep learning architectures: deep autoencoders, convolutional networks, and long-term short-term memory (LSTM). In all of these cases, we find that AggMo works as a drop-in replacement for classical momentum, in the sense that it works at least as well for a given β parameter. But due to its stability at higher β values, it often delivers substantially faster convergence than both classical and Nesterov momentum when its maximum β value is tuned. 2 Background: momentum-based optimization Classical momentum We consider a function f : R → R to be minimized with respect to some variable θ. Classical momentum (CM) minimizes this function by taking some initial point θ0 and running the following iterative scheme, vt = βvt−1 −∇θf(θt−1) θt = θt−1 + γtvt (1) where γt denotes a learning rate schedule, β is the damping coefficient and we set v0 = 0. Momentum can speed up convergence but it is often difficult to choose the right damping coefficient, β. Even with momentum, progress in a low curvature direction may be very slow. If the damping coefficient is increased to overcome this then high curvature directions may cause instability and oscillations. Nesterov momentum Nesterov’s Accelerated Gradient [19, 20] is a modified version of the gradient descent algorithm with improved convergence and stability. It can be written as a momentum-based method [28], vt = βvt−1 −∇θf(θt−1 + γt−1βvt−1) θt = θt−1 + γtvt (2) Nesterov momentum seeks to solve stability issues by correcting the error made after moving in the direction of the velocity, v. In fact, it can be shown that for a quadratic function Nesterov momentum adapts to the curvature by effectively rescaling the damping coefficients by the eigenvalues of the quadratic [28]. Quadratic convergence We begin by studying convergence on quadratic functions, which have been an important test case for analyzing convergence behavior [28, 21, 4], and which can be considered a proxy for optimization behavior near a local minimum [21]. We analyze the behavior of these optimizers along the eigenvectors of a quadratic function in Figure 1. In (a) we use a low damping coefficient (β = 0.9) while (b) shows a high damping coefficient (β = 0.999). When using a low damping coefficient it takes many iterations to find the optimal solution. On the other hand, increasing the damping coefficient from 0.9 to 0.999 causes oscillations which prevent convergence. When using CM in practice we seek the critical damping coefficient which allows us to rapidly approach the optimum without becoming unstable [4]. On the other hand, Nesterov momentum with β = 0.999 is able to converge more quickly within high curvature regions than CM but retains oscillations for the quadratics exhibiting lower curvature. Convergence vs. condition number Figure 3 displays the convergence rate of each optimizer for quadratics with condition numbers (κ) from 10 to 10. The blue dashed line displays the optimal convergence rate achievable by CM with knowledge of the condition number — an unrealistic scenario in practice. The two curves corresponding to CM (red and purple) each meet the optimal convergence rate when the condition number is such that β is critical. On the left of this critical point, where the convergence rates for CM are flat, the system is ”under-damped” — it is in this setting that we observe oscillations during optimization. Nesterov momentum (green) is able to provide improved convergence in the under-damped regime because it dampens the oscillations. Details on the computation of these convergence rates are presented in Appendix A.		James Lucas;Richard S. Zemel;Roger Grosse	2018	CoRR		applied mathematics;mathematical optimization;curvature;convex optimization;special case;mathematics;oscillation;gradient descent;instability;convergence (routing);momentum	ML	77.89345393909235	26.201301732130275	115589
b20c2d54ae6decfbb4326a6e5f8db52dd80e4253	interior proximal method for variational inequalities: case of non-paramonotone operators	maximal monotone operators;regularization;bregman function;proximal point methods;variational inequalities	For variational inequalities characterizing saddle points of Lagragians associated with convex programming problems in Hilbert spaces, the convergence of an interior proximal method based on Bregman distance functionals is studied. The convergence results admit a successive approximation of the variational inequality and an inexact treatment of the proximal iterations.	approximation;bregman divergence;calculus of variations;convex optimization;hilbert space;iteration;social inequality;variational inequality;variational principle	Alexander Kaplan;Rainer Tichatschke	2001	Universität Trier, Mathematik/Informatik, Forschungsbericht			ML	72.63221044482515	21.34484308898269	115615
97af3182ff8636b2987989439f64b7ec51219386	the hermitian reflexive solutions to the matrix inverse problem ax=b	analisis numerico;matematicas aplicadas;matrice h;mathematiques appliquees;aproximacion optima;condition necessaire suffisante;matrix inversion;problema inverso;analyse numerique;numerical analysis;optimal approximation;approximation optimale;inverse problem;hermitian reflexive matrix;necessary and sufficient condition;matrice hermitienne;aproximacion matriz;matrix optimal approximation problem;matrix approximation;15a57;matriz h;matriz hermitiana;applied mathematics;matrix inverse problem;probleme inverse;hermitian matrix;frobenius norm;condicion necesaria suficiente;approximation matricielle;h matrix	In this paper, the necessary and sufficient conditions for the solvability of the matrix inverse problem AX 1⁄4 B in Hermitian reflexive matrix set HJ n are given, and the solution expression of this matrix inverse problem is also presented. Moreover, an optimal approximation to any matrix in the above solution set in Frobenius norm is provided. 2009 Elsevier Inc. All rights reserved.	approximation;heterojunction;matrix multiplication;microsoft dynamics ax;the matrix	Weiwei Xu;Wen Li	2009	Applied Mathematics and Computation	10.1016/j.amc.2009.01.034	hermitian matrix;matrix function;combinatorics;mathematical analysis;eigendecomposition of a matrix;applied mathematics;nonnegative matrix;numerical analysis;single-entry matrix;centrosymmetric matrix;inverse problem;matrix norm;calculus;invertible matrix;hamiltonian matrix;square matrix;mathematics;positive-definite matrix;pascal matrix;state-transition matrix;matrix exponential;algorithm;transpose;symmetric matrix;algebra;involutory matrix	Theory	80.03957744687105	20.489759639106847	115705
290425042a075230894f45042fdd87c7ce1f6561	some extremal bounds for subclasses of univalent functions	starlike function;analisis numerico;matematicas aplicadas;uniformly convex;fonction etoilee;mathematiques appliquees;uniformly convex function;starlike functions;conic domain;k uniformly convex functions;analyse numerique;convex functions;numerical analysis;convex function;univalent function;borne inferieure;applied mathematics;fonction convexe;lower bound;funcion convexa;cota inferior	In this paper we obtain the sharp lower bound for Ref^'(z),z@?U, for functions f that are k-uniformly convex in the unit disk U. Next we consider the problem of finding the minimum of Ref^'(z) for functions f that are k-uniformly convex in the disk of radius r. Corresponding results for the class of starlike functions related to the class of k-uniformly convex functions are presented.		Agnieszka Wisniowska-Wajnryb	2009	Applied Mathematics and Computation	10.1016/j.amc.2009.09.001	convex function;convex analysis;subderivative;mathematical optimization;combinatorics;mathematical analysis;convex optimization;convex combination;mathematics;proper convex function	ML	72.22773935906304	18.28028564739068	115918
d846cfd251b49306a62c8042a4d0f883c344e80b	a reliable treatment of singular emden-fowler initial value problems and boundary value problems	equation derivee partielle;sistema lineal;iterative method;calculo de variaciones;emden fowler problems;problema valor limite;partial differential equation;ecuacion derivada parcial;analisis numerico;valor singular;convergence;matematicas aplicadas;mathematiques appliquees;boundary value problem;singular value;65n99;65kxx;optimization method;etude methode;lagrange multiplier;estudio metodo;65k10;metodo optimizacion;linear system;analyse numerique;initial value problem;metodo iterativo;calcul variationnel;convergencia;iteraccion;numerical analysis;mathematical programming;valeur singuliere;methode iterative;65f08;multiplicateur lagrange;49r50;algebra lineal numerica;65nxx;algebre lineaire numerique;methode optimisation;iteration;multiplicador lagrange;variational iteration method;problema valor inicial;numerical linear algebra;method study;probleme valeur initiale;systeme lineaire;iteration method;applied mathematics;probleme valeur limite;programmation mathematique;programacion matematica;65f10;variational calculus;boundary singularity;65m99;35e15;65mxx	In this paper, the variational iteration method (VIM) is used to study the singular Emden– Fowler initial value problems and boundary value problems arising in physics and astrophysics. The VIM overcomes the singularity at the origin. The Lagrange multipliers for all cases of the equations are determined. The work is supported by analyzing few initial value problems and boundary value problems where the convergence of the results is emphasized. 2011 Elsevier Inc. All rights reserved.	iteration;lagrange multiplier;singularity project;technological singularity;variational principle	Abdul-Majid Wazwaz	2011	Applied Mathematics and Computation	10.1016/j.amc.2011.04.084	mathematical optimization;boundary value problem;calculus;mathematics;geometry;iterative method;algebra	AI	81.90393901459854	19.13386326827861	115924
bea38d8f36ef9935d54b52278a4dcd189173ea4b	optimization problems in contracted tensor networks		We discuss the calculus of variations in tensor representat io s with a special focus on tensor networks and apply it to functionals of practical interest. The surve y provides all necessary ingredients for applying minimization methods in a general setting. The important ca ses of target functionals which are linear and quadratic with respect to the tensor product are discuss ed, and combinations of these functionals are presented in detail. As an example, we consider the represen tation rank compression in tensor networks. For the numerical treatment, we use the nonlinear block Gaus s-Seidel method. We demonstrate the rate of convergence in numerical tests.	calculus of variations;nonlinear system;numerical analysis;rate of convergence	Michael Espig;Wolfgang Hackbusch;Stefan Handschuh;Reinhold Schneider	2011	Computat. and Visualiz. in Science	10.1007/s00791-012-0183-y	tensor product;mathematical optimization;combinatorics;mathematical analysis;tensor;cartesian tensor;tensor;mathematics;geometry;tensor product network;algebra	ML	73.93129143937179	21.887667045680278	116269
a80101ac2e515831f137fa9f8634f9c54e298cee	new approaches for solving large sylvester equations	gmres;analisis numerico;escuacion sylvester;sylvester equation;matematicas aplicadas;mathematiques appliquees;equation matricielle;analyse numerique;equation sylvester;fom;algorithme arnoldi;numerical analysis;matrix equation;algorithme gmres;sylvester matrix equation;ecuacion matricial;applied mathematics;modified global arnoldi;sylvester equations	In this paper, we propose two new algorithms based on modified global Arnoldi algorithm for solving large Sylvester matrix equations AX + XB = C where A ∈ Rn×n, B ∈ Rs×s, X and C ∈ Rn×s. These algorithms are based on the global FOM and GMRES algorithms and we call them by Global FOM-SylvesterLike(GFSL) and Global GMRES-Sylvester-Like(GGSL) algorithms, respectively. Some theoretical results and numerical examples are alsogiven. AMS Subject Classification : 65F10, 65F50.	algorithm;arnoldi iteration;broadcast auxiliary service;generalized minimal residual method;numerical analysis;sylvester matrix;vhdl-ams	Davod Khojasteh Salkuyeh;Faezeh Toutounian	2006	Applied Mathematics and Computation	10.1016/j.amc.2005.02.063	mathematical optimization;mathematical analysis;numerical analysis;calculus;generalized minimal residual method;sylvester equation;mathematics;matrix;algebra	ML	82.05534123777944	19.117346792022612	116293
7a46476ca16ce73a5ea30611332102e878bce9e9	polynomial approximations for infinite-dimensional optimization problems			approximation;infinite-dimensional optimization;mathematical optimization;polynomial	Dimitra Bampou	2013			polynomial function theorems for zeros;wilkinson's polynomial;matrix polynomial;l-reduction;square-free polynomial;polynomial	Theory	71.4374535659161	23.56848023070221	116378
f015163edcccb430027d0559ec2e65b3cd600899	algebraic approach to the interval linear static identification, tolerance, and control problems, or one more application of kaucher arithmetic	interval arithmetic;linear system;newton method;interval estimation;numerical method;linear equations	In this paper, theidentification problem, thetolerance problem, and thecontrol problem are treated for the interval linear equation Ax=b. These problems require computing an inner approximation of theunited solution set Σ∃∃(A, b)={x ∈ ℝn | (∃A ∈ A)(Ax ∈ b)}, of thetolerable solution set Σ∀∃(A, b)={x ∈ ℝn | (∀A ∈ A)(Ax ∈ b)}, and of thecontrollable solution set Σ∃∀(A, b)={x ∈ ℝn | (∀b ∈ b)(Ax ∈b)} respectively. Analgebraic approach to their solution is developed in which the initial problem is replaced by that of finding analgebraic solution of some auxiliary interval linear system in Kaucher extended interval arithmetic. The algebraic approach is proved almost always to give inclusion-maximal inner interval estimates of the solutionsets considered. We investigate basic properties of the algebraic solutions to the interval linear systems and propose a number of numerical methods to compute them. In particular, we present the simple and fastsubdifferential Newton method, prove its convergence and discuss numerical experiments.		Sergey P. Shary	1996	Reliable Computing	10.1007/BF02388185	mathematical optimization;mathematical analysis;discrete mathematics;mathematics;algebra	Theory	76.73982724600975	19.28706303929119	116582
1ec4c31f2ec8b486519f5e371d2543dd21271d09	the applications of the fan-browder fixed point theorem in topological ordered spaces	δ convex mapping;equilibrio nash;topological space;matematicas aplicadas;fixed point theorem;espacio ordenado;nash equilibrium;mathematiques appliquees;application δ convexe;point equilibre;equilibrio juego;solution generalisee;open lower section;generalized solution;equilibrium point;theoreme point fixe;teorema punto fijo;punto equilibrio;equilibre nash;nash equilibrium point;generalized quasi ky fan inequality;quasi ky fan inequality;espace ordonne;ensemble convexe;inegalite quasi ky fan;ordered space;espace topologique;equilibre jeu;convex set;game equilibrium;applied mathematics;δ convex set;existence;conjunto convexo;section inferieure ouverte;espacio topologico	Abstract   In this work, for topological ordered spaces, by using the Fan–Browder fixed point theorem, we obtain existence results for solutions for some generalized quasi-Ky Fan inequalities and Nash equilibrium points for a game system.	fixed point (mathematics);fixed-point theorem	Qingyang Luo	2006	Appl. Math. Lett.	10.1016/j.aml.2006.01.016	equilibrium point;closed graph theorem;mathematical optimization;topological vector space;bourbaki–witt theorem;brouwer fixed-point theorem;kakutani fixed-point theorem;topology;schauder fixed point theorem;topological tensor product;isolated point;calculus;mathematics;fixed point;convex set;picard–lindelöf theorem;open mapping theorem;fixed-point theorem;topological space;compact space;locally convex topological vector space;fréchet space;nash equilibrium;fixed-point property;homeomorphism	Theory	71.37118723364468	19.322583498344112	116710
00f5e952d849b917c275c2dc240ded9727ea24b0	constrained evolution for a quasilinear parabolic equation	35k59;35k20;34h05;management science and operations research;control and optimization;93b52;quasilinear parabolic equation;convex sets;applied mathematics;80m50;feedback control;monotone nonlinearities	In the present contribution, a feedback control law is studied for a quasilinear parabolic equation. First, we prove the well-posedness and some regularity results for the Cauchy---Neumann problem for this equation, modified by adding an extra term which is a multiple of the subdifferential of the distance function from a closed convex set of the space of square-integrable functions. Then, we consider convex sets of obstacle or double-obstacle type and prove rigorously the following property: if the factor in front of the feedback control is sufficiently large, then the solution reaches the convex set within a finite time and then moves inside it.	bellman equation;parabolic antenna	Pierluigi Colli;Gianni Gilardi;Jürgen Sprekels	2016	J. Optimization Theory and Applications	10.1007/s10957-016-0970-6	convex analysis;subderivative;support function;mathematical optimization;mathematical analysis;discrete mathematics;convex optimization;closed convex function;convex combination;linear matrix inequality;convex hull;absolutely convex set;convexity in economics;feedback;mathematics;convex set;logarithmically convex function;proper convex function	Theory	71.96488734280233	20.0431679191284	116835
971abb4742dc192652662555bd86abda9d37d00b	on the curse of dimensionality in the ritz method	ritz method;90c06;90c26;curse of dimensionality;qa75 electronic computers computer science;extended ritz method;approximation schemes;infinite dimensional optimization;90c48	It is shown that the classical Ritz method of the calculus of variations suffers from the “curse of dimensionality,” i.e., an exponential growth, as a function of the number of variables, of the dimension a linear subspace needs in order to achieve a desired relative improvement in the accuracy of approximation of the optimal solution value. The proof is constructive and is obtained by exhibiting a family of infinite-dimensional optimization problems for which this happens, namely those with quadratic functional and spherical constraint. The results provide a theoretical motivation for the search of alternative solution methods, such as the so-called “extended Ritz method,” to deal with the curse of dimensionality.	approximation algorithm;calculus of variations;curse of dimensionality;final fantasy tactics advance;graph coloring;infinite-dimensional optimization;mathematical optimization;time complexity	Giorgio Gnecco	2016	J. Optimization Theory and Applications	10.1007/s10957-015-0804-y	mathematical optimization;combinatorics;mathematical analysis;curse of dimensionality;ritz method;mathematics	ML	73.63373712271427	23.141773673568775	116923
bfcd97f24ff8eeb2fd2bad5ec462a9d7092056e1	extension of wolfe method for solving quadratic programming with interval coefficients		Quadratic programming with interval coefficients developed to overcome cases in classic quadratic programming where the coefficient value is unknown and must be estimated. This paper discusses the extension of Wolfe method. The extended Wolfe method can be used to solve quadratic programming with interval coefficients.The extension process ofWolfe method involves the transformation of the quadratic programming with interval coefficients model into linear programming with interval coefficients model. The next step is transforming linear programming with interval coefficients model into two classic linear programming models with special characteristics, namely, the optimum best and the worst optimum problem.		Syaripuddin;Herry Suprajitno;Fatmawati Fatmawati	2017	J. Applied Mathematics	10.1155/2017/9037857	mathematical optimization;quadratic function;quadratic programming;mathematics;second-order cone programming;wolfe conditions;linear programming;sequential quadratic programming	AI	70.99981113201052	23.02556169383529	117075
aa11f0df49336cc71dcea5e3d7947f50a6b75c78	a multidimensional tropical optimization problem with nonlinear objective function and linear constraints	non linear objective function;linear inequality constraints;65k10;multidimensional optimization problem;15a80;90b35;project scheduling;90c48;idempotent semifield	We examine a multidimensional optimisation problem in the tropical mathematics setting. The problem involves the minimisation of a nonlinear function defined on a finite-dimensional semimodule over an idempotent semifield subject to linear inequality constraints. We start with an overview of known tropical optimisation problems with linear and nonlinear objective functions. A short introduction to tropical algebra is provided to offer a formal framework for solving the problem under study. As a preliminary result, a solution to a linear inequality with an arbitrary matrix is presented. We describe an example optimisation problem drawn from project scheduling and then offer a general representation of the problem. To solve the problem, we introduce an additional variable and reduce the problem to the solving of a linear inequality, in which the variable plays the role of a parameter. A necessary and sufficient condition for the inequality to hold is used to evaluate the parameter, whereas the solution to the inequality is considered a solution to the problem. Based on this approach, a complete direct solution in a compact vector form is derived for the optimisation problem under fairly general conditions. Numerical and graphical examples for two-dimensional problems are given to illustrate the obtained results. Key-Words: idempotent semifield, multidimensional optimisation problem, nonlinear objective function, linear inequality constraints, project scheduling. MSC (2010): 65K10, 15A80, 90C48, 90B35 Faculty of Mathematics and Mechanics, Saint Petersburg State University, 28 Universitetsky Ave., Saint Petersburg, 198504, Russia, nkk@math.spbu.ru. The work was supported in part by the Russian Foundation for Humanities under Grant #13-02-00338.	graphical user interface;idempotence;irreducibility;linear inequality;linear programming;mathematical optimization;nonlinear system;numerical analysis;numerical method;optimization problem;scheduling (computing);social inequality	Nikolai Krivulin	2013	CoRR	10.1080/02331934.2013.840624	optimization problem;mathematical optimization;combinatorics;discrete mathematics;linear matrix inequality;nonlinear programming;cutting stock problem;mathematics;schedule	AI	70.2754872189479	22.731329717499964	117296
a01fc8fc97894fa25b141924e1ac7d12efcf678b	analytical solution methods for the fuzzy weighted average	α cut method;fuzzy weighted average fwa;analytical solution;期刊论文;karnik mendel km algorithm	For the fuzzy weighted average (FWA), despite various discrete solution algorithms and their improvements, attempts at analytical solutions are very rare. This paper provides an analytical solution method for the FWA based on the conclusions of the Karnik-Mendel (KM) algorithm. Compared with the two current popular kinds of @a-cut based computational methods for the FWA (mathematical programming transformations and direct iterate computations), our method is precise, and, has a concise structure, efficient computation process, and sound theoretical proofs. We propose two algorithms for computing the analytical solution of the FWA. Two numerical examples illustrate our proposed approach.		Xinwang Liu;Jerry M. Mendel;Dongrui Wu	2012	Inf. Sci.	10.1016/j.ins.2011.10.006	closed-form expression;mathematical optimization;combinatorics;machine learning;mathematics;algorithm;statistics	AI	80.92759947492954	27.8124582213697	118119
34be96ecd433adb414c8eb7a58ada670b1e58e98	first-order perturbation analysis of secsi with generalized unfoldings		Tensor decompositions are regarded as a powerful tool for multidimensional signal processing. In this contribution, we focus on the well-known Canonical Polyadic (CP) decomposition and present a first-order perturbation analysis of the SEmi-algebraic framework for approximate CP decompositions via SImultaneous matrix diagonalization with Generalized Unfoldings (SECSI-GU), which is advantageous for tensors of an order higher than three. Numerical results indicate that the analytical relative Mean Square Factor Error (rMSFE) of the estimated factor matrices resulting from each generalized unfolding considered in SECSI -GU matches the empirical rMSFE very well. As SECSI -GU considers all possible partitionings of the tensor modes resulting in a large number of candidate factor matrix estimates, an exhaustive search-based criterion to select the final factor matrix estimates leads to a prohibitive computational complexity. The accurate performance prediction achieved by the first-order perturbation analysis conducted in this paper will significantly facilitate the selection of the final factor matrix estimates in an efficient manner and will therefore contribute to a low-complexity enhancement of SECSI-GU.	approximation algorithm;brute-force search;computational complexity theory;first-order predicate;first-order reduction;linear algebra;mean squared error;multidimensional signal processing;numerical method;performance prediction;perturbation theory;unfolding (dsp implementation);whole earth 'lectronic link	Yao Cheng;Sher Ali Cheema;Martin Haardt;Amir Weiss;Arie Yeredor	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8462287	tensor;multidimensional signal processing;mathematical optimization;diagonalizable matrix;signal processing;brute-force search;perturbation theory;matrix decomposition;matrix (mathematics);computer science	ML	75.27396394921045	28.985337688331747	118393
a37335f6014be639a10d86741af7b21dc6177772	nondifferentiable multiobjective programming under generalized d	multiobjective programming;pareto efficiency;nonlinear programming;inequality constraint;karush kuhn tucker;generalized convexity;optimality condition	In this paper, we are concerned with a nondifferentiable multiobjective programming problem with inequality constraints. We introduce four new classes of generalized convex functions by combining the concepts of weak strictly pseudoinvex, strong pseudoinvex, weak quasi invex, weak pseudoinvex and strong quasi invex functions in Aghezzaf and Hachimi [Numer. Funct. Anal. Optim. 22 (2001) 775], d-invex functions in Antczak [Europ. J. Oper. Res. 137 (2002) 28] and univex functions in Bector et al. [Univex functions and univex nonlinear programming, Proc. Admin. Sci. Assoc. Canada, 1992, p. 115]. By utilizing the new concepts, we derive a Karush–Kuhn–Tucker sufficient optimality condition and establish Mond–Weir type and general Mond–Weir type duality results for the nondifferentiable multiobjective programming problem. 2003 Elsevier B.V. All rights reserved.	multi-objective optimization	Shashi Kant Mishra;Shouyang Wang;Kin Keung Lai	2005	European Journal of Operational Research	10.1016/S0377-2217(03)00439-9	mathematical optimization;mathematical analysis;nonlinear programming;mathematics;mathematical economics;karush–kuhn–tucker conditions	Robotics	71.73116366869283	21.66552861457156	118495
13388f6fd703af7f7f1405018e168be5de133e3f	a weighted least squaes study of robustness in interior point linear programming	interior point methods;modified newton method;analytic center;interior point;weighted least square;boundary behavior;analytic centers;linear program;newton method;affine scaling;interior point method	This paper studies the robustness of interior point linear programming algorithms with respect to initial iterates that are too close to the boundary. Weighted least squares analysis is used in studying the near-boundary behavior of the affine scaling and Newton centering directions, which are often combined by interior point methods. This analysis leads to the development of a modified Newton centering direction exhibiting better near-boundary behavior than the two directions. Theoretical and computational results from the NETLIB test set are presented indicating that an approach which uses the modified Newton direction as the centering direction is more robust than both the pure affine scaling approach and one which uses the Newton direction as the centering direction.	affine scaling;algorithm;computation;image scaling;interior point method;least squares;linear programming;netlib;newton;test set	Alexander L. Hipolito	1993	Comp. Opt. and Appl.	10.1007/BF01299141	mathematical optimization;mathematical analysis;linear programming;newton fractal;interior point method;mathematics;geometry	Vision	75.31466008462792	23.235804604804724	118510
6143eed29bd19967af91fbf240733c61e68f3c9f	hankel matrix transformation of the walsh-fourier series	walsh fourier series;hankel matrix;toeplitz matrix	The Hankel matrix has various applications. In this paper we prove that Hankel matrix is strongly regular and apply to obtain the necessary and sufficient conditions to sum the Walsh–Fourier series of a function of bounded variation. 2013 Elsevier Inc. All rights reserved.	apply;bounded variation;hadamard transform;strongly regular graph;transformation matrix	Mohammad A. Alghamdi;Mohammad Mursaleen	2013	Applied Mathematics and Computation	10.1016/j.amc.2013.08.075	matrix function;mathematical analysis;discrete mathematics;hankel matrix;nonnegative matrix;single-entry matrix;dft matrix;hilbert matrix;band matrix;hamiltonian matrix;toeplitz matrix;mathematics;hankel transform;pascal matrix;state-transition matrix;block matrix;symmetric matrix;low-rank approximation;algebra	AI	78.8026848162838	20.092540053328417	118528
303e9da955facb09ec708c478275284b8dea8e8e	a semismooth newton method for the nearest euclidean distance matrix problem	qa mathematics;49m45;semismooth newton method;90c25;quadratic convergence;90c33;euclidean distance matrix	The Nearest Euclidean distance matrix problem (NEDM) is a fundamental computational problem in applications such as multidimensional scaling and molecular conformation from nuclear magnetic resonance data in computational chemistry. Especially in the latter application, the problem is often large scale with the number of atoms ranging from a few hundreds to a few thousands. In this paper, we introduce a semismooth Newton method that solves the dual problem of (NEDM). We prove that the method is quadratically convergent. We then present an application of the Newton method to NEDM with H-weights. We demonstrate the superior performance of the Newton method over existing methods including the latest quadratic semi-definite programming solver. This research also opens a new avenue towards efficient solution methods for the molecular embedding problem.	algorithm;computational chemistry;computational problem;distance matrix;duality (optimization);duality gap;euclidean distance;experiment;iteration;maxima and minima;multidimensional scaling;newton;newton's method;numerical analysis;rate of convergence;resonance;semiconductor industry;semidefinite programming;solver	Houduo Qi	2013	SIAM J. Matrix Analysis Applications	10.1137/110849523	mathematical optimization;calculus;mathematics;geometry;euclidean distance matrix;rate of convergence;algorithm	ML	75.04800938957095	25.94202743320413	118533
b7c545464c557d99a72ba56582fa3b1a9f5262a4	a global optimization algorithm for polynomial programming problems using a reformulation-linearization technique	reformulation linearization technique;objective function;linear programming relaxation;global optimization;branch and bound;lower bound	This paper is concerned with the development of an algorithm to solve continuous polynomial programming problems for which the objective function and the constraints are specified polynomials. A linear programming relaxation is derived for the problem based on a Reformulation Linearization Technique (RLT), which generates nonlinear (polynomial) implied constraints to be included in the original problem, and subsequently linearizes the resulting problem by defining new variables, one for each distinct polynomial term. This construct is then used to obtain lower bounds in the context of a proposed branch and bound scheme, which is proven to converge to a global optimal solution. A numerical example is presented to illustrate the proposed algorithm.	algorithm;branch and bound;converge;global optimization;linear programming relaxation;loss function;mathematical optimization;nonlinear system;numerical analysis;optimization problem;polynomial	Hanif D. Sherali;Cihan H. Tuncbilek	1992	J. Global Optimization	10.1007/BF00121304	mathematical optimization;combinatorics;discrete mathematics;integer programming;criss-cross algorithm;linear-fractional programming;nonlinear programming;linear programming relaxation;branch and price;mathematics;matrix polynomial;upper and lower bounds;branch and bound;branch and cut;global optimization	Robotics	71.59124197726385	23.643557487249733	118692
6469e690008417b18bb6cd383dad989cd62eeb73	a projected subgradient algorithm for bilevel equilibrium problems and applications	bilevel equilibrium problems;subgradient method;projection method;strong monotonicity;pseudoparamonotonicity;65 k10;65 k15;90 c25;90 c33	In this paper, we propose a new algorithm for solving a bilevel equilibrium problem in a real Hilbert space. In contrast to most other projection-type algorithms, which require to solve subproblems at each iteration, the subgradient method proposed in this paper requires only to calculate, at each iteration, two subgradients of convex functions and one projection onto a convex set. Hence, our algorithm has a low computational cost. We prove a strong convergence theorem for the proposed algorithm and apply it for solving the equilibrium problem over the fixed point set of a nonexpansive mapping. Some numerical experiments and comparisons are given to illustrate our results. Also, an application to Nash–Cournot equilibrium models of a semioligopolistic market is presented.	algorithm;subderivative;subgradient method	Le Quang Thuy;Ngoc Hai Trinh	2017	J. Optimization Theory and Applications	10.1007/s10957-017-1176-2	projection method;hilbert space;subgradient method;mathematical analysis;mathematical optimization;mathematics;fixed point;algorithm;convex function;convergence (routing);convex set	Theory	73.63521253774442	23.338294924260087	118857
d46f7fb6e03a6ec17a2fbbdf9f5febd0dba1ada3	aar-based decomposition algorithm for non-linear convex optimisation	decomposition;non linear optimisation;averaged alternating reflections aar;convex optimisation;second order cone program socp;article	The analysis of the bearing capacity of structures with a rigid-plastic behaviour can be achieved resorting to computational limit analysis. Recent techniques [3],[4] have allowed scientists and engineers to determine upper and lower bounds of the load factor under which the structure will collapse. Despite the attractiveness of these results, their application to practical examples is still hampered by the size of the resulting optimisation process.	algorithm;association for automated reasoning;computation;convex optimization;hash table;limit analysis;mathematical optimization;nonlinear system	Nima Rabiei;Jose J. Muñoz	2014	Comp. Opt. and Appl.	10.1007/s10589-015-9750-8	mathematical optimization;conic optimization;mathematics;decomposition;algorithm	ML	72.54934321897247	26.422221178436097	118884
d920699a7aba540f0f322f106c846aaabb554b1a	approximation and optimization of polyhedral discrete and differential inclusions		In the first part of the paperoptimization of polyhedral discrete and differential inclusions is considered, the problem is reduced to convex minimization problem and the necessary and sufficient condition for optimality is derived. The optimality conditions for polyhedral differential inclusions based on discrete-approximation problem according to continuous problems are formulated. In particular, boundedness of the set of adjoint discrete solutions and upper semicontinuity of the locally adjoint mapping are proved. In the second part of paper an optimization problem described by convex inequality constraint is studied. By using the equivalence theorem concerning the subdifferential calculus and approximating method necessary and sufficient condition for discrete-approximation problem with inequality constraint is established.	approximation;polyhedral	Elimhan N. Mahmudov	2012		10.1007/978-3-642-31724-8_38	dual cone and polar cone;inequality;equivalence (measure theory);subderivative;mathematical optimization;convex optimization;uniform boundedness;differential inclusion;mathematics;optimization problem	Theory	71.70263251966067	20.679606619695644	119067
c76eec9d94dca0c39257133b4a584435379d4aa3	new conjugate gradient method for unconstrained optimization	line search;global convergence;conjugate gradient method;unconstrained optimization	In this paper, a new conjugate gradient method is proposed for large-scale unconstrained optimization. This method includes the already existing three practical nonlinear conjugate gradient methods, which produces a descent search direction at every iteration and converges globally provided that the line search satisfies the Wolfe conditions. The numerical experiments are done to test the efficiency of the new method, which confirms the promising potentials of the new method.	conjugate gradient method;mathematical optimization	Badreddine Sellami;Yacine Chaib	2016	RAIRO - Operations Research	10.1051/ro/2015064	gradient descent;mathematical optimization;conjugate residual method;gradient method;derivation of the conjugate gradient method;mathematics;conjugate gradient method;nonlinear conjugate gradient method;biconjugate gradient method;line search	Vision	76.57941651446112	23.735155370048354	119223
caef7384107eb6ad7f3b06e64ce5bbac5f7f0574	large step volumetric potential reduction algorithms for linear programming	iterative algorithm;linear program;barrier function	We consider the construction of potential reduction algorithms using volumetric, and mixed volumetric — logarithmic, barriers. These are true “large step” methods, where dual updates produce constant-factor reductions in the primal-dual gap. Using a mixed volumetric — logarithmic barrier we obtain an\(O(\sqrt {nmL} )\) iteration algorithm, improving on the best previously known complexity for a large step method. Our results complement those of Vaidya and Atkinson on small step volumetric, and mixed volumetric — logarithmic, barrier function algorithms. We also obtain simplified proofs of fundamental properties of the volumetric barrier, originally due to Vaidya.	algorithm;linear programming	Kurt M. Anstreicher	1996	Annals OR	10.1007/BF02206828	mathematical optimization;mathematical analysis;barrier function;linear programming;mathematics;geometry;iterative method	EDA	73.6032908555473	26.011518633257765	119389
fe71600be6298e1ccbb729b8929237ead8f61f86	acceleration techniques for approximating the matrix exponential operator	linear algebra;fonction rationnelle;preconditionnement;iterative method;evaluation performance;analisis numerico;comportement;performance evaluation;metodo subespacio krylov;developpement mathematique;krylov subspace method;evaluacion prestacion;methode sousespace krylov;preconditioning;vecteur;matriz simetrica;shape function;mathematical expansion;analyse numerique;acceleration;proyeccion espectral;matrix exponential;action;symmetric matrix;metodo iterativo;iterative methods;funcion forma;26cxx;numerical analysis;operateur matriciel;conducta;fonction forme;methode iterative;algebre lineaire;65f08;spectral projection;sous espace;algebra lineal;matrice symetrique;precondicionamiento;vector;krylov subspace;large matrices;rational functions;65f50;funcion racional;65f60;behavior;accion;65f30;rational function;65f10;aceleracion;acceleration techniques;projection spectrale;expansion	In this paper we investigate some well established and more recent methods that aim at approximating the vector exp(A)v when A is a large symmetric negative semidefinite matrix, by efficiently combining subspace projections and spectral transformations. We show that some recently developed acceleration procedures may be restated as preconditioning techniques for the partial fraction expansion form of an approximating rational function. These new results allow us to devise a-priori strategies to select the associated acceleration parameters; theoretical and numerical results are shown to justify these choices. Moreover, we provide a performance evaluation among several numerical approaches to approximate the action of the exponential of large matrices. Our numerical experiments provide a new, in some cases unexpected picture of the actual behavior of the discussed methods.	approximation algorithm;experiment;igor muttik;iterative method;lanczos resampling;numerical analysis;performance evaluation;preconditioner;the matrix;time complexity	M. Popolizio;Valeria Simoncini	2008	SIAM J. Matrix Analysis Applications	10.1137/060672856	rational function;linear algebra;calculus;mathematics;geometry;iterative method;algebra	ML	82.68643556722533	20.529878923365416	119485
1648cb3dc28cdae8ece42ad86f5e14ec3793fa18	comparison of lasserre's measure-based bounds for polynomial optimization to bounds obtained by simulated annealing		We consider the problem of minimizing a continuous function f over a compact set K. We compare the hierarchy of upper bounds proposed by Lasserre in [SIAM J. Optim. 21(3) (2011), pp. 864− 885] to bounds that may be obtained from simulated annealing. We show that, when f is a polynomial and K a convex body, this comparison yields a faster rate of convergence of the Lasserre hierarchy than what was previously known in the literature.	convergence (action);polynomial hierarchy;rate of convergence;simulated annealing	Etienne de Klerk;Monique Laurent	2018	Math. Oper. Res.	10.1287/moor.2017.0906	rate of convergence;simulated annealing;mathematical optimization;convex body;compact space;closed set;polynomial;continuous function;hierarchy;mathematics	Theory	72.11355502287161	23.7619731433235	119624
8ea78e3d06067b38bea57cd5baaa6570c136def7	superlinearly convergent algorithms for solving singular equations and smooth reformulations of complementarity problems	superlinear convergence;convergence analysis;regularity;differential equation;90c30;47j07;singular solution;nonlinear equation;quadratic convergence;90c33;nonlinear equations;complementarity;complementarity problem;reformulation;46t20;singularity;nonlinear complementarity problem;regularity condition	We propose a new algorithm for solving smooth nonlinear equations in the case where their solutions can be singular. Compared to other techniques for computing singular solutions, a distinctive feature of our approach is that we do not employ second derivatives of the equation mapping in the algorithm and we do not assume their existence in the convergence analysis. Important examples of once but not twice differentiable equations whose solutions are inherently singular are smooth equation-based reformulations of the nonlinear complementarity problems. Reformulations of complementarity problems serve both as illustration of and motivation for our approach, and one of them we consider in detail. We show that the proposed method possesses local superlinear/quadratic convergence under reasonable assumptions. We further demonstrate that these assumptions are in general not weaker and not stronger than regularity conditions employed in the context of other superlinearly convergent Newton-type algorithms for solving complementarity problems, which are typically based on nonsmooth reformulations. Therefore our approach appears to be an interesting complement to the existing ones.	algorithm;complementarity (physics);complementarity theory;mixed complementarity problem;newton;nonlinear system;quadratic function;rate of convergence	Alexey F. Izmailov;Mikhail V. Solodov	2002	SIAM Journal on Optimization	10.1137/S1052623401372946	singularity;mathematical optimization;mathematical analysis;nonlinear system;complementarity;calculus;singular solution;mathematics;differential equation	Vision	76.89663448603824	21.95034788747017	119735
7db8bff2cfcdb50a05b789ba40ec920b9ea861e3	convergence of broyden's method in banach spaces	65j15;quasi newton update;banach space;47h17;49d15;broyden s method;q superlinear convergence	This paper proves new convergence theorems for convergence of Broyden’s method when applied to nonlinear equations in Banach spaces. The convergence is in the norm of the Banach space itself, rather than in the norm of some Hilbert space that contains the Banach space. It is shown that the norms in which q-superlinear convergence takes place are determined by the smoothing properties of the error in the Frechet derivative approximation and not by the inner product in which Broyden’s method is implemented. Among the consequences of the results in this paper are a proof of sup-norm local q-superlinear convergence when Broyden’s method is applied to integral equations with continuous kernels, global q-superlinear convergence of the Broyden iterates for singular and nonsingular linear compact fixed point problems in Banach space, a new method for integral equations having derivatives with sparse kernels, and q-superlinear convergence for a new method for integral equations when part of the Frechet derivative ...	broyden's method;spaces	D. M. Hwang;C. T. Kelley	1992	SIAM Journal on Optimization	10.1137/0802025	mathematical optimization;uniform convergence;mathematical analysis;topology;modes of convergence;compact convergence;broyden's method;mathematics;convergence tests;weak convergence;unconditional convergence;normal convergence;banach space;c0-semigroup	Theory	79.66167907993666	20.825005769579175	119847
ddcc76429d7acf093848601d8ae9c98cedbb2972	existence and strong convergence theorems for generalized mixed equilibrium problems of a finite family of asymptotically nonexpansive mappings in banach spaces		We first prove the existence of solutions for a generalized mixed equilibrium problem under the new conditions imposed on the given bifunction and introduce the algorithm for solving a common element in the solution set of a generalized mixed equilibrium problem and the common fixed point set of finite family of asymptotically nonexpansivemappings. Next, the strong convergence theorems are obtained, under some appropriate conditions, in uniformly convex and smooth Banach spaces. The main results extend various results existing in the current literature.	algorithm;asymptote;fixed point (mathematics);perturbation function;spaces;uniformly convex space	Rabian Wangkeeree;Hossein Dehghan;Pakkapon Preechasilp	2012	J. Applied Mathematics	10.1155/2012/859492	mathematical optimization;mathematical analysis;discrete mathematics;mathematics	Theory	72.83701869693158	19.874845085263416	119908
54e4e7b73043386057751d8d758bb3ac9d5d838b	on the linear convergence of the alternating direction method of multipliers	dual ascent;bepress selected works;alternating directions of multipliers;linear convergence;linear convergence alternating directions of multipliers error bound dual ascent;error bound	We analyze the convergence rate of the alternating direction method of multipliers (ADMM) for minimizing the sum of two or more nonsmooth convex separable functions subject to linear constraints. Previous analysis of the ADMM typically assumes that the objective function is the sum of only two convex functions defined on two separable blocks of variables even though the algorithm works well in numerical experiments for three or more blocks. Moreover, there has been no rate of convergence analysis for the ADMM without strong convexity in the objective function. In this paper we establish the global linear convergence of the ADMM for minimizing the sum of any number of convex separable functions. This result settles a key question regarding the convergence of the ADMM when the number of blocks is more than two or if the strong convexity is absent. It also implies the linear convergence of the ADMM for several contemporary applications including LASSO, Group LASSO and Sparse Group LASSO without any strong convexity assumption. Our proof is based on estimating the distance from a dual feasible solution to the optimal dual solution set by the norm of a certain proximal residual, and by requiring the dual stepsize to be sufficiently small.		Mingyi Hong;Zhi-Quan Luo	2017	Math. Program.	10.1007/s10107-016-1034-2	mathematical optimization;combinatorics;discrete mathematics;mathematics;rate of convergence	ML	74.40993783466261	25.24866589493246	120150
21b00616a04c2133ce6e4cb20045f66ed43034dd	on unbounded operators and applications	ill posed problems;operador lineal;metodo regularizacion;espace hilbert;problema mal planteado;regularisation;matematicas aplicadas;discrepancy principle;espacio hilbert;mathematiques appliquees;unbounded linear operators;operador acotado;46cxx;regularization method;probleme mal pose;methode regularisation;ecuacion lineal;probleme variationnel;variational problem;unbounded operator;regularization;hilbert space;operateur borne;numerical analysis;linear operator;ill posed problem;06a15;bounded operator;65f22;47axx;regularizacion;46axx;linear equations;linear equation;applied mathematics;operateur lineaire;equation lineaire;spectral theory	is a solvable linear equation in a Hilbert space H , A is a linear, closed, densely defined, unbounded operator in H , which is not boundedly invertible, so problem (1) is ill-posed. It is proved that the closure of the operator (A A + α I )−1 A, with the domain D(A), where α > 0 is a constant, is a linear bounded everywhere defined operator with norm ≤ 1 2 √ α . This result is applied to the variational problem F(u) := ‖Au − f ‖2 + α‖u‖2 = min, where f is an arbitrary element of H , not necessarily belonging to the range of A. Variational regularization of problem (1) is constructed, and a discrepancy principle is proved. c © 2007 Elsevier Ltd. All rights reserved.	calculus of variations;decision problem;discrepancy function;hilbert space;linear equation;matrix regularization;maxima and minima;variational principle;well-posed problem	Alexander G. Ramm	2008	Appl. Math. Lett.	10.1016/j.aml.2007.05.007	mathematical optimization;mathematical analysis;topology;calculus;mathematics;linear equation;algebra	Theory	73.73200794942406	20.033592670625264	120217
665e93b006530b2714d5ff5abc0a83f98eac7115	a new infeasible interior-point algorithm for linear programming	interior point methods;merit function;newton s method;satisfiability;primal dual;linear program;central path;interior point method;path following;interior point algorithm	In this paper we present an infeasible path-following interior-point algorithm for solving linear programs using a relaxed notion of the central path, called quasicentral path, as a central region. The algorithm starts from an infeasible point which satisfies that the norm of the dual condition is less than the norm of the primal condition. We use weighted sets as proximity measures of the quasicentral path, and a new merit function for making progress toward this central region. We test the algorithm on a set of NETLIB problems obtaining promising numerical results.	algorithm;interior point method;linear programming;netlib;numerical analysis	Miguel Argáez;Leticia Velázquez	2003		10.1145/948542.948545	mathematical optimization;combinatorics;mathematical analysis;mathematics	Theory	73.63471775511584	22.89186671374401	120331
0a8fff885c660a9c0eebd279c0a9926e8febfa9d	a preconditioned minres method for nonsymmetric toeplitz matrices	15b05;65 numerical analysis;mathematics;minres;numerical analysis;65f08;circulant preconditioner;nonsymmetric matrix;toeplitz matrix;65f10	Circulant preconditioning for symmetric Toeplitz linear systems is well-established; theoretical guarantees of fast convergence for the conjugate gradient method are descriptive of the convergence seen in computations. This has led to robust and highly efficient solvers based on use of the fast Fourier transform exactly as originally envisaged in Gil Strang’s ‘Proposal for Toeplitz Matrix Calculations’ (Studies in Applied Mathematics, 74, pp. 171–176, 1986.). For nonsymmetric systems, the lack of generally descriptive convergence theory for most iterative methods of Krylov type has provided a barrier to such a comprehensive guarantee, though several methods have been proposed and some analysis of performance with the normal equations is available. In this paper, by the simple device of reordering, we rigorously establish a circulant preconditioned short recurrence Krylov subspace iterative method of minimum residual type for nonsymmetric (and possibly highly nonnormal) Toeplitz systems. Convergence estimates similar to those in the symmetric case are established.	bilinear filtering;circulant matrix;coefficient;column (database);computation;conjugate gradient method;existential quantification;fast fourier transform;generalized minimal residual method;iterative method;krylov subspace;krylov–bogolyubov theorem;linear least squares (mathematics);linear system;numerical analysis;preconditioner;rate of convergence;recurrence relation;toeplitz hash algorithm	Jennifer Pestana;Andrew J. Wathen	2015	SIAM J. Matrix Analysis Applications	10.1137/140974213	mathematical optimization;combinatorics;discrete mathematics;numerical analysis;toeplitz matrix;mathematics;algorithm;algebra	ML	82.46518908326561	22.99103871710323	120412
a852f14b9ad70a466c0a71c74583bfd744dc33a3	primal-dual interior-point methods for semidefinite programming in finite precision	optimal solution;primal dual interior point method;semidefinite programming;65f05;49m15;90c3;65g05;primal dual;error analysis;15a06;65k05;gaussian elimination;floating point;interior point method;semidefinite program	Recently, a number of primal-dual interior-point methods for semideenite programming have been developed. To reduce the number of oating point operations, each iteration of these methods typically performs block Gaussian elimination with block pivots that are close to singular near the optimal solution. As a result, these methods often exhibit complex numerical properties in practice. We consider numerical issues related to some of these methods. Our error analysis indicates that these methods could be numerically stable if certain coeecient matrices associated with the iterations are well-conditioned, but are unstable otherwise. With this result, we explain why one particular method, the one introduced by Alizadeh, Haeberly and Overton is in general more stable than others. We also explain why the so called least squares variation, introduced for some of these methods, does not yield more numerical accuracy in general. Finally, we present results from our numerical experiments to support our analysis.	condition number;control theory;error analysis (mathematics);experiment;gaussian elimination;iscb overton prize;interior point method;iteration;least squares;numerical analysis;numerical stability;semidefinite programming	Ming Gu	2000	SIAM Journal on Optimization	10.1137/S105262349731950X	mathematical optimization;gaussian elimination;combinatorics;discrete mathematics;floating point;interior point method;mathematics;semidefinite embedding;semidefinite programming	ML	78.132781826314	23.831606453116628	121056
b87d8fb6e72c39d469c66a86d578930eb8562873	an algorithmic framework of variable metric over-relaxed hybrid proximal extra-gradient method		We propose a novel algorithmic framework of Variable Metric Over-Relaxed Hybrid Proximal Extra-gradient (VMOR-HPE) method with a global convergence guarantee for the maximal monotone operator inclusion problem. Its iteration complexities and local linear convergence rate are provided, which theoretically demonstrate that a large over-relaxed step-size contributes to accelerating the proposed VMORHPE as a byproduct. Specifically, we find that a large class of primal and primal-dual operator splitting algorithms are all special cases of VMOR-HPE. Hence, the proposed framework offers a new insight into these operator splitting algorithms. In addition, we apply VMORHPE to the Karush-Kuhn-Tucker (KKT) generalized equation of linear equality constrained multiblock composite convex optimization, yielding a new algorithm, namely nonsymmetric Proximal Alternating Direction Method of Multipliers with a preconditioned Extra-gradient step in which the preconditioned metric is generated by a blockwise Barzilai-Borwein line search technique (PADMMEBB). We also establish iteration complexities of PADMM-EBB in terms of the KKT residual. Finally, we apply PADMM-EBB to handle the nonnegative dual graph regularized low-rank representation problem. Promising results on synthetic and real datasets corroborate the efficacy of PADMM-EBB.	augmented lagrangian method;constrained optimization;convex optimization;diffusing update algorithm;dual graph;experiment;gradient method;iteration;karush–kuhn–tucker conditions;line search;linear equation;list of operator splitting topics;local convergence;low-rank approximation;mathematical optimization;maximal set;numerical analysis;rate of convergence;synthetic intelligence;tucker decomposition;monotone	Li Shen;Peter P Sun;Yitong Wang;Wei Liu;Tong Zhang	2018			mathematical optimization;residual;line search;rate of convergence;dual graph;mathematics;gradient method;monotonic function;convex optimization;karush–kuhn–tucker conditions	ML	74.11993614742916	24.943874037563603	121155
bae49d1df2420df8e5f44191ace1bb53fe97844d	on sufficient conditions of the injectivity: development of a numerical test algorithm via interval analysis	injectivity;injectivite;algorithm analysis;analisis intervalo;algorithme numerique;numerical algorithm;inyectividad;algoritmo numerico;analyse algorithme;aritmetica intervalo;interval arithmetic;arithmetique intervalle;analyse intervalle;analisis algoritmo;interval analysis	This paper presents a new numerical algorithm based on interval analysis able to verify that a continuously differentiable function is injective. The efficiency of the method is demonstrated by illustrative examples. These examples have been treated by a C++ solver which is made available.	algorithm;c++;distribution (mathematics);indeterminacy in concurrent computation;interval arithmetic;local consistency;numerical analysis;numerical method;software propagation;solver;word lists by frequency	Sebastien Lagrange;Nicolas Delanoue;Luc Jaulin	2007	Reliable Computing	10.1007/s11155-007-9042-9	calculus;mathematics;interval arithmetic;algorithm;algebra	EDA	76.72991248229827	19.109808238117285	121212
d128cea8733705afa9678ac8046e6ff942bcabb5	remark on algorithm 58: matrix inversion	matrix inversion		algorithm;gene expression programming	George W. Struble	1962	Commun. ACM	10.1145/368637.368696	eigendecomposition of a matrix;sparse matrix;computer science;band matrix;sample matrix inversion	Graphics	80.19563905156255	20.690128859059907	121306
f81dd1680b2b6101256a5de6ac1f5df3d5604559	on frank-wolfe and equilibrium computation		We consider the Frank-Wolfe (FW) method for constrained convex optimization, and we show that this classical technique can be interpreted from a different perspective: FW emerges as the computation of an equilibrium (saddle point) of a special convex-concave zero sum game. This saddle-point trick relies on the existence of no-regret online learning to both generate a sequence of iterates but also to provide a proof of convergence through vanishing regret. We show that our stated equivalence has several nice properties, as it exhibits a modularity that gives rise to various old and new algorithms. We explore a few such resulting methods, and provide experimental results to demonstrate correctness and efficiency.	computation;concave function;convex optimization;correctness (computer science);frank–wolfe algorithm;mathematical optimization;modularity (networks);regret (decision theory);turing completeness	Jacob D. Abernethy;Jun-Kun Wang	2017			machine learning;mathematical optimization;iterated function;equivalence (measure theory);artificial intelligence;regret;saddle;convex optimization;zero-sum game;correctness;saddle point;mathematics	ML	73.35550942472257	21.998403941950613	121353
8e0edbb3feeac644e47f4fbe77f34b1f85c02e02	a generalized alternative theorem of partial and generalized cone subconvexlike set-valued maps and its applications in linear spaces	期刊论文	We first introduce a new notion of the partial and generalized cone subconvexlike set-valued map and give an equivalent characterization of the partial and generalized cone subconvexlike set-valued map in linear spaces. Secondly, a generalized alternative theorem of the partial and generalized cone subconvexlike set-valued map was presented. Finally, Kuhn-Tucker conditions of set-valued optimization problems were established in the sense of globally proper efficiency.		Zhi-Ang Zhou;Jian-Wen Peng	2012	J. Applied Mathematics	10.1155/2012/370654	mathematical optimization;mathematical analysis;topology;mathematics	ML	71.11281404016496	19.681252302170204	121388
4a293af883ba90c0536fb6f437c078f59145ab92	support recovery without incoherence: a case for nonconvex regularization		We develop a new primal-dual witness proof framework that may be used to establish variable selection consistency and `1-bounds for sparse regression problems, even when the loss function and regularizer are nonconvex. We use this method to prove two theorems concerning support recovery and `1-guarantees for a regression estimator in a general setting. Notably, our theory applies to all potential stationary points of the objective and certifies that the stationary point is unique under mild conditions. Our results provide a strong theoretical justification for the use of nonconvex regularization: For certain nonconvex regularizers with vanishing derivative away from the origin, any stationary point can be used to recover the support without requiring the typical incoherence conditions present in `	feature selection;loss function;sparse matrix;stationary process	Po-Ling Loh;Martin J. Wainwright	2014	CoRR		econometrics;mathematical optimization;mathematics;statistics	ML	74.42423707411707	25.450777724147997	121477
d36c58334615aa645fea047ffe69b3adb381d205	necessary optimality conditions for constrained optimization problems under relaxed constraint qualifications	second order;optimisation sous contrainte;criterio optimalidad;constrained optimization;calculo de variaciones;regularite;non linear programming;regularidad;right hand side;4 49k27;programacion no lineal;regularity;90c30;sigma term;courbure;programmation non lineaire;lagrange multiplier;reference point;optimizacion con restriccion;optimization problem;condicion optimalidad;calcul variationnel;condition optimalite;47j07;mathematical programming;multiplicateur lagrange;ensemble convexe;multiplicador lagrange;curvatura;curvature;optimality criterion;constraint qualification;convex set;critere optimalite;constrained optimization problem;programmation mathematique;abstract constraints;programacion matematica;necessary optimality condition;variational calculus;optimality condition;conjunto convexo	We derive firstand second-order necessary optimality conditions for set-constrained optimization problems under the constraint qualificationtype conditions significantly weaker than Robinson’s constraint qualification. Our development relies on the so-called 2-regularity concept, and unifies and extends the previous studies based on this concept. Specifically, in our setting constraints are given by an inclusion, with an arbitrary closed convex set on the right-hand side. Thus, for the second-order analysis, some curvature characterizations of this set near the reference point must be taken into account.	algorithm characterizations;constrained optimization;convex set;karush–kuhn–tucker conditions;mathematical optimization	Aram V. Arutyunov;Evgeniy R. Avakov;Alexey F. Izmailov	2008	Math. Program.	10.1007/s10107-006-0082-4	optimization problem;mathematical optimization;constrained optimization;calculus;mathematics;geometry;curvature;convex set;constraint;lagrange multiplier;second-order logic;calculus of variations	AI	71.96812896642128	20.953777005869306	121755
00dff1f14bf2e0bea7a68254663bc6da12832572	on solutions of the quaternion matrix equation ax=bax=b and their applications in color image restoration	moore penrose generalized inverse;quaternion matrices;image restoration;matrix equation;least squares solution;kronecker product	By using the complex representation of quaternion matrices, and the Moore-Penrose generalized inverse, we derive the expressions of the least squares solution with the least norm, the least squares pure imaginary solution with the least norm, and the least squares real solution with the least norm for the quaternion matrix equation AX=B, respectively. Finally, we discuss their applications in color image restoration.	circuit restoration;color image;image restoration	Shi-Fang Yuan;Qing-Wen Wang;Xue-Feng Duan	2013	Applied Mathematics and Computation	10.1016/j.amc.2013.05.069	total least squares;image restoration;iteratively reweighted least squares;mathematical analysis;non-linear iterative partial least squares;mathematics;geometry;non-linear least squares;kronecker product;least squares;matrix;algebra	Vision	79.52550128373296	19.929762015113955	121772
b7bbaf370a8958bf80b9548d8b3d82e1efbdd9b9	solving semidefinite programs using preconditioned conjugate gradients	inexact gauss newton method;optimal solution;matrix representation;preconditioned conjugate gradient method;max cut problem;semidefinite programming;interior point;gauss newton;crossover;positive definite;general techniques;preconditioned conjugate gradients;matrix approximation;operator equation;preconditioned conjugate gradient;large sparse problems;path following;gauss newton method;optimality condition;semidefinite program	The contribution of this article is to describe a general technique to solve some classes of large but sparse semidefinite problems via a robust primal–dual interior-point technique, which uses an inexact Gauss–Newton approach with a matrix free preconditioned conjugate gradient method. This approach avoids the ill-conditioning pitfalls that result from symmetrization and from forming the so-called normal equations, while maintaining the primal–dual framework. First, we apply a preprocessing step that reduces the optimality conditions before linearization and results in a single, well-conditioned, overdetermined bilinear operator equation. We then use preconditioned conjugate-gradients to approximately solve the linearization at every step of a path-following approach. We do not form the matrix representation of the linearization. In addition, once close to the optimal solution, we apply a crossover technique after which the iterates are no longer forced to be positive definite. In the experimental part o...	conjugate gradient method;preconditioner;semidefinite programming	Henry Wolkowicz	2004	Optimization Methods and Software	10.1080/1055678042000193162	mathematical optimization;maximum cut;crossover;mathematical analysis;matrix representation;interior point method;calculus;mathematics;positive-definite matrix;semidefinite programming	ML	76.2540545324132	24.354172722347336	121890
678a62aeeb8746906d5621c5b3502a25b2c4a2bf	first- and second-order optimality conditions for piecewise smooth objective functions	tangential stationarity;decomposition;projected hessian;karush kuhn tucker;normal growth;abs normal form;second order optimality;piecewise linearization	Any piecewise smooth function that is specified by an evaluation procedure involving smooth elemental functions and piecewise linear functions like min and max can be represented in the so-called abs-normal form. By an extension of algorithmic, or automatic, differentiation, one can then compute certain first and second order derivative vectors and matrices that represent a local piecewise linearization and provide additional curvature information. On the basis of these quantities we characterize local optimality by first and second order necessary and sufficient conditions, which generalize the corresponding KKT theory for smooth problems. The key assumption is the Linear Independence Kink Qualification (LIKQ), a generalization of LICQ familiar from nonlinear optimization. It implies that the objective has locally a so-called VU decomposition and renders everything tractable in terms of matrix factorizations and other simple linear algebra operations. By yielding descent directions whenever they are violated the new optimality conditions point the way to a superlinearly convergent generalized QP solver, which is currently under development. We exemplify the theory on two nonsmooth examples of Nesterov.	a. b. and c.;algorithm;approximation;augmented lagrangian method;automatic differentiation;boris mordukhovich;buckling;cobham's thesis;complementarity theory;computation;descent direction;edmund m. clarke;elemental;exemplification;first-generation programming language;gradient;hessian;instability;iterative method;kernel (linear algebra);lagrange multiplier;linear algebra;linear function;linear programming;local optimum;mathematical optimization;maxima and minima;microsoft outlook for mac;np-hardness;nash equilibrium;nonlinear programming;nonlinear system;optimization problem;parabolic antenna;penalty method;piecewise linear continuation;polynomial;programming paradigm;rate of convergence;rendering (computer graphics);social inequality;solver;stationary process;subgradient method	Andreas Griewank;Andrea Walther	2016	Optimization Methods and Software	10.1080/10556788.2016.1189549	mathematical optimization;mathematical analysis;discrete mathematics;mathematics;decomposition;karush–kuhn–tucker conditions;piecewise linear manifold;piecewise	ML	72.5892713645024	20.984963968589224	121933
5ed4ea9facb3b98c378c81310b6a5dbd1ddf916d	stability analysis of numerical boundary conditions in domain decomposition algorithms	equation derivee partielle;problema neumann;numerical stability;interface conditions;systeme equation;partial differential equation;ecuacion derivada parcial;analisis numerico;decomposition domaine;condiciones limites;condicion necesaria;domain decomposition;algorithm analysis;probleme neumann;forward time central sace algorithm;condition aux limites;ftcs algorithm;estabilidad numerica;metodo descomposicion;condition necessaire suffisante;methode decomposition;descomposicion dominio;analyse stabilite;matrix analysis;periodicite;analyse numerique;condition;condition suffisante;algorithme;interfase;periodicity;algorithm;decomposition method;sistema ecuacion;periodicidad;numerical analysis;necessary condition;condicion suficiente;overlapping domain decomposition;boundary condition;partial differential equations;necessary and sufficient condition;equation system;condicion;interface;analyse spectrale;stability analysis;interface condition;von neumann analysis;analisis espectral;analyse algorithme;stabilite numerique;condition necessaire;neumann problem;sufficient condition;spectral analysis;condition interface;analisis algoritmo;condicion necesaria suficiente;analyse von neumann;algoritmo	This paper discusses numerical stability of a class of non-overlapping domain decomposition algorithms. Inherent shortcomings associated with different methods for proving stability are pointed out. Von Neumann analysis yields only a necessary condition for stability because it does not consider the overall effect of the boundary conditions between subdomains. Matrix analysis produces also a necessary condition for stability since the matrices of coefficients associated with the algorithms are not symmetric. The GKSO, on the other hand, produces necessary and sufficient conditions for stability. However, it is difficult to apply systematically this analysis to the algorithms.	algorithm;domain decomposition methods;numerical analysis	Wilson Rivera	2003	Applied Mathematics and Computation	10.1016/S0096-3003(02)00137-6	mathematical analysis;calculus;mathematics;partial differential equation;numerical stability;algorithm;algebra	Theory	80.82114221035346	19.696822222847207	121995
efe70e3328aef434b6efea11c53ec14df41dd6d7	a class of nonmontone line search method with perturbations	global convergence;hybrid projection method;perturbation;nonmonotone line search method	In this paper, a new kind of nonmontone line search method which is called new hybrid projection method with perturbations is proposed. At the same time, global convergence of this kind of method is proved only in the case where the gradient function is uniformly continuous on an open convex set containing the iteration sequence. In doing so, we remove various boundedness conditions. Furthermore, we obtain that the convergence property of gradient-type method with new nonmontone linear search method will not be changed when search directions are perturbed slightly. Numerical examples are given in the thi rd section of this paper.	convex set;gradient;iteration;line search;linear search;local convergence;numerical method;perturbation theory	Meixia Li	2012	JCP	10.4304/jcp.7.4.941-946	mathematical optimization;mathematical analysis;perturbation;calculus;backtracking line search;mathematics;line search	ML	76.05917781311425	23.649199423794222	122227
a5a7cccd7d5773e1413a5b6ae8c71dfe0ab8acf2	l- and r-localized solvabilities of max-separable interval linear equations and its applications	computacion informatica;r localized solvability;l;ciencias basicas y experimentales;matematicas;interval system;grupo a;max separable;l localized solvability;r	In this paper, we propose new types of solutions (an L-localized solution and an R-localized solution) to interval systems ofmax-separable linear equationsA⊗x = b, whereA = [A, A] and b = [b, b]. We provide some small application examples to emphasize the importance of our proposed solutions. We give necessary and sufficient conditions for checking L-localized and R-localized solvabilities and apply them to achieve the closed forms of the sets of all L-localized solutions and R-localized solutions, respectively. © 2014 Elsevier B.V. All rights reserved.	linear equation;linear system;soliton;system of linear equations	Worrawate Leela-apiradee;Phantipa Thipwiwatpotjana	2015	J. Computational Applied Mathematics	10.1016/j.cam.2014.11.024	mathematical optimization;discrete mathematics;l;calculus;mathematics;algorithm	EDA	76.1744547171864	18.744837657862846	122253
090f92e58b28763c3eeaaf9253968bb533e657d7	optimizing over coherent risk measures and non-convexities: a robust mixed integer optimization approach	coherent risk measure minimization;robust optimization;portfolio optimization;binary classification;nonconvexity	Recently, coherent risk measure minimization was formulated as robust optimization and the correspondence between coherent risk measures and uncertainty sets of robust optimization was investigated. We study minimizing coherent risk measures under a norm equality constraint with the use of robust optimization formulation. Not only existing coherent risk measures but also a new coherent risk measure is investigated by setting a new uncertainty set. The norm equality constraint itself has a practical meaning or plays a role to prevent a meaningless solution, the zero vector, in the context of portfolio optimization or binary classification in machine learning, respectively. For such advantages, the convexity is sacrificed in the formulation. However, we show a condition for an input of our problem which guarantees that the nonconvex constraint is convexified without changing the optimality of the problem. If the input does not satisfy the condition, we propose to solve a mixed integer optimization problem by using the 1 or ∞-norm. The numerical experiments show that our approach has good performance for portfolio optimization and binary classification and also imply its flexibility of modelling that makes it possible to deal with various coherent risk measures.	binary classification;bregman divergence;cvar;coherence (physics);coherent risk measure;experiment;fits;machine learning;mathematical optimization;numerical analysis;optimization problem;optimizing compiler;robust optimization;statistical classification	Dimitris Bertsimas;Akiko Takeda	2015	Comp. Opt. and Appl.	10.1007/s10589-015-9755-3	binary classification;mathematical optimization;combinatorics;discrete mathematics;robust optimization;entropic value at risk;portfolio optimization;mathematics	ML	70.62236937507116	24.8800525813614	122282
8d1c893900d26cfb7d3c23b01d416611dbfd0e59	a combined phase i-phase ii projective algorithm for linear programming	algorithme karmarkar;limite inferior;improvement;projection method;duality;dual problem;algorithme;algorithm;dualite;programacion lineal;methode projection;mathematical programming;amelioration;metodo proyeccion;linear programming;mejoria;programmation lineaire;linear program;dualidad;phase ii;programmation mathematique;limite inferieure;programacion matematica;lower bound;algoritmo	We devise a projective algorithm which explicitly considers the constraint that an artificial variable be zero at the solution. Inclusion of such a constraint allows the algorithm to be applied to a (possibly infeasible) standard form linear program, without the addition of any “bigM“ terms or conversion to a primal-dual problem.	karmarkar's algorithm;linear programming	Kurt M. Anstreicher	1989	Math. Program.	10.1007/BF01582290	mathematical optimization;constraint programming;duality;constraint satisfaction;linear programming;calculus;mathematics;algorithm;difference-map algorithm;hybrid algorithm	Theory	75.3488028500882	22.11296604424116	122305
80e9acea4f4948d4348fd32cdbecffa1a979f9be	from an abstract maximal element principle to optimization problems, stationary point theorems and common fixed point theorems	fixed point theorem;maximal element;existence theorem;optimization problem;μ bounded;dancs hegedus medvegyev s principle;minimization problem;common fixed point;stationary point theorem;equilibrium theorem;sizing up;multivalued mapping;intersection theorem	In this paper, we first establish an existence theorem related with intersection theorem, maximal element theorem and common fixed point theorem for multivalued maps by applying an abstract maximal element principle proved by Lin and Du. Some new stationary point theorems, minimization problems, new fixed point theorems and a system of nonconvex equilibrium theorem are also given.	fixed point (mathematics);mathematical optimization;maximal set;stationary process	Lai-Jiu Lin;Wei-Shih Du	2010	J. Global Optimization	10.1007/s10898-009-9423-1	maximum theorem;mean value theorem;optimization problem;maximal element;mathematical optimization;carlson's theorem;mathematical analysis;discrete mathematics;brouwer fixed-point theorem;rolle's theorem;kakutani fixed-point theorem;arzelà–ascoli theorem;schauder fixed point theorem;factor theorem;intersection theorem;shift theorem;max-flow min-cut theorem;danskin's theorem;no-go theorem;kelvin–stokes theorem;fundamental theorem;mathematics;picard–lindelöf theorem;fixed-point theorem;compactness theorem;fixed-point property	Theory	71.84469200576778	21.020843259002927	122466
848b49e95bdecfa6d1ab42ee266ed270d227038b	updating of conjugate direction matrices using members of broyen's family	variable metric method;variable metric algorithms;optimisation;matrix factorization;non linear programming;mise a jour;direction conjuguee;optimizacion;matrix factorizations;programacion no lineal;positive definite;programmation non lineaire;linear constraint;satisfiability;iterative algorithm;matrice mathematique;algorithme;algorithm;linear constraints;factorization;mathematical matrix;factorizacion;methode bfgs;conjugate directions;metodo metrico variable;factorisation;matriz matematica;theoreme dixon;optimization;contrainte lineaire;puesta al dia;methode metrique variable;nonlinear optimization;updating;algoritmo	Many iterative algorithms for optimization calculations use a second derivative approximation,B say, in order to calculate the search directiond = −B−1∇f(x). In order to avoid invertingB we work with matricesZ, whose columns satisfy the conjugacy relationsZTBZ = I. We present an update ofZ that is compatible with members of the Broyden family that generate positive definite second derivative approximations. The algorithm requires only 3n2+O(n) flops for the update ofZ and the calculation ofd. The columns of the resultantZ matrices have interesting conjugacy and orthogonality properties with respect to previous second derivative approximations and function gradients, respectively. The update also provides a simple proof of Dixon's theorem. For the BFGS method we adapt the algorithm in order to obtain a null space method for linearly constrained calculations.		Dirk Siegel	1993	Math. Program.	10.1007/BF01580608	mathematical optimization;combinatorics;nonlinear programming;calculus;mathematics;factorization	ML	76.74715385077364	22.936863658892822	122484
2066f66a8df7190b3cde890e9aa26cd78489535d	algorithm 902: gpops, a matlab software for solving multiple-phase optimal control problems using the gauss pseudospectral method	computational methods;software tool;cost function;programming language;nonlinear programming;numerical method;computational method;optimal control;large scale;pseudospectral method;differential algebraic equation;phases;fortran;nonlinear optimization;optimal control problem;dynamic optimization	An algorithm is described to solve multiple-phase optimal control problems using a recently developed numerical method called the Gauss pseudospectral method. The algorithm is well suited for use in modern vectorized programming languages such as FORTRAN 95 and MATLAB. The algorithm discretizes the cost functional and the differential-algebraic equations in each phase of the optimal control problem. The phases are then connected using linkage conditions on the state and time. A large-scale nonlinear programming problem (NLP) arises from the discretization and the significant features of the NLP are described in detail. A particular reusable MATLAB implementation of the algorithm, called GPOPS, is applied to three classical optimal control problems to demonstrate its utility. The algorithm described in this article will provide researchers and engineers a useful software tool and a reference when it is desired to implement the Gauss pseudospectral method in other programming languages.	algebraic equation;algorithm;discretization;fortran;gpops-ii;gauss pseudospectral method;linear algebra;linkage (software);matlab;mathematical optimization;natural language processing;nonlinear programming;nonlinear system;numerical method;optimal control;programming language;programming tool;pseudo-spectral method	Anil V. Rao;David A. Benson;Christopher Darby;Michael A. Patterson;Camila Francolin;Ilyssa Sanders;Geoffrey T. Huntington	2010	ACM Trans. Math. Softw.	10.1145/1731022.1731032	gauss pseudospectral method;mathematical optimization;phase;pseudospectral optimal control;pseudo-spectral method;optimal control;numerical analysis;nonlinear programming;differential algebraic equation;computer science;theoretical computer science;mathematics;algorithm	PL	81.13654657219263	26.720381137907573	122575
a1fef6b44dc28397b85ac73e6acac5d7a039cac5	fast algorithms for designing unimodular waveform(s) with good correlation properties		We develop new fast and efficient algorithms for designing single or multiple unimodular waveforms with good auto- and cross-correlation or weighted correlation properties, which are highly desired in radar and communication systems. The waveform design is based on the minimization of the integrated sidelobe level (ISL) and weighted ISL (WISL) of waveforms. As the corresponding problems can quickly grow to a large scale with increasing the code length and the number of waveforms, the main issue turns to be the development of fast large-scale optimization techniques. The difficulty is also that the corresponding optimization problems are nonconvex, but the required accuracy is high. Therefore, we formulate the ISL and WISL minimization problems as nonconvex quartic optimization problems in frequency domain, and then simplify them into quadratic problems via majorization-minimization technique, which is one of the basic techniques for addressing large-scale and/or nonconvex optimization problems. While designing our fast algorithms, we explore and use the inherent algebraic structures in objective functions to rewrite them into quartic forms, and in the case of WISL minimization, to derive additionally an alternative quartic form that allows us to apply the quartic-quadratic transformation. Our algorithms are applicable to large-scale unimodular waveform design problems as they are proved to have lower or comparable computational burden (analyzed theoretically) and faster convergence speed (confirmed by comprehensive simulations) than the state-of-the-art algorithms. In addition, the waveforms designed by our algorithms demonstrate better correlation properties compared to their counterparts.	algorithm;computation;cross-correlation;empirical risk minimization;fast fourier transform;linear algebra;mathematical optimization;optimization problem;quadratic programming;quartic function;radar;rewrite (programming);simulation;time complexity;unimodular polynomial matrix;waveform;isl	Yongzhe Li;Sergiy A. Vorobyov	2018	IEEE Transactions on Signal Processing	10.1109/TSP.2017.2787104	mathematical optimization;frequency domain;algorithm;quartic function;mathematics;algorithm design;correlation;unimodular matrix;waveform;optimization problem;convergence (routing)	ML	75.10258363570648	28.70788177731009	122767
2f60dcea5bfc0ca282a02c6a6f2bae57a9838264	two modified dai-yuan nonlinear conjugate gradient methods	line search;global convergence;conjugate gradient method	In this paper, we propose two modified versions of the Dai-Yuan (DY) nonlinear conjugate gradient method. One is based on the MBFGS method (Li and Fukushima, J Comput Appl Math 129:15–35, 2001) and inherits all nice properties of the DY method. Moreover, this method converges globally for nonconvex functions even if the standard Armijo line search is used. The other is based on the ideas of Wei et al. (Appl Math Comput 183:1341–1350, 2006), Zhang et al. (Numer Math 104:561–572, 2006) and possesses good performance of the Hestenes-Stiefel method. Numerical results are also reported.	distributed artificial intelligence;line search;nonlinear conjugate gradient method;nonlinear system;numerical method	Xiang Lin	2008	Numerical Algorithms	10.1007/s11075-008-9213-8	mathematical optimization;mathematical analysis;conjugate residual method;calculus;derivation of the conjugate gradient method;mathematics;conjugate gradient method;nonlinear conjugate gradient method;line search	AI	77.41052508654835	22.884539289376782	122811
ab49da997ec4b5d588704877c4f159050e61f8df	efficient solution of two-stage stochastic linear programs using interior point methods	operations research mathematical programming;interior point algorithms;mathematics;statistics general;schur complement;operation research decision theory;linear system;performance improvement;computer experiment;linear program;optimization;convex and discrete geometry;stochastic programming;interior point method;interior point algorithm	Solving deterministic equivalent formulations of two-stage stochastic linear programs using interior point methods may be computationally diflicult due to the need to factorize quite dense search direction matrices (e.g., AAT). Several methods for improving the algorithmic efficiency of interior point algorithms by reducing the density of these matrices have been proposed in the literature. Reformulating the program decreases the effort required to find a search direction, but at the expense of increased problem size. Using transpose product formulations (e.g., A*A) works well but is highly problem dependent, Schur complements may require solutions with potentially near singular matrices. Explicit factorizations of the search direction matrices eliminate these problems while only requiring the solution to several small, independent linear systems. These systems may be distributed across multiple processors. Computational experience with these methods suggests that substantial performance improvements are possible with each method and that, generally, explicit factorizations require the least computational effort.	algorithm;algorithmic efficiency;analysis of algorithms;angularjs;art & architecture thesaurus;central processing unit;cholesky decomposition;column (database);columns;computation;instability;interior point method;iteration;linear programming;linear system;numerical analysis;numerical stability;sparse matrix;stochastic process	John R. Birge;Derek F. Holmes	1992	Comp. Opt. and Appl.	10.1007/BF00249637	stochastic programming;mathematical optimization;combinatorics;discrete mathematics;computer experiment;linear programming;interior point method;mathematics;schur complement;linear system	Theory	79.77132970445697	25.14508687185256	123659
5b43e29d1a8cdec8f413127b7c49bb2c843e8ce6	an inner convex approximation algorithm for bmi optimization and applications in control	concave programming;linear matrix inequalities approximation theory concave programming control system synthesis convergence feedback iterative methods;convergence;publikationer;compl e ib library inner convex approximation algorithm bmi optimization local optimization method nonconvex semidefinite programming problems nonconvex sdp problem inner positive semidefinite convex approximations parameterization technique iterative procedure bilinear matrix inequality constraints static output feedback control design numerical tests;konferensbidrag;approximation theory;iterative methods;feedback;control system synthesis;artiklar;rapporter;optimization approximation methods approximation algorithms programming symmetric matrices output feedback software algorithms;sista;linear matrix inequalities	In this work, we propose a new local optimization method to solve a class of nonconvex semidefinite programming (SDP) problems. The basic idea is to approximate the feasible set of the nonconvex SDP problem by inner positive semidefinite convex approximations via a parameterization technique. This leads to an iterative procedure to search a local optimum of the nonconvex problem. The convergence of the algorithm is analyzed under mild assumptions. Applications to optimization problems with bilinear matrix inequality (BMI) constraints in static output feedback control are benchmarked and numerical tests are implemented based on the data from the COMPLeib library.	approximation algorithm;bilinear filtering;block cipher mode of operation;brain–computer interface;feasible region;feedback;iterative method;local optimum;mathematical optimization;numerical analysis;semidefinite programming;social inequality	Dinh Quoc Tran;Wim Michiels;Sebastien Gros;Moritz Diehl	2012	2012 IEEE 51st IEEE Conference on Decision and Control (CDC)	10.1109/CDC.2012.6427102	mathematical optimization;conic optimization;combinatorics;discrete mathematics;convergence;second-order cone programming;linear matrix inequality;nonlinear programming;control theory;feedback;mathematics;iterative method;semidefinite programming;approximation theory	Robotics	72.69648737278263	24.316781941850486	123682
14dbd8a4967a1ec2acf532d8134804bb73326a4d	a tropical optimization problem with application to project scheduling with minimum makespan		We consider multidimensional optimization problems in the framework of tropical mathematics. The problems are formulated to minimize a nonlinear objective function that is defined on vectors over an idempotent semifield and calculated by means of multiplicative conjugate transposition. We start with an unconstrained problem and offer two complete direct solutions to demonstrate different practicable argumentation schemes. The first solution consists of the derivation of a sharp lower bound for the objective function and the solving of an equation to find all vectors that yield the bound. The second is based on extremal properties of the spectral radius of matrices and involves the evaluation of this radius for a certain matrix. This solution is then extended to problems with boundary constraints that specify the feasible solution set by a double inequality, and with a linear inequality constraint given by a matrix. To illustrate one application of the results obtained, we solve problems in project scheduling under the minimum makespan criterion subject to various precedence constraints on the time of initiation and completion of activities in the project. Simple numerical examples are given to show the computational technique used for solutions. Key-Words: constrained optimization problem; direct solution; idempotent semifield; tropical mathematics; project scheduling; minimum makespan MSC (2010): 65K10, 15A80, 65K05, 90C48, 90B35	computation;constrained optimization;constraint (mathematics);idempotence;linear inequality;makespan;mathematical optimization;nonlinear system;numerical analysis;optimization problem;scheduling (computing);social inequality	Nikolai Krivulin	2017	Annals OR	10.1007/s10479-015-1939-9	mathematical optimization;combinatorics;discrete mathematics;mathematics	AI	70.34741847257595	23.77619946570227	123831
d3764c8b9ab13410c6a655aae41a9c77cf8a1ff1	backward errors for the inverse eigenvalue problem	backward error estimation;eigenvalue problem;inegalite mirsky;probleme valeur propre;perturbation techniques;problema inverso;matriz simetrica;solution rapprochee;symmetric matrix;stability;estimation retro error;estimation erreur;inverse problem;error estimation;approximate solution;estimacion error;mirsky inequality;probleme valeur propre inverse;technique perturbation;matrice symetrique;backward error;stabilite;inverse eigenvalue problem;probleme inverse;estabilidad;problema valor propio	Backward errors for the symmetric matrix inverse eigenvalue problem with respect to an approximate solution are defined, and explicit expressions of the backward errors are derived. The expressions may be useful for testing the stability of practical algorithms.		Jiguang Sun	1999	Numerische Mathematik	10.1007/s002110050422	divide-and-conquer eigenvalue algorithm;mathematical optimization;stability;inverse problem;calculus;mathematics;statistics;symmetric matrix;algebra	Robotics	80.89653245840448	19.472073244871872	124025
1c63189cbaf987bd88dc34bd3551a7e0e916380f	equivalent conditions for jacobian nonsingularity in linear symmetric cone programming	conic programming;calculo de variaciones;regularite;programmation conique;matrice jacobi;non linear programming;regularidad;constraint nondegeneracy;analisis non regular;programacion no lineal;approximation algorithm;sistema no degenerado;coaccion;primal dual method;regularity;contrainte;karush kuhn tucker method;sous differentiel;karush kuhn tucker;programmation non lineaire;generalized jacobian;methode primale duale;subdifferential;nonsmooth analysis;optimisation combinatoire;methode karush kuhn tucker;jacobi matrix;systeme non degenere;calcul variationnel;constraint;programacion lineal;metodo primal dual;linear symmetric cone programming;strong regularity;matriz jacobi;variational analysis;regular variation;algoritmo aproximacion;linear programming;programmation lineaire;analyse non lisse;non degenerate system;algorithme approximation;combinatorial optimization;variational calculus;subdiferencial;programacion conica;nonsingularity;metodo karush kuhn tucker;optimizacion combinatoria	In this paper we consider the linear symmetric cone programming (SCP). At a KarushKuhn-Tucker (KKT) point of SCP, we present the important equivalent conditions for the nonsingularity of Clarke’s generalized Jacobian of the KKT nonsmooth system, such as primal and dual constraint nondegeneracy, the strong regularity, and the nonsingularity of the B-subdifferential of the KKT system. This affirmatively answers an open question by Chan and Sun [SIAM J. Optim. 19 (2008), pp. 370-396].	cone;conic optimization;edmund m. clarke;jacobian matrix and determinant;subderivative;tucker decomposition	Lingchen Kong;Levent Tunçel;Naihua Xiu	2011	J. Optimization Theory and Applications	10.1007/s10957-010-9758-2	jacobian matrix and determinant;subderivative;mathematical optimization;variational analysis;combinatorial optimization;nonlinear programming;linear programming;calculus;mathematics;geometry;constraint;karush–kuhn–tucker conditions;approximation algorithm;calculus of variations	Theory	74.392141535225	21.576106763902196	124041
355af05604bd5c5c1c40c0b3624b89803b56cc2e	solving conic systems via projection and rescaling	rescaling;symmetric cones;condition measure;conic systems;90c25;90c60;52a20;65f35	We propose a simple projection and rescaling algorithm to solve the feasibility problem find x ∈ L ∩ Ω, where L and Ω are respectively a linear subspace and the interior of a symmetric cone in a finite-dimensional vector space V . This projection and rescaling algorithm is inspired by previous work on rescaled versions of the perceptron algorithm and by Chubanov’s projectionbased method for linear feasibility problems. As in these predecessors, each main iteration of our algorithm contains two steps: a basic procedure and a rescaling step. When L ∩ Ω 6= ∅, the projection and rescaling algorithm finds a point x ∈ L∩Ω in at most O(log(1/δ(L∩Ω))) iterations, where δ(L∩Ω) ∈ (0, 1] is a measure of the most interior point in L ∩ Ω. The ideal value δ(L ∩ Ω) = 1 is attained when L ∩ Ω contains the center of the symmetric cone Ω. We describe several possible implementations for the basic procedure including a perceptron scheme and a smooth perceptron scheme. The perceptron scheme requires O(r) perceptron updates and the smooth perceptron scheme requires O(r) smooth perceptron updates, where r stands for the Jordan algebra rank of V .	algorithm;cone (formal languages);iteration;perceptron	Javier Peña;Negar Soheili	2017	Math. Program.	10.1007/s10107-016-1105-4	mathematical optimization;calculus;mathematics;geometry	AI	75.21785413231325	20.832776592229266	124290
acce5deede03531ed0364870318bebea89e5bfa4	optimization tools for solving equilibrium problems with nonsmooth data	mat 09 ricerca operativa;secs s 06 metodi matematici dell economia e delle scienze attuariali e finanziarie	The paper deals with the gap function approach for equilibrium problems with locally Lipschitz data. The gap function inherits the locally Lipschitz continuity of the data. Hence, the connections between its generalized directional derivatives, monotonicity conditions on the equilibrium bifunction and descent properties, can be analyzed. In turn, this analysis leads to devise two descent methods. Finally, the results of preliminary numerical tests are reported.		Giancarlo Bigi;Massimo Pappalardo;Mauro Passacantando	2016	J. Optimization Theory and Applications	10.1007/s10957-016-0974-2	mathematical optimization;calculus;mathematics;mathematical economics	Theory	72.63760993926118	21.055043349242013	124337
c6fb453f716d1e75f0377a39fb6c7a00ba01edeb	stabilities of cubic mappings in various normed spaces: direct and fixed point methods		In 1940 and 1964, Ulam proposed the general problem: “When is it true that by changing a little the hypotheses of a theorem one can still assert that the thesis of the theorem remains true or approximately true?”. In 1941, Hyers solved this stability problem for linear mappings. According to Gruber 1978 this kind of stability problems are of the particular interest in probability theory and in the case of functional equations of different types. In 1981, Skof was the first author to solve the Ulam problem for quadratic mappings. In 1982–2011, J. M. Rassias solved the above Ulam problem for linear and nonlinear mappings and established analogous stability problems even on restricted domains. The purpose of this paper is the generalized Hyers-Ulam stability for the following cubic functional equation: f mx y f mx − y mf x y mf x − y 2 m3 − m f x ,m ≥ 2 in various normed spaces.		Hassan Azadi Kenary;Hamid Rezaei;S. Talebzadeh;Sung Jin Lee	2012	J. Applied Mathematics	10.1155/2012/546819	mathematical optimization;mathematical analysis;discrete mathematics;topology;mathematics	Theory	69.79298126282758	18.800543830747923	124349
f51316705ede684fb16eca5e3652ac36e7acabd6	kernel-function based primal-dual algorithms for p  * ( κ) linear complementarity problems	kernel functions;interior point;central paths;fonction reguliere;approximation algorithm;primal dual method;kernel function;methode point interieur;methode primale duale;optimisation combinatoire;65k05;programacion lineal;metodo primal dual;metodo punto interior;mathematical programming;probleme complementarite lineaire;funcion nucleo;fonction noyau;algoritmo aproximacion;linear programming;programmation lineaire;90c33;probleme complementarite;problema complementariedad;large update;complementarity problem;funcion regular;primal dual algorithm;algorithme approximation;problema complementariedad lineal;combinatorial optimization;linear complementarity problem;interior point method;programmation mathematique;programacion matematica;small update;smooth function;optimizacion combinatoria	Recently, [Y.Q. Bai, M. El Ghami and C. Roos, SIAM J. Opt. 15 (2004) 101―128] investigated a new class of kernel functions which differs from the class of self-regular kernel functions. The class is defined by some simple conditions on the growth and the barrier behavior of the kernel function. In this paper we generalize the analysis presented in the above paper for P * (κ) Linear Complementarity Problems (LCPs). The analysis for LCPs deviates significantly from the analysis for linear optimization. Several new tools and techniques are derived in this paper.	complementarity (physics);complementarity theory	Mohamed El Ghami;Trond Steihaug	2010	RAIRO - Operations Research	10.1051/ro/2010014	kernel;mathematical optimization;combinatorial optimization;linear programming;interior point method;calculus;mathematics;approximation algorithm;algorithm	Theory	75.25556607949343	22.02146794776709	124402
ac61467bb470a2591582d9bebef55285ffb0a45b	a globally convergent filter method for nonlinear programming	nonlinear programming;filter methods;90c30;global convergence;65k05;49m37	In this paper we present a filter algorithm for nonlinear programming and prove its global convergence to stationary points. Each iteration is composed of a feasibility phase, which reduces a measure of infeasibility, and an optimality phase, which reduces the objective function in a tangential approximation of the feasible set. These two phases are totally independent, and the only coupling between them is provided by the filter. The method is independent of the internal algorithms used in each iteration, as long as these algorithms satisfy reasonable assumptions on their efficiency. Under standard hypotheses, we show two results: for a filter with minimum size, the algorithm generates a stationary accumulation point; for a slightly larger filter, all accumulation points are stationary.	approximation;feasible region;iteration;local convergence;loss function;nonlinear programming;nonlinear system;optimization problem;peterson's algorithm;stationary process;tree accumulation	Clóvis C. Gonzaga;Elizabeth W. Karas;Márcia Vanti	2004	SIAM Journal on Optimization	10.1137/S1052623401399320	adaptive filter;mathematical optimization;discrete mathematics;kernel adaptive filter;nonlinear programming;control theory;mathematics;filter design	ML	75.24042965629957	24.134042136373697	124410
1f756a982f78a5b07cc3f4550e10e96ca1e1d4fd	accuracy certificates for computational problems with convex structure	monotone operator;computation in convex structures;nash equilibrium;variational inequalities;cutting plane algorithm;saddle points;convex minimization;certificates;convex function;convex nash equilibrium;variational inequality;convexity;saddle point	The goal of the current paper is to introduce the notion of certificates which verify the accuracy of solutions of computational problems with convex structure; such problems include minimizing convex functions, variational inequalities with monotone operators, computing saddle points of convex-concave functions and solving convex Nash equilibrium problems. We demonstrate how the implementation of the Ellipsoid method and other cutting plane algorithms can be augmented with the computation of such certificates without essential increase of the computational effort. Further, we show that (computable) certificates exist whenever an algorithm is capable to produce solutions of guaranteed accuracy.	algorithm;calculus of variations;computable function;computation;computational problem;concave function;convex function;cutting-plane method;ellipsoid method;nash equilibrium;public key certificate;variational inequality;monotone	Arkadi Nemirovski;Shmuel Onn;Uriel G. Rothblum	2010	Math. Oper. Res.	10.1287/moor.1090.0427	convex analysis;subderivative;support function;mathematical optimization;conic optimization;combinatorics;convex optimization;convex polytope;variational inequality;convex combination;ellipsoid method;convex body;linear matrix inequality;convex conjugate;quasiconvex function;convex hull;absolutely convex set;convexity in economics;mathematics;saddle point;convex set;mathematical economics;effective domain;proper convex function;choquet theory	ML	73.2056094715848	22.40404756724906	124443
2b01b54d7cd9de40516a44660f505b45209ee73f	a derivative-free algorithm for bound constrained optimization	bound constrained optimization;bound constraints;linesearch technique;global convergence;derivative free algorithm;objective function;numerical experiment	In this work, we propose a new globally convergent derivative-free algorithm for the minimization of a continuously differentiable function in the case that some of (or all) the variables are bounded. This algorithm investigates the local behaviour of the objective function on the feasible set by sampling it along the coordinate directions. Whenever a “suitable” descent feasible coordinate direction is detected a new point is produced by performing a linesearch along this direction. The information progressively obtained during the iterates of the algorithm can be used to build an approximation model of the objective function. The minimum of such a model is accepted if it produces an improvement of the objective function value. We also derive a bound for the limit accuracy of the algorithm in the minimization of noisy functions. Finally, we report the results of a preliminary numerical experience.	algorithm;constrained optimization;program optimization	Stefano Lucidi;Marco Sciandrone	2002	Comp. Opt. and Appl.	10.1023/A:1013735414984	coordinate descent;mathematical optimization;combinatorics;machine learning;mathematics	EDA	75.00486066449072	24.52004051582218	124451
a3c758cd2b93b3a6c0282cbdac627aabfd9b0fa7	dual greedy algorithm for conic optimization problem		In the paper we propose an algorithm for nding approximate sparse solutions of convex optimization problem with conic constraints and examine convergence properties of the algorithm with application to the index tracking problem and unconstrained l1-penalized regression.	approximation algorithm;computation;conic optimization;convex function;convex optimization;greedy algorithm;lagrange multiplier;mathematical optimization;optimization problem;rate of convergence;sparse matrix	Sergei P. Sidorov;Sergei V. Mironov;Michael Pleshakov	2016			greedy randomized adaptive search procedure;conic optimization;greedy algorithm;mathematics;mathematical optimization	ML	73.80436588967811	24.193136498388167	124491
162fdc85ec5763ec8b92f924d1a7bff043d7e88f	an indefinite variant of lobpcg for definite matrix pencils	lobpcg;eigenvalue;minimization principle;definite matrix pencil	In this paper, we propose a novel preconditioned solver for generalized Hermitian eigenvalue problems. More specifically, we address the case of a definite matrix pencil A − λ B $A-\lambda B$ , that is, A, B are Hermitian and there is a shift λ 0 $\lambda _{0}$ such that A − λ 0 B $A-\lambda _{0} B$ is definite. Our new method can be seen as a variant of the popular LOBPCG method operating in an indefinite inner product. It also turns out to be a generalization of the recently proposed LOBP4DCG method by Bai and Li for solving product eigenvalue problems. Several numerical experiments demonstrate the effectiveness of our method for addressing certain product and quadratic eigenvalue problems.	experiment;kernel (linear algebra);lobpcg;numerical analysis;preconditioner;semiconductor industry;solver	Daniel Kressner;Marija Miloloza Pandur;Meiyue Shao	2013	Numerical Algorithms	10.1007/s11075-013-9754-3	mathematical optimization;combinatorics;eigenvalues and eigenvectors;mathematics;quantum mechanics;algebra	ML	80.34959098338689	22.56374099100917	124513
c9388e0c8bf9123d1cc9ac0cbcfc3860c04bc74a	a fractional order collocation method for second kind volterra integral equations with weakly singular kernels		In this paper, we develop a fractional order spectral collocationmethod for solving second kind Volterra integral equations with weakly singular kernels. It is well known that the original solution of second kind Volterra integral equations with weakly singular kernels usually can be split into two parts, the first is the singular part and the second is the smooth part with the assumption that the integer m being its smooth order. On the basis of this characteristic of the solution, we first choose the fractional order Lagrange interpolation function of Chebyshev type as the basis of the approximate space in the collocation method, and then construct a simple quadrature rule to obtain a fully discrete linear system. Consequently, with the help of the Lagrange interpolation approximate theory we establish that the fully discrete approximate equation has a unique solution for sufficiently large n, where n + 1 denotes the dimension of the approximate space. Moreover, we prove that the approximate solution arrives at an optimal convergence orderO(n−m log n) in the infinite norm andO(n−m) in the weighted square norm. In addition, we prove that for sufficiently large n, the infinity-norm condition number of the coefficient matrix corresponding to the linear system is O(log2 n) and its spectral condition number isO(1). Numerical examples are presented to demonstrate the effectiveness of the proposed method.	approximation algorithm;coefficient;collocation method;condition number;euler–lagrange equation;interpolation;lagrange polynomial;linear system;navier–stokes equations;numerical method	Haotao Cai;Yanping Chen	2018	J. Sci. Comput.	10.1007/s10915-017-0568-7	mathematical optimization;mathematical analysis;mathematics;discrete mathematics;collocation method;condition number;lagrange polynomial;linear system;coefficient matrix;volterra integral equation;integer;gaussian quadrature	Theory	80.00786603458923	18.418139053171405	124664
3718edb2d7cede5d91720b3c087d56c6faf1636d	global convergence property of the affine scaling methods for primal degenerate linear programming problems	interior point methods;convergence;algorithme karmarkar;degeneracion;karmarkar s method;duality;methode point interieur;affine scaling method;dualite;convergencia;programacion lineal;global convergence property;degeneration;mathematical programming;linear programming;degenerate problems;programmation lineaire;dualidad;interior point method;programmation mathematique;programacion matematica;degenerescence;karmarkar algorithm	In this paper we investigate the global convergence property of the affine scaling method under the assumption of dual nondegeneracy. The behavior of the method near degenerate vertices is analyzed in detail on the basis of the equivalence between the affine scaling methods for homogeneous LP problems and Karmarkar's method. It is shown that the step-size 1/8, where the displacement vector is normalized with respect to the distance in the scaled space, is sufficient to guarantee the global convergence for dual nondegenerate LP problems. The result can be regarded as a counterpart to Dikin's global convergence result on the affine scaling method assuming primal nondegeneracy.	affine scaling;displacement mapping;image scaling;linear programming;local convergence;turing completeness	Takashi Tsuchiya	1992	Math. Oper. Res.	10.1287/moor.17.3.527	mathematical optimization;mathematical analysis;linear programming;interior point method;affine hull;affine transformation;mathematics;geometry;affine shape adaptation;affine combination	Theory	74.38759179129188	21.742601489106665	124802
6e426996378b02399d795fe2db593520803b707b	inverse subspace problems with applications	modified singular value decomposition;matrix nearness problem;arnoldi method;lanczos method;ill posed problem;lanczos bidiagonalization;blurring matrix	Given a square matrix A, the inverse subspace problem is concerned with determining a closest matrix to A with a prescribed invariant subspace. When A is Hermitian, the closest matrix may be required to be Hermitian. We measure distance in the Frobenius norm and discuss applications to Krylov subspace methods for the solution of large-scale linear systems of equations and eigenvalue problems as well as to the construction of blurring matrices. Extensions that allow the matrix A to be rectangular and applications to Lanczos bidiagonalization, as well as to the recently proposed subspace-restricted singular value decomposition method for the solution of linear discrete ill-posed problems, also are considered.	arnoldi iteration;bidiagonalization;competitive analysis (online algorithm);deblurring;iterative method;krylov subspace;lanczos resampling;linear system;matrix multiplication;singular value decomposition;system of linear equations;the matrix;well-posed problem	Silvia Noschese;Lothar Reichel	2014	Numerical Lin. Alg. with Applic.	10.1002/nla.1914	mathematical optimization;mathematical analysis;lanczos algorithm;krylov subspace;calculus;generalized minimal residual method;mathematics;arnoldi iteration	ML	80.98209248457738	21.855012509739133	125187
61e3696df694951a437c25daa1b56a69b02b9f76	nonsmooth spectral gradient methods for unconstrained optimization		To solve nonsmooth unconstrained minimization problems, we combine the spectral choice of step length with two well-established subdifferential-type schemes: the gradient sampling method and the simplex gradient method. We focus on the interesting case in which the objective function is continuously differentiable almost everywhere, and it is often not differentiable at minimizers. In the case of the gradient sampling method, we also present a simple differentiability test that allows us to use the exact gradient direction as frequently as possible, and to build a stochastic subdifferential direction only if the test fails. The proposed spectral gradient sampling method is combined with a monotone line search globalization strategy. On the other hand, the simplex gradient method is a direct search method that only requires function evaluations to build an approximation to the gradient direction. In this case, the proposed spectral simplex gradient method is combined with a suitable nonmonotone line search strategy. For both scenarios, we discuss convergence properties and present numerical results on a set of nonsmooth test functions. These numerical results indicate that using a spectral step length can improve the practical performance of both methods.	gradient;mathematical optimization	Milagros Loreto;Hugo Aponte;Debora Cores;Marcos Raydan	2017	EURO J. Computational Optimization	10.1007/s13675-017-0080-8	gradient descent;pattern search;mathematical optimization;mathematical analysis;random search;gradient method;backpropagation;calculus;mathematics;nonlinear conjugate gradient method	Vision	75.43357403329964	24.03085883367193	125196
b84862cd24038659e20b2544465a54cfe1391536	global optimization for sum of geometric fractional functions	optimal solution;calculo de variaciones;global solution;analisis numerico;solution optimale;fonction geometrique;matematicas aplicadas;mathematiques appliquees;49j30;30cxx;branch and bound algorithm;65kxx;optimization method;65k10;metodo optimizacion;geometric function;analyse numerique;linear relaxation;algorithme;49xx;algorithm;calcul variationnel;sum of ratios;numerical analysis;programacion lineal;mathematical programming;solucion optima;methode optimisation;linear programming;programmation lineaire;linear program;global optimization;solution globale;49k30;applied mathematics;geometric constraints;15404;fractional function;branch and bound;programmation mathematique;equivalent transformation;solucion global;programacion matematica;variational calculus;algoritmo	This paper presents an efficient branch and bound algorithm for globally solving sum of geometric fractional functions under geometric constraints, which arise in various practical problems. By using an equivalent transformation and a new linear relaxation technique, a linear relaxation programming problem of the equivalent problem is obtained. The proposed algorithm is convergent to the global optimal solution by means of the subsequent solutions of a series of linear programming problems. Numerical results are reported to show the feasibility of our algorithm.	global optimization;mathematical optimization	Chun-Feng Wang;San-Yang Liu;Pei-Ping Shen	2010	Applied Mathematics and Computation	10.1016/j.amc.2010.03.061	mathematical optimization;criss-cross algorithm;linear programming;calculus;mathematics;branch and bound;algorithm	Theory	75.39720062316283	22.39891819626098	125310
40a4736f84f2341e01a8ec90cd002bf4c8b30877	perturbation analysis of fuzzy linear systems	decomposition;numbers;right hand side;fuzzy linear systems;linear system;relative error;solving systems;perturbation analysis;spectral norm	This paper deals with the perturbation analysis of fuzzy linear systems. Three cases of perturbation are considered: (a) the right hand side is perturbed while the coefficient matrix remains unchanged; (b) the coefficient matrix is perturbed while the right hand side remains unchanged, and (c) both the coefficient matrix and the right hand side are perturbed. For all of the three cases, the respective relative error bounds for solutions of fuzzy linear system are derived. The results are illustrated by numerical examples.	fuzzy logic;linear system;perturbation theory	Zengfeng Tian;Liangjian Hu;David Greenhalgh	2010	Inf. Sci.	10.1016/j.ins.2010.07.018	mathematical optimization;approximation error;matrix norm;coefficient matrix;calculus;perturbation theory;control theory;mathematics;linear system;decomposition	Logic	79.47149751838403	21.32559596006085	125332
a2fc64a9fe8d392df833829a6a0ad1f48ee41185	on borwein--wiersma decompositions of monotone linear relations	monotone operator;adjoint;asplund decomposition;symmetric operator;linear relation;47b25;hilbert space;symmetric matrices;linear operator;functional analysis;convex function;borwein wiersma decomposition;irreducible operator;90c25;47h05;subdifferential operator;maximal monotone operator;47a06;skew operator	Monotone operators are of basic importance in optimization as they generalize simultaneously subdifferential operators of convex functions and positive semidefinite (not necessarily symmetric) matrices. In 1970, Asplund studied the additive decomposition of a maximal monotone operator as the sum of a subdifferential operator and an “irreducible” monotone operator. In 2007, Borwein and Wiersma [SIAM J. Optim. 18 (2007), pp. 946–960] introduced another additive decomposition, where the maximal monotone operator is written as the sum of a subdifferential operator and a “skew” monotone operator. Both decompositions are variants of the well-known additive decomposition of a matrix via its symmetric and skew part. This paper presents a detailed study of the Borwein-Wiersma decomposition of a maximal monotone linear relation. We give sufficient conditions and characterizations for a maximal monotone linear relation to be Borwein-Wiersma decomposable, and show that Borwein-Wiersma decomposability implies Asplund decomposability. We exhibit irreducible linear maximal monotone operators without full domain, thus answering one of the questions raised by Borwein and Wiersma. The Borwein-Wiersma decomposition of any maximal monotone linear relation is made quite explicit in Hilbert space. 2000 Mathematics Subject Classification: Primary 47H05; Secondary 47B25, 47A06, 90C25.	additive state decomposition;convex function;hilbert space;irreducibility;mathematical optimization;mathematics subject classification;maximal set;siam journal on scientific computing;subderivative;utility functions on indivisible goods;whole earth 'lectronic link;monotone	Heinz H. Bauschke;Xianfu Wang;Liangjin Yao	2010	SIAM Journal on Optimization	10.1137/09078016X	bernstein's theorem on monotone functions;functional analysis;convex function;mathematical optimization;pseudo-monotone operator;mathematical analysis;discrete mathematics;strongly monotone;mathematics;linear map;symmetric matrix;hilbert space	Theory	72.17070920344152	18.957697536374017	125363
53540283807233710e9a517ee42c1cb26b92aa49	algebraic multigrid and algebraic multilevel methods: a theoretical comparison	c b s constant;preconditioning;theoretical analysis;algebraic method;algebraic multigrid;iterative solution;multilevel;multilevel method;sparse linear system;lower bound	We consider algebraic methods of the two-level type for the iterative solution of large sparse linear systems. We assume that a ne/coarse partitioning and an algebraic interpolation have been de ned in one way or another, and review di erent schemes that may be built with these ingredients. This includes algebraic multigrid (AMG) schemes, two-level approximate block factorizations, and several methods that exploit generalized hierarchical bases. We develop their theoretical analysis in a uni ed way, gathering some known results, rewriting some other and stating some new. This includes lower bounds, that is, we do not only investigate su cient conditions of convergence, but also look at necessary conditions. Copyright ? 2005 John Wiley & Sons, Ltd.	algebraic riccati equation;algebraic equation;approximation algorithm;interpolation;iterative method;john d. wiley;linear algebra;linear system;multigrid method;ne (complexity);rewriting;sparse matrix	Yvan Notay	2005	Numerical Lin. Alg. with Applic.	10.1002/nla.435	mathematical optimization;combinatorics;singular point of an algebraic variety;theoretical computer science;dimension of an algebraic variety;a¹ homotopy theory;mathematics;preconditioner;real algebraic geometry;algebraic extension;upper and lower bounds;function field of an algebraic variety;algebraic function;differential algebraic geometry;multigrid method;algebra	Theory	82.44212595826409	22.320373972237945	125460
95d9e7e981e2df974dd56c464adf49e63ed7f296	sphere methods for lp		The sphere method for solving linear programs operates with only a subset of constraints in the model in each iteration, and thus has the advantage of handling instances which may not be very sparse. In this paper we discuss enhancements, and improved versions Sphere Methods 2, 2.1 which improve performance by 20 to 80%. Additional steps that can improve performance even more are also presented.	coefficient;computation;experiment;high- and low-level;iteration;language code;linear programming;matlab;numerical analysis;numerical linear algebra;simplex algorithm;sparse matrix;the matrix	Katta G. Murty;Mohammad R. Oskoorouchi	2010	Algorithmic Operations Research		mathematical optimization;combinatorics;mathematics;geometry;algorithm	Robotics	76.0174702598874	25.26324939223119	125583
8e0e4035fdcd0eb76c45e1d278070b1e26771bd2	further investigation on the relaxed hybrid steepest-descent methods for variational inequalities with k-strict pseudocontractions		We modify the relaxed hybrid steepest-descent methods to the case of variational inequality for finding a solution over the set of common fixed points of a finite family of strictly pseudocontractive mappings. The strongly monotone property defined on cost operator was extended to relaxed cocoercive in convergence analysis. Results presented in this paper may be viewed as a refinement and important generalizations of the previously known results announced by many other authors.	descent;variational inequality;variational principle	Qian-Fen Gong;Dao-Jun Wen	2014	J. Applied Mathematics	10.1155/2014/381592	mathematical optimization;mathematical analysis;discrete mathematics;mathematics	AI	73.52439718647902	22.40481489923965	125667
16c7064e95840ab8c96783e49bf31734a1d28e34	multiple penalized principal curves: analysis and computation	principal curves;geometry of data;curve fitting;49m25;65d10;62g99;65d18;65k10;49q20	We study the problem of determining the one-dimensional structure that best represents a given data set. More precisely, we take a variational approach to approximating a given measure (data) by curves. We consider an objective functional whose minimizers are a regularization of principal curves and introduce a new functional which allows for multiple curves. We prove existence of minimizers and investigate their properties. While both of the functionals used are non-convex, we show that enlarging the configuration space to allow for multiple curves leads to a simpler energy landscape with fewer undesirable (high-energy) local minima. We provide an efficient algorithm for approximating minimizers of the functional and demonstrate its performance on real and synthetic data. The numerical examples illustrate the effectiveness of the proposed approach in the presence of substantial noise, and the viability of the algorithm for high-dimensional data.	approximation algorithm;calculus of variations;computation;matrix regularization;maxima and minima;numerical analysis;synthetic data	Slav Kirov;Dejan Slepcev	2017	Journal of Mathematical Imaging and Vision	10.1007/s10851-017-0730-8	mathematical optimization;combinatorics;mathematical analysis;topology;mathematics;geometry;family of curves	ML	75.98118663880183	20.219479194758126	126219
4d02ced8b5d05d283539500b1ec30550c9cf1e2b	parametric ekeland's variational principle	equilibrio nash;minimax problem;principio variacional;nash equilibrium;problema minimax;ekeland s variational principle;continuous selection;probleme minimax;equilibre nash;principe variationnel;variational principle	A parametrized version of Ekeland's variational principle is proved, showing that under suitable conditions, the minimum point of the perturbed function can be chosen to depend continuously on a parameter. Applications of this result are given.	calculus of variations;ekeland's variational principle;ivar ekeland	Pando G. Georgiev	2001	Appl. Math. Lett.	10.1016/S0893-9659(01)80028-8	mathematical optimization;variational principle;calculus;mathematics;mathematical economics;nash equilibrium;quantum mechanics	ML	72.63449745119138	20.353692082881317	126253
a2d3168fbfe525a77ffdcc9544b1bfdd3bae15e0	numerical methods for computing svd in the d-orthogonal group	hypemormal matrices;numerical method;singular value decomposition;quadratic groups;positive definite matrix;algebraic method;hypernormal matrices;qr decomposition;orthogonal group	In this paper, we consider the problem to compute a special kind of singular value decomposition of a square matrix A = UΣV , whereU andV belong to the same D-orthogonal group, i.e. they are orthogonal with respect to a real diagonal orthogonal matrix D, while Σ is a real diagonal positive definite matrix. In this work, we propose an algebraic method and derive a continuous approach using the projected gradient technique. The differential systems given by the continuous approach are solved using a s lic Q ©	gradient;linear algebra;numerical analysis;singular value decomposition	Tiziano Politi;Alessandro Pugliese	2006	Future Generation Comp. Syst.	10.1016/j.future.2004.11.025	mathematical optimization;eigendecomposition of a matrix;orthogonal matrix;rotation matrix;schur decomposition;orthogonal group;lu decomposition;numerical analysis;skew-symmetric matrix;sylvester's law of inertia;square matrix;orthogonal procrustes problem;positive-definite matrix;matrix decomposition;singular value decomposition;diagonal matrix;qr decomposition;matrix;symmetric matrix;principal component analysis	ML	80.07674248961389	21.110445437831363	126460
2626d008ce6abf1826ede84ca71e140d05327a56	asymptotic convergence of an inertial proximal method for unconstrained quasiconvex minimization	second order;convergence theorem;convergence analysis;critical point;inertial algorithm;49m45;over relaxation;hilbert space;objective function;65c25;proximal algorithm;quasiconvex minimization	This paper deals with the convergence analysis of a second order proximal method for approaching critical points of a smooth and quasiconvex objective function defined on a real Hilbert space. The considered method, well-known in the convex case, unifies proximal method, relaxation and inertial-type extrapolation. The convergence theorems established in this new setting improve recent ones.	quasiconvex function	Paul-Emile Maingé	2009	J. Global Optimization	10.1007/s10898-008-9388-5	mathematical optimization;mathematical analysis;topology;quasiconvex function;modes of convergence;compact convergence;mathematics;convergence tests;weak convergence;critical point;second-order logic;hilbert space	Vision	73.78187703877511	22.762353044933192	126710
cd63d978209f63bd16f2e5a5816d4d9ff17c9ad0	improved optimal conditions and iterative parameters for the optimal control problems with an integral constraint in square	convergence;spectral method;integral constraint;optimal control problem	A distributed optimal control problem is considered with an inequality constraint on the state variable. And the constraint reads that the integral of the state is not more than a given positive constant. An efficient approach is introduced to investigate optimality conditions of this problem. Based on the Uzawa algorithm, an efficient algorithm is designed and its convergence is discussed with details. Especially, the bounds of two iterative parameters are investigated. With the Legendre-Galerkin spectral method, numerical results show that the algorithm is highly feasible.	algorithm;galerkin method;iteration;iterative method;karush–kuhn–tucker conditions;numerical analysis;numerical method;optimal control;social inequality;spectral method	Jianwei Zhou	2016	J. Computational Applied Mathematics	10.1016/j.cam.2015.12.014	constraint logic programming;mathematical optimization;combinatorics;mathematical analysis;binary constraint;convergence;constraint satisfaction dual problem;mathematics;difference-map algorithm;hybrid algorithm;spectral method	AI	76.25597475783437	22.881017429918685	127088
edac6df910fcf86666a43738db04ccb64448e332	perturbation analysis for block downdating of the generalized cholesky factorization	block downdating;rigorous perturbation bound;generalized cholesky factorization;first order perturbation bound;perturbation analysis	The generalized Cholesky factorization is a generalization of the classical Cholesky factorization and its block downdating problem means finding the downdated generalized Cholesky factorization when a matrix XXTXXT is subtracted from the original matrix, where X is full column rank. In this paper, we consider the perturbation analysis of this problem. Some first order perturbation bounds are first obtained using the refined matrix equation approach and the matrix–vector equation approach. These results generalize the corresponding ones for the block downdating problem of the classical Cholesky factorization. Then, the rigorous perturbation bounds are also obtained using the combination of the classical and refined matrix equation approaches. Each of these bounds is composed of a small constant multiple of the first order term of the corresponding first order perturbation bound and an additional second order term.	cholesky decomposition;perturbation theory	Hanyu Li;Hu Yang;Hua Shao	2012	Applied Mathematics and Computation	10.1016/j.amc.2012.03.034	mathematical optimization;combinatorics;mathematical analysis;incomplete cholesky factorization;perturbation theory;mathematics;minimum degree algorithm;cholesky decomposition;quantum mechanics	Logic	81.90107803802485	22.233929724481598	127189
c8e7a327e8a6acd95158fff157bb6ea809a88d7d	a class of p-matrices with applications to the localization of the eigenvalues of a real matrix	15a18;sign regular matrix;p matrix;grupo de excelencia;symmetric matrix;eigenvalues localization;15a48;ciencias basicas y experimentales;matematicas;15a42;tecnologias generalidades;tecnologias;65f15;gerschgorin circles	A matrix with positive row sums and all its off-diagonal elements bounded above by their corresponding row means is called a B-matrix. It is proved that the class of B-matrices is a subset of the class of P-matrices. Properties of B-matrices are used to localize the real eigenvalues of a real matrix and the real parts of all eigenvalues of a real matrix.		Juan M. Peña	2001	SIAM J. Matrix Analysis Applications	10.1137/S0895479800370342	matrix analysis;matrix function;hollow matrix;combinatorics;eigendecomposition of a matrix;spectrum of a matrix;nonnegative matrix;single-entry matrix;centrosymmetric matrix;sylvester's law of inertia;matrix of ones;square matrix;mathematics;geometry;diagonalizable matrix;positive-definite matrix;augmented matrix;square root of a 2 by 2 matrix;matrix differential equation;matrix;integer matrix;symmetric matrix;algebra;involutory matrix	Theory	78.71677084220288	20.05012972844742	127431
262dc92a1776aedbfeb8e042f92d7a2794a8a669	on deflation and multiplicity structure	deflation;multiplicity structure;multiplication matrix;newton s method;inverse system	This paper presents two new constructions related to singular solutions of polynomial systems. The first is a new deflation method for an isolated singular root. This construction uses a single linear differential form defined from the Jacobian matrix of the input, and defines the deflated system by applying this differential form to the original system. The advantages of this new deflation is that it does not introduce new variables and the increase in the number of equations is linear in each iteration instead of the quadratic increase of previous methods. The second construction gives the coefficients of the so-called inverse system or dual basis, which defines the multiplicity structure at the singular root. We present a system of equations in the original variables plus a relatively small number of new variables that completely deflates the root in one step. We show that the isolated simple solutions of this new system correspond to roots of the original system with given multiplicity structure up to a given order. Both constructions are “exact” in that they permit one to treat all conjugate roots simultaneously and can be used in certification procedures for singular roots and their multiplicity structure with respect to an exact rational polynomial system.	coefficient;iteration;jacobian matrix and determinant;system of polynomial equations	Jonathan D. Hauenstein;Bernard Mourrain;Ágnes Szántó	2017	J. Symb. Comput.	10.1016/j.jsc.2016.11.013	deflation;inverse system;mathematical optimization;mathematical analysis;calculus;mathematics;newton's method;algebra	Theory	80.44162984864973	20.963156419842853	127438
41102c866739646cefdd112f063805b7c9b56c18	the linear convergence of a merit function method for nonlinear complementarity problems		Based on a family of generalized merit functions, a merit function method for solving nonlinear complementarity problems was proposed by Lu, Huang and Hu [Properties of a family of merit functions and a merit function method for the NCP, Appl. Math.– J. Chinese Univ., 2010, 25: 379–390], where, the global convergence of the method was proved. However, no the result on the convergence rate of the method was reported. In this short paper, we show that the method proposed in the above paper is globally linearly convergent under suitable assumptions.	complementarity theory;mixed complementarity problem;nonlinear programming;rate of convergence	Xiaoqin Jiang;Liyong Lu	2012		10.1007/978-3-642-34289-9_56	mathematical optimization;calculus;mixed complementarity problem;mathematical economics;complementarity theory	Robotics	76.91291163318154	22.14099969693221	127501
b75e387f88088ea8f60ce05edc6e1340a6f4861c	some convergence properties of the conjugate gradient method	rate of convergence;conjugate gradient method;objective function	It has been conjectured that the conjugate gradient method for minimizing functions of several variables has a superlinear rate of convergence, but Crowder and Wolfe show by example that the conjecture is false. Now the stronger result is given that, if the objective function is a convex quadratic and if the initial search direction is an arbitrary downhill direction, then either termination occurs or the rate of convergence is only linear, the second possibility being more usual. Relations between the starting point and the initial search direction that are necessary and sufficient for termination in the quadratic case are studied.	conjugate gradient method	M. J. D. Powell	1976	Math. Program.	10.1007/BF01580369	mathematical optimization;mathematical analysis;harmonic conjugate;calculus;mathematics;convergence tests;conjugate gradient method;nonlinear conjugate gradient method;rate of convergence	Graphics	75.4113393888292	23.573099692710144	127886
a1422f58c87b059bfb5c14b6e211cea561d9dd50	vpastab: stabilised vector-padé approximation with application to linear systems	linear system;fixed point;generating function;numerical experiment;linear equations	An algorithm called VPAStab is given for the acceleration of convergence of a sequence of vectors. It combines a method of vector-Padé approximation with a successful technique for stabilisation. More generally, this algorithm is designed to find the fixed point of the generating function of the given sequence of vectors, analogously to the way in which ordinary Padé approximants can accelerate the convergence of a given scalar sequence. VPAStab is justified in the context of its application to the solution of a large sparse system of linear equations. The possible breakdowns of the algorithm are listed. Numerical experiments indicate that these breakdowns can be classified either as pivot-type (type L) or as ghost-type (type D).	algorithm;approximation;experiment;fixed point (mathematics);linear equation;numerical method;padé approximant;series acceleration;sparse matrix;system of linear equations	Peter R. Graves-Morris	2003	Numerical Algorithms	10.1023/A:1025532525878	mathematical optimization;generating function;mathematical analysis;discrete mathematics;calculus;control theory;mathematics;fixed point;linear equation;linear system;algebra	ML	80.83860732313327	20.97352961592435	128048
ad05c5b2a627c459b45ace016a40a5cda8dd6a4a	generalized system for relaxed cocoercive mixed variational inequalities in hilbert spaces	espace hilbert;analisis numerico;convergence of resolvent method;convergence;47a10;matematicas aplicadas;desigualdad variacional;resolvent;espacio hilbert;mathematiques appliquees;inequality;inegalite variationnelle;46cxx;58e35;resolvent method;analyse numerique;relaxed cocoercive mixed variational inequality;hilbert space;49j40;convergencia;resolvente;numerical analysis;relaxed cocoercive mapping;relaxed cocoercive mixed variational;variational inequality;resolvante;applied mathematics	The approximate solvability of a generalized system for relaxed cocoercive mixed variational inequality is studied by using the resolvent operator technique. The results presented in this paper are more general and include many previously known results as special cases.	calculus of variations;hilbert space;variational inequality	Zhenhua He;Feng Gu	2009	Applied Mathematics and Computation	10.1016/j.amc.2009.03.056	mathematical optimization;mathematical analysis;variational inequality;topology;convergence;numerical analysis;inequality;mathematics;algebra;resolvent;hilbert space	Robotics	73.60456309251636	20.68033674852949	128112
1a296a1577478654a54a9f801f93f71b7d853c53	an incremental gradient(-projection) method with momentum term and adaptive stepsize rule	convergence analysis;49m07;nonlinear neural network training;gradient method;level set;90c30;constrained minimization;backpropagation;lipschitz continuity;gradient projection;numerical experiment;gradient projection method;49m37;neural network;incremental gradient method	We consider an incremental gradient method with momentum term for minimizing the sum of continuously differentiable functions. This method uses a new adaptive stepsize rule that decreases the stepsize whenever sufficient progress is not made. We show that if the gradients of the functions are bounded and Lipschitz continuous over a certain level set, then every cluster point of the iterates generated by the method is a stationary point. In addition, if the gradient of the functions have a certain growth property, then the method is either linearly convergent in some sense or the stepsizes are bounded away from zero. The new stepsize rule is much in the spirit of heuristic learning rules used in practice for training neural networks via backpropagation. As such, the new stepsize rule may suggest improvements on existing learning rules. Finally, extension of the method and the convergence results to constrained minimization is discussed, as are some implementation issues and numerical experience.	adaptive stepsize;artificial neural network;backpropagation;gradient method;heuristic;numerical analysis;stationary process;stochastic gradient descent	Paul Tseng	1998	SIAM Journal on Optimization	10.1137/S1052623495294797	gradient descent;mathematical optimization;discrete mathematics;adaptive stepsize;delta rule;gradient method;level set;backpropagation;mathematics;lipschitz continuity;artificial neural network	ML	75.61881009049786	23.661693954704106	128119
b6faca6916ab763de000ca1510ba7d76487019b7	improved set-membership partial-update pseudo affine projection algorithm	identification problem;set membership filtering;adaptive filtering;partial update	In this paper, an improved set-membership partial-update pseudo affine projection (I-SM-PUPAP) algorithm is presented. An approximation that leads to solving a linear system with a direct method is used. It is proved that I-SM-PUPAP algorithm has a much lower numerical complexity and memory requirements than recently proposed I-SM-PUAP algorithm. Simulation results identify an inherent compromise between the convergence rate, complexity reduction and the number of updates.	adaptive filter;algorithm;algorithmic efficiency;approximation;direct method in the calculus of variations;linear system;numerical analysis;numerical linear algebra;rate of convergence;reduction (complexity);requirement;simulation;system identification	Felix Albu;Paulo S. R. Diniz	2016	2016 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2016.7732092	adaptive filter;mathematical optimization;combinatorics;discrete mathematics;parameter identification problem;computer science;mathematics	EDA	81.59222267020347	22.110602099169206	128266
540399a903fa1a543eb0dfcc7245950ac1b8a8d9	strict l∞ isotonic regression	nonparametric;isotonic regression;supremum norm;mini max;shape constrained optimization	Given a function f and weights w on the vertices of a directed acyclic graph G, an isotonic regression of (f, w) is an order-preserving real-valued function that minimizes the weighted distance to f among all order-preserving functions. When the distance is given via the supremum norm there may be many isotonic regressions. One of special interest is the strict isotonic regression, which is the limit of p-norm isotonic regression as p approaches infinity. Algorithms for determining it are given. We also examine previous isotonic regression algorithms in terms of their behavior as mappings from weighted functions over G to isotonic functions over G, showing that the fastest algorithms are not monotonic mappings. In contrast, the strict isotonic regression is monotonic.	algorithm;directed acyclic graph;fastest;isotonic regression	Quentin F. Stout	2012	J. Optimization Theory and Applications	10.1007/s10957-011-9865-8	nonparametric statistics;mathematical optimization;mathematical analysis;discrete mathematics;uniform norm;isotonic regression;mathematics	ML	68.47376778150326	21.50446542767987	128401
2e84997b8262d9d1882496e14742a4fa0071dda5	new optimality conditions for nonsmooth control problems	nonsmooth optimal control;90c26;optimality conditions;artigo;49k15;generalized invexity	This work considers nonsmooth optimal control problems and provides two new sufficient conditions of optimality. The first condition involves the Lagrange multipliers while the second does not. We show that under the first new condition all processes satisfying the Pontryagin Maximum Principle (called MP-processes) are optimal. Conversely, we prove that optimal control problems in which every MP-process is optimal necessarily obey our first optimality condition. The second condition is more natural, but it is only applicable to normal problems and the converse holds just for smooth problems. Nevertheless, it is proved that for the class of normal smooth optimal control problems the two conditions are equivalent. Some examples illustrating the features of these sufficient concepts are presented.	closing (morphology);edmund m. clarke;gradient;lagrange multiplier;loss function;mp/m;mathematical optimization;optimal control;pontryagin's maximum principle;scott continuity;turing completeness	Valeriano Antunes de Oliveira;Geraldo Nunes Silva	2013	J. Global Optimization	10.1007/s10898-012-0003-4	mathematical optimization;mathematical analysis;control theory;mathematics	ML	72.0766661246408	20.506470371250725	128436
27767664c4ac51efed4623298fb491238dfb3c8b	semidefinite programming relaxations for the graph partitioning problem	preconditioned conjugate gradient method;interior point;semidefinite programming relaxations;linear system;graph partitioning;positive semidefinite matrices;15a48;90c27;lagrangian relaxations;90c25;constraint qualification;semidefinite programming relaxation;lower bound;lagrangian relaxation;semidefinite program	A new semide nite programming, SDP, relaxation for the general graph partitioning problem, GP, is derived. The relaxation arises from the dual of the (homogenized) Lagrangian dual of an appropriate quadratic representation of GP. The quadratic representation includes a representation of the 0,1 constraints in GP. The special structure of the relaxation is exploited in order to project onto the minimal face of the cone of positive-semide nite matrices which contains the feasible set. This guarantees that the Slater constraint quali cation holds, which allows for a numerically stable primal–dual interior-point solution technique. A gangster operator is the key to providing an e cient representation of the constraints in the relaxation. An incomplete preconditioned conjugate gradient method is used for solving the large linear systems which arise when nding the Newton direction. Only dual feasibility is enforced, which results in the desired lower bounds, but avoids the expensive primal feasibility calculations. Numerical results illustrate the e cacy of the SDP relaxations. ? 1999 Elsevier Science B.V. All rights reserved. MSC: 90C25; 90C27; 15A48	adjacency matrix;bridge (graph theory);column (database);conjugate gradient method;conjunctive query;duality gap;graph partition;interior point method;iteration;kernel (linear algebra);lambda lifting;laplacian matrix;lifting scheme;linear programming relaxation;linear system;newton;numerical method;numerical stability;partition problem;quadratic assignment problem;semidefinite programming;slater determinant;social inequality;the matrix	Henry Wolkowicz;Qing Zhao	1999	Discrete Applied Mathematics	10.1016/S0166-218X(99)00102-X	mathematical optimization;combinatorics;discrete mathematics;lagrangian relaxation;graph partition;interior point method;quadratically constrained quadratic program;mathematics;semidefinite embedding;linear system;upper and lower bounds;semidefinite programming	ML	79.49113248561049	24.157528267534207	128854
32218cbe62c218a16061724d7c1cec38d8c2858e	on general vector quasi-optimization problems	condicion existencia;optimizacion vectorial;key words general quasi optimization problem;desigualdad variacional;quasi variational inequalities;inegalite variationnelle;quasi equilibrium problem;equilibrium problem;condition suffisante;optimization problem;condicion suficiente;general quasi optimization problem;existencia de solucion;existence of solution;probleme quasi optimisation;condition existence;optimisation vectorielle;variational inequality;vector optimization;probleme complementarite;problema complementariedad;complementarity problem;upper and lower c convex multivalued mappings;sufficient condition;multivalued mapping;vector optimization problem;existence condition;upper and lower c continuous multi valued mappings;strictly c quasiconvex and properly c quasiconvex multivalued mappings;existence solution	Vector general quasi-optimization problems are formulated and some sufficient conditions on the existence of solutions for these problems are shown. These concern the existence of solutions, the stability and the structure of solution set of general vector problems. As special case, we obtain results on the existence of solutions of vector quasi-optimization problem. Some relationships between these problems and others, as vector quasi-equilibrium problems, quasi-variational inequalities, complementarity problems,..., are shown. From these we extend some well-known results obtained by Blum and Oettli [4], Park [21], Chan and Pang [6], Parida and Sen [20], Browder and Minty [18], Fan [8], etc.	mathematical optimization	Angelo Guerraggio;Nguyen Xuan Tan	2002	Math. Meth. of OR	10.1007/s001860200212	optimization problem;mathematical optimization;combinatorics;mathematical analysis;variational inequality;mathematics;vector optimization	Theory	71.73156041640988	21.361889954255993	128924
26710a194470a462d8d64a72b144cd10495cd7ec	on pseudo-convex functions of nonnegative variables	quadratic programming;theorems;functions mathematics;matrices mathematics;convex function;convex sets	Abstract : Two proofs are given for a conjecture of Bela Martos concerning conditions under which a quasi-convex quadratic function of nonnegative variables is actually a pseudo-convex function. (Author)	convex function	Richard W. Cottle;Jacques A. Ferland	1971	Math. Program.	10.1007/BF01584075	convex function;convex analysis;support function;mathematical optimization;parent function;mathematical analysis;discrete mathematics;convex optimization;theorem;mathematics;logarithmically convex function;quadratic programming;proper convex function	Theory	70.94723749033851	21.275613010210044	129051
13aa761273e757c61a3d6370f6546ae2f86a2619	approximations of schatten norms via taylor expansions		In this paper we consider symmetric, positive semidefinite (SPSD) matrix A and present two algorithms for computing the p-Schatten norm ‖A‖p. The first algorithm works for any SPSD matrix A. The second algorithm works for non-singular SPSD matrices and runs in time that depends on κ = λ1(A) λn(A) , where λi(A) is the i-th eigenvalue of A. Our methods are simple and easy to implement and can be extended to general matrices. Our algorithms improve, for a range of parameters, recent results of Musco, Netrapalli, Sidford, Ubaru and Woodruff (ITCS 2018.) and match the running time of the methods by Han, Malioutov, Avron, and Shin (SISC 2017) while avoiding computations of coefficients of Chebyshev polynomials.	algorithm;approximation;chebyshev polynomials;coefficient;computation;han unification;polynomial;time complexity	Vladimir Braverman	2018	CoRR		chebyshev polynomials;approximations of π;positive-definite matrix;combinatorics;eigenvalues and eigenvectors;matrix (mathematics);mathematics;taylor series	Theory	79.18421524703719	21.00170993953476	129094
be25583633a55a619e2b294ab301d28d13e5d632	an efficient approach to the linear least squares problem	sistema lineal;linear algebra;analisis numerico;orthogonal matrix;65f20;pseudoinverses;65f05;decomposition qr;orthogonalisation;linear least square;matriz ortogonal;linear system;analyse numerique;15a23;pseudoinverses 65f25;descomposicion matricial;14c20;pseudoinverse;seudoinverso;numerical analysis;matrice definie positive;positive definite matrix;decomposition matricielle;matrix decomposition;least squares problem;algebre lineaire;15a09;matrice orthogonale;65f25;problema minimos cuadrados;orthogonalization;algebra lineal;symmetric system;qr decomposition;ortogonalizacion;systeme lineaire;matriz definida positiva;systeme symetrique;sistema simetrico;orthogonalization process;probleme moindre carre	In this paper we present a partially orthogonal decomposition for a matrix A. Using this decomposition the linear least squares problem is reduced to solving two linear systems. The matrix of the first system is symmetric and positive definite, and the matrix of the second system is nonsingular upper triangular. We show that this approach can provide computational savings.		K. Tunyan;Karen O. Egiazarian;A. Tuniev;Jaakko Astola	2004	SIAM J. Matrix Analysis Applications	10.1137/S0895479801386596	system of linear equations;total least squares;orthogonalization;iteratively reweighted least squares;gaussian elimination;combinatorics;orthogonal matrix;non-linear iterative partial least squares;lu decomposition;ordinary least squares;numerical analysis;moore–penrose pseudoinverse;linear algebra;calculus;mathematics;positive-definite matrix;non-linear least squares;linear system;state-transition matrix;matrix decomposition;singular value decomposition;least squares;cholesky decomposition;qr decomposition;matrix;linear least squares;symmetric matrix;algebra	Theory	81.60135662211175	20.780035870778867	129394
e789ce9d338e12a7ad6d1fb4e263f3aea3ed2045	a minima tracking variant of semi-infinite programming for the treatment of path constraints within direct solution of optimal control problems	system dynamics;shooting method;benchmark problem;optimal control;65k05;semi infinite programming;sampling technique;90c34;path constraints;local reduction;49j15;theoretical foundation;direct multiple shooting;78m50;semi infinite;optimal control problem	We present an extension of the direct multiple shooting method for the solution of optimal control problems with path constraints. The extension allows for fully automatic and accurate treatment of path constraints in case of relatively coarse discretizations of the control while maintaining a prescribed resolution of the system dynamics. It is based on a reduction principle from classical theory of semi-infinite programming. On the basis of these theoretical foundations the minima tracking algorithm is designed. We theoretically and numerically compare this method with a sampling technique. The numerical test case is a well-known, difficult space shuttle reentry benchmark problem.	algorithm;benchmark (computing);direct multiple shooting method;matlab;maxima and minima;numerical analysis;optimal control;sampling (signal processing);semi-infinite programming;semiconductor industry;simulation;system dynamics;test case;whole earth 'lectronic link	Andreas Potschka;Hans Georg Bock;Johannes P. Schlöder	2009	Optimization Methods and Software	10.1080/10556780902753098	shooting method;sampling;mathematical optimization;optimal control;semi-infinite;control theory;mathematics;system dynamics	ML	73.12320898796887	26.62423283416898	129582
83424548c1aaad712eb971aaee73f0a673e4802d	the geometry of strict maximality	secs s 06 metodi mat dell economia e scienze attuariali e finanziarie;strict maximality;settore secs s 06 metodi matematici dell economia e delle scienze attuariali e finanziarie;articolo su rivista scientifica specializzata;linear scalarization;mat 09 ricerca operativa;90c29;proper maximality;base for a cone;vector optimization;settore mat 09 ricerca operativa;46n10	The notion of a strictly maximal point is a concept of proper maximality that plays an important role in the study of the stability of vector optimization problems. The aim of this paper is to study some properties of this notion with particular attention to geometrical aspects. More precisely, we individuate some relationships between strict maximality and the properties of the bases of the ordering cone. In order to prove this result, a new characterization of the existence of a bounded base for a closed convex cone is given. Moreover, we link strict maximality to the geometrical notion of strongly exposed points of a given set. Finally, we deal with the linear scalarization for the strictly maximal points.	cone (formal languages);convex cone;mathematical optimization;maximal set;vector optimization	Emanuele Casini;Enrico Miglierina	2010	SIAM Journal on Optimization	10.1137/090748858	mathematical optimization;mathematical analysis;topology;calculus;mathematics;vector optimization	Theory	70.278677759746	19.574450057453607	129720
77f00c33611b27bf922580870a1a0304ba364c8d	a general duality scheme for nonconvex minimization problems with a strict inequality constraint	strict inequality constraint;duality;inequality constraint;convex function;lipschitzian optimization;d c optimization	We establish a duality formula for the problem Minimize f (x)+ g(x) for h(x)+ k(x) < 0 whereg, k are extended-real-valued convex functions and f , h belong to the class of functions that can be written as the lower envelope of an arbitrary family of convex functions. Applications in d.c. and Lipschitzian optimization are given.	convex function;emoticon;mathematical optimization;semi-continuity;social inequality	Bernard Lemaire;M. Volle	1998	J. Global Optimization	10.1023/A:1008285930026	perturbation function;convex function;convex analysis;mathematical optimization;mathematical analysis;discrete mathematics;convex optimization;duality;duality gap;linear matrix inequality;weak duality;mathematics;strong duality;proper convex function	ML	71.6550510385652	21.329750422721446	130097
9b79a7dffbcbd8f9342341a09fedc0a2defd9a37	second-order kuhn-tucker invex constrained problems	second order;90c26;inequality constraint;90c30;satisfiability;26b25;second order kuhn tucker invex problems;pseudoconvex problems;global optimization;invexity;49k30;49j52;necessary optimality condition	A new notion of a second-order KT invex problem (P) with inequality constraints is introduced in this paper. This class of problems strictly includes the KT invex ones. Some properties of the second-order KT invex problems are presented. For example, (P) is second-order KT invex if and only if each point, which satisfies the second-order Kuhn-Tucker necessary optimality conditions, is a global minimizer. A problem with quasiconvex data is (second-order) KT invex if and only if it is (second-order) KT pseudoconvex.	invex function;karush–kuhn–tucker conditions;kosterlitz–thouless transition;quasiconvex function;social inequality;tucker decomposition	Vsevolod I. Ivanov	2011	J. Global Optimization	10.1007/s10898-010-9610-0	mathematical optimization;mathematical analysis;mathematics;mathematical economics;second-order logic;global optimization;satisfiability	Theory	72.25418887993548	21.313768082270826	130180
55e721b557f38e2150bef5e4227b1855cb9403fe	linear volterra integro-differential equation and schauder bases	sistema lineal;metodo directo;systeme equation;equation differentielle;analisis numerico;matematicas aplicadas;fixed point theorem;numerical solution;mathematiques appliquees;approximation error;approximation numerique;65f05;numerical method;espacio banach;espace fonctionnel;methode approchee;37c25;equation volterra;banach space;differential equation;ecuacion volterra;metodo aproximado;ecuacion lineal;punto fijo;integro differential equation;34xx;approximate method;matrix inversion;inversion matriz;linear system;error aproximacion;volterra equation;banach spaces;theoreme point fixe;aproximacion numerica;analyse numerique;teorema punto fijo;ecuacion diferencial;function space;integrodifferential equation;14c20;58j20;sistema ecuacion;numerical analysis;estimation erreur;schauder bases;metodo numerico;error estimation;point fixe;equation system;estimacion error;algebra lineal numerica;algebre lineaire numerique;inversion matrice;equation integrale volterra;volterra integro differential equation;46bxx;ecuacion integrodiferencial;numerical linear algebra;numerical approximation;linear equations;systeme lineaire;linear equation;applied mathematics;volterra integral equations;methode directe;fix point;solution numerique;methode numerique;espacio funciones;espace banach;direct method;erreur approximation;equation lineaire;equation integrodifferentielle	In this paper we present a numerical method to approximate the solution of the linear Volterra integro-differential equation. By making use of the expression of a function of the Banach space C([0,1]) in terms of a basis Schauder, we are able to define a sequence of functions which approximate (in the uniform sense) the solution of such equation. Likewise, we study the error committed in each approximation. Some advantages that this method possess are that it is not necessary to solve linear equation systems and moreover, the involved integrals are immediate.		M. I. Berenguer;Miguel A. Fortes;A. I. Garralda Guillem;Manuel Ruiz Galán	2004	Applied Mathematics and Computation	10.1016/j.amc.2003.08.132	mathematical analysis;numerical analysis;calculus;schauder basis;mathematics;geometry;linear equation;banach space;algebra	Theory	81.08958584204309	18.812920766058824	130460
f18fc2b3ceabf9e409082cc1a57a791ed9ecd1b4	an approach to making spai and psai preconditioning effective for large irregular sparse linear systems	krylov solver;regular sparse;preconditioning;irregular sparse;sparse approximate inverse;sherman morrison woodbury formula;65f10;f norm minimization	We investigate the SPAI and PSAI preconditioning procedures and shed light on two important features of them: (i) For the large linear system Ax = b with A irregular sparse, i.e., with A having s relatively dense columns, SPAI may be very costly to implement, and the resulting sparse approximate inverses may be ineffective for preconditioning. PSAI can be effective for preconditioning but may require excessive storage and be unacceptably time consuming; (ii) the situation is improved drastically when A is regular sparse, that is, all of its columns are sparse. In this case, both SPAI and PSAI are efficient. Moreover, SPAI and, especially, PSAI are more likely to construct effective preconditioners. Motivated by these features, we propose an approach to making SPAI and PSAI more practical for Ax = b with A irregular sparse. We first split A into a regular sparse Ã and a matrix of low rank s. Then exploiting the Sherman–Morrison–Woodbury formula, we transform Ax = b into s+ 1 new linear systems with the same coefficient matrix Ã, use SPAI and PSAI to compute sparse approximate inverses of Ã efficiently and apply Krylov iterative methods to solve the preconditioned linear systems. Theoretically, we consider the non-singularity and conditioning of Ã obtained from some important classes of matrices. We show how to recover an approximate solution of Ax = b from those of the s+ 1 new systems and how to design reliable stopping criteria for the s+1 systems to guarantee that the approximate solution of Ax = b satisfies a desired accuracy. Given the fact that irregular sparse linear systems are common in applications, this approach widely extends the practicability of SPAI and PSAI. Numerical results demonstrate the considerable superiority of our approach to the direct application of SPAI and PSAI to Ax = b.	approximation algorithm;coefficient;column (database);experiment;irregular matrix;iterative method;krylov subspace;linear system;numerical method;numerical stability;parallel computing;preconditioner;solver;sparse matrix	Zhongxiao Jia;Qian Zhang	2013	SIAM J. Scientific Computing	10.1137/120900800	mathematical optimization;combinatorics;theoretical computer science;sparse approximation;mathematics;preconditioner;algebra	HPC	82.565887965216	24.048849447553692	130529
78dcc9a90c867593645715013e54f5e020891a98	approximating data in \mathfrakrn by a quadratic underestimator with specified hessian minimum and maximum eigenvalues	underestimating approximation;protein docking;global minimization;eigenvalues;eigenvalue bounds	The problem of approximating m data points (xi , yi ) in n+1, with a quadratic function q(x, p) with s parameters, m ≥ s, is considered. The parameter vector p ∈ s is to be determined so as to satisfy three conditions: (1) q(x, p) must underestimate all m data points, i.e. q(xi , p) ≤ yi , i = 1, . . . , m. (2) The error of the approximation is to be minimized in the L1 norm. (3) The eigenvalues of H are to satisfy specified lower and upper bounds, where H is the Hessian of q(x, p) with respect to x . This is called the Quadratic Underestimator with Bounds on Eigenvalues (QUBE) problem. An algorithm for its solution (QUBE algorithm) is given and justified, and computational results presented. The QUBE algorithm has application to finding the global minimum of a basin (or funnel) shaped function with a large number of local minima. Such problems arise in computational biology where it is desired to find the global minimum of an energy surface, in order to predict native protein-ligand docking geometry (drug design) or protein structure. Computational results for a simulated docking energy surface, with n = 15, are presented. It is shown that specifying a small condition number for H improves the ability of the underestimator to correctly predict the global minimum point.	algorithm;approximation;computation;computational biology;condition number;data point;docking (molecular);hessian;maxima and minima;protein–ligand docking;quadratic function;scattering parameters;taxicab geometry	J. Ben Rosen;John Glick	2006	J. Global Optimization	10.1007/s10898-006-9021-4	mathematical optimization;combinatorics;discrete mathematics;eigenvalues and eigenvectors;mathematics;macromolecular docking	Theory	76.15880104812689	20.159350287330774	130589
f52e24d3197f14d286144594c5dfccceee7ca7e4	bound constrained quadratic programming via piecewise quadratic functions	quadratic programming;estimacion;symmetric positive definite;line search;quadratic program;programmation quadratique;estimateur huber;convex programming;methode newton;estimacion m;newton iteration;duality;programmation convexe;condition;algorithme;cholesky factorization;algorithm;factorization;dualite;iteraccion;estimation;factorizacion;condicion;factorisation;iteration;estimation m;programacion cuadratica;dualidad;metodo newton;newton method;m estimation;algoritmo;programacion convexa	We consider the strictly convex quadratic programming problem with bounded variables. A dual problem is derived using Lagrange duality. The dual problem is the minimization of an unconstrained, piecewise quadratic function. It involves a lower bound of λ1, the smallest eigenvalue of a symmetric, positive definite matrix, and is solved by Newton iteration with line search. The paper describes the algorithm and its implementation including estimation of λ1, how to get a good starting point for the iteration, and upand downdating of Cholesky factorization. Results of extensive testing and comparison with other methods for constrained QP are given.	algorithm;cholesky decomposition;convex function;duality (optimization);iteration;lagrange multiplier;line search;newton's method;quadratic function;quadratic programming	Kaj Madsen;Hans Bruun Nielsen;Mustafa Ç. Pinar	1999	Math. Program.	10.1007/s101070050049	mathematical optimization;combinatorics;incomplete cholesky factorization;power iteration;calculus;mathematics;newton's method;factorization;quadratic programming	ML	75.60034397897279	22.83750243854187	130913
86c7391eb89578f769c9c25dc9cdf903cd341096	regularized mixed variational-like inequalities		We use auxiliary principle technique coupled with iterative regularization method to suggest and analyze some new iterative methods for solving mixed variational-like inequalities. The convergence analysis of these new iterative schemes is considered under some suitable conditions. Some special cases are also discussed. Our method of proofs is very simple as compared with other methods. Our results represent a significant refinement of the previously known results.	calculus of variations;iterative method;matrix regularization;refinement (computing);variational principle	Muhammad Aslam Noor;Khalida Inayat Noor;Saira Zainab;Eisa A. Al-Said	2012	J. Applied Mathematics	10.1155/2012/863450	mathematical optimization;combinatorics;discrete mathematics;mathematics;iterative method	ML	74.86466509838252	24.572786105697805	130981
952522b5f95232d43018ed5ffd0d0c12438db0cb	eigenvalues of a symmetric 3 × 3 matrix	moment of inertia;rigid body;eigenvalues	Recently, in order to find the principal moments of inertia of a large number of rigid bodies, it was necessary to compute the eigenvalues of many real, symmetric 3 × 3 matrices. The available eigenvalue subroutines seemed rather heavy weapons to turn upon this little problem, so an explicit solution was developed. The resulting expressions are remarkably simple and neat, hence this note.	neuroevolution of augmenting topologies;subroutine	Oliver K. Smith	1961	Commun. ACM	10.1145/355578.366316	mathematical optimization;rigid body;eigenvalues and eigenvectors;calculus;moment of inertia;mathematics;geometry	Theory	77.9881386800737	18.596288464636	131013
21e80fff33436f9191f3a4338209685b09063695	exploiting separability in large-scale linear support vector machine training	interior point methods;software;quadratic program;support vector machines;noisy data;cholesky factorization;decomposition method;large scale;support vector machine;separable quadratic program;numerical experiment;low rank approximation;interior point method;ordinal regression	Linear support vector machine training can be represented as a large quadratic program. We present an efficient and numerically stable algorithm for this problem using interior point methods, which requires only O(n) operations per iteration. Through exploiting the separability of the Hessian, we provide a unified approach, from an optimization perspective, to 1-norm classification, 2-norm classification, universum classification, ordinal regression and -insensitive regression. Our approach has the added advantage of obtaining the hyperplane weights and bias directly from the solver. Numerical experiments indicate that, in contrast to existing methods, the algorithm is largely unaffected by noisy data, and they show training times for our implementation are consistent and highly competitive. We discuss the effect of using multiple correctors, and monitoring the angle of the normal to the hyperplane to determine termination.	active set method;algorithm;approximation;blas;cache (computing);central processing unit;cholesky decomposition;column (database);computation;computational complexity theory;experiment;hessian;instability;interior point method;iteration;level of measurement;linear separability;machine learning;mathematical optimization;maxima and minima;no-communication theorem;nonlinear system;numerical linear algebra;numerical stability;ordinal data;ordinal regression;outer product;quadratic programming;rsa problem;signal-to-noise ratio;solver;support vector machine;the matrix	Kristian Woodsend;Jacek Gondzio	2011	Comp. Opt. and Appl.	10.1007/s10589-009-9296-8	support vector machine;mathematical optimization;discrete mathematics;machine learning;interior point method;mathematics;relevance vector machine;quadratic programming;structured support vector machine	ML	78.22460504398306	23.37342894929629	131041
28fe7e450d1494439872a247b14a7a6eb8189363	exact solution of general integer systems of linear equations	discrete distribution;systeme equation;random number generator;exact solution;ecuacion lineal;arithmetique;congruencia;sistema ecuacion;aritmetica;resolucion ecuacion;equation system;exact computation;arithmetic;resolution equation;equation resolution;linear equations;linear equation;equation lineaire;congruence;general techniques of generation	Methods are known for the exact computation of the solution of integer systems of linear equations <italic>AX</italic> = <italic>B</italic> with a nonsingular coefficient matrix <italic>A</italic> by congruence techniques. These methods are now generalized for systems with an arbitrary integer coefficient matrix <italic>A</italic>. To make congruence techniques applicable, a common denominator of all elements of the solution <italic>X</italic> = <italic>A</italic><supscrpt>+</supscrpt><italic>B</italic> must be computed. This is achieved by defining the natural denominator CODE of <italic>A</italic><supscrpt>+</supscrpt> and describing it by some formulas. Methods for the exact computation of additional results (consistency, null space, solution of at most <italic>R</italic> nonzero elements), a recursive test to save computing time, and a comparison with some results from the literature are presented.	coefficient;computation;congruence of squares;kernel (linear algebra);linear equation;recursion;system of linear equations	Jörn Springer	1986	ACM Trans. Math. Softw.	10.1145/5960.5961	mathematical optimization;combinatorics;mathematical analysis;discrete mathematics;mathematics;linear equation;algebra	Graphics	81.28114264325458	20.80623857177199	131157
e947f62c5498dfa4c0a39b4c794112b5e93fafc9	an iterative algorithm for computing mean first passage times of markov chains	convergence;rank one updates;eigenvalues;iterative algorithm;mean first passage times;markov chains	Mean first passage times are an essential ingredient in both the theory and the applications of Markov chains. In the literature, they have been expressed in elegant closed-form formulas. These formulas involve explicit full matrix inversions and, if computed directly, may incur numerical instability.#R##N##R##N#In this paper, we present a new iterative algorithm for computing mean first passage times in a manner that does not rely on explicit full matrix inversions. Results regarding the convergence behavior of this algorithm are also developed.	algorithm;iterative method;markov chain	Jianhong Xu	2015	Applied Mathematics and Computation	10.1016/j.amc.2014.11.001	markov chain;mathematical optimization;combinatorics;discrete mathematics;convergence;eigenvalues and eigenvectors;mathematics;iterative method;algebra	Metrics	81.62696399121722	23.253223419821833	131253
3060eaa71631c567d7f01e96fd145a4e62b10271	a generalized project metric algorithm for mathematical programs with equilibrium constraints	superlinear convergence;computacion informatica;global convergence;generalized project metric algorithm;ciencias basicas y experimentales;matematicas;grupo a;mathematical programs with equilibrium constraints	This paper discusses a kind of mathematical programs with equilibrium constraints (MPEC for short). By using a complementarity function and a kind of disturbed technique, the original (MPEC) problem is transformed into a nonlinear equality and inequality constrained optimization problem. Then, we combine a generalized gradient projection matrix with penalty function technique to given a generalized project metric algorithm with arbitrary initial point for the (MPEC) problems. In order to avoid Mataros effect, a high-order revised direction is obtained by an explicit formula. Under some relative weaker conditions, the proposed method is proved to possess global convergence and superlinear convergence.	algorithm;mathematical programming with equilibrium constraints	Minglei Fang;Zhibin Zhu	2015	J. Computational Applied Mathematics	10.1016/j.cam.2015.04.007	mathematical optimization;mathematical analysis;calculus;mathematics	Theory	72.67070614258814	22.035344522556667	131342
7644e128aeb246ca7e71de75fe790dadd77adaab	smoothed aggregation solvers for anisotropic diffusion	energy minimization;anisotropic diffusion	SUMMARY#R##N#A smoothed aggregation-based algebraic multigrid solver for anisotropic diffusion problems is presented. Algebraic multigrid is a popular and effective method for solving sparse linear systems that arise from discretizing partial differential equations. However, although algebraic multigrid was designed for elliptic problems, the case of non-grid-aligned anisotropic diffusion is not adequately addressed by existing methods. To achieve scalable performance, it is shown that neither new coarsening nor new relaxation strategies are necessary. Instead, a novel smoothed aggregation approach is developed that combines long-distance interpolation, coarse-grid injection, and an energy-minimization strategy that finds the interpolation weights. Previously developed theory by Falgout and Vassilevski is used to discern that existing coarsening strategies are sufficient, but that existing interpolation methods are not. In particular, an interpolation quality measure tracks ‘closeness’ to the ideal interpolant and guides the interpolation sparsity pattern choice. Although the interpolation quality measure is computable for only small model problems, an inexact, but computable, measure is proposed for larger problems. This paper concludes with encouraging numerical results that also potentially show broad applicability (e.g., for linear elasticity). Copyright © 2012 John Wiley & Sons, Ltd.	anisotropic diffusion;smoothed analysis;smoothing	Jacob B. Schroder	2012	Numerical Lin. Alg. with Applic.	10.1002/nla.1805	spline interpolation;mathematical optimization;combinatorics;discrete mathematics;mathematics	HPC	82.79799493732492	24.798329020983232	131513
99e147fc56e5523dacc742a4e067b9d3cbf4547b	a generalized proximal point algorithm and its convergence rate	94a08;90c30;convex optimization;convergence rate;operator splitting methods;90c25;proximal point algorithm	We propose a generalized proximal point algorithm (PPA), in the generic setting of finding a zero point of a maximal monotone operator. In addition to the classical PPA, a number of benchmark operator splitting methods in PDE and optimization literatures such as the Douglas-Rachford splitting method, Peaceman-Rachford splitting method, alternating direction method of multipliers, generalized alternating direction method of multipliers and split inexact Uzawa method can be retrieved by this generalized PPA scheme. We establish the convergence rate of this generalized PPA scheme under different conditions, including estimating the worst-case iteration complexity under mild assumptions and deriving the linear convergence rate under certain stronger conditions. Throughout our discussion, we pay particular attention to the special case where the operator is the sum of two maximal monotone operators, and specify our theoretical results in generic setting to this special case. Our result turns out to be a general and unified study on the convergence rate of a number of existing methods, and subsumes some existing results in the literature.	algorithm;approximation;augmented lagrangian method;benchmark (computing);best, worst and average case;ergodicity;hilbert space;iteration;list of operator splitting topics;mathematical optimization;maximal set;rate of convergence;tag (game);tensor operator;monotone	Etienne Corman;Xiaoming Yuan	2014	SIAM Journal on Optimization	10.1137/130940402	mathematical optimization;mathematical analysis;discrete mathematics;convex optimization;mathematics;rate of convergence	ML	74.12930169606251	24.749917153262537	131605
22e3cf701a8917a377e4a3d2360996f12e297354	explicit reformulations for robust optimization problems with general uncertainty sets	90c30;robust optimization;data uncertainty;mathematical programming;90c34;90c25;90c15;90c05;convex analysis;homogeneous functions	We consider a rather general class of mathematical programming problems with data uncertainty, where the uncertainty set is represented by a system of convex inequalities. We prove that the robust counterparts of this class of problems can be equivalently reformulated as finite and explicit optimization problems. Moreover, we develop simplified reformulations for problems with uncertainty sets defined by convex homogeneous functions. Our results provide a unified treatment of many situations that have been investigated in the literature, and are applicable to a wider range of problems and more complicated uncertainty sets than those considered before. The analysis in this paper makes it possible to use existing continuous optimization algorithms to solve more complicated robust optimization problems. The analysis also shows how the structure of the resulting reformulation of the robust counterpart depends both on the structure of the original nominal optimization problem and on the structure of the uncertainty set.	algorithm;continuous optimization;convex analysis;convex function;lh (complexity);mathematical optimization;optimization problem;program optimization;robust optimization	Igor Averbakh;Yun-Bin Zhao	2008	SIAM Journal on Optimization	10.1137/060650003	stochastic programming;convex analysis;probabilistic-based design optimization;mathematical optimization;homogeneous function;combinatorics;discrete mathematics;robust optimization;nonlinear programming;mathematics	ML	70.37054892592117	21.66379484013385	131616
13e22c7c80061044e624f8cf43b54820886afb55	an alternative algorithm for computing the pseudo-remainder of multivariate polynomials	divider;matematicas aplicadas;mathematiques appliquees;pseudo remainder;efficiency;methode gauss;diviseur;polynome multivariable;calculo automatico;computing;algorithme;calcul automatique;algorithm;eficacia;multivariate polynomial;divisor;gauss method;determinante;polynomial system solving;efficacite;determinant;gaussian elimination;metodo gauss;applied mathematics;determinant of a matrix;algoritmo	This paper presents an alternative method to compute the pseudo-remainder for multivariate polynomials, which plays a very important role in polynomial system solving by many known elimination methods. The efficiency of the new approach dependents heavily on the method for computing the determinant of a matrix. Some examples show that the new algorithm is efficient when one computes the determinants of matrices by using Gaussian elimination in Maple system.	algorithm;polynomial	Yong-Bin Li	2006	Applied Mathematics and Computation	10.1016/j.amc.2005.04.087	mathematical optimization;combinatorics;determinant;calculus;mathematics;algebra	Theory	80.9259548580883	20.291531627273283	131678
04d71f3eccafbe0efe0799fc1a8102940d9a1fde	introduction: new approaches to linear programming	nonlinear optimization;interior-point methods;rescaling.;interior point method;linear program	This issue ofAlgorithmica present papers on various aspects of nonlinear methods for solving linear programming problems, inspired by the work of Karmarkar. This introduction describes some of these aspects and briefly mentions other recent developments in the field. A bibliography of recent articles is included.	linear programming;nonlinear system	Nimrod Megiddo	1986	Algorithmica	10.1007/BF01840453	applied mathematics;computer science;algorithm	Theory	69.03168753074942	22.63625037546555	131808
b03bbf8dd5c3d3e56be8f541b6fbc2e65cb44593	calculating the eigenvectors of diagonally dominant matrices	transformations mathematics;theorems;iterations;perturbation theory;eigenvalues;matrices mathematics;invariant subspace;algorithms;high relative accuracy;operators mathematics;eigenvectors	An algorithm is proposed for calculating the eigenvectors of a diagonally dominant matrix all of whose elements are known to high relative accuracy. Eigenvectors corresponding to pathologically close eigenvalues are treated by computing the invariant subspace that they span. If the off-diagonal elements of the matrix are sufficiently small, the method is superior to standard techniques, and indeed it may produce a complete set of eigenvectors with an amount of work proportional to the square of the order of the matrix. An analysis is given of the effects of perturbations in the matrix on the eigenvectors.	algorithm;diagonally dominant matrix;perturbation theory;the matrix	M. M. Blevins;G. W. Stewart	1974	J. ACM	10.1145/321812.321821	combinatorics;mathematical analysis;eigendecomposition of a matrix;defective matrix;eigenvalues and eigenvectors;diagonally dominant matrix;eigenvalues and eigenvectors of the second derivative;mathematics;diagonalizable matrix;modal matrix;matrix differential equation;eigenvalue perturbation;algebra	Theory	78.77429460776736	20.723629449239986	132142
ad25c60ee10730e4d54d2df49adc7488723b7335	a ky fan minimax inequality for quasiequilibria on finite-dimensional spaces	quasiequilibrium problem;ky fan minimax inequality;set-valued map;fixed point;47j20;49j35;49j40;90c30	Several results concerning existence of solutions of a quasiequilibrium problem defined on a finite-dimensional space are established. The proof of the first result is based on a Michael selection theorem for lower semicontinuous set-valued maps which holds in finite-dimensional spaces. Furthermore, this result allows one to locate the position of a solution. Sufficient conditions, which are easier to verify, may be obtained by imposing restrictions either on the domain or on the bifunction. These facts make it possible to yield various existence results which reduce to the well-known Ky Fan minimax inequality when the constraint map is constant and the quasiequilibrium problem coincides with an equilibrium problem. Lastly, a comparison with other results from the literature is discussed.	minimax;social inequality;spaces	Marco Castellani;Massimiliano Giuli;Massimo Pappalardo	2018	J. Optimization Theory and Applications	10.1007/s10957-018-1319-0	mathematical optimization;mathematics;mathematical analysis;inequality;michael selection theorem;minimax;fixed point	ML	71.06466825101825	20.834526706633113	132154
5745c773fd3bc6651854fe0c5d5d50fa9c51a33d	accelerated gradient descent methods with line search	gradient descent method;line search;gradient descent methods;uniformly convex function;convergence rate;satisfiability;unconstrained optimization;linear convergence;newton method	We introduced an algorithm for unconstrained optimization based on the transformation of the Newton method with the line search into a gradient descent method. Main idea used in the algorithm construction is approximation of the Hessian by an appropriate diagonal matrix. The steplength calculation algorithm is based on the Taylor’s development in two successive iterative points and the backtracking line search procedure. The linear convergence of the algorithm is proved for uniformly convex functions and strictly convex quadratic functions satisfying specified conditions.	algorithm;approximation;backtracking line search;central processing unit;convex function;gradient descent;hessian;iterative method;mathematical optimization;newton's method;norm (social);quadratic function;rate of convergence;uniformly convex space	Predrag S. Stanimirovic;Marko Miladinovic	2009	Numerical Algorithms	10.1007/s11075-009-9350-8	gradient descent;coordinate descent;frank–wolfe algorithm;mathematical optimization;combinatorics;mathematical analysis;gradient method;backpropagation;backtracking line search;descent direction;random coordinate descent;newton's method in optimization;mathematics;stochastic gradient descent;proximal gradient methods;conjugate gradient method;nonlinear conjugate gradient method;rate of convergence;line search;broyden–fletcher–goldfarb–shanno algorithm;algorithm	ML	75.75192000710699	23.23358785236143	132458
a59912b4ab6913bda1f091750e907b813e495b96	scaling techniques for ε-subgradient methods	step size selection rules;65k05;90c25;scaled primal dual hybrid gradient algorithm;forward backward epsilon subgradient method;tv restoration;variable metric	The recent literature on first order methods for smooth optimization shows that significant improvements on the practical convergence behavior can be achieved with variable step size and scaling for the gradient, making this class of algorithms attractive for a variety of relevant applications. In this paper we introduce a variable metric in the context of the $\epsilon$-subgradient methods for nonsmooth, convex problems, in combination with two different step size selection strategies. We develop the theoretical convergence analysis of the proposed approach in the general framework of forward-backward $\epsilon$-subgradient splitting methods and we also discuss practical implementation issues. In order to illustrate the effectiveness of the method, we consider a specific problem in the image restoration framework and we numerically evaluate the effects of a variable scaling and of the step length selection strategy on the convergence behavior.		Silvia Bonettini;Alessandro Benfenati;Valeria Ruggiero	2016	SIAM Journal on Optimization	10.1137/14097642X	mathematical optimization;combinatorics;mathematics;mathematical economics	Theory	73.48905818130204	24.75808709538928	132633
7ead2486ffde7850f9710fc86665fe8abf27a2ba	a projection method for the uncapacitated facility location problem	location problem;orthogonality;piecewise linear;probleme localisation;linearity;variables;complexite calcul;complejidad calculo;iterations;functions mathematics;estudio comparativo;exact solution;projection method;computing complexity;etude comparative;position location;probleme combinatoire;problema combinatorio;programacion lineal;methode projection;convex function;projective techniques;metodo proyeccion;convex bodies;comparative study;linear programming relaxation;linear programming;orthogonal projection;programmation lineaire;linear program;algorithms;relaxation;facilities;formulations;problema localizacion;uncapacitated facility location problem;combinatory problem;problem solving	Several algorithms already exist for solving the uncapacitated facility location problem. The most efficient are based upon the solution of the strong linear programming relaxation. The dual of this relaxation has a condensed lk)rm which consists of minimizing a certain piecewise linear convex function. This paper presents a new method for solving the uncapacitated facility location problem based upon the exact solution of the condensed dual via orthogonal projections. The amount of work per iteration is of the same order as that of a simplex iteration for a linear program in rn variables and constraints, where m is the number of clients. For comparison, the underlying linear programming dual has mn+ m + n wtriables and rnn + n constraints, where n is the number of potential locations for the facilities. The method is flexible as it can handle side constraints. In particular, when there is a duality gap, the linear programming formulation can be strengthened by adding cuts. Numerical results for some classical test problems are included.	algorithm;convex function;duality gap;facility location problem;iteration;linear programming formulation;linear programming relaxation;numerical method;piecewise linear continuation;projection method (fluid dynamics);turing test	Andrew R. Conn;Gérard Cornuéjols	1990	Math. Program.	10.1007/BF01585746	variables;convex function;mathematical optimization;combinatorics;iteration;projective test;piecewise linear function;orthogonality;linear programming;linear programming relaxation;comparative research;calculus;relaxation;formulation;mathematics;linearity;projection method;orthographic projection	Logic	75.5002766231704	22.233206212870513	132748
62bffd668de3b5995c2eb12b3d17de98698664c5	efficient implementation of the riccati recursion for solving linear-quadratic control problems	predictive control;sparse matrices complexity theory heuristic algorithms equations tin vectors approximation methods;riccati equations linear quadratic control matrix decomposition predictive control;matrix decomposition;linear quadratic control;riccati equations;flops reduction linear quadratic control problems active set algorithms interior point algorithms model predictive control computational effort lq control problem riccati recursion based solver dense matrices cholesky factorization	In both Active-Set (AS) and Interior-Point (IP) algorithms for Model Predictive Control (MPC), sub-problems in the form of linear-quadratic (LQ) control problems need to be solved at each iteration. The solution of these sub-problems is typically the main computational effort at each iteration. In this paper, we compare a number of solvers for an extended formulation of the LQ control problem: a Riccati recursion based solver can be considered the best choice for the general problem with dense matrices. Furthermore, we present a novel version of the Riccati solver, that makes use of the Cholesky factorization of the Pn matrices to reduce the number of flops. When combined with regularization and mixed precision, this algorithm can solve large instances of the LQ control problem up to 3 times faster than the classical Riccati solver.	active set method;algorithm;cholesky decomposition;flops;iteration;iterative method;iterative refinement;letter-quality printer;precision and recall;recursion;refinement (computing);solver;sparse matrix	Gianluca Frison;John Bagterp Jørgensen	2013	2013 IEEE International Conference on Control Applications (CCA)	10.1109/CCA.2013.6662901	linear-quadratic-gaussian control;mathematical optimization;linear-quadratic regulator;combinatorics;discrete mathematics;algebraic riccati equation;mathematics	Robotics	79.6227392005055	24.754321444001757	132758
7f2edb3558425a3a31fdc396725d3314777b4ab0	progressive construction of a parametric reduced-order model for pde-constrained optimization	optimization	An adaptive approach to using reduced-order models as surrogates in PDE-constrained optimization is introduced that breaks the traditional offline-online framework of model order reduction. A sequence of optimization problems constrained by a given Reduced-Order Model (ROM) is defined with the goal of converging to the solution of a given PDE-constrained optimization problem. For each reduced optimization problem, the constraining ROM is trained from sampling the High-Dimensional Model (HDM) at the solution of some of the previous problems in the sequence. The reduced optimization problems are equipped with a nonlinear trust-region based on a residual error indicator to keep the optimization trajectory in a region of the parameter space where the ROM is accurate. A technique for incorporating sensitivities into a Reduced-Order Basis (ROB) is also presented, along with a methodology for computing sensitivities of the reduced-order model that minimizes the distance to the corresponding HDM sensitivity, in a suitable norm. The proposed reduced optimization framework is applied to subsonic aerodynamic shape optimization and shown to reduce the number of queries to the HDM by a factor of 4-5, compared to the optimization problem solved using only the HDM, with errors in the optimal solution far less than 0.1%.	algorithm;append;approximation;central processing unit;computation;computational complexity theory;computational science;constrained optimization;constraint (mathematics);galerkin method;least squares;machine epsilon;mathematical optimization;model order reduction;nonlinear system;online and offline;optimization problem;physical review a;qr decomposition;sampling (signal processing);shape optimization;singular value decomposition;subsonic;surrogates;traffic collision avoidance system;trust region	Matthew J. Zahr;Charbel Farhat	2014	CoRR			Vision	78.58486785772114	26.961180277186838	132778
ef7ae513a9cfe7f53b3b73ae0b4b23f15e4d5837	a splitting preconditioner for saddle point problems	iterative method;convergence;preconditioning;saddle point problems	For large sparse systems of linear equations iterative techniques are attractive. In this paper, we study a splitting method for an important class of symmetric and indefinite system. Theoretical analyses show that this method converges to the unique solution of the system of linear equations for all t>0 (t is the parameter). Moreover, all the eigenvalues of the iteration matrix are real and nonnegative and the spectral radius of the iteration matrix is decreasing with respect to the parameter t. Besides, a preconditioning strategy based on the splitting of the symmetric and indefinite coefficient matrices is proposed. The eigensolution of the preconditioned matrix is described and an upper bound of the degree of the minimal polynomials for the preconditioned matrix is obtained. Numerical experiments of a model Stokes problem and a least-squares problem with linear constraints presented to illustrate the effectiveness of the method. © 2011 John Wiley & Sons, Ltd.	preconditioner	Yang Cao;Mei-Qun Jiang;Ying-Long Zheng	2011	Numerical Lin. Alg. with Applic.	10.1002/nla.772	mathematical optimization;convergence;mathematics;preconditioner;iterative method;algebra	Theory	81.22777301303235	21.881145679528373	132800
089ecb945dd765fba473a49ab7ba5e80ffb5cef5	extending sdp integrality gaps to sherali-adams with applications to quadratic programming and maxcutgain	quadratic program;semidefinite relaxation	We show how under certain conditions one can extend constructions of integrality gaps for semidefinite relaxations into ones that hold for stronger systems: those SDP to which the so-called k-level constraints of the Sherali-Adams hierarchy are added. The value of k above depends on properties of the problem. We present two applications, to the Quadratic Programming problem and to the MaxCutGain problem.#R##N##R##N#Our technique is inspired by a paper of Raghavendra and Steurer [Raghavendra and Steurer, FOCS 09] and our result gives a doubly exponential improvement for Quadratic Programming on another result by the same authors [Raghavendra and Steurer, FOCS 09]. They provide tight integrality-gap for the system above which is valid up to k=(loglogn)Ω(1) whereas we give such a gap for up to k=nΩ(1).	anomaly detection at multiple scales;linear multistep method;quadratic programming	Siavosh Benabbas;Avner Magen	2010		10.1007/978-3-642-13036-6_23	mathematical optimization;combinatorics;discrete mathematics;mathematics;quadratic programming	Theory	70.73328616117846	25.974910401127783	132900
3c99ff49d9a2c69c574a971f844ce13300357005	a rigorous global filtering algorithm for quadratic constraints*	sistema lineal;systeme equation;contrainte quadratique;equation quadratique;quadratic constraints;global constraint;constraint satisfaction;linear system;linear relaxation;satisfaction contrainte;sistema ecuacion;programacion lineal;relajacion lineal;linearisation;contrainte globale;linearizacion;equation system;linear programming;programmation lineaire;linearization;relaxation lineaire;constraint system;satisfaccion restriccion;systeme lineaire;quadratic equation;ecuacion segundo grado;global constraints;safe linearizations	This article introduces a new filtering algorithm for handling systems of quadratic equations and inequations. Such constraints are widely used to model distance relations in numerous application areas ranging from robotics to chemistry. Classical filtering algorithms are based upon local consistencies and thus, are often unable to achieve a significant pruning of the domains of the variables occurring in quadratic constraint systems. The drawback of these approaches comes from the fact that the constraints are handled independently. We introduce here a global filtering algorithm that works on a tight linear relaxation of the quadratic constraints. The Simplex algorithm is then used to narrow the domains. Since most implementations of the Simplex work with floating point numbers and thus, are unsafe, we provide a procedure to generate safe linearizations. We also exploit a procedure provided by Neumaier and Shcherbina to get a safe objective value when calling the Simplex algorithm. With these two procedures, we prevent the Simplex algorithm from removing any solution while filtering linear constraint systems. Experimental results on classical benchmarks show that this new algorithm yields a much more effective pruning of the domains than local consistency filtering algorithms.	benchmark (computing);cryptographic service provider;global optimization;interval arithmetic;linear programming relaxation;local consistency;quadratic equation;quadratically constrained quadratic program;robotics;simplex algorithm;solver	Yahia Lebbah;Claude Michel;Michel Rueher	2004	Constraints	10.1007/s10601-004-5307-7	quadratic equation;mathematical optimization;constraint satisfaction;linear programming;calculus;mathematics;geometry;linear system;simplex algorithm;linearization;algorithm	AI	77.01706537446412	22.55915136034711	132980
b0b0b03cbf9d4a18ee98385d747dd6c69319db3a	a convexification method for a class of global optimization problems with applications to reliability optimization	global solution;approximation method;concave minimization;reliability optimization;monotone optimization;convexification method;complex system;global optimization;mixed integer nonlinear programming	A convexification method is proposed for solving a class of global opti mization problems with certain monotone properties. It is shown that this class of probl ems can be transformed into equivalent concave minimization problems using the proposed convexifi cation schemes. An outer approximation method can then be used to find the global solution of the transformed problem. Applications to mixed-integernonlinear programming problems arisi ng in reliability optimization of complex systems are discussed and satisfactory numerical results are pres ented.	approximation;complex systems;concave function;convex hull;global optimization;mathematical optimization;numerical analysis;optimization problem;monotone	Xiaoling Sun;K. I. M. McKinnon;Duan Li	2001	J. Global Optimization	10.1023/A:1011962605464	probabilistic-based design optimization;optimization problem;mathematical optimization;combinatorics;mathematical analysis;mathematics;global optimization	EDA	71.37063063834617	23.896216776859344	133198
0e938fa2de84920ae493b272c6d293d28bf41920	backward error analysis of polynomial eigenvalue problems solved by linearization	institutional repositories;15a18;fedora;strong linearization;dual minimal basis;vital;stability;polynomial eigenvalue problem;linearization;backward error;vtls;65f15;ils	One of the most frequently used techniques to solve polynomial eigenvalue problems is linearization, in which the polynomial eigenvalue problem is turned into an equivalent linear eigenvalue problem with the same eigenvalues, and with easily recoverable eigenvectors. The eigenvalues and eigenvectors of the linearization are usually computed using a backward stable solver such as the QZ algorithm. Such backward stable algorithms ensure that the computed eigenvalues and eigenvectors of the linearization are exactly those of a nearby linear pencil, where the perturbations are bounded in terms of the machine precision and the norms of the matrices defining the linearization. With respect to the linearization, we may have solved a nearby problem, but we would also like to know if our computed solution is the exact solution of a nearby polynomial eigenvalue problem. We perform a structured backward error analysis of polynomial eigenvalue problems solved via linearization. Through the use of dual minimal bases, we unify the construction of strong linearizations for many different polynomial bases. By inspecting the prototypical linearizations for polynomials expressed in a number of classical bases, we are able to identify a small number of driving factors involved in the growth of the backward error. One of the primary factors is found to be the norm of the block vector of coefficients of the polynomial, which is consistent with the current literature. We derive upper bounds for the backward errors for specific linearizations, and these are shown to be reasonable estimates for the computed backward errors.	algorithm;coefficient;error analysis (mathematics);machine epsilon;numerical stability;polynomial;schedule (computer science);solver	Piers W. Lawrence;Marc Van Barel;Paul Van Dooren	2016	SIAM J. Matrix Analysis Applications	10.1137/15M1015777	mathematical optimization;mathematical analysis;stability;calculus;mathematics;linearization;statistics;algebra	Theory	79.35290183376428	22.984172167623967	133330
079b41b37c066bc6c76c79e3bf3954129771adc3	newton's method for computing a normalized equilibrium in the generalized nash game through fixed point formulation	constant rank constraint qualification;normalized equilibrium;fixed point characterization;nonsmooth newton method;computable generalized jacobian;generalized nash equilibrium problem;local superlinear quadratic convergence	Abstract: We consider the generalized Nash equilibrium problem (GNEP), where not only the players’ cost functions but also their strategy spaces depend on the rivals’ decision variables. Existence results for GNEPs are typically shown by using a fixed point argument for a certain set-valued function. Here we use a regularization of this set-valued function in order to obtain a single-valued function that is easier to deal with from a numerical point of view. We show that the fixed points of the latter function constitute an important subclass of the generalized equilibria called normalized equilibria. This fixed point formulation is then used to develop a nonsmooth Newton method for computing a normalized equilibrium. The method uses a so-called computable generalized Jacobian that is much easier to compute than Clarke generalized Jacobian or B-subdifferential. We establish local superlinear/quadratic convergence of the method under the constant rank constraint qualification, which is weaker than the frequently used linear independence constraint qualification, and a suitable second-order condition. Some numerical results are presented to illustrate the performance of the method.	clarke's generalized jacobian;computable function;computer performance;edmund m. clarke;fixed point (mathematics);jacobian matrix and determinant;karush–kuhn–tucker conditions;nash equilibrium;newton's method;numerical analysis;rate of convergence;subderivative	Anna von Heusinger;Christian Kanzow;Masao Fukushima	2012	Math. Program.	10.1007/s10107-010-0386-2	mathematical optimization;mathematical analysis;mathematics;mathematical economics	ML	73.06174920213998	21.492923457527287	133517
b7e071cfabfc0355a778e13acf9fc190cbfeb8ff	generalized implicit variational-like inclusion problems involving g-eta-monotone mappings	espace hilbert;matematicas aplicadas;resolvent;espacio hilbert;mathematiques appliquees;generalized implicit variational like inclusion;probleme variationnel;variational problem;hilbert space;resolvente;monotone mapping;generalized resolvent operator technique;resolvante;g η monotone mappings;resolubilite;applied mathematics;probleme inclusion;solvability;application monotone;resolubilidad	A new class of G-η-monotone mappings is introduced and studied in Hilbert space. By using the properties of G-η-monotone mappings, the solvability of a class of nonlinear variational inclusions using the generalized resolvent operators technique is given.	variational principle;monotone	Qing-bang Zhang	2007	Appl. Math. Lett.	10.1016/j.aml.2006.04.002	mathematical optimization;mathematical analysis;discrete mathematics;topology;applied mathematics;mathematics;algebra;resolvent;hilbert space	Theory	73.36290829302638	20.146273432222934	133560
7a334fd407c3806016566e7149b15e7f32728fc7	subproblem and overall convergence for a method-of-centers algorithm		This paper considers convergence of a method-of-centers algorithm for solving nonlinear programming problems. An upper bound is derived for the number of steps needed to solve each subproblem, defined by the algorithm, when the method of steepest ascent is employed. Also, an upper bound is found for the total number of subproblem steps needed to solve all of the subproblems required to find a feasible point having an objective value with a prescribed maximum deviation from the optimal value.	algorithm	Robert Mifflin	1975	Operations Research	10.1287/opre.23.4.796	mathematical optimization;combinatorics;mathematics;algorithm	ML	70.93125813932143	25.17608417420526	133625
b5551e56a35caa10986212fa59d03f3409a9d0ce	a class of differential hemivariational inequalities in banach spaces	differential hemivariational inequality;$$c_0$$c0-semigroup;rothe method;pseudomonotone;clarke subdifferential;35l15;35l86;35l87;74hxx;74m10	In this paper we investigate an abstract system which consists of a hemivariational inequality of parabolic type combined with a nonlinear evolution equation in the framework of an evolution triple of spaces which is called a differential hemivariational inequality [(DHVI), for short]. A hybrid iterative system corresponding to (DHVI) is introduced by using a temporally semi-discrete method based on the backward Euler difference scheme, i.e., the Rothe method, and a feedback iterative technique. We apply a surjectivity result for pseudomonotone operators and properties of the Clarke subgradient operator to establish existence and a priori estimates for solutions to an approximate problem. Finally, through a limiting procedure for solutions of the hybrid iterative system, the solvability of (DHVI) is proved without imposing any convexity condition on the nonlinear function \(u\mapsto f(t,x,u)\) and compactness of \(C_0\)-semigroup \(e^{A(t)}\).		Stanislaw Migórski;Shengda Zeng	2018	J. Global Optimization	10.1007/s10898-018-0667-5	mathematical optimization;mathematical analysis;backward euler method;inequality;parabola;mathematics;banach space;nonlinear system;compact space;subgradient method;convexity	Vision	73.92034863119969	19.45874586584043	133686
2c6c289164bc1d7de8247eacbeedf6290074e4f9	a family of quasi-minimal residual methods for nonsymmetric linear systems	general and miscellaneous mathematics computing and information science;convergence;data;transpose free methods;quasi minimal residual approach;mathematical logic;65n20;iterative methods;lanczos method;comparative evaluations;quasi minimal residual;calculation methods;nonsymmetric linear systems;theoretical data;algorithms;evaluation;numerical data 990200 mathematics computers;65f10;information	The quasi-minimal residual (QMR) algorithm by Freund, Gutknecht, and Nachtigal [Research Institute forAdvanced Computer Science Tech. Report 91.09, NASA Ames Research Center, Moffett Field, CA; Numer. Math., 60 (1991), pp. 315–339], in addition to its ability to avoid breakdowns, also improves the irregular convergence behavior encountered by the biconjugate gradient (BiCG) method through the incorporation of quasi minimization. This paper studies this quasi-minimal residual approach applied, for simplicity, to the nonlook-ahead version of the QMR method. In particular, a family of quasi-minimal residual methods is defined where on both ends lie the original QMR method and a minimal residual method while lying in between are the variants derived here. These variants are shown to consume a little more computational work per iterations than the original QMR method, but generally give an even better convergence behavior as well as lower iteration counts. The same idea was also applied to a recently proposed ...	linear system	Charles H. Tong	1994	SIAM J. Scientific Computing	10.1137/0915006	mathematical optimization;mathematical logic;mathematical analysis;convergence;information;computer science;theoretical computer science;evaluation;mathematics;iterative method;algorithm;quantum mechanics;statistics;data;algebra	HPC	82.71500962854647	23.12258172562109	133715
c0eb7351224c0f77d17c7f7d64261836e28ae866	exact worst-case convergence rates of the proximal gradient method for composite convex minimization	proximal gradient method;composite convex optimization;convergence rates;worst-case analysis;90c25;90c22;90c20	We study the worst-case convergence rates of the proximal gradient method for minimizing the sum of a smooth strongly convex function and a non-smooth convex function, whose proximal operator is available. We establish the exact worst-case convergence rates of the proximal gradient method in this setting for any step size and for different standard performance measures: objective function accuracy, distance to optimality and residual gradient norm. The proof methodology relies on recent developments in performance estimation of first-order methods, based on semidefinite programming. In the case of the proximal gradient method, this methodology allows obtaining exact and non-asymptotic worst-case guarantees, that are conceptually very simple, although apparently new. On the way, we discuss how strong convexity can be replaced by weaker assumptions, while preserving the corresponding convergence rates. We also establish that the same fixed step size policy is optimal for all three performance measures. Finally, we extend recent results on the worst-case behavior of gradient descent with exact line search to the proximal case.	best, worst and average case;convex function;convex optimization;first-order predicate;gradient descent;line search;loss function;optimization problem;proximal gradient method;proximal gradient methods for learning;proximal operator;semidefinite programming	Adrien B. Taylor;Julien M. Hendrickx;François Glineur	2018	J. Optimization Theory and Applications	10.1007/s10957-018-1298-1	gradient descent;mathematical optimization;proximal gradient methods for learning;combinatorics;mathematical analysis;mathematics;stochastic gradient descent;proximal gradient methods	ML	73.16862050503867	24.843840714086593	133946
fc877aaf3c4fc41ee049c90cd590024448ebd756	proof mining in r-trees and hyperbolic spaces	proof mining;uniformly convex;hyperbolic geometry;functional analysis;hyperbolic space;asymptotic regularity;fixed point theory;nonexpansive mapping;nonexpansive functions;r trees;hyperbolic spaces	This paper is part of the general project of proof mining, developed by Kohlenbach. By ”proof mining” we mean the logical analysis of mathematical proofs with the aim of extracting new numerically relevant information hidden in the proofs. We present logical metatheorems for classes of spaces from functional analysis and hyperbolic geometry, like Gromov hyperbolic spaces, R-trees and uniformly convex hyperbolic spaces. Our theorems are adaptations to these structures of previous metatheorems of Gerhardy and Kohlenbach, and they guarantee a-priori, under very general logical conditions, the existence of uniform bounds. We give also an application in nonlinear functional analysis, more specifically in metric fixed-point theory. Thus, we show that the uniform bound on the rate of asymptotic regularity for the Krasnoselski-Mann iterations of nonexpansive mappings in uniformly convex hyperbolic spaces obtained in a previous paper is an instance of one of our metatheorems.	fixed-point theorem;iteration;nonlinear system;numerical analysis;r-tree;spaces;uniformly convex space	Laurentiu Leustean	2006	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2006.05.039	functional analysis;r-tree;hyperbolic manifold;hyperbolic equilibrium point;relatively hyperbolic group;mathematical analysis;discrete mathematics;topology;hyperbolic space;hyperbolic geometry;hyperbolic group;mathematics;fixed-point theorem;hyperbolic tree	Theory	73.13357988927434	18.86773260379805	134173
21264204d04659fd533896feb6d46ea1cf9c4c8a	locating the eigenvalues of matrix polynomials	15a18;matrix polynomials;tropical algebra;polynomial eigenvalue problems;rouche s theorem;15a80;pellet s theorem;newton s polygon;47j10;location of roots;15a22	Some known results for locating the roots of polynomials are extended to the case of matrix polynomials. In particular, a theorem by A.E. Pellet [Bulletin des Sciences Mathématiques, (2), vol 5 (1881), pp.393-395], some results of D.A. Bini [Numer. Algorithms 13:179-200, 1996] based on the Newton polygon technique, and recent results of M. Akian, S. Gaubert and M. Sharify (see in particular [LNCIS, 389, Springer p.p.291-303] and [M. Sharify, Ph.D. thesis, École Polytechnique, ParisTech, 2011]). These extensions are applied for determining effective initial approximations for the numerical computation of the eigenvalues of matrix polynomials by means of simultaneous iterations, like the Ehrlich-Aberth method. Numerical experiments that show the computational advantage of these results are presented. AMS classification: 15A22,15A80,15A18,47J10.		Dario Bini;Vanni Noferini;Meisam Sharify	2013	SIAM J. Matrix Analysis Applications	10.1137/120886741	difference polynomials;matrix analysis;polynomial matrix;combinatorics;discrete orthogonal polynomials;classical orthogonal polynomials;calculus;mathematics;orthogonal polynomials;algebra;wilson polynomials	Theory	78.01422300359582	20.91237882569422	134400
1b80ac8a43f5a4bc3efb33411816070bde265dbd	efficient implementations of the generalized lasso dual path algorithm	qr decomposition	We consider efficient implementations of the generalized lasso dual path algorithm of Tibshirani & Taylor (2011). We first describe a generic approach that covers any penalty matrix D and any (full column rank) matrix X of predictor variables. We then describe fast implementations for the special cases of trend filtering problems, fused lasso problems, and sparse fused lasso problems, both with X = I and a general matrix X. These specialized implementations offer a considerable improvement over the generic implementation, both in terms of numerical stability and efficiency of the solution path computation. These algorithms are all available for use in the genlasso R package, which can be found in the CRAN repository.	algorithm;computation;kerrison predictor;lasso;numerical stability;r language;sparse matrix	Taylor Arnold;Ryan J. Tibshirani	2014	CoRR		mathematical optimization;combinatorics;machine learning;mathematics;qr decomposition;elastic net regularization	ML	79.96146530439135	24.136723579353582	134538
905a94aea8d0c1b1031da5b561958aaed69a79f0	a parallel algorithm for block-tridiagonal linear systems	sistema lineal;metodo directo;distributed memory;algoritmo paralelo;matriz bloque;analisis numerico;parallel algorithm;matematicas aplicadas;algorithm performance;algorithm complexity;mathematiques appliquees;complejidad algoritmo;matrix inversion;inversion matriz;linear system;analyse numerique;tridiagonal matrix;algorithme parallele;numerical analysis;complexite algorithme;matrice bloc;theoretical analysis;resultado algoritmo;matriz tridiagonal;algebra lineal numerica;algebre lineaire numerique;performance algorithme;inversion matrice;block matrix;numerical linear algebra;numerical experiment;systeme lineaire;applied mathematics;methode directe;matrice tridiagonale;direct method	In this paper, a parallel algorithm for solving block-tridiagonal linear systems on distributed-memory multi-computer is presented. According to theoretical analysis, convergent velocity and complexity of this algorithm are the same as BSOR method's, and parallelism is the same as BJ method's. Moreover, two examples have been implemented on HP rx2600 cluster, and the numerical experiments indicate that our algorithm is feasible and effective.	linear system;parallel algorithm	Xining Cui;Quanyi Lü	2006	Applied Mathematics and Computation	10.1016/j.amc.2005.04.037	direct method;tridiagonal matrix;distributed memory;ramer–douglas–peucker algorithm;numerical analysis;calculus;mathematics;parallel algorithm;numerical linear algebra;linear system;quantum algorithm for linear systems of equations;block matrix;algorithm;algebra	Logic	81.90656905142227	20.618378774503952	135058
7b1afcba8b78687a954a5b133f86ff623f435618	positive bases in numerical optimization	linear algebra;numerical optimization;positive bases;design and implementation;numerical algorithm;nonlinear optimization	The theory of positive bases introduced by C. Davis in 1954 does not appear in most modern texts on linear algebra but has re-emerged in publications in optimization journals. In this paper some simple properties of this highly useful theory are highlighted and applied to both theoretical and practical aspects of the design and implementation of numerical algorithms for nonlinear optimization.	algorithm;linear algebra;mathematical optimization;nonlinear programming;nonlinear system;numerical analysis	I. D. Coope;Cathy J. Price	2002	Comp. Opt. and Appl.	10.1023/A:1013760716801	probabilistic-based design optimization;discrete optimization;mathematical optimization;test functions for optimization;meta-optimization;nonlinear programming;theoretical computer science;linear algebra;mathematics;continuous optimization;numerical stability;algorithm;global optimization	ML	69.14387862505448	22.669047197050162	135136
76e544dfcbacfab999e1a50c668f740066a67c26	extended lanczos bidiagonalization algorithm for low rank approximation and its applications	institutional repositories;fedora;singular value decomposition;vital;lanczos bidiagonalization;vtls;low rank approximation;ils	We propose an extended Lanczos bidiagonalization algorithm for finding a low rank approximation of a given matrix. We show that this method can yield better low-rank approximations than standard Lanczos bidiagonalization algorithm, without increasing the cost too much. We also describe a partial reorthogonalization process that can be used to maintain an adequate level of orthogonality of the Lanczos vectors in order to produce accurate low-rank approximations. We demonstrate the effectiveness and applicability of our algorithm for a number of applications.	algorithm;bidiagonalization;low-rank approximation	Xuansheng Wang;François Glineur;Linzhang Lu;Paul Van Dooren	2016	J. Computational Applied Mathematics	10.1016/j.cam.2015.12.039	mathematical optimization;combinatorics;discrete mathematics;lanczos algorithm;lanczos approximation;mathematics;singular value decomposition;low-rank approximation;algebra	Theory	82.16661373473805	22.929290567195505	135317
5da1cff6ddd50af9cc980090acad5899058134f3	a relaxed nonmonotone adaptive trust region method for solving unconstrained optimization problems	adaptive trust region methods;trust region methods;nonmonotone techniques	In this paper, we present a new relaxed nonmonotone trust region method with adaptive radius for solving unconstrained optimization problems. The proposed method combines a relaxed nonmonotone technique with a modified version of the adaptive trust region strategy proposed by Shi and Guo (J Comput Appl Math 213:509– 520, 2008). Under some suitable and standard assumptions, we establish the global convergence property as well as the superlinear convergence rate for the new method. Numerical results on some test problems show the efficiency and effectiveness of the new proposed method in practice.	local convergence;mathematical optimization;numerical method;rate of convergence;trust region;turing test	M. Reza Peyghami;D. Ataee Tarzanagh	2015	Comp. Opt. and Appl.	10.1007/s10589-015-9726-8	mathematical optimization;calculus;mathematics;trust region;mathematical economics	AI	76.88950724387888	23.169248290390435	135527
97a80a1731f1f55ce143180e50e95a2176ff8045	numerical determination of partial spectrum of hermitian matrices using a lánczos method with selective reorthogonalization	lanczos;krylov;hermitian;spectrum;eigenvector;eigenvalue;lanso;lattice	We introduce a new algorithm for finding the eigenvalues and eigenvectors of Hermitian matrices within a specified region, based upon the LANSO algorithm of Parlett and Scott. It uses selective reorthogonalization to avoid the duplication of eigenpairs in finite-precision arithmetic, but uses a new bound to decide when such reorthogonalization is required, and only reorthogonalizes with respect to eigenpairs within the region of interest. We investigate its performance for the Hermitian Wilson–Dirac operator γ5D in lattice quantum chromodynamics, and compare it with previous methods. © 2012 Elsevier B.V. All rights reserved.	computation;dirac operator;lanczos algorithm;lattice qcd;region of interest	Chris Johnson;A. D. Kennedy	2013	Computer Physics Communications	10.1016/j.cpc.2012.11.003	combinatorics;mathematical analysis;eigenvalues and eigenvectors;mathematics;physics;quantum mechanics;algebra	Theory	81.9343413091232	22.66052639145741	135593
d0626fb10421f4daed72fe1e9b47a3169151b959	trust regions based on conic functions in linear and nonlinear programming	interior point methods;centering;conjugate gradients;nonlinear programming;trust region;karmarkar s algorithm;nonlinear minimization;linear programming;trust regions;conic functions	Abstract#R##N#An optimization method is developed based on ellipsoidal trust regions that are defined by conic functions. It provides a powerful unifying theory from which can be derived a variety of interesting and potentially useful optimization algorithms, in particular, conjugate-gradient-like algorithms for nonlinear minimization and Karmarkar-like interior-point algorithms for linear programming.	nonlinear programming	J. L. Nazareth	1995	Numerical Lin. Alg. with Applic.	10.1002/nla.1680020305	mathematical optimization;combinatorics;discrete mathematics;criss-cross algorithm;linear-fractional programming;nonlinear programming;linear programming;interior point method;mathematics;trust region;karmarkar's algorithm	HPC	72.1883924522458	22.678021854880075	135659
894575c584defa1f033383a11159602551edd693	semidefinite relaxation for linear programs with equilibrium constraints	bilevel program;lpec;semidefinite relaxation	Abstract#R##N##R##N#In this paper, we present a semidefinite programming (SDP) relaxation for linear programs with equilibrium constraints (LPECs) to be used in a branch-and-bound (B&B) algorithm. The procedure utilizes the global optimal solution of LPECs and was motivated by the B&B algorithm proposed by Bard and Moore for linear/quadratic bilevel programs, where complementarities are recursively enforced. We propose the use of the SDP relaxation to generate bounds at the nodes of the B&B tree. Computational results compare the quality of the bounds given by the SDP relaxation with the ones given by conventional linear programming relaxations.	linear programming relaxation	Marcia H. C. Fampa;Wendel A. X. Melo;Nelson Maculan	2013	ITOR	10.1111/j.1475-3995.2012.00869.x	mathematical optimization;combinatorics;mathematics;mathematical economics	Theory	72.59355667139974	23.20198868972971	135840
f75ffa13ebb21d382f8acaac478047dadd3c13c3	strong stationarity for optimal control of the obstacle problem with control constraints	obstacle problem;35j86;strong stationarity;49k21;optimal control;control constraints;complementarity conditions	We consider the distributed optimal control of the obstacle problem with control constraints. Since Mignot proved in 1976 the necessity of a system which is equivalent to strong stationarity, it has been an open problem whether such a system is still necessary in the presence of control constraints. Using moderate regularity of the optimal control and an assumption on the control bounds (which is implied by ua < 0 ≤ ub quasi-everywhere (q.e.) in Ω in the case of an upper obstacle y ≤ ψ), we can answer this question in the afrmative. We also present counterexamples showing that strong stationarity may not hold if ua < 0 or 0 ≤ ub are violated.	obstacle problem;optimal control;stationary process	Gerd Wachsmuth	2014	SIAM Journal on Optimization	10.1137/130925827	mathematical optimization;mathematical analysis;optimal control;mathematics;obstacle problem;mathematical economics	Theory	70.34657208611058	18.68757181633428	136156
766ff8618085ca9834c67ade0ada6d2f0b9a59e4	inexact methods in the numerical solution of stiff initial value problems	equation non lineaire;ecuacion no lineal;equation differentielle;probleme raide;matrice jacobi;convergence;numerical solution;stiff problem;methode newton;inexact newton method;implementation;ordinary differential equation;estudio comparativo;differential equation;initial value problem;ecuacion diferencial;etude comparative;ejecucion;jacobi matrix;convergencia;inexact method;matriz jacobi;methode inexacte;comparative study;problema rigido;nonlinear equation;problema valor inicial;metodo newton;newton method;probleme valeur initiale;non linear equation	Inexact Newton methods can be effectively used in codes for large stiff initial value problems for ordinary differential equations. In this paper we give a new convergence result for Inexact Newton methods. Further, we indicate how this general result can be used and actually implemented to obtain an efficient code for solving stiff initial value problems.	code;newton;numerical partial differential equations	Benedetta Morini;Maria Macconi	1999	Computing	10.1007/s006070050034	l-stability;ordinary differential equation;mathematical optimization;mathematical analysis;convergence;nonlinear system;calculus;mathematics;implementation;backward differentiation formula;initial value problem;differential equation	Theory	82.37459997875244	18.33260226677995	136168
e6e0cf18b9e0dbe214710d7b429c8d01ed6a2773	a convex envelope formula for multilinear functions	concave extension;convex envelope;bilinear programming;necessary and sufficient condition;linearization;global optimization;nonlinear 0 1 optimization	Convex envelopes of multilinear functions on a unit hypercube are polyhedral. This wellknown fact makes the convex envelope approximation very useful in the linearization of non-linear 0–1 programming problems and in global bilinear optimization. This paper presents necessary and sufficient conditions for a convex envelope to be a polyhedral function and illustrates how these conditions may be used in constructing of convex envelopes. The main result of the paper is a simple analytical formula, which defines some faces of the convex envelope of a multilinear function. This formula proves to be a generalization of the well known convex envelope formula for multilinear monomial functions.	approximation;bilinear filtering;convex function;convex hull;mathematical optimization;monomial;nonlinear system;polyhedron;whole earth 'lectronic link	Anatoliy D. Rikun	1997	J. Global Optimization	10.1023/A:1008217604285	convex analysis;subderivative;support function;mathematical optimization;conic optimization;combinatorics;mathematical analysis;convex optimization;convex polytope;pseudoconvex function;convex combination;linear matrix inequality;convex conjugate;nonlinear programming;quasiconvex function;convex hull;absolutely convex set;convexity in economics;mathematics;convex set;logarithmically convex function;linearization;effective domain;proper convex function;global optimization	Theory	70.81368736199313	21.24732010614077	136383
c138832e74f1ee00a6afcc9a3848e4fba2373c47	new algorithms for computing the matrix sine and cosine separately or simultaneously	65 numerical analysis;double angle formula;matrix cosine;pade approximation;rational approximation;matrix theory;forward error;matrix exponential;pad e approximation;matrix sine;15 linear and multilinear algebra;triple angle formula;matrix function;backward error;65f60;65f30;matlab	Several existing algorithms for computing the matrix cosine employ polynomial or rational approximations combined with scaling and use of a double angle formula. Their derivations are based on forward error bounds. We derive new algorithms for computing the matrix cosine, the matrix sine, and both simultaneously, that are backward stable in exact arithmetic and behave in a forward stable manner in floating point arithmetic. Our new algorithms employ both Padé approximants of sinx and new rational approximants to cosx and sinx obtained from Padé approximants to ex. The amount of scaling and the degree of the approximants are chosen to minimize the computational cost subject to backward stability in exact arithmetic. Numerical experiments show that the new algorithms have backward and forward errors that rival or surpass those of existing algorithms and are particularly favorable for triangular matrices.	algorithm;algorithmic efficiency;approximation;experiment;image scaling;numerical stability;padé approximant;polynomial;the matrix;triangular matrix	Awad H. Al-Mohy;Nicholas J. Higham;Samuel D. Relton	2015	SIAM J. Scientific Computing	10.1137/140973979	padé approximant;matrix function;mathematical optimization;combinatorics;mathematical analysis;mathematics;matrix exponential;matrix;algebra	Theory	80.51388368904291	19.001802519829223	136449
7df4145a66ffdaaddfa0fe2c33d3eb64c4d62ad4	local linear convergence of ista and fista on the lasso problem	local convergence analysis;65k05;90c25;90 08;linear convergence;lasso;first order algorithms	We establish local linear convergence bounds for the ISTA and FISTA iterations on the model LASSO problem. We show that FISTA can be viewed as an accelerated ISTA process. Using a spectral analysis, we show that, when close enough to the solution, both iterations converge linearly, but FISTA slows down compared to ISTA, making it advantageous to switch to ISTA toward the end of the iteration processs. We illustrate the results with some synthetic numerical examples.	algorithm;best, worst and average case;converge;iteration;lasso;linear logic;numerical analysis;rate of convergence;spectral density estimation;synthetic intelligence;the matrix;worst-case complexity	Shaozhe Tao;Daniel Boley;Shuzhong Zhang	2016	SIAM Journal on Optimization	10.1137/151004549	econometrics;mathematical optimization;combinatorics;lasso;mathematics;rate of convergence	ML	81.44675262471297	23.96528205653061	136561
67a26f35810a0391dcfac4c0e3ab4a10b3309de6	updating the principal angle decomposition	filtering;filtrage;15a29;analisis numerico;65f20;data compression;65f05;implementation;metodo descomposicion;filtrado;methode decomposition;metodo secuencial;sequential method;systeme adaptatif;analyse numerique;15a23;algorithme;identificacion sistema;algorithm;decomposition method;numerical analysis;system identification;adaptive system;methode sequentielle;sistema adaptativo;compresion dato;wiener filter;implementacion;65f15;identification systeme;compression donnee;algoritmo	A class of fast Householder-based sequential algorithms for updating the Principal Angle Decomposition is introduced. The updated Principal Angle Decomposition is of key importance in the adaptive implementation of several fundamental operations on correlated processes, such as adaptive Wiener filtering, rank-adaptive system identification, and rank and data compression concepts using canonical coordinates. An instructive example of rank-adaptive system identification is examined experimentally.		Peter Strobach	2008	Numerische Mathematik	10.1007/s00211-008-0156-8	data compression;filter;mathematical optimization;decomposition method;system identification;numerical analysis;adaptive system;calculus;mathematics;wiener filter;implementation;algorithm	ML	81.72126561458468	20.90593033672748	136673
dc14bf4fff15517d38cb68c03952e65d98fe64a2	a complexity reduction for the long-step path-following algorithm for linear programming	goldstein armijo rule;complexity reduction;polynomial algorithm;linear programming;linear program;logarithmic barrier function;90c05;interior point method;path following	A modification of previously published long-step path-following algorithms for the solution of the linear programming problem is presented. This modification uses the simple Goldstein–Armijo rule. A $\sqrt{n} $ reduction in the complexity bound is obtained, while a linesearch may still be done. Depending on the updating scheme for the barrier parameter, the resulting complexity bounds are $O(n^3 L)$ or $O(n^{3.5} L)$.	algorithm;linear programming;reduction (complexity)	Dick den Hertog;Kees Roos;Jean-Philippe Vial	1992	SIAM Journal on Optimization	10.1137/0802006	mathematical optimization;combinatorics;discrete mathematics;linear programming;interior point method;mathematics;reduction	Theory	73.64945751908344	26.15445090440749	136726
508c9190546afa918f0c950ed7c6abe4cd5a8ff4	the large equal radius conditions and time of arrival geolocation algorithms	metodo cuadrado menor;sistema lineal;metodo directo;calcul scientifique;minimum variance;methode moindre carre;analisis numerico;nonlinear least squares;bancroft method;critical point;least squares method;65f05;point critique;62l12;matrix inversion;inversion matriz;linear system;analyse numerique;algorithme;algorithm;computacion cientifica;numerical analysis;global positioning system;numero de condicionamiento;algebra lineal numerica;algebre lineaire numerique;condition number;65f35;inversion matrice;15a12;time of arrival;punto critico;numerical linear algebra;systeme lineaire;scientific computation;65h10;methode directe;35b38;65f15;indice conditionnement;geolocation;direct method;variance;variancia;algoritmo	We show that several methods for solving time of arrival (TOA) geolocation problems depend crucially on what we call the large equal radius (LER) conditions. For example, we analyze Bancroft's method and show that it works well assuming the LER conditions are satisfied. However, when these conditions are violated, we show that the method significantly deteriorates and does not achieve the minimum possible variance for a least squares solution. A direct method is also presented that finds all the critical points of the least squares functional. The method is found to perform well when the LER conditions are not satisfied but suffers from conditioning problems when they are satisfied. We also show that a simple but statistically inferior method that we call the reduced least squares (RLS) method deteriorates when the LER conditions are satisfied.	algorithm;geolocation;time of arrival	Louis A. Romero;Jeff Mason;David M. Day	2008	SIAM J. Scientific Computing	10.1137/070699020	direct method;minimum-variance unbiased estimator;global positioning system;numerical analysis;condition number;time of arrival;calculus;geolocation;mathematics;non-linear least squares;variance;numerical linear algebra;linear system;critical point;least squares;algorithm	HPC	82.73973903329731	20.088405387700252	136919
b49bf129a44bcdc3a94a165c29e7c7b2e7b61662	practical criteria for positive-definite matrix, m-matrix and hurwitz matrix	analisis numerico;critere stabilite;matematicas aplicadas;mathematiques appliquees;criterio estabilidad;positive definite;m matrix;time delay;analyse numerique;numerical analysis;matrice definie positive;positive definite matrix;inner operations of determinants;determinante;stability analysis;sign preserving operations of determinants;determinant;gaussian elimination;hurwitz matrix;stability criterion;reseau neuronal;matriz definida positiva;applied mathematics;red neuronal;neural network	All the conventional criterions to verify if a matrix is positive definite, Hurwitz or an M-matrix, such as Sylvester condition and Hurwitz theorem, require us to compute n determinants. This paper develops a new and simple criterion, based on a new type of Gaussian elimination process. In contrast to the traditional criterions, the new criterion only computes one determinate. Thus, the computing cost is greatly reduced. An illustrative example is given to show the application of the new criterion in the stability analysis of Hopfied-type neural networks with time delays.	ampersand;artificial neural network;computational complexity theory;gaussian elimination;instability;property (philosophy);routh–hurwitz stability criterion	Wei Li	2007	Applied Mathematics and Computation	10.1016/j.amc.2006.06.098	routh–hurwitz stability criterion;calculus;mathematics;positive-definite matrix;hurwitz matrix;hurwitz polynomial;artificial neural network;algorithm;algebra	ML	81.02474019709757	19.841328550299394	137247
45a8d8cea4b2d38ed47e1ffdfa30b38d362a52f5	on two ways of stabilizing the hierarchical basis multilevel methods	65n30;transformation ondelette;base hierarchique;methode hb;preconditionnement;precontionneur additif;numerical method;methode n niveaux;multigrille;preconditioning;partition n niveau;metodo numerico;multigrid;multigrilla;algebraic multilevel iteration amli methods;approximate wavelets;precondicionamiento;transformacion ondita;hierarchical basis;optimal order preconditioners;iteration method;multilevel methods;additive preconditioner;multilevel method;65f10;methode numerique;wavelet transformation	A survey of two approaches for stabilizing the hierarchical basis (HB) multilevel preconditioners, both additive and multiplicative, is presented. The first approach is based on the algebraic extension of the two-level methods, exploiting recursive calls to coarser discretization levels. These recursive calls can be viewed as inner iterations (at a given discretization level), exploiting the already defined preconditioner at coarser levels in a polynomially-based inner iteration method. The latter gives rise to hybrid-type multilevel cycles. This is the so-called (hybrid) algebraic multilevel iteration (AMLI) method. The second approach is based on a different direct multilevel splitting of the finite element discretization space. This gives rise to the so-called wavelet multilevel decomposition based on L2-projections, which in practice must be approximated. Both approaches—the AMLI one and the one based on approximate wavelet decompositions—lead to optimal relative condition numbers of the multilevel preconditioners.	approximation algorithm;condition number;discretization;exploit (computer security);finite element method;iteration;linear algebra;preconditioner;recursion (computer science);utility functions on indivisible goods;wavelet	Panayot S. Vassilevski	1997	SIAM Review	10.1137/S0036144595282211	mathematical optimization;discrete mathematics;numerical analysis;calculus;mathematics;preconditioner;iterative method;multigrid method;algebra	EDA	82.72938540803555	21.160845786236447	137267
5fec88643964a5eb487348e36ae1e61d64dd37f4	solving the generalized sylvester matrix equation av + bw = evf via a kronecker map	kronecker map;matematicas aplicadas;mathematiques appliquees;generalized sylvester matrix equation;equation matricielle;matrix equation;sylvester sum;39b42;sylvester matrix equation;ecuacion matricial;parametric solutions;applied mathematics;15a24	This note considers the solution to the generalized Sylvester matrix equation AV + BW = EV F with F being an arbitrary matrix, where V and W are the matrices to be determined. With the help of the Kronecker map, some properties of the Sylvester sum are first proposed. By applying the Sylvester sum as tools, an explicit parametric solution to this matrix equation is established. The proposed solution is expressed by the Sylvester sum, and allows the matrix F to be undetermined. c © 2008 Elsevier Ltd. All rights reserved.	extended validation certificate;sylvester matrix;the matrix	Ai-Guo Wu;Feng Zhu;Guang-Ren Duan	2008	Appl. Math. Lett.	10.1016/j.aml.2007.12.004	matrix difference equation;matrix function;sylvester matrix;combinatorics;applied mathematics;sylvester's law of inertia;calculus;sylvester equation;sylvester's formula;mathematics;matrix addition;matrix exponential;matrix;algebra	AI	78.49218317329375	19.596719593609944	137408
563f39b1bfe05ca3db8655a38dadce44fac16819	all solutions of the yang-baxter-like matrix equation for rank-one matrices		Let A = pqT , where p and q are two nonzero n-dimensional complex vectors. We solve the quadratic matrix equation AXA = XAX to nd all the solutions. Abbreviated title: Solving the Yang-Baxter-like Matrix Equation Keywords: Rank-one matrix; matrix equation; Jordan form; spectral perturbation. AMS Subject Classications: 15A18		Haiyan Tian	2016	Appl. Math. Lett.	10.1016/j.aml.2015.07.009	matrix difference equation;mathematical optimization;mathematical analysis;mathematics;algebra	Theory	78.55747278040273	20.729751292703885	137450
91427e2af211576f3a9f329a506819b628830ab5	a dual-active-set algorithm for positive semi-definite quadratic programming	quadratic programming;optimisation;quadratic program;generic algorithm;programmation quadratique;convex programming;positive definite;convex optimization;programmation convexe;algorithme;active set method;mathematical programming;algorithms;goldfarb idnani method;programacion cuadratica;optimization;positive semi definite;programmation mathematique;positive semi definite programming;programacion convexa	Because of the many important applications of quadratic programming, fast and efficient methods for solving quadratic programming problems are valued. Gotdfarb and Idnani (1983) describe one such method. Well known to be efficient and numerically stable, the Goldfarb and Idnani method suffers only from the restriction that in its original form it cannot be applied to problems which are positive semi-definite rather than positive definite. In this paper, we present a generalization of the Goldfarb and Idnani method to the positive semi-definite case and prove finite termination of the generalized algorithm. In our generalization, we preserve the spirit of the Goldfarb and Idnani method, and extend their numerically stable implementation in a natural way. © 1997 The Mathematical Programming Society, Inc. Published by Elsevier Science B.V.	active set method;algorithm;mathematical optimization society;numerical stability;quadratic programming;semiconductor industry	Natashia Boland	1997	Math. Program.	10.1007/BF02614503	mathematical optimization;combinatorics;discrete mathematics;convex optimization;mathematics;positive-definite matrix;quadratic programming	ML	74.72628417934988	22.3960460232181	137459
ed2412c5072ec5860c3faff9530fa4291e1e7f36	an algorithm for some special nondifferentiable optimization problems	optimization problem	We present an algorithm for a special class of nonlinear optimization problems with a nondifferentiable objective function and prove its convergence. The method does not use the notion of subgradient and no convexity assumption is made. We discuss the application of the method to an exact penalty function and to an environmental noise-management problem.	algorithm	Yves Smeers	1977	Operations Research	10.1287/opre.25.5.808	optimization problem;mathematical optimization;combinatorics;mathematical analysis;mathematics	Theory	72.07079051382384	22.282373321655406	137492
bd807fa6474c285c92e33d03f18a1e9db1d24aaa	constraint theory, part iii: inequality and discrete relations	convergence;numerical analysis;discrete model;statistical analysis;machine learning;probability distribution;pattern classification;mathematical model;pattern recognition;constraint theory;constraint theory mathematical model pattern classification convergence pattern recognition machine learning statistical analysis numerical analysis probability distribution jacobian matrices;jacobian matrices	Parts I and II of this three-part paper provided the fundamental concepts underlying constraint theory whose goal is the systematic determination of whether a mathematical model and its computations are well posed. In addition to deriving results for the general relation, special relations defined as universal and regular were treated. This concluding part treats two more special relations: inequality and discrete. Employing the axiom of transitivity for inequalities, results relating to the consistency of a mathematical model of inequalities in terms of its model graph are derived. Rules for the simultaneous propagation of four types of constraint, over, point, interval, and slack, through a heterogeneous model graph are established. In contrast to other relation types, discrete relations point constrain every relevant variable, so that finding intrinsic constraint sources is trivial. A general procedure is provided to determine the allowability of requested computations on a discrete model.	social inequality	George J. Friedman;Cornelius T. Leondes	1969	IEEE Trans. Systems Science and Cybernetics	10.1109/TSSC.1969.300260	constraint logic programming;probability distribution;mathematical optimization;combinatorics;discrete mathematics;binary constraint;convergence;numerical analysis;computer science;constraint graph;mathematical model;mathematics;statistics	Logic	69.87697465570777	19.605456798821862	137513
6ff23b3e3b91f719b4b15cdd3419ab44a3020b03	double-regularization proximal methods, with complementarity applications	proximal algorithms;variational inequality problem;variational inequalities;smoothing method;augmented lagrangian method;variational inequality;complementarity;maximal monotone operator;augmented lagrangian	We consider the variational inequality problem formed by a general set-valued maximal monotone operator and a possibly unbounded “box” in Rn , and study its solution by proximal methods whose distance regularizations are coercive over the box. We prove convergence for a class of double regularizations generalizing a previously-proposed class of Auslender et al. Using these results, we derive a broadened class of augmented Lagrangian methods. We point out some connections between these methods and earlier work on “pure penalty” smoothing methods for complementarity; this connection leads to a new form of augmented Lagrangian based on the “neural” smoothing function. Finally, we computationally compare this new kind of augmented Lagrangian to three previously-known varieties on the MCPLIB problem library, and show that the neural approach offers some advantages. In these tests, we also consider primal-dual approaches that include a primal proximal term. Such a stabilizing term tends to slow down the algorithms, but makes them more robust.	algorithm;augmented lagrangian method;calculus of variations;complementarity theory;matrix regularization;maximal set;smoothing;social inequality;thinking outside the box;variational inequality;monotone	Paulo J. S. Silva;Jonathan Eckstein	2006	Comp. Opt. and Appl.	10.1007/s10589-005-3065-0	mathematical optimization;combinatorics;mathematical analysis;variational inequality;augmented lagrangian method;mathematics	ML	73.55182607226365	22.215700987817716	138034
4a55dccdd5f5c58cdedf2d6d0c0b44d9a69173de	bregman monotone optimization algorithms	monotone operator;iterative method;subgradient projection;optimisation;convex feasibility problem;bregman monotone;resolvent;bregman distance;algorithm analysis;optimizacion;nova;espacio banach;optimal method;essentially strict convex function;banach space;research repository;university of newcastle;polinomio legendre;legendre polynomial;monotonie;metodo iterativo;block iterative method;convex function;methode iterative;monotonicity;bregman projection;90c25;fejer monotone;numerical algorithm;proximal mapping;47h05;proximal point algorithm;optimization;analyse algorithme;monotonia;legendre function;polynome legendre;iteration method;26 xx real functions;optimal algorithm;90c48;analisis algoritmo;essentially smooth function;institutional repository;espace banach;bb class operator;research online	A broad class of optimization algorithms based on Bregman distances in Banach spaces is unified around the notion of Bregman monotonicity. A systematic investigation of this notion leads to a simplified analysis of numerous algorithms and to the development of a new class of parallel block-iterative surrogate Bregman projection schemes. Another key contribution is the introduction of a class of operators that is shown to be intrinsically tied to the notion of Bregman monotonicity and to include the operators commonly found in Bregman optimization methods. Special emphasis is placed on the viability of the algorithms and the importance of Legendre functions in this regard. Various applications are discussed.	algorithm;bregman divergence;email;iteration;mathematical optimization;optimization problem;optimizing compiler;monotone	Heinz H. Bauschke;Jonathan M. Borwein;Patrick L. Combettes	2003	SIAM J. Control and Optimization	10.1137/S0363012902407120	mathematical optimization;mathematical analysis;calculus;mathematics;iterative method;bregman divergence;algebra	Theory	73.73154105428726	21.8223579945696	138049
3f0dcb0106b4bd3378ea5478dcbca09c22a65094	p-factor-approach to degenerate optimization problems	nonlinear mapping;nonlinear programming;inequality constraint;lagrange multiplier;objective function;optimization problem;mathematical program with equilibrium constraints;degeneration;nonlinear problem;system of equations;nonlinear optimization;optimality condition	The paper describes and analyzes an application of the p-regularity theory to nonregular, (irregular, degenerate) nonlinear optimization problems. The pregularity theory, also known as the factor-analysis of nonlinear mappings, has been developing successfully for the last twenty years. The p-factor-approach is based on the construction of a p-factor-operator, which allows us to describe and analyze nonlinear problems in the degenerate case. First, we illustrate how to use the p-factor-approach to solve degenerate optimization problems with equality constraints, in which the Lagrange multiplier associated with the objective function might be equal to zero. We then present necessary and sufficient optimality conditions for a degenerate optimization problem with inequality constraints. The p-factor-approach is also used for solving mathematical programs with equilibrium constraints (MPECs). We show that the constraints are 2-regular at the solution of the MPEC. This property allows us to localize the minimizer independently of the objective function. The same idea is applied to some other nonregular nonlinear programming problems and allows us to reduce these problems to a regular system of equations without an objective function. keywords: Lagrange optimality conditions, degeneracy, p-regularity	degeneracy (graph theory);factor analysis;lagrange multiplier;mathematical optimization;mathematical programming with equilibrium constraints;nonlinear programming;nonlinear system;optimization problem;social inequality	Olga Brezhneva;Alexey Tret'yakov	2005		10.1007/0-387-33006-2_8	fractional programming;optimization problem;mathematical optimization;mathematical analysis;nonlinear programming;mathematics;mathematical economics;vector optimization;karush–kuhn–tucker conditions	ML	71.9686397607377	21.715702436926534	138073
313c16a77ffced45fe4a40a8df09d753d9a5e129	alternating projection method for a class of tensor equations		Abstract This paper considers how to solve a class of tensor equations arising from the unified definition of tensor–vector products. Of special interest is the order-3 tensor equation whose solutions are the intersection of a group of quadrics from a geometric point of view. Inspired by the method of alternating projections for set intersection problems, we develop a hybrid alternating projection algorithm for solving order-3 tensor equations. The local linear convergence of the alternating projection method is established under suitable conditions. Some numerical experiments are conducted to evaluate the effect of the proposed algorithm.		Zhibao Li;Yu-Hong Dai;Huan Gao	2019	J. Computational Applied Mathematics	10.1016/j.cam.2018.07.013	mathematics;mathematical analysis;projection method;mathematical optimization;rate of convergence;tensor;quadric;dykstra's projection algorithm;intersection (set theory)	Vision	73.84101681370362	22.53194969634227	138087
2b05083e4387f0fd6bae6aa939dd033115a9ae9f	the linearized bregman method via split feasibility problems: analysis and generalizations	sparse solutions;bregman projections;65k10;linearized bregman method;68u10;90c25;split feasibility problems	The linearized Bregman method is a method to calculate sparse solutions to systems of linear equations. We formulate this problem as a split feasibility problem, propose an algorithmic framework based on Bregman projections and prove a general convergence result for this framework. Convergence of the linearized Bregman method will be obtained as a special case. Our approach also allows for several generalizations such as other objective functions, incremental iterations, incorporation of nongaussian noise models or box constraints.	bregman divergence;bregman method;iteration;linear equation;sparse matrix;system of linear equations	Dirk A. Lorenz;Frank Schöpfer;Stephan Wenger	2014	SIAM J. Imaging Sciences	10.1137/130936269	econometrics;mathematical optimization;mathematical analysis;mathematics;bregman divergence	ML	74.49553302664249	24.689561953748107	138587
95bddc8c96b02b329f16e5d4bd0d97cec2c95ec5	strong convergence theorems of a general iterative process for a finite family of lambdai-strict pseudo-contractions in q-uniformly smooth banach spaces	strong convergence;iterative algorithm;fixed point;λ strict pseudo contraction;uniformly smooth banach space;q uniformly smooth banach spaces	"""In this paper, we study a general iterative process to have strong convergence for a finite family of @l""""i-strict pseudo-contractive non-self-mappings in the framework of q-uniformly smooth Banach spaces. Our results improve and extend the corresponding results announced by many others."""	iterative method;uniformly smooth space	Gang Cai;Chang song Hu	2010	Computers & Mathematics with Applications	10.1016/j.camwa.2009.07.068	eberlein–šmulian theorem;mathematical optimization;banach manifold;mathematical analysis;topology;uniformly convex space;modes of convergence;interpolation space;mathematics;fixed point;iterative method;unconditional convergence;lp space;c0-semigroup;algebra	Vision	74.31713088271569	18.781802092476298	138604
1d1fe24821d3a61b18f3dd4a6680d34520eb0cd5	iterative approximation of solutions of equations of hammerstein type in certain banach spaces	equations of hammerstein type;modulus of smoothness;uniformly gâteaux differentiable norm;accretive operators;duality maps	"""Let E be a q-uniformly smooth real Banach space. For each i=1,2,...m, let F""""i,K""""i:E->E be bounded and accretive mappings. Assume that the generalized Hammerstein equation u+@?""""i""""=""""1^mK""""iF""""iu=0 has a solution in E. Our purpose in this paper is to construct a new explicit iterative sequence and prove strong convergence of the sequence to a solution of the generalized Hammerstein equation. Our result generalizes and extends the results of Chidume and Ofoedu (2011) [C.E. Chidume, E.U. Ofoedu, Solution of nonlinear integral equations of Hammerstein type, Nonlinear Anal. 74 (2011) 4293-4299] and Chidume and Shehu (2012) [C.E. Chidume, Y. Shehu, Approximation of solutions of generalized equations of Hammerstein type, Comput. Math. Appl. 63 (2012) 966-974]. Numerical example of our result is also included."""	approximation;iterative method	Charles E. Chidume;Yekini Shehu	2013	Applied Mathematics and Computation	10.1016/j.amc.2012.11.041	mathematical optimization;mathematical analysis;discrete mathematics;mathematics	Theory	75.41640442606806	18.42819288671032	138736
d46d68d243b29a29ba19e3ad737688a8a5ac6b37	orthogonality in multiobjective optimization	ensemble balance;multiobjective programming;optimizacion vectorial;optimum pareto;programmation multiobjectif;orthogonality;condicion necesaria;matematicas aplicadas;mathematiques appliquees;cost function;balance set;probleme non lineaire;karush kuhn tucker;lagrange multiplier;nonlinear problems;funcion coste;necessary condition;multiplicateur lagrange;optimisation vectorielle;multiplicador lagrange;vector optimization;fonction cout;multiobjective optimization;condition necessaire;conjunto finito;applied mathematics;pareto optimum;optimo pareto;orthogonalite;ortogonalidad;programacion multiobjetivo	Abs t rac t -P roper t i e s of nonlinear multiobjective problems implied by the Karush-Kuhn-Tucker necessary conditions are investigated. It is shown that trajectories of Lagrange multipliers corresponding to the components of the vector cost function are orthogonal to the corresponding trajectories of vector deviations in the balance space (to the balance set for Pareto solutions). ~) 2003 Elsevier Science Ltd. All rights reserved.	karush–kuhn–tucker conditions;lagrange multiplier;loss function;mathematical optimization;multi-objective optimization;nonlinear system;pareto efficiency;tucker decomposition	Alejandro Balbás;Efim A. Galperin;Pedro Jiménez Guerra	2003	Appl. Math. Lett.	10.1016/S0893-9659(03)80066-6	mathematical optimization;orthogonality;multi-objective optimization;calculus;mathematics;mathematical economics;lagrange multiplier;vector optimization;karush–kuhn–tucker conditions	AI	73.71934071084567	20.432659163927525	138823
f70ad1b0129fd2d2a68660ecf269ddafbd609220	differentiation by integration with jacobi polynomials	ill posed problems;derivada funcion;algebraic approach;metodo polinomial;jacobi orthogonal polynomials;65b99;analisis numerico;65d30;computacion informatica;matematicas aplicadas;58c20;mathematiques appliquees;approximation numerique;integracion numerica;polinomio ortogonal;numerical differentiation ill posed problems jacobi orthogonal polynomials orthogonal series;stochastic method;numerical method;fonction reguliere;noisy data;relacion convergencia;orthogonal polynomial;taux convergence;taylor expansion;65k15;convergence rate;polinomio jacobi;65d25;42c20;metodo lanczos;polynome jacobi;time delay;aproximacion numerica;analyse numerique;truncamiento;numerical differentiation;42c05;65c20;acceleration convergence;iii posed problems;function derivative;numerical analysis;estimation erreur;lanczos method;serie ortogonal;metodo numerico;troncature;polynomial method;error estimation;ciencias basicas y experimentales;numerical integration;methode jacobi;serie orthogonale;matematicas;ill posed problem;methode lanczos;estimacion error;aceleracion convergencia;metodo jacobi;methode stochastique;funcion regular;numerical approximation;polynome orthogonal;truncation;grupo a;applied mathematics;methode polynomiale;derivee fonction;integration numerique;28xx;orthogonal series;jacobi polynomial;methode numerique;33c45;smooth function;jacobi method;convergence acceleration;metodo estocastico	Équipe Projet ALIEN, INRIA Lille-Nord Europe, Parc Scientifique de la Haute Borne 40, avenue Halley Bât.A, Park Plaza, 59650 Villeneuve d’Ascq, France Paul Painlevé (CNRS, UMR 8524), Université de Lille 1, 59650, Villeneuve d’Ascq, France Arts et Métiers ParisTech centre de Lille, Laboratory of Applied Mathematics and Metrology (L2MA), 8 Boulevard Louis XIV, 59046 Lille Cedex, France LAGIS (CNRS, UMR 8146), École Centrale de Lille, BP 48, Cité Scientifique, 59650 Villeneuve d’Ascq, France	halley's method;jacobi method;linear algebra;polynomial	Dayan Liu;Olivier Gibaru;Wilfrid Perruquetti	2011	J. Computational Applied Mathematics	10.1016/j.cam.2010.12.023	mathematical analysis;numerical analysis;calculus;mathematics;geometry;algorithm;algebra	Comp.	81.25177630303823	19.219189160972526	138885
bca26012f1d522dfd9a05a0dcb9fdee88f3002a1	a method for nonsmooth optimization problems	convergence theorem;variational inequality problem;satisfiability;variational inequality;unconstrained optimization;nonsmooth optimization	We introduce a method for solving a nonsmooth optimization problem. The algorithm constructs a sequence {xk } where xk +1 = [xtilde] and [xtilde] is a solution of a variational inequality problem and satisfies an Armijo-type condition. A convergence theorem for the algorithm is established and some numerical results are reported.	mathematical optimization	Gianfranco Corradi	2004	Int. J. Comput. Math.	10.1080/0020716031000148197	mathematical optimization;mathematical analysis;discrete mathematics;variational inequality;mathematics;satisfiability	Theory	73.78651908632312	22.24838758481291	139384
10636c68eef9257e8ddc5da87df720f5145677bd	technical note - further necessary conditions for the existence of a solution to the multi-index problem	indexation	This note gives three new sets of necessary conditions for the existence of a solution to the multi-index problem. One includes as subsets all of the Haley conditions, and all of the 1967 Moravek and Vlach conditions; the second includes as subsets the 1967 Moravek and Vlach conditions; while the third includes as subsets both the first and second new sets of conditions.		Graham Smith	1973	Operations Research	10.1287/opre.21.1.380	mathematical optimization;mathematical analysis;mathematics;mathematical economics	Theory	69.57430797947147	20.91274603071704	139460
14923a3a91bf6ea933851a086b3876f7da31071d	adaptive lanczos methods for recursive condition estimation	recursive least square;singular value;eigenvalues;lanczos method;signal processing;adaptive method;nonlinear problem;scientific computing;condition number;numerical experiment	Estimates for the condition number of a matrix are useful in many areas of scientific computing, including: recursive least squares computations, optimization, eigenanalysis, and general nonlinear problems solved by linearization techniques where matrix modification techniques are used. The purpose of this paper is to propose anadaptiveLanczosestimator scheme, which we callale, for tracking the condition number of the modified matrix over time. Applications to recursive least squares (RLS) computations using the covariance method with sliding data windows are considered.ale is fast for relatively smalln-parameter problems arising in RLS methods in control and signal processing, and is adaptive over time, i.e., estimates at timet are used to produce estimates at timet+1. Comparisons are made with other adaptive and non-adaptive condition estimators for recursive least squares problems. Numerical experiments are reported indicating thatale yields a very accurate recursive condition estimator.	computation;computational science;condition number;experiment;mathematical optimization;microsoft windows;nonlinear system;recursion (computer science);recursive least squares filter;signal processing	William R. Ferng;Gene H. Golub;Robert J. Plemmons	1991	Numerical Algorithms	10.1007/BF02145580	mathematical optimization;mathematical analysis;discrete mathematics;lanczos resampling;eigenvalues and eigenvectors;lanczos approximation;condition number;signal processing;mathematics;singular value;algebra	ML	82.20540551021892	21.81210417815463	139471
2fc893534fea18a6e6c5b3f39fe4287cc43c7fbb	polynomial convergence of second-order mehrotra-type predictor-corrector algorithms over symmetric cones	euclidean jordan algebra;interior-point methods;mehrotra-type algorithm;polynomial complexity;second-order methods;symmetric cone	This paper presents an extension of the variant of Mehrotra’s predictor– corrector algorithm which was proposed by Salahi and Mahdavi-Amiri (Appl. Math. Comput. 183:646–658, 2006) for linear programming to symmetric cones. This algorithm incorporates a safeguard in Mehrotra’s original predictor–corrector algorithm, which keeps the iterates in the prescribed neighborhood and allows us to get a reasonably large step size. In our algorithm, the safeguard strategy is not necessarily used when the affine scaling step behaves poorly, which is different from the algorithm of Salahi and Mahdavi-Amiri. We slightly modify the maximum step size in the affine scaling step and extend the algorithm to symmetric cones using the machinery of Euclidean Jordan algebras. Based on the Nesterov–Todd direction, we show that the iteration-complexity bound of the proposed algorithm is O(r log ε−1), where r is the rank of the associated Euclidean Jordan algebras and ε > 0 is the required precision.	affine scaling;algorithm;image scaling;iteration;kerrison predictor;linear programming;polynomial;predictor–corrector method	Changhe Liu;Hongwei Liu;Xinze Liu	2012	J. Optimization Theory and Applications	10.1007/s10957-012-0018-5	mathematical optimization;combinatorics;discrete mathematics;mathematics	Theory	78.1562106757537	24.318160814365445	139851
6bac0f4f9e7284a5a05958be81740cfac52f79fc	a rank-one fitting method with descent direction for solving symmetric nonlinear equations	global convergence;rank one update;nonlinear equation;nonlinear equations;descent direction	In this paper, a rank-one updated method for solving symmetric nonlinear equations is proposed. This method possesses some features: 1) The updated matrix is positive definite whatever line search technique is used; 2) The search direction is descent for the norm function; 3) The global convergence of the given method is established under reasonable conditions. Numerical results show that the presented method is interesting.	curve fitting;descent direction;line search;local convergence;nonlinear programming;nonlinear system;numerical method	Gonglin Yuan;Zhongxing Wang;Zengxin Wei	2009	IJCNS	10.4236/ijcns.2009.26061	mathematical optimization;combinatorics;mathematical analysis;nonlinear system;descent direction;mathematics	Robotics	75.92695063043273	23.61988783187689	139932
c1a9b209970d3fad90bbf74d0c54ae433f72c97e	mixed quasi complementarity problems in topological vector spaces	variational inequalities;quasi variational inequalities;topological vector space;49j40;variational inequality;complementarity problems;90c33;complementarity problem;existence results	In this paper, we introduce and consider a new class of complementarity problems, which are called the generalized mixed quasi-complementarity problems in a topological vector space. We show that the generalized mixed quasi-complementarity problems are equivalent to the generalized mixed quasi variational inequalities. Using a new type of KKM mapping theorem, we study the existence of a solution of the generalized mixed quasi-variational inequalities and generalized mixed quasicomplementarity problems. Several special cases are also discussed. The results obtained in this paper can be viewed as extension and generalization of the previously known results. 2000 Mathematics subject classification: 49J40, 90C33.	calculus of variations;complementarity theory;karp's 21 np-complete problems;mathematics subject classification;variational inequality	Ali P. Farajzadeh;Muhammad Aslam Noor;Saira Zainab	2009	J. Global Optimization	10.1007/s10898-008-9368-9	mathematical optimization;mathematical analysis;variational inequality;topology;mathematics;mixed complementarity problem;complementarity theory	Theory	72.38539799719109	20.248577968030922	139969
4e7f9a293ed16e8365b6e8c9181cd90cf64d8287	shrinking dimer dynamics and its applications to saddle point search	convergence analysis;shrinking dimer dynamics;49k35;dimer methods;34k28;saddle point search algorithms;nonlinear stability;65p99;linear stability;37n30;37m05	Saddle point search on an energy surface has broad applications in fields like materials science, physics, chemistry, and biology. In this paper, we present the shrinking dimer dynamics (SDD), a dynamic system which can be applied to locate a transition state on an energy surface corresponding to an index-1 saddle point where the Hessian has a negative eigenvalue. By searching for the saddle point and the associated unstable direction simultaneously in a single dynamic system defined in an extended space, we show that unstable index-1 saddle points of the energy become linearly stable steady equilibria of the SDD which makes the SDD a robust approach for the computation of saddle points. The time discretization of the SDD is connected to various iterative algorithms, including the popular dimer method used in many practical applications. Our study of these discretization schemes lays a rigorous mathematical foundation for the corresponding iterative saddle point search algorithms. Both linear local asymptotic stability analysis and optimal error reduction (convergence) rates are presented and further confirmed by numerical experiments. Global convergence and nonlinear asymptotic stability are also illustrated for some simpler systems. Applications of the SDD in both finiteand infinite-dimensional energy spaces are discussed.	algorithmic efficiency;align (company);approximation algorithm;authorization;benchmark (computing);command & conquer:yuri's revenge;computation;control theory;discretization;domino tiling;dual ec drbg;dynamical system;euler;experiment;first-order predicate;gradient;hessian;in-phase and quadrature components;iteration;iterative method;krylov–bogolyubov theorem;linear programming relaxation;local convergence;mathematical optimization;nl (complexity);newton;nonlinear system;numerical analysis;numerical aperture;rate of convergence;reduction strategy (code optimization);search algorithm;ut-vpn	Jingyan Zhang;Qiang Du	2012	SIAM J. Numerical Analysis	10.1137/110843149	linear stability;mathematical optimization;combinatorics;control theory;mathematics;saddle point	ML	80.35004882914608	24.19126561820334	139975
1c6c6c2edcf04c371d66bbeb3975a3ee608a515f	infeasibility spheres for finding robust solutions of blending problems with quadratic constraints	quadratic programming;quadratic program;branch and bound algorithm;blending;robust solutions;branch and bound	The blending problem is studied as a problem of finding cheap robust feasible solutions on the unit simplex fulfilling linear and quadratic inequalities. Properties of a regular grid over the unit simplex are discussed. Several tests based on spherical regions are described and evaluated to check the feasibility of subsets and robustness of products. These tests have been implemented into a Branch-and-Bound algorithm that reduces the set of points evaluated on the regular grid. The whole is illustrated numerically.	algorithm;alpha compositing;branch and bound;hallin's spheres;numerical analysis;regular grid	Leocadio G. Casado;Eligius M. T. Hendrix;Inmaculada García	2007	J. Global Optimization	10.1007/s10898-007-9157-x	mathematical optimization;combinatorics;discrete mathematics;mathematics;quadratic programming;branch and bound	Visualization	71.24837847411297	24.303299853088905	140002
d4d47c16258b371c6b1d1be0e868caf55e202348	galerkin trigonometric wavelet methods for the natural boundary integral equations	ondelette hermite;boundary integral method;problema valor limite;galerkin method;integral equation;metodo galerkin;methode ondelette galerkin;hypersingular kernel;hermite wavelets;biharmonic equations;degree of freedom;boundary integral equation;ondelette;boundary value problem;natural boundary integral equations;methode galerkin;noyau hypersingulier;methode integrale frontiere;nbie;approximate solution;block diagonalization;galerkin wavelet method;equation integrale;hermite wavelet;metodo integral frontera;equation biharmonique;ecuacion integral;natural bondary integral equation;error estimate;biharmonic equation;probleme valeur limite;wavelets;ecuacion biarmonica	A simply supported boundary value problem of biharmonic equations in the unit disk is reduced into an equivalent second kind natural boundary integral equation (NBIE) with hypersingular kernel (in the sense of Hadamard finite part). Trigonometric Hermite interpolatory wavelets introduced by Quak [Math. Comput. 65 (1996) 683-722] as trial functions are used to its Galerkin discretization with 2^J^+^1 degrees of freedom on the boundary. It is proved that the stiffness matrix is a block diagonal matrix and its diagonal elements are some symmetric and block circulant submatrices. The simple computational formulae of the entries in stiffness matrix are obtained. These show that we only need to compute 2(2^J+J+1) elements of a 2^J^+^2x2^J^+^2 stiffness matrix. The error estimates for the approximation solutions are established. Finally, numerical examples are given.	galerkin method;poisson wavelet	Wensheng Chen;Wei Lin	2001	Applied Mathematics and Computation	10.1016/S0096-3003(99)00263-5	wavelet;biharmonic equation;mathematical optimization;mathematical analysis;boundary value problem;calculus;galerkin method;mathematics;degrees of freedom;integral equation	Robotics	82.54769211415208	19.555685320727612	140129
06fc997b4e435fb45940251955667f75c42e9ae8	on computing of arbitrary positive integer powers for one type of even order symmetric circulant matrices - i	analisis numerico;jordan s form;matematicas aplicadas;mathematiques appliquees;polinomio chebychev;circulant matrix;forme jordan;calculo automatico;matriz simetrica;eigenvalues;chebyshev polynomial;eigenvector;analyse numerique;computing;matrice circulante;symmetric matrix;eigenvalue;calcul automatique;polynome tchebychev;vector propio;numerical analysis;chebyshev polynomials;valor propio;circulant matrices;matrice symetrique;valeur propre;applied mathematics;vecteur propre;puissance expression;matriz circulante;eigenvectors	Abstract   In this paper we derive the general expression of the  l -th power ( l  ∈  N ) for one type of symmetric circulant matrices of order  n  = 2 p  ( p  ∈  N ,  p  ⩾ 2).	circulant matrix	Jonas Rimas	2006	Applied Mathematics and Computation	10.1016/j.amc.2005.01.121	combinatorics;eigenvalues and eigenvectors;calculus;mathematics;algebra	Theory	78.74853863121511	19.342395547198073	140242
218c119847853a84a693f7eea122f861021a6be1	equivalent conditions for convergence of splittings of non-hermitian indefinite matrices	matrix with a dominant symmetric part;boundary value problem;indefinite matrix;non hermitian matrix;convergent splitting;linear equations;iteration method;hermitian matrix	We present some sufficient and necessary conditions for convergent splitting of a non-Hermitian indefinite matrix. Some sufficient conditions to determinate a matrix with a (strongly) dominant symmetric part for a class of boundary value problem are also obtained. These results are applicable to identify the convergence of iterative methods for solving large sparse systems of linear equations.		Jinhai Chen;Weiguo Li	2007	J. Sci. Comput.	10.1007/s10915-005-9022-3	matrix splitting;hermitian matrix;matrix function;mathematical optimization;mathematical analysis;eigendecomposition of a matrix;nonnegative matrix;eigenvalues and eigenvectors;single-entry matrix;boundary value problem;centrosymmetric matrix;band matrix;convergent matrix;hamiltonian matrix;square matrix;mathematics;diagonalizable matrix;positive-definite matrix;iterative method;pascal matrix;linear equation;state-transition matrix;matrix decomposition;quantum mechanics;matrix;transpose;symmetric matrix;algebra	Theory	80.70423188864444	21.67833699343124	140285
bee4ebb1f9139697e72b7c6dc7754c138629e3e6	role of hull-consistency in the hiba_usne multithreaded solver for nonlinear systems		This paper considers incorporating a hull-consistency enforcing procedure in an interval branch-and-prune method. Hull-consistency has been used with interval algorithms in several solvers, but its implementation in a multithreaded environment is non-trivial. We describe arising issues and discuss the ways to deal with them. Numerical results for some benchmark problems are presented and analyzed.	nonlinear system;solver;thread (computing)	Bartlomiej Jacek Kubica	2017		10.1007/978-3-319-78054-2_36	parallel computing;hull;theoretical computer science;nonlinear system;multithreading;computer science;solver	Robotics	76.85647391861997	20.55950954669351	140326
c2c488a6bc664498acff26be5d0833012bd5cb89	on the spectral radius of nonnegative matrices	null	We give lower bounds for the spectral radius of nonnegative matrices and nonnegative symmetric matrices, and prove necessary and sufficient conditions to achieve these bounds.		Zhou Bo	2000	Australasian J. Combinatorics		mathematical analysis;matrix (mathematics);spectral radius;nonnegative matrix;mathematics	Theory	78.517911003147	20.22022546640505	140343
6cd3715eaa357d9da3e455a8995d97506e3265f9	local convergence analysis of inexact gauss-newton like methods under majorant condition	nonlinear least squares;computacion informatica;nonlinear least squares problems;inexact gauss newton like methods;gauss newton;local convergence;satisfiability;least squares problem;ciencias basicas y experimentales;matematicas;grupo a;majorant condition	In this paper, we present a local convergence analysis of inexact Gauss-Newton like methods for solving nonlinear least squares problems. Under the hypothesis that the derivative of the function associated with the least squares problem satisfies a majorant condition, we obtain that the method is well-defined and converges. Our analysis provides a clear relationship between the majorant function and the function associated with the least squares problem. It also allows us to obtain an estimate of convergence ball for inexact Gauss-Newton like methods and some important, special cases.	gauss–newton algorithm;local convergence;newton	Orizon Pereira Ferreira;M. L. N. Gonçalves;P. Roberto Oliveira	2012	J. Computational Applied Mathematics	10.1016/j.cam.2011.12.008	local convergence;generalized least squares;total least squares;mathematical optimization;mathematical analysis;calculus;mathematics;non-linear least squares;least squares;satisfiability;recursive least squares filter	ML	81.187844497772	18.631814048543	140425
3a69cc6d4d516fe9be80da90376b6b1cfd7bb0f0	solving sparse rational linear systems	structured matrix;sparse integer matrix;linear system;linear system solving;linear equations	We propose a new algorithm to find a rational solution to a sparse system of linear equations over the integers. This algorithm is based on a p-adic lifting technique combined with the use of block matrices with structured blocks. It achieves a sub-cubic complexity in terms of machine operations subject to a conjecture on the effectiveness of certain sparse projections. A LinBox-based implementation of this algorithm is demonstrated, and emphasizes the practical benefits of this new method over the previous state of the art.	algorithm;cubic function;lifting scheme;linear equation;linear system;sparse matrix;system of linear equations	Wayne Eberly;Mark Giesbrecht;Pascal Giorgi;Arne Storjohann;Gilles Villard	2006		10.1145/1145768.1145785	system of linear equations;mathematical optimization;combinatorics;discrete mathematics;sparse approximation;mathematics;linear equation;linear system;matrix-free methods;quantum algorithm for linear systems of equations;algebra	AI	81.27918066805631	22.4899996149367	140715
89a8e476772f6876cdff08513b997363152e3bba	global optimality conditions in nonconvex optimization	d.c. functions;inequality constraints;global optimality conditions;lagrange function;saddle point;linearized problem;90c26	In this paper, we address the nonconvex optimization problem, with the goal function and the inequality constraints given by the functions represented by the difference of convex functions. The effectiveness of the classical Lagrange function and the max-merit function is being investigated as the merit functions of the original problem. In addition to the classical apparatus of optimization theory, we apply the new global optimality conditions for the auxiliary problems related to the Lagrange and max-merit functions.	convex function;lagrange multiplier;lagrangian (field theory);loss function;mathematical optimization;optimization problem;program optimization;social inequality	Alexander S. Strekalovsky	2017	J. Optimization Theory and Applications	10.1007/s10957-016-0998-7	mathematical optimization;mathematical analysis;convex optimization;mathematics;mathematical economics	ML	72.23434252864865	21.732330108252366	141003
5cd1111af4c4d85cd388631b2a6ed8440b3c29ca	a globally convergent lp-newton method for piecewise smooth constrained equations: escaping nonstationary accumulation points	constrained equation;piecewise smooth equation;lp-newton method;global convergence;quadratic convergence;90c33;91a10;49m05;49m15	The LP-Newton method for constrained equations, introduced some years ago, has powerful properties of local superlinear convergence, covering both possibly nonisolated solutions and possibly nonsmooth equation mappings. A related globally convergent algorithm, based on the LP-Newton subproblems and linesearch for the equation’s infinity-norm residual, has recently been developed. In the case of smooth equations, global convergence of this algorithm to B-stationary points of the residual over the constraint set has been shown, which is a natural result: nothing better should generally be expected in variational settings. However, for the piecewise smooth case only a property weaker than B-stationarity could be guaranteed. In this paper, we develop a procedure for piecewise smooth equations that avoids undesirable accumulation points, thus achieving the intended property of B-stationarity.	algorithm;line search;local convergence;newton;newton's method;rate of convergence;stationary process;tree accumulation;variational principle	Andreas Fischer;Markus Herrich;Alexey F. Izmailov;W. Scheck;Mikhail V. Solodov	2018	Comp. Opt. and Appl.	10.1007/s10589-017-9950-5	piecewise;mathematical optimization;residual;mathematical analysis;rate of convergence;mathematics;newton's method;uniform norm;convergence (routing)	ML	75.39637524413367	23.980686544904064	141127
16f1f3ca23e0062e69c5b20bd299d0e15e693b41	estimating the jacobian of the singular value decomposition	linear algebra;image recognition;reconocimiento imagen;decomposition valeur singuliere;estimation mouvement;estimacion movimiento;singular value decomposition;motion estimation;metodo factorizacion;reconstruction image;reconstruccion imagen;factorization method;image reconstruction;reconnaissance image;pattern recognition;uncertainty relation;decomposicion valor singular;reconnaissance forme;reconocimiento patron;methode factorisation;multivariate regression	The Singular Value Decomposition (SVD) of a matrix is a linear algebra tool that has been successfully applied to a wide variety of domains. The present paper is concerned with the problem of estimating the Jacobian of the SVD components of a matrix with respect to the matrix itself. An exact analytic technique is developed that facilitates the estimation of the Jacobian using calculations based on simple linear algebra. Knowledge of the Jacobian of the SVD is very useful in certain applications involving multivariate regression or the computation of the uncertainty related to estimates obtained through the SVD. The usefulness and generality of the proposed technique is demonstrated by applying it to the estimation of the uncertainty for three different vision problems, namely self-calibration, epipole computation and rigid motion estimation. Key-words: Singular Value Decomposition, Jacobian, Uncertainty, Calibration, Structure from Motion. M. Lourakis was supported by the VIRGO research network (EC Contract No ERBFMRX-CT96-0049) of the TMR Programme. Calcul de la Jacobienne de la Décomposition en Valeurs Singulières: Théorie et applications Résumé : La technique de Décomposition en Valeurs Singulières (SVD) d’une matrice est un outil algèbrique qui a trouvé de nombreuses applications en vision par ordinateur. Dans ce rapport, nous nous intéressons au problème de l’estimation de la jacobienne de la SVD par rapport aux coefficients de la matrice initiale. Cette jacobienne est très utile pour toute une gamme d’applications faisant intervenir des estimations aux moindres carrés (pour lesquelles on utilise la SVD) ou bien des calculs d’incertitude pour des grandeurs estimées de cette manière. Une solution analytique simple à ce problème est présentée. Elle exprime la jacobienne à partir de la SVD de la matrice à l’aide d’opérations très simples d’algèbre linéaire. L’utilité et la généralité de la technique est démontrée en l’appliquant à trois problèmes de vision: l’auto-calibration, le calcul d’épipoles et l’estimation de mouvements rigides. Mots-clés : Décomposition en valeurs singulières, Jacobienne, Incertitude, Calibration, Structure à partir du mouvement. Estimating the Jacobian of the Singular Value Decomposition: Theory and Applications 3	coefficient;computation;estdomains;general linear model;jacobian matrix and determinant;les trophées du libre;linear algebra;motion estimation;singular value decomposition;structure from motion;the matrix;triple modular redundancy;virgo	Théodore Papadopoulo;Manolis I. A. Lourakis	2000		10.1007/3-540-45054-8_36	iterative reconstruction;computer vision;multivariate statistics;mathematical optimization;linear algebra;calculus;motion estimation;mathematics;geometry;singular value decomposition	Vision	81.56141259756522	29.21174857882728	141214
f6c8cd2d5de8d7fbc1e8a0af02e9f2a8db325934	strong solvability of interval max-plus systems and applications to optimal control	interval max-plus systems;strong solvability;uniquely strong solvability;algorithm;optimal control	Abstract   This paper considers the strong solvability of interval max-plus systems. The solvable entry and solvable interval are introduced to establish the necessary and sufficient condition for strong solvability of interval max-plus systems. It is proved that such a criterion is equivalent to the criterion proposed by Cechlarova and Cuninghame-Green. The interval strong solution of interval max-plus systems is described and the necessary and sufficient condition for uniquely strong solvability is given. The method of testing strong solvability of interval max-plus systems is constructive and leads to an effective algorithm. To illustrate our results, a workshop optimal control problem is addressed.	optimal control	Hongwei Zhang;Yuegang Tao;Zilong Zhang	2016	Systems & Control Letters	10.1016/j.sysconle.2016.07.005	mathematical optimization;mathematical analysis;discrete mathematics;mathematics	Embedded	69.55103746797148	20.889979098445114	141609
06ee64c0588af6070498236ca47ef71458250498	well-posedness for optimization problems with constraints defined by variational inequalities having a unique solution	monotone operator;monotone operators;optimization problems with variational inequalities constraints;optimization problem;penalty method;approximating sequences;strong well posedness;stackelberg problems;variational inequality;generalized well posedness;bilevel programming;bilevel programming problems;gap function	We introduce various notions of well-posedness for a family of variational inequalities and for an optimization problem with constraints defined by variational inequalities having a unique solution. Then, we give sufficient conditions for well-posedness of these problems and we present an application to an exact penalty method.	variational inequality;variational principle	M. Beatrice Lignola;Jacqueline Morgan	2000	J. Global Optimization	10.1023/A:1008370910807	optimization problem;mathematical optimization;combinatorics;mathematical analysis;variational inequality;penalty method;mathematics;bilevel optimization	Theory	71.92186189862838	21.529591397803916	141913
173e8edf1328fc3ea27fb7c45102dc56eba25af7	the * congruence class of the solutions to a system of matrix equations		has attracted many people’s attention and many results have been obtained about system (1) with various constraints, such as Hermitian, positive definite, positive semidefinite, reflexive, and generalized reflexive solutions (see [2–10]). Studying the least-square solutions of the system of matrix equations (1) is also a very active research topic (see [11–16]). It is well known that Hermitian, positive definite and positive semidefinite matrices are the special case of congruence. Therefore investigating the congruence class of a solution of the matrix equation (1) is very meaningful. In 2005, Horn et al. [1] studied the possible congruence class of a square solution when linear matrix equation AX = B is solvable. In 2009, Zheng et al. [17] describe congruence class of least-square and minimum norm leastsquare solutions of the equation AX = B when it is not solvable and discuss a congruence class of the solutions of the system (1) when it is solvable. To our knowledge, so far there has been little investigation of congruence class of the least-square andminimum norm least-square solutions to (1) when it is not solvable. Motivated by the work mentioned above, we investigate the congruence class of the least-square and the minimum norm least-square solutions to the system of complex matrix equation (1) by generalized singular value decomposition (GSVD) and canonical correlation decomposition (CCD).		Yu-Ping Zhang;Chang-Zhou Dong	2014	J. Applied Mathematics	10.1155/2014/703529	congruence of squares;mathematical analysis;topology;lu decomposition;mathematics;matrix decomposition;singular value decomposition;algebra	AI	78.4833680051903	21.762487279025716	142003
5b66a27df532299ac55a6fe7eb0f5cac0ac6c04b	comparison results of the preconditioned aor methods for l-matrices	preconditioned aor paor method;krylov subspace method;satisfiability;linear system;preconditioner;l matrix	In this paper, we first provide comparison results of several types of the preconditioned AOR (PAOR) methods for solving a linear system whose coefficient matrix is an  L -matrix satisfying some weaker conditions than those used in the recent literature. Next, we propose an application of PAOR method to a preconditioner of Krylov subspace method. Lastly, numerical results are provided to show that Krylov subspace method with the PAOR preconditioner performs quite well as compared with the ILU (0) preconditioner.	biconjugate gradient stabilized method;coefficient;conjugate gradient method;experiment;generalized minimal residual method;integrated circuit layout design protection;iterative method;krylov subspace;linear system;numerical analysis;preconditioner	Jae Heon Yun	2011	Applied Mathematics and Computation	10.1016/j.amc.2011.08.085	mathematical optimization;mathematical analysis;control theory;mathematics;preconditioner;linear system;algebra;satisfiability	Robotics	81.68959769861355	21.836725159105274	142347
29e0d2ea5d342be6dab3d9f598e7e678e52524dd	a primal-dual regularized interior-point method for convex quadratic programs		Interior-point methods in augmented form for linear and convex quadratic programming require the solution of a sequence of symmetric indefinite linear systems which are used to derive search directions. Safeguards are typically required in order to handle free variables or rank-deficient Jacobians. We propose a consistent framework and accompanying theoretical justification for regularizing these linear systems. Our approach can be interpreted as a simultaneous proximal-point regularization of the primal and dual problems. The regularization is termed exact to emphasize that, although the problems are regularized, the algorithm recovers a solution of the original problem, for appropriate values of the regularization parameters.	algorithm;free variables and bound variables;interior point method;jacobian matrix and determinant;linear system;matrix regularization;quadratic programming	Michael P. Friedlander;Dominique Orban	2012	Math. Program. Comput.	10.1007/s12532-012-0035-2	mathematical optimization;proximal gradient methods for learning;mathematical analysis;mathematics;geometry	ML	77.27853883568255	24.578429732348074	142374
e9082e120ff80fc3389eb04e61af013ffb5527f5	globally optimal solutions to vision using convex and quasi-convex optimization	iterative refinement;global solution;optimization technique;projective geometry;convex optimization;fractional programming;computer vision;semi definite program;second order cone program;global optimization;branch and bound;gauss newton method;bundle adjustment	"""Vision Geometry is the are of Computer Vision that deals with computing the geometry of the 3D world from sequences of images. It grew out of Photogrammetry, a field that goes back at least to the start of the 20th century. In the 1990s this field was transformed by the application of methods of Projective Geometry, leading to many new algorithms and deployment of the new methods in a wide variety of applications.#R##N##R##N#The algorithmic basis for Vision Geometry still ultimately relied on a technique called """"bundle adjustment"""", involving iterative refinement of initial solutions by Newton or Gauss-Newton methods. These had the disadvantage of often finding local rather than global minima.#R##N##R##N#Recent work has focussed on applying different optimization techniques, particularly Convex Optimization techniques to attempt to find guaranteed global solutions to these problems. I will talk about progress in this area, through the use of methods such as Second Order Cone Programming, branch-and-bound fractional programming and semidefinite programming."""	convex optimization;program optimization	Richard Hartley	2007		10.1007/978-3-540-76928-6_3	fractional programming;convex geometry;mathematical optimization;conic optimization;combinatorics;projective geometry;convex optimization;second-order cone programming;mathematics;geometry;bundle adjustment;branch and bound;global optimization	Vision	72.50397905012788	25.020367138132716	142414
a72d602580fc648316e87177687925e8b2de25e9	interior epigraph directions method for nonsmooth and nonconvex optimization via generalized augmented lagrangian duality	90c90;90c26;deflected subgradient method;nonconvex optimization;kissing number problem;nonsmooth feasible directions algorithm;nonsmooth optimization;49m29;49m37;augmented lagrangian duality	We propose and study a new method, called the Interior Epigraph Directions (IED) method, for solving constrained nonsmooth and nonconvex optimization. The IED method considers the dual problem induced by a generalized augmented Lagrangian duality scheme, and obtains the primal solution by generating a sequence of iterates in the interior of the dual epigraph. First, a deflected subgradient (DSG) direction is used to generate a linear approximation to the dual problem. Second, this linear approximation is solved using a Newton-like step. This Newton-like step is inspired by the Nonsmooth Feasible Directions Algorithm (NFDA), recently proposed by Freire and co-workers for solving unconstrained, nonsmooth convex problems. We have modified the NFDA so that it takes advantage of the special structure of the epigraph of the dual function. We prove that all the accumulation points of the primal sequence generated by the IED method are solutions of the original problem. We carry out numerical experiments by using test problems from the literature. In particular, we study several instances of the Kissing Number Problem, previously solved by various approaches such as an augmented penalty method, the DSG method, as well as the popular differentiable solvers ALBOX (a predecessor of ALGENCAN), Ipopt and LANCELOT. Our experiments show that the quality of the solutions obtained by the IED method is comparable with (and sometimes favourable over) those obtained by the other solvers mentioned.	algorithm;augmented lagrangian method;docsis set-top gateway;duality (optimization);epigraph (mathematics);experiment;finite element method;lancelot;linear approximation;mathematical optimization;newton;numerical analysis;penalty method;subderivative;subgradient method;the matrix;tree accumulation	Regina Sandra Burachik;Wilhelm P. Freire;C. Yalçin Kaya	2014	J. Global Optimization	10.1007/s10898-013-0108-4	kissing number problem;mathematical optimization;combinatorics;mathematical analysis;duality;mathematics	ML	73.82644197150582	23.669512983848005	142563
98cfd6a041a0a6c17e06e10a1945ab373731f907	non-quadratic proximal regularization with application to variational inequalities in hilbert spaces	monotone operator;distance function;functional dependency;hilbert space;variational inequality;weak convergence;maximal monotone operator;proximal point method	We consider a generalized proximal point method for solving variational inequalities with maximal monotone operators in a Hilbert space. It proves to be that the conditions on the choice of a non-quadratic distance functional depend on the geometrical properties of the operator in the variational inequality, and – in particular – a standard assumption on the strict convexity of the kernel of the distance functional can be weakened if this operator possesses a certain ”reserve of monotonicity”. A successive approximation of the operator (using the -enlargement concept) and of the ”feasible set” is performed, and the arising auxiliary problems are solved approximately. Weak convergence of the proximal iterates to a solution of the original problem is proved.	approximation;calculus of variations;convex function;feasible region;hilbert space;matrix regularization;maximal set;proximal gradient method;social inequality;variational inequality;variational principle;monotone	Alexander Kaplan;Rainer Tichatschke	2001	Universität Trier, Mathematik/Informatik, Forschungsbericht		operator space;multiplication operator;mathematical optimization;pseudo-monotone operator;mathematical analysis;strongly monotone;topology;compact operator on hilbert space;hilbert manifold;mathematics;weak convergence;weak operator topology;unitary operator	ML	72.57789757251183	21.312218966022378	142596
a75ec36f0438f44bfdc093bd2cf02224a6e0926a	fast matrix computations for pair-wise and column-wise commute times and katz scores	information network;adaptive algorithm;conjugate gradient;matrix computation;numerical analysis;upper and lower bounds;numerical linear algebra	We first explore methods for approximating the commute time and Katz score between a pair of nodes. These methods are based on the approach of matrices, moments, and quadrature developed in the numerical linear algebra community. They rely on the Lanczos process and provide upper and lower bounds on an estimate of the pair-wise scores. We also explore methods to approximate the commute times and Katz scores from a node to all other nodes in the graph. Here, our approach for the commute times is based on a variation of the conjugate gradient algorithm, and it provides an estimate of all the diagonals of the inverse of a matrix. Our technique for the Katz scores is based on exploiting an empirical localization property of the Katz matrix. We adopt algorithms used for personalized PageRank computing to these Katz scores and theoretically show that this approach is convergent. We evaluate these methods on 17 real world graphs ranging in size from 1000 to 1,000,000 nodes. Our results show that our pair-wise commute time method and column-wise Katz algorithm both have attractive theoretical properties and empirical performance.	approximation algorithm;bilinear filtering;computation;conjugate gradient method;emoticon;heuristic;iteration;lanczos resampling;matrix multiplication;numerical analysis;numerical linear algebra;pagerank;personalization;sensor;sparse matrix;the matrix	Francesco Bonchi;Pooya Esfandiar;David F. Gleich;Chen Greif;Laks V. S. Lakshmanan	2012	Internet Mathematics	10.1080/15427951.2012.625256	mathematical optimization;combinatorics;discrete mathematics;mathematics;numerical linear algebra;statistics	AI	82.1575778205038	25.10465738077826	142714
25e55a22267dc37fb47911309a3d9ccceadad731	a pivotal method for affine variational inequalities	affine variational inequality;algebra afin;desigualdad variacional;methode suivi chemin;inegalite variationnelle;methode pivotage;algorithme;algorithm;algebre affine;mathematical programming;resolucion ecuacion;path following algorithm;pivoting method;probleme complementarite lineaire;normal map;variational inequality;linear transformation;probleme complementarite;problema complementariedad;resolution equation;complementarity problem;equation resolution;path following method;carte normale;linear complementarity problem;programmation mathematique;programacion matematica;path following;affine algebra;metodo pivotaje;algoritmo	"""We explain and justify a path-following algorithm for solving the equations Af^ix) = a, where A is a. linear transformation from R"""" to R"""", C is a polyhedral convex subset of R"""", and Ac is the associated normal map. When A^ is coherently oriented, we are able to prove that the path following method terminates at the unique solution of A^ix) = a, which is a generalization of the weU known fact that Lemke's method terminates at the unique solution of LCP (q, M) when Af is a P = matrix. Otherwise, we identify two classes of matrices which are analogues of the class of copositive-plus and L-matrices in the study of the linear complementarity problem. We then prove that our algorithm processes A^ix) = a when A is the linear transformation associated with such matrices. That is, when applied to such a problem, the algorithm will find a solution unless the problem is infeasible in a well specified sense."""	complementarity theory;convex set;lemke's algorithm;linear complementarity problem;normal mapping;polyhedron;variational inequality;variational principle	Menglin Cao;Michael C. Ferris	1996	Math. Oper. Res.	10.1287/moor.21.1.44	mathematical optimization;mathematical analysis;variational inequality;calculus;fundamental resolution equation;mathematics;linear map;linear complementarity problem;normal mapping	Theory	74.14048593610045	20.4888271984987	142729
99937f99c3c0249f77cadc845b41e89047b2c143	semi-convergence analysis of preconditioned deteriorated pss iteration method for singular saddle point problems	singular saddle point problem;deteriorated pss iteration method;semi-convergence;preconditioning;65f10;65f50	In this paper, we propose a two-parameter preconditioned variant of the deteriorated PSS iteration method (J. Comput. Appl. Math., 273, 41–60 (2015)) for solving singular saddle point problems. Semi-convergence analysis shows that the new iteration method is convergent unconditionally. The new iteration method can also be regarded as a preconditioner to accelerate the convergence of Krylov subspace methods. Eigenvalue distribution of the corresponding preconditioned matrix is presented, which is instructive for the Krylov subspace acceleration. Note that, when the leading block of the saddle point matrix is symmetric, the new iteration method will reduce to the preconditioned accelerated HSS iteration method (Numer. Algor., 63 (3), 521–535 2013), the semi-convergence conditions of which can be simplified by the results in this paper. To further improve the effectiveness of the new iteration method, a relaxed variant is given, which has much better convergence and spectral properties. Numerical experiments are presented to investigate the performance of the new iteration methods for solving singular saddle point problems.	experiment;high-speed serial interface;iteration;krylov subspace;lagrangian relaxation;numerical method;physical symbol system;preconditioner;rate of convergence;semiconductor industry;singular value decomposition	Zhao-Zheng Liang;Guo-Feng Zhang	2017	Numerical Algorithms	10.1007/s11075-017-0380-3	mathematical analysis;mathematical optimization;mathematics;iterative method;preconditioner;fixed-point iteration;arnoldi iteration;generalized minimal residual method;power iteration;krylov subspace;saddle point	AI	80.83635319667839	22.118093522584147	142783
7db6168e7dc346e15eb2079fbc521fa10dd424dd	inertia-controlling methods for general quadratic programming	direction recherche;quadratic programming;methode primale realisable;quadratic program;primal feasible methods;programmation quadratique;hessian matrix;systeme karush kuhn tucker;schur complement;equation recurrence;positive definite;karush kuhn tucker;lagrange multiplier;satisfiability;matriz simetrica;eigenvalues;symmetric matrix;90c20;65k05;condition optimalite;general quadratic programming;iteraccion;matrice definie positive;active set method;positive definite matrix;mathematical programming;multiplicateur lagrange;condicion estado optimo;active set methods;recurrence equation;iteration;multiplicador lagrange;recurrence relation;matrice symetrique;programacion cuadratica;karush kuhn tucker system;matriz definida positiva;programmation mathematique;matrice hessienne;programacion matematica;optimality condition;ecuacion recurrencia	Active-set quadratic programming (QP) methods use a working set to define the search direction and multiplier estimates. In the method proposed by Fletcher in 1971, and in several subsequent mathematically equivalent methods, the working set is chosen to control the inertia of the reduced Hessian, which is never permitted to have more than one nonpositive eigenvalue. (We call such methods inertia-controlling.) This paper presents an overview of a generic inertia-controlling QP method, including the equations satisfied by the search direction when the reduced Hessian is positive definite, singular and indefinite. Recurrence relations are derived that define the search direction and Lagrange multiplier vector through equations related to the Karush-Kuhn-Tucker system. We also discuss connections with inertia-controlling methods that maintain an explicit factorization of the reduced Hessian matrix.	active set method;computation;eigenvalue algorithm;fletcher's checksum;hessian;iteration;karush–kuhn–tucker conditions;lagrange multiplier;quadratic programming;recurrence relation;sparse matrix;surround sound;tucker decomposition;working set	Philip E. Gill;Walter Murray;Michael A. Saunders;Margaret H. Wright	1991	SIAM Review	10.1137/1033001	mathematical optimization;mathematical analysis;calculus;mathematics;geometry;positive-definite matrix;hessian equation;quadratic programming;algorithm;algebra	Robotics	78.98664572354376	22.280478434717963	142786
90623f19d504211137201d19284f9cc676941b51	linear equality constraints and homomorphous mappings in pso	unconstrained optimization algorithm linear equality constraints homomorphous mapping particle swarm optimisation algorithm lower dimensional problem;unconstrained optimization algorithm;constrained optimization;particle swarm optimisation algorithm;homomorphous mapping;lower dimensional problem;particle swarm optimization;linear equality constraints;homorphous mapping;constraint optimization computer science particle swarm optimization subspace constraints support vector machines algorithm design and analysis equations transforms;particle swarm optimisation	We present a homomorphous mapping that converts problems with linear equality constraints into fully unconstrained and lower-dimensional problems for optimization with PSO. This approach, in contrast with feasibility preservation methods, allows any unconstrained optimization algorithm to be applied to a problem with linear equality constraints, making available tools that are known to be effective and simplifying the process of choosing an optimizer for these kinds of constrained problems. The application of some PSO algorithms to a problem that has undergone the mapping presented here is shown to be more effective and more consistent than other approaches to handling linear equality constraints in PSO	algorithm;gaussian elimination;linear equation;linear inequality;mathematical optimization;particle swarm optimization;phase-shift oscillator;social inequality	Christopher K. Monson;Kevin D. Seppi	2005	2005 IEEE Congress on Evolutionary Computation	10.1109/CEC.2005.1554669	mathematical optimization;multi-swarm optimization;constrained optimization;combinatorics;machine learning;mathematics;constraint;particle swarm optimization	EDA	72.62637837991433	23.725596741260592	142971
5edef222f03925d1ace5e55fb97774ccbe046bcc	nonmonotone backtracking inexact quasi-newton algorithms for solving smooth nonlinear equations	equation non lineaire;ecuacion no lineal;analisis numerico;line search;matematicas aplicadas;mathematiques appliquees;nonmonotone technique;quasi newton;global convergence;local convergence;68wxx;analyse numerique;metodo cuasi newton;methode quasi newton;numerical analysis;inexact quasi newton methods;backtracking;nonlinear equation;quasi newton method;nonlinear equations;non linear equation;applied mathematics	A family of inexact quasi-Newton algorithms in association with nonmonotone backtracking line search technique is proposed for solving smooth nonlinear equations. Global convergence of the proposed algorithms are established under the reasonable conditions. We characterize the order of local convergence based on convergence behaviour of the approximate matrix of the Jacobian and indicate how to choose an inexact forcing sequence which preserves the rapid convergence of the proposed algorithms.	algorithm;backtracking;newton;nonlinear system;quasi-newton method	Detong Zhu	2005	Applied Mathematics and Computation	10.1016/j.amc.2003.12.074	local convergence;mathematical optimization;mathematical analysis;quasi-newton method;nonlinear system;numerical analysis;calculus;mathematics;line search;backtracking	AI	81.78371055964236	18.392230807117212	143013
ba0551e7723476b91355611d2ef96ae7ef4cd025	a study of the dual affine scaling continuous trajectories for linear programming	continuous method;linear programming;affine scaling;interior point method	In this paper, a continuous method approach is adopted to study both the entire process and the limiting behaviors of the dual affine scaling continuous trajectories for linear programming. Our approach is different from the method presented by Adler and Monteiro (Adler and Monteiro, Math. Program. 50:29–51, 1991). Many new theoretical results on the trajectories resulting from the dual affine scaling continuous method model for linear programming are obtained.	affine scaling;image scaling;linear programming	Li-Zhi Liao	2014	J. Optimization Theory and Applications	10.1007/s10957-013-0495-1	mathematical optimization;combinatorics;discrete mathematics;affine coordinate system;linear-fractional programming;linear programming;interior point method;affine geometry of curves;mathematics;affine shape adaptation;affine combination	PL	73.80908250195438	22.216262802505636	143046
a815fb20576bc71a00ae88852146a999cab6d7fa	accelerated norm-optimal iterative learning control algorithms using successive projection	espace hilbert;control optimo;optimisation;norm optimisation;espacio hilbert;commande repetitive;control inteligente;iterative learning control;projection method;intelligent control;hilbert space;optimal control;qa75 electronic computers computer science;sistema fasico supraminimo;systeme phase non minimum;formal verification;methode projection;commande optimale;metodo proyeccion;commande ilc;verification formelle;commande intelligente;control ilc;control repetitivo;algoritmo optimo;algorithme optimal;optimal algorithm;repetitive control;projection methods;numerical simulation;non minimum phase system	This article proposes a novel technique for accelerating the convergence of the previously published norm-optimal iterative learning control (NOILC) methodology. The basis of the results is a formal proof of an observation made by D.H. Owens, namely that the NOILC algorithm is equivalent to a successive projection algorithm between linear varieties in a suitable product Hilbert space. This leads to two proposed accelerated algorithms together with well-defined convergence properties. The results show that the proposed accelerated algorithms are capable of ensuring monotonic error norm reductions and can outperform NOILC by more rapid reductions in error norm from iteration to iteration. In particular, examples indicate that the approach can improve the performance of NOILC for the problematic case of non-minimum phase systems. Realisation of the algorithms is discussed and numerical simulations are provided for comparative purposes and to demonstrate the numerical performance and effectiveness of the proposed methods.	algorithm;iterative method	Bing Chu;David H. Owens	2009	Int. J. Control	10.1080/00207170802512824	mathematical optimization;optimal control;formal verification;calculus;control theory;mathematics;projection method;algorithm;iterative learning control;intelligent control;hilbert space	Robotics	76.84528176451389	22.270278589917343	143080
12e4d3fbacc1638cccb92826e8782ab15e60122c	a distributed approach to the opf problem	signal image and speech processing;quantum information technology spintronics	This paper presents a distributed approach to optimal power flow (OPF) in an electrical network, suitable for application in a future smart grid scenario where access to resource and control is decentralized. The non-convex OPF problem is solved by an augmented Lagrangian method, similar to the widely known ADMM algorithm, with the key distinction that penalty parameters are constantly increased. A (weak) assumption on local solver reliability is required to always ensure convergence. A certificate of convergence to a local optimum is available in the case of bounded penalty parameters. For moderate sized networks (up to 300 nodes, and even in the presence of a severe partition of the network), the approach guarantees a performance very close to the optimum, with an appreciably fast convergence speed. The generality of the approach makes it applicable to any (convex or non-convex) distributed optimization problem in networked form. In the comparison with the literature, mostly focused on convex SDP approximations, the chosen approach guarantees adherence to the reference problem, and it also requires a smaller local computational complexity effort.	algorithm;approximation;augmented lagrangian method;computational complexity theory;local optimum;mathematical optimization;optimization problem;solver	Tomaso Erseghe	2015	EURASIP J. Adv. Sig. Proc.	10.1186/s13634-015-0226-x	mathematical optimization;telecommunications;computer science;theoretical computer science;machine learning;mathematics;distributed computing;algorithm	ML	73.55437629502681	28.409699976291936	143145
a9e31ba42c8dc1d284c4177cb02dfa942f5aa648	spectrum slicing for sparse hermitian definite matrices based on zolotarev's functions	journal article	This paper proposes an efficient method for computing selected generalized eigenpairs of a sparse Hermitian definite matrix pencil (A,B). Based on Zolotarev’s best rational function approximations of the signum function and conformal maps, we construct the best rational function approximation of a rectangular function supported on an arbitrary interval. This new best rational function approximation is applied to construct spectrum filters of (A,B). Combining fast direct solvers and the shift-invariant GMRES, a hybrid fast algorithm is proposed to apply spectral filters efficiently. Assuming that the sparse Hermitian matrices A and B are of size N ×N with O(N) nonzero entries, the computational cost for computing O(1) interior eigenpairs is bounded by that of solving a shifted linear system (A − σB)x = b. Utilizing the spectrum slicing idea, the proposed method computes the full eigenvalue decomposition of a sparse Hermitian definite matrix pencil via solving O(N) linear systems. The efficiency and stability of the proposed method are demonstrated by numerical examples of a wide range of sparse matrices. Compared with existing spectrum slicing algorithms based on contour integrals, the proposed method is faster and more reliable.	algorithm;approximation;computational complexity theory;generalized minimal residual method;iterative method;krylov subspace;linear system;map;numerical analysis;numerical method;particle filter;solver;sparse matrix;the matrix	Yingzhou Li;Haizhao Yang	2017	CoRR		mathematical optimization;combinatorics;mathematical analysis;discrete mathematics;mathematics;algebra	Robotics	82.13582988262579	22.475670888568096	143209
47856ddcaeeab6ac63b8d82574f785ece52b9bcf	an infeasible point method for minimizing the lennard-jones potential	global solution;operations research mathematical programming;non linear programming;mathematics;penalty methods;nonlinear programming;statistics general;basin of attraction;operation research decision theory;linear constraint;lennard jones;penalty method;global optimization;optimization;convex and discrete geometry;local minima	Minimizing the Lennard-Jones potential, the most-studied model problem for molecular conformation, is an unconstrained global optimization problem with a large number of local minima. In this paper, the problem is reformulated as an equality constrained nonlinear programming problem with only linear constraints. This formulation allows the solution to approached through infeasible configurations, increasing the basin of attraction of the global solution. In this way the likelihood of finding a global minimizer is increased. An algorithm for solving this nonlinear program is discussed, and results of numerical tests are presented.	algorithm;global optimization;jones calculus;lennard-jones potential;mathematical optimization;maxima and minima;nonlinear programming;nonlinear system;numerical analysis;numerical method;optimization problem	Mark S. Gockenbach;Anthony J. Kearsley;William W. Symes	1997	Comp. Opt. and Appl.	10.1023/A:1008627606581	mathematical optimization;combinatorics;mathematical analysis;nonlinear programming;penalty method;mathematics	EDA	74.33916500456765	24.187197985072157	143403
c308daf2f2eb159e0fd17df0207761263e58ab54	an autoadaptative limited memory broyden's method to solve systems of nonlinear equations	classical limit;superlinear convergence;65b99;systeme equation;analisis numerico;convergence;matematicas aplicadas;mathematiques appliquees;relacion convergencia;taux convergence;convergence rate;analyse numerique;algorithme;acceleration convergence;algorithm;convergencia;sistema ecuacion;numerical analysis;rank reduction;equation system;autoadaptativity;systeme non lineaire;aceleracion convergencia;applied mathematics;sistema no lineal;systems of nonlinear equations;limited memory broyden method;non linear system;algoritmo;convergence acceleration	We propose a new Broyden-like method that we call autoadaptative limited memory method. Unlike classical limited memory method, we do not need to set any parameters such as the maximal size, that solver can use. In fact, the autoadaptative algorithm automatically increases the approximate subspace when the convergence rate decreases. The convergence of this algorithm is superlinear under classical hypothesis. A few numerical results with well-known benchmarks functions are also provided and show the efficiency of the method.	broyden's method;nonlinear system	Mohammed Ziani;Frédéric Guyomarc'h	2008	Applied Mathematics and Computation	10.1016/j.amc.2008.06.047	mathematical optimization;mathematical analysis;convergence;numerical analysis;calculus;mathematics;classical limit;rate of convergence;algorithm	Logic	77.24492966222687	22.97813181331794	143660
41ed4c45f35c7364b31e024f562c47e777ef6c11	on the condition of four-parameter sine wave fitting	sample size;least square error;perturbation theory;eigenvalues;iterative algorithm;four parameter sine wave;matrix perturbation;matrix equation;condition number	Matrix equations need to be solved in each round of an iterative algorithm for four-parameter sine wave fitting. The condition number (CN) of its coefficient matrix was studied approximately under a practical scenario (large sample size, and the frequency being well inside [0, Nyquist frequency]). Upon the first approximate level, secondary entries in the coefficient matrix are ignored. Thereafter, their contribution is examined by the matrix perturbation theory on the second approximate level. This study shows that, firstly, the CN does not depend on the offset. Secondly, the CN at the first approximate level only depends on the amplitude. Thirdly, the secondary entries modulate the CN weakly, and when the amplitude deviates away from the optimal value, this modulation is sinusoidal. © 2006 Elsevier B.V. All rights reserved.	approximation algorithm;coefficient;color gradient;condition number;continuous integration;iterative method;modulation;numerical analysis;numerical aperture;numerical integration;nyquist frequency;nyquist–shannon sampling theorem;offset (computer science);optimization problem;order of approximation;perturbation theory;sampling (signal processing);the matrix	Kui Fu Chen	2007	Computer Standards & Interfaces	10.1016/j.csi.2006.03.004	sample size determination;mathematical optimization;eigenvalues and eigenvectors;condition number;perturbation theory;iterative method;state-transition matrix;matrix	AI	79.85462432645868	19.170022096207788	143773
18adbed983ada7e23e31637cb9517436b4cdf392	fast linear iterations for distributed averaging	sistema lineal;approximation asymptotique;radio espectral;programmation semi definie;semidefinite programming;distributed consensus;heuristic method;subgradient method;metodo heuristico;methode point interieur;spectral radius;laplacien graphe;linear system;initial value problem;connected graph;consensus reparti;iteraccion;metodo punto interior;iteration;problema valor inicial;methode heuristique;asymptotic approximation;probleme valeur initiale;graph laplacian;systeme lineaire;programacion semi definida;interior point method;optimal algorithm;graphe connexe;rayon spectral;aproximacion asintotica;semidefinite program;semi definite programming;grafo conexo	We consider the problem of finding a linear iteration that yields distributed averaging consensus over a network, i.e., that asymptotically computes the average of some initial values given at the nodes. When the iteration is assumed symmetric, the problem of finding the fastest converging linear iteration can be cast as a semidefinite program, and therefore efficiently and globally solved. These optimal linear iterations are often substantially faster than several common heuristics that are based on the Laplacian of the associated graph. We show how problem structure can be exploited to speed up interior-point methods for solving the fastest distributed linear iteration problem, for networks with up to a thousand or so edges. We also describe a simple subgradient method that handles far larger problems, with up to one hundred thousand edges. We give several extensions and variations on the basic problem.	fastest;heuristic (computer science);interior point method;iteration;semidefinite programming;speedup;subgradient method	Lin Xiao;Stephen P. Boyd	2004	Systems & Control Letters	10.1016/j.sysconle.2004.02.022	mathematical optimization;combinatorics;consensus;iteration;laplacian matrix;connectivity;subgradient method;interior point method;calculus;mathematics;linear system;spectral radius;initial value problem;semidefinite programming	ML	76.50879628436441	22.09011707431446	144122
aee810da5080f0f9a23ea1f214a95157033b88b2	accelerated distributed nesterov gradient descent for convex and smooth functions		This paper considers the distributed optimization problem over a network, where the objective is to optimize a global function formed by an average of local functions, using only local computation and communication. We develop an Accelerated Distributed Nesterov Gradient Descent (Acc-DNGD) method for convex and smooth objective functions. We show that it achieves a O(1/t1.4-ε) (∀ε ε (0,1.4)) convergence rate when a vanishing step size is used. The convergence rate can be improved to O(1/t2) when we use a fixed step size and the objective functions satisfy a special property. To the best of our knowledge, Acc-DNGD is the fastest among all distributed gradient-based algorithms that have been proposed so far.	algorithm;computation;fastest;gradient descent;mathematical optimization;optimization problem;rate of convergence	Guannan Qu;Na Li	2017	2017 IEEE 56th Annual Conference on Decision and Control (CDC)	10.1109/CDC.2017.8263979	rate of convergence;mathematical optimization;computer science;distributed algorithm;acceleration;computation;linear programming;gradient descent;optimization problem;convergence (routing)	ML	74.03598677209816	25.56241916385625	144459
3ae6b8fb13058526157fb0dad0cb338897ad0e7b	condition-based complexity of convex optimization in conic linear form via the ellipsoid algorithm	90c60;convex optimization;conditioning;90c;error analysis;ellipsoid method;90c05;q175 m41 o616 no 321 97;complexity of convex optimization	"""A convex optimization problem in conic linear form is an optimization problem of the form CP(d) : maximize cTx s.t. b Ax E Cy x E Cx, where Cx and Cy are closed convex cones in nand m-dimensional spaces X and Y, respectively, and the data for the system is d = (A, b, c). We show that there is a version of the ellipsoid algorithm that can be applied to find an e-optimal solution of CP(d) in at most O (n2 n ((d)ld)) iterations of the ellipsoid algorithm, where each iteration must either perform a separation cut on one of the cones Cx or Cy, or must perform a related optimality cut. The quantity C(d) is the """"condition number"""" of the program CP(d) originally developed by Renegar, and essentially is a scale invariant reciprocal of the smallest data perturbation Ad = (AA, Ab, Ac) for which the system CP(d + Ad) becomes either infeasible or unbounded. The scalar quantity cl is a constant that depends only on particular properties of the cones and the norms used, and is independent of the problem data d = (A, b, c), but may depend on the dimensions m and/or n. AMS Subject Classification: 90C, 90C05, 90C60"""	algorithm;condition number;convex optimization;cylinder-head-sector;ellipsoid method;iteration;mathematical optimization;optimization problem;vhdl-ams	Robert M. Freund;Jorge R. Vera	1999	SIAM Journal on Optimization	10.1137/S105262349732829X	mathematical optimization;combinatorics;convex optimization;ellipsoid method;conditioning;mathematics;geometry	ML	71.30701790701123	22.595664267465764	144498
8adba01d3091f7d48b49ac724ffdc081d3b82e5c	smoothing under diffeomorphic constraints with homeomorphic splines	65dxx;time dependent;splines;physique mathematique;image matching;ordinary differential equation;62g08;inequality constraint;constrained smoothing;reproducing kernel hilbert space;monotonicity;nonparametric regression;linear program;numerical experiment;diffeomorphism;vector field;image warping;time dependent vector field	In this paper we introduce a new class of diffeomorphic smoothers based on general spline smoothing techniques and on the use of some tools that have been recently developed in the context of image warping to compute smooth diffeomorphisms. This diffeomorphic spline is defined as the solution of an ordinary differential equation governed by an appropriate time-dependent vector field. This solution has a closed form expression which can be computed using classical unconstrained spline smoothing techniques. This method does not require the use of quadratic or linear programming under inequality constraints and has therefore a low computational cost. In a one dimensional setting incorporating diffeomorphic constraints is equivalent to impose monotonicity. Thus, as an illustration, it is shown that such a monotone spline can be used to monotonize any unconstrained estimator of a regression function, and that this monotone smoother inherits the convergence properties of the unconstrained estimator. Some numerical experiments are proposed to illustrate its finite sample performances, and to compare them with another monotone estimator. We also provide a two-dimensional application on the computation of diffeomorphisms for landmark and image matching.	algorithmic efficiency;computation;experiment;i-spline;image registration;image warping;linear programming;numerical analysis;performance;quadratic function;smoothing spline;social inequality;spline (mathematics);monotone	Jérémie Bigot;Sébastien Gadat	2010	SIAM J. Numerical Analysis	10.1137/080727555	image warping;spline;diffeomorphism;ordinary differential equation;mathematical optimization;mathematical analysis;vector field;smoothing spline;monotonic function;linear programming;reproducing kernel hilbert space;mathematics;geometry;thin plate spline;nonparametric regression;statistics	Vision	75.5947013090564	19.499695947777873	144521
80b703cd4894b962f01aeba010f3de1384735dcd	analysis of the structure of the krylov subspace in various preconditioned cgs algorithms		An improved preconditioned conjugate gradient squared (PCGS) algorithm has recently been proposed, and it performs much better than the conventional PCGS algorithm. In this paper, the improved PCGS algorithm is verified as a coordinative to the left-preconditioned system, and it has the advantages of both the conventional and the left-PCGS; this is done by comparing, analyzing, and executing numerical examinations of various PCGS algorithms, including another improved one. We show that the direction of the preconditioned system for the CGS method is determined by the operations of αk and βk in the PCGS algorithm. By comparing the logical structures of these algorithms, we show that the direction of the preconditioned system can be switched by the construction and setting of the initial shadow residual vector.	algorithm;conjugate gradient method;krylov subspace;numerical analysis;preconditioner	Shoji Itoh;Masaaki Sugihara	2016	CoRR		mathematical optimization;computer science;theoretical computer science;algorithm	Theory	82.86706420839288	22.580882401501075	145044
a2fbb3c74d47627711411655d3a5e06bfab785f3	strongly convex functions, moreau envelopes, and the generic nature of convex functions with strong minimizers	strongly convex;49k40;52a41;attouch wets metric;epi topology;meager set;complete metric space;primary;generic set;convex function;secondary;90c25;baire category;proximal mapping;moreau envelope;54e52;epi convergence;strong minimizer	In this work, using Moreau envelopes, we define a complete metric for the set of proper lower semicontinuous convex functions. Under this metric, the convergence of each sequence of convex functions is epiconvergence. We show that the set of strongly convex functions is dense but it is only of the first category. On the other hand, it is shown that the set of convex functions with strong minima is of the second category. Disciplines Engineering | Science and Technology Studies Publication Details Planiden, C. & Wang, X. (2016). Strongly convex functions, Moreau envelopes and the generic nature of convex functions with strong minimizers. SIAM Journal on Optimization, 26 (2), 1341-1364. This journal article is available at Research Online: http://ro.uow.edu.au/eispapers1/1343 Strongly convex functions, Moreau envelopes and the generic nature of convex functions with strong minimizers C. Planiden∗ X. Wang† April 13, 2016 Abstract In this work, using Moreau envelopes, we define a complete metric for the set of proper lower semicontinuous convex functions in a finite-dimensional space. Under this metric, the convergence of each sequence of convex functions is epi-convergence. We show that the set of strongly convex functions is dense but it is only of the first category. On the other hand, it is shown that the set of convex functions with strong minima is of the second category.In this work, using Moreau envelopes, we define a complete metric for the set of proper lower semicontinuous convex functions in a finite-dimensional space. Under this metric, the convergence of each sequence of convex functions is epi-convergence. We show that the set of strongly convex functions is dense but it is only of the first category. On the other hand, it is shown that the set of convex functions with strong minima is of the second category. AMS Subject Classification: Primary 54E52, 52A41, 90C25; Secondary 49K40.	comment (computer programming);convex conjugate;convex function;convex set;fenchel's duality theorem;fenchel–moreau theorem;generic programming;hilbert space;maxima and minima;rate of convergence;semi-continuity;uniformly convex space;vhdl-ams	C. Planiden;Xinhua Wang	2016	SIAM Journal on Optimization	10.1137/15M1035550	convex function;convex analysis;subderivative;support function;mathematical optimization;mathematical analysis;convex optimization;convex polytope;pseudoconvex function;topology;closed convex function;convex combination;convex body;linear matrix inequality;convex conjugate;quasiconvex function;convex hull;absolutely convex set;convexity in economics;mathematics;convex set;logarithmically convex function;effective domain;proper convex function;choquet theory	ML	71.16642974770386	20.687931372782728	145150
f972e44db3e3d780afbb42d2cc28a500d77cc4c2	convex methods for rank-constrained optimization problems		This paper considers optimization problems which are convex, except for a constraint on the rank of a matrix variable. Minimizing or penalizing the nuclear norm of a matrix has proven to be an effective method for generally keeping its rank small, and a vast amount of recent work has focused on this technique; however, many problems require finding a matrix whose rank is constrained to be a particular value. We present a new method for these problems, introducing a convex constraint that forces the rank to be at least the desired value, while using the nuclear norm penalty to keep the rank from rising above that value. This results in a convex optimization problem that will attempt to satisfy the constraints, to minimize the objective, and will usually produce the desired rank. We further study the choice of parameter used with the nuclear norm penalty, both with and without the constraint. It is shown that another convex optimization problem can be formulated from the dual problem which will find the best parameter in some cases, and will still produce a useful result in other cases. We find that considering parameters which are negative, that is, considering rewarding the nuclear norm, as well as penalizing it, can result in better performance with the desired rank. The methods developed are demonstrated on rank-constrained semidefinite programming problems (SDPs). The first three authors contributed equally to this work. Department of Electrical and Computer Engineering and Institute for Systems Research, University of Maryland, College Park, MD 20740. email:vsmai@umd.edu Department of Electrical and Computer Engineering and Institute for Systems Research, University of Maryland, College Park, MD 20740. email:dmaity@umd.edu Department of Electrical and Computer Engineering and Institute for Systems Research, University of Maryland, College Park, MD 20740. email:rbhaskar@umd.edu Department of Electrical and Computer Engineering and Institute for Systems Research, University of Maryland, College Park, MD 20740. email:mcrotk@umd.edu	computer engineering;constrained optimization;convex optimization;duality (optimization);effective method;heuristic (computer science);mathematical optimization;molecular dynamics;numerical method;optimization problem;program optimization;semidefinite programming;whole earth 'lectronic link	Van Sy Mai;Dipankar Maity;Bhaskar Ramasubramanian;Michael Rotkowitz	2015		10.1137/1.9781611974072.18	mathematical optimization;rank (linear algebra);duality (optimization);convex optimization;matrix norm;mathematics;semidefinite programming;constrained optimization;effective method;optimization problem	ML	71.84533667810882	26.333331633443887	145220
d579bafe1c8e4e041bac07541e18251c3d77219b	on numerical radius of a matrix and estimation of bounds for zeros of a polynomial		We obtain inequalities involving numerical radius of a matrix A ∈ M n C. Using this result, we find upper bounds for zeros of a given polynomial. We also give a method to estimate the spectral radius of a given matrix A ∈ M n C up to the desired degree of accuracy.	numerical analysis;numerical linear algebra;polynomial	Kallol Paul;Santanu Bag	2012	Int. J. Math. Mathematical Sciences	10.1155/2012/129132	mathematical optimization;polynomial matrix;combinatorics;discrete mathematics;polynomial function theorems for zeros;degree of a polynomial;mathematics;matrix polynomial;characteristic polynomial	Theory	78.12419710916721	18.438998764900166	145231
7adcc17c8184897b85d4f1db8cfc18bf784a5340	a derivative-free conjugate gradient method and its global convergence for solving symmetric nonlinear equations		We suggest a conjugate gradient (CG) method for solving symmetric systems of nonlinear equations without computing Jacobian and gradient via the special structure of the underlying function. This derivative-free feature of the proposed method gives it advantage to solve relatively large-scale problems (500,000 variables) with lower storage requirement compared to some existing methods. Under appropriate conditions, the global convergence of our method is reported. Numerical results on some benchmark test problems show that the proposed method is practically effective.		Mohammed Yusuf Waziri;Jamilu Sabi'u	2015	Int. J. Math. Mathematical Sciences	10.1155/2015/961487	gradient descent;mathematical optimization;mathematical analysis;conjugate residual method;gradient method;calculus;derivation of the conjugate gradient method;mathematics;conjugate gradient method;nonlinear conjugate gradient method;biconjugate gradient method	AI	77.0294890263646	23.503741648245143	145439
5e821f78000b54e763c67d7e230618926c04d23d	convergence of inner-iteration gmres methods for rank-deficient least squares problems	65f20;least squares problems;inner outer iteration;iterative methods;gmres method;preconditioner;rank deficient problem;65f08;stationary iterative method;65f50;65f10	We develop a general convergence theory for the generalized minimal residual method preconditioned by inner iterations for solving least squares problems. The inner iterations are performed by stationary iterative methods. We also present theoretical justifications for using specific inner iterations such as the Jacobi and SOR-type methods. The theory improves previous work [K. Morikuni and K. Hayami, SIAM J. Matrix Anal. Appl., 34 (2013), pp. 1–22], particularly in the rank-deficient case. We also characterize the spectrum of the preconditioned coefficient matrix by the spectral radius of the iteration matrix for the inner iterations and give a convergence bound for the proposed methods. Finally, numerical experiments show that the proposed methods are more robust and efficient compared to previous methods for some rank-deficient problems.	coefficient;comstock–needham system;experiment;generalized minimal residual method;iteration;iterative method;jacobi method;least squares;numerical analysis;stationary process	Keiichi Morikuni;Ken Hayami	2015	SIAM J. Matrix Analysis Applications	10.1137/130946009	mathematical optimization;mathematical analysis;calculus;mathematics;preconditioner;iterative method;algebra	ML	80.85949038083912	21.83247573127892	145474
24db038838d085fd6e6922aef673e57e75dd5000	a proximal point method for the variational inequality problem in banach spaces	variational inequality problem;convergence;algorithmic scheme;banach space;49d45;banach spaces;49d37;90c25;proximal point algorithm;proximal point method;maximal monotone operators	In this paper we prove well-definedness and weak convergence of the generalized proximal point method when applied to the variational inequality problem in reflexive Banach spaces. The proximal version we consider makes use of Bregman functions, whose original definition for finite dimensional spaces has here been properly extended to our more general framework.	bregman divergence;calculus of variations;social inequality;spaces;variational inequality;variational principle	Regina Sandra Burachik;Susana Scheimberg	2000	SIAM J. Control and Optimization	10.1137/S0363012998339745	functional analysis;eberlein–šmulian theorem;mathematical optimization;banach manifold;mathematical analysis;birnbaum–orlicz space;topology;interpolation space;mathematics;banach space;lp space	Theory	72.64120031592621	21.260964815853637	145479
041e82f9cf0e4617a464d15b9362fbd44e3ba174	some new algorithms for solving mixed equilibrium problems	mixed equilibrium problem;hilbert spaces;projection method;equilibrium problem;hilbert space;variational inequality;projection methods	In this paper, we suggest and analyze two projection methods (one implicit and one explicit) for finding a particular solution of a mixed equilibrium problem in a real Hilbert space. Furthermore, we prove that the proposed projection methods converge strongly to a particular solution of the mixed equilibrium problem. © 2010 Elsevier Ltd. All rights reserved.	algorithm;converge;hilbert space;nash equilibrium	Yonghong Yao;Muhammad Aslam Noor;Yeong-Cheng Liou;Shin Min Kang	2010	Computers & Mathematics with Applications	10.1016/j.camwa.2010.06.016	mathematical optimization;combinatorics;mathematical analysis;mathematics;quantum mechanics;algebra;hilbert space	AI	73.8550238077056	20.731317828331836	145722
a2c98b3d7fe739aa2fcda59bc1c32bb6f8647aea	a robust primal-dual interior-point algorithm for nonlinear programs	line search;nonlinear inequality constraints;90c26;nonlinear programming;merit function;regularity conditions;90c30;karush kuhn tucker;global convergence;65k10;optimization problem;trust region method;90c51;49m30;90c22;interior point method;nonlinear optimization;interior point algorithm;49m37;regularity condition	We present a primal-dual interior point algorithm of line-search type for nonlinear programs, which uses a new decomposition scheme of sequential quadratic programming. The algorithm can circumvent the convergence difficulties of some existing interior point methods. Global convergence properties are derived without assuming regularity conditions. The penalty parameter ρ in the merit function is updated automatically such that the search directions are descent directions for the merit function. It is shown that if ρ is bounded, then every limiting point of the sequence generated by the algorithm is a KKT point, whereas if ρ is unbounded, then the sequence has a limiting point which is either a Fritz-John point of the feasible set or an infeasible stationary point of minimizing the `2-norm of infeasibility. Numerical results confirm that the algorithm produces the correct results for some hard problems including the examples provided by Wächter and Biegler and Byrd et al. for which many of the existing line-search type interior point methods have failed to find the right answers.	algorithm;approximation;converge;descent direction;feasible region;interior point method;line search;local convergence;nonlinear system;numerical analysis;numerical method;sequential quadratic programming;stationary process	Xinwei Liu;Jie Sun	2004	SIAM Journal on Optimization	10.1137/S1052623402400641	optimization problem;mathematical optimization;combinatorics;mathematical analysis;nonlinear programming;interior point method;mathematics;line search;karush–kuhn–tucker conditions	Theory	74.87865331941433	23.837947800755263	145751
ebc63667b7370589ac37b80bd61ec312c2e4ac6e	new properties of forward-backward splitting and a practical proximal-descent algorithm	monotone operator;forward backward splitting;proximal descent algorithm;lipschitz continuity	In this paper, we discuss a proximal-descent algorithm for finding a zero of the sum of two maximal monotone operators in a real Hilbert space. Some new properties of forward–backward splitting are given, which extend the well-known properties of the usual projection. Then, they are used to analyze the weak convergence of the proximal-descent algorithm without assuming Lipschitz continuity of the forward operator. We also give a new technique of choosing trial values of the step length involved in an Armijo-like condition, which returns the (not necessarily decreasing) step length self-adaptively. Rudimentary numerical experiments show that it is effective in practical implementations.	algorithm	Yuanyuan Huang;Yunda Dong	2014	Applied Mathematics and Computation	10.1016/j.amc.2014.03.062	mathematical optimization;mathematical analysis;discrete mathematics;forward–backward algorithm;mathematics;lipschitz continuity	Crypto	74.70880986800029	23.508597682640715	145800
40039f6a5c4617c7a41ee3c187db5954308b9c8d	finding a feasible control for real process under uncertainty	existence theorem;mathematical model	In the paper the problem of finding a feasible control for real process is discussed. It is assumed that the available mathematical model of the process differs from its real mathematical description but there is some consistence between them. Under such assumption the feasible control for real process is found by using the process mathematical model only. To show that this approach makes sense the existence theorems are given.		M. Brdys	1975		10.1007/3-540-07623-9_318	mathematical optimization;combinatorics;discrete mathematics;mathematics	Robotics	68.69124109870339	18.363704823564223	145882
20bde02a147772036aeb9c7e1a4811980afce233	a new filled function method for global optimization	unconstrained global optimization;global minimizer;filled function method;local minimizer	A new definition of the filled function is given in this paper. Based on the definition, a new filled function which has better characters is presented. And an algorithm for the global optimization problem is developed. As a result, the computational cost of the proposed filled function method is much less than that of other filled function methods. The implementation of the algorithm on several test problems is reported with satisfactory numerical results.	algorithm;computation;computational complexity theory;global optimization;mathematical optimization;numerical analysis;optimization problem	Yuelin Gao;Yongjian Yang;Mi You	2015	2015 IEEE International Conference on Digital Signal Processing (DSP)	10.1016/j.amc.2015.06.090	mathematical optimization;mathematical analysis;mathematics;algorithm;global optimization	Robotics	75.94526269468344	23.841628054565945	145919
dc8936f7f7b9697f50e15e81adb53d3f5032a266	best proximity points: global optimal approximate solutions	distance function;metric space;exact solution;satisfiability;fixed point equation;iterative algorithm;global optimal approximate solution;fixed point;approximate solution;best proximity point;global optimization;contractive mapping	Let A and B be non-empty subsets of a metric space. As a non-self mapping $${T:A\longrightarrow B}$$ does not necessarily have a fixed point, it is of considerable interest to find an element x in A that is as close to Tx in B as possible. In other words, if the fixed point equation Tx = x has no exact solution, then it is contemplated to find an approximate solution x in A such that the error d(x, Tx) is minimum, where d is the distance function. Indeed, best proximity point theorems investigate the existence of such optimal approximate solutions, called best proximity points, to the fixed point equation Tx = x when there is no exact solution. As the distance between any element x in A and its image Tx in B is at least the distance between the sets A and B, a best proximity pair theorem achieves global minimum of d(x, Tx) by stipulating an approximate solution x of the fixed point equation Tx = x to satisfy the condition that d(x, Tx) = d(A, B). The purpose of this article is to establish best proximity point theorems for contractive non-self mappings, yielding global optimal approximate solutions of certain fixed point equations. Besides establishing the existence of best proximity points, iterative algorithms are also furnished to determine such optimal approximate solutions.	approximation algorithm	S. Sadiq Basha	2011	J. Global Optimization	10.1007/s10898-009-9521-0	mathematical optimization;combinatorics;mathematical analysis;metric;metric space;mathematics;fixed point;iterative method;global optimization;satisfiability	EDA	69.40854743683411	24.647814489769647	146580
38eb13e7e8a0920edef0f30eaac955394557b0aa	a practical method for solving large-scale trs	l bfgs method;inner product;eigenvalues;trust region subproblem;cholesky factorization;large scale;trust region;negative curvature;negative curvature direction;nearly exact method;numerical experiment;large scale problem;large scale optimization	We present a nearly-exact method for the large scale trust region subproblem (TRS) based on the properties of the minimal-memory BFGS method. Our study in concentrated in the case where the initial BFGS matrix can be any scaled identity matrix. The proposed method is a variant of the Moré-Sorensen method that exploits the eigenstructure of the approximate Hessian B, and incorporates both the standard and the hard case. The eigenvalues are expressed analytically, and consequently a direction of negative curvature can be computed immediately by performing a sequence of inner products and vector summations. Thus, the hard case is handled easily while the Cholesky factorization is completely avoided. An extensive numerical study is presented, for covering all the possible cases arising in the TRS with respect to the eigenstructure of B. Our numerical experiments confirm that the method is suitable for very large scale problems.	approximation algorithm;broyden–fletcher–goldfarb–shanno algorithm;cholesky decomposition;experiment;hessian;iteration;matrix multiplication;numerical analysis;trust region	M. S. Apostolopoulou;D. G. Sotiropoulos;C. A. Botsaris;Panayiotis E. Pintelas	2011	Optimization Letters	10.1007/s11590-010-0201-2	mathematical optimization;combinatorics;dot product;eigenvalues and eigenvectors;calculus;mathematics;trust region;cholesky decomposition	Robotics	80.9242413580159	23.992320454987283	146735
65b9d63c3a010b55160e00a063b72569fd09c3b1	lp-based tractable subcones of the semidefinite plus nonnegative cone		The authors in a previous paper devised certain subcones of the semidefinite plus nonnegative cone and showed that satisfaction of the requirements for membership of those subcones can be detected by solving linear optimization problems (LPs) with O(n) variables and O(n) constraints. They also devised LP-based algorithms for testing copositivity using the subcones. In this paper, they investigate the properties of the subcones in more detail and explore larger subcones of the positive semidefinite plus nonnegative cone whose satisfaction of the requirements for membership can be detected by solving LPs. They introduce a semidefinite basis (SD basis) that is a basis of the space of n× n symmetric matrices consisting of n(n+ 1)/2 symmetric semidefinite matrices. Using the SD basis, they devise two new subcones for which detection can be done by solving LPs with O(n) variables and O(n) constraints. The new subcones are larger than the ones in the previous paper and inherit their nice properties. The authors also examine the efficiency of those subcones in numerical experiments. The results show that the subcones are promising for testing copositivity as a useful application.	algorithm;cobham's thesis;experiment;gröbner basis;linear programming;mathematical optimization;numerical analysis;requirement	Akihiro Tanaka;Akiko Yoshise	2018	Annals OR	10.1007/s10479-017-2720-z	mathematical optimization;positive-definite matrix;mathematics;matrix decomposition;linear programming;matrix (mathematics);symmetric matrix	ML	78.13931644141985	22.07181060811239	146774
036b30b33a8f694b59240dfe5ffe7a02851e006e	a dual optimization approach to inverse quadratic eigenvalue problems with partial eigenstructure	partial eigenstructure;semidefinite complementarity problems;calcul scientifique;damping;measured modes;analisis numerico;eigenvalue problem;stiffness matrix;methode newton;matriz rigidez;improvement;65kxx;90c30;optimization method;probleme valeur propre;valued functions;problema inverso;satisfiability;matriz simetrica;eigenvalues;metodo optimizacion;analyse numerique;nearest correlation matrix;symmetric matrix;eigenvalue;58c15;49xx;symmetric matrices;computacion cientifica;numerical analysis;inverse problem;stiffness;mathematical programming;valor propio;methode optimisation;optimal weighted orthogonalization;quadratic convergence;nondegeneracy;matrice symetrique;valeur propre;metodo newton;newton method;numerical experiment;matrice rigidite;scientific computation;nonlinear optimization;quadratic eigenvalue problem;inverse eigenvalue problem;programmation mathematique;amortiguacion;article;probleme inverse;65f15;programacion matematica;65h17;amortissement;problema valor propio;15a22	The inverse quadratic eigenvalue problem (IQEP) arises in the field of structural dynamics. It aims to find three symmetric matrices, known as the mass, the damping and the stiffness matrices, respectively such that they are closest to the given analytical matrices and satisfy the measured data. The difficulty of this problem lies in the fact that in applications the mass matrix should be positive definite and the stiffness matrix positive semidefinite. Based on an equivalent dual optimization version of the IQEP, we present a quadratically convergent Newton-type method. Our numerical experiments confirm the high efficiency of the proposed method.	algorithm;conjugate gradient method;experiment;lagrange multiplier;linear equation;mass matrix;mathematical optimization;newton;newton's method;numerical analysis;optimization problem;preconditioner;rate of convergence;stiffness matrix;structural dynamics	Zheng-Jian Bai;Delin Chu;Defeng Sun	2007	SIAM J. Scientific Computing	10.1137/060656346	mathematical optimization;combinatorics;mathematical analysis;eigenvalues and eigenvectors;nonlinear programming;calculus;quadratically constrained quadratic program;mathematics;quadratic programming;symmetric matrix;algebra	ML	79.84898788795186	21.901735400842263	146821
7b10feabd1fc0e0e2e8f1f466e4f0324145e96c5	computing sparse representation in a highly coherent dictionary based on difference of l1 and l2	difference of convex programming;simulated annealing;comparison with formula presented;highly coherent dictionary;formula presented minimization;l 1 l 2 minimization;sparse representation	We study analytical and numerical properties of the L1−L2 minimization problem for sparse representation of a signal over a highly coherent dictionary. Though the L1 −L2 metric is non-convex, it is Lipschitz continuous. The difference of convex algorithm (DCA) is readily applicable for computing the sparse representation coefficients. The L1 minimization appears as an initialization step of DCA. We further integrate DCA with a non-standard simulated annealing (SA) methodology to approximate globally sparse solutions. Non-Gaussian random perturbations are more effective than standard Gaussian perturbations for improving sparsity of solutions. In numerical experiments, we conduct an extensive comparison among sparse penalties such as L0, L1, Lp for p ∈ (0, 1) based on data from three specific applications (over-sampled discreet cosine basis, differential absorption optical spectroscopy, and image denoising) where highly coherent dictionaries arise. We find numerically that the L1 − L2 minimization persistently produces better results than L1 minimization, especially when the sensing matrix is ill-conditioned. In addition, the DCA method outperforms many existing algorithms for other nonconvex metrics. ∗Department of Mathematics, UC Irvine, Irvine, CA 92697, USA. The work was partially supported by NSF grants DMS0928427 and DMS-1222507. Manuscript revised on July 19, 2014.	approximation algorithm;coefficient;coherent;compressed sensing;computer cooling;condition number;converge;convex function;dictionary;experiment;ibm notes;iterative method;noise reduction;numerical analysis;rayleigh–ritz method;simulated annealing;sparse approximation;sparse matrix;the matrix;uc browser	Yifei Lou;Penghang Yin;Qi He;Jack Xin	2015	J. Sci. Comput.	10.1007/s10915-014-9930-1	mathematical optimization;combinatorics;simulated annealing;machine learning;sparse approximation;mathematics	ML	76.11867315234954	26.11067183686141	147107
ec5af0cd5ee03f1aa143c17f5524e81a18edfe7d	hamilton-jacobi-bellman equations for optimal control processes with convex state constraints	state constraint sets;optimal control problems;convex constraints;hjb equations;viscosity solutions	This work aims at studying some optimal control problems with convex state constraint sets. It is known that for state constrained problems, and when the state constraint set coincides with the closure of its interior, the value function satisfies a Hamilton–Jacobi equation in the  constrained viscosity sense . This notion of solution has been introduced by H.M. Soner (1986) and provides a characterization of the value functions in many situations where an  inward pointing condition  (IPC) is satisfied. Here, we first identify a class of control problems where the constrained viscosity notion is still suitable to characterize the value function without requiring the IPC. Moreover, we generalize the notion of constrained viscosity solutions to some situations where the state constraint set has an empty interior.	bellman equation;hamilton–jacobi–bellman equation;jacobi method;optimal control	Cristopher Hermosilla;Richard B. Vinter;Hasnaa Zidani	2017	Systems & Control Letters	10.1016/j.sysconle.2017.09.004		Robotics	71.76853331272905	20.143378069054766	147268
29fe0b742d503ea454bfe12242242cb6d65fcaa5	how to guess a generating function	05a15;nombre schroder;numerical sequence;equation differentielle;preferential arrangement;arrangement preferentiel;fonction generatrice;suite numerique;sucesion numerica;differential equation;schro der numbers;ecuacion diferencial;descomposicion matricial;schroder numbers;decomposition matricielle;matrix decomposition;matrice hankel;funcion generatriz;generating function;matriz hankel;differential equations;hankel matrix	In this note, a new method for taking the first few terms of a sequence and making an educated guess as to the generating function of the sequence is described. The method involves a matrix factorization into lower triangular, diagonal, and upper triangular matrices (the LDU decomposition), generating functions, and solving a first-order differential equation.		Seyoum Getu;Louis W. Shapiro;Wen-Jin Woan;Leon C. Woodson	1992	SIAM J. Discrete Math.	10.1137/0405040	generating function;combinatorics;mathematical analysis;calculus;mathematics;differential equation;algebra	Theory	79.06866223711599	18.648338290607327	147441
1167f63d895d1f9b2758955389868ea078791774	a primal-proximal heuristic applied to the french unit-commitment problem	90c06;90c90;lagrangiano aumentado;relaxation lagrange;optimisation sousgradient;heuristic method;primal dual method;metodo heuristico;methode primale duale;optimisation combinatoire;metodo primal dual;lagrangien augmente;mathematical programming;subgradient optimization;primal dual heuristics;proximal algorithm;optimizacion subgradiente;methode heuristique;combinatorial optimization;programmation mathematique;augmented lagrangian;programacion matematica;lower bound;proximal;lagrangian relaxation;optimizacion combinatoria;unit commitment problem	This paper is devoted to the numerical resolution of unit-commitment problems, with emphasis on the French model optimizing the daily production of electricity. The solution process has two phases. First a Lagrangian relaxation solves the dual to find a lower bound; it also gives a primal relaxed solution. We then propose to use the latter in the second phase, for a heuristic resolution based on a primal proximal algorithm. This second step comes as an alternative to an earlier approach, based on augmented Lagrangian (i.e. a dual proximal algorithm). We illustrate the method with some real-life numerical results. A companion paper is devoted to a theoretical study of the heuristic in the second phase.	algorithm;augmented lagrangian method;heuristic;lagrangian relaxation;linear programming relaxation;numerical analysis;real life	Louis Dubost;Robert Gonzalez;Claude Lemaréchal	2005	Math. Program.	10.1007/s10107-005-0593-4	mathematical optimization;anatomical terms of location;augmented lagrangian method;lagrangian relaxation;combinatorial optimization;mathematics;mathematical economics;upper and lower bounds;algorithm	ML	75.32895529258545	21.882044760373443	147665
306c9ce44c678b0d3e412fb76a29b425d2fdd064	the smallest eigenvalue of large hankel matrices		We investigate the large N behavior of the smallest eigenvalue, λN , of an (N + 1) × (N + 1) Hankel (or moments) matrix HN , generated by the weight w(x) = xα(1 − x)β, x ∈ [0, 1], α > −1, β > −1. By applying the arguments of Szegö, Widom and Wilf, we establish the asymptotic formula for the orthonormal polynomials Pn(z), z ∈ C \ [0, 1], associated with w(x), which are required in the determination of λN . Based on this formula, we produce the expressions for λN , for large N . Using the parallel algorithm presented by Emmart, Chen and Weems, we show that the theoretical results are in close proximity to the numerical results for sufficiently large N . Keyword: Asymptotics, Smallest eigenvalue, Hankel matrices, Orthogonal polynomials, Parallel algorithm		Mengkun Zhu;Yang Chen;Niall Emmart;Charles C. Weems	2018	Applied Mathematics and Computation	10.1016/j.amc.2018.04.012	mathematical analysis;asymptotic formula;matrix (mathematics);eigenvalues and eigenvectors;mathematics;parallel algorithm;orthonormal basis;expression (mathematics);polynomial	Theory	79.14339535961267	20.510841440529383	147729
af49dacf93705f2f914779c9b6c3bb3f7362291e	rate of convergence analysis of discretization and smoothing algorithms for semiinfinite minimax problems	convergence;algorithms;article;approximation mathematics;problem solving	Discretization algorithms for solving semi-infinite minimax problems replace the original problem by an approximation involving the maximization over a finite number of functions and then solve the resulting approximate problem. The approximate problem gives rise to a discretization error and its solution results in an optimization error as a minimizer of that problem is rarely achievable with a finite computing budget. Accounting for both discretization and optimization errors, we determine the rate of convergence of discretization algorithms as a computing budget tends to infinity. We find that the rate of convergence depends on the class of optimization algorithms used to solve the approximate problem as well as the policy for selecting discretization level and number of optimization iterations. We construct optimal policies that achieve the best possible rate of convergence and find that under certain circumstances the better rate is obtained by inexpensive gradient methods.	approximation algorithm;discretization error;expectation–maximization algorithm;gradient;iteration;linear programming;mathematical optimization;minimax;rate of convergence;semiconductor industry;smoothing;subgradient method	Johannes O. Royset;E. Y. Pee	2012	J. Optimization Theory and Applications	10.1007/s10957-012-0109-3	discretization error;mathematical optimization;combinatorics;discrete mathematics;convergence;discretization;mathematics;discretization of continuous features	ML	72.74574923523335	25.43801131911833	147861
61bf918ab6789f6ff8bfce7240b80c596431d062	using semi-definite programming for multi-constrainedh2 controller design in active noise and vibration control	cost function;structured matrices;semi definite program;least squares problem;vibration control;controller design;kronecker product	We consider the practical design of linear controllers to meet a given set of H2 specifications. The Q-parametrization reduces the problem to a quadratic minimization subject to multiple quadratic constraints, which we solve using semi-definite programming (SDP) methods. Each SDP iteration requires calculating a primal and dual search direction and minimizing the cost function along the plane defined by these search directions. The primal direction requires solving a least squares problem whose normal equation matrix is composed of a blockToeplitz portion plus other structured matrices. We make use of Kronecker products and FFT's to greatly reduce the calculation. The dual search direction and plane search are accelerated by low-rank representations of the SDP structured matrices. As an example, we design controllers which explore the optimal tradeoff between in-band residual and outof-band enhancement of acoustic radiation from a (mathematically modeled) submerged spherical shell, while simultaneously constraining two sensitivity measures. For this example we show that significant reduction in outof-band enhancement is possible with only minor in-band penalties.	acoustic cryptanalysis;fast fourier transform;iteration;loss function;ordinary least squares;quadratic programming;semiconductor industry;semidefinite programming	Julia A. Olkin;Paul J. Titterton	1996	VLSI Signal Processing	10.1007/BF00925268	mathematical optimization;machine learning;vibration control;control theory;mathematics;kronecker product;statistics	ML	75.37894394765783	27.646932987244668	147892
ebb74bd02d66019835241a5ad14fb8dee751369b	matrix krylov subspace methods for linear systems with multiple right-hand sides	scalar product;right hand side;krylov subspace method;linear system;multiple right hand sides;ciencias basicas y experimentales;matematicas;krylov subspace;krylov method;grupo a	In this paper, we first give a result which links any global Krylov method for solving linear systems with several right-hand sides to the corresponding classical Krylov method. Then, we propose a general framework for matrix Krylov subspace methods for linear systems with multiple right-hand sides. Our approach use global projection techniques, it is based on the Global Generalized Hessenberg Process (GGHP) – which use the Frobenius scalar product and construct a basis of a matrix Krylov subspace – and on the use of a Galerkin or a minimizing norm condition. To accelerate the convergence of global methods, we will introduce weighted global methods. In these methods, the GGHP uses a different scalar product at each restart. Experimental results are presented to show the good performances of the weighted global methods.	algorithm;arnoldi iteration;galerkin method;karl hessenberg;krylov subspace;krylov–bogolyubov theorem;linear system;lyapunov fractal;matrix multiplication;navier–stokes equations;numerical analysis;performance;scope (computer science);series acceleration	M. Heyouni;A. Essai	2005	Numerical Algorithms	10.1007/s11075-005-1526-2	mathematical optimization;mathematical analysis;conjugate residual method;dot product;krylov subspace;calculus;generalized minimal residual method;control theory;mathematics;iterative method;linear system;algebra	ML	81.41262254257475	22.076938606827103	148129
7405022404ad5e881f44e1b4754391724f63ea86	certified pde-constrained parameter optimization using reduced basis surrogate models for evolution problems	reduced basis method;pde constrained optimization;reduced order modeling;a posteriori error estimation;surrogate model;parameter optimization	We consider parameter optimization problems which are subject to constraints given by parametrized partial differential equations (PDE). Discretizing this problem may lead to a largescale optimization problem which can hardly be solved rapidly. In order to accelerate the process of parameter optimization we will use a reduced basis surrogate model for numerical optimization. For many optimization methods sensitivity information about the functional is needed. In the following we will show that this derivative information can be calculated efficiently in the reduced basis framework in the case of a general linear output functional and parametrized evolution problems with linear parameter separable operators. By calculating the sensitivity information directly instead of applying the more widely used adjoint approach we can rapidly optimize different cost functionals using the same reduced basis model. Furthermore, we will derive rigorous a-posteriori error estimators for the solution, the gradient and the optimal parameters, which can all be computed online. The method will be applied to two parameter optimization problems with an underlying advection-diffusion equation.	gradient;mathematical optimization;numerical partial differential equations;optimization problem;surrogate model	Markus A. Dihlmann;Bernard Haasdonk	2015	Comp. Opt. and Appl.	10.1007/s10589-014-9697-1	probabilistic-based design optimization;optimization problem;mathematical optimization;multi-swarm optimization;combinatorics;meta-optimization;derivative-free optimization;surrogate model;mathematics;continuous optimization;vector optimization;random optimization;statistics	Vision	78.05374052620424	27.265214490142526	148320
af28a851b3997139d17047a1f56129e83e4cc48d	on the dual formulation of regularized linear systems with convex risks	convex duality;online learning;logistic regression;linear system;linear predictive;learning methods;linear model;numerical algorithm;learning problems;regulation;support vector machine;augmented lagrangian	In this paper, we study a general formulation of linear prediction algorithms including a number of known methods as special cases. We describe a convex duality for this class of methods and propose numerical algorithms to solve the derived dual learning problem. We show that the dual formulation is closely related to online learning algorithms. Furthermore, by using this duality, we show that new learning methods can be obtained. Numerical examples will be given to illustrate various aspects of the newly proposed algorithms.	algorithm;convex function;numerical analysis;numerical linear algebra;numerical method;online machine learning	Tong Zhang	2002	Machine Learning	10.1023/A:1012498226479	convex analysis;support vector machine;regulation;mathematical optimization;proximal gradient methods for learning;combinatorics;duality;augmented lagrangian method;computer science;online machine learning;machine learning;linear model;mathematics;logistic regression;linear system	ML	73.58610220861574	25.166685147615407	148453
36e399de9a40602160ed37eb2da865ca8c7dcb46	boundary of subdifferentials and calmness moduli in linear semi-infinite optimization		This paper was originally motivated by the problem of providing a pointbased formula (only involving the nominal data, and not data in a neighborhood) for estimating the calmness modulus of the optimal set mapping in linear semi-infinite optimization under perturbations of all coefficients. With this aim in mind, the paper establishes as a key tool a basic result on finite-valued convex functions in the ndimensional Euclidean space. Specifically, this result provides an upper limit characterization of the boundary of the subdifferential of such a convex function. When applied to the supremum function associated with our constraint system, this characterization allows us to derive an upper estimate for the aimed calmness modulus in linear semi-infinite optimization under the uniqueness of nominal optimal solution.	coefficient;convex function;level of measurement;mathematical optimization;modulus of continuity;optimization problem;semiconductor industry;subderivative	María J. Cánovas;Abderrahim Hantoute;Juan Parra;F. Javier Toledo-Moreo	2015	Optimization Letters	10.1007/s11590-014-0767-1	mathematical optimization;mathematical analysis;discrete mathematics;mathematics	ML	71.46521908890338	20.62851984571364	148458
81d47647c2ba14016befdbf36ca6039a1a6553b7	target radius methods for nonsmooth convex optimization		Abstract We consider the target radius method of A. Ouorou (2009) and propose some extensions to handle particular classes of (a) nonsmooth convex (nonlinearly) constrained optimization problems; (b) bilevel optimization problems. The given approaches generalize the well-known level bundle methods , and one of the new algorithms possesses (nearly) dimension-independent iteration complexity.	convex optimization;mathematical optimization	Welington de Oliveira	2017	Oper. Res. Lett.	10.1016/j.orl.2017.10.010	proper convex function;convex analysis;combinatorics;subderivative;mathematical optimization;convex optimization;constrained optimization;conic optimization;mathematics;bilevel optimization;linear matrix inequality	ML	72.80645778142642	23.43582318740117	149144
e1d315fb264e2aaf9503af608d2f394f717623d7	a component-based dual decomposition method for the opf problem		This paper proposes a component-based dual decomposition of the nonconvex AC optimal power flow (OPF) problem, where the modified dual function is solved in a distributed fashion. The main contribution of this work is that is demonstrates that a distributed method with carefully tuned parameters can converge to globally optimal solutions despite the inherent nonconvexity of the problem and the absence of theoretical guarantees of convergence. This paper is the first to conduct extensive numerical analysis resulting in the identification and tabulation of the algorithmic parameter settings that are crucial for the convergence of the method on 72 AC OPF test instances. Moreover, this work provides a deeper insight into the geometry of the modified Lagrange dual function of the OPF problem and highlights the conditions that make this function differentiable. This numerical demonstration of convergence coupled with the scalability and the privacy preserving nature of the proposed method makes it well suited for smart grid applications such as multi-period OPF with demand response (DR) and security constrained unit commitment (SCUC) with contingency constraints and multiple transmission system operators (TSOs).	component-based software engineering;converge;duality (optimization);lagrange multiplier;lagrangian relaxation;maxima and minima;numerical analysis;scalability;sysop;table (information)	Sleiman Mhanna;Gregor Verbic;Archie C. Chapman	2017	CoRR		mathematical optimization;decomposition method (constraint satisfaction);differentiable function;scalability;distributed computing;computer science;transmission system;numerical analysis;smart grid;convergence (routing);power system simulation	ML	73.55309023674708	28.436588541396034	149256
15f5672e9cab2d1e2e3bb5f1b28c2f400ecefd29	applying powell's symmetrical technique to conjugate gradient methods	symmetrical technique;line search;descent algorithm;global convergence;conjugate gradient method;conjugate gradient;numerical experiment;spectral analysis	A new conjugate gradient method is proposed for applying Powell's symmetrical technique to conjugate gradient methods in this paper, which satisfies the sufficient descent property for any line search. Using Wolfe line searches, the global convergence of the method is derived from the spectral analysis of the conjugate gradient iteration matrix and Zoutendijk's condition. Based on this, two concrete descent algorithms are developed. 200s numerical experiments are presented to verify their performance and the numerical results show that these algorithms are competitive compared with the PRP + algorithm. Finally, a new explanation of the relationship between conjugate gradient methods and quasi-Newton methods is discussed.	algorithm;conjugate gradient method;experiment;formal methods;iteration;line search;local convergence;newton;numerical analysis;parallel redundancy protocol;powell's method;quasi-newton method	Dongyi Liu;Genqi Xu	2011	Comp. Opt. and Appl.	10.1007/s10589-009-9302-1	gradient descent;mathematical optimization;conjugate residual method;gradient method;calculus;derivation of the conjugate gradient method;descent direction;mathematics;conjugate gradient method;biconjugate gradient stabilized method;nonlinear conjugate gradient method;biconjugate gradient method;line search;algorithm	ML	77.41413261489716	23.17994827789575	149543
fc193be5153d15f76fb56972314b7ad23d0917e3	a proximal point algorithm for dc fuctions on hadamard manifolds	90c26;nonconvex optimization;dc functions;49m30;proximal point algorithm;90c48;hadamard manifolds	An extension of a proximal point algorithm for difference of two convex functions is presented in the context of Riemannian manifolds of nonposite sectional curvature. If the sequence generated by our algorithm is bounded it is proved that every cluster point is a critical point of the function (not necessarily convex) under consideration, even if minimizations are performed inexactly at each iteration. Application in maximization problems with constraints, within the framework of Hadamard manifolds is presented.	convex function;critical point (network science);expectation–maximization algorithm;hadamard transform;iteration	Joao Carlos de Oliveira Souza;P. Roberto Oliveira	2015	J. Global Optimization	10.1007/s10898-015-0282-7	mathematical optimization;mathematical analysis;topology;hadamard three-lines theorem;mathematics	ML	72.7657535713547	22.22750314999961	149699
bced66cd2b3a9fe0f7866a894397eb4a28cf001a	accelerated methods for the socp-relaxed component-based distributed optimal power flow		In light of the increased focus on distributed methods, this paper proposes two accelerated subgradient methods and an adaptive penalty parameter scheme to speed-up the convergence of ADMM on the component-based dual decomposition of the second-order cone programming (SOCP) relaxation of the OPF. This work is the first to apply an adaptive penalty parameter method along with an accelerated subgradient method together in one scheme for distributed OPF. This accelerated scheme is demonstrated to reach substantial speed-ups, as high as 87%, on real-world test systems with more than 9000 buses, as well as on other difficult test cases.	component-based software engineering;consensus (computer science);gradient descent;lagrangian relaxation;linear programming relaxation;network topology;second-order cone programming;subgradient method;test case	Sleiman Mhanna;Archie C. Chapman;Gregor Verbic	2018	2018 Power Systems Computation Conference (PSCC)		electric power transmission;mathematics;acceleration;mathematical optimization;test case;subgradient method;convergence (routing)	HPC	73.54664915175695	28.515712131260607	149919
dc2d6f4f401e6443436e39e575e358f8d0880113	higher order duality in vector optimization over cones		In this paper higher order cone convex, pseudo convex, strongly pseudo convex, and quasiconvex functions are introduced. Higher order sufficient optimality conditions are given for a weak minimum, minimum, strong minimum and Benson proper minimum solution of a vector optimization problem. A higher order dual is associated and weak and strong duality results are established under these new generalized convexity assumptions.	mathematical optimization;vector optimization	Meetu Bhatia	2012	Optimization Letters	10.1007/s11590-010-0248-0	perturbation function;convex analysis;subderivative;mathematical optimization;conic optimization;combinatorics;mathematical analysis;convex cone;convex optimization;duality;convex combination;duality gap;linear matrix inequality;quasiconvex function;weak duality;mathematics;strong duality;proper convex function	EDA	72.21625524155995	21.26088350802986	149924
54d8d382ba94d1ae01615d541568b2d38aaa634c	continuum of zero points of a mapping on a compact, convex set	54c60;continuum of zero points;constrained equilibria;algorithm;65k05;variational inequality;nonlinear equation;zero point problem;nonlinear equations;54h25;91b50;convex set;91b24;stationary point;intersection theorem	Let X be a nonempty, compact and convex set in Rn and φ be an outer semicontinuous mapping from X to the collection of nonempty, compact convex subsets of Rn. We show that for any nonzero vector c in Rn there exists a set of stationary points of φ on X with respect to c connecting a point in the boundary of X at which c x is minimized on X to another point in the boundary of X at which c x is maximized on X. We provide several conditions on φ under which there exists a continuum of zero points of φ connecting two such points in the boundary of X, and an intersection result on a convex, compact set. An application to constrained equilibria is also given.	convex set;semi-continuity;stationary process;triune continuum paradigm	Dolf Talman;Yoshitsugu Yamamoto	2004	SIAM Journal on Optimization	10.1137/S1052623402415469	subderivative;mathematical optimization;stationary point;mathematical analysis;variational inequality;topology;intersection theorem;nonlinear system;convex hull;mathematics;convex set	Theory	71.46018145891831	20.188002431553528	149992
876bb19dd3f7183b17a80e66b5e7a884864a3125	stability property in bifunction-set optimization	bifunction-set optimization;set optimization;ky fan inequality;set-valued map;semicontinuity;90c29;49j40;49k40	This paper gives sufficient conditions for the lower and upper semicontinuities of the solution mapping of a model, called the parametric bifunction-set optimization problem, which provides a bridge between several parametric set optimization problems and parametric generalized vector Ky Fan inequality problems. Our main theorems, applied to the just mentioned problems, give some new or sharper results.	perturbation function;program optimization	Pham Huu Sach	2018	J. Optimization Theory and Applications	10.1007/s10957-018-1280-y	mathematical optimization;ky fan inequality;mathematics;parametric statistics;optimization problem	Theory	71.80043390764658	21.269606184575096	150028
b0a12ad31bbc4ed000e56b6f71dbed181f84765f	a new t-f function approach for discrete global optimization	tunnel function;filled function;t f function;indexing terms;satisfiability;journal;global optimization;discrete global optimization	The T-F function method is an approach to find the global minimum of a multidimensional function. This paper gives a new definition of T-F function for discrete global optimization. A T-F function satisfying this definition is proposed. Furthermore, we discuss the properties of the proposed T-F function and design a new discrete T-F function algorithm. Numerical results on several test problems indicate that the proposed algorithm is reliable and efficient.	algorithm;global optimization;mathematical optimization;maxima and minima;numerical method	Weixiang Wang;Youlin Shang	2009	JCP	10.4304/jcp.4.3.179-183	discrete optimization;mathematical optimization;index term;computer science;theoretical computer science;evaluation function;mathematics;continuous optimization;rastrigin function;algorithm;piecewise;global optimization;satisfiability	ML	75.94012251964857	23.794675837607457	150122
b3fbf596389966a3dc12aa51f1b2c5cab5a99ba8	global optimization with spline constraints: a new branch-and-bound method based on b-splines	knot insertion;nonconvex;nonlinear;mixed integer;b splines;branch and bound;piecewise polynomials	This paper discusses the use of splines as constraints in mathematical programming. By combining themature theory of the B-spline and thewidely used branch-and-bound framework a novel spatial branch-and-bound (sBB) method is obtained. The method solves nonconvexmixed-integer nonlinear programming (MINLP) problemswith spline constraints to global optimality.Abroad applicability follows from the fact that a splinemay represent any (piecewise) polynomial and accurately approximate other nonlinear functions. The method relies on a reformulation–convexification technique which results in lifted polyhedral relaxations that are efficiently solved by an LP solver. The method has been implemented in the sBB solver Convex ENvelopes for Spline Optimization (CENSO). In this paper CENSO is compared to several state-of-the-art MINLP solvers on a set of polynomially constrained NLP problems. To further display the versatility of the method a realistic pump synthesis problem of class MINLP is solved with exact and approximated pump characteristics.	approximation algorithm;b-spline;branch and bound;convex hull;global optimization;mathematical optimization;natural language processing;nonlinear programming;nonlinear system;polyhedron;polynomial;solver;spline (mathematics)	Bjarne Grimstad;Anders Sandnes	2016	J. Global Optimization	10.1007/s10898-015-0358-4	b-spline;mathematical optimization;combinatorics;discrete mathematics;nonlinear system;mathematics;branch and bound	EDA	69.32375812369405	23.997648431656753	150676
2fb70fa7b2d7485d8d0cf26ecb030e1b24dc6d6e	efficient neural networks for solving variational inequalities	co coercive mappings;variational inequalities;exponential stability;projection neural networks;pseudo monotone mapping	In this paper, we propose efficient neural network models for solving a class of variational inequality problems. Our first model can be viewed as a generalization of the basic projection neural network proposed by Friesz et al. [3]. As the basic projection neural network, it only needs some function evaluations and projections onto the constraint set, which makes the model very easy to implement, especially when the constraint set has some special structure such as a box, or a ball. Under the condition that the underlying mapping F is pseudo-monotone with respect to a solution, a condition that is much weaker than those required by the basic projection neural network, we prove the global convergence of the proposed neural network. If F is strongly pseudo-monotone, we prove its globally exponential stability. Then to improve the efficient of the neural network, we modify it by choosing a new direction that is bounded away from zero. Under the condition that the underlying mapping F is co-coercive, a condition that is a little stronger than pseudo-monotone but is still weaker than those required by the basic projection neural network, we prove the exponential stability and global convergence of the improved model. We also reported some computational results, which illustrated that the new method is more efficient than that of Friesz et al. [3].	artificial neural network;variational inequality;variational principle	Suoliang Jiang;Deren Han;Xiaoming Yuan	2012	Neurocomputing	10.1016/j.neucom.2012.01.020	mathematical optimization;combinatorics;discrete mathematics;mathematics;exponential stability	ML	75.05515707172593	23.937761764054603	150802
dae31957123e135a3bd5100d614fb607185173aa	on constructing total orders and solving vector optimization problems with total orders	total order;nonconvex analysis;nonconvex optimization;optimization problem;scalarization;vector optimization	In this paper, we introduce a construction method of total ordering cone on Rn . It is shown that any total ordering cone on Rn is isomorphic to the cone Rn lex . Existence of a total ordering cone that contain given cone with a compact base is shown. By using this cone, a solving method of vector and set valued optimization problems is presented.	cone (formal languages);convex cone;lex (software);mathematical optimization;vector optimization	Mahide Küçük;Mustafa Soyertem;Yalçin Küçük	2011	J. Global Optimization	10.1007/s10898-010-9576-y	optimization problem;mathematical optimization;combinatorics;mathematical analysis;mathematics;vector optimization;total order	Theory	70.82284455812656	22.68783615070345	150929
1d6cdd7fec207197b88e8c83033a529432910900	spatial branch-and-bound algorithm for miqcps featuring multiparametric disaggregation	nonlinear programming;discretization;bilinear terms;quadratic optimization;mixed integer nonlinear programming	Spatial branch-and-bound (B&B) is widely used for the global optimization of non-convex problems. It basically works by iteratively reducing the domain of the variables so that tighter relaxations can be achieved that ultimately converge to the global optimal solution. Recent developments for bilinear problems have brought us piecewise relaxation techniques that can prove optimality for a sufficiently large number of partitions and hence avoid spatial B&B altogether. Of these, normalized multiparametric disaggregation (NMDT) exhibits a good performance due to the logarithmic increase in the number of binary variables with the number of partitions. We now propose to integrate NMDT with spatial B&B for solving mixed-integer quadratically constrained minimization problems. Optimality-based bound tightening is also part of the algorithm so as to compute tight lower bounds in every step of the search and reduce the number of nodes to explore. Through the solution of a set of benchmark problems from the literat...	algorithm;alpha compositing;baron;bilinear filtering;branch and bound;discrepancy function;discretization;display resolution;international symposium on fundamentals of computation theory;item unique identification;linear programming relaxation;mathematical optimization;natural language processing;search tree;switch statement;time complexity	Pedro M. Castro	2017	Optimization Methods and Software	10.1080/10556788.2016.1264397	mathematical optimization;combinatorics;discrete mathematics;nonlinear programming;discretization;mathematics	NLP	73.00265737075068	25.483510706626934	150970
55eb53e7d2f78571cac11e13928524a4808b200b	on the iteratively regularized gauss-newton method for solving nonlinear ill-posed problems	frechet derivative;iterative method;problema mal planteado;regularisation;convergence;nonlinear ill posed problems;stopping rule;relacion convergencia;probleme mal pose;exact solution;regularisation tikhonov;taux convergence;convergence rate;nonlinear ill posed problem;regularization;regla parada;derivada frechet;metodo gauss newton;metodo iterativo;the iteratively regularized gauss newton method;convergencia;methode iterative;indexation;ill posed problem;rates of convergence;non linearite;american mathematical society;no linealidad;nonlinearity;regularizacion;tikhonov regularization;methode gauss newton;derivee frechet;gauss newton method;regle arret	The iteratively regularized Gauss-Newton method is applied to compute the stable solutions to nonlinear ill-posed problems F (x) = y when the data y is given approximately by yδ with ‖yδ − y‖ ≤ δ. In this method, the iterative sequence {xk} is defined successively by xk+1 = x δ k − (αkI +F (xk)F (xk)) ( F (xk) ∗(F (xk)− y) +αk(xk − x0) ) , where x0 := x0 is an initial guess of the exact solution x † and {αk} is a given decreasing sequence of positive numbers admitting suitable properties. When xk is used to approximate x †, the stopping index should be designated properly. In this paper, an a posteriori stopping rule is suggested to choose the stopping index of iteration, and with the integer kδ determined by this rule it is proved that ‖xδkδ − x †‖ ≤ C inf { ‖xk − x†‖+ δ √ αk : k = 0, 1, . . . } with a constant C independent of δ, where xk denotes the iterative solution corresponding to the noise free case. As a consequence of this result, the convergence of xδkδ is obtained, and moreover the rate of convergence is derived when x0 − x† satisfies a suitable “source-wise representation”. The results of this paper suggest that the iteratively regularized Gauss-Newton method, combined with our stopping rule, defines a regularization method of optimal order for each 0 < ν ≤ 1. Numerical examples for parameter estimation of a differential equation are given to test the theoretical results.	approximation algorithm;emoticon;estimation theory;gauss–newton algorithm;iteration;newton's method;nonlinear system;numerical method;rate of convergence;well-posed problem	Jin Qi-nian	2000	Math. Comput.	10.1090/S0025-5718-00-01199-6	fréchet derivative;regularization;mathematical optimization;mathematical analysis;convergence;nonlinear system;calculus;mathematics;iterative method;rate of convergence;tikhonov regularization;algebra	ML	81.46724618317151	18.523728885039297	151117
84c02c488928c9ee8d4533c7973d1879521efb5b	properties of equation reformulation of the karush-kuhn-tucker condition for nonlinear second order cone optimization problems	second order;strong second order sufficient condition;merit function;karush kuhn tucker;satisfiability;sqp type method;second order optimality conditions;sequential quadratic programming;optimization problem;second order cone;constraint nondegeneracy condition;optimality condition;equation reformulation;implicit function theorem	We give an equation reformulation of the Karush–Kuhn–Tucker (KKT) condition for the second order cone optimization problem. The equation is strongly semismooth and its Clarke subdifferential at the KKT point is proved to be nonsingular under the constraint nondegeneracy condition and a strong second order sufficient optimality condition. This property is used in an implicit function theorem of semismooth functions to analyze the convergence properties of a local sequential quadratic programming type (for short, SQP-type) method by Kato and Fukushima (Optim Lett 1:129–144, 2007). Moreover, we prove that, a local solution x∗ to the second order cone optimization problem is a strict minimizer of the Han penalty merit function when the constraint nondegeneracy condition and the strong second order optimality condition are satisfied at x∗.	courant–friedrichs–lewy condition;edmund m. clarke;han unification;karush–kuhn–tucker conditions;mathematical optimization;nonlinear system;optimization problem;penalty method;sequential quadratic programming;subderivative;tucker decomposition	Yun Wang;Liwei Zhang	2009	Math. Meth. of OR	10.1007/s00186-008-0241-x	optimization problem;mathematical optimization;mathematical analysis;implicit function theorem;calculus;mathematics;sequential quadratic programming;karush–kuhn–tucker conditions;second-order logic;satisfiability	ML	72.61745471325793	21.562346943006126	151398
c8a54122e5a37cf6d7289cbf59d32716e24978c1	saddle point and exact penalty representation for generalized proximal lagrangians	exact penalty representations;90c26;constrained nonlinear programming;saddle points;generalized proximal lagrangian;90c46	In this paper, we introduce a generalized proximal Lagrangian function for the constrained nonlinear programming problem and discuss existence of its saddle points. In particular, the local saddle point is obtained by using the second-order sufficient conditions, and the global saddle point is given without requiring compactness of constraint set and uniqueness of the optimal solution. Finally, we establish equivalent relationship between global saddle points and exact penalty representations.	nonlinear programming;nonlinear system	Jinchuan Zhou;Naihua Xiu;Changyu Wang	2012	J. Global Optimization	10.1007/s10898-011-9784-0	mathematical optimization;combinatorics;mathematical analysis;mathematics;saddle point	ML	72.44299581540741	22.077034815502028	151550
cb36a234eb1cee88402f5da0fc3ef2a2781d62dd	computing the level set convex hull		Quasiconvex (QC) functions are functions whose level sets are convex. The quasiconvex envelope (QCE) of a given function, g, is the maximal QC function below g. The QCE provides a level set representation for the convex hull of every level set of a given function. We present a nonlocal line solver for computing the QCE of a given function. The algorithm is based on solving the one dimensional problem on lines, which can be done by a fast marching or sweeping method. Convergence of the algorithm is established.	convex hull	Bilal Abbasi;Adam M. Oberman	2018	J. Sci. Comput.	10.1007/s10915-017-0522-8	mathematical optimization;mathematical analysis;mathematics;subderivative;proper convex function;convex hull;convexity in economics;absolutely convex set;quasiconvex function;support function;convex set	Theory	73.30853139867388	20.36319375518532	151897
5fc4b7716df0c14a0dc338f140aec7d9892a040c	an algorithm for convex quadratic programming that requires o(n3.5l) arithmetic operations	minimisation;interior point methods;quadratic programming;minimization;karmarkars algorithm;algorithme karmarkar;programmation quadratique;complexite calcul;complejidad calculo;analytic center;interior point;convex quadratic programming;minimizacion;computing complexity;method of centers;algorithme;politope;algorithm;mathematical programming;convex function;programacion cuadratica;programmation mathematique;point interieur;fonction convexe;programacion matematica;funcion convexa;algoritmo;polytope	A new interior point method for minimizing a convex quadratic function over a polytope is developed. We show that our method requires O(n3.5L) arithmetic operations. In our algorithm we construct a sequence Pz0, Pz1, …, Pzk, … of nested convex sets that shrink towards the set of optimal solution(s). During iteration k we take a partial Newton step to move from an approximate analytic center of Pzk − 1 to an approximate analytic center of Pzk. A system of linear equations is solved at each iteration to find the step direction. The solution that is available after O(√mL) iterations can be converted to an optimal solution. Our analysis indicates that inexact solutions to the linear system of equations could be used in implementing this algorithm.	algorithm;quadratic programming	Sanjay Mehrotra;Jie Sun	1990	Math. Oper. Res.	10.1287/moor.15.2.342	mathematical optimization;combinatorics;interior point method;mathematics;geometry;quadratic programming	Theory	74.99925485253904	22.75815625811788	151950
c365bd244331dbc6e7271e4c4bb05af5a4cc8ddc	semi-infinite programming: theory, methods, and applications	optimisation;programacion semi infinita;non linear programming;aplicacion;optimality conditions;optimizacion;nonlinear programming;programacion no lineal;duality;90c30;programmation non lineaire;theory methods;algorithme;90c;65d15;condicion optimalidad;algorithm;analyse parametrique;dualite;65k05;condition optimalite;semi infinite programming;algorithms for functional approximation;approximation fonctionnelle;duality theory;mathematical programming;mathematical programming and optimization techniques;parametric analysis;dualidad;optimization;programmation semi infinie;application;programmation mathematique;programacion matematica;optimality condition;algoritmo;49d39	Starting from a number of motivating and abundant applications in §2, including control of robots, eigenvalue computations, mechanical stress of materials, and statistical design, the authors describe a class of optimization problems which are referred to as semi-infinite, because their constraints bound functions of a finite number of variables on a whole region. In §§3–5, first- and second-order optimality conditions are derived for general nonlinear problems as well as a procedure for reducing the problem locally to one with only finitely many constraints. Another main effort for achieving simplification is through duality in §6. There, algebraic properties of finite linear programming are brought to bear on duality theory in semi-infinite programming. Section 7 treats numerical methods based on either discretization or local reduction with the emphasis on the design of superlinearly convergent (SQP-type) methods. Taking this differentiable point of view, this paper can be considered to be complementar...	semi-infinite programming;semiconductor industry	Rainer Hettich;Kenneth O. Kortanek	1993	SIAM Review	10.1137/1035089	mathematical optimization;combinatorics;mathematical analysis;discrete mathematics;duality;duality;nonlinear programming;mathematics;geometry;parametric statistics;algorithm;algebra	Theory	73.6697773748727	21.38287929324312	152036
a1ea152dabc173919738776fec011a47443b916f	singularities of monotone vector fields and an extragradient-type algorithm	extragradient algorithm;hadamard manifold;objective function;monotone vector field;riemannian manifold;euclidean space;global optimization;convex analysis;vector field;constrained optimization problem	Bearing in mind the notion of monotone vector field on Riemannian manifolds, see [12–16], we study the set of their singularities and for a particular class of manifolds develop an extragradient-type algorithm convergent to singularities of such vector fields. In particular, our method can be used for solving nonlinear constrained optimization problems in Euclidean space, with a convex objective function and the constraint set a constant curvature Hadamard manifold. Our paper shows how tools of convex analysis on Riemannian manifolds can be used to solve some nonconvex constrained problem in a Euclidean space.	algorithm;constrained optimization;convex analysis;convex function;mathematical optimization;nonlinear system;optimization problem;monotone	Orizon Pereira Ferreira;L. R. Lucambio Pérez;Sandor Z. Németh	2005	J. Global Optimization	10.1007/s10898-003-3780-y	convex analysis;norm;statistical manifold;mathematical optimization;scalar curvature;mathematical analysis;vector field;topology;manifold;euclidean space;mathematics;global analysis;convex set;vector optimization;global optimization;pseudo-riemannian manifold	Vision	72.61852807863688	21.733900597877586	152288
6c24af7973764da66fecacdca9640a802f0a5de7	method of hill tunneling via simplex centroid for continuous piecewise linear programming	minimization;high definition video;linear programming;optimization;programming;algorithm design and analysis;tunneling	This paper works on a heuristic algorithm with determinacy for the global optimization of continuous piecewise linear (CPWL) programming. CPWL is widely applied since it can be equivalently transformed into D.C. programming, and further, concave optimization over a polyhedron. Considering that the super-level sets of concave piecewise linear functions are polyhedra, we propose the hill tunneling via simplex centroid (HTSC) algorithm, which is able to escape a local optimum to the other side of its contour surface by cutting across the super-level set. The searching path for the hill tunneling is established by using the centroid of a constructed simplex. In the numerical experiments, the proposed HTSC algorithm is compared with CPLEX and the hill detouring (HD) method, which shows its superior performance on the numerical efficiency and the global search capability.	cplex;concave function;experiment;global optimization;heuristic (computer science);hill climbing;linear function;linear programming;local optimum;mathematical optimization;numerical analysis;piecewise linear continuation;polyhedron;simplex algorithm;split tunneling;tunneling protocol	Zhiming Xu;Kuangyu Liu;Xiangming Xi;Shuning Wang	2015	2015 54th IEEE Conference on Decision and Control (CDC)	10.1109/CDC.2015.7403260	algorithm design;programming;mathematical optimization;combinatorics;linear-fractional programming;computer science;linear programming;mathematics;quantum tunnelling;simplex algorithm;algorithm	EDA	73.20934046276274	24.03210081606381	152441
f0d786792a6acca1581eab664fac5a7db2264292	an accurate active set conjugate gradient algorithm with project search for bound constrained optimization	box constrained optimization;global convergence;prp method	In the paper, we propose an active set identification technique which accurately identifies active constraints in a neighborhood of an isolated stationary point without strict complementarity conditions. Based on the identification technique, we propose a conjugate gradient algorithm for large-scale bound constrained optimization. In the algorithm, the recently developed modified Polak-Ribiére-Polyak method is used to update the variables with indices outside of the active set, while the projected gradient method is used to update the active variables. Under appropriate conditions, we show that the proposed method is globally convergent. Numerical experiments are presented using bound constrained problems in the CUTEr test problem library.	active set method;algorithm;cuter;complementarity theory;conjugate gradient method;constrained optimization;experiment;iteration;local convergence;mathematical optimization;numerical method;parallel redundancy protocol;stationary process	Wanyou Cheng;Qunfeng Liu;Donghui Li	2014	Optimization Letters	10.1007/s11590-013-0609-6	mathematical optimization;constrained optimization;combinatorics;gradient method;control theory;mathematics;nonlinear conjugate gradient method	ML	75.60175775946563	24.283386419995633	152591
9d903ec480e2b8a09050dbfc9169f617f79577ab	fast estimates of hankel matrix condition numbers and numeric sparse interpolation	hankel system;early termination;polynomial interpolation;sparse polynomial interpolation;condition number;hankel matrix;monte carlo;vandermonde system;gohberg semencul formula	We investigate our early termination criterion for sparse polynomial interpolation when substantial noise is present in the values of the polynomial. Our criterion in the exact case uses Monte Carlo randomization which introduces a second source of error. We harness the Gohberg-Semencul formula for the inverse of a Hankel matrix to compute estimates for the structured condition numbers of all arising Hankel matrices in quadratic arithmetic time overall, and explain how false ill-conditionedness can arise from our randomizations. Finally, we demonstrate by experiments that our condition number estimates lead to a viable termination criterion for polynomials with about 20 non-zero terms and of degree about 100, even in the presence of noise of relative magnitude 10-5.	condition number;eisenstein's criterion;experiment;monte carlo method;polynomial interpolation;second source;sparse matrix	Erich Kaltofen;Wen-shin Lee;Zhengfeng Yang	2011		10.1145/2331684.2331704	vandermonde matrix;mathematical optimization;combinatorics;discrete mathematics;hankel matrix;mathematics	Theory	82.56541095631495	24.351761242630104	152838
6b811e90102530bfe9e4fa6c26600c4fc3bf5268	ascim - an accelerated constrained simplex technique		Abstract   In many design studies similar optimization problems have to be solved. In these problems a non-linear objective,function, defined in terms of a small number of variables, has to be minimized subject to the additional requirement that the variables must satisfy non-linear and/or linear inequality constrainss. There are still relatively few algorithms that claim to solve this type of problem, especially when analytical expression for the first derivatives of the function and constraints are not available.  In the author's experience, these derivatives are rarely available and not always continuous over all the feasible region and the algorithm described here is initially a combination of the simplex technique with the ideas of projection and hemstitching. A switch to quadratic hillclimbing techniques is made when the computer senses that the region of the minimum has been achieved, thus enabling rapid ultimate convergence to be obtained.		L. C. W. Dixon	1973	Computer-Aided Design	10.1016/0010-4485(73)90150-4	mathematical optimization;calculus;mathematics;algorithm	EDA	74.18544511419694	24.28742992913112	152845
61645ce1e5590839ef84569ee01993b8ea870fb0	a nonlinear primal-dual method for total variation-based image restoration	operador lineal;problema neumann;gaussian noise;preconditionnement;primal dual interior point method;euler lagrange equation;ecuacion euler lagrange;regularisation;convergence;restauration image;probleme neumann;equation euler lagrange;methode newton;laplacian;primal dual method;ruido gaussiano;level set;regularisation tikhonov;preconditioning;image restoration;global convergence;operateur convolution;lagrange multiplier;methode primale duale;conjugate gradient method;65k10;regularization;primal dual;objective function;restauracion imagen;laplacien;convergencia;conjugate gradient;laplaciano;linear operator;metodo primal dual;metodo gradiente conjugado;68u10;bruit gaussien;multiplicateur lagrange;multiplicador lagrange;quadratic convergence;unconstrained optimization;precondicionamiento;total variation;metodo newton;newton method;optimizacion sin restriccion;ams subject classification;regularizacion;convolution operator;neumann problem;tikhonov regularization;methode gradient conjugue;optimisation sans contrainte;operateur lineaire;65f10;variation totale	We present a new method for solving total variation (TV) minimization problems in image restoration. The main idea is to remove some of the singularity caused by the nondifferentiability of the quantity |∇u| in the definition of the TV-norm before we apply a linearization technique such as Newton’s method. This is accomplished by introducing an additional variable for the flux quantity appearing in the gradient of the objective function, which can be interpreted as the normal vector to the level sets of the image u. Our method can be viewed as a primal-dual method as proposed by Conn and Overton [A Primal-Dual Interior Point Method for Minimizing a Sum of Euclidean Norms, preprint, 1994] and Andersen [Ph.D. thesis, Odense University, Denmark, 1995] for the minimization of a sum of Euclidean norms. In addition to possessing local quadratic convergence, experimental results show that the new method seems to be globally convergent.	conn;circuit restoration;gradient;iscb overton prize;image restoration;interior point method;loss function;newton;newton's method;normal (geometry);optimization problem;rate of convergence;t-norm;technological singularity	Tony F. Chan;Gene H. Golub;Pep Mulet	1999	SIAM J. Scientific Computing	10.1137/S1064827596299767	mathematical optimization;mathematical analysis;calculus;mathematics;conjugate gradient method;algebra	Vision	76.52182517882527	22.424163231844027	153312
82fec75b454cd9fc7be6e16690a9840f764e3709	new choices of preconditioning matrices for generalized inexact parameterized iterative methods	sistema lineal;metodo directo;preconditionnement;iterative method;analisis numerico;saddle point problem;computacion informatica;matematicas aplicadas;mathematiques appliquees;65f05;stochastic method;numerical method;problema stokes;preconditioning;matrix inversion;inversion matriz;gran sistema;linear system;analyse numerique;65c20;metodo iterativo;14c20;numerical analysis;metodo numerico;large system;ciencias basicas y experimentales;methode iterative;matematicas;65f08;algebra lineal numerica;saddle point problems;algebre lineaire numerique;inversion matrice;methode stochastique;precondicionamiento;65w05;numerical linear algebra;probleme stokes;point selle;numerical experiment;stokes problem;systeme lineaire;grupo a;iteration method;applied mathematics;65h10;methode directe;sparse linear system;65f10;methode numerique;direct method;grand systeme;metodo estocastico	For large sparse saddle point problems, Chen and Jiang recently studied a class of generalized inexact parameterized iterative methods (see [F. Chen, Y.-L. Jiang, A generalization of the inexact parameterized Uzawamethods for saddle point problems, Appl. Math. Comput. 206 (2008) 765–771]). In this paper, the methods are modified and some choices of preconditioning matrices are given. These preconditioning matrices have advantages in solving large sparse linear system. Numerical experiments of a model Stokes problem are presented. © 2010 Elsevier B.V. All rights reserved.	entity–relationship model;experiment;iteration;iterative method;linear system;navier–stokes equations;numerical linear algebra;parameterized complexity;preconditioner;sparse matrix	Yang Cao;Mei-Qun Jiang;Lin-Quan Yao	2010	J. Computational Applied Mathematics	10.1016/j.cam.2010.05.054	numerical analysis;calculus;mathematics;geometry;iterative method;algorithm;algebra	AI	82.39188657038845	20.92156980461451	153366
26010e94d24c63a1ef48ed3a8d0423d0d78bb680	optimality conditions of type kkt for optimization problem with interval-valued objective function via generalized derivative	kkt conditions;derivative for set valued functions;interdisciplinar;optimization under interval uncertainty;interval valued functions	This paper addresses the optimization problems with interval-valued objective function. For this we consider two types of order relation on the interval space. For each order relation, we obtain KKT conditions using of the concept of generalized Hukuhara derivative ( $$gH$$ -derivative) for interval-valued functions. The $$gH$$ -derivative is a concept more general of derivative for this class of functions than other concepts of derivative. We make some comparison with previous result given by other authors and we show some advantages of our result.	distribution (mathematics);karush–kuhn–tucker conditions;mathematical optimization;optimization problem	Yurilev Chalco-Cano;Weldon A. Lodwick;Antonio Rufián-Lizana	2013	FO & DM	10.1007/s10700-013-9156-y	weak derivative;fréchet derivative;mathematical optimization;mathematical analysis;total derivative;second derivative;directional derivative;calculus;mathematics;partial derivative;reduced derivative;karush–kuhn–tucker conditions;parametric derivative;symmetric derivative;functional derivative;generalizations of the derivative	EDA	71.96124571473374	21.666980316001336	153558
ef3b5df572ba80bea3f4c77546ade334cf4b1f79	inexact inverse subspace iteration with preconditioning applied to non-hermitian eigenvalue problems	47a15;linear algebra;preconditionnement;iterative method;analisis numerico;inverso;eigenvalue problem;convergence;sous espace invariant;eigenvalue approximation;aproximacion;rango;probleme valeur propre;preconditioning;inverse subspace iteration;analyse numerique;approximation;metodo iterativo;eigenvalue;algorithme;iterative methods;algorithm;convergencia;matrice creuse;iteraccion;numerical analysis;methode iterative;tolerancia;algebre lineaire;valor propio;rang;65f08;invariant subspace;coste;iteration;sous espace;algebra lineal;precondicionamiento;valeur propre;tolerance;subespacio invariante;sparse matrix;65f50;65f15;65h17;65f10;inverse;rank;problema valor propio;matriz dispersa;cout;algoritmo	Convergence results are provided for inexact inverse subspace iteration applied to the problem of finding the invariant subspace associated with a small number of eigenvalues of a large sparse matrix. These results are illustrated by the use of block-GMRES as the iterative solver. The costs of the inexact solves are measured by the number of inner iterations needed by the iterative solver at each outer step of the algorithm. It is shown that for a decreasing tolerance the number of inner iterations should not increase as the outer iteration proceeds, but it may increase for preconditioned iterative solves. However, it is also shown that an appropriate small rank change to the preconditioner can produce significant savings in costs, and in particular, can produce a situation where there is no increase in the costs of the iterative solves even though the solve tolerances are reducing. Numerical examples are provided to illustrate the theory.	algorithm;generalized minimal residual method;iteration;numerical method;preconditioner;solver;sparse matrix	Mickaël Robbé;Miloud Sadkane;Alastair Spence	2009	SIAM J. Matrix Analysis Applications	10.1137/060673795	mathematical optimization;combinatorics;linear algebra;calculus;mathematics;preconditioner;iterative method;algebra	ML	82.32143144773801	21.0924694864429	153627
b49f7b921ec2cf04dee95fbeb926d06a29bbcd67	the gap function of a convex multicriteria optimization problem	multicriteria optimization;multicriteria analysis;optimizacion vectorial;optimisation;fonction valeur;optimizacion;convex programming;duality;programmation convexe;optimization problem;dualite;programacion lineal;cone;optimisation vectorielle;linear programming;programmation lineaire;vector optimization;dualidad;optimization;value function;analisis multicriterio;analyse multicritere;cono;programacion convexa	We generalize the concept of a gap function previously defined for a convex (scalar) optimization problem to a convex multicriteria optimization problem and study its various properties.	loss function;mathematical optimization;optimization problem	G. Y. Chen;C. J. Goh;X. Q. Yang	1998	European Journal of Operational Research	10.1016/S0377-2217(97)00300-7	convex analysis;optimization problem;subderivative;support function;mathematical optimization;conic optimization;combinatorics;discrete mathematics;convex optimization;duality;convex combination;cone;ellipsoid method;entropy maximization;linear matrix inequality;nonlinear programming;quasiconvex function;linear programming;convexity in economics;random coordinate descent;mathematics;bellman equation;vector optimization;effective domain;proper convex function	Theory	71.70263940548331	21.372078122966098	153893
4d0701f6e55a3a56809d252352aaf9313f6e50d5	convex separable minimization problems with a linear constraint and bounded variables	inequality constraint;polynomial complexity;linear constraint;iterative algorithm;objective function;convex function;necessary and sufficient condition	Consider the minimization problem with a convex separable objective function over a feasible region defined by linear equality constraint(s)/linear inequality constraint of the form “greater than or equal to” and bounds on the variables. A necessary and sufficient condition and a sufficient condition are proved for a feasible solution to be an optimal solution to these two problems, respectively. Iterative algorithms of polynomial complexity for solving such problems are suggested and convergence of these algorithms is proved. Some convex functions, important for problems under consideration, as well as computational results are presented.	algorithm;computation;convex function;feasible region;iterative method;linear equation;loss function;optimization problem;polynomial;regular expression;social inequality;time complexity	Stefan M. Stefanov	2005	Int. J. Math. Mathematical Sciences	10.1155/IJMMS.2005.1339	constraint logic programming;convex function;convex analysis;mathematical optimization;combinatorics;mathematical analysis;convex optimization;binary constraint;convex combination;ellipsoid method;constraint satisfaction;linear matrix inequality;mathematics;iterative method;constraint;logarithmically convex function;proper convex function	Theory	71.39166241171033	22.879320008099192	153922
f27a261f8c8d2212e1e4c7acf5d741d80d2c929e	solving fuzzy linear systems based on the structured element method	fuzzy numbers;general fuzzy linear systems;same formal function;the structured element method	According to the structured element method, this paper investigates general fuzzy linear systems of the form Ax = y with A matrices of crisp coefficients and y fuzzy number vectors. The necessary and sufficient condition for a fuzzy solution existence is given.	coefficient;convex function;fuzzy control system;fuzzy number;linear system;numerical analysis;numerical method;singular value decomposition;monotone	Xu-dong Sun;Si-zong Guo	2008		10.1007/978-3-540-88914-4_34	mathematical optimization;fuzzy logic;of the form;matrix (mathematics);mathematics;linear system;fuzzy number	HPC	77.79512255239243	18.615450735549487	153991
391f2dc840133094606e98fbe13ad8a67663f464	the newton and cauchy perspectives on computational nonlinear optimization	optimisation;conjugate gradients;optimizacion;cauchy s method;model based;methode newton;probleme non lineaire;newton s method;metric;metric based;conjugate gradient method;nonlinear problems;90;65;metodo gradiente conjugado;metrico;optimization;metodo newton;newton method;methode gradient conjugue;49;nonlinear optimization;metrique;variable metric;affine reduction	The methods of Newton and Cauchy provide complementary perspectives on computational nonlinear optimization, and a study of the interplay between these two classical approaches sheds new light on other more recent methods for minimizing a nonlinear function. A coherent picture (Newton/Cauchy framework) emerges at the root level, which relates the basic methods of the subject to one another and reveals some promising directions for further algorithmic development. By way of illustration, a particular algorithm motivated by the framework is formulated and studied numerically.	computation;newton;nonlinear programming;program optimization	John L. Nazareth	1994	SIAM Review	10.1137/1036053	mathematical optimization;mathematical analysis;nonlinear programming;calculus;mathematics;geometry;newton's method	ML	74.38583237632353	21.13805233033803	154076
8698ce02a4a7fe650509506dbf7b70a330757901	some extensions of the karnik-mendel algorithms for computing an interval type-2 fuzzy set centroid	convergence;root finding;newton raphson root finding method;type 2 fuzzy logic system;centroid;approximation algorithms;numerical integration technique;karnik mendel iterative algorithm;algorithm design and analysis convergence approximation algorithms frequency selective surfaces newton method approximation methods;centroid computation method;fuzzy set theory;interval type 2 fuzzy set;iterative methods;karnik mendel km algorithms;interval type 2 fuzzy set centroid;iterative methods fuzzy set theory;type 2 fuzzy set;newton method;approximation methods;weighted enhanced km algorithm;centroid computation method karnik mendel iterative algorithm interval type 2 fuzzy set centroid type 2 fuzzy logic system newton raphson root finding method weighted enhanced km algorithm numerical integration technique;algorithm design and analysis;root finding interval type 2 fuzzy set centroid karnik mendel km algorithms;frequency selective surfaces	Computing the centroid of an interval type-2 fuzzy set is an important operation in a type-2 fuzzy logic system, and is usually implemented by Karnik-Mendel (KM) iterative algorithms. This paper proves that the centroid computation of an interval type-2 fuzzy set (IT2 FS) using KM algorithms is equivalent to the Newton-Raphson root-finding method in numerical analysis, and explains how continuous enhanced KM (CEKM) algorithms can be used to compute that centroid. Weighted enhanced KM (WEKM) algorithms are proposed to connect EKM algorithms and CEKM algorithms together using numerical integration techniques. Three new kinds of centroid computation methods are summarized as root finding, CEKM algorithms and WEKM algorithms. Numerical examples illustrate the applications of these new centroid computation methods, and demonstrate the superity of the WEKM algorithms.	computation;farthest-first traversal;fuzzy logic;fuzzy set;iterative method;newton;newton's method;numerical analysis;numerical integration;numerical linear algebra;root-finding algorithm	Xinwang Liu;Jerry M. Mendel	2011	2011 IEEE Symposium on Advances in Type-2 Fuzzy Logic Systems (T2FUZZ)	10.1109/T2FUZZ.2011.5949546	mathematical optimization;discrete mathematics;machine learning;mathematics	EDA	80.69271203619638	18.483165126166423	154171
c1a408a102c00cbf5e9b0f55ebae9837b49165bf	independent component analysis of fixed-point algorithm based on secant method	fixed point;independent component analysis		algorithm;fixed-point iteration;independent component analysis;secant method	Yong-Hyun Cho;Yong-Soo Park	2003			independent component analysis;mathematical analysis;secant method;fixed point;mathematical optimization;mathematics	EDA	81.10529567274169	18.51903728825266	154209
18b566996350eff9f9fc8e628b3d6f21251c99dd	curse of dimensionality reduction in max-plus based approximation methods: theoretical estimates and improved pruning algorithms	optimisation;bregman distance;high dimensionality;measurement;approximation error;approximation method;combinatorial optimization problem;curse of dimensionality;accuracy optimal control approximation error measurement aerospace electronics equations;optimal control;approximation theory;accuracy;function approximation;algebra;aerospace electronics;dimensionality free method dimensionality reduction max plus based approximation methods theoretical error estimation pruning algorithms value function approximation high dimensional optimal control problems approximation problem facility location problems k center combinatorial optimization problems connection costs bregman distance basis functions;value function;error estimate;combinatorial mathematics;facility location;optimal control problem;optimisation algebra approximation theory combinatorial mathematics facility location function approximation optimal control	Max-plus based methods have been recently developed to approximate the value function of possibly high dimensional optimal control problems. A critical step of these methods consists in approximating a function by a supremum of a small number of functions (max-plus “basis functions”) taken from a prescribed dictionary. We study several variants of this approximation problem, which we show to be continuous versions of the facility location and k-center combinatorial optimization problems, in which the connection costs arise from a Bregman distance. We give theoretical error estimates, quantifying the number of basis functions needed to reach a prescribed accuracy. We derive from our approach a refinement of the curse of dimensionality free method introduced previously by McEneaney, with a higher accuracy for a comparable computational cost.	algorithmic efficiency;approximation algorithm;basis function;bellman equation;bregman divergence;combinatorial optimization;computation;curse of dimensionality;dictionary;dimensionality reduction;mathematical optimization;metric k-center;optimal control;pruning (morphology);refinement (computing)	Stéphane Gaubert;William M. McEneaney;Zheng Qu	2011	IEEE Conference on Decision and Control and European Control Conference	10.1109/CDC.2011.6161386	mathematical optimization;approximation error;combinatorics;curse of dimensionality;optimal control;function approximation;facility location problem;machine learning;mathematics;accuracy and precision;bellman equation;measurement;statistics;approximation theory	ML	71.94602368811843	24.496620267136986	154240
f0dd7ac022b128a236d0165aaeccf8376d26a2ca	a useful form of unitary matrix obtained from any sequence of unit 2-norm n-vectors	linear algebra;analyse erreur;15a18;orthogonality;analisis numerico;decomposition valeur singuliere;matriz cuadrada;algorithm analysis;loss of orthogonality;factorisation qr;connaissance;singular value decomposition;calcul erreur;conocimiento;bidiagonalization;65gxx;vecteur;elemento matricial;qr factorization;modified gram schmidt;matrice carree;vector theory;analyse numerique;rounding error analysis;matrice unitaire;15a23;square matrix;error analysis;factorizacion qr;metodo factorizacion;knowledge;65g50;teoria vectorial;numerical analysis;orthogonalisation gram schmidt;loss of biorthogonality;factorization method;algebre lineaire;65f25;algorithme qr;65f35;elementary unitary transformations;orthogonal matrices;matriz unitaria;calculo error;algebra lineal;15a12;matrix element;biorthogonalite;analyse algorithme;vector;decomposicion valor singular;error redondear;15a57;unitary matrix;theorie vectorielle;65f50;biorthogonal sets of vectors;methode factorisation;65f30;cero;element matriciel;analisis algoritmo;orthogonalite;rounding error;ortogonalidad;erreur arrondi;zero	Charles Sheffield pointed out that the modified Gram–Schmidt (MGS) orthogonalization algorithm for the QR factorization of B ∈ Rn×k is mathematically equivalent to the QR factorization applied to the matrix B augmented with a k × k matrix of zero elements on top. This is true in theory for any method of QR factorization, but for Householder’s method it is true in the presence of rounding errors as well. This knowledge has been the basis for several successful but difficult rounding error analyses of algorithms which in theory produce orthogonal vectors but significantly fail to do so because of rounding errors. Here we show that the same results can be found more directly and easily without recourse to the MGS connection. It is shown that for any sequence of k unit 2-norm n-vectors there is a special (n+k)-square unitary matrix which we call a unitary augmentation of these vectors and that this matrix can be used in the analyses without appealing to the MGS connection. We describe the connection of this unitary matrix to Householder matrices. The new approach is applied to an earlier analysis to illustrate both the improvement in simplicity and advantages for future analyses. Some properties of this unitary matrix are derived. The main theorem on orthogonalization is then extended to cover the case of biorthogonalization.	algorithm;householder transformation;modelling of general systems;qr decomposition;round-off error;rounding;schmidt decomposition;sparse matrix;the matrix	Christopher C. Paige	2009	SIAM J. Matrix Analysis Applications	10.1137/080725167	orthogonality;vector;numerical analysis;linear algebra;unitary matrix;calculus;square matrix;mathematics;knowledge;singular value decomposition;qr decomposition;algebra	Theory	81.96168613736207	20.990603789491956	154591
03c86f411eaa96757fce6bcf3fc3d12cec2b9a8b	guaranteed tensor decomposition: a moment approach		We develop a theoretical and computational framework to perform guaranteed tensor decomposition, which also has the potential to accomplish other tensor tasks such as tensor completion and denoising. We formulate tensor decomposition as a problem of measure estimation from moments. By constructing a dual polynomial, we demonstrate that measure optimization returns the correct CP decomposition under an incoherence condition on the rank-one factors. To address the computational challenge, we present a hierarchy of semidefinite programs based on sums-of-squares relaxations of the measure optimization problem. By showing that the constructed dual polynomial is a sum-of-squares modulo the sphere, we prove that the smallest SDP in the relaxation hierarchy is exact and the decomposition can be extracted from the solution under the same incoherence condition. One implication is that the tensor nuclear norm can be computed exactly using the smallest SDP as long as the rank-one factors of the tensor are incoherent. Numerical experiments are conducted to test the performance of the moment approach.	approximation algorithm;experiment;interpolation;linear programming relaxation;mathematical optimization;modulo operation;moment matrix;noise reduction;optimization problem;polynomial;semidefinite programming	Gongguo Tang;Parikshit Shah	2015			tensor product;symmetric tensor;mathematical optimization;combinatorics;discrete mathematics;tensor;cartesian tensor;tensor;tensor contraction;mathematics	ML	78.87971183393003	23.878649490017807	154630
c9d924aaf0065c53c8b63e9051cb7817b0fa990b	a damped semismooth newton method for mixed linear complementarity problems	semismooth newton method;non smooth equation;90c33;complementarity problem;monotone convergence;linear complementarity problem;65h10;90c53	where F : R → R is affine and X ⊆ R is a box set given by F(x) = Ax − b and X = {x ∈ R | i ≤ xi ≤ i ∀ i = 1, 2, . . . , n} with A = (aij ) ∈ Rn×n. Denote = ( 1, . . . , n) and = ( 1, . . . , n). Throughout the paper, we assume that the strict inequality i < i holds for all i = 1, . . . , n and that A is an M-matrix. A matrix A is called an M-matrix if it has non-positive off-diagonals and A−1 exists with A−1 ≥ 0, i.e. A−1 is non-negative. For detailed discussion of M-matrices, we refer the reader to [3,10].	complementarity theory;mixed linear complementarity problem;newton's method;social inequality	Zhe Sun;Jinping Zeng	2011	Optimization Methods and Software	10.1080/10556780903575680	mathematical optimization;mathematical analysis;calculus;mathematics;mixed complementarity problem;linear complementarity problem;complementarity theory	ML	77.35026219929254	20.426441084925482	154640
dbbe82d2af37e409d672f833b75fef3e4c1c2730	computing the block factorization of complex hankel matrices	15b05;calcul scientifique;matriz bloque;analisis numerico;65fxx;implementation;matriz triangular;triangular matrix;46c07;polynomial;matriz toeplitz;analyse numerique;15a23;algorithme;algorithm;triangular toeplitz matrix;computacion cientifica;numerical analysis;matrice bloc;block diagonalization;polinomio;matrice hankel;matrice toeplitz;matrice triangulaire;block matrix;matriz hankel;schur complementation;euclidean algorithm;11cxx;hankel matrix;scientific computation;implementacion;toeplitz matrix;polynome;algoritmo	In this paper, we present an algorithm for finding an approximate block diagonalization of complex Hankel matrices. Our method is based on inversion techniques of an upper triangular Toeplitz matrix, specifically, by simple forward substitution. We also consider an approximate block diagonalization of complex Hankel matrices via Schur complementation. An application of our algorithm by calculating the approximate polynomial quotient and remainder appearing in the Euclidean algorithm is also given. We have implemented our algorithms in Matlab. Numerical examples are included. They show the effectiveness of our strategy.	approximation algorithm;condition number;error analysis (mathematics);euclidean algorithm;matlab;numerical analysis;numerical method;polynomial;software propagation;toeplitz hash algorithm;triangular matrix	Skander Belhaj	2010	Computing	10.1007/s00607-010-0080-5	euclidean algorithm;combinatorics;hankel matrix;numerical analysis;calculus;toeplitz matrix;triangular matrix;mathematics;implementation;block matrix;algorithm;polynomial;algebra	Theory	81.30134139495811	20.55563086454248	154806
976243f1484f95ba48d55ce0945165a3731d0be9	solution of the fully fuzzy linear systems using the decomposition procedure	the adomian decomposition method;sistema lineal;metodo directo;fully fuzzy linear system ffls;iterative method;methode decomposition adomian;analisis numerico;numero difuso;fuzzy number;nombre flou;matrix inversion;satisfiability;inversion matriz;linear system;analyse numerique;metodo iterativo;matrice floue;numerical analysis;methode iterative;methode jacobi;algebra lineal numerica;algebre lineaire numerique;metodo jacobi;inversion matrice;sistema difuso;numerical linear algebra;systeme flou;systeme lineaire;ffls;methode directe;adomian decomposition method;fuzzy system;direct method;arithmetique floue;jacobi method	Abstract   This paper tries to extend the Adomian decomposition method for solving fully fuzzy linear systems (shown as FFLS). For finding a positive fuzzy vector      x    ˜      that satisfies      A    ∼      x    ˜    =    b    ˜     , where      A    ∼      and      b    ˜      are respectively a fuzzy matrix and a fuzzy vector, we employ Dubois and Prade’s approximate arithmetic operators on LR fuzzy numbers. We also transform the FFLS and use the Adomian decomposition method for solving an FFLS. Then, we compare this method with the Jacobi iterative technique. Additionally, we give some numerical examples.	linear system	Mehdi Dehghan;Behnam Hashemi	2006	Applied Mathematics and Computation	10.1016/j.amc.2006.05.043	direct method;jacobi method;numerical analysis;fuzzy number;calculus;mathematics;adomian decomposition method;iterative method;numerical linear algebra;linear system;algorithm;fuzzy control system;algebra;satisfiability	Logic	81.54949530140196	19.964661598771002	155089
4031c550b725ddcebce75f0e8fb8a571cace6108	sparsity-aware adaptive filtering based on a douglas-rachford splitting	minimization;standards;sparse echo cancellation problem sparsity aware adaptive filtering convex functions douglas rachford splitting method cost functions upper bound;cost function;convex functions;convex functions cost function signal processing algorithms minimization standards algorithm design and analysis noise;signal processing algorithms;minimisation adaptive filters convex programming;algorithm design and analysis;noise	In this paper, we propose a novel online scheme for the sparse adaptive filtering problem. It is based on a formulation of the adaptive filtering problem as a minimization of the sum of (possibly nonsmooth) convex functions. Our proposed scheme is a time-varying extension of the so-called Douglas-Rachford splitting method. It covers many existing adaptive filtering algorithms as special cases. We show several examples of special choices of the cost functions that reproduce those existing algorithms. Our scheme achieves a monotone decrease of an upper bound of the distance to the solution set of the minimization under certain conditions. We applied a simple algorithm that falls under our scheme to a sparse echo cancellation problem where it shows excellent convergence performance.	adaptive filter;algorithm;convex function;echo suppression and cancellation;loss function;sparse matrix;monotone	Isao Yamada;Silvia Gandy;Masao Yamagishi	2011	2011 19th European Signal Processing Conference		mathematical optimization;combinatorics;convex optimization;machine learning;mathematics	Vision	74.2630047646501	26.40545988449492	155152
a154f8af4e3fd77f5958d4da545143442455105b	a globally convergent inexact newton-like cayley transform method for inverse eigenvalue problems		We propose an inexact Newton method for solving inverse eigenvalue problems (IEP).This method is globalized by employing the classical backtracking techniques. A global convergence analysis of this method is provided and the R-order convergence property is proved under some mild assumptions. Numerical examples demonstrate that the proposed method is very effective in solving the IEP with distinct eigenvalues.		Yonghui Ling;Xiubin Xu	2013	J. Applied Mathematics	10.1155/2013/630618	mathematical optimization;combinatorics;mathematical analysis;mathematics	ML	80.27878349587402	21.25293914962778	155528
4f920e89bd50b8eb1b1d56d83b7fce6aed85a8b8	stochastic perturbation of reduced gradient & grg methods for nonconvex programming problems	nonconvex programming;constraints optimization;stochastic perturbation;numerical computation;reduced gradient and grg methods	In this paper, we consider nonconvex differentiable programming under linear and nonlinear differentiable constraints. A reduced gradient and GRG (generalized reduced gradient) descent methods involving stochastic perturbation are proposed and we give a mathematical result establishing the convergence to a global minimizer. Numerical examples are given in order to show that the method is effective to calculate. Namely, we consider classical tests such as the statistical problem, the octagon problem, the mixture problem and an application to the linear optimal control servomotor problem.	gradient	Abdelkrim El Mouatasim;Rachid Ellaia;José Eduardo Souza de Cursi	2014	Applied Mathematics and Computation	10.1016/j.amc.2013.10.024	mathematical optimization;combinatorics;mathematical analysis;mathematics	Robotics	73.53510262043895	23.455936182834137	156374
ad96271893d3376ab6f8cc3e6315a9e5a1b42643	just scratching the surface: partial exploration of initial values in reach-set computation		We suggest a method for significantly reducing the so-called wrapping effect, i.e., the accumulation of approximation errors incurred during reach-set computation of differential equations when repeatedly over-approximating intermediate reach sets by tractable computational representations of sets in the Rn. Our method can be implemented on top of any known reach-set computation method and generalizes bracketing systems [1] by being based on dimension-wise enclosures of the reach sets of appropriate lower-dimensional surfaces of the initial set selected due to monotonicity properties. Thus exploring just low-volume sub-sets rather than the entire initial and intermediate reach sets, accuracy is enhanced as the approximation error tends to be correlated with set volume. At the same time, the curse of dimensionality that set partitioning methods are prone to is avoided by resorting to a number of subsets linear in the problem dimension. Technically, we first conduct sensitivity analysis of the solution mapping with respect to initial states based on a simulation-based technique, and then determine subsets which are extracted from the boundary of the initial set for performing reachability analysis. We test our method by using it on top of the validated ODE solver VNODE-LP and demonstrate its effect by comparison with existing methods, using illustrative examples of non-linear dynamics.	approximation error;cobham's thesis;computation;control system;control theory;curse of dimensionality;dimensionality reduction;dynamical system;extrapolation;nonlinear system;reachability;simulation;solver;tree accumulation;wrapping (graphics)	Bai Xue;Martin Fränzle;Peter Nazier Mosaad	2017	2017 IEEE 56th Annual Conference on Decision and Control (CDC)	10.1109/CDC.2017.8263905	bracketing;mathematical optimization;curse of dimensionality;computer science;differential equation;reachability;nonlinear system;monotonic function;approximation error;solver	Robotics	76.89525702802945	18.58636494114857	156404
e0f2f33d2a366c3132911b9f29ef6765a78350ee	a tolerant algorithm for linearly constrained optimization calculations	optimisation sous contrainte;constrained optimization;convergence theorem;optimisation;matrix factorization;optimizacion;funcion no lineal;selective constraint;right hand side;implementation;performance;contrainte inegalite;inequality constraint;non linear function;linear constraint;satisfiability;algorithme;optimizacion con restriccion;algorithm;ejecucion;semi infinite programming;active set method;degeneration;constrenimiento desigualdad;fonction non lineaire;linearly constrained optimization;optimization;fortran;rendimiento;programmation semi infinie;nonlinear optimization;algoritmo	Two extreme techniques when choosing a search direction in a linearly constrained optimization calculation are to take account of all the constraints or to use an active set method that satisfies selected constraints as equations, the remaining constraints being ignored. We prefer an intermediate method that treats all inequality constraints with “small” residuals as inequalities with zero right hand sides and that disregards the other inequality conditions. Thus the step along the search direction is not restricted by any constraints with small residuals, which can help efficiency greatly, particularly when some constraints are nearly degenerate. We study the implementation, convergence properties and performance of an algorithm that employs this idea. The implementation considerations include the choice and automatic adjustment of the tolerance that defines the “small” residuals, the calculation of the search directions, and the updating of second derivative approximations. The main convergence theorem imposes no conditions on the constraints except for boundedness of the feasible region. The numerical results indicate that a Fortran implementation of our algorithm is much more reliable than the software that was tested by Hock and Schittkowski (1981). Therefore the algorithm seems to be very suitable for general use, and it is particularly appropriate for semi-infinite programming calculations that have many linear constraints that come from discretizations of continua.	algorithm;constrained optimization;linear programming;mathematical optimization	M. J. D. Powell	1989	Math. Program.	10.1007/BF01589118	mathematical optimization;constrained optimization;combinatorics;discrete mathematics;performance;mathematics;active set method;constraint;matrix decomposition;implementation;satisfiability	Theory	77.20779176613662	23.465788054933096	156618
8f873e97eaf8a0c8be19efa168dd7f8ef98b0cba	the majorant method and convergence for solving nondifferentiable equations in banach space	convergence theorem;convergence;majorant method;methode newton;espacio banach;banach space;newton s method;non differentiable operator;convergencia;nondifferentiable operator;operateur non derivable;approximate solution;methode majorante;metodo newton;newton method;espace banach	In this paper we consider the problem of approximation solutions of equations with nondierentiable operator and establish convergence theorems using the majorant method. The convergence conditions are extended and improved compared with recent results. Meanwhile, the sharp estimates are also given in this paper. Ó 2001 Elsevier Science Inc. All rights reserved.	approximation	Danfu Han	2001	Applied Mathematics and Computation	10.1016/S0096-3003(99)00183-6	mathematical optimization;mathematical analysis;modes of convergence;calculus;mathematics;newton's method	AI	74.53218425516576	21.996731289297994	156850
7d213d3129d8fb55233c36fe3fc4960cf9fd52f5	computation of derivatives of repeated eigenvalues and the corresponding eigenvectors of symmetric matrix pencils	multiple eigenvalues;grupo de excelencia;symmetric matrix;ciencias basicas y experimentales;matematicas;tecnologias generalidades;tecnologias;eigenvalue and eigenvector sensitivities;65f15;close eigenvalues;eigenvectors;15a22	This paper presents and analyzes new algorithms for computing the numerical values of derivatives, of arbitrary order, and of eigenvalues and eigenvectors of A(ρ)x(ρ) = λ(ρ)B(ρ)x(ρ) at a point ρ = ρ0 at which the eigenvalues considered are multiple. Here A(ρ) and B(ρ) are hermitian matrices which depend analytically on a single real variable ρ, and B(ρ0) is positive definite. The algorithms are valid under more general conditions than previous algorithms. Numerical results support the theoretical analysis and show that the algorithms are also useful when eigenvalues are merely very close rather than coincident.	algorithm;computation;numerical analysis;numerical linear algebra	Alan L. Andrew;Roger C. E. Tan	1998	SIAM J. Matrix Analysis Applications	10.1137/S0895479896304332	eigenvalues and eigenvectors;calculus;mathematics;geometry;symmetric matrix;algebra	Theory	79.91110353101088	19.445989555400548	157033
cb400d5992c27cb6b815927d4f69210e6e6f50b6	weighted sampling and signal reconstruction in spline subspaces	iterative method;traitement signal;spline;reconstruccion senal;esplin;echantillonnage;metodo subespacio;convergence rate;iterative algorithm;methode sous espace;metodo iterativo;sampling;algorithme;algorithm;methode iterative;signal processing;tratamiento digital;reproducing kernel;subspace method;signal reconstruction;digital processing;reconstruction signal;shift invariant space;muestreo;procesamiento senal;traitement numerique;spline subspace;irregular sampling;algoritmo	In this paper, we construct reproducing kernel in spline sub-spaces and use the reproducing kernel to obtain reconstructions formula. We also improve the A-P iterative algorithm, and use the algorithm to implement the reconstruction from weighted samples, and obtain the explicit convergence rate of the algorithm in spline subspaces.	iterative method;liu hui's π algorithm;numerical analysis;rate of convergence;sampling (signal processing);signal reconstruction;spline (mathematics)	Jun Xian;Shi-Ping Luo;Wei Lin	2004	First International Symposium on Control, Communications and Signal Processing, 2004.	10.1016/j.sigpro.2005.05.013	mathematical optimization;smoothing spline;computer science;calculus;signal processing;mathematics;geometry;iterative method;algorithm	Embedded	81.49109183771138	20.060011702354704	157140
1f56c9705090b06650030b48e877762d377cc1fe	modified newton methods for solving a semismooth reformulation of monotone complementarity problems	rate of convergence;fonction merite;optimisation;convergence;generalized newton method;methode newton;merit function;non linear complementarity problem;global convergence;nonlinear complementarity problems;algorithme;probleme complementarite non lineaire;merit functions;convergencia;semismooth functions;mathematical programming;algorithms;probleme complementarite;problema complementariedad;complementarity problem;optimization;metodo newton;newton method;fonction semi lisse;direction finding;linear equations;programmation mathematique;nonlinear complementarity problem	In this paper, we propose a Newton-type method for solving a semismooth reformulation of monotone complementarity problems. In this method, a direction-finding subproblem, which is a system of linear equations, is uniquely solvable at each iteration. Moreover, the obtained search direction always affords a direction of sufficient decrease for the merit function defined as the squared residual for the semismooth equation equivalent to the complementarity problem. We show that the algorithm is globally convergent under some mild assumptions. Next, by slightly modifying the direction-finding problem, we propose another Newton-type method, which may be considered a restricted version of the first algorithm. We show that this algorithm has a superlinear, or possibly quadratic, rate of convergence under suitable assumptions. Finally, some numerical results are presented.	complementarity theory;newton;monotone	Nobuo Yamashita;Masao Fukushima	1996	Math. Program.	10.1007/BF02614394	mathematical optimization;mathematical analysis;convergence;calculus;mathematics;newton's method;linear equation;rate of convergence	AI	75.81218211572208	22.856395855715935	157261
57a757134a9050ef4a5df9efd06b9eb650a69a30	an approach to homotopy and degree theory	cramer s rule;degree theory;differential equations;fixed points;homotopy invariance theorem	Spawned by Scarf's pioneering work on the calculation of fixed points, an entire new field in mathematical programming has emerged. A wide array of problems that can be posed as fixed point problems, such as problems involving equilibria, games, systems of equations, global optimization, and structural mechanics, have come into the purview of these new constructive techniques. Underlying these techniques are certain mathematical concepts from differential topology, more specifically degree theory Brouwer and homotopies. Usually, the mathematical machinery required for these concepts necessitates a keen familiarity with piecewise linear and/or differential topology.#R##N##R##N#In this paper, the development is made by following the paths arising from an ordinary differential equation. The differential equation results from Cramer's rule for linear equations. The applicability and ease of our approach are shown by obtaining simple proofs of theorems in differential topology.		C. B. García;Willard I. Zangwill	1979	Math. Oper. Res.	10.1287/moor.4.4.390	mathematical optimization;examples of differential equations;mathematical analysis;discrete mathematics;cramer's rule;geometric analysis;integrating factor;stochastic partial differential equation;linear differential equation;delay differential equation;mathematics;fixed point;differential equation;algorithm;statistics	Theory	70.68320199304671	19.55292243036489	157346
5a2c3199db469b1cc807374fc6d9ec7b439a9f5c	a remark concerning mei zhen's paper on singular nonlinear equations	equation non lineaire;ecuacion no lineal;singular equation;convergence numerique;algorithme;numerical convergence;resolucion sistema ecuacion;algorithm;resolution systeme equation;nonlinear equation;quadratic convergence;equation system solving;non linear equation;equation singuliere;convergencia numerica;ecuacion singular;algoritmo	By analyzing Mei Zhen's approach to singular nonlinear equations, cf. [1], it is set forth for what kind of algorithm quadratic convergence can be proved in a relatively easy manner. Thus, Mei Zhen's convergence result is extended. In addition, a relation to the approach of Hoy, cf. [2], is revealed. Ausgehend von einer Analyse des Zuganges, der von Mei Zhen in [1] für singuläre nichtlineare Gleichungen vorgeschlagen wird, zeigen wir, für welchen Algorithmentyp quadratische Konvergenz in relativ einfacher Weise bewiesen werden kann. Damit wird Mei Zhen's Konvergenzergebnis erweitert. Außerdem wird eine Beziehung zu dem Zugang von Hoy in [2] aufgezeigt.	algorithm;eine and zwei;jacobian matrix and determinant;kernel (linear algebra);multidimensional digital pre-distortion;nonlinear programming;nonlinear system;rate of convergence	Annegret Hoy	1990	Computing	10.1007/BF02241273	mathematical optimization;mathematical analysis;nonlinear system;calculus;mathematics;rate of convergence;algorithm	Theory	81.51269324115502	18.529147569501035	157617
34b65452b82ed8a50bfc866c89916777410ab7bb	second order symmetric duality in nondifferentiable multiobjective programming	second order;multiobjective programming;efficient solutions;ηpseudobonvexity;properly efficient solutions;nondifferentiable programming;η pseudobonvexity;multiobjective symmetric duality	A pair of Mond–Weir type second order symmetric nondifferentiable multiobjective programs is formulated. Weak, strong and converse duality theorems are established under g-pseudobonvexity assumptions. Special cases are discussed to show that this paper extends some work appeared in this area. 2004 Elsevier Inc. All rights reserved.	linear programming;minimax;multi-objective optimization;yang	Izhar Ahmad	2005	Inf. Sci.	10.1016/j.ins.2004.06.002	mathematical optimization;combinatorics;weak duality;mathematics;strong duality;mathematical economics;second-order logic	AI	71.82511056901689	21.707413268073232	157720
568f9aa824917d597e62479eb6236ebe66049f70	d-iteration method or how to improve gauss-seidel method		The aim of this paper is to present the recently proposed fluid diffusion based algorithm in the general context of the matrix inversion problem associated to the Gauss-Seidel method. We explain the simple intuitions that are behind this diffusion method and how it can outperform existing methods. Then we present some theoretical problems that are associated to this representation as open research problems. We also illustrate some connected problems such as the graph transformation and the PageRank problem.	algorithm;gauss–seidel method;graph rewriting;iteration;linear equation;linear programming;open research;pagerank;the matrix	Dohy Hong	2012	CoRR		computer science;theoretical computer science;mathematics;algorithm	Robotics	81.4709822610183	24.992167392574103	157823
c25250075212cc2a2faf4a852ea642be19b110f6	new characterizations of weak sharp minima	conjugate functions;error bounds;weak sharp minima	In this paper, we study the concept of weak sharp minima by using conjugate functions. Not only some well-known results can be obtained in this unified way, but also new characterizations are developed. Finally, under rather weak conditions, we establish the finite termination property for convex programming and variational inequality problem, respectively.	calculus of variations;convex optimization;maxima and minima;social inequality;variational inequality;weak measurement	Jinchuan Zhou;Changyu Wang	2012	Optimization Letters	10.1007/s11590-011-0369-0	mathematical optimization;mathematical analysis;calculus;mathematics	DB	72.56344987402484	21.18052202589492	157841
d520cf9c2b4aa6d466f22cd441ea8c2815a3e098	closure of the right-hand-side set for systems of nonlinear inequalities	right hand side	Consider the set of all vectors b such that the system gx â¦ b has a solution, where g: D â Rm, D â Rn. This paper presents necessary and sufficient conditions for this set to be closed, with special results on continuous and convex functions, and gives an application to mathematical programming.		F. J. Gould;H. Pashner	1969	Operations Research	10.1287/opre.17.5.883	mathematical optimization;combinatorics;discrete mathematics;solution set;mathematics	Theory	71.24751983000375	22.38020321835886	157888
6d050195f5df7d0f263c374de134068d5ae533b8	on linear conic relaxation of discrete quadratic programs	linear conic relaxation;discrete quadratic program;90c 10;rlt method;90c 20;90c 26	On linear conic relaxation of discrete quadratic programs Tiantian Nie, Shu-Cherng Fang, Zhibin Deng & John E. Lavery To cite this article: Tiantian Nie, Shu-Cherng Fang, Zhibin Deng & John E. Lavery (2016): On linear conic relaxation of discrete quadratic programs, Optimization Methods and Software, DOI: 10.1080/10556788.2015.1134528 To link to this article: http://dx.doi.org/10.1080/10556788.2015.1134528	academy;display resolution;experiment;fitts's law;lagrangian relaxation;linear programming relaxation;mathematical optimization;numerical analysis;procedural generation;randomness	Tiantian Nie;Shu-Cherng Fang;Zhibin Deng;John E. Lavery	2016	Optimization Methods and Software	10.1080/10556788.2015.1134528	mathematical optimization;combinatorics;discrete mathematics;lagrangian relaxation;mathematics	ML	70.24608583246645	27.279123987962375	157949
4311023a115bfb14099e0fea36cfb2be8218ab17	on the solution of highly degenerate linear programmes	planificacion integral;metodo simplejo;integrated planning;simplex method;programacion lineal;degeneration;mathematical programming;linear programming;programmation lineaire;scheduling problem;linear program;programmation mathematique;methode simplexe;programacion matematica;ordonnancement	The solution of scheduling problems often gives rise to highly degenerate linear programmes which cause significant computational difficulties for the revised simplex method. Wolfe's highly effective “ad hoc” method for overcoming the cycling or stalling problems associated with degeneracy is described. Here it is given a geometric interpretation in terms of finding a direction of recession for a reduced problem which is fundamental to a full understanding of the procedure. An example of an aircrew scheduling problem is used to illustrate the effectiveness of the method.		D. M. Ryan;Michael R. Osborne	1988	Math. Program.	10.1007/BF01580776	mathematical optimization;linear programming;calculus;mathematics;integrated business planning;simplex algorithm;algorithm	Crypto	69.74642123220765	26.030897074301457	157959
9221f6aa7e6bc041adedf43893025725b09c37b5	projected gradient approach to the numerical solution of the scotlass	optimisation sous contrainte;constrained optimization;analisis componente principal;computacion informatica;methode plus grande pente;numerical solution;optimality conditions;contrainte lasso;analisis datos;steepest descent method;gradiente;contrainte inegalite;shrinkage estimator;inequality constraint;dynamic system;global convergence;gradient;satisfiability;dynamical system;least absolute value method;optimizacion con restriccion;systeme dynamique;condicion optimalidad;data analysis;condition optimalite;simple structure components;estimateur retrecissement;funcion penalidad;constrenimiento desigualdad;ciencias basicas y experimentales;lasso constraint;principal component analysis;metodo mas grande inclinacion;matematicas;statistical computation;calculo estadistico;metodo menor valor absoluto;analyse composante principale;gradient dynamical system on manifolds;hydrology;steepest ascent vector flows;simple structure component;analyse donnee;calcul statistique;least absolute shrinkage and selection operator;551 geology;methode moindre valeur absolue;composante structure simple;sistema dinamico;fonction penalite;grupo a;meteorology;solution numerique;optimality condition;penalty function	The SCoTLASS problem—principal component analysis modified so that the components satisfy the Least Absolute Shrinkage and Selection Operator (LASSO) constraint—is reformulated as a dynamical system on the unit sphere. The LASSO inequality constraint is tackled by exterior penalty function. A globally convergent algorithm is developed based on the projected gradient approach. The algorithm is illustrated numerically and discussed on a well-known data set. © 2004 Elsevier B.V. All rights reserved.	algorithm;dynamical system;gradient;lasso;least squares;numerical analysis;numerical partial differential equations;penalty method;principal component analysis;social inequality	Nickolay T. Trendafilov;Ian T. Jolliffe	2006	Computational Statistics & Data Analysis	10.1016/j.csda.2004.07.017	mathematical optimization;constrained optimization;dynamical system;calculus;mathematics;geometry;statistics	ML	76.30426919270104	21.085636729909563	158242
7629027b2d9acf9b44f219d08bfb6557c3433c4c	modified multistep iterative process for some common fixed point of a finite family of nonself asymptotically nonexpansive mappings	calcul scientifique;strong convergence;computer aided analysis;convergence forte;iterative process;uniformly convex banach;convergencia debil;matematicas aplicadas;analyse assistee;fixed point theorem;modele mathematique;convergencia fuerte;kadec klee property;mathematiques appliquees;espacio banach;condition necessaire suffisante;banach space;modelo matematico;computer modelling;asymptotically nonexpansive mapping;fixed point iteration;theoreme point fixe;teorema punto fijo;modified multistep iterative sequence;fixed point;proceso iterativo;processus iteratif;computacion cientifica;nonself asymptotically nonexpansive mappings;necessary and sufficient condition;common fixed point;mathematical model;uniformly convex banach space;analisis asistido;weak convergence;scientific computation;applied mathematics;convergence faible;condicion necesaria suficiente;espace banach	Let E be a real uniformly convex Banach space and K be a nonempty closed convex subset of E which is also a nonexpansive retract of E . Let Ti : K → E (i = 1, 2, . . . , r) be nonself asymptotically nonexpansive mappings. It is proved that the modified multistep iterative sequence converges weakly and strongly to some common fixed point of {Ti }i=1 under suitable conditions. The results of this paper improve and extend the corresponding results of [J. Schu, Iterative construction of fixed points of asymptotically nonexpansive mappings, J. Math. Anal. Appl. 158 (1991) 407–413; M.O. Osilike, S.C. Aniagbosor, Weak and strong convergence theormes for fixed points of asymptotically nonexpansive mappings, Math. Comput. Modelling 32 (2000) 1181–1191; W. Takahashi, G.E. Kim, Strong convergence of approximants to fixed points of nonexpansive nonself-mappings in Banach spaces, Nonlinear Anal. 3 (32) (1998) 447–454; C.E. Chidume, E.U. Ofoedu, H. Zegeye, Strong and weak convergence theorems for asymptotically nonexpansive mappings, J. Math. Anal. Appl. 280 (2003) 364–374; L. Wang, Strong and weak convergence theorems for common fixed points of nonself asymptotically nonexpansive mappings, J. Math. Anal. Appl. 323 (2006) 550–557; B.L. Xu, M.A. Noor, Fixed point iterations for asymptotically nonexpansive mappings in Banach spaces, J. Math. Anal. Appl. 267 (2002) 444–453]. On the other hand, it is shown that the necessary and sufficient conditions for the strong convergence of the modified multistep iterative sequence to some common fixed points of {Ti }i=1. c © 2006 Elsevier Ltd. All rights reserved.	asymptote;comstock–needham system;convex set;fixed point (mathematics);iteration;minimal mappings;tomotaka takahashi;uniformly convex space	Liping Yang	2007	Mathematical and Computer Modelling	10.1016/j.mcm.2006.09.013	fixed-point iteration;mathematical optimization;mathematical analysis;topology;iterative and incremental development;mathematical model;mathematics;fixed point;fixed-point theorem;weak convergence;banach space	Theory	74.717341839909	18.54071941223012	158278
db736aa97dd954602fdcea6f54bd82fb707546f5	conditioning of semidefinite programs	perturbation method;programmation semi definie;semidefinite programming;perturbation theory;programacion lineal;mathematical programming;metodo perturbacion;block diagonalization;numero de condicionamiento;linear programming;condition number;programmation lineaire;linear program;kantorovic theory;methode perturbation;programmation mathematique;programacion matematica;indice conditionnement;semidefinite program	This paper studies the conditioning of semidefinite programs by analyzing the effect of small perturbations in problem data on the solution. Under the assumptions of strict complementarity and nondegeneracy, an explicit bound on the change in the solution is derived in a primal-dual framework, using tools from the Kantorovǐ c theory. This approach also quantifies the size of permissible perturbations. We include a discussion of these results for block diagonal semidefinite programs, of which linear programming is a special case.	complementarity theory;linear programming;perturbation theory;semidefinite programming	Madhu V. Nayakkankuppam;Michael L. Overton	1999	Math. Program.	10.1007/s101070050070	mathematical optimization;combinatorics;linear programming;condition number;calculus;perturbation theory;mathematics;semidefinite embedding;semidefinite programming	Theory	71.70968701253766	22.038847053758918	158429
e01a7098238038755bf27aba5393101553aa1d44	using inexact gradients in a multilevel optimization algorithm	optimization based multigrid;inexact gradient evaluation;nonlinear optimization;multilevel optimization	Many optimization algorithms require gradients of the model functions, but computing accurate gradients can be computationally expensive. We study the implications of using inexact gradients in the context of the multilevel optimization algorithm MG/Opt. MG/Opt recursively uses (typically cheaper) coarse models to obtain search directions for finer-level models. However, MG/Opt requires the gradient on the fine level to define the recursion. Our primary focus here is the impact of the gradient errors on the multilevel recursion. We analyze, partly through model problems, how MG/Opt is affected under various assumptions about the source of the error in the gradients, and demonstrate that in many cases the effect of the errors is benign. Computational experiments are included.	algorithm;analysis of algorithms;computation;experiment;gradient;mathematical optimization;mg (editor);recursion	Robert Michael Lewis;Stephen G. Nash	2013	Comp. Opt. and Appl.	10.1007/s10589-013-9546-7	mathematical optimization;nonlinear programming;theoretical computer science;mathematics;algorithm	ML	77.81063452945354	27.32712338846561	158473
e1104f33504ea7791fdd31bcc190246e5ade9036	a system of quaternary coupled sylvester-type real quaternion matrix equations	quaternion;sylvester-type equations;moore–penrose inverse;rank;solution;solvability	In this paper we establish some necessary and sufficient solvability conditions for a system of quaternary coupled Sylvester-type real quaternion matrix equations in terms of ranks and generalized inverses of matrices. An expression of the general solution to this system is givenwhen it is solvable. Also, a numerical example is presented to illustrate themain result of this paper. The findings of this paperwidely generalize the known results in the literature. The main results are also valid over the real number field and the complex number field. © 2017 Elsevier Ltd. All rights reserved.	decision problem;numerical analysis	Zhuo-Heng He;Qing-Wen Wang;Yonghui Zhang	2018	Automatica	10.1016/j.automatica.2017.09.008	moore–penrose pseudoinverse;quaternion;real number;matrix (mathematics);mathematics;complex number;mathematical analysis	AI	78.19888886790311	19.53306981759997	158575
d46331b4949e2fb58c2b143a285f829692fd8e92	a block algorithm for computing rank-revealing qr factorizations	65f20;block algorithm;rank revealing qr factorization;singular value;qr factorization;numerical rank;efficient implementation;numerical experiment;ams mos 65f25	We present a block algorithm for computing rank-revealing QR factorizations (RRQR factorizations) of rank-deficient matrices. The algorithm is a block generalization of the RRQR-algorithm of Foster and Chan. While the unblocked algorithm reveals the rank by peeling off small singular values one by one, our algorithm identifies groups of small singular values. In our block algorithm, we use incremental condition estimation to compute approximations to the nullvectors of the matrix. By applying another (in essence also rank-revealing) orthogonal factorization to the nullspace matrix thus created, we can then generate triangular blocks with small norm in the lower right part ofR. This scheme is applied in an iterative fashion until the rank has been revealed in the (updated) QR factorization. We show that the algorithm produces the correct solution, under very weak assumptions for the orthogonal factorization used for the nullspace matrix. We then discuss issues concerning an efficient implementation of the algorithm and present some numerical experiments. Our experiments show that the block algorithm is reliable and successfully captures several small singular values, in particular in the initial block steps. Our experiments confirm the reliability of our algorithm and show that the block algorithm greatly reduces the number of triangular solves and increases the computational granularity of the RRQR computation.	approximation;computation;dijkstra's algorithm;experiment;iterative method;kernel (linear algebra);numerical analysis;qr decomposition;the matrix	Christian H. Bischof;Per Christian Hansen	1992	Numerical Algorithms	10.1007/BF02139475	mathematical optimization;combinatorics;discrete mathematics;ramer–douglas–peucker algorithm;mathematics;singular value;qr decomposition;algebra	HPC	82.57934113022822	22.7469904132132	158599
aa4356fec5ad6981aa3fcd88e5fc34cfc0a7d96c	new nonsmooth equations-based algorithms for ℓ1-norm minimization and applications	null	Recently, Xiao et al. proposed a nonsmooth equations-based method to solve the 1-norm minimization problem 2011 . The advantage of this method is its simplicity and lower storage. In this paper, based on new nonsmooth equations reformulation, we investigate new nonsmooth equations-based algorithms for solving 1-norm minimization problems. Under mild conditions, we show that the proposed algorithms are globally convergent. The preliminary numerical results demonstrate the effectiveness of the proposed algorithms.	algorithm;navier–stokes equations;numerical analysis;taxicab geometry	Lei Wu;Zhe Sun	2012	J. Applied Mathematics	10.1155/2012/139609	mathematical optimization;mathematical analysis;calculus;mathematics	ML	75.9548770683978	24.345895295930035	158870
b8d0b7f7a13f780acec9ea3ebc5e338bb2815b76	systematic study of selected diagonalization methods for configuration interaction matrices	diagonalization methods;hamiltonian matrix;large scale;davidson method;ab inito methods;configuration interaction;subspace method;on the fly;symmetric eigenvalue problem	Several modifications to the Davidson algorithm are systematically explored to establish their performance for an assortment of configuration interaction (CI) computations. The combination of a generalized Davidson method, a periodic two-vector subspace collapse, and a blocked Davidson approach for multiple roots is determined to retain the convergence characteristics of the full subspace method. This approach permits the efficient computation of wave functions for large-scale CI matrices by eliminating the need to ever store more than three expansion vectors (bi) and associated matrix-vector products (σ i), thereby dramatically reducing the I/O requirements relative to the full subspace scheme. The minimal-storage, single-vector method of Olsen is found to be a reasonable alternative for obtaining energies of well-behaved systems to within μEh accuracy, although it typically requires around 50% more iterations and at times is too inefficient to yield high accuracy (ca. 10−10 Eh) for very large CI problems. Several approximations to the diagonal elements of the CI Hamiltonian matrix are found to allow simple on-the-fly computation of the preconditioning matrix, to maintain the spin symmetry of the determinant-based wave function, and to preserve the convergence characteristics of the diagonalization procedure. © 2001 John Wiley & Sons, Inc. J Comput Chem 22: 1574–1589, 2001	algorithm;approximation;circa;computation;configuration interaction;input/output;iteration;john d. wiley;preconditioner;q-chem;requirement	Matthew L. Leininger;C. David Sherrill;Wesley D. Allen;Henry F. Schaefer	2001	Journal of Computational Chemistry	10.1002/jcc.1111	mathematical optimization;combinatorics;hamiltonian matrix;orthogonal diagonalization;computational chemistry;mathematics;physics;quantum mechanics	HPC	82.22764914445122	23.354354654529235	158943
6e8ab9886aa576e6918fd3838ef9bcc423801e0a	robust least square semidefinite programming with applications	infeasibility;spectral gradient projection method;robust correlation stress testing;robust optimization;uncertain least square sdp;strong duality	In this paper, we consider a least square semidefinite programming problem under ellipsoidal data uncertainty. We show that the robustification of this uncertain problem can be reformulated as a semidefinite linear programming problem with an additional second-order cone constraint. We then provide an explicit quantitative sensitivity analysis on how the solution under the robustification depends on the size/shape of the ellipsoidal data uncertainty set. Next, we prove that, under suitable constraint qualifications, the reformulation has zero duality gap with its dual problem, even when the primal problem itself is infeasible. The dual problem is equivalent to minimizing a smooth objective function over the Cartesian product of second-order cones and the Euclidean space, which is easy to project onto. Thus, we propose a simple variant of the spectral projected gradient method [7] to solve the dual problem. While it is well-known that any accumulation point of the sequence generated from the algorithm is a dual optimal solution, we show in addition that the dual objective value along the sequence generated converges to a finite value if and only if the primal problem is feasible, again under suitable constraint qualifications. This latter fact leads to a simple certificate for primal infeasibility in situations when the primal feasible set lies in a known compact set. As an application, we consider robust correlation stress testing where data uncertainty arises due to untimely recording of portfolio holdings. In our computational experiments on this particular application, our algorithm performs reasonably well on medium-sized problems for real data when finding the optimal solution (if exists) or identifying primal infeasibility, and usually outperforms the standard interior-point solver SDPT3 in terms of CPU time. ∗Department of Applied Mathematics, University of New South Wales, Sydney 2052, Australia. E-mail: g.li@unsw.edu.au. This author was partially supported by a research grant from Australian Research Council. †Department of Finance, The Chinese University of Hong Kong, Shatin, Hong Kong. E-mail: alfred@baf.msmail.cuhk.edu.hk ‡Department of Combinatorics and Optimization, University of Waterloo, Waterloo, ON, Canada, N2L 3G1. E-mail: ptingkei@uwaterloo.ca. This author was supported by research grants from AFOSR and NSERC.	algorithm;cartesian closed category;central processing unit;cobham's thesis;computation;constraint logic programming;convex set;duality (optimization);duality gap;epigraph (mathematics);experiment;gradient method;karush–kuhn–tucker conditions;linear programming;mathematical model;optimization problem;public key certificate;rate of convergence;robustification;semidefinite programming;solver;stress testing;tree accumulation	Guoyin Li;Alfred Ka Chun Ma;Ting Kei Pong	2014	Comp. Opt. and Appl.	10.1007/s10589-013-9634-8	mathematical optimization;combinatorics;discrete mathematics;robust optimization;duality;duality gap;mathematics;strong duality	ML	71.02360643942663	23.35441558012427	158990
927da9bfe93cf0730b8b570ef40fc8c9db973f71	symmetric duality in quadratic programming and matrix game equivalence	juego matricial;quadratic programming;equilibrio nash;quadratic program;matematicas aplicadas;relation equivalence;nash equilibrium;mathematiques appliquees;programmation quadratique;duality;nash equilibria;dual problem;dualite;equilibre nash;equivalence relation;matrix game;programacion cuadratica;dualidad;applied mathematics;jeu matriciel;relacion equivalencia;zero sum game	K e y w o r d s S y m m e t r i c duality, Quadratic programming, Matrix game. 1. I N T R O D U C T I O N Dorn [1] gave a symmetric duality theorem for quadratic programs while Dantzig et aL [2] and Mond [3] formulated a pair of symmetric dual programs involving a scalar function f (x , y), x E R n, y E R m, under the condition that f(. , y) is convex and f (x , .) is concave. Cottle [4] presented a slightly different pair of symmetric dual quadratic programs. A different pair of symmetric dual nonlinear programs was given by Mond and Weir [5], which allows the weakening of the convexity hypothesis for f (x , y). Mond and Weir [5] and Chandra et aL [6] studied symmetric dual and symmetric dual fractional programming problems for pseudo-convex/pseudo-concave functions. In [7], some equivalence results between linear programming duality and a matrix game are given. For the infinite-dimensional case, similar results have been obtained by Tijs [8] for semiinfinite programming, and Forgo [9] and Underwood [10] for continuous linear programming. And also Chandra et al. [11] studied continuous linear programs and continuous matrix game equivalence. Chandra, Craven and Mond [12] presented analogues of results from [7] for certain classes of nonlinear programming problems. Kemp and Kimura [13] gave an equivalence theorem where the matrix game is not necessarily a symmetric game. In this case, the matrix game depends on primal and dual variables. Preda [14] gave analogues of Theorem 17 of [13] for a certain class of nonlinear programming problems, where matrix games depend only on primal variables. These problems are finite-dimensional and satisfy certain generalized convexity requirements. The work was supported by Korea Research Foundation KRF-2000-015-DP0044. 0893-9659/04/$ see front matter (~) 2004 Elsevier Ltd. All rights reserved. doi: 10.1016/j.aml.2004.02.003 Typeset by .A.A48-~EX 1248 D.S. KIM AND K. A. NOH The purpose of this paper is to establish some equivalence relations between symmetric duality in quadratic programming and matrix games. We present two different zero-sum games whose Nash equilibria correspond to the solutions of the pair of symmetric quadratic dual problems. 2 . Q U A D R A T I C S Y M M E T R I C D U A L I T Y A N D M A T R I X G A M E E Q U I V A L E N C E A matr ix game is defined by a real m x n matrix A together with the Cartesian product X x Y of all m-dimensional probability vectors X and all n-dimensional probability vectors Y. A point (2, Y) in X x Y is an equilibrium point of the game A if xA9 T < 2A9 T < 2Ay T for all x C X and for all y C Y and 2A9 T = v, where v is the value of the game. A game of strategy is described by its set of rules. These rules specify clearly what each person called a player is allowed or required to do under all possible circumstances. The rules define the amount of information, if any, each player receives. If the game requires the use of chance devices, the rules describe how the chance events shall be interpreted. They also define the time the game ends, the amount each player pays or receives, and the objective of each player. We denote by 2 and 9 the optimal strategies of Players 1 and 2, respectively. The optimal strategies have the following properties. (i) If Player 1 chooses 2, then no mat ter what strategy Player 2 chooses, Player 1 can get at least v. (ii) If Player 2 chooses Y, then no mat ter what strategy Player 1 chooses, Player 1 can get at most v. (iii) If Player i were to announce in advance that he planned to play strategy 2, Player 2 could not thereby take advantage of this information and reduce Player l's payoff. Similarly, if Player 2 were to announce Y, Player 1 could not increase his payoff. Now, consider the quadratic symmetric dual programs: F(x,y) = l y r Dy + l x T c x + minimize pTx, subject to Dy + Ax >= -b, (QP) x ~ 0 , ] 1 maximize G(u, v) = ~ v T D v 9 u T C u -bT v, subject to --AT v + Cu >= -p, (QD) v ~ 0 . Here A is an m x n matrix; C, a symmetric positive semidefinite n x n matrix; D, a symmetric positive semidefinite m x m matrix; b, an m x 1 vector; y, v, m x 1 vectors; p, an n x 1 vector; x, u, n x 1 vectors. The entries of A, C, D, b, and p are constants, whereas those of x, y, u, and v are variables. The following lemma, which characterizes the optimal s trategy of a certain type of matr ix game, will be needed in the proof of Theorems 1 and 2. For the proof of this lemma, see [12]. LEMMA 1. For a matr /x game described by a skew symmetric matrix B, the value of the game is zero, and 9 is an optimal strategy for Player II (or I) if and only if B9 < O. C a se I. A S y m m e t r i c M a t r i x G a m e We define the (n + m + 1) x (n + m + 1) skew symmetric matr ix B(x, y) and s tudy the relations between the primal-dual pair (QP) and (QD) and the matr ix game B(x, y), where = o ( D T y + b ) x T C + p T y-rD+b T 0 Symmetric Duality 1249 THEOREM 1. Let (2, f]) be a feasible solution to both (QP) and (QD), such that F(~, f]) = G(~, f]). Let z* = 1/(1 + y~j~ j + ~-'~if]i), x* = z*~2, and y* = z'f]. Then (x*,y*,z*) soIves the matr ix game B(ff:, f]). PROOF. we have	cartesian closed category;concave function;convex set;duality (optimization);emoticon;fenchel's duality theorem;fractional programming;gary kimura;information;linear programming;nash equilibrium;nonlinear programming;nonlinear system;quadratic programming;quantum dot;requirement;symmetric multiprocessing;the matrix;turing completeness	Do Sang Kim;K. A. Noh	2004	Appl. Math. Lett.	10.1016/j.aml.2004.02.003	mathematical optimization;combinatorics;duality;mathematics;normal-form game;mathematical economics;quadratic programming;symmetric game;nash equilibrium;symmetric equilibrium	Theory	71.31351911061657	21.747897259442215	159266
7f8fdd1df97b9858fa6032ead2a66759f6b464e4	two new dai-liao-type conjugate gradient methods for unconstrained optimization problems	unconstrained optimization;dai–liao-type methods;strong convergence;global convergence;strong wolfe line search;65k05;90c26;90c30	In this paper, we present two new Dai–Liao-type conjugate gradient methods for unconstrained optimization problems. Their convergence under the strong Wolfe line search conditions is analysed for uniformly convex objective functions and general objective functions, respectively. Numerical experiments show that our methods can outperform some existing Dai–Liao-type methods by using Dolan andMoré’s performance profile.	conjugate gradient method;convex function;experiment;line search;mathematical optimization;numerical method;uniformly convex space	Yutao Zheng;Bing Zheng	2017	J. Optimization Theory and Applications	10.1007/s10957-017-1140-1		ML	74.63428231874319	23.393249063647193	159271
bfe8d39db7a0d5fa36af3e0901daddd96788a2cf	maximum entropy algorithm with inexact upper entropy bound based on fup basis functions with compact support	lagrangien;optimisation;shannon entropy;polinomio ortogonal;numerical method;funcion densidad probabilidad;probability density function;orthogonal polynomial;calculation;satisfiability;water engineering;methode calcul;higher order;algorithme;upper bound;optimization problem;fonction densite probabilite;technique calcul;metodo numerico;loi beta;ley beta;beta distribution;entropie;calculation methods;statistical inference;non linearite;compact support;no linealidad;algorithms;nonlinearity;lagrangiano;optimization;entropy;vattenteknik;polynome orthogonal;fup basis functions;classic moment problem;maximum entropy algorithm;borne superieure;moment problem;constrained optimization problem;lagrangian;methode numerique;maximum entropy;cota superior	The maximum entropy (MaxEnt) principle is a versatile tool for statistical inference of the probability density function (pdf) from its moments as a least-biased estimation among all other possible pdf’s. It maximizes Shannon entropy, satisfying the moment constraints. Thus, the MaxEnt algorithm transforms the original constrained optimization problem to the unconstrained dual optimization problem using Lagrangian multipliers. The Classic Moment Problem (CMP) uses algebraic power moments, causing typical conventional numerical methods to fail for higher-order moments ðm > 5—10Þ due to different sensitivities of Lagrangian multipliers and unbalanced nonlinearities. Classic MaxEnt algorithms overcome these difficulties by using orthogonal polynomials, which enable roughly the same sensitivity for all Lagrangian multipliers. In this paper, we employ an idea based on different principles, using Fupn basis functions with compact support, which can exactly describe algebraic polynomials, but only if the Fup order-n is greater than or equal to the polynomial’s order. Our algorithm solves the CMP with respect to the moments of only low order Fup2 basis functions, finding a Fup2 optimal pdf with better balanced Lagrangian multipliers. The algorithm is numerically very efficient due to localized properties of Fup2 basis functions implying a weaker dependence between Lagrangian multipliers and faster convergence. Only consequences are an iterative scheme of the algorithm where power moments are a sum of Fup2 and residual moments and an inexact entropy upper bound. However, due to small residual moments, the algorithm converges very quickly as demonstrated on two continuous pdf examples – the beta distribution and a bi-modal pdf, and two discontinuous pdf examples – the step and double Dirac pdf. Finally, these pdf examples present that Fup MaxEnt algorithm yields smaller entropy value than classic MaxEnt algorithm, but differences are very small for all practical engineering purposes. 2009 Elsevier Inc. All rights reserved.		Hrvoje Gotovac;Blaz Gotovac	2009	J. Comput. Physics	10.1016/j.jcp.2009.09.011	hydraulic engineering;entropy;mathematical optimization;combinatorics;nonlinear system;calculus;mathematics;algorithm;quantum mechanics;statistics	Theory	78.17894816403093	26.126106263290925	159403
409c09d9efe06f3bc9e8da5b84995d94485c2ac7	new fast algorithms for toeplitz-plus-hankel matrices	calcul matriciel;algorithme rapide;wz factorization;65f05;grupo de excelencia;matriz toeplitz;15a23;15a06;ciencias basicas y experimentales;toeplitz plus hankel matrix;zw factorization;15a09;fast algorithm;matematicas;matrice hankel;matrice toeplitz;factorisation zw;factorisation wz;factorizacion incompleta;matriz hankel;tecnologias generalidades;matrix calculus;tecnologias;hankel matrix;toeplitz matrix;algoritmo rapido;calculo de matrices	New fast algorithms for the solutions of linear systems with a Toeplitz-plus-Hankel coefficient matrix M, of both Levinson- and Schur-type, are presented that require fewer operations than previous ones. The Schur-type algorithm produces a ZW-factorization of M, and the Levinson-type algorithm produces a WZ-factorization of M-1. The new algorithms are in spirit close to the split Levinson and Schur algorithms of Delsarte and Genin.	fast fourier transform;toeplitz hash algorithm	Georg Heinig;Karla Rost	2003	SIAM J. Matrix Analysis Applications	10.1137/S0895479802410074	combinatorics;hankel matrix;linear prediction;matrix calculus;calculus;toeplitz matrix;mathematics;algebra	Theory	81.49845440683195	21.282886354865333	159430
334d6c71b6bce8dfbd376c4203004bd4464c2099	biconvex relaxation for semidefinite programming in computer vision		References [1] Wang, P., Shen, C., van den Hengel, A., A fast semidefinite approach to solving binary quadratic problems. CVPR 2013. [2] Candes, E.J., Li, X., Soltanolkotabi, M., Phase retrieval via wirtinger flow: Theory and algorithms. IEEE Transaction on Information Theory, 2015. [3] Zheng, Q., Lafferty, J., A convergent gradient descent algorithm for rank minimization and semidefinite programming from random linear measurements. NIPS 2015. [4] Netrapalli, P., Jain, P., Sanghavi, S., Phase retrieval using alternating minimization. NIPS 2013. Many important problems requires solving for low rank SDPs with PSD constraint matrices.	adobe photoshop;algorithm;biconvex optimization;cvpr;computer vision;gradient descent;information theory;linear programming relaxation;low-rank approximation;nips;p (complexity);phase retrieval;quadratic programming;semidefinite programming	Sohil Shah;Abhay Kumar Yadav;David W. Jacobs;Christoph Studer;Tom Goldstein	2016		10.1007/978-3-319-46466-4_43	mathematical optimization;combinatorics;machine learning;mathematics;algorithm	Vision	73.55118797572287	27.204878582744584	159443
3777571b0fde125518bf8776d309b25bdf274773	generalized invexity-type conditions in constrained optimization	constrained optimization;necessary and sufficient condition;shashi k mishra norma g rueda 条件优化 凸型 广义 约束优化问题 通用型 generalized invexity type conditions in constrained optimization;duality results;constrained optimization problem;type i functions	This paper defines a new class of generalized type I functions, and obtains Kuhn-Tucker necessary and sufficient conditions and duality results for constrained optimization problems in the presence of the aforesaid weaker assumptions on the objective and constraint functions involved in the problem.	constrained optimization;mathematical optimization	Shashi Kant Mishra;Norma G. Rueda	2011	J. Systems Science & Complexity	10.1007/s11424-011-8234-x	mathematical optimization;constrained optimization;combinatorics;mathematical analysis;duality;mathematics	Logic	72.09901761323812	21.362840167216895	159469
340625635a9ef1f56575175bd4669a7697e845ac	large-scale robust beamforming via $\ell _{\infty }$ -minimization		"""In this paper, linearly constrained and robust <inline-formula><tex-math notation=""""LaTeX"""">$\ell _{\infty }$ </tex-math></inline-formula>-norm beamforming techniques are proposed for non-Gaussian signals. A conventional approach for <inline-formula><tex-math notation=""""LaTeX"""">$\ell _{\infty }$</tex-math></inline-formula>-minimization needs to solve a linear programming (LP) or second-order cone programming (SOCP). However, this strategy is computationally prohibitive for “big data” because the existing algorithms for LP or SOCP, such as simplex method or interior point method, can only solve small- or medium-scale problems. In this paper, the alternating direction method of multipliers (ADMM) is devised for large-scale <inline-formula><tex-math notation=""""LaTeX"""">$\ell _{\infty }$ </tex-math></inline-formula>-beamforming problems, where the core subproblems can be formulated concisely as a linearly or second-order cone constrained least squares and the proximity operator of the <inline-formula> <tex-math notation=""""LaTeX"""">$\ell _{\infty }$</tex-math></inline-formula>-norm in each iteration. Remarkably, a linear-time complexity algorithm is devised that efficiently computes the <inline-formula><tex-math notation=""""LaTeX""""> $\ell _{\infty }$</tex-math></inline-formula>-norm proximity operator. Simulation results verify the high efficiency of the ADMM and the superiority of the <inline-formula><tex-math notation=""""LaTeX"""">$\ell _{\infty }$</tex-math> </inline-formula>-norm beamforming techniques over several representative beamformers, indicating that its performance can approach the optimal upper bound."""	augmented lagrangian method;beamforming;big data;interior point method;iteration;linear least squares (mathematics);linear programming;second-order cone programming;simplex algorithm;simulation;time complexity	Xue Jiang;Jiayi Chen;Hing Cheung So;Xingzhao Liu	2018	IEEE Transactions on Signal Processing	10.1109/TSP.2018.2841887	mathematical optimization;operator (computer programming);least squares;mathematics;linear programming;convex function;beamforming;interior point method;simplex algorithm;upper and lower bounds	EDA	74.33445751988947	27.108010649293632	159529
e0145065d7eb3d1184f43164a5d91562109c121d	parameterized solution of linear interval parametric systems	linear interval parametric systems;parameterized solution;properties and use	Let A ( p ) x = b ( p ) , p ? p denote a linear interval parametric system whose elements a ij ( p ) and b i ( p ) are given functions of the entries of the parameter vector p. In this paper, a new type of solution x ( p ) of the parametric system considered is suggested. The new solution x ( p ) is of the form x ( p ) = Lp + a , p ? p where L is a real n × m matrix (n and m are the sizes of the square matrix A and vector p, respectively) whereas a is an interval vector. It is shown that the parameterized solution can be employed for solving various constrained optimization problems related to the linear interval parametric system given. Thus, an iterative method for determining the interval hull solution of A ( p ) x = b ( p ) , p ? p for the case of linear parametric dependencies has been proposed which is based on the use of x ( p ) and a simple constraint satisfaction technique.		Lubomir V. Kolev	2014	Applied Mathematics and Computation	10.1016/j.amc.2014.08.037	mathematical optimization;combinatorics;discrete mathematics;mathematics	Theory	69.13593849445577	23.56397562347347	160051
fb82f03b512f162dd0db7c4146d3d0f6c2b362ea	uniqueness of integer solution of linear equations	absolute value equations;linear programming;integer solution uniqueness;linear program;linear equations	We consider the system of m linear equations inn integer variablesAx = d and give sufficient conditions for the uniqueness of its integer solution x ∈ {−1,1}n by reformulating the problem as a linear program. Uniqueness characterizations of ordinary linear programmin g solutions are utilized to obtain sufficient uniqueness conditions such as the intersection of the kernel of A and the dual cone of a diagonal matrix of ±1’s is the origin in Rn. This generalizes the well known condition that ker (A) = 0 for the uniqueness of a non-integer solution x of Ax = d. A zero maximum of a single linear program ensures the uniquene ss of a given integer solution of a linear equation.	dual cone and polar cone;linear equation;linear programming formulation;microsoft outlook for mac;system of linear equations;uniqueness type;whole earth 'lectronic link	Olvi L. Mangasarian;Michael C. Ferris	2010	Optimization Letters	10.1007/s11590-010-0183-0	system of linear equations;mathematical optimization;mathematical analysis;discrete mathematics;basic solution;integer programming;linear programming;branch and price;coefficient matrix;integer points in convex polyhedra;mathematics;linear equation	Crypto	69.99849854596236	21.86186731058331	160069
00ac7c52b5b57e5a160f7d4701c39185a3b8ece4	global optimization over a box via canonical dual function	computacion informatica;concave function;canonical dual function;ciencias basicas y experimentales;matematicas;global optimization;grupo a	In this paper, we study global concave optimization by the canonical dual function. A differential flow on the dual feasible space is introduced. We show that the flow reaches a global minimizer of the concave function over a box. An example is illustrated.	dual polyhedron;duality (optimization);global optimization;mathematical optimization	Jinghao Zhu;Chao Wang;David Gao	2011	J. Computational Applied Mathematics	10.1016/j.cam.2010.07.023	mathematical optimization;calculus;logarithmically concave function;mathematics;geometry;global optimization;concave function	ML	72.3595823214875	21.010480845845468	160094
36920acb218d5f553256a8bd0c6a400de955c669	nonsmooth milyutin-dubovitskii theory and clarke's tangent cone	non linear programming;multiplier rule;programmation non lineaire;principe intersection;mathematical programming;clarke tangent cone;intersection principle;non differential programming;cone tangent clarke;programmation mathematique;tangent cone;locally lipschitz function;programmation non differentiable	In optimization theory the idea of approximating nonconvex sets by convex cones which satisfy an abstract condition---the Intersection Principle---is due to Milyutin and Dubovitskii. This approach has been successfully applied to optimization problems with differentiable data. The validity of the Intersection Principle for the Clarke tangent cone, which is the apposite approximant for nonsmooth constraints, is established via an intersection theorem which relates the Clarke tangent cone of an intersection of sets to the intersection of the individual Clarke tangent cones, provided the latter are inseparable. This allows an extension of the Milyutin-Dubovitskii approach to nonsmooth optimization theory. A multiplier rule, of the type obtained by Clarke and Hiriart-Urruty, for a nonsmooth inequality constrained problem, is thereby encompassed within Milyutin-Dubovitskii theory.	cone	G. G. Watkins	1986	Math. Oper. Res.	10.1287/moor.11.1.70	mathematical optimization;tangent cone;tangent vector;mathematical analysis;topology;nonlinear programming;mathematics	Theory	72.11774319652893	21.263922241131198	160394
23bca1190867abe48d6f51ee2a9bbb3f5a1000e5	an inexact successive quadratic approximation method for l-1 regularized optimization	inexact proximal newton;90c55;sparse optimization;49m15;orthant based quasi newton	We study a Newton-like method for the minimization of an objective function φ that is the sum of a smooth function and an 1 regularization term. This method, which is sometimes referred to in the literature as a proximal Newton method, computes a step by minimizing a piecewise quadratic model qk of the objective function φ. In order to make this approach efficient in practice, it is imperative to perform this inner minimization inexactly. In this paper, we give inexactness conditions that guarantee global convergence and that can be used to control the local rate of convergence of the iteration. Our inexactness conditions are based on a semi-smooth function that represents a (continuous) measure of the optimality conditions of the problem, and that embodies the soft-thresholding iteration. We give careful consideration to the algorithm employed for the inner minimization, and report numerical results on two test sets originating in machine learning. To Jong-Shi Pang for his important contributions to optimization and his constant support. Richard H. Byrd was supported by National Science Foundation Grant DMS-1216554 and Department of Energy Grant DE-SC0001774. Jorge Nocedal was supported by National Science Foundation Grant DMS-0810213, and by ONR Grant N00014-14-1-0313 P00002. Figen Oztoprak was supported by US Department of Energy Grant DE-FG02-87ER25047 and by Scientific and Technological Research Council of Turkey Grant Number 113M500. Part of this work was completed while the author was at Istanbul Technical University. B Jorge Nocedal nocedal@eecs.northwestern.edu 1 Department of Computer Science, University of Colorado, Boulder, CO, USA 2 Department of Industrial Engineering and Management Sciences, Northwestern University, Evanston, IL, USA 3 Department of Industrial Engineering, Istanbul Bilgi University, Istanbul, Turkey	approximation algorithm;computer science;imperative programming;industrial engineering;iteration;jorge urrutia galicia;local convergence;loss function;machine learning;management science;mathematical optimization;newton;newton's method;newton-x;numerical analysis;optimization problem;quadratic equation;rate of convergence;semiconductor industry;soft computing;thresholding (image processing)	Richard H. Byrd;Jorge Nocedal;Figen Öztoprak	2016	Math. Program.	10.1007/s10107-015-0941-y	mathematical optimization;mathematical analysis;calculus;mathematics	ML	71.81403285353514	26.373628913241816	160401
f8a30468aed41b4dd2114d9f8336322b393340d5	generalized derivatives of dynamic systems with a linear program embedded	computacion informatica;grupo de excelencia;nonsmooth dynamic systems;ciencias basicas y experimentales;linear programming;generalized derivatives;lexicographic derivative;nonsmooth optimization	Dynamic systems with a linear program (LP) embedded can be found in control and optimization of bioreactor models based on dynamic flux balance analysis (DFBA). Derivatives of the dynamic states with respect to a parameter vector are essential for open and closed-loop dynamic optimization and parameter estimation of such systems. These derivatives, given by a forward sensitivity system, may not exist because the optimal value of a linear program as a function of the right-hand side of the constraints is not continuously differentiable. Therefore, nonsmooth analysis must be applied which provides optimality conditions in terms of subgradients, for convex functions, or Clarke’s generalized gradient, for nonconvex functions. This work presents an approach to compute the necessary information for nonsmooth optimization, i.e., an element of the generalized gradient. Moreover, a numerical implementation of the results is introduced. The approach is illustrated through a large-scale dynamic flux balance analysis example. © 2015 Elsevier Ltd. All rights reserved.	convex function;dynamic programming;dynamical system;edmund m. clarke;embedded system;estimation theory;flux balance analysis;gradient;linear programming;mathematical optimization;numerical analysis;optimization problem;subderivative	Kai Höffner;Kamil A. Khan;Paul I. Barton	2016	Automatica	10.1016/j.automatica.2015.10.026	mathematical optimization;linear programming;calculus;mathematics;mathematical economics	AI	72.69949765378587	20.32909509289148	160426
306f0913a591ee41bf55a751bf06b010f04f95f0	non-negative regularization for systems of linear algebraic equations			algebraic equation	Victor S. Antyufeev	2011	Monte Carlo Meth. and Appl.	10.1515/mcma.2011.016	system of linear equations;regularization perspectives on support vector machines;regularization;singular point of an algebraic variety;differential algebraic equation;coefficient matrix;equation;dimension of an algebraic variety;real algebraic geometry;function field of an algebraic variety;algebraic function;differential algebraic geometry;algebraic analysis	AI	79.46104007038767	19.69342630762866	160569
c1fafd5309487c6ea51183ae52a70f96f0d4d9f7	computing the stackelberg/nash equilibria using the extraproximal method: convergence analysis and implementation details for markov chains games		In this paper we present the extraproximal method for computing the Stackelberg/Nash equilibria in a class of ergodic controlled finite Markov chains games. We exemplify the original game formulation in terms of coupled nonlinear programming problems implementing the Lagrange principle. In addition, Tikhonov’s regularization method is employed to ensure the convergence of the cost-functions to a Stackelberg/Nash equilibrium point. Then, we transform the problem into a system of equations in the proximal format. We present a two-step iterated procedure for solving the extraproximal method: (a) the first step (the extra-proximal step) consists of a “prediction” which calculates the preliminary position approximation to the equilibrium point, and (b) the second step is designed to find a “basic adjustment” of the previous prediction. The procedure is called the “extraproximal method” because of the use of an extrapolation. Each equation in this system is an optimization problem for which the necessary and efficient condition for a minimum is solved using a quadratic programming method. This solution approach provides a drastically quicker rate of convergence to the equilibrium point. We present the analysis of the convergence as well the rate of convergence of the method, which is one of the main results of this paper. Additionally, the extraproximal method is developed in terms of Markov chains for Stackelberg games. Our goal is to analyze completely a three-player Stackelberg game consisting of a leader and two followers. We provide all the details needed to implement the extraproximal method in an efficient and numerically stable way. For instance, a numerical technique is presented for computing the first step parameter (λ) of the extraproximal method. The usefulness of the approach is successfully demonstrated by a numerical example related to a pricing oligopoly model for airlines companies.	approximation;emoticon;ergodicity;exemplification;extrapolation;iteration;markov chain;mathematical optimization;nash equilibrium;nonlinear programming;nonlinear system;numerical analysis;numerical stability;optimization problem;quadratic programming;rate of convergence	Kristal K. Trejo;Julio B. Clempner;Alexander S. Poznyak	2015	Applied Mathematics and Computer Science		markov chain;simulation;computer science;mathematics;mathematical economics;implementation;statistics	ML	76.31207196410809	26.57723945699881	160613
3eecb1bb5784cbfb30dbf841211ce3bfd848b7d1	on the differential properties of the support function of the ∈-subdifferential of a convex function	second order;duality theory;convex function;convex analysis;support function	We study differential properties of the support function of the∈-subdifferential of a convex function; applications in algorithmics are also given.	convex function	Alfred Auslender	1982	Math. Program.	10.1007/BF01585110	perturbation function;convex function;convex analysis;subderivative;support function;mathematical optimization;conic optimization;mathematical analysis;convex optimization;pseudoconvex function;duality;topology;closed convex function;convex combination;danskin's theorem;linear matrix inequality;convex conjugate;quasiconvex function;convexity in economics;mathematics;convex set;logarithmically convex function;effective domain;second-order logic;proper convex function;concave function	ML	71.9528523206254	20.74563863955119	160737
f1b017fe83c25918d8ba2735d48d9b3c4faedf9b	an improved initialization for low-rank matrix completion based on rank-l updates		Given a data matrix with partially observed entries, the low-rank matrix completion problem is one of finding a matrix with the lowest rank that perfectly fits the given observations. While there exist convex relaxations for the low-rank completion problem, the underlying problem is inherently nonconvex, and most algorithms (alternating projection, Riemannian optimization, etc.) heavily depend on the initialization. This paper proposes an improved initialization that relies on successive rank-l updates. Further, the paper proposes theoretical guarantees under which the proposed initialization is closer to the unknown optimal solution than the all zeros initialization in the Frobenius norm. To cope with the problem of local minima, the paper introduces and uses random norms to change the position of the local minima while preserving the global one. Using a Riemannian optimization routine, simulation results reveal that the proposed solution succeeds in completing Gaussian partially observed matrices with a random set of revealed entries close to the information-theoretical limits, thereby significantly improving on prior methods.	algorithm;existential quantification;fits;low-rank approximation;mathematical optimization;matrix multiplication;maxima and minima;simulation	Ahmed Douik;Babak Hassibi	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8461826	matrix completion;mathematical optimization;data matrix;matrix (mathematics);initialization;matrix norm;computer science;low-rank approximation;matrix decomposition;maxima and minima	ML	75.03513212098852	25.647456439202497	160982
f4a68ae80312fcd41e74b991480061be21ed7dc8	a conical algorithm for globally minimizing a concave function over a closed convex set	nonlinear programming;concave minimization;convex set	In this paper we are concerned with the problem of finding the global minimum of a concave function over a closed, convex, possibly unbounded set in Rn. The intrinsic difficulty of this problem is due to the fact that a local minimum of the objective function may fail to be a global one—which makes the conventional methods of local optimization almost useless.	algorithm;concave function;convex set	H. Tuy;T. V. Thieu;Ng Q. Thai	1985	Math. Oper. Res.	10.1287/moor.10.3.498	convex function;convex analysis;subderivative;support function;mathematical optimization;conic optimization;combinatorics;mathematical analysis;convex optimization;pseudoconvex function;closed convex function;convex combination;nonlinear programming;convex hull;absolutely convex set;convexity in economics;logarithmically concave function;mathematics;convex set;logarithmically convex function;effective domain;proper convex function;concave function	ML	72.10088304613132	22.48942876520428	161634
caec471b56a20abd4746fe08c86eb8f6bbb9472b	generalized s-lemma and strong duality in nonconvex quadratic programming	nonconvex quadratic optimization;journal article;90c20;global optimization;90c22;generalized s lemma;topological minimax theorem;strong duality;90c10	On the basis of a new topological minimax theorem, a simple and unified approach is developed to Lagrange duality in nonconvex quadratic programming. Diverse generalizations as well as equivalent forms of the S-Lemma, providing a thorough study of duality for single constrained quadratic optimization, are derived along with new strong duality conditions for multiple constrained quadratic optimization. The results allow many quadratic programs to be solved by solving one or just a few SDP’s (semidefinite programs) of about the same size, rather than solving a sequence, often infinite, of SDP’s or linear programs of a very large size as in most existing methods. Copyright Springer Science+Business Media, LLC. 2013	quadratic programming;strong duality	Hoang Tuy;Hoang Duong Tuan	2013	J. Global Optimization	10.1007/s10898-012-9917-0	mathematical optimization;combinatorics;discrete mathematics;duality;duality gap;weak duality;quadratically constrained quadratic program;mathematics;strong duality;quadratic programming;global optimization	NLP	71.87380169243559	22.972439480872158	161667
c963a70102b7e0632715da2b7c8a2d6b1e83bb3b	on the convexifiability of pseudoconvex c2-functions	generalized convexity	"""In mathematical programming as well as in mathematical economics there has been much interest in functions that are convex transformable (see [1-4], [6], [9], [11-13], [15]). Such functions are quasiconvex and often pseudoconvex. In this paper we characterize pseudoconvex and strictly pseudoconvex functions that are convex transformable by some increasing function. We assume that functions to be characterized are twice continuously differentiable. Many of our results are derived using recent second order characterizations of pseudoconvex functions by Avriel and Schaible [2]. Let f E C2(S) be any twice continuously differentiable function on an open convex set S in R"""". We want to characterize such functions f that are convex transformable in the following sense: for any compact convex subset A C S, there exists a twice continuously differentiable function Ga, satisfying G~ > 0 such that Ga(f(x)) is defined and convex on A. In the special case of a quadratic function f this implies the existence of a convexifying function G such that G(f(x)) is convex on the whole of the open set S (Schaible [12], [14]). However , nonquadratic functions may well be convexifiable on every compact subset a of S but not so on S (see [2], Example 4). In this paper we shall not consider the smaller class of functions that are convexifiable on open convex sets. Criteria for these functions were derived in [2, Proposit ions 6-8] and [4]. Instead we are characterizing"""	convex function;convex optimization;convex set;mathematical optimization;pseudoconvex function;quadratic function;quasiconvex function	Siegfried Schaible;Israel Zang	1980	Math. Program.	10.1007/BF01581649	mathematical optimization;mathematical analysis;pseudoconvex function;topology;mathematics	ML	70.99011427865112	20.600077589267304	161832
932893257fa58c85a602870dade1b6c9a1f9b137	some properties of the lower bound of optimal values in interval convex quadratic programming		One of the fundamental problems in interval quadratic programming is to compute the range of optimal values. In this paper, we derive some results on the lower bound of interval convex quadratic programming. We first develop complementary slackness conditions of a quadratic program and its Dorn dual. Then, some interesting and useful characteristics of the lower bound of interval quadratic programming are established based on these conditions. Finally, illustrative examples and remarks are given to get an insight into the problem discussed.	quadratic programming	Wei Li;Jianghong Jin;Mengxue Xia;Haohao Li;Qi Luo	2017	Optimization Letters	10.1007/s11590-016-1097-2	mathematical optimization;combinatorics;discrete mathematics;quadratic field;quadratic residuosity problem;second-order cone programming;quadratic function;binary quadratic form;quadratically constrained quadratic program;mathematics;sequential quadratic programming;quadratic programming	Theory	70.27659436853607	21.56719884970596	162024
ddd5bc4865a2ca6256bd5e91da66a1ac49a27a35	corrector-predictor methods for monotone linear complementarity problems in a wide neighborhood of the central path	superlinear convergence;systeme degenere;arithmetic operation;line search;algoritmo busqueda;metodo predictor corrector;algorithme recherche;operation arithmetique;search algorithm;predictor corrector method;methode predicteur correcteur;methode point interieur;operacion aritmetica;superlinear convergence 49m15;convergence numerique;degenerate system;higher order;equation polynomiale;polynomial equation;numerical convergence;sistema degenerado;65k05;metodo punto interior;ecuacion polinomial;degeneration;mathematical programming;probleme complementarite lineaire;90c33;probleme complementarite;problema complementariedad;complementarity problem;large neighborhood;central path;problema complementariedad lineal;linear complementarity problem;interior point method;programmation mathematique;convergencia numerica;programacion matematica;interior point algorithm	Two corrector–predictor interior point algorithms are proposed for solving monotone linear complementarity problems. The algorithms produce a sequence of iterates in the N−∞ neighborhood of the central path. The first algorithm uses line search schemes requiring the solution of higher order polynomial equations in one variable, while the line search procedures of the second algorithm can be implemented in O(m n1+α) arithmetic operations, where n is the dimension of the problems, α ∈ (0, 1] is a constant, and m is the maximum order of the predictor and the corrector. If m = Ω(log n) then both algorithms have O( √ nL) iteration complexity. They are superlinearly convergent even for degenerate problems.	algorithm;centrality;complementarity theory;iteration;kerrison predictor;line search;linear complementarity problem;numerical analysis;polynomial;rate of convergence;the matrix;monotone	Florian A. Potra	2008	Math. Program.	10.1007/s10107-006-0068-2	mathematical optimization;mathematical analysis;predictor–corrector method;higher-order logic;interior point method;calculus;mathematics;linear complementarity problem;line search;search algorithm	Theory	75.75502547215206	22.254744079436207	162181
664cb6926da1ea5515c91c7d95f4b737ab192fa3	approximation of common fixed points of an infinite family of nonexpansive mappings in banach spaces	strong convergence;banach space;iterative algorithm;fixed point;common fixed point;variational inequality;nonexpansive mapping;contraction	"""Let C be a closed convex subset of a real uniformly smooth and strictly convex Banach space E. Consider the following iterative algorithm given by {x""""0=x@?Carbitrarily chosen ,y""""n=@b""""nx""""n+(1-@b""""n)W""""nx""""n,x""""n""""+""""1=@a""""nf(x""""n)+(1-@a""""n)y""""n,@?n>=0, where f is a contraction on C and W""""n is a mapping generated by an infinite family of nonexpansive mappings {T""""i}""""i""""=""""1^~. Assume that the set of common fixed points of this infinite family of nonexpansive mappings is not empty. In this paper, we prove that the sequence {x""""n} generated by the above iterative algorithm converges strongly to a common fixed point of {T""""i}""""i""""=""""1^~, which solves some variational inequality. Our results improve and extend the results announced by many others."""	approximation	Yeol Je Cho;Shin Min Kang;Xiaolong Qin	2008	Computers & Mathematics with Applications	10.1016/j.camwa.2008.03.035	mathematical optimization;function composition;mathematical analysis;discrete mathematics;variational inequality;generic property;contraction;mathematics;fixed point;iterative method;banach space;algebra	Theory	73.78903932113981	19.011187889324166	162210
00715f02ba59f772f469800e25bf07e834b631a2	on the complexity analysis of randomized block-coordinate descent methods	90c06;convergence rate;randomized block coordinate descent;accelerated coordinate descent;65k05;composite minimization;90c25;iteration complexity;90c05	In this paper we analyze the randomized block-coordinate descent (RBCD) methods proposed in [11, 15] for minimizing the sum of a smooth convex function and a blockseparable convex function, and derive improved bounds on their convergence rates. In particular, we extend Nesterov’s technique developed in [11] for analyzing the RBCD method for minimizing a smooth convex function over a block-separable closed convex set to the aforementioned more general problem and obtain a sharper expected-value type of convergence rate than the one implied in [15]. As a result, we also obtain a better high-probability type of iteration complexity. In addition, for unconstrained smooth convex minimization, we develop a new technique called randomized estimate sequence to analyze the accelerated RBCD method proposed by Nesterov [11] and establish a sharper expected-value type of convergence rate than the one given in [11].	analysis of algorithms;convex function;convex optimization;convex set;coordinate descent;iteration;randomized algorithm;rate of convergence	Zhaosong Lu;Lin Xiao	2015	Math. Program.	10.1007/s10107-014-0800-2	mathematical optimization;combinatorics;discrete mathematics;random coordinate descent;mathematics;rate of convergence	ML	73.35271266061848	25.00807893106991	162486
122f9e006af82fa4c4813dda6b5196f6ec7611ee	computing singular value decompositions of parameterized matrices with total nonpositivity to high relative accuracy		In the last years, much effort has been devoted to high relative accuracy algorithms for the singular value problem. However, such algorithms have been constructed only for a few classes of matrices with certain structure or properties. In this paper, we study a different class of matrices—parameterized matrices with total nonpositivity. We develop a new algorithm to compute singular value decompositions of such matrices to high relative accuracy. Our numerical results confirm the high relative accuracy of our algorithm.		Rong Huang;Delin Chu	2017	J. Sci. Comput.	10.1007/s10915-016-0315-5	mathematical optimization;combinatorics;discrete mathematics;mathematics	Theory	75.33483470330094	28.867295706324967	162509
48f6402dfd0ceab3c2b58b0221134de7f3f8ec82	bounding the set of solutions of a perturbed global optimization problem	global solution;objective function;error analysis;sensitivity analysis;global optimization;interval analysis	Consider a global optimization problem in which the objective function and/or the constraints are expressed in terms of parameters. Suppose we wish to know the set of global solutions as the parameters vary over given intervals. In this paper we discuss procedures using interval analysis for computing guaranteed bounds on the solution set. This provides a means for doing a sensitivity analysis or simply bounding the effect of errors in data.		Eldon R. Hansen	1991	J. Global Optimization	10.1007/BF00130831	mathematical optimization;combinatorics;discrete mathematics;mathematics;sensitivity analysis;global optimization	ML	68.81806228032485	23.36422580057572	162575
6ed02d1b7c4a49e892f6c7e59077b0b575440bdc	a note on the condition numbers of convex sets	polyhedral sets;condition numbers of convex sets;convex programming;26b25;90c25	In this note, we give a counterexample to show that the characterization formula for the condition numbers of a convex set $$C$$ C of $$\mathbb R^n$$ R n given in Coulibaly and Crouzeix (Math Program 116:79---113, 2009) is incorrect.	condition number;convex set	Wen Song;Xiaomei Xu;Jen-Chih Yao	2015	Math. Program.	10.1007/s10107-014-0758-0	convex geometry;convex analysis;subderivative;support function;mathematical optimization;combinatorics;discrete mathematics;convex optimization;convex polytope;convex combination;orthogonal convex hull;convex body;linear matrix inequality;convex hull;absolutely convex set;convexity in economics;mathematics;convex set;logarithmically convex function;proper convex function	ML	70.79853914577676	20.936265364045642	162742
2dc4d1a420c691fb182e839f31c4d8a46b94e55f	some inexact hybrid proximal augmented lagrangian algorithms	second order;convex programming;interdisciplinar;karush kuhn tucker;computer experiment;ciencias basicas y experimentales;matematicas;grupo a;augmented lagrangian	In this work, Solodov–Svaiter's hybrid projection-proximal and extragradient-proximal methods [16,17] are used to derive two algorithms to find a Karush–Kuhn–Tucker pair of a convex programming problem. These algorithms are variations of the proximal augmented Lagrangian. As a main feature, both algorithms allow for a fixed relative accuracy of the solution of the unconstrained subproblems. We also show that the convergence is Q-linear under strong second order assumptions. Preliminary computational experiments are also presented.	algorithm;augmented lagrangian method;computation;convex optimization;experiment;karush–kuhn–tucker conditions;rate of convergence;tucker decomposition	Carlos Humes;Paulo J. S. Silva;Benar Fux Svaiter	2004	Numerical Algorithms	10.1023/B:NUMA.0000021768.30330.4b	mathematical optimization;convex optimization;computer experiment;augmented lagrangian method;calculus;mathematics;geometry;karush–kuhn–tucker conditions;second-order logic	ML	74.34089239785607	23.467062530011262	162745
821067cab223e1de8e75017b252bfef95efd63b3	a modification of the αbb method for box-constrained optimization and an application to inverse kinematics	nonconvex programming;alpha hbox bb α bb method;non convex programming;optimal solution set;global optimization;bb method;robotic design	For many practical applications it is important to determine not only a numerical approximation of one but a representation of the whole set of globally optimal solutions of a non-convex optimization problem. Then one element of this representation may be chosen based on additional information which cannot be formulated as a mathematical function or within a hierarchical problem formulation. We present such an application in the field of robotic design. This application problem can be modeled as a smooth box-constrained optimization problem. We extend the well-known \(\alpha \hbox {BB}\) method such that it can be used to find an approximation of the set of globally optimal solutions with a predefined quality. We illustrate the properties and give a proof for the finiteness and correctness of our modified \(\alpha \hbox {BB}\) method.	constrained optimization;inverse kinematics;mathematical optimization	Gabriele Eichfelder;Tobias Gerlach;Susanne Sumi	2016	EURO J. Computational Optimization	10.1007/s13675-015-0056-5	mathematical optimization;calculus;mathematics;algorithm	Robotics	73.56489212904341	23.034952887929233	163067
078969767f4a8eec1021bfbad8cc37f28b73df5b	book reviews: boris s. mordukhovich, variational analysis and generalized differentiation i: basic theory series: grundlehren der mathematischen wissenschaften, vol 330, volume package: variational analysis and generalized differentiation, 2006, xxii, 582 pp., hardcover isbn: 3-540-25437-4 and boris	boris s. mordukhovich;variational analysis;generalized differentiation ii;generalized differentiation;grundlehren der mathematischen wissenschaften	Optimization theory is perhaps as old as our civilization. In addition, the area of optimization has become more interdisciplinary with a diverse spectrum of problems needed to be analyzed and solved. New theories, mathematical and algorithmic tools have been developed during the last decades. Variational methods has been developed to study important classes of optimization and equilibrium problems. Progress in variational analysis has permitted us to handle problems whose equilibrium constraints are not obtained by the minimization of a functional. This two volume comprehensive monograph reflects the abiding and enthusiastic interest of the author in variational analysis. It covers the major theoretical issues of variational analysis and generalized differentiation in both finite-dimensional and infinite-dimensional spaces with a full calculus of all the constructions and properties involved.	algorithm;boris mordukhovich;international standard book number;mathematical optimization;theory;variational analysis;variational principle	Panos M. Pardalos	2006	J. Global Optimization	10.1007/s10898-006-9015-2	calculus;mathematics;mathematical physics;mathematical economics	Theory	70.82898505716035	21.962126069886025	163182
0d8d3da5a9ad84da6e6c4c7452ad95a63a3804ed	an infeasible full-nt step interior point algorithm for cqsco	convex quadratic symmetric cone optimization;interior-point method;infeasible method;euclidean jordan algebra;polynomial complexity;90c51	In this paper, we propose a full Nesterov-Todd (NT) step infeasible interior-point algorithm for convex quadratic symmetric cone optimization based on Euclidean Jordan algebra. The algorithm uses only one feasibility step in each main iteration. The complexity result coincides with the best-known iteration bound for infeasible interior-point methods.	algorithm;interior point method;iteration;mathematical optimization;numerical analysis	Behrouz Kheirfam	2016	Numerical Algorithms	10.1007/s11075-016-0140-9	mathematical optimization;combinatorics;mathematical analysis;mathematics	ML	73.79091084840731	23.451607982519295	163329
4306d19affc4b900ef7c31ba8d1e3273000d015d	algorithms for the construction of high-order kronrod rule extensions with application to sparse-grid integration	kronrod rule;patterson extension;nested quadrature;smolyak construction;genz-keister rule;sparse quadrature;high-dimensional quadrature;orthogonal polynomial;stieltjes polynomial	Gauss quadrature points are not nested so search for quadrature rules with nested points and similar efficiency are important. A well-studied source of candidates are the Kronrod-Patterson extensions. Under suitable conditions, it is possible to build towers of nested rules. We investigate this topic further and give a detailed description of the algorithms used for constructing such iterative extensions. Our new implementation combines several important ideas spread out in theoretical research papers. We apply the resulting algorithms to the classical orthogonal polynomials and build sparse high-dimensional quadrature rules for each class.	algorithm;gaussian quadrature;gauss–kronrod quadrature formula;iterative method;numerical integration;polynomial;sparse grid;sparse matrix	Raoul Bourquin	2017	Numerical Algorithms	10.1007/s11075-017-0273-5	mathematical optimization;combinatorics;mathematical analysis;theoretical computer science;calculus;gauss–kronrod quadrature formula;mathematics;algorithm	ML	82.67000637746442	22.163761435131885	163338
caafa0b0a582e6e84531bea8de85ca430650b781	a second-order cone cutting surface method: complexity and application	cutting plane;semidefinite programming;analytic center;semidefinite inequality;second order cone;cutting plane techniques;software package;linear quadratic;semidefinite relaxation;semidefinite program	We present an analytic center cutting surface algorithm that uses mixed linear and multiple second-order cone cuts. Theoretical issues and applications of this technique are discussed. From the theoretical viewpoint, we derive two complexity results. We show that an approximate analytic center can be recovered after simultaneously adding p second-order cone ∗This work has been completed with the support of the partial research grant from the College of Business Administration, California State University San Marcos, and the University Professional Development Grant. †This material is based upon work supported by the National Science Foundation under Grant No. 0317323. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.	approximation algorithm;central processing unit;coefficient;computation;feasible region;interior point method;linear programming relaxation;newton;numerical analysis;polynomial;procedural generation;semidefinite programming	Mohammad R. Oskoorouchi;John E. Mitchell	2009	Comp. Opt. and Appl.	10.1007/s10589-007-9141-x	mathematical optimization;combinatorics;discrete mathematics;second-order cone programming;mathematics;semidefinite embedding;semidefinite programming;cutting-plane method	Vision	71.57449637395304	26.618404003799473	163422
5789b30e2ab61b7234075fc870ec3c99589a1add	new look-ahead lanczos-type algorithms for linear systems	linear algebra;iterative method;systeme equation;methode sous espace krylov;krylov subspace method;orthogonal polynomial;look ahead;methode horner;ecuacion lineal;linear system;metodo lanczos;metodo iterativo;algorithme;algorithm;sistema ecuacion;lanczos method;methode iterative;algebre lineaire;equation system;methode lanczos;algebra lineal;recurrence relation;polynome orthogonal;linear equation;equation lineaire;horner method;algoritmo	A breakdown (due to a division by zero) can arise in the algorithms for implementing Lanczos' method because of the non{existence of some formal orthogonal polynomials or because the recurrence relationship used is not appropriate. Such a breakdown can be avoided by jumping over the polynomials involved. This strategy was already used in some algorithms such as the MRZ and its variants. In this paper, we propose new implementations of the recurrence relations of these algorithms which only need the storage of a xed number of vectors, independent of the length of the jump. These new algorithms are based on Horner's rule and on a diierent way for computing the coeecients of the recurrence relationships. Moreover, these new algorithms seem to be more stable than the old ones and they provide better numerical results. Numerical examples and comparisons with other algorithms will be given. 1 Lanczos type algorithms and Orthogonal polynomials Let us consider a system of linear equations in C n. Ax = b where A is a nonsingular matrix. Lanczos' method 27, 28] for solving this system consists of constructing a sequence of vectors (x k) as follows choose two arbitrary nonzero vectors x 0 and y in C n set r 0 = b ? Ax 0 determine x k such that where A is the conjugate transpose of A.	division by zero;horner's method;lanczos algorithm;lanczos resampling;linear equation;linear system;numerical analysis;numerical method;polynomial;recurrence relation;system of linear equations	Claude Brezinski;Michela Redivo-Zaglia;Hassane Sadok	1999	Numerische Mathematik	10.1007/s002110050439	randomized algorithms as zero-sum games;combinatorics;mathematical analysis;recurrence relation;linear algebra;calculus;mathematics;iterative method;linear equation;orthogonal polynomials;linear system;algebra	Theory	81.395562209858	21.11095976270556	163426
6b0394e581056700d68e6d34da6d3fd1b4a5d2e2	asymptotical good behavior on inequalities with completely approximate k-t concept	penalty methods	We characterize a wide class of regular convex functionals that are asymptotically well behaved on a convex set given by (infinite) inequalities, namely, those restricted functions whose stationary sequences (bounded or not) are minimizing ones. After showing the equivalence with the Kuhn–Tucker type stationarity, we prove that the class of such functions remains unchanged when the Kuhn–Tucker system is completely relaxed. This allows us to proceed for enlarging the scope of convergence of certain penalty (exterior as well as interior) methods including a new exterior penalization for infinite inequalities. © 2011 Elsevier B.V. All rights reserved.	approximation algorithm;asymptote;convex function;convex set;penalty method;stationary process;tucker decomposition;turing completeness	M. El Maghri;B. Radi	2012	Oper. Res. Lett.	10.1016/j.orl.2011.12.005	mathematical optimization;mathematical analysis;discrete mathematics;mathematics	Theory	70.72038097890318	18.812366403107568	163439
a3018601318b2e00fac653f2513174610b9b69fa	the waveguide eigenvalue problem and the tensor infinite arnoldi method		The function κ(x, z) is piecewise constant and is assumed to satisfy: κ(x, z) = κ− when x ≤ x−, κ(x, z) = κ+ when x ≥ x+ and κ(x, z) = κ(x, z + 1). This problem can be rephrased as an equivalent problem on a finite domain by means of a Dirichlet–to–Neumann map. A particular type of finite-element discretization of the finite-domain problem leads to the following nonlinear eigenvalue problem, which consists of finding pairs (γ, v) ∈ C× (C \ {0}) such that ( Q(γ) C1(γ) C 2 RΛ(γ)R −1 )	algorithm;arnoldi iteration;benchmark (computing);computational complexity theory;discretization;iterative method;noise-equivalent power;nonlinear system	Elias Jarlebring;Giampaolo Mele;Olof Runborg	2017	SIAM J. Scientific Computing	10.1137/15M1044667	divide-and-conquer eigenvalue algorithm;mathematical optimization;combinatorics;mathematical analysis;inverse iteration;mathematics;arnoldi iteration	ML	80.35497673060246	19.363135901428755	163469
630654c7b7e34e0de00cc70dd498895fa3bc6c2d	a convex dual approach to the computation of nmr complex spectra	optimisation;entropia;metodo entropia maxima;optimizacion;key words entropy optimization;resonancia magnetica nuclear;convex duality;spectral data;duality;nuclear magnetic resonance;problema inverso;analisis matematico;mathematical analysis;fonction objectif;objective function;dualite;caracteristica espectral;inverse problem;duality theory;convex function;entropie;analyse spectrale;funcion objetivo;resonance magnetique nucleaire;analisis espectral;dualidad;optimization;entropy;caracteristique spectrale;spectral analysis;entropy method;methode entropie maximum;analyse mathematique;method of maximum entropy;fonction convexe;probleme inverse;funcion convexa	The particular entropy method proposed by Hoch et al 7] for the computation of NMR complex spectra allows an elegant application of the concepts of duality theory. Correspondingly, duality theory casts new light on their choice of entropy. The purpose of this paper is to present the relevant theoretical developments together with some numerical illustrations.	computation;numerical analysis;numerical method	Jonathan M. Borwein;Pierre Maréchal;David Naugler	2000	Math. Meth. of OR	10.1007/s001860050004	entropy;mathematical optimization;calculus;mathematics;geometry	NLP	73.42326982611698	20.20278285398828	163911
a81e129156323c673ca3b27a3f1492d999586a48	fast simplicial finite element algorithms using bernstein polynomials	producto tensorial;algorithme rapide;analisis numerico;46b28;stiffness matrix;algorithm complexity;methode element fini;metodo elemento finito;complejidad algoritmo;matriz rigidez;produit tensoriel;finite element method;finite element;analyse numerique;tensor product;methode matricielle;numerical analysis;bernstein basis;complexite algorithme;bernstein polynomial;fast algorithm;matrix method;algebra lineal numerica;algebre lineaire numerique;metodo matriz;numerical linear algebra;matrice rigidite;65f30;polinomio bernstein;polynome bernstein;algoritmo rapido	Fast algorithms for applying finite element mass and stiffness operators to the B-form of polynomials over d-dimensional simplices are derived. These rely on special properties of the Bernstein basis and lead to stiffness matrix algorithms with the same asymptotic complexity as tensor-product techniques in rectangular domains. First, special structure leading to fast application of mass matrices is developed. Then, by factoring stiffness matrices into products of sparse derivative matrices with mass matrices, fast algorithms are also obtained for stiffness matrices.	algorithm;bernstein polynomial;code generation (compiler);coefficient;computational complexity theory;exterior derivative;fast fourier transform;finite element method;integer factorization;mass matrix;numerical integration;phil bernstein;recursion (computer science);simplicial complex;sparse matrix;stiffness matrix;time complexity	Robert C. Kirby	2011	Numerische Mathematik	10.1007/s00211-010-0327-2	finite element method;calculus;direct stiffness method;mathematics;geometry;algorithm;algebra	Theory	81.0194315278103	20.449261461085523	163954
4967628abc274d9e619a1f502a4448f7f25efb71	a new non-interior continuation method for second-order cone programming		A new smoothing function of the well known Fischer-Burmeister function is given. Based on this new function, a non-interior continuation algorithm is proposed for solving second-order cone programming. At each iteration, the proposed algorithm solves only one system of linear equations and performs only one line search. This algorithm can start from an arbitrary point and it is Q-quadratically convergent under a mild assumption.	cone (formal languages);conic optimization;numerical continuation;second-order cone programming	Jingyong Tang;Guoping He;Liang Fang	2013	J. Num. Math.	10.1515/jnum-2013-0012	second-order cone programming	Theory	75.17105879089279	23.155954287978194	163977
50ebca3f6f3e6be109ba324404dccf67035a841a	stability of solutions to parameterized nonlinear complementarity problems	continuite lipschitz;non linear complementarity problem;derivee b;parametrization;stabilite solution;parametrizacion;problema complementariedad no lineal;probleme complementarite non lineaire;lipschitz continuity;variational analysis;differentiabilite;parametrisation;stability of solutions;local upper lischitz continuity;nonlinear complementarity problem	We consider the stability properties of solutions to parameterized nonlinear complementarity problems Find x ∈ lRn such thatx ≥ 0, F(x,u)− v ≥ 0, and(F(x,u)− v)T · x = 0 where these are vector inequalities. We characterize the local upper Lipschitz continuity of the (possibly set-valued) solution mapping which assigns solutions x to each parameter pair (v,u). We also characterize when this solution mapping is locally a single-valued Lipschitzian mapping (so solutions exist, are unique, and depend Lipschitz continuously on the parameters). These characterizations are automatically sufficient conditions for the more general (and usual) case where v = 0. Finally, we study the differentiability properties of the solution mapping in both the single-valued and set-valued cases, in particular obtaining a new characterization of B-differentiability in the single-valued case, along with a formula for the B-derivative. Though these results cover a broad range of stability properties, they are all derived from similar fundamental principles of variational analysis.	complementarity (physics);complementarity theory;mixed complementarity problem;nonlinear system;scott continuity;variational analysis	Adam B. Levy	1999	Math. Program.	10.1007/s101070050063	parametrization;mathematical optimization;mathematical analysis;lipschitz domain;calculus;mathematics;lipschitz continuity	Theory	71.84090647134137	19.720321894088148	165007
45c98e67c67956f438707bf3b75b4372bd011acc	partial orthogonal rank-one decomposition of complex symmetric tensors based on the takagi factorization		Abstract This paper is devoted to the computation of rank-one decomposition of complex symmetric tensors. Based on the Takagi factorization of complex symmetric matrices, we derive algorithm for computing the partial orthogonal rank-one decomposition of complex symmetric tensors with an order being a power of two, denoted by CSTPOROD . We consider the properties of this decomposition. We design a strategy (tensor embedding) to computing the partial orthogonal rank-one decomposition of complex symmetric tensors, whose order is not the power of two. Similar to the case of complex symmetric tensors, we consider how to compute the partial orthogonal rank-one decomposition of general complex tensors. We illustrate our algorithms via numerical examples.		Xuezhong Wang;Maolin Che;Yimin Wei	2018	J. Computational Applied Mathematics	10.1016/j.cam.2017.09.050	tensor;mathematical analysis;mathematics;power of two;symmetric tensor;embedding;factorization;invariants of tensors;symmetric matrix;ricci decomposition	Vision	79.3906042223633	21.18876998724364	165012
0707d8dcc72be545adba292d0a39b37a01308f96	convergence analysis of the focuss algorithm	convergence algorithm design and analysis optimization vectors learning systems signal processing algorithms educational institutions;mathematics computing convergence;convergence analysis;convergence;mathematics computing;focal underdetermined system solver focuss algorithm;global convergence theorem auxiliary function convergence focal underdetermined system solver focuss algorithm;auxiliary function;learning systems;vectors;focal underdetermined system solver;stability analysis;stability analysis convergence analysis focal underdetermined system solver basis selection inverse problems focuss algorithm auxiliary function;optimization;focuss algorithm;basis selection;signal processing algorithms;global convergence theorem;algorithm design and analysis;inverse problems	Focal Underdetermined System Solver (FOCUSS) is a powerful and easy to implement tool for basis selection and inverse problems. One of the fundamental problems regarding this method is its convergence, which remains unsolved until now. We investigate the convergence of the FOCUSS algorithm in this paper. We first give a rigorous derivation for the FOCUSS algorithm by exploiting the auxiliary function. Following this, we further prove its convergence by stability analysis.	cns disorder;convergence (action);dicom derivation;focal (programming language);solver;algorithm	Kan Xie;Zhaoshui He;Andrzej Cichocki	2015	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2014.2323985	algorithm design;mathematical optimization;von neumann stability analysis;auxiliary function;convergence;computer science;inverse problem;theoretical computer science;compact convergence;machine learning;mathematics	ML	77.8753074359993	23.807886212153726	165148
54f457f2316f85e673c669c8518fc39da09e2573	a duality view of spectral methods for dimensionality reduction	duality theory;nonlinear dimensionality reduction;spectral method;maximum variance unfolding;sparse matrix;dimensional reduction;local linear embedding;optimality condition;eigenvectors	We present a unified duality view of several recently emerged spectral methods for nonlinear dimensionality reduction, including Isomap, locally linear embedding, Laplacian eigenmaps, and maximum variance unfolding. We discuss the duality theory for the maximum variance unfolding problem, and show that other methods are directly related to either its primal formulation or its dual formulation, or can be interpreted from the optimality conditions. This duality framework reveals close connections between these seemingly quite different algorithms. In particular, it resolves the myth about these methods in using either the top eigenvectors of a dense matrix, or the bottom eigenvectors of a sparse matrix --- these two eigenspaces are exactly aligned at primal-dual optimality.	algorithm;arc diagram;isomap;nonlinear dimensionality reduction;nonlinear system;semidefinite embedding;sparse matrix;spectral method;unfolding (dsp implementation)	Lin Xiao;Jun Sun;Stephen P. Boyd	2006		10.1145/1143844.1143975	mathematical optimization;combinatorics;discrete mathematics;duality;duality;duality gap;sparse matrix;eigenvalues and eigenvectors;weak duality;mathematics;nonlinear dimensionality reduction;spectral method	ML	74.79329262716934	26.51510796092558	165213
a97fa761a06fa57b6f660e93b95fa26a4bdc5cfb	common fixed points of chatterjea type fuzzy mappings on closed balls		We establish some common fixed point theorems for Chatterjea fuzzy mappings on closed balls in a complete metric space. Our investigation is based on the fact that fuzzy fixed point results can be obtained simply from the fixed point theory of mappings on closed balls. In real-world problems, there are various mathematical models in which the mappings are contractive on the subsets of a space under consideration but not on the whole space itself. It seems that this technique of finding the fuzzy fixed points was ignored. Our results generalize several important results of the literature.	complete (complexity);contraction mapping;fixed point (mathematics);fixed-point theorem;mathematical model	Akbar Azam;Saqib Hussain;Muhammad Arshad	2012	Neural Computing and Applications	10.1007/s00521-012-0907-4	coincidence point;mathematical analysis;discrete mathematics;topology;mathematics;fixed-point property	Theory	70.06961033336417	18.921079106611575	165553
ddabe6e62b894c11a115d1fd203bfba205d8b0d9	boundedness of certain sets of lagrange multipliers in vector optimization	optimality conditions;constraint qualifications;90c29;49k05;mp subdifferential;nonsmooth optimization;49j52	In this paper, we establish Lagrange multiplier rules in terms of Michel–Penot subdifferential for nonsmooth vector optimization problem. A constraint qualification or regularity condition in terms of Michel–Penot subdifferential is given and under this regularity condition the boundedness of certain sets of Lagrange multipliers are discussed. © 2015 Elsevier Inc. All rights reserved.	karush–kuhn–tucker conditions;lagrange multiplier;mathematical optimization;michel hénon;optimization problem;subderivative;vector optimization	Triloki Nath;S. R. Singh	2015	Applied Mathematics and Computation	10.1016/j.amc.2015.05.112	constraint algorithm;mathematical optimization;karush–kuhn–tucker conditions	AI	72.29488154677297	21.525700077468546	165573
35d9e4efebc8900288532c23838ec2bda4b5db0c	a primer on the differential calculus of 3d orientations		The proper handling of 3D orientations is a central element in many optimization problems in engineering. Unfortunately many researchers and engineers struggle with the formulation of such problems and often fall back to suboptimal solutions. The existence of many different conventions further complicates this issue, especially when interfacing multiple differing implementations. This document discusses an alternative approach which makes use of a more abstract notion of 3D orientations. The relative orientation between two coordinate systems is primarily identified by the coordinate mapping it induces. This is combined with the standard exponential map in order to introduce representation-independent and minimal differentials, which are very convenient in optimization based methods.	mathematical optimization;optimization problem;primer;time complexity	Michael Bloesch;Hannes Sommer;Tristan Laidlow;Michael Burri;Gabriel Nützi;Peter Fankhauser;Dario Bellicoso;Christian Gehring;Stefan Leutenegger;Marco Hutter;Roland Siegwart	2016	CoRR		computer science;engineering;artificial intelligence;mathematics;algorithm	AI	70.03574745855865	20.148355537514874	165585
d16bb190225308b6b2bf0c43c6cc3dcde681d2e9	relationship of eigenvalue between mpsd iterative matrix and jacobi iterative matrix	eigenvalues and eigenfunctions;jacobi iterative matrix;jacobian matrices eigenvalues and eigenfunctions iterative methods;p cyclic matrix mpsd iterative matrix jacobi iterative matrix eigenvalue;iterative methods artificial intelligence;block p cyclic matrix;eigenvalues;eigenvalue;iterative methods;block p cyclic matrix jacobi iterative matrix eigenvalue mpsd iterative matrix;mpsd iterative matrix;artificial intelligence;p cyclic matrix;jacobian matrices	Relationship of eigenvalue between MPSD iterative matrix and Jacobi iterative matrix for block p-cyclic case is obtained. The results in corresponding references are improved and perfected.	iterative method;jacobi method	Zhuan-De Wang;Chuan-Sheng Yang;Li Tan	2010		10.1109/ICMLC.2010.5580703	matrix splitting;matrix function;mathematical optimization;hollow matrix;combinatorics;eigendecomposition of a matrix;sparse matrix;nonnegative matrix;eigenvalues and eigenvectors;single-entry matrix;band matrix;convergent matrix;jacobi eigenvalue algorithm;hamiltonian matrix;mathematics;iterative method;pascal matrix;state-transition matrix;block matrix;eigenvalue algorithm;matrix;symmetric matrix;algebra	HPC	80.69124331079111	21.64991162194942	165647
1abc7fc71ea6dac71f6601c41a22c513f8409cd1	alternating two-stage methods for consistent linear systems with applications to the parallel solution of markov chains	linear systems;parallel algorithm;singular system;linear system;alternating two stage methods;block methods;probability distribution;parallel computer;block method;parallel implementation;linear equations;markov chains;markov chain;parallel algorithms	Two-stage methods in which the inner iterations are accomplished by an alternating method are developed. Convergence of these methods is shown in the context of solving singular and nonsingular linear systems. These methods are suitable for parallel computation. Experiments related to finding stationary probability distribution of Markov chains are performed. These experiments demonstrate that the parallel implementation of these methods can solve singular systems of linear equations in substantially less time than the sequential counterparts. 2009 Civil-Comp Ltd. and Elsevier Ltd. All rights reserved.	computation;experiment;iteration;iterative method;linear algebra;linear equation;linear system;markov chain;numerical analysis;parallel computing;stationary process;stochastic matrix;system of linear equations;the matrix;time complexity;monotone	Héctor Migallón Gomis;Violeta Migallón;José Penadés	2010	Advances in Engineering Software	10.1016/j.advengsoft.2008.12.021	markov chain;mathematical optimization;combinatorics;discrete mathematics;examples of markov chains;mathematics;parallel algorithm;linear system;statistics	AI	81.84409103786273	23.573139642402666	165652
1797e0d6da4b6bde466232f43eb5ccc2643cd8f0	a multiprojection algorithm using bregman projections in a product space	signal detection;optimization problem;orthogonal projection;convex set;product space	Generalized distances give rise to generalized projections into convex sets. An important question is whether or not one can use within the same projection algorithm different types of such generalized projections. This question has practical consequences in the area of signal detection and image recovery in situations that can be formulated mathematically as a convex feasibility problem. Using an extension of Pierra's product space formalism, we show here that a multiprojection algorithm converges. Our algorithm is fully simultaneous, i.e., it uses in each iterative stepall sets of the convex feasibility problem. Different multiprojection algorithms can be derived from our algorithmic scheme by a judicious choice of the Bregman functions which govern the process. As a by-product of our investigation we also obtain blockiterative schemes for certain kinds of linearly constraned optimization problems.	algorithm;bregman divergence;convex set;detection theory;iterative method;map projection;mathematical optimization;semantics (computer science)	Yair Censor;Tommy Elfving	1994	Numerical Algorithms	10.1007/BF02142692	convex analysis;optimization problem;subderivative;mathematical optimization;combinatorics;mathematical analysis;discrete mathematics;product topology;mathematics;proximal gradient methods;convex set;orthographic projection;detection theory	ML	73.19127080053109	21.47962795405142	165672
69ab1a81507e8143a5d8f0376d783427b3247674	new algorithms for computing the real structured pseudospectral abscissa and the real stability radius of large and sparse matrices	15a18;lyapunov equation;abscissa;eigenvalue;stability radius;sparse matrix;65f50;65f15;real structured pseudospectrum;15a24	We present two new algorithms for investigating the stability of large and sparse matrices subject to real perturbations. The first algorithm computes the real structured pseudospectral abscissa and is based on the algorithm for computing the pseudospectral abscissa proposed by Guglielmi and Overton [SIAM J. Matrix Anal. Appl., 32 (2011), pp. 1166-1192]. It entails finding the rightmost eigenvalues for a sequence of large matrices, and we demonstrate that these eigenvalue problems can be solved in a robust manner by an unconventional eigenvalue solver. We also develop an algorithm for computing the real stability radius of a real and stable matrix, which utilizes a recently developed technique for detecting the loss of stability in a large dynamical system. Both algorithms are tested on large and sparse matrices.	algorithm;comstock–needham system;dynamical system;gauss pseudospectral method;iscb overton prize;lyapunov fractal;modulus of continuity;robustness (computer science);sensor;solver;sparse matrix;stability radius;unconventional computing	Minghao W. Rostami	2015	SIAM J. Scientific Computing	10.1137/140975413	divide-and-conquer eigenvalue algorithm;mathematical optimization;combinatorics;discrete mathematics;abscissa;sparse matrix;eigenvalues and eigenvectors;lyapunov equation;mathematics;algebra	Vision	81.08638334200471	23.54572897287566	165709
1640c2294cf879587ad08af88ca8e815d44b7920	amps: an augmented matrix formulation for principal submatrix updates with application to power grids		We present AMPS, an augmented matrix approach to update the solution to a linear system of equations when the matrix is modified by a few elements within a principal submatrix. This problem arises in the dynamic security analysis of a power grid, where operators need to perform N − k contingency analysis, i.e., determine the state of the system when exactly k links from N fail. Our algorithms augment the matrix to account for the changes in it, and then compute the solution to the augmented system without refactoring the modified matrix. We provide two algorithms, a direct method, and a hybrid direct-iterative method for solving the augmented system. We also exploit the sparsity of the matrices and vectors to accelerate the overall computation. We analyze the time complexity of both algorithms, and show that it is bounded by the number of nonzeros in a subset of the columns of the Cholesky factor that are selected by the nonzeros in the sparse right-handside vector. Our algorithms are compared on three power grids with PARDISO, a parallel direct solver, and CHOLMOD, a direct solver with the ability to modify the Cholesky factors of the matrix. We show that our augmented algorithms outperform PARDISO (by two orders of magnitude), and CHOLMOD (by a factor of up to 5). Further, our algorithms scale better than CHOLMOD as the number of elements updated increases. The solutions are computed with high accuracy. Our algorithms are capable of computing N−k contingency analysis on a 778 thousand bus grid, updating a solution with k = 20 elements in 16 milliseconds on an Intel Xeon processor.	algorithm;cholesky decomposition;code refactoring;column (database);computation;direct method in the calculus of variations;experiment;finite element method;flow-based programming;iterative method;linear system;matrix mechanics;solver;sparse matrix;system of linear equations;the matrix;time complexity	Yu Hong Yeung;Alex Pothen;Mahantesh Halappanavar;Zhenyu Huang	2017	SIAM J. Scientific Computing	10.1137/16M1082755	mathematics;mathematical optimization;matrix splitting;band matrix;system of linear equations;sparse matrix;augmented matrix;matrix (mathematics);matrix-free methods;block matrix	HPC	81.58847622734709	24.330949349463413	165912
e64412e5fb98d94e44c8876aa1588be2528b71f8	expected residual minimization method for a class of stochastic quasivariational inequality problems		We consider the expected residual minimization method for a class of stochastic quasivariational inequality problems SQVIP . The regularized gap function for quasivariational inequality problem QVIP is in general not differentiable. We first show that the regularized gap function is differentiable and convex for a class of QVIPs under some suitable conditions. Then, we reformulate SQVIP as a deterministic minimization problem that minimizes the expected residual of the regularized gap function and solve it by sample average approximation SAA method. Finally, we investigate the limiting behavior of the optimal solutions and stationary points.		Hui-qiang Ma;Nan-jing Huang	2012	J. Applied Mathematics	10.1155/2012/816528	mathematical optimization;mathematical analysis;discrete mathematics;mathematics	ML	73.38041798605533	24.794881633330643	165943
700094cec47e7c91718e96f2f440124b56d68864	algorithm 776: srrit: a fortran subroutine to calculate the dominant invariant subspace of n nonsymmetric matrix	eigenvalue problem;projection method;satisfiability;eigenvalues;project method;invariant subspace;fortran;nonsymmetric eigenvalue problem	SRRT is a Fortran program to calculate an approximate orthonomral basis fr a dominant invariant subspace of a real matrix <italic>A</italic> by the method of simultaneous iteration. Specifically, given an integer <italic>m</italic>, SRRIT computes a matrix <italic>Q</italic> with <italic>m</italic> orthonormal columns and real quasi-triangular matrix <italic>T</italic> or order <italic>m</italic> such that the equation <italic>AQ = QT</italic> is satisfied up to a tolerance specified by the user. The eigenvalues of <italic>T</italic> are approximations to the <italic>m</italic> eigenvalues of largest absolute magnitude of <italic>A</italic> and the columns of <italic>Q</italic> span the invariant subspace corresponding to those eigenvalues. SRRIT references <italic>A</italic> only through a user-provided subroutine to form the product <italic>AQ</italic>; hence it is suitable for large sparse problems.	approximation algorithm;column (database);fortran;iteration;oracle advanced queuing;sparse matrix;subroutine;triangular matrix	Zhaojun Bai;G. W. Stewart	1997	ACM Trans. Math. Softw.	10.1145/279232.279234	mathematical optimization;combinatorics;discrete mathematics;eigenvalues and eigenvectors;mathematics;projection method;project method;satisfiability	Theory	81.22086155263587	22.783361964874793	165992
ddc0cfc1f2fba35d9b905d8ffee3cefd9e66d8d1	a fast and robust unconstrained optimization method requiring minimum storage	optimisation;methode direction conjuguee;conjugate gradient method;mathematical programming;unconstrained optimization;quasi newton method;optimization;optimisation sans contrainte;computational efficiency;programmation mathematique	This paper describes a new unconstrained optimisation procedure employing conjugate directions and requiring only threen-dimensional vectors. The method has been tested for computational efficiency and stability on a large set of test functions and compared with numerical data of other major methods. Results show that the method possesses strong superiority over other existing conjugate gradient methods on all problems and can out-perform or is at least as efficient as quasi-Newton methods on many tested problems.	mathematical optimization	D. Le	1985	Math. Program.	10.1007/BF01585658	mathematical optimization;quasi-newton method;conjugate residual method;gradient method;calculus;mathematics;conjugate gradient method;nonlinear conjugate gradient method;algorithm	Theory	76.74629572516007	23.505671602660072	166178
c56836ac998b6aaaa071c358e53effcfa0fbe695	optimization reformulations of the generalized nash equilibrium problem using regularized indicator nikaidô-isoda function	regularized indicator nikaido isoda function;90c30;optimization reformulations;quasi variational inequality problem;generalized nash equilibrium problem;91a10;normalized nash equilibria	In this paper, we extend the literature by adapting the Nikaidô–Isoda function as an indicator function termed as regularized indicator Nikaidô–Isoda function, and this is demonstrated to guarantee existence of a solution. Using this function, we present two constrained optimization reformulations of the generalized Nash equilibrium problem (GNEP for short). The first reformulation characterizes all the solutions of GNEP as global minima of the optimization problem. Later this approach is modified to obtain the second optimization reformulation whose global minima characterize the normalized Nash equilibria. Some numerical results are also included to illustrate the behaviour of the optimization reformulations.	constrained optimization;mathematical optimization;maxima and minima;nash equilibrium;numerical analysis;optimization problem	C. S. Lalitha;Mansi Dhingra	2013	J. Global Optimization	10.1007/s10898-012-9978-0	mathematical optimization;calculus;mathematics;mathematical economics	ML	72.20938645114781	22.199165328342367	166260
c19d1f484b05e85e8928ce5aad40c5f5e9a8d18d	"""erratum to """"mann and ishikawa iterative processes for multivalued mappings in banach spaces"""" [comput. math. appl 54 (2007) 872-877]"""	strong convergence;banach space;uniformly convex banach spaces;uniformly convex banach space;multivalued mapping;ishikawa iterates	We show strong convergence for Mann and Ishikawa iterates of multivalued nonexpansive mapping T under some appropriate conditions, which revises a gap in Panyanak [B. Panyanak, Mann and Ishikawa iterative processes for multivalued mappings in Banach spaces, Comput. Math. Appl. 54 (2007) 872–877]. Furthermore, we also give an affirmative answer to Panyanak’s open question. c © 2008 Elsevier Ltd. All rights reserved.	ishikawa diagram;iterative method	Yisheng Song;Hongjun Wang	2008	Computers & Mathematics with Applications	10.1016/j.camwa.2007.11.042	mathematical analysis;discrete mathematics;topology;mathematics;banach space;algebra	AI	74.778184804613	18.617406367026035	166512
8dfd7ddaf63e6c92862ae378156771fd6dcf6f42	an interior point potential reduction algorithm for the linear complementarity problem	linear complementarity;quadratic programming;fonction potentiel;quadratic program;linearity;metodo reduccion;programmation quadratique;p matrix;interior point;linearite;methode point interieur;linearidad;programacion lineal;positive semi definite matrix;mathematical programming;funcion potencial;linear programming;programmation lineaire;linear program;probleme complementarite;problema complementariedad;programacion cuadratica;complementarity problem;methode reduction;potential function;linear complementarity problem;interior point method;reduction method;programmation mathematique;programacion matematica	Abbreviated title: Interior-point algorithm for the LCP Abstract. The linear complementarity problem (LCP) can be viewed as the problem of minimizing x T y subject to y = M x + q and x; y 0. We are interested in nding a point with x T y < for a given > 0: The algorithm proceeds by iteratively reducing the potential function f (x; y) = ln x T y ? X ln x j y j ; where, for example, = 2n. The direction of movement in the original space can be viewed as follows. First, apply a linear scaling transformation to make the coordinates of the current point all equal to 1. Take a gradient step in the transformed space using the gradient of the transformed potential function, where the step size is either predetermined by the algorithm or decided by line search to minimize the value of the potential. Finally, map the point back to the original space. A bound on the worst-case performance of the algorithm depends on the parameter = (M;), which is deened as the minimum of the smallest eigenvalue of a matrix of the form where X and Y vary over the nonnegative diagonal matrices such that e T XY e and X jj Y jj n 2. If M is a P-matrix, is positive and the algorithm solves the problem in polynomial time in terms of the input size, j log j, and 1==. It is also shown that when M is positive semi-deenite, the choice of = 2n + p 2n yields a polynomial-time algorithm. This covers the convex quadratic minimization problem.	ab initio quantum chemistry methods;algorithm;best, worst and average case;classical xy model;complementarity theory;emoticon;gradient;image scaling;information;line search;linear complementarity problem;polynomial;potential method;quadratic programming;semiconductor industry;time complexity	Masakazu Kojima;Nimrod Megiddo;Yinyu Ye	1992	Math. Program.	10.1007/BF01586054	mathematical optimization;linear programming;interior point method;calculus;mathematics;geometry;quadratic programming	Theory	74.99116787852621	22.634211110094782	166632
8a2b8b5b70be89288b0b564262c1d79577d1446d	rational qr-iteration without inversion	15a18;fonction rationnelle;analisis numerico;convergence;qr factorization;analyse numerique;26cxx;similarity transformation;convergencia;iteraccion;numerical analysis;iteration;funcion racional;rational function	In this manuscript a new method will be presented for performing a QR-iteration with (A − σ I)(A − κ I)−1 = QR without explicit inversion of the factor (A − κ I)−1. A QR-method driven by a rational function is attractive since convergence can occur at both sides of the matrix. Each step of this new iteration consists of two substeps. In the explicit version, first an RQ-factorization of the initial matrix A − κ I = RQ will be computed, followed by a QR-factorization of the matrix (A − σ I)Q H . The factorization of (A − σ I)Q H can be computed in an intelligent manner, exploiting properties of the already known RQ-factorization of A − κ I. The similarity transformation yielding the QR-step is defined by the unitary factor Q in the QR-factorization of the transformed matrix (A − σ I)Q H . Examples will be given, illustrating how to efficiently compute the factorization for some specific classes of matrices. The novelties of this approach with respect to these matrix classes will be discussed.	iteration;qr algorithm	Raf Vandebril;Marc Van Barel;Nicola Mastronardi	2008	Numerische Mathematik	10.1007/s00211-008-0177-3	matrix similarity;rational function;mathematical analysis;iteration;convergence;numerical analysis;calculus;mathematics;qr decomposition;algorithm;algebra	Crypto	80.59645307043273	21.037109592470696	166802
85cc7e72875a10d3f081110599201fc0e1717b44	a reduction method for variational inequalities	desigualdad variacional;metodo reduccion;variational inequalities;inegalite variationnelle;variational techniques;equilibrium problem;mathematical programming;equilibre economique;variational inequality;complementarity problems;probleme complementarite;problema complementariedad;complementarity problem;methode reduction;walrasian equilibrium;reduction method;equilibrio economico;computational general equilibrium;computable general equilibrium;economic equilibrium;technique variationnelle	This paper explains a method by which the number of variables in a variational inequality having a certain form can be substantially reduced by changing the set over which the variational inequality is posed. The method applies in particular to certain economic equilibrium problems occurring in applications. We explain and justify the method, and give examples of its application, including a numerical example in which the solution time for the reduced problem was approximately 2% of that for the problem in its original form. © 1998 The Mathematical Programming Society, Inc. Published by Elsevier Science B.V.	calculus of variations;variational inequality	Stephen M. Robinson	1998	Math. Program.	10.1007/BF01581724	mathematical optimization;computable general equilibrium;variational inequality;calculus;mathematics;mathematical economics	ML	74.6375773584679	20.97866181405605	166908
776012dd3fc48c76a2b9c9b9bc977a3ef8bd15e9	on new algorithms for inverting hessenberg matrices	hessenberg matrix;matrix factorization;computacion informatica;65f05;15a23;inverse matrix;computational complexity;ciencias basicas y experimentales;15a09;matematicas;15a15;grupo a;65y20	A modification of the Ikebe algorithm for computing the lower half of the inverse#N#of an (unreduced) upper Hessenberg matrix, extended to compute the entries of the#N#superdiagonal, is considered in this paper. It enables us to compute the inverse of a#N#quasiseparable Hessenberg matrix in O(n2) times. A new factorization expressing the#N#inverse of a nonsingular Hessenberg matrix as a product of two suitable matrices is#N#obtained. Because this allows us the use of back substitution for the inversion of triangular#N#matrices, the inverse is computed with complexity O(n3). Some comparisons with results#N#obtained using other recent inversion algorithms are also provided.	algorithm;karl hessenberg	J. Abderramán Marrero;M. Rachidi;V. Tomeo	2013	J. Computational Applied Mathematics	10.1016/j.cam.2012.11.003	combinatorics;calculus;mathematics;matrix decomposition;computational complexity theory;hessenberg matrix;eigenvalue algorithm;algorithm;algebra	Theory	81.54109263873966	22.693771110140574	167033
0c0240620b007bf110ab244fb4093b27c3c58d9b	interior point and newton methods in solving high dimensional flow distribution problems for pipe networks		In this paper optimal flow distribution problem in pipe network is considered. The investigated problem is a convex sparse optimization problem with linear equality and inequality constrains. Newton method is used for problem with equality constrains only and obtains an approximate solution, which may not satisfy inequality constraints. Then Dikin Interior Point Method starts from the approximate solution and finds an optimal one. For problems of high dimension sparse matrix methods, namely Conjugate Gradient and Cholesky method with nested dissection, are applied. Since Dikin Interior Point Method works much slower then Newton Method on the matrices of big size, such approach allows us to obtain good starting point for this method by using comparatively fast Newton Method. Results of numerical experiments are presented.	newton	Oleg O. Khamisov;Valery A. Stennikov	2017		10.1007/978-3-319-69404-7_10	mathematical optimization;convex optimization;cholesky decomposition;nested dissection;sparse matrix;mathematical analysis;newton's method;conjugate gradient method;interior point method;optimization problem;mathematics	ML	75.38895015260397	26.024024505793243	167112
7fc5ed6657bf37c1fc5b94a875dbea3b988c9e2d	a reduced-space line-search method for unconstrained optimization via random descent directions		Abstract In this paper, we propose an iterative method based on reduced-space approximations for unconstrained optimization problems. The method works as follows: among iterations, samples are taken about the current solution by using, for instance, a Normal distribution; for all samples, gradients are computed (approximated) in order to build reduced-spaces onto which descent directions of cost functions are estimated. By using such directions, intermediate solutions are updated. The overall process is repeated until some stopping criterion is satisfied. The convergence of the proposed method is theoretically proven by using classic assumptions in the line search context. Experimental tests are performed by using well-known benchmark optimization problems and a non-linear data assimilation problem. The results reveal that, as the number of sample points increase, gradient norms go faster towards zero and even more, in the data assimilation context, error norms are decreased by several order of magnitudes with regard to prior errors when the assimilation step is performed by means of the proposed formulation.	line search;mathematical optimization	Elias D. Niño;Carlos J. Ardila;Jesús G Estrada;Rafael Capacho	2019	Applied Mathematics and Computation	10.1016/j.amc.2018.08.020	iterative method;line search;mathematical optimization;data assimilation;assimilation (phonology);normal distribution;mathematics;optimization problem;convergence (routing)	Vision	75.71696479235432	24.239378890447107	167298
1039e9ba441bffbaa6c58ef67046efeb44e7f134	nonmonotone globalization techniques for the barzilai-borwein gradient method	gradient method;global convergence;nonmonotone techniques;unconstrained optimization;barzilai borwein method;steepest descent	In this paper we propose new globalization strategies for the Barzilai and Borwein gradient method, based on suitable relaxations of the monotonicity requirements. In particular, we define a class of algorithms that combine nonmonotone watchdog techniques with nonmonotone linesearch rules and we prove the global convergence of these schemes. Then we perform an extensive computational study, which shows the effectiveness of the proposed approach in the solution of large dimensional unconstrained optimization problems.	algorithm;gradient method;line search;local convergence;mathematical optimization;requirement;watchdog timer	Luigi Grippo;Marco Sciandrone	2002	Comp. Opt. and Appl.	10.1023/A:1020587701058	gradient descent;mathematical optimization;gradient method;calculus;mathematics;mathematical economics	ML	75.51394439327441	22.559170466041945	167361
15045f0a5616a9499d2b4b7ab519e271c6915336	a modified spectral conjugate gradient projection algorithm for total variation image restoration	constrained optimization;image restoration;global convergence;conjugate gradient projection;total variation	In this study, a modified spectral conjugate gradient projection method is presented to solve total variation image restoration, which is transferred into the nonlinear constrained optimization with the closed constrained set. The global convergence of the proposed scheme is analyzed. In the end, some numerical results illustrate the efficiency of this method.	algorithm;circuit restoration;conjugate gradient method;image restoration	Benxin Zhang;Zhibin Zhu;Shuang'an Li	2014	Appl. Math. Lett.	10.1016/j.aml.2013.08.006	image restoration;mathematical optimization;constrained optimization;mathematical analysis;conjugate residual method;gradient method;derivation of the conjugate gradient method;mathematics;geometry;conjugate gradient method;nonlinear conjugate gradient method;biconjugate gradient method;total variation	Vision	77.66660138821172	22.79005213925569	167363
1c2a13d47c2d138ab0be6779ed388701cc2e30c1	modulus-based synchronous multisplitting iteration methods for linear complementarity problems	convergence;linear complementarity problem	SUMMARY#R##N#By an equivalent reformulation of the linear complementarity problem into a system of fixed-point equations, we construct modulus-based synchronous multisplitting iteration methods based on multiple splittings of the system matrix. These iteration methods are suitable to high-speed parallel multiprocessor systems and include the multisplitting relaxation methods such as Jacobi, Gauss–Seidel, successive overrelaxation, and accelerated overrelaxation of the modulus type as special cases. We establish the convergence theory of these modulus-based synchronous multisplitting iteration methods and their relaxed variants when the system matrix is an H + -matrix. Numerical results show that these new iteration methods can achieve high parallel computational efficiency in actual implementations. Copyright © 2012 John Wiley & Sons, Ltd.	complementarity theory;iteration;iterative method;linear complementarity problem;modulus of continuity	Zhong-Zhi Bai;Li-Li Zhang	2013	Numerical Lin. Alg. with Applic.	10.1002/nla.1835	mathematical optimization;mathematical analysis;discrete mathematics;mathematics	Robotics	80.77814101498446	22.267247247990177	167471
1bc9ec52a1640cb18d37957264731a4a368a4549	extending the cgls algorithm for least squares solutions of the generalized sylvester-transpose matrix equations		This paper deals with the solution to the least squares problem               min    X    ‖     ∑    i  =  1    s      A    i      XB    i    +    ∑    j  =  1    t      C    j      X    T      D    j    −  E   ‖  ,           corresponding to the generalized Sylvester-transpose matrix equation. The conjugate gradient least squares (CGLS) method is extended to obtain a matrix algorithm for solving this problem. We show that the matrix algorithm can solve this problem within a finite number of iterations in the absence of roundoff errors. Also the descent property of the norm of residuals is obtained. Finally numerical results demonstrate the accuracy and robustness of the algorithm.	algorithm;least squares	Masoud Hajarian	2016	J. Franklin Institute	10.1016/j.jfranklin.2015.05.024	mathematical optimization;combinatorics;discrete mathematics;non-linear iterative partial least squares;mathematics;non-linear least squares	ML	80.55429901133853	21.87426799221229	167482
6399f23dc4a3ab7a07af469bcc71471c652e927b	lieb-thirring inequalities for jacobi matrices	jacobi matrix;ciencias basicas y experimentales;matematicas;higher dimensions;anu;grupo a;jacobi matrices	For a Jacobi matrix J on ?2(Z+) with Ju(n)=an?1u(n?1)+bnu(n)+anu(n+1), we prove that??E?u003e2(E2?4)1/2??n?bn?+4?n?an?1?. We also prove bounds on higher moments and some related results in higher dimension.	jacobi method;jacobian matrix and determinant	Dirk Hundertmark;Barry Simon	2002	Journal of Approximation Theory	10.1006/jath.2002.3704	jacobian matrix and determinant;calculus;mathematics;geometry;algebra	Theory	78.71104310856548	19.084229608960012	167572
3df1cd31a3e04bd4859701b88fc5bcc674156101	preconditioning for allen-cahn variational inequalities with non-local constraints	krylov subspace solver;general;510 mathematik;preconditioning;allen cahn model;pde constrained optimization;newton method;ddc 510;saddle point systems	The solution of Allen-Cahn variational inequalities with mass constraints is of interest in many applications. This problem can be solved both in its scalar and vector-valued form as a PDE-constrained optimization problem by means of a primal-dual active set method. At the heart of this method lies the solution of linear systems in saddle point form. In this paper we propose the use of Krylov-subspace solvers and suitable preconditioners for the saddle point systems. Numerical results illustrate the competitiveness of this approach. 1. Introduction. The solution of Allen-Cahn variational inequalities with non-local constraints can be formulated as an optimal control problem, which can be solved using a primal-dual active set method [4, 6, 7]. This method has proven very efficient in a variety of applications [27, 29, 35, 36]. As we will show in the course of this paper the solution to a linear system of the form Kx = b with K a real symmetric matrix is at the heart of this method. The sparse linear systems are usually of very large dimension and in combination with 3-dimensional experiments the application of direct solvers such as UMFPack [11] becomes infeasible. As a result iterative methods have to be employed (see e.g. [24, 43] for introductions to this field). For symmetric and indefinite systems the Minimal Residual method (minres) [39] is a common solver as it minimizes the 2-norm of the residual r k = b − Kx k over the Krylov subspace span r 0 , Kr 0 ,. .. , K k−1 r 0. The convergence behaviour of the iterative scheme depends on the conditioning of the problem and the clustering of the eigenvalues and can usually be enhanced with preconditioning techniques.P −1 Kx = P −1 b In this paper, we provide an efficient preconditioner P for the solution of Allen-Cahn variational inequalities combining methods for indefinite problems [31, 39, 48, 3] and algebraic multigrid developed for elliptic systems [15, 43, 42]. The arising linear systems lead to matrices K which have the following saddle point block-structure which arise in a variety of applications [3]	active set method;algebraic equation;approximation;calculus of variations;cluster analysis;competitive analysis (online algorithm);constrained optimization;constraint (mathematics);discretization;experiment;generalized minimal residual method;iteration;iterative method;krylov subspace;linear algebra;linear system;mathematical optimization;multigrid method;numerical analysis;numerical method;optimal control;optimization problem;preconditioner;solver;sparse matrix;umfpack;variational inequality;variational principle	Luise Blank;Lavinia Sarbu;Martin Stoll	2012	J. Comput. Physics	10.1016/j.jcp.2012.04.035	mathematical optimization;mathematical analysis;calculus;mathematics;preconditioner;newton's method	ML	80.13412491179034	25.38777978817313	167574
4825f72a06f9ed0c76d23a14216004593aae2c88	computation of functions of hamiltonian and skew-symmetric matrices	hamiltonian matrices;geometric integration;numerical solution;numerical method;linear system;symmetric matrix;symmetric matrices;matrix functions;matrix function;symplectic matrix;rational function;skew symmetric matrices;analytic function	In this paper we consider numerical methods for computing functions of matrices being Hamiltonian and skew-symmetric. Analytic functions of this kind of matrices (i.e., exponential and rational functions) appear in the numerical solutions of orthosymplectic matrix differential systems when geometric integrators are involved. The main idea underlying the presented techniques is to exploit the special block structure of a Hamiltonian and skew-symmetric matrix to gain a cheaper computation of the functions. First, we will consider an approach based on the numerical solution of structured linear systems and then another one based on the Schur decomposition of the matrix. Splitting techniques are also considered in order to reduce the computational cost. Several numerical tests and comparison examples are shown. © 2008 IMACS. Published by Elsevier B.V. All rights reserved.	algorithmic efficiency;approximation algorithm;computation;hamiltonian (quantum mechanics);iterative method;linear system;matlab;magma;numerical linear algebra;numerical method;numerical partial differential equations;symplectic integrator;the matrix;time complexity	Nicoletta Del Buono;Luciano Lopez;Tiziano Politi	2008	Mathematics and Computers in Simulation	10.1016/j.matcom.2008.03.011	matrix splitting;matrix analysis;matrix function;combinatorics;mathematical analysis;discrete mathematics;eigendecomposition of a matrix;nonnegative matrix;matrix multiplication;centrosymmetric matrix;skew-symmetric matrix;band matrix;convergent matrix;hamiltonian matrix;square matrix;mathematics;diagonalizable matrix;state-transition matrix;augmented matrix;matrix decomposition;matrix exponential;matrix;integer matrix;symmetric matrix;algebra	Theory	81.42407597484423	21.196859328084745	167714
75e4b32afe18b0fb1e1f3b3d0b325de81bb31f3f	a lagrangian dual method with self-concordant barriers for multi-stage stochastic convex programming	interior point methods;fonction barriere;non linear programming;polynomial time complexity;nonlinear programming;convex programming;complejidad polinomial;programacion no lineal;duality;programmation stochastique;polynomial complexity;methode point interieur;global convergence;programmation non lineaire;programmation convexe;barriere autoconcordante;lagrangian dual;objective function;multi stage stochastic nonlinear programming;complexite polynomial;dualite;metodo punto interior;self concordant barrier;methode lagrange;metodo lagrange;polynomial time;lagrangian method;dualidad;stochastic programming;interior point method;programacion estocastica;funcion barrera;barrier function;complexite temps polynomial;programmation multietage;programacion convexa	This paper presents an algorithm for solving multi-stage stochastic convex nonlinear programs. The algorithm is based on the Lagrangian dual method which relaxes the nonanticipativity constraints, and the barrier function method which enhances the smoothness of the dual objective function so that the Newton search directions can be used. The algorithm is shown to be of global convergence and of polynomial-time complexity.	convex optimization;lagrange multiplier;self-concordant function	Gongyun Zhao	2005	Math. Program.	10.1007/s10107-003-0471-x	mathematical optimization;mathematical analysis;nonlinear programming;interior point method;calculus;mathematics	ML	74.9481502860873	22.377758179383864	167965
4fed28cdb5659facd74deee5ade2fe39f6787901	inexact subgradient methods with applications in stochastic programming	optimisation;subgradient methods;optimizacion;optimisation sousgradient;gradient method;optimal method;aproximacion;subgradient method;programmation stochastique;approximation;algorithme;objective function;optimization problem;multi dimensional;algorithm;stochastic optimization;subgradient optimization;approximation scheme;linear program;optimization;optimizacion subgradiente;stochastic programming;programacion estocastica;algoritmo;approximations	In many instances, the exact evaluation of an objective function and its subgradients can be computationally demanding. By way of example, we cite problems that arise within the context of stochastic optimization, where the objective function is typically defined via multi-dimensional integration. In this paper, we address the solution of such optimization problems by exploring the use of successive approximation schemes within subgradient optimization methods. We refer to this new class of methods as inexact subgradient algorithms. With relatively mild conditions imposed on the approximations, we show that the inexact subgradient algorithms inherit properties associated with their traditional (i.e., exact) counterparts. Within the context of stochastic optimization, the conditions that we impose allow a relaxation of requirements traditionally imposed on steplengths in stochastic quasi-gradient methods. Additionally, we study methods in which steplengths may be defined adaptively, in a manner that reflects the improvement in the objective function approximations as the iterations proceed. We illustrate the applicability of our approach by proposing an inexact subgradient optimization method for the solution of stochastic linear programs.	stochastic programming;subgradient method	Kelly T. Au;Julia L. Higle;Suvrajeet Sen	1994	Math. Program.	10.1007/BF01582059	mathematical optimization;combinatorics;linear programming;subgradient method;stochastic optimization;machine learning;mathematics	ML	73.82246602006256	25.090787860436624	167969
30a2ebae0c72e13ecf4021afbbba4fb186ac99d8	"""comments on paper """"on the relation between two approaches to necessary optimality conditions in problems with state constraints"""""""	optimal control;maximum principle;state constraints;49n25	This Forum Note concerns the question of necessary optimality conditions in optimal control problems subject to state constraints. Some critical remarks about a recently published paper are made.	karush–kuhn–tucker conditions	D. Y. Karamzin	2018	J. Optimization Theory and Applications	10.1007/s10957-018-1370-x		Logic	69.31558693892269	20.472682623011384	168124
6108327fb9c55e92affaaa5bb44f9b4a273cecac	approximating method of frames	digital signal processing;approximation method;610 medicine health;linear least square;decomposition method;iteration method	In this paper, the well-known method of frames approach to the signal decomposition prob reformulated as a certain bilevel goal-attainment linear least squares problem. As a conseq numerically robust variant of the method, named approximating method of frames, is propo the basis of a certain minimal Euclidean norm approximating splitting pseudo-iteration-wise m  2003 Elsevier Science (USA). All rights reserved.	approximation algorithm;dictionary;discretization;experiment;framing (world wide web);iteration;linear least squares (mathematics);meta-object facility;numerical analysis;numerical stability;ver (command);whole earth 'lectronic link	Felipe Marti-Lopez;Thomas Koenig	2003	Digital Signal Processing	10.1016/S1051-2004(02)00024-6	mathematical optimization;combinatorics;discrete mathematics;decomposition method;digital signal processing;mathematics;iterative method	AI	78.0069227493324	21.473950960838938	168314
4d1b19ac35d3386c3005ac0babccc3050237fab6	an equivalent transformation of multi-objective optimization problems	proper efficiency;multi objective optimization;vector optimization;pareto optimality	A new equivalent definition of proper efficiency is presented. With the aid of the new definition of properness, a transformation technique is proved to transform a multi-objective problem to a more convenient one. Some conditions are determined under which the original and the transformed problems have the same Pareto and properly efficient solutions. This transformation could be employed for the sake of convexification and simplification in order to improve the computational efficiency for solving the given problem. Moreover, some existing results about the weighted sum method in the multi-objective optimization literature are generalized using the special case of the proposed transformation scheme.	mathematical optimization;multi-objective optimization	M. Zarepisheh;Panos M. Pardalos	2017	Annals OR	10.1007/s10479-014-1782-4	mathematical optimization;combinatorics;multi-objective optimization;calculus;mathematics;vector optimization	EDA	71.26243915138281	24.076573153661368	168535
e2e3a0c00baeb6e4fd6213c5fe0d905c1d776ee7	vector topical function, abstract convexity and image space analysis	vector topical function;envelope;abstract convexity;vector conjugation;vector topical optimization;image space analysis;90c29;90c30	In this paper, we introduce a new type of vector topical function. It contains some other categories of topical functions as special cases and can be interpreted as weak separation functions in image space analysis. We establish its envelope result and investigate its properties in the frame of abstract convexity. Then, we present the corresponding conjugation and subdifferential, and observe the relationships among these concepts. Finally, as applications, we obtain some dual results for some vector optimization, where the object is expressed as the difference of vector topical functions.		Chaoli Yao;Shengjie Li	2018	J. Optimization Theory and Applications	10.1007/s10957-018-1215-7	mathematical analysis;mathematical optimization;subderivative;vector optimization;mathematics;convexity	PL	71.09312230302626	20.09611750277021	168610
9c5752ce9d5d1f5cba01dd28bb1623a4f8430338	a superadditive solution for cephoidal bargaining problems	cephoid;two dimensions;bargaining;superadditive solution	We present a bargaining solution defined on a class of polytopes in Rn called “cephoids”. The solution generalizes the superadditive solution exhibited by Maschler and Perles for two dimensions. It is superadditive on a subclass of cephoids.		Diethard Pallaschke;Joachim Rosenmüller	2007	Int. J. Game Theory	10.1007/s00182-006-0057-y	mathematical optimization;two-dimensional space;mathematics;mathematical economics;welfare economics	Theory	70.17921643980267	20.180110345141994	168787
c71bfbe3441966e07867b0a91aaba86eb6a5c6e6	"""a note on """"a continuous approach to nonlinear integer programming"""""""	nonlinear integer programming;calculo de variaciones;analisis numerico;non linear programming;programacion entera;matematicas aplicadas;functional transformation method;mathematiques appliquees;programacion no lineal;probleme non lineaire;65kxx;metodo penalidad;optimization method;programmation non lineaire;65k10;metodo optimizacion;nonlinear problems;programmation en nombres entiers;analyse numerique;49xx;calcul variationnel;penalty method;methode penalite;numerical analysis;integer programming;49m30;mathematical programming;methode optimisation;global optimization;filled function method;applied mathematics;programmation mathematique;programacion matematica;variational calculus;penalty function	Ge and Huang (1989) proposed an approach to transform nonlinear integer programming problems into nonlinear global optimization problems, which are then solved by the filled function transformation method. The approach has recently attracted much attention. This note indicates that the formulae to determine a penalty parameter in two fundamental theorems are incorrect, and presents the corrected formulae and revised theorems.	integer programming	Guoqing Zhang	2009	Applied Mathematics and Computation	10.1016/j.amc.2009.08.015	mathematical optimization;integer programming;calculus;penalty method;mathematics;global optimization	Robotics	75.93523137173571	22.054218771573105	168793
8fa2f04fb266637848b144f1d1a49a7abd11dd0d	mathematical programs with complementarity constraints: stationarity, optimality, and sensitivity	bouligand stationarity;weak stationarity;mathematical program with complementarity constraints;strong stationarity;stability;vertical complementarity constraints;exact penalty function;sensitivity;clarke stationarity;equilibrium constraints	We study mathematical programs with complementarity constraints. Several stationarity concepts, based on a piecewise smooth formulation, are presented and compared. The concepts are related to stationarity conditions for certain smooth programs as well as to stationarity concepts for a nonsmooth exact penalty function. Further, we present Fiacco-McCormick type second order optimality conditions and an extension of the stability results of Robinson and Kojima to mathematical programs with complementarity constraints.	complementarity theory;sensitivity and specificity;stationary process	Holger Scheel;Stefan Scholtes	2000	Math. Oper. Res.	10.1287/moor.25.1.1.15213	econometrics;mathematical optimization;stability;sensitivity;mathematics;mixed complementarity problem;mathematical economics;statistics	Logic	72.40142404294426	21.18895655898026	169047
89250211e14c2ff34a3b1f8a19bdb2de3bd26b88	constraint generalized sylvester matrix equations	computacion informatica;generalized sylvester matrix equation;grupo de excelencia;explicit solution;ciencias basicas y experimentales;moore penrose inverse;rank;linear matrix equation	In this paper, some necessary and sufficient conditions are established for the constraint generalized Sylvester matrix equations to have a common solution. The expression of the general common solution is also given under the solvable conditions. In addition, a numerical example is presented to illustrate the presented theory.	sylvester matrix	Qing-Wen Wang;Abdur Rehman;Zhuo-Heng He;Yonghui Zhang	2016	Automatica	10.1016/j.automatica.2016.02.024	matrix function;sylvester matrix;mathematical analysis;rank;sylvester's law of inertia;moore–penrose pseudoinverse;calculus;sylvester equation;mathematics;state-transition matrix;algebra	Robotics	78.67760917907007	19.734091011235016	169099
84f93d41b6ca5560c47a1af78d7d46667529c513	the ppa-based numerical algorithm with the o(1/t) convergence rate for variant variational inequalities	variant variational inequality;convergence rate;proximal point algorithm	Since proximal point algorithms (PPAs) are attractive for solving monotone variational inequalities, various versions of PPAs are developed for variant variational inequalities. In this paper, we prove a worst-case \(O(1/t)\) convergence rate in an ergodic sense for the PPA-based numerical algorithm in literature. Some numerical results are also reported to verify the computational preference.	algorithm;calculus of variations;numerical analysis;rate of convergence;variational inequality	Min Li;Zhikai Jiang	2014	Optimization Letters	10.1007/s11590-013-0674-x	mathematical optimization;combinatorics;mathematical analysis;mathematics;rate of convergence	ML	74.03099125024447	23.3146734495852	169110
1c3f15fd1037f3b072cb2fa9141024b64e4dc8bc	conjugate gradient bundle adjustment	levenberg marquardt;qr factorization;conjugate gradient;least square;matematik;bundle adjustment	Bundle adjustment for multi-view reconstruction is traditionally done using the Levenberg-Marquardt algorithm with a direct linear solver, which is computationally very expensive. An alternative to this approach is to apply the conjugate gradients algorithm in the inner loop. This is appealing since the main computational step of the CG algorithm involves only a simple matrix-vector multiplication with the Jacobian. In this work we improve on the latest published approaches to bundle adjustment with conjugate gradients by making full use of the least squares nature of the problem. We employ an easy-to-compute QR factorization based block preconditioner and show how a certain property of the preconditioned system allows us to reduce the work per iteration to roughly half of the standard CG algorithm.	analysis of algorithms;bundle adjustment;cholesky decomposition;computation;conjugate gradient method;convex conjugate;inner loop;iteration;jacobian matrix and determinant;least squares;levenberg–marquardt algorithm;matrix multiplication;numerical linear algebra;preconditioner;qr decomposition;solver	Martin Byröd;Kalle Åström	2010		10.1007/978-3-642-15552-9_9	mathematical optimization;conjugate residual method;levenberg–marquardt algorithm;computer science;machine learning;calculus;mathematics;geometry;conjugate gradient method;bundle adjustment;least squares;qr decomposition	Vision	79.84778114557629	23.742236596464497	169148
26bb2404c03bb8dc3ce5765da12d92d78b1c7a2d	on parallel multisplitting methods for symmetric positive semidefinite linear systems	singular linear systems;symmetric positive semidefinite matrix;semiconvergence;linear system;modified diagonally compensated reduction;multisplitting	In this paper we propose some parallel multisplitting methods for solving consistent symmetric positive semidefinite linear systems, based on modified diagonally compensated reduction. The semiconvergence of the parallel multisplitting method is discussed. The results here generalize some known results for the nonsingular linear systems to the singular systems. Copyright © 2008 John Wiley u0026 Sons, Ltd.	linear system	Guangxi Cao;Yongzhong Song	2009	Numerical Lin. Alg. with Applic.	10.1002/nla.619	mathematical optimization;combinatorics;control theory;mathematics;linear system;semidefinite programming;algebra	HPC	81.02353993785458	22.05067998117952	169232
a833a40f4bcf3fd69d932133337cd96220658dc8	optimality conditions for semi-infinite and generalized semi-infinite programs via lower order exact penalty functions	generalized semi infinite program;generalized second order derivative;lower order exact penalization;optimality conditions;semi infinite programming;49m30;90c34;90c46	In this paper, we will study optimality conditions of semi-infinite programs and generalized semi-infinite programs by employing lower order exact penalty functions and the condition that the generalized second-order directional derivative of the constraint function at the candidate point along any feasible direction for the linearized constraint set is non-positive. We consider three types of penalty functions for semi-infinite program and investigate the relationship among the exactness of these penalty functions. We employ lower order integral exact penalty functions and the second-order generalized derivative of the constraint function to establish optimality conditions for semi-infinite programs. We adopt the exact penalty function technique in terms of a classical augmented Lagrangian function for the lower-level problems of generalized semi-infinite programs to transform them into standard semi-infinite programs and then apply our results for semi-infinite programs to derive the optimality condition for generalized semi-infinite programs. We will give various examples to illustrate our results and assumptions.	semiconductor industry	Xiaoqi Yang;Zhangyou Chen;Jinchuan Zhou	2016	J. Optimization Theory and Applications	10.1007/s10957-016-0914-1	mathematical optimization;mathematical analysis;discrete mathematics;penalty method;mathematics	Theory	72.99504634631435	22.575703661873337	169580
b649fccf4ec1c2c2b9e44517d0b751061618ef84	convergence properties of trust region methods for linear and convex constraints	optimisation sous contrainte;constrained optimization;rate of convergence;quadratic programming;minimization;optimisation;linear independence;general and miscellaneous mathematics computing and information science;mathematics;convergence;optimizacion;programmation quadratique;methode newton;relacion convergencia;taux convergence;global convergence;convergence rate;linear constraint;sequential quadratic programming;optimizacion con restriccion;iterative methods;region confiance;convergencia;trust region;trust region method;identification;algorithms;identificacion;programacion cuadratica;optimization;metodo newton;newton method;mathematical logic 990230 mathematics mathematical models 1987 1989;methode projection gradient;truncated newton method;generalized convexity;gradient projection method;degenerescence	We develop a convergence theory for convex and linearly constrained trust region methods which only requires that the step between iterates produce a sufficient reduction in the trust region subproblem. Global convergence is established for general convex constraints while the local analysis is for linearly constrained problems. The main local result establishes that if the sequence converges to a nondegenerate stationary point then the active constraints at the solution are identified in a finite number of iterations. As a consequence of the identification properties, we develop rate of convergence results by assuming that the step is a truncated Newton method. Our development is mainly geometrical; this approach allows the development of a convergence theory without any linear independence assumptions.	trust region	James V. Burke;Jorge J. Moré;Gerardo Toraldo	1990	Math. Program.	10.1007/BF01580867	mathematical optimization;constrained optimization;mathematical analysis;modes of convergence;compact convergence;calculus;mathematics;convergence tests;sequential quadratic programming;trust region;rate of convergence;normal convergence;quadratic programming	Vision	75.88311383921636	22.747919779535362	169931
568779f0ae04cc6d4d270972615f1a248de6aaf3	efficient evaluation of scaled proximal operators		Quadratic-support functions [Aravkin, Burke, and Pillonetto; J. Mach. Learn. Res. 14(1), 2013] constitute a parametric family of convex functions that includes a range of useful regularization terms found in applications of convex optimization. We show how an interior method can be used to efficiently compute the proximal operator of a quadratic-support function under different metrics. When the metric and the function have the right structure, the proximal map can be computed with cost nearly linear in the input size. We describe how to use this approach to implement quasi-Newton methods for a rich class of nonsmooth problems that arise, for example, in sparse optimization, image denoising, and sparse logistic regression.	approximation algorithm;computation;converge;convex function;convex optimization;hessian;information;iteration;krylov–bogolyubov theorem;logistic regression;mathematical optimization;newton;noise reduction;preconditioner;proximal operator;quasi-newton method;sparse approximation;sparse matrix;william l. burke	Michael P. Friedlander;Gabriel Goh	2016	CoRR		mathematical optimization;proximal gradient methods for learning;mathematical analysis;discrete mathematics;mathematics;proximal gradient methods	ML	75.20080579603034	25.409724702323	169956
0fa366a8580e2005a78f61729f505ec6f236aa91	improving the condition number of a simple eigenvalue by a rank one matrix		In this work a technique to improve the condition number si of a simple eigenvalue λi of a matrix A ∈ Cn×n is given. This technique obtains a rank one updated matrix that is similar to A with the eigenvalue condition number of λi equal to one. More precisely, the similar updated matrix A + viq∗, where Avi = λivi and q is a fixed vector, has si = 1 and the remaining condition numbers are at most equal to the corresponding initial condition numbers. Moreover an expression to compute the vector q, using only the eigenvalue λi and its eigenvector vi, is given.	condition number;initial condition	Rafael Bru;Rafael Cantó;Ana M. Urbano	2016	Appl. Math. Lett.	10.1016/j.aml.2016.01.010	mathematical optimization;combinatorics;discrete mathematics;mathematics	Theory	79.27244171028488	20.756495351009807	170015
6d4c909af6157d2a20b1c1b759e4e157684a7de4	preservation of prox-regularity of sets with applications to constrained optimization	constrained optimization;49j53;90c30;hypomonotonicity;26b25;prox regular set;metric regularity;semiconvexity;49j52;sweeping process	In this paper, we first provide counterexamples showing that sublevels of prox-regular functions and levels of differentiable mappings with Lipschitz derivatives may fail to be prox-regular. Then, we prove the uniform prox-regularity of such sets under usual verifiable qualification conditions. The preservation of uniform prox-regularity of intersection and inverse image under usual qualification conditions is also established. Applications to constrained optimization problems are given.	constrained optimization;program optimization	Samir Adly;F. Nacry;Lionel Thibault	2016	SIAM Journal on Optimization	10.1137/15M1032739	mathematical optimization;constrained optimization;combinatorics;discrete mathematics;mathematics	Theory	72.13708220417219	20.548212848323526	170502
331f8f15b612bcc56ed3c7d33f3bd0ad28c03f3a	computation of channel capacity based on self-concordant functions	self-concordant function;channel capacity;polynomial time;maximum entropy;new algorithm;classical issue;information theory;unit cost	The computation of channel capacity is a classical issue in information theory. Much attention has been paid to it [1–3]. Despite some progress [3], the Arimoto-Blahut algorithm [4–7] is still the most used method. This method has been developed into a general algorithm, alternative minimization method [8], and has found applications in various fields [9–11]. Recently, the computation of channel capacity with constraints, such as the computation of capacity-cost function, is becoming increasingly important [5, 12–15]. The Arimoto-Blahut algorithm is very limited in dealing with such issues. Due to lack of appropriate means to eliminate the Lagrange multipliers, the method in [5], which is based on Arimoto-Blahut algorithm, cannot obtain the optimal point. The computation of channel capacity has always been an optimization problem, but no one optimization algorithm, such as the gradient method and Newton’s method, and so forth, has become the mainstream method. One reason is that so far no one optimization algorithm can be as effective as the Arimoto-Blahut algorithm. However, with advances in optimization theory and with the increasing importance of constrained channel capacity issues, the situation is changing. In this paper, some new algorithms based on self-concordant function are proposed. Besides the effectiveness—it is of polynomial time—the algorithm can compute constrained channel capacity easily. In Section 2, we introduced the definition of the self-concordant function and the main properties, as well as some algorithms for convex optimization problems. In Section 3, we give some new algorithms for channel capacity with or without constraints and for channel capacity per unit cost. Because it is based on the self-concordant function, these algorithms are of polynomial time. Some conclusions are given in Section 4.		Da-gang Tian;Yi-qun Huang	2012	J. Electrical and Computer Engineering	10.1155/2012/318946	mathematical optimization;theoretical computer science;mathematics;algorithm	ML	72.82277733317115	26.183502935484096	170585
ec7158457ab1bd3af391fb219bd0a1644e96d50f	the polynomial numerical hull of a matrix and algorithms for computing the numerical range	polynomial numerical hull;rang numerique;calculo automatico;algorithme numerique;computing;calcul automatique;numerical range;numerical algorithm;algoritmo numerico;enveloppe numerique polynomial	Let A be an n · n matrix. In this paper we discuss theoretical properties of the polynomial numerical hull of A of degree one and assemble them into three algorithms to computing the numerical range of A. 2006 Elsevier Inc. All rights reserved.	algorithm;numerical analysis;numerical integration;polynomial	Amir Abdollahi	2006	Applied Mathematics and Computation	10.1016/j.amc.2005.12.042	numerical range;combinatorics;computing;mathematics;geometry;algorithm;algebra	Theory	78.95681904045347	19.20974142157331	170618
3acc0cc1757bc70a6efd19204a474e2386f934a3	a self-adaptive gradient projection algorithm for the nonadditive traffic equilibrium problem	self adaptive scheme;nonadditive route cost;gradient projection algorithm;traffic equilibrium problem	Gradient projection (GP) algorithm has been shown as an efficient algorithm for solving the traditional traffic equilibrium problem with additive route costs. Recently, GP has been extended to solve the nonadditive traffic equilibrium problem (NaTEP), in which the cost incurred on each route is not just a simple sum of the link costs on that route. However, choosing an appropriate stepsize, which is not known a priori, is a critical issue in GP for solving the NaTEP. Inappropriate selection of the stepsize can significantly increase the computational burden, or even deteriorate the convergence. In this paper, a self-adaptive gradient projection (SAGP) algorithm is proposed. The self-adaptive scheme has the ability to automatically adjust the stepsize according to the information derived from previous iterations. Furthermore, the SAGP algorithm still retains the efficient flow update strategy that only requires a simple projection onto the nonnegative orthant. Numerical results are also provided to illustrate the efficiency and robustness of the proposed algorithm. Published by Elsevier Ltd.	algorithm;gradient;iteration;utility functions on indivisible goods	Anthony Chen;Zhong Zhou;Xiangdong Xu	2012	Computers & OR	10.1016/j.cor.2011.02.018	mathematical optimization;combinatorics;mathematics;mathematical economics	ML	73.57621601616917	25.751577665341248	170977
1e1d75c5fc369e334a245282b3c3f7a67f0ae81a	accelerated uzawa methods for convex optimization	convex optimization;convergence rate;acceleration;black box oracle;uzawa method	We focus on a linearly constrained strongly convex minimization model and discuss the application of the classical Uzawa method. Our principal goal is to show that some existing acceleration schemes can be used to accelerate the Uzawa method in the sense that the worst-case convergence rate (measured by the iteration complexity) of the resulting accelerated Uzawa schemes is O(1/k2) where k represents the iteration counter. Our discussion assumes that the objective function is given by a black-box oracle; thus an inexact version of the Uzawa method with a sequence of dynamically-chosen step sizes is implemented. A worst-case convergence rate of O(1/k) is also shown for this inexact version. Some preliminary numerical results are reported to verify the acceleration effectiveness of the accelerated Uzawa schemes and their superiority over some existing methods.	best, worst and average case;black box;convex function;convex optimization;deblurring;download;event dispatching thread;experiment;iteration;loss function;mathematical optimization;modulus robot;numerical analysis;optimization problem;rate of convergence;runge–kutta methods	Min Tao;Xiaoming Yuan	2017	Math. Comput.	10.1090/mcom/3145	acceleration;mathematical optimization;conic optimization;mathematical analysis;convex optimization;linear matrix inequality;mathematics;mathematical economics;rate of convergence	ML	74.41786962489508	23.821790707477316	171157
02ab9264ea5ce1056960349c46613bd8bad63156	on adaptively accelerated arnoldi method for computing pagerank	eigenvalue and eigenvector;weighted least squares problem;power method;pagerank;arnoldi process	A generalized refined Arnoldi method based on the weighted inner product is presented for computing PageRank. The properties of the generalized refined Arnoldi method were studied. To speed up the convergence performance for computing PageRank, we propose to change the weights adaptively where the weights are calculated based on the current residual corresponding to the approximate PageRank vector. Numerical results show that the proposed Arnoldi method converges faster than existing methods, in particular when the damping factor is large. Copyright © 2011 John Wiley & Sons, Ltd.	academy;approximation algorithm;arnoldi iteration;damping factor;functional renormalization group;john d. wiley;least squares;markov chain;numerical method;pagerank;the matrix	Jun-Feng Yin;Guo-Jian Yin;Michael K. Ng	2012	Numerical Lin. Alg. with Applic.	10.1002/nla.789	google matrix;mathematical optimization;combinatorics;power iteration;eigenvalues and eigenvectors;theoretical computer science;mathematics;algebra;arnoldi iteration	ML	82.20472033247998	22.875897944281718	171491
47f6fcf5fee63124a71e7ab29d4671f0c390aebe	variational gram functions: convex analysis and optimization		We propose a new class of convex penalty functions, called variational Gram functions (VGFs), that can promote pairwise relations, such as orthogonality, among a set of vectors in a vector space. These functions can serve as regularizers in convex optimization problems arising from hierarchical classification, multitask learning, and estimating vectors with disjoint supports, among other applications. We study necessary and sufficient conditions under which a VGF is convex, and give a characterization of its subdifferential. We show how to compute its proximal operator, and discuss efficient optimization algorithms for regularized loss minimization problems where the loss admits a simple variational representation and the regularizer is a VGF. We also establish a general representer theorem for such learning problems. Lastly, numerical experiments on a hierarchical classification problem are presented to demonstrate the effectiveness of VGFs and the associated optimization algorithms.	algorithm;calculus of variations;computer multitasking;convex analysis;convex function;convex optimization;experiment;mathematical optimization;numerical analysis;proximal operator;representer theorem;subderivative;variational principle	Amin Jalali;Lin Xiao;Maryam Fazel	2017	SIAM Journal on Optimization	10.1137/16M1087424	convex analysis;subderivative;mathematical optimization;conic optimization;combinatorics;mathematical analysis;convex optimization;test functions for optimization;mathematics;proximal gradient methods;proper convex function	ML	69.98238327360066	23.047394120775415	171509
228c0f102a1cb2598541da30357a6e4f8fb29634	a novel proof of the existence of solutions for a new system of generalized mixed quasi-variational-like inclusions involving ( a , eta , m )-accretive operators	η;fixed point theorem;a η m accretive operator;iterative algorithm;relaxed cocoercive mapping;existence of solution;a;m accretive operator;existence;system of generalized mixed quasi variational like inclusions	In this paper, we introduce a new system of generalized mixed quasi-variational-like inclusions with (A, η, m)-accretive operators and relaxed cocoercive mappings. By using the fixed point theorem of Nadler, we prove the existence of solutions for this general system of generalized mixed quasi-variational-like inclusions and its special cases. The results in this paper unify, extend and improve some known results in the literature. The novel proof method is simpler than those iterative algorithm approach for proving the existence of solutions of all classes of system of set-valued variational inclusions in the literature.	variational principle	Jian-Wen Peng	2008	J. Global Optimization	10.1007/s10898-008-9278-x	mathematical optimization;mathematical analysis;discrete mathematics;hapticity;mathematics;iterative method;fixed-point theorem	Theory	72.93978949457556	21.443231820675187	171543
0a579d9eb19633553d39c5c785d12610d4bcae5a	on a connection between kernel pca and metric multidimensional scaling	metric multidimensional scaling;kernel function;objective function;multidimensional scaling;eigenproblem;kernel pca;mds	In this note we show that the kernel PCA algorithm of Schölkopf, Smola, and Müller (Neural Computation, 10, 1299–1319.) can be interpreted as a form of metric multidimensional scaling (MDS) when the kernel function k(x, y) is isotropic, i.e. it depends only on ‖x − y‖. This leads to a metric MDS algorithm where the desired configuration of points is found via the solution of an eigenproblem rather than through the iterative optimization of the stress objective function. The question of kernel choice is also discussed.	algorithm;computation;iterative method;kernel (operating system);kernel principal component analysis;loss function;mathematical optimization;multidimensional scaling;optimization problem	Christopher K. I. Williams	2000	Machine Learning	10.1023/A:1012485807823	kernel;mathematical optimization;combinatorics;mathematical analysis;kernel embedding of distributions;radial basis function kernel;multidimensional scaling;kernel principal component analysis;computer science;machine learning;mathematics;variable kernel density estimation;polynomial kernel;kernel smoother	ML	75.42975084327318	19.97351008411218	171672
bcfbbd01983f18f8b396736568b2294a58fb3f99	preprocessing of sparse unassembled linear systems for efficient solution using element-by-element preconditioners	element by element;linear system;direct method	We believe that these preliminary results indicate that the amalgamation methods considered here hold promise, especially for large, ill-conditioned problems for which direct methods are inappropriate and simple element-by-element pre-conditioners ineffective. However, we must be cautious as our proposals are merely heuristics, and believe that further experimentation is necessary to assess the full potential of the methods.	preconditioner;preprocessor;sparse	Michel J. Daydé;Jean-Yves L'Excellent;Nicholas I. M. Gould	1996		10.1007/BFb0024682	direct method;theoretical computer science;machine learning;distributed computing;linear system	HPC	82.76822692963921	24.617661541177345	171866
eb474307fdbc6454b2b8e02fa22ee9e0ffa4eb13	homotopy methods to compute equilibria in game theory	operations research and management science	This paper presents a survey of the use of homotopy methods in game theory. Homotopies allow for a robust computation of game-theoretic equilibria and their refinements. Homotopies are also suitable to compute equilibria that are selected by various selection theories. We present the relevant techniques underlying homotopy algorithms. We give detailed expositions of the Lemke-Howson algorithm and the van den Elzen-Talman algorithm to compute Nash equilibria in 2-person games, and the Herings-van den Elzen, Herings-Peeters, and McKelvey-Palfrey algorithms to compute Nash equilibria in general n-person games. We explain how the main ideas can be extended to compute equilibria in extensive form and dynamic games, and how homotopies can be used to compute all Nash equilibria. JEL Classification Codes: C62, C63, C72, C73.	approximation algorithm;complementarity theory;computation;converge;game theory;kerrison predictor;lemke–howson algorithm;linear complementarity problem;nash equilibrium;nonlinear system;numerical analysis;numerical stability;predictor–corrector method;quantum;simplex algorithm	P. Jean-Jacques Herings;Ronald J. A. P. Peeters	2007			mathematical optimization;combinatorics;mathematics;normal-form game;mathematical economics	Theory	73.6447183262694	20.77883826207359	172239
5bc7f1b71b1fee79cbe8401e5c28e90467072894	ccomp: an efficient algorithm for complex roots computation of determinantal equations		Abstract In this paper a free Python algorithm, entitled CCOMP (Complex roots COMPutation), is developed for the efficient computation of complex roots of determinantal equations inside a prescribed complex domain. The key to the method presented is the efficient determination of the candidate points inside the domain which, in their close neighborhood, a complex root may lie. Once these points are detected, the algorithm proceeds to a two-dimensional minimization problem with respect to the minimum modulus eigenvalue of the system matrix. In the core of CCOMP exist three sub-algorithms whose tasks are the efficient estimation of the minimum modulus eigenvalues of the system matrix inside the prescribed domain, the efficient computation of candidate points which guarantee the existence of minima, and finally, the computation of minima via bound constrained minimization algorithms. Theoretical results and heuristics support the development and the performance of the algorithm, which is discussed in detail. CCOMP supports general complex matrices, and its efficiency, applicability and validity is demonstrated to a variety of microwave applications. Program summary Program Title: CCOMP Program Files doi: http://dx.doi.org/10.17632/x6fx4zssft.1 Licensing provisions: GPLv3 Programming language: Python Nature of problem: The determination of the resonances of a physical system, arising from determinantal type equations. These resonances arise from the non trivial solution of a homogeneous system of equations, whose system matrix depends on a parameter z , which can be either real or complex depending on the application. The values of z for which the determinant of the aforementioned system matrix is zero, are the resonances of the physical system. Solution method: An open-source software is developed for the efficient detection of all complex roots inside a prescribed complex domain D . The program is written in Python programming language [1] in conjunction with NumPy [2], SciPy [3], and Matplotlib [4]. The key to the method presented is the efficient determination of the candidate points inside D which in their close neighborhood the existence of a minimum is guaranteed. Once these points are detected, the algorithm proceeds to a two-dimensional minimization problem with respect to the minimum modulus eigenvalue of the system matrix, which is a positive function inside D . If the minimization yields global minima (near zero), the roots are found. For local minima (values of minimum modulus eigenvalue function away from zero), no roots exist. For all other points which have not been flagged as candidates, the algorithm does not proceed to the minimization problem. Additional comments including restrictions and unusual features: Python library psutil is used to compute memory consumption. [1] The Python programming language, https://www.python.org/ . [2] NumPy, http://numpy.org/ . [3] SciPy, http://scipy.org/ . [4] Matplotlib, http://matplotlib.org/ .	algorithm;computation	Grigorios P. Zouros	2018	Computer Physics Communications	10.1016/j.cpc.2017.09.023	mathematical optimization;eigenvalues and eigenvectors;computation;maxima and minima;numpy;system of linear equations;mathematics;physical system;matrix (mathematics);complex number;algorithm	Theory	80.84845764775265	24.207334272647028	172600
1d2bfee2b70d8cfb7e4b11e677a6a556ed915c26	simple lu and qr based non-orthogonal matrix joint diagonalization	minimisation;traitement signal;minimization;fonction orthogonale;analisis componente principal;high dimensionality;separacion ciega;cost function;separation aveugle source;matrix diagonalization;blind source separation;factorisation qr;parameterization;minimizacion;qr factorization;parametrizacion;factorizacion lu;blind separation;factorizacion qr;diagonalizacion matriz;diagonalisation matrice;joint diagonalization;signal processing;principal component analysis;analyse composante principale;separation aveugle;invariante;orthogonal function;funcion ortogonal;jacobi matrices;procesamiento senal;factorisation lu;lu factorization;parametrisation;invariant;scale invariance;numerical simulation	A class of simple Jacobi-type algorithms for non-orthogonal matrix joint diagonalization based on the LU or QR factorization is introduced. By appropriate parametrization of the underlying manifolds, i.e. using triangular and orthogonal Jacobi matrices we replace a high dimensional minimization problem by a sequence of simple one dimensional minimization problems. In addition, a new scale-invariant cost function for non-orthogonal joint diagonalization is employed. These algorithms are step-size free. Numerical simulations demonstrate the efficiency of the methods.	algorithm;computational fluid dynamics;gauss–jacobi quadrature;jacobi method;lu decomposition;loss function;numerical linear algebra;qr code;qr decomposition;simulation	Bijan Afsari	2006		10.1007/11679363_1	parametrization;minimisation;mathematical optimization;combinatorics;computer science;machine learning;invariant;scale invariance;calculus;signal processing;orthogonal diagonalization;mathematics;statistics;principal component analysis	Robotics	80.337671730645	20.865344460505746	172627
b9f79ec29ce3f84ba926f20cd3d4222ead0c4e43	new implicit and explicit approximation methods for solutions of integral equations of hammerstein type	equations of hammerstein type;real hilbert space;psuedocontractive mapping;iterative methods;accretive operators	Let H be a real Hilbert spae and F , K : H ? H be mappings such that D ( K ) = D ( F ) = H . Suppose that Hammerstein equation of the type u + KFu = 0 has a solution in H, then we studied in this paper methods that contain an auxiliary mapping (defined on an appropriate real Hilbert space in terms of the mappings K and F) which is pseudocontractive whenever K and F are monotone; and approximation of a fixed point of this pseudocontractive mapping induces approximation of a solution of the equation u + KFu = 0 . Moreover, the mappings K and F need not be defined on compact subset of H or angle bounded on H. Furthermore, our methods which do not involve K - 1 provide an implicit algorithm for approximation of solutions of the equation u + KFu = 0 whenever K and F are assumed to be bounded and continuous; if K and F are assumed to be Lipschitz continuous, then an explicit iterative algorithm for computation of solutions of the equation u + KFu = 0 is provided, still without involving K - 1 .	approximation	Eric U. Ofoedu;Charles E. Onyi	2014	Applied Mathematics and Computation	10.1016/j.amc.2014.08.057	mathematical optimization;mathematical analysis;discrete mathematics;mathematics;iterative method;algebra	Theory	75.8805050740951	18.395013785862833	172803
88bb1429573218b8749d6f13a8062f16dddcec8a	weak subdifferential in nonsmooth analysis and optimization	nonsmooth analysis;necessary optimality condition	Some properties of the weak subdifferential are considered in this paper. By using the definition and properties of the weak subdifferential which are described in the papers Azimov and Gasimov, 1999; Kasimbeyli and Mammadov, 2009; Kasimbeyli and Inceoglu, 2010 , the author proves some theorems connecting weak subdifferential in nonsmooth and nonconvex analysis. It is also obtained necessary optimality condition by using the weak subdifferential in this paper.	calculus of variations;hamilton–jacobi–bellman equation;mathematical optimization;numerical method;optimal control;subderivative;sum rule in quantum mechanics;variational analysis	Sahlar F. Meherrem;Refet Polat	2011	J. Applied Mathematics	10.1155/2011/204613	mathematics	AI	72.45336147037034	21.12457053043085	172899
095a67c7d0d104a85f4823d6e69714bf785fed80	on the orbit-stabilizer problem for integral matrix actions of polycyclic groups	congruence subgroup;integral matrix action;theorie groupe;symbolic computation;optimisation;group action;polycyclic group;optimizacion;accion grupo;group theory;calculo simbolico;orbital stability;orbit stabilizer problem;sous groupe congruence;groupe polycycloque;optimization;probleme stabilisation orbite;action groupe;action matrice integrale;calcul symbolique;teoria grupo	We present an algorithm to solve the orbit-stabilizer problem for a polycyclic group G acting as a subgroup of GL(d,Z) on the elements of Qd. We report on an implementation of our method and use this to observe that the algorithm is practical.	algorithm	Bettina Eick;Gretchen Ostheimer	2003	Math. Comput.	10.1090/S0025-5718-03-01493-5	symbolic computation;topology;group action;calculus;mathematics;geometry;group theory;algebra	Theory	78.79102049589112	18.656612773377866	172959
f10f4ae512fbce86fd81396096e8c5d022304530	restricted $p$ -isometry properties of nonconvex matrix recovery	minimisation;concave programming;minimization;routing protocols;low rank matrix recovery;restricted p isometry properties;lmr reconstruction;probability;atmospheric measurements;protocole transmission;etude theorique;gaussian processes;random gaussian linear transformation;isometrie;matrix algebra;noise measurement;relajacion;approximation;approximation theory;protocolo transmision;restricted p isometry constants;vectors;isometria;matrix decomposition;gaussian measurements;期刊论文;nonconvex matrix recovery;transformation lineaire;estudio teorico;linear transformation;low rank matrix recovery lmr;schatten p quasinorm minimization;schatten p quasi norm minization;relaxation;protocole routage;random gausian linear transformation;schatten p quasi norm minimization;approximation methods;theoretical study;p rip condition;isometry;nuclear norm minimization;linear matrix inequalities;transformacion lineal;transmission protocol	Recently, a nonconvex relaxation of low-rank matrix recovery (LMR), called the Schatten- p quasi-norm minimization (0 <; p <; 1), was introduced instead of the previous nuclear norm minimization in order to approximate the problem of LMR closer. In this paper, we introduce a notion of the restricted p-isometry constants (0 <; p ≤ 1) and derive a p -RIP condition for exact reconstruction of LMR via Schatten-p quasi-norm minimization. In particular, we determine how many random, Gaussian measurements are needed for the p-RIP condition to hold with high probability, which gives a theoretical result that it needs fewer measurements with small p for exact recovery via Schatten-p quasi-norm minimization than when p=1.	approximation algorithm;emoticon;linear programming relaxation;magnetic storage;multidimensional digital pre-distortion;numerical method;with high probability	Min Zhang;Zheng-Hai Huang	2013	IEEE Transactions on Information Theory	10.1109/TIT.2013.2250577	minimisation;mathematical optimization;mathematical analysis;isometry;noise measurement;approximation;relaxation;probability;gaussian process;mathematics;geometry;linear map;routing protocol;matrix decomposition;statistics;approximation theory	Theory	68.81239704220782	19.13540574472931	173232
006e5a2baa69c69fbc31c7f090e3dbbd0d9a1a0f	semismoothness of spectral functions	minimisation;calcul matriciel;minimization;spectral function;symmetric function;methode newton;analisis non regular;funcion simetrica;49m45;fonction symetrique;grupo de excelencia;minimizacion;matriz simetrica;funcion espectral;nonsmooth analysis;symmetric matrix;fonction spectrale;ciencias basicas y experimentales;matematicas;90c25;semismooth function;fonction semireguliere;90c33;analyse non lisse;matrice symetrique;tecnologias generalidades;metodo newton;newton method;matrix calculus;tecnologias;journal magazine article;calculo de matrices	Any spectral function can be written as a composition of a symmetric function f : Rn → R and the eigenvalue function λ(·) : S → Rn, often denoted by (f ◦ λ), where S is the subspace of n × n symmetric matrices. In this paper, we present some nonsmooth analysis for such spectral functions. Our main results are (a) (f ◦ λ) is directionally differentiable if f is semidifferentiable, (b) (f ◦λ) is LC1 if and only if f is LC1, and (c) (f ◦λ) is SC1 if and only if f is SC1. Result (a) is complementary to a known (negative) fact that (f ◦λ) might not be directionally differentiable if f is directionally differentiable only. Results (b) and (c) are particularly useful for the solution of LC1 and SC1 minimization problems which often can be solved by fast (generalized) Newton methods. Our analysis makes use of recent results on continuously differentiable spectral functions as well as on nonsmooth symmetric–matrix-valued functions.	computable function;emoticon;newton;newton's method;spectral density;subderivative	Houduo Qi;Xiaoqi Yang	2003	SIAM J. Matrix Analysis Applications	10.1137/S0895479802417921	minimisation;matrix calculus;calculus;mathematics;newton's method;algorithm;symmetric function;symmetric matrix;algebra	Theory	78.94588939294991	20.35366674392101	173301
5b84babe391f672ccbc1f4da3946190136147966	on convergence of the gauss-newton method for convex composite optimization	optimisation;optimizacion;convex programming;programmation convexe;metodo gauss newton;convex function;quadratic convergence;optimization;methode gauss newton;convergence quadratique;fonction convexe;gauss newton method;funcion convexa;programacion convexa	The local quadratic convergence of the Gauss-Newton method for convex composite optimization f = h ◦ F is established for any convex function h with the minima set C, extending Burke and Ferris’ results in the case when C is a set of weak sharp minima for h.	convex function;gauss–newton algorithm;mathematical optimization;maxima and minima;newton's method;rate of convergence;william l. burke	Chong Li;Xinghua Wang	2002	Math. Program.	10.1007/s101070100249	convex function;convex analysis;subderivative;mathematical optimization;conic optimization;mathematical analysis;convex optimization;convex combination;linear matrix inequality;calculus;mathematics;rate of convergence;proper convex function	ML	74.97387777973454	22.059342718497472	173325
11752bb4d2836d3e203f3c703c19cdc20c09996c	efficient decomposition methods of fuzzy relation and their application to image decomposition	cost function;gradient method;fuzzy relation;decomposition method;optimization;image decomposition	Two optimizations for decomposition problem of fuzzy relation (image) are proposed. The first optimization is a fast decomposition method of fuzzy relation based on the properties of max and min operations and the simultaneous updating of the prototype. The second optimization corresponds to an improvement of a cost function, in order to obtain a good quality of the solution of the decomposition problem.		Hajime Nobuhara;Kaoru Hirota;Salvatore Sessa;Witold Pedrycz	2005	Appl. Soft Comput.	10.1016/j.asoc.2004.09.002	mathematical optimization;discrete mathematics;decomposition method;computer science;gradient method;machine learning;mathematics	AI	71.52577800158163	24.10755713095083	173340
1f5d9ae71533123c9fdefe893e95180ce0d7d6e3	on some symmetric dual models in multiobjective programming	second order;second order symmetric duality;multiobjective programming;programmation multiobjectif;analisis numerico;matematicas aplicadas;mathematiques appliquees;proper efficiency;efficiency;duality;analyse numerique;dualite;numerical analysis;first order;first order symmetric duality;dualidad;applied mathematics;programacion multiobjetivo	Certain omissions have been pointed out in some papers on symmetric duality in multiobjective programming. Corrective measures have also been discussed.	multi-objective optimization	T. R. Gulati;Geeta Mehndiratta	2009	Applied Mathematics and Computation	10.1016/j.amc.2009.05.032	mathematical optimization;combinatorics;duality;applied mathematics;numerical analysis;calculus;first-order logic;mathematics;efficiency;second-order logic	NLP	74.1053398459146	20.378614225197758	173387
536425a3cab340dbaef98d4e864fb4702621661e	a primal-dual interior point method whose running time depends only on the constraint matrix	optimisation sous contrainte;metodo cuadrado menor;constrained optimization;methode moindre carre;time dependent;primal dual interior point method;algorithm complexity;least squares method;interior point;complejidad algoritmo;strongly polynomial time;primal dual method;methode point interieur;complexity;methode primale duale;optimizacion con restriccion;metodo primal dual;metodo punto interior;complexite algorithme;mathematical programming;least square;polynomial time;linear programming;programmation lineaire;linear program;layered least squares;central path;interior point method;programmation mathematique;path following;chemin central	"""This note provides a simpliied proof concerning the paper \A Primal-Dual Interior Point Method Whose Running Time Depends Only on the Constraint Matrix"""" by the same authors. In particular, we prove that Case II, one of the three cases in the method, can never occur."""	dtime;interior point method;time complexity	Stephen A. Vavasis;Yinyu Ye	1996	Math. Program.	10.1007/BF02592148	mathematical optimization;linear programming;interior point method;calculus;mathematics;geometry;least squares	Theory	75.43158560592866	21.969079820929966	173602
8ad1818a902e71534a7d9f8395340a1425bca705	generic rank-one corrections for value iteration in markovian decision problems	iteration sur valeur;dynamic programming;chemin plus court stochastique;gauss seidel method;stochastic shortest path;methode gauss seidel;markovian decision problem;decision markov;dynamic program;convergence rate;value iteration;decision problem;metodo gauss seidel;methode jacobi;programmation dynamique;metodo jacobi;markov decision;error bound;tk7855 m41 e3845 no 2212;gauss seidel;jacobi method	Givena linear iteration ftheformx := F (x),we consider modifledversions oftheform x := F (x + °d),whered isa flxeddirection, and ° ischosentominimizethenorm ofthe residual kx + °d ¡ F (x + °d)k. W e proposeways tochoosed sothattheconvergencerate ofthemodiflediteration isgovernedby thesubdominant eigen valueoftheoriginal. In the special casewhereF relates toa Markoviandecision problem,we obtaina new extrap olation method forvalueiteration. Inparticular, ourmethod accelerates theGauss-Seidel version of thevalueiteration method fordiscoun tedproblemsinthesame way thatMacQueen’serror boundsaccelerate thestandard version. Furthermore, ourmethod applies equally welltoMarkov Renewalandundiscoun tedproblems. 1 Researc h supportedby NSF underGrantCCR-9103804.ThanksareduetoDavidCastanon forstimulating discussions. 2 DepartmentofElectrical Engineering andComputerScience, M.I.T., Cam bridge, Mass.,02139. 1 1.Introduction 1. INTR ODUCTION Consider a linear iteration ftheformx :=F (x),where	decision problem;eigen (c++ library);ibm notes;interrupt request (pc architecture);iteration;markov decision process	Dimitri P. Bertsekas	1995	Oper. Res. Lett.	10.1016/0167-6377(95)00007-7	mathematical optimization;combinatorics;power iteration;calculus;mathematics;algorithm;gauss–seidel method	ML	77.9659124631503	22.969497682460243	173616
35a6b58311ba5b1b7d51ce53fcced652e78a244c	solution of linear complementarity problems using minimization with simple bounds	constrained optimization;constrained minimization;numerical experiment;linear complementarity problem;optimality condition	We deene a minimization problem with simple bounds associated to the horizontal linear complementarity problem (HLCP). When the HLCP is solvable, its solutions are the global minimizers of the associated problem. When the HLCP is feasible, we are able to prove a number of properties of the stationary points of the associated problem. In many cases, the stationary points are solutions of the HLCP. The theoretical results allow us to conjecture that local methods for box constrained optimization applied to the associated problem are eecient tools for solving linear complementarity problems. Numerical experiments seem to connrm this conjecture.	complementarity theory;constrained optimization;decision problem;experiment;linear complementarity problem;mathematical optimization;numerical linear algebra;stationary process	Ana Friedlander;José Mario Martínez;Sandra Augusta Santos	1995	J. Global Optimization	10.1007/BF01099464	mathematical optimization;constrained optimization;combinatorics;mathematical analysis;mathematics;mixed complementarity problem;linear complementarity problem	Theory	73.72695877684356	22.943503363494752	173704
29b264fa49b64c84af256cc6349950192aba2648	trade-off preservation in inverse multi-objective convex optimization		Abstract Given an input solution that may not be Pareto optimal, we present a new inverse optimization methodology for multi-objective convex optimization that determines a weight vector producing a weakly Pareto optimal solution that preserves the decision maker’s trade-off intention encoded in the input solution. We introduce a notion of trade-off preservation, which we use as a measure of similarity for approximating the input solution, and show its connection with minimizing an optimality gap. We propose a linear approximation to the inverse model and a successive linear programming algorithm that balance between trade-off preservation and computational efficiency, and show that our model encompasses many of the existing inverse optimization models from the literature. We demonstrate the proposed method using clinical data from prostate cancer radiation therapy.	convex optimization;mathematical optimization	Timothy C. Y. Chan;Taewoo Lee	2018	European Journal of Operational Research	10.1016/j.ejor.2018.02.045	mathematical optimization;proper convex function;nonlinear programming;convex optimization;mathematics;quasiconvex function;conic optimization;linear matrix inequality;successive linear programming;ellipsoid method	Robotics	68.54105214456516	24.190666363878353	173819
b62a1576601ec7f7beece0132a906c21496ef0ac	lanczos based preconditioner for discrete ill-posed problems	image deblurring;structured matrices;semiconvergent;preconditioner;general methods;ill posed problem;lanczos bidiagonalization;tikhonov regularization;kronecker product	In this paper we use the Lanczos process for preconditioning discrete ill-posed problems. We show that by few steps of this process one can obtain a well qualified and efficient preconditioner. This is a general method in the sense that it is not limited only to special structured matrices and the matrix–vector multiplications can be carried out in O(n) operations. Also even in problems with structured matrices this preconditioner performs more efficiently than the circulant and Kronecker product approximate preconditioners.	approximation algorithm;bidiagonalization;circulant matrix;deblurring;experiment;karp's 21 np-complete problems;lanczos resampling;matrix regularization;numerical analysis;numerical linear algebra;preconditioner;the matrix;well-posed problem	Mansoor Rezghi;Seyed Mohammad Hosseini	2010	Computing	10.1007/s00607-010-0090-3	mathematical optimization;combinatorics;mathematical analysis;mathematics;preconditioner;kronecker product;tikhonov regularization;algebra	Theory	82.73378834655237	21.391443243327483	173925
ea9b495b3e56d150c738bedcd5811e975bbad22a	methods for sparse and low-rank recovery under simplex constraints		The de-facto standard approach of promoting sparsity by means of l1regularization becomes ineffective in the presence of simplex constraints, i.e., the target is known to have non-negative entries summing up to a given constant. The situation is analogous for the use of nuclear norm regularization for lowrank recovery of Hermitian positive semidefinite matrices with given trace. In the present paper, we discuss several strategies to deal with this situation, from simple to more complex. As a starting point, we consider empirical risk minimization (ERM) which turns out to enjoy similar theoretical properties w.r.t. prediction and l2-estimation error as l1-regularization. In light of this, we argue that ERM combined with a subsequent sparsification step like thresholding represents a sound alternative to the heuristic of using l1-regularization after dropping the sum constraint and subsequent normalization. At the next level, we show that any sparsity-promoting regularizer under simplex constraints cannot be convex. A novel sparsity-promoting regularization scheme based on the inverse or negative of the squared l2-norm is proposed, which avoids shortcomings of various alternative methods from the literature. Our approach naturally extends to HerStatistica Sinica: Newly accepted Paper (accepted version subject to English editing)	comment (computer programming);computation;empirical risk minimization;heuristic;ibm notes;iterative method;li-chen wang;matrix regularization;numerical analysis;simulation;sparse matrix;taxicab geometry;the matrix;thresholding (image processing)	Ping Li;Syama Sundar Rangapuram;Martin Slawski	2016	CoRR		mathematical optimization;combinatorics;calculus;mathematics;statistics	AI	77.37228243276395	25.370892889438924	174047
160e55db5746bd374ba41ab6e72d3ae5cf276d81	comment on using quaternions to calculate rmsd [j. comp. chem. 25, 1849 (2004)]	rotational superposition;least squares	Coutsias et al. have recently published a method to find the optimal rotational superposition of two molecular structures, which is based on a representation of rotations by quaternions (J. Comp. Chem. 25(15), 1849 (2004)). The method, which has been suggested by other authors before, is compared to the one by Kabsch, where the elements of the rotation matrix are directly used as variables of the optimization problem. The statement that the two methods are equivalent is misleading in the sense that the Kabsch method may yield an improper optimal rotation, which must be explicitly checked for, whereas the quaternion method does not mix proper and improper rotations. Nevertheless, both types of solutions can be considered by solving the same eigenvector problem. The relation between the two types of solutions is briefly discussed and bounds for the eigenvalues are given.	checking (action);chemical procedure;horner's method;mathematical optimization;molecular structure;optimization problem;scientific publication;solutions	Gerald R. Kneller	2005	Journal of computational chemistry	10.1002/jcc.20296	combinatorics;chemistry;calculus;computational chemistry;mathematics;least squares	Comp.	77.79366470297033	19.812830168843337	174075
120beda0875bb9a436261c742ac2d2c40fb37bed	sufficient conditions for low-rank matrix recovery, translated from sparse signal recovery	vision system;sparse signal recovery;ranking function;upper bound;optimization problem;statistical computing;necessary and sufficient condition;signal and image processing;linear transformation;nuclear norm minimization;information theory	The low-rank matrix recovery (LMR) is a rank minimization problem subject to linear equality constraints, and it arises in many fields such as signal and image processing, statistics, computer vision, system identification and control. This class of optimization problems is NP-hard and a popular approach replaces the rank function with the nuclear norm of the matrix variable. In this paper, we extend the concept of s-goodness for a sensing matrix in sparse signal recovery (proposed by Juditsky and Nemirovski [Math Program, 2011]) to linear transformations in LMR. Then, we give characterizations of s-goodness in the context of LMR. Using the two characteristic s-goodness constants, γs and γ̂s, of a linear transformation, not only do we derive necessary and sufficient conditions for a linear transformation to be s-good, but also provide sufficient conditions for exact and stable s-rank matrix recovery via the nuclear norm minimization under mild assumptions. Moreover, we give computable upper bounds for one of the s-goodness characteristics which leads to verifiable sufficient conditions for exact low-rank matrix recovery.	computable function;computer vision;constraint (mathematics);detection theory;formal verification;image processing;late move reductions;linear equation;magnetic storage;mathematical optimization;np-hardness;sparse matrix;system identification;the matrix	Lingchen Kong;Levent Tunçel;Naihua Xiu	2011	CoRR		optimization problem;mathematical optimization;combinatorics;discrete mathematics;information theory;mathematics;linear map;upper and lower bounds;statistics	ML	69.90165514579641	22.31453738092377	174242
0dd0a6f3f6862d8aacf54ce0c7432dc2d6692a60	adaptive constraint reduction for convex quadratic programming and training support vector machines	computer science adaptive constraint reduction for convex quadratic programming and training support vector machines university of maryland;dissertation;college park dianne p o leary jung;jin hyuk	Title of dissertation: Adaptive Constraint Reduction for Convex Quadratic Programming and Training Support Vector Machines Jin Hyuk Jung, Doctor of Philosophy, 2008 Dissertation directed by: Professor Dianne P. O’Leary Department of Computer Science Convex quadratic programming (CQP) is an optimization problem of minimizing a convex quadratic objective function subject to linear constraints. We propose an adaptive constraint reduction primal-dual interior-point algorithm for convex quadratic programming with many more constraints than variables. We reduce the computational effort by assembling the normal equation matrix with a subset of the constraints. Instead of the exact matrix, we compute an approximate matrix for a well chosen index set which includes indices of constraints that seem to be most critical. Starting with a large portion of the constraints, our proposed scheme excludes more unnecessary constraints at later iterations. We provide proofs for the global convergence and the quadratic local convergence rate of an affine scaling variant. A similar approach can be applied to Mehrotra’s predictor-corrector type algorithms. An example of CQP arises in training a linear support vector machine (SVM), which is a popular tool for pattern recognition. The difficulty in training a support vector machine (SVM) lies in the typically vast number of patterns used for the training process. In this work, we propose an adaptive constraint reduction primaldual interior-point method for training the linear SVM with l1 hinge loss. We reduce the computational effort by assembling the normal equation matrix with a subset of well-chosen patterns. Starting with a large portion of the patterns, our proposed scheme excludes more and more unnecessary patterns as the iteration proceeds. We extend our approach to training nonlinear SVMs through Gram matrix approximation methods. Promising numerical results are reported. Adaptive Constraint Reduction for Convex Quadratic Programming and Training Support Vector Machines by Jin Hyuk Jung Dissertation submitted to the Faculty of the Graduate School of the University of Maryland, College Park in partial fulfillment of the requirements for the degree of Doctor of Philosophy 2008 Advisory Commmittee: Professor Dianne P. O’Leary, Chair/Advisor Professor Kyu Yong Choi Professor Hanan Samet Professor G. W. Stewart Professor André L. Tits c © Copyright by Jin Hyuk Jung 2008	affine scaling;approximation algorithm;computer science;gramian matrix;hinge loss;image scaling;interior point method;iteration;jung;kerrison predictor;local convergence;mathematical optimization;nonlinear system;numerical analysis;optimization problem;ordinary least squares;pattern recognition;predictor–corrector method;quadratic function;quadratic programming;rate of convergence;requirement;singular value decomposition;support vector machine	Jin Hyuk Jung	2008			mathematical optimization;combinatorics;second-order cone programming;machine learning;mathematics;quadratic programming	AI	78.56839676889902	24.51249818926356	174517
c0331976077c2a2a0497c025778ab604e0c5dabd	representative functions of maximally monotone operators and bifunctions		The aim of this paper is to show that every representative function of a maximally monotone operator is the Fitzpatrick transform of a bifunction corresponding to the operator. In fact, for each representative function \(\varphi \) of the operator, there is a family of equivalent saddle functions (i.e., bifunctions which are concave in the first and convex in the second argument) each of which has \(\varphi \) as Fitzpatrick transform. In the special case where \(\varphi \) is the Fitzpatrick function of the operator, the family of equivalent saddle functions is explicitly constructed. In this way we exhibit the relation between the recent theory of representative functions, and the much older theory of saddle functions initiated by Rockafellar.	monotone	Monica Bianchi;Nicolas Hadjisavvas;Rita Pini	2018	Math. Program.	10.1007/s10107-016-1020-8	mathematical optimization;mathematical analysis;discrete mathematics;mathematics	Theory	70.71812221779146	18.83266451402254	174570
9ee064ba5b78506f45dc2c4b0addfa3bbc239430	equivalence of nonlinear complementarity problems and least element problems in banach lattices	banach lattice;least element problem;variational inequality;complementarity problem;nonlinear z map;nonlinear complementarity problem	A class of not necessarily linear operators A: V → V* is introduced, where the Banach space V and its dual V* carry dual vector-lattice orderings ≥. These operators, called Z-maps, generalize the n × n real matrices with nonnegative off-diagonal elements. If A is a strictly monotone Z-map with certain regularity and growth conditions, and if F denotes the set of all vectors v ∈ V for which v ≥ 0, and Av ≥ 0, then it is shown that the complementarity problem, to find v ∈ F such that   = 0, and the least element problem, to find v ∈ F with v ≤ w for all w ∈ F, have the same unique solution. Some other problems equivalent to these, and some examples are discussed.	complementarity (physics);mixed complementarity problem;turing completeness	R. C. Riddell	1981	Math. Oper. Res.	10.1287/moor.6.3.462	mathematical optimization;combinatorics;variational inequality;calculus;mathematics;complementarity theory;algebra	Theory	72.48525986868272	19.197443322426043	174769
0a6722b3d54f45c195c21abaa3c629d5c3eeaffa	the nonconvex geometry of low-rank matrix optimizations with general objective functions		This work considers the minimization of a general convex function f (X) over the cone of positive semi-definite matrices whose optimal solution X∗ is of low-rank. Standard first-order convex solvers require performing an eigenvalue decomposition in each iteration, severely limiting their scalability. A natural nonconvex reformulation of the problem factors the variable X into the product of a rectangular matrix with fewer columns and its transpose. For a special class of matrix sensing and completion problems with quadratic objective functions, local search algorithms applied to the factored problem have been shown to be much more efficient and, in spite of being nonconvex, to converge to the global optimum. The purpose of this work is to extend this line of study to general convex objective functions f (X) and investigate the geometry of the resulting factored formulations. Specifically, we prove that when f (X) satisfies the restricted well-conditioned assumption, each critical point of the factored problem either corresponds to the optimal solution X∗ or a strict saddle where the Hessian matrix has a strictly negative eigenvalue. Such a geometric structure of the factored formulation ensures that many local search algorithms can converge to the global optimum with random initializations.	column (database);condition number;cone (formal languages);converge;convex function;critical point (network science);first-order reduction;global optimization;hessian;iteration;local search (optimization);scalability;search algorithm;semiconductor industry	Qiuwei Li;Gongguo Tang	2017	2017 IEEE Global Conference on Signal and Information Processing (GlobalSIP)	10.1093/imaiai/iay003	mathematical optimization;combinatorics;mathematical analysis;mathematics	ML	75.13087299096435	24.709343176558626	174986
05592be67107ce35baab923ec9d99a24e1929051	non-euler-lagrangian pareto-optimality conditions for dynamic multiple criterion decision problems	differential equation;objective function;decision problem;multiple criterion decision making;non euler lagrangian;optimality theory;inventory control;pareto optimality	In this paper the problem of verifying the Pareto-optimality of a given solution to a dynamic multiple-criterion decision (DMCD) problem is investigated. For this purpose, some new conditions are derived for Pareto-optimality of DMCD problems. In the literature, Pareto-optimality is characterized by means of Euler-Lagrangian differential equations. There exist problems in production and inventory control to which these conditions cannot be applied directly (Song 1997). Thus, it is necessary to explore new conditions for Pareto-optimality of DMCD problems. With some mild assumptions on the objective functionals, we develop necessary and/or sufficient conditions for Pareto-optimality in the sprit of optimization theory. Both linear and non-linear cases are considered. Copyright Springer-Verlag 2006	euler;pareto efficiency	Augustine O. Esogbue;Qiang Song;Donovan Young	2006	Math. Meth. of OR	10.1007/s00186-005-0044-2	inventory control;mathematical optimization;combinatorics;discrete mathematics;decision problem;mathematics;differential equation	ML	70.82253853368418	21.325787158891945	175162
338cb3bbeb98f13bb56903e5cdd616bf5851b074	uniform approximation of min/max functions by smooth splines	spline;uniform approximation;minimax;computacion informatica;morgan scott partition;ciencias basicas y experimentales;matematicas;grupo a	"""In some optimization problems, min(f""""1,...,f""""n) usually appears as the objective or in the constraint. These optimization problems are typically non-smooth, and so are beyond the domain of smooth optimization algorithms. In this paper, we construct smooth splines to approximate min(f""""1,...,f""""n) uniformly so that such optimization problems can be dealt with as smooth ones."""	approximation;maxima and minima;spline (mathematics)	Guohui Zhao;Zhirui Wang;Haining Mou	2011	J. Computational Applied Mathematics	10.1016/j.cam.2011.06.023	spline;minimax;mathematical optimization;combinatorics;mathematical analysis;calculus;mathematics	ML	72.29767799298077	24.162189695712954	175441
e655ea99d0bb9527b77827534419baa59da2e0ba	an efficient duality-based approach for pde-constrained sparse optimization	optimal control;sparsity;finite element;duality approach;accelerated block coordinate descent;49n05;65n30;49m25;68w15	In this paper, elliptic optimal control problems involving the (L^1)-control cost ((L^1)-EOCP) is considered. To numerically discretize (L^1)-EOCP, the standard piecewise linear finite element is employed. However, different from the finite dimensional (l^1)-regularization optimization, the resulting discrete (L^1)-norm does not have a decoupled form. A common approach to overcome this difficulty is employing a nodal quadrature formula to approximately discretize the (L^1)-norm. It is clear that this technique will incur an additional error. To avoid the additional error, solving (L^1)-EOCP via its dual, which can be reformulated as a multi-block unconstrained convex composite minimization problem, is considered. Motivated by the success of the accelerated block coordinate descent (ABCD) method for solving large scale convex minimization problems in finite dimensional space, we consider extending this method to (L^1)-EOCP. Hence, an efficient inexact ABCD method is introduced for solving (L^1)-EOCP. The design of this method combines an inexact 2-block majorized ABCD and the recent advances in the inexact symmetric Gauss–Seidel (sGS) technique for solving a multi-block convex composite quadratic programming whose objective contains a nonsmooth term involving only the first block. The proposed algorithm (called sGS-imABCD) is illustrated at two numerical examples. Numerical results not only confirm the finite element error estimates, but also show that our proposed algorithm is more efficient than (a) the ihADMM (inexact heterogeneous alternating direction method of multipliers), (b) the accelerated proximal gradient method.	mathematical optimization;sparse approximation;sparse matrix	Xiaoliang Song;Bo Chen;Bo Yu	2018	Comp. Opt. and Appl.	10.1007/s10589-017-9951-4	mathematical optimization;proximal gradient methods;mathematical analysis;duality (optimization);mathematics;discretization;finite element method;convex optimization;quadratic programming;coordinate descent;piecewise linear function	Vision	77.44706737309818	24.70816921272582	175493
8500ccf5c26cdd805770c7427d09657435f4c2b4	on weak and strong solutions of f-implicit generalized variational inequalities with applications	normed space;solution forte;solucion debil;matematicas aplicadas;desigualdad variacional;mathematiques appliquees;inegalite variationnelle;f implicit generalized variational inequalities;weak solution;variational inequality;solution faible;f implicit generalized complementarity problems;probleme complementarite;problema complementariedad;complementarity problem;strong solution;applied mathematics;existence;solucion fuerte	In this work, we study theF-implicit generalized variational inequalities in a real normed space setting. Weak solutions and strong solutions are introduced. Several existence results are derived. As an application, we study the F-implicit generalized complementarity problems and some existence results are obtained. c © 2005 Elsevier Ltd. All rights reserved.	calculus of variations;complementarity theory;variational inequality;variational principle	Lu-Chuan Zeng;Yen-Cherng Lin;Jen-Chih Yao	2006	Appl. Math. Lett.	10.1016/j.aml.2005.10.001	mathematical optimization;mathematical analysis;variational inequality;applied mathematics;weak solution;calculus;mathematics;normed vector space	AI	73.92168111836908	20.688913123505223	175609
ab7cfae78a4ac8d261177904390c758c84e51f9a	application of eigensolvers in quadratic eigenvalue problems for brake systems analysis	eigenvalues;stability;brake systems;quadratic eigenvalue problems;eigenvectors	We compare, in terms of computing time and precision, three different implementations of eigensolvers for the unsymmetric quadratic eigenvalue problem. This type of problems arises, for instance, in structural Mechanics to study the stability of brake systems that require the computation of some of the smallest eigenvalues and corresponding eigenvectors. The usual procedure is linearization but it transforms quadratic problems into generalized eigenvalue problems of twice the dimension. Examples show that the application of the QZ method directly to the linearized problem is less expensive that the projection onto a space spanned by the eigenvectors of an approximating symmetric problem, this is not an obvious conclusion. We show that programs where this projection version is quicker may not be so accurate. We also show the feasibility of this direct linearization version in conjunction with an iterative method, as Arnoldi, in the partial solution of very large real life problems.		Sandra M. Aires;Filomena D. d'Almeida	2014		10.1007/978-3-319-09153-2_38	divide-and-conquer eigenvalue algorithm;mathematical optimization;combinatorics;mathematical analysis;eigenvalues and eigenvectors;mathematics;eigenvalue perturbation;statistics	Theory	78.71682547047573	23.987875453549005	175761
3416104adbccd6e2e58f2085259a34e9307a3495	"""""""backward differential flow"""" may not converge to a global minimizer of polynomials"""	polynomial optimization;trajectory methods;global optimization	We provide a simple counter-example to prove and illustrate that the backward differential flow approach, proposed by Zhu, Zhao and Liu for finding a global minimizer of coercive even-degree polynomials, can converge to a local minimizer rather than a global minimizer. We provide additional counter-examples to stress that convergence to a local minimum via the backward differential flow method is not a rare occurence.	converge;maxima and minima;polynomial ring	Orhan Arikan;Regina Sandra Burachik;C. Yalçin Kaya	2015	J. Optimization Theory and Applications	10.1007/s10957-015-0727-7	mathematical optimization;mathematical analysis;calculus;mathematics;global optimization	Theory	76.9830133471756	21.460459746937993	175815
9d8b99b31cb454b7567004cf095456f06c9e17eb	a class of polynomial volumetric barrier decomposition algorithms for stochastic semidefinite programming	stochastic linear programming;decomposition;semidefinite programming;decomposition algorithm;stochastic semidefinite programming;self concordance;volumetric barrier;semidefinite program	Ariyawansa and Zhu have recently proposed a new class of optimization problems termed stochastic semidefinite programs (SSDPs). SSDPs may be viewed as an extension of two-stage stochastic (linear) programs with recourse (SLPs). Zhao has derived a decomposition algorithm for SLPs based on a logarithmic barrier and proved its polynomial complexity. Mehrotra and Özevin have extended the work of Zhao to the case of SSDPs to derive a polynomial logarithmic barrier decomposition algorithm for SSDPs. An alternative to the logarithmic barrier is the volumetric barrier of Vaidya. There is no work based on the volumetric barrier analogous to that of Zhao for SLPs or to the work of Mehrotra and Özevin for SSDPs. The purpose of this paper is to derive a class of volumetric barrier decomposition algorithms for SSDPs, and to prove polynomial complexity of certain members of the class.	algorithm;mathematical optimization;polynomial;semidefinite programming;stochastic gradient descent;stochastic process;time complexity	K. A. Ariyawansa;Yuntao Zhu	2011	Math. Comput.	10.1090/S0025-5718-2010-02449-4	large margin nearest neighbor;mathematical optimization;combinatorics;discrete mathematics;second-order cone programming;quadratically constrained quadratic program;mathematics;semidefinite embedding;decomposition;semidefinite programming	Theory	73.22008637391866	24.908458467815393	175936
8a19d1507e7dcf7dccab7d8737df0bf8dcf83ccd	solving implicit mathematical programs with fuzzy variational inequality constraints based on the method of centres with entropic regularization	parametric membership function;90c70;tolerance approach;90c30;fuzzy mathematical program;90c33;method of centres with entropic regularization;fuzzy implicit variational inequality	The purpose of this paper is to consider a class of mathematical programs with fuzzy implicit variational inequality constraints in finite dimension real spaces. By using the “tolerance approach” and the fuzzy set theory, we also show that solving the fuzzy mathematical program problem with fuzzy implicit variational inequality constraints is equivalent to solving a fuzzy implicit complementarity constrained optimization problem, and the fuzzy implicit complementarity constrained optimization problem can be converted to a regular nonlinear parametric programming problem. Further, a new smoothing approach based on a version of the “method of centres” with entropic regularization for solving the resulting optimization problem and our main results are presented and a numerical example is provided to illustrate our main results applying quasi-Newton line search of MATLAB software.	bregman divergence;calculus of variations;complementarity theory;constrained optimization;constraint (mathematics);fuzzy set;line search;matlab;mathematical optimization;newton;nonlinear system;numerical analysis;optimization problem;parametric programming;randomness;set theory;smoothing;social inequality;variational inequality	Heng-you Lan;Juan J. Nieto	2015	FO & DM	10.1007/s10700-015-9207-7	mathematical optimization;mathematical analysis;discrete mathematics;fuzzy number;mathematics	ML	72.14470104964788	21.7669895993567	176049
e65fbe33f8650e4b9e3cd4a788586e437e1cb232	global convergence of nonlinear successive overrelaxation via linear theory	equation non lineaire;methode sor non lineaire;convergence;global convergence;overrelaxation method;non linear equation;methode surrelaxation	The global convergence of nonlinear successive overrelaxation is established by utilising estimates from linear SOR theory. Die globale Konvergenz des nichtlinearen SOR-Verfahrens wird mit Hilfe von Abschätzungen aus der linearen SOR-Theorie gezeigt.	linear system;local convergence;nonlinear system;successive over-relaxation	M. E. Brewster;Rangachary Kannan	1985	Computing	10.1007/BF02242174	mathematical optimization;mathematical analysis;discrete mathematics;convergence;mathematics	Theory	81.64838826984764	18.315442762348013	176303
010e4fa5249dcc82bacb04a5ff7222bf6aa3665c	an lqp method for pseudomonotone variational inequalities	pseudomonotone operator;variational inequality problem;global convergence;logarithmic quadratic proximal method;variational inequality	In this paper, we proposed a modified Logarithmic-Quadratic Proximal (LQP) method [Auslender et al.: Comput. Optim. Appl. 12, 31–40 (1999)] for solving variational inequalities problems. We solved the problem approximately, with constructive accuracy criterion. We show that the method is globally convergence under that the operator is pseudomonotone which is weaker than the monotonicity and the solution set is nonempty. Some preliminary computational results are given.	letter-quality printer;variational inequality;variational principle	Abdellah Bnouhachem	2006	J. Global Optimization	10.1007/s10898-006-9013-4	mathematical optimization;mathematical analysis;variational inequality;calculus;mathematics	ML	73.64170284962395	22.298172280601754	176375
5096cca5f2e92e3fa170c59bcea237456f6a6a02	on the linear convergence of the circumcentered-reflection method		Abstract In order to accelerate the Douglas–Rachford method we recently developed the circumcentered-reflection method, which provides the closest iterate to the solution among all points relying on successive reflections, for the best approximation problem related to two affine subspaces. We now prove that this is still the case when considering a family of finitely many affine subspaces. This property yields linear convergence and incites embedding of circumcenters within classical reflection and projection based methods for more general feasibility problems.	rate of convergence	Roger Behling;José Yunier Bello Cruz;Luiz-Rafael Santos	2018	Oper. Res. Lett.	10.1016/j.orl.2017.11.018	discrete mathematics;mathematical optimization;linear subspace;rate of convergence;embedding;mathematics;affine transformation	Robotics	74.03460499155963	23.326907740368526	176398
8f6163eeab0b96d5d049e4e86dcc418aa530c951	block-diagonal preconditioning for optimal control problems constrained by pdes with uncertain inputs	saddle point system;schur complement;preconditioning;pde constrained optimization;iterative methods;60h35;65f50;stochastic galerkin system;low rank solution;65f10;60h15;35r60;65n22	This talk is aimed at the efficient numerical simulation of optimization problems governed by either steady-state or unsteady partial differential equations involving random coefficients. This class of problems often leads to prohibitively high dimensional saddle point systems with Kronecker product structure, especially when discretized with the stochastic Galerkin finite element method. Here, we derive robust Schur complement-based preconditioners for solving the resulting stochastic optimality systems with all-at-once low-rank solvers. Moreover, we illustrate the effectiveness of our solvers with numerical experiments for the heat equation, the Stokes equations and the StokesBrinkman equations. ∗Computational Methods in Systems and Control Theory, Max Planck Institute for Dynamics of Complex Technical Systems, Sandtorstr. 1, 39106 Magdeburg Germany, (stollm@mpi-magdeburg.mpg.de) ‡Computational Methods in Systems and Control Theory, Max Planck Institute for Dynamics of Complex Technical Systems, Sandtorstr. 1, 39106 Magdeburg Germany, (stollm@mpi-magdeburg.mpg.de)	coefficient;discretization;experiment;finite element method;galerkin method;iteration;iterative method;karush–kuhn–tucker conditions;linear system;mathematical optimization;microsoft outlook for mac;numerical analysis;numerical weather prediction;optimal control;preconditioner;simulation;solver;stationary process;steady state	Peter Benner;Akwum Onwunta;Martin Stoll	2016	SIAM J. Matrix Analysis Applications	10.1137/15M1018502	mathematical optimization;combinatorics;mathematical analysis;mathematics;preconditioner;iterative method;schur complement;algebra	ML	80.33570254755884	25.467612290017453	176404
2fd9c06523a53c4e6e1da45c5611bdd2b17ab424	a new norm-relaxed method of strongly sub-feasible direction for inequality constrained optimization	optimisation sous contrainte;strong convergence;constrained optimization;convergence forte;analisis numerico;linear independence;non linear programming;matematicas aplicadas;convergencia fuerte;mathematiques appliquees;nonlinear programming;programacion no lineal;recherche profondeur d abord;optimization method;metodo direccion realizable;global convergence;programmation non lineaire;satisfiability;metodo optimizacion;methode direction realisable;analyse numerique;algorithme;optimizacion con restriccion;algorithm;numerical analysis;mathematical programming;methode optimisation;convergence globale;method of feasible direction;phase ii;direction finding;method of strongly sub feasible direction;applied mathematics;programmation mathematique;feasible direction method;programacion matematica;direct method;independance lineaire;algoritmo	Combining the norm-relaxed method of feasible direction (MFD) with the idea of strongly sub-feasible direction method, we present a new convergent algorithm with arbitrary initial point for inequality constrained optimization. At each iteration, the new algorithm solves one direction finding subproblem (DFS) which always possesses a solution. Some good properties of the new algorithm are that it can unify automatically the operations of initialization (Phase I) and optimization (Phase II) and the number of the functions satisfying the inequality constrains is nondecreasing, particularly, a feasible direction of descent can be obtained by solving DFS whenever the iteration point gets into the feasible set. Under some mild assumptions without the linear independence, the global and strong convergence of the algorithm can be obtained.	constrained optimization;mathematical optimization;social inequality	Jin-Bao Jian;Hai-Yan Zheng;Qingjie Hu;Chun-Ming Tang	2005	Applied Mathematics and Computation	10.1016/j.amc.2004.08.009	direct method;linear independence;mathematical optimization;numerical analysis;nonlinear programming;calculus;mathematics;algorithm;satisfiability	ML	75.61135324394364	22.6716949696159	176466
920069cc582a26998f735f19820795fc70a10fbd	solving variational inequalities by a modified projection method with an effective step-size	methode a pas;variational inequality problem;analisis numerico;convergence;matematicas aplicadas;desigualdad variacional;mathematiques appliquees;variational inequalities;inegalite variationnelle;projection method;58e35;global convergence;65k15;probleme variationnel;variational problem;analyse numerique;pseudomonotone mapping;49j40;step method;convergencia;numerical analysis;methode projection;metodo proyeccion;variational inequality;applied mathematics;metodo a paso	In this paper, we propose a new projection method for the solution of variational inequality problems. The method is simple, which uses only function evaluations and projections onto the feasible set. We adopt a new step-size rule and a new search direction in the new method. Under the mild conditions, we prove the proposed method is globally convergent. Preliminary numerical results are reported.	calculus of variations;projection method (fluid dynamics);variational inequality	Xiuyun Zheng;Hongwei Liu	2010	Applied Mathematics and Computation	10.1016/j.amc.2010.05.069	mathematical optimization;mathematical analysis;variational inequality;applied mathematics;calculus;mathematics	ML	75.55038818800017	22.234713963504348	176594
70534bb8caf7258b492616d14a46966e656aedd5	alternative principles and their applications	minimax inequality;54c60;analytic alternative;kakutani map;topological vector space;set valued mapping;satisfiability;equilibrium problem;fixed point;49j35;existence of solution;52a07;54h25;matching theorem	In this paper we establish two alternative principles of the following type: If X and Y are convex subsets of two locally convex Hausdorff topological vector spaces and F, S : X Y are two set-valued mappings satisfying certain conditions, then either there exists x0 ∈ X such that F(x0) = ∅ or ⋂x∈X S(x) = ∅. As first applications of the alternative principles we obtain two matching theorems of Ky Fan type. Next, are given several analytic alternatives and minimax inequalities. Finally we establish two very general alternative theorems concerning existence of solutions of a vector equilibrium problem.	convex set;hausdorff dimension;minimax	Mircea Balaj	2011	J. Global Optimization	10.1007/s10898-010-9612-y	mathematical optimization;topological vector space;combinatorics;mathematical analysis;mathematics;fixed point;satisfiability	Theory	70.84283004390497	20.11372712872182	176693
adff432a7213b838c6c8e905d2fccf92f5b07676	quadratic support functions in quadratic bilevel problems		We consider bilevel problem in the following formulation. The follower problem is a convex quadratic problem which linearly depends on the leader variable. The leader problem is a quadratic problem. We are looking for an optimistic solution. Optimal value function of the follower problem is used to reformulate bilevel problem as a standard (one-level) optimization problem. By this way we obtain a nonconvex multiextremal problem with implicit nonconvex constraint generated by the optimal value function. We show how to construct an explicit piecewise quadratic function which is support to the optimal value function at a given point. Usage of the support functions allows us to approximate the implicit one-level problem by a number of explicit nonconvex quadratic problems. In our talk we describe an iterative procedure based on such approximation.		Oleg Khamisov	2017		10.1007/978-3-319-89920-6_15	discrete mathematics;mathematics;piecewise;mathematical optimization;quadratic function;quadratic equation;regular polygon;bellman equation;optimization problem	Crypto	72.76394876626806	23.216362564902738	176919
671d221e4c07afdde3c96c4aa7c5134ed35f408e	how to make simpler gmres and gcr more stable	sistema lineal;linear algebra;numerical stability;analisis numerico;formule recursion;systeme grande taille;comportement;metodo subespacio krylov;65f05;conditionnement;numerical method;implementation;krylov subspace method;estabilidad numerica;methode approchee;influencia;methode sousespace krylov;large scale nonsymmetric linear systems;solucion aproximada;metodo aproximado;large scale system;approximate method;krylov subspace methods;conditioning;linear system;rounding errors;analyse numerique;minimo;influence;gmres method;large scale;14c20;sistema coordenadas;65g50;numerical analysis;metodo numerico;conducta;metodo gmres;solution approchee;approximate solution;minimum;algebre lineaire;nonsymmetric linear systems;65f35;methode gmres;algebra lineal;15a12;acondicionamiento;stabilite numerique;error redondear;systeme coordonnee;behavior;systeme lineaire;implementacion;65f10;methode numerique;sistema gran escala;rounding error;minimum residual methods;coordinate system;erreur arrondi	In this paper we analyze the numerical behavior of several minimum residual methods which are mathematically equivalent to the GMRES method. Two main approaches are compared: one that computes the approximate solution in terms of a Krylov space basis from an upper triangular linear system for the coordinates, and one where the approximate solutions are updated with a simple recursion formula. We show that a different choice of the basis can significantly influence the numerical behavior of the resulting implementation. While Simpler GMRES and ORTHODIR are less stable due to the ill-conditioning of the basis used, the residual basis is well-conditioned as long as we have a reasonable residual norm decrease. These results lead to a new implementation, which is conditionally backward stable, and they explain the experimentally observed fact that the GCR method delivers very accurate approximate solutions when it converges fast enough without stagnation.	approximation algorithm;authorization;computation;condition number;experiment;generalized minimal residual method;ghost-canceling reference;karl hessenberg;krylov subspace;linear system;norm (social);numerical analysis;numerical stability;recursion;triangular matrix;whole earth 'lectronic link	Pavel Jiránek;Miroslav Rozlozník;Martin H. Gutknecht	2008	SIAM J. Matrix Analysis Applications	10.1137/070707373	mathematical optimization;numerical analysis;linear algebra;calculus;mathematics;algorithm;algebra	DB	82.80034198500283	20.826470230532365	176993
d1caf30d265ad8c2b62ce991a539b801ec335e66	cublas-aided long vector algorithms		The paper propose vector methods that allow you to use GPU-processors more rationally. The approach is based on using long vectors of arguments instead of short matrix rows. Efficiency of the method is verified by comparisons with a library OpenCurrent.	algorithm	D. G. Vorotnikova;D. L. Golovashkin	2014	J. Math. Model. Algorithms in OR	10.1007/s10852-014-9267-7	econometrics;mathematical optimization;algorithm	Theory	82.17028431477755	25.96103327568703	177006
73581bfad03d33bed17f9646b6ac8a74b0231193	global convergence of a modified spectral fr conjugate gradient method	metodo espectral;analisis numerico;line search;convergence;matematicas aplicadas;mathematiques appliquees;metodo descenso;numerical method;optimization method;global convergence;conjugate gradient method;metodo optimizacion;analyse numerique;objective function;optimization problem;large scale;convergencia;numerical analysis;metodo numerico;metodo gradiente conjugado;spectral method;methode optimisation;unconstrained optimization;methode spectrale;optimizacion sin restriccion;methode gradient conjugue;descent method;applied mathematics;optimisation sans contrainte;methode numerique;methode descente	The nonlinear conjugate gradient method is widely used in unconstrained optimization. However, the line search is very difficult or expensive sometimes. In this paper, we proper a spectral FR method without line search, the global convergence of this method is also given.	line search;local convergence;mathematical optimization;nonlinear conjugate gradient method;nonlinear system	Shou-qiang Du;Yuan-yuan Chen	2008	Applied Mathematics and Computation	10.1016/j.amc.2008.03.020	gradient descent;mathematical optimization;conjugate residual method;numerical analysis;gradient method;calculus;derivation of the conjugate gradient method;mathematics;geometry;conjugate gradient method;nonlinear conjugate gradient method;biconjugate gradient method;line search	ML	77.5814944532147	22.596728371587265	177203
4156e03f7481aa9324eed3fb03631f601abbcb44	new algorithms for solving rfmlrr circulant linear systems in information		In this paper, fast algorithms for solving RFMLrR circulant linear systems are presented by the fast algorithm for computing polynomials. The extended algorithms are used to solve the RLMFrL circulant linear systems. Examples show the effectiveness of the algorithms.	algorithm;circulant matrix		2013		10.1007/978-3-642-53932-9_17	mathematical optimization;combinatorics;theoretical computer science;circulant matrix;mathematics	Crypto	81.2754514287016	21.400984947268643	177394
fe4d5ecb2cc32edb26fd70f3ee3107e802fc08f5	on computing the distance to stability for matrices using linear dissipative hamiltonian systems	dissipative hamiltonian systems;distance to stability;convex optimization	In this paper, we consider the problem of computing the nearest stable matrix to an unstable one. We propose new algorithms to solve this problem based on a reformulation using linear dissipative Hamiltonian systems: we show that a matrix $A$ is stable if and only if it can be written as $A = (J-R)Q$, where $J=-J^T$, $R \succeq 0$ and $Q \succ 0$ (that is, $R$ is positive semidefinite and $Q$ is positive definite). This reformulation results in an equivalent optimization problem with a simple convex feasible set. We propose three strategies to solve the problem in variables $(J,R,Q)$: (i) a block coordinate descent method, (ii) a projected gradient descent method, and (iii) a fast gradient method inspired from smooth convex optimization. These methods require $\mathcal{O}(n^3)$ operations per iteration, where $n$ is the size of $A$. We show the effectiveness of the fast gradient method compared to the other approaches and to several state-of-the-art algorithms.		Nicolas Gillis;Punit Sharma	2017	Automatica	10.1016/j.automatica.2017.07.047	mathematical optimization;combinatorics;discrete mathematics;mathematics	Robotics	74.31516444341702	24.22985289743463	177551
c5815ca1e8d081dcf7fcb0f629015c67ac33c52a	convergence rate analysis of primal-dual splitting schemes	averaged operator;convergence rates;primal dual algorithms;forward backward splitting;fixed point algorithm;65k15;nonexpansive operator;douglas rachford splitting;65k05;90c25;47h05;proximal point algorithm;forward backward forward splitting;peaceman rachford splitting	Primal-dual splitting schemes are a class of powerful algorithms that solve complicated monotone inclusions and convex optimization problems that are built from many simpler pieces. They decompose problems that are built from sums, linear compositions, and infimal convolutions of simple functions so that each simple term is processed individually via proximal mappings, gradient mappings, and multiplications by the linear maps. This leads to easily implementable and highly parallelizable or distributed algorithms, which often obtain nearly state-of-the-art performance. In this paper, we analyze a monotone inclusion problem that captures a large class of primal-dual splittings as a special case. We introduce a unifying scheme and use some abstract analysis of the algorithm to prove convergence rates of the proximal point algorithm, forward-backward splitting, Peaceman-Rachford splitting, and forward-backward-forward splitting applied to the model problem. Our ergodic convergence rates are deduced under variable metrics, stepsizes, and relaxation. Our nonergodic convergence rates are the first shown in the literature. Finally, we apply our results to a large class of primal-dual algorithms that are a special case of our scheme and deduce their convergence rates.	convex optimization;convolution;distributed algorithm;duality (optimization);ergodicity;gradient;linear programming relaxation;map;mathematical optimization;merge sort;monotone polygon;rate of convergence	Damek Davis	2015	SIAM Journal on Optimization	10.1137/151003076	mathematical optimization;combinatorics;mathematical analysis;mathematics	ML	73.62646631913623	24.40121439539722	177617
b68d0b1a4cf7efbbf122e6d4615c6be212d7f524	a sequential quadratically constrained quadratic programming method for differentiable convex minimization	maratos effect;90c55;convex programming;global convergence;65k05;quadratically constrained quadratic programming;90c25;quadratic convergence;quadratically constrained quadratic program	This paper presents a sequential quadratically constrained quadratic programming (SQCQP) method for solving smooth convex programs. The SQCQP method solves at each iteration a subproblem that involves convex quadratic inequality constraints as well as a convex quadratic objective function. Such a quadratically constrained quadratic programming problem can be formulated as a second-order cone program, which can be solved efficiently by using interior point methods. We consider the following three fundamental issues on the SQCQP method: the feasibility of subproblems, the global convergence, and the quadratic rate of convergence. In particular, we show that the Maratos effect is avoided without any modification to the search direction, even though we use an ordinary $\ell_1$ exact penalty function as the line search merit function.	convex optimization;quadratic programming;quadratically constrained quadratic program	Masao Fukushima;Zhi-Quan Luo;Paul Tseng	2003	SIAM Journal on Optimization	10.1137/S1052623401398120	mathematical optimization;combinatorics;mathematical analysis;convex optimization;quadratic residuosity problem;second-order cone programming;quadratically constrained quadratic program;mathematics;sequential quadratic programming;rate of convergence;quadratic programming	ML	74.62045743494812	23.643262867236228	177739
a7790d7efdc8a2adef6584d26f446cf49fd45c30	a smooth newton method for nonlinear programming problems with inequality constraints	newton method;nonlinear programming;kkt conditions	The paper presents a reformulation of the Karush-KuhnTucker (KKT) system associated nonlinear programming problem into an equivalent system of smooth equations. Classical Newton method is applied to solve the system of equations. The superlinear convergence of the primal sequence, generated by proposed method, is proved. The preliminary numerical results with a problems test set are presented.	complementarity theory;emoticon;karush–kuhn–tucker conditions;local convergence;mathematical optimization;newton;newton's method;nonlinear programming;numerical analysis;point of view (computer hardware company);rate of convergence;requirement;smoothing;social inequality;test set	Vasile Moraru	2011	The Computer Science Journal of Moldova		local convergence;mathematical optimization;mathematical analysis;newton fractal;calculus;mathematics	ML	75.10442860917861	23.003465359350034	177764
74d4715551208b6a877b33c41a01afd045475cb9	approximate efficiency for n-set multiobjective fractional programming	e approximate solution;multiobjective fractional programming;n set function;theorem of alternative;e parametric approach	Optimization problems involving set functions are shown (Blaga and Kolumban, 1992; Morris, 1979) to arise in diverse areas including shape optimization, electrical insulator design, crop distribution subject to rainfall, and many more. The solution of optimization problems involving optimal choice of a subset of a given measurable space is not adequately catered for by existing optimization theory. This is partly because the collection of subsets of a vector space fails to possess a linear space structure. This motivated Morris (1979) to develop an optimization theory for problems involving set functions. Subsequently, significant contributions have been made by several authors to the growth of this class of optimization problems, notable among them being the works of Corley (1987), Hsia et al. (1991), Lin (1991) and Bector et al. (1994).	approximation algorithm;convex set;fractional programming;mathematical optimization;minimax;optimization problem;shape optimization;topological insulator	Narender Kumar;R. K. Budhraja;Aparna Mehra	2004	APJOR	10.1142/S0217595904000199	fractional programming;mathematical optimization;combinatorics;mathematical analysis;mathematics	AI	70.56443491022063	21.599801838790214	178030
399f0e34a98f94e3804925ce74fb135b25f914dc	inexact newton methods for inverse eigenvalue problems	sistema lineal;metodo directo;iterative method;analisis numerico;eigenvalue problem;matematicas aplicadas;mathematiques appliquees;methode newton;inexact newton method;relacion convergencia;probleme valeur propre;taux convergence;convergence rate;matrix inversion;problema inverso;inversion matriz;gran sistema;linear system;analyse numerique;metodo iterativo;acceleration convergence;numerical analysis;inverse problem;large system;methode iterative;algebra lineal numerica;algebre lineaire numerique;aceleracion convergencia;inversion matrice;metodo newton;newton method;numerical linear algebra;systeme lineaire;systeme non symetrique;iteration method;applied mathematics;inverse eigenvalue problem;methode directe;jacobien;article;probleme inverse;direct method;grand systeme;problema valor propio;convergence acceleration	In this paper, we survey some of the latest development in using inexact Newton-like methods for solving inverse eigenvalue problems. These methods require the solutions of nonsymmetric and large linear systems. One can solve the approximate Jacobian equation by iterative methods. However, iterative methods usually oversolve the problem in the sense that they require far more (inner) iterations than is required for the convergence of the Newton-like (outer) iterations. The inexact methods can reduce or minimize the oversolving problem and improve the efficiency. The convergence rates of the inexact methods are superlinear and a good tradeoff between the required inner and outer iterations can be obtained. AMS Subject Classifications. 65F18, 65F10, 65F15.	approximation algorithm;iteration;iterative method;jacobian matrix and determinant;linear system;newton;vhdl-ams	Zheng-Jian Bai	2006	Applied Mathematics and Computation	10.1016/j.amc.2004.11.023	mathematical optimization;calculus;mathematics;geometry;iterative method;algorithm;algebra	ML	81.80777541413036	20.471895551709586	178200
dc289eee330043021c862aaebae4f1733a5fed92	asymmetric hermitian and skew-hermitian splitting methods for positive definite linear systems	iterative method;splitting method;krylov subspace method;positive definite;non hermitian positive definite matrix;linear system;hermitian and skew hermitian splitting;positive definite matrix;linear equations;iteration method	In this paper, efficient iterative methods for the large sparse non-Hermitian positive definite systems of linear equations, based on the Hermitian and skew-Hermitian splitting of the coefficient matrix, are studied. These methods include an asymmetric Hermitian/skew-Hermitian (AHSS) iteration and its inexact version, the inexact asymmetric Hermitian/skew-Hermitian (IAHSS) iteration, which employs some Krylov subspace methods as its inner process. We theoretically study the convergence properties of the AHSS method and the IAHSS method. Moreover, the contraction factor of the AHSS iteration is derived. Numerical examples illustrating the effectiveness of both AHSS and IAHSS iterations are presented. c © 2007 Elsevier Ltd. All rights reserved.	coefficient;iteration;iterative method;krylov subspace;linear equation;linear system;numerical method;sparse matrix;system of linear equations	Liang Li;Ting-Zhu Huang;Xing-Ping Liu	2007	Computers & Mathematics with Applications	10.1016/j.camwa.2006.12.024	mathematical optimization;mathematical analysis;jacobi method for complex hermitian matrices;hermitian function;krylov subspace;mathematics;positive-definite matrix;iterative method;algebra	AI	81.45245750924404	21.853251431138474	178329
ef63bada522a38247b39af02a97348449e7bf648	a primal-dual trust-region algorithm for non-convex nonlinear programming	minimisation;second order;quadratic programming;minimization;non convex programming;non linear programming;quadratic program;critical point;nonlinear programming;programmation quadratique;constrenimiento igualdad;programacion no lineal;merit function;primal dual method;programmation non convexe;minimizacion;programmation non lineaire;methode primale duale;fonction objectif;objective function;condicion optimalidad;equality constraint;programacion no convexa;trust region;condition optimalite;metodo primal dual;mathematical programming;funcion objetivo;programacion cuadratica;primal dual algorithm;programmation mathematique;programacion matematica;optimality condition;contrainte egalite	A new primal-dual algorithm is proposed for the minimization of non-convex objective functions subject to general inequality and linear equality constraints. The method uses a primal-dual trust-region model to ensure descent on a suitable merit function. Convergence is proved to second-order critical points from arbitrary starting points. Numerical results are presented for general quadratic programs.	diffusing update algorithm;experiment;image scaling;karush–kuhn–tucker conditions;linear equation;loss function;mathematical optimization;nonlinear programming;nonlinear system;numerical analysis;numerical method;optimization problem;social inequality;technological singularity;trust region	Andrew R. Conn;Nicholas I. M. Gould;Dominique Orban;Philippe L. Toint	2000	Math. Program.	10.1007/s101070050112	minimisation;mathematical optimization;combinatorics;nonlinear programming;mathematics;trust region;critical point;quadratic programming;second-order logic;algorithm	ML	75.06136655393838	22.625368840882537	178388
3665b91de0dff5c7b4949838cadb681872cdee5b	distributed asynchronous computation of fixed points	convergence theorem;shortest path;distributed computing;dynamic program;fixed point;optimization problem;nonlinear system;distributed algorithm;shortest path problem	We present an algorithmic model for distributed computation of fixed points whereby several processors participate simultaneously in the calculations while exchanging information via communication links. We place essentially no assumptions on the ordering of computation and communication between processors thereby allowing for completely uncoordinated execution. We provide a general convergence theorem for algorithms of this type, and demonstrate its applicability to several classes of problems including the calculation of fixed points of contraction and monotone mappings arising in linear and nonlinear systems of equations, optimization problems, shortest path problems, and dynamic programming.	computation;fixed point (mathematics)	Dimitri P. Bertsekas	1983	Math. Program.	10.1007/BF02591967	distributed algorithm;mathematical optimization;combinatorics;discrete mathematics;nonlinear system;mathematics;shortest path problem;k shortest path routing	Logic	72.39016620296789	25.44635005980861	178777
fb89ad45e673d6ae0fe354ee85d8c9ef059e5a3b	mirror descent in non-convex stochastic programming		In this paper, we examine a class of nonconvex stochastic optimization problems which we call variationally coherent, and which properly includes all quasi-convex programs. In view of solving such problems, we focus on the widely used stochastic mirror descent (SMD) family of algorithms, and we establish that the method’s last iterate converges with probability 1. We further introduce a localized version of variational coherence which ensures local convergence of SMD with high probability. These results contribute to the landscape of nonconvex stochastic optimization by showing that quasiconvexity is not essential for convergence: rather, variational coherence, a much weaker requirement, suffices. Finally, building on the above, we reveal an interesting insight regarding the convergence speed of SMD: in variationally coherent problems with sharp minima (e.g. generic linear programs), the last iterate of SMD reaches an exact global optimum in a finite number of steps (a.s.), even in the presence of persistent noise. This result is to be contrasted with existing work on black-box stochastic linear programs which only exhibit asymptotic convergence rates.	algorithm;black box;cache coherence;calculus of variations;coherence (physics);convex optimization;global optimization;iteration;linear programming;local convergence;mathematical optimization;maxima and minima;quasiconvex function;service mapping description;stochastic optimization;stochastic programming;variational principle;with high probability	Zhengyuan Zhou;Panayotis Mertikopoulos;Nicholas Bambos;Stephen P. Boyd;Peter W. Glynn	2017	CoRR		stochastic optimization;mathematics;local convergence;mathematical optimization;stochastic programming;maxima and minima;regular polygon;convergence (routing);coherence (physics);finite set	ML	73.27079730274893	25.725848703314853	179006
b850121945b83f285feca8fa140236d2aad95442	a projection algorithm for general variational inequalities with perturbed constraint sets	analisis numerico;comparative analysis;convergence;matematicas aplicadas;desigualdad variacional;algorithm analysis;mathematiques appliquees;general variational inequality;inegalite variationnelle;projection method;contrainte inegalite;inequality constraint;probleme variationnel;variational problem;monotonie;analyse numerique;convergencia;numerical analysis;methode projection;computer experiment;constrenimiento desigualdad;monotonicity;metodo proyeccion;generalized monotonicity;espace operateur;variational inequality;analyse algorithme;monotonia;point of view;applied mathematics;analisis algoritmo;epiconvergence	We consider the problem of general variational inequalities, GVI, with nonmonotone operator, in a finite dimensional space. We propose a method to solve GVI that at each iteration considers only one projection on an easy approximation of the constraint set, which is important from a practical point of view. We analyse the convergence of the algorithm under a weak cocoercivity condition, using variational metric analysis. Computational experience is reported and comparative analysis with other two algorithms is also given for the monotone case.	algorithm;variational inequality;variational principle	Pedro M. Santos;Susana Scheimberg	2006	Applied Mathematics and Computation	10.1016/j.amc.2006.01.050	qualitative comparative analysis;mathematical optimization;mathematical analysis;variational inequality;computer experiment;convergence;applied mathematics;monotonic function;numerical analysis;calculus;mathematics;projection method	ML	73.95891643476509	21.472664591783335	179044
b389da2d899c819ab6ab799c1a00e2f35ec154d5	successive iterations and positive extremal solutions for a hadamard type fractional integro-differential equations on infinite domain		A Hadamard type fractional integro-differential equation on infinite intervals is considered. By using monotone iterative technique, we not only get the existence of positive solutions, but also seek the positive minimal and maximal solutions and get two explicit monotone iterative sequences which converge to the extremal solutions. At last, to illustrative the main result, an example is also discussed. © 2017 Elsevier Inc. All rights reserved.	converge;iteration;maximal set;monotone	Ke Pei;Guotao Wang;Yanyan Sun	2017	Applied Mathematics and Computation	10.1016/j.amc.2017.05.056	mathematical optimization;mathematical analysis;discrete mathematics;hadamard three-lines theorem;mathematics;hadamard matrix	AI	74.14040057642087	18.6916248502258	179214
d5430b5f72ecd063cd668062430879c93f7ef39b	probability-one homotopy methods for constrained clustering		Abstract Many algorithms for constrained clustering have been developed in the literature that aim to balance vector quantization requirements of cluster prototypes against the discrete satisfaction requirements of constraint (must-link or must-not-link) sets. A significant amount of research has been devoted to designing new algorithms for constrained clustering and understanding when constraints help clustering. However, no method exists to systematically characterize solution sets as constraints are gently introduced and how to assist practitioners in choosing a sweet spot between vector quantization and constraint satisfaction. A homotopy method is presented that can smoothly track solutions from unconstrained to constrained formulations of clustering. Beginning the homotopy zero curve tracking where the solution is (fairly) well-understood, the curve can then be tracked into regions where there is only a qualitative understanding of the solution set, finding multiple local solutions along the way. Experiments demonstrate how the new homotopy method helps identify better tradeoffs and reveals insight into the structure of solution sets not obtainable using pointwise exploration of parameters.	cluster analysis;constrained clustering	David R. Easterling;Layne T. Watson;Naren Ramakrishnan	2018	J. Computational Applied Mathematics	10.1016/j.cam.2018.04.035	mathematical optimization;cluster analysis;vector quantization;homotopy;constrained clustering;mathematics;solution set;pointwise;constraint satisfaction	ML	68.86565644287518	24.7715816995344	179399
050375003defcc74968ea7156c7ef56c4edb9c89	a low-rank coordinate-descent algorithm for semidefinite programming relaxations of optimal power flow	semidefinite programming;power system analysis;optimization	A novel rank-constrained re-formulation of alternating-current optimal power flow problem makes it possible to derive novel semidefinite programming (SDP) relaxations. For those, we develop a solver, which is often as fast as Matpower’s interior point method, within the same accuracy.	algorithm;coordinate descent;flow network;interior point method;semidefinite programming;solver	Jakub Marecek;Martin Takác	2017	Optimization Methods and Software	10.1080/10556788.2017.1288729	large margin nearest neighbor;mathematical optimization;combinatorics;discrete mathematics;mathematics;semidefinite embedding;semidefinite programming	ML	73.87667243505366	27.10333436483548	179433
49ffb3457164d9b2f9e7135599f5d8c1a7c96c7a	an iterative solver-based infeasible primal-dual path-following algorithm for convex quadratic programming	linear algebra;interior point methods;iterative solver;convex quadratic programming;augmented normal equation;satisfiability;linear system;90c20;90c51;degeneration;university of illinois at urbana champaign;iterative linear solver;approximate solution;90c25;condition number;65f35;maximum weight basis preconditioner;linear equations;inexact search directions;interior point method;path following;65f10;polynomial convergence;primal dual path following methods	In this paper we develop a long-step primal-dual infeasible path-following algorithm for convex quadratic programming (CQP) whose search directions are computed by means of a preconditioned iterative linear solver. We propose a new linear system, which we refer to as the augmented normal equation (ANE), to determine the primal-dual search directions. Since the condition number of the ANE coefficient matrix may become large for degenerate CQP problems, we use a maximum weight basis preconditioner introduced in [A. R. L. Oliveira and D. C. Sorensen, Linear Algebra Appl., 394 (2005), pp. 1–24; M. G. C. Resende and G. Veiga, SIAM J. Optim., 3 (1993), pp. 516–537; P. Vaida, Solving Linear Equations with Symmetric Diagonally Dominant Matrices by Constructing Good Preconditioners, Tech. report, Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL, 1990] to precondition this matrix. Using a result obtained in [R. D. C. Monteiro, J. W. O’Neal, and T. Tsuchiya, SIAM J. Optim., 15 (2004), pp. 96–100], we establish a uniform bound, depending only on the CQP data, for the number of iterations needed by the iterative linear solver to obtain a sufficiently accurate solution to the ANE. Since the iterative linear solver can generate only an approximate solution to the ANE, this solution does not yield a primal-dual search direction satisfying all equations of the primal-dual Newton system. We propose a way to compute an inexact primal-dual search direction so that the equation corresponding to the primal residual is satisfied exactly, while the one corresponding to the dual residual contains a manageable error which allows us to establish a polynomial bound on the number of iterations of our method.	approximation algorithm;coefficient;computer science;condition number;diagonally dominant matrix;iteration;linear equation;linear system;newton;numerical linear algebra;ordinary least squares;polynomial;precondition;preconditioner;quadratic programming;solver	Zhaosong Lu;Renato D. C. Monteiro;Jerome W. O'Neal	2006	SIAM Journal on Optimization	10.1137/04060771X	mathematical optimization;combinatorics;discrete mathematics;linear algebra;interior point method;mathematics	Theory	78.57546782789353	24.179331108071032	179486
3f61aa94ebb84b7396961112df489d2717f4a209	polynomial interior point cutting plane methods	analytic center;interior point;convex optimization;cutting plane algorithm;cutting plane methods;linear programming relaxation;nonsmooth optimization;volumetric center;barrier function;cutting plane method	Polynomial cutting plane methods based on the logarithmic barrier function and on the volumetric center are surveyed. These algorithms construct a linear programming relaxation of the feasible region, find an appropriate approximate center of the region, and call a separation oracle at this approximate center to determine whether additional constraints should be added to the relaxation. Typically, these cutting plane methods can be developed so as to exhibit polynomial convergence. The volumetric cutting plane algorithm achieves the theoretical minimum number of calls to a separation oracle. Long-step versions of the algorithms for solving convex optimization problems are presented.	approximation algorithm;barrier function;convex optimization;cutting-plane method;feasible region;integer programming;linear programming relaxation;mathematical optimization;newton;optimization problem;polynomial;term (logic)	John E. Mitchell	2003	Optimization Methods and Software	10.1080/10556780310001607956	mathematical optimization;combinatorics;barrier function;convex optimization;linear programming relaxation;interior point method;mathematics;geometry;cutting-plane method	ML	72.59576816806057	24.823917280493912	179524
5050ccac358f81a7c47b17b663a521821d20659e	on characterization of solution sets of set-valued pseudoinvex optimization problems	normal subdifferential;set valued maps;stampacchia variational like inequality;characterization;solution set	In this paper by using the scalarization method, we consider Stamppachia variational-like inequalities in terms of normal subdifferential for set-valued maps and study their relations with set-valued optimization problems. Furthermore, some characterizations of the solution sets of K-pseudoinvex extremum problems are given.	calculus of variations;map;mathematical optimization;maxima and minima;subderivative	M. Oveisiha;Jafar Zafarani	2014	J. Optimization Theory and Applications	10.1007/s10957-013-0509-z	mathematical optimization;mathematical analysis;solution set;topology;mathematics	Theory	71.88532281682323	20.88719219352888	179657
a72e979a058f32fecd0e8dbcd53d3f5429a1141d	some properties of nonnegative integral matrices			nörlund–rice integral	Joseph Y.-T. Leung;W.-D. Wei	1995	Ars Comb.		combinatorics;mathematics;nonnegative matrix;matrix (mathematics)	Vision	78.62402274497116	20.096355719560115	179682
c78184e7c18e7d14e4c9d859508604f86877901b	a hybrid algorithm for the solution of a single commodity spatial equilibrium model	metodo predictor corrector;methode pivotage;predictor corrector method;methode predicteur correcteur;methode point interieur;equilibrium problem;equilibre spatial;algorithme;algorithm;large scale;metodo punto interior;computer experiment;mathematical programming;pivoting method;probleme complementarite lineaire;equilibre economique;network structure;positive semi definite;linear complementarity problem;interior point method;programmation mathematique;equilibrio economico;programacion matematica;economic equilibrium;hybrid algorithm;metodo pivotaje;algoritmo	In this paper we propose a hybrid algorithm for the solution of a large-scale single commodity spatial equilibrium model. This model can be stated as a Linear Complementarity Problem (LCP) with a singular Symmetric Positive Semi-Definite (SPSD) matrix whose structure is closely related to the network structure of the model. The hybrid scheme is a combination of Predictor-Corrector (PC) and Parametric Principal Pivoting (PPP) algorithms and its implementation takes full advantage of the structure of the matrix of the LCP. We report computational experience on the solution of large-scale spatial equilibrium problems with up to 1000 regions that shows the great efficiency of the approach discussed in this paper.	hybrid algorithm	Luis F. Portugal;Joaquim Júdice	1996	Computers & OR	10.1016/0305-0548(95)00072-0	mathematical optimization;predictor–corrector method;computer experiment;hybrid algorithm;computer science;interior point method;calculus;mathematics;positive-definite matrix;mathematical economics;linear complementarity problem	Vision	76.06623619455081	21.60472994390331	180406
4d05fc1a382b31162a86dfb77c663e8c585e68d9	well-posedness of nonconvex integral functionals	minimisation;calculus of variations;calculo de variaciones;49k40;minimization;49a50;convexite;minimizacion;convexidad;well posedness;sobolev space;nonconvex integrals;calcul variationnel;espacio sobolev;extreme points;extremo;well posed problem;analyse non convexe;integral functional;non convex analysis;convexity;espace sobolev;problema bien planteado;probleme bien pose;variational calculus;extremum;analisis no convexo	We find a sufficient condition guaranteeing well-posedness in a strong sense of the minimization of a multiple integral on the Sobolev space W1,1(Ω; Rm) with boundary datum equal to zero. We remark that this condition does not involve global convexity of the integrand and therefore it allows us to find well-posedness properties of two classes of nonconvex problems recently studied: functionals depending only on the gradient and radially symmetric functionals.	geodetic datum;gradient;well-posed problem	Silvia Villa	2004	Proceedings of the 44th IEEE Conference on Decision and Control	10.1137/S0363012903437289	mathematical optimization;mathematical analysis;topology;mathematics;calculus of variations	Robotics	72.6864066192065	20.61059586142429	180414
62d9e0f48637e3a1fe35a71e598f81ab20b86393	effective results on a fixed point algorithm for families of nonlinear mappings	proof mining;parallel algorithm;strict pseudocontractions;03f10;effective bounds;03f60;asymptotic regularity;47h09;47j25	We use proof mining techniques to obtain a uniform rate of asymptotic regularity for the instance of the parallel algorithm used by Lopez-Acedo and Xu to find common fixed points of finite families of  k -strict pseudocontractive self-mappings of convex subsets of Hilbert spaces. We show that these results are guaranteed by a number of logical metatheorems for classical and semi-intuitionistic systems.	algorithm;fixed point (mathematics);fixed-point iteration;nonlinear system	Andrei Sipos	2017	Ann. Pure Appl. Logic	10.1016/j.apal.2016.09.001	mathematical optimization;combinatorics;mathematical analysis;discrete mathematics;topology;mathematics;parallel algorithm	Logic	73.1324037164766	19.015713813907997	180417
41f74c425fa9f1ceedc942fdd25f989cebb0dfa8	a modified frank-wolfe algorithm for computing minimum-area enclosing ellipsoidal cylinders: theory and algorithms	qa mathematics;minimum area cylinders;minimum volume cylinders;linear convergence;d optimality;dk optimality;frank wolfe algorithm;technical report;minimum volume ellipsoids;d k	We study a first-order method to find the minimum cross-sectional area ellipsoidal cylinder containing a finite set of points. This problem arises in optimal design in statistics when one is interested in a subset of the parameters. We provide convex formulations of this problem and its dual, and analyze a method based on the Frank-Wolfe algorithm for their solution. Under suitable conditions on the behavior of the method, we establish global and local convergence properties. However, difficulties may arise when a certain submatrix loses rank, and we describe a technique for dealing with this situation.	cross-sectional data;cylinder seal;first-order predicate;frank–wolfe algorithm;local convergence;optimal design;rank (j programming language)	Selin Damla Ahipasaoglu;Michael J. Todd	2013	Comput. Geom.	10.1016/j.comgeo.2011.11.004	frank–wolfe algorithm;mathematical optimization;combinatorics;technical report;calculus;mathematics;geometry;rate of convergence	ML	71.58865043569055	24.452739229893854	180615
50bf3da3f990809df9740e528bc597245abbbd0f	a note on minty type vector variational inequalities	optimizacion vectorial;desigualdad variacional;existence of solutions;inegalite variationnelle;solution generalisee;generalized solution;monotonie;primitivo;minty vector variational inequality;90c29;vector optimization 47j20;existencia de solucion;monotonicity;existence of solution;optimisation vectorielle;variational inequality;vector optimization;primitif;monotonia;increasing along rays property;primitive;49j52;existence solution	The existence of solutions to a scalar Minty variational inequality of differential type is usually related to monotonicity property of the primitive function. On the other hand, solutions of the variational inequality are global minimizers for the primitive function. The present paper generalizes these results to vector variational inequalities putting the Increasing Along Rays (IAR) property into the center of the discussion. To achieve that infinite elements in the image space Y are introduced. Under quasiconvexity assumptions we show that solutions of generalized variational inequality and of the primitive optimization problem are equivalent. Finally, we discuss the possibility to generalize the scheme of this paper to get further applications in vector optimization.	calculus of variations;mathematical optimization;optimization problem;quasiconvex function;social inequality;variational inequality;variational principle;vector optimization	Giovanni P. Crespi;Ivan Ginchev;Matteo Rocca	2005	RAIRO - Operations Research	10.1051/ro:2006005	mathematical optimization;mathematical analysis;variational inequality;monotonic function;calculus;mathematics;vector optimization	ML	72.28452420584975	21.13806741821402	181064
83e7f05dd0989db6f8ace47d5340793a85f3ef7f	a combined preconditioning strategy for nonsymmetric systems	65n30;additive schwarz method;preconditioning;65n20;mathematics computing and information science preconditioning;nonsymmetric matrices;normal matrix form;65f10	We present and analyze a class of nonsymmetric preconditioners within a normal (weighted least-squares) matrix form for use in GMRES to solve nonsymmetric matrix problems that typically arise in finite element discretizations. An example of the additive Schwarz method applied to nonsymmetric but definite matrices is presented for which the abstract assumptions are verified. A variable preconditioner, combining the original nonsymmetric one and a weighted leastsquares version of it, is shown to be convergent and provides a viable strategy for using nonsymmetric preconditioners in practice. Numerical results are included to assess the theory and the performance of the proposed preconditioners.	additive schwarz method;finite element method;generalized minimal residual method;least squares;preconditioner;utility functions on indivisible goods	Blanca Ayuso de Dios;Andrew T. Barker;Panayot S. Vassilevski	2014	SIAM J. Scientific Computing	10.1137/120888946	mathematical optimization;additive schwarz method;calculus;mathematics;preconditioner;algebra	HPC	82.00203159693174	21.95433238412547	181107
e4b161d389df31efc26295a59f614845a1340895	stochastic reformulations of linear systems: algorithms and convergence theory		We develop a family of reformulations of an arbitrary consistent linear system into a stochastic problem. The reformulations are governed by two user-defined parameters: a positive definite matrix defining a norm, and an arbitrary discrete or continuous distribution over random matrices. Our reformulation has several equivalent interpretations, allowing for researchers from various communities to leverage their domain specific insights. In particular, our reformulation can be equivalently seen as a stochastic optimization problem, stochastic linear system, stochastic fixed point problem and a probabilistic intersection problem. We prove sufficient, and necessary and sufficient conditions for the reformulation to be exact. Further, we propose and analyze three stochastic algorithms for solving the reformulated problem— basic, parallel and accelerated methods—with global linear convergence rates. The rates can be interpreted as condition numbers of a matrix which depends on the system matrix and on the reformulation parameters. This gives rise to a new phenomenon which we call stochastic preconditioning, and which refers to the problem of finding parameters (matrix and distribution) leading to a sufficiently small condition number. Our basic method can be equivalently interpreted as stochastic gradient descent, stochastic Newton method, stochastic proximal point method, stochastic fixed point method, and stochastic projection method, with fixed stepsize (relaxation parameter), applied to the reformulations. ∗All theoretical results in this paper were obtained by August 2016 and a first draft was circulated to a few selected colleagues in September 2016. The first author gave several talks on these results before this draft was made publicly available on arXiv: Linear Algebra and Parallel Computing at the Heart of Scientific Computing, Edinburgh, UK (Sept 21, 2016), Seminar on Combinatorics, Games and Optimisation, London School of Economics, London, UK, (Nov 16, 2016), Workshop on Distributed Machine Learning, Télécom ParisTech, Paris, France (Nov 25, 2016), Skoltech Seminar, Moscow, Russia (Dec 1, 2016), BASP Frontiers Workshop, Villars-sur-Ollon, Switzerland (Feb 1, 2017), and SIAM Conference on Optimization, Vancouver, Canada (May 22, 2017). In addition, the first author has included the results of this paper in the MSc/PhD course Modern optimization methods for big data problems, delivered in Spring 2017 at the University of Edinburgh, as an introduction into the role of randomized decomposition in linear algebra, optimization and machine learning. All main results of this paper were distributed to the students in the form of slides. †KAUST and University of Edinburgh ‡Lehigh University 1 ar X iv :1 70 6. 01 10 8v 2 [ m at h. N A ] 6 J un 2 01 7	big data;condition number;first draft of a report on the edvac;fixed point (mathematics);fixed-point iteration;lagrangian relaxation;linear algebra;linear programming relaxation;linear system;machine learning;mathematical optimization;newton's method;optimization problem;parallel computing;preconditioner;randomized algorithm;rate of convergence;shadow volume;stochastic gradient descent;stochastic optimization;switzerland	Peter Richtárik;Martin Takác	2017	CoRR		mathematical analysis;mathematical optimization;mathematics;stochastic optimization;discrete mathematics;stochastic control;linear system;condition number;matrix (mathematics);algorithm;stochastic gradient descent;continuous-time stochastic process;stochastic approximation	ML	80.27836349013832	25.664050293402184	181144
69af0bd3a1893c191957aa06d8e591437c0f861e	modified alternating direction method of multipliers for convex quadratic semidefinite programming	convergence analysis;quadratic semidefinite programming;alternating direction method of multipliers;nearest correlation matrix problems	The dual form of convex quadratic semidefinite programming (CQSDP) problem, with nonnegative constraints on the matrix variable, is a 4-block convex optimization problem. It is known that, the directly extended 4-block alternating direction method of multipliers (ADMM) is very efficient to solve this dual, but its convergence are not guaranteed. In this paper, we reformulate it as a 3-block convex programming by introducing an extra variable, and propose a new modified ADMM to solve it. At each iteration, the proposed method does not have to work out the sub-problem with the primal variable, compared with the existing ADMM methods. This confirms that at least our methods require less computation than the existing ADMM in one iteration. Under satisfying a condition on the penalty parameter, the convergence of modified ADMM to a KKT point is proved by using a fixed-point argument. Numerical experiments on the various classes of CQSDP problems (including least squares semidefinite programming (LSSDP)) show that, our proposed algorithm performs comparable or slightly better than the directly extended 4-block ADMM with unit step-length.	algorithm;augmented lagrangian method;computation;convex optimization;experiment;fixed point (mathematics);fixed-point arithmetic;iteration;karush–kuhn–tucker conditions;least squares;mathematical optimization;numerical linear algebra;optimization problem;semidefinite programming;the matrix	Xiaokai Chang;Sanyang Liu;Xu Li	2016	Neurocomputing	10.1016/j.neucom.2016.06.043	mathematical optimization;combinatorics;mathematical analysis;quadratically constrained quadratic program;mathematics;quadratic programming	ML	74.9700242177238	24.77855592989437	181587
5b0f408b07474fe6ad08c64040793d84af97d21b	nonsmooth constrained optimization and multidirectional mean value inequalities	constrained optimization;fermat rule;subdifferential;49k27;constrained variational principle;multidirectional mean value inequality;49j45;49j52;optimality condition;smooth norms	"""We establish a general Fermat rule for the problem of minimizing a lower semicontinuous function on a convex subset of a Banach space. Our basic tool is a constrained variational principle derived from the """"smooth"""" variational principle of Borwein and Preiss. Specializing the Fermat rule to the case when the convex set is a """"drop,"""" we obtain a multidirectional Rolle-type inequality from which, in turn, we deduce a multidirectional mean value inequality, in the line of Clarke and Ledyaev. We follow the abstract approach of our previous paper [Trans. Amer. Math. Soc., 347 (1995), pp. 4147--4161], thus covering all standard situations met in applications, while stressing the links between the results and the few key properties that are needed."""	constrained optimization	Didier Aussel;Jean-Noël Corvellec;Marc Lassonde	1999	SIAM Journal on Optimization	10.1137/S105262349732339X	subderivative;mathematical optimization;constrained optimization;mathematical analysis;calculus;mathematics	Theory	71.28300253957417	21.218418130744425	181655
017c630fbb428fa94ad189253e1e29980fafa533	a condition number analysis of an algorithm for solving a system of polynomial equations with one degree of freedom	polynomial system;degree of freedom;newton s method;subdivision method;65d02;condition number;condition number analysis	This article considers the problem of solving a system of n real polynomial equations in n+1 variables. We propose an algorithm based on Newton’s method and subdivision for this problem. Our algorithm is intended only for nondegenerate cases, in which case the solution is a 1-dimensional curve. Our first main contribution is a definition of a condition number measuring reciprocal distance to degeneracy that can distinguish poor and well conditioned instances of this problem. (Degenerate problems would be infinitely ill conditioned in our framework.) Our second contribution, which is the main novelty of our algorithm, is an analysis showing that its running time is bounded in terms of the condition number of the problem instance as well as n and the polynomial degrees.	algorithm;approximation;computation;condition number;degeneracy (graph theory);experiment;like button;newton;newton's method;occam's razor;rate of convergence;round-off error;subdivision surface;system of polynomial equations;time complexity;well-posed problem	Gun Srijuntongsiri;Stephen A. Vavasis	2011	SIAM J. Scientific Computing	10.1137/090780547	mathematical optimization;combinatorics;mathematical analysis;condition number;calculus;degree of a polynomial;mathematics;newton's method;degrees of freedom;algebra	Theory	76.33102870111762	18.258874319886914	181720
7b31b268ab6eef2e5ea672b0d42394a973fb2c3f	a new algorithm for computing eigenpairs of matrices	equation non lineaire;calculo de variaciones;computer aided analysis;ecuacion no lineal;analisis numerico;matriz no simetrica;ecuacion trascendente;methode particulaire;matematicas aplicadas;analyse assistee;modele mathematique;mathematiques appliquees;stochastic method;restarted arnoldi method;ecuacion algebraica;metodo arnoldi;65kxx;matrice non symetrique;equation transcendante;optimization method;metodo particula;modelo matematico;65k10;eigenvalues;eigenvalues and eigenvectors;metodo optimizacion;eigenvector;arnoldi method;analyse numerique;eigenvalue;algorithme;49xx;particle method;vector propio;algorithm;calcul variationnel;numerical analysis;particle swarm optimizer;mathematical programming;transcendental equation;particle swarm optimization;valor propio;algebra lineal numerica;algebre lineaire numerique;methode optimisation;mathematical model;methode stochastique;analisis asistido;equation algebrique;valeur propre;numerical linear algebra;non linear equation;applied mathematics;nonsymmetric matrix;non symmetrical matrix;programmation mathematique;65c35;algebraic equation;65f15;programacion matematica;65h17;vecteur propre;variational calculus;methode arnoldi;eigenvectors;algoritmo;metodo estocastico	In this paper, we have proposed a hybrid of restarted Arnoldi algorithm and particle swarm optimization (PSO) method for calculating eigenvalues and eigenvectors of a nonsymmetric matrix which is called the PSO-RA algorithm. Numerical examples are used to show the good numerical properties.	algorithm	Hossein Aminikhah;Ali Jamalian	2011	Mathematical and Computer Modelling	10.1016/j.mcm.2011.01.043	mathematical optimization;eigenvalues and eigenvectors;calculus;mathematics;algebra;arnoldi iteration	Vision	82.06949550916914	18.688483508319536	181800
96a3dc9873cb630d0a1f1eac529d77d910e8fa73	"""erratum to """"nontrivial solution of third-order nonlinear eigenvalue problems"""" [appl. math. comput. 176(2006) 714-721]"""	nonlinear eigenvalue problem	where k > 0 is a parameter, 1 2 6 g < 1 is a constant, f : 1⁄20; 1 R R! R is continuous, R 1⁄4 ð 1;þ1Þ. The nonlinear term is involved explicitly with the first-order derivative. Particularly, the paper [2] does not need any monotonicity and nonnegative assumptions on f , which was essential for the technique used in many existed literature. The authors’ approach is based on Leray–Schauder nonlinear alternative. However, there is a fundamental error in the paper [2] which is presented below: Lemma 2.2 used by the authors is Let X be a real Banach space, X be a bounded open subset of X , 0 2 X, T : X! X is a completely continuous operator. Then, either there exists x 2 @X, l > 1 such that T ðxÞ 1⁄4 lx, or there exists a fixed point x 2 oX. Lemma 2.2 is wrong. Obviously, the correct form [1] of this is Let X be a real Banach space, X be a bounded open subset of X , 0 2 X, T : X! X is a completely continuous operator. Then, either there exists x 2 oX; l > 1 such that T ðxÞ 1⁄4 lx, or there exists a fixed point x 2 X.	farkas' lemma;first-order predicate;fixed point (mathematics);nonlinear system	Yingxin Guo	2008	Applied Mathematics and Computation	10.1016/j.amc.2007.10.058	mathematics	AI	75.75165632690118	18.273625231566488	182250
17082b0417e1d6fe0a9345a090949cdac8f73ebb	solution of linear inequalities	linear systems;inequalities;satisfiability;power method;numerical analysis;mathematical programming;linear programming;pattern recognition;linear program;algorithms;classifiers;performance prediction;optimization;algorithms classifiers linear inequalities linear programming mathematical programming numerical analysis optimization pattern recognition;linear inequalities	A method for solving systems of linear inequalities, consistent and inconsistent, corresponding to the separable and nonseparable cases in pattern recognition is presented. Attempts are made to evaluate the speed and efficiency of the algorithm. It seems to compare in speed with the best algorithms for consistent systems of inequalities in the consistent case and retains remarkable speed in the inconsistent case. There are indications that in many cases it may be relied on to find a complete solution if several runs are taken, most runs achieving a nearly complete solution. The experimental evidence suggests that this is the most efficient and powerful method currently available for finding a solution which satisfies as many cases as possible in a set of linear inequalities.	algorithm;linear inequality;pattern recognition	Peter H. Mengert	1970	IEEE Transactions on Computers	10.1109/T-C.1970.222877	mathematical optimization;combinatorics;discrete mathematics;basic solution;criss-cross algorithm;second-order cone programming;power iteration;linear-fractional programming;numerical analysis;linear inequality;linear programming;inequality;mathematics;linear system;algorithm;satisfiability	Theory	77.9935334331026	24.90380074261663	182458
e065a7b1e83978c9b84645e98622274d97b0608c	a proximal admm for decentralized composite optimization		In this letter, we propose a proximal alternating direction method of multiplier (ADMM) to solve the composite optimization problem over a decentralized network. Compared with existing methods, such as PG-EXTRA and IC-ADMM, the proposed decentralized proximal ADMM method does not rely on assuming a smooth + nonsmooth structure on the objective functions, thus covering a wider range of composite optimization problems. Simulation results show that the proposed proximal ADMM presents a considerable performance advantage over existing state-of-the-art algorithms for both nonsmooth + nonsmooth and smooth + nonsmooth composite optimization problems.	algorithm;mathematical optimization;optimization problem;simulation	Bin Wang;Hongyu Jiang;Jun Fang;Huiping Duan	2018	IEEE Signal Processing Letters	10.1109/LSP.2018.2841648	mathematical optimization;mathematics;composite number	DB	75.07450258518021	24.94396794197362	182473
eaf8c775617075a342414e6798de1b62663c8261	fast polynomial transforms based on toeplitz and hankel matrices		Many standard conversion matrices between coefficients in classical orthogonal polynomial expansions can be decomposed using diagonally-scaled Hadamard products involving Toeplitz and Hankel matrices. This allows us to derive O(N(logN)) algorithms, based on the fast Fourier transform, for converting coefficients of a degree N polynomial in one polynomial basis to coefficients in another. Numerical results show that this approach is competitive with state-of-the-art techniques, requires no precomputational cost, can be implemented in a handful of lines of code, and is easily adapted to extended precision arithmetic.	coefficient;computation;extended precision;fast fourier transform;numerical method;polynomial basis;source lines of code;time complexity;toeplitz hash algorithm	Alex Townsend;Marcus Webb;Sheehan Olver	2018	Math. Comput.	10.1090/mcom/3277	mathematical analysis;discrete mathematics;mathematics;matrix polynomial;reciprocal polynomial;algebra	Theory	81.93818866221514	21.437569038846124	182589
3f6bb3f688a3378a1bc7f7071faa4e94f39f38f8	distance majorization and its applications	health research;constrained optimization;uk clinical guidelines;biological patents;majorization minimization mm;europe pubmed central;62j02;citation search;90c30;sequential unconstrained minimization;65k05;uk phd theses thesis;projection;90c25;life sciences;uk research reports;medical journals;europe pmc;biomedical research;bioinformatics	The problem of minimizing a continuously differentiable convex function over an intersection of closed convex sets is ubiquitous in applied mathematics. It is particularly interesting when it is easy to project onto each separate set, but nontrivial to project onto their intersection. Algorithms based on Newton's method such as the interior point method are viable for small to medium-scale problems. However, modern applications in statistics, engineering, and machine learning are posing problems with potentially tens of thousands of parameters or more. We revisit this convex programming problem and propose an algorithm that scales well with dimensionality. Our proposal is an instance of a sequential unconstrained minimization technique and revolves around three ideas: the majorization-minimization principle, the classical penalty method for constrained optimization, and quasi-Newton acceleration of fixed-point algorithms. The performance of our distance majorization algorithms is illustrated in several applications.		Eric C. Chi;Hua Zhou;Kenneth Lange	2014	Mathematical programming	10.1007/s10107-013-0697-1	mathematical optimization;constrained optimization;projection;computer science;theoretical computer science;data mining;mathematics	ML	74.77297146406096	25.959250157536264	182737
235d5967a6d33c93387efb83a6b36faa333b2992	on randomized lanczos algorithms	power series;lanczos algorithm;integer factorization;linear system;symmetric matrix;algorithm;multiplication	Las Vegas algorithms that are basedon Lanczos’s method for solving symmetric linear systems are presented and analyzed. These arecompared toasimilar randomized Lanczos algorithm that has been used for integer factorization, and to the (provably reliable) algorithm of Wiedemann. The analysis suggests that our Lanczos algorithms are preferable to several versions of Wiedemann’s method for computations over large fields, especially for certain symmetric matrix computations.	computation;integer factorization;lanczos algorithm;las vegas algorithm;linear system;randomized algorithm	Wayne Eberly;Erich Kaltofen	1997		10.1145/258726.258776	dixon's factorization method;mathematical optimization;combinatorics;lanczos algorithm;lanczos approximation;integer factorization;mathematics;linear system;power series;multiplication;factor base;symmetric matrix;algebra	HPC	82.23115246802942	24.28325815497658	182762
9bbe867dbe19d7e0e4100c6c94778a18da885146	rank equalities and inequalities for kronecker products of matrices with applications	linear algebra;matriz bloque;produit kronecher;matematicas aplicadas;mathematiques appliquees;egalite rang;dimension;upper bound;matrice bloc;algebre lineaire;transformation lineaire;borne inferieure;linear transformation;algebra lineal;block matrix;upper and lower bounds;borne superieure;applied mathematics;kronecker product;lower bound;rank equality;transformacion lineal;gamme;cota superior;cota inferior;range	Some rank equalities and inequalities are established for Kronecker products of matrices. As applications, upper and lower bounds are presented for the dimensions of the ranges of the two well-known linear transformations T1ðX Þ 1⁄4 X AXB and T2ðX Þ 1⁄4 AX XB. 2003 Elsevier Inc. All rights reserved.	microsoft dynamics ax	Jianjun Chuai;Yongge Tian	2004	Applied Mathematics and Computation	10.1016/S0096-3003(03)00203-0	combinatorics;mathematical analysis;linear algebra;mathematics;geometry;kronecker product;upper and lower bounds;algebra	Theory	78.48083370140218	19.68119674161616	182830
d0e0250218e6e489120b87cc5323ce30a6500a61	a survey on inverse and generalized inverse eigenvalue problems for jacobi matrices	lanczos algorithm;unicidad solucion;analisis numerico;matrice jacobi;eigenvalue problem;matematicas aplicadas;mathematiques appliquees;solution uniqueness;probleme valeur propre;problema inverso;47b36;analyse numerique;tridiagonal matrix;algorithme;algorithm;jacobi matrix;unicite solution;1509;numerical analysis;inverse problem;matriz jacobi;matriz tridiagonal;generalized inverse;applied mathematics;inverse eigenvalue problem;jacobi matrices;probleme inverse;matrice tridiagonale;problema valor propio;generalized inverse eigenvalue problem;algoritmo	In this paper, we give a survey of well-known works of constructing an specific tridiagonal matrix using prescribed data. Then we focus on two well-known methods namely, m-functions and Lanczos algorithm. We describe the first method for a generalized inverse eigenvalue problem and the second one for a classic inverse eigenvalue problem. We show that these two methods are linked together. In both method we prove that the solution is unique.	gauss–jacobi quadrature;jacobi method	K. Ghanbari	2008	Applied Mathematics and Computation	10.1016/j.amc.2007.05.035	jacobian matrix and determinant;divide-and-conquer eigenvalue algorithm;tridiagonal matrix;mathematical optimization;mathematical analysis;lanczos algorithm;generalized inverse;numerical analysis;inverse problem;calculus;inverse iteration;mathematics;algebra	ML	81.5865215390716	20.076145341851415	182861
3cf48820738f7bd817a0529ae2d5060888614a40	viscosity approximation methods for generalized equilibrium problems and fixed point problems	strong convergence;general equilibrium;approximation method;viscosity approximation method;fixed point;hilbert space;generalized equilibrium problem;nonexpansive mapping;existence and uniqueness;fixed points;nonexpansive mappings	The purpose of this paper is to investigate the problem of finding a common element of the set of solutions of a generalized equilibrium problem (for short, GEP) and the set of fixed points of a nonexpansive mapping in the setting of Hilbert spaces. By using well-known Fan-KKM lemma, we derive the existence and uniqueness of a solution of the auxiliary problem for GEP. On account of this result and Nadler’s theorem, we propose an iterative scheme by the viscosity approximation method for finding a common element of the set of solutions of GEP and the set of fixed points of a nonexpansive mapping. Furthermore, it is proven that the sequences generated by this iterative scheme converge strongly to a common element of the set of solutions of GEP and the set of fixed points of a nonexpansive mapping.	approximation;converge;farkas' lemma;fixed point (mathematics);gene expression programming;hilbert space;iterative method	Lu-Chuan Ceng;Qamrul Hasan Ansari;Jen-Chih Yao	2009	J. Global Optimization	10.1007/s10898-008-9342-6	mathematical optimization;mathematical analysis;discrete mathematics;general equilibrium theory;mathematics;fixed point	Theory	73.05910286381535	22.447649835718195	182896
6e07cd9fb38c19b85ae54af22133bb5d35afb78c	local and global convergence of a general inertial proximal splitting scheme	fista;momentum methods;local linear convergence;inertial forward backward splitting;inertial proximal gradient;lasso	This paper is concerned with convex composite minimization problems in a Hilbert space. In these problems, the objective is the sum of two closed, proper, and convex functions where one is smooth and the other admits a computationally inexpensive proximal operator. We analyze a general family of inertial proximal splitting algorithms (GIPSA) for solving such problems. We establish finiteness of the sum of squared increments of the iterates and optimality of the accumulation points. Weak convergence of the entire sequence then follows if the minimum is attained. Our analysis unifies and extends several previous results. We then focus on `1-regularized optimization, which is the ubiquitous special case where the nonsmooth term is the `1-norm. For certain parameter choices, GIPSA is amenable to a local analysis for this problem. For these choices we show that GIPSA achieves finite “active manifold identification”, i.e. convergence in a finite number of iterations to the optimal support and sign, after which GIPSA reduces to minimizing a local smooth function. Local linear convergence then holds under certain conditions. We determine the rate in terms of the inertia, stepsize, and local curvature. Our local analysis is applicable to certain recent variants of the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA), for which we establish active manifold identification and local linear convergence. Our analysis motivates the use of a momentum restart scheme in these FISTA variants to obtain the optimal local linear convergence rate. ? This manuscript is related to the preprint arxiv:1502.02281, entitled: “A Lyapunov Analysis of FISTA with Local Linear Convergence for Sparse Optimization”. This manuscript is a thoroughly revised and rewritten version which includes new material and several new results. Patrick R. Johnstone (B) · Pierre Moulin Coordinated Science Laboratory, University of Illinois, Urbana, IL 61801, USA e-mail: prjohns2@illinois.edu, moulin@ifp.uiuc.edu 2 Patrick R. Johnstone, Pierre Moulin	algorithm;ampersand;convex function;email;fast fourier transform;hilbert space;iteration;lyapunov fractal;mathematical optimization;proximal operator;rate of convergence;sparse approximation;thresholding (image processing);tree accumulation	Patrick R. Johnstone;Pierre Moulin	2017	Comp. Opt. and Appl.	10.1007/s10589-017-9896-7	mathematical optimization;mathematical analysis;lasso;mathematics;geometry	ML	74.8175287646965	25.06820518434386	182968
82995b9e15b8bb2b79861040f067cec7b7f8e92f	the convergence guarantee of the iterative thresholding algorithm with suboptimal feedbacks for large systems		Thresholding based iterative algorithms have the trade-off between effectiveness and optimality. Some are effective but involving sub-matrix inversions in every step of iterations. For systems of large sizes, such algorithms can be computationally expensive and/or prohibitive. The null space tuning algorithm with hard thresholding and feedbacks (NST+HT+FB) has a mean to expedite its procedure by a suboptimal feedback, in which sub-matrix inversion is replaced by an eigenvalue-based approximation. The resulting suboptimal feedback scheme becomes exceedingly effective for large system recovery problems. An adaptive algorithm based on thresholding, suboptimal feedback and null space tuning (AdptNST+HT+subOptFB) without a prior knowledge of the sparsity level is also proposed and analyzed. Convergence analysis is the focus of this article. Numerical simulations are also carried out to demonstrate the superior efficiency of the algorithm compared with state-of-the-art iterative thresholding algorithms at the same level of recovery accuracy, particularly for large systems.	adaptive algorithm;analysis of algorithms;approximation;calculus of variations;feedback;inversion (discrete mathematics);iteration;kernel (linear algebra);numerical analysis;numerical method;simulation;sparse matrix;the matrix;thresholding (image processing)	Zhanjie Song;Shidong Li;Ningning Han	2017	CoRR		adaptive algorithm;kernel (linear algebra);mathematical optimization;mathematics;eigenvalues and eigenvectors;algorithm;inversion (meteorology);thresholding;convergence (routing)	ML	79.43403632681026	24.29057914995954	183138
1c7d8930cc3275ed50e4f7e624c88bc12a41913b	a new stabilization strategy for incomplete lu preconditioning of indefinite matrices	linear algebra;preconditionnement;barth matrix;analisis numerico;incomplete factorization;matematicas aplicadas;mathematiques appliquees;shifted incomplete factorization;factorisation incomplete decalee;incomplete lu factorization;preconditioning;analyse numerique;matrice creuse;numerical analysis;algebre lineaire;algebra lineal;precondicionamiento;sparse matrix;numerical experiment;applied mathematics;factorisation lu incomplete;matrice barth;matriz dispersa	A two step combined preconditioning strategy is proposed to construct stable and accurate incomplete LU factorization of indefinite matrices arising from CFD applications. This preconditioning procedure is divided into two steps, each step is a factorization of a shifted matrix. Numeral experiments show that a preconditioner can be computed with high accuracy and low fill-in, and that the new strategy is robust on some difficult indefinite matrix test problems. 2002 Elsevier Science Inc. All rights reserved.	experiment;incomplete lu factorization;lu decomposition;preconditioner	Li Wang;Jun Zhang	2003	Applied Mathematics and Computation	10.1016/S0096-3003(02)00392-2	sparse matrix;incomplete lu factorization;numerical analysis;linear algebra;calculus;mathematics;preconditioner;algorithm;algebra	AI	82.8613520104819	20.8544526981053	183173
57faab284b6011a919f98754adf826154463f3bf	parallel synchronous algorithm for nonlinear fixed point problems	convex programming;functional calculus;fixed point;numerical analysis;convex analysis;saddle point	We give in this paper a convergence result concerning parallel synchronous algorithm for nonlinear fixed point problems with respect to the euclidian norm in R. We then apply this result to some problems related to convex analysis like minimization of functionals, calculus of saddle point, convex programming...	algorithm;convex analysis;convex optimization;fixed point (mathematics);nonlinear system	Ahmed Addou;Abdenasser Benahmed	2005	Int. J. Math. Mathematical Sciences	10.1155/IJMMS.2005.3175	convex analysis;subderivative;mathematical optimization;combinatorics;discrete mathematics;convex optimization;functional calculus;convex polytope;pseudoconvex function;schauder fixed point theorem;convex combination;linear matrix inequality;numerical analysis;convex hull;absolutely convex set;mathematics;fixed point;saddle point;convex set;proper convex function	Theory	72.81404098262807	21.382623401967468	183254
c20538979d16212c13eeb982aa693366082b37c0	the generalized tykhonov well-posedness for system of vector quasi-equilibrium problems	approximating solution sequence;equilibrium problem;system of vector quasi equilibrium problems;generalized tykhonov well posedness;approximate solution;gap function	In this paper, the notion of the generalized Tykhonov well-posedness for system of vector quasi-equilibrium problems are investigated. By using the gap functions of the system of vector quasi-equilibrium problems, we establish the equivalent relationship between the generalized Tykhonov well-posedness of the system of vector quasi-equilibrium problems and that of the minimization problems. We also present some metric characterizations for the generalized Tykhonov well-posedness of the system of vector quasi-equilibrium problems. The results in this paper are new and extend some known results in the literature.	well-posed problem	Jian-Wen Peng;Soon-Yi Wu	2010	Optimization Letters	10.1007/s11590-010-0179-9	simulation;control theory	Theory	71.87639611730914	21.10778899936116	183403
ee9aa904264ad7b7dc433d0a2972a31e4ca0f076	variational analysis in semi-infinite and infinite programming, i: stability of linear inequality systems of feasible solutions	semi infinite and infinite programming;49j53;robust stability;90c34;coderivatives;variational analysis;65f22;generalized differentiation;90c05;linear infinite inequality systems;49j52	This paper concerns applications of advanced techniques of variational analysis and generalized differentiation to parametric problems of semi-infinite and infinite programming, where decision variables run over finite-dimensional and infinite-dimensional spaces, respectively. Part I is primarily devoted to the study of robust Lipschitzian stability of feasible solutions maps for such problems described by parameterized systems of infinitely many linear inequalities in Banach spaces of decision variables indexed by an arbitrary set T. The parameter space of admissible perturbations under consideration is formed by all bounded functions on T equipped with the standard supremum norm. Unless the index set Tis finite, this space is intrinsically infinite-dimensional (nonreflexive and nonseparable) of the l00-type. By using advanced tools of variational analysis and exploiting specific features of linear infinite systems, we establish complete characterizations of robust Lipschitzian · -st-abi1ity--entirely-v1a-tnelrinit1ah:lata·with-c-omputirrg·the exact·bnund-of bipschitzian-moduli;··-A· -· crucial part of our analysis addresses the precise computation of the coderivative of the feasible set mapping and its norm. The results obtained are new in both semi-infinite and infinite frameworks.	computation;decision theory;feasible region;linear inequality;semiconductor industry;social inequality;variational analysis	María J. Cánovas;Marco A. López;Boris S. Mordukhovich;Juan Parra	2009	SIAM Journal on Optimization	10.1137/090765948	mathematical optimization;mathematical analysis;discrete mathematics;variational analysis;mathematics	Theory	70.70982324537798	21.109975876234106	183607
4b4a32d0e9ba90691b8ef4f973db4ceb3f40b490	necessary conditions for super minimizers in constrained multiobjective optimization	49j53;super efficiency and super minimizers;90c29;variational analysis;generalized differentiation;multiobjective optimization;nonsmooth and multiobjective optimization;49j52;necessary optimality condition;necessary optimality conditions	This paper concerns the study of the so-called super minimizers related to the concept of super efficiency in constrained problems of multiobjective optimization, where cost mappings are generally set-valued. We derive necessary conditions for super minimizers on the base of advanced tools of variational analysis and generalized differentiation that are new in both finite-dimensional and infinite-dimensional settings for problems with single-valued and set-valued objectives.	mathematical optimization;multi-objective optimization;variational analysis	Truong Q. Bao;Boris S. Mordukhovich	2009	J. Global Optimization	10.1007/s10898-008-9336-4	mathematical optimization;variational analysis;multi-objective optimization;mathematics;mathematical economics	ML	71.70011011466791	21.445536815262873	183998
15889f55d02563acfd2d5991943b7b17e200861a	modified newton-nss method for solving systems of nonlinear equations	normal and skew-hermitian splitting;modified newton-nss method;successive overrelaxation;modified newton-snss method	By making use of the normal and skew-Hermitian splitting (NSS) method as the inner solver for the modified Newton method, we establish a class of modified Newton-NSS method for solving large sparse systems of nonlinear equations with positive definite Jacobian matrices at the solution points. Under proper conditions, the local convergence theorem is proved. Furthermore, the successive-overrelaxation (SOR) technique has been proved quite successfully in accelerating the convergence rate of the NSS or the Hermitian and skew-Hermitian splitting (HSS) iteration method, so we employ the SOR method in the NSS iteration, and we get a new method, which is called modified Newton SNSS method. Numerical results are given to examine its feasibility and effectiveness.	high-speed serial interface;iteration;jacobian matrix and determinant;local convergence;newton;newton's method;nonlinear system;numerical method;rate of convergence;solver;sparse matrix;successive over-relaxation	Ping-Fei Dai;Qing-Biao Wu;Min-Hong Chen	2017	Numerical Algorithms	10.1007/s11075-017-0301-5	mathematical optimization;mathematical analysis;calculus;mathematics	EDA	80.50518865391084	22.31564244617602	184089
ce77da209b7aa7f646664c034b0e81bcb6412d74	minimization of sc1 functions and the maratos effect	minimisation;second order;superlinear convergence;minimization;optimisation;unconstrained minimization;line search;unit stepsize;full search;effet maratos;minimizacion;semismoothness;convergence superlineaire;unconstrained optimization;optimization;optimizacion sin restriccion;optimisation sans contrainte	We consider the unconstrained minimization of a continuously diierentiable function with semismooth gradient by line search methods; in particular we focus on the problem of the acceptance of the unit stepsize. We show that, under mild conditions, if the full search direction brings superlinear convergence, then the unit stepsize is eventually accepted so that the Maratos eeect does not occur. The relevance to this issue of a generalization of the classical second order suucient condition for optimality is pointed out.	constrained optimization;convex function;line search;rate of convergence;relevance;vanishing gradient problem	Francisco Facchinei	1995	Oper. Res. Lett.	10.1016/0167-6377(94)00059-F	minimisation;mathematical optimization;calculus;mathematics;geometry;line search;second-order logic	ML	75.5663058641395	23.425080595242484	184181
0ae6ee28aadf1fc95a85c58f577e4fc217608042	preconditioned conjugate gradient without linesearch: a comparison with the half-quadratic approach for edge-preserving image restoration	metodo cuadrado menor;sistema lineal;iterative method;methode moindre carre;algorithm performance;restauration image;image processing;least squares method;half quadratic;efficient algorithm;procesamiento imagen;image restoration;conjugate gradient method;penalized least square;linear system;traitement image;metodo iterativo;algorithme;difference scheme;algorithm;restauracion imagen;resultado algoritmo;metodo gradiente conjugado;methode iterative;performance algorithme;algorithms;methode gradient conjugue;systeme lineaire;preconditioned conjugate gradient;4230v;algoritmo	Our contribution deals with image restoration. The adopted approach consists in minimizing a penalized least squares (PLS) criterion. Here, we are interested in the search of efficient algorithms to carry out such a task. The minimization of PLS criteria can be addressed using a half-quadratic approach (HQ). However, the nontrivial inversion of a linear system is needed at each iteration. In practice, it is often proposed to approximate this inversion using a truncated preconditioned conjugate gradient (PCG) method. However, we point out that theoretical convergence is not proved for such approximate HQ algorithms, referred here as HQ+PCG. In the proposed contribution, we rely on a different scheme, also based on PCG and HQ ingredients and referred as PCG+HQ1D. General linesearch methods ensuring convergence of PCG type algorithms are difficult to code and to tune. Therefore, we propose to replace the linesearch step by a truncated scalar HQ algorithm. Convergence is established for any finite number of HQ1D sub-iterations. Compared to the HQ+PCG approach, we show that our scheme is preferable on both the theoretical and practical grounds.	approximation algorithm;circuit restoration;conjugate gradient method;experiment;frank–wolfe algorithm;global optimization;image restoration;iteration;least squares;line search;linear system;preconditioner;smoothing spline;stochastic gradient descent;wolfe conditions	Christian Labat;Jérôme Idier	2006		10.1117/12.641663	image restoration;mathematical optimization;image processing;iterative method;conjugate gradient method;linear system;least squares;algorithm	ML	76.80183354661463	23.3408255629027	184198
8b4d9cb870502d2266f16fab41bebfd39bb3f0db	generating the weakly efficient set of nonconvex multiobjective problems	90c31;weakly efficient solution;scalarization;nonconvex multiobjective problem;multiobjective optimization	We present a method for generating the set of weakly efficient solutions of a nonconvex multiobjective optimization problem. The convergence of the method is proven and some numerical examples are encountered.	approximation;computation;experiment;global optimization;mathematical optimization;multi-objective optimization;numerical analysis;optimization toolbox;optimization problem;polyhedron;program optimization	Daniel Gourion;Dinh The Luc	2008	J. Global Optimization	10.1007/s10898-007-9263-9	mathematical optimization;combinatorics;mathematical analysis;multi-objective optimization;mathematics	Robotics	72.65910182705072	22.91322160034628	184277
60a0956ea63b6f2972db590b7c5ebf1fe4b2e4fe	scaled stability in variational problems	90c31;numerical optimization;variational problem;stability analysis;parametric optimization	We introduce tools called tolerance functions for the study of a new and practical kind of stability for variational problems on normed vector spaces. Our notion of stability is distinguished by the fact that it explicitly addresses distinctions of scale. To support the stability analysis, we study the continuity and differentiability properties of tolerance functions, paying particular attention to comparisons between tolerance functions and the set-valued mappings that are used to encode variational problems. We end with a discussion of how tolerance functions can be used to analyze the convergence of numerical optimization procedures.	calculus of variations;encode;mathematical optimization;numerical analysis;scott continuity;variational principle	Adam B. Levy	2002	SIAM Journal on Optimization	10.1137/S1052623400376895	mathematical optimization;combinatorics;von neumann stability analysis;mathematical analysis;mathematics	ML	72.182672084962	20.578806744494496	184290
3c972f246967f382218ead928a5bcb8492cd98c3	nonlinear eigenvalue problems with specified eigenvalues	sylvester like operator;nonlinear eigenvalue problem;47a56;65f18;analytic matrix valued function;backward error;65f15	This work considers eigenvalue problems that are nonlinear in the eigenvalue parameter. Given such a nonlinear eigenvalue problem T , we are concerned with finding the minimal backward error such that T has a set of prescribed eigenvalues with prescribed algebraic multiplicities. We consider backward errors that only allow constant perturbations, which do not depend on the eigenvalue parameter. While the usual resolvent norm addresses this question for a single eigenvalue of multiplicity one, the general setting involving several eigenvalues is significantly more difficult. Under mild assumptions, we derive a singular value optimization characterization for the minimal perturbation that addresses the general case.	mathematical optimization;nonlinear system;resolution (logic)	Michael Karow;Daniel Kressner;Emre Mengi	2014	SIAM J. Matrix Analysis Applications	10.1137/130927462	divide-and-conquer eigenvalue algorithm;mathematical optimization;combinatorics;mathematical analysis;inverse iteration;mathematics;eigenvalue perturbation	Theory	79.62197494039843	21.005686941231193	184755
63e4a4b318f90eb75222a70581271af92be14c05	dc optimization approach to metric regularity of convex multifunctions with applications to infinite systems	convex multifunctions;variational analysis;optimization;metric regularity	The paper develops a new approach to the study of metric regularity and related well-posedness properties of convex set-valued mappings between general Banach spaces by reducing them to unconstrained minimization problems with objectives given as the difference of convex (DC) functions. In this way, we establish new formulas for calculating the exact regularity bound of closed and convex multifunctions and apply them to deriving explicit conditions ensuring well-posedness of infinite convex systems described by inequality and equality constraints.		Boris S. Mordukhovich;T. T. A. Nghia	2012	J. Optimization Theory and Applications	10.1007/s10957-012-0092-8	convex metric space;convex analysis;subderivative;support function;mathematical optimization;conic optimization;mathematical analysis;convex optimization;convex polytope;topology;convex combination;convex body;variational analysis;linear matrix inequality;convex conjugate;convex hull;absolutely convex set;convexity in economics;mathematics;convex set;logarithmically convex function;effective domain;proper convex function;choquet theory	Theory	71.59337540497134	20.38542118600008	184991
676016561104bb4a09ebcf7de341d1fbd9c93d85	viscosity approximation methods for pseudocontractive mappings in banach spaces	solucion viscosidad;analisis numerico;matematicas aplicadas;desigualdad variacional;mathematiques appliquees;approximation method;espacio banach;inegalite variationnelle;aproximacion;pseudocontractive mappings;37c25;banach space;58e35;punto fijo;solution viscosite;satisfiability;analyse numerique;approximation;fixed point;49j40;viscosity solution;iteraccion;numerical analysis;strongly pseudocontractive mappings;point fixe;uniform gâteaux differentiable norms;variational inequality;iteration;nonexpansive mapping;46bxx;applied mathematics;s nonexpansive mappings;fix point;espace banach	Abstract   Let  K  be a closed convex subset of a Banach space  E  and let  T  :  K  →  E  be a continuous weakly inward pseudocontractive mapping. Then for  t  ∈ (0, 1), there exists a sequence { y   t  } ⊂  K  satisfying  y   t   = (1 −  t ) f ( y   t  ) +  tT ( y   t  ), where  f  ∈  Π   K   ≔ { f  :  K  →  K , a contraction with a suitable contractive constant}. Suppose further that  F ( T ) ≠ ∅ and  E  is reflexive and strictly convex which has uniformly Gâteaux differentiable norm. Then it is proved that { y   t  } converges strongly to a fixed point of  T  which is also a solution of certain variational inequality. Moreover, an explicit iteration process which converges strongly to a fixed point of  T  and hence to a solution of certain variational inequality is constructed provided that  T  is Lipschitzian.	approximation	Habtu Zegeye;Naseer Shahzad;Tefera Mekonen	2007	Applied Mathematics and Computation	10.1016/j.amc.2006.07.063	mathematical optimization;mathematical analysis;variational inequality;iteration;topology;applied mathematics;numerical analysis;approximation;viscosity solution;mathematics;fixed point;banach space;algorithm;algebra;satisfiability	Theory	73.79137422866422	19.202156954353875	185097
ca3710941d76392bd78d3fa62fe569296d260b24	introducing one step back iterative approach to solve linear and non linear fixed point problem		In this paper, we introduce a new iterative method which we call one step back approach: the main idea is to anticipate the consequence of the iterative computation per coordinate and to optimize on the choice of the sequence of the coordinates on which the iterative update computations are done. The method requires the increase of the size of the state vectors and one iteration step loss from the initial vector. We illustrate the approach in linear and non linear iterative equations.	computation;fixed point (mathematics);iteration;iterative and incremental development;iterative method	Dohy Hong	2013	CoRR		local convergence;mathematical optimization;combinatorics;mathematics;iterative method;iterative proportional fitting;relaxation;algorithm	Vision	78.82185907194952	23.051936084988466	185164
23c4aae0c22c7169b1ada4746be4cdffb7f80efc	high performance grid and cluster computing for some optimization problems	optimisation grid computing workstation clusters;optimisation;primal dual interior point method;cluster computing;polynomial system;optimal method;homotopy method;optimization problem;quadratic optimization;parallel implementation;workstation clusters;semidefinite programming relaxation;grid computing;high performance;semidefinite program;grid computing high performance computing optimization methods relaxation methods polynomials large scale systems equations power generation economics computer networks distributed computing;parallel implementations high performance grid computing high performance cluster computing optimization methods successive convex relaxation method quadratic optimization problems polyhedral homotopy method polynomial systems primal dual interior point method semidefinite programming problems	The aim of this short article is to show that grid and cluster computing provides tremendous power to optimization methods. The methods that the article picks up are a successive convex relaxation method for quadratic optimization problems, a polyhedral homotopy method for polynomial systems of equations and a primal-dual interior-point method for semidefinite programming problems. Their parallel implementations on grids and clusters together with numerical results are reported.	computer cluster;convex optimization;interior point method;linear programming relaxation;mathematical optimization;numerical analysis;polyhedron;relaxation (approximation);relaxation (iterative method);semidefinite programming;system of polynomial equations	Katsuki Fujisawa;Masakazu Kojima;Akiko Takeda;Makoto Yamashita	2004	2004 International Symposium on Applications and the Internet Workshops. 2004 Workshops.	10.1109/SAINTW.2004.1268696	optimization problem;mathematical optimization;computer cluster;computer science;theoretical computer science;quadratically constrained quadratic program;quadratic programming;grid computing	HPC	79.72768356203999	25.966590115252593	185182
3435cc0e28d88e0dd982565fd15f2bf786709292	an accelerated iterative method with diagonally scaled oblique projections for solving linear feasibility problems	exact projection;oblique projection;projected aggregation methods;oblique projections;linear system;incomplete projections;linear equations;iteration method	The Projected Aggregation Methods (PAM) for solving linear systems of equalities and/or inequalities, generate a new iterate xk+1 by projecting the current point xk onto a separating hyperplane generated by a given linear combination of the original hyperplanes or halfspaces. In Scolnik et al. (2001, 2002a) and Echebest et al. (2004) acceleration schemes for solving systems of linear equations and inequalities respectively were introduced, within a PAM like framework. In this paper we apply those schemes in an algorithm based on oblique projections reflecting the sparsity of the matrix of the linear system to be solved. We present the corresponding theoretical convergence results which are a generalization of those given in Echebest et al. (2004). We also present the numerical results obtained applying the new scheme to two algorithms introduced by Garcı́a-Palomares and González-Castaño (1998) and also the comparison of its efficiency with that of Censor and Elfving (2002).	approximation algorithm;censoring (statistics);convex set;iteration;iterative method;linear equation;linear system;numerical analysis;oblique projection;parallel computing;sparse matrix;system of linear equations;the matrix	Nélida E. Echebest;M. T. Guardarucci;Hugo D. Scolnik;M. C. Vacchino	2005	Annals OR	10.1007/s10479-005-2456-z	mathematical optimization;oblique projection;combinatorics;discrete mathematics;mathematics;iterative method;linear equation;linear system	AI	81.68605265457198	22.705730140835247	185356
805934c52ea4053a5e5f703e8ee81467c06adf42	the geometry of linear infeasibility	systeme equation;polyedre;best approximation;poliedro;geometry;geometrie;ecuacion lineal;polyhedron;linear system;resolucion sistema ecuacion;resolution systeme equation;sistema ecuacion;programacion lineal;equation system;linear programming;programmation lineaire;mejor aproximacion;linear program;generalized inverse;geometria;hiperplano;equation system solving;linear equations;linear equation;hyperplane;hyperplan;equation lineaire;meilleure approximation	The best approximation of an unsolvable system of linear equations is shown to lie in a set that is bounded by finite many hyperplanes but need not be convex. This candidate set, defined to be the polyhedral interior of the linear system, is the same for the best approximations with respect to all p-norms, 1=		Thomas Kämpke	2002	Applied Mathematics and Computation	10.1016/S0096-3003(01)00042-X	mathematical optimization;mathematical analysis;linear programming;calculus;mathematics;geometry;linear equation;arrangement of hyperplanes;algebra	Theory	75.57035212048902	20.63901988602484	185854
fda12172bbf096572ed1ed400a6ae2d4b1045c54	optimization techniques for solving elliptic control problems with control and state constraints: part 1. boundary control	discretization techniques;nonlinear programming;interior point optimization methods;optimization technique;control and state constraints;modeling language;control problem;mathematical programming;boundary condition;state constraints;bang bang control;neumann boundary condition;elliptic control problems;boundary control;nlp methods;interior point method;distributed control;active control	Part 2 continues the study of optimization techniques for elliptic control problems subject to control and state constraints and is devoted to distributed control. Boundary conditions are of mixed Dirichlet and Neumann type. Necessary conditions of optimality are formally stated in form of a local Pontryagin minimum principle. By introducing suitable discretization schemes, the control problem is transcribed into a nonlinear programming problem. The problems are formulated as AMPL (R. Fourer, D.M. Gay, and B.W. Kernighan, “AMPL: A modeling Language for Mathematical Programming”, Duxbury Press, Brooks-Cole Publishing Company, 1993) scripts and several optimization codes are applied. In particular, it is shown that a recently developed interior point method is able to solve theses problems even for high discretizations. Several numerical examples with Dirichlet and Neumann boundary conditions are provided that illustrate the performance of the algorithm for different types of controls including bang–bang controls. The necessary conditions of optimality are checked numerically in the presence of active control and state constraints.	control theory	Helmut Maurer;Hans D. Mittelmann	2000	Comp. Opt. and Appl.	10.1023/A:1008725519350	mathematical optimization;nonlinear programming;boundary value problem;interior point method;control theory;mathematics;bang–bang control;neumann boundary condition;modeling language	Robotics	69.17546520399283	20.29889602647627	185996
1e4d56d507d432fb8d2c7c1958aac9b69ddc4cfb	continuous location under the effect of 'refraction'		In this paper we address the problem of locating a new facility on a d-dimensional space when the distance measure ($$\ell _p$$lp- or polyhedral-norms) is different at each one of the sides of a given hyperplane $$\mathcal {H}$$H. We relate this problem with the physical phenomenon of refraction, and extend it to any finite dimensional space and different distances at each one of the sides of any hyperplane. An application to this problem is the location of a facility within or outside an urban area where different distance measures must be used. We provide a new second order cone programming formulation, based on the $$\ell _p$$lp-norm representation given in Blanco et al. (Comput Optim Appl 58(3):563---595, 2014) that allows to solve the problem in any finite dimensional space with second order cone or semidefinite programming tools. We also extend the problem to the case where the hyperplane is considered as a rapid transit media (a different third norm is also considered over $$\mathcal {H}$$H) that allows the demand to travel, whenever it is convenient, through $$\mathcal {H}$$H to reach the new facility. Extensive computational experiments run in Gurobi are reported in order to show the effectiveness of the approach. Some extensions of these models are also presented.		Víctor Blanco;Justo Puerto;Diego Ponce	2017	Math. Program.	10.1007/s10107-016-1002-x	mathematical optimization;combinatorics;mathematics;geometry	HCI	69.65760075169031	25.15660751463146	186064
4cdb5c0c3d453ccb4b1b822e2ae3261b4d5576f9	a partitioned ∈-relaxation algorithm for separable convex network flow problems	relaxation methods;network flows;asynchronous	A relaxation method for separable convex network flow problems is developed that is well-suited for problems with large variations in the magnitude of the nonlinear cost terms. The arcs are partitioned into two sets, one of which contains only arcs corresponding to strictly convex costs. The algorithm adjusts flows on the other arcs whenever possible, and terminates with primal-dual pairs that satisfy complementary slackness on the strictly convex arc set and e-complementary slackness on the remaining arcs. An asynchronous parallel variant of the method is also developed. Computational results demonstrate that the method is significantly more efficient on ill-conditioned networks than existing methods, solving problems with several thousand nonlinear arcs in one second or less.	algorithm;flow network	Renato De Leone;Robert R. Meyer;Armand Zakarian	1999	Comp. Opt. and Appl.	10.1023/A:1008667714641	mathematical optimization;combinatorics;discrete mathematics;flow network;asynchronous communication;mathematics	Theory	73.23202067431738	26.083874028280377	186221
d563bffb3b582ccd2c45ef7c917196a238138718	the (m, n)-symmetric procrustes problem	optimal solution;analisis numerico;solution optimale;decomposition valeur singuliere;matematicas aplicadas;numerical solution;mathematiques appliquees;49j30;aproximacion optima;singular value decomposition;matriz simetrica;generalized singular value decomposition;analyse numerique;symmetric matrix;m;numerical analysis;optimal approximation;approximation optimale;approximate solution;solucion optima;procrustes problem;matrice symetrique;n symmetric matrix;decomposicion valor singular;numerical experiment;49k30;applied mathematics;algoritmo optimo;algorithme optimal;optimal algorithm;solution numerique	An p×q matrix A is said to be (M,N)-symmetric if MAN=(MAN)T for given M∈Rn×p,N∈Rq×n. In this paper, the following (M,N)-symmetric Procrustes problem is studied. Find the (M,N)-symmetric matrix A which minimizes the Frobenius norm of AX-B, where X and B are given rectangular matrices. We use Project Theorem, the singular-value decomposition and the generalized singular-value decomposition of matrices to analysis the problem and to derive a stable method for its solution. The related optimal approximation problem to a given matrix on the solution set is solved. Furthermore, the algorithm to compute the optimal approximate solution and the numerical experiment are given.		Juan Peng;Xi-Yan Hu;Lei Zhang	2008	Applied Mathematics and Computation	10.1016/j.amc.2007.08.094	mathematical optimization;combinatorics;haplogroup m;numerical analysis;calculus;orthogonal procrustes problem;mathematics;singular value decomposition;symmetric matrix;algebra	Theory	79.80197417241997	21.34953369844428	186224
2bfa2efa677df14569e09cb7ddb1faefad84c275	a general convergence theorem for the decomposition method	optimal solution;convergence theorem;solution optimale;convex programming;programmation convexe;intelligence artificielle;satisfiability;convergence numerique;numerical convergence;decomposition method;mathematical programming;solucion optima;machine exemple support;quadratic optimization;artificial intelligence;inteligencia artificial;support vector machine;maquina ejemplo soporte;vector support machine;programmation mathematique;convergencia numerica;programacion matematica;programacion convexa	The decomposition method is currently one of the major methods for solving the convex quadratic optimization problems being associated with support vector machines. Although there exist some versions of the method that are known to converge to an optimal solution, the general convergence properties of the method are not yet fully understood. In this paper, we present a variant of the decomposition method that basically converges for any convex quadratic optimization problem provided that the policy for working set selection satisfies three abstract conditions. We furthermore design a concrete policy that meets these requirements.	converge;convex optimization;existential quantification;mathematical optimization;optimization problem;quadratic programming;requirement;support vector machine;working set	Nikolas List;Hans Ulrich Simon	2004		10.1007/978-3-540-27819-1_25	support vector machine;mathematical optimization;convex optimization;decomposition method;computer science;artificial intelligence;mathematics;quadratic programming;algorithm;satisfiability	ML	74.955892760053	21.742064701829378	186264
2db9c5c14b9c058a9d03e9299ffbd39b230b3b16	symbol approach in a signal-restoration problem involving block toeplitz matrices	matrix valued symbol;computacion informatica;ciencias basicas y experimentales;matematicas;block toeplitz matrix;grupo a;missing samples	We consider a special type of signal restoration problem where some of the sampling data are not available. The formulation related to samples of the function and its derivative leads to a possibly large linear system associated to a nonsymmetric block Toeplitz matrix which can be equipped with a 2 × 2 matrix-valued symbol. The aim of the paper is to study the eigenvalues of the matrix. We first identify in detail the symbol and its analytical features. Then, by using recent results on the eigenvalue distribution of block Toeplitz matrix-sequences, we formally describe the cluster sets and the asymptotic spectral distribution of the matrix-sequences related to our problem. The localization areas, the extremal behavior, and the conditioning are only observed numerically, but their behavior is strongly related to the analytical properties of the symbol, even though a rigorous proof is still missing in the block case.	applicative programming language;circuit restoration;computer cluster;inpainting;linear system;nonlinear system;numerical analysis;reconstruction conjecture;sampling (signal processing);the matrix;toeplitz hash algorithm	Vincenza Del Prete;Fabio Di Benedetto;Marco Donatelli;Stefano Serra Capizzano	2014	J. Computational Applied Mathematics	10.1016/j.cam.2013.05.018	mathematical optimization;combinatorics;mathematical analysis;discrete mathematics;calculus;power residue symbol;toeplitz matrix;mathematics;block matrix;algebra	Theory	79.55875993272265	20.819620271326244	186710
5df11d742adec19949336bb6e52f88c984db4e68	a modified non-monotone bfgs method for non-convex unconstrained optimization	bfgs method;non convex optimization;global convergence;non monotone line search	In this paper, a modified non-monotone BFGS (MNBFGS) method for non-convex unconstrained optimization is proposed. Under some mild conditions, the global convergence of the given method is established, when the objective function is non-convex. Preliminary numerical comparisons, which show the proposed method is competitive, are also reported.	broyden–fletcher–goldfarb–shanno algorithm;program optimization;monotone	Liying Liu;Shengwei Yao;Zengxin Wei	2014	APJOR	10.1142/S021759591450033X	mathematical optimization;combinatorics;discrete mathematics;quasi-newton method;mathematics	Vision	74.46642900592188	23.447053101353493	186718
092103acb2a399b4fddf18e9758a4f84cacd12e6	the equation a⊗x=b⊗y over (max, +)	sistema lineal;systeme equation;systeme evenement discret;reachability;algorithm complexity;pseudopolynomial algorithm;modelo homogeneo;teoria sistema;complejidad algoritmo;max algebra;modele homogene;matrix algebra;linear system;algorithme pseudopolynomial;finite element;condition suffisante;convergence speed;sistema acontecimiento discreto;discrete event system;sistema ecuacion;condicion suficiente;complexite algorithme;systems theory;algebre max plus;theorie systeme;asequibilidad;equation system;velocidad convergencia;algebra max plus;atteignabilite;sufficient condition;max plus algebra;linear equations;systeme lineaire;element fini;algebre matricielle;vitesse convergence;homogeneous model;elemento finito	For the two-sided homogeneous linear equation system A ⊗ x = B ⊗ y over (max;+), with no in.nite rows or columns in A or B, an algorithm is presented which converges to a .nite solution from any .nite starting point whenever a .nite solution exists. If the .nite elements of A, B are all integers, convergence is in a .nite number of steps, for which a precise bound can be calculated if moreover one of A, B has only .nite elements. The algorithm is thus pseudopolynomial in complexity. c © 2002 Elsevier Science B.V. All rights reserved.	algorithm;ccir system a;column (database);linear equation;pseudo-polynomial time;system of linear equations;vergence	Raymond A. Cuninghame-Green;Peter Butkovic	2003	Theor. Comput. Sci.	10.1016/S0304-3975(02)00228-1	combinatorics;finite element method;calculus;mathematics;geometry;linear equation;linear system;reachability;systems theory;algorithm	Theory	79.20606730978447	18.95303804892565	186785
66bd0b13c28fcc94dfcc0116c7d42fc1bc369e2e	lurupa - rigorous error bounds in linear programming	linear program	Linear Programming has numerous applications, e.g., operations research,rnrelaxations in global optimization, computational geometry. Recently it hasrnbeen shown that many real world problems exhibit numerical difficulties due tornill-conditioning.rnLurupa is a software package for computing rigorous optimal value bounds. Thernpackage can handle point and interval problems. Numerical experience with thernNetlib lp library is given.	linear programming	Christian Keil	2005			global optimization;software;computational geometry;theoretical computer science;linear programming;mathematical optimization;mathematics;netlib	ML	72.2094049595554	24.495195788847845	187172
f1e0c38c58cf4e18fd37d38b805edf5e551c9b51	superfast solution of linear convolutional volterra equations using qtt approximation	tensor train format;computacion informatica;65f05;45e10;15a69;triangular toeplitz matrix;ciencias basicas y experimentales;matematicas;superfast fourier transform;fractional calculus;fast convolution;26a33;grupo a;article;divide and conquer	We address a linear fractional differential equation and develop effective solution methods using algorithms for inversion of triangular Toeplitz matrices and the recently proposed QTT format. The inverses of such matrices can be computed by the divide and conquer and modified Bini’s algorithms, for which we present the versions with the QTT approximation. We also present an efficient formula for the shift of vectors given in QTT format, which is used in the divide and conquer algorithm. As the result, we reduce the complexity of inversion from the fast Fourier level O(n log n) to the speed of superfast Fourier transform, i.e., O(log n). The results of the paper are illustrated by numerical examples.	approximation algorithm;fast fourier transform;lotka–volterra equations;numerical analysis;numerical method;toeplitz hash algorithm	Jason A. Roberts;Dmitry V. Savostyanov;Eugene E. Tyrtyshnikov	2014	J. Computational Applied Mathematics	10.1016/j.cam.2013.10.025	mathematical optimization;combinatorics;mathematical analysis;divide and conquer algorithms;fractional calculus;calculus;mathematics	ML	81.85000357600636	21.124370067509215	187185
b3f0c5aab62bc526794cfc3ffbec4090c9fbb648	coupling proximal methods and variational convergence	optimisation;optimizacion;approximation method;aproximacion;operateur monotone maximal;approximation;inverse method;distance hausdorff;hausdorff distance;proximal point algorithm;optimization;convergence variationnelle;maximal monotone operator;methode point proximal;proximal point method;inverse partiel	An approximation method which combines a data perturbation by variational convergence with the proximal point algorithm, is presented. Conditions which guarantee convergence, are provided and an application to the partial inverse method is given.	algorithm;approximation;calculus of variations;rate of convergence;vergence	Abdellatif Moudafi	1993	ZOR - Meth. & Mod. of OR	10.1007/BF01416609	hausdorff distance;mathematical optimization;mathematical analysis;topology;modes of convergence;compact convergence;approximation;mathematics;normal convergence	ML	74.89352866180495	21.831438583079095	187212
eeb93158d33c367114f98efff99082b4f717559e	convergence of multi-step curve search method for unconstrained optimization		A new multi-step curve search method for unconstrained minimization problems is proposed. The convergence of the algorithm is proved under some mild conditions. The linear convergence rate is also investigated when the objective function is uniformly convex. This method uses previous multi-step iterative information and curve search rule to generate new iterative points. Using more previous iterative information and curve search rule can make the new method converge more stably than traditional descent methods and be suitable to solve large scale problems.	algorithm;computation;converge;hessian;iteration;iterative method;loss function;mathematical optimization;numerical method;optimization problem;rate of convergence;uniformly convex space	Zhenjun Shi	2004	J. Num. Math.	10.1515/1569395042571292	quadratic unconstrained binary optimization;mathematics;metaheuristic	AI	75.63138393934867	23.989583367686254	187350
582c076e3fe5c8209732e8fa1de79045de9b41e3	explicit convex and concave envelopes through polyhedral subdivisions		5 In this paper, we derive explicit characterizations of convex and concave envelopes of several 6 nonlinear functions over various subsets of a hyper-rectangle. These envelopes are obtained 7 by identifying polyhedral subdivisions of the hyper-rectangle over which the envelopes can be 8 constructed easily. In particular, we use these techniques to derive, in closed-form, the concave 9 envelopes of concave-extendable supermodular functions and the convex envelopes of disjunctive 10 convex functions. 11	algorithm;biclustering;computation;concave function;convex function;convex hull;disjunctive normal form;extensibility;hyper-heuristic;linear function;linear programming;linear separability;mathematical optimization;non-convexity (economics);nonlinear system;optimization problem;polyhedron;polynomial;subdivision surface;supermodular function;time complexity	Mohit Tawarmalani;Jean-Philippe P. Richard;Chuanhui Xiong	2013	Math. Program.	10.1007/s10107-012-0581-4	convex analysis	ML	70.85943312535457	23.288810143817816	187384
dc12f5fdc1e05a440f0aaf136a4eb66faa5232e6	perturbation analysis of global error bounds for systems of linear inequalities	analyse perturbation;sistema infinito;programacion semi infinita;borne erreur;condition slater;desigualdad;inequality;perturbation theory;inegalite;calcul erreur;satisfiability;analyse globale;error analysis;semi infinite programming;estimation erreur;error estimation;inegalite lineaire;estimacion error;perturbation analysis;calculo error;error bound;programmation semi infinie;theorie perturbation;systeme infini;teoria perturbacion;infinite system;global analysis	This paper studies the existence of a uniform global error bound when a system of linear inequalities is under local arbitrary perturbations. Specifically, given a possibly infinite system of linear inequalities satisfying the Slater’s condition and a certain compactness condition, it is shown that for sufficiently small arbitrary perturbations the perturbed system is solvable and there exists a uniform global error bound if and only if the original system is bounded or its homogeneous system has a strict solution.	courant–friedrichs–lewy condition;decision problem;linear inequality;perturbation theory;system of linear equations	Hui Hu	2000	Math. Program.	10.1007/s101070050017	mathematical optimization;mathematical analysis;calculus;perturbation theory;mathematics	DB	71.89683705539824	19.19945886538188	187621
2ab56ae20f64a6c57d0eb723d7d60b3f82832b27	almost p0-matrices and the class q	p 0 matrix;linearity;classe of matrices;r0 matrix;linearite;p0 matrix;matrice mathematique;linearidad;q matrix;classes of matrices;r 0 matrix;mathematical matrix;mathematical programming;r matrix;matriz matematica;probleme complementarite;problema complementariedad;complementarity problem;matrice r;linear complementarity problem;programmation mathematique;programacion matematica;matriz r	This paper demonstrates that within the class of those n × n real matrices, each of which has a negative determinant, nonnegative proper principal minors and inverse with at least one positive entry, the class of Q-matrices coincides with the class of regular matrices. Each of these classes of matrices plays an important role in the theory of the linear complementari ty problem. Lastly, analogous results are obtained for nonsingular matrices which possess only nonpositive principal minors.		Wallace C. Pye	1992	Math. Program.	10.1007/BF01581093	cauchy–binet formula;mathematical optimization;combinatorics;calculus;mathematics;linearity;linear complementarity problem;algebra	Theory	77.66087905744715	19.8038723595831	187662
e2d336887c12c64a1599105096d9b6863ff3a5d7	some properties of (f-k)-convex mapping in vector spaces	k convex mapping e convex sets f k convex sets semi f;k convex mapping;semi f;computational intelligence security;computational intelligence;convex analysis f k convex mapping vector spaces optimization theory operational research convex set set convexity e convex set semie convex function set valued analysis;k convex sets;vectors convex programming set theory;e convex sets;f;security	Optimization theory is the most important part of the optimization as well as an important theoretical basis in operational research. Convex set and convex mapping, as basic theoretical results in optimization theory, are applied in many fields of mathematics. Accordingly, development of the convexity of sets and functions has practical significance. In 1999 Youness first introduced the concept of E-convex set and E-convex function, then Yang and Chen defined the semi-E-convex function and listed its properties, In recent decades, set-valued analysis and convex analysis, as the tools of researching the optimized theorem, have made great progress, as well as provides new ideas, new methods for the generalized convexity study, and many useful conclusions, which also contributed to develop generalized convexity to be a focus for domestic and foreign scholars. In this paper, we introduce (F, K) -- convex set, (F, K) -- convex mapping and semi (F, K) -- convex mapping in vector spaces, and some properties of these concepts are given.	convex analysis;convex function;convex set;entity–relationship model;mathematical optimization;operations research;semiconductor industry;yang	Yuhui Liu	2015	2015 11th International Conference on Computational Intelligence and Security (CIS)	10.1109/CIS.2015.56	convex analysis;subderivative;support function;mathematical optimization;conic optimization;combinatorics;discrete mathematics;convex optimization;krein–milman theorem;convex polytope;convex combination;orthogonal convex hull;convex body;linear matrix inequality;computer science;quasiconvex function;artificial intelligence;information security;convex hull;computational intelligence;absolutely convex set;convexity in economics;mathematics;convex set;computer security;effective domain;proper convex function;choquet theory	Robotics	69.27018836881527	21.422723881997594	188176
b51c9f7080ff3d43ad48093ed8332e7fe48b6b4c	continuous method for solving fixed point problem on square grid by cubic spline boundary		Fixed point problem can be used as a model of solving the optimization problem, such as facility location or gravity center problem on Euclidean space. In the actual application, the plane area under study is simplified by square grid. Thus, the constraint set consisting of feasible points is a connected grid of such squares. The outer edges of all squares of an edge connected square grid are enclosed into a polygon most of which are nonempty compact but non-convex set. Weaker conditions that constraint set can be extended to the non-convex bounded set for the fixed point problem are given. The upper and lower bounds function of the inequality are treated by the cubic spline interpolation function. The numerical method combined cubic spline boundary interpolation and tracing the homotopy pathway can be used to solve the fixed point problem on square grid. The numerical example shows that this method is much effective.	convex set;cubic hermite spline;cubic function;fixed point (mathematics);gene regulatory network;loss function;mathematical optimization;numerical method;optimization problem;social inequality;spline (mathematics);spline interpolation;square tiling	Tao Yang	2017	2017 10th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)	10.1109/CISP-BMEI.2017.8302145	artificial intelligence;spline interpolation;grid;interpolation;pattern recognition;mathematical optimization;facility location problem;computer science;fixed point;square tiling;spline (mathematics);optimization problem	Robotics	70.7402633181046	23.968756977906885	188530
4c5cb95abb66bc6d972ef06bca0ef5633498415f	campary: cuda multiple precision arithmetic library and applications	error free transform;gpgpu computing;multiple precision library;dynamical systems;henon map;floating point arithmetic;floating point expansions;semi definite programming	Many scientific computing applications demand massive numerical computations on parallel architectures such as Graphics Processing Units (GPUs). Usually, either floating-point single or double precision arithmetic is used. Higher precision is generally not available in hardware, and software extended precision libraries are much slower and rarely supported on GPUs. We develop CAMPARY: a multipleprecision arithmetic library, using the CUDA programming language for the NVidia GPU platform. In our approach, the precision is extended by representing real numbers as the unevaluated sum of several standard machine precision floating-point numbers. We make use of error-free transforms algorithms, which are based only on native precision operations, but keep track of all rounding errors that occur when performing a sequence of additions and multiplications. This offers the simplicity of using hardware highly optimized floating-point operations, while also allowing for rigorously proven rounding error bounds. This also allows for easy implementation of an interval arithmetic. Currently, all basic multiple-precision arithmetic operations are supported. Our target applications are in chaotic dynamical systems or automatic control.	algorithm;arbitrary-precision arithmetic;automatic control;cuda;central processing unit;chaos theory;computation;computational science;double-precision floating-point format;dynamical system;elementary function;extended precision;graphics processing unit;interval arithmetic;iteration;library (computing);linear algebra;machine epsilon;numerical analysis;programming language;proof assistant;prototype;round-off error;rounding	Mioara Joldes;Jean-Michel Muller;Valentina Popescu;Warwick Tucker	2016		10.1007/978-3-319-42432-3_29	floating-point unit;double-precision floating-point format;mathematical optimization;quadruple-precision floating-point format;parallel computing;dynamical systems theory;arbitrary-precision arithmetic;binary scaling;computer hardware;computer science;floating point;theoretical computer science;operating system;saturation arithmetic;mathematics;extended precision;programming language;machine epsilon;semidefinite programming	PL	82.16495898831764	27.105500650696694	188686
d064fd132af15e04038788b7f4c55be2be3a1174	heuristics to accelerate the dixon resultant	quantifier elimination;polynomial system;heuristic method;dynamic system;computational chemistry;bezout;signal processing;resultant;dixon;system of equations	The Dixon resultant method solves a system of polynomial equations by computing its resultant. It constructs a square matrix whose determinant (det) is a multiple of the resultant (res). The naı̈ve way to proceed is to compute det, factor it, and identify res. But often det is too large to compute or factor, even though res is relatively small. In this paper we describe three heuristic methods that often overcome these problems. The first, although sometimes useful by itself, is often a subprocedure of the second two. The second may be used on any polynomial system to discover factors of det without producing the complete determinant. The third applies when res appears as a factor of det in a certain exponential pattern. This occurs in some symmetrical systems of equations. We show examples from computational chemistry, signal processing, dynamical systems, quantifier elimination, and pure mathematics. © 2007 IMACS. Published by Elsevier B.V. All rights reserved.	algebraic equation;computational chemistry;dixon's factorization method;dynamical system;earliest deadline first scheduling;gaussian elimination;gröbner basis;heuristic;interpolation;interrupt;quantifier (logic);resultant;signal processing;subroutine;system of polynomial equations;time complexity	Robert H. Lewis	2008	Mathematics and Computers in Simulation	10.1016/j.matcom.2007.04.007	system of linear equations;combinatorics;quantifier elimination;computer science;dynamical system;signal processing;resultant;mathematics;algorithm;algebra	AI	80.76021573302374	20.581761809396674	188995
6ce1198f9ba54bcf44b25c4d68a1db85084a5282	lifted collocation integrators for direct optimal control in acado toolkit	newton-type methods;direct optimal control;collocation methods;optimization algorithms;65m70;49m15;90c30	This paper presents an efficient Newton-type algorithm for solving the Nonlinear Programs (NLPs) arising from applying a direct collocation approach to continuous time optimal control. The method is based on an implicit lifting technique including a condensing and expansion step, such that the structure of each subproblem corresponds to that of the multiple shooting method for direct optimal control. We establish the mathematical equivalence between the Newton iteration based on direct collocation and the proposed approach, and we discuss the computational advantages of the lifted collocation method. In addition, we investigate different inexact versions of the proposed schemes and study their convergence and computational properties. The presented algorithms are implemented as part of the open-source ACADO code generation software for embedded optimization. Their performance is illustrated on a benchmark case study of the optimal control for a chain of masses. Based on these results, the use of lifted collocation within direct multiple shooting allows for a computational speedup of factor 5 − 10 compared to a standard collocation integrator and a factor 10 − 50 compared to direct collocation using a general-purpose sparse NLP solver. This research was supported by the EU via ERC-HIGHWIND (259 166), FP7-ITN-TEMPO (607 957), and H2020-ITN-AWESCO (642 682). R. Quirynen holds a PhD fellowship of the Research Foundation – Flanders (FWO). Rien Quirynen Department ESAT-STADIUS, KU Leuven University, 3001 Leuven, Belgium E-mail: rien.quirynen@esat.kuleuven.be Sébastien Gros Department of Signals and Systems, Chalmers University of Technology, Göteborg, Sweden Boris Houska School of Information Science and Technology, ShanghaiTech University, Shanghai, China Moritz Diehl Department IMTEK, University of Freiburg, 79110 Freiburg, Germany 2 Rien Quirynen et al.	algorithm;benchmark (computing);code generation (compiler);collocation method;computation;control flow;converge;eniac;embedded system;general-purpose modeling;information science;iteration;lambda lifting;lifting scheme;mathematical optimization;natural language processing;newton;newton's method;nonlinear programming;numerical analysis;open-source software;optimal control;real-time clock;shooting method;solver;sparse matrix;speedup;turing completeness	Rien Quirynen;Sebastien Gros;Boris Houska;Moritz Diehl	2017	Math. Program. Comput.	10.1007/s12532-017-0119-0	mathematical optimization;orthogonal collocation;collocation method;control theory;mathematics	AI	80.97495339222576	26.267698241623698	189244
1cfc881afeb3005fd5169643816361d8064bad16	reduction of generalized semi-infinite programming problems to semi-infinite or piece-wise smooth programming problems	semi infinite programming		generalized semi-infinite programming;semiconductor industry	E. Levitin	2001	Universität Trier, Mathematik/Informatik, Forschungsbericht		fractional programming;mathematical optimization;constraint programming;combinatorics;discrete mathematics	Theory	72.43872215426669	22.675766583892365	189460
1d0b186061f140ff0b91882f33e7add8df24681d	high-order approximation of soft constrained nash strategy for weakly coupled large-scale systems	soft constrained nash strategy;weakly coupled large scale systems;riccati equations game theory large scale systems newton method;game theory;large scale system;iterative algorithm;newton method soft constrained nash strategy weakly coupled large scale systems cross coupled sign indefinite algebraic riccati equations iterative algorithm;large scale systems riccati equations iterative algorithms newton method convergence robustness control systems stability vectors linear systems;algebraic riccati equation;riccati equations;newton method;iterative solution;cross coupled sign indefinite algebraic riccati equations;large scale systems	In this paper, a high-order soft constrained Nash strategy for weakly coupled large-scale systems is investigated. In order to solve the cross-coupled sign-indefinite algebraic Riccati equations (CSAREs) that results in such strategy, the iterative algorithm that is based on the Newton's method is applied. Using these iterative solutions, a high-order soft constrained Nash strategy is designed. As a result, it is proved that the proposed high-order approximate equilibrium strategies achieve better performance. Finally, in order to demonstrate the efficiency of the algorithm, a numerical example is given.	approximation algorithm;existential quantification;iterative method;linear algebra;nash equilibrium;newton;newton's method;numerical analysis;rate of convergence	Hiroaki Mukaidani	2007	2007 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2007.4413623	local convergence;game theory;mathematical optimization;combinatorics;algebraic riccati equation;control theory;mathematics;iterative method;newton's method	Robotics	80.50242246913257	22.07397916424614	189607
3c65630b942ffb29f27d1aa8dc955f1523739ac0	generalized convexity and characterization of (weak) pareto-optimality in nonsmooth multiobjective optimization problems	multiple objective optimization;clarke s gradient;weak pareto optimality;functional analysis;invexity	E®orts to characterize optimality in nonsmooth and/or nonconvex optimization problems have made rapid progress in the past four decades. Nonsmooth analysis, which refers to di®erential analysis in the absence of di®erentiability, has grown rapidly in recent years, and plays a vital role in functional analysis, information technology, optimization, mechanics, di®erential equations, decision making, etc. Furthermore, convexity has been increasingly important nowadays in the study of many pure and applied mathematical problems. In this paper, some new connections between three major ̄elds, nonsmooth analysis, convex analysis, and optimization, are provided that will help to make these ̄elds accessible to a wider audience. In this paper, at ̄rst, we address some newly reported and interesting applications of multiobjective optimization in Management Science and Biology. Afterwards, some su±cient conditions for characterizing the feasible and improving directions of nonsmooth multiobjective optimization problems are given, and using these results a necessary optimality condition is proved. The su±cient optimality conditions are given utilizing a generalized convexity notion. Establishing necessary and su±cient optimality conditions for nonsmooth fractional programming problems is the next aim of the paper. We follow the paper by studying (strictly) prequasiinvexity and pseudoinvexity. Finally, some connections between these notions as well as some applications of these concepts in optimization are given.	convex analysis;convex cone;convex function;convex set;edmund m. clarke;fractional programming;gradient;management science;mathematical optimization;multi-objective optimization;nl (complexity);numerical aperture;pareto efficiency;quasiconvex function;subderivative;variational principle;vector optimization	Majid Soleimani-Damaneh	2015	International Journal of Information Technology and Decision Making	10.1142/S0219622014500394	functional analysis;mathematical optimization;mathematics;mathematical economics;welfare economics	AI	72.54783807734303	21.783438991154316	189924
c78cbf40f7ca60a8290662617c2f9a5528ba4361	an iterative method for solving nonlinear integral equations of hammerstein type	integral equations;accretive operators;hammerstein equations	Suppose H is a real Hilbert space and F,K:H->H are continuous bounded monotone maps with D(K)=D(F)=H. Assume that the Hammerstein equation u+KFu=0 has a solution. An explicit iteration process is proved to converge strongly to a solution of this equation. No invertibility assumption is imposed on K and the operator F is not restricted to be angle-bounded. Our theorem complements the Galerkin method of Brezis and Browder to provide methods for approximating solutions of nonlinear integral equations of Hammerstein type.	iterative method;nonlinear system	Charles E. Chidume;Ngalla Djitté	2013	Applied Mathematics and Computation	10.1016/j.amc.2012.11.051	mathematical optimization;mathematical analysis;discrete mathematics;mathematics;integral equation	Robotics	75.8171964530492	18.41363913148067	190071
692b87799f1acc2509152d47d8a1b922ff37a083	structure-preserving low multilinear rank approximation of antisymmetric tensors		This paper is concerned with low multilinear rank approximations to antisymmetric tensors, that is, multivariate arrays for which the entries change sign when permuting pairs of indices. We show which ranks can be attained by an antisymmetric tensor and discuss the adaption of existing approximation algorithms to preserve antisymmetry, most notably a Jacobi algorithm. Particular attention is paid to the important special case when choosing the rank equal to the order of the tensor. It is shown that this case can be addressed with an unstructured rank-1 approximation. This allows for the straightforward application of the higher-order power method, for which we discuss effective initialization strategies.	antisymmetry;approximation algorithm;greedy algorithm;jacobi method;local convergence;low-rank approximation;power iteration	Erna Begovic Kovac;Daniel Kressner	2017	SIAM J. Matrix Analysis Applications	10.1137/16M106618X	mathematical optimization;combinatorics;antisymmetric tensor;mathematical analysis;discrete mathematics;mathematics;algebra	ML	80.00413927778644	23.145366419157675	190145
f9b818d9404279c52cec92900370d3be4ddcf7c4	theory of globally convergent probability-one homotopies for nonlinear programming	constrained optimization;convergence theorem;probability one homotopy;homotopy algorithm;nonlinear programming;globally convergent;inequality constraint;global convergence;65k10;parallel computation;homotopy method;optimization problem;numerical analysis;technical report departmental;historical collection till dec 2001;fixed point theory;nonlinear equations;65f50;optimal algorithm;65h10;65f10	For many years globally convergent probability-one homotopy methods have been remarkably successful on difficult realistic engineering optimization problems, most of which were attacked by homotopy methods because other optimization algorithms failed or were ineffective. Convergence theory has been derived for a few particular problems, and considerable fixed point theory exists, but generally convergence theory for the homotopy maps used in practice for nonlinear constrained optimization has been lacking. This paper derives some probability-one homotopy convergence theorems for unconstrained and inequality constrained optimization, for linear and nonlinear inequality constraints, and with and without convexity. Some insight is provided into why the homotopies used in engineering practice are so successful, and why this success is more than dumb luck. By presenting the theory as variations on a prototype probability-one homotopy convergence theorem, the essence of such convergence theory is elucidated.	algorithm;constrained optimization;fixed point (mathematics);fixed-point theorem;mathematical optimization;nonlinear programming;nonlinear system;prototype;social inequality	Layne T. Watson	2001	SIAM Journal on Optimization	10.1137/S105262349936121X	optimization problem;mathematical optimization;constrained optimization;mathematical analysis;discrete mathematics;numerical analysis;nonlinear programming;mathematics;fixed-point theorem	Theory	73.82374970952148	22.347959727255965	190228
2969734bc08d8ee7fde5dfbbfefbfe5910da79af	on the projected descent direction methods for solving convex programming problems	convex programming;projected descent direction method;primal dual potential reduction;interior point method;direct method	A recent paper [14] has considered the possibility of combining interior point strategy with steepest descent method when solving convex programming problems, in such a way that the convergence property of the interior point method remains valid but many iterations do not request the solution of a system of equations. Motivated by this general idea, the paper [14] proposed a hybrid algorithm which combines a primal-dual potential reduction algorithm with the use of the steepest descent direction of the potential function. The O(√n|ln e|) complexity of the potential reduction algorithm remains valid but the overall computational cost can be reduced. In this paper, we discuss the relation between this method and general projected descent direction methods, and compare it with a projected steepest descent direction method for solving complex programming problems.	convex optimization;descent direction	Y. Shi	2003	Neural Parallel & Scientific Comp.		gradient descent;mathematical optimization;combinatorics;calculus;descent direction;random coordinate descent;mathematics;stochastic gradient descent	ML	75.64520495198703	24.030305363909992	190335
6fb8cb8e81dc466859472018dd8adf074a89aa60	a first-order block-decomposition method for solving two-easy-block structured semidefinite programs		In this paper, we consider a first-order block-decomposition method for minimizing the sum of a convex differentiable function with Lipschitz continuous gradient, and two other proper closed convex (possibly, nonsmooth) functions with easily computable resolvents. The method presented contains two important ingredients from a computational point of view, namely: an adaptive choice of stepsize for performing an extragradient step; and the use of a scaling factor to balance the blocks. We then specialize the method to the context of conic semidefinite programming (SDP) problems consisting of two easy blocks of constraints. Without putting them in standard form, we show that four important classes of graph-related conic SDP problems automatically possess the above two-easy-block structure, namely: SDPs for θ-functions and θ+-functions of graph stable set problems, and SDP relaxations of binary integer quadratic and frequency assignment problems. Finally, we present computational results on the aforementioned classes of SDPs showing that our method outperforms the three most competitive codes for large-scale conic semidefinite programs, namely: the boundary point (BP) method introduced by Povh et al., a Newton-CG augmented Lagrangian method, called SDPNAL, by Zhao et al., and a variant of the BP method, called the SPDAD method, by Wen et al.	augmented lagrangian method;code;computable function;computation;first-order predicate;gradient;image scaling;newton;semidefinite programming	Renato D. C. Monteiro;Camilo Ortiz;Benar Fux Svaiter	2014	Math. Program. Comput.	10.1007/s12532-013-0062-7	mathematical optimization;conic optimization;combinatorics;discrete mathematics;mathematics	ML	73.25900187212535	24.235473488194394	190678
6c4be181112c643fb6d409dc89fb4d7194ee7abc	error bounds in mathematical programming	optimisation;convergence;borne erreur;desigualdad variacional;desigualdad;inequality;variational inequalities;article synthese;inegalite variationnelle;inegalite;equilibrium problem;algorithme;etat actuel;convergencia;funcion penalidad;equilibre;mathematical programming;state of the art;error bounds;variational inequality;complementarity problems;algorithms;probleme complementarite;problema complementariedad;estado actual;complementarity problem;convergence of algorithms;optimization;metric regularity;equilibrium;error bound;equilibrio;fonction penalite;iteration method;reviews;programmation mathematique;inequality systems;penalty function	Originated from the practical implementation and numerical considerations of iterative methods for solving mathematical programs, the study of error bounds has grown and proliferated in many interesting areas within mathematical programming. This paper gives a comprehensive, state-of-the-art survey of the extensive theory and rich applications of error bounds for inequality and optimization systems and solution sets of equilibrium problems.	mathematical optimization	Jong-Shi Pang	1997	Math. Program.	10.1007/BF02614322	mathematical optimization;variational inequality;calculus;mathematics;mathematical economics	Theory	73.70678110150284	21.626131846536776	190851
c2a6c2ad115580fc068e30a1143a49132d07c822	a sparse decomposition of low rank symmetric positive semidefinite matrices	matrix factorization;pivoted cholesky decomposition;joint diagonalization;principal component analysis;68r10;sparse pca;68q25;68u05;intrinsic sparse mode decomposition	Suppose that A ∈ RN×N is symmetric positive semidefinite with rank K ≤ N . Our goal is to decompose A into K rank-one matrices ∑K k=1 gkg T k where the modes {gk} K k=1 are required to be as sparse as possible. In contrast to eigen decomposition, these sparse modes are not required to be orthogonal. Such a problem arises in random field parametrization where A is the covariance function and is intractable to solve in general. In this paper, we partition the indices from 1 to N into several patches and propose to quantify the sparseness of a vector by the number of patches on which it is nonzero, which is called patchwise sparseness. Our aim is to find the decomposition which minimizes the total patch-wise sparseness of the decomposed modes. We propose a domain-decomposition type method, called intrinsic sparse mode decomposition (ISMD), which follows the “local-modes-construction + patching-up” procedure. The key step in ISMD is to construct local pieces of the intrinsic sparse modes by a joint diagonalization problem. Thereafter a pivoted Cholesky decomposition is utilized to glue these local pieces together. Optimal sparse decomposition, consistency with different domain decomposition and robustness to small perturbation are proved under the so called regular sparse assumption (see Definition 1.2). We provide simulation results to show the efficiency and robustness of the ISMD. We also compare ISMD to other existing methods, e.g., eigen decomposition, pivoted Cholesky decomposition and convex relaxation of sparse principal component analysis [28, 41].	cholesky decomposition;domain decomposition methods;eigen (c++ library);linear programming relaxation;matroid rank;neural coding;principal component analysis;semidefinite programming;simulation;sparse matrix	Thomas Y. Hou;Qin Li;Pengchuan Zhang	2017	Multiscale Modeling & Simulation	10.1137/16M107760X	mathematical optimization;sparse pca;combinatorics;machine learning;sparse approximation;mathematics;matrix decomposition;principal component analysis	ML	79.97496921250332	26.675735119463916	191340
67e6117c065fad3ce89cb0c68aac2c6174ca4e06	existence and iterative approximation of solutions of generalized mixed equilibrium problems	mixed equilibrium problem;cocoercive mapping;variational inequalities;set valued mapping;maximal strongly η monotone mapping;iterative algorithm;fixed point iteration;equilibrium problem;fixed point;hilbert space;lipschitz continuous mapping;maximal strongly η;lipschitz continuity;variational inequality;yosida approximation	In this paper, we consider a generalized mixed equilibrium problem involving nonmonotone set-valued mappings in real Hilbert space. We extend the notions of the Yosida approximation and its corresponding regularized operator given by Moudafi and Thera [A. Moudafi, M. Thera, Proximal and dynamical approaches to equilibrium problems, in: Lecture Notes in Econom. and Math. System, vol. 477, Springer-Verlag, Berlin, 2002, pp. 187–201] and discuss some of their properties. Further, we consider a generalized Wiener–Hopf equation problem and show its equivalence with the generalized mixed equilibrium problem. Using a fixed point formulation of the generalized Wiener–Hopf equation problem, we construct an iterative algorithm. Furthermore, we extend the notion of stability given by Harder and Hick [A.M. Harder, T.L. Hicks, Stability results for fixedpoint iteration procedures, Math. Japonica 33 (5) (1998) 693–706]. We prove the existence of a solution and discuss the convergence and stability of the iterative algorithm for the generalized Wiener–Hopf equation problem. Since the generalized mixed equilibrium problems include variational inequalities as special cases, the results presented in this paper continue to hold for these problems. © 2008 Elsevier Ltd. All rights reserved.	algorithm;approximation;bellman equation;calculus of variations;fixed point (mathematics);hilbert space;iteration;iterative method;springer (tank);turing completeness;variational inequality	Kaleem Raza Kazmi;F. A. Khan	2008	Computers & Mathematics with Applications	10.1016/j.camwa.2007.11.051	mathematical optimization;mathematical analysis;discrete mathematics;variational inequality;mathematics	Theory	73.4129099531838	21.733686119932198	191429
bfc4a9e1a96481cec1a0234b78c5865b0c438dc9	a method for convex minimization based on translated first-order approximations	nonsmooth optimization;convex optimization;bundle methods	We describe an algorithm for minimizing convex, not necessarily smooth, functions of several variables, based on a descent direction finding procedure that inherits some characteristics both of standard bundle method and of Wolfe’s conjugate subgradient method. This is obtained by allowing appropriate upward shifting of the affine approximations of the objective function which contribute to the classic definition of the cutting plane function. The algorithm embeds a proximity control strategy. Finite termination is proved at a point satisfying an approximate optimality condition and some numerical results are provided.	approximation algorithm;control theory;convex optimization;cutting-plane method;descent direction;direction finding;first-order predicate;loss function;numerical analysis;optimization problem;subgradient method	Annabella Astorino;Manlio Gaudioso;Enrico Gorgone	2017	Numerical Algorithms	10.1007/s11075-017-0280-6	mathematical optimization;combinatorics;mathematical analysis;subgradient method;calculus;random coordinate descent;mathematics	ML	72.73174088101861	21.56777157306114	191688
a282c17cbe78911d1b77c16817eeabac9a067f33	reformulation of the modified goal programming for logarithmic piecewise linear function	fonction lineaire par morceaux;metodo polinomial;piecewise linear;matematicas aplicadas;mathematiques appliquees;piecewise linear function;operations research;funcion logaritmica;logarithmic function;polynomial method;fonction logarithmique;programmation objectif;goal programming;applied mathematics;programacion objetivo;methode polynomiale	The purpose of this paper is to present a reformulation of the model presented by Chang [A modified goal programming model for piecewise linear function, European Journal of Operational Research 139 (2002) 62-67]. The modified goal programming (MGP) model of Chang has been accepted as an efficient method published for solving polynomial/posynomial (P/P) programming problems. However, it still cannot be used to fully practical applications because the problem with negative variable is undefined in logarithmic piecewise linear function (LPLF). This paper presents some appropriate strategies to overcome this undefined problem. The reformulation MGP model can be used to solve P/P programming problems with negative/positive variables. In addition, an illustrative example is included for demonstrating the correctness of the proposed method.	goal programming;linear function;piecewise linear continuation	Ching-Ter Chang	2006	Applied Mathematics and Computation	10.1016/j.amc.2005.05.006	mathematical optimization;combinatorics;mathematical analysis;piecewise linear function;calculus;mathematics;algorithm	ML	76.46473593608944	20.963335306078925	191797
4970d9fa9014e72b18ab59f1029af37af954c6c1	minimax theorems for extended real-valued abstract convex-concave functions	abstract convexity;extended $$\varphi $$φ-convex functions;minimax theorems;32f17;49j52;49k27;49k35;52a01	In this paper, we provide sufficient and necessary conditions for the minimax equality for extended real-valued abstract convex–concave functions. As an application, we get sufficient and necessary conditions for the minimax equality for extended real-valued convex–concave functions.	concave function;minimax	Monika Syga	2018	J. Optimization Theory and Applications	10.1007/s10957-017-1210-4		Theory	71.51337623960565	20.753164415425392	191881
90ead7953747368b5edb376b3ddbdd5c235cab85	modified hybrid projection methods for finding common solutions to variational inequality problems	extragradient method;gradient method;equilibrium problem;generalized equilibrium problem;variational inequality	In this paper we propose several modified hybrid projection methods for solving common solutions to variational inequality problems involving monotone and Lipschitz continuous operators. Based on differently constructed half-spaces, the proposed methods reduce the number of projections onto feasible sets as well as the number of values of operators needed to be computed. Strong convergence theorems are established under standard assumptions imposed on the operators. An extension of the proposed algorithm to a system of generalized equilibrium problems is considered and numerical experiments are also presented.	algorithm;experiment;numerical analysis;social inequality;variational inequality;variational principle;monotone	Dang Van Hieu;Pham Ky Anh;Le Dung Muu	2017	Comp. Opt. and Appl.	10.1007/s10589-016-9857-6	mathematical optimization;mathematical analysis;variational inequality;gradient method;mathematics;mathematical economics	AI	73.57570680401652	22.728215450051707	192027
f57a3325bdd6299ab99f14bdd3872ef27dbfde89	on the exact solution of the euclidean three-matching problem	exact solution			Gábor Magyar;Mika Johnsson;Olli Nevalainen	1999	Acta Cybern.		exact solutions in general relativity;computer science	Theory	77.89674823321477	21.4714886593569	192435
39b3ca4a17fc15bd4202475abd41965e767dab3d	a robust implementation of a sequential quadratic programming algorithm with successive error restoration	sqp;line search;noisy functions;nonlinear programming;quasi newton;sequential quadratic programming;numerical algorithm;restart	We consider sequential quadratic programming (SQP) methods for solving constrained nonlinear programming problems. It is generally believed that SQP methods are sensitive to the accuracy by which partial derivatives are provided. One reason is that differences of gradients of the Lagrangian function are used for updating a quasi-Newton matrix, e.g., by the BFGS formula. The purpose of this paper is to show by numerical experimentation that the method can be stabilized substantially. Even in case of large random errors leading to partial derivatives with at most one correct digit, termination subject to an accuracy of 10−7 can be achieved, at least in 90 % of 306 problems of a standard test suite. The algorithm is stabilized by a non-monotone line search and, in addition, by internal and external restarts in case of errors when computing the search direction due to inaccurate derivatives. Additional safeguards are implemented to overcome the situation that an intermediate feasible iterate might get an objective function value smaller than the final stationary point. In addition, we show how initial and periodic scaled restarts improve the efficiency in a situation with very slow convergence.	best, worst and average case;broyden–fletcher–goldfarb–shanno algorithm;circuit restoration;distributed computing;experiment;gradient;iteration;line search;loss function;newton;nonlinear programming;nonlinear system;numerical analysis;numerical differentiation;optimization problem;spmd;sequential quadratic programming;stationary process;test suite;monotone	Klaus Schittkowski	2011	Optimization Letters	10.1007/s11590-010-0207-9	mathematical optimization;machine learning;mathematics;sequential quadratic programming;line search;algorithm	ML	76.35872360397681	24.40168470989952	193438
0607837080e134ba5f4e83cc408013d8af95906d	decomposition-based interior point methods for two-stage stochastic semidefinite programming	interior point methods;semidefinite programming;benders decomposition;90c20;90c25;primal methods;90c15;90c22;90c05;stochastic programming;interior point method;semidefinite program	We introduce two-stage stochastic semidefinite programs with recourse and present an interior point algorithm for solving these problems using Bender’s decomposition. This decomposition algorithm and its analysis extend Zhao’s results [Math. Program., 90 (2001), pp. 507-536] for stochastic linear programs. The convergence results are proved by showing that the logarithmic barrier associated with the recourse function of two-stage stochastic semidefinite programs with recourse is a strongly self-concordant barrier on the first stage solutions. The short-step variant of the algorithm requires $O(\sqrt{p+Kr} \ln \mu^0/\epsilon)$ Newton iterations to follow the first stage central path from a starting value of the barrier parameter $\mu^0$ to a terminating value e. The long-step variant requires $O((p+Kr) \ln \mu^0/\epsilon)$ damped Newton iterations. The calculation of the gradient and Hessian of the recourse function and the first stage Newton direction decomposes across the second stage scenarios.	interior point method;semidefinite programming	Sanjay Mehrotra;M. Gökhan Özevin	2007	SIAM Journal on Optimization	10.1137/050622067	mathematical optimization;combinatorics;mathematical analysis;interior point method;mathematics;semidefinite programming	Theory	74.21453299764732	24.326503837488257	193514
95e6ec5305239b2229d5bb6edd3d3c5d1d9db083	a fast newton's method for a nonsymmetric algebraic riccati equation	linear algebra;experimental design;algorithme rapide;15xx;adaptacion;riccati equation;newton s iteration;arithmetic operation;analisis numerico;singularite;decalaje;algorithm complexity;65f05;05bxx;matrice cauchy;methode newton;complejidad algoritmo;operation arithmetique;aproximacion;matrice coefficient;ecuacion algebraica;equation matricielle;newton iteration;plan experiencia;rango;transport theory;cauchy matrix;equation structurelle;operacion aritmetica;m matrix;decalage;matriz cauchy;methode algebrique;analyse numerique;structured matrices;approximation;58c15;symmetric algebra;iteraccion;numerical analysis;62k99;matrix equation;complexite algorithme;plan experience;equation riccati;algebre lineaire;fast algorithm;adaptation;algebraic method;rang;algebraic riccati equation;39b42;nonsymmetric algebraic riccati equation;matriz m;coste;singularidad;ecuacion matricial;singular solution;iteration;quadratic convergence;algebra lineal;equation algebrique;metodo newton;newton method;shift;numerical linear algebra;ecuacion riccati;metodo algebraico;numerical experiment;matrice m;structure equation;65h10;convergence quadratique;jacobien;algebraic equation;algoritmo rapido;solution singuliere;rank;solucion singular;structural properties;singularity;cout;15a24	A special instance of the algebraic Riccati equation XCX−XE−AX+B = 0 where the n × n matrix coefficients A,B,C,E are rank structured matrices is considered. Relying on the structural properties of Cauchy-like matrices, an algorithm is designed for performing the customary Newton iteration in O(n2) arithmetic operations (ops). The same technique is used to reduce the cost of the algorithm proposed by L.-Z. Lu in [Numer. Linear Algebra Appl., 12 (2005), pp. 191– 200] from O(n3) to O(n2) ops while still preserving quadratic convergence in the generic case. As a byproduct we show that the latter algorithm is closely related to the customary Newton method by simple formal relations. In critical cases where the Jacobian at the required solution is singular and quadratic convergence turns to linear, we provide an adaptation of the shift technique in order to get rid of the singularity. The original equation is transformed into an equivalent Riccati equation where the singularity is removed while the matrix coefficients maintain the same structure as in the original equation. This leads to a quadratically convergent algorithm with complexity O(n2) which provides approximations with full precision. Numerical experiments and comparisons which confirm the effectiveness of the new approach are reported.	algebraic riccati equation;algebraic equation;algorithm;approximation;coefficient;experiment;iteration;jacobian matrix and determinant;lu decomposition;linear algebra;newton's method;rate of convergence;technological singularity;the matrix	Dario Bini;Bruno Iannazzo;Federico Poloni	2008	SIAM J. Matrix Analysis Applications	10.1137/070681478	mathematical optimization;mathematical analysis;linear algebra;algebraic riccati equation;calculus;riccati equation;mathematics;newton's method;algebra	ML	80.84905280246845	20.92218951107105	193585
cde1a8ffe0e6beb985d6be2e30b2c383f14ae113	complexity of the derivative-free solution of systems of ivps with unknown singularity hypersurface	piecewise regularity;derivative free scheme;complexity;initial value problems;locating singularities;unknown switching hypersurface	It is well-known that classical integration schemes designed for regular systems of IVPs become inefficient for piecewise regular problems. To avoid a significant increase of the cost of obtaining an e -approximation for piecewise regular problems, special adaptive algorithms are usually designed. Constructing such algorithms is often based on the knowledge of a 'switching function' whose zeros define the singularity hypersurface, or it relies on certain heuristic arguments. In this paper we analyze the worst-case e -complexity of globally continuous and piecewise regular problems with unknown smooth switching function. We consider problems with right-hand side functions from the class F r , ? which consists of piecewise r -smooth functions with Holder r -th partial derivatives with the exponent ? ? ( 0 , 1 . In contrast to our previous work, we restrict ourselves to information defined only by values of the right-hand side function (computation of partial derivatives is not allowed). We design an algorithm that is based on the computation of Lagrange interpolation polynomials to locate singularity points and to approximate a function of a single variable at intermediate stages. A rigorous analysis (independent of heuristic arguments) of its error and cost is given. It turns out that both the error and the cost of the algorithm are of the same order as for regular problems. That is, no reduction of the order is observed when passing through singularities, and the cost of the algorithm remains at the same level as for regular problems. In the class F r , ? the algorithm has the worst-case error O ( m - ( r + ? ) ) (in the Chebyshev norm) and the total cost O ( m ) , where m is the number of discretization points. This leads to the conclusion that the worst-case e -complexity of the considered problem in F r , ? is ? ( e - 1 / ( r + ? ) ) , and the minimal cost is achieved by the defined algorithm that only needs function evaluations. The auxiliary algorithm that locates singularities and approximates a piecewise regular function of a single variable is also original, and it may be of interest per se.		Boleslaw Z. Kacewicz;Pawel Przybylowicz	2015	J. Complexity	10.1016/j.jco.2014.07.002	mathematical optimization;mathematical analysis;discrete mathematics;complexity;mathematics;initial value problem;algorithm;piecewise;algebra	Logic	76.27391722334549	19.203998525087247	193713
42fd3d4450bb94a5b273cfb493a35629ad58e658	duality for higher order variational control programming problems	higher order duality;optimal control;generalized convexity;mixed type dual	In this article, a variational control problem is considered. We introduce the concept of higher order (F, G, ρ)convex function for a continuous case. Further, we formulate higher order mixed-type dual for the considered problem and obtain appropriate duality results using aforesaid assumptions.	control theory;variational principle	Sarita Sharma	2017	ITOR	10.1111/itor.12192	mathematical optimization;combinatorics;duality;optimal control;duality gap;weak duality;mathematics;strong duality;mathematical economics	ML	72.25568891794062	21.16936468370509	193795
98d3963ff55aa771dc2ac4d924b52d573f1ad383	generalized solution sets of the interval generalized sylvester matrix equation ∑i=1paixi + ∑j=1qyjbj = c and some approaches for inner and outer estimations		In this work, we extend the concept of generalized solution sets that is introduced for the first time by Shary (1996, 1999), to the interval generalized Sylvester matrix equation ? i = 1 p A i X i + ? j = 1 q Y j B j = C . Then AE-solution sets for this equation will be characterized and we develop some approaches (called algebraic approaches) in which the inner and outer estimation problems for some special cases of AE-solution sets reduce to problems of computing algebraic solutions of some auxiliary interval equations. Also we present a numerical technique that is an extension of the well-known interval Gauss-Seidel method for outer estimation of the solution set.	algebraic equation;algorithm;gauss–seidel method;iterative method;linear algebra;newton;newton's method;numerical analysis;subderivative;sylvester matrix	Marzieh Dehghani-Madiseh;Mehdi Dehghan	2014	Computers & Mathematics with Applications	10.1016/j.camwa.2014.10.014	mathematical optimization;mathematical analysis;discrete mathematics;mathematics	ML	76.86946981492123	19.290357417725097	193945
5ca6b6c022fda3e1b3c7f0cb2d7627a942218545	conjugate gradient least squares algorithm for solving the generalized coupled sylvester-conjugate matrix equations		Abstract In this study, we consider the minimum-norm least squares solution of the generalized coupled Sylvester-conjugate matrix equations by conjugate gradient least squares algorithm. When the system is consistent, the exact solution can be obtained. When the system is inconsistent, the least squares solution can be obtained within finite iterative steps in the absence of round-off error for any initial matrices. Furthermore, we can get the minimum-norm least squares solution by choosing special types of initial matrices. Finally, some numerical examples are given to demonstrate the algorithm considered is quite effective in actual computation.	algorithm;conjugate gradient method;least squares	Jing-Jing Hu;Changfeng Ma	2018	Applied Mathematics and Computation	10.1016/j.amc.2018.03.119	mathematical analysis;mathematical optimization;conjugate transpose;computation;exact solutions in general relativity;mathematics;matrix (mathematics);least squares;algorithm;conjugate gradient method	ML	81.22762656688755	21.62061451893102	194113
c325e7f461b81802d5f02fe4720b95f5dacaa275	modified subspace barzilai-borwein gradient method for non-negative matrix factorization	alternating least squares;active sets;non negative matrix factorization;non monotone technique	Non-negative matrix factorization (NMF) is a problem to obtain a representation of data using non-negativity constraints. Since the NMF was first proposed by Lee, NMF has attracted much attention for over a decade and has been successfully applied to numerous data analysis problems. Recent years, many variants of NMF have been proposed. Common methods are: iterative multiplicative update algorithms, gradient descent methods, alternating least squares (ANLS). Since alternating least squares has nice optimization properties, various optimization methods can be used to solve ANLS's subproblems. In this paper, we propose a modified subspace Barzilai-Borwein for subproblems of ANLS. Moreover, we propose a modified strategy for ANLS. Global convergence results of our algorithm are established. The results of numerical experiments are reported to show the effectiveness of the proposed algorithm.	gradient method;non-negative matrix factorization	Hongwei Liu;Xiangli Li	2013	Comp. Opt. and Appl.	10.1007/s10589-012-9507-6	mathematical optimization;combinatorics;machine learning;mathematics;non-negative matrix factorization	Vision	75.64794175864543	25.79173703047476	194209
ce74dc0a3d8df053174c7ed5ae0467277ca10c9c	projected landweber iteration for matrix completion	algorithme rapide;calculo de variaciones;analisis numerico;quadratic program;convergence;computacion informatica;matematicas aplicadas;mathematiques appliquees;probleme non lineaire;echantillonnage;65kxx;optimization method;65k10;metodo optimizacion;nonlinear problems;projected landweber iteration;analyse numerique;sampling;49xx;calcul variationnel;convergencia;iteraccion;numerical analysis;matrix completion;mathematical programming;ciencias basicas y experimentales;fast algorithm;matematicas;nuclear norm;methode optimisation;iteration;optimization;muestreo;grupo a;applied mathematics;programmation mathematique;programacion matematica;algoritmo rapido;variational calculus	Recovering an unknown low-rank or approximately low-rank matrix from a sampling set of its entries is known as the matrix completion problem. In this paper, a nonlinear constrained quadratic program problem concerning the matrix completion is obtained. A new algorithm named the projected Landweber iteration (PLW) is proposed, and the convergence is proved strictly. Numerical results show that the proposed algorithm can be fast and efficient under suitable prior conditions of the unknown low-rank matrix.	landweber iteration	H. Zhang;L. Z. Cheng	2010	J. Computational Applied Mathematics	10.1016/j.cam.2010.06.010	sampling;mathematical optimization;iteration;convergence;power iteration;numerical analysis;matrix norm;landweber iteration;calculus;mathematics;state-transition matrix;quadratic programming;algorithm;calculus of variations	ML	77.92984837360714	23.095220785400702	194289
5259ea8f797334f9feef0963e117d93aa33c07cd	on the convergence of ica algorithms with symmetric orthogonalization	optimisation;convergence;critical point;convergence independent component analysis covariance matrix symmetric matrices blind source separation source separation particle separators matrix decomposition cost function engineering profession;blind source separation;search matrix independent component analysis symmetric orthogonalization contrast function maximization;convergence independent component analysis blind source separation symmetric orthogonalization fixed point algorithms;independent component analysis;symmetric orthogonalization;fixed point;optimization problem;optimisation blind source separation convergence independent component analysis;fixed point algorithms;contrast function maximization;search matrix	Independent component analysis (ICA) problem is often posed as the maximization/minimization of an objective/cost function under a unitary constraint, which presumes the prewhitening of the observed mixtures. The parallel adaptive algorithms corresponding to this optimization setting, where all the separators are jointly trained, are typically implemented by a gradient-based update of the separation matrix followed by the so-called symmetrical orthogonalization procedure to impose the unitary constraint. This article addresses the convergence analysis of such algorithms, which has been considered as a difficult task due to the complication caused by the minimum-(Frobenius or induced 2-norm) distance mapping step. We first provide a general characterization of the stationary points corresponding to these algorithms. Furthermore, we show that fixed point algorithms employing symmetrical orthogonalization are monotonically convergent for convex objective functions. We later generalize this convergence result for nonconvex objective functions. At the last part of the article, we concentrate on the kurtosis objective function as a special case. We provide a new set of critical points based on Householder reflection and we also provide the analysis for the minima/maxima/saddle-point classification of these critical points.	expectation–maximization algorithm;fixed point (mathematics);gradient;householder transformation;independent computing architecture;independent component analysis;loss function;mathematical optimization;maxima and minima;optimization problem;stationary process	Alper T. Erdogan	2008	2008 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2008.4518012	independent component analysis;optimization problem;mathematical optimization;convergence;computer science;machine learning;mathematics;blind signal separation;fixed point;critical point;statistics	ML	72.8232891543414	27.326259273519568	194376
27ca72c43c9550dafd200cb5c8c1e933786129eb	successive convex approximation algorithms for sparse signal estimation with nonconvex regularizations		In this paper, we propose a successive convex approximation framework for sparse optimization where the nonsmooth regularization function in the objective function is nonconvex and it can be written as the difference of two convex functions. The proposed framework is based on a nontrivial combination of the majorization–minimization framework and the successive convex approximation framework proposed in literature for a convex regularization function. The proposed framework has several attractive features, namely, first, flexibility, as different choices of the approximate function lead to different types of algorithms; second, fast convergence, as the problem structure can be better exploited by a proper choice of the approximate function and the stepsize is calculated by the line search; third, low complexity, as the approximate function is convex and the line search scheme is carried out over a differentiable function; fourth, guaranteed convergence to a stationary point. We demonstrate these features by two example applications in subspace learning, namely the network anomaly detection problem and the sparse subspace clustering problem. Customizing the proposed framework by adopting the best-response type approximation, we obtain soft-thresholding with exact line search algorithms for which all elements of the unknown parameter are updated in parallel according to closed-form expressions. The attractive features of the proposed algorithms are illustrated numerically.		Yang Yang;Marius Pesavento;Symeon Chatzinotas;Björn E. Ottersten	2018	2018 IEEE 10th Sensor Array and Multichannel Signal Processing Workshop (SAM)	10.1109/JSTSP.2018.2877584	differentiable function;mathematical optimization;computer science;approximation algorithm;convex function;regularization (mathematics);subspace topology;cluster analysis;expression (mathematics);stationary point	ML	74.40959662357449	25.847398671123717	194832
d1403f5c7426ffcbd50f0f85d0d631382a0517e4	a new extrapolation method for pagerank computations	trace;power method;pagerank;extrapolation method;arnoldi type algorithm	The PageRank algorithm is widely considered these years because of its great significance in search engine technology and other scientific domains. Though the power method is the initial measure to settle the PageRank problem, it gives poor convergence when the damping factor is sufficiently close to   1     1       . This difficulty encourages researchers to present improved iterative methods for accelerating PageRank computations. In this paper, a cheap and practical extrapolation approach which is determined by the trace of the Google matrix is proposed, and it is more attractive when combined with the well-known Arnoldi-type algorithm. Convergence analysis of our algorithms is given. Numerical examples illustrate the efficiency of these accelerated schemes.	computation;extrapolation;pagerank	Xueyuan Tan	2017	J. Computational Applied Mathematics	10.1016/j.cam.2016.08.034	mathematical optimization;power iteration;theoretical computer science;trace;mathematics;extrapolation;algorithm;algebra	NLP	82.7930152389569	24.601422327893026	194911
7f7234f5a629b655f17fec7792bddc600de60af7	approximate algorithms to derive exact solutions to systems of linear equations	approximate algorithm;exact solution;linear equations	Without Abstract	approximation algorithm;linear equation;system of linear equations	Robert T. Moenck;John H. Carter	1979		10.1007/3-540-09519-5_60	system of linear equations;mathematical optimization;coefficient matrix;quantum algorithm for linear systems of equations;relaxation	EDA	77.89478080130132	21.649449837299628	194978
8635334b22e625f8a5fff94a18fe5537499b182e	a new distillation algorithm for floating-point summation	somme partielle;calcul scientifique;destilacion;analisis numerico;65b10;floating point summation;distiilation 65g05;borne erreur;partial sums;summation;sumacion;rounding errors;65g05;analyse numerique;algorithme;algorithm;computacion cientifica;numerical analysis;estimation erreur;error estimation;calcul numerique;numerical computation;calculo numerico;estimacion error;suma parcial;floating point;coma flotante;error redondear;error bound;floating point arithmetic;scientific computation;distillation;sommation;rounding error;virgule flottante;erreur arrondi;algoritmo;partial sum	The summation of n floating-point numbers is ubiquitous in numerical computations. We present a new distillation algorithm for floating-point summation which is stable, efficient, and accurate. The algorithm iteratively “distills” the summands without discarding any significant digit until the partial sums cannot change the whole sum. It uses standard floating-point arithmetic and does not rely on the choice of radix or any other specific assumption. Furthermore, the error bound of our algorithm is independent of n and less than 1 ulp.	command & conquer:yuri's revenge;computation;dijkstra's algorithm;ewald summation;nl (complexity);numerical analysis;numerical aperture;operational amplifier;recursion;significant figures;time complexity	Yong-Kang Zhu;Jun-Hai Yong;Guo-Qin Zheng	2005	SIAM J. Scientific Computing	10.1137/030602009	arithmetic;kahan summation algorithm;summation;floating point;summation by parts;pairwise summation;calculus;mathematics;algorithm	PL	82.60439335140211	20.622830743555596	194992
de82c00c5ae4d74b78d97f572cdcceaffe41a9c2	superlinear convergence of primal-dual interior point algorithms for nonlinear programming	superlinear convergence;primal dual interior point method;90c26;nonlinear programming;merit function;90c30;local convergence;componentwise q superlinear convergence;65k05;90c51;interior point algorithm	The local convergence properties of a class of primal-dual interior point methods are analyzed. These methods are designed to minimize a nonlinear, nonconvex, objective function subject to linear equality constraints and general inequalities. They involve an inner iteration in which the log-barrier merit function is approximately minimized subject to satisfying the linear equality constraints, and an outer iteration that specifies both the decrease in the barrier parameter and the level of accuracy for the inner minimization. Under nondegeneracy assumptions, it is shown that, asymptotically, for each value of the barrier parameter, solving a single primal-dual linear system is enough to produce an iterate that already matches the barrier subproblem accuracy requirements. The asymptotic rate of convergence of the resulting algorithm is Q-superlinear and may be chosen arbitrarily close to quadratic. Furthermore, this rate applies componentwise. These results hold in particular for the method described in [A. R. Conn, N. I. M. Gould, D. Orban, and P. L. Toint, Math. Program. Ser. B, 87 (2000), pp. 215--249] and indicate that the details of its inner minimization are irrelevant in the asymptotics, except for its accuracy requirements.	algorithm;nonlinear programming;rate of convergence	Nicholas I. M. Gould;Dominique Orban;Annick Sartenaer;Philippe L. Toint	2001	SIAM Journal on Optimization	10.1137/S1052623400370515	local convergence;mathematical optimization;mathematical analysis;nonlinear programming;calculus;mathematics	Theory	75.00028151026243	24.590943493785367	195268
b1369b25a5a49ce3a186e943e7ae5a2ff4ecc103	superquadratic convergence of dlasq for computing matrix singular values	analisis numerico;valor singular;computacion informatica;matematicas aplicadas;mathematiques appliquees;65fxx;matrice reelle;implementation;bidiagonal matrix;singular value;convergence asymptotique;arithmetique;analyse numerique;real matrix;algorithme;convergencia asintotica;algorithm;numerical analysis;aritmetica;arithmetics;valeur singuliere;ciencias basicas y experimentales;matematicas;algebra lineal numerica;algebre lineaire numerique;asymptotic convergence;numerical linear algebra;dlasq;implementacion;grupo a;applied mathematics;65f15;matriz real;algoritmo	DLASQ is a routine in LAPACK for computing the singular values of a real upper bidiagonal matrix with high accuracy. The basic algorithm, the so-called dqds algorithm, was first presented by Fernando-Parlett, and implemented as the DLASQ routine by Parlett-Marques. DLASQ is now recognized as one of the most efficient routines for computing singular values. In this paper, we prove the asymptotic superquadratic convergence of DLASQ in exact arithmetic.		Kensuke Aishima;Takayasu Matsuo;Kazuo Murota;Masaaki Sugihara	2010	J. Computational Applied Mathematics	10.1016/j.cam.2009.07.021	numerical analysis;calculus;singular solution;mathematics;numerical linear algebra;implementation;bidiagonal matrix;singular value;algorithm;algebra	HPC	81.22648719496114	19.950365410089432	195697
e0c58ac8fc34fd9a4571a723479a21db24bdc0e5	existence and iterative algorithm of solutions for a system of generalized nonlinear mixed variational-like inequalities		We introduce and study a system of generalized nonlinear mixed variational-like inequality problems SGNMVLIPs in Banach spaces. The auxiliary principle technique is applied to study the existence and iterative algorithm of solutions for the SGNMVLIP. First, the existence of solutions of the auxiliary problems for the SGNMVLIP is shown. Second, an iterative algorithm for solving the SGNMVLIP is constructed by using this existence result. Finally, not only the existence of solutions of the SGNMVLIP is shown but also the convergence of iterative sequences generated by the algorithm is also proven. The technique and results presented in this paper generalize and unify the corresponding techniques and results given in the literature.		Yan-Mei Du	2012	J. Applied Mathematics	10.1155/2012/176283	mathematical optimization;combinatorics;mathematical analysis;mathematics;iterative method	ML	73.18789948286357	21.437933116760163	195712
31988bdbab2b2a5fef2287c95ac8e2546ec8077d	an eigenvalue symmetric matrix contractor		We propose an eigenvalue contractor for symmetric matrices. Given a symmetric interval matrix A and an interval approximation of its eigenvalue sets λ1, . . . , λn the contractor reduces the entries of A S such that no matrix with eigenvalues in λ1, . . . , λn is omitted. Our contractor is based on sequentially reducing the entries of A. We discuss properties of the method and demonstrate its performance on examples.	approximation;bisection method;experiment;numerical analysis	Milan Hladík;Luc Jaulin	2011	Reliable Computing		combinatorics;mathematical analysis;discrete mathematics;mathematics	ML	79.57826121161017	21.165203212602083	195900
39c2fb91f9ad0fc2559c1b744a963ec2317e7187	low rank solution of lyapunov equations	alternating direction implicit iteration;linear algebra;iterative method;optimal solution;solution optimale;exposant lyapunov;mimo system;sous espace invariant;metodo subespacio krylov;invaraint subspace;lyapunov equation;best approximation;right hand side;krylov subspace method;methode sousespace krylov;93c05;multiple input multiple output system;metodo iterativo;algorithme;cholesky factorization;iterative methods;algorithm;adi algorithm;methode iterative;algebre lineaire;solucion optima;invariant subspace;lyapunov exponent;algebra lineal;exponente lyapunov;iteration implicite;dominant invariant subspace;krylov subspace;alternating direction implicit algorithm;low rank approximation;iteration method;implicit interation;65f30;65f10;algoritmo;15a24	This paper presents the Cholesky factor–alternating direction implicit (CF–ADI) algorithm, which generates a low rank approximation to the solution X of the Lyapunov equation AX +XAT = −BBT . The coefficient matrix A is assumed to be large, and the rank of the righthand side −BBT is assumed to be much smaller than the size of A. The CF–ADI algorithm requires only matrix-vector products and matrix-vector solves by shifts of A. Hence, it enables one to take advantage of any sparsity or structure in A. This paper also discusses the approximation of the dominant invariant subspace of the solution X. We characterize a group of spanning sets for the range of X. A connection is made between the approximation of the dominant invariant subspace of X and the generation of various low order Krylov and rational Krylov subspaces. It is shown by numerical examples that the rational Krylov subspace generated by the CF–ADI algorithm, where the shifts are obtained as the solution of a rational minimax problem, often gives the best approximation to the dominant invariant subspace of X.	algorithm;alternating direction implicit method;approximation algorithm;cholesky decomposition;coefficient;file spanning;krylov subspace;low-rank approximation;lyapunov fractal;minimax;numerical analysis;sparse matrix	Jing-Rebecca Li;Jacob K. White	2002	SIAM J. Matrix Analysis Applications	10.1137/S0895479801384937	mathematical optimization;combinatorics;mathematical analysis;krylov subspace;linear algebra;mathematics;iterative method;algebra	ML	80.56827543786308	21.23547360365723	196181
8f5543d992ed2940d64774fac5050bfceaf2fd47	a branch-and-bound method for reversed geometric programming	institutional repositories;fedora;methods;geometric program;vital;branch and bound method;vtls;programs;ils	A general or signomial geometric program is a nonlinear mathematical program involving general polynomials in several variables both in the objective function and the constraints. A branch-and-bound method is proposed for this extensive class of nonconvex optimization programs guaranteeing convergence to the global optimum. The subproblems to be solved are convex but the method can easily be combined with a cutting plane technique to generate subproblems which are linear. A simple example is given to illustrate the technique. The combined method involving linear subproblems has been coded and numerical experience with this code will be reported later.	branch and bound;geometric programming	Willy Gochet;Yves Smeers	1979	Operations Research	10.1287/opre.27.5.982	mathematical optimization;overlapping subproblems;theoretical computer science;mathematics;algorithm	Robotics	72.33413533150622	24.08423959145881	196788
d9476857dc194fe5cf861530822454a522a6b37d	large step path-following methods for linear programming, part i: barrier function method	interior point methods;49d;barrier function methods;linear programming;linear program;path following methods;path following;barrier function	The algorithm proposed in this paper is the classical logarithmic barrier function method with Newton–Raphson steps for the internal minimization of the penalized function. Polynomial behavior is obtained by stopping each internal cycle when the iterates approach the central trajectory. Each master iteration updates the penalty parameter by a constant factor, and the overall complexity bound depends on this factor: $O(nL)$ Newton iterations for an arbitrary constant, and $O(\sqrt{n} L)$ iterations for a constant dependent on $\sqrt{n} $.	barrier function;linear programming	Clóvis C. Gonzaga	1991	SIAM Journal on Optimization	10.1137/0801018	mathematical optimization;combinatorics;mathematical analysis;barrier function;linear programming;interior point method;mathematics	Theory	75.03235620353365	24.663008925091546	196813
439e2d8633f911c6d7515e732fde809a9fa01877	concurrent iterative algorithm for toeplitz-like linear systems	parallelisme;iterative algorithms linear systems arithmetic real time systems computer science phase change random access memory concurrent computing vectors equations parallel algorithms;toeplitz like linear systems;newton iteration;matrix algebra;matriz toeplitz;iterative algorithm;parallel arithmetic steps parallel algorithm iterative algorithm toeplitz like linear systems;linear system;rang deplacement;parallelism;paralelismo;computational complexity;index termsparallel algorithm;numerical computation;matrice toeplitz;displacement rank;parallel arithmeticsteps;toeplitz matrix;parallel algorithms computational complexity matrix algebra;parallel algorithms;iteration newton	Suppose that a nonsingular n x 17 matrix A is given with its short displacement generator and thus has small displacement rank bounded by a fixed constant. (The class of such matrices generalizes Toeplitz matrices.) Furthermore, suppose that a good initial approximation to a short displacement generator for A-’ is readily available (as in the case of updating the solution to the linear system A z = b in real time). Then we show how to refine this approximation and numerically compute a displacement generator of A-’ and the solution vector z = X 1 b to a linear system Az = B by using O(log2 n ) parallel arithmetic steps and n processors. The techniques of our earlier work enable us to extend these results to some other important classes of dense structure matrices.	approximation;ccir system a;central processing unit;displacement mapping;iterative method;linear system;numerical analysis;toeplitz hash algorithm;tree structure	Victor Y. Pan	1993	IEEE Trans. Parallel Distrib. Syst.	10.1109/71.224221	mathematical optimization;computer science;toeplitz matrix;parallel algorithm;iterative method;newton's method;linear system;computational complexity theory;algorithm	Theory	82.22833774424358	26.897868785915904	196835
5cf209511542808c5106dfd85db46d8143371b5e	can the tpri structure help us to solve the algebraic eigenproblem?	similar arrow-head matrix;tpri matrix;recent unitary similarity;customary approach;tpri form;algebraic eigenproblem;qr algorithm;tpri structure;hessenberg matrix;resulting eigenproblem;rank-one matrix;similarity transformation	We modify the customary approach to solving the algebraic eigenproblem. Instead of applying the QR algorithm to a Hessenberg matrix, we begin with the recent unitary similarity transform into a triangular plus rank-one matrix. Then we show nonunitary transforms of this matrix at a low arithmetic cost into similar arrow-head matrices. The resulting eigenproblem can be effectively solved by the known algorithms. Based on some properties of the TPRI matrices, we also show that the similarity transforms into both Hessenberg and TPRI forms tend to decrease the geometric multiplicities of the eigenvalues, and we discuss some relevant research topics.	karl hessenberg;linear algebra;qr algorithm	Victor Y. Pan	2005				Theory	78.94495822714062	21.109063417972084	197036
74a56690f24b37021f5d81b7c2030880d284c42c	a projection subgradient method for solving optimization with variational inequality constraints	��-pseudomonotone mapping;��-strongly pseudomonotone mapping;projection-subgradient methods	In this paper, we propose a projection subgradient method for solving some classical variational inequality problem over the set of solutions of mixed variational inequalities. Under the conditions that \(T\) is a \(\Theta \)-pseudomonotone mapping and \(A\) is a \(\rho \)-strongly pseudomonotone mapping, we prove the convergence of the algorithm constructed by projection subgradient method. Our algorithm can be applied for instance to some mathematical programs with complementarity constraints.	calculus of variations;mathematical optimization;social inequality;subgradient method;variational inequality	Fu-quan Xia;Tao Li;Yun-Zhi Zou	2014	Optimization Letters	10.1007/s11590-012-0573-6	mathematical optimization;combinatorics;mathematical analysis;mathematics	ML	72.73542965431416	21.931135336922548	197090
e2b9eb08f226a68bf31a490b287d4217e56bdae5	efficient coordinate-wise leading eigenvector computation		We develop and analyze efficient ”coordinate-wise” methods for finding the leading eigenvector, where each step involves only a vector-vector product. We establish global convergence with overall runtime guarantees that are at least as good as Lanczos’s method and dominate it for slowly decaying spectrum. Our methods are based on combining a shift-and-invert approach with coordinate-wise algorithms for linear regression.	bitap algorithm;computation;lanczos resampling;local convergence	Jialei Wang;Weiran Wang;Dan Garber;Nathan Srebro	2018			discrete mathematics;mathematical optimization;computation;lanczos resampling;eigenvalues and eigenvectors;mathematics;linear regression;convergence (routing)	ML	82.07251423382525	23.738694304444742	197294
657c2bbf6241c5e0fbb0726ce8a181753c6355f8	an optimal control problem governed by quasi-linear variational inequalities	49j20;regularity;optimal control;necessary condition;variational inequality;quasi linear;existence;optimal control problem;35j70	An optimal control problem governed by quasi-linear variational inequality is considered. The cost functional to be minimized contains the solution of a quasi-linear variational inequality. To get the existence, regularity, and necessary condition for the optimal pair, a new related control problem is introduced. By proving the existence of an optimal pair to such a new problem, the existence and regularity of the optimal pair to the original problem are obtained. It turns out that the regularity obtained is sharp in general. Some necessary conditions of the optimal pair are also obtained.	calculus of variations;mathematical optimization;optimal control;social inequality;variational inequality	Hongwei Lou	2002	SIAM J. Control and Optimization	10.1137/S0363012900375032	mathematical optimization;combinatorics;mathematical analysis;variational inequality;optimal control;control theory;mathematics	Theory	72.12511419858757	20.447510671056314	197331
2e2f41b43067d0d09852aa55a8bbce905f2773e1	quasi-newton methods for nonlinear equations	generic algorithm;quasi newton;newton raphson method;nonlinear systems;nonlinear equation;algorithms;quasi newton method;nonlinear equations;iterative solution;nonlinear system;systems of nonlinear equations;simultaneous equations;newton methods;problem solving	A unified derivation is presented of the quasi-Newton methods for solving systems of nonlinear equations. The general algorithm contains, as special cases, all of the previously proposed quasi-Newton methods.	algorithm;newton;nonlinear system;quasi-newton method	Frank J. Zeleznik	1968	J. ACM	10.1145/321450.321458	local convergence;mathematical optimization;mathematical analysis;genetic algorithm;quasi-newton method;simultaneous equations;nonlinear system;calculus;newton's method in optimization;mathematics;newton's method;cfd-dem	Theory	76.94860222770711	23.807861256955295	197418
26b080ef3a1cfdfb0d5615ba31a7397a4921d3a6	on finite termination of an iterative method for linear complementarity problems	matrice jacobi;terminaison finie;methode newton;generalized jacobian;equation non lisse;jacobi matrix;mathematical programming;matriz jacobi;complementarite lineaire;probleme complementarite;problema complementariedad;complementarity problem;metodo newton;newton method;iteration method;linear complementarity problem;programmation mathematique;systems of nonlinear equations	Based on a well-known reformulation of the linear complementarity problem (LCP) as a nondifferentiable system of nonlinear equations, a Newton-type method will be described for the solution of LCPs. Under certain assumptions, it will be shown that this method has a finite termination property, i.e., if an iterate is sufficiently close to a solution of LCP, the method finds this solution in one step. This result will be applied to a recently proposed algorithm by Harker and Pang in order to prove that their algorithm also has the finite termination property.	algorithm;complementarity theory;iteration;iterative method;linear complementarity problem;newton;nonlinear system	Andreas Fischer;Christian Kanzow	1996	Math. Program.	10.1007/BF02592200	jacobian matrix and determinant;mathematical optimization;mathematical analysis;calculus;mathematics;iterative method;newton's method;linear complementarity problem	EDA	75.98063608048406	22.531552046588303	197427
e49b8609984ab3870dda8d7099595bb9e6a323d0	computations of multi-resultant with mechanization	complexite;resultant dixon;analisis numerico;polynomial system;matematicas aplicadas;multi resultant;mathematiques appliquees;zero de polynome;calcul formel;complejidad;zero of polynomial;theory and method;complexity;calculo automatico;analyse numerique;computing;calculo formal;equation polynomiale;polynomial equation;algorithme;calcul automatique;resolucion sistema ecuacion;algorithm;resolution systeme equation;dixon resultant;numerical analysis;ecuacion polinomial;maple software;logiciel maple;cero de polinomio;resultant macaulay;equation system solving;maple;macaulay resultant;applied mathematics;computer algebra;multiresultant;mechanization;algoritmo;racine polynome	Resultant is a classical algebraic tool for determining whether or not a polynomial systems have a common root without explicitly solving for the roots. But, the complexity of resultants can be very high in practice. In this paper, by using the theories and methods of computer algebra, two reliable algorithms for computing multi-resultant problems—Dixon resultant and Macaulay resultant were established, two new Maple procedures dixonres and macaulayres were established, too. Some examples are presented to illustrate the implementations of the programs. This would be useful for solving a system of polynomial equations.	computation;resultant	Weiming Wang;Xinzhe Lian	2005	Applied Mathematics and Computation	10.1016/j.amc.2004.11.034	computing;complexity;symbolic computation;mechanization;numerical analysis;calculus;mathematics;algorithm	Theory	79.25456265343512	18.29007701541022	197462
e2a675acb0093de6eda36ec69def30ad4856dd07	rank-one solutions for sdp relaxation of qcqps in power systems		It has been shown that a large number of computationally difficult problems can be equivalently reformulated into quadratically constrained quadratic programs (QCQPs) in the literature of power systems. Due to the NP-hardness of general QCQPs, main effort of this stream of problems has been put into deriving near-optimal solutions with low computational complexity. Recently, semidefinite programming (SDP) relaxation has been recognized as a promising technique to solve QCQPs from various applications such as the alternating current (ac) optimal power flow (OPF) problem. However, this technique has not been guaranteed to achieve a rank-one solution, which is a necessary condition to recover a feasible solution of the original QCQPs. In this paper, instead of investigating the conditions under which a rank-one solution exists, we propose a general solution framework to derive near-optimal but rank-one solutions for the SDP relaxation of QCQPs. In the proposed algorithm, all the parameters are provided in a systematic manner. In order to demonstrate the effectiveness of our method, the proposed algorithm is applied to solve the ac-OPF and state estimation problems in various settings. Extensive numerical results show that our method succeeds in obtaining rank-one solutions in all our case studies and only small optimality gaps are induced by our approach.		Tian Liu;Bo Sun;Danny H. K. Tsang	2019	IEEE Transactions on Smart Grid	10.1109/TSG.2017.2729082	discrete mathematics;electric power system;semidefinite programming;computational complexity theory;mathematical optimization;mathematics;quadratically constrained quadratic program;quadratic growth;symmetric matrix	EDA	74.48143408574757	26.80719981067947	197628
0ecc1e074e39d36c9b35343544ef613f83b64d50	nonlinear programming without a penalty function or a filter	optimisation sous contrainte;constrained optimization;global solution;non convex programming;matrice jacobi;non linear programming;90c26;intervalo confianza;90c55;nonlinear programming;constrenimiento igualdad;programacion no lineal;equality constraints;condicion estacionaria;90c30;programmation non convexe;global convergence;programmation non lineaire;condition stationnaire;convergence numerique;fonction objectif;optimizacion con restriccion;objective function;numerical convergence;equality constraint;jacobi matrix;confidence interval;programacion no convexa;trust region;65k05;numerical analysis;funcion penalidad;mathematical programming;filter;matriz jacobi;intervalle confiance;numerical algorithm;numerical algorithms;stationary condition;filtre;funcion objetivo;solution globale;numerical experiment;fonction penalite;nonlinear optimization;programmation mathematique;convergencia numerica;solucion global;filtro;programacion matematica;contrainte egalite;penalty function	A new method is introduced for solving equality constrained nonlinear optimization problems. This method does not use a penalty function, nor a filter, and yet can be proved to be globally convergent to first-order stationary points. It uses different trust-regions to cope with the nonlinearities of the objective function and the constraints, and allows inexact SQP steps that do not lie exactly in the nullspace of the local Jacobian. Preliminary numerical experiments on CUTEr problems indicate that the method performs well.	algorithm;cuter;critical point (network science);experiment;first-order predicate;jacobian matrix and determinant;kernel (linear algebra);mathematical optimization;nonlinear programming;nonlinear system;numerical analysis;optimization problem;penalty method;preconditioner;sequential quadratic programming;stationary process	Nicholas I. M. Gould;Philippe L. Toint	2010	Math. Program.	10.1007/s10107-008-0244-7	jacobian matrix and determinant;mathematical optimization;mathematical analysis;confidence interval;numerical analysis;filter;nonlinear programming;calculus;penalty method;mathematics;trust region	ML	75.98789013945694	21.81382682735768	197648
c58fd644cb2e193ff6ac18f700d1011cd352408c	scalarization of constraints system in some vector optimization problems and applications		In this work we employ a new method to penalize a constrained non solid vector optimization problem by means of a scalarization functional applied to the constraints system. Then, we formulate optimality conditions which mainly use several types of regularity for single and set-valued maps. In order to motivate our demarche, we discuss in detail the assumptions used in the main results and we show how it can be verified.	mathematical optimization;vector optimization	Marius Durea;Radu Strugariu	2014	Optimization Letters	10.1007/s11590-013-0690-x	mathematical optimization;combinatorics;mathematics;algorithm	EDA	72.17981101745733	21.917635061968003	197669
431e4d913e38a3ca602a0aae850c1c03277420c7	theoretical bounds for algebraic multigrid performance: review and analysis	predictive theory;algebraic multigrid;convergence bounds	Algebraic multigrid methods continue to grow in robustness as effective solvers for the large and sparse linear systems of equations that arise in many applications. Unlike geometric multigrid approaches, however, the theoretical analysis of algebraic multigrid is less predictive of true performance. Multigrid convergence factors naturally depend on the properties of the relaxation, interpolation, and coarse-grid correction routines used, yet without the tools of Fourier analysis, optimal and practical bounds for algebraic multigrid are not easily quantified. In this paper, we survey bounds from existing literature, with particular focus on the predictive capabilities of the theory, and provide new results relating existing bounds. We highlight the impact of these theoretical observations through several model problems and discuss the role of theoretical bounds on practical performance. Copyright © 2014 John Wiley & Sons, Ltd.	algebraic equation;computability;critical graph;fourier analysis;ibm notes;interpolation;john d. wiley;linear algebra;linear programming relaxation;linear system;multigrid method;sparse matrix	Scott P. MacLachlan;Luke N. Olson	2014	Numerical Lin. Alg. with Applic.	10.1002/nla.1930	mathematical optimization;combinatorics;theoretical computer science;mathematics;real algebraic geometry;multigrid method	AI	82.70183564634921	24.64809108658805	197757
f4f66ca47894a82864f144b51a13eab49e4c8dc9	necessary conditions and duality for inexact nonlinear semi-infinite programming problems	nonlinear programming;convex programming;semi infinite programming;first order	First order necessary conditions and duality results for general inexact nonlinear programming problems formulated in nonreflexive spaces are obtained. The Dubovitskii–Milyutin approach is the main tool used. Particular cases of linear and convex programs are also analyzed and some comments about a comparison of the obtained results with those existing in the literature are given. 1 Problem statements In this paper we deal with the following inexact nonlinear programming problem: min x J (x) = min x max c∈C f (x, c), s.t. h(x, ε) ∈ B, a.e. ε ∈ [0, 1] , g(x, ε) ∈ D + R+, ε ∈ [0, 1] , (1) with the assumptions: (H1) x ∈ Rn and C is a convex and compact set in Rm, (H2) The functions f : Rn × Rm −→ R, h : Rn × [0, 1] −→ Rp and g : R n × [0, 1] −→ Rq are supposed to be continuously differentiable with respect to x, (H3) f and g are continuous and h is measurable and essentially bounded with respect to their second arguments, J. A. Gómez Departamento Ingeniería Matemática, Universidad de La Frontera, Temuco, Chile E-mail: jagomez@ufro.cl P. Bosch (B) Facultad de Ingeniería, Universidad Diego Portales, Santiago, Chile E-mail: paul.bosch@prof.udp.cl 46 J. A. Gómez and P. Bosch (H4) The sets B ⊂ Rp and D ⊂ Rq are supposed to be convex with nonempty interior. Some examples of important particular cases of (1) are the inexact semi-infinite NLP problem: min x max c∈C f (x, c) s.t. g(x, ε) ∈ D + R+, ∀ε ∈ [0, 1], x ∈ Rn, C ⊂ Rn, D ⊂ Rm, the inexact second order cone LP problem: min x max c∈C cT x s.t. Ax ∈ D + Lm+1, where Lm+1 is the Lorentz cone: L m+1 = (ξ1, . . . , ξm, ξm+1) ∈ Rm+1 : ‖(ξ1, . . . , ξm)‖ ≤ ξm+1 } , and the inexact semi-infinite, semi-definite (LP) NLP problem: min x max c∈C f (x, c) F(x, ε) ∈ B + S+, ∀ε ∈ [0, 1], x ∈ Rn, C ⊂ Rn, B ⊂ Sm, where F(x, ε) = F(x, F1(ε), . . . , Fn(ε)) is a (linear) function on x for n given fixed matrices F1, . . . , Fn belonging to the set Sm of symmetric m × m matrices. B is a convex subset of the cone S+ of positive semi-definite symmetric matrices. All the above problems become exact (i.e. with exact data) when B, C and D are singletons. As first pointed out in Tichatschke et al. (1989) and also in Amaya and Gómez (2001), Vandenberghe and Boyd (1998), all these problems are essentially nonlinear problems, even if f, g are affine, then nonlinear programming tools should be used to obtain necessary optimality conditions (mostly in duality theorem forms) and/or to design computational algorithms. An increasing number of papers and books were published in recent years dealing with duality results and algorithmic ideas for the above exact or inexact problems. As an incomplete list of references on these subjects let us only mention Borwein (1981), Goberna and López (1998), Hettich (1986), Hettich and Kortanek (1993), Ramana et al. (1997), Reemtsen (1991), Reemtsen and Görner (1998), Shapiro (1998), Tichatschke et al. (1989), Vandenberghe and Boyd (1998) for semiinfinite programming, Amaya and de Ghellinck (1997), Amaya and Gómez (2001), Matloka (1992), Soyster (1973, 1974) dealing with inexact LP, and Alizadeh and Goldfarb (2002), Ben Tal and Nemirovski (1998, 2001), Goldfarb (2002), Krishnan and Mitchell (2002, 2003a,b), Nesterov and Nemirovskii (1993), Vandenberghe and Boyd (1996) concerning on theory and methods for semidefinite and second order cone programming. It is well known that those models have a large field of applications, including control of robots, eigenvalue computations, mechanical stress of materials, statistical design and others (see, for instance, Ben Tal and Nemirovski 2001; Lobo et al. 1987; Reemtsen 1991). Furthermore, some practical applications of the above more complex models are recently appearing in the context of robust Inexact nonlinear semi-infinite programming problems 47 optimization models in economy (see, for example, Leibfritz and Maruhn 2005). But in fact, any robust formulation of semi-infinite programming problems would also be considered inexact (see Ben Tal and Nemirovski 2001). The use of duality relations for algorithmic design is a classical idea and this approach generally leads to discretization methods and/or cutting plane algorithms (see, for instance, Goberna and López 1998; Hettich 1986; Hettich and Kortanek 1993; Reemtsen 1991). Necessary optimality conditions in the form of strong duality theorems can be obtained using different approaches, where the more general ones are the parameter perturbation and conjugate functionals approach (see, for instance, Rockafellar 1974; Shapiro 1998, 2001), the Lagrangian functional and the Dubovitskii–Milyutin theory (see, for instance, Amaya and Gómez 2001). The Dubovitskii–Milyutin (D–M) approach, first published in Dubovitskii and Milyutin (1965), became very popular and is described extensively in the book of Girsanov (1972). It was used by the authors to obtain a general version of Karush– Kuhn–Tucker theorem for problems containing equality and inequality operator constraints (Bosch and Gómez 1997), the local Pontryagin’s maximum principle for smooth optimal control problems with mixed state constraints (Bosch and Gómez 2000), and more recently, in Amaya and Gómez (2001) and Gómez et al. (2005), to obtain appropriate definitions of dual problems and strong duality theorems for inexact linear programming and inexact semi-infinite linear programming. An extense literature about duality can be found in the references. Our main objective is to obtain first order necessary conditions for problem (1) and strong duality relations in nonlinear and convex cases. For this end we shall use (D–M) approach defining appropriate cones of decreasing, feasible and tangent directions and computing the respective dual cones. Then, we shall apply the following well known (D–M) Lemma: Lemma 1 Let K1, . . . , K p, K p+1 be convex cones with apex θ in a topological vector space X (θ represents the origin of X), where K1, . . . , K p are open. We suppose that we are not in the singular case, where one cone is empty and all the others are the whole space X. Then, ⋂p+1 i=1 Ki = ∅ if and only if there exist linear functionals fi ∈ K ∗ i , i = 1, . . . , p + 1, not all zero, such that: f1 + · · · + f p + f p+1 = θ∗, where symbol K ∗ i denotes the dual cone of the set Ki , i.e. the set of all linear and continuous functionals which are non negative on Ki . The direct application of (D–M) approach to problem (1) in the space Rn is not clear. It is well known that, even for the linear case, infinite dimensional spaces appear naturally to obtain necessary conditions and duality results for exact SIP problems, for example, the space of generalized finite sequences (Goberna and López 1998, Chap. 2) or the space of continuous functions (Shapiro 1998, Cap. 4). Incidentally, any generalization of these results to the inexact case must consider the non-reflexive space of continuous functions Cq for the inequality constraints. These are the reasons explaining why infinite dimensional and non reflexive spaces were considered here. 48 J. A. Gómez and P. Bosch Problem (1) shall be transformed in the following way. Define the functional sets: B := {y ∈ L∞ : y(ε) ∈ B, a.e. ε ∈ [0, 1]} , D := z ∈ Cq : z(ε) ∈ D, ∀ε ∈ [0, 1] } , P := z ∈ Cq : z(ε) ≥ 0, ∀ε ∈ [0, 1] } , and the constraint operators: H : Rn × L∞ −→ L∞ and G : Rn × Cq −→ Cq : H(x, y)(ε) = h(x, ε) − y(ε), G(x, z)(ε) = g(x, ε) − z(ε). Here L∞ = L∞ ([0, 1] , Rp) denotes the Banach space of essentially bounded measurable Rp-valued functions and Cq = C ([0, 1] , Rq) is the Banach space of continuous Rq -valued functions, with the usual (essential) supremum norm defined on each space. By assumptions, operators H and G become continuously Frechet differentiable with respect to (x, y) ∈ Rn × L∞ and (x, z) ∈ Rn ×Cq respectively, and the functional J is directional differentiable. We shall denote Frechet differentials with symbol D and directional differentials with symbol δ. Note that the sets B and D have nonempty interior due to the same property of B and D. This last observation is another reason to consider non-reflexive spaces L∞ and Cp, since for the usual reflexive spaces L p (1 < p < ∞) the sets B and D does not have interior points. The constraints shall be written as follows: H(x, y) = θ∞, (2) G(x, z) θC ⇐⇒ G(x, z) ∈ P, (3) where θC (θ∞) denotes the null function on Cq (L∞). In fact, all the results in this paper are still true if we suppose P a convex cone in Cq which is pointed (θC ∈ P), proper (P ∩ {−P} = θC ) and with nonempty interior (int(P) = ∅). In this more general case we keep the notation G(x, z) θC for G(x, z) ∈ P, since P defines a partial order in Cq , and we read the relation G(x, z) θC as G(x, z) ∈ int (P). In addition, convexity assumption implies P = cl(int(P)). Problem (1) becomes: min x J (x) = min x max c∈C f (x, c), s.t. H(x, y) = θ∞ G(x, z) θC (x, z, y) ∈ Rn × D × B. (4) The hypothesis (H3) about function ε → h(x, ε) demands the new variable y(.) ∈ L∞, and this make the first constraint of (4) equivalent to the corresponding one in (1). The continuity assumption for the variable z(.) should be analized in order to problems (1) and (4) be considered equivalent. In fact, the first component of any solution (x, z, y) of (4) produces, trivially, a solution x for (1). On the other hand, if x is a solution of (1), g(x, ε) ∈ D + R+, ∀ε ∈ [0, 1]. Then, we must prove that, for each x , there exists a continuous function z(.) such that z(ε) ∈ D and gi (x, ε) ≥ zi (ε), i = 1, m. This statement is equivalent to the relation z(ε) ∈ x (ε) = (g(x, ε)−R+)∩D, and the made assumptions give that every set x (ε) Inexact nonlinear semi-infinite programming problems 49 is convex. Therefore, the theorem of continuous selection (see Michael 1956–1957 or Michael 1970) guarantees the existence of a continuous function z(.) with the necessary properties. Finally, defining y(ε) = h(x, ε), we have that (x, z, y) is also a solution of (4) and both problems become equivalent. 	algorithm;amaya;apex (geometry);binary prefix;book;challenge-handshake authentication protocol;computable function;computation;cone (formal languages);convex cone;convex function;convex optimization;convex set;cutting-plane method;discretization;dual cone and polar cone;emoticon;existential quantification;fréchet derivative;karush–kuhn–tucker conditions;linear programming;mathematical optimization;maxima and minima;natural language processing;nonlinear programming;nonlinear system;null function;optimal control;programming tool;rp (complexity);reflexive closure;relational operator;robot;scott continuity;second-order cone programming;semi-infinite programming;semiconductor industry;social inequality;strong duality;tucker decomposition	Juan Alfredo Gómez;Paul Bosch	2007	Math. Meth. of OR	10.1007/s00186-006-0099-8	fractional programming;convex analysis;mathematical optimization;constraint programming;combinatorics;discrete mathematics;duality;nonlinear programming;first-order logic;mathematics	ML	70.99903239630186	21.439222139248848	197886
3b736a6c7c24316f29a71c1660ea5693b637b481	a computational approach to the maximum principle	maximum principle	"""Most of the current work on the maximum principle in control theory, with the exception of Young [11] and McShane [12], deals with necessary conditions that a [any] postulated optimal solution must satisfy. The essential weakness here is that necessary conditions are proved for an often nonexistent solution--Young [11] cites a paradox of Perron to illustrate the logical difficulty this leads to. In partial resolution, Young and McShane first prove existence of relaxed controls and then deduce necessary conditions in the form of the maximum principle for such a solution. In this paper, we take a strictly computational approach to the problem. We develop a computational procedure for the control problem and the maximum principle essentially """"pops out"""" in the process. The basic idea [1] is easily explained in reference to the simple canonical control problem:"""	computation;control theory	A. V. Balakrishnan	1971	J. Comput. Syst. Sci.	10.1016/S0022-0000(71)80032-6	computer science;maximum principle	Theory	69.55245967786463	18.961638505666272	198000
3c026c6a54dbbdf6a4787f0dd2926cbd84c72627	on the coderivative of the solution mapping to a second-order cone constrained parametric variational inequality	parametric variational inequality;lipschitz like property;90c30;second order cone;期刊论文;bilevel programming;coderivative	The explicit representation of Mordukhovich coderivative of a solution mapping to a second-order cone constrained parametric variational inequality is established by the reduction approach. The result obtained is used to obtain a necessary and sufficient condition for the Lipschitz-like property of the solution mapping to the parametric variational inequality and global optimality conditions for a bilevel programming with a second-order cone constrained lower level problem.	calculus of variations;social inequality;variational inequality	Jie Zhang;Yuxin Li;Liwei Zhang	2015	J. Global Optimization	10.1007/s10898-014-0181-3	mathematical optimization;combinatorics;mathematical analysis;mathematics;bilevel optimization	ML	72.3236901220696	21.06165938612773	198045
97cc6cae2cc19968b6d17a3a95ee364bd9bf2a33	on directional metric subregularity and second-order optimality conditions for a class of nonsmooth mathematical programs	generalized polyhedral multifunctions;49j53;second order optimality conditions;49k27;metric subregularity;90c48	We study infinite dimensional optimization problems where the constraint mapping is given as the sum of a smooth function and a generalized polyhedral multifunction, e.g. the normal cone mapping of a convex polyhedral set. By using advanced techniques of variational analysis we obtain first-order and second-order characterizations, both necessary and sufficient, for directional metric subregularity of the constraint mapping. These results are used to obtain second-order optimality conditions for the optimization problem.	convex cone;first-order predicate;infinite-dimensional optimization;mathematical optimization;multi-function printer;optimization problem;polyhedron;variational analysis	Helmut Gfrerer	2013	SIAM Journal on Optimization	10.1137/120891216	mathematical optimization;mathematical analysis;topology;mathematics	Robotics	71.5581893895117	20.745996346739716	198353
5e1f1f4c5f83a7205cc4b7f0d380b0e65fe5a2cb	network flows that solve least squares for linear equations		This paper presents a first-order distributed continuous-time algorithm for computing the leastsquares solution to a linear equation over networks. Given the uniqueness of the solution, with nonintegrable and diminishing step size, convergence results are provided for fixed graphs. The exact rate of convergence is also established for various types of step size choices falling into that category. For the case where non-unique solutions exist, convergence to one such solution is proved for constantly connected switching graphs with square integrable step size, and for uniformly jointly connected switching graphs under the boundedness assumption on system states. Validation of the results and illustration of the impact of step size on the convergence speed are made using a few numerical examples.	algorithm;fastest;first-order predicate;least squares;linear equation;numerical analysis;rate of convergence;solver	Yang Liu;Youcheng Lou;Brian D. O. Anderson;Guodong Shi	2018	CoRR		flow network;rate of convergence;uniqueness;mathematical optimization;square-integrable function;mathematics;least squares;linear equation;convergence (routing);graph	ML	75.22681616114858	19.725901761610064	198729
bbe7205347bd3a35e8c325475a0aab2986499844	guaranteed simultaneous asymmetric tensor decomposition via orthogonalized alternating least squares		We consider the asymmetric orthogonal tensor decomposition problem, and present an orthogonalized alternating least square algorithm that converges to rank-r of the true tensor factors simultaneously in O(log(log( 1 ǫ ))) steps under our proposed Trace Based Initialization procedure. Trace Based Initialization requires O(1/log( λr λr+1 )) number of matrix subspace iterations to guarantee a “good” initialization for the simultaneous orthogonalized ALS method, where λr is the r th largest singular value of the tensor. We are the first to give a theoretical guarantee on orthogonal asymmetric tensor decomposition using Trace Based Initialization procedure and the orthogonalized alternating least squares. Our Trace Based Initialization also improves convergence for symmetric orthogonal tensor decomposition.		Jialin Li;Furong Huang	2018	CoRR		tensor;mathematics;singular value;mathematical optimization;initialization;subspace topology;least squares;matrix (mathematics)	ML	78.90315400978555	25.72434520814122	198737
4a558d724bfdc8b3fbdd453d16e3ecf685d20f5e	a mathematical program generator mpgenr	nonlinear programming;software verification;mathematical programming;large scale optimization	MPGENR is a subroutine system designed to generate linear programs and nonlinear programs with quadratic and linear functions of variable size with predeternnned optnnal solutions. MPGENR is written in American National Standard Fortran and uses a maclnne-independent pseudorandom number generator to provide the capability of generating the same problem on different machines. Through a set of input parameters, the user can specify the problem dimensions, solution characteristics, constraint types, sparsity, specml structure, and output format. Most parameters have default settings. Test results are presented.	fortran;linear function;linear programming;nonlinear system;pseudorandom number generator;quadratic function;sparse matrix;subroutine	William M. Michaels;Richard P. O'Neill	1980	ACM Trans. Math. Softw.	10.1145/355873.355876	computational science;mathematical optimization;reactive programming;software verification;nonlinear programming;functional reactive programming;search-based software engineering;computer science;theoretical computer science;mathematics;procedural programming;inductive programming;programming language	Graphics	71.43933514588608	26.79595054197616	198781
495fb748d71b8a84bce54c67981bf02b5204e8c5	using tropical optimization to solve constrained minimax single-facility location problems with rectilinear distance	minimax location problem;rectilinear distance;idempotent semifield;tropical optimization;constrained optimization;explicit solution;90b85;15a80;65k05;90c48	We consider a constrained minimax rectilinear single-facility location problem with addends on the plane. We first formulate the problem in a standard form, and then represent it in terms of tropical mathematics as a tropical optimization problem with known solution. The solution is used to derive closed-form solutions to the initial location problem and its special cases. Solutions of example problems are obtained and graphical illustrations are given. Key-Words: minimax location problem, rectilinear distance, idempotent semifield, tropical optimization, explicit solution. MSC (2010): 65K10, 15A80, 90B85, 65K05, 90C48	facility location problem;graphical user interface;idempotence;mathematical optimization;minimax;optimization problem;regular grid;taxicab geometry	Nikolai Krivulin	2017	Comput. Manag. Science	10.1007/s10287-017-0289-2	optimization problem;mathematical optimization;constrained optimization;combinatorics;mathematics;geometry;1-center problem	Theory	71.00119246740364	23.98311478109102	198869
549483a142ba392bbe253eadfdd3eeabf1fdc429	general polynomial time decomposition algorithms	rate of convergence;optimal solution;metodo polinomial;optimisation;solution optimale;temps polynomial;optimizacion;constrenimiento igualdad;convex programming;relacion convergencia;temps lineaire;taux convergence;programmation convexe;intelligence artificielle;convergence rate;tiempo lineal;equality constraint;polynomial method;decomposition algorithm;solucion optima;linear time;polynomial time;quadratic optimization;artificial intelligence;optimization;inteligencia artificial;support vector machine;methode polynomiale;contrainte egalite;tiempo polinomial;programacion convexa	We present a general decomposition algorithm that is uniformly applicable to every (suitably normalized) instance of Convex Quadratic Optimization and efficiently approaches an optimal solution. The number of iterations required to be within ε of optimality grows linearly with 1/ε and quadratically with the number m of variables. The working set selection can be performed in polynomial time. If we restrict our considerations to instances of Convex Quadratic Optimization with at most k0 equality constraints for some fixed constant k0 plus some so-called box-constraints (conditions that hold for most variants of SVM-optimization), the working set is found in linear time. Our analysis builds on a generalization of the concept of rate certifying pairs that was introduced by Hush and Scovel. In order to extend their results to arbitrary instances of Convex Quadratic Optimization, we introduce the general notion of a rate certifying q-set. We improve on the results by Hush and Scovel (2003) in several ways. First our result holds for Convex Quadratic Optimization whereas the results by Hush and Scovel are specialized to SVM-optimization. Second, we achieve a higher rate of convergence even for the special case of SVM-optimization (despite the generality of our approach). Third, our analysis is technically simpler. We prove furthermore that the strategy for working set selection which is based on rate certifying sets coincides with a strategy which is based on a so-called “sparse witness of sub-optimality”. Viewed from this perspective, our main result improves on convergence results by List and Simon (2004) and Simon (2004) by providing convergence rates (and by holding under more general conditions).	algorithm;experiment;greedy algorithm;iteration;karush–kuhn–tucker conditions;mathematical optimization;p (complexity);polynomial;program optimization;quadratic programming;rate of convergence;simon;sparse matrix;time complexity;working set	Nikolas List;Hans Ulrich Simon	2005		10.1007/11503415_21	time complexity;mathematical optimization;convex optimization;computer science;calculus;mathematics;rate of convergence;quadratic programming;algorithm	ML	75.0553445778433	21.795647853081558	199008
6b6c3887aacfa144c497c68a783079bc0ca647d6	a variable fixing version of the two-block nonlinear constrained gauss-seidel algorithm for \(\ell _1\) -regularized least-squares	ell _1 l1 regularized least squares;active set;sparse approximation;gauss seidel algorithm;ell _1 l 1 regularized least squares	The problem of finding sparse solutions to underdetermined systems of linear equations is very common in many fields as e.g. in signal/image processing and statistics. A standard tool for dealing with sparse recovery is the l1-regularized least-squares approach that has recently attracted the attention of many researchers. In this paper, we describe a new version of the two-block nonlinear constrained GaussSeidel algorithm for solving l1-regularized least-squares that at each step of the iteration process fixes some variables to zero according to a simple active-set strategy. We prove the global convergence of the new algorithm and we show its efficiency reporting the results of some preliminary numerical experiments.	active set method;algorithm;compressed sensing;experiment;first-order reduction;gauss–seidel method;image processing;iteration;least squares;linear equation;local convergence;nonlinear system;numerical analysis;penalty method;programming paradigm;sparse matrix;system of linear equations;variable splitting	Margherita Porcelli;Francesco Rinaldi	2014	Comp. Opt. and Appl.	10.1007/s10589-014-9653-0	mathematical optimization;machine learning;sparse approximation;mathematics;statistics	ML	78.17221735288551	23.32024188083484	199048
c12decb81c5104491a510a49450ec5b4d0829aab	partial augmented lagrangian method and mathematical programs with complementarity constraints	second order;linear independence;constraint optimization;mathematical program with complementarity constraints;satisfiability;objective function;augmented lagrangian method;constraint qualification;numerical experiment;augmented lagrangian;necessary optimality condition;complementarity constraints	In this paper, we apply a partial augmented Lagrangian method to mathematical programs with complementarity constraints (MPCC). Specifically, only the complementarity constraints are incorporated into the objective function of the augmented Lagrangian problem while the other constraints of the original MPCC are retained as constraints in the augmented Lagrangian problem. We show that the limit point of a sequence of points that satisfy second-order necessary conditions of the partial augmented Lagrangian problems is a strongly stationary point (hence a B-stationary point) of the original MPCC if the limit point is feasible to MPCC, the linear independence constraint qualification for MPCC and the upper level strict complementarity condition hold at the limit point. Furthermore, this limit point also satisfies a second-order necessary optimality condition of MPCC. Numerical experiments are done to test the computational performances of several methods for MPCC proposed in the literature.	augmented lagrangian method;complementarity theory	X. X. Huang;X. Q. Yang;Kok Lay Teo	2006	J. Global Optimization	10.1007/s10898-005-3837-1	mathematical optimization;constrained optimization;combinatorics;mathematical analysis;augmented lagrangian method;mathematics	EDA	72.60647081933217	22.43966967206379	199174
a272a4a8ccf5959d1bf095e530823c2c175164fc	on memory gradient method with trust region for unconstrained optimization	line search;memory gradient method;mathematics;gradient method;90c30;global convergence;objective function;trust region;65k05;trust region method;ciencias basicas y experimentales;matematicas;unconstrained optimization;numerical experiment;grupo a;large scale optimization;49m37	In this paper we present a new memory gradient method with trust region for unconstrained optimization problems. The method combines line search method and trust region method to generate new iterative points at each iteration and therefore has both advantages of line search method and trust region method. It sufficiently uses the previous multi-step iterative information at each iteration and avoids the storage and computation of matrices associated with the Hessian of objective functions, so that it is suitable to solve large scale optimization problems. We also design an implementable version of this method and analyze its global convergence under weak conditions. This idea enables us to design some quick convergent, effective, and robust algorithms since it uses more information from previous iterative steps. Numerical experiments show that the new method is effective, stable and robust in practical computation, compared with other similar methods.	algorithm;computation;experiment;gradient method;hessian;iteration;line search;local convergence;mathematical optimization;numerical method;trust region	Zhen-Jun Shi;Jie Shen	2005	Numerical Algorithms	10.1007/s11075-005-9008-0	mathematical optimization;gradient method;theoretical computer science;mathematics;iterative method;trust region;line search	ML	76.34135284792147	24.069269257374373	199402
6120e70450d9fc75ee6cf017595dc0c33beb8727	fritz john necessary optimality conditions of the alternative-type	kkt conditions;nonlinear programming;fritz john conditions	An alternative-type version of the Fritz John optimality conditions is established at points not necessarily optimal, which covers situations where no result appearing elsewhere is applicable. As a by-product, a versatile formulation of these necessary Fritz John optimality conditions along with a simple proof is provided. This encompasses several versions appearing in the literature. A variant of the KKT conditions is also presented.	capability maturity model;conjunctive query;convex function;karush–kuhn–tucker conditions;mathematical optimization;mathematical programming with equilibrium constraints;shape optimization;social inequality	Fabián Flores Bazán	2014	J. Optimization Theory and Applications	10.1007/s10957-013-0456-8	mathematical optimization;fritz john conditions;nonlinear programming;calculus;mathematics;mathematical economics;karush–kuhn–tucker conditions	ML	71.49718079307088	21.558754918118503	199420
be9bc518c0310ce068ffda72eaa627cfe6cd5774	on test sets for nonlinear integer maximization	espace hilbert;superadditive;test sets;non linear programming;programacion entera;espacio hilbert;programacion no lineal;optimum global;programmation non lineaire;global optimum;programmation en nombres entiers;fonction objectif;hilbert space;objective function;optimization problem;integer programming;gordan lemma;hilbert basis;funcion objetivo;integer program;optimo global	A finite test set for an integer optimization problem enables us to verify whether a feasible point attains the global optimum. In this paper, we establish several general results that apply to integer optimization problems with nonlinear objective functions. c © 2008 Elsevier B.V. All rights reserved.	entropy maximization;global optimization;mathematical optimization;nonlinear system;optimization problem;test set	Jon Lee;Shmuel Onn;Robert Weismantel	2008	Oper. Res. Lett.	10.1016/j.orl.2008.02.002	superadditivity;optimization problem;mathematical optimization;mathematical analysis;discrete mathematics;integer programming;special ordered set;nearest integer function;mathematics;global optimum;algorithm;global optimization;cutting-plane method;hilbert space	AI	71.81121798445008	22.884670011112046	199520
36e0387e0ca1bfe0621702245dd49cb1ca13ac1f	a modified phase estimation algorithm to compute the eigenvalue of a matrix with a positive eigenvector		Quantum phase estimation algorithm finds the ground state energy, the lowest eigenvalue, of a quantum Hamiltonian more efficiently than its classical counterparts. Furthermore, with different settings, the algorithm has been successfully adapted as a sub frame of many other algorithms applied to a wide variety of applications in different fields. However, the requirement of a good approximate eigenvector given as an input to the algorithm hinders the application of the algorithm to the problems where we do not have any prior knowledge about the eigenvector. This paper presents a modification to the phase estimation algorithm for the positive operators to determine the eigenvalue corresponding to the positive eigenvector without the necessity of the existence of an initial approximate eigenvector. Moreover, by this modification, we show that the success probability of the algorithm becomes to depend on the normalized absolute sum of the matrix elements of the unitary operator whose eigenvalue is being estimated. This provides a priori information to know the success probability of the algorithm beforehand and makes the algorithm output the right solution with high probability in many cases.	approximation algorithm;ground state;hamiltonian (quantum mechanics);quantum phase estimation algorithm;the matrix;with high probability	Anmer Daskin	2015	CoRR		quantum phase estimation algorithm;divide-and-conquer eigenvalue algorithm;generalized eigenvector;eigenvalues and eigenvectors;mathematics;mathematical optimization;inverse iteration;matrix (mathematics)	ML	82.43946683658173	23.78821277502926	199529
7e3f00ebaa29c00caf5246b295b69d0e994dfc3c	design approximation problems for linear-phase nonrecursive digital filters	linear phase;digital filter;ciencias basicas y experimentales;matematicas;grupo a	The topic of this paper is the study of four real, linear, possibly constrained minimum norm approximation problems, which arise in connection with the design of linear-phase nonrecursive digital filters and are distinguished by the type of used trigonometric approximation functions. In the case of unconstrained minimax designs these problems are normally solved by the Parks?McClellan algorithm, which is an application of the second algorithm of Remez to these problems and which is one of the most popular tools in filter design. In this paper the four types of approximation problems are investigated for all Lp and lp norms, respectively. It is especially proved that the assumptions for the Remez algorithm are satisfied in all four cases, which has been claimed, but is not obvious for three of them. Furthermore, results on the existence and uniqueness of solutions and on the convergence and the rate of convergence of the approximation errors are derived.	approximation;digital filter;linear phase	Rembert Reemtsen	2002	Journal of Approximation Theory	10.1006/jath.2002.3672	mathematical optimization;mathematical analysis;linear phase;digital filter;calculus;mathematics;remez algorithm;minimax approximation algorithm;approximation algorithm;statistics;algebra	Theory	75.73451938119322	18.570023844848603	199658
b8af8783afd1e668bd527e20ca181ba67b061a52	a qp-free constrained newton-type method for variational inequality problems	optimisation sous contrainte;constrained optimization;regularite;variational inequality problem;condition kuhn tucker;desigualdad variacional;condicion kuhn tucker;linear system of equations;regularidad;methode newton;inegalite variationnelle;regularity;karush kuhn tucker;global convergence;optimizacion con restriccion;semismoothness;variational inequality;convergence globale;quadratic convergence;metodo newton;newton method;convergence quadratique;kuhn tucker condition;regularity condition	We consider a simply constrained optimization reformulation of the KarushKuhn-Tucker conditions arising from variational inequalities. Based on this reformulation, we present a new Newton-type method for the solution of variational inequalities. The main properties of this method are: (a) it is well-defined for an arbitrary variational inequality problem, (b) it is globally convergent at least to a stationary point of the constrained reformulation, (c) it is locally superlinearly/quadratically convergent under a certain regularity condition, (d) all iterates remain feasible with respect to the constrained optimization reformulation, and (e) it has to solve just one linear system of equations at each iteration. Some preliminary numerical results indicate that this method is quite promising.	calculus of variations;constrained optimization;emoticon;iteration;linear system;mathematical optimization;newton;newton's method;numerical analysis;rate of convergence;social inequality;stationary process;system of linear equations;tucker decomposition;variational inequality;variational principle	Christian Kanzow;Houduo Qi	1999	Math. Program.	10.1007/s101070050047	system of linear equations;mathematical optimization;constrained optimization;mathematical analysis;variational inequality;calculus;mathematics;newton's method;rate of convergence;karush–kuhn–tucker conditions	ML	75.10323444692868	22.659168044087004	199664
2f0c22ac552faf76fb6ea9aec094f16bb62e75d9	highly efficient gradient computation for density-constrained analytical placement	iterative solver;integrated circuit layout;nonlinear programming;placement;nonlinear programming gradient methods integrated circuit layout;physical design;overlap removal;augmented lagrangian method;iterative methods;density functional theory;penalty method;smoothing methods;placement iterative solvers overlap constraints overlap removal;gradient methods;smoothing methods physics computing density functional theory poisson equations functional programming wire lagrangian functions scalability differential equations transforms;lagrangian functions;iterative solvers;direct method;overlap constraints;overlap removal gradient computation density constrained analytical placement analytical global placers nonoverlap constraints density smoothing techniques global smoothing nonlinear programming based placement framework force directed placement methods augmented lagrangian method quadratic penalty method iterative solvers	Recent analytical global placers use density constraints to approximate nonoverlap constraints, and these show very successful results. This paper unifies a wide range of density smoothing techniques called global smoothing and presents a highly efficient method for computing the gradient of such smoothed densities used in several well-known analytical placers. This method reduces the complexity of the gradient computation by a factor of  n  compared with a naive method, where  n  is the number of modules. Furthermore, with this efficient gradient computation, it is able to support an efficient nonlinear programming-based placement framework, which supersedes the existing force-directed placement methods. Experiments show that replacing the approximated gradient computation in mPL6 with the exact gradient computation improves wire length by 15% on the IBM-HB+ benchmark and by 3% on average on the modified International Symposium on Physical Design 2005 (ISPD'05) and ISPD'06 placement contest benchmarks with movable macros. The results also show that the augmented Lagrangian method outperforms the quadratic penalty method with the exact gradient computation.	computation;gradient	Jason Cong;Guojie Luo;Eric Radke	2008	IEEE Trans. on CAD of Integrated Circuits and Systems	10.1109/TCAD.2008.2006158	direct method;physical design;mathematical optimization;combinatorics;augmented lagrangian method;nonlinear programming;theoretical computer science;penalty method;mathematics;iterative method;integrated circuit layout;density functional theory;placement	EDA	77.39931842879197	27.55045727325612	199667
2669db64d82ade658e824ad31c22866e9cee9869	optimization of active algorithms for recovery of monotonic functions from hölder's class	monotone function		algorithm	Nikolaj P. Korneichuk	1994	J. Complexity	10.1006/jcom.1994.1013	mathematical analysis;monotonic function;theoretical computer science;machine learning;mathematics;algorithm;statistics	DB	73.75402304837213	19.481139219831416	199750
3f4fdb47205da425ea015261df2d67a2404421e8	explicit predictive control with non-convex polyhedral constraints	multiparametric programming;computacion informatica;articulo;grupo de excelencia;non convex constraints;info eu repo semantics article;model predictive control;ciencias basicas y experimentales;sum of squares	This paper proposes an explicit solution to the model predictive control of linear systems subject to non-convex polyhedral constraints. These constraints are modeled as the union of a finite number of convex polyhedra. The algorithm is based on calculating the explicit solution to a modified problem with linear constraints defined as the convex hull of the original ones and classifying its regions by their relation with the regions of the explicit solution to the original problem. Some of the regions are divided, and a procedure based on sum-of-squares programming is designed to determine which of the possible solutions are in fact optimal. Finally, the online algorithm is shown to be better in terms of computational cost and memory requirements than an algorithm based on obtaining and comparing the solutions of the problem using as constraints the polyhedra whose union forms the non-convex regions, both theoretically and by the results of an example.	polyhedron	Emilio Pérez;Carlos Ariño;Xavier Blasco Ferragud;Miguel A. Martínez	2012	Automatica	10.1016/j.automatica.2011.07.011	mathematical optimization;combinatorics;engineering;control theory;mathematics;explained sum of squares;model predictive control;algorithm;statistics	Robotics	69.8518311890732	23.72408360738489	199933
cc5eab47b17944ee9e2211165ee944e7a6f4476c	quadratic regularization projected barzilai–borwein method for nonnegative matrix factorization	projected barzilai borwein method;alternating nonnegative least squares;nonnegative matrix factorization;quadratic regularization	In this paper, based on the alternating nonnegative least squares framework, we present a new efficient method for nonnegative matrix factorization that uses a quadratic regularization projected Barzilai–Borwein (QRPBB) method to solve the subproblems. At each iteration, the QRPBB method first generates a point by solving a strongly convex quadratic minimization problem, which has a simple closed-form solution that is inexpensive to calculate, and then applies a projected Barzilai–Borwein method to update the solution of NMF. Global convergence result is established under mild conditions. Numerical comparisons of methods on both synthetic and real-world datasets show that the proposed method is efficient.	algorithm;central processing unit;experiment;han unification;horner's method;iteration;matrix regularization;non-negative least squares;non-negative matrix factorization;numerical method;quadratic programming;synthetic intelligence	Yakui Huang;Hongwei Liu;Shuisheng Zhou	2014	Data Mining and Knowledge Discovery	10.1007/s10618-014-0390-x	mathematical optimization;combinatorics;mathematical analysis;nonnegative matrix;computer science;machine learning;mathematics;non-negative matrix factorization	ML	75.65201483896357	25.70745667492324	199970
509d3f8ecefcf4131df20c779416c0e94a8ecca4	a practical, robust and fast method for location localization in range-based systems	newton’s method;indoor localization;location localization;positioning algorithm	Location localization technology is used in a number of industrial and civil applications. Real time location localization accuracy is highly dependent on the quality of the distance measurements and efficiency of solving the localization equations. In this paper, we provide a novel approach to solve the nonlinear localization equations efficiently and simultaneously eliminate the bad measurement data in range-based systems. A geometric intersection model was developed to narrow the target search area, where Newton's Method and the Direct Search Method are used to search for the unknown position. Not only does the geometric intersection model offer a small bounded search domain for Newton's Method and the Direct Search Method, but also it can self-correct bad measurement data. The Direct Search Method is useful for the coarse localization or small target search domain, while the Newton's Method can be used for accurate localization. For accurate localization, by utilizing the proposed Modified Newton's Method (MNM), challenges of avoiding the local extrema, singularities, and initial value choice are addressed. The applicability and robustness of the developed method has been demonstrated by experiments with an indoor system.	experiment;intersection of set of elements;line search;maxima and minima;newton;newton's method;nonlinear system;robustness (computer science);ultra-wideband	Shiping Huang;Zhifeng Wu;Anil Misra	2017		10.3390/s17122869	gravitational singularity;initial value problem;mathematical optimization;engineering;robustness (computer science);electronic engineering;nonlinear system;bounded function;newton's method;maxima and minima	Robotics	82.63107588397536	28.478879408144408	199991
