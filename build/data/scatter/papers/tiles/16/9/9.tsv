id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
e2fac4d56660e324f086d67577f7bf98c2487fbf	optimal pattern matching on meshes	pattern matching	Abs t rac t . Parallel pattern matching on a mesh-connected array of processors is considered. The problem is to find all occurrences of a pattern in a text. The input text is a string of n symbols placed in a v ~ • x/~ mesh, each processor storing one symbol. The pattern is stored similarly in a contiguous portion of the mesh. An algorithm solving the problem in time O(x/~ is presented. It applies a novel technique to design parallel pattern-matching algorithms based on the notion of a pseudo-period.	algorithm;central processing unit;pattern matching;string (computer science)	Bogdan S. Chlebus;Leszek Gasieniec	1994		10.1007/3-540-57785-8_143	computer science;pattern matching;mathematics	Theory	13.253771040932744	28.804162743028265	48695
6f5c528450b99317519b9ca98ff5aa8cb2ca1ca9	on the implementation of mst-based heuristics for the steiner problem in graphs	matematicas constructivas;metodo caso peor;problema arbol steiner;approximation asymptotique;constructive mathematics;algorithm complexity;steiner problem in graphs;complejidad algoritmo;heuristic method;probleme arbre steiner;metodo heuristico;miminum spanning tree mst;mathematiques constructives;complexite algorithme;efficient implementation;minimum spanning tree;arbre recouvrant minimal;methode cas pire;methode heuristique;asymptotic approximation;steiner tree problem;worst case method;aproximacion asintotica	Some of the most widely used constructive heuristics for the Steiner Problem in Graphs are based on algorithms for the Minimum Spanning Tree problem. In this paper, we examine efficient implementations of heuristics based on the classic algorithms by Prim, Kruskal, and Bor̊uvka. An extensive experimental study indicates that the theoretical worst-case complexity of the algorithms give little information about their behavior in practice. Careful implementation improves average computation times not only significantly, but asymptotically. Running times for our implementations are within a small constant factor from that of Prim’s algorithm for the Minimum Spanning Tree problem, suggesting that there is little room for improvement.	best, worst and average case;computation;experiment;heuristic (computer science);kruskal's algorithm;prim's algorithm;spanning tree;steiner tree problem;worst-case complexity	Marcus Poggi de Aragão;Renato F. Werneck	2002		10.1007/3-540-45643-0_1	mathematical optimization;combinatorics;steiner tree problem;prim's algorithm;minimum spanning tree;constructivism;mathematics;distributed minimum spanning tree;algorithm	Theory	19.326488290212737	26.22487209312265	48934
a0f6504ebf09ab9bc5e115f0fffa9cede5d5727a	approximate deadline-scheduling with precedence constraints		We consider the classic problem of scheduling a set of n jobs non-preemptively on a single machine. Each job j has non-negative processing time, weight, and deadline, and a feasible schedule needs to be consistent with chain-like precedence constraints. The goal is to compute a feasible schedule that minimizes the sum of penalties of late jobs. Lenstra and Rinnoy Kan [Annals of Disc. Math., 1977] in their seminal work introduced this problem and showed that it is strongly NP-hard, even when all processing times and weights are 1. We study the approximability of the problem and our main result is an O(log k)-approximation algorithm for instances with k distinct job deadlines. We also point out a surprising connection to a model for technology diffusion processes in networks that was recently proposed by Goldberg and Liu [SODA, 2013]. In an instance of such a problem one is given an undirected graph and a non-negative, integer threshold θ(v) for each of its vertices v. Vertices v in the graph are either active or inactive, and an inactive vertex v activates whenever it lies in component of size at least θ(v) in the graph induced by itself and all active vertices. The goal is now to find a smallest cardinality seed set of active vertices that leads to the activation of the entire graph. Goldberg and Liu showed that this problem has no o(log(n))-approximation algorithms unless NP has quasi-polynomial time algorithms, and the authors presented an O(rk log(n))approximation algorithm, where r is the radius of the given network, and k is the number of distinct vertex thresholds. The open question is whether the dependence of the approximation guarantee on r and k is avoidable. We answer this question affirmatively for instances where the underlying graph is a spider. In such instances technology diffusion and precedence constrained scheduling problem with unit processing times and weights are equivalent problems.	approximation algorithm;arjen lenstra;dijkstra's algorithm;directed graph;graph (discrete mathematics);job stream;np-hardness;quasi-polynomial;scheduling (computing);strong np-completeness;time complexity;vertex (geometry)	Hossein Esfandiari;Mohammad Taghi Hajiaghayi;Jochen Könemann;Hamid Mahini;David L. Malec;Laura Sanità	2015		10.1007/978-3-662-48350-3_41	mathematical optimization;real-time computing;mathematics	Theory	21.849242066428026	20.59718826676509	49267
a95d0380ceb7b888909063e9af8ef28aadcabe3e	catching the k-naesat threshold	random structures;statistical mechanics;moment method;qa mathematics;benchmark problem;discrete mathematics;k naesat;phase transition;existence of solution;phase transitions;constraint satisfaction problem;upper and lower bounds;second moment method;survey propagation	The best current estimates of the thresholds for the existence of solutions in random constraint satisfaction problems ('CSPs') mostly derive from the first and the second moment method. Yet apart from a very few exceptional cases these methods do not quite yield matching upper and lower bounds. According to deep but non-rigorous arguments from statistical mechanics, this discrepancy is due to a change in the geometry of the set of solutions called condensation that occurs shortly before the actual threshold for the existence of solutions (Krzakala, Montanari, Ricci-Tersenghi, Semerjian, Zdeborova: PNAS~2007). To cope with condensation, physicists have developed a sophisticated but non-rigorous formalism called Survey Propagation (Me-zard, Parisi, Zecchina: Science 2002). This formalism yields precise conjectures on the threshold values of many random CSPs. Here we develop a new Survey Propagation inspired second moment method for the random k-NAESAT problem, which is one of the standard benchmark problems in the theory of random CSPs. This new technique allows us to overcome the barrier posed by condensation rigorously. We prove that the threshold for the existence of solutions in random k-NAESAT is 2k-1ln2-(ln/2 2+1/4)+εk, where |εk| ≤ 2-(1-ok(1))k, thereby verifying the statistical mechanics conjecture for this problem.	benchmark (computing);condensation algorithm;constraint satisfaction problem;cryptographic service provider;discrepancy function;semantics (computer science);software propagation;verification and validation	Amin Coja-Oghlan;Konstantinos Panagiotou	2012		10.1145/2213977.2214058	phase transition;combinatorics;discrete mathematics;statistical mechanics;calculus;mathematics;algorithm;statistics	Theory	11.559692904101071	19.121188694354885	49300
1fb086e4cee4b3fab7cd56de45ca5213031817be	the affix array data structure and its applications to rna secondary structure analysis	sufijo;memoire;text;complete genome;algorithmique;aplicacion;analisis estructural;rna secondary structure;05c05;structure arborescente;suffix;performance;temps lineaire;complex structure;large data sets;32xx;index;texte;tree data structures;tiempo lineal;structure donnee arborescente;68wxx;red;suffix array;algorithme;suffix tree;algorithm;large scale;affix tree;arbre suffixe;construccion;rna;memoria;algorithmics;estructura arborescente;algoritmica;indice;informatique theorique;pattern matching;reseau arrangement;tree structure;estructura datos;linear time;genome;68p05;array;structure donnee;concordance forme;genoma;suffixe;rendimiento;analyse structurale;application;structural analysis;texto;data structure;construction;memory;similarity index;computer theory;algoritmo;informatica teorica	Efficient string-processing in large data sets like complete genomes is strongly connected to the suffix tree and similar index data structures. With respect to complex structural string analysis like the search for RNA secondary structure patterns, unidirectional suffix tree algorithms are inferior to bidirectional algorithms based on the affix tree data structure. The affix tree incorporates the suffix tree and the suffix tree of the reverse text in one tree structure but suffers from its large memory requirements. In this paper I present a new data structure, denoted affix array, which is equivalent to the affix tree with respect to its algorithmic functionality, but with smaller memory requirements and improved performance. I will show a linear time construction of the affix array without making use of the linear time construction of the affix tree. I will also show how bidirectional affix tree traversals can be transferred to the affix array and present the impressive results of large scale RNA secondary structure analysis based on the new data structure.	array data structure	Dirk Strothmann	2007	Theor. Comput. Sci.	10.1016/j.tcs.2007.09.029	time complexity;generalized suffix tree;combinatorics;rna;construction;data structure;performance;computer science;artificial intelligence;trie;pattern matching;generalized complex structure;mathematics;structural analysis;compressed suffix array;fractal tree index;tree structure;search tree;tree;memory;nucleic acid secondary structure;programming language;algorithmics;tree traversal;algorithm;genome	Theory	15.914533872105318	27.735854693543374	49422
76b6756422181cb2efc26db3022987a3902a2403	a parallel algorithm for computing minimum spanning trees	graphe non oriente;graph theory;graphe value;non directed graph;parallel algorithm;shared memory;multiprocessor;computation theory;memoria compartida;arbre maximal;clase complejidad;weighting;ponderacion;theorie graphe;parallel random access machine;parallel computation;processing time;algorithme parallele;calculo paralelo;classe complexite;complexity class;arbol maximo;systeme multitraitement;grafo no orientado;informatique theorique;minimum spanning tree;time use;graph algorithm;temps traitement;spanning tree;multiprocessing systems;ponderation;weighted graph;multiprocesador;calcul parallele;tiempo proceso;memoire partagee;parallel algorithms;multiprocesseur	Abstract   present a simple and implementable algorithm that computes a minimum spanning tree of an undirected weighted graph  G  = ( V ,  E ) of  n  = | V | vertices and  m  = | E | edges on an EREW PRAM in  O (log 3/2  n ) time using  n  +  m  processors. This represents a substantial improvement in the running time over the previous results for this problem using at the same time the weakest of the PRAM models. It also implies the existence of algorithms having the same complexity bounds for the EREW PRAM, for connectivity, ear decomposition, biconnectivity, strong orientation,  st -numbering and Euler tours problems.	parallel algorithm	Donald B. Johnson;Panagiotis Takis Metaxas	1995	J. Algorithms	10.1006/jagm.1995.1043	combinatorics;discrete mathematics;computer science;graph theory;mathematics;parallel algorithm;algorithm	Theory	18.596331056045308	28.344681650632705	49437
7a113c5e61fe9f0877bee5c7dc44748e39496493	primality and identity testing via chinese remaindering	primality testing;schwartz zippel test;randomized algorithms;input polynomial;computational complexity number theory testing randomised algorithms;chen kao test;randomised algorithms;univariate identity;polynomial identity testing;testing polynomials circuits arithmetic computer science binary decision diagrams;arithmetic circuit size;testing;error parameter;polynomials;finite field;number theory;rational numbers;chinese remainder theorem;binary decision diagrams;multivariate polynomial;computational complexity;arithmetic;random bits;circuits;primality testing algorithm;univariate polynomial;computer science;lewin vadhan test;identity testing algorithm;polynomial time algorithms;univariate polynomial primality testing algorithm identity testing algorithm chinese remainder theorem univariate identity randomized algorithms multivariate polynomial finite field rational numbers polynomial time algorithms arithmetic circuit size input polynomial error parameter random bits schwartz zippel test chen kao test lewin vadhan test	Gives a simple and new primality testing algorithm by reducing primality testing for a number n to testing if a specific univariate identity over Z/sub n/ holds. We also give new randomized algorithms for testing if a multivariate polynomial, over a finite field or over rationals, is identically zero. The first of these algorithms also works over Z/sub n/ for any n. The running time of the algorithms is polynomial in the size of the arithmetic circuit representing the input polynomial and the error parameter. These algorithms use fewer random bits and work for a larger class of polynomials than all the previously known methods, e.g. the Schwartz-Zippel test (J.T. Schwartz, 1980; R.E. Zippel, 1979), the Chen-Kao (1997) test and the Lewin-Vadhan (1998) test. Our algorithms first transform the input polynomial to a univariate polynomial and then use Chinese remaindering over univariate polynomials to effectively test if it is zero.		Manindra Agrawal;Somenath Biswas	1999		10.1109/SFFCS.1999.814592	electronic circuit;combinatorics;number theory;discrete mathematics;lucas primality test;miller–rabin primality test;primality certificate;stable polynomial;chinese remainder theorem;mathematics;software testing;solovay–strassen primality test;primality test;randomized algorithm;computational complexity theory;finite field;square-free polynomial;algorithm;rational number;statistics;polynomial;algebra	Crypto	10.286511739601487	22.77312655716121	49506
11cd846e75cc63d1f964c543024899716275bebc	lower bounds for approximation schemes for closest string	004;closest string ptas efficient ptas	In the Closest String problem one is given a family S of equal-length strings over some fixed alphabet, and the task is to find a string y that minimizes the maximum Hamming distance between y and a string from S. While polynomial-time approximation schemes (PTASes) for this problem are known for a long time [Li et al.; J. ACM’02], no efficient polynomial-time approximation scheme (EPTAS) has been proposed so far. In this paper, we prove that the existence of an EPTAS for Closest String is in fact unlikely, as it would imply that FPT = W[1], a highly unexpected collapse in the hierarchy of parameterized complexity classes. Our proof also shows that the existence of a PTAS for Closest String with running time f(ε) · n, for any computable function f , would contradict the Exponential Time Hypothesis. Institute of Informatics, University of Warsaw, Poland, cygan@mimuw.edu.pl. Supported by Polish National Science Centre grant DEC-2012/05/D/ST6/03214. Department of Informatics, University of Bergen, Norway, daniello@ii.uib.no. Supported by the BeHard grant under the recruitment programme of the of Bergen Research Foundation. Institute of Informatics, University of Warsaw, Poland, marcin.pilipczuk@mimuw.edu.pl. Supported by Polish National Science Centre grant DEC-2012/05/D/ST6/03214. Institute of Informatics, University of Warsaw, Poland, michal.pilipczuk@mimuw.edu.pl. Supported by Polish National Science Centre grant DEC-2013/11/D/ST6/03073 and by the Foundation for Polish Science via the START stipend programme. During the work on these results, Micha l Pilipczuk held a post-doc position at Warsaw Center of Mathematics and Computer Science. Institute of Mathematical Sciences, India, saket@imsc.res.in, and Department of Informatics, University of Bergen, Norway, Saket.Saurabh@ii.uib.no. Supported by PARAPPROX, ERC starting grant no. 306992.	closest string;complexity class;computable function;computer science;exptime;exponential time hypothesis;fastest;hamming distance;informatics;ptas reduction;parameterized complexity;polynomial;polynomial-time approximation scheme;time complexity	Marek Cygan;Daniel Lokshtanov;Marcin Pilipczuk;Michal Pilipczuk;Saket Saurabh	2016		10.4230/LIPIcs.SWAT.2016.12	mathematical optimization;combinatorics;discrete mathematics;computer science;mathematics;string searching algorithm	Theory	12.573822262764505	21.585659372407655	49670
83def7801620cbbb4586a777d167c980b6c0ffb9	on the complexity of monotonic inheritance with roles	time complexity;polynomial algorithm	We investigate the complexity of reasoning with monotonic inheritance hierarchies that contain, beside ISA edges, also ROLE (or FUNCTION) edges. A ROLE edge is an edge labelled with a name such as spouse-of or brother-of. We call such networks ISAR networks. Given a network with n vertices and m edges, we consider two problems: (P,) determining whether the network implies an isa relation between two particular nodes, and (P,) determining all isa relations implied by the network. As is well known, without ROLE edges the time complexity of P, is O(m), and the time complexity of P, is O(n3). Unfortunately, the results do not extend naturally to ISAR networks, except in a very restricted case. For general ISAR network we frost give an polynomial algorithm by an easy reduction to proposional Horn theory. As the degree of the polynomial is quite high (O(mn4) for P,, O(mn’) for P,), we then develop a more direct algorithm. For both P, and P, its complexity is O(n3 + m*). Actually, a finer analysis of the algorithm reveals a complexity of O(nr(Zog r) + n*r + n3), where r is the number of different ROLE labels. One corolary is that if we fix the number of ROLE labels, the complexity of our algorithm drops back to O(n3).	algorithm;p (complexity);polynomial;regular expression;time complexity	Ramiro A. de T. Guerreiro;Andrea S. Hemerly;Yoav Shoham	1990			time complexity;average-case complexity;computer science;machine learning;algorithm	AI	22.158864464675922	23.59402022697991	50062
215af1a4d6b5cc0ee5ffc9bd7961902e6a64c54d	efficient dynamic programming alignment of cyclic strings by shift elimination	dynamic programming;alignement;programacion dinamica;time complexity;analisis forma;chaine caractere;dynamic program;complexite temps;reconnaissance caractere;data dependence;computational complexity;cadena caracter;programmation dynamique;alineamiento;pattern analysis;cyclic patterns;experimental evaluation;string matching;complejidad tiempo;character recognition;alignment;analyse forme;reconocimiento caracter;character string	Abstract   Optimal alignment of two strings of length  m  and  n  is computed in time  O ( mn ) by dynamic programming. When the strings represent cyclic patterns, the alignment computation must consider all possible shifts and the computation complexity increases accordingly. We present an algorithm for efficient dynamic programming alignment of cyclic strings which uses a previously established channeling technique to reduce the complexity of each alignment and a new shift elimination technique to reduce the number of alignments carried out. The result is a data-dependent time complexity that varies between  O (2 mn ) and  O ( mn  log 2   n ). An experimental evaluation is provided using randomly generated sequences.	dynamic programming	Jens Gregor;Michael G. Thomason	1996	Pattern Recognition	10.1016/0031-3203(95)00144-1	time complexity;string;computer science;artificial intelligence;theoretical computer science;dynamic programming;mathematics;computational complexity theory;algorithm;string searching algorithm	Vision	14.536134491632899	26.84033170849151	50131
5231833c569b666cdcc9a230794c596af9f36704	faster subsequence and don't-care pattern matching on compressed texts	matching algorithm;local subsequence recognition;minimal subsequence occurrence;t-care pattern;nm log m;fixed length;time algorithm;towards approximate matching;principal problem;faster subsequence;time complexity;subsequence pattern	Subsequence pattern matching problems on compressed text were first considered by Cégielski et al. (Window Subsequence Problems for Compressed Texts, Proc. CSR 2006, LNCS 3967, pp. 127–136), where the principal problem is: given a string T represented as a straight line program (SLP) T of size n, a string P of size m, compute the number of minimal subsequence occurrences of P in T . We present an O(nm) time algorithm for solving all variations of the problem introduced by Cégielski et al.. This improves the previous best known algorithm of Tiskin (Towards approximate matching in compressed strings: Local subsequence recognition, Proc. CSR 2011), which runs in O(nm log m) time. We further show that our algorithms can be modified to solve a wider range of problems in the same O(nm) time complexity, and present the first matching algorithms for patterns containing VLDC (variable length don’t care) symbols, as well as for patterns containing FLDC (fixed length don’t care) symbols, on SLP compressed texts.	approximation algorithm;lecture notes in computer science;pattern matching;regular expression;superword level parallelism;time complexity	Takanori Yamamoto;Hideo Bannai;Shunsuke Inenaga;Masayuki Takeda	2011		10.1007/978-3-642-21458-5_27	combinatorics;longest increasing subsequence;theoretical computer science;mathematics;longest alternating subsequence;algorithm	Theory	13.594092845440914	27.086498769918034	50152
1d391e0484a5d751dc6cfbefec559000803fdb5b	subcubic equivalences between path, matrix, and triangle problems		We say an algorithm on <i>n</i> × <i>n</i> matrices with integer entries in [−<i>M</i>,<i>M</i>] (or <i>n</i>-node graphs with edge weights from [−<i>M</i>,<i>M</i>]) is <i>truly subcubic</i> if it runs in <i>O</i>(<i>n</i><sup>3 − Δ</sup> ċ poly(log <i>M</i>)) time for some Δ > 0. We define a notion of <i>subcubic reducibility</i> and show that many important problems on graphs and matrices solvable in <i>O</i>(<i>n</i><sup>3</sup>) time are <i>equivalent</i> under subcubic reductions. Namely, the following weighted problems either <i>all</i> have truly subcubic algorithms, or none of them do:  •The all-pairs shortest paths problem on weighted digraphs (APSP).  •Detecting if a weighted graph has a triangle of negative total edge weight.  •Listing up to <i>n</i><sup>2.99</sup> negative triangles in an edge-weighted graph.  •Finding a minimum weight cycle in a graph of non-negative edge weights.  •The replacement paths problem on weighted digraphs.  •Finding the second shortest simple path between two nodes in a weighted digraph.  •Checking whether a given matrix defines a metric.  •Verifying the correctness of a matrix product over the (min, +)-semiring.  •Finding a maximum subarray in a given matrix.  Therefore, if APSP cannot be solved in <i>n</i><sup>3−ϵ</sup> time for any ϵ > 0, then many other problems also need essentially cubic time. In fact, we show generic equivalences between matrix products over a large class of algebraic structures used in optimization, verifying a matrix product over the same structure, and corresponding triangle detection problems over the structure. These equivalences simplify prior work on subcubic algorithms for all-pairs path problems, since it now suffices to give appropriate subcubic triangle detection algorithms.  Other consequences of our work are new combinatorial approaches to Boolean matrix multiplication over the (OR,AND)-semiring (abbreviated as BMM). We show that practical advances in triangle detection would imply practical BMM algorithms, among other results. Building on our techniques, we give two improved BMM algorithms: a derandomization of the combinatorial BMM algorithm of Bansal and Williams (FOCS’09), and an improved quantum algorithm for BMM.	business motivation model;correctness (computer science);decision problem;directed graph;graph (discrete mathematics);linear algebra;many-one reduction;mathematical optimization;matrix multiplication;maxima and minima;minimum weight;minimum-weight triangulation;path (graph theory);quantum algorithm;randomized algorithm;shortest path problem;time complexity;verification and validation	Virginia Vassilevska Williams;Richard Ryan Williams	2018	J. ACM	10.1145/3186893	discrete mathematics;combinatorics;logical matrix;matrix multiplication;quantum algorithm;maximum subarray problem;path (graph theory);matrix (mathematics);multiplication;computer science;integer	Theory	22.089165888761222	23.932425858762915	50274
fcc4b51483f9f6807d6a1a18927cb1d9fc305f4c	linear-time construction of two-dimensional suffix trees	open problem;linear-time construction;s. linear-time algorithm;linear-time algorithm;suffix tree;two-dimensional suffix tree;two-dimensional suffix trees;compacted trie;linear time;divide and conquer	The suux tree of a string S is a compacted trie that represents all suuxes of S. Linear-time algorithms for constructing the suf-x tree have been known for quite a while. In two dimensions, however, linear-time construction of two-dimensional suux trees has been an open problem. We present the rst linear-time algorithm for constructing two-dimensional suux trees.	algorithm;time complexity;trie;while	Dong Kyue Kim;Kunsoo Park	1999		10.1007/3-540-48523-6_43	combinatorics;discrete mathematics;mathematics;algorithm	Theory	13.815770515373577	27.258405669616845	50329
27bc428e38d47aefb14c75efbce87fed1b1a83b3	a scheme for single instance representation in hierarchical assembly graphs	directed acyclic graph;data association;hash table;affine transformation;geometric model	The Hierarchical Assembly Graph (HAG) is a common representation for geometric models. The HAG is a directed acyclic graph. Nodes in the graph represent objects, and arcs denote the sub-part relation between objects. Affine transformations and other instantiation parameters are attached to the arcs. An instance of an object in a HAG is defined as a path ending at its node. Information common to all instances whose paths end at a given node can be attached to this node. Data associated with a single instance cannot be attached to any single node or arc in the graph. Such private data can be stored in an external list, hash table, or a partial expansion of the graph into a tree, but all of these schemes have severe drawbacks in terms of storage, access efficiency, or update efficiency. In this paper we present a scheme for representing single instances in the assembly graph itself, by identifying an instance with the last node in its path when the only way of reaching the last node is through a unique path starting at the first node of the path. We give an algorithm for singling an instance in the graph, i.e. transforming the graph into an equivalent one in which the instance can be identified with a node. We also show how to undo an instance’s singling when its private data is no longer needed.	algorithm;directed acyclic graph;hash table;information privacy;single-instance storage;undo;universal instantiation	Ari Rappoport	1993		10.1007/978-3-642-78114-8_13	lattice graph;combinatorics;geometric graph theory;discrete mathematics;feedback arc set;null model;directed graph;graph bandwidth;null graph;graph property;theoretical computer science;simplex graph;aperiodic graph;mathematics;voltage graph;distance-hereditary graph;graph;moral graph;path;butterfly graph;complement graph;line graph;string graph;strength of a graph	DB	18.637437654897962	24.06664815038543	50663
ee603f5c48f7aa0f9d4865d49b53758cb44591a6	treewidth, pathwidth and cospan decompositions with applications to graph-accepting tree automata	cospans;graph decompositions;tree automata;treewidth;pathwidth	We will revisit the categorical notion of cospan decompositions of graphs and compare it to the well-known notions of path decomposition and tree decomposition from graph theory. More specifically, we will define several types of cospan decompositions with appropriate width measures and show that these width measures coincide with pathwidth and treewidth. Such graph decompositions of small width are used to efficiently decide graph properties, for instance via graph automata. Hence we will give an application by defining graph-accepting tree automata, thus integrating previous work by Courcelle into the setting of cospan decompositions. Furthermore we will show that regardless of whether we consider path or tree decompositions, we arrive at the same notion of recognizability.	algorithm;automata theory;binary decision diagram;deterministic finite automaton;finite-state machine;graph property;graph rewriting;graph theory;heuristic (computer science);join (sql);pathwidth;singular value decomposition;time complexity;tree automaton;tree decomposition;treewidth;whole earth 'lectronic link	Christoph Blume;H. J. Sander Bruggink;Martin Friedrich;Barbara König	2013	J. Vis. Lang. Comput.	10.1016/j.jvlc.2012.10.002	pathwidth;tree-depth;treewidth;partial k-tree;tree decomposition	Logic	20.023715013030646	24.30684487108595	50678
f909bff0c80054da6315b0aee069430a75a55f96	single-sink network design with vertex connectivity requirements	network design;004;rent or buy;buy at bulk;network design vertex connectivity buy at bulk rent or buy approximation;approximation;disjoint paths;vertex connectivity	We study single-sink network design problems in undirected graphs with vertex connectivity requirements. The input to these problems is an edge-weighted undirected graph G = (V, E), a sink/root vertex r, a set of terminals T ⊆ V, and integer k. The goal is to connect each terminal t ∈ T to r via k vertex-disjoint paths. In the connectivity problem, the objective is to find a min-cost subgraph of G that contains the desired paths. There is a 2-approximation for this problem when k ≤ 2 [9] but for k ≥ 3, the first non-trivial approximation was obtained in the recent work of Chakraborty, Chuzhoy and Khanna [4]; they describe and analyze an algorithm with an approximation ratio ofO(kO(k 2) log n) where n = |V|. In this paper, inspired by the results and ideas in [4], we show an O(kO(k) log |T|)-approximation bound for a simple greedy algorithm. Our analysis is based on the dual of a natural linear program and is of independent technical interest. We use the insights from this analysis to obtain an O(kO(k) log |T|)-approximation for the more general single-sink rent-or-buy network design problem with vertex connectivity requirements. We further extend the ideas to obtain a poly-logarithmic approximation for the single-sink buy-at-bulk problem when k = 2 and the number of cable-types is a fixed constant; we believe that this should extend to any fixed k. We also show that for the nonuniform buy-at-bulk problem, for each fixed k, a small variant of a simple algorithm suggested by Charikar and Kargiazova [5] for the case of k = 1 gives an 2 √ log |T|) approximation for larger k. These results show that for each of these problems, simple and natural algorithms that have been developed for k = 1 have good performance for small k > 1.	approximation algorithm;graph (discrete mathematics);greedy algorithm;k-vertex-connected graph;linear programming;maxima and minima;network planning and design;requirement	Chandra Chekuri;Nitish Korula	2008		10.4230/LIPIcs.FSTTCS.2008.1747	mathematical optimization;network planning and design;combinatorics;discrete mathematics;computer science;approximation;mathematics	Theory	22.820466702988984	18.614453286552727	50856
136687163c275cc070294d75b32da79a93f9982b	on time optimal solutions of the firing squad synchronization problem for two-dimensional paths	optimal solution;solution optimale;machine turing;algorithme reseau;solution temps optimal;fssp;automate deterministe;combinatorial problems;turing machine;reseau;firing squad synchronization problem;red;algorithme;polynomial time algorithm;synchronisation;algorithm;deterministic automaton;computational complexity;synchronization;solucion optima;timing optimization;automata determinista;finite automata;network algorithm;time optimal solution;network algorithms;sincronizacion;maquina turing;probleme synchronisation peloton execution;network;exhaustive search;algoritmo	The firing squad synchronization problem for two-dimensional paths (2-Path FSSP, for short) is a variation of the firing squad synchronization problem where finite automata are placed along a path in the two-dimensional array space. Whether 2-Path FSSP has a time optimal solution or not is an open problem. We introduce a combinatorial problem which we call the two-dimensional path extension problem (2-PEP, for short), and show that if 2-Path FSSP has a time optimal solution then 2-PEP has a polynomial time algorithm. The computational complexity of 2-PEP is not well understood and the exhaustive search requiring exponential time is the only algorithm we know for it at present.	firing squad synchronization problem	Kojiro Kobayashi	2001	Theor. Comput. Sci.	10.1016/S0304-3975(99)00332-1	synchronization;combinatorics;computer science;theoretical computer science;mathematics;finite-state machine;algorithm	ECom	17.44095801252448	26.2847627756038	50873
53dec4d61b0407be2f892ea917d2148532b8bbfa	large independent sets in random regular graphs	conjunto independiente;graph theory;random graph;control theory;approximate algorithm;probability;algorithmique;loi probabilite;ley probabilidad;approximation algorithms;independent set;lower bounds;approximation algorithm;random regular graph;grafo aleatorio;graphe aleatoire;set theory;journal article;random graphs;ensemble independant;approximation algorithms approximation algorithms;random regular graphs;algorithmics;algoritmica;keywords independent set;independent sets;informatique theorique;probability distribution;probabilidad;68r10;05c80;probabilite;algoritmo aproximacion;borne inferieure;ensemble aleatoire;grafo regular;graphe regulier;algorithme approximation;68w25;random d regular graphs;random set;lower bound;cota inferior;regular graph;computer theory;conjunto aleatorio;informatica teorica	"""We present algorithmic lower bounds on the size s""""d of the largest independent sets of vertices in random d-regular graphs, for each fixed d>=3. For instance, for d=3 we prove that, for graphs on n vertices, s""""d>=0.43475n with probability approaching one as n tends to infinity."""	random regular graph	William Duckworth;Michele Zito	2009	Theor. Comput. Sci.	10.1016/j.tcs.2009.08.025	random regular graph;random graph;combinatorics;discrete mathematics;independent set;mathematics;chordal graph;indifference graph;approximation algorithm;algorithm;statistics	Theory	23.315114681081695	31.31895462433544	50938
380ea1cd2494d7f448191140cdf1b1dd4a5c7daf	approximation and parameterized runtime analysis of evolutionary algorithms for the maximum cut problem	max cut;runtime analysis approximation performance evolutionary algorithm ea local search maximum cut max cut problem;approximation algorithms;runtime analysis of evolutionary algorithms;runtime;approximation solution;experimental investigations;期刊论文;optimization;approximation methods;runtime approximation methods approximation algorithms optimization search problems algorithm design and analysis switches;search problems;run time analysis;graph theory approximation theory computational complexity;approximation performance;switches;local search algorithms approximation parameterized runtime analysis evolutionary algorithms maximum cut problem max cut problem graph vertex partition ea np complete problem expected runtime complexity;local search;algorithm design and analysis;evolutionary algorithms eas	The maximum cut (MAX-CUT) problem is to find a bipartition of the vertices in a given graph such that the number of edges with ends in different sets reaches the largest. Though, several experimental investigations have shown that evolutionary algorithms (EAs) are efficient for this NP-complete problem, there is little theoretical work about EAs on the problem. In this paper, we theoretically investigate the performance of EAs on the MAX-CUT problem. We find that both the (1+1) EA and the solutions of (m/2) + (1/4)s(G) and (m/2) + (1/2)(√8m + 1 - 1), (1+1) EA*, two simple EAs, efficiently achieve approximation where m and s(G) are respectively the number of edges and the number of odd degree vertices in the input graph. We also reveal that for a given integer k the (1+1) EA* finds a cut of size at least k in expected runtime O(nm + 1/δ4k) and a cut of size at least (m/2) + k in expected runtime O(n2m + 1/δ(64/3)k2), where δ is a constant mutation probability and n is the number of vertices in the input graph. Finally, we show that the (1+1) EA and the (1+1) EA* are better than some local search algorithms in one instance, and we also show that these two simple EAs may not be efficient in another instance.	approximation;cux1 gene;combinatorial optimization;eas chromatography, quinine;each (qualifier value);ethacrynic acid;evolutionary algorithm;global optimization;graph - visual representation;incised wound;integer (number);karp's 21 np-complete problems;largest;like button;local search (optimization);mathematical optimization;max;maximum cut;multi-objective optimization;mutation;np-completeness;optimization problem;parameterized complexity;population parameter;search algorithm;simulation;solutions;vertex (geometry)	Yuren Zhou;Xinsheng Lai;Kangshun Li	2015	IEEE Transactions on Cybernetics	10.1109/TCYB.2014.2354343	algorithm design;mathematical optimization;maximum cut;combinatorics;network switch;computer science;local search;mathematics;approximation algorithm;algorithm	Theory	21.319417362251723	18.456679110702513	50972
26419669678cd805628a11d43b54877697385113	approximate shared-memory counting despite a strong adversary	crash failure;consensus;shared memory;random sampling;distributed computing;relative error;expanders;approximate counting;martingales;data structure	A new randomized asynchronous shared-memory data structure is given for implementing an approximate counter that can be incremented once by each of <i>n</i> processes in a model that allows up to <i>n</i>−1 crash failures. For any fixed &epsis;, the counter achieves a relative error of Δ with high probability, at the cost of <i>O</i>(((1/Δ) log <i>n</i>)<sup><i>O</i>(1/&epsis;)</sup>) register operations per increment and <i>O</i>(<i>n</i><sup>4/5+&epsis;</sup>((1/Δ) log <i>n</i>)<sup><i>O</i>(1/&epsis;)</sup>) register operations per read. The counter combines randomized sampling for estimating large values with an expander for estimating small values. This is the first counter implementation that is sublinear the number of processes and works despite a strong adversary scheduler that can observe internal states of processes.  An application of the improved counter is an improved protocol for solving randomized shared-memory consensus, which reduces the best previously known individual work complexity from <i>O</i>(<i>n</i> log <i>n</i>) to an optimal <i>O</i>(<i>n</i>), resolving one of the last remaining open problems concerning consensus in this model.	adversary (cryptography);approximation algorithm;approximation error;data structure;randomized algorithm;sampling (signal processing);scheduling (computing);shared memory;with high probability	James Aspnes;Keren Censor-Hillel	2009	ACM Trans. Algorithms	10.1145/1721837.1721841	shared memory;sampling;mathematical optimization;approximation error;combinatorics;consensus;martingale;data structure;computer science;theoretical computer science;mathematics;distributed computing;algorithm	Theory	11.035649739907624	30.913809022110005	51041
5462691b9d7e0fe3325a66bd7be4f619c7e265a5	optimality of randomized algorithms for the intersection problem	search problem;complexite;algoritmo aleatorizado;search engine;buscador;algorithmique;redundancia;complejidad;plan randomise;algorithme deterministe;algorithme randomise;complexity;problema investigacion;approche deterministe;deterministic approach;aleatorizacion;deterministic algorithms;plan aleatorizado;redundancy;algorithmics;algoritmica;randomized design;indexation;enfoque determinista;borne inferieure;randomized algorithm;randomisation;moteur recherche;randomization;algoritmo optimo;probleme recherche;algorithme optimal;optimal algorithm;lower bound;cost model;redondance;cota inferior	The ”Intersection of sorted arrays” problem has applications in indexed search engines such as Google. Previous works propose and compare deterministic algorithms for this problem, and offer lower bounds on the randomized complexity in different models (cost model, alternation model). We refine the alternation model into the redundancy model to prove that randomized algorithms perform better than deterministic ones on the intersection problem. We present a randomized and simplified version of a previous algorithm, optimal in this model.	analysis of algorithms;indexed search;randomized algorithm;web search engine	Jérémy Barbay	2003		10.1007/978-3-540-39816-5_3	randomization;randomized algorithms as zero-sum games;combinatorics;complexity;search problem;computer science;mathematics;redundancy;upper and lower bounds;randomized algorithm;deterministic system;completely randomized design;algorithmics;algorithm;search engine	Theory	16.583679533389148	27.12616900131099	51054
1ce7f9a9d8226d10df50819cec2aea00f4255a87	dimension spectra of random subfractals of self-similar fractals	effective dimension	The (constructive Hausdorff) dimension of a point x in Euclidean space is the algorithmic information density of x. Roughly speaking, this is the least real number dim(x) such that r×dim(x) bits suffices to specify x on a general-purpose computer with arbitrarily high precisions 2−r. The dimension spectrum of a set X in Euclidean space is the subset of [0, n] consisting of the dimensions of all points in X. The dimensions of points have been shown to be geometrically meaningful (Lutz 2003, Hitchcock 2003), and the dimensions of points in self-similar fractals have been completely analyzed (Lutz and Mayordomo 2008). Here we begin the more challenging task of analyzing the dimensions of points in random fractals. We focus on fractals that are randomly selected subfractals of a given self-similar fractal. We formulate the specification of a point in such a subfractal as the outcome of an infinite two-player game between a selector that selects the subfractal and a coder that selects a point within the subfractal. Our selectors are algorithmically random with respect to various probability measures, so our selector-coder games are, from the coder’s point of view, games against nature. We determine the dimension spectra of a wide class of such randomly selected subfractals. We show that each such fractal has a dimension spectrum that is a closed interval whose endpoints can be computed or approximated from the parameters of the fractal. In general, the maximum of the spectrum is determined by the degree to which the coder can reinforce the randomness in the selector, while the minimum is determined by the degree to which the coder can cancel randomness in the selector. This constructive and destructive interference between the players’ randomnesses is somewhat subtle, even in the simplest cases. Our proof techniques include van Lambalgen’s theorem on independent random sequences, measure preserving transformations, an application of network flow theory, a Kolmogorov complexity lower bound argument, and a nonconstructive proof that this bound is tight. ∗Linkedin Corporation, 2029 Stierlin Court, Mountain View, CA 94043, USA Email: xgu@linkedin.com †Department of Computer Science, Iowa State University, Ames, IA 50011, USA. Email: lutz@cs.iastate.edu Research supported in part by National Science Foundation Grants 0652569 and 0728806, and by Spanish Government MEC Grant TIN 2005-08832-C03-02. Part of this work was done during a sabbatical at Caltech and the Isaac Newton Institute for Mathematical Sciences at the University of Cambridge. ‡Departamento de Informática e Ingenieŕıa de Sistemas, Instituto de Investigación en Ingenieŕıa de Aragón, Universidad de Zaragoza, 50018 Zaragoza, Spain. Email: elvira(at)unizar.es §Research supported in part by Spanish Government MEC Grants TIN2005-08832-C03-02, TIN2008-06582-C0302, and TIN2011-27479-C04-01. ¶Part of this author’s research was performed during a visit at Iowa State University, supported by Spanish Government (Secretaŕıa de Estado de Universidades e Investigación del Ministerio de Educación y Ciencia) grant for research stays PR2007-0368. ‖Department of Computer Science, National University of Ireland, Maynooth. Maynooth, Co. Kildare. Ireland. Email: pmoser(at)cs.nuim.ie.	algorithmically random sequence;approximation algorithm;computer science;decision theory;email;fractal;general-purpose modeling;hausdorff dimension;information design;interference (communication);jack lutz;kolmogorov complexity;newton;randomness;self-similarity;serial digital video out	Xiaoyang Gu;Jack H. Lutz;Elvira Mayordomo;Philippe Moser	2014	Ann. Pure Appl. Logic	10.1016/j.apal.2014.07.001	combinatorics;mathematical analysis;discrete mathematics;topology;effective dimension;hausdorff dimension;mathematics;minkowski–bouligand dimension;algorithm;algebra	Theory	19.274123419191767	18.849644526474446	51103
9f26ebb8c486a8184e7ac83530d1dcc54fb5393a	budget management with applications	conjunto independiente;graphe transitif;directed acyclic graph;grafo aciclico;metodo polinomial;concepcion asistida;computer aided design;gestion budget;temps polynomial;independent set;teoria conjunto;circuit vlsi;theorie ensemble;graphe acyclique;set theory;constraint satisfaction;slack assignment;acyclic graph;polynomial time algorithm;satisfaction contrainte;graphe pondere;vlsi circuit;ensemble independant;polynomial method;directed graph;graphe oriente;grafo transitivo;polynomial time;conception assistee;grafo orientado;satisfaccion restriccion;weighted graph;circuito vlsi;methode polynomiale;transitive graph;tiempo polinomial;maximum independent set;timing;time constraint	Given a directed acyclic graph with timing constraints, the budget management problem is to assign to each vertex an incremental delay such that the sum of these delays is maximized without violating given constraints. We propose the notion of slack sensitivity and budget gradient to demonstrate the characteristics of budget management. We develop a polynomial-time algorithm for the budget management problem, based on the maximum independent set of an established transitive graph . We show the comparison of our approach with the well-known zero-slack algorithm , and extend it to general weighted graphs . Applications to a class of problems in VLSI CAD are also discussed.	algorithm;analysis of algorithms;computer-aided design;decision problem;directed acyclic graph;gradient;heuristic;independent set (graph theory);polynomial;sensitivity and specificity;slack variable;time complexity;very-large-scale integration;whole earth 'lectronic link	Chunhong Chen;Elaheh Bozorgzadeh;Ankur Srivastava;Majid Sarrafzadeh	2002	Algorithmica	10.1007/s00453-002-0964-7	mathematical optimization;budget constraint;combinatorics;discrete mathematics;independent set;computer aided design;mathematics;geometry;directed acyclic graph;algorithm	DB	21.36943582876858	26.95628775926177	51119
deab1e467cf5732fceb24d68710c553cb9229567	efficient computation of pairwise minimax distance measures		We study efficient computation of Minimax distances measures, which enable to capture the correct structures via taking the transitive relations into account. We analyze in detail two settings, the dense graphs and the sparse graphs. In particular, we show that an adapted variant of the Kruskal’s algorithm is the most efficient approach for computing pairwise Minimax distances. However, for dense graphs we require a preprocessing step based on the Prim’s algorithm, in order to reduce the set of candidate edges to be investigated. For each case, we study the correctness, efficiency and computational optimality of our approach. We perform numerical experiments on several datasets to validate the superior performance of our methods.	computation;correctness (computer science);dijkstra's algorithm;experiment;file spanning;floyd–warshall algorithm;kruskal's algorithm;minimax;minimum spanning tree;numerical analysis;preprocessor;prim's algorithm;sparse matrix;supercomputer;turing completeness	Morteza Haghir Chehreghani	2017	2017 IEEE International Conference on Data Mining (ICDM)	10.1109/ICDM.2017.95	artificial intelligence;machine learning;distance measures;mathematical optimization;kruskal's algorithm;correctness;computation;computer science;minimax;pairwise comparison;transitive relation;data structure	Robotics	19.371881900926418	21.54926103217518	51392
41dcedd4a23cffc707e6ae873c75829351113b9f	shortest paths algorithms: theory and experimental evaluation	camino mas corto;graph theory;shortest path;shortest paths;optimisation;etude theorique;shortest path algorithm;etude experimentale;directed hypergraphs;edit distance;plus court chemin;reseau;theory and experimental evaluation of algorithms;theorie graphe;red;algorithme;evolutionary trees;theoretical analysis;estudio teorico;network optimization;algorithms;recombination;optimization;experimental evaluation;theoretical study;bottleneck optimality;computational biology;graph algorithms;estudio experimental;network	We conduct an extensive computational study of shortest paths algorithms, including some very recent algorithms. We also suggest new algorithms motivated by the experimental results and prove interesting theoretical results suggested by the experimental data. Our computational study is based on several natural problem classes which identify strengths and weaknesses of various algorithms. These problem classes and algorithm implementations form an environment for testing the performance of shortest paths algorithms. The interaction between the experimental evaluation of algorithm behavior and the theoretical analysis of algorithm performance plays an important role in our research.	genetic algorithm;shortest path problem	Boris V. Cherkassky;Andrew V. Goldberg;Tomasz Radzik	1994		10.1007/BF02592101	combinatorics;probabilistic analysis of algorithms;phylogenetic tree;dijkstra's algorithm;edit distance;graph theory;machine learning;mathematics;shortest path problem;algorithm;recombination	ML	16.260327178578265	21.077007630632906	51499
c1698406207c34143fc56222c37dd439a8f078d3	an extension of the string-to-string correction problem	string modification;correction;spelling correction;string correction;string-to-string correction problem	"""The string-to-string correction problem asks for a sequence S of """"edit operations"""" of minimal cost such that ~(A) = B, for given strings A and B. The edit operations previously investigated allow changing one symbol of a string into another single symbol, deleting one symbol from a string, or inserting a single symbol into a string. This paper extends the set of allowable edit operations to include the operation of interchanging the positions of two adjacent characters Under certain restrictions on edit-operation costs, it is shown that the extended problem can still be solved in time proportional to the product of the lengths of the given strings."""	algorithm;string (computer science);string-to-string correction problem	Roy Lowrance;Robert A. Wagner	1975	J. ACM	10.1145/321879.321880	arithmetic;combinatorics;edit distance;mathematics;string-to-string correction problem;algorithm	Theory	11.171149986680797	27.800616001501535	51519
0bcb27daea1142a62c5a9202107b7324e9bef900	transitive closure algorithm memtc and its performance analysis	search problem;congres;strongly connected component;europa;gestion memoire;transitive closure algorithm;gestion;approximation algorithm;storage management;experience;cerradura transitiva;italia;result;problema investigacion;operations research;congreso;algorithme memtc;1998;algorithme;algorithm;gestion memoria;algorithme fermeture transitive;fermeture transitive;compact representation;recherche operationnelle;italie;performance analysis;resultado;italy;transitive closure;resultat;experiencia;europe;memtc algorithm;algorithme approximation;probleme recherche;management;investigacion operacional;algoritmo;congress	We present a new algorithm for computing the full transitive closure designed for operation in layered memories. We analyze its average-case performance experimentally in an environment where two layers of memory of diierent speed are used. In our analysis, we use trace-based simulation of memory operations.	algorithm;best, worst and average case;experiment;profiling (computer programming);trace-based simulation;transitive closure	Vesa Hirvisalo;Esko Nuutila;Eljas Soisalon-Soininen	2001	Discrete Applied Mathematics	10.1016/S0166-218X(00)00304-8	floyd–warshall algorithm;search problem;calculus;mathematics;transitive closure;strongly connected component;approximation algorithm;algorithm	HPC	16.42070173839896	27.363014700144287	51605
040515ef54570c13c61ed29ef69ff5edc23a92d7	testing linearity against non-signaling strategies		Non-signaling strategies are collections of distributions with certain non-local correlations. They have been studied in Physics as a strict generalization of quantum strategies to understand the power and limitations of Nature’s apparent non-locality. Recently, they have received attention in Theoretical Computer Science due to connections to Complexity and Cryptography. We initiate the study of Property Testing against non-signaling strategies, focusing first on the classical problem of linearity testing (Blum, Luby, and Rubinfeld; JCSS 1993). We prove that any non-signaling strategy that passes the linearity test with high probability must be close to a quasi-distribution over linear functions. Quasi-distributions generalize the notion of probability distributions over global objects (such as functions) by allowing negative probabilities, while at the same time requiring that “local views” follow standard distributions (with non-negative probabilities). Quasi-distributions arise naturally in the study of Quantum Mechanics as a tool to describe various non-local phenomena. Our analysis of the linearity test relies on Fourier analytic techniques applied to quasidistributions. Along the way, we also establish general equivalences between non-signaling strategies and quasi-distributions, which we believe will provide a useful perspective on the study of Property Testing against non-signaling strategies beyond linearity testing.	blum axioms;cryptography;journal of computer and system sciences;linear function;locality of reference;michael luby;negative probability;property testing;quantum mechanics;quasi-quotation;theoretical computer science;with high probability	Alessandro Chiesa;Peter Manohar;Igor Shinkar	2018	Electronic Colloquium on Computational Complexity (ECCC)	10.4230/LIPIcs.CCC.2018.17	mathematics;discrete mathematics;probability distribution;combinatorics;fourier transform;cryptography;linearity;linear function;property testing;quantum	Theory	10.622887556275238	21.204653272870534	51779
0a388e78b9c0793ca922e5142789591c9bf33c0d	the complexity of the extendibility problem for finite posets	totally symmetric idempotent and near unanimity operations;finite posets;near unanimity operation;bounded width problem;finite poset p;constraint satisfaction problem;np-complete;strict width;broader class;extendibility problem for posets;finite poset;monotone total map;zigzags;bounded strict width constraint;finite relational structure;complexity class np;extendibility problem;polynomial time;decision problem;np complete;complexity class	For a finite poset P let EXT(P ) denote the following decision problem. Given a finite poset Q and a partial map f from Q to P , decide whether f extends to a monotone total map from Q to P . It is easy to see that EXT(P ) is in the complexity class NP. In [SIAM J. Comput., 28 (1998), pp. 57–104], Feder and Vardi define the classes of width 1 and of bounded strict width constraint satisfaction problems for finite relational structures. Both classes belong to the broader class of bounded width problems in P. We prove that for any finite poset P , if EXT(P ) has bounded strict width, then it has width 1. In other words, if a poset admits a near unanimity operation, it also admits a totally symmetric idempotent operation of any arity. In [Fund. Inform., 28 (1996), pp. 165–182], Pratt and Tiuryn proved that SAT(P ), a polynomial-time equivalent of EXT(P ) is NPcomplete if P is a crown. We generalize Pratt and Tiuryn’s result on crowns by proving that EXT(P ), is NP-complete for any finite poset P which admits no nontrivial idempotent Malcev condition.	complexity class;constraint satisfaction problem;crown group;decision problem;idempotence;knuth–morris–pratt algorithm;np (complexity);np-completeness;olami–feder–christensen model;polynomial;polynomial-time reduction;time complexity;monotone	Benoit Larose;László Zádori	2003	SIAM J. Discrete Math.		combinatorics;discrete mathematics;np-complete;mathematics;geometry;algorithm;algebra	Theory	12.797662312636193	20.42168015983435	51877
9b7110357b9f8c26c970963af09432340e6138de	approximating edge dominating set in dense graphs	dense instances;approximation lower bounds;approximate algorithm;approximation algorithms;edge dominating set;dominating set;computational complexity;minimum maximal matching;lower bound	We study the approximation complexity of the Minimum Edge Dominating Set problem in everywhere e-dense and average e-dense graphs. More precisely, we consider the computational complexity of approximating a generalization of the Minimum Edge Dominating Set problem, the so called Minimum Subset Edge Dominating Set problem. As a direct result, we obtain for the special case of the Minimum Edge Dominating Set problem in everywhere e-dense and average e-dense graphs by using the techniques of Karpinski and Zelikovsky, the approximation ratios of min{2, 3/(1+2e)} and of min{2, 3/(3-2√1 -e)}, respectively. On the other hand, we show that it is UGC-hard to approximate the Minimum Edge Dominating Set problem in everywhere e-dense graphs with a ratio better than 2/(1 + e) with e > 1/3 and 2/(2 -√1 - e) with e > 5/9 in average e-dense graphs.	edge dominating set	Richard Schmied;Claus Viehmann	2011		10.1007/978-3-642-20877-5_5	mathematical optimization;combinatorics;discrete mathematics;independent set;bidimensionality;dominating set;mathematics;maximal independent set;upper and lower bounds;computational complexity theory;approximation algorithm;algorithm	Theory	22.620514808530015	21.496752591998803	51954
6f5d9b6da794d9c92cef071d6b278201de2c8d7a	interchange rearrangement: the element-cost model	approximate algorithm;interchange rearrangement;approximation algorithm;strings rearrangement distances;rearrangement cost models;input;permutation;unit;transposition;informatique theorique;permutacion;algoritmo aproximacion;entree ordinateur;coste;entrada ordenador;algorithme approximation;algoritmo optimo;algorithme optimal;optimal algorithm;68w25;cost model;unite;unidad;computer theory;cout;transposicion;informatica teorica	Abstract   Given an input string   S   and a target string   T   when   S   is a permutation of   T  , the  interchange rearrangement problem  is to apply on   S   a sequence of interchanges, such that   S   is transformed into   T  . The  interchange  operation exchanges the position of the two elements on which it is applied. The goal is to transform   S   into   T   at the minimum cost possible, referred to as the distance between   S   and   T  . The distance can be defined by several cost models that determine the cost of every operation. There are two known models: The  Unit-cost model  and the  Length-cost model . In this paper, we suggest a natural cost model: The  Element-cost model . In this model, the cost of an operation is determined by the elements that participate in it. Though this model has been studied in other fields, it has never been considered in the context of rearrangement problems. We consider both the special case where all elements in   S   and   T   are distinct, referred to as a  permutation string , and the general case, referred to as a  general string . An efficient optimal algorithm for the  permutation string  case and efficient approximation algorithms for the  general string  case, which is   N  P  -hard, are presented. The study is broadened to include the  transposition rearrangement problem  under the  Element-cost model  and under the other known models, in order to provide additional perspective on the new model.	analysis of algorithms	Oren Kapah;Gad M. Landau;Avivit Levy;Nitsan Oz	2009	Theor. Comput. Sci.	10.1016/j.tcs.2009.07.013	combinatorics;unit;transposition;input/output;calculus;mathematics;permutation;approximation algorithm;algorithm;algebra	ECom	17.79136286865124	25.872136666637527	52065
3a658fe783d58e5e0ae854867b8dae788b25812b	error-correcting data structures	004;locally decodable codes;error correcting codes;data structures;data structures error correcting codes locally decodable codes membership;membership	We study data structures in the presence of adversarial noise. We want to encode a given object in a succinct data structure that enables us to efficiently answer specific queries about the object, even if the data structure has been corrupted by a constant fraction of errors. We measure the efficiency of a data structure in terms of its length (the number of bits in its representation) and queryanswering time, measured by the number of bit-probes to the (possibly corrupted) representation. The main issue is the trade-off between these two. This new model is the common generalization of (static) data structures and locally decodable error-correcting codes (LDCs). We prove a number of upper and lower bounds on various natural error-correcting data structure problems. In particular, we show that the optimal length of t-probe error-correcting data structures for the Membership problem (where we want to store subsets of size s from a universe of size n such that membership queries can be answered efficiently) is approximately the optimal length of t-probe LDCs that encode strings of length s. It has been conjectured that LDCs with small t must be superpolynomially long. This bad probes-versus-length trade-off carries over to error-correcting data structures for Membership and many other data structure problems. We then circumvent this problem by defining so-called relaxed error-correcting data structures, inspired by the notion of “relaxed locally decodable codes” developed in the PCP literature. Here the decoder is required to answer most queries correctly with high probability, and for the remaining queries the decoder with high probability either answers correctly or declares “don’t know.” Furthermore, if there is no noise on the data structure, it answers all queries correctly with high probability. We obtain positive results for the following two data structure problems: (1) Membership. We construct a relaxed error-correcting data structure for this problem with length nearly linear in s logn that answers membership queries with O(1) bit-probes. This nearly matches the asymptotically optimal parameters for the noiseless case: length O(s logn) and one bit-probe, due to Buhrman et al. (2) Univariate Polynomial Evaluation (namely, we want to store a univariate polynomial g of degree deg(g) ≤ s over the integers modulo n such that evaluation queries can be answered efficiently; i.e., we can evaluate the output of g on a given integer modulo n). We construct a relaxed error-correcting data structure for this problem with length nearly linear in s logn that answers evaluation queries with polylog(s) · log(n) bit-probes. This nearly matches the parameters of the best known noiseless construction due to Kedlaya and Umans.	asymptotically optimal algorithm;encode;error detection and correction;forward error correction;lagrangian relaxation;locally decodable code;modulo operation;polynomial hierarchy;succinct data structure;time complexity;with high probability	Ronald de Wolf	2009		10.4230/LIPIcs.STACS.2009.1802	block code;reed–muller code;discrete mathematics;data structure;computer science;theoretical computer science;locally decodable code;mathematics;locally testable code;programming language;error floor;algorithm	Theory	11.557467961968584	25.787732652350268	52081
0f1fbb3196eb7e0ba30929e03ee7d35c7b2f617c	a unifying graph model for designing parallel algorithms for tridiagonal systems	parallel algorithm;graph transformation;tridiagonal matrix;matrix computation;graph model	A framework based on graph theoretic notations is described for the design and analysis of a wide range of parallel tridiagonal matrix algorithms. It comprises of three basic types of graph transformation operations: partition, selection, elimination and update. We use the framework to present a unified description of many known parallel algorithms for the solution of tridiagonal systems. We also discuss the use of this framework to design parallel algorithms.	parallel algorithm	Hai-Xiang Lin	2001	Parallel Computing	10.1016/S0167-8191(01)00075-8	tridiagonal matrix;combinatorics;discrete mathematics;parallel computing;directed graph;graph bandwidth;null graph;computer science;graph partition;theoretical computer science;mathematics;voltage graph;parallel algorithm;graph;numerical linear algebra;quartic graph;adjacency matrix;tridiagonal matrix algorithm;algebra;graph rewriting	HPC	19.67094533289049	30.622654456472972	52118
56a708542328ab8416ce3c76ec30361165011d04	an optimization of alpha-beta search		Most computer programs that play games like chess choose moves by searching a lookahead tree of possible continuations. After this search computes the minimax values of the root's subtrees, the subtree with maximum value is chosen as the program's move. The alpha-beta algorithm speeds up this search by pruning irrelevant branches from the tree. In this note we present a slight modification of the alpha-beta algorithm that can further reduce the size of the root's last subtree. We assume that the reader is familiar with the negamax version of the alpha-beta algorithm as presented in [i]:	algorithm;alpha–beta pruning;computer program;continuation;dec alpha;mathematical optimization;minimax;negamax;parsing;relevance;tree (data structure)	John P. Fishburn	1980	SIGART Newsletter	10.1145/1056447.1056450	computer science;artificial intelligence;buoy;support surface;alpha–beta pruning;mechanical engineering	AI	15.22559570286921	27.988018031548258	52234
4bd92f79707992eeb691b94273830cdab65f5643	polynomial time sat decision, hypergraph transversals and the hermitian rank	linear algebra;probleme satisfiabilite;graph theory;graphe lineaire;teoria grafo;hermite interpolation;temps polynomial;interpolation hermite;heuristic method;satisfiabilite;transversal hypergraphe;metodo heuristico;logique propositionnelle;satisfiability;constraint satisfaction;theorie graphe;grafo lineal;satisfaction contrainte;interpolacion hermite;propositional logic;algebre lineaire;problema satisfactibilidad;polynomial time;algebra lineal;methode heuristique;satisfaccion restriccion;logica proposicional;satisfiability problem;hypergraph transversal;transversal hipergrafo;satisfactibilidad;linear graph;tiempo polinomial	Combining graph theory and linear algebra, we study SAT problems of low “linear algebra complexity”, considering formulas with bounded hermitian rank. We show polynomial time SAT decision of the class of formulas with hermitian rank at most one, applying methods from hypergraph transversal theory. Applications to heuristics for SAT algorithms and to the structure of minimally unsatisfiable clause-sets are discussed.	algorithm;graph partition;graph theory;heuristic (computer science);linear algebra;p (complexity);time complexity	Nicola Galesi;Oliver Kullmann	2004		10.1007/11527695_8	time complexity;combinatorics;discrete mathematics;constraint satisfaction;graph theory;hermite interpolation;linear algebra;mathematics;linear equation;propositional calculus;boolean satisfiability problem;algorithm;algebra;satisfiability	Theory	21.526902244466235	26.916519781460565	52283
fcaf23ce4a7fbb9531c98a36886bc2fda413762b	on the hardness of approximating some optimization problems that are supposedly easier than max clique	optimization problem;hardness of approximation		clique problem;max	Oleg Verbitsky	1995	Combinatorics, Probability & Computing	10.1017/S0963548300001553	optimization problem;mathematical optimization;combinatorics;discrete mathematics;mathematics;hardness of approximation	Theory	17.936442608596288	19.945821932851533	52447
70082977376add9d94973050e35d64305a8b9417	improved lower bounds for locally decodable codes and private information	private information retrieval;locally decodable code;automaton;satisfiability;automata;quantum computer;quantum physics;computational complexity;automate;lower bound	We prove new lower bounds for locally decodable codes and private information retrieval. We show that a 2-query LDC encoding nbit strings over an -bit alphabet, where the decoder only uses b bits of each queried position, needs code length m = exp (		RetrievalStephanie Wehner;Ronald de Wolf	2005		10.1007/11523468_115	discrete mathematics;computer science;artificial intelligence;theoretical computer science;machine learning;mathematics;distributed computing;automaton;algorithm	Theory	10.29814355934971	25.20243248860893	52461
f7c4730ff72898ef61f2a6e1ec7829ab9cb0f100	a new algorithm for finding minimal cycle-breaking sets of turns in a graph	computational complexity;computer simulation;upper bound;lower bound;connected graph	We consider the problem of constructing a minimal cycle-breaking set of turns for a given undirected graph. This problem is important for deadlock-free wormhole routing in computer and communication networks, such as Networks of Workstations. The proposed Cycle Breaking algorithm, or CB algorithm, guarantees that the constructed set of prohibited turns is minimal and that the fraction of the prohibited turns does not exceed 1/3 for any graph. The computational complexity of the proposed algorithm is O(N∆), where N is the number of vertices, and ∆ is the maximum node degree. The memory complexity of the algorithm is O(N∆). We provide lower bounds on the minimum size of cycle-breaking sets for connected graphs. Further, we construct minimal cycle-breaking sets and establish bounds on the minimum fraction of prohibited turns for two important classes of graphs, namely, t-partite graphs and graphs with small degrees. The upper bounds are tight and demonstrate the optimality of the CB algorithm for certain classes of graphs. Results of computer simulations illustrate the superiority of the proposed CB algorithm as compared to the well-known and the widely used Up/Down technique. Article Type Communicated by Submitted Revised regular paper Khuller July 2005 April 2006 This work was supported by the NSF under Grant MIP 9630096. Levitin et al., Cycle-Breaking Sets of Turns, JGAA, 10(2) 387–420 (2006) 388	algorithm;algorithmic efficiency;computational complexity theory;computer simulation;deadlock;graph (discrete mathematics);ibm notes;irreducibility;journal of graph algorithms and applications;procedural generation;routing;telecommunications network;whole earth 'lectronic link;workstation;wormhole switching	Lev B. Levitin;Mark G. Karpovsky;Mehmet Mustafa;Lev Zakrevski	2006	J. Graph Algorithms Appl.		computer simulation;pathwidth;combinatorics;discrete mathematics;independent set;theoretical computer science;hopcroft–karp algorithm;mathematics;modular decomposition;upper and lower bounds;chordal graph;indifference graph;algorithm	Theory	19.59732440067313	29.304434366199484	52545
e44819fc059ec8128299bef3f9cf90e473ccb5bc	on constructing primitive roots in finite fields with advice		"""Finding primitive roots in a finite field of <inline-formula> <tex-math notation=""""LaTeX"""">$p^{n}$ </tex-math></inline-formula> elements of characteristic <inline-formula> <tex-math notation=""""LaTeX"""">$p$ </tex-math></inline-formula> remains to be a hard computational problem with the bottlenecks coming from both locating a small set of possible candidates and also factoring <inline-formula> <tex-math notation=""""LaTeX"""">$p^{n}-1$ </tex-math></inline-formula> in order to test these candidates. Kopparty <italic>et al.</italic> (2016) have introduced a question of designing a fast algorithm to find primitive roots with a short advice from an oracle. Trivially, for an <inline-formula> <tex-math notation=""""LaTeX"""">$m$ </tex-math></inline-formula>-bit prime <inline-formula> <tex-math notation=""""LaTeX"""">$p$ </tex-math></inline-formula>, such a primitive root can be fully described by about <inline-formula> <tex-math notation=""""LaTeX"""">$mn$ </tex-math></inline-formula> bits of information received from an all-powerful oracle. Here, we have shown that one can achieve this in polynomial time and with about <inline-formula> <tex-math notation=""""LaTeX"""">$(1/2+o(1))m + O(\log n)$ </tex-math></inline-formula> bits of advice."""	algorithm;computation;computational problem;integer factorization;oracle database;roots;time complexity	Igor E. Shparlinski	2018	IEEE Transactions on Information Theory	10.1109/TIT.2018.2810938	time complexity;discrete mathematics;computational problem;prime (order theory);combinatorics;oracle;finite element method;primitive root modulo n;computer science;binary logarithm;finite field	Theory	16.54307886467341	30.270278511623697	52582
a8027a7719eac4b94c06416f203268bd09ae823e	consistent polynomial-time unseeded graph matching for lipschitz graphons		We propose a consistent polynomial-time method for the unseeded node matching problem for networks with smooth underlying structures. Despite widely conjectured by the research community that the structured graph matching problem to be significantly easier than its worst case counterpart, well-known to be NP-hard, the statistical version of the problem has stood a challenge that resisted any solution both provable and polynomial-time. The closest existing work requires quasi-polynomial time. Our method is based on the latest advances in graphon estimation techniques and analysis on the concentration of empirical Wasserstein distances. Its core is a simple yet unconventional sampling-and-matching scheme that reduces the problem from unseeded to seeded. Our method allows flexible efficiencies, is convenient to analyze and potentially can be extended to more general settings. Our work enables a rich variety of subsequent estimations and inferences.	best, worst and average case;graphon;matching (graph theory);np-hardness;polynomial;provable security;quasi-polynomial;sampling (signal processing);time complexity	Yuan Zhang	2018	CoRR		mathematical optimization;time complexity;mathematics;matching (graph theory);lipschitz continuity	ML	18.560815970277755	20.143510812735617	52663
76572c99c68ffdca1bb497eb6da75cc0ff7add7f	efficient pram simulation on a distributed memory machine	distributed memory machine;processor sharing	We present a randomized simulation of a <italic>n</italic>log log (<italic>n</italic>) log (<italic>n</italic>)-processor shared memory machine (DMM) with optimal expected delay O(log log (<italic>n</italic>)) per step of simulation. The time bound for the delay is guaranteed with overwhelming probability. The algorithm is based on hashing and uses a novel simulation scheme. The best previous simulations use a simpler scheme based on hashing and have much larger expected delay: &THgr;(log(<italic>n</italic>)/log log (<italic>n</italic>)) for the simulation of an <italic>n</italic>-processor PRAM on an <italic>n</italic> processor DMM, and &THgr;(log(<italic>n</italic>)) in the case where the simulation preserves the processor-time product.	cryptographic hash function;digital molecular matter (dmm);distributed memory;randomized algorithm;shared memory;simulation	Richard M. Karp;Michael Luby;Friedhelm Meyer auf der Heide	1992		10.1145/129712.129743	parallel computing;real-time computing;computer science;theoretical computer science;mathematics	Theory	11.008619497082268	31.23281404046879	52695
2339c61d8f597d04b1adc6aca46cdd3e880824f2	randomized computations on large data sets: tight lower bounds	relation algebra;query processing;data stream;computer model;large data sets;query optimization;complexity;real time data;decision problem;semi structured data;complex data;computational complexity;query evaluation;monte carlo algorithm;xml;external memory;error bound;query processing query optimization;data streams real time data;random access;lower bound	We study the randomized version of a computation model (introduced in [9, 10]) that restricts random access to external memory and internal memory space. Essentially, this model can be viewed as a powerful version of a data stream model that puts no cost on sequential scans of external memory (as other models for data streams) and, in addition, (like other external memory models, but unlike streaming models), admits several large external memory devices that can be read and written to in parallel.We obtain tight lower bounds for the decision problems set equality, multiset equality, and checksort. More precisely, we show that any randomized one-sided-error bounded Monte Carlo algorithm for these problems must perform Ω(logN) random accesses to external memory devices, provided that the internal memory size is at most O(4√N/logN), where N denotes the size of the input data.From the lower bound on the set equality problem we can infer lower bounds on the worst case data complexity of query evaluation for the languages XQuery, XPath, and relational algebra on streaming data. More precisely, we show that there exist queries in XQuery, XPath, and relational algebra, such that any (randomized) Las Vegas algorithm that evaluates these queries must perform Ω(logN) random accesses to external memory devices, provided that the internal memory size is at most O(4√N/logN).	best, worst and average case;computer data storage;dspace;decision problem;existential quantification;las vegas algorithm;model of computation;monte carlo algorithm;monte carlo method;random access;randomized algorithm;relational algebra;stream (computing);streaming algorithm;xpath;xquery	Martin Grohe;André Hernich;Nicole Schweikardt	2006		10.1145/1142351.1142387	computer simulation;query optimization;real-time data;semi-structured data;complexity;xml;computer science;theoretical computer science;decision problem;data mining;relation algebra;database;upper and lower bounds;programming language;computational complexity theory;algorithm;random access;monte carlo algorithm;complex data type	DB	12.599909177833801	24.59391629767999	52705
409b4a5f62e569962045bff49e17e8d9068c169a	fractional pebbling game lower bounds		Fractional pebbling is a generalization of black-white pebbling introduced recently. In this reasearch paper we solve an open problem by proving a tight lower bound on the pebble weight required to fractionally pebble a balanced d-ary tree of height h. This bound has close ties with branching programs and the separation of P from NL.	nl (complexity);p (complexity)	Frank Vanderzwet	2011	CoRR		combinatorics;discrete mathematics;mathematics;algorithm	Theory	19.485882837423663	23.63544281177022	52774
49d9fdbfad434ccea85c578373ee950dab4601ae	improved combinatorial group testing algorithms for real-world problem sizes	world;bloom filter;combinatorial group testing;combinatorial algorithm;chinese remaindering;algorithme combinatoire;68wxx;bloom filters;borne inferieure;68q25;monde;mundo;theorie information;information theoretic;data structure;lower bound;information theory;group testing;cota inferior;teoria informacion	We study practically efficient methods for performing combi natorial group testing. We present efficient non-adaptive and two-stage combinatorial group test ing algorithms, which identify the at most d items out of a given set of n items that are defective, using fewer tests for all practica l set sizes. For example, our two-stage algorithm matches the information t heoretic lower bound for the number of tests in a combinatorial group testing regimen.	algorithm	David Eppstein;Michael T. Goodrich;Daniel S. Hirschberg	2007	SIAM J. Comput.	10.1137/050631847	mathematical optimization;combinatorics;data structure;information theory;all-pairs testing;computer science;bloom filter;mathematics;programming language;algorithm	Theory	16.57182399855468	25.1768150164205	52778
57a3bdb3731f24bdf2431378e48c1afa1d384cfe	quadratic time, linear space algorithms for gram-schmidt orthogonalization and gaussian sampling in structured lattices		A procedure for sampling lattice vectors is at the heart of many lattice constructions, and the algorithm of Klein (SODA 2000) and Gentry, Peikert, Vaikuntanathan (STOC 2008) is currently the one that produces the shortest vectors. But due to the fact that its most time-efficient (quadratic-time) variant requires the storage of the GramSchmidt basis, the asymptotic space requirements of this algorithm are the same for general and ideal lattices. The main result of the current work is a series of algorithms that ultimately lead to a sampling procedure producing the same outputs as the Klein/GPV one, but requiring only linear-storage when working on lattices used in ideal-lattice cryptography. The reduced storage directly leads to a reduction in key-sizes by a factor of Ω(d), and makes cryptographic constructions requiring lattice sampling much more suitable for practical applications. At the core of our improvements is a new, faster algorithm for computing the Gram-Schmidt orthogonalization of a set of vectors that are related via a linear isometry. In particular, for a linear isometry r : R → R which is computable in time O(d) and a d-dimensional vector b, our algorithm for computing the orthogonalization of (b, r(b), r(b), . . . , rd−1(b)) uses O(d) floating point operations. This is in contrast to O(d) such operations that are required by the standard Gram-Schmidt algorithm. This improvement is directly applicable to bases that appear in ideal-lattice cryptography because those bases exhibit such “isometric structure”. The above-mentioned algorithm improves on a previous one of Gama, Howgrave-Graham, Nguyen (EUROCRYPT 2006) which used different techniques to achieve only a constant-factor speed-up for similar lattice bases. Interestingly, our present ideas can be combined with those from Gama et al. to achieve an even an larger practical speed-up. We next show how this new Gram-Schmidt algorithm can be applied towards lattice sampling in quadratic time using only linear space. The main idea is that rather than pre-computing and storing the GramSchmidt vectors, one can compute them “on-the-fly” while running the ? This research was partially supported by the ANR JCJC grant “CLE”. c ©IACR 2015. This article is the final version submitted by the authors to the IACR and to SpringerVerlag on 2015-01-30, to be published in Advances in Cryptology EUROCRYPT 2015. 2 GS Orthogonalization and Gaussian Sampling in Structured Lattices sampling algorithm. We also rigorously analyze the required arithmetic precision necessary for achieving negligible statistical distance between the outputs of our sampling algorithm and the desired Gaussian distribution. The results of our experiments involving NTRU lattices show that the practical performance improvements of our algorithms are as predicted in theory.	algorithm;ambiguous name resolution;computable function;conformal loop ensemble;eurocrypt;experiment;gibbs sampling;graham scan;ideal lattice cryptography;isometric projection;ntru;precomputation;requirement;sampling (signal processing);schmidt decomposition;significant figures;symposium on theory of computing;time complexity	Vadim Lyubashevsky;Thomas Prest	2015	IACR Cryptology ePrint Archive	10.1007/978-3-662-46800-5_30	mathematical optimization;combinatorics;discrete mathematics;mathematics;algorithm;statistics;algebra	Theory	11.879705408244781	22.51355263929687	53001
9b620c705819a7c7934b5d0ee3689b65105fb40a	the relationship between the gossip complexity in vertex-disjoint paths mode and the vertex bisection width	graph theory;reseau communication;teoria grafo;algorithm complexity;complejidad algoritmo;communication complexity;grid graph;complexite communication;theorie graphe;interconnection network;algorithme;algorithm;complexite algorithme;communication algorithms;graph bisection;hypercube graph;parallel computations;red de comunicacion;communication network;algorithm design;red interconexion;lower bound;planar graph;disjoint paths;algoritmo;reseau interconnexion	The one-way and two-way communication modes used for sending messages to processors of interconnection networks via vertex-disjoint paths in one communication step are investigated. The complexity of communication algorithms is measured by the number of communication steps (rounds). This paper reveals a direct relationship between the gossip complexity and the vertex bisection width. More precisely, the main results are the following: We prove that for any 2-way gossip algorithm running on a graph Gn,k of n nodes and vertex bisection width k, the lower bound on the number of rounds is 2 log, n-log, k-log, log, k-6. We prove that there is a graph G,,k of n nodes and vertex bisection width k such that there exists a 2-way gossip algorithm for it running in 2 log, n log, k log, log, k + 7 rounds. We prove that for any l-way “well-structured” gossip algorithm running on a graph G,,,k of n nodes and vertex bisection width k, the lower bound on the number of rounds is 2 log, n 0.56. . (log, k + log, log, k) 0( 1) (to the best of the authors’ knowledge, to date all gossip algorithms designed in vertex-disjoint paths mode have been “well-structured”). We prove that there is a graph Gn,k of n nodes and vertex bisection width k such that there exists a l-way gossip algorithm for it running in 2 log, rz-0.56.. :(log, k+log, log, k)+O( I ) rounds. These results improve the previous results and prove their optimality with respect to the class of graphs G,,k of n nodes and vertex bisection width k. Tight lower bounds on one-way “well-structured” gossip algorithms in d-dimensional grids, d>3, several results for gossiping in graphs of constant degree and planar graphs, as well as similar results for the related edge-disjoint paths mode follow.	central processing unit;dijkstra's algorithm;interconnection;one-way function;planar graph;symmetric multiprocessing	Ralf Klasing	1998	Discrete Applied Mathematics	10.1016/S0166-218X(97)00112-1	algorithm design;combinatorics;discrete mathematics;graph theory;hypercube graph;communication complexity;mathematics;upper and lower bounds;algorithm;telecommunications network;planar graph	Theory	19.167225764023424	29.014168436396695	53051
2226f7903a05b011ca2028a65a28c0085b2aa55f	practical algorithmic techniques for several string processing problems	ordered set;data mining;suffix array;deterministic finite automata;automata theory;data structure;algorithms and data structure;formal language;knowledge discovery	The domains of data mining and knowledge discovery make use of large amounts of textual data, which need to be handled efficiently. Specific problems, like finding the maximum weight ordered common subset of a set of ordered sets or searching for specific patterns within texts, occur frequently in this context. In this paper we present several novel and practical algorithmic techniques for processing textual data (strings) in order to efficiently solve multiple problems. Our techniques make use of efficient string algorithms and data structures, like KMP, suffix arrays, tries and deterministic finite automata. Keywords-string processing; prefix query; trie; suffix array; KMP; deterministic finite automaton	algorithm;automata theory;comparison of programming languages (string functions);data mining and knowledge discovery;data structure;deterministic finite automaton;finite-state machine;klee's measure problem;string (computer science);suffix array;text corpus;trie	Mugurel Ionut Andreica;Nicolae Tapus	2009	CoRR		combinatorics;formal language;data structure;computer science;theoretical computer science;deterministic finite automaton;automata theory;data mining;programming language;algorithm	DB	13.308961845815116	27.318523414839976	53179
3a9e59271c5ae81d7fb791e971777df9c6fb1bf0	on the hardness of approximate and exact (bichromatic) maximum inner product		In this paper we study the (Bichromatic) Maximum Inner Product Problem (Max-IP), in which we are given sets A and B of vectors, and the goal is to find a ∈ A and b ∈ B maximizing inner product a · b. Max-IP is very basic and serves as the base problem in the recent breakthrough of [Abboud et al., FOCS 2017] on hardness of approximation for polynomial-time problems. It is also used (implicitly) in the argument for hardness of exact `2-Furthest Pair (and other important problems in computational geometry) in poly-log-log dimensions in [Williams, SODA 2018]. We have three main results regarding this problem. • Characterization of Multiplicative Approximation. First, we study the best multiplicative approximation ratio for Boolean Max-IP in sub-quadratic time. We show that, for Max-IP with two sets of n vectors from {0,1}d , there is an n2−Ω(1) time (d/ logn)Ω(1)-multiplicative-approximating algorithm, and we show this is conditionally optimal, as such a (d/ logn)-approximating algorithm would refute SETH. Similar characterization is also achieved for additive approximation for Max-IP. • 2O(log ∗ n)-dimensional Hardness for Exact Max-IP Over The Integers. Second, we revisit the hardness of solving Max-IP exactly for vectors with integer entries. We show that, under SETH, for Max-IP with sets of n vectors from Zd for some d = 2O(log ∗ n), *Supported by an Akamai Fellowship. ACM Classification: F.1.3 AMS Classification: 68Q17	acm computing classification system;approximation algorithm;computation;computational geometry;hardness of approximation;internet protocol suite;monochrome;polyhedron;polynomial;symposium on foundations of computer science;time complexity;utility functions on indivisible goods	Lijie Chen	2018		10.4230/LIPIcs.CCC.2018.14		Theory	19.55252390110875	18.938934393704063	53418
753f71fbff8a37ffc926c35e7245444feea01096	lower bounds for symbolic computation on graphs: strongly connected components, liveness, safety, and diameter		A model of computation that is widely used in the formal analysis of reactive systems is symbolic algorithms. In this model the access to the input graph is restricted to consist of symbolic operations, which are expensive in comparison to the standard RAM operations. We give lower bounds on the number of symbolic operations for basic graph problems such as the computation of the strongly connected components and of the approximate diameter as well as for fundamental problems in model checking such as safety, liveness, and co-liveness. Our lower bounds are linear in the number of vertices of the graph, even for constant-diameter graphs. For none of these problems lower bounds on the number of symbolic operations were known before. The lower bounds show an interesting separation of these problems from the reachability problem, which can be solved with O(D) symbolic operations, where D is the diameter of the graph. Additionally we present an approximation algorithm for the graph diameter which requires Õ(n √ D) symbolic steps to achieve a (1 + )-approximation for any constant > 0. This compares to O(n · D) symbolic steps for the (naive) exact algorithm and O(D) symbolic steps for a 2-approximation. Finally we also give a refined analysis of the strongly connected components algorithms of [GPP08], showing that it uses an optimal number of symbolic steps that is proportional to the sum of the diameters of the strongly connected components. ar X iv :1 71 1. 09 14 8v 1 [ cs .D S] 2 4 N ov 2 01 7	approximation algorithm;diameter (protocol);distance (graph theory);exact algorithm;liveness;model checking;model of computation;random-access memory;reachability problem;strongly connected component;symbolic computation	Krishnendu Chatterjee;Wolfgang Dvorák;Monika Henzinger;Veronika Loitzenbauer	2018		10.1137/1.9781611975031.151	model checking;combinatorics;discrete mathematics;mathematics;distance;approximation algorithm;symbolic computation;liveness;strongly connected component;exact algorithm;model of computation	Theory	20.68949559578602	23.614273412586364	53713
77ce3405ded0e3b8b53c5a05a59f788a306d74d2	detecting a singleton attractor in a boolean network utilizing sat algorithms	mathematical model;np hard;steady state;boolean network;satisfiability;fixed point;sat	The Boolean network (BN) is a mathematical model of genetic networks. It is known that detecting a singleton attractor, which is also called a fixed point, is NP-hard even for AND/OR BNs (i.e., BNs consisting of AND/OR nodes), where singleton attractors correspond to steady states. Though a naive algorithm can detect a singleton attractor for an AND/OR BN in O(n2 n ) time, no O((2 ― ∈) n ) (∈ u003e 0) time algorithm was known even for an AND/OR BN with non-restricted indegree, where n is the number of nodes in a BN. In this paper, we present an O(1.787 n ) time algorithm for detecting a singleton attractor of a given AND/OR BN, along with related results. We also show that detection of a singleton attractor in a BN with maximum indegree two is NP-hard and can be polynomially reduced to a satisfiability problem.	algorithm;boolean network;boolean satisfiability problem;rössler attractor	Takeyuki Tamura;Tatsuya Akutsu	2009	IEICE Transactions		combinatorics;discrete mathematics;boolean network;computer science;np-hard;mathematical model;mathematics;fixed point;steady state;algorithm;satisfiability	EDA	19.230927514364573	25.317733841473558	53715
90f382f735dc731bf1c878ea0363e9944638a7a8	maximum matching in the online batch-arrival model		Consider a two-stage matching problem, where edges of an input graph are revealed in two stages (batches) and in each stage we have to immediately and irrevocably extend our matching using the edges from that stage. The natural greedy algorithm is half competitive. Even though there is a huge literature on online matching in adversarial vertex arrival model, no positive results were previously known in adversarial edge arrival model. For two-stage bipartite matching problem, we show that the optimal competitive ratio is exactly 2/3 in both the fractional and the randomized-integral models. Furthermore, our algorithm for fractional bipartite matching is instance optimal— achieves the best competitive ratio for any given first stage graph. We also study natural extensions of this problem to general graphs and to s stages, and present randomized-integral algorithms with competitive ratio 12 + 2 . Our algorithms use a novel LP and combine graph decomposition techniques with online primal-dual analysis.	competitive analysis (online algorithm);greedy algorithm;matching (graph theory);randomized algorithm	Euiwoong Lee;Sahil Singla	2017		10.1007/978-3-319-59250-3_29	online algorithm;matching (graph theory);competitive analysis;mathematical optimization;computer science;vertex (geometry);greedy algorithm;graph	Theory	21.981445505925535	20.902640250486858	53739
227d90cb2db59a8aaf720e728b21d7f28aecdbb2	on the computational complexity of upward and rectilinear planarity testing	graph theory;curva;orthogonality;test statistique;teoria grafo;plane;circuito y;approximate algorithm;06a06;ordered set;graph drawing;visualizacion;complexite calcul;05c62;call graph;rectilinear drawing;probleme np complet;bord;vertex;approximation algorithm;test estadistico;upward drawing;statistical test;plan;courbe;problema np duro;65d18;ensemble ordonne;layout;theorie graphe;curve;diagramme;planar drawing;borde;algorithm;diagram;np hard problem;visualization;complejidad computacion;68q17;estimation erreur;visualisation;edge;probleme np difficile;error estimation;computational complexity;graphe planaire;directed graph;vertex graph;plano;graphe oriente;estimacion error;algoritmo aproximacion;graphs and networks;subroutine;completitud;and circuit;grafo orientado;problema np completo;sous programme;vertice;completeness;grafo planario;algorithme approximation;completude;circuit et;vertice grafo;orthogonalite;planar graph;orthogonal drawing;sommet graphe;np complete problem;entity relationship;subprograma;conjunto ordenado;ortogonalidad;diagrama	A directed graph is upward planar if it can be drawn in the plane such that every edge is a monotonically increasing curve in the vertical direction, and no two edges cross. An undirected graph is recti-linear planar if it can be drawn in the plane such that every edge is a horizontal or vertical segment, and no two edges cross. Testing upward planarity and rectilinear planarity are fundamental problems in the eeec-tive visualization of various graph and network structures. In this paper we show that upward planarity testing and rectilinear planarity testing are NP-complete problems. We also show that it is NP-hard to approximate the minimum number of bends in a planar orthogonal drawing of an n-vertex graph with an O(n 1?) error, for any > 0.	approximation algorithm;computational complexity theory;directed graph;graph (discrete mathematics);hasse diagram;karp's 21 np-complete problems;np-hardness;planar graph;planarity testing;regular grid;upward planar drawing	Ashim Garg;Roberto Tamassia	2001	SIAM J. Comput.	10.1137/S0097539794277123	mathematical optimization;combinatorics;discrete mathematics;visualization;planarity testing;graph theory;mathematics;approximation algorithm;algorithm	Theory	23.239028933867985	28.829795907248815	53761
46deb546565b297c7edd87230cfc873939a74bd6	optimal node disjoint paths on partial 2-trees: a linear algorithm and polyhedral results	arbre graphe;graph theory;optimisation;teoria grafo;optimizacion;tree graph;theorie graphe;algorithme;algorithm;2 tree;bipath;optimization;total length;arbol grafo;convex hull;disjoint paths;polyhedral characterization;algoritmo	We present anO(p · n) algorithm for the problem of finding disjoint simple paths of minimum total length betweenp given pairs of terminals on oriented partial 2-trees withn nodes and positive or negative arc lengths. The algorithm is inO(n) if all terminals are distinct nodes. We characterize the convex hull of the feasible solution set for the casep=2.	algorithm;polyhedron	François Margot;Alain Prodon;Thomas M. Liebling	1995	Math. Meth. of OR	10.1007/BF01432363	mathematical optimization;combinatorics;topology;graph theory;convex hull;mathematics;tree	Theory	22.614887394775334	28.920913017798956	53901
17ef1081e62ec7e4a9bf5eec10814311021b0351	computational topology and normal surfaces: theoretical and experimental complexity bounds		In three-dimensional computational topology, the theory of normal surfaces is a tool of great theoretical and practical significance. Although this theory typically leads to exponential time algorithms, very little is known about how these algorithms perform in “typical” scenarios, or how far the best known theoretical bounds are from the real worstcase scenarios. Here we study the combinatorial and algebraic complexity of normal surfaces from both the theoretical and experimental viewpoints. Theoretically, we obtain new exponential lower bounds on the worst-case complexities in a variety of settings that are important for practical computation. Experimentally, we study the worst-case and average-case complexities over a comprehensive body of roughly three billion input triangulations. Many of our lower bounds are the first known exponential lower bounds in these settings, and experimental evidence suggests that many of our theoretical lower bounds on worst-case growth rates may indeed be asymptotically tight.	algorithm;best, worst and average case;computation;computational topology;experiment;lieb-robinson bounds;time complexity	Benjamin A. Burton;João Paixão;Jonathan Spreer	2013		10.1137/1.9781611972931.7	mathematical optimization;combinatorics;discrete mathematics;mathematics	Theory	18.72033457050283	20.161358197435362	54135
769d17b481aeb61fb1da1f9172470f2ed1942a8e	a fast parallel algorithm for six-colouring of planar graphs (extended abstract)	fast parallel algorithm;planar graphs;extended abstract;planar graph;parallel algorithm;parallel random access machine;parallel computer	We present a parallel algorithm for colouring outer-planar graphs with minimal possible number of colours (at most 3). The algorithm runs in O(log n) time, O(n) space and uses O(n) processors on a concurrent-read concurrent-write (CRCW) parallel random access machine, where n is the number of vertices of a given graph. As an application we obtain a six-colouring parallel algorithm for planar graphs running on the same model of parallel computations in O(log n) time, O(n3) space and using O(n4) processors.	parallel algorithm	Krzysztof Diks	1986		10.1007/BFb0016251	1-planar graph;hopcroft–karp algorithm;planar straight-line graph;parallel algorithm	Theory	18.729761581423535	28.53950164745855	54328
5f26852e0989572d10dfdd9a99b9725716359770	the complexity of arc-colorings for directed hypergraphs	complexite;hipergrafico;wall;optimisation;coloracion grafo;temps polynomial;optimizacion;05c65;complejidad;complexity;directed hypergraph;coloration graphe;informatique theorique;directed graph;graphe oriente;polynomial time;grafo orientado;optimization;arc coloring;hypergraph;brick coloring;graph colouring;hypergraphe;computer theory;tiempo polinomial;informatica teorica	We address some complexity questions related to the arc-coloring of directed hypergraphs. Such hypergraphs arise as a generalization of digraphs, by allowing the tail of each arc to consist of more than one node. The related arc-coloring extends the notion of digraph arc-coloring, which has been studied by diverse authors. Using two classical results we easily prove that the optimal coloring of a digraph, as well as the 2-coloring test for every directed hypergraph, require polynomial time. Instead, the k-colorability problem for some fixed degree d is shown to be NP-complete if k ≥ d ≥ 2 and k ≥ 3, even if the input is restricted to the so-called non-overlapping hypergraphs. We also describe a subclass of hypergraphs for which the 3-colorability test is polynomially decidable. Some results are rephrased and proved using suitable adjacency matrices, namely walls.		Andrea Vietri	2004	Discrete Applied Mathematics	10.1016/j.dam.2004.04.002	time complexity;combinatorics;discrete mathematics;complexity;directed graph;mathematics;algorithm	Theory	23.23787209565183	25.134876678806968	54534
a1086cd8c5779e03d7f9de2ceed782de22645c99	the complexity of phylogeny constraint satisfaction	004;constraint satisfaction problems computational complexity phylogenetic reconstruction ramsey theory model theory	We systematically study the computational complexity of a broad class of computational problems in phylogenetic reconstruction. The class contains for example the rooted triple consistency problem, forbidden subtree problems, the quartet consistency problem, and many other problems studied in the bioinformatics literature. The studied problems can be described as constraint satisfaction problems where the constraints have a first-order definition over the rooted triple relation. We show that every such phylogeny problem can be solved in polynomial time or is NP-complete. On the algorithmic side, we generalize a well-known polynomial-time algorithm of Aho, Sagiv, Szymanski, and Ullman for the rooted triple consistency problem. Our algorithm repeatedly solves linear equation systems to construct a solution in polynomial time. We then show that every phylogeny problem that cannot be solved by our algorithm is NP-complete. Our classification establishes a dichotomy for a large class of infinite structures that we believe is of independent interest in universal algebra, model theory, and topology. The proof of our main result combines results and techniques from various research areas: a recent classification of the model-complete cores of the reducts of the homogeneous binary branching C-relation, Leeb’s Ramsey theorem for rooted trees, and universal algebra. 1998 ACM Subject Classification F.2.2 Nonnumerical Algorithms and Problems	bioinformatics;computational complexity theory;computational problem;constraint satisfaction problem;first-order predicate;linear equation;np-completeness;phylogenetic tree;phylogenetics;polynomial;ramsey's theorem;sethi–ullman algorithm;time complexity;tree (data structure)	Manuel Bodirsky;Peter Jonsson;Van Trung Pham	2016		10.4230/LIPIcs.STACS.2016.20	combinatorics;discrete mathematics;computer science;mathematics;complexity of constraint satisfaction;constraint satisfaction problem;algorithm;local consistency	Theory	17.60936840396579	21.565484735337712	54686
812315fc4036676de8454d4e99e2bc85b9a900ab	why simple hash functions work: exploiting the entropy in a data stream	equi partition;performance guarantee;fairness;bloom filter;data stream;online scheduling;non clairvoyant algorithm;theoretical analysis;hash function;renyi entropy;precedences;randomness extractors;algorithms and data structure;random mapping	"""Hashing is fundamental to many algorithms and data structures widely used in practice. For theoretical analysis of hashing, there have been two main approaches. First, one can assume that the hash function is truly random, mapping each data item independently and uniformly to the range. This idealized model is unrealistic because a truly random hash function requires an exponential number of bits to describe. Alternatively, one can provide rigorous bounds on performance when explicit families of hash functions are used, such as 2-universal or O(1)-wise independent families. For such families, performance guarantees are often noticeably weaker than for ideal hashing.  In practice, however, it is commonly observed that simple hash functions, including 2-universal hash functions, perform as predicted by the idealized analysis for truly random hash functions. In this paper, we try to explain this phenomenon. We demonstrate that the strong performance of universal hash functions in practice can arise naturally from a combination of the randomness of the hash function and the data. Specifially, following the large body of literature on random sources and randomness extraction, we model the data as coming from a """"block source,"""" whereby each new data item has some """"entropy"""" given the previous ones. As long as the (Renyi) entropy per data item is sufficiently large, it turns out that the performance when choosing a hash function from a 2-universal family is essentially the same as for a truly random hash function. We describe results for several sample applications, including linear probing, balanced allocations, and Bloom filters."""	algorithm;bloom filter;cryptographic hash function;data item;data structure;linear probing;randomness;rényi entropy;time complexity;universal hashing	Michael Mitzenmacher;Salil P. Vadhan	2008	Theory of Computing	10.4086/toc.2013.v009a030	feature hashing;mathematical optimization;hash table;double hashing;combinatorics;discrete mathematics;hash function;perfect hash function;dynamic perfect hashing;rényi entropy;primary clustering;collision resistance;computer science;theoretical computer science;bloom filter;universal hashing;mathematics;k-independent hashing;rolling hash;programming language;2-choice hashing;algorithm;cryptographic hash function;statistics;swifft;hash tree;hash filter	Theory	11.184163398218343	24.651173730192	54887
790732765cbbab2ecb588bf8078c035a2cf0751e	loss bounds and time complexity for speed priors		This paper establishes for the first time the predictive performance of speed priors and their computational complexity. A speed prior is essentially a probability distribution that puts low probability on strings that are not efficiently computable. We propose a variant to the original speed prior (Schmidhuber, 2002), and show that our prior can predict sequences drawn from probability measures that are estimable in polynomial time. Our speed prior is computable in doubly-exponential time, but not in polynomial time. On a polynomial time computable sequence our speed prior is computable in exponential time. We show better upper complexity bounds for Schmidhuber’s speed prior under the same conditions, and that it predicts deterministic sequences that are computable in polynomial time; however, we also show that it is not computable in polynomial time, and the question of its predictive properties for stochastic sequences remains open.	computable function;computational complexity theory;polynomial;time complexity	Daniel Filan;Marcus Hutter;Jan Leike	2016			mathematical optimization;combinatorics;discrete mathematics;mathematics;computable function;computable number	ML	10.212499351491537	19.539918175842562	55013
4c154a4d4691289e115ae328ce1f55c604928a45	an optimal randomized logarithmic time connectivity algorithm for the erew pram (extended abstract)	time use;long chain;connected component	Improving a long chain of works we obtain a randomized EREW PRAM algorithm for finding the connected components of a graph G=(V,E) with <italic>n</italic> vertices and <italic>m</italic> edges in O(log <italic>n</italic>) time using an optimal number of O((<italic>m</italic>+<italic>n</italic>)/log <italic>n</italic>) processors. The result returned by the algorithm is always correct. The probability that the algorithm will not complete in O(log <italic>n</italic>) time is at most <italic>n</italic><supscrpt>-<italic>c</italic></supscrpt> for any desired <italic>c</italic> > 0. The best deterministic EREW PRAM connectivity algorithm, obtained by Chong and Lam, runs in O(log <italic>n</italic> log log <italic>n</italic>) time using <italic>m</italic> + <italic>n</italic> processors.	central processing unit;connected component (graph theory);lam/mpi;parallel random-access machine;randomized algorithm;time complexity	Shay Halperin;Uri Zwick	1994		10.1145/181014.181017	combinatorics;discrete mathematics;connected component;mathematics;algorithm	Theory	17.684125672644253	28.461165003029507	55041
7ff2437bc57923f6fb9f1b543a93a5407ec84c20	rounds versus time for the two person pebble game	person pebble game	The two person pebble game is used by Dymond and Tompa to obtain the simulation result (DTIME(t(n)) subseteq ATIME(t(n)))and later by Paul, Pippenger, Szemeredi and Trotter to simulate deterministic almost time computations by linear time nondeterministic computations. We show the following results for the two person pebble game.	pebble game	Bala Kalyanasundaram;Georg Schnitger	1990	Inf. Comput.	10.1016/0890-5401(90)90002-Y	combinatorics;discrete mathematics;mathematics;algorithm	Logic	13.349907197712964	19.101884746025394	55354
0068eed40ac7469083e408e1ec39faa3bf70de47	faster and simpler algorithms for multicommodity flow and other fractional packing problems	multicommodity flow;graph theory;fractional packing problems;shortest path;concurrent computing;design engineering;combinatorial algorithm;concurrent computing throughput computer science design engineering polynomials;polynomials;computational complexity;computational complexity graph theory;combinatorial algorithms;min cost flow computations combinatorial algorithms multicommodity flows fractional packing problems shortest path computations;computer science;shortest path computations;multicommodity flows;min cost flow computations;throughput	This paper considers the problem of designing fast, approxi mate, combinatorial algorithms for multicommodity flows and other fractional packing problems. We pr sent new faster and much simpler algorithms for these problems.	algorithm;fractional fourier transform;set packing	Naveen Garg;Jochen Könemann	1998		10.1109/SFCS.1998.743463	mathematical optimization;throughput;combinatorics;concurrent computing;multi-commodity flow problem;computer science;graph theory;theoretical computer science;mathematics;shortest path problem;computational complexity theory;algorithm;polynomial	Theory	22.195185039856952	18.508675001941413	55708
d3bb7473a7345af1f2d40ea94168ba27f53d5682	an efficient algorithm for solving the homogeneous set sandwich problem	graph theory;strongly connected component;maximum degree;teoria grafo;algorithme efficace;homogeneous sets;time complexity;efficient algorithm;efficiency;arbre maximal;structure sandwich;graphe sandwich;theorie graphe;sandwich problems;sandwich graph;algorithme;connected graph;algorithm;strongly connected components;sandwich structure;eficacia;arbol maximo;spanning subgraphs;efficacite;algorithms;spanning tree;estructura sandwich;efficient algorith;graphe connexe;algoritmo;grafo conexo	Abstract   A set   H   of vertices of graph   G(V,E)   is  a homogeneous set  if each vertex in   V\H   is either adjacent to all vertices of   H   or to none of the vertices in   H  , where   V   and   E   are the vertex set and edge set, respectively, of graph   G  . A graph   G     s   (V,E     s   )   is called a sandwich graph for the pair of graphs   G(V,E)   and   G     t   (V,E     t   )   if   E     t   ⫅E     s   ⫅E  . The homogeneous set sandwich problem is to determine whether there exists a sandwich graph for the pair of graphs   G   and   G     t    such that there is a homogeneous set in   G     s   . In this paper, we shall propose an   O  (Δn     2   )   time algorithm for solving the homogeneous set sandwich problem, where   Δ   is the maximum degree of   G  . Furthermore, a surprising result comes from the  bias graph  which is an auxiliary graph for reducing the homogeneous set sandwich problem to the problem of finding strongly connected components. Using the bias graph, we can easily find all homogeneous sets for a sandwich graph problem with the same time complexity, i.e.,   O  (Δn     2   )  .	algorithm	Shyue-Ming Tang;Fu-Long Yeh;Yue-Li Wang	2001	Inf. Process. Lett.	10.1016/S0020-0190(00)00145-9	graph power;combinatorics;discrete mathematics;independent set;topology;graph bandwidth;feedback vertex set;degree;graph toughness;regular graph;distance-regular graph;graph theory;mixed graph;cycle graph;cubic graph;vertex;symmetric graph;graph factorization;mathematics;bound graph;complement graph;strongly connected component;line graph;neighbourhood;algorithm;string graph;strength of a graph	DB	22.37205313365375	27.50012043851274	55781
2811c4615dfa0d1723e01d2645a673ac9511e6c4	fast deterministic distributed algorithms for sparse spanners	algorithme rapide;modelizacion;distributed algorithms;randomized algorithms;deterministic;grado grafo;time complexity;graph spanners;arbre maximal;algorithme deterministe;modelisation;deterministic algorithms;complexite temps;arbol maximo;linial s free model;fast algorithm;algorithme reparti;algoritmo repartido;degre graphe;spanning tree;complejidad tiempo;distributed algorithm;modeling;algoritmo rapido;minimum degree;graph degree	This paper concerns the efficient construction of sparse and low stretch spanners for unweighted arbitrary graphs with n nodes. All previous deterministic distributed algorithms, for constant stretch spanner of o(n2) edges, have a running time Ω(ne) for some constant e> 0 depending on the stretch. Our deterministic distributed algorithms construct constant stretch spanners of o(n2) edges in o(ne) time for any constant e> 0#R##N##R##N#More precisely, in the Linial's free model, we construct in $n^{O(1/\sqrt{\log n})}$ time, for every graph, a 5-spanner of O(n3/2) edges. The result is extended to O( k2.322)-spanners with O(n1+1/k) edges for every parameter k ≥1. If the minimum degree of the graph is $\Omega(\sqrt{n})$, then, in the same time complexity, a 9-spanner with O(n) edges can be constructed	distributed algorithm	Bilel Derbel;Cyril Gavoille	2006		10.1007/11780823_9	time complexity;distributed algorithm;combinatorics;discrete mathematics;systems modeling;spanning tree;computer science;mathematics;distributed computing;randomized algorithm;algorithm;determinism	Theory	19.495101288210737	28.46175174129197	55859
104bd483c65011400a6f256c88f119cbf9cff0f5	algorithms for path-constrained sequence alignment	dynamic programming;regular expressions;mirna;sequence alignment	"""We define a novel variation on the constrained sequence alignment problem in which the constraint is given in the form of a regular expression. Given two sequences, an alphabet @C describing pairwise sequence alignment operations, and a regular expression R over @C, the problem is to compute the highest scoring sequence alignment A of the given sequences, such that A@?@C^@?L(R)@C^@?. Two algorithms are given for solving this problem. The first basic algorithm is general and solves the problem in O(nmrlog^2r) time and O(min{n,m}r) space, where m and n are the lengths of the two sequences and r is the size of the NFA for R. The second algorithm is restricted to rigid patterns and exploits this restriction to reduce the NFA size factor r in the time complexity to a smaller factor corresponding to the length of the rigid pattern. A rigid pattern P is a regular expression of the form P=P""""1@?...@?P""""k, where P""""i does not contain the Kleene-closure star or union. |P| is compacted by supporting alignment patterns P that do not contain the Kleene-closure star, and exploits this constraint to reduce the NFA size factor r in the time complexity to a smaller factor |P|. |P| is compacted by supporting alignment patterns extended by meta-characters including general insertion, deletion and match operations, as well as some cases of substitutions. meta-characters used in P. {m,i}^@? or P@?(@C@?{m,d})^@?, the problem can be solved in time O(nm), while for a pattern P@?(@C@?{m,i,d})^@?, the problem can be solved in time O(nmlog|P|). For a pattern P@?(@C@?{m,s,i,d})^@?, the problem can be solved in time O(nmlog|P|) in some cases: one case is for scoring functions Score for which there exists Score^':@S->R such that Score(@n,@s)=Score^'(@n)+Score^'(@s) for every @n @s, and the other is when occ""""s(P)=O(log(max{n,m})). For a rigid pattern P=P""""1@?...@?P""""k, these time bounds range from O(knm) to O(knmlog(max{|P""""i|})), depending on the meta-characters used in P. An additional result obtained along the way is an extension of the algorithm of Fischer and Paterson for String Matching with Wildcards. Our extension allows the input strings to include ''negation symbols'' (that match all letters but a specific one) while retaining the original time complexity. We implemented both algorithms and applied them to data-mine new miRNA seeding patterns in C. elegans Clip-seq experimental data."""	algorithm;binary logarithm;data mining;kleene algebra;michael j. fischer;programming tool;project milo;regular expression;semiconductor industry;sequence alignment;string searching algorithm;time complexity;wildcard character	Tamar Pinhas;Nimrod Milo;Gregory Kucherov;Michal Ziv-Ukelson	2014	J. Discrete Algorithms	10.1016/j.jda.2013.09.003	mathematical optimization;combinatorics;computer science;theoretical computer science;dynamic programming;sequence alignment;mathematics;programming language;regular expression;algorithm;microrna	Theory	13.922445862228104	26.44523938223321	55902
b94d69b03a7afa113300b0913f1cfbb40659f3b0	the maximum concurrent flow problem	multicommodity flow;complexite;flow;algorithmique;algorithm analysis;complejidad;oleada;packet switched;complexity;reseau;simultaneite;red;probleme combinatoire;problema combinatorio;cluster analysis;simultaneidad;algorithmics;algoritmica;recherche operationnelle;simultaneity;fully polynomial time approximation scheme;analyse algorithme;flot;primal dual algorithm;combinatory problem;operational research;network flow;capacity constraint;investigacion operacional;analisis algoritmo;polynomial time approximation scheme;network	The maximum concurrent flow problem (MCFP) is a multicommodity flow problem in which every pair of entities can send and receive flow concurrently. The ratio of the flow supplied between a pair of entities to the predefined demand for that pair is called throughput and must be the same for all pairs of entities for a concurrent flow. The MCFP objective is to maximize the throughput, subject to the capacity constraints. We develop a fully polynomial-time approximation scheme for the MCFP for the case of arbitrary demands and uniform capacity. Computational results are presented. It is shown that the problem of associating costs (distances) to the edges so as to maximize the minimum-cost of routing the concurrent flow is the dual of the MCFP. A path-cut type duality theorem to expose the combinatorial structure of the MCFP is also derived. Our duality theorems are proved constructively for arbitrary demands and uniform capacity using the algorithm. Applications include packet-switched networks [1, 4, 8], and cluster analysis [16].	algorithm;cluster analysis;computation;entity;flow network;multi-commodity flow problem;network packet;packet switching;polynomial;polynomial-time approximation scheme;routing;throughput;time complexity	Farhad Shahrokhi;David W. Matula	1990	J. ACM	10.1145/77600.77620	mathematical optimization;combinatorics;polynomial-time approximation scheme;computer science;mathematics;algorithmics;algorithm	Theory	20.7459581484201	28.886811758216542	55919
84a8e901702a1cace78cb57a9a6aacd1927d6a31	the complexity of comparing multiply-labelled trees by extending phylogenetic-tree metrics		A multilabeled tree (or MUL-tree) is a rooted tree in which every leaf is labelled by an element from some set, but in which more than one leaf may be labelled by the same element of that set. In phylogenetics, such trees are used in biogeographical studies, to study the evolution of gene families, and also within approaches to construct phylogenetic networks. A multilabelled tree in which no leaf-labels are repeated is called a phylogenetic tree, and one in which every label is the same is also known as a tree-shape. In this paper, we consider the complexity of computing metrics on MUL-trees that are obtained by extending metrics on phylogenetic trees. In particular, by restricting our attention to tree shapes, we show that computing the metric extension on MUL-trees is NP-complete for two well-known metrics on phylogenetic trees, namely, the path-difference and Robinson Foulds distances. We also show that the extension of the Robinson Foulds distance is fixed parameter tractable with respect to the distance parameter. The path distance complexity result allows us to also answer an open problem concerning the complexity of solving the quadratic assignment problem for two matrices that are a Robinson similarity and a Robinson dissimilarity, which we show to be NP-complete. We conclude by considering the maximum agreement subtree (MAST) distance on phylogenetic trees to MUL-trees. Although its extension to MUL-trees can be computed in polynomial time, we show that computing its natural generalization to more than two MUL-trees is NP-complete, although fixed-parameter tractable in the maximum degree when the number of given trees is bounded.	cobham's thesis;gene family;karp's 21 np-complete problems;np-completeness;parameterized complexity;phylogenetic network;phylogenetic tree;phylogenetics;polynomial;quadratic assignment problem;time complexity;tree (data structure)	Manuel Lafond;Nadia El-Mabrouk;Katharina T. Huber;Vincent Moulton	2018	CoRR		discrete mathematics;combinatorics;degree (graph theory);time complexity;phylogenetics;phylogenetic tree;tree (data structure);quadratic assignment problem;open problem;bounded function;mathematics	Theory	18.395559789693067	22.344765293435064	55942
3baa0b2dc11d623274daf067a84e8b51c44392e6	an optimal algorithm for large frequency moments using o(n^(1-2/k)) bits	streaming algorithms randomized algorithms frequency moments heavy hitters;004	In this paper, we provide the first optimal algorithm for the remaining open question from the seminal paper of Alon, Matias, and Szegedy: approximating large frequency moments. Given a stream D = {p1, p2, . . . , pm} of numbers from {1, . . . , n}, a frequency of i is defined as fi = |{j : pj = i}|. The k-th frequency moment of D is defined as Fk = ∑n i=1 f k i . We give an upper bound on the space required to find a k-th frequency moment of O(n1−2/k) bits that matches, up to a constant factor, the lower bound of [48] for constant and constant k. Our algorithm makes a single pass over the stream and works for any constant1 k > 3. It is based upon two major technical accomplishments: first, we provide an optimal algorithm for finding the heavy elements in a stream; and second, we provide a technique using Martingale Sketches which gives an optimal reduction of the large frequency moment problem to the all heavy elements problem. Additionally, this reduction works for any function g of the form ∑n i=1 g(fi) that requires sub-linear polynomial space, and it works in the more general turnstile model. As a result, we also provide a polylogarithmic improvement for frequency moments, frequency based functions, spatial data streams, and measuring independence of data sets. 1998 ACM Subject Classification F.2 Analysis of Algorithms And Problem Complexity	analysis of algorithms;moment problem;pspace;polylogarithmic function;polynomial;streaming algorithm;turnstile	Vladimir Braverman;Jonathan Katzman;Charles Seidell;Gregory Vorsanger	2014		10.4230/LIPIcs.APPROX-RANDOM.2014.531	mathematical optimization;combinatorics;mathematics;algorithm	Theory	13.114674854575243	24.217140036044658	55964
06955bf43bb5b1ae745ae846ed672826bdbb3e81	computing the atom graph of a graph and the union join graph of a hypergraph		The atom graph of a graph is the graph whose vertices are the atoms obtained by clique minimal separator decomposition of this graph, and whose edges are the edges of all possible atom trees of this graph. We provide two efficient algorithms for computing this atom graph, with a complexity in O(min(n log n, nm,n(n+m)) time, which is no more than the complexity of computing the atoms in the general case. We extend our results to α-acyclic hypergraphs. We introduce the notion of union join graph, which is the union of all possible join trees; we apply our algorithms for atom graphs to efficiently compute union join graphs.	algorithm;atom;constraint satisfaction dual problem;directed acyclic graph;graph (discrete mathematics)	Anne Berry;Geneviève Simonet	2016	CoRR		graph power;edge-transitive graph;factor-critical graph;combinatorics;clique graph;discrete mathematics;directed graph;multiple edges;null graph;graph labeling;distance-regular graph;theoretical computer science;simplex graph;hypercube graph;cycle graph;cubic graph;mathematics;voltage graph;butterfly graph;quartic graph;complement graph;line graph;strength of a graph;coxeter graph	DB	23.673323349279936	27.132049761230657	56011
7a229353f253824fd436ce668a801e2d19eaf88e	parameterized algorithms and computational lower bounds: a structural approach	computational lower bounds;thesis;computational complexity;algorithms;book;parameterized computation	"""Many problems of practical significance are known to be NP-hard, and hence, are unlikely to be solved by polynomial-time algorithms. There are several ways to cope with the NP-hardness of a certain problem. The most popular approaches include heuristic algorithms, approximation algorithms, and randomized algorithms. Recently, parameterized computation and complexity have been receiving a lot of attention. By taking advantage of small or moderate parameter values, parameterized algorithms provide new venues for practically solving problems that are theoretically intractable. #R##N#In this dissertation, we design efficient parameterized algorithms for several well-known NP-hard problems and prove strong lower bounds for some others. In doing so, we place emphasis on the development of new techniques that take advantage of the structural properties of the problems. #R##N#We present a simple parameterized algorithm for Vertex Cover that uses polynomial space and runs in time O(1.2738k + kn). It improves both the previous O(1.286 k + kn)-time polynomial-space algorithm by Chen, Kanj, and Jia, and the very recent O(1.2745kk 4 + kn)-time exponential-space algorithm, by Chandran and Grandoni. This algorithm stands out for both its performance and its simplicity. Essential to the design of this algorithm are several new techniques that use structural information of the underlying graph to bound the search space. #R##N#For Vertex Cover on graphs with degree bounded by three, we present a still iv better algorithm that runs in time O(1.194k + kn), based on an """"almost-global"""" analysis of the search tree. #R##N#We also show that an important structural property of the underlying graphs---the graph genus---largely dictates the computational complexity of some important graph problems including Vertex Cover, Independent Set and Dominating Set. #R##N#We present a set of new techniques that allows us to prove almost tight computational lower bounds for some NP-hard problems, such as Clique, Dominating Set, Hitting Set, Set Cover, and Independent Set. The techniques are further extended to derive computational lower bounds on polynomial time approximation schemes for certain NP-hard problems. Our results illustrate a new approach to proving strong computational lower bounds for some NP-hard problems under reasonable conditions."""	approximation algorithm	Ge Xia	2005			parameterized complexity;mathematical optimization;combinatorics;discrete mathematics;probabilistic analysis of algorithms;vertex cover;computational resource;mathematics;asymptotic computational complexity;approximation algorithm	Theory	18.654999517320377	20.316388801487488	56014
6349f4e8078e974d1c38f7817f406fc6f43e5d06	on embedding a graph in the grid with the minimum number of bends	concepcion asistida;computer aided design;microwave;interconnection;integrated circuit;planar embedding;routing;simulation;hyperfrequence;circuit vlsi;simulacion;circuito integrado;94c15;algorithme;grid;algorithm;algorritmo;vlsi circuit;telecomunicacion optica;telecommunication optique;90b10;68r10;interconnexion;hiperfrecuencia;conception assistee;optical telecommunication;encaminamiento;68u05;vlsi layout;circuito vlsi;68q35;network flow;bends;circuit integre;interconeccion;acheminement	Given a planar graph G together with a planar representation P, a region preserving grid embedding of G is a planar embedding of G in the rectilinear grid that has planar representation isomorphic to P. In this paper, an algorithm is presented that computes a region preserving grid embedding with the minimum number of bends in edges. This algorithm makes use of network flow techniques, and runs in time $O(n^2 \log n)$, where n is the number of vertices of the graph. Constrained versions of the problem are also considered, and most results are extended to k-gonal graphs, i.e., graphs whose edges are sequences of segments with slope multiple of ${{180} / k}$ degrees. Applications of the above results can be found in several areas: VLSI circuit layout, architectural design, communication by light or microwave, transportation problems, and automatic layout of graphlike diagrams.	bend minimization	Roberto Tamassia	1987	SIAM J. Comput.	10.1137/0216030	routing;combinatorics;polyhedral graph;flow network;graph embedding;multiple edges;microwave;computer science;theoretical computer science;integrated circuit;interconnection;computer aided design;linkless embedding;planar straight-line graph;mathematics;grid;book embedding;algorithm;planar graph	Theory	22.783290683403106	29.67950978875287	56101
2f5742791d65d86d237bd03e4e318a0f22fc350c	two lower bounds for branching programs	boolean function;branching program;lower bound	"""The first result concerns branching programs having width (log n) °{*). We give an f l (n log n~ log log n) lower bound for the size of such branching programs computing almost any symmetric Boolean fnnction and in part icular the following explicit fnnction: """"the sum of the input variables is a quadratic residue mod p"""" where p is any given prime between n 1/4 and n 1/3. This is a strengthening of previous nonlinear lower bounds obtained by Chandra, Furst, Lipton and by Pudlgk. We mention that by i terat ing our method the result can be further strengthened to lfl(nlog n). The second result is a C """" lower bound for read-onceonly branching programs computing an explicit Boolean function. For n = (~), the function computes the parity of the number of triangles in a graph on v vertices. This improves previous exp(cx/n ) lower bounds for other graph functions by Wegener and Z£k. The result implies a linear lower bound for the space complexity of this Boolean function on """"eraser machines"""", i.e. machines that erase each input bit immediately af-"""	binary decision diagram;dspace;ingo wegener;nonlinear system;parity bit;quadratic residue	Miklós Ajtai;László Babai;Péter Hajnal;János Komlós;Pavel Pudlák;Vojtech Rödl;Endre Szemerédi;György Turán	1986		10.1145/12130.12134	computer science;mathematics;boolean function;upper and lower bounds;algorithm	Theory	10.307998839809962	23.020827641648964	56295
7f4724f1c899b1f1ccb4efc39b5f7a344e710dc2	finding optimal flows efficiently	quantum measurement;upper bound;polynomial time algorithm;quantum computer;quantum physics;necessary and sufficient condition;polynomial time;graph algorithm;quantum entanglement	Among the models of quantum computation, the One-way Quantum Computer [10, 11] is one of the most promising proposals of physical realization [12], and opens new perspectives for parallelization by taking advantage of quantum entanglement [2]. Since a one-way quantum computation is based on quantum measurement, which is a fundamentally nondeterministic evolution, a sufficient condition of global determinism has been introduced in [4] as the existence of a causal flow in a graph that underlies the computation. A O(n)-algorithm has been introduced [6] for finding such a causal flow when the numbers of output and input vertices in the graph are equal, otherwise no polynomial time algorithm was known for deciding whether a graph has a causal flow or not. Our main contribution is to introduce a O(n)-algorithm for finding a causal flow, if any, whatever the numbers of input and output vertices are. This answers the open question stated by Danos and Kashefi [4] and by de Beaudrap [6]. Moreover, we prove that our algorithm produces an optimal flow (flow of minimal depth.) Whereas the existence of a causal flow is a sufficient condition for determinism, it is not a necessary condition. A weaker version of the causal flow, called gflow (generalized flow) has been introduced in [3] and has been proved to be a necessary and sufficient condition for a family of deterministic computations. Moreover the depth of the quantum computation is upper bounded by the depth of the gflow. However, the existence of a polynomial time algorithm that finds a gflow has been stated as an open question in [3]. In this paper we answer this positively with a polynomial time algorithm that outputs an optimal gflow of a given graph and thus finds an optimal correction strategy to the nondeterministic evolution due to measurements.	algorithm;backtracking;causal filter;circuit complexity;computable function;computation;computational problem;dependence analysis;graph state;input/output;level of detail;measurement in quantum mechanics;one-way function;one-way quantum computer;p (complexity);povm;parallel computing;philippe kruchten;polynomial;quantum computing;quantum entanglement;recursion;vertex (geometry)	Mehdi Mhalla;Simon Perdrix	2008		10.1007/978-3-540-70575-8_70	time complexity;combinatorics;discrete mathematics;mathematics;upper and lower bounds;quantum computer;quantum entanglement;quantum algorithm;physics;quantum mechanics	Theory	19.064244203474672	23.529651462153584	56360
0abc3d6b2d9918a7e06dcbde7cf53508951fcf03	a generalization of thue freeness for partial words	lettre alphabet;freeness;chevauchement;05xx;mot thue morse;palabra infinita;alphabet fini;thue morse word;combinatorics on words;mot infini;thue morse;hoyo;68r15;overlap;imbricacion;infinite word;combinatorial problem;probleme combinatoire;problema combinatorio;trou;informatique theorique;letra alfabeto;word combinatorics;letter;68r05;partial words;overlap freeness;combinatoire mot;hole;computer theory;square freeness;informatica teorica	This paper approaches the combinatorial problem of Thue freeness for partial words. Partial words are sequences over a finite alphabet that may contain a number of ―holes‖. First, we give an infinite word over a three-letter alphabet which avoids squares of length greater than two even after we replace an infinite number of positions with holes. Then, we give an infinite word over an eight-letter alphabet that avoids longer squares even after an arbitrary selection of its positions are replaced with holes, and show that the alphabet size is optimal. We find similar results for overlap-free partial words.	algorithm;critical exponent of a word;omega language;partial word;randomness;square-free polynomial	Francine Blanchet-Sadri;Robert Mercas;Geoffrey Scott	2009	Theor. Comput. Sci.	10.1016/j.tcs.2008.11.006	partial word;combinatorics;discrete mathematics;letter;mathematics;algorithm;combinatorics on words;algebra	Theory	10.838828095373584	25.808163104074723	56497
da2fd025e4d6d16bb7ac073400aa17f596c20919	hardness of string similarity search and other indexing problems	search problem;lenguaje programacion;approximation lineaire;approximate algorithm;programming language;distance measure;approximation algorithm;factor calidad;edit distance;temps lineaire;exact solution;interrogation base donnee;linear approximation;chaine caractere;interrogacion base datos;automaton;solucion exacta;tiempo lineal;problema investigacion;similitude;permutation;automata;longest common subsequence;vecino mas cercano;hamming distance;indexing;approximate solution;automate;indexation;linear time;cadena caracter;nearest neighbor;permutacion;distance hamming;facteur qualite;aproximacion lineal;similarity;indizacion;algoritmo aproximacion;borne inferieure;langage programmation;plus proche voisin;nearest neighbour;similitud;solution exacte;algorithme approximation;probleme recherche;database query;distancia hamming;similarity search;lower bound;q factor;character string;cota inferior	Similarity search is a fundamental problem in computer science. Given a set of points from a universe and a distance measure , it is possible to pose similarity search queries on a point in the form of nearest neighbors (find the string that has the smallest edit distance to a query string) or in the form of furthest neighbors (find the string that has the longest common subsequence with a query string). Exact similarity search appears to be a very hard problem for most application domains; available solutions require either a preprocessing time/space exponential with or query time exponential with . For such problems approximate solutions have recently attracted considerable attention. Approximate nearest (furthest) neighbor search aims to find a point in whose distance to query point is within a small multiplicative factor of that between and its nearest (furthest) neighbor. In this paper, we study hardness of several important similarity search problems for strings as well as other combinatorial objects, for which exact solutions have proven to be very difficult to achieve. We show here that even the approximate versions of these problems are quite hard; more specifically they are as hard as exact similarity search in Hamming space. Thus available cell probe lower bounds for exact similarity search in Hamming space apply for approximate similarity search in string spaces (under Levenshtein edit distance and longest common subsequence) as well as other spaces. As a consequence of our reductions we also make observations about pairwise approximate distance computations. One such observation gives a simple linear time 2-approximation algorithm for permutation edit distance. School of Computing Science, Simon Fraser University, BC, Canada, email:cenk@cs.sfu.ca . Department of Computer Science, University of Maryland, College Park, USA email:utis@cs.umd.edu .	approximation algorithm;coefficient;computation;computer science;edit distance;hamming space;longest common subsequence problem;preprocessor;query string;similarity search;string metric;time complexity;web search query	Süleyman Cenk Sahinalp;Andrey Utis	2004		10.1007/978-3-540-27836-8_90	combinatorics;discrete mathematics;edit distance;approximate string matching;computer science;artificial intelligence;machine learning;mathematics;automaton;nearest neighbor search;string metric;jaro–winkler distance;approximation algorithm;algorithm	Theory	15.46669218425383	25.34052940434274	56511
23e81573da5afa3aab902db51f260e44097442b7	a note on amortized branching program complexity		In this paper, we show that while almost all functions require exponential size branching programs to compute, for all functions f there is a branching program computing a doubly exponential number of copies of f which has linear size per copy of f . This result disproves a conjecture about non-uniform catalytic computation, rules out a certain type of bottleneck argument for proving non-monotone space lower bounds, and can be thought of as a constructive analogue of Razborov’s result that submodular complexity measures have maximum value O(n). Acknowledgement: This material is based upon work supported by the National Science Foundation under agreement No. CCF-1412958 and by the Simons Foundation. The author would like to thank Avi Wigderson for helpful conversations. .	amortized analysis;binary decision diagram;computation;keneth alden simons;submodular set function;time complexity;monotone	Aaron Potechin	2017		10.4230/LIPIcs.CCC.2017.4	amortized analysis;proof of o(log*n) time complexity of union–find	Theory	10.874969434026502	22.022435895952697	56833
2e2630801f81646a6dcdf26cd414d3a574191f2e	a top down algorithm for constrained tree inclusion	labeled tree;xml query;tree inclusion;constrained tree inclusion	An ordered labeled tree is a tree which nodes are labeled and in which the left-to-right order among siblings is significant. Given two ordered labeled trees P and T , the constrained tree inclusion problem is to determine whether it is possible to obtain P from T by deleting degree-one or degreetwo nodes. G. Valiente proposed a bottom up algorithm which solves the problem in O(|P ||T |) time and space. In this paper, a top down matching algorithm is presented, which solves the problem in O(|P ||T |) time and O(|P ||leaves(T )|) space.	algorithm;computation;dspace;ordered pair;top-down and bottom-up design;tree (data structure)	Yu-Hsiang Hsiao;Keh-Ning Chang	2015	J. Discrete Algorithms	10.1016/j.jda.2015.05.012	mathematical optimization;combinatorics;discrete mathematics;mathematics	Theory	18.727179232574233	24.122751048449356	56901
decf63306a408036572e1c75cba6b3fc68868d16	succinct data structures for searchable partial sums with optimal worst-case performance	partial sums;succinct data structure;time complexity;indexation;information theoretic;lower bound	The notion of succinct indexes can be dated back from the debut of Jacobson’s thesis (1988) [14], and has triggered many results in the last decade. In traditional indexing, some given data are preprocessed so as to support online queries (and updates) on the data as efficiently as possible. When succinctness is involved, we are restricted to index the data using only an information–theoreticallyminimum number of bits. This paper concerns the succinct indexing schemes for a well-studied problem called Searchable Partial Sums (SPS). In SPS, an array A of n non-negative k-bit integers is preprocessed so as to support online sum and search queries, and possibly update operation of individual entry. A succinct indexing scheme would allow only kn + o(kn) bits to represent the array A. The only known result is that when k = 1 (in this case, it is known as the Dynamic Bit Array Problem), we can support both queries in O(logb n) time and update in O(b) amortized time for any b with lg n/ lg lg n ≤ b ≤ n. This paper shows that even for k = O(lg lg n), we can index A succinctly such that both query and update operations can be supported using the same time complexities. Moreover, the time for update becomes the worst-case time. Furthermore, the tradeoff between the query times and the update time is optimal as implied by Pǎtraşcu and Demaine’s lower bound result (2006) [24]. In general when k = O(lgU), we show a lower bound of Ω( √ lg n/ lg lg n) time for the search query irrespective of the update time. This gives a tighter lower bound as compared to that of Pǎtraşcu and Demaine’s, which is a consequence of the requirement of succinctness. On the other hand, we give a succinct index that can support sum in O(logb n) time, search in O(τ logb n) time, and update in O(b) time, where τ = min  lg lg n lg lgU/ lg lg lgU, √ lg n/ lg lg n  . The query times are optimal when b = n . This paper also extends the Searchable Partial Sums with insert and delete operations, and provides a succinct data structure for some cases. © 2011 Elsevier B.V. All rights reserved.	amortized analysis;best, worst and average case;bit array;data pre-processing;ibm 1401 symbolic programming system;maxima and minima;raman scattering;succinct data structure;web search query	Wing-Kai Hon;Kunihiko Sadakane;Wing-Kin Sung	2011	Theor. Comput. Sci.	10.1016/j.tcs.2011.05.023	time complexity;succinct data structure;computer science;theoretical computer science;data mining;mathematics;upper and lower bounds;series;algorithm	Theory	12.201631888206874	26.522014166116122	56910
18bae6775bc8229b0826f837e8c71d472cdd8d6e	buy-at-bulk network design: approximating the single-sink edge installation problem	randomized algorithms;network design;approximation algorithms;diagrams;least median of squares regression;robust estimation;line fitting;network analysis;algorithms;design;line arrangements;mathematics computers information science management law miscellaneous	"""1 Introduction. We initiate the algorithmic study of an important but NP-hard problem that arises commonly in network design. The input consists of (1) An undirected graph with one sink node and multiple source nodes, a specified length for each edge, and a specified demand, dem,, for each source node v. (2) A small set of cable types, where each cable type is specified by its capacity and its cost per unit length. The cost per unit capacity per unit length of a high-capacity cable may be significantly less than that of a low-capacity cable, reflecting an economy of scale, i.e., the payoff for buymg at bulk may be very high. The goal is to design a minimum-cost network that can (simultaneously) route all the demands at the sources to the sink, by installing 0, 1,2,3,. .. copies of each cable type on each edge of the graph. An additional restriction is that the demand of each source must follow a single path from the source to the sink. We call this problem the single-sink edge-installation problem. We present efficient approximation algorithms for key special cases of this problem that arise in practice. For the general problem, we introduce a new """" moat type """" lower bound on the optimal value and we prove a useful structural property of near-optimal solutions: For every instance of our problem, there is a near-optimal solution whose graph is acyclic (with cost no more than twice the optimal cost). For points in the Euclidean plane, we give an approximation algorithm with performance guarantee O(logD), where D is the total demand. When the metric is arbitrary, we consider the case where the network to be designed is restricted to a two-level tree, i.e., a tree such that every source-sink path has at most two edges. For this problem, we present an algorithm with performance guarantee O(logn), where n is the number of nodes in the input graph. 1.1 The problem. An oil company wishes to construct a network of pipelines to carry oil from several remote wells to a major refinery. For each edge of the network, the company can install either zero or more copies of a cheap but thin pipe (say, the diameter is 10 inches and the cost is $1,000 per mile) or zero or more copies of a more expensive but thicker pipe (say, the diameter is 100 inches and the …"""	approximation algorithm;directed acyclic graph;graph (discrete mathematics);np-hardness;network planning and design;optimization problem;pipeline (computing);tree (data structure)	F. Sibel Salman;Joseph Cheriyan;R. Ravi;S. Subramanian	1997			design;mathematical optimization;combinatorics;simulation;network analysis;computer science;theoretical computer science;mathematics;geometry;approximation algorithm;algorithm;statistics	Theory	23.510671717766357	18.328543278365952	57025
2e597ebc1fe2d90a69d936f16fd595a8033d0f6a	a short proof that nmf is np-hard		We give a short combinatorial proof that the nonnegative matrix factorization is an NP-hard problem. Moreover, we prove that NMF remains NP-hard when restricted to 01-matrices, answering a recent question of Moitra. The (exact) nonnegative matrix factorization is the following problem. Given an integer k and a matrix A with nonnegative entries, do there exist k nonnegative rank-one matrices that sum to A? The smallest k for which this is possible is called the nonnegative rank of A and denoted by rank + (A). We give a short combinatorial proof of a seminal result of Vavasis [6] stating that NMF is NP-hard. Moreover, we prove that NMF remains hard when restricted to Boolean matrices, answering a recent question of Moitra [4].	existential quantification;np-hardness;non-negative matrix factorization	Yaroslav Shitov	2016	CoRR		calculus;mathematics;algorithm	Theory	15.616048326060682	20.32711824479026	57218
65ededc57043f3487c3f91cb551c51ea2a43f2c8	polynomial-time recognition of 2-monotonic positive boolean functions given by an oracle	fonction booleenne;polynomial time identification;temps polynomial;boolean functions;fonction monotone;monotone function;reconocimiento;boolean function;2 monotonic boolean function;funcion monotona;68t05;algorithme;polynomial time algorithm;recognition;positive boolean function;identification;polynomial time;monotonic function;algorithms;identificacion;68q25;90c09;reconnaissance;oracle;monotone boolean function;tiempo polinomial	We consider the problem of identifying an unknown Boolean function f by asking an oracle the functional values f(a) for a selected set of test vectors a ∈ {0, 1}n. Furthermore, we assume that f is a positive (or monotone) function of n variables. It is not known yet whether the whole task of generating test vectors and checking if the identification is completed can be carried out in polynomial time in n and m or not, where m = |min T (f)| + |max F (f)| and min T (f) (respectively, max F (f)) denotes the set of minimal true (respectively, maximal false) vectors of f . To partially answer this question, we propose here two polynomial time algorithms that, given an unknown positive function f of n variables, decide whether f is 2-monotonic or not, and if f is 2-monotonic, output both sets min T (f) and max F (f). The first algorithm uses O(nm2 + n2m) time and O(nm) queries, while the second one uses O(n3m) time and O(n3m) queries.	algorithm;maxima and minima;maximal set;polynomial;time complexity;monotone	Endre Boros;Peter L. Hammer;Toshihide Ibaraki;Kazuhiko Kawakami	1997	SIAM J. Comput.	10.1137/S0097539793269089	combinatorics;discrete mathematics;monotonic function;computer science;mathematics;boolean function;algorithm	Theory	16.233812619132607	23.665599079951594	57220
1caf0a853dac30d7c5aa2a6ee4329be31e63e6e1	the complexity of online manipulation of sequential elections	elections;sequential voting;computational complexity;manipulation;online algorithms;computational social choice;preferences	Most work on manipulation assumes that all preferences are k nown to the manipulators. However, in many settings elections are open and sequential, an d m ipulators may know the already cast votes but may not know the future votes. We introduce a fr amework, in which manipulators can see the past votes but not the future ones, to model online coalitional manipulation of sequential elections, and we show that in this setting manipulatio n can be extremely complex even for election systems with simple winner problems. Yet we also sh ow that for some of the most important election systems such manipulation is simple in cer tain settings. This suggests that when using sequential voting, one should pay great attention to t he details of the setting in choosing one’s voting rule. Among the highlights of our classifications are: We show that , depending on the size of the manipulative coalition, the online manipulation problem c an be complete for each level of the polynomial hierarchy or even for PSPACE. We obtain the most d ramatic contrast to date between the nonunique-winner and unique-winner models: Online wei ght d manipulation for plurality is in P in the nonunique-winner model, yet is coNP-hard (constr uctive case) and NP-hard (destructive case) in the unique-winner model. And we obtain what to t he best of our knowledge are the first PNP[1]-completeness and P NP-completeness results in the field of computational social ∗Supported in part by grant NSF-CCF-1101452 and a Friedrich W il elm Bessel Research Award. Work done in part while visiting Heinrich-Heine-Universität Düsseldorf . †Supported in part by grants NSF-CCF{0915792,1101479 } and ARC-DP110101792, and a Friedrich Wilhelm Bessel Research Award. Work done in part while visiting Heinrich-H eine-Universität Düsseldorf. ‡Supported in part by DFG grant RO-1202/15-1, SFF grant “Coop erative Normsetting” of HHU Düsseldorf, ARCDP110101792, and a DAAD grant for a PPP project in the PROCOPE program.	algorithm;bessel filter;cer computer;co-np;eine and zwei;ibm notes;np-completeness;np-hardness;pspace;polynomial hierarchy	Edith Hemaspaandra;Lane A. Hemaspaandra;Jörg Rothe	2013	J. Comput. Syst. Sci.	10.1016/j.jcss.2013.10.001	election;online algorithm;computer science;artificial intelligence;machine learning;mathematics;computational complexity theory;algorithm	AI	16.871588711267997	18.966252249905875	57712
e63a015940646fd9bf428be61a2534b3055d1766	las vegas does n-queens	probabilistic algorithm;n queens problem;las vegas algorithm;random permutation	This paper presents two Las Vegas algorithms to generate single solutions to the n-queens problem. One algorithm generates and improves on random permutation vectors until it achieves one that is a successful solution, while the other algorithm randomly positions queens within each row in positions not under attack from above.	las vegas algorithm;random permutation;randomness	Timothy J. Rolfe	2006	SIGCSE Bulletin	10.1145/1138403.1138429	random permutation;las vegas algorithm;computer science;eight queens puzzle;randomized algorithm;algorithm	Crypto	13.76519355502469	22.292934361190316	57780
589af7a85b8ab321be176ae9ef828ecd6fe486de	shortest paths in digraphs of small treewdith. part ii: optimal parallel algorithms	camino mas corto;parallel computing;dynamic programming;graph theory;distributed system;algoritmo paralelo;shortest path;pram;programacion dinamica;teoria grafo;systeme reparti;parallel algorithm;algoritmo busqueda;dynamic algorithm;time complexity;algorithme recherche;search algorithm;plus court chemin;theorie graphe;parallel computation;algorithme parallele;series parallel graph;complexite temps;calculo paralelo;sistema repartido;estructura datos;programmation dynamique;shortest path tree;time use;parallel computer;structure donnee;treewidth;model of computation;outerplanar graph;erew pram;complejidad tiempo;algoritmo optimo;algorithme optimal;optimal algorithm;calcul parallele;data structure	We consider the problem of preprocessing an n vertex digraph with real edge weights so that subsequent queries for the shortest path or distance between any two vertices can be e ciently answered We give parallel algorithms for the EREWPRAMmodel of computation that depend on the treewidth of the input graph When the treewidth is a constant our algorithms can answer distance queries in O n time using a single processor after a preprocessing of O log n time and O n work where n is the inverse of Ackermann s function The class of constant treewidth graphs contains outerplanar graphs and series parallel graphs among others To the best of our knowledge these are the rst parallel algorithms which achieve these bounds for any class of graphs except trees We also give a dynamic algorithm which after a change in an edge weight updates our data structures in O log n time using O n work for any constant Moreover we give an algorithm of independent interest computing a shortest path tree or nding a negative cycle in O log n time using O n work	ackermann function;computation;data structure;directed graph;dynamic problem (algorithms);outerplanar graph;parallel algorithm;preprocessor;shortest path problem;time complexity;treewidth	Shiva Chaudhuri;Christos D. Zaroliagis	1998	Theor. Comput. Sci.	10.1016/S0304-3975(98)00021-8	model of computation;1-planar graph;outerplanar graph;time complexity;combinatorics;discrete mathematics;data structure;dynamic problem;computer science;graph theory;dynamic programming;mathematics;parallel algorithm;shortest path problem;treewidth;partial k-tree;chordal graph;algorithm;shortest-path tree;search algorithm	Theory	18.2312708915051	28.12147859596192	57793
867a197d7b37d573be85105d56a029715e26522a	on recovering syntenic blocks from comparative maps	optimal solution;metodo polinomial;recuperacion;approximate algorithm;temps polynomial;approximation algorithms;probleme np complet;approximation algorithm;np completeness;heuristic method;bioinformatique;problema np duro;metodo heuristico;recovery;optimisation combinatoire;np hard problem;aproximacion polinomial;comparative mapping;probleme np difficile;polynomial method;genome;approximation polynomiale;algoritmo aproximacion;polynomial time;completitud;problema np completo;total length;recuperation;methode heuristique;genoma;bioinformatica;genome mapping;completeness;algorithme approximation;combinatorial optimization;methode polynomiale;completude;np complete problem;polynomial approximation;optimizacion combinatoria;bioinformatics;tiempo polinomial	A genomic map is represented by a sequence of gene markers, and a gene marker can appear in several different genomic maps, in either positive or negative form. A strip (syntenic block) is a sequence of distinct markers that appears as subsequences in two or more maps, either directly or in reversed and negated form. Given two genomic maps G and H , the problem Maximal Strip Recovery (MSR) is to find two subsequences G′ and H ′ of G and H , respectively, such that the total length of disjoint strips in G′ and H ′ is maximized. Previously only a heuristic was provided for this problem, which does not guarantee finding the optimal solution, and it was unknown whether the problem is NP-complete or polynomially solvable. In this paper, we develop a factor-4 polynomial-time approximation algorithm for the problem, and show that several close variants of the problem are intractable.	approximation algorithm;decision problem;gene prediction;heuristic;map;maximal set;np-completeness;optimization problem;polynomial;strips;synteny;time complexity	Zhixiang Chen;Bin Fu;Minghui Jiang;Binhai Zhu	2009	J. Comb. Optim.	10.1007/s10878-009-9233-x	mathematical optimization;combinatorics;np-complete;computer science;mathematics;approximation algorithm;algorithm	ML	16.53481096254322	23.488689053323807	57885
b6e40990ef7cbfaee854289219dcb1b1ef073203	experiments with a fast string searching algorithm	search algorithm	Abstract   Consider the problem of finding the first occurrence of a particular pattern in a (long) string of characters. Boyer and Moore (1977) found a fast algorithm for doing this. Here we consider how this algorithm behaves when executed on a multiprocessor. It is shown that a simple implementation performs very well. This claim is based on experiments performed on the Multi-Maren multiprocessor by the present authors (1982).	string searching algorithm	Peter Møller-Nielsen;Jørgen Staunstrup	1984	Inf. Process. Lett.	10.1016/0020-0190(84)90015-2	commentz-walter algorithm;computer science;theoretical computer science;machine learning;boyer–moore string search algorithm;mathematics;algorithm;string searching algorithm;search algorithm	DB	13.151775121088923	27.766308291156967	57887
05ce1b6b83f1162b9be57c038c7600541ac56166	optimal-time text indexing in bwt-runs bounded space		Indexing highly repetitive texts — such as genomic databases, software repositories and versioned text collections — has become an important problem since the turn of the millennium. A relevant compressibility measure for repetitive texts is r, the number of runs in their Burrows-Wheeler Transform (BWT). One of the earliest indexes for repetitive collections, the Run-Length FM-index, used O(r) space and was able to efficiently count the number of occurrences of a pattern of length m in the text (in loglogarithmic time per pattern symbol, with current techniques). However, it was unable to locate the positions of those occurrences efficiently within a space bounded in terms of r. Since then, a number of other indexes with space bounded by other measures of repetitiveness — the number of phrases in the Lempel-Ziv parse, the size of the smallest grammar generating the text, the size of the smallest automaton recognizing the text factors — have been proposed for efficiently locating, but not directly counting, the occurrences of a pattern. In this paper we close this long-standing problem, showing how to extend the Run-Length FM-index so that it can locate the occ occurrences efficiently within O(r) space (in loglogarithmic time each), and reaching optimal time O(m + occ) within O(r logn) space, on a RAM machine of w = Ω(logn) bits. Within O(r logn log∗ n) space, our index can count in optimal time O(m). Raising the space to O(r w ) for any constant > 0, we support count and locate in O(m log σ/w) and O(m log σ/w+occ) time, which is optimal in the packed setting and had not been obtained before in compressed space. We also describe a structure using O(r log(n/r)) space that replaces the text and efficiently extracts any text substring, with O(log(n/r)) additive time penalty over the optimum. Within O(r log(n/r) log∗ n) space, we similarly provide direct access to suffix array, inverse suffix array, and longest common prefix array cells, and extend this to full suffix tree functionality, typically in O(log(n/r)) time per operation. Finally, we uncover new relations between r, the size of the smallest grammar generating the text, the Lempel-Ziv parsing, and the optimal bidirectional parsing.	automaton;burrows–wheeler transform;database;fm broadcasting;fm-index;lempel–ziv–stac;parsing expression grammar;random access;random-access machine;random-access memory;run-length encoding;software repository;software versioning;substring;suffix array;suffix tree;utility functions on indivisible goods	Travis Gagie;Gonzalo Navarro;Nicola Prezza	2018		10.1137/1.9781611975031.96	computer science;combinatorics;discrete mathematics;binary logarithm;parsing;search engine indexing;substring;bounded function;orders of magnitude (numbers)	Theory	12.37105861039918	27.39734135825009	57955
b2f592ec464120af20f47e75f39e8209c02ef05e	mathematical engineering technical reports on the boolean connectivity problem for horn relations	directed graph;boolean satisfiability;transitive closure	Gopalan et al. studied in ICALP06 [17] connectivity properties of the solution-space of Boolean formulas, and investigated complexity issues on the connectivity problems in Schaefer’s framework. A set S of logical relations is Schaefer if all relations in S are either bijunctive, Horn, dual Horn, or affine. They conjectured that the connectivity problem for Schaefer is in P . We disprove their conjecture by showing that there exists a set S of Horn relations such that the connectivity problem for S is coNP-complete. We also show that the connectivity problem for bijunctive relations can be solved in O(min{n|φ|, T (n)}) time, where n denotes the number of variables, φ denotes the corresponding 2-CNF formula, and T (n) denotes the time needed to compute the transitive closure of a directed graph of n vertices. Furthermore, we investigate a tractable aspect of Horn and dual Horn relations with respect to characteristic sets.	co-np;cobham's thesis;directed graph;logical relations;transitive closure	Kazuhisa Makino;Suguru Tamaki;Masaki Yamamoto	2007		10.1007/978-3-540-72788-0_20	combinatorics;discrete mathematics;mathematics;algorithm	Theory	21.64538478301017	24.03727987540021	58094
6e6a7626c4611f9695c4310edbfbd4d873a997d3	designing modular linear systolic arrays using dependance graph regular partitions	dependence graph;systolic array	In this paper, we use a variant of the geometric method to derive efficient modular linear systolic algorithms for the transitive closure and shortest path problems. Furthermore, we show that partially-pipelined modular linear systolic algorithms with an output operation, for matrix multiplication, can be as efficient as fully-pipelined ones, and in some cases needs less cells.		Jean Frédéric Myoupo;Anne-Cécile Fabret	1992		10.1007/3-540-55895-0_421	combinatorics;discrete mathematics;theoretical computer science;mathematics	Vision	18.361002163343063	30.31647067692126	58422
6a83eb1fed9c41b5bcbaafeb5d95fa7e8ae88d4e	sorting by reversals is difficult	complexity;sorting by reversals;alternating cycle decomposition	We prove that the problem of sorting a permutation by the minimum number of reversals is NP-hard, thus answering a major question on the complexity of a problem which has widely been studied in the last years. The proof is based on the strong relationship between this problem and the problem of finding the maximum number of edge-disjoint alternating cycles in a suitably-defined bicolored graph. For this latter problem we derive a number of structural properties, that can be used for showing its NP-hardness.	algorithm;command & conquer:yuri's revenge;computer scientist;decision problem;empty string;max;modulo operation;np-hardness;sorting;standard business reporting;time complexity	Alberto Caprara	1997		10.1145/267521.267531	complexity	Theory	17.73685738065475	20.295520653595933	58427
87275726eeec178750163095e15da9d49db10d46	the asymmetric median tree - a new model for building consensus trees	approximate algorithm;optimization problem;polynomial time algorithm;standard model;evolutionary trees;computer calculations;taxonomy;algorithms;optimization;biology and medicine basic studies;maximum independent set	Inferring the consensus of a set of different evolutionary trees for a given species set is a well-studied problems, for which several different models have been proposed. In this paper, the authors propose a new optimization problem for consensus tree construction, which they call the asymmetric median tree, or AMT. Their main theoretical result is the equivalence between the asymmetric median tree problem on {kappa} trees and the maximum independent set (MIS) problem on {kappa}-colored graphs. Although the problem is NP-hard for three or more trees, the authors have polynomial time algorithms to construct the AMT for two trees and an approximation algorithm for three or more trees. They define a measure of phylogenetic resolution and show that their algorithms (both exact and approximate) produce consensus trees that on every input are at least as resolved as the standard models (strict consensus and majority tree) in use. Finally, they show that the AMT combines desirable features of many of the standard consensus tree models in use. 23 refs.		Cynthia A. Phillips;Tandy J. Warnow	1996	Discrete Applied Mathematics	10.1016/S0166-218X(96)00071-6	optimization problem;standard model;mathematical optimization;combinatorics;phylogenetic tree;independent set;tree rearrangement;mathematics;weight-balanced tree;algorithm;taxonomy	Theory	18.28151180383675	22.008197934947397	58503
b5d5ff30b6ee8307ed167bb45b479fe2cd17e48b	computing maximum hamiltonian paths in complete graphs with tree metric	complete graph;undirected tree;linear time algorithm;maximum weight hamiltonian path;famous book;weighted complete graph;computing maximum hamiltonian path;size n;nodes u;tree metric;hugo steinhaus;total size	We design a linear time algorithm computing the maximum weight Hamiltonian path in a weighted complete graph KT , where T is a given undirected tree. The vertices of KT are nodes of T and weight(i, j) is the distance between i, j in T . The input is the tree T and two nodes u, v ∈ T , the output is the maximum weight Hamiltonian path between these nodes. The size n of the input is the size of T (however the total size of the complete graph KT is quadratic with respect to n). Our algorithm runs in O(n) time. Correctness is based on combinatorics of alternating sequences. The problem has been inspired by a similar (but much simpler) problem in a famous book of Hugo Steinhaus on elementary mathematical problems.	algorithm;correctness (computer science);graph (discrete mathematics);hgnc;hamiltonian path;quadratic function;time complexity	Wojciech Rytter;Bartosz Szreder	2012		10.1007/978-3-642-30347-0_34	combinatorics;discrete mathematics;topology;mathematics;hamiltonian path problem	Theory	24.42670348223148	26.259703531662787	58574
62b6fe865cdee60219af59d94525c6ba9f22d0bc	extracting common motifs under the levenshtein measure: theory and experimentation	edit distance;hamming distance;measure theory	Using our techniques for extracting approximate non-tandem repeats[1] on well constructed maximal models, we derive an algorithm to find common motifs of length P that occur in N sequences with at most D differences under the Edit distance metric. We compare the effectiveness of our algorithm with the more involved algorithm of Sagot[17] for Edit distance on some real sequences. Her method has not been implemented before for Edit distance but only for Hamming distance[12],[20]. Our resulting method turns out to be simpler and more efficient theoretically and also in practice for moderately large P and D.	approximation algorithm;consensus sequence;edit distance;f1 score;hamming code;hamming distance;matrix representation;maximal set;relevance;squirrel;window function	Ezekiel F. Adebiyi;Michael Kaufmann	2002		10.1007/3-540-45784-4_11	damerau–levenshtein distance;hamming distance;edit distance;measure;computer science;wagner–fischer algorithm;machine learning;pattern recognition;mathematics;string-to-string correction problem;jaro–winkler distance;algorithm;statistics	Comp.	17.495449464275122	23.41092596444867	58594
1217dfe16ff2c7747c35ff1606a5e71f36f26565	on dynamic shortest paths problems	shortest paths;dynamic algorithms;spanners;graph	We obtain the following results related to dynamic versions of the shortest-paths problem: (i) Reductions that show that the incremental and decremental single-source shortest-paths problems, for weighted directed or undirected graphs, are, in a strong sense, at least as hard as the static all-pairs shortest-paths problem. We also obtain slightly weaker results for the corresponding unweighted problems. (ii) A randomized fully-dynamic algorithm for the all-pairs shortest-paths problem in directed unweighted graphs with an amortized update time of $\tilde {O}(m\sqrt{n})$ (we use $\tilde {O}$ to hide small poly-logarithmic factors) and a worst case query time is O(n 3/4). (iii) A deterministic O(n 2log n) time algorithm for constructing an O(log n)-spanner with O(n) edges for any weighted undirected graph on n vertices. The algorithm uses a simple algorithm for incrementally maintaining single-source shortest-paths tree up to a given distance. Reductions that show that the incremental and decremental single-source shortest-paths problems, for weighted directed or undirected graphs, are, in a strong sense, at least as hard as the static all-pairs shortest-paths problem. We also obtain slightly weaker results for the corresponding unweighted problems. A randomized fully-dynamic algorithm for the all-pairs shortest-paths problem in directed unweighted graphs with an amortized update time of $\tilde {O}(m\sqrt{n})$ (we use $\tilde {O}$ to hide small poly-logarithmic factors) and a worst case query time is O(n 3/4). A deterministic O(n 2log n) time algorithm for constructing an O(log n)-spanner with O(n) edges for any weighted undirected graph on n vertices. The algorithm uses a simple algorithm for incrementally maintaining single-source shortest-paths tree up to a given distance.	2.5d;amortized analysis;best, worst and average case;dynamic problem (algorithms);dynamic programming;graph (discrete mathematics);randomized algorithm;shortest path problem;vertex (geometry)	Liam Roditty;Uri Zwick	2010	Algorithmica	10.1007/s00453-010-9401-5	mathematical optimization;combinatorics;discrete mathematics;floyd–warshall algorithm;mathematics;graph	Theory	20.354452263529836	23.347196183337132	58652
702e59aa77f418bc4db58a5fe897f9f6b6e849e5	kernel lower bounds using co-nondeterminism: finding induced hereditary subgraphs	hereditary graph properties;co nondeterminism;kernelization lower bounds	This work further explores the applications of co-nondeterminism for showing kernelization lower bounds. The only known example prior to this work excludes polynomial kernelizations for the so-called Ramsey problem of finding an independent set or a clique of at least k vertices in a given graph [Kratsch 2012]. We study the more general problem of finding induced subgraphs on k vertices fulfilling some hereditary property Π, called Π-Induced Subgraph. The problem is NP-hard for all nontrivial choices of Π by a classic result of Lewis and Yannakakis [1980]. The parameterized complexity of this problem was classified by Khot and Raman [2002] depending on the choice of Π. The interesting cases for kernelization are for Π containing all independent sets and all cliques, since the problem is trivially polynomial time solvable or W[1]-hard otherwise.  Our results are twofold. Regarding Π-Induced Subgraph, we show that for a large choice of natural graph properties Π, including chordal, perfect, cluster, and cograph, there is no polynomial kernel with respect to k. This is established by two theorems, each one capturing different (but not necessarily exclusive) sets of properties: one using a co-nondeterministic variant of OR-cross-composition and one by a polynomial parameter transformation from Ramsey.  Additionally, we show how to use improvement versions of NP-hard problems as source problems for lower bounds, without requiring their NP-hardness. For example, for Π-Induced Subgraph our compositions may assume existing solutions of size k--1. This follows from the more general fact that source problems for OR-(cross-)compositions need only be NP-hard under co-nondeterministic reductions. We believe this to be useful for further lower-bound proofs, for example, since improvement versions simplify the construction of a disjunction (OR) of instances required in compositions. This adds a second way of using co-nondeterminism for lower bounds.	co-np;cograph;decision problem;graph property;hereditary property;independent set (graph theory);induced subgraph;kernel (operating system);kernelization;many-one reduction;np (complexity);np-hardness;parameterized complexity;polynomial kernel;raman scattering;subgraph isomorphism problem;time complexity;vertex (geometry)	Stefan Kratsch;Marcin Pilipczuk;Ashutosh Rai;Venkatesh Raman	2014	TOCT	10.1145/2691321	mathematical optimization;combinatorics;discrete mathematics;mathematics;induced subgraph isomorphism problem;algorithm;algebra	Theory	22.561013512516908	22.851990780568883	58698
94b3517d5668b2414b9c40655d1553050d957b25	approximate maxima finding of continuous functions under restricted budget (extended abstract)	polynomial algorithm	A function is distributed among nodes of a graph in acontinuous way, i.e., such that the difference between values stored at adjacent nodes is small. The goal is to find a node of maximum value by probing some nodes under a restricted budget. Every node has an associated cost which has to be paid for probing it and a probe reveals the value of the node. If the total budget is too small to allow probing every node, it is impossible to find the maximum value in the worst case. Hence we seek an Approximate Maxima Finding (AMF) algorithm that offers the best worst-case guarantee g, i.e., for any continuous distribution of values it finds a node whose value differs from the maximum value by at most g.  Approximate Maxima Finding in graphs is related to a generalization of the multicenter problem and we get new results for this problem as well. For example, we give a polynomial algorithm to find a minimum cost solution for the multicenter problem on a tree, with arbitrary node costs.    	maxima	Evangelos Kranakis;Danny Krizanc;Andrzej Pelc;David Peleg	1996		10.1007/3-540-62559-3_22	mathematical optimization;combinatorics;discrete mathematics;mathematics	Theory	20.571969201979154	21.19993383829076	58706
e5208a39484ff87c737d916dc180081975b63c9c	theory and implementation of online multiselection algorithms		We introduce a new online algorithm for the multiselection problem which performs a sequence of selection queries on a given unsorted array. We show that our online algorithm is 1-competitive in terms of data comparisons. In particular, we match the bounds (up to lower order terms) from the optimal offline algorithm proposed by Kaligosi et al.[ICALP 2005]. We provide experimental results comparing online and offline algorithms. These experiments show that our online algorithms require fewer comparisons than the best-known offline algorithms. Interestingly, our experiments suggest that our optimal online algorithm (when used to sort the array) requires fewer comparisons than both quicksort and mergesort.	experiment;merge sort;online algorithm;online and offline;quicksort	Jérémy Barbay;Ankur Gupta;Seungbum Jo;S. Srinivasa Rao;Jonathan P. Sorenson	2013		10.1007/978-3-642-40450-4_10	competitive analysis;online algorithm;computer science;theoretical computer science;machine learning;distributed computing	ML	13.440873707496474	25.210282736403656	58765
1f2dd1bef5e521b144a3e166aef879a0cf4a9e3a	variant code transformations for linear quadtrees	linear quadtree;space complexity;code transformation;breath first traversal;depth first traversal	In this paper, general guidelines and specific algorithms for code transformations between breadth-first (BF) and depth-first (DF) linear quadtrees are proposed. Each algorithm has time and space complexities in OðlÞ, where l is the length of the code of a BF or DF linear quadtree. 2002 Elsevier Science B.V. All rights reserved.	algorithm;brainfuck;breadth-first search;code;depth-first search;direction finding;pixel;quadtree	Pei-Min Chen	2002	Pattern Recognition Letters	10.1016/S0167-8655(02)00060-0	discrete mathematics;computer science;theoretical computer science;quadtree;mathematics;dspace;algorithm;depth-first search	AI	15.583198758823354	28.445903711587743	58898
90b13255eec90ee0cc82d2d35d03576f98e9e895	a manually-checkable proof for the np-hardness of 11-color pattern self-assembly tile set synthesis	tile complexity;dna pattern self assembly;manually checkable proof	Patterned self-assembly tile set synthesis (Pats) aims at finding a minimum tile set to uniquely self-assemble a given rectangular (color) pattern. For k ≥ 1, k-Pats is a variant of Pats that restricts input patterns to those with at most k colors. A computer-assisted proof has been recently proposed for 2-Pats by Kari et al. [arXiv:1404.0967 (2014)]. In contrast, the best known manually-checkable proof is for the NP-hardness of 29Pats by Johnsen, Kao, and Seki [ISAAC 2013, LNCS 8283, pp. 699-710]. We propose a manually-checkable proof for the NP-hardness of 11-Pats.	color;computer-assisted proof;lecture notes in computer science;np-hardness;self-assembly;tile-based video game	Aleck C. Johnsen;Ming-Yang Kao;Shinnosuke Seki	2017	J. Comb. Optim.	10.1007/s10878-015-9975-6	combinatorics;discrete mathematics;probabilistically checkable proof;mathematics;algorithm	Theory	23.972933485925957	24.7240808744912	58952
7e25b5455722b28cebfe48f91e9dc7b45e33de7c	walking the complexity lines for generalized guarded existential rules	query entailment;generalized guarded existential rule;data complexity;bounded predicate arity;combined complexity;complexity line;gbts class;greedy bounded treewidth set;conjunctive query entailment problem;frontier-guarded rule;unbounded predicate arity;weakly frontier-guarded rule	We establish complexities of the conjunctive query entailment problem for classes of existential rules (i.e. Tuple-Generating Dependencies or Datalog+/rules). Our contribution is twofold. First, we introduce the class of greedy bounded treewidth sets (gbts), which covers guarded rules, and their known generalizations, namely (weakly) frontier-guarded rules. We provide a generic algorithm for query entailment with gbts, which is worst-case optimal for combined complexity with bounded predicate arity, as well as for data complexity. Second, we classify several gbts classes, whose complexity was unknown, namely frontier-one, frontier-guarded and weakly frontier-guarded rules, with respect to combined complexity (with bounded and unbounded predicate arity) and data complexity.	best, worst and average case;conjunctive query;connected component (graph theory);directed acyclic graph;generic programming;greedy algorithm;p (complexity);tree decomposition;treewidth;vertex-transitive graph	Jean-François Baget;Marie-Laure Mugnier;Sebastian Rudolph;Michaël Thomazo	2011		10.5591/978-1-57735-516-8/IJCAI11-126	combinatorics;discrete mathematics;mathematics;algorithm	AI	19.799256154988676	24.290240582975652	59071
62fcfc4d8f501cd9d165261b74aaf9fe157c6248	ancestral maximum likelihood of evolutionary trees is hard	vertex cover;maximum likelihood;evolutionary trees;proof of correctness;maximum parsimony	Maximum likelihood (ML) (Neyman, 1971) is an increasingly popular optimality criterion for selecting evolutionary trees. Finding optimal ML trees appears to be a very hard computational task — in particular, algorithms and heuristics for ML take longer to run than algorithms and heuristics for maximum parsimony (MP). However, while MP has been known to be NP-complete for over 20 years, no such hardness result has been obtained so far for ML. In this work we make a first step in this direction by proving that ancestral maximum likelihood (AML) is NP-complete. The input to this problem is a set of aligned sequences of equal length and the goal is to find a tree and an assignment of ancestral sequences for all of that tree's internal vertices such that the likelihood of generating both the ancestral and contemporary sequences is maximized. Our NP-hardness proof follows that for MP given in (Day, Johnson and Sankoff, 1986) in that we use the same reduction from Vertex Cover; however, the proof of correctness for this reduction relative to AML is different and substantially more involved.	algorithm;computation;correctness (computer science);heuristic (computer science);maximum parsimony (phylogenetics);np-completeness;np-hardness;optimality criterion;phylogenetic tree;tree (data structure);vertex cover	Louigi Addario-Berry;Benny Chor;Michael T. Hallett;Jens Lagergren;Alessandro Panconesi;Todd Wareham	2003		10.1007/978-3-540-39763-2_16	biology;mathematical optimization;combinatorics;phylogenetic tree;vertex cover;tree rearrangement;mathematics;maximum likelihood;maximum parsimony;statistics	Theory	17.548036745807288	22.51740630824128	59144
d0f893a4615b99ca64f22b94b87101d45a28841e	on the parallel dynamic dictionary matching problem: new results with applications	parallel algorithm	In the Parallel Dynamic Dictionary Matching (DDM) problem, a dictionary  D\mathcal{D}contains a set of patterns that can change over time under insertion and deletion of individual patterns.  D\mathcal{D}has to be properly maintained such that, given an arbitrary text T, for each position in T we have to list in parallel the longest pattern of  D\mathcal{D}occurring there. In this paper, we follow a completely new approach thus providing the first parallel algorithm for the DDM problem which achieves optimal (amortized) work for the update operations in the case of both a constant sized and an unbounded alphabet. It requires small space and still preserves the same time and work of the best known CRCW-PRAM algorithm [27] for answering queries on an arbitrary text.	dictionary	Paolo Ferragina;Fabrizio Luccio	1996		10.1007/3-540-61680-2_61	computer science;theoretical computer science;machine learning;distributed computing;parallel algorithm;cost efficiency	Theory	13.179402263574273	26.901694467429355	59294
e970233a5f3404f5ce0ab3842e208efc813638e4	the canadian traveller problem revisited		This study investigates a generalization of the Canadian Traveller Problem (CTP), which finds real applications in dynamic navigation systems used to avoid traffic congestion. Given a road network G = (V, E) in which there is a source s and a destination t in V , every edge e in E is associated with two possible distances: original d(e) and jam d(e). A traveller only finds out which one of the two distances of an edge upon reaching an end vertex incident to the edge. The objective is to derive an adaptive strategy for travelling from s to t so that the competitive ratio, which compares the distance traversed with that of the static s, t-shortest path in hindsight, is minimized. This problem was defined by Papadimitriou and Yannakakis. They proved that it is PSPACE-complete to obtain an algorithm with a bounded competitive ratio. In this paper, we propose tight lower bounds of the problem when the number of “traffic jams” is a given constant k; and we introduce a simple deterministic algorithm with a min{r, 2k + 1}-ratio, which meets the proposed lower bound, where r is the worst-case performance ratio. We also consider the uniform jam cost model, i.e., for every edge e, d(e) = d(e)+c, for a constant c. Finally, we discuss an extension to the metric Travelling Salesman Problem (TSP) and propose a touring strategy within an O( √ k)-competitive ratio.	analysis of algorithms;best, worst and average case;canadian traveller problem;competitive analysis (online algorithm);deterministic algorithm;jam;network congestion;pspace-complete;shortest path problem;travelling salesman problem	Yamming Huang;Chung-Shou Liao	2012		10.1007/978-3-642-35261-4_38	computer science;competitive analysis;canadian traveller problem;combinatorics;travelling salesman problem;vertex (geometry);bounded function;deterministic algorithm;traffic congestion;upper and lower bounds	Theory	21.862139840955784	21.44738743363754	59457
9071655dfb816f6f822ca7731a382787833c2282	bdd operations for quantum graph states		A quantum graph state determined by an underlying graph is very fundamental in quantum computation and information, such as measurement-based quantum computing, stabilizer states and codes. For a graph state, a reversible operation, called the local complementation, transforms it to another graph state, and local Clifford operations map it to a stabilizer state. Besides these operations, taking the inner product of a graph/stabilizer state with an arbitrary complete product state leads to analyzing measurements for an arbitrary basis and solving a #Pcomplete problem of computing the partition function of a graph in Ising model. We recently observe that a graph state naturally corresponds to a Boolean function associated with a graph, and apply our top-down construction algorithm for the function. In this paper, we further discuss BDD operations for the above-mentioned operations on them. Specific bounds on the sizes and computational times of these BDDs are given in terms of the linear rank-width of a graph, and an efficient exact exponential algorithm for the Ising partition function is derived.	algorithm;computation;directed graph;graph state;ising model;one-way quantum computer;partition function (mathematics);product state;quantum computing;quantum graph;stabilizer code;time complexity;top-down and bottom-up design	Hidefumi Hiraishi;Hiroshi Imai	2014		10.1007/978-3-319-08494-7_17	theoretical computer science	Theory	20.535523358491826	31.940533801087422	59731
991cd69b3ee6bfdf7e76d57b02e680c777ddb4a6	a note on the expected path length of trees with known fringe	camino mas corto;shortest path;algorithms and data structures;plus court chemin;binary trees;algorithme;arbol binario;data structures;arbre binaire;algorithms;structure donnee;path length;algorithms and data structure;binary tree	The path length of a tree, that is the sum of the lengths of all root-leaf paths, is an important measure of eeciency of the tree. The fringe of a tree is the diierence between the lengths of the longest path and the shortest path in the tree. The minimal and the maximal path length of trees with N leaves and given fringe have been well studied. In this paper, we initiate the study of the expected path length of the class of trees with N leaves and fringe by giving a closed expression for the expected path length when = 2.	longest path problem;maximal set;shortest path problem	Roberto De Prisco;Giuseppe Parlati;Giuseppe Persiano	1996	Inf. Process. Lett.	10.1016/0020-0190(96)00129-9	combinatorics;discrete mathematics;widest path problem;data structure;binary tree;computer science;mathematics;algorithm;shortest-path tree	Theory	19.780692028858496	25.84941065321829	59748
6d7818f4ff8b5584444eabe06a65f4e188babde6	unfolding of multirate data-flow graph to compute iteration bound	data flow graph dfg;iteration bound;data flow graph;concurrent processing;petri nets;petri net;concurrent process	Parhietal. find the iteration bound (IB) by considering the equivalent single-rate data-flow graph (SRDFG) N' of N, which is generally an exponential time task and the transformed SRDFG is much larger (grows exponentially) than the MRDFG. Ito et al. proposed a novel algorithm to remove node/edge redundancies taking extra time and memory, but losing schedule information of removed nodes. We propose to reduce the MRDFG in a loop-wise fashion (reduce the nodes/edges in a loop as a whole) with fewer nodes/edges. The scheduling of nodes in the MRDFG can be derived from that of the reduced SRDFG., where one invocation of a node n corresponds to a consecutive number of invocations of n in the MRDFG.	iteration;net (polyhedron)	Daniel Yuh Chao	2009		10.1007/978-3-642-03095-6_44	parallel computing;null graph;computer science;theoretical computer science;simplex graph;multigraph;cycle graph;voltage graph;distributed computing;programming language;random geometric graph;petri net;complement graph;algorithm;strength of a graph;ordered graph	Logic	17.064629679016086	30.73934100930354	60039
0f4f015da70a36e07edd976bf9fded890102328c	tight lower bounds on graph embedding problems		We prove that unless the Exponential Time Hypothesis (ETH) fails, deciding if there is a homomorphism from graph <i>G</i> to graph <i>H</i> cannot be done in time |<i>V</i>(<i>H</i>)|<sup><i>o</i>(|<i>V</i>(<i>G</i>)|)</sup>. We also show an exponential-time reduction from Graph Homomorphism to Subgraph Isomorphism. This rules out (subject to ETH) a possibility of |<i>V</i>(<i>H</i>)|<sup><i>o</i>(|<i>V</i>(<i>H</i>)|)</sup>-time algorithm deciding if graph <i>G</i> is a subgraph of <i>H</i>. For both problems our lower bounds asymptotically match the running time of brute-force algorithms trying all possible mappings of one graph into another. Thus, our work closes the gap in the known complexity of these fundamental problems.  Moreover, as a consequence of our reductions, conditional lower bounds follow for other related problems such as Locally Injective Homomorphism, Graph Minors, Topological Graph Minors, Minimum Distortion Embedding and Quadratic Assignment Problem.	algorithm;clipping (computer graphics);clique-width;dll hell;decision problem;degree (graph theory);distortion;exptime;exponential time hypothesis;graph (discrete mathematics);graph coloring;graph embedding;graph homomorphism;graph minor;immersion (virtual reality);journal of the acm;p versus np problem;polynomial;quadratic assignment problem;subgraph isomorphism problem;time complexity;topological graph	Marek Cygan;Fedor V. Fomin;Alexander Golovnev;Alexander S. Kulikov;Ivan Mihajlin;Jakub W. Pachocki;Arkadiusz Socala	2017	J. ACM	10.1145/3051094	graph power;petersen graph;mathematical optimization;factor-critical graph;combinatorics;discrete mathematics;graph embedding;topology;null graph;graph property;distance-regular graph;simplex graph;forbidden graph characterization;cubic graph;graph factorization;mathematics;voltage graph;distance-hereditary graph;graph homomorphism;butterfly graph;graph minor;complement graph;line graph;algorithm;string graph;circulant graph	Theory	23.22775086925771	22.78931525231598	60072
788477ee5ebec3c9f643af89d38be0c91b332147	ranking on arbitrary graphs: rematch via continuous lp with monotone and boundary condition constraints	conference_paper;maximum matching;oblivious algorithms;continuous linear programming;primal dual methods	Motivated by online advertisement and exchange settings, greedy randomized algorithms for the maximum matching problem have been studied, in which the algorithm makes (random) decisions that are essentially oblivious to the input graph. Any greedy algorithm can achieve performance ratio 0.5, which is the expected number of matched nodes to the number of nodes in a maximum matching. Since Aronson, Dyer, Frieze and Suen proved that the Modified Randomized Greedy (MRG) algorithm achieves performance ratio 0.5 + (where = 1 400000 ) on arbitrary graphs in the midnineties, no further attempts in the literature have been made to improve this theoretical ratio for arbitrary graphs until two papers were published in FOCS 2012. Poloczek and Szegedy also analyzed the MRG algorithm to give ratio 0.5039, while Goel and Tripathi used experimental techniques to analyze the Ranking algorithm to give ratio 0.56. However, we could not reproduce the experimental results of Goel and Tripathi. In this paper, we revisit the Ranking algorithm using the LP framework. Special care is given to analyze the structural properties of the Ranking algorithm in order to derive the LP constraints, of which one known as the boundary constraint requires totally new analysis and is crucial to the success of our LP. We use continuous LP relaxation to analyze the limiting behavior as the finite LP grows. Of particular interest are new duality and complementary slackness characterizations that can handle the monotone and the boundary constraints in continuous LP. We believe our work achieves the currently best theoretical performance ratio of 2(5− √ 7) 9 ≈ 0.523 on arbitrary graphs. Moreover, experiments suggest that Ranking cannot perform better than 0.724 in general. ∗Department of Computer Science, the University of Hong Kong. {hubert,fchen,xwwu,zczhao}@cs.hku.hk ar X iv :1 30 7. 26 96 v2 [ cs .D S] 1 1 Ju l 2 01 3	design of experiments;experiment;greedy algorithm;lp-type problem;linear programming relaxation;matching (graph theory);online advertising;randomized algorithm;symposium on foundations of computer science;monotone	T.-H. Hubert Chan;Fei Chen;Xiaowei Wu;Zhichao Zhao	2014		10.1137/1.9781611973402.82	mathematical optimization;combinatorics;discrete mathematics;mathematics;algorithm;matching	Theory	21.98525635067336	20.03749552365891	60088
6a3f4a421466d3f83ec02da847d57598acd25347	best-first minimax search: othello results	evaluation function;time complexity;search algorithm;space complexity	We present a very simple selective search algorithm for two-player games. It always expands next the frontier node that determines the minimax value of the root. The algorithm requires no information other than a static evaluation function, and its time overhead per node is similar to that of alpha-beta minimax. We also present an implementation of the algorithm that reduces its space complexity from exponential to linear in the search depth, at the cost of increased time complexity. In the game of Othello, using the evaluation function from BiIl (Lee & Mahajan 1990), bestfirst minimax outplays alpha-beta at moderate depths. A hybrid best-first extension algorithm, which combines alpha-beta and best-first minimax, performs significantly better than either pure algorithm even at greater depths. Similar results were also obtained for a class of random game trees. Introduction and Overview The best chess machines are competitive with the best humans, but generate millions of positions per move. Their human opponents, however, only examine tens of positions, but search much deeper along some lines of play. Obviously, people are more selective in their choice of positions to examine. The importance of selective search was first recognized by (Shannon 1950). Most work on game-tree search has focussed on algorithms that make the same decisions as fullwidth, fixed-depth minimax. This includes alpha-beta pruning (Knuth & Moore 1975), fixed and dynamic node ordering (Slagle & Dixon 1969), SSS* (Stockman 1979), Scout (Pearl 1984), aspiration-windows (Kaindl, Shams, & Horacek 1991), etc. We define a selective search algorithm as one that makes different decisions than full-width, fixed-depth minimax. These include B* (Berliner 1979), conspiracy search (McAllester 1988), min/max approximation (Rivest 1987), meta-greedy search (Russell & Wefald 1989), and singular extensions (Anantharaman, Campbell, & Hsu 1990). All of these algorithms, except singular extensions, require exponential memory, and most have large time overheads per node expansion. In addition, B* and meta-greedy search require more information than a single static evaluation function. Singular extensions is the only algorithm to be successfully incorporated into a high-performance program. We describe a very simple selective search algorithm, called best-first minimax. It requires only a single static evaluator, and its time overhead per node is roughly the same as alpha-beta minimax. We describe an implementation of the algorithm that reduces its space complexity from exponential to linear in the search depth. We also explore best-first extensions, a hybrid combination of alpha-beta and best-first minimax. Experimentally, best-first extensions outperform alpha-beta in the game of Othello, and on a class of random game trees. Earlier reports on this work include (Korf 1992) and (Korf & Chickering 1993). Best-First Minimax Search The basic idea of best-first minimax is to always explore further the current best line of play. Given a partially expanded game tree, with static evaluations of the leaf nodes, the value of an interior MAX node is the maximum of its children’s values, and the value of an interior MIN node is the minimum of its children’s values. There exists a path, called the principal variation, from the root to a leaf node, in which every node has the same value. This leaf node, whose evaluation determines the mini max value of the root, is called the principal leaf. Best-first minimax always expands next the current principal leaf node, since it has the greatest affect on the minimax value of the root. Consider the example in figure 1, where squares represent MAX nodes and circles represent MIN nodes. Figure 1A shows the situation after the root has been expanded. The values of the children are their static values, and the value of the root is 6, the maximum of its children’s values. Thus, the right child is the principal leaf, and is expanded next, resulting in the situation in figure 1B. The new frontier nodes are statically evaluated at 5 and 2, and the value of their MIN parent changes to 2, the minimum of its children’s values. This changes the value of the root to 4, the maximum of its children’s values. Thus, the left child of the root is the new principal leaf, and is expanded next, resultTwo-Player Games 1365 From: AAAI-94 Proceedings. Copyright © 1994, AAAI (www.aaai.org). All rights reserved.	alpha–beta pruning;approximation;binary tree;dspace;dixon's factorization method;evaluation function;experiment;greedy algorithm;interpreter (computing);max;maxima and minima;microsoft windows;minimax;overhead (computing);reversi;scout;search algorithm;shannon (unit);time complexity;tree (data structure);variation (game tree)	Richard E. Korf;David Maxwell Chickering	1994			time complexity;minimax;mathematical optimization;expectiminimax tree;computer science;artificial intelligence;machine learning;evaluation function;dspace;killer heuristic;minimax approximation algorithm;alpha–beta pruning;negamax;game complexity;search algorithm	AI	15.436585673991534	28.013328509915365	60139
5c29b5c3073dfc2ff620cf84d227aa754b20b7be	transposition of banded matrices in hypercubes: a nearly isotropic task	linear algebra;hypercube;largeur bande;tk7855 m41 e3845 no 2135;transposition;matrice creuse;algebre lineaire;isotropic task;anchura banda;hypercubes;bandwidth;algebra lineal;procesador;sparse matrix;processeur;communication algorithm;algoritmo optimo;algorithme optimal;optimal algorithm;communication;comunicacion;banded matrices;processor;lower bound;matriz dispersa;transposicion;hipercubo	"""A class of communication tasks, called isotropic, was introduced in [VaB92], and minimum completion time algorithms for all tasks in this class were found. Isotropic tasks are characterized by a type of symmetry with respect to origin node. In this paper we consider the problem of transposing a sparse matrix of size N x N with a diagonal band of size 2P+1 + 1, which is stored by columns in a hypercube network of N = 2d processors. We propose an assignment of matrix columns to hypercube nodes such that the transposition task becomes a """"nearly isotropic"""" task, that is, it looks """"almost identical"""" to all nodes. Under this assignment, we give an algorithm to transpose the matrix in 2A steps. We prove that the algorithm given is optimal over all affine assignments of columns to processors. We also derive a lower bound on the minimum number of steps required to transpose a banded matrix, which holds for any possible assignment of matrix columns to hypercube processors. In the case that 2P+1+1 = O(Nc), for some constant c E (0, 1], we prove that the completion time of our transposition algorithm is of the same order of magnitude with the lower bound. We further show that Ld//3J banded matrices, each of bandwidth 2P+1 + 1, can be stored by columns in a hypercube so that all of them can be concurrently transposed in 2P+1 steps. Finally, we modify our algorithms so that they apply to arbitrary matrix bandwidths and multiple column storage by each processor, while maintaining their efficiency. 1 Research supported by NSF under Grant NSF-DDM-8903385 and by the ARO under Grants DAAL03-86-K-00171, and DAAL03-92-G-0309. 2 Electrical and Computer Engineering, University of California, Santa Barbara, CA 093106. 3 Laboratory for Information and Decision Systems, Massachusetts Institute of Technology, Cambridge, MA 02139."""	algorithm;central processing unit;column (database);column-oriented dbms;computer engineering;ibm notes;like button;sparse matrix;the matrix	Emmanouel A. Varvarigos;Dimitri P. Bertsekas	1995	Parallel Computing	10.1016/0167-8191(94)00031-5	combinatorics;discrete mathematics;parallel computing;linear algebra;mathematics;algorithm;hypercube;algebra	Theory	13.555477412555017	31.691615902263738	60234
140d1fb9bc3ec40e33673bd2c502a09baa126d23	top-down skiplists		We describe todolists (top-down skiplists), a variant of skiplists (Pugh 1990) that can execute searches using at most log2−εn +O(1) binary comparisons per search and that have amortized update timeO(ε−1 logn). A variant of todolists, called working-todolists, can execute a search for any element x using log2−εw(x)+ o(logw(x)) binary comparisons and have amortized search time O(ε−1 logw(w)). Here, w(x) is the “working-set number” of x. No previous data structure is known to achieve a bound better than 4log2w(x) comparisons. We show through experiments that, if implemented carefully, todolists are comparable to other common dictionary implementations in terms of insertion times and outperform them in terms of search times. ∗School of Computer Science, Carleton University and Département d’Informatique, Université Libre de Bruxelles, lbarbafl@ulb.ac.be †School of Computer Science, Carleton University, morin@scs.carleton.ca ar X iv :1 40 7. 79 17 v1 [ cs .D S] 3 0 Ju l 2 01 4	amortized analysis;b-tree;cpu cache;cache-oblivious algorithm;computer science;data structure;dictionary;experiment;red–black tree;skip list;top-down and bottom-up design;working set	Luis Barba;Pat Morin	2014	CoRR		speech recognition;mathematics;algorithm;statistics	Theory	12.085982180241368	28.479403553923625	60317
e4e4f6c5ea8d15aeb1f3265ca60b25241e8c9c41	on counting perfect matchings in general graphs		Counting perfect matchings has played a central role in the theory of counting problems. The permanent, corresponding to bipartite graphs, was shown to be #P-complete to compute exactly by Valiant (1979), and a fully polynomial randomized approximation scheme (FPRAS) was presented by Jerrum, Sinclair, and Vigoda (2004) using a Markov chain Monte Carlo (MCMC) approach. However, it has remained an open question whether there exists an FPRAS for counting perfect matchings in general graphs. In fact, it was unresolved whether the same Markov chain defined by JSV is rapidly mixing in general. In this paper, we show that it is not. We prove torpid mixing for any weighting scheme on hole patterns in the JSV chain. As a first step toward overcoming this obstacle, we introduce a new algorithm for counting matchings based on the Gallai–Edmonds decomposition of a graph, and give an FPRAS for counting matchings in graphs that are sufficiently close to bipartite. In particular, we obtain a fixed-parameter tractable algorithm for counting matchings in general graphs, parameterized by the greatest “order” of a factor-critical subgraph.	cobham's thesis;computing the permanent;counting problem (complexity);cryptocurrency tumbler;edmonds' algorithm;markov chain monte carlo;matching (graph theory);monte carlo method;p-complete;parameterized complexity;polynomial;polynomial-time approximation scheme;randomized algorithm;sharp-p	Daniel Stefankovic;Eric Vigoda;John Wilmes	2018		10.1007/978-3-319-77404-6_63	combinatorics;discrete mathematics;mathematics;parameterized complexity;existential quantification;bipartite graph;markov chain monte carlo;counting problem;polynomial;markov chain;weighting	Theory	21.446855978046322	22.27447280805519	60331
83fb2aef7236683ec78c45efb1983867ef782d85	on the convergence of multicast games in directed networks	union bidireccional;fixed cost;distributed system;multicast communication;valeur shapley;reseau communication;anarquia;systeme reparti;game theory;liaison bidirectionnelle;costo fijo;lower and upper bound;pago;cout fixe;egoisme;shapley cost allocation;price of anarchy;multidestinatario;telecommunication network;selfishness;teoria juego;theorie jeu;payment;upper bound;anarchy;paiement;sharing;sistema repartido;shapley value;particion;limited number of best response moves;community networks;red telecomunicacion;egoismo;juego no cooperativo;non cooperative network;borne inferieure;reseau telecommunication;multicast game;non cooperative networks;upper and lower bounds;non cooperative game;anarchie;cost sharing;partage;red de juegos;borne superieure;bidirectional link;valor shapley;red de comunicacion;game network;communication network;multidestinataire;lower bound;multicast;cota superior;jeu non cooperatif;reseau de jeux;cota inferior;multicast games	We investigate the convergence of the price of anarchy after a limited number of moves in the classical multicast communication game when the underlying communication network is directed. Namely, a subset of nodes of the network are interested in receiving the transmission from a given source node and can share the cost of the used links according to fixed cost sharing methods. At each step, a single receiver is allowed to modify its communication strategy, that is to select a communication path from the source, and assuming a selfish or rational behavior, it will make a best response move, that is it will select a solution yielding the minimum possible payment or shared cost. We determine lower and upper bounds on the price of anarchy, that is the highest possible ratio among the overall cost of the links used by the receivers and the minimum possible cost realizing the required communications, after a limited number of moves under the fundamental Shapley cost sharing method. In particular, assuming that the initial set of connecting paths can be arbitrary, we show an $O(r\sqrt{r})$ upper bound on the price of anarchy after 2 rounds, during each of which all the receivers move exactly once, and a matching lower bound, that we also extend to $\Omega(r\sqrt[k]{r})$ for any number k≥2 rounds, where r is the number of receivers. Similarly, exactly matching upper and lower bounds equal to r are determined for any number of rounds when starting from the empty state in which no path has been selected. Analogous results are obtained also with respect to other three natural cost sharing methods considered in the literature, that is the egalitarian, path-proportional and egalitarian-path proportional ones. Most results are also extended to the undirected case in which the communication links are bidirectional.	anarchy;graph (discrete mathematics);induced path;multicast;nash equilibrium;price of stability;stable marriage problem;telecommunications network	Angelo Fanelli;Michele Flammini;Luca Moscardelli	2008	Algorithmica	10.1007/s00453-008-9212-0	game theory;mathematical optimization;combinatorics;simulation;telecommunications;mathematics;mathematical economics;upper and lower bounds;algorithm	Theory	19.13129572897475	32.21881351305453	60385
93fb34a9e48798c0619a1354b480921489ba3bfa	hydras: complexity on general graphs and a subclass of trees	horn cnf;horn minimization;hydra formula;caterpillar	Hydra formulas were introduced in (Sloan, Stasi, and Turán 2012). A hydra formula is a Horn formula consisting of definite Horn clauses of size 3 specified by a set of bodies of size 2, and containing clauses formed by these bodies and all possible heads. A hydra formula can be specified by the undirected graph formed by the bodies occurring in the formula. The minimal formula size for hydras is then called the hydra number of the underlying graph. In this paper we aim to answer some open questions regarding complexity of determining the hydra number of a graph which were left open in (Sloan, Stasi, and Turán 2012). In particular we show that the problem of checking, whether a graph G = (V,E) is single-headed, i.e. whether the hydra number of G is equal to the number of edges, is NP-complete. We also consider hydra number of trees and we describe a family of trees for which the hydra number can be determined in polynomial time.	algorithm;directed graph;graph (discrete mathematics);horn clause;karp's 21 np-complete problems;line graph;np-completeness;path cover;polynomial;time complexity	Petr Kucera	2014	Theor. Comput. Sci.	10.1016/j.tcs.2016.05.037	combinatorics;discrete mathematics;mathematics;algorithm	Theory	23.393037524589893	24.30684621688387	60404
ef87e4c8405fb757c6006e89b28a88e90759e1fc	on external-memory mst, sssp and multi-way planar graph separation	camino mas corto;shortest path;algorithmique;minimal spanning tree;05c05;efficient algorithm;plus court chemin;calculo automatico;computing;calcul automatique;algorithmics;algoritmica;i o efficiency;graphe planaire;68r10;minimum spanning tree;chemin plus court;single source shortest path;graph algorithm;memoire externe;external memory;algorithme graphe;grafo planario;breadth first search;efficacite entree sortie;arbre maximal minimal;planar graph;massive data sets	Recently external memory graph problems have received considerable attention because massive graphs arise naturally in many applications involving massive data sets. Even though a large number of I/O-efficient graph algorithms have been developed, a number of fundamental problems still remain open.The results in this paper fall in two main classes. First we develop an improved algorithm for the problem of computing a minimum spanning tree (MST) of a general undirected graph. Second we show that on planar undirected graphs the problems of computing a multi-way graph separation and single source shortest paths (SSSP) can be reduced I/O-efficiently to planar breadth-first search (BFS). Since BFS can be trivially reduced to SSSP by assigning all edges weight one, it follows that in external memory planar BFS, SSSP, and multi-way separation are equivalent. That is, if any of these problems can be solved I/O-efficiently, then all of them can be solved I/O-efficiently in the same bound. Our planar graph results have subsequently been used to obtain I/O-efficient algorithms for all fundamental problems on planar undirected graphs.	minimum spanning tree;planar graph;shortest path problem	Lars Arge;Gerth Stølting Brodal;Laura Toma	2004	J. Algorithms	10.1016/j.jalgor.2004.04.001	outerplanar graph;block graph;graph power;combinatorics;discrete mathematics;polyhedral graph;level structure;null graph;computer science;minimum spanning tree;pancyclic graph;forbidden graph characterization;comparability graph;planar straight-line graph;mathematics;voltage graph;trémaux tree;graph;butterfly graph;cycle basis;algorithmics;graph minor;book embedding;line graph;algorithm;planar graph;friendship graph	Theory	20.867780318725657	27.631331563435275	60474
015cca0964f80785f8da1d8e581ab45afb531a7c	efficient algorithms for decomposing graphs under degree constraints	article accepte pour publication ou publie;graph theory;generalizacion;optimisation;teoria grafo;nombre entier;descomposicion grafo;combinatorics;grado grafo;temps polynomial;optimizacion;combinatoria;vertex;efficient algorithm;metodo descomposicion;coaccion;combinatoire;methode decomposition;contrainte;theorie graphe;ciclo;polynomial time algorithm;integer;decomposition method;generalisation;constraint;preuve;particion;graphe contrainte;informatique theorique;polynomial algorithm;vertex partition;graphe sans triangle;entero;vertex graph;68r10;polynomial time;partition;graph algorithm;algorithme polynomial;05cxx;optimization;value function;vertice;degre graphe;algorithme graphe;cycle;generalization;vertice grafo;sommet graphe;vertex degree;graph degree;graph decomposition;computer theory;decomposition graphe;tiempo polinomial;informatica teorica	Stiebitz (1996) proved that if every vertex v in a graph G has degree d(v) ≥ a(v)+ b(v) + 1 (where a and b are arbitrarily given nonnegative integer-valued functions) then G has a nontrivial vertex partition (A, B) such that dA(v) ≥ a(v) for every v ∈ A and dB(v) ≥ b(v) for every v ∈ B. Kaneko (1998) and Diwan (2000) strengthened this result, proving that it suffices to assume d(v) ≥ a+b (a, b ≥ 1) or just d(v) ≥ a+b−1 (a, b ≥ 2) if G contains no cycles shorter than 4 or 5, respectively. The original proofs contain nonconstructive steps. In this paper we give polynomialtime algorithms that find such partitions. Constructive generalizations for k-partitions are also presented.	algorithm;best, worst and average case;bilateral filter;diwan-khane;fastest;girth (graph theory);np-completeness;polynomial;search algorithm;time complexity	Cristina Bazgan;Zsolt Tuza;Daniel Vanderpooten	2007	Discrete Applied Mathematics	10.1016/j.dam.2006.10.005	generalization;combinatorics;discrete mathematics;topology;degree;graph theory;mathematics;algorithm	Theory	23.14115791437327	29.89202019099022	60564
7e74a9435beeab1e888aca12db800e05a6d45ff0	approximate sequence comparison: a study with histograms	dynamic programming;sequence comparison;programacion dinamica;algorithm analysis;algorithme;algorithm;histogram;histogramme;pattern matching;programmation dynamique;pattern recognition;analyse algorithme;concordance forme;reconnaissance forme;reconocimiento patron;histograma;analisis algoritmo;algoritmo	Abstract   A dynamic programming algorithm is developed for computing the distance between two sequences, i.e. the minimum cost to transform a sequence to another. The alphabet of the sequences may be infinite. The allowed operations to edit a sequence are insertion, deletion and substitution. The cost function to weigh the editing operations must have at least two of the metric properties: non-negative values and the triangle inequality property. The time complexity of the algorithm is  O ( n  · min( z  + 1 +  s / Δ ,  m )), where  n  is the length of the longer input sequence,  z  is the number of items that can be deleted or inserted in the sequences with zero cost,  s  is the distance between the sequences. Δ is the smallest positive insertion or deletion cost and  m  is the length of the shorter input sequence. The algorithm needs linear space. It is especially designed for sequences where the costs of different deletions and insertions vary considerably. Experiments with integer number sequences show that our algorithm works at least as well as two other dynamic programming algorithms.		Liisa Räihä	1990	Pattern Recognition	10.1016/0031-3203(90)90056-Q	combinatorics;complementary sequences;computer science;dynamic programming;pattern matching;histogram;mathematics;algorithm	Vision	14.404401018389256	26.61176569282718	60602
1200860e88da301862875cbc8388d5b033b343b5	faster recovery of approximate periods over edit distance		The approximate period recovery problem asks to compute all approximate word-periods of a given word S of length n: all primitive words P (|P | = p) which have a periodic extension at edit distance smaller than τp from S, where τp = b n (3.75+ )·pc for some > 0. Here, the set of periodic extensions of P consists of all finite prefixes of P∞. We improve the time complexity of the fastest known algorithm for this problem of Amir et al. [Theor. Comput. Sci., 2018] from O(n) to O(n logn). Our tool is a fast algorithm for Approximate Pattern Matching in Periodic Text. We consider only verification for the period recovery problem when the candidate approximate word-period P is explicitly given up to cyclic rotation; the algorithm of Amir et al. reduces the general problem in O(n) time to a logarithmic number of such more specific instances.	approximation algorithm;circular shift;edit distance;fastest;pattern matching;time complexity	Tomasz Kociumaka;Jakub Radoszewski;Wojciech Rytter;Juliusz Straszynski;Tomasz Walen;Wiktor Zuba	2018		10.1007/978-3-030-00479-8_19	discrete mathematics;periodic graph (geometry);edit distance;mathematical analysis;mathematics;prefix	Theory	13.479367410363004	26.933512945891962	60772
7f7fb7f041ecc390596c7de1065235ef4e222505	on the power of statistical zero knowledge		We examine the power of statistical zero knowledge proofs (captured by the complexity class SZK) and their variants. First, we give the strongest known relativized evidence that SZK contains hard problems, by exhibiting an oracle relative to which SZK (indeed, even NISZK) is not contained in the class UPP, containing those problems solvable by randomized algorithms with unbounded error. This answers an open question of Watrous from 2002. Second, we lift this oracle separation to the setting of communication complexity, thereby answering a question of G&#xf6;&#xf6;s et al. (ICALP 2016). Third, we give relativized evidence that perfect zero knowledge proofs (captured by the class PZK) are weaker than general zero knowledge proofs. Specifically, we exhibit oracles which separate SZK from PZK, NISZK from NIPZK and PZK from coPZK. The first of these results answers a question raised in 1991 by Aiello and H&#xe5;stad (Information and Computation), and the second answers a question of Lovett and Zhang (2016). We also describe additional applications of these results outside of structural complexity.The technical core of our results is a stronger hardness amplification theorem for approximate degree, which roughly says that composing the gapped-majority function with any function of high approximate degree yields a function with high threshold degree.	approximation algorithm;communication complexity;complexity class;decision problem;hardness of approximation;icalp;information and computation;majority function;oracle machine;randomized algorithm;structural complexity (applied mathematics);zero-knowledge proof	Adam Bouland;Lijie Chen;Dhiraj Holden;Justin Thaler;Prashant Nalini Vasudevan	2017	2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS)	10.1109/FOCS.2017.71	discrete mathematics;combinatorics;oracle;computation;mathematics;cryptography;randomized algorithm;communication complexity;quantum computer;complexity class;zero-knowledge proof	Theory	10.958138408378424	21.727568185906	60804
953329f19587e24b0969b7e0b55d5e92f3dcab2a	ratio based stable in-place merging	asymptotic optimality;point of view	We investigate the problem of stable in-place merging from a ratio k = n m based point of view where m, n are the sizes of the input sequences with m ≤ n . We introduce a novel algorithm for this problem that is asymptotically optimal regarding the number of assignments as well as comparisons. Our algorithm uses knowledge about the ratio of the input sizes to gain optimality and does not stay in the tradition of Mannila and Ukkonen’s work [8] in contrast to all other stable in-place merging algorithms proposed so far. It has a simple modular structure and does not demand the additional extraction of a movement imitation buffer as needed by its competitors. For its core components we give concrete implementations in form of Pseudo Code. Using benchmarking we prove that our algorithm performs almost always better than its direct competitor proposed in [6]. As additional sub-result we show that stable in-place merging is a quite simple problem for every ratio k ≥ √ m by proving that there exists a primitive algorithm that is asymptotically optimal for such ratios.	asymptotically optimal algorithm;computation;in-place algorithm;merge sort;numerical analysis;polynomial greatest common divisor;pseudocode	Pok-Son Kim;Arne Kutzner	2008		10.1007/978-3-540-79228-4_22	mathematical optimization;combinatorics;discrete mathematics;mathematics;algorithm;statistics	Theory	13.220104338579137	23.51004812330505	60836
cfcb60adfaea0bd5636dc0095e8c32627d0bf677	computation of chromatic polynomials using triangulations and clique trees	graph theory;grafo triangular;raisonnement base sur cas;razonamiento fundado sobre caso;teoria grafo;geometrie algorithmique;chromatic polynomial;computational geometry;polinomio cromatico;theorie graphe;triangulacion;informatique theorique;borne inferieure;graphe triangule;clique tree;contraccion;geometria computacional;triangulation;case based reasoning;polynome chromatique;minimal triangulation;lower bound;chordal graphs;cota inferior;computer theory;chordal graph;contraction;informatica teorica	In this paper, we present a new algorithm for computing the chromatic polynomial of a general graph G. Our method is based on the addition of edges and contraction of non-edges of G, the base case of the recursion being chordal graphs. The set of edges to be considered is taken from a triangulation of G. To achieve our goal, we use the properties of triangulations and clique-trees with respect to the previous operations, and guide our algorithm to efficiently divide the original problem. Furthermore, we give some lower bounds of the general complexity of our method, and provide experimental results for several families of graphs. Finally, we exhibit an original measure of a triangulation of a graph.	algorithm;chromatic polynomial;computation;decision tree;graph (discrete mathematics);recursion;tree decomposition;triangulation (geometry)	Pascal Berthomé;Sylvain Lebresne;Kim Nguyen	2005		10.1007/11604686_32	case-based reasoning;chromatic polynomial;combinatorics;discrete mathematics;triangulation;computational geometry;graph theory;contraction;mathematics;geometry;upper and lower bounds;chordal graph;algorithm	Theory	22.318098820311935	27.386049319772837	60893
2b1b303593c3405375a436b24d9e0f10ffacbf04	logarithmic lower bounds in the cell-probe model	somme partielle;foret graphe;partial sums problem;cell size;partial sums;lower bounds;communication complexity;pregunta documental;intercambio comercial;upper bound;complexite cell probe;graphe simple;graph connectivity;68q17;single cell;echange commercial;computational complexity;data structures;dynamic data structure;modele comparaison;estructura datos;conectividad grafo;dynamic graph problem;borne inferieure;cell probe complexity;suma parcial;query;bosque grafo;dynamic graph problems;structure donnee;ams subject classification;upper and lower bounds;curve matching;model of computation;external memory;borne superieure;forest graph;connectivite graphe;trade;data structure;lower bound;requete;disjoint paths;cota superior;cota inferior;probleme graphe dynamique;partial sum	We develop a new technique for proving cell-probe lower bounds on dynamic data structures. This technique enables us to prove an amortized randomized (lg n) lower bound per operation for several data structural problems on n elements, including partial sums, dynamic connectivity among disjoint paths (or a forest or a graph), and several other dynamic graph problems (by simple reductions). Such a lower bound breaks a long-standing barrier of (lg n= lg lgn) for any dynamic language membership problem. It also establishes the optimality of several existing data structures, such as Sleator and Tarjan’s dynamic trees. We also prove the rst (log B n) lower bound in the external-memory model without assumptions on the data structure (such as the comparison model). Our lower bounds also give a query-update trade-o curve matched, e.g., by several data structures for dynamic connectivity in graphs. We also prove matching upper and lower bounds for partial sums when parameterized by the word size and the maximum additive change in an update.	amortized analysis;cell-probe model;data structure;dynamic connectivity;dynamic data;dynamization;link/cut tree;randomized algorithm;utility functions on indivisible goods	Mihai Patrascu;Erik D. Demaine	2006	SIAM J. Comput.	10.1137/S0097539705447256	mathematical optimization;combinatorics;discrete mathematics;data structure;computer science;mathematics;upper and lower bounds;programming language;algorithm	Theory	17.24106301453018	27.87461751254497	60989
4244d13113cf200e7a5fa87ead922752051c4e8b	fast parallel graph searching with applications	time complexity.;parallel graph;crcw pram;simd computer;activity network;minimum-weighted paths;breadth-depth search~ minimum-depth search;parallel algorithms;directed acyclic graph;shared memory;time complexity;active network;parallel algorithm	This paper presents fast parallel algorithms for the following graph theoretic problems: breadth-depth search of directed acyclic graphs; minimum-depth search of graphs; finding the minimum-weighted paths between all node-pairs of a weighted graph and the critical activities of an activity-on-edge network. The first algorithm hasO(logdlogn) time complexity withO(n3) processors and the remaining algorithms achieveO(logd loglogn) time bound withO(n2[n/loglogn]) processors, whered is the diameter of the graph or the directed acyclic graph (which also represents an activity-on-edge network) withn nodes. These algorithms work on an unbounded shared memory model of the single instruction stream, multiple data stream computer that allows both read and write conflicts.		Pranay Chaudhuri	1988	BIT		implicit graph;transpose graph;time complexity;graph power;shared memory;active networking;factor-critical graph;combinatorics;feedback arc set;directed graph;graph bandwidth;null graph;clique-width;graph theory;theoretical computer science;simplex graph;comparability graph;aperiodic graph;mathematics;voltage graph;distance-hereditary graph;parallel algorithm;graph;moral graph;butterfly graph;complement graph;directed acyclic graph;line graph;algorithm;search algorithm	HPC	18.186572242764928	28.53669638908784	61148
d398320a61fe4c59e1499f087c0aca223e074fc4	blocking unions of arborescences		Given a digraph D = (V,A) and a positive integer k, a subset B ⊆ A is called a k-unionarborescence, if it is the disjoint union of k spanning arborescences. When also arc-costs c : A → R are given, minimizing the cost of a k-union-arborescence is well-known to be tractable. In this paper we take on the following problem: what is the minimum cardinality of a set of arcs the removal of which destroys every minimum c-cost k-union-arborescence. Actually, the more general weighted problem is also considered, that is, arc weights w : A → R+ (unrelated to c) are also given, and the goal is to find a minimum weight set of arcs the removal of which destroys every minimum c-cost k-union-arborescence. An equivalent version of this problem is where the roots of the arborescences are fixed in advance. In an earlier paper [A. Bernáth and G. Pap, Blocking optimal arborescences, Integer Programming and Combinatorial Optimization, Springer, 2013] we solved this problem for k = 1. This work reports on other partial results on the problem. We solve the case when both c and w are uniform – that is, find a minimum size set of arcs that covers all k-union-arbosercences. Our algorithm runs in polynomial time for this problem. The solution uses a result of [M. Bárász, J. Becker, and A. Frank, An algorithm for source location in directed graphs, Oper. Res. Lett. 33 (2005)] saying that the family of so-called insolid sets (sets with the property that every proper subset has a larger in-degree) satisfies the Helly-property, and thus can be (efficiently) represented as a subtree hypergraph. We also give an algorithm for the case when only c is uniform but w is not. This algorithm is only polynomial if k is not part of the input.	algorithm;cobham's thesis;combinatorial optimization;directed graph;donald becker;file spanning;graph partition;integer programming;minimum weight;polynomial;springer (tank);time complexity;tree (data structure)	Attila Bernáth;Gyula Pap	2015	CoRR		mathematical optimization;combinatorics;mathematics;algorithm	Theory	23.58454176228182	19.29554003959562	61207
60c113c1c2eb11763bf19bcd967c83446b670b39	reassessing top-down join enumeration	dynamic programming;minimal cut;complexity theory;cost function;top down join enumeration;004 informatik;query optimization;minimal cut query optimization join ordering top down join enumeration memoization graph partitioning;join ordering;graph partitioning;data structures;heuristic algorithms;memoization;partitioning algorithms complexity theory data structures heuristic algorithms dynamic programming cost function;buildings;partitioning algorithms	Finding an optimal execution order of join operations is a crucial task in every cost-based query optimizer. Since there are many possible join trees for a given query, the overhead of the join (tree) enumeration algorithm per valid join tree should be minimal. In the case of a clique-shaped query graph, the best known top-down algorithm has a complexity of \Theta (n^2) per join tree, where n is the number of relations. In this paper, we present an algorithm that has an according O(1) complexity in this case. We show experimentally that this more theoretical result has indeed a high impact on the performance in other nonclique settings. This is especially true for cyclic query graphs. Further, we evaluate the performance of our new algorithm and compare it with the best top-down and bottom-up algorithms described in the literature.	algorithm;bottom-up proteomics;experiment;mathematical optimization;overhead (computing);query optimization;top-down and bottom-up design;tree decomposition	Pit Fender;Guido Moerkotte	2012	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2011.235	hash join;recursive join;query optimization;combinatorics;discrete mathematics;memoization;data structure;computer science;graph partition;theoretical computer science;dynamic programming;join dependency;mathematics;programming language;sort-merge join	DB	13.065088865825038	24.82128368557075	61393
da67636156981cfd880ffb55b51cfd1f870275b8	a 2-approximation algorithm for weighted directed hypergraph embedding in a cycle	minimisation;integer linear programming hypergraph embedding;directed graphs;embedding;integer linear programming;approximate algorithm;complexity theory;approximation algorithms;approximation algorithm;linear time algorithm;greedy algorithms;maximal congestion minimization problem;polynomials;greedy algorithms approximation algorithms polynomials application software computer networks embedded computing physics computing routing multicast communication concurrent computing;roads;weighted directed hypergraph cycle embedding;computational complexity;maximal congestion minimization problem approximation algorithm weighted directed hypergraph cycle embedding np complete problem linear time algorithm greedy algorithm;greedy algorithm;minimisation computational complexity directed graphs greedy algorithms;approximation methods;hypergraph;economics;performance ratio;algorithm design and analysis;integer linear program;np complete problem	The problem of weighted directed hypergraph embedding in a cycle (denoted by WDHEC for short) is to embed the weighted directed hyperedges of a hypergraph as the weighted directed paths around a cycle such that maximal congestion-the sum of weight of the paths that use any physical link in the cycle-is minimized. In this paper we show that the WDHEC problem is NP-complete even when each hyperedge contains exactly two vertices. A linear-time algorithm with performance ratio two is presented by a greedy algorithm.	approximation;greedy algorithm;karp's 21 np-complete problems;maximal set;network congestion;time complexity	Qi Wang;Xiuheng Zhao;Xiaowei Zheng;Xiaoshan Liu	2008	2008 Fourth International Conference on Natural Computation	10.1109/ICNC.2008.241	mathematical optimization;combinatorics;discrete mathematics;mathematics	Robotics	23.442460577885942	21.21030131018505	61608
813a97fb3558f2283ce9a3707ac1c1546d9f6756	an optimal cache-oblivious priority queue and its application to graph algorithms	insertion;file attente;delecion;hierarchy;aplicacion;03d55;connaissance;priorite;queue;cache memory;conocimiento;68wxx;antememoria;cache oblivious algorithms;knowledge;antememoire;insercion;priority queue;estructura datos;68r10;jerarquia;68p05;graph algorithm;structure donnee;memory hierarchy;external memory;algorithme graphe;hierarchie;priority;application;prioridad;data structure;fila espera;deletion;memoire cache	We develop an optimal cache-oblivious priority queue data structure, supporting insertion, deletion, and delete-min operations in O( 1 B logM/B N B ) amortized memory transfers, where M and B are the memory and block transfer sizes of any two consecutive levels of a multilevel memory hierarchy. In a cache-oblivious data structure, M and B are not used in the description of the structure. Our structure is as efficient as several previously developed external memory (cache-aware) priority queue data structures, which all rely crucially on knowledge about M and B. Priority queues are a critical component in many of the best known external memory graph algorithms, and using our cache-oblivious priority queue we develop several cache-oblivious graph algorithms.	amortized analysis;cpu cache;cache-oblivious algorithm;graph theory;list of algorithms;maxima and minima;memory hierarchy;oblivious data structure;priority queue	Lars Arge;Michael A. Bender;Erik D. Demaine;Bryan Holland-Minkley;J. Ian Munro	2007	SIAM J. Comput.	10.1137/S0097539703428324	insertion;parallel computing;data structure;double-ended priority queue;computer science;theoretical computer science;distributed computing;knowledge;programming language;queue;priority queue;hierarchy	Theory	17.15893860962976	28.149513833038437	61687
498ffe5ca5fb0fce8c42cd81ae500de9de9bfcb5	an incomplex algorithm for fast suffix array construction	suffix arrays;suffix array;strings;indexing;permutations	Our aim is to provide full text indexing data structures and algorithms for universal usage in text indexing. We present a practical algorithm for suffix array construction. The fundamental algorithm is less complex than other construction algorithms. We achieve very fast construction times for common strings as well as for worst case strings by enhancing our basic algorithms with further techniques.	best, worst and average case;central processing unit;data structure;genetic algorithm;parallel computing;string (computer science);suffix array;time complexity;worst-case complexity	Klaus-Bernd Schürmann;Jens Stoye	2005	Softw., Pract. Exper.	10.1002/spe.768	generalized suffix tree;search engine indexing;longest common substring problem;computer science;theoretical computer science;permutation;compressed suffix array;fm-index;algorithm;string searching algorithm	Theory	12.37996998443324	27.997730717435804	61768
c2449b96fe4d6b0e26680a3e9c173ddc66efb1dc	on the parameterized complexity of the multi-mct and multi-mcst problems	evolutionary history;parameterized complexity;computational complexity;tree structure;w hierarchy;multi mct;multi mcst	The comparison of tree structured data is widespread since trees can be used to represent wide varieties of data, such as XML data, evolutionary histories, or carbohydrate structures. Two graph-theoretical problems used in the comparison of such data are the problems of finding the maximum common subtree (MCT) and the minimum common supertree (MCST) of two trees. These problems generalize to the problem of finding the MCT and MCST of multiple trees (Multi-MCT and Multi-MCST, respectively). In this paper, we prove parameterized complexity hardness results for the different parameterized versions of the Multi-MCT and Multi-MCST problem under isomorphic embeddings.	mobile data terminal;multi-core processor;parameterized complexity	Wenbin Chen;Matthew C. Schmidt;Nagiza F. Samatova	2011	J. Comb. Optim.	10.1007/s10878-009-9220-2	parameterized complexity;mathematical optimization;combinatorics;computer science;theoretical computer science;mathematics;tree structure;computational complexity theory;algorithm	Theory	17.93868102746882	22.036713205555248	62056
8907c81e9e25590812b288ec1213474cfdee7a40	a polynomial-time algorithm for the maximum cardinality cut problem in proper interval graphs	maximum cut;proper interval graph;graph algorithms	It is known that the maximum cardinality cut problem is NP-hard even in chordal graphs. On the positive side, the problem is known to be polynomial time solvable in some subclasses of proper interval graphs which is in turn a subclass of chordal graphs. In this paper, we consider the time complexity of the problem in proper interval graphs, and propose a polynomial-time dynamic programming algorithm.	algorithm;maximum cut;time complexity	Arman Boyaci;Tinaz Ekim;Mordechai Shalom	2017	Inf. Process. Lett.	10.1016/j.ipl.2017.01.007	1-planar graph;block graph;pathwidth;mathematical optimization;maximum cut;split graph;combinatorics;discrete mathematics;cograph;interval graph;graph product;longest path problem;computer science;clique problem;hopcroft–karp algorithm;trivially perfect graph;graph coloring;trapezoid graph;mathematics;maximal independent set;modular decomposition;treewidth;partial k-tree;chordal graph;indifference graph	Theory	24.01290507759652	24.358049586787523	62117
93e6aa92bc937adcab4dc454c5fc70c854b4f792	nesting of prime substructures in k-ary relations	graphe non oriente;graph theory;settore inf 01 informatica;teoria grafo;non directed graph;decomposition;imbrication graphe;systeme modulaire;graph nesting;graphe quotient;decomposition modulaire;linear time algorithm;quotient graph;sistema modular;theorie graphe;congruencia;particion;grafo no orientado;modular decomposition;directed graph;modular system;congruence partition;partition;partition congruence;descomposicion;lower bound;congruence	"""The modular decomposition of a graph or relation has a large number of com-binatorial applications. It divides the structure into a set of \prime"""" induced sub-structures, which cannot be further decomposed. Recent work on graphs and k-ary relations has focused on the discovery that prime induced substructures are densely nested when they occur. Lower bounds on the \nesting density"""" of prime substruc-tures in graphs are used heavily in the only known linear-time algorithm for directed graphs. We improve on the previously known lower bounds for k-ary relations, and show that no further improvement is possible."""	algorithm;directed graph;graph (discrete mathematics);modular decomposition;time complexity	Paola Bonizzoni;Ross M. McConnell	2001	Theor. Comput. Sci.	10.1016/S0304-3975(00)00017-7	partition;combinatorics;discrete mathematics;directed graph;topology;graph theory;congruence;mathematics;modular decomposition;upper and lower bounds;decomposition;algorithm;algebra	Theory	23.2242577119215	30.46628529376053	62159
454e1e5fc369968fc9cbfc6c6b9def9b07701269	the equivalence of sampling and searching	relational problems;extended church turing thesis;function problems;kolmogorov complexity;sampling problems;search problems;algorithmic information theory;quantum computing;fbqp	In a sampling problem, we are given an input x∈{0,1} n , and asked to sample approximately from a probability distribution $\mathcal{D}_{x}$ over $\operatorname{poly} ( n ) $ -bit strings. In a search problem, we are given an input x∈{0,1} n , and asked to find a member of a nonempty set A x with high probability. (An example is finding a Nash equilibrium.) In this paper, we use tools from Kolmogorov complexity to show that sampling and search problems are “essentially equivalent.” More precisely, for any sampling problem S, there exists a search problem R S such that, if $\mathcal{C}$ is any “reasonable” complexity class, then R S is in the search version of $\mathcal{C}$ if and only if S is in the sampling version. What makes this nontrivial is that the same R S works for every  $\mathcal{C}$ . As an application, we prove the surprising result that SampP=SampBQP if and only if FBPP=FBQP. In other words, classical computers can efficiently sample the output distribution of every quantum circuit, if and only if they can efficiently solve every search problem that quantum computers can solve.	complexity class;computer;gibbs sampling;kolmogorov complexity;nash equilibrium;quantum circuit;quantum computing;sampling (signal processing);search problem;turing completeness;with high probability	Scott Aaronson	2010	Theory of Computing Systems	10.1007/s00224-013-9527-3	combinatorics;discrete mathematics;computer science;mathematics;quantum computer;algorithm;algorithmic information theory	Theory	10.455033390339207	20.86901865562631	62169
028e8de22b1cef8f85ced2a38e668c9501a6bc26	approximation algorithms for k-source bottleneck routing cost spanning tree problems	metric graph;approximate algorithm;source routing;objective function;spanning tree;polynomial time approximation scheme	In this paper, we investigate two spanning tree problems of graphs with k given sources. Let G = (V, E, w) be an undirected graph with nonnegative edge lengths and S ⊂ V a set of k specified sources. The first problem is the k-source bottleneck vertex routing cost spanning tree (k-BVRT) problem, in which we want to find a spanning tree T such that the maximum total distance from any vertex to all sources is minimized, i.e., we want to minimize maxv∈V {∑ s∈S dT (s, v) } , in which dT (s, v) is the length of the path between s and v on T . The other problem is the k-source bottleneck source routing cost spanning tree (k-BSRT) problem, in which the objective function is the maximum total distance from any source to all vertices, i.e., maxs∈S {∑ v∈V dT (s, v) } . In this paper, we present a polynomial time approximation scheme (PTAS) for the 2-BVRT problem. For the 2-BSRT problem, we first give (2 + ε)approximation algorithm for any ε > 0, and then present a PTAS for the case that the input graphs are restricted to metric graphs. Finally we show that there is a simple 3-approximation algorithm for both the two problems with arbitrary k.	approximation algorithm;arjen lenstra;chao (sonic);dijkstra's algorithm;emoticon;file spanning;gene kan;graph (discrete mathematics);jim hall (programmer);journal of interconnection networks;loss function;michael garey;multi-source;np-completeness;network planning and design;optimization problem;p (complexity);ptas reduction;polynomial;polynomial-time approximation scheme;regular expression;siam journal on computing;shortest path problem;source routing;spanning tree;time complexity	Yen Hung Chen;Bang Ye Wu;Chuan Yi Tang	2004		10.1007/978-3-540-24767-8_37	euclidean minimum spanning tree;mathematical optimization;combinatorics;discrete mathematics;source routing;kruskal's algorithm;polynomial-time approximation scheme;spanning tree;prim's algorithm;computer science;minimum spanning tree;connected dominating set;k-minimum spanning tree;reverse-delete algorithm;distributed minimum spanning tree	Theory	23.39248072453208	19.018375678654287	62508
b5affe208f7c675b9b4219fd4d5571d4069d6891	comparing first-fit and next-fit for online edge coloring	edge coloring;online algorithm;relative worst order ratio;algorithm performance;algorithme en ligne;color;algoritmo en linea;algorithm en ligne;resultado algoritmo;informatique theorique;68r10;performance algorithme;couleur;online algorithms;next fit;analyse competitive;competitive ratio;computer theory;first fit;informatica teorica	We study the performance of the algorithms First-Fit and Next-Fit for two online edge coloring problems. In the min-coloring problem, all edges must be colored using as few colors as possible. In the max-coloring problem, a fixed number of colors is given, and as many edges as possible should be colored. Previous analysis using the competitive ratio has not separated the performance of First-Fit and Next-Fit, but intuition suggests that First-Fit should be better than Next-Fit. We compare First-Fit and Next-Fit using the relative worst order ratio, and show that First-Fit is better than Next-Fit for the min-coloring problem. For the max-coloring problem, we show that First-Fit and Next-Fit are not strictly comparable, i.e., there are graphs for which First-Fit is better than Next-Fit and graphs where Next-Fit is slightly better than First-Fit.	algorithmica;amortized analysis;anna karlin;bin packing problem;cache (computing);color;communications of the acm;competitive analysis (online algorithm);edge coloring;graph coloring;maxima and minima;occam's razor;p (complexity);paging;randomized algorithm;set packing;symposium on discrete algorithms	Martin R. Ehmsen;Lene M. Favrholdt;Jens S. Kohrt;Rodica Mihai	2010	Theor. Comput. Sci.	10.1016/j.tcs.2010.01.015	online algorithm;combinatorics;computer science;mathematics;geometry;algorithm	Theory	22.965743067944416	24.71796558441373	62703
cbabf0b2d060c7f6f8d62be05cda445713f633fe	fast recognition of the nilpotency of permutation groups	permutation group	Let G be a subgroup of the symmetric group on n points, given by a set of generators, S. We describe algorithms that decide whether or not G is nilpotent, and whether or not G is a p-group for some prime p. The algorithms utilize the imprimitivity structure of such groups. The running time of the algorithms is O(log n a(n, 41S/n)), where Q(Z, y) denotes the time required for z Union and y Find operations in a Union-Find data structure, the asymptotically best implementation of which runs in time o(~ log* (x + y)). Standard methods for answering the question require the computation of a point stabilizer series, that takes o(lslnz + ns) time. Implementation of the algorithms in GAP shows that these algorithms run faster than the built-in ones, except for very small cases. For the nilpotence test this is the case even if we just measure the time after the computation of the point stabi~lzer series.	algorithm;computation;disjoint-set data structure;time complexity	Ferenc Rakoczi	1995		10.1145/220346.220380	parity of a permutation;mathematics;permutation group;base;cyclic permutation;algebra	Theory	16.493636325663726	30.381373581208855	62782
b1528a02435701887ab884715befd41db2baf2fc	storing a compressed function with constant time access	false positive rate;random access;information theory	We consider the problem of representing, in a space-efficient way, a function f : S → Σ such that any function value can be computed in constant time on a RAM. Specifically, our aim is to achieve space usage close to the 0th order entropy of the sequence of function values. Our technique works for any set S of machine words, without storing S, which is crucial for applications. Our contribution consists of two new techniques, of independent interest, that we use in combination with an existing result of Dietzfelbinger and Pagh (ICALP 2008). First of all, we introduce a way to support more space efficient approximate membership queries (Bloom filter functionality) with arbitrary false positive rate. Second, we present a variation of Huffman coding using approximate membership, providing an alternative that improves the classical bounds of Gallager (IEEE Trans. Information Theory, 1978) in some cases. The end result is an entropy-compressed function supporting constant time random access to values associated with a given set S. This improves both space and time compared to a recent result by Talbot and Talbot (ANALCO 2008).	approximation algorithm;bloom filter;huffman coding;icalp;information theory;loss function;random access;random-access memory;space–time tradeoff;time complexity	Jóhannes B. Hreinsson;Morten Krøyer;Rasmus Pagh	2009		10.1007/978-3-642-04128-0_65	mathematical optimization;combinatorics;discrete mathematics;information theory;false positive rate;computer science;theoretical computer science;mathematics;algorithm;random access;statistics	Theory	11.804825058111401	25.962725619571103	62928
1c48e327b4b3a51d235a9aa764636a4348126ec5	all maximal independent sets and dynamic dominance for sparse graphs	dominating set;polynomial time;maximal independent set;data structure	We describe algorithms, based on Avis and Fukuda's reverse search paradigm, for listing all maximal independent sets in a sparse graph in polynomial time and delay per output. For bounded degree graphs, our algorithms take constant time per set generated; for minor-closed graph families, the time is O(n) per set, and for more general sparse graph families we achieve subquadratic time per set. We also describe new data structures for maintaining a dynamic vertex set S in a sparse or minor-closed graph family, and querying the number of vertices not dominated by S; for minor-closed graph families the time per update is constant, while it is sublinear for any sparse graph family. We can also maintain a dynamic vertex set in an arbitrary m-edge graph and test the independence of the maintained set in time O(√m) per update. We use the domination data structures as part of our enumeration algorithms.	algorithm;data structure;dominating set;graph minor;line graph;maximal independent set;maximal set;polynomial;programming paradigm;sparse graph code;sparse matrix;time complexity;vertex (graph theory)	David Eppstein	2005	CoRR		1-planar graph;claw-free graph;time complexity;graph power;pathwidth;split graph;combinatorics;discrete mathematics;independent set;topology;graph bandwidth;data structure;bipartite graph;dominating set;degree;computer science;cycle graph;cubic graph;mathematics;voltage graph;distance-hereditary graph;graph;maximal independent set;complement graph;line graph;circulant graph;coxeter graph	Theory	23.73646048885655	26.985020118859143	62965
1269234e10b86eaa3f4d025efdbfba14f1f1ea6d	approaching the 5/4-approximation for rectilinear steiner trees	efficient algorithm;satisfiability;steiner tree problem;performance ratio;steiner tree	The rectilinear Steiner tree problem requires a shortest tree spanning a given vertex subset in the plane with rectilinear distance. It was proved that the output length of Zelikovsky's 25] and Berman/Ramaiyer 3] heuristics is at most 1.375 and 97 72 1:347 of the optimal length, respectively. It was claimed that these bounds are not tight. Here we improve these bounds to 1.3125 and 61 48 1:271, respectively, and give eecient algorithms to nd approximations of such quality. We also prove that for Zelikovsky's heuristic this bound cannot be less than 1.3.	approximation algorithm;berman–hartmanis conjecture;binary logarithm;file spanning;heuristic (computer science);rectilinear steiner tree;regular grid;steiner tree problem;taxicab geometry	Piotr Berman;Ulrich Fößmeier;Marek Karpinski;Michael Kaufmann;Alex Zelikovsky	1994		10.1007/BFb0049397	mathematical optimization;combinatorics;discrete mathematics;steiner tree problem;computer science;mathematics;algorithm	Theory	23.96243734695881	20.741873083577314	63010
2568c49193578c2048d3a440271bfa140f4c0707	data partitioning over data streams based on change-aware sampling	streaming algorithms;epsilon nets;streaming algorithm;data stream;range counting;least median of squares;robust statistics;iceberg queries;motion estimation;probabilistic approach;data streams;sampling;geometric data;problem complexity;robust regression;lower bound	We present memory-efficient deterministic algorithms for constructing ε-nets and ε-approximations of streams of geometric data. Unlike probabilistic approaches, these deterministic samples provide guaranteed bounds on their approximation factors. We show how our deterministic samples can be used to answer approximate online iceberg geometric queries on data streams. We use these techniques to approximate several robust statistics of geometric data streams, including Tukey depth, simplicial depth, regression depth, the Thiel-Sen estimator, and the least median of squares. Our algorithms use only a polylogarithmic amount of memory, provided the desired approximation factors are at least inverse-polylogarithmic. We also include a lower bound for noniceberg geometric queries.	approximation algorithm;polylogarithmic function;sampling (signal processing)	Amitabha Bagchi;Amitabh Chaudhary;David Eppstein;Michael T. Goodrich	2005	IEEE International Conference on e-Business Engineering (ICEBE'05)	10.1145/1240233.1240239	mathematical optimization;computer science;theoretical computer science;machine learning;mathematics;streaming algorithm;data stream mining;statistics	DB	13.664901996830253	23.989623713111616	63029
1c672fba3403498fff3f34974975e90b7a09ced6	local dependency dynamic programming in the presence of memory faults	unreliable memories fault tolerant algorithms local dependency dynamic programming edit distance;004;unreliable memories;edit distance;dynamic program;fault tolerant algorithms;local dependency dynamic programming	We investigate the design of dynamic programming algorithms in unreliable memories, i.e., in the presence of faults that may arbitrarily corrupt memory locations during the algorithm execution. As a main result, we devise a general resilient framework that can be applied to all local dependency dynamic programming problems, where updates to entries in the auxiliary table are determined by the contents of neighboring cells. Consider, as an example, the computation of the edit distance between two strings of length n and m. We prove that, for any arbitrarily small constant ε ∈ (0, 1] and n ≥ m, this problem can be solved correctly with high probability in O ( nm+ αδ1+ε ) worst-case time and O(nm + nδ) space, when up to δ memory faults can be inserted by an adversary with unbounded computational power and α ≤ δ is the actual number of faults occurring during the computation. We also show that an optimal edit sequence can be constructed in additional time O ( nδ + αδ1+ε ) . It follows that our resilient algorithms match the running time and space usage of the standard non-resilient implementations while tolerating almost linearly-many faults. 1998 ACM Subject Classification B.8 [Performance and reliability]; F.2 [Analysis of algorithms and problem complexity]; I.2.8 [Dynamic programming].	adversary (cryptography);algorithm;analysis of algorithms;best, worst and average case;computation;dynamic programming;edit distance;space–time tradeoff;time complexity;with high probability	Saverio Caminiti;Irene Finocchi;Emanuele G. Fusco	2011		10.4230/LIPIcs.STACS.2011.45	real-time computing;edit distance;computer science;wagner–fischer algorithm;theoretical computer science;mathematics;algorithm	Theory	10.86868197270201	29.52578008699544	63155
2cab3b7c0ae184f5c6585cedd4377131f1814a40	structural pattern matching - succinctly		Let T be a text of length n containing characters from an alphabet Σ, which is the union of two disjoint sets: Σs containing static characters (s-characters) and Σp containing parameterized characters (p-characters). Each character in Σp has an associated complementary character from Σp. A pattern P (also over Σ) matches an equal-length substring S of T iff the s-characters match exactly, there exists a one-to-one function that renames the p-characters in S to the p-characters in P , and if a p-character x is renamed to another p-character y then the complement of x is renamed to the complement of y. The task is to find the starting positions (occurrences) of all such substrings S. Previous indexing solution [Shibuya, SWAT 2000], known as Structural Suffix Tree, requires Θ(n logn) bits of space, and can find all occ occurrences in time O(|P | log σ+occ), where σ = |Σ|. In this paper, we present the first succinct index for this problem, which occupies n log σ +O(n) bits and offers O(|P | log σ + occ · logn log σ) query time. 1998 ACM Subject Classification F.2.2 Pattern Matching	complement (complexity);one-to-one (data model);pattern matching;swat;structural pattern;substring;suffix tree	Arnab Ganguly;Rahul Shah;Sharma V. Thankachan	2017		10.4230/LIPIcs.ISAAC.2017.35	sigma;combinatorics;suffix tree;discrete mathematics;computer science;parameterized complexity;binary logarithm;pattern matching;existential quantification;substring;disjoint sets	Theory	13.462721722478463	26.443195138939153	63423
93051096d1b7f13c7bcd704f79269563e9a56db8	the clique problem on inductive $k$-independent graphs		A graph is inductive k-independent if there exists an ordering of its vertices v1, ..., vn such that α(G[N(vi)∩Vi]) ≤ k where N(vi) is the neighbourhood of vi, Vi = {vi, ..., vn} and α is the independence number. In this article we design a polynomial time approximation algorithm with ratio ∆/ log(log(∆)/(k+1)) for the maximum clique problem and also show that the decision version of this problem is fixed parameter tractable, with parameter p, the size of the clique, for this particular family of graphs, with complexity O(p(p + k − 1)n). Then we study a subclass of inductive k-independent graphs, namely k-degenerate graphs. A graph is k-degenerate if any induced subgraph has a vertex of degree at most k. Our contribution is an algorithm computing a maximum clique for this class of graphs in time O((n− k)f(k)), where f(k) is the time complexity of computing a maximum clique in a graph of order k, thus improving the previous best result. We also prove some structural properties for inductive k-independent graphs.	approximation algorithm;clique (graph theory);clique problem;cobham's thesis;decision problem;independent set (graph theory);induced subgraph;inductive reasoning;neighbourhood (graph theory);polynomial;time complexity	George Manoussakis	2014	CoRR		combinatorics;discrete mathematics;mathematics;algorithm	Theory	22.68803124771569	23.715010910435332	63467
00ab49aeec864f804309224ca5ee72405c8dbd59	a linear time algorithm for the lowest common ancestors problem	random access memory;mathematics;heart;complexity theory;approximation algorithms;bismuth;linear time algorithm;binary trees;tree graphs;random access machine;computational modeling;tuning;indexing;heuristic algorithms;timing optimization;read write memory computer science indexing algorithm design and analysis computational modeling tree graphs mathematics binary trees;transforms;merging;lowest common ancestor;computer science;read write memory;algorithm design and analysis;buildings;partitioning algorithms	We investigate two lowest common ancestor (LCA) problems on trees. We give a linear time algorithm for the off-line problem, on a random access machine (RAM). The half-line problem is one in which LCA queries on a fixed tree are arriving on line. We extend our RAM algorithm to answer each individual query in 0(1) time, with 0(n) preprocessing time. Tarjan observed that this result helps to explicate the difference in power between RAM and pointer machines. We also show how to modify our algorithm to achieve a linear preprocessing time, optimal query time, algorithm on a reference machine.	algorithm;lowest common ancestor;online and offline;pointer (computer programming);pointer machine;preprocessor;random access;random-access machine;random-access memory;time complexity	Dov Harel	1980	21st Annual Symposium on Foundations of Computer Science (sfcs 1980)	10.1109/SFCS.1980.6	algorithm design;search engine indexing;combinatorics;binary tree;computer science;theoretical computer science;machine learning;bismuth;mathematics;computational model;tree;approximation algorithm;heart;algorithm;lowest common ancestor	Theory	13.707120316858802	28.091686833142866	63520
71d766cd0c4e4e61b5b2bf2a1f76a91ef1f860b8	reporting exact and approximate regular expression matches	combinatorial problem;probleme combinatoire;analyse syntaxique;problema combinatorio;analisis sintaxico;pattern matching;syntactic analysis;approximate matching;appariement chaine;concordance forme;string matching;algoritmo optimo;algorithme optimal;optimal algorithm;regular expression	"""While much work has been done on determining if a document or a line of a document contains an exact or approximate match to a regular expression, less e ort has been expended in formulating and determining what to report as \the match"""" once such a \hit"""" is detected. For exact regular expression pattern matching, we give algorithms for nding a longest match, all symbols involved in some match, and nding optimal submatches to tagged parts of a pattern. For approximate regular expression matching, we develop notions of what constitutes a signi cant match, give algorithms for them, and also for nding a longest match and all symbols in a match."""	approximation algorithm;open road tolling;pattern matching;regular expression	Eugene W. Myers;Paulo Oliva;Katia S. Guimarães	1998		10.1007/BFb0030783	combinatorics;discrete mathematics;approximate string matching;computer science;parsing;pattern matching;mathematics;programming language;regular expression;algorithm;string searching algorithm	Theory	14.417002164360602	26.58711984766216	63624
9260ea1b6f46b8539ef5b33504fabdb5b033e4ba	an effective algorithm for computing all-terminal reliability bounds	sum of disjoint products;all terminal network reliability;bounds computation;network availability;binary decision diagram	Received March 2015; accepted August 2015 Correspondence to: T. Gomes; e-mail: teresa@deec.uc.pt Contract grant sponsor: ICIS Project CENTRO-07-0224-FEDER-002003 (to J.S.) Contract grant sponsor: Portuguese Foundation for Science and Technology (to T.G. and L.M.); Contract grant numbers: UID/MULTI/00308/2013 and ICIS Project CENTRO-07-0224-FEDER-002003 DOI 10.1002/net.21634 Published online 31 August 2015 in Wiley Online Library (wileyonlinelibrary.com). © 2015 Wiley Periodicals, Inc. approach results from the fact that it does not require the enumeration of all mincuts or all minpaths as required by other bounds. The proposed algorithm uses maximally disjoint minpaths, prior to their sequential generation, and also uses a binary decision diagram for the calculation of their union probability. The numerical results show that the proposed approach is computationally feasible, reasonably accurate and much faster than the previous version of the algorithm. This allows one to obtain tight bounds when it not possible to enumerate all mincuts or all minpaths as revealed by extensive tests on real-world networks. © 2015 Wiley Periodicals, Inc. NETWORKS, Vol. 66(4), 282–295 2015	algorithm;binary decision diagram;email;enumerated type;file spanning;icis;influence diagram;item unique identification;john d. wiley;minimum spanning tree;numerical analysis;sandy bridge;time complexity	Jaime Silva;Teresa Gomes;David Tipper;Lúcia Martins;Velin Kounev	2015	Networks	10.1002/net.21634	mathematical optimization;combinatorics;discrete mathematics;computer science;mathematics;binary decision diagram;algorithm	AI	20.06818737786156	19.10402288793518	63667
86bf620e7a8d8e8d070ddaf34efde8e53f3f6826	parameterized counting matching and packing: a family of hard problems that admit fptras			set packing	Yunlong Liu;Shaokai Wang;Jianxing Wang	2018	Theor. Comput. Sci.	10.1016/j.tcs.2017.09.022		ECom	24.11290969466299	24.032880101358664	63939
90c6059e41b9b2057f9eefe5f26c65f196d097e1	longest common factor after one edit operation		It is well known that the longest common factor (LCF) of two strings over an integer alphabet can be computed in time linear in the total length of the two strings. Our aim here is to present an algorithm that preprocesses two strings S and T in order to answer the following type of queries: Given a position i on S and a letter (alpha ), return an LCF of T and (Su0027), where (Su0027) is the string resulting from S after substituting S[i] with (alpha ). In what follows, we present an algorithm that, given two strings of length at most n, constructs in (mathcal {O}(n log ^4 n)) expected time a data structure of (mathcal {O}(nlog ^3 n)) space that answers such queries in (mathcal {O}(log ^3 n)) time per query. After some trivial modifications, our approach can also support the case of single letter insertions or deletions in S.		Amihood Amir;Panagiotis Charalampopoulos;Costas S. Iliopoulos;Solon P. Pissis;Jakub Radoszewski	2017		10.1007/978-3-319-67428-5_2	suffix tree;heavy path decomposition;computer science;combinatorics;alphabet;integer;data structure	Crypto	13.412503256441314	27.01853592259363	64020
9939ffb24499e9c8bdc7659f3a6ace492e6af99e	a randomized parallel algorithm for planar graph isomorphism	graph theory;parallel algorithm;isomorphic graph;randomised algorithms;algorithme randomise;probability of failure;theorie graphe;algorithme parallele;graph isomorphism;grafo isomorfo;bfs trees;graphe planaire;randomized algorithm;graphe isomorphe;grafo planario;planar graphs;parallel processing;planar graph;parallel algorithms	We present a parallel randomized algorithm running on aCRCW PRAM, to determine whether two planar graphs are isomorphic, and if so to find the isomorphism. We assume that we have a tree of separators for each planar graph (which can be computed by known algorithms inO(log2n) time withn1+?processors, for any ?0). Ifnis the number of vertices, our algorithm takesO(log(n)) time with processors and with a probability of failure of 1/nat most. The algorithm needs 2·log(m)?log(n)+O(log(n)) random bits. The number of random bits can be decreased toO(log(n)) by increasing the number of processors ton3/2+?, for any ?0. Our parallel algorithm has significantly improved processor efficiency, compared to the previous logarithmic time parallel algorithm of Miller and Reif (Siam J. Comput.20(1991), 1128?1147), which requiresn4randomized processors orn5deterministic processors.	graph isomorphism;parallel algorithm;planar graph;randomized algorithm	Hillel Gazit;John H. Reif	1998	J. Algorithms	10.1006/jagm.1998.0943	parallel processing;mathematical optimization;combinatorics;discrete mathematics;graph theory;theoretical computer science;mathematics;parallel algorithm;graph isomorphism;algorithm;planar graph	Theory	18.15158574613685	28.358682713815522	64103
1643b21a6f844112bbb8bd5742dafaa2a5bd835b	a fast filtration algorithm for the substring matching problem	sequence comparison;approximate pattern matching;approximate string matching;oligonucleotide probe;molecular biology	Given a text of length n and a query of length q we p r w n t an dgorithm for finding dl locations of m-tupiu in the Cut m d ia the query that differ by at most t mismatches. Thia problem ia motivated by the dot-matrix constructions for q u e n c e comparison and optimal oligonodcotide probe selection routinely used in molecolu biology. In the c~de q = m the problem coindda with the classical apprwzirnate string mulching with k rniamatches problem. We pr-t a new approach to thk problem b w d on multiple filtration which may have dnntagca over wme rophistieated and theoretically efficient math& that hare been proposed. This paper describes a t-stye procers. The h t r tqe (mdtipk filtration) uses a new technique to prwlect roughly rimiLr m-tuplrs. The m u d rtage c o m p u a these m-tuples wing .IL accurate method. We demonstrate the advantages of multiple filtr&ioa in comparison with other techniques for approximate pattern matching. '	approximation algorithm;dot matrix;pattern matching;stochastic process;substring	Pavel A. Pevzner;Michael S. Waterman	1993		10.1007/BFb0029806	combinatorics;approximate string matching;commentz-walter algorithm;computer science;bioinformatics;oligomer restriction;theoretical computer science;mathematics;programming language;bitap algorithm	Theory	14.254255515706392	25.550150980779513	64108
aef5294bdcf1d1cec2d45b065d4e60852aa89b5e	fast algorithms for the elimination of common subexpressions	fast algorithm	We give two algorithms for computing the set of available expressions at entrance to the nodes of a flow graph. The first takes O(mn) isteps on a program flow graph (one in which no node has more than two successors), where n is the number of nodes and m the number of expressions which are ever computed. A modified version of this algorithm requires O(n 2) steps of an extended type, where bit vector operations are regarded as one step. We present another algorithm which works only for reducible flow graphs. It requires O(n log n) extended steps.	algorithm;bit array;control flow;interval (graph theory)	Jeffrey D. Ullman	1973	Acta Informatica	10.1007/BF00289078	combinatorics;computer science;theoretical computer science;mathematics;algorithm	Theory	15.749416949971053	29.195861276484077	64117
42a77a61499516a4a8805282aedae586b28c2209	advice lower bounds for the dense model theorem	004;pseudorandomness advice lower bounds dense model theorem	"""We prove a lower bound on the amount of nonuniform advice needed by black-box reductions for the Dense Model Theorem of Green, Tao, and Ziegler, and of Reingold, Trevisan, Tulsiani, and Vadhan. The latter theorem roughly says that for every distribution D that is delta-dense in a distribution that is epsilon'-indistinguishable from uniform, there exists a """"dense model"""" for D, that is, a distribution that is delta-dense in the uniform distribution and is epsilon-indistinguishable from D. This epsilon-indistinguishability is with respect to an arbitrary small class of functions F. For the natural case where epsilon' >=  Omega(epsilon delta) and epsilon >= delta^{O(1)}, our lower bound implies that Omega(sqrt{(1/epsilon)log(1/delta)} log|F|) advice bits are necessary. There is only a polynomial gap between our lower bound and the best upper bound for this case (due to Zhang), which is O((1/epsilon^2)log(1/delta) log|F|). Our lower bound can be viewed as an analog of list size lower bounds for list-decoding of error-correcting codes, but for """"dense model decoding"""" instead. Our proof introduces some new techniques which may be of independent interest, including an analysis of a majority of majorities of p-biased bits. The latter analysis uses an extremely tight lower bound on the tail of the binomial distribution, which we could not find in the literature."""		Thomas Watson	2013		10.4230/LIPIcs.STACS.2013.634	combinatorics;discrete mathematics;computer science;calculus;mathematics	Theory	11.12812539631699	21.47612977108825	64222
761ef6ac845a2869a678fc71707cd074eb295d3a	the cpbt: a method for searching the prefixes using coded prefixes in b-tree	lmp;insert;lpm;memory access;smv;binary search tree;ip routing;lookup;delete	"""Due to the increasing size of IP routing table and the growing rate of their lookups, many algorithms are introduced to achieve the required speed in table search and update or optimizing the required memory size. Many of these algorithms are suitable for IPv4 but cannot be used for IPv6. Nevertheless new efforts are going on to fit the available algorithms and techniques to the IPv6 128 bits addresses and prefixes. The challenging issues in the design of these prefix search structures are minimizing the worst case memory access, processing and pre-processing time for search and update procedures, e.g. Binary Tries have a worst case performance of O(32) memory accesses for IPv4 and O(128) for IPv6. Other compressed Tries have better worst case lookup performance however their update performance is degraded. One of the proposed schemes to make the lookup algorithm independent of IP address length is to use a binary search tree and keep the prefix endpoints within its nodes. Some new methods are introduced based on this idea such as """"Prefixes in B-Tree"""", PIBT. But, they need up to 2n nodes for n prefixes. This paper proposes """"Coded Prefixes in B-Tree"""", the CPBT. In which, prefixes are coded to make them scalar and therefore suitable as ordinary B-tree keys, without the need for additional node complexity which PIBT requires. We will finally compare the CPBT method with the recent PIBT method and show that although both of them have the same search complexity; this scheme has much better storage requirements and about half of the memory accesses during the update procedures."""	algorithm;b-tree;best, worst and average case;lookup table;newton's method;preprocessor;requirement;routing table;search tree	Mohammad Behdadfar;Hossein Saidi	2008		10.1007/978-3-540-79549-0_49	binary search tree;computer science;theoretical computer science;database;algorithm;insert	Networks	11.394222520669691	28.75167109358836	64259
0e4d078d74deb76668fd03d89383e5158bb8b934	a constructive algorithm for realizing a distance matrix	camino mas corto;shortest path;matrice distance;reduction;heuristic method;plus court chemin;metrics;metric;metodo heuristico;websearch;matriz simetrica;symmetric matrix;graphe pondere;grafo pondero;metrico;matrice symetrique;total length;heuristics;methode heuristique;weighted graph;articles scientifiques;arodes;distance matrix;metrique	The natural metric of a weighted graph is the length of the shortest paths between all pairs of vertices. The investigated problem consists in a representation of a given metric by a graph, such that the total length of the graph is minimized. For that purpose, we give a constructive algorithm based on a technique of reduction, fusion and deletion. We then show some results on a set of various distance matrices whose optimal realization is known.	algorithm;distance matrix;graph (discrete mathematics);maxima and minima;shortest path problem	Sacha C. Varone	2006	European Journal of Operational Research	10.1016/j.ejor.2005.02.071	graph power;resistance distance;combinatorics;discrete mathematics;directed graph;topology;graph bandwidth;distance matrix;level structure;graph center;reduction;metric;null graph;floyd–warshall algorithm;metric k-center;median graph;distance-regular graph;heuristics;johnson's algorithm;mathematics;voltage graph;path;shortest path problem;random geometric graph;distance;quartic graph;complement graph;metrics;strength of a graph;symmetric matrix	Theory	22.235323019338086	28.69891075877851	64426
03546521b86b695f4de8ace0a5d70a685619a22c	monotone circuits for matching require linear depth	monotone computation;graphe biparti;complexite calcul;complejidad calculo;limite inferior;grafo bipartido;fonction monotone;circuit adaptation;sistema informatico;matching circuit;perfect matching;graph clique;computer system;computing complexity;circuit depth;funcion monotona;graphe communication;informatique theorique;monotonic function;circuito adaptacion;communication graph;systeme informatique;clique graphe;bipartite graph;limite inferieure;lower bound;grafo comunicacion;computer theory;informatica teorica;perfect match	It is proven that monotone circuits computing the perfect matching function on <italic>n</italic>-vertex graphs require &OHgr;(<italic>n</italic>) depth. This implies an exponential gap between the depth of monotone and nonmonotone circuits.	matching (graph theory);time complexity;monotone	Ran Raz;Avi Wigderson	1992	J. ACM	10.1145/146637.146684	clique;combinatorics;discrete mathematics;bipartite graph;monotonic function;mathematics;upper and lower bounds;algorithm;matching	Theory	20.941502964639447	26.74996133597056	64560
3ffa496665d0ff82677642d1d22f6dc94e59287f	an improved approximation algorithm for knapsack median using sparsification		Knapsack median is a generalization of the classic k-median problem in which we replace the cardinality constraint with a knapsack constraint. It is currently known to be 32-approximable. We improve on the best known algorithms in several ways, including adding randomization and applying sparsification as a preprocessing Preliminary version appeared in Proceedings of ESA 2015. Jarosław Byrka supported by NCN Grants DEC-2012/07/B/ST6/01534 and 2015/18/E/ST6/00456. Thomas Pensyl and Khoa Trinh supported in part by NSF Awards CNS 1010789 and CCF 1422569. Bartosz Rybicki supported by NCN 2012/07/N/ST6/03068 Grant. Aravind Srinivasan supported in part by NSF Awards CNS 1010789 and CCF 1422569, and by a research award from Adobe, Inc. B Aravind Srinivasan srin@cs.umd.edu Jarosław Byrka jby@cs.uni.wroc.pl Thomas Pensyl tpensyl@cs.umd.edu Bartosz Rybicki bry@cs.uni.wroc.pl Joachim Spoerhase joachim.spoerhase@uni-wuerzburg.de Khoa Trinh khoa@cs.umd.edu 1 Institute of Computer Science, University of Wroclaw, Wrocław, Poland 2 Department of Computer Science, University of Maryland, College Park, MD 20742, USA 3 Department of Computer Science and Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742, USA	approximation algorithm;cns;cardinality (data modeling);computer science;constraint logic programming;esa;ibm notes;k-medians clustering;knapsack problem;microsoft customer care framework;molecular dynamics;preprocessor	Jaroslaw Byrka;Thomas Pensyl;Bartosz Rybicki;Joachim Spoerhase;Aravind Srinivasan;Khoa Trinh	2015		10.1007/978-3-662-48350-3_24	mathematical optimization;combinatorics;polynomial-time approximation scheme;machine learning	Theory	21.134111172206005	19.862671665985623	64749
297cf4c0fdba1d8a4b9be57a5519fead79f1b01b	lower bounds for swapping arthur and merlin	random access machine;linear time;polynomial time;lower bound	We prove a lower bound for swapping the order of Arthur and Merlin in two-round MerlinArthur games using black-box techniques. Namely, we show that any AM-game requires time Ω(t) to black-box simulate MA-games running in time t. Thus, the known simulations of MA by AM with quadratic overhead, dating back to Babai’s original paper on Arthur-Merlin games, are tight within this setting. The black-box lower bound also yields an oracle relative to which MA-TIME[n] * AM-TIME[o(n)]. Complementing our lower bounds for swapping Merlin in MA-games, we prove a time-space lower bound for simulations that drop Merlin entirely. We show that for any c < √ 2, there exists a positive d such that there is a language recognized by linear-time MA-games with one-sided error but not by probabilistic random-access machines with two-sided error that run in time n and space n. This improves recent results that give such lower bounds for problems in the second level of the polynomial-time hierarchy. This paper is the full version of an extended abstract to appear at RANDOM 2007. Supported by NSF Career award CCR-0133693 and Cisco Systems Distinguished Graduate Fellowship.	am broadcasting;arthur–merlin protocol;black box;hot swapping;ibm notes;monte carlo algorithm;overhead (computing);paging;polynomial hierarchy;random access;simulation;time complexity	Scott Diehl	2007		10.1007/978-3-540-74208-1_33	combinatorics;computer science;artificial intelligence;algorithm	Theory	12.34764402322108	22.34047899460335	64762
08bb4daf7d947406d9124c3ae3938172476b3a37	a distribution-sensitive dictionary with low space overhead	distribution sensitivity;dictionary problem;space overhead;working set property	The time required for a sequence of operations on a data structure is usually measured in terms of the worst possible such sequence. This, however, is often an overestimate of the actual time required. Distribution-sensitive data structures attempt to take advantage of underlying patterns in a sequence of operations in order to reduce time complexity, since access patterns are non-random in many applications. Many of the distribution-sensitive structures in the literature require a great deal of space overhead in the form of pointers. We present a dictionary data structure that makes use of both randomization and existing space-efficient data structures to yield low space overhead while maintaining distribution sensitivity in the expected sense. We further show a modification that allows predecessor searches in a similar time bound.	dictionary;overhead (computing)	Prosenjit Bose;John Howat;Pat Morin	2012	J. Discrete Algorithms	10.1016/j.jda.2011.11.003	combinatorics;real-time computing;computer science;theoretical computer science;overhead;algorithm	Theory	10.889439788478791	29.350055687153418	64861
3c3034fcead0f3a56475c0217be1e3d10fbf0db9	a space efficient streaming algorithm for triangle counting using the birthday paradox	streaming algorithms;triangle counting	We design a space efficient algorithm that approximates the transitivity (global clustering coefficient) and total triangle count with only a single pass through a graph given as a stream of edges. Our procedure is based on the classic probabilistic result, the birthday paradox. When the transitivity is constant and there are more edges than wedges (common properties for social networks), we can prove that our algorithm requires O(√n) space (n is the number of vertices) to provide accurate estimates. We run a detailed set of experiments on a variety of real graphs and demonstrate that the memory requirement of the algorithm is a tiny fraction of the graph. For example, even for a graph with 200 million edges, our algorithm stores just 60,000 edges to give accurate results. Being a single pass streaming algorithm, our procedure also maintains a real-time estimate of the transitivity/number of triangles of a graph, by storing a miniscule fraction of edges.	clustering coefficient;epr paradox;experiment;graph (discrete mathematics);real-time clock;social network;streaming algorithm;vertex-transitive graph	Madhav Jha;Seshadhri Comandur;Ali Pinar	2013		10.1145/2487575.2487678	mathematical optimization;combinatorics;discrete mathematics;topological graph;multiple edges;computer science;pseudoforest;hopcroft–karp algorithm;mixed graph;multigraph;hypercube graph;cycle graph;mathematics;streaming algorithm;path;complement graph;algorithm;strength of a graph;matching	ML	20.554997778331867	23.250806994607853	65476
2b829ebc9676b043e1dd1f6163b19e3674b274d4	depth-first search and the vertex cover problem	vertex cover;depth first search		depth-first search;vertex cover	Carla D. Savage	1982	Inf. Process. Lett.	10.1016/0020-0190(82)90022-9	breadth-first search;vertex cover;computer science;edge cover;vertex;mathematics;reachability;neighbourhood;algorithm	DB	22.99499516657058	26.12791834179385	65520
917b99a2e6dd7858291aaefa802d44f96a57b846	"""polynomial testing of the query """"is ab ≥ cd?"""" with application to finding a minimal cost reliability ratio spanning tree"""	algorithm analysis;algorithme recherche;arbre maximal;search algorithm;test;algorithme polynomial;analyse algorithme;spanning tree	Suppose that a network is given in which for each edge there is a probability of functioning. It is well known, (see [14]), that a maximally reliable spanning tree can be found by imitating the procedure for finding a minimum spanning tree. One way to view this approach is to imagine that a logarithmic operation is implicitly applied to each probability number associated with an edge. The greedy algorithm for finding a minimum spanning tree will then have to compare logarithms of two probabilities, say lnp, and Inpi. Of course, one can compare instead pi and pi. Therefore the logarithmic operation (which is not even a finite process), does not explicitly have to be used. A recent paper considered the problem of finding a minimal cost reliability ratio spanning tree [6]. Using the above approach to the problem requires comparisons of (a Inpi) with (b lnpj), where a and 6, as well as pi and pi are input data of the original problem. Of course, a comparison can be executed by generating (pi)” and (pj)b, but the length of these numbers in binary encoding is not polynomial in the input length of the original problem. Therefore, such a comparison cannot be viewed as a polynomial operation. As shown in [6], if the finite operation of computing (pi)” is counted as a single operation, the running time of the algorithm that finds the optimal tree is bounded by a polynomial in the number of edges of the network. In this paper we will show that comparing (a lnp,) with (b Inpi) (or equivalently (pi)” to (pj)b)* can be performed in time which is polynomial in the input length	binary file;file spanning;greedy algorithm;linear-nonlinear-poisson cascade model;minimum spanning tree;polynomial;time complexity	Ramaswamy Chandrasekaran;Arie Tamir	1984	Discrete Applied Mathematics	10.1016/0166-218X(84)90013-1	combinatorics;spanning tree;calculus;mathematics;software testing;algorithm;search algorithm	Theory	17.939300774052114	25.103273949540004	65542
3dc1e41747f72d4dc6183419b7f36130808fd6cf	sequential optimization of paths in directed graphs relative to different cost functions	directed acyclic graph;cost function;dynamic program;optimal path;directed graph	Abstract This paper is devoted to the consideration of an algorithm for sequential optimization of paths in directed graphs relative to di_erent cost functions. The considered algorithm is based on an extension of dynamic programming which allows to represent the initial set of paths and the set of optimal paths after each application of optimization procedure in the form of a directed acyclic graph.	mathematical optimization	Jewahir AbuBekr;Igor Chikalov;Shahid Hussain;Mikhail Ju. Moshkov	2011		10.1016/j.procs.2011.04.137	transpose graph;dependency graph;mathematical optimization;suurballe's algorithm;nearest neighbor graph;combinatorics;discrete mathematics;feedback arc set;cycle rank;directed graph;null graph;longest path problem;floyd–warshall algorithm;acyclic dependencies principle;computer science;comparability graph;mathematics;graph;moral graph;path;topological sorting;directed acyclic word graph;strongly connected component;directed acyclic graph;closure problem	Theory	20.20004139475426	27.06033040321617	65561
2ce340315431ac3a91423ce6b62ae39ef712dfd0	a parallel algorithm for finding minimum cutsets in reducible graphs	parallelisme;graph theory;algoritmo paralelo;teoria grafo;parallel algorithm;algorithm complexity;graphe reductible;reducible graph;complejidad algoritmo;theorie graphe;algorithme parallele;parallelism;paralelismo;complexite algorithme	We present a parallel algorithm for finding minimum cutsets in reducible graphs. For a reducible graph that has N nodes our algorithm runs in O(log 3  N) time using O(N 3 /log N) PEs on the EREW P-RAM model of computation. We also present a parallel heuristic for finding minimal cutsets in general graphs	parallel algorithm	Eliezer Dekel;Jie Hu	1994	J. Parallel Distrib. Comput.	10.1006/jpdc.1994.1004	combinatorics;discrete mathematics;parallel computing;computer science;graph theory;mathematics;parallel algorithm;algorithm	HPC	18.512349767194664	28.345906492605938	65834
ae35696bca4873f9ed8bddfaf9e725003a79efed	on the probability of generating a lattice	analysis of quantum algorithm;random generation of lattice;riemann zeta function;infrastructure	We study the problem of determining the probability that m vectors selected uniformly at random from the intersection of the full-rank lattice @L in R^n and the window [0,B)^n generate @L when B is chosen to be appropriately large. This problem plays an important role in the analysis of the success probability of quantum algorithms for solving the Discrete Logarithm Problem in infrastructures obtained from number fields and also for computing fundamental units of number fields. We provide the first complete and rigorous proof that 2n+1 vectors suffice to generate @L with constant probability (provided that B is chosen to be sufficiently large in terms of n and the covering radius of @L and the last n+1 vectors are sampled from a slightly larger window). Based on extensive computer simulations, we conjecture that only n+1 vectors sampled from one window suffice to generate @L with constant success probability. If this conjecture is true, then a significantly better success probability of the above quantum algorithms can be guaranteed.		Felix Fontein;Pawel Wocjan	2014	J. Symb. Comput.	10.1016/j.jsc.2013.12.002	combinatorics;discrete mathematics;mathematics;riemann zeta function;algebra	Logic	11.097509436262818	21.49652105707283	65836
1f30c0b4f1928b84dd0219a0003b5c956d7b4805	correction networks	sorting;comparators circuits	We consider the problem of sorting sequences obtained from a sorted sequence of n keys by changing the values of at most k keys at some unknown positions. Since even for k = 1 a lower bound Ω(logn) on the number of parallel comparison steps applies, any comparator network solving this problem cannot be asymptotically faster than the AKS sorting network. We design a comparator network which sorts the sequences considered for a large range of k’s, has a simple architecture and achieves a runtime c · logn, for a small constant c. We present such networks of depth 4logn+O(log2 k loglogn) with a small constant hidden behind the big “Oh”. In particular, for k = o(2 √ logn/ loglogn) the networks are of depth 4logn+o(logn).	comparator;sorting network	Marcin Kik;Miroslaw Kutylowski;Marek Piotrów	1999		10.1109/ICPP.1999.797386	computer science;sorting;theoretical computer science;algorithm	Theory	11.493481944775178	30.38307798205889	65951
e29616a71f4a3f8006439ad7245ed3d7ad703961	technical report column		“Inverting a permutation is as hard as unordered search,” Ashwin Nayak, TR10-121. “Algorithms for Testing Monomials in Multivariate Polynomials,” Zhixiang Chen, Bin Fu, Yang Liu, Robert Schweller, TR10-122. “Limitation on the rate of families of locally testable codes,” Eli Ben-Sasson, TR10-123. “Approximating Multilinear Monomial Coefficients and Maximum Multilinear Monomials in Multivariate Polynomials,” Zhixiang Chen, Bin Fu, TR10-124. “Understanding Space in Proof Complexity: Separations and Trade-offs via Substitutions,” Eli Ben-Sasson, Jakob Nordstrȯm, TR10-125. “Query Complexity in Errorless Hardness Amplification,” Thomas Watson, TR10-126. “Building Injective Trapdoor Functions From Oblivious Transfer,” Brett Hemenway, Rafail Ostrovsky, TR10-127. “The Equivalence of Sampling and Searching,” Scott Aaronson, TR10-128. “Pseudorandom Generators, Typically-Correct Derandomization, and Circuit Lower Bounds,” Jeff Kinne, Dieter van Melkebeek, Ronen Shaltiel, TR10-129. “Locally Testable vs. Locally Decodable Codes,” Tali Kaufman, Michael Viderman, TR10-130. “The Power of Nondeterminism in Self-Assembly,” Nathaniel Bryans, Ehsan Chiniforooshan, David Doty, Lila Kari, Shinnosuke Seki, TR10-131.	circuit complexity;coefficient;decision tree model;eli;entity–relationship model;execution unit;lila kari;locally decodable code;monomial;oblivious transfer;proof complexity;pseudorandom number generator;rafail ostrovsky;randomized algorithm;ruth aaronson bari;self-assembly;tali'zorah;turing completeness;yang	Dean Kelley	2010	SIGACT News	10.1145/1907450.1907529		Theory	12.169202299229312	21.90136783876036	66265
a795886c2a90e78d76e994f4d8aee81640db59d8	a fast method to exactly calculate the diameter of incremental disconnected graphs	diameter;graph mining;dynamic graphs;incremental graphs	The breadth of problems requiring graph analytics is growing rapidly. Diameter is one of the most important metrics of a graph. The diameter is important in both designing algorithms for graphs and understanding the nature and evolution of graphs. Besides, the real world graphs are always changing. So detecting diameter in both static and dynamic graphs is very important. We first present an algorithm to calculate the diameter of the static graphs. The main goal of this algorithm is to reduce the number of breadth-first searches required to determine diameter of the graph. In addition, another algorithm is presented for calculating the diameter of incremental graphs. This algorithm uses the proposed static algorithm in its body. Based on experimental results, our proposed algorithm can detect diameter of both static and incremental graphs faster than existing approaches. To the best of our knowledge, the second algorithm is the first one that is able to efficiently determine the diameter of disconnected graphs that will be connected over time by adding new vertices.	algorithm;breadth-first search;connectivity (graph theory);distance (graph theory);graph (abstract data type);sensor;telecommunications network;time complexity	Masoud Sagharichian;Morteza Alipour Langouri;Hassan Naderi	2016	World Wide Web	10.1007/s11280-016-0394-0	1-planar graph;block graph;random regular graph;pathwidth;cograph;independent set;graph product;longest path problem;dense graph;theoretical computer science;hopcroft–karp algorithm;pancyclic graph;metric dimension;diameter;trapezoid graph;odd graph;maximal independent set;modular decomposition;treewidth;partial k-tree;chordal graph;indifference graph	Theory	20.76764675895655	29.67645188406993	66474
0ab4312be5609e419ae31060ca30f69804607bda	kinetic dictionaries: how to shoot a moving target	metodo caso peor;algorithmique;blanco movil;dictionnaire;algorithmics;algoritmica;cinetique;estructura datos;dictionaries;poursuite cible;methode cas pire;cible mobile;structure donnee;target tracking;kinetics;worst case method;data structure;diccionario;cinetica;moving target	A kinetic dictionary is a data structure for storing a set S of continuously moving points on the real line, such that at any time we can quickly determine for a given query point q whether q ∈ S. We study trade-offs between the worst-case query time in a kinetic dictionary and the total cost of maintaining it during the motions of the points.	best, worst and average case;data dictionary;data structure;turbulence kinetic energy	Mark de Berg	2003		10.1007/978-3-540-39658-1_18	data structure;computer science;artificial intelligence;programming language;algorithmics;algorithm;kinetics	Theory	15.493832741183487	26.68814350435715	66753
2935a227fcbac66b01ce020cb5b2e8e2651afddf	hash tables with finite buckets are less resistant to deletions	high speed networks;dynamic hash tables;queuing theory and analysis	We show that when memory is bounded, i.e. buckets are finite, dynamic hash tables that allow insertions and deletions behave significantly worse than their static counterparts that only allow insertions. This behavior differs from previous results in which, when memory is unbounded, the two models behave similarly. We show the decrease in performance in dynamic hash tables using several hash-table schemes. We also provide tight upper and lower bounds on the achievable overflow fractions in these schemes. Finally, we propose an architecture with content-addressable memory (CAM), which mitigates this decrease in performance.	content-addressable memory;hash table	Josef Kanizo;David Hay;Isaac Keslassy	2010	2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton)	10.1016/j.comnet.2011.12.010	hash function;perfect hash function;dynamic perfect hashing;primary clustering;computer science;theoretical computer science;operating system;distributed computing;rolling hash;algorithm;hash tree;hash filter	DB	10.702761116635175	30.573621096555463	66859
abedf69088d62d452053a66f46f955df3e203ff0	optimal parallel algorithms for direct dominance problems	parallel algorithm;direct dominance;computational geometry;dominance problems;parallel algorithms	We present optimal parallel solutions to direct dominance problems for planar point sets and provide an application. The algorithms presented here are deterministic and designed to run on the concurrent read exclusive write parallel random-access machine (CREW PRAM). In particular, we provide algorithms for counting the number of points that are directly dominated by each point of an n-point set and for reporting these point sets. The counting algorithm runs in O(log n) time using O(n) processors; the reporting algorithm runs in O(log n) time using O(n + k/ log n) processors, where k is the size of the output. The parallel work of our algorithms matches the time complexity of known optimal sequential algorithms. As an application of our results, we present a parallel algorithm for the maximum empty rectangle problem.	parallel algorithm	Amitava Datta;Anil Maheshwari;Jörg-Rüdiger Sack	1996	Nord. J. Comput.		mathematical optimization;computational geometry;computer science;theoretical computer science;analysis of parallel algorithms;parallel algorithm;algorithm;cost efficiency	Theory	12.847772394526237	31.7497995338543	66889
551f0e56c6dcb058abdd49b01fa15995eb84ab8a	bubble-flip - a new generation algorithm for prefix normal words		We present a new recursive generation algorithm for prefix normal words. These are binary words with the property that no factor has more 1s than the prefix of the same length. The new algorithm uses two operations on binary words, which exploit certain properties of prefix normal words in a smart way. We introduce infinite prefix normal words and show that one of the operations used by the algorithm, if applied repeatedly to extend the word, produces an ultimately periodic infinite word, which is prefix normal and whose period’s length and density we can predict from the original word.	algorithm	Ferdinando Cicalese;Zsuzsanna Lipták;Massimiliano Rossi	2018		10.1007/978-3-319-77313-1_16	bubble;combinatorics;discrete mathematics;periodic graph (geometry);recursion;flip;computer science;algorithm;exploit;binary number;prefix	NLP	14.112525009558547	29.28683674232199	66939
25eb79ef6a77c8d165858c9b420e640b757d89bd	efficient search of combinatorial maps using signatures	complejidad espacio;espace lineaire;temps lineaire;espacio lineal;database;base dato;68p15;complejidad lineal;calculo automatico;tiempo lineal;linear complexity;isomorphism;computing;isomorfismo;calcul automatique;informatique theorique;linear time;signature;base de donnees;map isomorphism;space complexity;isomorphisme;46axx;complexite espace;linear space;complexite lineaire;computer theory;combinatorial map;canonical representation;informatica teorica	In this paper, we address the problem of computing canonical representations of n-dimensional combinatorial maps and of using them for efficiently searching for a map in a database. We define two combinatorial map signatures: the first one has a quadratic space complexity and may be used to decide of isomorphism with a new map in linear time whereas the second one has a linear space complexity and may be used to decide of isomorphism in quadratic time. We show that these signatures can be used to efficiently search for a map in a database.	algorithm;antivirus software;bertrand (programming language);best, worst and average case;boundary representation;central processing unit;combinatorial map;computation;computer vision;computer-aided design;dspace;data mining;database;electronic signature;generalized map;image segmentation;international journal of computational geometry and applications;jack edmonds;lecture notes in computer science;linear algebra;outerplanar graph;pattern recognition;pixel;polyhedron;polynomial;procedural generation;randomness;simplified instructional computer;springer (tank);symposium on computational geometry;time complexity;type signature	Stéphane Gosselin;Guillaume Damiand;Christine Solnon	2011	Theor. Comput. Sci.	10.1016/j.tcs.2010.10.029	generalized map;computing;computer science;mathematics;geometry;signature;isomorphism;algorithm;linear space;algebra	Theory	17.498185844938522	26.661509473338477	66975
863006c82cb2a9a927edb11e5dca24782d3a5bf7	unique perfect matchings and proof nets		This paper establishes a bridge between linear logic and mainstream graph theory, building previous work by Retoré (2003). We show that the problem of correctness for MLL+Mix proof nets is equivalent to the problem of uniqueness of a perfect matching. By applying matching theory, we obtain new results for MLL+Mix proof nets: a linear-time correctness criterion, a quasi-linear sequentialization algorithm, and a characterization of the sub-polynomial complexity of the correctness problem. We also use graph algorithms to compute the dependency relation of Bagnol et al. (2015) and the kingdom ordering of Bellin (1997), and relate them to the notion of blossom which is central to combinatorial maximum matching algorithms. 2012 ACM Subject Classification Theory of computation → Linear logic, Mathematics of computing → Matchings and factors, Mathematics of computing → Graph algorithms	algorithm;correctness (computer science);dependency relation;graph theory;linear logic;matching (graph theory);polynomial;theory of computation;time complexity	Lê Thành Dung Nguyen	2018		10.4230/LIPIcs.FSCD.2018.25	uniqueness;graph theory;discrete mathematics;matching (graph theory);correctness;dependency relation;mathematics;graph;linear logic	Theory	21.047393408797017	31.623296532652812	67055
c609a8d30cc33af39d5770c64852bd4760bf540f	the constrained shortest common supersequence problem	approximation algorithms;fixed parameter algorithms;computational complexity;inf 01 informatica;shortest common supersequence	Shortest common supersequence and longest common subsequence are two widely used measures to compare sequences in different fields, from AI planning to Bioinformatics. Inspired by recently proposed variants of these two measures, we introduce a new version of the shortest common supersequence problem, where the solution is required to satisfy a given constraint on the number of occurrences of each symbol. First, we investigate the computational and approximation complexity of the problem, then we give a 32-approximation algorithm. Finally, we investigate the parameterized complexity of the problem, and we present a fixed-parameter algorithm.	apx;approximation algorithm;computation;p (complexity);parameterized complexity;shortest common supersequence problem;time complexity	Riccardo Dondi	2013	J. Discrete Algorithms	10.1016/j.jda.2013.03.004	shortest common supersequence;mathematical optimization;combinatorics;discrete mathematics;computer science;longest common subsequence problem;mathematics;computational complexity theory;approximation algorithm;algorithm	Theory	15.622690525818026	22.385745114837498	67070
6074ad14332791f3860726fb1c7038409a4e8a02	binary search trees with limited rotation	binary search tree	A parameterized binary search tree callediR tree is defined in this paper. A user is allowed to select a level of balance he desires. SR tree is a special case ofiR tree wheni=1. There are two new concepts in SR trees: (1) local balancing scheme that balances the tree locally; (2) consecutive storage for brother nodes that reduces pointer space. Although we may introduce empty nodes into the tree, we can show that only 1/8 of the nodes may be empty on the average, so it may still be advantageous in cases when record sizes are small. Insertion (and deletion) into SR trees can be done in timeh + O(1) whereh is the height of the tree. The average searching time for SR trees is shown to be 1.188 log2k wherek is the number of keys. Generalization of the results of SR trees toiR in general is also given.		Shou-Hsuan Stephen Huang;Chak-Kuen Wong	1983	BIT		random binary tree;optimal binary search tree;left-child right-sibling binary tree;red–black tree;combinatorics;discrete mathematics;binary search tree;tree rotation;exponential tree;binary tree;rotation;order statistic tree;scapegoat tree;range tree;k-d tree;(a,b)-tree;k-ary tree;interval tree;mathematics;tree structure;search tree;weight-balanced tree;ternary search tree;tree traversal;algorithm;quantum mechanics;avl tree	Theory	14.92368689582233	28.143515659203796	67272
45ba660f56215a8d0d5b965aa0d111e2fb569392	exact algorithms for coloring graphs while avoiding monochromatic cycles	approximate algorithm;acyclic graph;undirected graph;phase transition;directed graph;exact algorithm;np complete;integer program;bipartite graph	We consider the problem of deciding whether a given directed graph can be vertex partitioned into two acyclic subgraphs. Applications of this problem include testing rationality of collective consumption behavior, a subject in micro-economics. We prove that the problem is NP-complete even for oriented graphs and argue that the existence of a constant-factor approximation algorithm is unlikely for an optimization version which maximizes the number of vertices that can be colored using two colors while avoiding monochromatic cycles. We present three exact algorithms, namely an integer-programming algorithm based on cycle identification, a backtracking algorithm, and a branch-and-check algorithm. We compare these three algorithms both on real-life instances and on randomly generated graphs. We find that for the latter set of graphs, every algorithm solves instances of considerable size within few seconds; however, the CPU time of the integer-programming algorithm increases with the number of vertices in the graph more clearly than the the CPU time of the two other procedures. For real-life instances, the integer-programming algorithm solves the largest instance in about a half hour while the branch-and-check algorithm takes about ten minutes and the backtracking algorithm less than five minutes. Finally, for every algorithm, we also study empirically the transition from a high to a low probability of a YES answer as function of the number of arcs divided by the number of vertices.	apx;approximation algorithm;backtracking;central processing unit;color;directed acyclic graph;directed graph;graph coloring;integer programming;mathematical optimization;monochrome;np-completeness;optimization problem;planar graph;procedural generation;random graph;rationality;real life	Fabrice Talla Nobibon;Cor A. J. Hurkens;Roel Leus;Frits C. R. Spieksma	2010		10.1007/978-3-642-14355-7_24	phase transition;graph power;mathematical optimization;suurballe's algorithm;combinatorics;discrete mathematics;feedback arc set;np-complete;independent set;directed graph;graph bandwidth;bipartite graph;null graph;hopcroft–karp algorithm;comparability graph;cycle graph;johnson's algorithm;graph coloring;mathematics;voltage graph;blossom algorithm;moral graph;path;directed acyclic graph;line graph;algorithm;matching	Theory	23.67468158624944	22.475309332504136	67462
977acc6b268eecd8b2807c3b008fd8acb30de6ce	response time minimization for processing simple queries in star networks	minimisation;tiempo respuesta;dynamic programming;minimization;data base query;programacion dinamica;base donnee repartie;distributed database;algorithm performance;reseau ordinateur;interrogation base donnee;base repartida dato;interrogacion base datos;response time;minimizacion;computer network;temps reponse;resultado algoritmo;programmation dynamique;performance algorithme;red ordenador;algoritmo optimo;algorithme optimal;optimal algorithm	Abstract#R##N##R##N#It has already been shown that the problem of obtaining an optimal strategy for a general query on a distributed database system is NP-hard. On the other hand, simple queries have been known as one class of queries for which an optimal strategy can be obtained efficiently. So far, it has been shown that if the topology of a computer network is a complete graph, then a strategy with the minimum response time for simple queries can be obtained in O(n2), where n is the number of sites of the network.#R##N##R##N##R##N##R##N#This paper discusses the problem of obtaining a strategy with the minimum response time for simple queries when the topology of a computer network is a star (in brief, problem RT). We assume that each communication channel of a star network has equal communication capacity, and that each site has an equal capability for data processing. Here, we also assume that the center of a star network is capable of performing so-called broadcasting. In this paper, we present an algorithm called RT of O(n2) time for solving problem RT. Algorithm RT constructs a schedule graph that represents an optimal strategy by using the dynamic programming method.	responsiveness	Jun'ichi Miyao;Tohru Kikuno;Noriyoshi Yoshida	1987	Systems and Computers in Japan	10.1002/scj.4690180402	minimisation;computer science;artificial intelligence;dynamic programming;distributed computing;response time;distributed database;algorithm;statistics	OS	19.067361292040065	27.415694651381354	67524
f899d71ec296a4e8020ea3b75aa9b7f88e081380	a combinatorial analysis of the average time for open-address hash coding insertion		In analysing a well-known hash-coding method, Knuth gave an exact expression for the average number of rejections encountered by players of a variant of musical chairs. We study a variant more closely related to musical chairs itself and deduce the same expression by a purely combinatorial approach. In an analysis of the average time to insert an item when using openaddress hash-coding, Knuth [1, p. 528-530] reduced the problem to the following question about musical chairs. Givenm chairs arranged in a circle and numbered clockwise from 0 to m− 1, if in turn each of n people arrives at a randomly selected chair (his initial chair) and walks clockwise until he finds an empty chair (his final chair), what is the average number of rejections (chairs found occupied during the search) per player? For m ≥ n, Knuth showed that the average number of rejections is	randomness	Vaughan R. Pratt	2012	CoRR		combinatorics;theoretical computer science;mathematics;algorithm	Theory	13.701885462425166	25.907142762702527	67645
5496752619258d7b506bcc7ad97e3dffa211ea61	an efficient algorithm for minimumk-covers in weighted graphs	efficient algorithm;weighted sums;weighted graph	Consider an edge-weighted graph G = (V, L), and define a k-cover C as a subset of the edges L such that each vertex in Vis incident to at least one edge of C, and ICI =k. Given G and k, the problem is to find a k-cover of minimum weight sum. This paper presents characterizations of minimum k-covers, and shows their weight to be convex with the parameter k. An efficient algorithm is presented which generates minimum k-covers continuously as the parameter k ranges over all feasible values, together with a proof of optimality. The computational order of this algorithm is found to be I VI • ILI 2.		Lee J. White;Mark L. Gillenson	1975	Math. Program.	10.1007/BF01580426	mathematical optimization;combinatorics;discrete mathematics;mathematics	Theory	24.26831907670181	20.0586869962816	67734
4dd4f4f8aa017b283b14b2e6a61672ebc6b1ac46	query-competitive algorithms for cheapest set problems under uncertainty	online algorithms;competitive analysis;matroid;verification	Considering the model of computing under uncertainty where element weights are uncertain but can be obtained at a cost by query operations, we study the problem of identifying a cheapest (minimum-weight) set among a given collection of feasible sets using a minimum number of queries of element weights. For the general case we present an algorithm that makes at most d ? OPT + d queries, where d is the maximum cardinality of any given set and OPT is the optimal number of queries needed to identify a cheapest set. For the minimum multi-cut problem in trees with d terminal pairs, we give an algorithm that makes at most d ? OPT + 1 queries. For the problem of computing a minimum-weight base of a given matroid, we give an algorithm that makes at most 2 ? OPT queries, generalizing a known result for the minimum spanning tree problem. For each of the above algorithms we give matching lower bounds. We also settle the complexity of the verification version of the general cheapest set problem and the minimum multi-cut problem in trees under uncertainty.		Thomas Erlebach;Michael Hoffmann;Frank Kammer	2014		10.1007/978-3-662-44465-8_23	mathematical optimization	Robotics	24.250995800068036	18.68807968560824	67888
fc6723021953ebce89de860f4bdbf1ccb9f9dd50	l(p, q)-labeling of digraphs	optimisation;l p q labeling;nombre entier;combinatorics;digraph;camino grafo;tree;optimizacion;05c05;graph path;combinatoria;vertex;arbol;combinatoire;digrafo;orientation;integer;informatique theorique;directed graph;l p;cycle graphe;marcacion grafo;entero;68r10;graphe oriente;path;chemin graphe;arbre;orientacion;grafo orientado;etiqueta;optimization;vertice;q labeling;cycle graph;etiquette;marquage graphe;cycle;label;graph labelling;l p q;computer theory;ciclo diagrama;informatica teorica;digraphe	"""Given a graph G and two positive integers p,q with p>q an L(p,q)-labeling of G is a function f from the vertex set V(G) to the set of all nonnegative integers such that |f(x)-f(y)|>=p if d""""G(x,y)=1 and |f(x)-f(y)|>=q if d""""G(x,y)=2. A k-L(p,q)-labeling is an L(p,q)-labeling such that no label is greater than k. The L(p,q)-labeling number of G, denoted by @l""""p"""",""""q(G) is the smallest number k such that G has a k-L(p,q)-labeling. When considering the digraph D, we use @l""""p"""",""""q^*(D) in place of @l""""p"""",""""q(D). We study the L(p,q)-labeling number of a digraph D in this paper. We find some relations between the L(p,q)-labeling number of a graph G and an orientation D of G, and give some results for the L(p,q)-labeling numbers of k-partite digraphs. We also study the L(p,q)-labeling numbers for those graphs D for which the underlying graphs are paths, cycles or trees."""	directed graph	Yi-Ting Chen;Ma-Lian Chia;David Kuo	2009	Discrete Applied Mathematics	10.1016/j.dam.2008.12.007	combinatorics;discrete mathematics;directed graph;mathematics;algorithm	Theory	24.211782675570426	30.538533362534555	68113
05f9848f85c85ad503aaed4445eb150517795bcd	improved approximations for cubic and cubic bipartite tsp		We show improved approximation guarantees for the traveling salesman problem on cubic graphs, and cubic bipartite graphs. For cubic bipartite graphs with n nodes, we improve on recent results of Karp and Ravi (2014) by giving a simple “local improvement” algorithm that finds a tour of length at most 5/4n − 2. For 2-connected cubic graphs, we show that the techniques of Mömke and Svensson (2011) can be combined with the techniques of Correa, Larré and Soto (2012), to obtain a tour of length at most (4/3− 1/8754)n.	algorithm;approximation;cubic function;travelling salesman problem	Anke van Zuylen	2015	CoRR		mathematical optimization;combinatorics;discrete mathematics;mathematics	Theory	23.39927784378804	21.988888454843583	68467
c82276f525ab37cda42025714134b496cdad9988	an information statistics approach to data stream and communication complexity	sun;hellinger distance;computational modeling;probability;statistics;communication model;algorithm design and analysis;communication protocol;frequency;communication complexity;computer science;lp norm;lower bound;protocols;approximation algorithms	We present a new method for proving strong lower bounds in communication complexity. This method is based on the notion of the conditional information complexity of a function which is the minimum amount of information about the inputs that has to be revealed by a communication protocol for the function. While conditional information complexity is a lower bound on communication complexity, we show that it also admits a direct sum theorem. Direct sum decomposition reduces our task to that of proving conditional information complexity lower bounds for simple problems (such as the AND of two bits). For the latter, we develop novel techniques based on Hellinger distance and its generalizations. Our paradigm leads to two main results: (1) An improved lower bound for the multi-party set-disjointness problem in the general communication complexity model, and a nearly optimal lower bound in the one-way communication model. As a consequence, we show that for any real k42; approximating the kth frequency moment in the data stream model requires essentially Oðn Þ space; this resolves a conjecture of Alon et al. (J. Comput. System Sci. 58(1) (1999) 137). (2) A lower bound for the Lp approximation problem in the general communication model; this solves an open problem of Saks and Sun (in: Proceedings of the 34th Annual ACM Symposium on Theory of Computing (STOC), 2002, pp. 360–369). As a consequence, we show that for p42; approximating the Lp norm to within a factor of ne in the data stream model with constant number of passes requires Oðn 4e Þ space. r 2003 Elsevier Inc. All rights reserved. ARTICLE IN PRESS Corresponding author. E-mail addresses: ziv@almaden.ibm.com (Z. Bar-Yossef), jayram@almaden.ibm.com (T.S. Jayram), ravi@ almaden.ibm.com (R. Kumar), siva@almaden.ibm.com (D. Sivakumar). Part of this work was done while the first author was a student at UC Berkeley, and a visitor at IBM. Supported by NSF ITR Grant CCR-0121555. 0022-0000/$ see front matter r 2003 Elsevier Inc. All rights reserved. doi:10.1016/j.jcss.2003.11.006	approximation;communication complexity;communications protocol;conditional entropy;ibm notes;k42;one-way function;programming paradigm;streaming algorithm;symposium on theory of computing;uc browser	Ziv Bar-Yossef;T. S. Jayram;Ravi Kumar;D. Sivakumar	2002		10.1109/SFCS.2002.1181944	communications protocol;combinatorics;discrete mathematics;average-case complexity;decision tree model;computer science;theoretical computer science;structural complexity theory;worst-case complexity;communication complexity;complexity index;mathematics;approximation algorithm;descriptive complexity theory;statistics	Theory	11.567282871045977	22.327701365251183	68667
70ae8f6afd93d0db519d9db77bdc828f3492e684	sensitivity analysis of 0-1 multiterminal network flows	arbre graphe;graphe non oriente;delecion;non directed graph;tree graph;coupe graphe;flujo optimo;reseau;red;cut tree;algorithme;algorithm;corte grafo;grafo no orientado;sensitivity analysis;graph cut;edge graph;addition;arete graphe;flot optimal;network flow;arbol grafo;optimal flow;flot reseau;arista grafico;network;deletion;adiccion;algoritmo	Abstract#R##N##R##N#Given an undirected 0-1 flow network G with n nodes, Gomory and Hu's algorithm solves the multiterminal maximum flow problem by constructing a cut-tree for G. Their algorithm requires solving n − 1 maximum flow problems. A perturbed network of G is a network generated from G by deleting or adding an edge to G. For a given network G and an edge (s, t) to be deleted from (resp., added to) G, a marginal cut-tree T is a cut-tree for G from which a cut-tree for the perturbed network can be generated easily by reducing (resp., increasing) by 1 the weight of every edge in the path between s and t in T. Given an undirected 0-1 flow network G, a cut-tree for G, and an edge to be deleted from or added to G, we present an algorithm to generate a cut-tree for the perturbed network by first transforming the given cut-tree for G into a marginal cut-tree and then transforming the marginal cut-tree into a cut-tree for the perturbed network. This algorithm also requires solving the maximum flow problem; depending on the input data, the number of such problems to be solved is between 0 and n − 2 for edge deletion and between 0 and n − 1 for edge addition. Properties on the input data that lead, respectively, to the best-case and worst-case complexities of the algorithm are identified, and numerical experiments are included.	multiseat configuration	Shiow C. Lin;Eva Ma	1991	Networks	10.1002/net.3230210703	mathematical optimization;combinatorics;discrete mathematics;flow network;cut;mathematics;addition;sensitivity analysis;tree;algorithm	Metrics	21.84402656262816	28.68489867976543	68873
3787b925d2a269b7fc28f6e574158cbe46140a80	on approximation of max-vertex-cover	performance guarantee;vertex cover;approximate algorithm;semidefinite programming;polynomial approximation algorithm;linear programming relaxation;linear programming;randomized algorithm;greedy algorithm;linear program;max vertex cover;weighted graph;polynomial approximation;semidefinite program	We consider the max-vertex-cover (MVC) problem, i.e., find  k  vertices from an undirected and edge-weighted graph  G =( V , E ), where | V |= n ⩾ k , such that the total edge weight covered by the  k  vertices is maximized. There is a 3/4-approximation algorithm for MVC, based on a linear programming relaxation. We show that the guaranteed ratio can be improved by a simple greedy algorithm for  k >(3/4) n , and a simple randomized algorithm for  k >(1/2) n . Furthermore, we study a semidefinite programming (SDP) relaxation based approximation algorithms for MVC. We show that, for a range of  k , our SDP-based algorithm achieves the best performance guarantee among the four types of algorithms mentioned in this paper.	approximation;vertex cover	Qiaoming Han;Yinyu Ye;Hantao Zhang;Jiawei Zhang	2002	European Journal of Operational Research	10.1016/S0377-2217(02)00330-2	mathematical optimization;combinatorics;greedy algorithm;discrete mathematics;vertex cover;criss-cross algorithm;computer science;linear programming;linear programming relaxation;mathematics;randomized algorithm;approximation algorithm	Robotics	23.211121596698543	19.78418875770467	68952
68cca3369166b11cdae23019691f86809b4dd046	multidisk file design: an analysis of folding buckets to disks	upper bound	A technique called folding for mapping file buckets to multiple disks is evaluated. In particular, an upper bound to expected costs given any size partial match query is found. Folding is compared to Disk Modulo allocation.	floppy disk	Mee Yee Chan	1984	BIT		parallel computing;computer science;mathematics;upper and lower bounds;algorithm	Theory	13.866410728264546	28.449841366207934	68956
6ad760da7244257c508c8b2556c7b79b84f998fa	recal - a new efficient algorithm for the exact analysis of multiple-chain closed queuing networks	performance measure;closed systems;sistema cerrado;complexite calcul;complejidad calculo;normalisation;efficient algorithm;performance;algoritmo recursivo;chain;computing complexity;red cola espera;mean value analysis;algorithme recursif;reseau file attente;dynamic scaling;chaine;cadena;queuing network;recursive algorithm;queuing networks;rendimiento;multiple;systeme ferme	RECAL, a Recursion by Chain Algorithm for computing the mean performance measures of product-form multiple-chain closed queuing networks, is presented. It is based on a new recursive expression that relates the normalization constant of a network with r closed routing chains to those of a set of networks having (r - 1) chains. It relies on the artifice of breaking down each chain into constituent subchains that each have a population of one. The time and space requirements of the algorithm are shown to be polynomial in the number of chains. When the network contains many routing chains, the proposed algorithm is substantially more efficient than the convolution or mean value analysis algorithms. The algorithm, therefore, extends the range of queuing networks that can be analyzed efficiently by exact means.	algorithm;convolution;polynomial;recursion;requirement;routing	Adrian E. Conway;Nicolas D. Georganas	1986	J. ACM	10.1145/6490.6495	mean value analysis;performance;computer science;theoretical computer science;mathematics;distributed computing;closed system;chain;algorithm;multiple;statistics;recursion	Theory	14.658390740297174	30.979969211077666	69018
060cd62ac143ab034a9cc0d499edd0f2ecf8f8cc	nuclear norm minimization for the planted clique and biclique problems	compressed sensing;exact solution;optimization problem;65k05;maximum clique;numerical analysis;90c25;68q25;bipartite graph;data structure;nuclear norm minimization	We consider the problems of finding a maximum clique in a graph and finding a maximum-edge biclique in a bipartite graph. Both problems are NP-hard. We write both problems as matrix-rank minimization and then relax them using the nuclear norm. This technique, which may be regarded as a generalization of compressive sensing, has recently been shown to be an effective way to solve rank optimization problems. In the special cases that the input graph has a planted clique or biclique (i.e., a single large clique or biclique plus diversionary edges), our algorithm successfully provides an exact solution to the original instance. For each problem, we provide two analyses of when our algorithm succeeds. In the first analysis, the diversionary edges are placed by an adversary. In the second, they are placed at random. In the case of random edges for the planted clique problem, we obtain the same bound as Alon, Krivelevich and Sudakov as well as Feige and Krauthgamer, but we use different techniques.	adversary (cryptography);algorithm;clique (graph theory);clique problem;compressed sensing;mathematical optimization;np-hardness;planted clique	Brendan P. W. Ames;Stephen A. Vavasis	2011	Math. Program.	10.1007/s10107-011-0459-x	clique;optimization problem;mathematical optimization;combinatorics;clique graph;discrete mathematics;data structure;bipartite graph;numerical analysis;clique problem;simplex graph;mathematics;compressed sensing	ML	18.196329336871273	19.979101769731397	69110
c14f6538a86c4a9c6ecec0c490f27ac81cfb74e4	a method to compute the sparse graphs for traveling salesman problem based on frequency quadrilaterals		In this paper, an iterative algorithm is designed to compute the sparse graphs for traveling salesman problem (TSP) according to the frequency quadrilaterals so that the computation time of the algorithms for TSP will be lowered. At each computation cycle, the algorithm first computes the average frequency f̅(e) of an edge e with N frequency quadrilaterals containing e in the input graph G(V,E). Then the 1/3|E| edges with low frequency are eliminated to generate the output graph with a smaller number of edges. The algorithm can be iterated several times and the original optimal Hamiltonian cycle is preserved with a high probability. The experiments demonstrate the algorithm computes the sparse graphs with the O(nlog2n) edges containing the original optimal Hamiltonian cycle for most of the TSP instances in the TSPLIB. The computation time of the iterative algorithm is O(Nn).	approximation algorithm;carrier-to-noise ratio;computation;experiment;genus (mathematics);hamiltonian path;heuristic (computer science);iteration;iterative method;planar graph;polynomial;precondition;sparse graph code;sparse matrix;time complexity;travelling salesman problem;treewidth	Yong Wang;Jeffrey B. Remmel	2018		10.1007/978-3-319-78455-7_22	iterated function;combinatorics;mathematics;iterative method;overline;discrete mathematics;computation;travelling salesman problem;hamiltonian path;quadrilateral;dense graph	Theory	22.98639533012767	23.753324024994626	69134
01812956576df0c24db5fe313f1df791dae73633	almost all k-colorable graphs are easy to color	graph theory;teoria grafo;heuristic method;metodo heuristico;theorie graphe;algorithme;algorithm;coloration graphe;coloracion diagrama;methode heuristique;graph coloration;algoritmo	We describe a simple and eecient heuristic algorithm for the graph coloring problem and show that for all k 1, it nds an optimal coloring for almost all k-colorable graphs. We also show that an algorithm proposed by Br elaz and justiied on experimental grounds optimally colors almost all k-colorable graphs. EEcient implementations of both algorithms are given. The rst one runs in O(n+m log k) time where n is the number of vertices and m the number of edges. The new implementation of Br elaz's algorithm runs in O(m log n) time. We observe that the popular greedy heuristic works poorly on k-colorable graphs.	color;graph coloring;greedy algorithm;heuristic (computer science)	Jonathan S. Turner	1988	J. Algorithms	10.1016/0196-6774(88)90005-3	1-planar graph;combinatorics;independent set;graph theory;hopcroft–karp algorithm;complete coloring;edge coloring;graph coloring;mathematics;chordal graph;greedy coloring;indifference graph;algorithm	Theory	19.658209596283264	26.69521916777776	69264
6354a5f22df54f442decf9688b9807eaa04f9657	simulating sparse hamiltonians with star decompositions	maximum degree;efficient algorithm;connected component	Quantum simulation of Hamiltonian dynamics is a well-studied problem [1–3] and is one of the main motivations for building a quantum computer. Since the best known classical algorithms for simulating quantum systems are inefficient, this was the original application of quantum computers [4]. Besides simulating physics, Hamiltonian simulation has many algorithmic applications, such as adiabatic optimization, unstructured search, and the implementation of quantum walks. The input to the Hamiltonian simulation problem is a Hamiltonian H and a time t; the problem is to implement the unitary operator e−iHt. We say that a Hamiltonian acting on an N -dimensional quantum system can be simulated efficiently if there is a quantum circuit using poly(logN, t, 1/ ) oneand two-qubit gates that approximates (with error at most ) the evolution according to H for time t. Lloyd presented a method for simulating quantum systems that can be described by a sum of local Hamiltonians [1]. A Hamiltonian is called local if it acts non-trivially on at most a fixed number of qubits, independent of the size of the system. This was later generalized by Aharonov and Ta-Shma [2] to the case of sparse (and efficiently row-computable) Hamiltonians. A Hamiltonian is sparse if it has at most poly(logN) nonzero entries in any row. It is efficiently row-computable if there is an efficient procedure to determine the location and matrix elements of the nonzero entries in each row. The complexity of this simulation was improved by Childs [5] and further improved by Berry, Ahokas, Cleve and Sanders [3]. Their algorithm has query complexity (d4(log∗N) ‖H‖)1+o(1), where d is the maximum degree of the graph of the Hamiltonian H. These algorithms decompose ? Work supported by MITACS, NSERC, QuantumWorks, and the US ARO/DTO. the Hamiltonian into a sum of Hamiltonians, each of which is easy to simulate. We present a different method of decomposing the Hamiltonian. We decompose a general sparse Hamiltonian into a small sum of Hamiltonians, each of whose graph of non-zero entries is a forest of star graphs. This is done using ideas from distributed computing for decomposing a graph into a sum of forests [8] and vertex coloring forests [9, 10]. We then show that a Hamiltonian whose graph of non-zero entries is a forest of stars can be efficiently simulated. This leads to an algorithm with query complexity (d2(d+ log∗N) ‖H‖)1+o(1). The simulation of Ref. [3] has also been improved using a completely different approach [6, 7]. That algorithm is more efficient in terms of all parameters except the error , on which its dependence is considerably worse. The algorithm we present here maintains the same dependence on as in Ref. [3], providing the best-known method for high-precision simulation of sparse Hamiltonians.	aharonov–bohm effect;algorithm;c date and time functions;computable function;computational complexity theory;computer;data transfer object;decision tree model;distributed computing;forest of stars;graph coloring;hamiltonian (quantum mechanics);mathematical optimization;quantum circuit;quantum computing;quantum system;quantum walk;qubit;simulation;sparse matrix	Andrew M. Childs;Robin Kothari	2010		10.1007/978-3-642-18073-6_8	mathematical optimization;combinatorics;discrete mathematics;connected component;mathematics	Theory	20.027490083664304	30.171825889283422	69324
738ef03553b4ae44d2e86c6146cb6b52a394ad63	worst-case analysis of the set-union problem with extended backtracking	metodo caso peor;algorithm analysis;complexite calcul;complejidad calculo;worse case method;computing complexity;worst case analysis;informatique theorique;backtracking;methode cas pire;analyse algorithme;analisis algoritmo;computer theory;informatica teorica	In this paper, an extension of the well known set union problem is considered, where backtracking over sequences of Union operations is allowed. A data structure is presented which maintains a partition of zn n-item set making it possible to perform each Union in O(1g lg n) time, each Find in O(lg n) time and allows backtracking over the Unions in O(1) time. Moreover, it is shown that the data structure can be slightly modified as to present an O(k iin lg n) time complexity on a sequence of k Unions and Backtracks and m Finds. The space complexity of both versions of such a data structure is O(n). The set-union problem, togl;ther with its variants, is certainly one of the most extensively studied problems in recent years [l, 3, 4, 9, H-13, 15, 18, 19, 21, 22, 241. The original problem is that of maintaining a representation of a partition of aset S={l,2,..., n} in equivalence classes under the following two operations: Unio&K, f): return a new partition of S in which classes X, Y are merged into a new equivalence class X u Y named X. Find (x) : given an item y E s, return the name of the equivalence class containing	backtracking;dspace;data structure;emoticon;time complexity;turing completeness;whole earth 'lectronic link	Giorgio Gambosi;Giuseppe F. Italiano;Maurizio Mastropasqua Talamo	1989	Theor. Comput. Sci.	10.1016/0304-3975(89)90119-9	computer science;artificial intelligence;calculus;mathematics;algorithm;backtracking	Theory	15.642537579986751	27.19884742237065	69384
21dc1e47e775e0dfa23369165469d3030b5a758a	a fast cost-optimal parallel algorithm for the lowest common ancestor problem	algoritmo paralelo;pram;base donnee repartie;distributed database;parallel algorithm;procesamiento informacion;multiprocessor;analyse efficacite cout;structure arborescente;base repartida dato;algorithme parallele;cost optimization;estructura arborescente;tree structure;information processing;lowest common ancestor;ancetre commun minimal;multiprocesador;memoire repartie;traitement information;algoritmo optimo;algorithme optimal;optimal algorithm;data structure;multiprocesseur	Abstract   The problem of computing the lowest common ancestors of all pairs of nodes in a rooted tree is central in a large number of practical applications. The purpose of this note is to propose a very simple cost-optimal parallel algorithm to solve the lowest common ancestor problem. More precisely, with an  n -node rooted tree as input, our algorithm runs in O(log  n ) time using O( n  2  lopg  n ) processors on the EREW-PRAM model.	cost efficiency;lowest common ancestor;parallel algorithm	Rong Lin;Stephan Olariu	1992	Parallel Computing	10.1016/0167-8191(92)90086-M	range query;combinatorics;parallel computing;multiprocessing;data structure;information processing;computer science;theoretical computer science;mathematics;parallel algorithm;tree structure;programming language;distributed database;algorithm;lowest common ancestor	HPC	17.208173511373563	28.06249092843417	69509
78bf70b099355e16d30711a098a0a1d12f97c0c7	an o(n log n log log n) parallel maximum matching algorithm for bipartite graphs	algoritmo paralelo;graphe biparti;parallel algorithm;algorithm performance;multiprocessor;complexite calcul;complejidad calculo;computing complexity;maximum matching;algorithme parallele;resultado algoritmo;performance algorithme;grafico bipartido;multiprocesador;bipartite graph;multiprocesseur		algorithm;matching (graph theory)	Taenam Kim;Kyung-Yong Chwa	1987	Inf. Process. Lett.	10.1016/0020-0190(87)90193-1	combinatorics;discrete mathematics;multiprocessing;bipartite graph;computer science;mathematics;parallel algorithm;algorithm;matching	Theory	18.471758047800968	28.352945646263574	69564
b71f4277c32ccf85e4167972f95988174fce223f	randomness-optimal sampling, extractors, and constructive leader election	leader election	We present the first efficient universal oblivious sampler that uses an optimal number of random bits, up to an arbitrary constant factor bigger than 1. Specifically, for any a >0 and c(m) ~ exp(–cr210K* ‘m), our sampler can use (1 + a) (m + log 7-1 ) random bits to output d = poly(c-l, log-y-l, m) sample points Z1, ..., z~ C {O, I}m such that for any function ~ : {O, l}m ~ [0, 1], Our proof is based on an improved extractor construction. An extractor is a procedure which takes as input the output of a defective random source and a small number of truly random bits, and outputs a nearly-random string. We present the first optimal extractor, up to constant factors, for defective random sources with constant entropy rate. We give two applications of these tools. First, we exhibit a constructive O(log n) round protocol for leader election in the full information model that is resilient against any coalition of size /3n for any constant ~ < 1/2. Each player sends only log n bits per round. Second, given a 2g (n) round AM proof for L in which Arthur sends l(n) random bits per round and Merlin responds with a q(n) bit string, we construct a g(n) round AM proof for a language L in which Arthur sends O(l(n) + q(n)) random bits per round and Merlin’s response remains of polynomial length. “ Dept. of Computer sciences, The University af T.sxaa at Austin, Austin, TX 7S712, diz@cs.utexas.edu. Supported in part by NSF NY1 Grant No. CCF&9457799. Permieeion to make dlgitrd/hard copies of all or part of Wla materiel for pereonel or cleaaroom uee is granted without fee provided that the copies are not made or distributed for profit or commercial advantage, the copyright notice, the title of the publication and ita date appear, and notice is given that copyright ia by permiaaion of the ACM, Inc. To copy otherwiee, to republish, to poet on acrvem or to redatribute ta Mite, mquima epwific venniasion andlor fee. STOC’96, Philadelphia PA, USA ~ 199(j ACM &89791-785.5/96/05. .$3+50	baudot code;bit array;entropy rate;ibm notes;information model;leader election;polynomial;randomness extractor;sampling (signal processing)	David Zuckerman	1996		10.1145/237814.237878	computer science;leader election;mathematics	Theory	11.62183716856831	22.023273395119105	69701
1b1160599ef783fae2eb58311afdff0255fc4912	going after the k-sat threshold	random structures;belief propagation;phase transitions;second moment method;k sat	"""Random k-SAT is the single most intensely studied example of a random constraint satisfaction problem. But despite substantial progress over the past decade, the threshold for the existence of satisfying assignments is not known precisely for any k≥3. The best current results, based on the second moment method, yield upper and lower bounds that differ by an additive k ⋅ {ln2}/2, a term that is unbounded in k (Achlioptas, Peres: STOC 2003). The basic reason for this gap is the inherent asymmetry of the Boolean values 'true' and 'false' in contrast to the perfect symmetry, e.g., among the various colors in a graph coloring problem. Here we develop a new asymmetric second moment method that allows us to tackle this issue head on for the first time in the theory of random CSPs. This technique enables us to compute the k-SAT threshold up to an additive ln2-1/2+O(1/k) ~0.19. Independently of the rigorous work, physicists have developed a sophisticated but non-rigorous technique called the """"cavity method"""" for the study of random CSPs (Mezard, Parisi, Zecchina: Science~2002). Our result matches the best bound that can be obtained from the so-called """"replica symmetric"""" version of the cavity method, and indeed our proof directly harnesses parts of the physics calculations."""	boolean satisfiability problem;color;constraint satisfaction problem;cryptographic service provider;graph coloring;peres–horodecki criterion;symposium on theory of computing;utility functions on indivisible goods	Amin Coja-Oghlan;Konstantinos Panagiotou	2013		10.1145/2488608.2488698	phase transition;combinatorics;discrete mathematics;computer science;mathematics;geometry;second moment method;algorithm;statistics;belief propagation;algebra	Theory	10.194211252467971	20.712237575322327	69788
6195638eedb9b8d16ee7d9dfefed6230d62686ec	ℓ 2 2 spreading metrics for vertex ordering problems	methode diviser pour regner;minimum linear arrangement;minimum containing interval graph;approximate algorithm;graphe intervalle;programmation semi definie;semidefinite programming;temps polynomial;minimum storage time product;ordre lineaire;execution time;interval graph;grafo intervalo;relacion orden;approximation algorithm;linear arrangement;metodo dividir para vencer;linear ordering;ordering;metric;temps minimal;permutation;relation ordre;divide and conquer method;permutacion;algoritmo aproximacion;polynomial time;minimum time;temps execution;metrico;programacion semi definida;algorithme approximation;tiempo ejecucion;tiempo minimo;orden lineal;divide and conquer;vertex ordering problem;metrique;semidefinite program;semi definite programming;tiempo polinomial	"""We design approximation algorithms for the vertex ordering problems MINIMUM LINEAR ARRANGEMENT, MINIMUM CONTAINING INTERVAL GRAPH, and MINIMUM STORAGE-TIME PRODUCT, achieving approximation factors of <i>O</i>√log <i>n</i> log log <i>n</i>), <i>O</i>√log <i>n</i> log log <i>n</i>), and <i>O</i>√log <i>T</i> log log <i>T</i>), respectively, the last running in time polynomial in <i>T</i> (<i>T</i> being the sum of execution times). The technical contribution of our paper is to introduce <i>l</i><sup>2</sup><inf>2</inf> spreading metrics"""" (that can be computed by semidefinite programming) as relaxations for both undirected and directed """"permutation metrics,"""" which are induced by permutations of {1, 2, . . ., <i>n</i>}. The techniques introduced in the recent work of Arora, Rao and Vazirani can be adapted to exploit the geometry of such <i>l</i><sup>2</sup><inf>2</inf> spreading metrics, giving a powerful tool for the design of divide-and-conquer algorithms. In addition to their applications to approximation algorithms, the study of such <i>l</i><sup>2</sup><inf>2</inf> spreading metrics as relaxations of permutation metrics is interesting in its own right. We show how our results imply that, in a certain sense we make precise, <i>l</i><sup>2</sup><inf>2</inf> spreading metrics approximate permutation metrics on <i>n</i> points to a factor of <i>O</i>√log <i>n</i> log log <i>n</i>)."""	approximation algorithm;graph (discrete mathematics);polynomial;semidefinite programming	Moses Charikar;Mohammad Taghi Hajiaghayi;Howard J. Karloff;Satish Rao	2006	Algorithmica	10.1007/s00453-008-9191-1	time complexity;mathematical optimization;combinatorics;divide and conquer algorithms;interval graph;metric;order theory;mathematics;geometry;permutation;algorithm;semidefinite programming	Theory	20.525796719772284	27.06111524184153	69893
760b59f98afe5518aebb64407b5e5462a55bccc6	making the long code shorter	noise graph;cayley graphs;boolean hypercube;new code;unique games conjecture;eigenvalues;long code local testability;long code;approximation theory;log n;boolean algebra;matrix algebra;adjacency matrix;variable integrality gap;certain related cayley graph;small set expansion;linear code;game theory;sdp-sherali adams hierarchy;vertex graph;graph theory;known result;locally testable codes;linear constraints modulo;eigenvalues and eigenfunctions;hypercube networks;approximation hardness;hardness of approximation;computer science;cayley graph	The long code is a central tool in hardness of approximation, especially in questions related to the Unique Games Conjecture. We construct a new code that is exponentially more efficient, but can still be used in many of these applications. Using the new code we obtain exponential improvements over several known results, including the following: (1) For any ε > 0, we show the existence of an n-vertex graphG where every set of o(n) vertices has expansion 1−ε, but G’s adjacency matrix has more than exp(log n) eigenvalues larger than 1 − ε, where δ depends only on ε. This answers an open question of Arora, Barak, and Steurer [Proceedings of the 2010 IEEE 51st Annual Symposium on Foundations of Computer Science, 2010, pp. 563–572], who asked whether one can improve over the noise graph on the Boolean hypercube that has poly(log n) such eigenvalues. (2) A gadget that reduces Unique Games instances with linear constraints modulo K into instances with alphabet k with a blowup of kpolylog(K), improving over the previously known gadget with blowup of kΩ(K). (3) An n-variable integrality gap for Unique Games that survives exp(poly(log logn)) rounds of the semidefinite programming version of the Sherali–Adams hierarchy, improving on the previously known bound of poly(log logn). We show a connection between the local testability of linear codes and Small-Set Expansion in certain related Cayley graphs and use this connection to derandomize the noise graph on the Boolean hypercube.	adjacency matrix;code;hardness of approximation;linear programming relaxation;magma;modulo operation;semidefinite programming;symposium on foundations of computer science;time complexity;unique games conjecture	Boaz Barak;Parikshit Gopalan;Johan Håstad;Raghu Meka;Prasad Raghavendra;David Steurer	2015	SIAM J. Comput.	10.1137/130929394	game theory;mathematical optimization;combinatorics;discrete mathematics;graph theory;cayley graph;mathematics;algorithm;algebra	Theory	22.289752341150564	24.61559760470336	70440
4dd7bb84e62d253a810ad6df58f0b8ce52643a9a	time lower bounds for sorting on multi-dimensional mesh-connected processor arrays	parallel calculus;programme tri;time complexity;complexite calcul;complejidad calculo;limite inferior;sorting;programa ordenacion;procesador panel;computing complexity;tria;array processor;red mallada cerrada;multi dimensional;processeur tableau;sort routine;complexite temps;calculo paralelo;reseau maille;triage;meshed network;complejidad tiempo;calcul parallele;limite inferieure;lower bound	The problem of sorting on a mesh-connected processor array has been studied much [l-7,9-11]. It is known that (2d l)n steps are optimal computing time within the leading term for sorting nd items into d-dimensional snake-like order on the d-dimensional mesh-connected model [4,5,10]. However, upto now \ve do not know whether the snake-thke order is the best for sorting. A question whether the distance bound 2n 2 13 ultimately achievable on the n X n mesh-connected modei by using some super indetig scheme has been raised [7]. The authors of the present paper have shown that 2.2247~ steps are a time lower bound independent of indexing schemes for sorting n2 items on the n x 1 mesh-connected model El]. This lower bound has recently been improved to 2.2% steps [3]; The ez.k ~GC.~*~*e~~~ of a puor indexing scheme with 4n 26 3 time lower bound h;rs also been shod [l]. These results have bee. obtained using a new technique called the chain argument [l]. In this paper we develop the chain argument in order that we can apply its extended versiun to derive nontrivial lower bounds for sorting. We show a theorem that gives a relation between computing time for sorting nd items and the number of processors in a certain region of the mesh-connected model. ?Ve can numerically calculate the best lower bound obtainable from the theorem. For each d >, 2, our result is significantly better than the distance bound 2d.	central processing unit;numerical analysis;processor array;sorting	Yijie Han;Yoshihide Igarashi	1990	Inf. Process. Lett.	10.1016/0020-0190(90)90190-9	time complexity;combinatorics;computer science;sorting;calculus;mathematics;upper and lower bounds;algorithm	Theory	13.730819554120728	30.692295812881973	70448
cec11cad4c6a64ab0aa2214fc8d13d262c2f7fec	some connections between learning and optimization	computational complexity of learning;algoritmo aleatorizado;learning;complexite calcul;qa mathematics;combinatorial optimization problem;optimization method;algorithme randomise;metodo optimizacion;apprentissage machine;optimisation combinatoire;aprendizaje;49xx;optimization problem;probabilistic model;complejidad computacion;apprentissage;machine learning;computational complexity;informatique theorique;methode optimisation;modele probabiliste;randomized algorithm;optimization;combinatorial optimization;optimizacion combinatoria;computer theory;modelo probabilista;informatica teorica	This article is a brief exposition of some of the important links between machine learning and combinatorial optimization. We explain how efficient `learnability' in standard probabilistic models of learning is linked to the existence of efficient randomized algorithms for certain natural combinatorial optimization problems, and we discuss the complexity of some of these optimization problems.	mathematical optimization	Martin Anthony	2004	Discrete Applied Mathematics	10.1016/j.dam.2004.06.005	probabilistic-based design optimization;optimization problem;statistical model;mathematical optimization;combinatorial optimization;artificial intelligence;machine learning;mathematics;randomized algorithm;computational complexity theory;l-reduction;algorithm	ML	15.35931260968236	19.206331548830086	70455
0dff616806a4c7fc8d982e6c50bf605a0be76f9f	improving christofides' algorithm for the s-t path tsp	approximation algorithms;tsp traveling salesman problem;linear programming relaxations and rounding algorithms;theory;algorithms	We present a deterministic (1+√5/2)-approximation algorithm for the s-t path TSP for an arbitrary metric. Given a symmetric metric cost on n vertices including two prespecified endpoints, the problem is to find a shortest Hamiltonian path between the two endpoints; Hoogeveen showed that the natural variant of Christofides' algorithm is a 5/3-approximation algorithm for this problem, and this asymptotically tight bound in fact has been the best approximation ratio known until now. We modify this algorithm so that it chooses the initial spanning tree based on an optimal solution to the Held-Karp relaxation rather than a minimum spanning tree; we prove this simple but crucial modification leads to an improved approximation ratio, surpassing the 20-year-old ratio set by the natural Christofides' algorithm variant. Our algorithm also proves an upper bound of 1+√5/2 on the integrality gap of the path-variant Held-Karp relaxation. The techniques devised in this article can be applied to other optimization problems as well: these applications include improved approximation algorithms and improved LP integrality gap upper bounds for the prize-collecting s-t path problem and the unit-weight graphical metric s-t path TSP.	approximation algorithm;christofides algorithm;dijkstra's algorithm;file spanning;hamiltonian path;linear programming relaxation;mathematical optimization;minimum spanning tree	Hyung-Chan An;Robert D. Kleinberg;David B. Shmoys	2012	J. ACM	10.1145/2818310	mathematical optimization;combinatorics;discrete mathematics;christofides algorithm;computer science;mathematics;approximation algorithm;theory;algorithm	Theory	21.894118264854765	19.437818732541828	70466
1d59964610bebaf1f060f1443d332304b75caf4c	improved pseudorandom generators for combinatorial rectangles	complexite calcul;variable aleatoire;variable aleatoria;optimisation combinatoire;complejidad computacion;computational complexity;informatique theorique;random variable;combinatorial optimization;optimizacion combinatoria;computer theory;informatica teorica	We construct a pseudorandom generator which uses O(log m + log d + log 3=2 1==) bits and approximates the volume of any combinatorial rectangle in f1; : : : ; mg d to within error. O(log m + log d + log 2 1==) bits. For a subclass of rectangles with at most t log 1== nontriv-ial dimensions and each dimension being an interval, we also give a pseudorandom generator using O(log log d + log 1== log 1=2 t log 1==) bits, which again improves the previous upper bound O(log log d + log 1== log t log 1==)	pseudorandom generator;pseudorandomness	Chi-Jen Lu	1998		10.1007/BFb0055056	random variable;mathematical optimization;combinatorics;discrete mathematics;combinatorial optimization;mathematics;computational complexity theory;algorithm;statistics	Theory	17.06290476355783	25.600342678068973	70601
873518ae0f1601c57ebd8fd8cc8ec83a67a94cea	#bis-hardness for 2-spin systems on bipartite bounded degree graphs in the tree non-uniqueness region	004;spin systems approximate counting complexity bis hardness phase transition	"""Counting independent sets on bipartite graphs (#BIS) is considered a canonical counting problem of intermediate approximation complexity. It is conjectured that #BIS neither has an FPRAS nor is as hard as #SAT to approximate. We study #BIS in the general framework of two-state spin systems in bipartite graphs. Such a system is parameterized by three numbers (beta,gamma,lambda), where beta (respectively gamma) represents the weight of an edge (or """"interaction strength"""") whose endpoints are of the same 0 (respectively 1) spin, and lambda is the weight of a 1 vertex, also known as an """"external field"""". By convention, the edge weight with unequal 0/1 end points and the vertex weight with spin 0 are both normalized to 1. The partition function of the special case beta=1, gamma=0, and lambda=1 counts the number of independent sets. We define two notions, nearly-independent phase-correlated spins and symmetry breaking. We prove that it is #BIS-hard to approximate the partition function of any two-spin system on bipartite graphs supporting these two notions.#R##N##R##N#As a consequence, we show that #BIS on graphs of degree at most 6 is as hard to approximate as #BIS~without degree bound. The degree bound 6 is the best possible as Weitz presented an FPTAS to count independent sets on graphs of maximum degree 5. This result extends to the hard-core model and to other anti-ferromagnetic two-spin models. In particular, for all antiferromagnetic two-spin systems, namely those satisfying beta*gamma<1, we prove that when the infinite (Delta-1)-ary tree lies in the non-uniqueness region then it is #BIS-hard to approximate the partition function on bipartite graphs of maximum degree Delta, except for the case beta=gamma and lambda=1. The exceptional case is precisely the antiferromagnetic Ising model without an external field, and we show that it  has an FPRAS on bipartite graphs. Our inapproximability results match the approximability results of Li et al., who presented an FPTAS for general graphs of maximum degree Delta when the parameters lie in the uniqueness region."""		Jin-Yi Cai;Andreas Galanis;Leslie Ann Goldberg;Heng Guo;Mark Jerrum;Daniel Stefankovic;Eric Vigoda	2014		10.4230/LIPIcs.APPROX-RANDOM.2014.582	combinatorics;discrete mathematics;mathematics;maximal independent set	Theory	21.37763759028373	22.99740666167208	70865
05ecc1ab564f455b3334b6067bc2d33338d0fb5c	an optimal algorithm for shortest paths on weighted interval and circular-arc graphs, with applications	shortest path;circular arc graph;interval graph;linear time algorithm;single source shortest path;optimal algorithm	We give the first linear-time algorithm for computing single-source shortest paths in a weighted interval or circular-arc graph, when we are given the model of that graph, i.e., the actual weighted intervals or circular-arcsand the sorted list of the interval endpoints. Our algorithm solves this problem optimally inO(n) time, wheren is the number of intervals or circular-arcs in a graph. An immediate consequence of our result is anO(qn + n logn)-time algorithm for the minimum-weight circle-cover problem, whereq is the minimum number of arcs crossing any point on the circle; then logn term in this time complexity is from a preprocessing sorting step when the sorted list of endpoints is not given as part of the input. The previously best time bounds were0(n logn) for this shortest paths problem, andO(qn logn) for the minimum-weight circle-cover problem. Thus we improve the bounds of both problems. More importantly, the techniques we give hold the promise of achieving similar (logn)-factor improvements in other problems on such graphs.	preprocessor;shortest path problem;sorting algorithm;the circle (file system);time complexity	Mikhail J. Atallah;Danny Ziyi Chen;D. T. Lee	1995	Algorithmica	10.1007/BF01192049	block graph;mathematical optimization;suurballe's algorithm;combinatorics;discrete mathematics;interval graph;widest path problem;graph bandwidth;constrained shortest path first;longest path problem;floyd–warshall algorithm;computer science;pathfinding;euclidean shortest path;yen's algorithm;johnson's algorithm;mathematics;path;shortest path problem;distance;indifference graph;k shortest path routing;shortest path faster algorithm;algorithm;matching	Theory	20.95103136162958	23.803307538872257	71167
fc15afd9cf85925877f4480baf850be94a1f9929	computing the directed cartesian-product decomposition of a directed graph from its undirected decomposition in linear time	directed graphs;linear time algorithm;graph decompositions;cartesian product	In this paper, we design an algorithm that, given a directed graph G and the Cartesian-product decomposition of its underlying undirected graph G̃, produces the directed Cartesian-product decomposition of G in linear time. This is the first time that the linear complexity is achieved for this problem, which has two major consequences. Firstly, it shows that the directed and undirected versions of the Cartesian-product decomposition of graphs are linear-time equivalent problems. And secondly, as there already exists a linear-time algorithm for solving the undirected version of the problem, combined together, it provides the first lineartime algorithm for computing the directed Cartesian-product decomposition of a	adjacency list;algorithm;cartesian closed category;color;data structure;directed graph;graph (discrete mathematics);integer factorization;parsing;time complexity;vertex (graph theory)	Christophe Crespelle;Eric Thierry	2015	Discrete Mathematics	10.1016/j.disc.2015.06.001	transpose graph;graph power;mathematical optimization;robbins' theorem;combinatorics;discrete mathematics;feedback arc set;cycle rank;directed graph;null graph;comparability graph;cartesian product;mathematics;voltage graph;tree-depth;graph;moral graph;modular decomposition;path;arborescence;topological sorting;strongly connected component;tournament;tree;directed acyclic graph	Theory	23.49897365352289	27.017027125195575	71697
a866d6d4d7b64276e23615d8fb00f9f59905e610	ordered 3-colorings		We introduce three variants of proper 3-colorings of graphs. Motivation for these colorings comes from the problem of reconstructing haplotype structure via genotype data in bioinformatics [2,3]. We show relationship of these problems to other problems and study their computational complexities. Input graphs have vertices uniquely labeled with integers from 1 to |V (G)|. These labels will set some restrictions on the 3-colorings. Let the three colors be A, B, and C. In the first problem, we require every vertex colored A to have label smaller than label of every vertex colored B. We reduce this problem to 2-SAT, hence prove that it is in P . In the second problem, we require for every edge joining a vertex colored A to a vertex colored B that the label of the vertex colored A be smaller than the label of the vertex colored B. Note that a vertex colored A can have label greater that a vertex colored B provided they are not adjacent. We show that this problem is NP -complete by reducing a graph homomorphism problem into it. Then, we consider random graphs that have such a coloring (feasible random graphs) and describe an algorithm that colors almost all such graphs. In the third problem, we require for every bipartite component of the subgraph induced by vertices colored A or B, that labels of vertices colored A be smaller than labels of vertices colored B. Note that a vertex colored A Electronic Notes in Discrete Mathematics 22 (2005) 299–300	2-satisfiability;algorithm;analysis of algorithms;bioinformatics;color;discrete mathematics;graph coloring;graph homomorphism;ordered pair;random graph;vertex (geometry)	Arvind Gupta;Ján Manuch;Ladislav Stacho;Xiaohong Zhao	2005	Electronic Notes in Discrete Mathematics	10.1016/j.endm.2005.06.047	combinatorics;mathematics	Theory	24.594693276561298	23.982897364717935	71862
42b48be54a2982d1d50d1db7e104a016c1c2c1bf	a simple measure of the kolmogorov complexity	kolmogorov complexity	In this article we propose a simple method to estimate the Kolmogorov complexity of a finite word written over a finite alphabet. Usually it is estimated by the ratio of the length of a word’s archive to the original length of the word. This approach is not satisfactory for the theory of information because it does not give an abstract measure. Moreover Kolmogorov complexity approach is not satisfactory in the practical tasks of the compressibility estimation because it measures the potential compressibility by means of the compression itself. There is another measure of a word’s complexity subword complexity, which is equal to the number of different subwords in the word. We show the computation difficulties connected with the usage of subword complexity and propose a new simple measure of a word’s complexity, which is practically convenient development of the notion of subword complexity.	approximation algorithm;archive;computation;kolmogorov complexity;substring	Evgeny Ivanko	2009			kolmogorov structure function;kolmogorov equations;computer science;chain rule for kolmogorov complexity	NLP	11.06062817748209	26.512637885380258	71878
30baf825cd4e62be79f96b9003597c6eced99a0c	exact algorithms for cluster editing: evaluation and experiments	cutting plane;lower and upper bound;time complexity;exact solution;satisfiability;exact algorithm;linear program;dependent data;data reduction;integer linear program	We present empirical results for the CLUSTER EDITING problem using exact methods from fixed-parameter algorithmics and linear programming. We investigate parameter-independent data reduction methods and find that effective preprocessing is possible if the number of edge modifications k is smaller than some multiple of |V|. In particular, combining parameter-dependent data reduction with lower and upper bounds we can effectively reduce graphs satisfying k ≤ 25 |V|.#R##N##R##N#In addition to the fastest known fixed-parameter branching strategy for the problem, we investigate an integer linear program (ILP) formulation of the problem using a cutting plane approach. Our results indicate that both approaches are capable of solving large graphs with 1000 vertices and several thousand edge modifications. For the first time, complex and very large graphs such as biological instances allow for an exact solution, using a combination of the above techniques.		Sebastian Böcker;Sebastian Briesemeister;Gunnar W. Klau	2008		10.1007/978-3-540-68552-4_22	mathematical optimization;combinatorics;discrete mathematics;mathematics	OS	19.476251820299815	19.639048427200876	71936
a7d9e72d2b8af3a7e53f4090a09bb34d83808c5e	a lower bound on the edge linfinitely radius of saitou and nei's method for phylogenetic reconstruction	68w40;procesamiento informacion;algorithm analysis;voisinage;05c05;vecindad;performance;phylogenetic reconstruction;neighbourhood;68wxx;analysis of algorithms;evolutionary tree;upper bound;reconstruction phylogenetique;evolutionary trees;informatique theorique;information processing;borne inferieure;68q25;analyse algorithme;neighbor joining;arbre evolutionniste;rendimiento;borne superieure;traitement information;analisis algoritmo;lower bound;cota superior;cota inferior;computer theory;informatica teorica	In this paper, we study the performance of Saitou and Nei’s neighbor-joining method for phylogenetic reconstruct show that the edge l∞ radius of the method is at least 6. This partially answers a question by Atteson (1999). Previously, an upper bound of 4 was known.  2005 Elsevier B.V. All rights reserved.	neighbor joining;phylogenetics	Yin-Feng Xu;Wenqiang Dai;Binhai Zhu	2005	Inf. Process. Lett.	10.1016/j.ipl.2005.02.006	phylogenetic tree;information processing;computer science;artificial intelligence;mathematics;upper and lower bounds;algorithm	Theory	20.71608751557436	27.24671172403683	72180
c527ac8ec362c5d86d0de22b4c2736bd4f2cfeec	fast algorithms for two dimensional and multiple pattern matching (preliminary version)	preliminary version;multiple pattern matching;fast algorithm;fast algorithms;string matching;search algorithm;pattern matching;probability theory	A new algorithm for searching a two dimensional m × m pattern in a two dimensional n × n text is presented. It performs on the average less comparisons than r, 2 the size of the text: ~using rn 2 extra space. It improves on previous results in both time: n 2 and space: n or m 2, and is nearly optimal. Basically, it uses multiple string matching on only n/m rows of the text. We also present a new multi-string searching algorithm based in the Boyer-Moore string searching algorithm. A theoretical analysis is performed relying on original methods, word enumeration and probability theory, quite new in this field, and powerful. The average number of comparisons is proved to be asymptotically c~-~-, c~ < I. 1 I n t r o d u c t i o n Text searching is a very important component in many areas, including information retrieval, programming languages, and text processing. We address two problems: the search of two m × m dimensional patterns in two dimensional n × n objects (texts), and the multiple string searching problem: search several strings simultaneously in a text. Between the applications of the first problem, we have the detection of edges in digitM pictures, local conditions in games, and other image processing and pattern recognition problems. Among the applications of the second, let us cite bibliographic searches, text editing, and other information retrieval problems. We propose two new algorithms. Our algorithm to search two dimensional (2D) patterns (see Section 3) is based on any multiple string searching algorithm, and we also propose a new such algorithm in Section 4. Then, we extensively discuss performances. We show that using a multi-string searching algorithm yields a 2D algorithm optimal in the worst case (linear). Moreover, it improves drastically on all other algorithms in the	best, worst and average case;boyer–moore string search algorithm;fast fourier transform;image processing;information retrieval;pattern matching;pattern recognition;performance;programming language;regular expression;string searching algorithm;text editor	Ricardo A. Baeza-Yates;Mireille Régnier	1990		10.1007/3-540-52846-6_102		Theory	12.888249201039788	27.670121313138996	72333
085a2728c7df40ef37ed378dba28a96120efe946	reconciling multiple genes trees via segmental duplications and losses		Reconciling gene trees with a species tree is a fundamental problem to understand the evolution of gene families. Many existing approaches reconcile each gene tree independently. However, it is well-known that the evolution of gene families is interconnected. In this paper, we extend a previous approach to reconcile a set of gene trees with a species tree based on segmental macroevolutionary events, where segmental duplication events and losses are associated with cost δ and λ, respectively. We show that the problem is polynomial-time solvable when δ ≤ λ (via LCA-mapping), while if δ > λ the problem is NP-hard, even when λ = 0 and a single gene tree is given, solving a long standing open problem on the complexity of the reconciliation problem. On the positive side, we give a fixed-parameter algorithm for the problem, where the parameters are δ/λ and the number d of segmental duplications, of time complexity O(d δ λe d · n · δ λ ). Finally, we demonstrate the usefulness of this algorithm on two previously studied real datasets: we first show that our method can be used to confirm or refute hypothetical segmental duplications on a set of 16 eukaryotes, then show how we can detect whole genome duplications in yeast genomes. 2012 ACM Subject Classification F.2.2 Nonnumerical Algorithms and Problems, G.2.1 Combinatorics, G.2.2 Graph Theory, J.3 Life and Medical Sciences		Riccardo Dondi;Manuel Lafond;Céline Scornavacca	2018		10.4230/LIPIcs.WABI.2018.5	time complexity;combinatorics;open problem;genome;gene family;gene;biology;lambda;segmental duplication;pattern recognition;artificial intelligence	Comp.	17.980754300603866	21.746815085741154	72345
e9ad7e6accd91baaab86e006434ea838bb517490	strategic directions in research in theory of computing	modelizacion;programme recherche;algorithmique;securite;complexite calcul;etude experimentale;information retrieval;activity;reseau ordinateur;search strategy;actividad;computer network;modelisation;modelo;complejidad computacion;theory of computing;algorithmics;algoritmica;recherche information;computational complexity;estructura datos;activite;safety;strategie recherche;red ordenador;programa investigacion;structure donnee;modele;recuperacion informacion;seguridad;modeling;data structure;estudio experimental;research program;models;estrategia investigacion	This report focuses oll two core areas of theory of computing: discrete algorithms and computational complexity theory. The report reviews the purposes and goals of theoretical research, summarizes selected past and recent achievements, explains the importance of sustaining core research, and identifies promising opportunities for filture research. Some research opportunities build bridges between theory of computing and other areas of computer science, and other science and engineering disciplines.	theory of computing	Michael C. Loui	1996	ACM Comput. Surv.	10.1145/242223.242240	systems modeling;data structure;computer science;artificial intelligence;programming language;computational complexity theory;operations research;algorithmics;thermodynamic activity	Theory	15.936009044876714	21.328843399851383	72433
1059fed9882e959e0f5ecc4e5f386216d02354a5	parallel dynamic programming	dynamic programming concurrent computing parallel processing very large scale integration binary search trees phase change random access memory labeling conferences computer architecture histograms;parallelisme;dynamic programming;distributed system;algoritmo paralelo;histograms;pram;programacion dinamica;systeme reparti;parallel algorithm;concurrent computing;algorithm analysis;time complexity;binary search trees;very large scale integration;reduction;parallel dynamicprogramming;dynamic program;optimal binary search tree;phase change random access memory;algorithme parallele;optimal triangulation ofpolygons;parallel dynamic programming;computer architecture;parallelism;sistema repartido;paralelismo;binary search tree;computational complexity;crew pram algorithm;programmation dynamique;index termsdynamic programming;reduccion;analyse algorithme;optimal triangulation of polygons;matrix multiplication;procesador;parallel algorithms dynamic programming computational complexity;processeur;time complexity parallel dynamic programming matrix multiplication optimal binary search tree optimal triangulation of polygons crew pram algorithm;analisis algoritmo;parallel processing;processor;labeling;conferences;parallel algorithms	Algorithm design paradigms are particularly useful for designing new and e cient algorithms. However, several sequential algorithm design paradigms seem to fail in the design of e cient parallel algorithms. This dissertation focuses on the dynamic programming paradigm, which until recently has only been used to design sequential algorithms. A graph structure is given that allows the e cient parallel solution of some problems amenable to the dynamic programming paradigm. Using these graphs we show that dynamic programming is a viable parallel algorithm design paradigm. Several new parallel algorithms are given for two well-known optimization problems. First an approximation algorithm is given. Then an algorithm that works by nding shortest paths in special graphs is given. Finally, the last two and most e cient of these parallel algorithms are given. These algorithms work by using new and e cient techniques for exploiting monotonic problem constraints. xii	acta informatica;algorithm design;algorithmic paradigm;algorithmica;all nearest smaller values;analysis of algorithms;approximation algorithm;archi;automata theory;automaton;belief revision;bellman equation;chou's invariance theorem;communications of the acm;compendium;complex systems;computational geometry;computers and intractability: a guide to the theory of np-completeness;concave function;convex function;coppersmith–winograd algorithm;dynamic programming;entity–relationship model;european association for theoretical computer science;greedy algorithm;handbook;holographic principle;iceland spar;introduction to algorithms;john d. wiley;john reif;journal of symbolic computation;journal of the acm;kosaraju's algorithm;lecture notes in computer science;local optimum;mathematical optimization;matrix multiplication;maxima and minima;michael garey;parallel algorithm;parallel random-access machine;proceedings of the ieee;programming paradigm;quadrangle (geography);raman scattering;random access;random-access memory;randomized algorithm;siam journal on computing;spaa;stacs;swat;sequential algorithm;sethi–ullman algorithm;shannon (unit);shared memory;shor's algorithm;shortest path problem;sorting;springer (tank);sudoku solving algorithms;symposium on discrete algorithms;symposium on foundations of computer science;symposium on theory of computing;the matrix;thomas j. watson research center;turing machine;well-formed formula;yao graph;monotone	Shou-Hsuan Stephen Huang;Hongfei Liu;Venkatraman Viswanathan	1994	IEEE Trans. Parallel Distrib. Syst.	10.1109/71.277784	parallel processing;parallel computing;binary search tree;concurrent computing;computer science;theoretical computer science;database;distributed computing;parallel algorithm;programming language;algorithm	Theory	15.448606429422242	25.535583850114932	72622
22fa19e805373296d806cfade9187d5c2ed5027e	computing the threshold for q-gram filters	camino mas corto;shortest path;approximate string matching;seuil;chaine caractere;threshold;plus court chemin;methode calcul;metodo calculo;general methods;probleme plus court chemin avec contrainte;cadena caracter;q gram filter;appariement chaine;umbral;string matching;computing method;shortest path problem;character string	A popular and much studied class of filters for approximate string matching is based on finding common q-grams, substrings of length q, between the pattern and the text. A variation of the basic idea uses gapped q-grams and has been recently shown to provide significant improvements in practice. A major difficulty with gapped q-gram filters is the computation of the so-called threshold which defines the filter criterium. We describe the first general method for computing the threshold for q-gram filters. The method is based on a carefully chosen precise statement of the problem which is then transformed into a constrained shortest path problem. In its generic form the method leaves certain parts open but is applicable to a large variety of q-gram filters and may be extensible even to other classes of filters. We also give a full algorithm for a specific subclass. For this subclass, the algorithm has been implemented and used succesfully in an experimental comparison.	approximate string matching;computation;grams;n-gram;shortest path problem;string searching algorithm;substring	Juha Kärkkäinen	2002		10.1007/3-540-45471-3_36	combinatorics;computer science;calculus;mathematics;shortest path problem;programming language;algorithm	ML	14.492826382048007	26.181361956760693	72624
aa12d683e27e53ae549379b88992c31513dc2594	fixed-parameter tractable algorithms for testing upward planarity	fixed parameter tractable	We consider the problem of testing a digraph G = (V,E) for upward planarity. In particular we present two fixed-parameter tractable algorithms for testing the upward planarity of G. Let n = |V|, let t be the number of triconnected components of G, and let c be the number of cut-vertices of G. The first upward planarity testing algorithm we present runs in O(2t · t! · n2)–time. The previously known best result is an O(t! · 8t · n3 + 23·2c · t3·2c · t! · 8t · n)-time algorithm by Chan. We use the kernelisation technique to develop a second upward planarity testing algorithm which runs in O(n2 + k4(2k + 1)!) time, where k = |E| – |V|. We also define a class of non upward planar digraphs.	algorithm;hasse diagram;left-right planarity test	Patrick Healy;Karol Lynch	2005		10.1007/978-3-540-30577-4_23	combinatorics;discrete mathematics;computer science;mathematics;algorithm	ML	22.827570338010464	24.629963578149646	72633
4e3648063ba21493861299b3dfb5931a12ea9f02	an algorithm for finding k minimum spanning trees	kth minimum spanning tree;computational complexity;minimum spanning tree;graph algorithm	This paper presents an algorithm for finding K minimum spanning trees in an undirected graph. The required time is $O(Km + \min (n^2 ,m\log \log n))$ and the space is $O(K + m)$, where n is the number of vertices and m is the number of edges. The algorithm is based on three subroutines. The first two subroutines are used to obtain the second minimum spanning tree in $O(\min (n^2 ,m\alpha (m,n)))$ steps, where $\alpha (m,n)$ is Tarjan’s inverse of Ackermann’s function [12] which is very slowly growing. The third one obtains the kth minimum spanning tree in $O(m)$ steps when the jth minimum spanning trees for $j = 1,2, \cdots ,k - 1$ are given.	algorithm;minimum spanning tree	Naoki Katoh;Toshihide Ibaraki;Hisashi Mine	1981	SIAM J. Comput.	10.1137/0210017	euclidean minimum spanning tree;mathematical optimization;combinatorics;discrete mathematics;kruskal's algorithm;minimum degree spanning tree;spanning tree;computer science;minimum spanning tree;mathematics;reverse-delete algorithm;computational complexity theory;algorithm;shortest-path tree	Theory	20.114842304993505	25.863157127609906	72640
210371388cf261a76d4ae5e5d9a3524d0b70bc05	dna solution of a graph coloring problem	graph coloring problem	The graph-theoretic parameter that has probably received the most attention over the years is the chromatic number. As is well-known, the coloring problem is an NP-Complete problem. In this paper, it has been solved by means of molecular biology techniques. The algorithm is highly parallel and has satisfactory fidelity. This work shows further evidence for the ability of DNA computing to solve NP-Complete problems.	algorithm;complete (complexity);computation (action);dna computing;graph - visual representation;graph coloring;graph theory;molecular biology;np-completeness;population parameter	Yachun Liu;Jin Xu;Linqiang Pan;Shiying Wang	2002	Journal of chemical information and computer sciences	10.1021/ci010016o	graph power;mathematical optimization;combinatorics;discrete mathematics;chemistry;fractional coloring;computer science;complete coloring;edge coloring;graph coloring;graph factorization;mathematics;list coloring;greedy coloring	Theory	23.32892654932637	25.608656846884326	72667
f63a77b1a6b03e28bd53ac294979d7e3c3c89305	predicting model and algorithm in rna folding structure including pseudoknots		The prediction of RNA structure with pseudoknots is a nondeterministic polynomial-time hard (NP-hard) problem; according to minimum free energy models and computational methods, we investigate the RNA-pseudoknotted structure. Our paper presents an efficient algorithm for predicting RNA structure with pseudoknots, and the algorithm takes O(n3) time and O(n2) space, the experimental tests in Rfam10.1 and PseudoBase indicate that the algorithm is more effective and precise. The predicting accuracy, the time complexity and space complexity outperform existing algorithms, such as Maximum Weight Matching (MWM) algorithm, PKNOTS algorithm and Inner Limiting Layer (ILM) algorithm, and the algorithm can predict arbitrary pseudoknots. And there exists a 1+e (eu003e0) polynomial time approximation scheme in searching maximum number of stackings, and we give the proof of the approximation scheme in RNA-pseudoknotted structure. We have improved several types of pseudoknots considered in RNA folding structure, and analyze ...	algorithm	Zhendong Liu;Daming Zhu;Qionghai Dai	2018	IJPRAI	10.1142/S0218001418510059	mathematics;nondeterministic algorithm;time complexity;polynomial-time approximation scheme;rna;nucleic acid structure;existential quantification;algorithm;limiting	NLP	16.187017899549446	23.005738100622814	72792
45ea07abc82f4d45ab8166113f351d7a53dbbef1	이동객체 데이타베이스에서 tp 최근접 쌍 질의의 처리	유효시간;yongzhe quan;vol 30 no 2 ii;tp nn 질의;jinuk bae;time parameterized closest pair queries in moving object database;influence time;tp query;tp 최근접쌍 질의;tp 질의;thesis;이동객체 데이타베이스에서 tp 최근접 쌍 질의의 처리;이석호;korea information science society;권영철;sukho lee;tp window query;tp closest pair query;tp 범위 질의;한국정보과학회 2003년도 가을 학술발표논문집 제30권 제2호 ii;배진욱;한국정보과학회;tp nn query	최근 들어 위치정보 시스템(GIS)의 발전으로 움직이는 물체의 위치는 쉽게 알 수 있게 되어서 이를 기반으로 미래의 최근접 쌍을 찾는 질의가 중요하게 되었다. 하지만 이동객체는 계속해서 움직이므로 현재의 질의 결과는 시간이 흐름에 따라 유효하지 않게 된다. 본 논문에서는 사용자에게 현재의 가장 인접한 쌍과 이 인접한 쌍이 유효한 시간, 그리고 그 유효한 시간 후에 바뀐 결과를 알려주는 질의를 효율적으로 처리할 수 있는 알고리즘을 제안한다.		권영철;배진욱;이석호	2003		10.1007/978-0-387-39940-9_3850	computer science;data mining;database;algorithm	NLP	14.803782641148993	25.18576070660313	72808
b8870597558890cecdc5570010224a41c00b399b	t-theory applications to online algorithms for the server problem	online algorithm;discrete mathematics;indexation	Although largely unnoticed by the online algorithms community, T-theory, a field of discrete mathematics, has contributed to the development of several online algorithms for the k-server problem. A brief summary of the k-server problem, and some important application concepts of T-theory, are given. Additionally, a number of known k-server results are restated using the established terminology of T-theory. Lastly, a previously unpublished 3-competitiveness proof, using T-theory, for the Harmonic algorithm for two servers is presented. Keywords/Phrases: Double Coverage, Equipoise, Harmonic, Injective Hull, Isolation Index, Prime Metric, Random Slack, T-Theory, Tight Span, Trackless Online Algorithm School of Computer Science, University of Nevada, Las Vegas, NV 89154. Email: larmore@cs.unlv.edu. Research supported by NSF grant CCR-0312093. School of Computer Science, University of Nevada, Las Vegas, NV 89154. Email: james.oravec@gmail.com. Research supported by NSF grant CCR-0312093.	computer science;discrete mathematics;email;ibm notes;k-server problem;nv network;online algorithm;server (computing);slack variable;t-theory	Lawrence L. Larmore;James A. Oravec	2006	CoRR		online algorithm;mathematical optimization;combinatorics;discrete mathematics;computer science;theoretical computer science;mathematics;k-server problem;algorithm	Theory	20.145012808705957	19.650098780499402	72872
ac39ac45741781792028a83cb726e924a78c6020	sorting multisets stably in minimum space	stable selection;programme tri;complexite calcul;selection;programa ordenacion;optimal worst case time;minimo;complejidad computacion;sort routine;programme utilitaire;utility program;computational complexity;minimum;stable unpartitioning;model of computation;seleccion;algoritmo optimo;algorithme optimal;optimal algorithm;lower bound;programa utilitario;minimum space	We consider the problem of sorting a multiset of sizen containingm distinct elements, where theith distinct element appearsn i times. Under the assumption that our model of computation allows only the operations of comparing elements and moving elements in the memory,Ω(n logn − ∑ i=1 m n i logn i +n) is known to be a lower bound for the computational complexity of the sorting problem. In this paper we present aminimum space algorithm that sortsstably a multiset in asymptoticallyOptimal worst-case time. A Quicksort type approach is used, where at each recursive step the median is chosen as the partitioning element. To obtain a stable minimum space implementation, we develop linear-time in-place algorithms for the following problems, which have interest of their own: Stable unpartitioning: Assume that ann-element arrayA is stably partitioned into two subarraysA 0 andA 1. The problem is to recoverA from its constitutentsA 0 andA 1. The information available is the partitioning element used and a bit array of sizen indicating whether an element ofA 0 orA 1 was originally in the corresponding position ofA. Stable selection: The task is to find thekth smallest element in a multiset ofn elements such that the relative order of identical elements is retained.	best, worst and average case;bit array;computational complexity theory;in-place algorithm;model of computation;quicksort;recursion;sorting;stable marriage problem;time complexity	Jyrki Katajainen;Tomi Pasanen	1994	Acta Informatica	10.1007/BF01178508	model of computation;selection;mathematical optimization;combinatorics;computer science;mathematics;upper and lower bounds;computational complexity theory;algorithm	Theory	14.987517952816793	30.155172183942074	73038
55006bc65fde3eaa452cac27e9bf7b8c180e8e52	on unknown small subsets and implicit measures: new techniques for parameterized algorithms	期刊论文	Parameterized computation is a recently proposed alternative approach to dealing with NP-hard problems. Developing efficient parameterized algorithms has become a very active research area in the current research in theoretical computer science. In this paper, we investigate a number of new algorithmic techniques that were proposed and initiated by ourselves in our research in parameterized computation. The techniques have proved to be very useful and promising, and have led to improved parameterized algorithms for many well-known NP-hard problems.	approximation algorithm;computation;np-hardness;parameterized complexity;theoretical computer science	Jianer Chen;Qilong Feng	2014	Journal of Computer Science and Technology	10.1007/s11390-014-1474-1	parameterized complexity;mathematical optimization;computer science;theoretical computer science;algorithm	Theory	18.53150823688277	20.207905013206414	73039
6a597bb1c50bd24086a8095918abc8b30a06abff	constant time parallel sorting: an empirical view	parallel sorting	It is well known that sorting can be done with O(n log n) comparisons. It is also known that (in the comparison decision tree model) sorting requires Ω(n log n) comparisons. What happens if you allow massive parallelism? In the extreme case you can sort n elements in one round by using (n 2 ) processors to make all the comparisons at once. It is easy to show that sorting in one round requires (n 2 ) processors. Can you sort in two rounds with a subquadratic number of processors? What about k rounds? We survey the known literature and discuss simulations of these algorithms that we have carried out. One of our main points will be that nonconstructive algorithms can be useful. We use the parallel decision tree model introduced by Valiant [25]. If p processors are used then every node is a set of p comparisons and has 2p children corresponding to all possible answers. We think of a node as having information about how the comparisons that led to that node were answered (formally a DAG on {x1, . . . , xn}) and all information derivable from that information (formally the transitive closure of that dag). The	algorithm;approximation;central processing unit;decision tree model;dijkstra's algorithm;directed acyclic graph;genetic algorithm;graph coloring;linear temporal logic to büchi automaton;newton's method;parallel computing;power of two;preprocessor;randomized algorithm;recursion;simulation;sorting;transitive closure	William I. Gasarch;Evan Golub;Clyde P. Kruskal	2003	J. Comput. Syst. Sci.	10.1016/S0022-0000(03)00040-0	computer science;theoretical computer science;analysis of parallel algorithms;algorithm	Theory	14.389944867984646	30.039993312390195	73044
d0e615d2f56264f627bd735312255d52d3ab25ab	verifiable stream computation and arthur-merlin communication	004;qa76 electronic computers computer science computer software;arthur merlin communication complexity streaming interactive proofs;streaming interactive proofs;arthur merlin communication complexity	In the setting of streaming interactive proofs (SIPs), a client (verifier) needs to compute a given function on a massive stream of data, arriving online, but is unable to store even a small fraction of the data. It outsources the processing to a third party service (prover), but is unwilling to blindly trust answers returned by this service. Thus, the service cannot simply supply the desired answer; it must convince the verifier of its correctness via a short interaction after the stream has been seen. In this work we study “barely interactive” SIPs. Specifically, we show that two or three rounds of interaction suffice to solve several query problems—including Index, Median, Nearest Neighbor Search, Pattern Matching, and Range Counting—with polylogarithmic space and communication costs. Such efficiency with O(1) rounds of interaction was thought to be impossible based on previous work. On the other hand, we initiate a formal study of the limitations of constant-round SIPs by introducing a new hierarchy of communication models called Online Interactive Proofs (OIPs). The online nature of these models is analogous to the streaming restriction placed upon the verifier in an SIP. We give upper and lower bounds that (1) characterize, up to quadratic blowups, every finite level of the OIP hierarchy in terms of other well-known communication complexity classes, (2) separate the first four levels of the hierarchy, and (3) reveal that the hierarchy collapses to the fourth level. Our study of OIPs reveals marked contrasts and some parallels with the classic Turing Machine theory of interactive proofs, establishes limits on the power of existing techniques for developing constant-round SIPs, and provides a new characterization of (non-online) Arthur–Merlin communication in terms of an online model. ∗This work was supported in part by the Simons Institute for the Theory of Computing, and was performed while the authors were visiting the Institute. †Department of Computer Science, Dartmouth College. Supported in part by NSF grant CCF-1217375. ‡Department of Computer Science, University of Warwick. §Department of Computer Science, UMass Amherst. Supported in part by NSF grant IIS-1251110. ¶Simons Institute for the Theory of Computing, UC Berkeley. Supported by a Research Fellowship from the Simons Institute for the Theory of Computing. ‖School of Computing, University of Utah. Supported in part by NSF grant IIS-1251049 and the Simons Institute for the Theory of Computing. ISSN 1433-8092 Electronic Colloquium on Computational Complexity, Report No. 86 (2014)	am broadcasting;arthur–merlin protocol;cobham's thesis;communication complexity;complexity class;computation;computer science;computer-mediated communication;correctness (computer science);electronic colloquium on computational complexity;ibm notes;interactive proof system;international standard serial number;keneth alden simons;nearest neighbor search;parallels desktop for mac;pattern matching;polylogarithmic function;simulation;streaming media;theory of computing;turing machine;uc browser	Amit Chakrabarti;Graham Cormode;Andrew McGregor;Justin Thaler;Suresh Venkatasubramanian	2014		10.4230/LIPIcs.CCC.2015.217	computer science;artificial intelligence;theoretical computer science;machine learning;mathematics;distributed computing;algorithm;statistics	Theory	11.683171586640958	21.89987479865082	73080
b6f55c4d3f8d978fdef20a7ada77c10637d4132c	two-page book embedding of trees under vertex-neighborhood constraints	arbre graphe;book embedding;embedding;tree graph;deploiement;temps lineaire;circuit vlsi;livre;vlsi circuit;libro;plongement;inmersion;circuito vlsi;book;arbol grafo;algoritmo optimo;algorithme optimal;optimal algorithm;2 pages	We study the VLSI-related problem of embedding .graphs in books. A book embedding of a graph G=(V,E) consists.of two parts; namely, (1) an ordering of ~ along the spine of the book, and (2) an assignment of each ee E to a page of the book, so that edges assigned to the same page do nOt int&sect. In devising an.embedding, one seeks to minimize the number of pages used. A black/white (blw) graph is a pair (G,U), where G is a graph and U~V is a subset of distinguished black vertices (the vertices of V-U are called white). A black/white (blw) book embedding of a blw graph (G ,U) is a book embedding of G, where the vertices of U are placed consecutively on the spine. The 'need for b/w embeddings may arise, for example, when the input p<rts of a multilayer VLSI chip are to be separated from the output ports. In this paper we prove that every b/w tree admits a two--page b/w embedding. The proof takes the form of a linear time algorithm, which uses an extension of the unfolding technique inttoduced in [MW]. Combining this algorithm with the one in [MW] results in a linear time algorithm for optimal b/w embedding of trees. Technion Computer Science Department Technical Report CS0552 1989	algorithm;book embedding;computer science;microwave;time complexity;unfolding (dsp implementation);vertex (geometry);very-large-scale integration	Shlomo Moran;Yaron Wolfsthal	1993	Discrete Applied Mathematics	10.1016/0166-218X(93)90114-4	mathematical optimization;combinatorics;discrete mathematics;graph embedding;computer science;embedding;mathematics;tree;book embedding;algorithm	Theory	22.41490074745012	25.54370562474113	73160
56d957ec64a7ab2ac63c1856af5db3f9beb0dab6	efficient noise-tolerant learning from statistical queries	learning algorithm;learning model;graph minors;artificial intelligent;theory of computing;machine learning;relational model;pattern recognition;treewidth;pathwidth;statistical query;information theoretic;computational learning theory;graph algorithms;probability and statistics;partial k trees	In this paper, we study the extension of Valiant’s learning model [25] in which the positive or negative classificw tion label provided with each random example may be corrupted by random noise. This extension was first examined in the learning theory literature by Angluin and Laird [1], who formalized the simplest type of white label noise and then sought algorithms tolerating the highest possible rate of noise. In addition to being the subject of a number of the oretical studies [1, 15, 24, 11], the classification noise model has become a common paradigm for experimental machine learning research. Angluin and Laird provided an algorithm for learning boolean conjunctions that tolerates a noise rate approaching the information-theoretic barrier of 1/2, Subsequently, there have been some isolated instances of efficient noisetolerant algorithms [14, 20, 22], but little work on characterizing which classes can be efficiently learned in the presence of noise, and no general transformations of Valiant model algorithms into noise-tolerant algorithms. The primary contribution of the present paper is in making significant progress in both of these areas, We identify and formalize an apparently rather weak sufficient condition on learning algorithms in VaJiant’s model that permits the immediate derivation of nois~tolerant learning algorithms, More precisely, we define a nat urrd restriction on Valiant model algorithms that allows them to be reliably and efficiently simulated in the presence of arbitrarily large rates of clzasification noise. This allows us to obtain efficient noise-tolerant learning algorithms for practically every concept class for which an efficient learning algorithm in the original noise-free Valiant model is known, A notable exception is the class of parity functions, whose properties we investigate in some detail. our sufficient condition is formalized by the introduction of a new model of learning from statistical queries, in which the standard Valiant model oracle EX(~, D) (giving random examples of the target function j with respect to an input distribution D over X) is replaced by the weaker oracle flTAZ’(~, D). This oracle, rather than supplying the learning algorithm with individual random examples, instead provides accurate estimates for probabilities over the sample space generated by EX ( f, D). Taking as input a query of the form (x, a), where x = X(Z, 1) is any boolean function over inputs z G X and 1?c {O, 1}, STA2’(f, D) returns an	algorithm;concept class;information theory;machine learning;network address translation;noise (electronics);parity function;programming paradigm;simulation	Michael Kearns	1993		10.1145/167088.167200	pathwidth;probability and statistics;combinatorics;relational model;computer science;theoretical computer science;machine learning;pattern recognition;mathematics;treewidth;computational learning theory	Theory	10.050401546269889	19.692965401540757	73422
ab109f8c95bfa90313f4c19dea562f0a0f343427	an optimal randomized cell probe lower bound for approximate nearest neighbor searching	65dxx;informatica;pire cas;search problem;lenguaje programacion;prueba;eliminacion;programming language;complexite calcul;geometrie algorithmique;sonda;aproximacion;cell probe model;52xx;computational geometry;communication complexity;complexite communication;pregunta documental;65d18;automaton;calculo automatico;problema investigacion;geometria discreta;probe;68wxx;recherche;computing;approximation;calcul automatique;automata;aleatorizacion;complejidad computacion;65d99;68q17;preuve;computational complexity;geometrie discrete;automate;discrete geometry;68xx;borne inferieure;palabra;randomized algorithm;query;68p05;algorithme polynomial;randomisation;langage programmation;approximate nearest neighbor;geometria computacional;word;informatique;nearest neighbor search;elimination;computer science;68u05;theorie information;68n15;randomization;probleme recherche;proof;investigacion;sonde;68w20;round elimination;lower bound;information theory;requete;mot;cota inferior;teoria informacion	We consider the approximate nearest neighbor search problem on the Hamming cube $\{0,1\}^d$. We show that a randomized cell probe algorithm that uses polynomial storage and word size $d^{O(1)}$ requires a worst case query time of $\Omega({\rm log}\,{\rm log}\,d/{\rm log}\,{\rm log}\,{\rm log}\,d)$. The approximation factor may be as loose as $2^{{\rm log}^{1-\eta}d}$ for any fixed $\eta>0$. Our result fills a major gap in the study of this problem since all earlier lower bounds either did not allow randomization [A. Chakrabarti et al., A lower bound on the complexity of approximate nearest-neighbor searching on the Hamming cube, in Discrete and Computational Geometry, Springer, Berlin, 2003, pp. 313-328; D. Liu, Inform. Process. Lett., 92 (2004), pp. 23-29] or did not allow approximation [A. Borodin, R. Ostrovsky, and Y. Rabani, Proceedings of the 31st Annual ACM Symposium on Theory of Computing, 1999, pp. 312-321; O. Barkol and Y. Rabani, Proceedings of the 32nd Annual ACM Symposium on Theory of Computing, 2000, pp. 388-396; T. S. Jayram et al., J. Comput. System Sci., 69 (2004), pp. 435-447]. We also give a cell probe algorithm that proves that our lower bound is optimal. Our proof uses a lower bound on the round complexity of the related communication problem. We show, additionally, that considerations of bit complexity alone cannot prove any nontrivial cell probe lower bound for the problem. This shows that the “richness technique” [P. B. Miltersen et al., J. Comput. System Sci., 57 (1998), pp. 37-49] used in a lot of recent research around this problem would not have helped here. Our proof is based on information theoretic techniques for communication complexity, a theme that has been prominent in recent research [A. Chakrabarti et al., Proceedings of the 42nd Annual IEEE Symposium on Foundations of Computer Science, 2001, pp. 270-278; Z. Bar-Yossef et al., Proceedings of the 43rd Annual IEEE Symposium on Foundations of Computer Science, 2002, pp. 209-218; P. Sen, Proceedings of the 18th Annual IEEE Conference on Computational Complexity, 2003, pp. 73-83; R. Jain, J. Radhakrishnan, and P. Sen, Proceedings of the 30th International Colloquium on Automata, Languages and Programming, 2003, pp. 300-315].	randomized algorithm	Amit Chakrabarti;Oded Regev	2010	SIAM J. Comput.	10.1137/080729955	discrete geometry;mathematical optimization;combinatorics;information theory;computational geometry;computer science;artificial intelligence;mathematics;automaton;algorithm	Theory	15.289031079823928	25.048102537893783	73423
0c89bd305bcb39a8eec8f9adf5a6c0ada332e7a1	assessing student answers to balanced tree problems		Problems in the domain of balanced binary tree operations usually involve the students constructing a sequence of transformations to insert or delete a value. An Intelligent Tutoring System (ITS) in this area must be able to perform.	self-balancing binary search tree	Chun Wai Liew;Huy Nguyen;Darren J. Norton	2017		10.1007/978-3-319-61425-0_57	machine learning;binary tree;artificial intelligence;computer science;data structure;intelligent tutoring system	NLP	14.642841784765045	28.77180723289138	73546
9a0333450970a4284c905732b150bd818d4e7d74	ptas for map assignment on pairwise markov random fields in planar graphs		We present a PTAS for computing the maximum a posteriori assignment on Pairwise Markov Random Fields with non-negative weights in planar graphs. This algorithm is practical and not far behind state-of-the-art techniques in image processing. MAP on Pairwise Markov Random Fields with (possibly) negative weights cannot be approximated unless P = NP, even on planar graphs. We also show via reduction that this yields a PTAS for one scoring function of Correlation Clustering in planar graphs.	approximation algorithm;correlation clustering;image processing;markov chain;markov random field;p versus np problem;ptas reduction;planar graph;scoring functions for docking	Eli Fox-Epstein;Roie Levin;David Meierfrankenfeld	2015	CoRR		mathematical optimization;combinatorics;discrete mathematics;mathematics	ML	24.583232662605464	21.014984502868725	73792
65b6a041c18a1e560ef4df7520684c45cd0318cc	hardness of improper one-sided learning of conjunctions for all uniformly falsifiable csps		We consider several closely related variants of PAC-learning in which false-positive and false-negative errors are treated differently. In these models we seek to guarantee a given, low rate of false-positive errors and as few false-negative errors as possible given that we meet the false-positive constraint. Bshouty and Burroughs first observed that learning conjunctions in such models would enable PAC-learning of DNF in the usual distribution-free model; in turn, results of Daniely and Shalev-Shwartz establish that learning of DNF would imply algorithms for refuting random k-SAT using far fewer constraints than believed possible. Such algorithms would violate a slight strengthening of Feige’s R3SAT assumption, and would violate the RCSP hypothesis of Barak et al. We show here that actually, an algorithm for learning conjunctions in this model would have much more far-reaching consequences: it gives refutation algorithms for all predicates that are falsified by one of the uniform constant strings. To our knowledge, this is the first hardness result of improper learning for such a large class of natural average-case problems with natural distributions.	algorithm;best, worst and average case;boolean satisfiability problem;cryptographic service provider;probably approximately correct learning	Alexander Durgin;Brendan Juba	2018	Electronic Colloquium on Computational Complexity (ECCC)		discrete mathematics;combinatorics;mathematics;falsifiability	Theory	10.134613480730625	19.45149780927679	74080
a33e77ad9be7f0d777b143ec57ce1a3248aa8511	an improved analysis of goemans and williamson's lp-relaxation for max sat	performance guarantee;approximate algorithm;algorithm performance;best approximation;building block;approximation algorithm;maximum satisfiability max sat;problema np duro;logique propositionnelle;constraint satisfaction;satisfiabilite maximum;satisfaction contrainte;np hard problem;probleme np difficile;resultado algoritmo;propositional logic;algoritmo aproximacion;performance algorithme;mejor aproximacion;satisfaccion restriccion;logica proposicional;algorithme approximation;lp relaxation;meilleure approximation	For MAX SAT, which is a well-known NP-hard problem, many approximation algorithms have been proposed. Two types of best approximation algorithms for MAX SAT were proposed by Asano and Williamson: one with best proven performance guarantee 0.7846 and the other with performance guarantee 0.8331 if a conjectured performance guarantee of 0.7977 is true in the Zwick's algorithm. Both algorithms are based on their sharpened analysis of Goemans and Williamson's LP-relaxation for MAX SAT. In this paper, we present an improved analysis which is simpler than the previous analysis. Furthermore, algorithms based on this analysis will play a role as a better building block in designing an improved approximation algorithm for MAX SAT. Actually we show an example that algorithms based on this analysis lead to approximation algorithms with performance guarantee 0.7877 and conjectured performance guarantee 0.8353 which are slightly better than the best known corresponding performance guarantees 0.7846 and 0.8331 respectively.	lagrangian relaxation;linear programming relaxation;max;maximum satisfiability problem	Takao Asano	2003		10.1007/978-3-540-45077-1_2	mathematical optimization;combinatorics;constraint satisfaction;computer science;linear programming relaxation;np-hard;mathematics;propositional calculus;approximation algorithm;algorithm	Logic	15.184141575968411	18.59816392157208	74339
33ff601e67504f43b40d81df0217b08f4f0142ff	polynomial and apx-hard cases of the individual haplotyping problem	dynamic programming;probleme arx difficile;biologie algorithmique;programacion dinamica;dynamic programming algorithm;fixed parameter tractable;haplotyping;apx hardness;fixed parameter tractability;haplotype;informatique theorique;polynomial algorithm;programmation dynamique;algorithme polynomial;computational biology;arx hardness;computer theory;informatica teorica	SNP haplotyping problems have been the subject of extensive research in the last few years, and are one of the hottest areas of Computational Biology today. In this paper we report on our work of the last two years, whose preliminary results were presented at the European Symposium on Algorithms (Proceedings of the Annual European Symposium on Algorithms (ESA), Vol. 2161. Lecture Notes in Computer Science, Springer, 2001, pp. 182–193.) and Workshop on Algorithms in Bioinformatics (Proceedings of the Annual Workshop on Algorithms in Bioinformatics (WABI), Vol. 2452. Lecture Notes in Computer Science, Springer, 2002, pp. 29–43.). We address the problem of reconstructing two haplotypes for an individual from fragment assembly data. This problem will be called the Singl Individual Haplotyping Problem. On the positive side, we prove that the problem can be solved effectively for gapless data, and give practical, dynamic programming algorithms for its solution. On the negative side, we show that it is unlikely that polynomial algorithms exist, even to approximate the solution arbitrarily well, when the data contain gaps. We remark that both the gapless and gapped data arise in different real-life applications. © 2005 Elsevier B.V. All rights reserved.	apx;approximation algorithm;bioinformatics;column (database);computation;computational biology;dynamic programming;esa;graph coloring;lecture notes in computer science;microsoft fingerprint reader;optimization problem;polynomial;real life;snp array;springer (tank);time complexity;wabi-sabi	Vineet Bafna;Sorin Istrail;Giuseppe Lancia;Romeo Rizzi	2005	Theor. Comput. Sci.	10.1016/j.tcs.2004.12.017	combinatorics;haplotype;computer science;theoretical computer science;dynamic programming;mathematics;algorithm;statistics	Theory	17.00807793922559	22.53221719777689	74356
837d5f245321c986debe1b4d80ef10cf54d97a21	data compression with long repeated strings	lempel ziv;document analysis;data compression;software systems;sliding window	Lempel–Ziv schemes compress data by encoding repeated strings that occur in a small sliding window. We propose a scheme that succinctly encodes long strings that appear far apart in the input text. Such long strings are rare in most documents, but occur frequently in data such as large software systems, subroutine libraries, news articles, and other corpora of real documents. Analysis shows that our scheme is computationally efficient, and experiments show that effectively compresses some classes of input.	data compression	Jon Louis Bentley;M. Douglas McIlroy	2001	Inf. Sci.	10.1016/S0020-0255(01)00097-4	data compression;sliding window protocol;computer science;theoretical computer science;data mining;world wide web;statistics;software system	DB	11.59094045494818	27.946721210171873	74507
3a1a9f35a4ee5d705f2bfcec893c3e0676e8ed7b	locality-preserving hashing in multidimensional spaces	pseudorandomness;pac learning;multiple instance learning;derandomization;random graphs;machine learning;approximations of distributions;ramsey graphs;multimedia indexing;explicit constructions;hash function;sample complexity;rectangles	We consider localitg-preserving hashing — in which adjacent points in the domain are mapped to adjacent or nearlyadjacent points in the range — when the domain is a ddimensional cube. This problem has applications to highdimensional search and multimedia indexing. We show that simple and natural classes of hash functions are provably good for this problem. We complement this with lower bounds suggesting that our results are essentially the best possible.	hash function;locality of reference;locality-preserving hashing;spaces	Piotr Indyk;Rajeev Motwani;Prabhakar Raghavan;Santosh Vempala	1997		10.1145/258533.258656	random graph;combinatorics;discrete mathematics;hash function;computer science;theoretical computer science;mathematics;pseudorandomness;probably approximately correct learning;algorithm	Theory	13.436105881861225	20.52995369721798	74618
f866bf9f121cafdee540ad41e6ca0f840bd78587	the on-line first-fit algorithm for radio frequency assignment problems	numero cromatico;assignment problem;interferencia;coloracion grafo;reseau communication;probleme affectation;algorithm performance;competitive algorithms;nombre chromatique;chromatic number;emetteur;interference;on line algorithms;radio frequency assignment problems;allocation frequence;algorithme competitif;radio frequency;coloration graphe;graph coloring problem;community networks;resultado algoritmo;frequency allocation;transmitter;unit disk graph;asignacion frecuencia;vertex coloring;performance algorithme;unit disk graphs;radio communication;radiocommunication;graph coloring problems;emisor;communication;red de comunicacion;comunicacion;affectation frequence radio;on line algorithm;communication network;radiocomunicacion;competitive ratio;graph colouring;first fit	The radio frequency assignment problem is to minimize the number of frequencies used by transmitters with no interference in radio communication networks; it can be modeled as the minimum vertex coloring problem on unit disk graphs. In this paper, we consider the on-line first-fit algorithm for the problem and show that the competitive ratio of the algorithm for the unit disk graphG with χ(G) = 2 is 3, whereχ(G) is the chromatic number of G. Moreover, the competitive ratio of the algorithm for the unit disk graphG with χ(G) > 2 is at least 4− 3/χ(G). The average performance for the algorithm is also discussed in this paper.  2002 Elsevier Science B.V. All rights reserved.	algorithm;assignment problem;best, worst and average case;bin packing problem;color;competitive analysis (online algorithm);graph coloring;interference (communication);online algorithm;online and offline;radio frequency;telecommunications network;transmitter	Yin-Te Tsai;Yaw-Ling Lin;Fang-Rong Hsu	2002	Inf. Process. Lett.	10.1016/S0020-0190(02)00283-1	competitive analysis;unit disk graph;transmitter;combinatorics;frequency allocation;telecommunications;computer science;graph coloring;mathematics;interference;assignment problem;radio frequency;algorithm;telecommunications network	Theory	20.77085015194227	28.333883281736522	74760
adf008911b0fa7041bf9f43e8b35ccc60c913789	dna physical mapping: three ways difficult	parameterized complexity;combinatorial problems;complexity analysis;conceptual framework;computational complexity;algorithm design;physical map	We study the computational complexity of the combinatorial problem Intervalizing Colored Graphs (or ICG) that has some applications in DNA physical mapping. The three distinct conceptual frameworks of N P-completeness, algorithmic techniques for bounded treewidth, and parameterized complexity theory are shown to fit together neatly in an integrated complexity analysis of ICG. It is shown that ICG is intractable in three different ways: (1) it is N P-complete, (2) it is hard for the parameterized complexity class W[1] and (3) it is not finite-state for bounded treewidth or pathwidth, and is therefore resistant to the usual algorithm design methodologies. The proofs of these three results are related in interesting ways which suggest useful heuristic connections between the three complexity frameworks for intractable problems of bounded treewidth and pathwidth.		Michael R. Fellows;Michael T. Hallett;Todd Wareham	1993		10.1007/3-540-57273-2_52	algorithm design;parameterized complexity;combinatorics;probabilistic analysis of algorithms;complexity;decision tree model;quantum complexity theory;computer science;bioinformatics;theoretical computer science;computational resource;worst-case complexity;conceptual framework;mathematics;computational complexity theory;asymptotic computational complexity;game complexity;algorithm;descriptive complexity theory	HCI	16.596382743699632	21.450679981747406	74776
437cdd8e3ccbbef9a0eac6dce860c72efeca681a	recognizing digraphs of kelly-width 2	optimisation;combinatorics;digraph;temps polynomial;optimizacion;combinatoria;vertex;relacion orden;temps lineaire;reconocimiento;combinatoire;digrafo;ordering;tiempo lineal;68wxx;input;polynomial time algorithm;relation ordre;recognition;digraph width parameter;informatique theorique;directed graph;linear time;graphe oriente;entree ordinateur;polynomial time;characterization;algorithme polynomial;largeur;grafo orientado;optimization;vertice;width;entrada ordenador;ancho;reconnaissance;kelly width;computer theory;tiempo polinomial;informatica teorica;digraphe	Kelly-width is a parameter of digraphs recently proposed by Hunter and Kreutzer as a directed analogue of treewidth. We give an alternative characterization of digraphs of bounded Kelly-width in support of this analogy, and the first polynomial-time algorithm recognizing digraphs of Kelly-width 2. For an input digraph G = (V , A) the algorithm outputs a vertex ordering and a digraph H = (V , B) with A ⊆ B witnessing either that G has Kelly-width at most 2 or that G has Kelly-width at least 3, in time linear in H . © 2009 Elsevier B.V. All rights reserved.	algorithm;directed graph;kelly criterion;polynomial;time complexity;treewidth	Daniel Meister;Jan Arne Telle;Martin Vatshelle	2010	Discrete Applied Mathematics	10.1016/j.dam.2009.09.018	time complexity;combinatorics;discrete mathematics;directed graph;mathematics;algorithm	Theory	21.82593774544909	27.843793940442648	75044
1793a1f45e90ddf458fec0e147bb62d4dd9a5e69	parallel search algorithms for trees and graphs.	search algorithm			Pranay Chaudhuri	1992	Australian Computer Journal		linear search;beam search;binary search tree;geometry of binary search trees;a* search algorithm;beam stack search;computer science;lexicographic breadth-first search;incremental heuristic search;iterative deepening depth-first search;best-first search;weight-balanced tree;combinatorial search;ternary search tree;sss*;fringe search;binary search algorithm;dichotomic search;search algorithm	Theory	15.63717964902034	27.839959305005042	75097
9ed8d42d745f3dc90f6a2f1591fe4b3c79e90538	simple and efficient modifications of elimination orderings	algoritmo paralelo;parallel algorithm;relacion orden;distributed computing;ordering;algorithme parallele;relation ordre;calculo repartido;calcul reparti;minimum degree	One of the most important and well studied problems related to sparse Cholesky factorization is to compute elimination orderings that give as few nonzero entries as possible in the resulting factors. We study the problem of modifying a given elimination ordering through local reorderings. We present new theoretical results on equivalent orderings, including a new characterization of such orderings. Based on these results, we define the notion of k-optimality for an elimination ordering, and we describe how to use this in a practical context to modify a given elimination ordering to obtain less fill. We experiment with different values of k, and report on percentage of fill that is actually reduced from an already good initial ordering, like Minimum Degree.	cholesky decomposition;sparse matrix	Pinar Heggernes;Yngve Villanger	2004		10.1007/11558958_95	combinatorics;parallel computing;order theory;computer science;mathematics;parallel algorithm;algorithm	Theory	14.526487972287232	32.040447437944955	75145
89bd17f8c47bb7444f61bf8a6e965d5f1ed3adb4	cleaning a network with brushes	configuracion;05bxx;vertex;reseau;pulga electronica;searching;red;chip;metafora;informatique theorique;68r10;chip firing;chip firing game;vertice;brush number;configuration;metaphor;puce electronique;article;cleaning sequence;network;computer theory;metaphore;informatica teorica	Following the decontamination metaphor for searching a graph, we introduce a cleaning process, which is related to both the chip-firing game and edge searching. Brushes (instead of chips) are placed on some vertices and, initially, all the edges are dirty. When a vertex is ‘fired’, each dirty incident edge is traversed by only one brush, cleaning it, but a brush is not allowed to traverse an already cleaned edge; consequently, a vertex may not need degree-many brushes to fire. The model presented is one where the edges are continually recontaminated, say by algae, so that cleaning is regarded as an on-going process. Ideally, the final configuration of the brushes, after all the edges have been cleaned, should be a viable starting configuration to clean the graph again. We show that this is possible with the least number of brushes if the vertices are fired sequentially but not if fired in parallel. We also present bounds for the least number of brushes required to clean graphs in general and some specific families of graphs.	plasma cleaning;sputter cleaning;traverse;vertex (geometry);vertex (graph theory)	Margaret-Ellen Messinger;Richard J. Nowakowski;Pawel Pralat	2008	Theor. Comput. Sci.	10.1016/j.tcs.2008.02.037	chip;vertex;combinatorics;mathematics;configuration;algorithm	Theory	20.685828238382957	28.931703951626503	75247
1c118a6251a1a888a4fcf88e02c745b3f92dd843	a lower bound for computing oja depth	data depth;metodo estadistico;decision tree;procesamiento informacion;algorithm analysis;geometrie algorithmique;computational statistics;gradiente;computational geometry;mediane multivariee;statistical method;gradient;calculo automatico;arbol decision;computing;bivariate medians;calcul automatique;profondeur donnee;test signe;methode statistique;informatique theorique;information processing;borne inferieure;algorithms;geometria computacional;analyse algorithme;model of computation;traitement information;arbre decision;analisis algoritmo;lower bound;sign test;cota inferior;computer theory;informatica teorica	Let S = {s1, . . . , sn} be a set of points in the plane. The Oja depth of a query point θ with respect to S is the sum of the areas of all triangles (θ, si, sj). This depth may be computed in O(n log n) time in the RAM model of computation. We show that a matching lower bound holds in the algebraic decision tree model. This bound also applies to the computation of the Oja gradient, the Oja sign test, and to the problem of computing the sum of pairwise distances among points on a line.	decision tree model;erkki oja;gradient;linear algebra;model of computation;random-access memory	Greg Aloupis;Erin McLeish	2005	Inf. Process. Lett.	10.1016/j.ipl.2005.05.014	model of computation;computing;sign test;information processing;computational geometry;computer science;decision tree;mathematics;gradient;computational statistics;algorithm;statistics	Theory	17.104926491095533	25.552581412674098	75528
3831d5dc70c357f0b33a470758265d7ab8ad80b4	multiple addition and prefix sum on a linear array with a reconfigurable pipelined bus system	parallel and distributed system;time complexity;prefix sum;linear array;optical computing;pipelined bus;fast algorithm;time use;addition;reconfigurable bus;matrix multiplication	We present several fast algorithms for multiple addition and prefix sum on the Linear Array with a Reconfigurable Pipelined Bus System (LARPBS), a recently proposed architecture based on optical buses. Our algorithm for adding N integers runs on an N log M-processor LARPBS in O(log* N) time, where log* N is the number of times logarithm has to be taken to reduce N below 1 and M is the largest integer in the input. Our addition algorithm improves the time complexity of several matrix multiplication algorithms proposed by Li, Pan and Zheng (IEEE Trans. Parallel and Distributed Systems, 9(8):705–720, 1998). We also present several fast algorithms for computing prefix sums of N integers on the LARPBS. For integers with bounded magnitude, our first algorithm for prefix sum computation runs in O(log log N) time using N processors and in O(1) time using N 1+ε processors, for $$\frac{1}{3}$$ ≤ ε < 1. For integers with unbounded magnitude, the first algorithm for multiple addition runs in O(log log N log* N) time using N log M processors, when M is the largest integer in the input. Our second algorithm for multiple addition runs in O(log* N) time using N 1+ε log M processors, for $$\frac{1}{3}$$ ≤ ε < 1. We also show suitable extensions of our algorithm for real numbers.	central processing unit;computation;matrix multiplication;multiplication algorithm;prefix sum;time complexity	Amitava Datta	2004	The Journal of Supercomputing	10.1023/B:SUPE.0000032783.66123.63	time complexity;parallel computing;prefix sum;matrix multiplication;computer science;theoretical computer science;distributed computing;optical computing;addition	Theory	13.218259457664875	31.852152722988418	75695
100b28ccf730cbe1370aafc736d8799fa001e8c3	hyperanf: approximating the neighbourhood function of very large graphs on a budget	web graph;shortest path;shortest paths;complex network;index of dispersion;information network;social network;local structure;indexation;neighbourhood function;probabilistic counters;effective diameter;data structure	The neighbourhood function NG(t) of a graph G gives, for each t ∈ N, the number of pairs of nodes x, y such that y is reachable from x in less that t hops. The neighbourhood function provides a wealth of information about the graph [10] (e.g., it easily allows one to compute its diameter), but it is very expensive to compute it exactly. Recently, the ANF algorithm [10] (approximate neighbourhood function) has been proposed with the purpose of approximating NG(t) on large graphs. We describe a breakthrough improvement over ANF in terms of speed and scalability. Our algorithm, called HyperANF, uses the new HyperLogLog counters [5] and combines them efficiently through broadword programming [8]; our implementation uses talk decomposition to exploit multi-core parallelism. With HyperANF, for the first time we can compute in a few hours the neighbourhood function of graphs with billions of nodes with a small error and good confidence using a standard workstation.  Then, we turn to the study of the distribution of the distances between reachable nodes (that can be efficiently approximated by means of HyperANF), and discover the surprising fact that its index of dispersion provides a clear-cut characterisation of proper social networks vs. web graphs. We thus propose the spid (Shortest-Paths Index of Dispersion) of a graph as a new, informative statistics that is able to discriminate between the above two types of graphs. We believe this is the first proposal of a significant new non-local structural index for complex networks whose computation is highly scalable.	a-normal form;algorithmic inference;approximation algorithm;complex network;computation;graph (discrete mathematics);hyperloglog;information;iteration;multi-core processor;parallel computing;scalability;shortest path problem;social network;systolic array;workstation	Paolo Boldi;Marco Rosa;Sebastiano Vigna	2011		10.1145/1963405.1963493	data structure;computer science;index of dispersion;theoretical computer science;machine learning;shortest path problem;programming language;complex network;social network	ML	20.14532572148281	23.152997497852898	76168
522a7d1cb5654bb474e5de3741eef36bc0a86f49	fully persistent graphs - which one to choose?	asymptotic optimality;functional programming;search trees;graph algorithm;data structure	Functional programs, by nature, operate on functional, or persistent, data structures. Therefore, persistent graphs are a prerequisite to express functional graph algorithms. In this paper we describe two implementations of persistent graphs and compare their running times on different graph problems. Both data structures essentially represent graphs as adjacency lists. The first uses the version tree implementation of functional arrays to make adjacency lists persistent. An array cache of the newest graph version together with a time stamping technique for speeding up deletions makes it asymptotically optimal for a class of graph algorithms that use graphs in a single-threaded way. The second approach uses balanced search trees to store adjacency lists. For both structures we also consider several variations, for example, ignoring edge labels or predecessor information.	adjacency list;asymptotically optimal algorithm;graph (discrete mathematics);graph theory;haskell;lazy evaluation;persistent data structure;pseudoforest;thread (computing)	Martin Erwig	1997		10.1007/BFb0055428	1-planar graph;outerplanar graph;block graph;pathwidth;mathematical optimization;data structure;computer science;clique-width;graph coloring;trémaux tree;programming language;functional programming;chordal graph;indifference graph;line graph	Theory	16.278203902660792	29.421059967451065	76176
a7e55cf24bb33a61fc4670316f7aa18589857752	counting linear extensions is #p-complete	random polynomials;partially ordered set;linear extension	We show that the problem of counting the number of linear extensions of a given partially ordered set is #P-complete. This settles a long-standing open question and contrssts with recent results giving randomized polynomial-time algorit hms for estimating the number of linear extensions. One consequence is that computing the volume of a rational polyhedron is strongly #P-hard. We also show that the closely related problems of determining the average height of an element c of a given poset, and of determining the probability that z lies below y in a random linear extension, are #P-complete.	human height;p (complexity);p-complete;polyhedron;rp (complexity);randomized algorithm;sharp-p;sharp-p-complete;time complexity	Graham R. Brightwell;Peter Winkler	1991		10.1145/103418.103441	partially ordered set;combinatorics;mathematical analysis;discrete mathematics;mathematics;total order;linear extension	Theory	17.53211171659468	20.65810050365817	76201
aaf3505bbbd6340e25e7ae8b02e19c9aa533a512	how to distribute a dictionary in a complete network	random function;randomized algorithm;distribution dynamics;hash function;monte carlo	"""The algorithm applies a novel, randomized construction of hash functions. These functions can be evaluated in constant time, constructed on sublinear space in sublinear expected time, and have many features of random functions. The algorithm further makes use of a new Monte Carlo type sequential dictionary with worst case constant time per instruction, which was recently developed by the authors. Applications of the distributed dictionary are e. g. two improvements of PRAM-simulations: A PRAM with p processors can be simulated by a complete network with p processors with expected delay log p / log log p (before: logp), and on one with p / l o g p processors with optimal expected delay logp (before: p l -e processors, delay pC). 1. I N T R O D U C T I O N A dictionary is a dynamic data structure that supports the operations Insert, Delete, and Lookup. It is one of the most fundamental data structures. In this paper we describe and analyze a distributed dictionary, implemented on a complete, synchronized network of p processors, i.e., randomized RAMs. Its construction is based on hashing: The keys z to be inserted, deleted or looked up are assigned to the processors via a hash function, and processed within these processors using a dynamic sequential hashing strategy from [DM90]. *Suppor ted in pa r t by DFG Grants ME 872/1-3 and W E 1066/2-1 Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. Some important properties of our distributed dictionary are as follows: 1.) Each processor is fed (by its virtual user) with a new instruction whenever it asks for one. 2.) n > pl+~ arbitrary instructions, n /p per processor, can be executed in expected time O(n/p) , where c > 0 can be chosen arbitrarily small. 3.) Each processor needs space O(n/p) , if n elements are in the dictionary. 4.) Every lookup has expected constant response time. 5.) If p lookups are executed concurrently, then the expected time needed to answer all of them is O(log p~ log log p). 6.) If k >__ logp lookups from each processor are executed, then all of them are answered after expected time o(k). The efficiency bounds in 2 . ) 4 . ) are asymptotically optimal. 5.) and 6.) can also be looked upon to be optimal, if one assumes that the keys are mapped to the distribution hash processors using a hash function, the """" """" """" """" function"""" D H , and if one does not allow any (or only constant) redundancy (as is necessary to achieve the optimal space bound). In this case, the statements in 5.) and 6.) provide the best possible bounds, because they correspond to a D H that is a truly random function. In fact, one of the contributions of this paper is an efficient randomized construction of functions with many features of random functions and constant evaluation time. More precisely, we present a class 7~ of hash functions h : U ~ { 1 , . . . , p } (U is a finite universe) such that given a set S C U of n keys, there is 7~(S) C ~ , [7~(S)1/17~1 close to 1, such that a randomly chosen h E 7~(S) fulfills for each i E { 1 , . . . , p } : Prob( Ihl ( i ) f3 5'1 > u) is roughly as small as if h were a random function. Further, the construction of a random h E 7~ needs time and space o(p) only. (If we allow time and space O(p 1+~) for arbitrary 6 > 0, we even get that a random h E R(S) is a random function on S.) Similar types of random functions can be found in [DM90]. © 1990 ACM 089791-361-2/90/0005/0117 $1.50 117 Thus, when analyzing our data structure, we can argue that , from the point of view of a single processor, the distribution hash function behaves like a random function. In order to handle duplicates of keys many extra considerations for the algori thm and its analysis are necessary (see Chapter 5). S e q u e n t i a l d i c t i o n a r i e s . The first opt imal sequential dictionary (based on hashing) was introduced in [DKM88] inspired by the perfect hashing scheme from [FKS84]. It guarantees that n instructions can be executed in t ime O(L.n) with probabil i ty 1 2 z , for all L. I t turns out that this is not sufficient for the purposes of our distributed dictionary, because we need that even the slowest one of p concurrently working dictionaries is fast. A substantial improvement is shown in [DM90]. There a Monte Carlo type dictionary is presented that needs worst case constant t ime per instruction, and has failure probabil i ty O(n-k), when n keys are currently stored in the dictionary, k can be made an arbitrarily large constant. This dictionary uses hash functions similar to the class R described above. R e l a t e d w o r k a n d a p p l i c a t i o n s o f t h e m a i n re su i t . An opt imal parallel dictionary, implemented on a PRAM, was presented in [DM89]. As PRAMs have a shared memory, one can essentially use a single dict ionary and does not have to distribute keys over the processors. Closely related to our results are the shared memory emulations in [Upf84], [KU86], [Ran87], [MV84], [KRS88]. The first three papers and parts of the fourth apply polynomials of degree ~ logp as distribution hash functions. Thus, these techniques cannot be used for our purposes, because they yield """"best case"""" t ime bounds f~(logp) for the response t ime of lookups, and ~(n logp/p) for executing n instructions. In [Sie89] universal classes of hash functions are introduced that can be evaluated in constant t ime and yet perform much better than polynomials of constant degree (see Chapter 3). Possibly, those functions could also be used for the needs of the present paper. However, in [Sie89] the size of the universe is restricted to pk, where the evaluation t ime of the functions grows exponentially with k. In our approach we can handle a universe of arbi trary size. The algorithm for the distributed dictionary can be modified so as to obtain a method for simulating T steps of a (priority) PRAM with p processors and a shared memory of arbi t rary size on a complete network of p processors in O(T. logp/loglogp) steps. The space needed for representing the memory can be made as small as O(T/p +pC) per processor. (Compare property 5.) above.) Until now, the best known t imebound for a simulation of this kind was O(Tlogp), see [MV84]. In [KRS88], it is shown how to simulate a PRAM with p processors on a complete network of p l -~ processors with opt imal delay O(p~). In tha t paper, polynomials of constant degree as distribution hash functions are used. Combining these techniques with our improved sequential dictionary yields O(n/p) expected t ime for executing n instructions. But as p instructions have to be executed concurrently to obtain the above t ime bound, the expected response t ime for lookups is O(p*). It is easy to conclude from our result that a simulation as [KRS88] can already be achieved when the network has only p/logp processors. The delay becomes O(logp). (Compare property 6.) f rom above.) The paper is organized as follows. In Chapter 2 we define the computat ional model and distributed dictionaries in more detail. In Chapter 3 we describe and analyze our new hash functions and quote several useful results on hashing and sequential dictionaries. The remaining chapters contain the description and analysis of the distributed dictionary. 2. D E F I N I T I O N S In this paper we consider synchronized, complete networks of some number p of processors P 1 , . . . , Pp with usual sequential capabilities of randomized random access machines. We assume the uniform cost criterion. For communication purposes, each processor has a communicat ion window. Each processor can read from or write into any such window. We allow concurrent read and assume the P R I O R I T Y write conflict resolution rule for the windows: If several processors want to write simultaneously to the same window, then that one with the smallest index wins. Note that all our results also hold for the A R B I T R A R Y write conflict resolution rule. A distributed dictionary is a da ta structure implemented on a network that supports lookups, insertions and deletions of data identified by keys x from a given finite universe U. Each processor is connected to a user that feeds it with a new instruction whenever it asks for"""	artificial intelligence;asymptotically optimal algorithm;average-case complexity;best, worst and average case;central processing unit;cryptographic hash function;data dictionary;data structure;dynamic data;eisenstein's criterion;logp machine;lookup table;microsoft windows;monte carlo method;parallel random-access machine;perfect hash function;polynomial;random access;randomized algorithm;randomness;resolution (logic);response time (technology);shared memory;simulation;time complexity;window function	Martin Dietzfelbinger;Friedhelm Meyer auf der Heide	1990		10.1145/100216.100229	combinatorics;hash function;hybrid monte carlo;computer science;theoretical computer science;random function;mathematics;rolling hash;rejection sampling;randomized algorithm;algorithm;monte carlo algorithm;statistics;monte carlo method;randomization function	Theory	11.736565913723904	30.609566262164797	76425
f04eb5825f960c70d8a6e603ba73cc13d044764c	hamiltonian cycles in subcubic graphs: what makes the problem difficult	hamiltonian cycle;qa76 electronic computers computer science computer software;computational complexity	We study the computational complexity of the hamiltonian cycle problem in the class of graphs of vertex degree at most 3 Our goal is to distinguish boundary properties of graphs that make the problem difficult (NP-complete) in this domain In the present paper, we discover the first boundary class of graphs for the hamiltonian cycle problem in subcubic graphs.	hamiltonian (quantum mechanics)	Nicholas Korpelainen;Vadim V. Lozin;Alexander Tiskin	2010		10.1007/978-3-642-13562-0_29	hamiltonian path;combinatorics;discrete mathematics;vertex cover;longest path problem;theoretical computer science;mathematics;hamiltonian path problem;maximal independent set;computational complexity theory;chordal graph;indifference graph;algorithm	ML	24.258481175661405	25.693977587227156	76435
912a26c273f955da305321b9d02280e78354e927	new results on optimizing rooted triplets consistency	pseudorandomness;bottom up;approximate algorithm;rooted triplet;approximation algorithm;approximation;optimization problem;supertree;phylogenetic tree;polynomial time;triplet consistency;computational biology;hardness of approximation	A set of phylogenetic trees with overlapping leaf sets is consistent if it can be merged without conflicts into a supertree. In this paper, we study the polynomial-time approximability of two related optimization problems called the maximum rooted triplets consistency problem (MaxRTC) and the minimum rooted triplets inconsistency problem (MinRTI) in which the input is a set R of rooted triplets, and where the objectives are to find a largest cardinality subset of R which is consistent and a smallest cardinality subset of R whose removal from R results in a consistent set, respectively. We first show that a simple modification to Wu’s Best-Pair-Merge-First heuristic [25] results in a bottom-up-based 3-approximation for MaxRTC. We then demonstrate how any approximation algorithm for MinRTI could be used to approximate MaxRTC, and thus obtain the first polynomial-time approximation algorithm for MaxRTC with approximation ratio smaller than 3. Next, we prove that for a set of rooted triplets generated under a uniform random model, the maximum fraction of triplets which can be consistent with any tree is approximately one third, and then provide a deterministic construction of a triplet set having a similar property which is subsequently used to prove that both MaxRTC and MinRTI are NP-hard even if restricted to minimally dense instances. Finally, we prove that MinRTI cannot be approximated within a ratio of Ω(log n) in polynomial time, unless P = NP.		Jaroslaw Byrka;Sylvain Guillemot;Jesper Jansson	2010	Discrete Applied Mathematics	10.1016/j.dam.2010.03.004	time complexity;optimization problem;mathematical optimization;combinatorics;discrete mathematics;phylogenetic tree;supertree;computer science;approximation;top-down and bottom-up design;mathematics;hardness of approximation;pseudorandomness;approximation algorithm;algorithm	Theory	18.38610703335489	22.04667030746347	76483
0705e28b8ec561884ae37fde887a4ded5d2df107	on the power of randomization in online algorithms (extended abstract)	online algorithm;upper bound	"""Against an adaptive adversary, we show that the power of ran­ domization in online algorithms is severely limited! We prove the existence of an efficient """"simulation"""" of randomized online algorithms by deterministic ones, which is best possible in general. The proof of the upper bound is existential. We deal with the issue of computing the efficient deterministic algorithm, and show that this is possible in very general cases."""	adversary (cryptography);deterministic algorithm;online algorithm;randomized algorithm;simulation	Shai Ben-David;Allan Borodin;Richard M. Karp;Gábor Tardos;Avi Wigderson	1990		10.1145/100216.100268	randomized algorithms as zero-sum games;online algorithm;mathematical optimization;combinatorics;computer science;theoretical computer science;mathematics;distributed computing;upper and lower bounds;algorithm;adversary model	Theory	10.835928419704802	20.029112069829644	76484
1ee4a8bb824490f15ee5d26d9df7f9269171f74d	drawing clustered graphs using stress majorization and force-directed placements	torque computational complexity graph theory pattern clustering;layout clustering algorithms stress electronic mail force c languages electrical engineering;stress majorization clustered graph force directed placement graph drawing;clustered graph drawing algorithms layout algorithm stress model intracluster graph nodes spring electrical force model outside connectivity torque equilibrium heuristics force directed placement algorithm running time edge crossings stress majorization	We propose a novel layout algorithm to draw clustered graphs. Our algorithm applies a stress model to draw intra-cluster graphs and a spring-electrical force model to place clusters. Our strategy modifies the original stress model in graph drawing by integrating the force from the center to push and pull the intra-cluster nodes based on their outside connectivity. We also apply the idea of torque equilibrium, coupled with some heuristics, to realize our force-directed placement algorithm. To show the effectiveness of our design, we compare the running time and the number of edge crossings experimentally with the one using full stress majorization as well as other clustered graph drawing algorithms available in the literature. Our experimental results look promising.	algorithm;crossing number (graph theory);experiment;force-directed graph drawing;heuristic (computer science);stress majorization;time complexity	Yu-Jung Ko;Hsu-Chun Yen	2016	2016 20th International Conference Information Visualisation (IV)	10.1109/IV.2016.52	lattice graph;combinatorics;feedback arc set;directed graph;graph bandwidth;theoretical computer science;force-directed graph drawing;machine learning;mathematics;voltage graph;graph;moral graph;graph drawing;butterfly graph;strength of a graph	Robotics	23.84326440853619	27.239612522287267	76528
f637d2d16c3c9b385922a7ae86b1091f0fcb5d96	practical polytope volume approximation		We experimentally study the fundamental problem of computing the volume of a convex polytope given as an intersection of linear halfspaces. We implement and evaluate randomized polynomial-time algorithms for accurately approximating the polytope’s volume in high dimensions (e.g., few hundreds) based onhit-and-run random walks. To carry out this efficiently, we experimentally correlate the effect of parameters, such as random walk length and number of sample points, with accuracy and runtime. Our method is based on Monte Carlo algorithms with guaranteed speed and provably high probability of success for arbitrarily high precision. We exploit the problem’s features in implementing a practical rounding procedure of polytopes, in computing only partial “generations” of random points, and in designing fast polytope boundary oracles. Our publicly available software is significantly faster than exact computation and more accurate than existing approximation methods. For illustration, volume approximations of Birkhoff polytopes B11,…,B15 are computed, in dimensions up to 196, whereas exact methods have only computed volumes of up to B10.	approximation algorithm;birkhoff interpolation;computation;experiment;monte carlo method;polynomial;rp (complexity);randomized algorithm;rounding;time complexity	Ioannis Z. Emiris;Vissarion Fisikopoulos	2018	ACM Trans. Math. Softw.	10.1145/3194656	mathematical optimization;random walk;polytope;approximations of π;convex polytope;mathematics;monte carlo method;algorithm engineering;computation;rounding	Theory	18.827981295773544	20.263386907046453	76597
6a7f949adb820798eb2bf7bdaf0b01d6037a97ac	parameterized complexity of control problems in maximin election	parameterized complexity;procesamiento informacion;algorithm analysis;complexite calcul;variety;fixed parameter tractable;w 1 hard;fixed parameter tractability;maximin election;control problem;complejidad computacion;computational complexity;informatique theorique;information processing;superficie;area;control;analyse algorithme;variedad;traitement information;variete;analisis algoritmo;w 2 hard;computer theory;informatica teorica	Elections are a central model in a variety of areas. This paper studies parameterized computational complexity of five control problems in the Maximin election. We obtain the following results: constructive control by adding candidates is W[2]-hard with respect to the parameter ''number of added candidates''; both constructive and destructive control by adding/deleting voters are W[1]-hard with respect to the parameter ''number of added/deleted voters''.	cobham's thesis;complexity class;computational complexity theory;minimax;np-hardness;parameterized complexity	Hong Liu;Daming Zhu	2010	Inf. Process. Lett.	10.1016/j.ipl.2010.03.006	parameterized complexity;combinatorics;information processing;computer science;mathematics;area;variety;computational complexity theory;algorithm;scientific control	AI	15.403505759712953	19.360890316856953	76874
76eca7f878b6bb6e76230a91458de739121ac9eb	algorithms for generating convex sets in acyclic digraphs	acyclic digraphs;time complexity;embedded processors;convex sets;enumeration algorithms;convex set;generalized convexity;embedded processor	A set X of vertices of an acyclic digraph D is convex if X 6= ∅ and there is no directed path between vertices of X which contains a vertex not in X . A set X is connected if X 6= ∅ and the underlying undirected graph of the subgraph of D induced by X is connected. Connected convex sets and convex sets of acyclic digraphs are of interest in the area of modern embedded processor technology. We construct an algorithm A for enumeration of all connected convex sets of an acyclic digraph D of order n. The time complexity of A is O(n · cc(D)), where cc(D) is the number of connected convex sets in D. We also give an optimal algorithm for enumeration of all (not just connected) convex sets Department of Mathematical Sciences, University of Memphis, TN 38152-3240, USA. E-mail: pbalistr@memphis.edu Department of Mathematics, Royal Holloway, University of London, Egham, TW20 0EX, UK, E-mail: stefanie.gerke@rhul.ac.uk Department of Computer Science, Royal Holloway, University of London, Egham, TW20 0EX, UK, E-mail: gutin@cs.rhul.ac.uk Department of Computer Science, Royal Holloway, University of London, Egham, TW20 0EX, UK, E-mail: adrian@cs.rhul.ac.uk Department of Computer Science, Royal Holloway, University of London, Egham, TW20 0EX, UK, E-mail: joseph@cs.rhul.ac.uk Department of Computer Science, Royal Holloway, University of London, Egham, TW20 0EX, UK, E-mail: eas@cs.rhul.ac.uk Department of Computer Science, Royal Holloway, University of London, Egham, TW20 0EX, UK, E-mail: arezou@cs.rhul.ac.uk Department of Computer Science, Royal Holloway, University of London, Egham, TW20 0EX, UK, E-mail: anders@cs.rhul.ac.uk	algorithm;computer science;convex set;directed acyclic graph;directed graph;embedded system;graph (discrete mathematics);path (graph theory);processor technology;time complexity;twisted nematic field effect;vertex (geometry)	Paul N. Balister;Stefanie Gerke;Gregory Gutin;Adrian Johnstone;Joseph Reddington;Elizabeth Scott;Arezou Soleimanfallah;Anders Yeo	2009	J. Discrete Algorithms	10.1016/j.jda.2008.07.008	time complexity;convex analysis;subderivative;mathematical optimization;combinatorics;discrete mathematics;convex optimization;convex polytope;convex combination;orthogonal convex hull;convex body;linear matrix inequality;computer science;convex hull;absolutely convex set;mathematics;convex set;algorithm;proper convex function	Theory	23.70428080653388	28.32011378961871	76878
c628d8ab23de4fbc768f29397656ce7b6ba58aed	towards the existential control of boolean networks: a preliminary report		Given a Boolean network BN and a subset A of attractors of BN, we study the problem of identifying a minimal subset CBN of vertices of BN, such that the dynamics of BN can reach from a state s in any attractor As ∈ A to any attractor At ∈ A by controlling (toggling) a subset of vertices in CBN in a single time step. We describe a method based on the decomposition of the network structure into strongly connected components called ‘blocks’. The control subset can be locally computed for each such block and the results then merged to derive the global control subset CBN. This potentially improves the efficiency for many real-life networks that are large but modular and well-structured. We are currently in the process of implementing our method in software.	biological network;boolean network;computation;real life;strongly connected component;vertex (geometry)	Soumya Paul;Jun Pang;Cui Su	2018		10.1007/978-3-319-99933-3_10	boolean network;vertex (geometry);attractor;topology;mathematics;strongly connected component	ML	20.174673332577534	32.12963147707802	77170
e9ef2f3d8f04f1c4f6f14be77fdba036c22f367c	compact representations of automata for regular expression matching	regular expressions;algorithms;string matching	Glushkov's nondeterministic finite automaton leads to efficient regular expression matching. But it is memory greedy for long regular expressions. We develop space efficient representations for the deterministic finite automata obtained from Glushkov automata. The approach reduces the space of the DFA from O ( m 2 m ) bits to O ( m 2 k ) bits where k is the number of strings in the regular expression, m is the number of characters excluding operators. The average space usage is O ( m ( 1 + 1 / ź ) k ) bits where ź is the size of the alphabet. The state transition function runs in constant time when the length of a word is not less than m. Experiments show that our method is as efficient as the previous method and uses less space. Space efficient representations of Glushkov automata for regular expressions are proposed.The space of Glushkov automata is effectively reduced for regular expressions without many strings.The state transition uses both bitparallel and minimal perfect hash functions and is as efficient as previous methods.	automata theory;automaton;grammar-based code;regular expression	Meng Zhang;Yi Zhang;Chen Hou	2016	Inf. Process. Lett.	10.1016/j.ipl.2016.07.003	combinatorics;discrete mathematics;nondeterministic finite automaton;quantum finite automata;computer science;mathematics;regular expression;algorithm;string searching algorithm	Logic	12.99050968348467	27.007200667893034	77484
767b635d6e699a2f4e5a6afbeedc457b4525883c	a bijection from ordered trees to binary trees that sends the pruning order to the strahler number	combinatorics;combinatoria;nombre strahler;relacion orden;combinatoire;ordering;isomorphism;isomorfismo;relation ordre;induccion;induction;arbol binario;arbre binaire;isomorphisme;lenguaje formal;formal language;binary tree;langage formel	It is well known (e.g. [3], p. 60) that the number of ordered trees with n vertices equals the number of complete binary trees with n leaves. Vauchaussade de Chaumont and Viennot [4,6] ( see also [3], ch. 3, ex. 6 (p. 103)) discovered an interesting refinement of this fact. They proved that for any integers n and k, the number of ordered trees with IZ vertices and pruning order k equals the number of complete binary trees with n leaves and Strahler number k. In this communication I construct a bijection whose “shadow” is this result, thus giving a “bijective proof’ of the Vauchaussade-Viennot result and thereby solving their ten-bottlesof-wine problem [5]. This problem was also solved, independently, by Bender and Canfield [l]. First, definitions! It will be convenient to adhere to Schutzenberger’s philosophy of viewing combinatorial objects such as trees as words in an appropriate formal language. Let T and B stand for “an ordered tree” and “a complete binary tree” respectively. An ordered tree is a finite word in the 2-letter alphabet {( , )} defined recursively by:	binary tree;formal language;ordered pair;recursion;refinement (computing);strahler number;tree (data structure);vertex (geometry)	Doron Zeilberger	1990	Discrete Mathematics	10.1016/0012-365X(90)90047-L	combinatorics;formal language;discrete mathematics;order type;binary tree;order theory;mathematics;isomorphism;algorithm;algebra	Theory	16.733744932781303	30.495061342006718	77550
9a790f9b78ade420de4d631bc533fbbdd338985b	vc-dimension and shortest path algorithms	shortest path algorithm;low highway dimension;road network;point-to-point shortest path problem;set system;shortest path;query algorithm;related concept;graph algorithm design;average case definition;highway dimension	We explore the relationship between VC-dimension and graph algorithm design. In particular, we show that set systems induced by sets of vertices on shortest paths have VC-dimension at most two. This allows us to use a result from learning theory to improve time bounds on query algorithms for the point-to-point shortest path problem in networks of low highway dimension, such as road networks. We also refine the definitions of highway dimension and related concepts, making them more general and potentially more relevant to practice. In particular, we define highway dimension in terms of set systems induced by shortest paths, and give cardinality-based and average case definitions.	algorithm design;best, worst and average case;list of algorithms;point-to-point protocol;shortest path problem;vc dimension	Ittai Abraham;Daniel Delling;Amos Fiat;Andrew V. Goldberg;Renato F. Werneck	2011		10.1007/978-3-642-22006-7_58	combinatorics;discrete mathematics;constrained shortest path first;euclidean shortest path;machine learning;yen's algorithm;mathematics;shortest path problem;distance;k shortest path routing;shortest path faster algorithm	Theory	22.060329896751295	21.78128524076786	77789
3d5ebe70c09c37a401d3dd48412983e0e9d01896	worst and best case behaviour of an approximate graph coloring algorithm			dijkstra's algorithm;graph coloring	Karl Dürre;Johannes Heuft;Heinrich Müller	1981			combinatorics;list coloring;graph power;discrete mathematics;greedy coloring;fractional coloring;mathematical optimization;complete coloring;edge coloring;voltage graph;mathematics;graph factorization	Theory	23.791983231018058	26.00448135441712	77791
0b142486c7c4b512e1b183b78d63d7b4d2d959f9	on the complexity of finding common approximate substrings	complexite;parameterized complexity;concordance;complejidad;concordancia;solucion aproximada;complexity;polynomial time algorithm;common approximate substring;hamming distance;solution approchee;approximate solution;pattern matching;distance hamming;pattern recognition;parametrized complexity;reconnaissance forme;reconocimiento patron;distancia hamming;complexite parametree	Problems associated with #nding strings that are within a speci#ed Hamming distance of a given set of strings occur in several disciplines. In this paper, we use techniques from parameterized complexity to assess non-polynomial time algorithmic options and complexity for the COMMON APPROXIMATE SUBSTRING (CAS) problem. Our analyses indicate under which parameter restrictions useful algorithms are possible, and include both class membership and parameterized reductions to prove class hardness. In order to achieve #xed-parameter tractability, either a #xed string length or both a #xed size alphabet and #xed substring length are su7cient. Fixing either the string length or the alphabet size and Hamming distance is shown to be necessary, unless W [1] = FPT . An assortment of parameterized class membership and hardness results cover all other parameterized variants, showing in particular the e<ect of #xing the number of strings. c © 2003 Elsevier B.V. All rights reserved.	analysis of algorithms;approximation algorithm;cobham's thesis;generic programming;hamming distance;interaction;motif;primer;parameterized complexity;requirement;string (computer science);substring;time complexity	Patricia A. Evans;Andrew D. Smith;Todd Wareham	2003	Theor. Comput. Sci.	10.1016/S0304-3975(03)00320-7	parameterized complexity;combinatorics;discrete mathematics;complexity;hamming distance;approximate string matching;computer science;pattern matching;mathematics;algorithm;concordance	Theory	16.003313622175334	23.96497215808946	77825
9e7fd432a171b6ca6e3add44c745513ef0428763	approximation algorithm on multi-way maxcut partitioning	approximate algorithm;time complexity;vlsi design;covering problem	1 I n t r o d u c t i o n Given an undirected graph G = (V, E), I v l = n and IEI = e, a cut is a set of all edges of E with one end in a proper subset X of V, and the other end in X = V X. The maxcut problem is to maximize CUT(X,X--) : F_, (i,j)EE,i6X,jfEX where wi,j is the weight on the edge connecting vertices i and j in V. It is well known tha t even simple maxcut (maximum cut with edge weights restricted to value 1) is NP-complete [18] on general graphs. For a history of the maxcut problem see [18, 33]. Another well-known combinatorial optimization problem is the coloring problem tha t assigns a color to each vertex where adjacent vertices are assigned dif* This work has been supported in part by the National Science Foundation under Grant MIP-9207267.		Jun-Dong Cho;Salil Raje;Majid Sarrafzadeh	1994		10.1007/BFb0049405	time complexity;mathematical optimization;combinatorics;discrete mathematics;computer science;mathematics;very-large-scale integration;algorithm	Theory	24.274460390480115	22.97721547882123	77918
759f0ca0d52b281181ab51cf39b3e73f95dc5209	optimal parallel algorithms for rectilinear link-distance problems	shortest path;parallel algorithm;exclusive read exclusive write;computational geometry;parallel random access machine;time use;parallel computer;data structure;algorithms and data structure	We provide optimal parallel solutions to several link-distance problems set in trapezoided rectilinear polygons. All our main parallel algorithms are deterministic and designed to run on the exclusive read exclusive write parallel random access machine (EREW PRAM). LetP be a trapezoided rectilinear simple polygon withn vertices. InO(logn) time usingO(n/logn) processors we can optimally compute: 1. Minimum réctilinear link paths, or shortest paths in theL 1 metric from any point inP to all vertices ofP. 2. Minimum rectilinear link paths from any segment insideP to all vertices ofP. 3. The rectilinear window (histogram) partition ofP. 4. Both covering radii and vertex intervals for any diagonal ofP. 5. A data structure to support rectilinear link-distance queries between any two points inP (queries can be answered optimally inO(logn) time by uniprocessor). Minimum réctilinear link paths, or shortest paths in theL 1 metric from any point inP to all vertices ofP. Minimum rectilinear link paths from any segment insideP to all vertices ofP. The rectilinear window (histogram) partition ofP. Both covering radii and vertex intervals for any diagonal ofP. A data structure to support rectilinear link-distance queries between any two points inP (queries can be answered optimally inO(logn) time by uniprocessor). Our solution to 5 is based on a new linear-time sequential algorithm for this problem which is also provided here. This improves on the previously best-known sequential algorithm for this problem which usedO(n logn) time and space.5 We develop techniques for solving link-distance problems in parallel which are expected to find applications in the design of other parallel computational geometry algorithms. We employ these parallel techniques, for example, to compute (on a CREW PRAM) optimally the link diameter, the link center, and the central diagonal of a rectilinear polygon.	central processing unit;computational geometry;data structure;link distance;parallel algorithm;parallel random-access machine;regular grid;sequential algorithm;shortest path problem;time complexity;uniprocessor system	Andrzej Lingas;Anil Maheshwari;Jörg-Rüdiger Sack	1995	Algorithmica	10.1007/BF01206332	mathematical optimization;combinatorics;data structure;computational geometry;computer science;theoretical computer science;mathematics;parallel algorithm;parallel random-access machine;shortest path problem	Theory	13.725794474923896	31.071733204621587	77972
ff882a8bee12d2f6cb527085f334cd4a44d9c2c4	independent set orderings for parallel matrix factorization by gaussian elimination	matrix factorization;independent set;gaussian elimination	Commonly used matrix ordering techniques are designed to minimize fill, i.e., they are designed to minimize the number of zero elements which become nonzero during matrix factorization by Gaussian elimination. If Gaussian elimination is to be implemented on a parallel machine, however, minimum fill orderings are not necessarily optimal. Rather, the primary concern is to order a matrix so as to minimize the time required to complete its factorization. An ordering heuristic which appears to perform well with respect to parallel factorization time is one based on finding independent sets of vertices in the matrix adjacency graph.	gaussian elimination;heuristic;independent set (graph theory);parallel computing;the matrix	Michael R. Leuze	1989	Parallel Computing	10.1016/0167-8191(89)90016-1	mathematical optimization;gaussian elimination;combinatorics;discrete mathematics;eigendecomposition of a matrix;independent set;incomplete cholesky factorization;quadratic sieve;incomplete lu factorization;lu decomposition;nonnegative matrix;single-entry matrix;computer science;band matrix;mathematics;matrix decomposition	AI	19.60949474438528	30.430105411855134	78293
3e00dc895091a5e83d97a4c7dbfa0111f6b7c7f6	influence maximization in undirected networks	algorithms;design;graph algorithms;theory;network problems	We consider the problem of finding a set of k vertices of maximal total influence in a given undirected network, under the independent cascade (IC) model of influence spread. It is known that influence is submodular in the IC model, and hence a greedy algorithm achieves a (1 − 1/e) approximation to this problem; moreover, it is known to be NP-hard to achieve a better approximation factor in directed networks. We show that for undirected networks, this approximation barrier can be overcome: the greedy algorithm obtains an (1− 1/e+ c) approximation to the set of optimal influence, for some constant c > 0. Our proof proceeds via probabilistic analysis of bond percolation in arbitrary finite networks. We also show that the influence maximization problems remains APX-hard in undirected networks. ∗Dept. of Computer & Information Science, University of Pennsylvania, Philadelphia, PA 19104, USA. sanjeev@cis.upenn.edu †Microsoft Research New England, One Memorial Drive Cambridge, MA 02142. brlucier@microsoft.com	apx;approximation;expectation–maximization algorithm;graph (discrete mathematics);greedy algorithm;information science;maximal set;microsoft research;np-hardness;percolation theory;probabilistic analysis of algorithms;submodular set function	Sanjeev Khanna;Brendan Lucier	2014		10.1137/1.9781611973402.109	mathematical optimization;combinatorics;discrete mathematics;mathematics	Theory	21.391017167690478	20.399345759846454	78335
91f002d14a4bccc7aa472c6b754afd12b093dcae	worst-case efficient priority queues	priority queue;data structure	Au implementation of priority queues is presented that supports the operations MAKEQUEUE, FINDMIN, INSERT, MELD and DECREASEKEY in worst case time O(1) and DELETEMIN and DELETE in worst case time O(logn). The space requirement is linear. The data structure presented is the first achieving this worst case performance.	best, worst and average case;data structure;insert (sql);priority queue	Gerth Stølting Brodal	1996			priority inheritance;earliest deadline first scheduling;data structure;double-ended priority queue;computer science;deadline-monotonic scheduling;mathematics;queue management system;priority queue;priority ceiling protocol	Theory	11.928668996000434	32.06988082487932	78355
306ea85e34b2530e98d9d960feb8175738a5a056	encoding short ranges in tcam without expansion: efficient algorithm and applications		We present range encoding with no expansion RENÉ— a novel encoding scheme for short ranges on Ternary content addressable memory TCAM, which, unlike previous solutions, does not impose row expansion, and uses bits proportionally to the maximal range length. We provide theoretical analysis to show that our encoding is the closest to the lower bound of number of bits used. In addition, we show several applications of our technique in the field of packet classification, and also, how the same technique could be used to efficiently solve other hard problems, such as the nearest-neighbor search problem and its variants. We show that using TCAM, one could solve such problems in much higher rates than previously suggested solutions, and outperform known lower bounds in traditional memory models. We show by experiments that the translation process of RENÉ on switch hardware induces only a negligible 2.5% latency overhead. Our nearest neighbor implementation on a TCAM device provides search rates that are up to four orders of magnitude higher than previous best prior-art solutions.	algorithm;content-addressable memory;experiment;line code;maximal set;nearest neighbor search;network packet;overhead (computing);range encoding;search problem;telecommunications access method	Anat Bremler-Barr;Yotam Harchol;David Hay;Yacov Hel-Or	2016	IEEE/ACM Transactions on Networking	10.1109/TNET.2018.2797690	parallel computing;computer science;theoretical computer science;content-addressable memory;mathematics;k-nearest neighbors algorithm;algorithm;range encoding	Networks	11.443829833339487	28.89149021141831	78415
3b733fae84ed37f9d1bd061ccdabb6af73a4f9f4	synchronizing finite automata on eulerian digraphs	transition function;graph theory;coloracion grafo;problem;teoria grafo;digraph;automata estado finito;graphe eulerien;probleme;theorie graphe;synchronisation;fonction transition;coloration graphe;synchronization;eulerian graph;finite automata;finite automaton;sincronizacion;problema;grafo euleriano;automate fini;graph colouring;digraphe	1 Cern2 y’s conjecture and the road coloring problem are two open problems concerning synchronization of !nite automata. We prove these conjectures in the special case that the vertices have uniform inand outdegrees. c © 2002 Elsevier Science B.V. All rights reserved.	automata theory;automaton;directed graph;finite-state machine;graph coloring;road coloring theorem;synchronization (computer science)	Jarkko Kari	2003	Theor. Comput. Sci.	10.1016/S0304-3975(02)00405-X	synchronization;combinatorics;discrete mathematics;computer science;mathematics;finite-state machine;algorithm	Logic	22.835584627505863	30.869565921679527	78477
45ab87988b43e9bf146a28914b185adf135399f4	on counting the number of consistent genotype assignments for pedigrees	genetique;genetic analysis;metodo polinomial;cardinal number;temps polynomial;genetica;complexite calcul;genotype;complexity analysis;genetics;polynomial time algorithm;complejidad computacion;nombre cardinal;numero cardinal;polynomial method;computational complexity;informatique theorique;consistency checking;polynomial time;methode polynomiale;genotipo;computer theory;tiempo polinomial;informatica teorica	Consistency checking of genotype information in pedigrees plays an important role in genetic analysis and for complex pedigrees the computational complexity is critical. We present here a detailed complexity analysis for the problem of counting the number of complete consistent genotype assignments. Our main result is a polynomial time algorithm for counting the number of complete consistent assignments for non-looping pedigrees. We further classify pedigrees according to a number of natural parameters like the number of generations, the number of children per individual and the cardinality of the set of alleles. We show that even if we assume all these parameters as bounded by reasonably small constants, the counting problem becomes computationally hard (#P-complete) for looping pedigrees. The border line for counting problems computable in polynomial time (i.e. belonging to the class FP) and #P-hard problems is completed by showing that even for general pedigrees with unlimited number of generations and alleles but with at most one child per individual and for pedigrees with at most two generations and two children per individual the counting problem is in FP.	algorithm;analysis of algorithms;cobham's thesis;computable function;computational complexity theory;counting problem (complexity);p (complexity);p-complete;polynomial;sharp-p-complete;time complexity	Jirí Srba	2005		10.1007/11590156_38	cardinal number;time complexity;combinatorics;genotype;mathematics;computational complexity theory;genetic analysis;algorithm	Theory	17.125201005721095	23.210981442402467	78539
b31ba089185032541d351d478dc4ff781ab96c55	classification of de bruijn-based labeled digraphs	directed line graph;hamiltonian cycle path problem;alphabet overlap digraph;dna graph;quasi adjoint graph;dna sequence assembly	Labeled digraphs, thanks to their special properties, are widely used in modeling real-world problems. Starting from de Bruijn graphs, they were used, among others, in modeling communication networks, architecture of parallel computers, or–in the area of bioinformatics–DNA sequencing and assembly problems. One of their most important properties is polynomial-time solvability of the Hamiltonian cycle/path problem, which makes these graphs especially useful as computational models. The classification presented here shows relations between subclasses of labeled digraphs, such as de Bruijn graphs, DNA graphs and others, and their connection with adjoints and quasi-adjoint graphs. The most recently defined class of quasi-adjoint graphs has a widest applicability, since it contains as subclasses all the de Bruijn-based labeled digraphs considered in this paper. The current work can be treated as a support in choosing an appropriate combinatorial model, resulting in polynomial time solution of problems related to searching for the Hamiltonian cycle or path, which are strongly NP-hard in general.	de bruijn graph	Marta Kasprzak	2018	Discrete Applied Mathematics	10.1016/j.dam.2016.10.014	1-planar graph;hamiltonian path;block graph;pathwidth;combinatorics;discrete mathematics;cograph;topology;graph product;longest path problem;pancyclic graph;mathematics;hamiltonian path problem;graph isomorphism;modular decomposition;partial k-tree;chordal graph;indifference graph	ML	21.839907542927186	24.787718133833536	78703
2543f5732a3a385849269d1b50a080a5a9971d3f	algorithms for finding an optimal set of short disjoint paths in a communication network	camino mas corto;shortest path;algoritmo busqueda;algorithme recherche;efficient algorithm;search algorithm;telecommunication network;camino optimo;processing time;chemin optimal;optimal path;telecommunication network routing;community networks;red telecomunicacion;reseau telecommunication;chemin plus court;temps traitement;dijkstra algorithm pathfinding algorithms short disjoint paths communication network sequentially constructed optimal set;intelligent networks communication networks costs routing delay effects computer networks graph theory modems mathematical model length measurement;algoritmo optimo;algorithme optimal;optimal algorithm;tiempo proceso;disjoint paths	The motives for seeking a set of short disjoint paths in a communication network are explained. A sequentially constructed optimal set is defined. Three efficient algorithms, one that constructs an optimal set and two that construct approximations, are presented. One of the latter algorithms not only constructs a larger set of short disjoint paths than an iterated version of the standard Dijkstra algorithm, but also offers a major reduction in computation time for large networks. >	algorithm;telecommunications network	Don J. Torrieri	1992	IEEE Trans. Communications	10.1109/26.179933	computer science;theoretical computer science;disjoint sets;mathematics;distributed computing;shortest path problem;algorithm;telecommunications network;search algorithm	Theory	18.323620286392323	31.44014631494299	78918
ccbe75d2f3d15229bdf28f7af8e84c8836403704	new width parameters for model counting		We study the parameterized complexity of the propositional model counting problem #SAT for CNF formulas. As the parameter we consider the treewidth of the following two graphs associated with CNF formulas: the consensus graph and the conflict graph. Both graphs have as vertices the clauses of the formula; in the consensus graph two clauses are adjacent if they do not contain a complementary pair of literals, while in the conflict graph two clauses are adjacent if they do contain a complementary pair of literals. We show that #SAT is fixed-parameter tractable for the treewidth of the consensus graph but W[1]-hard for the treewidth of the conflict graph. We also compare the new parameters with known parameters under which #SAT is fixed-parameter tractable.	cobham's thesis;conjunctive normal form;counting problem (complexity);parameterized complexity;serializability;sharp-sat;treewidth	Robert Ganian;Stefan Szeider	2017		10.1007/978-3-319-66263-3_3	combinatorics;discrete mathematics;vertex (geometry);parameterized complexity;computer science;treewidth;counting problem;graph	AI	22.76666996145095	24.431761987844457	78926
4209d86ec4619eba992105ae9d0de04e09c948eb	characterization of graphs and digraphs with small process numbers	optimisation;wdm network;combinatorics;digraph;temps polynomial;optimizacion;combinatoria;vertex;reconocimiento;combinatoire;linear time algorithm;digrafo;algorithme temps lineaire;graphe processus;terme;recognition;process number;informatique theorique;directed graph;68r10;graphe oriente;polynomial time;grafo orientado;optimization;vertice;vertex separation;pathwidth;reconnaissance;rerouting;computer theory;tiempo polinomial;informatica teorica;digraphe	We introduce the process number of a digraph as a tool to study rerouting issues in wdm networks. This parameter is closely related to the vertex separation (or pathwidth). We consider the recognition and the characterization of (di)graphs with small process number. In particular, we give a linear time algorithm to recognize (and process) graphs with process number at most 2, along with a characterization in terms of forbidden minors, and a structural description. As for digraphs with process number 2, we exhibit a characterization that allows to recognize (and process) them in polynomial time.	algorithm;apollonian network;approximation algorithm;directed graph;forbidden graph characterization;graph (discrete mathematics);graph theory;heuristic;image scaling;matroid minor;pathwidth;refinement (computing);time complexity	David Coudert;Jean-Sébastien Sereni	2011	Discrete Applied Mathematics	10.1016/j.dam.2011.03.010	combinatorics;discrete mathematics;directed graph;mathematics;algorithm	Theory	22.43462470790693	28.337307195535292	79001
42ded7c76ffa5945367b0cc060fb573db5eb9420	pairing heaps: the forward variant		The pairing heap is a classical heap data structure introduced in 1986 by Fredman, Sedgewick, Sleator, and Tarjan. It is remarkable both for its simplicity and for its excellent performance in practice. The “magic” of pairing heaps lies in the restructuring that happens after the deletion of the smallest item. The resulting collection of trees is consolidated in two rounds: a left-to-right pairing round, followed by a right-to-left accumulation round. Fredman et al. showed, via an elegant correspondence to splay trees, that in a pairing heap of size n all heap operations take O(logn) amortized time. They also proposed an arguably more natural variant, where both pairing and accumulation are performed in a combined left-to-right round (called the forward variant of pairing heaps). The analogy to splaying breaks down in this case, and the analysis of the forward variant was left open. In this paper we show that inserting an item and deleting the minimum in a forward-variant pairing heap both take amortized time O(logn · 4 √ log ). This is the first improvement over the O( √ n) bound showed by Fredman et al. three decades ago. Our analysis relies on a new potential function that tracks parent-child rank-differences in the heap. 2012 ACM Subject Classification F.2.2 Nonnumerical Algorithms, E.1 Data Structures	algorithm;amortized analysis;data structure;heap (data structure);pairing heap;right-to-left;splay tree;tree accumulation	Dani Dorfman;Haim Kaplan;László Kozma;Uri Zwick	2018		10.4230/LIPIcs.MFCS.2018.13	discrete mathematics;splay tree;combinatorics;mathematics;binary logarithm;analogy;pairing;pairing heap;amortized analysis;heap (data structure)	Theory	13.56841822404086	23.575253695169174	79018
0a76d25507aac36ed90ab30543fe2cd6bf4fedff	s-t connectivity on digraphs with a known stationary distribution	random walk and stationary distribution;rl vs l;stationary distribution;random walk;directed graph;mixing time;graph reachability;logspace computation	We present a deterministic logspace algorithm for solving S-T Connectivity on directed graphs if: (i) we are given a stationary distribution of the random walk on the graph in which both of the input vertices s and t have nonnegligible probability mass and (ii) the random walk which starts at the source vertex s has polynomial mixing time. This result generalizes the recent deterministic logspace algorithm for S-T Connectivity on undirected graphs [Reingold, 2008]. It identifies knowledge of the stationary distribution as the gap between the S-T Connectivity problems we know how to solve in logspace (L) and those that capture all of randomized logspace (RL).	conditional (computer programming);directed graph;graph (discrete mathematics);l (complexity);polynomial;randomized algorithm;stationary process	Kai-Min Chung;Omer Reingold;Salil P. Vadhan	2007	Twenty-Second Annual IEEE Conference on Computational Complexity (CCC'07)	10.1145/1978782.1978785	mathematical optimization;combinatorics;stationary distribution;discrete mathematics;directed graph;mathematics;reachability;random walk	Theory	24.451292765606173	28.668499219189048	79179
6d1b6e223579b5d48b3341f861b21bec1637e89c	editorial: isaac 2008 special issue		This special issue contains a selection of six papers from the 19th Annual International Symposium on Algorithms and Computation (ISAAC 2008), which was held in Gold Coast, Australia on December 15–17, 2008. The ISAAC 2008 program committee accepted 78 papers among 229 high-quality submissions from 40 countries, after a rigorous review process. Among those accepted papers, the following six papers were selected and invited to this special issue, based on their evaluation by the program committee. These six papers all went through the standard refereeing process of Algorithmica before being accepted for publication. The papers in this issue show recent advances in various areas of algorithms and theory of computation research. The first paper, “Signature Theory in Holographic Algorithms” by Cai and Lu, develops the signature theory in holographic algorithms, in terms of d-realizability and d-admissibility. For the class of 2-realizable signatures, they prove a Birkhoff-type theorem which determines this class. This is followed by characterization theorems for 1-realizability and 1-admissibility. Finally, they present some counting problems solvable in polynomial time by holographic algorithms. The second paper, “Faster Parameterized Algorithms for MINIMUM FILL-IN” by Bodlaender, Heggernes and Villanger, presents two parameterized algorithms for the MINIMUM FILL-IN problem, also known as the CHORDAL COMPLETION problem. The first algorithm requires polynomial space, which improves the exponential part	algorithmica;antivirus software;birkhoff interpolation;decision problem;holographic algorithm;holography;international symposium on algorithms and computation;lu decomposition;pspace;theory of computation;time complexity	Seok-Hee Hong;Hiroshi Nagamochi	2011	Algorithmica	10.1007/s00453-011-9507-4	discrete mathematics;combinatorics;time complexity;chordal graph;exponential function;mathematics;parameterized complexity;computation;counting problem;pspace;theory of computation	Theory	12.666843167766727	20.722071035738583	79289
e9800197130f92d7d07d9e19e6c16cc38ea92bd0	successive edge-connectivity augmentation problems	graph theory;teoria grafo;aumentacion;augmentation;theorie graphe;optimisation combinatoire;graph connectivity;increase;conectividad grafo;combinatorial optimization;connectivite graphe;optimizacion combinatoria	,G1,G2,... of supergraphs of G such that Gi is a subgraph of Gj for any iu003cj and Gi is an optimal (l+i)-edge-connected augmentation of G for any i≥0.	k-edge-connected graph	Eddie Cheng;Tibor Jordán	1999	Math. Program.	10.1007/s101070050041	mathematical optimization;combinatorics;discrete mathematics;combinatorial optimization;connectivity;graph theory;mathematics	Theory	21.90979968108536	27.30578933255048	79342
2819ccd951b758bbf34670eb1c4f5698a3535ed8	the parameterized complexity of finding point sets with hereditary properties		We consider problems where the input is a set of points in the plane and an integer k, and the task is to find a subset S of the input points of size k such that S satisfies some property. We focus on properties that depend only on the order type of the points and are monotone under point removals. We show that not all such problems are fixed-parameter tractable parameterized by k, by exhibiting a property defined by three forbidden patterns for which finding a k-point subset with the property is W[1]-complete and (assuming the exponential time hypothesis) cannot be solved in time no(k/ log k). However, we show that problems of this type are fixed-parameter tractable for all properties that include all collinear point sets, properties that exclude at least one convex polygon, and properties defined by a single forbidden pattern. 2012 ACM Subject Classification Theory of computation → Design and analysis of algorithms	algorithm;analysis of algorithms;cobham's thesis;exponential time hypothesis;parameterized complexity;theory of computation;time complexity;monotone	David Eppstein;Daniel Lokshtanov	2018	CoRR		theoretical computer science;parameterized complexity;computer science	Theory	22.592710465691052	23.249078402742622	79506
43406b7ecceb3679ee711e11b1fb21a769305160	np-hardness of hypercube 2-segmentation		The hypercube 2-segmentation problem is a certain biclustering problem that was previously claimed to be NP-hard, but for which there does not appear to be a publicly available proof of NP-hardness. This manuscript provides such a proof.	biclustering;np-hardness	Uriel Feige	2014	CoRR		combinatorics;discrete mathematics;mathematics;algorithm	Theory	17.683160952850233	21.055991591108015	79869
475ca9a9c1c2dcf8586604b1df66e7d89462288d	cover time and broadcast time	004;random walk randomized algorithms parallel and distributed algorithms;random walk;data structure;minimum degree	We introduce a new technique for bounding the cover time of random walks by relating it to the runtime of randomized broadcast. In particular, we strongly confirm for dense graphs the intuition of Chandra et al. [8] that “the cover time of the graph is an appropriate metric for the performance of certain kinds of randomized broadcast algorithms”. In more detail, our results are as follows: • For any graph G = (V, E) of size n and minimum degree δ, we have R(G) = O( |E| δ · log n), where R(G) denotes the quotient of the cover time and broadcast time. This bound is tight for binary trees and tight up to logarithmic factors for many graphs including hypercubes, expanders and lollipop graphs. • For any δ-regular (or almost δ-regular) graph G it holds that R(G) = Ω( δ n · 1 log n ). Together with our upper bound on R(G), this lower bound strongly confirms the intuition of Chandra et al. for graphs with minimum degree Θ(n), since then the cover time equals the broadcast time multiplied by n (neglecting logarithmic factors). • Conversely, for any δ we construct almost δ-regular graphs that satisfy R(G) = O(max{√n, δ} · log n). Since any regular expander satisfies R(G) = Θ(n), the strong relationship given above does not hold if δ is polynomially smaller than n. Our bounds also demonstrate that the relationship between cover time and broadcast time is much stronger than the known relationships between any of them and the mixing time (or the closely related spectral gap).	binary tree;degree (graph theory);graph (discrete mathematics);random graph;randomized algorithm;spectral method;time complexity	Robert Elsässer;Thomas Sauerwald	2009		10.4230/LIPIcs.STACS.2009.1842	mathematical optimization;combinatorics;discrete mathematics;data structure;computer science;mathematics;programming language;random walk;algorithm;statistics	Theory	20.8182082331716	22.498320223248832	79892
7078cece612ff1c8646485b0c1622f50f503639e	belief propagation for minimum weight many-to-one matchings in the random complete graph	belief propagation;random graph	Abstract: In a complete bipartite graph with vertex sets of cardinalities n and m, assign random weights from exponential distribution with mean 1, independently to each edge. We show that, as n → ∞, with m = dn/αe for any fixed α > 1, the minimum weight of many-to-one matchings converges to a constant (depending on α). Many-to-one matching arises as an optimization step in an algorithm for genome sequencing and as a measure of distance between finite sets. We prove that a belief propagation (BP) algorithm converges asymptotically to the optimal solution. We use the objective method of Aldous to prove our results. We build on previous works on minimum weight matching and minimum weight edge-cover problems to extend the objective method and to further the applicability of belief propagation to random combinatorial optimization problems.	algorithm;backpropagation;belief propagation;combinatorial optimization;matching (graph theory);mathematical optimization;minimum weight;one-to-many (data model);software propagation;time complexity;whole genome sequencing	Mustafa Khandwawala	2014	CoRR		random graph;mathematical optimization;combinatorics;discrete mathematics;mathematics;belief propagation	Theory	22.690087947975485	19.748732270065457	80033
d7268e73c8651769785ffd343f4c4b908938d33d	a linear-time, constant-space algorithm for computing the spanning line segments in three dimensions	constante tiempo;concepcion asistida;computer aided design;time constant;polyedre;algorithm complexity;modelo 3 dimensiones;capsula convexa;three dimensions;time complexity;poliedro;modele 3 dimensions;complejidad algoritmo;three dimensional model;polyhedron;satisfiability;three dimensional;enveloppe convexe;complexite temps;complexite algorithme;linear time;conception assistee;working memory;completitud;completeness;complejidad tiempo;convex hull;completude;segment droite maximal;constante temps	Abstract   Given a polyhedron, the set of spanning line segments (SLS) satisfies two properties: (1) completeness property — the set SLS must cover all the extreme vertices on the convex hull of the polyhedron; (2) inseparability property — there cannot be a plane that separates the set SLS into two nonempty subsets without intersecting one of them. Owing to the above two properties, the set SLS is proposed as a representation for testing the intersection between a plane and a three-dimensional (3D) polyhedron. Given a 3D polyhedron with  N  vertices, this paper presents an incremental  O ( N )-time algorithm for constructing the set SLS. The proposed algorithm has the same time complexity as the previous best result [Wang ME, Woo TC, Chen LL, Chou SY. Computing spanning line segments in three dimensions, The Visual Computer 12 (1996) 173–180], but it reduces the working memory required in the previous work from  O ( N ) to  O (1).	algorithm;file spanning;time complexity	Kuo-Liang Chung;Shyh-Ming Chang;Tony C. Woo	2001	Computer-Aided Design	10.1016/S0010-4485(00)00111-1	time complexity;three-dimensional space;edge;combinatorics;computer aided design;mathematics;geometry;algorithm	EDA	22.54345361761848	27.306982814466583	80395
5947c06e0c2918f2aefcf70ec2379a26652ca5bc	a polynomial time approximation scheme for the two-source minimum routing cost spanning trees	spanning trees;metric space;approximate algorithm;espace metrique;temps polynomial;graph method;complexite calcul;ptas;approximation algorithms;espacio metrico;approximation algorithm;arbre maximal;problema np duro;metodo grafo;methode graphe;algorithme;algorithm;np hard problem;complejidad computacion;aproximacion polinomial;arbol maximo;probleme np difficile;computational complexity;approximate solution;cout routage;approximation polynomiale;algoritmo aproximacion;polynomial time;routing cost;approximation scheme;spanning tree;np hard;algorithme approximation;polynomial time approximation scheme;polynomial approximation;algoritmo;tiempo polinomial	Let G be an undirected graph with nonnegative edge lengths. Given two vertices as sources and all vertices as destinations, we investigated the problem how to construct a spanning tree of G such that the sum of distances from sources to destinations is minimum. In the paper, we show the NP-hardness of the problem and present a polynomial time approximation scheme. For any ε > 0, the approximation scheme finds a (1 + ε)approximation solution inO(n 1/ε+1 ) time. We also generalize the approximation algorithm to the weighted case for distances that form a metric space.  2002 Elsevier Science (USA). All rights reserved.	approximation algorithm;file spanning;graph (discrete mathematics);np-hardness;polynomial;polynomial-time approximation scheme;routing;spanning tree;time complexity;vertex (geometry)	Bang Ye Wu	2002	J. Algorithms	10.1016/S0196-6774(02)00205-5	mathematical optimization;combinatorics;spanning tree;calculus;np-hard;connected dominating set;k-minimum spanning tree;mathematics;approximation algorithm;algorithm	Theory	21.416950499586264	27.071071126763247	80545
369153669fb122262b2813fdac499a0391261d60	constraint satisfaction with bounded treewidth revisited	optimisation sous contrainte;arbre graphe;chevauchement;dynamic programming;constrained optimization;evaluation performance;programacion dinamica;parameterized complexity;incidence;performance evaluation;temps polynomial;tree graph;time complexity;analog;complexite calcul;complejidad polinomial;evaluacion prestacion;fixed parameter tractable;polynomial complexity;overlap;constraint satisfaction;imbricacion;analogo;upper bound;optimizacion con restriccion;satisfaction contrainte;complexite polynomial;complejidad computacion;complexite temps;analogue;mathematical programming;computational complexity;programmation dynamique;polynomial time;borne inferieure;treewidth;constraint satisfaction problem;anchura arbol;satisfaccion restriccion;complejidad tiempo;arbol grafo;borne superieure;largeur arborescente;programmation mathematique;programacion matematica;incidencia;lower bound;cota superior;cota inferior;tiempo polinomial	We consider the constraint satisfaction problem (CSP) parameterized by the treewidth of primal, dual, and incidence graphs, combined with several other basic parameters such as domain size and arity. We determine all combinations of the considered parameters that admit fixed-parameter tractability.	chomsky hierarchy;constraint satisfaction problem;incidence matrix;parameterized complexity;treewidth	Marko Samer;Stefan Szeider	2006		10.1007/11889205_36	1-planar graph;time complexity;mathematical optimization;constrained optimization;combinatorics;discrete mathematics;computer science;clique-sum;constraint satisfaction dual problem;mathematics;tree-depth;complexity of constraint satisfaction;upper and lower bounds;partial k-tree;chordal graph;algorithm	AI	20.790166056360302	26.19274206279102	80816
40313c53f47532ab296674ecb40a972f4a0f8cdd	searching lattice data structures of varying degrees of sortedness		Lattice data structures are space efficient and cache-suitable data structures. The basic searching, insertion, and deletion operations are of time complexity O(√ N). We give a jump searching algorithm of time complexity O(J(L) log(N)), where J(L) is the jump factor of the lattice. J(L) approaches 4 when the degree of sortedness of the lattice approaches √ N. A sorting procedure of time complexity O(√ N) that can be used, during the system idle time, to increase the degree of sortedness of the lattice is given.	best, worst and average case;cpu cache;cache (computing);computer data storage;data structure;randomized algorithm;search algorithm;sorting algorithm;t-tree;time complexity	Mohammad Obiedat	2016	CoRR		combinatorics;discrete mathematics;theoretical computer science;mathematics	Theory	14.189918743003402	28.337031081039562	80826
03b5e4a328df0a2e5e60effc4f874e8fdd88e2ef	equivalence of structural knowledges in distributed algorithms	distributed algorithm	Distributed algorithms correctness usually relies upon th e use of some knowledge about the underlying network (a specific topology , some metrics,...). We define equivalent structural knowledges to be such knowledges that can be computed distributively one knowing the other. W present a combinatorial characterization of this equivalence. Some app lications are also given: zero knowledge, classical metrics (size, diameter, ...). This characterization is defined in terms of graphs covering s and quasicoverings. The proofs are based upon an algorithm proposed b y A. Mazurkiewicz, and on techniques of termination detection by Shy, Szy manski, and Prywes.	correctness (computer science);distributed algorithm;turing completeness	Emmanuel Godard;Yves Métivier	2002			theoretical computer science;equivalence (measure theory);distributed algorithm;artificial intelligence;machine learning;computer science	Theory	20.483532172858855	31.516859012729874	81081
bf9176349a68969e5c7a4e7429623943e8572481	usefulness of the karp-miller-rosenberg algorithm in parallel computations on strings and arrays	optimisation;algorithm analysis;optimizacion;programacion paralela;parallel programming;algorithme;algorithm;estructura datos;parallel computer;optimization;analyse algorithme;structure donnee;data structure;analisis algoritmo;programmation parallele;algoritmo	The Karp-Miller-Rosenberg (1972) algorithm was one of the first efficient (almost linear) sequential algorithms for finding repeated patterns and for string matching. In the area of efficient sequential computations on strings it was soon superseded by more efficient (and more sophisticated) algorithms. We show that the Karp-Miller-Rosenberg algorithm (KMR) must be considered as a basic technique in parallel computations. For many problems, variations of KMR give the (known) most efficient parallel algorithms. The representation of the set of basic factors (subarrays) of a string (array) produced by the algorithm is an extremely useful data structure in parallel algorithms on strings and arrays. This gives also a general unifying framework for a large variety of problems. We show that the following problems for strings and arrays can be solved by almost optimal parallel algorithms: pattern-matching, longest repeated factor (subarray), longest common factor (subarray), maximal symmetric factor (subarray). Also the following problems for strings can be solved within the same complexity bounds: finding squares, testing even palstars and compositions of k palindromes for k=2, 3, 4, computing Lyndon factorization and building minimal pattern-matching automata. In the model without concurrent writes the parallel time is O(log(n)‘) (with n processors) and in the model with concurrent writes the time, for most of the problems, is O(log(n)) (with n processors). For two problems related to the one-dimensional case (longest repeated factor and longest common factor) there were designed parallel algorithms using suffix trees (Apostolico et al. 1988). However, our data structure is simpler and, furthermore, for the two-dimensional case suffix	array data structure;automata theory;central processing unit;computation;dijkstra's algorithm;maximal set;parallel algorithm;pattern matching;regular expression;sequential algorithm;string searching algorithm;suffix tree	Maxime Crochemore;Wojciech Rytter	1991	Theor. Comput. Sci.	10.1016/0304-3975(91)90073-B	combinatorics;data structure;computer science;theoretical computer science;analysis of parallel algorithms;mathematics;algorithm	Theory	13.64977136690539	30.487400171211696	81202
99b26a4ed56b306fd272e0812062bb7bfc750230	biased mutation operators for subgraph-selection problems	traveling salesman problem tsp;traveling salesman problem;graph theory;estimacion sesgada;minimum spanning tree problem mstp;approximation asymptotique;probability trees mathematics travelling salesman problems evolutionary computation;selection problem;optimisation;problema seleccion;teoria grafo;exponential distribution;euclidean theory;evolutionary computation;probability;optimum;subgrafo;optimizacion;ley exponencial;fonction repartition;loi exponentielle;edge detection;travelling salesman problem;arbre maximal;exponential function;biased operators;traveling salesman problem tsp biased operators graph problems minimum spanning tree problem mstp mutation;probabilistic approach;trees mathematics;indexing terms;satisfiability;theorie graphe;kruskal minimum spanning tree algorithm biased mutation operators subgraph selection problems degree constrained minimum spanning tree problem traveling salesman problem low weight edges evolutionary algorithms biased edge exchange mutation euclidean instance uniform random instances exponential functions optimal probabilities;deteccion contorno;problema viajante comercio;detection contour;funcion distribucion;large scale;graph problems;distribution function;degeneration;arbol maximo;probleme commis voyageur;theoretical analysis;sous graphe;enfoque probabilista;approche probabiliste;travelling salesman problems;optimo;minimum spanning tree;theorie euclidienne;genetic mutations tree graphs traveling salesman problems evolutionary computation clouds polynomials large scale systems algorithm design and analysis computer graphics computer science;algorithme evolutionniste;algoritmo evolucionista;optimization;spanning tree;asymptotic approximation;evolutionary algorithm;subgraph;biased estimation;estimation biaisee;mutation;teoria euclidiana;aproximacion asintotica;probleme selection	Many graph problems seek subgraphs of minimum weight that satisfy a set of constraints. Examples include the minimum spanning tree problem (MSTP), the degree-constrained minimum spanning tree problem (d-MSTP), and the traveling salesman problem (TSP). Low-weight edges predominate in optimum solutions to such problems, and the performance of evolutionary algorithms (EAs) is often improved by biasing variation operators to favor these edges. We investigate the impact of biased edge-exchange mutation. In a large-scale empirical investigation on Euclidean and uniform random instances, we describe the distributions of edges in optimum solutions of the MSTP, the d-MSTP, and the TSP in terms of the edges' weight-based ranks. We approximate these distributions by exponential functions and derive approximately optimal probabilities for selecting edges to be incorporated into candidate solutions during mutation. A theoretical analysis of the expected running time of a (1+1)-EA on nondegenerate instances of the MSTP shows that when using the derived probabilities for edge selection in mutation, the (1+1)-EA is asymptotically as fast as a classical implementation of Kruskal's minimum spanning tree algorithm. In experiments on the MSTP, d-MSTP, and the TSP, we compare the new edge-selection strategy to four alternative methods. The results of a (1+1)-EA on instances of the MSTP support the theory and indicate that the new strategy is superior to the other methods in practice. On instances of the d-MSTP, a more sophisticated EA with a larger population and unbiased recombination performs better with the new biased mutation than with alternate mutations. On the TSP, the advantages of weight-biased mutation are generally smaller, because the insertion of a specific new edge into a tour requires the insertion of a second dependent edge as well. Although we considered Euclidean and uniform random instances only, we conjecture that the same biasing toward low-weight edges also works well on other instance classes structured in different ways.	approximation algorithm;biasing;degree-constrained spanning tree;evolutionary algorithm;experiment;file spanning;kruskal's algorithm;list of algorithms;minimum spanning tree;minimum weight;time complexity;travelling salesman problem	Günther R. Raidl;Gabriele Koller;Bryant A. Julstrom	2006	IEEE Transactions on Evolutionary Computation	10.1109/TEVC.2006.871251	mathematical optimization;combinatorics;discrete mathematics;computer science;graph theory;machine learning;evolutionary algorithm;mathematics;travelling salesman problem;algorithm;evolutionary computation	Theory	21.698582979144362	18.676954306559676	81267
a3ac601e071565d41452ae63aff4a812f3cc1264	determinism versus non-determinism for linear time rams with memory restrictions	computer model;linear time algorithm;decision problem;random access machine;hamming distance;linear time;reading and writing	Our computational model is a random access machine with n read only input registers each containing c logn bits of information and a read and write memory. We measure the time by the number of accesses to the input registers. We show that for all k there is an ǫ > 0 so that if n is sufficiently large then the elements distinctness problem cannot be solved in time kn with ǫn bits of read and write memory, that is, there is no machine with this values of the parameters which decides whether there are two different input registers whose contents are identical. We also show that there is a simple decision problem that can be solved in constant time (actually in two steps) using non-deterministic computation, while there is no deterministic linear time algorithm with ǫn log n bits read and write memory which solves the problem. More precisely if we allow kn time for some fixed constant k, then there is an ǫ > 0 so that the problem cannot be solved with ǫn log n bits of read and write memory if n is sufficiently large. The decision problem is the following: “Find two different input registers, so that the Hamming distance of their contents is at most 1 4c logn”. 1 4 can be replaced by any fixed 0 < γ < 1 2 if c is sufficiently large with respect to γ. We actually show that the promise problem : “decide whether all occurring Hamming distances are greater than ( 2 − γ)c log n or there is at least one which is smaller than γc logn” where γ > 0 is an arbitrarily small constant, cannot be solved by a nonlinear algorithm with the described limitations even if we know that we only get inputs where one of these conditions hold. (In this case ǫ may depend on γ too). Introduction. One of the main goals of complexity theory is the separation of nondeterministic and deterministic computation. We solve the problem for random access machines with certain restrictions on the size of their working memory. Although the restrictions are strong, the working memory must be smaller than the input, still, under certain circumstances this computational model is realistic as we will explain later. We also use realistic search problems for the sepration results. The first problem is the element distinctness problem, that is, we have to decide whether there are different input registers with identical contents. This problem is of great practical and theoretical interest, it has been studied in great detail in various computational	computation;computational complexity theory;computational model;decision problem;deterministic automaton;element distinctness problem;hamming distance;nondeterministic algorithm;nonlinear system;processor register;promise problem;random access;random-access machine;time complexity;window function	Miklós Ajtai	1998	J. Comput. Syst. Sci.	10.1006/jcss.2002.1821	computer simulation;time complexity;real-time computing;hamming distance;computer science;theoretical computer science;decision problem;mathematics;programming language;algorithm	Theory	10.118366139936139	26.08948604136906	81361
1d7c597a9f74d66399b8016b4ea9f1b5fd9cd169	understanding the scalability of bayesian network inference using clique tree growth curves	probabilistic reasoning bayesian networks clique tree clustering clique tree growth c v ratio continuous approximation gompertz growth curves controlled experiments regression;estensibilidad;bayes estimation;evaluation performance;growth curve;bayesian network;learning algorithm;methode empirique;performance evaluation;aplicacion medical;temps polynomial;bepress selected works;tree growth;evaluacion prestacion;metodo empirico;neurology;continuous approximation;empirical method;time variation;distributed computing;bayes theorem;programmation stochastique;controlled experiment;intelligence artificielle;aprendizaje probabilidades;algorithme apprentissage;biology;biologia;variation temporelle;probabilistic approach;effet dimensionnel;polynomials;probability distribution functions;reseau bayes;gompertz growth curves;tradeoffs;estimacion bayes;regression;clique tree clustering;machine learning;red bayes;enfoque probabilista;approche probabiliste;size effect;controlled experiments;probability distribution;tree structure;inferencia;edge graph;polynomial time;bayes network;calculo repartido;arete graphe;apprentissage probabilites;artificial intelligence;gompertz curves;medical application;difference set;extensibilite;scalability;inteligencia artificial;efecto dimensional;probabilistic logic;stochastic programming;c v ratio;logique probabiliste;algoritmo aprendizaje;variacion temporal;programacion estocastica;calcul reparti;inference;probability learning;probabilistic reasoning;arista grafico;biologie;clique tree growth;estimation bayes;computation;application medicale;bayesian networks;tiempo polinomial	Bayesian networks (BNs) are used to represent and ef ciently compute with multi-variate probability distributions in a wide range of disciplines. One of the main approaches to perform computation in BNs is clique tree clustering and propagation. In this approach, BN computation consists of propagation in a clique tree compiled from a Bayesian network. There is a lack of understanding of how clique tree computation time, and BN computation time in more general, depends on variations in BN size and structure. On the one hand, complexity results tell us that many interesting BN queries are NP-hard or worse to answer, and it is not hard to nd application BNs where the clique tree approach in practice cannot be used. On the other hand, it is well-known that tree-structured BNs can be used to answer probabilistic queries in polynomial time. In this article, we develop an approach to characterizing clique tree growth as a function of parameters that can be computed in polynomial time from BNs, speci cally: (i) the ratio of the number of a BN's non-root nodes to the number of root nodes, or (ii) the expected number of moral edges in their moral graphs. Our approach is based on combining analytical and experimental results. Analytically, we partition the set of cliques in a clique tree into different sets, and introduce a growth curve for each set. For the special case of bipartite BNs, we consequently have two growth curves, a mixed clique growth curve and a root clique growth curve. In experiments, we systematically increase the degree of the root nodes in bipartite Bayesian networks, and nd that root clique growth is well-approximated by Gompertz growth curves. It is believed that this research improves the understanding of the scaling behavior of clique tree clustering, provides a foundation for benchmarking and developing improved BN inference and machine learning algorithms, and presents an aid for analytical trade-off studies of clique tree clustering using growth curves.	approximation algorithm;arithmetic circuit complexity;bayesian network;cluster analysis;compiler;computation;computational complexity theory;data mining;embedded system;experiment;gloss (annotation);gompertz function;machine learning;maximal set;np-hardness;probabilistic analysis of algorithms;real-time clock;real-time computing;requirement;scalability;software propagation;time complexity;tree (data structure);tree decomposition	Ole J. Mengshoel	2010	Artif. Intell.	10.1016/j.artint.2010.05.007	clique;combinatorics;clique graph;neurology;computer science;artificial intelligence;machine learning;bayesian network;mathematics;clique percolation method;statistics	ML	17.486639746552875	23.900969840049704	81503
e2f69253b294b46cae46b7822b061eca93006f58	approximation of rna multiple structural alignment	optimal solution;non coding rna;largest nested linear subgraph problem;polynomial time algorithm;longest common subsequence;rna structure;fast algorithm;linear graphs;rna structural alignment;structure alignment	"""In the context of non-coding RNA (ncRNA) multiple structural alignment, Davydov and Batzoglou (2006) introduced in [7] the problem of finding the largest nested linear graph that occurs in a set G of linear graphs, the so-called Max-NLS problem. This problem generalizes both the longest common subsequence problem and the maximum common homeomorphic subtree problem for rooted ordered trees. In the present paper, we give a fast algorithm for finding the largest nested linear subgraph of a linear graph and a polynomial-time algorithm for a fixed number (k) of linear graphs. Also, we strongly strengthen the result of Davydov and Batzoglou (2006) [7] by proving that the problem is NP-complete even if G is composed of nested linear graphs of height at most 2, thereby precisely defining the borderline between tractable and intractable instances of the problem. Of particular importance, we improve the result of Davydov and Batzoglou (2006) [7] by showing that the Max-NLS problem is approximable within ratio O(logm""""o""""p""""t) in O(kn^2) running time, where m""""o""""p""""t is the size of an optimal solution. We also present O(1)-approximation of Max-NLS problem running in O(kn) time for restricted linear graphs. In particular, for ncRNA derived linear graphs, a 14-approximation is presented."""	approximation	Marcin Kubica;Romeo Rizzi;Stéphane Vialette;Tomasz Walen	2011	J. Discrete Algorithms	10.1016/j.jda.2010.03.002	nucleic acid structure;mathematical optimization;structural alignment;combinatorics;discrete mathematics;longest path problem;longest common subsequence problem;mathematics;non-coding rna	Theory	21.750672807278203	24.103902272819898	81648
2878011bb6b0d4f996dbeb56da9542436ffee8ec	constructing trees in parallel	parallel algorithm	"""An O(log ~ n) time, n2 / logn processor as well as an O(log n) time, n3/log n processor CREW deterministic parallel algorithms are presented for constructing Huffman codes from a given list of frequences. The time can be reduced to O(log n(loglog n) 2) on an CRCW model, using only n2/(log log n) 2 processors. Also presented is an optimal O(log n) time, O(n/ log n) processor EREW parallel algorithm for constructing a tree given a list of leaf depths when the depths are monotonic. An O(log 2 n) time, n processor parallel algorithm is given for the general tree construction problem. We also give an O(log 2 n) time n2/ log2n processor algorithm which finds a nearly optimal binary search tree. An O(log 2 n) time n 2'36 processor algorithm for recognizing linear context free languages is given. A crucial ingredient in achieving those bounds is a formulation of these problems as multiplications of special matrices which we call concave matrices. The structure of these matrices makes their parallel multiplication dramatically more efficient than that of arbitrary matrices. *Depar tment of Computer Science, Purdue University. Supported by the Office of Naval Research under Grants N00014-84K-0502 and N00014-86-K-0689, and the National Science Foundation under Grant DCR-8451393, with matchlng funds from AT&T. tDepar tment of Computer Science, Johns Hopkins University. Suppor ted by National Science Foundation through grant CCR-88-04284 tICS, UC Irvine. §School of Compute r Science, CMU and Depar tment of Computer Science, USC. Supported by National Science Foundation through grant CCR-87-13489. I'ermission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. '1'o copy otherwise, or to republish, requires a fee and/or specific permission. ~,:~ 1989 ACM 0-89791-323-X/89/0006/0421 $1.50 421 1 I n t r o d u c t i o n In this paper we present several new parallel algorithms. Each algorithm uses substantially fewer processors than used in previously known algorithms. The four problems considered are: The Tree Construction Problem, The Huffman Code Problem, The Linear Context Free Language Recognition Problem, and The Optimal Binary Search Tree Problem. In each of these problems the computational expensive part of the problem is finding the associated tree. We shall show that these trees are not arbitrary trees but are special. We take advantage of the special form of these trees to decrease the number of processors used. All of the problems we consider in this paper, as well as many other problems, can be performed in sequential polynomial time using Dynamic Programming. Arc algorithms for each of these problems can be obtained by parallelization of Dynamic Programming. Unfortunately, this approach produces parallel algorithms which use O(n 6) or more processors. An algorithm which increases the work performed from O(n) or O(n 2) to O(n 6) is not of much practical value. In this paper we present several new paradigms for improving the processor efficiency for dynamic programming problems. For all the problems considered a tree or class of trees is given implicitly and the algorithm must find one such tree. The construction of optimal codes is a classical problem in communication. Let ~ = {0, 1 .... , o"""" 1} be an alphabet. A code £ = {cl . . . . . cn} over E is a finite nonempty set of distinct finite sequences over ~, Each sequence ci is called code word. A code C is a prefix code if no code-word in C is a prefix of another code-word. A message over. C is a word resulting from the concatenation of code words from d. We assume the words over a source alphabet a l , . . . , a n are to be transmitted over a communication channel which can transfer one symbol of ~ per unit of time, and the probability of appearance of ai is Pi C ~ . The H u f f m a n Cod ing P r o b l e m is to construct a prefix code C =: {c l , . . . , cn E ~*} such f g . that the average word length Ei=lp, •Icil is minimum, where Ici] is the length of ci. It is easy to see that prefix codes have the nice property that a message can be decomposed in code word in only one waythey are uniquely decipherable. It is interesting to point out that Kraft and McMillan proved that for any code which is uniquely decipherable there is always a prefix code with the same average word length [13]. In 1952, IIuffman [9] gave an elegant sequential algorithm which can generate an optimal prefix code in O(n log n) time. If the probabilities are presorted then his algorithm is actually linear time [11]. Using parallel dynamic programming, Kosaraju and Teng [18], independently, gave the first A/'C algorithm for the IIuffman Coding Problem. However, b~th constructions use n e processors. In this paper, we first show how to reduce the processor count to n s, while using O(log n) time, by showing that we may assume that the tree associated with the prefix code is left-justified (to be defined in Section 2). The n 3 processor count arises from the fact that we are multiplying n x n matrices over a closed semiring. We reduce the processor count still further to n2/log n by showing that, after suitable modification, the matrices which are multiplied are concave (to be defined later). The structure of these matrices makes their parallel multiplication dramatically more efficient than that of arbitrary matrices. An O(logn log log n) time nZ/log n processor CREW algorithm is presented for multiplying them. Also given is an O((loglogn) 2) time, n2/log log n processor CRCW algorithm for multiplying two concave"""" matrices 1. The algorithm for construction of a ttuffman code still uses n 2 processors, which is probably too large for practical consideration since Huffman's algorithm only takes O(n log n) sequential time. Shannon and Fano gave a code, the Shannon-Fano Code, which is only one bit off from optimal. That is, the expected length o fa Shannon-Fano code word is at most one bit longer than the Huffman code word. The construction of the Shannon-Fano Code reduces to the following Tree C o n s t r u c t i o n P r o b l e m , Def in i t ion 1.1 (Tree C o n s t r u c t i o n P r o b l e m ) Given n integer values ll , . . . . ln, construct an ordered Mnary tree with n leaves whose levels when read form left to right are 11,..., 1,. 1Independently, [1] and [2] improved the CREW algorithm results by showing that two concave matrices can be rnultiplied in O(logn) time, using n2/logn CREW PRAM processors. Also, [2] improved the CRCW algorithm by reducing the number ofCRCW PRAM processors required to n2/(log log n) 2. We give an O(log 2 n) time, n processor EREW PRAM parallel algorithm for the tree construction problem. In the case when ll, .. •, 1, are monotonic, we give an O(logn) time and n / l o g n processor EREW PRAM parallel algorithm. In fact, trees where the level of the leaves are monotone will be used for both constructing Huffman Codes and Shannon-Fano Codes. Using our solution of the tree construction problem we get an O(logn) time n / logn processor EREW PRAM algorithm for constructing ShannonFano Codes. We also consider the problem of parallel constructing optimal binary search trees as defined by Knuth [10]. The best known NC algorithm for this problem is the parallelization of dynamic programming which uses n 6 processors. In this paper, using the new concave matrix multiplication algorithm, we show how to compute nearly optimal binary search tree in O(log 2 n) time using n2/ logn processors. Our search trees are only off from optimal by an additive amount of 1/n k for any fixed k. Finally, we consider recognition of linear context free languages. A CFL is said to be linear if all productions are of the form A --~ bB, A ~ Bb or a ~ A where A and B are nonterminal variables and a and b are terminal variables. It is well known from Ruzzo [17] that the general CFL's recognition problem can be performed on a CRCW PRAM in O(log n) time using n 6 processors again by parallelization of dynamic programming. By observing that the parse tree of the linear context free language is of very restricted form, we construct an O(n 3) processor, O(log 2 n) time CREW PRAM algorithm for it. Using the fact that we are doing Boolean matrix multiplication, we can reduce the processor count to n 2 ~ . 2 Pre l iminar i e s Throughout this paper a tree will be a rooted tree. It is ordered if the children of each node are ordered from left to right. The level of a node in a tree is its distance from the root. A binary tree T is complete at level I if there are 2 z nodes in T at level I. A binary tree is empty at level I if there is no vertex at level i. A binary tree T is a left-justified tree if it satisfies the following property: 1. i fa vertex has only one child, then it is a left child; 2. if u and v are sibling nodes of T, where u is to the left of v, then if Tv is not empty at some level 1,"""	approximation algorithm;binary tree;cpu cache;carrier-to-noise ratio;central processing unit;channel (communications);cipher;code (cryptography);code word;computation;computer science;concatenation;concave function;context-free grammar;context-free language;courant–friedrichs–lewy condition;dynamic programming;huffman coding;kosaraju's algorithm;matrix multiplication algorithm;optimal binary search tree;parallel algorithm;parallel computing;parallel random-access machine;parse tree;parsing;prefix code;sequential algorithm;shannon (unit);shannon–fano coding;television;terminal and nonterminal symbols;time complexity;tree (data structure);uc browser;utility functions on indivisible goods;vertex (graph theory);monotone	Mikhail J. Atallah;S. Rao Kosaraju;Lawrence L. Larmore;Gary L. Miller;Shang-Hua Teng	1989		10.1145/72935.72980	parallel computing;computer science;parallel algorithm	Theory	11.90513857740618	25.90744551210397	81763
e44371064ae0a758ab846c6de2cc028c1520357c	computing runs on a general alphabet	general alphabet;runs;repetitions;algorithms	We describe a RAM algorithm computing all runs (maximal repetitions) of a given string of length n over a general ordered alphabet in O(n log 2 3 n) time and linear space. Our algorithm outperforms all known solutions working in Θ(n log σ) time provided σ = n, where σ is the alphabet size. We conjecture that there exists a linear time RAM algorithm finding all runs.	data structure;maximal set;online algorithm;random-access memory;time complexity	Dmitry Kosolobov	2016	Inf. Process. Lett.	10.1016/j.ipl.2015.11.016	combinatorics;discrete mathematics;computer science;mathematics;algorithm	Theory	13.257933698632549	26.911125318546468	81792
1f67e7ef5d9a0ee6af873c92359cc9d0f746912c	on the efficiency of a new method of dictionary construction		"""Programs for constructing dictionaries of texts, with computers, have sometimes been adaptat ions of methods suitable for manual construction with card indexes. With all card index methods it is customary to keep the different words collected in alphabetical order, for the structure of a card index lends itself to such a process: all tha t is necessary to insert ~ new word into the index between two existing ones is to make out a new card and to put it in the correct place. However, the insertion of a new word in the store of a computer where the words are kept in alphabetical order is a t ime-consuming process, for all the words below the one which is inserted have to be """"moved down"""" by one place. If, however, a computer method is used where the words are not stored in alphabetical order but in the order in which they occur, the position is even worse, for although the shifting of words is eliminated, the dict ionary search which is necessary for each word in the text, to establish whether it is a new word or not, must involve all the words collected so far, and not: iust a small number of them, as would be the ease if the words were in alphabetical order, and a logarithmic search (Booth, 1955) could be used. The method of construction described in this paper overcomes both these problems. I t is no t an adaptat ion of a manual method, but is designed specifically for computers. The method is based on a tree structure, which is discussed below."""	computer;dictionary;protologism;tree structure	Andrew Donald Booth;Andrew J. T. Colin	1960	Information and Control	10.1016/S0019-9958(60)90901-3	mathematics;combinatorics	Web+IR	10.22156701628201	28.15509931972727	81851
99ebece5f3fb724e96133bb876ddddc5fd1ed1cd	linear ordering based mip formulations for the vertex separation or pathwidth problem		We consider the task to compute the pathwidth of a graph which has been shown to be equivalent to the vertex separation problem. The latter is naturally modeled as a linear ordering problem w.r.t. the vertices of the graph. Mixed-integer programs proposed so far express linear orders using either position or set assignment variables. As we show, the lower bound on the pathwidth obtained when solving their linear programming relaxations is zero for any directed graph. We then present a new formulation based on conventional linear ordering variables and a slightly different perspective on the problem that sustains stronger lower bounds. An experimental evaluation of three mixed-integer programs, each representing one of the different modeling schemes, displays their potentials and limitations when used to solve the problem to optimality.		Sven Mallach	2018	J. Discrete Algorithms	10.1016/j.jda.2018.11.012	combinatorics;vertex (geometry);pathwidth;directed graph;integer programming;linear programming;graph;upper and lower bounds;mathematics	Theory	23.571479961983943	19.715549173414455	81888
f779353f73b89872767e42f7ff21fcc6bb111184	analysis and improvement of ac-bm algorithm in intrusion detection system	bad character shift technology;matching search;automaton;algorithm;snort	The advantage of the AC-BM algorithm is that it can conduct the matching search of multiple pattern strings simultaneously and the shift of the text string is optimized. However, it can only conduct the search in one text string at a time. In order to realize the search in multiple text strings simultaneously at a time, the multi-objective AC-BM algorithm is designed in the paper. With the automaton union operation technology, the multi-objective multi-pattern tree automaton is constructed; and with the bad character shift technology of the BM algorithm, the shift of text string set is calculated. In the Snort system, the 2-objective AC-BM algorithm and the 3-objective AC-BM algorithm is realized respectively. Experiment results show that the improved algorithms are obviously superior to the AC-BM algorithm regarding time property on the condition that if a pattern string is found in multiple text strings, the algorithm will stop (indicating that an attack is detected) .	algorithm;intrusion detection system;network analyzer (ac power)	Zhengcai Wang;Xiaofeng Wang;Zhengyi Tang	2013	JNW	10.4304/jnw.8.5.1191-1196	approximate string matching;commentz-walter algorithm;boyer–moore–horspool algorithm;computer science;theoretical computer science;machine learning;boyer–moore string search algorithm;automaton;string metric;algorithm;rabin–karp algorithm;string searching algorithm;search algorithm	ML	12.792149437859512	27.90427537227223	81935
f82c4d9cab235747c0de370bf7de191c682fa00d	fast index based filters for music retrieval	music retrieval;indexation;preprint	We consider two content-based music retrieval problems where the music is modeled as sets of points in the Euclidean plane, formed by the (on-set time, pitch) pairs. We introduce fast filtering methods based on indexing the underlying database. The filters run in a sublinear time in the length of the database, and they are lossless if a quadratic space may be used. By taking into account the application, the search space can be narrowed down, obtaining practically lossless filters using linear size index structures. For the checking phase, which dominates the overall running time, we exploit previously designed algorithms suitable for local checking. In our experiments on a music database, our best filter-based methods performed several orders of a magnitude faster than previous solutions.	algorithm;experiment;list of online music databases;lossless compression;time complexity	Kjell Lemström;Niko Mikkilä;Veli Mäkinen	2008			speech recognition;computer science;multimedia;information retrieval;preprint	DB	12.203813864779345	27.776217018739512	81949
00533a82a4aaf0e19ef5245b5a151980208c5c6d	a nearly best-possible approximation algorithm for node-weighted steiner trees.	performance guarantee;network design;approximate algorithm;algorithm performance;arbre steiner;algorithme glouton;aproximacion;reseau;red;probleme steiner;approximation;combinatorial problem;probleme combinatoire;problema combinatorio;complexity class;resultado algoritmo;polynomial time;performance algorithme;greedy algorithm;ponderation noeud;steiner tree problem;steiner tree;arbol steiner;network	We give the rst approximation algorithm for the node-weighted Steiner tree problem. Its performance guarantee is within a constant factor of the best possible unless ~ P NP. Our algorithm generalizes to handle other network design problems.	approximation algorithm;network planning and design;steiner tree problem	Philip N. Klein;R. Ravi	1993	J. Algorithms	10.1006/jagm.1995.1029	mathematical optimization;combinatorics;steiner tree problem;computer science;mathematics;algorithm	Theory	20.298074469990453	26.576318278123722	81993
9c27ff29cfa2f5de92836674017e660bb0272b7f	priority search trees in secondary memory (extended abstract)	optimal solution;shortest path;longest path;search trees;keyword search;data structure	In this paper we investigate how priority search trees can be adapted to secondary memory. We given an optimal solution for the static case, where the set of points to be stored is fixed. For the dynamic case we present data structures derived from B-trees and from a generalized version of red-black trees. The latter are interesting in the internal case, too, since they are better balanced than standard red-black trees, in that the ratio longest path/shortest path is smaller.	auxiliary memory	Christian Icking;Rolf Klein;Thomas Ottmann	1987		10.1007/3-540-19422-3_7	beam search;combinatorics;binary search tree;data structure;longest path problem;computer science;theoretical computer science;machine learning;jump search;mathematics;best-first search;shortest path problem;ternary search tree;k shortest path routing	Theory	14.406894140181125	27.875898279847565	82064
3deb299d685f32c9df2febc7c5618b1575eda82f	an approximation algorithm for the maximum independent set problem in cubic planar graphs	graph theory;teoria grafo;approximate algorithm;algorithm complexity;grafico cubico;complejidad algoritmo;theorie graphe;conectividad diagrama;construction graphe;graph connectivity;graphe cubique;grafico planario;complexite algorithme;graphe planaire;vertex graph;construccion diagrama;cuspide grafico;connectivite graphe;graph construction;planar graph;sommet graphe;cubic graph;maximum independent set	Abstract#R##N##R##N#A polynomial time approximation algorithm A for the problem of finding a maximal independent set for cubic planar graphs is presented. It is shown that MA > 6/7 in the case of cubic planar graphs and MA = 7/8 in the case of triangle free cubic planar graphs where MA is the worst-case ratio of the size of the independent set found by A to the size of the maximum independent set for the graph input to A.	approximation algorithm;cubic function;independent set (graph theory);planar graph	Elarbi Choukhmane;John Franco	1986	Networks	10.1002/net.3230160402	1-planar graph;pathwidth;split graph;combinatorics;discrete mathematics;independent set;cubic form;topology;dominating set;vertex cover;longest path problem;clique problem;connectivity;graph theory;hopcroft–karp algorithm;metric dimension;nowhere-zero flow;cubic graph;planar straight-line graph;vertex;mathematics;maximal independent set;chordal graph;indifference graph;book embedding;planar graph;matching	Theory	23.542741464727435	28.261108637273047	82319
3a315a14a2fc773d2d800186121905016de31705	a lazy version of eppstein's k shortest paths algorithm	camino mas corto;desviacion;arbre graphe;metodo caso peor;shortest path;evaluation performance;interet;algorithmique;performance evaluation;execution time;tree graph;shortest path algorithm;interes;longitud;evaluacion prestacion;deviation;plus court chemin;length;graphe pondere;grafo pondero;longueur;algorithmics;mathematical programming;algoritmica;shortest path tree;methode cas pire;temps execution;weighted graph;interest;tiempo ejecucion;arbol grafo;worst case method;programmation mathematique;programacion matematica	We consider the problem of enumerating, in order of increasing length, the K shortest paths between a given pair of nodes in a weighted digraph G with n nodes and m arcs. To solve this problem, Eppstein’s algorithm first computes the shortest path tree and then builds a graph D(G) representing all possible deviations from the shortest path. Building D(G) takes O(m+n log n) time in the basic version of the algorithm. Once it has been built, the K shortest paths can be obtained in order of increasing length in O(K log K) time. However, experimental results show that the time required to build D(G) is considerable, thereby reducing the practical interest of the algorithm. In this paper, we propose a modified version of Eppstein’s algorithm in which only the parts of D(G) which are necessary for the selection of the K shortest paths are built. This version maintains Eppstein’s worst-case running time and entails an important improvement in practical performance, according to experimental results that are also reported here.	asymptote;best, worst and average case;computation;dijkstra's algorithm;directed graph;lazy evaluation;procedural generation;randomness;recursion (computer science);recursively enumerable set;shortest path problem;time complexity;worst-case complexity	Víctor M. Jiménez;Andrés Marzal	2003		10.1007/3-540-44867-5_14	combinatorics;dijkstra's algorithm;floyd–warshall algorithm;interest;artificial intelligence;length;yen's algorithm;mathematics;deviation;shortest path problem;algorithmics;tree;k shortest path routing;shortest path faster algorithm;algorithm;shortest-path tree	Theory	19.19623906421338	26.425064929757035	82749
47615e58871c8f8536b1b30656092bec66c7e369	time and space optimal implementations of atomic multi-writer register	concurrent;complejidad espacio;time complexity;implementation;temps lineaire;complejidad lineal;metodo secuencial;tiempo lineal;space time;sequential method;linear complexity;espacio tiempo;registre;complexite temps;simultaneo;informatique theorique;estructura datos;linear time;borne inferieure;space complexity;68p05;methode sequentielle;simultane;structure donnee;multi writer multi reader;complexite espace;complejidad tiempo;implementacion;complexite lineaire;68q85;data structure;registro;register;espace temps;lower bound;cota inferior;computer theory;informatica teorica	This paper addresses the wide gap in space complexity of atomic, multi-writer, multi-reader register implementations. While the space complexity of all previous implementations is linear, the lower bounds are logarithmic. We present three implementations which close this gap: the first implementation is sequential and its role is to present the idea and data structures used in the second and third implementations. The second and third implementations are both concurrent, the second uses multi-reader physical registers while the third uses single-reader physical registers. Both the second and third implementations are optimal with respect to the two most important complexity criteria: their space complexity is logarithmic and their time complexity is linear.		Amos Israeli;Amnon Shaham	2005	Inf. Comput.	10.1016/j.ic.2004.11.004	time complexity;data structure;computer science;calculus;mathematics;programming language;algorithm	Theory	13.619305423414913	30.627184342111764	82913
56fa1405b4b716d7cd96ab11614a604bb26b6f45	minimum communication cost reordering for parallel sparse cholesky factorization	communication cosr0t;distributed memory;equivalent reordering;elimination tree;symmetric positive definite;cholesky factorization;distributed memory multiprocessor;parallel factorization;communication cost;sparse matrix	In this paper, we consider the problem of reducing the communication cost for the parallel factorization of a sparse symmetric positive de®nite matrix on a distributed-memory multiprocessor. We de®ne a parallel communication cost function and show that, with a contrived example, simply minimizing the height of the elimination tree is ineective for exploiting minimum communication cost and the discrepancy may grow in®nitely. We propose an algorithm to ®nd an ordering such that the communication cost to complete the parallel Cholesky factorization is minimum among all equivalent reorderings. Our algorithm consumes On log n m in time, where n is the number of nodes and m the sum of all maximal clique sizes in the ®lled graph. Ó 1999 Elsevier Science B.V. All rights reserved.	aggregate data;algorithm;cholesky decomposition;clique (graph theory);column (database);computation;discrepancy function;distributed memory;expect;integer factorization;loss function;maximal set;multiprocessing;parallel computing;sparse matrix;supernode (circuit);tree (data structure);tree decomposition	W.-Y. Lin;C.-L. Chen	1999	Parallel Computing	10.1016/S0167-8191(99)00027-7	mathematical optimization;distributed memory;incomplete cholesky factorization;sparse matrix;computer science;theoretical computer science;mathematics;distributed computing;minimum degree algorithm;cholesky decomposition;algebra	HPC	14.454576956579855	32.168586414846075	83090
76f968af7911b628937a9279dc5293a1342222cb	improved methods for approximating node weighted steiner trees and connected dominating sets	problema arbol steiner;connected dominating set;algorithm analysis;algorithme glouton;probleme arbre steiner;problema np duro;optimisation combinatoire;np hard problem;probleme np difficile;greedy algorithm;algoritmo gloton;analyse algorithme;technical report;steiner tree problem;algorithme approximation;combinatorial optimization;steiner tree;analisis algoritmo;optimizacion combinatoria	In this paper we study the Steiner tree problem in graphs for the case when vertices as well as edges have weights associated with them. A greedy approximation algorithm based on spider decompositions was developed by Klein and Ravi for this problem. This algorithm provides a worst case approximation ratio of 2ln κ, where κ is the number of terminals. However, the best known lower bound on the approximation ratio is (1 - o(1)) ln κ 1  assuming that NP DTIME[n O(log log  n) ], by a reduction from set cover. We show that for the unweighted case we can obtain an approximation factor of In κ. For the weighted case we develop a new decomposition theorem, and generalize the notion of spiders to branch-spiders, that are used to design a new algorithm with a worst case approximation factor of 1.5 ln κ. We then generalize the method to yield an approximation factor of (1.35+∈) ln κ, for any constant e > 0. These algorithms, although polynomial, are not very practical due to their high running time; since we need to repeatedly find many minimum weight matchings in each iteration. We also develop a simple greedy algorithm that is practical and has a worst case approximation factor of 1.6103 ln κ. The techniques developed for this algorithm imply a method of approximating node weighted network design problems defined by 0-1 proper functions as well. These new ideas also lead to improved approximation guarantees for the problem of finding a minimum node weighted connected dominating set. The previous best approximation guarantee for this problem was 3 ln n due to Guha and Khuller. By a direct application of the methods developed in this paper we are able to develop an algorithm with an approximation factor of (1.35 + ∈) ln n for any fixed ∈ > 0.	steiner tree problem	Sudipto Guha;Samir Khuller	1998		10.1007/978-3-540-49382-2_6	mathematical optimization;combinatorics;discrete mathematics;steiner tree problem;combinatorial optimization;computer science;mathematics;approximation algorithm;algorithm	Theory	22.403178634740986	19.593307200271255	83135
807f4a72495d071a21c1ac96d0d413127783fd66	streaming time series summarization using user-defined amnesic functions	amnesic approximation;streaming data;online algorithm;unfolding;optimized production technology;linguistique;streaming;informatique mobile;mobile device;performance evaluation;52478;rivers;funcion exponencial;streaming algorithm;deploiement;batch production;fonction exponentielle;sql;on line;amnesic;en linea;approximation algorithm;real time;exponential function;interrogation base donnee;despliegue;procede discontinu;interrogacion base datos;useful information;real time sensor;resumen;age;informacion util;flux donnee;flujo datos;time series;fidelity;approximation;time series summarization streaming;summarization;optimisation combinatoire;approximation theory;captador medida;time decaying;data analysis;transmission en continu;time series approximation theory data analysis functions;linguistica;measurement sensor;capteur mesure;produccion por lote;streaming algorithm time series amnesic approximation;monitoring;indexing;fidelite;resume;time series approximation;temps reel;indexation;robots;production par lot;serie temporelle;indizacion;algoritmo aproximacion;batch process;serie temporal;batch mode;user defined amnesic function;tiempo real;time series data;procedimiento discontinuo;predictive models;en ligne;repeaters;fidelidad;transmision fluyente;batch mode user defined amnesic function time series summarization streaming mobile device real time sensor online algorithm time series approximation;algorithme approximation;data flow;combinatorial optimization;mobile computing;abstract;empirical evaluation;robots indexing meteorology optimized production technology performance evaluation large scale systems monitoring predictive models rivers repeaters;functions;meteorology;database query;information utile;large scale systems;optimizacion combinatoria;edad;linguistics	The past decade has seen a wealth of research on time series representations. The vast majority of research has concentrated on representations that are calculated in batch mode and represent each value with approximately equal fidelity. However, the increasing deployment of mobile devices and real time sensors has brought home the need for representations that can be incrementally updated, and can approximate the data with fidelity proportional to its age. The latter property allows us to answer queries about the recent past with greater precision, since in many domains recent information is more useful than older information. We call such representations amnesic. While there has been previous work on amnesic representations, the class of amnesic functions possible was dictated by the representation itself. In this work, we introduce a novel representation of time series that can represent arbitrary, user-specified amnesic functions. We propose online algorithms for our representation, and discuss their properties. Finally, we perform an extensive empirical evaluation on 40 datasets, and show that our approach can efficiently maintain a high quality amnesic approximation.	approximation algorithm;batch processing;display resolution;mobile device;online algorithm;sensor;software deployment;time series	Themis Palpanas;Michail Vlachos;Eamonn J. Keogh;Dimitrios Gunopulos	2008	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2007.190737	simulation;combinatorial optimization;computer science;artificial intelligence;machine learning;time series;data mining;database;mathematics;mobile computing;approximation algorithm;algorithm;batch processing	DB	14.576311761809205	24.265381794811407	83203
09301c055d0d22340ffe431c19a545377ef8e760	analysis of tree algorithms for the simulation event list	simulation evenement discret;file attente;arbre recherche binaire;algorithm analysis;simulation;queue;analysis of algorithm;arbre p;binary search tree;priority queue;analyse algorithme;structure donnee;data structure	The simulation event list is a priority queue found in simulation packages like SIMULA. Its task is to hold the pending events of the simulation, so that they can be executed in the correct order. For varying reasons, most standard algorithms are poorly suited to the special demands of this application. Recent work, on the average-case performance of the binary search tree (BST) and p-tree algorithms under these conditions, is presented. The algorithms are shown to be worse than O(log(n)) in many cases.	algorithm;best, worst and average case;phylogenetic tree;priority queue;search tree;simula;simulation	Jeffrey H. Kingston	1985	Acta Informatica	10.1007/BF00290143	parallel computing;binary search tree;data structure;computer science;theoretical computer science;programming language;queue;priority queue;algorithm	Theory	10.919154283775338	31.646192010343587	83388
ea9c5799082913f8fe030d2756252a8dc46ab7f9	maximum independent set for intervals by divide-prune-and-conquer	maximum independent set	As an easy example of divide, prune, and conquer, we give an output-sensitive O(n log k)-time algorithm to compute, for n intervals, a maximum independent set of size k.	algorithm;independent set (graph theory)	Jack Snoeyink	2005			combinatorics;discrete mathematics;independent set;computer science	Theory	24.54754187312802	26.76162979401078	83548
b238f0dd35d59064aa12d3f26375919064b39b7e	connected and alternating vectors: polyhedra and algorithms	polyhedral combinatorics;combinatorial optimization	Given a graphG = (V, E), letaS, S ∈ L, be the edge set incidence vectors of its nontrivial connected subgraphs.	algorithm;polyhedron	Heinz Gröflin;Thomas M. Liebling	1981	Math. Program.	10.1007/BF01589348	mathematical optimization;combinatorics;discrete mathematics;polyhedral combinatorics;combinatorial optimization;mathematics	Theory	23.86424237674431	24.672527660384077	83551
90a6f926db19d30648d96b29f2b675d15317ab3e	a fast greedy sequential heuristic for the vertex colouring problem based on bitwise operations	эвристика;heuristic;sequential colouring;жадный алгоритм;последовательная раскраска;битовые операции;раскраска графа;bitwise operations;greedy algorithm;graph colouring	In this paper a fast greedy sequential heuristic for the vertex colouring problem is presented. The suggested algorithm builds the same colouring of the graph as the well-known greedy sequential heuristic in which on every step the current vertex is coloured in the minimum possible colour. Our main contributions include introduction of a special matrix of forbidden colours and application of efficient bitwise operations on bit representations of the adjacency and forbidden colours matrices. Computational experiments show that in comparison with the classical greedy heuristic the average speedup of the developed approach is 2.6 times on DIMACS instances.	bitwise operation;color;computation;experiment;graph coloring;greedy algorithm;heuristic;speedup;whole earth 'lectronic link	Larisa Komosko;Mikhail Batsyn;Pablo San Segundo;Panos M. Pardalos	2016	J. Comb. Optim.	10.1007/s10878-015-9862-1	greedy randomized adaptive search procedure;mathematical optimization;combinatorics;greedy algorithm;heuristic;bitwise operation;computer science;mathematics;algorithm	Theory	17.547663240225493	23.533234647085376	83582
9d891f768e08f1685128fe39f5c3f41a2b30ccb4	a faster fixed parameter algorithm for two-layer crossing minimization	graph drawing;bipartite crossing number;parameterized algorithms;graph algorithms;two layer crossing minimization	We give an algorithm that decides whether the bipartite crossing number of a given graph is at most k. The running time of the algorithm is upper bounded by 2 + n, where n is the number of vertices of the input graph, which improves the previously known algorithm due to Kobayashi et al. (TCS 2014) that runs in 2 log +n time. This result is based on a combinatorial upper bound on the number of two-layer drawings of a connected bipartite graph with a bounded crossing number.	algorithm;crossing number (graph theory);time complexity	Yasuaki Kobayashi;Hisao Tamaki	2016	Inf. Process. Lett.	10.1016/j.ipl.2016.04.012	graph power;edge-transitive graph;folded cube graph;mathematical optimization;complete bipartite graph;factor-critical graph;combinatorics;discrete mathematics;independent set;graph bandwidth;bipartite graph;computer science;simplex graph;toroidal graph;foster graph;cubic graph;mathematics;voltage graph;blossom algorithm;graph drawing;crossing number;biregular graph;quartic graph;line graph;algorithm;coxeter graph;matching	Theory	24.403010730639284	25.462114291045907	83591
5c3d14e76c74178312ae17fd0b9927210adcb6f8	fixed-parameter tractable reductions to sat		Today’s SAT solvers have an enormous importance and impact in many practical settings. They are used as efficient back-end to solve many NP-complete problems. However, many computational problems are located at the second level of the Polynomial Hierarchy or even higher, and hence polynomial-time transformations to SAT are not possible, unless the hierarchy collapses. In certain cases one can break through these complexity barriers by fixed-parameter tractable (fpt) reductions which exploit structural aspects of problem instances in terms of problem parameters. Recent research established a general theoretical framework that supports the classification of parameterized problems on whether they admit such an fpt-reduction to SAT or not. We use this framework to analyze some problems that are related to Boolean satisfiability. We consider several natural parameterizations of these problems, and we identify for which of these an fpt-reduction to SAT is possible. The problems that we look at are related to minimizing an implicant of a DNF formula, minimizing a DNF formula, and satisfiability of quantified Boolean formulas. 1 Institute of Information Systems, Vienna University of Technology, Vienna, Austria ∗ Supported by the European Research Council (ERC), project 239962 (COMPLEX REASON), and the Austrian Science Fund (FWF), project P26200 (Parameterized Compilation). Publication Information: This is the authors’ self-archived copy of a paper that appeared in the Proceedings of the 17th International Symposium on the Theory and Applications of Satisfiability Testing (SAT 2014), dx.doi.org/10.1007/978-3-319-09284-3 8. The final publication is available at www.springer.com. 2 INFSYS RR 1843-14-04	archive;boolean satisfiability problem;cobham's thesis;computation;computational problem;karp's 21 np-complete problems;parameterized complexity;polynomial hierarchy;rapid refresh;time complexity	Ronald de Haan;Stefan Szeider	2014		10.1007/978-3-319-09284-3_8	computational problem;discrete mathematics;combinatorics;implicant;true quantified boolean formula;disjunctive normal form;polynomial hierarchy;computer science;satisfiability;parameterized complexity;boolean satisfiability problem	Theory	19.62114333369074	19.632402336664374	83614
4e8ee3dd74e688d8ac7584fba418b9da2e2d1c62	the protein structure prediction problem: a constraint optimization approach using a new lower bound	optimisation sous contrainte;arbre graphe;search problem;constrained optimization;prediccion;biology computing;modelo reticular;proteine;constraint optimization;tree graph;redundancia;probleme np complet;efficient algorithm;base connaissance;search strategy;problema investigacion;optimizacion con restriccion;search trees;hp model;redundancy;lattice models;protein structure prediction;structure prediction;strategie recherche;borne inferieure;modele reticulaire;base conocimiento;protein folding;problema np completo;proteina;arbol grafo;computational biology;protein;probleme recherche;biologie informatique;lattice model;prediction;lower bound;redondance;np complete problem;cota inferior;estrategia investigacion;knowledge base	The protein structure prediction problem is one of the most (if not the most) important problem in computational biology. This problem consists of finding the conformation of a protein with minimal energy. Because of the complexity of this problem, simplified models like Dill's HP-lattice model [15], [16] have become a major tool for investigating general properties of protein folding. Even for this simplified model, the structure prediction problem has been shown to be NP-complete [5], [7]. We describe a constraint formulation of the HP-model structure prediction problem, and present the basic constraints and search strategy. Of course, the simple formulation would not lead to an efficient algorithm. We therefore describe redundant constraints to prune the search tree. Furthermore, we need bounding function for the energy of an HP-protein. We introduce a new lower bound based on partial knowledge about the final conformation (namely the distribution of H-monomers to layers).	algorithm;computation;computational biology;constrained optimization;lattice model (physics);np-completeness;protein structure prediction;search tree	Rolf Backofen	2001	Constraints	10.1023/A:1011485622743	computational problem;protein folding;mathematical optimization;constrained optimization;combinatorics;counting problem;lattice model;np-complete;prediction;search problem;computer science;artificial intelligence;cutting stock problem;protein structure prediction;constraint satisfaction dual problem;mathematics;redundancy;upper and lower bounds;tree;algorithm	Comp.	16.051704903574247	23.23122718912366	83636
390ee28f3c6c198fd790db535ce7c9dbcc74693b	probabilistic analysis of disjoint set union algorithms	random graph;algoritmo busqueda;algorithm analysis;probabilistic method;quick find;algorithme recherche;grafo aleatorio;weighting;search algorithm;aproximacion probabilista;graphe aleatoire;probabilistic approach;ponderacion;mathematical expectation;reunion ensembles disjoints;quick find algorithms;analysis of algorithms;random graphs;particion;approche probabiliste;equivalence algorithms;partition;disjoint set union;analyse algorithme;ponderation;esperanza matematica;expected time bounds;union find problem;analisis algoritmo;esperance mathematique	Abstract. A number of open questions are settled about the expected costs of two disjoint set Union and Find algorithms raised by Knuth and Schrnhage [Theoret. Comput. Sci., 6 (1978), pp. 281-315]. This paper shows that the expected time of the Weighted Quick-Find (QFW) algorithm to perform (n 1) randomly chosen unions is cn + o(n/ log n), where c 2.0847 Through an observation of Tarjan and Van Leeuwen in [J. Assoc. Comput. Mach., 22 (1975), pp. 215-225] this implies linear time bounds to perform O(n) unions and finds for a class of other union-find algorithms. It is also proved that the expected time of the Unweighted Quick-Find (QF) algorithm is n2/8 + O(n(logn)2). The expected costs of QFW and QF are analyzed when fewer than (n 1) unions are performed. Among other results, for QFW it is shown that the expected cost of m o(n) randomly chosen unions is m(1 + o(1)). If m tn/2, where ot _< e-E, this cost is m(1 + (ot) + o(1)), where (ct) -0 as t -0 and ;(e-2) < .026. For QF, the expected cost of n/2 n2/3(logn)2/3 randomly chosen unions is O(n log n).	algorithm;average-case complexity;carrier-to-noise ratio;disjoint-set data structure;mach;probabilistic analysis of algorithms;randomness;time complexity	Béla Bollobás;István Simon	1993	SIAM J. Comput.	10.1137/0222064	random graph;mathematical optimization;combinatorics;mathematics;algorithm	Theory	14.711561892660995	22.978852045041016	83645
90561205f4bc406a4c37bd878f25d9325d10726c	sparse dynamic programming for evolutionary-tree comparison	arbre graphe;dynamic programming;graph theory;arbre phylogenetique;evolucion biologica;evolution biologique;tree graph;05c05;bipartite matching;dynamic program;theorie graphe;arbol filogenetico;92b05;algorithme;biological evolution;evolutionary trees;phylogenetic tree;68c25;linear time;05c85;programmation dynamique;rooted tree;algorithms;arbol grafo;computational biology;05c90;sparse dynamic programming	Constructing evolutionary trees for species sets is a fundamental problem in biology. Unfortunately, there is no single agreed upon method for this task, and many methods are in use. Current practice dictates that trees be constructed using different methods and that the resulting trees should be compared for consensus. It has become necessary to automate this process as the number of species under consideration has grown. We study one formalization of the problem: the maximum agreement-subtree $($\MAST$)$ problem.#R##N#The $\MAST$ problem is as follows: given a set $A$ and two rooted trees $\cT_0$ and $\cT_1$ leaf-labeled by the elements of $A$, find a maximum-cardinality subset $B$ of $A$ such that the topological restrictions of $\cT_0$ and $\cT_1$ to $B$ are isomorphic. In this paper, we will show that this problem reduces to unary weighted bipartite matching ($\UWBM$) with an $O(n^{1+o(1)})$ additive overhead. We also show that $\UWBM$ reduces linearly to $\MAST$. Thus our algorithm is optimal unless $\UWBM$ can be solved in near linear time. The overall running time of our algorithm is $O(n^{1.5} \log n)$, improving on the previous best algorithm, which runs in $O(n^2)$. We also derive an $O(n c^{\sqrt{\log n}})$-time algorithm for the case of bounded degrees, whereas the previously best algorithm runs in $O(n^2),$ as in the unbounded case.	dynamic programming;phylogenetic tree;sparse	Martin Farach-Colton;Mikkel Thorup	1997	SIAM J. Comput.	10.1137/S0097539794262422	mathematical optimization;combinatorics;discrete mathematics;phylogenetic tree;graph theory;mathematics;algorithm;algebra	Theory	21.356219851532675	25.102506917139646	83694
9afa9610611168ccd645fbca010f5a1ba90da07d	parallel sorting machines: their speed and efficiency	parallel sorting;time complexity;efficiency measurement	A unified approach for analyzing and classifying parallel sorting machines is discussed. The approach accounts for a number of important factors that have significant impacts on the measure of the size and complexities of parallel sorters. These factors include the sorter architectures, sequential/parallel input, single/multiple passes, and the base of the comparitors. An efficiency measure is also introduced to describe the optimality and other general characteristics of the sorters. Finally, a new sorter with the time complexity of O(c*logbN), where b is an arbitrary base, is proposed. Its efficiency is independent of the number of processors, the size of the list, and the maximum value in the list.	algorithm;bitonic sorter;central processing unit;sorting;time complexity	Leon E. Winslow;Yuan-Chieh Chow	1981		10.1145/1500412.1500434	computer science;theoretical computer science;distributed computing;algorithm;cost efficiency	Theory	12.335868370411893	32.14522234170825	83733
233db86fdac365c38ab9f117e539b608b65f1e2e	time bounds for streaming problems		We give tight cell-probe bounds for the time to compute convolution, multiplication and Hamming distance in a stream. The cell probe model is a particularly strong computational model and subsumes, for example, the popular word RAM model. • We first consider online convolution where the task is to output the inner product between a fixed n-dimensional vector and a vector of the n most recent values from a stream. One symbol of the stream arrives at a time and the each output must be computed before the next symbols arrives. • Next we show bounds for online multiplication where the stream consists of pairs of digits, one from each of two n digit numbers that are to be multiplied. One pair arrives at a time and the task is to output a single new digit from the product before the next pair of digits arrives. • Finally we look at the online Hamming distance problem where the Hamming distance is outputted instead of the inner product. For each of these three problems, we give a lower bound of Ω ( δ w log n ) time on average per output, where δ is the number of bits needed to represent an input symbol and w is the cell or word size. We argue that these bound are in fact tight within the cell probe model.	alphabet (formal languages);computation;computational model;convolution;hamming distance;random-access memory	Raphaël Clifford;Markus Jalsenius;Benjamin Sach	2014	CoRR		arithmetic;combinatorics;theoretical computer science;mathematics;algorithm;statistics	Theory	11.31859879414037	25.671181676207645	83768
02a217288f7ea7ca3f9db1e6d645dabeee78f673	on some tighter inapproximability results	algorithm complexity;complejidad algoritmo;satisfiabilite;randomised algorithms;problema np duro;algorithme randomise;satisfiability;optimisation combinatoire;np hard problem;complexite algorithme;sorting by reversals;probleme np difficile;informatique theorique;algorithme approximation;combinatorial optimization;optimizacion combinatoria;computer theory;informatica teorica	We give a number of improved inapproximability results, including the best up to date explicit approximation thresholds for bounded occurence satis ability problems like MAX2SAT and E2-LIN-2, and the bounded degree graph problems, like MIS, Node Cover, and MAX CUT. We prove also for the rst time inapproximability of the problem of Sorting by Reversals and display an explicit approximation threshold.	cut (graph theory);hardness of approximation;max;maximum cut;sorting	Piotr Berman;Marek Karpinski	1998	Electronic Colloquium on Computational Complexity (ECCC)		mathematical optimization;combinatorics;combinatorial optimization;computer science;np-hard;mathematics;algorithm;satisfiability	Theory	18.357274758866414	25.650682533910683	83810
eb4750c8aca556d3bc976da53200af48277b936c	biconnectivity on symbolically represented graphs: a linear solution	concepcion asistida;graphe lineaire;symbolic computation;computer aided design;diagrama binaria decision;diagramme binaire decision;spine;rachis;recherche profondeur d abord;juego de funciones;circuit vlsi;raquis;jeu role;vlsi design;grafo lineal;ordered binary decision diagram;calculo simbolico;first depth search;vlsi circuit;conception assistee;depth first search;graphe ordonne;circuito vlsi;role playing;calcul symbolique;ordered graph;linear graph;binary decision diagram	We define an algorithm for determining, in a linear number of symbolic steps, the biconnected components of a graph implicitly represented with Ordered Binary Decision Diagrams (OBDDs). Working on symbolically represented data has potential: the standards achieved in graph sizes (playing a crucial role, for example, in verification, VLSI design, and CAD) are definitely higher. On the other hand, symbolic algorithm’s design generates constraints as well. For example, Depth First Search is not feasible in the symbolic setting, and our algorithm relies on the use of spine-sets, introduced in [8] for strongly connected components, as its substitute. Our approach suggests a symbolic framework to tackle those problems which are naturally solved by a DFS-based algorithm in the standard case.	algorithm;biconnected component;biconnected graph;binary decision diagram;computer-aided design;data structure;depth-first search;graph theory;strongly connected component;very-large-scale integration	Raffaella Gentilini;Alberto Policriti	2003		10.1007/978-3-540-24587-2_57	combinatorics;discrete mathematics;symbolic computation;spine;breadth-first search;computer science;artificial intelligence;machine learning;mathematics;linear equation;symbolic data analysis;very-large-scale integration;binary decision diagram;algorithm;ordered graph	Logic	20.01318352229738	29.63929737509263	84000
3fb3e3d90af0a7a6c9f9aa951a35732580f5afce	non-adaptive group testing in the presence of errors	experimental design;biology computing;experimental method;optimisation;optimizacion;complexite calcul;fonction repartition;plan experiencia;biologia molecular;plan randomise;genetics;combinatorial problem;funcion distribucion;distribution function;complejidad computacion;plan aleatorizado;probleme combinatoire;problema combinatorio;estimation erreur;plan experience;error estimation;computational complexity;random design;randomized design;molecular biology;estimacion error;cost effectiveness;optimization;biologie informatique;physical map;group testing;biologie moleculaire	"""In group testing, the task is to determine the distinguished members of a set of objects O by asking subset queries of the form \does the set Q O contain a distinguished object?"""" In biological applications of group testing, the task is to repeatedly screen a library of objects for those which are positive for a probe. The subset queries consist of screening a pooled subset of the objects with the probe. This procedure has become an important component of the experimental methods used for the compilation of physical maps of chromosomes and other genetic material. For many screening applications, it is most cost-eeective to ask many subset queries in parallel. This leads to non-adaptive group testing problems. An important aspect of most screening environments is that the screening results are far from reliable. In this report we discuss some of the error models that can be used and show how they aaect the design of non-adaptive screening experiments. We give a uniied treatment of the known methods for constructing pooling designs, provide explicit formulas for their performance under diierent error assumptions and discuss the asymptotic performance of random designs."""	compiler;experiment;map;randomized algorithm	Emanuel Knill;William J. Bruno;David C. Torney	1998	Discrete Applied Mathematics	10.1016/S0166-218X(98)00075-4	combinatorics;group testing;simulation;cost-effectiveness analysis;artificial intelligence;distribution function;mathematics;completely randomized design;computational complexity theory;design of experiments;algorithm	Theory	14.701678225475385	23.525023947817466	84006
3ef3a1e8d93df24a3ab928157759956f61158d3a	a column generation based algorithm for the robust graph coloring problem	set covering formulation;representatives formulation;robust graph coloring;reduced cost fixing;column generation	2 B. Yüceoğlu et al. / Discrete Applied Mathematics ( ) – the same operator if probability of a delay is high or a possible delay is costly. Similarly, in a course timetabling problem, it may not always be possible to create a timetable without any clashes. Furthermore, it is difficult to predict the choices of students beforehand. Therefore, it is desirable to create a timetable where number of clashes are minimized. In conference timetabling, two sessions that can be assigned to the same time slot can still attract the same set of people. Even though eliminating every possible clash may be difficult, it is still possible to minimize clashes. The robust graph coloring problem is a generalization of the original graph coloring problem. As in the original problem, adjacent vertices are not allowed to take the same color while having the same color may only be undesirable and penalized for other vertices in the robust version. The major difference is that the objective function is not to minimize the number of colors but tominimize the sum of penalties due to pairs of vertices with the same color. The problem is introduced by Yáñez and Ramírez [21]. They show that the original graph coloring problem can be too restrictive if one also considers secondary objectives. They present an assignment type of integer linear programming (ILP) formulation and use a genetic algorithm to solve the problem. Lim and Wang [14] use the robust graph coloring problem to model the robust aircraft assignment problem; they employ heuristics to solve the problem. Guo et al. [9] and Kong et al. [12] also present heuristics. Wang and Xu [20] model the problem as an unconstrained quadratic programming problem and develop new heuristics. To the best of our knowledge, the only exact approach for the robust graph coloring problem is the column generation approach in [2]. Archetti et al. [2] use the ILP formulation of Yáñez and Ramírez [21]. After establishing that the formulation is only suitable for small instances, they present a branch-and-price algorithm. They use both heuristics and exact methods to generate new columns. We present a new ILP formulation for the robust graph coloring problem based on the asymmetric representatives formulation of Campêlo et al. [6], which is originally used for the graph coloring problem. The formulation, introduced by Campêlo et al. [7], uses the idea that vertices with the same color can represent each other. Furthermore, Campêlo et al. [6] use an ordering of the vertices to create the asymmetrical representative formulation. This enhanced formulation reduces the number of variables and different representation of color classes compared to the original representatives formulation. Furthermore, the asymmetric representatives formulation eliminates symmetries that result from the interchangeability of colors compared to the original formulation of Méndez-Díaz and Zabala [18]. We compare asymmetric representatives formulation with the original formulation in [21]; we show that it performs better than the original as it yields a much improved lower bound for the problem but is still heavily restricted by the size of the instances solved to optimality. Even though the asymmetric representatives formulation performs better than the original formulation, it does not have a noticeable impact on the size of the instances solved to optimality. For this reason, we use the set-covering formulation in [2] and develop a column generation-based algorithm to solve it. Unlike Archetti et al. [2], we do not use branch-andprice; we employ a method in [19]. Even though this method enumerates all columns in the worst case, it performs well empirically. In Section 2, we present our notation and the new ILP formulation based on the asymmetric representatives formulation. In Section 3,we develop a set-covering based formulation and our columngeneration based solutionmethod. Computational experiments are presented in these sections. We discuss the results and conclude in Section 4. 2. Asymmetric representatives formulation Given a simple, undirected, and connected graph G = (V , E), where n = |V | is the number of vertices andm = |E| is the number of edges, two vertices i and j are adjacent if {i, j} ∈ E. N(i) = {j ∈ V | {i, j} ∈ E} is called the neighborhood of i. An ordering of G is a mapping σ : V → {1, . . . , n}, where σ(i) denotes the position of i in the ordering; we use an ordering of the vertices to eliminate the symmetries in the problem. We identify each vertex with its position in the ordering, i.e., the vertices are numbered 1, 2, . . . , n. For a given ordering of G, we call N(i) = {1, 2, . . . , i − 1} ∩ N(i) the in-neighborhood of i and N(i) = {i + 1, i + 2, . . . , n} ∩ N(i) the out-neighborhood of i. Ḡ = (V , Ē) denotes the complement of G, where Ē consists of {i, j} ∉ E; N̄(i) = {j ∈ V | {i, j} ∈ Ē} \ {i} is called the antineighborhood of i. The inand out-antineighborhoods of i in Ḡ are defined similarly as N̄(i) and N̄(i). The closed (anti)neighborhoods, where i is included, are denoted by N[i],N[i],N[i], N̄[i], N̄[i], N̄[i]. N̄[i] corresponds to the vertices that can be represented by i (including itself). N̄[i] corresponds to the vertices that can represent i (including itself). We call H = (VH , EH) an induced subgraph of G if VH ⊆ V and {i, j} ∈ EH if and only if i ∈ VH , j ∈ VH and {i, j} ∈ E. H is called a clique if all vertices inH are pairwise adjacent. Each vertex of a clique has to have a different color. An independent set is a set of vertices, in which no two vertices are adjacent. In other words, H is an independent set if EH = ∅. In any coloring, vertices having the same color form an independent set. 2.1. Mathematical model We modify the asymmetric representatives formulation introduced by Campêlo et al. [6] which selects a subset of the vertices to represent the colors. Representative vertices can represent other vertices in their out-antineighborhood. The vertices represented by a representative vertex must form an independent set. Vertices that do not represent a color are identified by the color of their representatives, i.e., a vertex in the in-antineighborhood of i. cij denotes a non-negative cost associated with two vertices i and j such that {i, j} ∉ E, it can be considered as the penalty of coloring two vertices with the same color. B. Yüceoğlu et al. / Discrete Applied Mathematics ( ) – 3 We define two types of decision variables: • Representation variables are used in order to determine colors of the vertices. For each vertex i, we define representation variables only for the vertices j ∈ N̄[i]; for i ≤ j and {i, j} ∉ E, xij =  1, if vertex i represents vertex j, 0, otherwise. If xii = 1, the vertex i is called a representative vertex; otherwise, i is represented by a representative vertex. • Color class variables are used in order to determinewhether two vertices have the same color. For each vertex i, we define color class variables only for the vertices j ∈ N̄(i); for i < j and {i, j} ∉ E, yij =  1, if vertex i and j have the same color, 0, otherwise. The asymmetric representatives formulation of the robust graph coloring problem is min  i∈V  j∈N̄+(i) cijyij, (1) subject to  i∈V xii = k, (2)  i∈N̄−[j] xij = 1, ∀j ∈ V , (3)  j∈C xij ≤ xii, ∀i ∈ V , ∀C ⊆ N̄(i), C maximal clique, (4) xij + xik ≤ 1 + yjk, ∀i ∈ V , ∀j, k ∈ N̄[i], j < k, {j, k} ∉ E (5) xij ∈ {0, 1}, ∀i ∈ V , ∀j ∈ N̄[i], (6) yij ∈ {0, 1}, ∀i ∈ V , ∀j ∈ N̄(i). (7) The objective function (1) minimizes total weight of coloring the vertices. Constraint (2) limits the number of colors used (or the number of representative vertices) to k. Constraints (3) ensure that each vertex j is represented only once, either by itself or by a representative vertex that can represent j. Constraints (4) guarantee that a representative vertex i represents at most one vertex of any maximal clique in its (open) out-antineighborhood. In our formulation, we check pairs and triplets of vertices to generate the inequalities since it is not possible to generate all maximal cliques in a graph in polynomial time, Bron and Kerbosch [4]. Note that a maximal clique can be a singleton vertex or two vertices that are connected to each other. Constraints (5) assert that for two distinct vertices j and k with the same representative, the variable yjk is selected. Constraints (6)–(7) are the domain constraints for the variables. 2.2. Computational results We conduct computational experiments in order to compare the asymmetric representatives formulation with the original formulation in [21]. We use two sets of instances that are also used by Archetti et al. [2]. The first set of instances (see Table 1) are taken from the DIMACS graph coloring library by setting cij = ij, for distinct vertices i and j that are not adjacent. The number of colors is set to  3 2 k̄  and  2k̄  , where k̄ is the best known upper bound on the chromatic number. We consider instances with at most 150 vertices. The problem instance miles1500 with k =  2k̄  = 156 cannot be solved as the number of colors exceeds the number of vertices; corresponding cells are shaded in gray Table 1. The second set of instances are generated by Archetti et al. [2] for the robust graph coloring problem (see Tables 2 and 3). The name of the instances contains information on the number of vertices n, the number of colors k, and an index for the instance (with five instances for each values of n and k). The number of colors is set to  n 3  and  n 3  + 1. The cost coefficients are generated according to a uniform distribution in the interval (0, 1) and the graphs have a density of 0.5. rcp_10_4_2 and rcp_10_4_4 are infeasible since they contain a clique of size 5. The experiments are carried out on an Intel Core i7, 3.2 GHz Windows desktop using CPLEX 12.6 to solve the problem with default settings [10]. The processing time is limited to one hour. In order to speed up the solver, we find a m	assignment problem;best, worst and average case;branch and price;branch predictor;cplex;clique (graph theory);coefficient;color;column (database);column generation;complement (complexity);computation;connectivity (graph theory);decision theory;desktop computer;expanded memory;experiment;exponential hierarchy;genetic algorithm;graph (discrete mathematics);graph coloring;heuristic (computer science);independent set (graph theory);index (publishing);induced subgraph;integer programming;linear programming;mathematical model;maxima and minima;maximal set;microsoft windows;neighbourhood (graph theory);optimization problem;polynomial;quadratic programming;schedule;set cover problem;shading;solver;uncertain data;vertex (graph theory)	Birol Yüceoglu;Güvenç Sahin;Stan P. M. van Hoesel	2017	Discrete Applied Mathematics	10.1016/j.dam.2016.09.006	column generation;graph power;mathematical optimization;combinatorics;discrete mathematics;independent set;graph bandwidth;fractional coloring;level structure;graph center;graph toughness;complete coloring;edge coloring;graph coloring;path graph;graph factorization;mathematics;k-vertex-connected graph;graph homomorphism;list coloring;bound graph;complement graph;greedy coloring	Theory	23.794565068811707	19.409261796406486	84062
7f7e390904d21ef1089b8ddf81dabb1c125629ec	construction of natural cycletrees	hamiltonian cycle;hamiltonian cycles;minimal spanning tree;top down;cycle hamiltonien;binary trees;algorithme;ciclo hamiltoniano;arbol binario;cycle graphe;arbre binaire;algorithms;cycle graph;spanning tree;arbre maximal minimal;ciclo diagrama;binary tree	In this article, the following question is answered: Given a cycle C, of size N, how can one compute, in O(N) time, a minimal set of edges E such that adjoining E to the cycle yields a graph with a binary spanning tree having minimal total path length? The answer is given through an algorithm for top-down construction of natural cycletrees, where the structure of each subtree is restricted by a split relation between certain properties of the subtree.		Margus Veanes;Jonas Barklund	1996	Inf. Process. Lett.	10.1016/S0020-0190(96)00179-2	combinatorics;discrete mathematics;topology;binary tree;computer science;mathematics	DB	24.539131736581343	30.136801737755416	84154
e72bcf390d505990036b92cf1e055cbffecd8988	bounded degree graph inference from walks	complexite;algorithm complexity;time complexity;probleme np complet;complejidad algoritmo;clase complejidad;complejidad;complexity;degree two bounded graphs;complexite temps;classe complexite;complexite algorithme;complexity class;theory;degree bound;teoria;inferencia;problema np completo;complejidad tiempo;inference;np complete problem;theorie;edge colored graph	Aslam and Rivest considered the problem of inferring the smallest edge-colored graph of degree bound k consistent with the sequence of colors seen in a walk of the graph. Using Church-Rosser properties of certain sets of rewrite rules, they gave a polynomial time algorithm for the case of k = 2. The straightforward implementation of their ideas results in an O(n^5) algorithm, where n is the length of the walk. In this paper, we develop their ideas further and give an O(n log n) algorithm for the same problem. We also show that if the degree bound k is greater than two, then the decision version of the problem is NP-complete, thus settling a conjecture of Aslam and Rivest.		Vijay Raghavan	1991	J. Comput. Syst. Sci.	10.1016/S0022-0000(05)80089-3	time complexity;complexity class;combinatorics;discrete mathematics;complexity;np-complete;degree;computer science;mathematics;theory;algorithm	Theory	17.656469754396173	23.103917024471567	84217
2785a5a24a2e4aa54e22bea166f4007f67eb839f	sorting stably, in-place, with o(n log n) comparisons and o(n) moves	sorting algorithm;lower bound	We settle a long-standing open question, namely whether it is possible to sort a sequence of n elements stably (i.e. preserving the original relative order of the equal elements), using O(1) auxiliary space and performing O(n log n) comparisons and O(n) data moves. Munro and Raman stated this problem in [J. Algorithms, 13, 1992] and gave an in-place but unstable sorting algorithm that performs O(n) data moves and O(n1+e) comparisons. Subsequently [Algorithmica, 16, 1996] they presented a stable algorithm with these same bounds. Recently, Franceschini and Geffert [FOCS 2003] presented an unstable sorting algorithm that matches the asymptotic lower bounds on all computational resources.	sorting	Gianni Franceschini	2005		10.1007/978-3-540-31856-9_52	arithmetic;combinatorics;computer science;sorting algorithm;mathematics;upper and lower bounds;algorithm	Theory	12.82638420020278	23.610737477610023	84319
24180ac0c2ed2d53fd3049a9009d76d867b767a9	recursive solution of a class of combinatorial problems: an example	combinatorial problems	6. Conel uMons Presented here was the sohitiou of two combimd;orial problems by vnA)eddilig the ,~ive, problem Jft a la.rger class of problems at]d Lher, solving the set; of problems by r e e u l ' s i o n, [{E<F:,[ V S:D J u >4 ~,;, 9t;4; t~E v [ s F,D J ! /N ~',, I !)65, 620 ( : o m , m ~ n l c a t h m s o f t h e A C S ! V o h m l e 8 / Ni t ro |pe r 1.0 / Oelo|J,~r, L965	emoticon;recursion (computer science)	W. C. Lynch	1965	Commun. ACM	10.1145/365628.365653	optimization problem;barycentric-sum problem;covering problems;computer science;combinatorial principles;combinatorial explosion;combinatorial search	AI	16.32382630520868	19.480913199006316	84484
4a401f1da43c1a447e6afd7d5ab128cc5f047b23	analysis of 3-line generalized feistel networks with double sd-functions	block design;impossible differential;procesamiento informacion;algorithm analysis;substitution;efficiency;block cipher;cosic;funcion corriente;difusion;fonction hachage;plan bloc;primitivo;plan bloque;linear cryptanalysis;11t71;eficacia;stream cipher;matrices;fonction courant;criptografia;informatique theorique;cryptography;substitution diffusion networks;information processing;borne inferieure;efficacite;cryptographie;block ciphers;primitif;analyse algorithme;hash function;generalized feistel networks;primitive;traitement information;matrice;diffusion;differential and linear cryptanalysis;analisis algoritmo;lower bound;stream function;substitucion;cota inferior;computer theory;informatica teorica	Generalized Feistel networks (GFN) are broadly employed in the design of primitives for block ciphers, stream ciphers, and hash functions. Lately, endowing the functions of GFNs with the structure of nonlinear substitution followed by linear diffusion (substitution-diffusion, SD) has received a great deal of attention. In this contribution, we prove tight lower bounds on the number of differentially and linearly active S-boxes for 3-line GFNs with double SD-functions where two SD-structures are applied one after another. We also show 8-round impossible differentials for 3-line GFNs with bijective functions. Moreover, we demonstrate that the proportion of active S-boxes in all S-boxes for such GFNs is by up to 14% higher than that for 4-line GFNs with double SD-functions, when instantiated with MDS matrices. This indicates that, rather surprisingly, the 3-line GFNs can be more efficient in practice than those with 4 lines.	feistel cipher	Andrey Bogdanov;Kyoji Shibutani	2011	Inf. Process. Lett.	10.1016/j.ipl.2011.04.002	arithmetic;block cipher;combinatorics;information processing;computer science;mathematics;diffusion;algorithm	Vision	14.753426582237754	31.654012916938996	84488
963546f7a6eeb1cc6c117bd494b3db1c38d5a06a	parameterized and approximation complexity of the detection pair problem in graphs		We study the complexity of the problem Detection Pair. A detection pair of a graph G is a pair (W,L) of sets of detectors with W ⊆ V (G), the watchers, and L ⊆ V (G), the listeners, such that for every pair u, v of vertices that are not dominated by a watcher of W , there is a listener of L whose distances to u and to v are different. The goal is to minimize |W |+ |L|. This problem generalizes the two classic problems Dominating Set and Metric Dimension, that correspond to the restrictions L = ∅ and W = ∅, respectively. Detection Pair was recently introduced by Finbow, Hartnell and Young [A. S. Finbow, B. L. Hartnell and J. R. Young. The complexity of monitoring a network with both watchers and listeners. Networks, accepted], who proved it to be NP-complete on trees, a surprising result given that both Dominating Set and Metric Dimension are known to be linear-time solvable on trees. It follows from an existing reduction by Hartung and Nichterlein for Metric Dimension that even on bipartite subcubic graphs of arbitrarily large girth, Detection Pair is NP-hard to approximate within a sub-logarithmic factor and W[2]-hard (when parameterized by solution size). We show, using a reduction to Set Cover, that Detection Pair is approximable within a factor logarithmic in the number of vertices of the input graph. Our two main results are a linear-time 2-approximation algorithm and an FPT algorithm for Detection Pair on trees.	approximation algorithm;decision problem;dominating set;exptime;girth (graph theory);hardness of approximation;metric dimension (graph theory);np-completeness;outerplanar graph;ptas reduction;parameterized complexity;planar graph;polynomial;sensor;series-parallel graph;time complexity;treewidth;vc dimension;vertex (geometry)	Florent Foucaud;Ralf Klasing	2017	J. Graph Algorithms Appl.	10.7155/jgaa.00449	mathematical optimization;combinatorics;discrete mathematics;mathematics;algorithm;statistics	Theory	24.172875649317877	23.391079928801624	84777
9e33005ec1bcffff2a0ed5660438a27f6734f0fe	a hybrid spatial model for representing indoor environments	modelizacion;graph theory;distributed system;hybrid spatial representation;indoor navigation;topology;hierarchical structure;teoria grafo;systeme reparti;systeme information geographique;geographic information system;routing;modelo hibrido;routage;theorie graphe;modele hybride;hybrid model;qualitative spatial representation;modelisation;sistema repartido;indoor environment;graph;edge graph;spatial representation;hierarchical graph;arete graphe;indoor wayfinding;indoor installation;instalacion interior;modeling;installation interieure;sistema informacion geografica;spatial model;arista grafico;enrutamiento	In this article we propose a hybrid spatial model for indoor environments. The model consists of hierarchically structured graphs with typed edges and nodes. The model is hybrid in the sense that nodes and edges can be labelled with qualitative as well as quantitative information. The graphs support wayfinding and, in addition, provide helpful information for generating human-oriented descriptions of an indoor (and outdoor) path.	algorithm;angularjs;markup language;physical symbol system;shortest path problem;switzerland	Bernhard Lorenz;Hans Jürgen Ohlbach;Edgar-Philipp Stoffel	2006		10.1007/11935148_10	routing;simulation;systems modeling;computer science;graph theory;geographic information system;graph;cartography	AI	21.326402871585216	30.563773823347585	84904
fd6cdddf1e45a5e3de0378d001c0ee064ee7ced8	quantum query complexity of boolean functions with small on-sets	boolean function;upper bound;query complexity;lower bound	The main objective of this paper is to show that the quantum query complexity Q(f) of an N -bit Boolean function f is bounded by a function of a simple and natural parameter, i.e., M = |{x | f(x) = 1}| or the size of f ’s on-set. We prove that: (i) For poly(N) ≤ M ≤ 2Nd for some constant 0 < d < 1, the upper bound of Q(f) is O( √ N log M/ log N). This bound is tight, namely there is a Boolean function f such that Q(f) = Ω( √ N log M/ log N). (ii) For the same range of M , the (also tight) lower bound of Q(f) is Ω( √ N). (iii) The average value of Q(f) is bounded from above and below by Q(f) = O(log M + √ N) and Q(f) = Ω(log M/ log N + √ N), respectively. The first bound gives a simple way of bounding the quantum query complexity of testing some graph properties. In particular, it is proved that the quantum query complexity of planarity testing for a graph with n vertices is Θ(N) where N = n(n−1) 2 .	decision tree model;graph property;planarity testing	Andris Ambainis;Kazuo Iwama;Masaki Nakanishi;Harumichi Nishimura;Raymond H. Putra;Seiichiro Tani;Shigeru Yamashita	2008		10.1007/978-3-540-92182-0_79	circuit complexity;and-inverter graph;combinatorics;circuit minimization for boolean functions;discrete mathematics;boolean conjunctive query;boolean expression;standard boolean model;maximum satisfiability problem;theoretical computer science;karp–lipton theorem;complexity index;mathematics;boolean function;upper and lower bounds;algorithm;parity function	Theory	10.277254140287798	22.690420620651096	84905
2d5105ff06219be2ab6f8e8e0f58c4aa807c5ccf	i/o-complexity of graph algorithms	approximation;graph connectivity;graph algorithm;connected component;lower bound;bounding box	We show lower bounds of Q( $ Sort(V)) for the I/Ocomplexity of graph theoretic problems like connected components, biconnected components, and minimum spanning trees, where E and V are the number of edges and vertices in the input graph, respectively. We also present a deterministic O($ Sort(V) ’ max(l,loglog 9)) algorithm for the problem of graph connectivity, where B and D denote respectively the block size and number of disks. Our algorithm includes a breadth first search; this maybe of independent	algorithm;biconnected component;biconnected graph;block size (cryptography);breadth-first search;connected component (graph theory);connectivity (graph theory);file spanning;input/output;linear temporal logic to büchi automaton;list of algorithms;minimum spanning tree;theory	Kamesh Munagala;Abhiram G. Ranade	1999			graph power;mathematical optimization;factor-critical graph;combinatorics;discrete mathematics;connected component;directed graph;graph bandwidth;null graph;minimum bounding box;regular graph;distance-regular graph;connectivity;simplex graph;approximation;cubic graph;mathematics;voltage graph;distance-hereditary graph;biconnected graph;windmill graph;upper and lower bounds;butterfly graph;quartic graph;complement graph;line graph;string graph;strength of a graph;coxeter graph	Theory	24.350307556674146	28.116221260601424	85353
66e52749c56f128080c4e4f60ba4f2413650ecce	are there any nicely structured preference~profiles~nearby?		We investigate the problem of deciding whether a given preference profile is close to having a certain nice structure, as for instance single-peaked, single-caved, singlecrossing, value-restricted, best-restricted, worst-restricted, medium-restricted, or groupseparable profiles. We measure this distance by the number of voters or alternatives that have to be deleted to make the profile a nicely structured one. Our results classify the problem variants with respect to their computational complexity, and draw a clear line between computationally tractable (polynomial-time solvable) and computationally intractable (NP-hard) questions.	algorithm;cobham's thesis;computation;computational complexity theory;decision problem;entity–relationship model;forbidden graph characterization;gerhard j. woeginger;li-chen wang;np-hardness;parameterized complexity;polynomial;quantum gate;time complexity;yang	Robert Bredereck;Jiehua Chen;Gerhard J. Woeginger	2013		10.1016/j.mathsocsci.2015.11.002	mathematical optimization;discrete mathematics;mathematics	AI	22.826502718704297	23.13114897977292	85411
7ee93042583ba06bbb354add5d44d877aee33c95	a 17/12-approximation algorithm for 2-vertex-connected spanning subgraphs on graphs with minimum degree at least 3		We obtain a polynomial-time 17 12 -approximation algorithm for the minimum-cost 2-vertexconnected spanning subgraph problem, restricted to graphs of minimum degree at least 3. Our algorithm uses the framework of ear-decompositions for approximating connectivity problems, which was previously used in algorithms for finding the smallest 2-edge-connected spanning subgraph by Cheriyan, Sebo and Szigeti (SIAM J.Discrete Math. 2001) who gave a 17 12 -approximation algorithm for this problem, and by Sebo and Vygen (Combinatorica 2014), who improved the approximation ratio to 4 3 .	approximation algorithm;combinatorica;file spanning;time complexity	Vishnu V. Narayan	2016	CoRR		combinatorics;discrete mathematics;minimum spanning tree;vertex (geometry);approximation algorithm;mathematics;minimum degree spanning tree;reverse-delete algorithm;graph	Theory	22.905354010440693	20.69996026908457	85419
4366ff0903389fd9d7164db38978376a374ab04e	circumventing d-to-1 for approximation resistance of satisfiable predicates strictly containing parity of width at least four	computer science	H̊astad established that any predicate P ⊆ {0, 1} containing parity of width at least three is approximation resistant for almost satisfiable instances. However, in comparison to for example the approximation hardness of Max-3SAT, the result only holds for almost satisfiable instances. This limitation was addressed by O’Donnell, Wu, and Huang who showed the threshold result that if a predicate strictly contains parity of width at least three, then it is approximation resistant also for satisfiable instances, assuming the d-to-1 Conjecture. We extend modern hardnessof-approximation techniques by Mossel et al. to projection games, eliminating dependencies on the degree of projections via Smooth Label Cover, and prove, subject only to P 6 = NP, the same approximation-resistance result for predicates of width four or greater. ∗Supported by ERC Advanced Investigator Grant 226203.	approximation;max-3sat	Cenny Wenner	2012	Electronic Colloquium on Computational Complexity (ECCC)	10.4086/toc.2013.v009a023	discrete mathematics;boolean function;mathematics;combinatorial optimization;parity (mathematics);hardness of approximation;constraint satisfaction problem;constraint satisfaction;betweenness centrality;gaussian elimination	Theory	11.593877319074135	20.071970994098532	85421
e074335599013b7fbc2bc583cafe6e2f54e2a524	finding the maximum common subgraph of a partial k-tree and a graph with a polynomially bounded number of spanning trees	arbre graphe;procesamiento informacion;maximum common subgraph;subgrafo;temps polynomial;algorithm analysis;tree graph;complexite calcul;05c05;arbre maximal;biologia molecular;problema np duro;tree width;connected graph;polynomial time algorithm;np hard problem;complejidad computacion;arbol maximo;probleme np difficile;sous graphe;computational complexity;informatique theorique;molecular biology;68r10;information processing;polynomial time;graph algorithm;algorithme polynomial;analyse algorithme;spanning tree;subgraph;arbol grafo;traitement information;largeur arbre;graph algorithms;graphe connexe;analisis algoritmo;computer theory;biologie moleculaire;tiempo polinomial;grafo conexo;informatica teorica	The maximum common subgraph problem is known to be NP-hard, although it has often been applied to various areas. In the field of molecular biology, we can reduce the problem space by analyzing the structures of chemical compounds. In doing so, we have found that the tree-width of chemical compounds are bounded by a constant, and that the possible spanning trees of any compound is polynomially bounded. We present a polynomial time algorithm for finding the maximum common connected induced subgraph of a degree-bounded partial k-tree and a connected graph, the number of whose possible spanning trees is polynomial.	file spanning;partial k-tree	Atsuko Yamaguchi;Kiyoko F. Aoki-Kinoshita;Hiroshi Mamitsuka	2004	Inf. Process. Lett.	10.1016/j.ipl.2004.06.019	time complexity;combinatorics;discrete mathematics;minimum degree spanning tree;information processing;spanning tree;computer science;connectivity;minimum spanning tree;np-hard;subgraph isomorphism problem;graph factorization;connected dominating set;k-minimum spanning tree;mathematics;induced subgraph isomorphism problem;maximum common subgraph isomorphism problem;treewidth;computational complexity theory;tree;algorithm	Theory	21.413899733155297	26.867626867188772	85464
3e91eca7b52c24629671ea27e00d4f5722c7cdb4	interactivegiotto: an algorithm for interactive orthogonal graph drawing	outil logiciel;software tool;graph drawing;ingenieria logiciel;software engineering;systeme conversationnel;construction graphe;flujo red;interactive system;herramienta controlada por logicial;sistema conversacional;genie logiciel;network flow;graph construction;flot reseau;construccion grafo	We present InteractiveGiotto, an interactive algorithm for orthogonal graph drawing based on the network flow approach to bend minimization.	algorithm;graph drawing	Stina S. Bridgeman;Jody Fanto;Ashim Garg;Roberto Tamassia;Luca Vismara	1997		10.1007/3-540-63938-1_73	spqr tree;lattice graph;combinatorics;flow network;graph bandwidth;computer science;force-directed graph drawing;mathematics;graph;moral graph;graph drawing;algorithm	HCI	22.025182574488188	29.972098230025917	85568
18d1ab43f38dbf01c598bef3b0280e80b0e8f3f0	an approximation algorithm for the maximum leaf spanning arborescence problem	directed graphs;arborescence;approximate algorithm;approximation algorithms;directed graph;maximum leaf	We present an O(&sqrt;opt)-approximation algorithm for the maximum leaf spanning arborescence problem, where opt is the number of leaves in an optimal spanning arborescence. The result is based upon an O(1)-approximation algorithm for a special class of directed graphs called willows. Incorporating the method for willow graphs as a subroutine in a local improvement algorithm gives the bound for general directed graphs.	approximation algorithm;directed graph;file spanning;subroutine;willow rosenberg	Matthew Drescher;Adrian Vetta	2010	ACM Trans. Algorithms	10.1145/1798596.1798599	edmonds' algorithm;mathematical optimization;combinatorics;discrete mathematics;directed graph;mathematics;arborescence;approximation algorithm	Theory	23.022533206346424	21.268756755097527	85739
b819f4bd9a85a9d65999605cbff272a2c3a03116	on finite reflexive homomorphism-homogeneous binary relational systems	systeme homogene;combinatorics;digraph;mathematiques discretes;matematicas discretas;systeme binaire;combinatoria;probleme chemin;finite digraphs;combinatoire;discrete mathematics;endomorfismo;digrafo;relation binaire;subestructura;homogeneidad;gran sistema;isomorphism;composante connexe;homomorphism homogeneous structures;homomorphism;isomorfismo;automorphism;46kxx;large system;directed graph;sistema binario;graphe oriente;sous structure;substructure;homomorphisme;binary relation;grafo orientado;endomorphism;isomorphisme;homogeneite;homomorfismo;endomorphisme;connected component;systeme fini;automorfismo;grand systeme;automorphisme;homogeneity;binary system;digraphe	A structure is called homogeneous if every isomorphism between finitely induced substructures of the structure extends to an automorphism of the structure. Recently, P.?J.?Cameron and J.?Nesetřil introduced a relaxed version of homogeneity: we say that a structure is homomorphism-homogeneous if every homomorphism between finitely induced substructures of the structure extends to an endomorphism of the structure.In this paper, we consider finite homomorphism-homogeneous relational systems with one reflexive binary relation. We show that for a large part of such relational systems (bidirectionally connected digraphs; a digraph is bidirectionally connected if each of its connected components can be traversed by ? -paths) the problem of deciding whether the system is homomorphism-homogeneous is coNP-complete. Consequently, for this class of relational systems there is no polynomially computable characterization (unless P = N P ). On the other hand, in case of bidirectionally disconnected digraphs we present the full characterization. Our main result states that if a digraph is bidirectionally disconnected, then it is homomorphism-homogeneous if and only if it is either a finite homomorphism-homogeneous quasiorder, or an inflation of a homomorphism-homogeneous digraph with involution (a specific class of digraphs introduced later in the paper), or an inflation of a digraph whose only connected components are C 3 ? and? 1 ? .		Dragan Masulovic;Rajko Nenadov;Nemanja Skoric	2011	Discrete Mathematics	10.1016/j.disc.2011.07.032	combinatorics;discrete mathematics;directed graph;topology;mathematics;algebra	Logic	23.192580370002926	32.14970337269239	85754
08069a912b1c99922076d79591f00cc63ebb6a27	new worst-case upper bound for x3sat		The rigorous theoretical analyses of algorithms for exact 3satisfiability (X3SAT) have been proposed in the literature. As we know, previous algorithms for solving X3SAT have been analyzed only regarding the number of variables as the parameter. However, the time complexity for solving X3SAT instances depends not only on the number of variables, but also on the number of clauses. Therefore, it is significant to exploit the time complexity from the other point of view, i.e. the number of clauses. In this paper, we present algorithms for solving X3SAT with rigorous complexity analyses using the number of clauses as the parameter. By analyzing the algorithms, we obtain the new worst-case upper bounds O(1.15855), where m is the number of clauses.	best, worst and average case;dpll algorithm;time complexity	Junping Zhou;Minghao Yin	2011	CoRR			AI	10.427879439876735	18.71661173884157	85816
2f30e82950453e3c11a719c4a1b623c4ab6924e4	monotonicity in graph searching	graph search;graph theory;teoria grafo;estrategia optima;information retrieval;search strategy;theorie graphe;monotonicite;algorithme;optimal strategy;algorithm;recherche information;strategie recherche;recuperacion informacion;strategie optimale;estrategia investigacion;algoritmo	"""We give a new proof of the result, due to A. LaPaugh, that a graph may be optimally """" searched """" without clearing any edge twice. o 19~~ Academic PWS, IIIC. Let us regard a graph as a system of tunnels containing a (lucky, invisible, fast) fugitive. We desire to capture this fugitive by """" searching """" all edges of the graph, in a sequence of discrete steps, while using the fewest possible """" guards. """" This problem was introduced by Breisch [2] and Parsons [6]. In the version of graph searching considered in [51 (which we call edge-searching, using terminology from [3]) a search step consists of placing a guard at a vertex, or removing a guard from a vertex, or sliding a guard along an edge. Further, an edge (u, v} is cleared by sliding a guard from u to u, while shielding u from contaminated (that is, uncleared) edges with appropriately placed guards (for example, by keeping another guard at u). If, at any point in time, there is a path from a contaminated edge e to a cleared edge e' that is not blocked by guards, e' becomes instantaneously recontaminated and must be cleared again. Our objective is to reach a state in which all edges are simultaneously cleared, so that the maximum number of guards used at any step is minimized. Any strategy that achieves this result is called optimal, and the optimal number of guards is the edge-search number of the graph. LaPaugh [5] proved that there always exists an optimal strategy that is monotone (without recontamination). One implication of this important result is that there is an optimal strategy that terminates after a linear number of steps."""	guard (computer science);maxima and minima;microsoft personal web server;norm (social);monotone	Daniel Bienstock;Paul D. Seymour	1991	J. Algorithms	10.1016/0196-6774(91)90003-H	graph power;edge contraction;factor-critical graph;combinatorics;graph bandwidth;null graph;degree;computer science;regular graph;artificial intelligence;graph theory;simplex graph;graph factorization;mathematics;distance-hereditary graph;complement graph;algorithm;strength of a graph;circulant graph	Theory	22.438826592072644	26.62132564931742	85876
9bd1cade1040eb55b2a0086e50cb397ce8e358d5	total palindrome complexity of finite words	lettre alphabet;complexite;estimacion;alphabet binaire;combinatorics;mathematiques discretes;longueur mot;matematicas discretas;combinatoria;palabra finita;complejidad;combinatoire;discrete mathematics;image;mot fini;complexity;moyenne;upper bound;finite words;palindrome complexity;estimation;word length;imagen;promedio;longitud palabra;average;letra alfabeto;letter;borne superieure;finite word;cota superior	"""The palindrome complexity function pal""""w of a word w attaches to each n@?N the number of palindromes (factors equal to their mirror images) of length n contained in w. The number of all the nonempty palindromes in a finite word is called the total palindrome complexity of that word. We present exact bounds for the total palindrome complexity and construct words which have any palindrome complexity between these bounds, for binary alphabets as well as for alphabets with the cardinal greater than 2. Denoting by M""""q(n) the average number of palindromes in all words of length n over an alphabet with q letters, we present an upper bound for M""""q(n) and prove that the limit of M""""q(n)/n is 0. A more elaborate estimation leads to M""""q(n)=O(n)."""		Mira-Cristiana Anisiu;Valeriu Anisiu;Zoltán Kása	2010	Discrete Mathematics	10.1016/j.disc.2009.08.002	arithmetic;estimation;combinatorics;complexity;letter;image;mathematics;upper and lower bounds;algorithm	Theory	16.477329204244892	26.13138672641528	85939
7830de560ef5c03b6a0a39c6d193b712fa8bd13e	an algorithm for listing all minimal double dominating sets of a tree	tree;domination;combinatorial bound;minimal double dominating set;listing algorithm;exact exponential algorithm;double domination	We provide an algorithm for listing all minimal double dominating sets of a tree of order n in time O(1.3248). This implies that every tree has at most 1.3248 minimal double dominating sets. We also show that this bound is tight.	algorithm;edge dominating set	Marcin Krzywkowski	2014	Fundam. Inform.	10.3233/FI-2014-998	mathematical optimization;combinatorics;discrete mathematics;dominating set;computer science;mathematics;tree;programming language	Theory	24.286028496023224	25.958587203885827	86090
04ad240af6e5851826f29346ddbd0528d93f069e	on some optimization problems on k-trees and partial k-trees	camino mas corto;arbre graphe;location problem;shortest path;optimisation;probleme localisation;optimizacion;tree graph;diametre;diameter;optimization problem;k câble;k arbre;chemin plus court;distancia;diametro;optimization;problema localizacion;k arbre partiel;arbol grafo;distance	A k-tree is a graph that can be reduced to the k-complete graph by sequentially removing k-degree vertices with completely connected neighbors. Partial k-trees are graphs embeddable in a k-tree with the same vertex set. In this paper we develop efficient algorithms for several path distance optimization problems on partial k-trees, and k-cable distance optimization problems on k-trees. Specifically, we develop a linear time algorithm to find shortest simple paths from a given vertex to all other vertices in a partial k-tree, we compute the diameter of a partial k-tree with equal edge lengths in linear time, and we construct an 0(nki2 ) algorithm to solve the simple plant location problem in an n-vertex partial k-tree. Then, we analyze some cable distance optimization problems in k-trees. We derive some properties of cable distance in k-trees and we present a new characterization of a k-path in k-trees. Finally, we develop algorithms to solve a certain k-cable decomposition problem in k-trees in O(n’) time and to compute the k-cable diameter of a k-tree with equal edge lengths in linear time.	algorithm;graph (discrete mathematics);mathematical optimization;partial k-tree;program optimization;time complexity;vertex (geometry)	Daniel Granot;Darko Skorin-Kapov	1994	Discrete Applied Mathematics	10.1016/0166-218X(92)00122-3	mathematical optimization;combinatorics;discrete mathematics;diameter;mathematics;distance;neighbourhood;algorithm	Theory	22.192348638456767	28.816948871733633	86094
f91151f5917ee980d7bbc8b1f05e39d93024c8ea	a region growing algorithm for detecting critical nodes		In this paper we apply a region growing bicriteria approximation algorithm of [5] for determining solutions to the critical node detection problem. This problem takes as input a number (K) and a connected, unweighted graph, and has the goal of selecting (le K) vertices to remove such that the residual network has minimum pairwise connectivity. This problem has numerous applications, including those in network security, disease mitigation, marketing and antiterrorism. The algorithm achieves an (mathcal {O}(log n)) approximation on the number of vertices needed to attain an (mathcal {O}(1)) bound on the objective function. Four random graph models and four real-world networks from different application areas are used to demonstrate that the algorithm performs within the predicted bounds.	algorithm;region growing;sensor	Mario Ventresca;Dionne M. Aleman	2014		10.1007/978-3-319-12691-3_44	computer science;discrete mathematics;residual;binary logarithm;approximation algorithm;vertex (geometry);region growing;algorithm;random graph;graph;pairwise comparison	ML	24.445660012916782	20.271363079903043	86189
76903e18be3d5b0b612974b6db8a39add7822a9e	proving lower bounds for linar decision trees	decision tree;lower bound		decision tree	Marc Snir	1981		10.1007/3-540-10843-2_25	computer science;decision tree;mathematics;upper and lower bounds	Theory	17.450090745901452	24.628616872831323	86258
b181237e908068f65ecd9e5ac89b197e661d4825	on the complexity of the $k$-anonymization problem	computational complexity	We study the problem of anonymizing tables containing personal information before releasing them for public use. One of the formulations considered in this context is the k-anonymization problem: given a table, suppress a minimum number of cells so that in the transformed table, each row is identical to atleast k − 1 other rows. The problem is known to be NP-hard and MAXSNP-hard; but in the known reductions, the number of columns in the constructed tables is arbitrarily large. However, in practical settings the number of columns is much smaller. So, we study the complexity of the practical setting in which the number of columns m is small. We show that the problem is NP-hard, even when the number of columns m is a constant (m = 3). We also prove MAXSNP-hardness for this restricted version and derive that the problem cannot be approximated within a factor of 6238 6237 . Our reduction uses alphabets Σ of arbitrarily large size. A natural question is whether the problem remains NP-hard when both m and |Σ| are small. We prove that the k-anonymization problem is in P when both m and |Σ| are constants.	approximation algorithm;column (database);data anonymization;decision table;np-hardness;personally identifiable information;snp (complexity)	Venkatesan T. Chakaravarthy;Vinayaka Pandit;Yogish Sabharwal	2010	CoRR		combinatorics;discrete mathematics;computer science;mathematics;computational complexity theory;algorithm	Theory	17.857886209456183	20.43860130750025	86273
4377b08491c41173354eef090b36912673326a13	prefix graphs and their applications	linear order	The range product problem is, for a given set S equipped with an associative operator 0, to preprocess a sequence al, ... , an of elements !rom S so as to enable efficient subsequent processing of queries of the form: Given a pair (s, t) ofintegers with 1 ::; s ::; t ::; n, return a. 0 a.+l 0 .•. 0 a, . The generic range product problem and special cases thereof, usually with 0 computing the maximum of its arguments according to some linear order on S, have been extensively studied. We show that a large number of previous sequential and parallel algorithms for these problems can be unified and simplified by means of prefix graphs.	parallel algorithm;preprocessor	Shiva Chaudhuri;Torben Hagerup	1994		10.1007/3-540-59071-4_49	kraft's inequality;mathematics;chordal graph;indifference graph;total order	Theory	14.680799986523281	29.611587949708728	86486
bd1cfbbec320e5aa1ec86b289e043424da44e247	an alternate view of complexity in k-sat problems		The satisfiability threshold for constraint satisfaction problems is that value of the ratio of constraints (or clauses) to variables, above which the probability that a random instance of the problem has a solution is zero in the large system limit. Two different approaches to obtaining this threshold have been discussed in the literature using first or second-moment methods which give rigorous bounds or using the non-rigorous but powerful replica-symmetry breaking (RSB) approach, which gives very accurate predictions on random graphs. In this paper, we lay out a different route to obtaining this threshold on a Bethe lattice. We need make no assumptions about the solution-space structure, a key assumption in the RSB approach. Despite this, our expressions and threshold values exactly match the best predictions of the cavity method under the 1-RSB (one-step RSB) hypothesis. Our method hence provides alternate interpretations as well as motivations for the key equations in the RSB approach.	bethe lattice;boolean satisfiability problem;constraint satisfaction problem;random graph;symmetry breaking	Supriya Krishnamurthy;Sumedha	2014	CoRR		combinatorics;discrete mathematics;mathematics;algorithm;statistics	Crypto	11.628491000293351	19.0736889593511	86513
e6519d9ce354cf0a5e4ef411e6978467d1463c4b	improving graph partitions using submodular functions	approximate algorithm;vlsi design;np hard problem;graph partitioning;polynomial time;network flow;submodular functions	We investigate into the role of submodular functions in designing new heuristics and approximate algorithms to some NP-hard problems arising in the field of VLSI Design Automation. In particular, we design and implement efficient heuristic for improving a bipartition of a graph in the sense of ratioCut (Discrete Appl. Math. 90 (1999) 3; 29th Annual Symposium on Foundations of Computer Science, 1988, p. 422). We also design an approximate algorithm for another NP-hard problem which is a dual of the well-known NP-hard problem of finding a densest k-subgraph of a graph (see J. Algorithms 34 (2000) 203; Proceedings of the 34th Annual Symposium on Foundations of Computer Science, 1993, p. 692). Our algorithms are based on submodular function and are implementable in polynomial time using efficient network flow based subroutines. To the best of our knowledge our algorithms are the first ones to use submodular functions based approach for the problems considered here. We also describe the experimental results which provide the evidence of our heuristic for improving the ratioCut.	submodular set function	Sachin B. Patkar;Harini Narayanan	2003	Discrete Applied Mathematics	10.1016/S0166-218X(02)00472-9	time complexity;mathematical optimization;combinatorics;discrete mathematics;flow network;graph partition;submodular set function;np-hard;mathematics;very-large-scale integration;algorithm	ML	20.83556458199339	18.786849829915255	86602
4159112f2974bc586286874b521986092be596f2	truncated dawgs and their application to minimal absent word problem		The directed acyclic word graph (DAWG) of a string y is the smallest (partial) DFA which recognizes all suffixes of y and has O(n) nodes and edges. Na et al. [11] proposed k-truncated suffix tree which is a compressed trie that represents substrings of a string whose length up to k. In this paper, we present a new data structure called k-truncated DAWGs, which can be obtained by pruning the DAWGs. We show that the size complexity of the k-truncated DAWG of a string y of length n is (O(min {n,kz})) which is equal to the truncated suffix tree’s one, where z is the size of LZ77 factorization of y. We also present an (O(nlog sigma )) time and (O(min { n,kz})) space algorithm for constructing the k-truncated DAWG of y, where (sigma ) is the alphabet size. As an application of the truncated DAWGs, we show that the set ( MAW _k(y)) of all minimal absent words of y whose length is smaller than or equal to k can be computed by using k-truncated DAWG of y in (O(min { n, kz} + | MAW _k(y)|)) time and (O(min { n,kz})) working space.		Yuta Fujishige;Takuya Takagi;Diptarama Hendrian	2018		10.1007/978-3-030-00479-8_12	computer science;suffix tree;combinatorics;trie;word problem (mathematics education);factorization;data structure;substring;alphabet;directed acyclic word graph	Theory	13.194034470473882	26.946577919186527	86894
2e2db4c0a2884739fdf54bea78236348a94cf5fa	finding real-valued single-source shortest paths in o(n3) expected time	camino mas corto;graph theory;shortest path;plus court chemin;algorithme deterministe;theorie graphe;deterministic algorithms;polynomial algorithm;single source shortest path;interconnection networks;large classes;reseau interconnexion	Given an n-vertex, m-edge directed network G with real costs on the edges and a designated source vertex s, we give a new algorithm to compute shortest paths from s. Our algorithm is a simple deterministic one with O(n logn) expected running time over a large class of input distributions. This is the first strongly polynomial algorithm in over 35 years to improve upon some aspect of the O(nm) running time of the Bellman-Ford algorithm. The result extends to an O(n log n) expected running time algorithm for finding the minimum mean cycle, an improvement over Karp’s O(nm) worst-case time bound when the underlying graph is dense. Both of our time bounds are shown to be achieved with high probability.	bellman–ford algorithm;best, worst and average case;directed graph;polynomial;shortest path problem;time complexity;with high probability	Stavros G. Kolliopoulos;Clifford Stein	1998	J. Algorithms	10.1006/jagm.1998.0937	mathematical optimization;suurballe's algorithm;combinatorics;floyd–warshall algorithm;graph theory;yen's algorithm;johnson's algorithm;mathematics;shortest path problem;k shortest path routing;shortest path faster algorithm;algorithm	Theory	21.174262582297377	22.981491511151653	86982
7f814d8b9bd64a6ed5419c34a8af779d240de0d7	fast detection and display of symmetry in outerplanar graphs	algorithme rapide;bon dessin;representation graphique;good drawing;axial symmetry;deteccion;representacion grafica;simetria axial;graphe extraplanaire;detection;symetrie;symmetry;graphe biconnexe;grafo;fast algorithm;graph;graphe;symetrie axiale;outerplanar graph;simetria;symetrie rotationnelle;algoritmo rapido;graphics;biconnected graph	Manning, J. and M. J. Atallah, Fast detection and display of symmetry in outerplanar graphs, Discrete Applied Mathematics 39 (1992) 13-35. The automatic construction of good drawings of abstract graphs is a problem of practical importance. Symmetry appears as one of the main criteria for achieving goodness. Algorithms are developed for enumerating all planar axial and rotational symmetries of a biconnected outerplanar graph, and it is shown how to construct a drawing which simultaneously displays all these symmetries. These results are then expanded to obtain algorithms for the detection and display of outerplanar axial and rotational symmetry in the entire class of outerplanar graphs. All algorithms run in both time and space which are linear in the size of the graph, and hence are optimal to within a constant factor.	algorithm;biconnected graph;outerplanar graph	Joseph Manning;Mikhail J. Atallah	1992	Discrete Applied Mathematics	10.1016/0166-218X(92)90112-N	axial symmetry;outerplanar graph;pathwidth;combinatorics;discrete mathematics;graphics;mathematics;geometry;biconnected graph;symmetry;graph;partial k-tree	Theory	23.42136513637341	28.23276441873016	87313
644c21a209b267591767643bd2a2c982eb005692	two algorithms for lcs consecutive suffix alignment	dynamic programming;sequence similarity;match point arithmetic;dynamic program;incremental algorithms;longest common subsequence;pattern matching;incremental algorithm	The problem of aligning two sequences A and B to determine their similarity is one of the fundamental problems in pattern matching. A challenging, basic variation of the sequence similarity problem is the incremental string comparison problem, denoted Consecutive Suffix Alignment, which is, given two strings A and B, to compute the alignment solution of each suffix of A versus B. Here, we present two solutions to the Consecutive Suffix Alignment Problem under the LCS (Longest Common Subsequence) metric, where the LCS metric measures the subsequence of maximal length common to A and B. The first solution is an O(nL) time and space algorithm for constant alphabets, where the size of the compared strings is O(n) and L=	algorithm	Gad M. Landau;Eugene W. Myers;Michal Ziv-Ukelson	2007	J. Comput. Syst. Sci.	10.1016/j.jcss.2007.03.004	generalized suffix tree;longest common substring problem;combinatorics;discrete mathematics;longest increasing subsequence;computer science;dynamic programming;pattern matching;longest common subsequence problem;mathematics;compressed suffix array;programming language;longest alternating subsequence;algorithm	Theory	13.653750894651312	26.838523781516265	87687
c6e66ad6350d8299b5bedc7ad7e8d41733cca875	an algorithm for finding input-output constrained convex sets in an acyclic digraph	input output;computer experiment;convex set;embedded processor	A set X of vertices of an acyclic graph is convex if any vertex on a directed path between elements of X is itself in X. We construct an algorithm for generating all input-output constrained convex (IOCC) sets in an acyclic digraph, which uses several novel ideas. We show that our algorithm is more efficient than algorithms described in the literature in both the worst case and computational experiments. IOCC sets of acyclic digraphs are of interest in the area of modern embedded processor technology.	algorithm;best, worst and average case;computation;directed acyclic graph;directed graph;embedded system;experiment;path (graph theory);processor technology	Gregory Gutin;Adrian Johnstone;Joseph Reddington;Elizabeth Scott;Anders Yeo	2008		10.1007/978-3-540-92248-3_19	convex analysis;input/output;subderivative;mathematical optimization;combinatorics;discrete mathematics;computer experiment;convex hull;mathematics;convex set;directed acyclic word graph	Theory	20.94346831684823	25.495637841294688	87935
0a501b1a13b0695351d4aa58e5e164daeb99f30a	dynamic optimality - almost [competitive online binary search tree]	optimisation;binary search trees data structures computer science tree data structures artificial intelligence laboratories information science read write memory;trees mathematics;trees mathematics optimisation computational complexity;competitive online binary search tree dynamic optimality;binary search tree;computational complexity;competitive ratio;dynamic optimization	We present an O(lg lg n)-competitive online binary search tree, improving upon the best previous (trivial) competitive ratio of O(lg n). This is the first major progress on Sleator and Tarjan's dynamic optimality conjecture of 1985 that O(1)-competitive binary search trees exist.	competitive analysis (online algorithm);optimal binary search tree;splay tree	Erik D. Demaine;Dion Harmon;John Iacono;Mihai Patrascu	2004		10.1109/FOCS.2004.23	random binary tree;optimal binary search tree;cartesian tree;competitive analysis;red–black tree;binary search tree;tree rotation;geometry of binary search trees;binary tree;computer science;treap;theoretical computer science;order statistic tree;scapegoat tree;machine learning;self-balancing binary search tree;k-d tree;weight-balanced tree;ternary search tree;threaded binary tree;computational complexity theory;tree traversal;algorithm;dichotomic search	Theory	15.521230207201123	29.59035625050888	87987
07f829deab9272fe4e1ff6bd8fcb931c279b5eb1	towards fast atomic decomposition using axiom dependency hypergraphs		Atomic decomposition of ontologies has been suggested as a tool to understand the modular structure of ontologies. It consists of a polynomial size representation of potentially exponentially many modules of an ontology. Tractable algorithms for computing the atomic decomposition for locality-based modules have been introduced, albeit leaving room for improvement in terms of running time. In this paper, we consider ontologies formulated in OWL-EL. We introduce a notion of an axiom dependency hypergraph for an ontology, which represents how axioms are included in locality-based modules. We use a standard algorithm from graph theory to compute strongly connected components of the axiom dependency hypergraph and show that such components correspond to atoms of the ontology. An empirical evaluation of the algorithm on large fragments of biomedical ontologies confirms a significant improvement in running time.	algorithm;best, worst and average case;fma instruction set;graph theory;locality of reference;mutual exclusion;ontology (information science);polynomial;reachability;semantic reasoner;speedup;strongly connected component;time complexity;web ontology language	Francisco Martín-Recuerda;Dirk Walther	2013			hypergraph;combinatorics;graph theory;discrete mathematics;open biomedical ontologies;polynomial;axiom;constraint graph;ontology (information science);strongly connected component;mathematics	AI	20.529023800500852	24.2648094350852	88206
19b984e82223153aea88a393f24201b3a96a2e1c	minimum common string partition parameterized by partition size is fixed-parameter tractable	algorithms;design;combinatorics;complexity measures;theory	The NP-hard Minimum Common String Partition problem asks whether two strings x and y can each be partitioned into at most k substrings, called blocks, such that both partitions use exactly the same blocks in a different order. We present the first fixed-parameter algorithm for Minimum Common String Partition using only parameter k.	algorithm;cobham's thesis;np-hardness;parameterized complexity;partition problem;substring;time complexity	Laurent Bulteau;Christian Komusiewicz	2014		10.1137/1.9781611973402.8	rank of a partition;mathematical optimization;combinatorics;discrete mathematics;mathematics	DB	23.320265229989758	22.514545586229048	88252
57b19fd3929247209dc524f46ce93e6fd97451e0	characterizing and bounding the imperfection ratio for some classes of graphs	graph theory;graphe biparti;teoria grafo;polytope des stables;imperfection ratio;grafo bipartido;graphe parfait;graph clique;grafo perfecto;problema np duro;perfect graph;theorie graphe;optimisation combinatoire;np hard problem;stable set polytope;probleme np difficile;mathematical programming;stable set polytope 90c57 90c27;grafo linea;polytope de conjuntos estables;clique graphe;line graph;combinatorial optimization;graphe ligne;bipartite graph;programmation mathematique;programacion matematica;optimizacion combinatoria	Perfect graphs constitute a well-studied graph class with a rich structure, reflected by many characterizations with respect to different concepts. Perfect graphs are, for instance, precisely those graphs G where the stable set polytope STAB(G) equals the fractional stable set polytope QSTAB(G). The dilation ratio min{t : QSTAB(G) ⊆ t STAB(G)} of the two polytopes yields the imperfection ratio of G. It is NP-hard to compute and, for most graph classes, it is even unknown whether it is bounded. For graphs G such that all facets of STAB(G) are rank constraints associated with antiwebs, we characterize the imperfection ratio and bound it by 3/2. Outgoing from this result, we characterize and bound the imperfection ratio for several graph classes, including near-bipartite graphs and their complements, namely quasi-line graphs, by means of induced antiwebs and webs, respectively.	dilation (morphology)	Sylvain Coulonges;Arnaud Pêcher;Annegret Wagler	2009	Math. Program.	10.1007/s10107-007-0182-9	clique;pathwidth;mathematical optimization;combinatorics;discrete mathematics;bipartite graph;perfect graph;combinatorial optimization;graph theory;np-hard;mathematics;geometry;chordal graph;indifference graph;line graph	Theory	22.917629179768724	26.225072401720166	88298
a027230d0bf731522dd4fd3549b5f2d7a155cbb9	a simple linear time algorithm for finding even triangulations of 2-connected bipartite plane graphs	graphe biparti;galeria arte;grafo bipartido;temps lineaire;linear time algorithm;tiempo lineal;connected graph;upper bound;resolucion problema;triangulacion;graphe planaire;linear time;triangulation;grafo planario;galerie art;bipartite graph;art gallery;graphe connexe;planar graph;problem solving;resolution probleme;plane graph;grafo conexo	Recently, Hoffmann and Kriegel proved an important combinatorial theorem [4]: Every 2-connected bipartite plane graph G has a triangulation in which all vertices have even degree (it's called an even triangulation). Combined with a classical Whitney's Theorem, this result implies that every such a graph has a 3-colorable plane triangulation. Using this result, Hoffmann and Kriegel significantly improved the upper bounds of several art gallery and prison guard problems. A complicated O(n2) time algorithm was obtained in [4] for constructing an even triangulation of G. Hoffmann and Kriegel conjectured that there is an O(n3/2) algorithm for solving this problem [4].In this paper, we develop a very simple O(n) time algorithm for solving this problem. Our algorithm is based on thorough study of the structure of all even triangulations of G. We also obtain a simple formula for computing the number of distinct even triangulations of G.	algorithm;time complexity	Huaming Zhang;Xin He	2002		10.1007/3-540-45749-6_78	mathematical optimization;combinatorics;discrete mathematics;pitteway triangulation;point set triangulation;mathematics;geometry;algorithm;planar graph	Theory	22.805624839928345	27.678104685812652	88404
632ada7da296c9ffc6eae73da76ec871781ef4e3	toughness and (a, b, k)-critical graphs	toughness;graphe critique;a b factor;nombre entier;procesamiento informacion;subgrafo;algorithm analysis;critical graph;combinatorial problems;satisfiability;integer;combinatorial problem;probleme combinatoire;problema combinatorio;sous graphe;informatique theorique;entero;graph;68r10;information processing;analyse algorithme;subgraph;grafo critico;traitement information;a b k critical graph;analisis algoritmo;minimum degree;computer theory;informatica teorica	Abstract   Let   a  ,  b  ,  k   be nonnegative integers with   2  ⩽  a    b   and   b  ⩾  a  (  k  +  1  )  . An   [  a  ,  b  ]  -factor of a graph  G  is defined as a spanning subgraph  F  of  G  such that   a  ⩽   d  F   (  x  )  ⩽  b   for each   x  ∈  V  (  G  )  . A graph  G  is called an   (  a  ,  b  ,  k  )  -critical graph if after deleting any  k  vertices of  G  the remaining graph of  G  has an   [  a  ,  b  ]  -factor. This toughness of a graph  G , denoted by   t  (  G  )  , is defined as   t  (  G  )  =  min  {    |  S  |    ω  (  G  −  S  )    :  S  ⊆  V  (  G  )  ,  ω  (  G  −  S  )  ⩾  2  }   if  G  is not complete; otherwise,   t  (  G  )  =  +  ∞  . In this paper, it is proved that a graph  G  is an   (  a  ,  b  ,  k  )  -critical graph if  G  satisfies   δ  (  G  )  ⩾  a  +  k   and   t  (  G  )  ⩾  a  −  1  +    (  a  −  1  )  (  k  +  1  )   b   . Furthermore, it is shown that the result in this paper is best possible in some sense.	graph toughness	Sizhong Zhou;Jiashang Jiang	2011	Inf. Process. Lett.	10.1016/j.ipl.2011.01.012	integer;combinatorics;toughness;information processing;mathematics;critical graph;graph;bound graph;algorithm;satisfiability	DB	24.3301114760128	29.95008036709945	88800
eb07b5025304a19413563ace7038a0bf1f78f2e9	algorithmic aspects of property testing in the dense graphs model	graph properties;property testing;graph model;adaptivity vs non adaptivity	In this paper we consider two basic questions regarding the query complexity of testing graph properties in the adjacency matrix model. The first question refers to the relation between adaptive and non-adaptive testers, whereas the second question refers to testability within complexity that is inversely proportional to the proximity parameter, denoted  ***  . The study of these questions reveals the importance of algorithmic design (also) in this model. The highlights of our study are:#R##N##R##N##R##N#A gap between the complexity of adaptive and non-adaptive testers. Specifically, there exists a (natural) graph property that can be tested using ${\widetilde{O}}(\epsilon^{-1})$ adaptive queries, but cannot be tested using  o  ( ***  *** 3/2) non-adaptive queries.#R##N#In contrast, there exist natural graph properties that can be tested using ${\widetilde{O}}(\epsilon^{-1})$ non-adaptive queries, whereas ***( ***  *** 1) queries are required even in the adaptive case.#R##N##R##N##R##N##R##N#We mention that the properties used in the foregoing conflicting results have a similar flavor, although they are of course different.	property testing	Oded Goldreich;Dana Ron	2009		10.1007/978-3-642-03685-9_39	combinatorics;discrete mathematics;null graph;graph property;computer science;clique-width;theoretical computer science;property testing;mathematics;graph;complement graph;algorithm	Theory	19.660265804370834	22.813390278317353	88855
ce92901d3358b3e4ab07c486c5e49bf89f527f79	parameterized algorithms for modular-width	datavetenskap datalogi;computer science	It is known that a number of natural graph problems which are FPT parameterized by treewidth become W-hard when parameterized by clique-width. It is therefore desirable to find a different structural graph parameter which is as general as possible, covers dense graphs but does not incur such a heavy algorithmic penalty. The main contribution of this paper is to consider a parameter called modular-width, defined using the well-known notion of modular decompositions. Using a combination of ILPs and dynamic programming we manage to design FPT algorithms for Coloring and Partitioning into paths (and hence Hamiltonian path and Hamiltonian cycle), which are W-hard for both clique-width and its recently introduced restriction, shrub-depth. We thus argue that modular-width occupies a sweet spot as a graph parameter, generalizing several simpler notions on dense graphs but still evading the “price of generality” paid by clique-width.	algorithm;clique-width;cobham's thesis;convex function;convex set;davis–putnam algorithm;decision table;dynamic programming;edge dominating set;hamiltonian path;parameterized complexity;treewidth;whole earth 'lectronic link	Jakub Gajarský;Michael Lampis;Sebastian Ordyniak	2013		10.1007/978-3-319-03898-8_15	1-planar graph;pathwidth;mathematical optimization;combinatorics;discrete mathematics;computer science;clique-width;mathematics;partial k-tree;algorithm	Theory	22.553209609093333	22.665799260105008	88947
8db321ee841cf69919161d3edee7faced4e8194b	a note on parallel complexity of maximum f-matching	parallel algorithm;complexite calcul;maximum matching;algorithme parallele;theory of computing;computational complexity;theory of computation;parallel algorithms	We present a randomized NC solution to the problem of constructing a maximum (cardinality) f-matching. constraints. @ 1998 Elsevier Science B.V.	randomized algorithm	Anders Dessmark;Oscar Garrido;Andrzej Lingas	1998	Inf. Process. Lett.	10.1016/S0020-0190(97)00196-8	mathematical optimization;combinatorics;theory of computation;maximum coverage problem;computer science;3-dimensional matching;mathematics;parallel algorithm;algorithm	AI	18.268822208468833	26.364320309216012	88975
703f0c155e6f4cf2e7f7d302e4374c658ba08ce2	fixed-parameter complexity of lambda-labelings	graph theory;complexite;europa;coloracion grafo;teoria grafo;algorithm complexity;allocation canal;probleme np complet;resource allocation;complejidad algoritmo;suiza;asignacion canal;complejidad;recubrimiento;overlay;suisse;complexity;theorie graphe;recouvrement;graph cover;optimisation combinatoire;multigraph;probleme recouvrement;coloration graphe;complexite algorithme;fixed parameter complexity;problema recubrimiento;multigrafo;marcacion grafo;workshop;problema np completo;asignacion recurso;switzerland;marquage graphe;europe;multigraphe;covering problem;graph labeling;allocation ressource;channel allocation;combinatorial optimization;atelier;1999;taller;graph labelling;np complete problem;graph colouring;channel assignment;optimizacion combinatoria	A -labeling of a graph G is an assignment of labels from the set {0; : : : ; } to the vertices of G such that vertices at distance of at most two get di.erent labels and adjacent vertices get labels which are at least two apart. We study the minimum value = (G) such that G admits a -labeling. We show that for every 3xed value k¿4 it is NP-complete to determine whether (G)6k. We further investigate this problem for sparse graphs (k-almost trees), extending the already known result for ordinary trees. In a generalization of this problem we wish to 3nd a labeling such that vertices at distance two are assigned labels that di.er by at least q and the labels of adjacent vertices di.er by at least p. We denote the minimum that allows such a labeling by L(G;p; q). We show several hardness results for L(G;p; q) including that for any p¿q¿1 there is a = (p; q) such that deciding if L(G;p; q)6 is NP-complete, and that for p¿2q, this decision is NP-complete for every ¿ (p; q). c © 2001 Published by Elsevier Science B.V.	decision problem;edge coloring;graph coloring;np-completeness;neighbourhood (graph theory);sparse matrix;time complexity;total coloring;treewidth;vertex (geometry)	Jirí Fiala;Ton Kloks;Jan Kratochvíl	2001	Discrete Applied Mathematics	10.1016/S0166-218X(00)00387-5	mathematical optimization;combinatorics;discrete mathematics;complexity;np-complete;graph labeling;combinatorial optimization;resource allocation;graph theory;multigraph;mathematics;overlay;algorithm	Theory	21.431866085690995	28.59577821015322	88990
5dd9a1df007d3baa7a09cfe59cfec36e571e24ec	integer representation and counting in the bit probe model	modelizacion;subtraction;acces contenu;approximation asymptotique;comptage;gray code;sustraccion;soustraction;contaje;modelisation;funcion logaritmica;logarithmic function;content access;complexity measure;data structures;mesure complexite;estructura datos;fonction logarithmique;counting;code gray;acceso contenido;structure donnee;asymptotic approximation;model of computation;codigo gray;modeling;bit probe model;data structure;medida complexidad;lower bound;aproximacion asintotica	We examine the problem of integer representation in a nearly minimal number of bits so that the increment and the decrement (and indeed the addition and the subtraction) operations can be performed using few bit inspections and fewer bit changes. The model of computation we considered is the bit probe model, where the complexity measure counts only the bitwise accesses to the data structure. We present several efficient data structures to represent integer that use a logarithmic number of bit inspections and a constant number of bit changes per operation. The most space-efficient data structure uses only one extra bit. We also present an extension to our data structure to support efficient addition and subtraction, where the larger value is replaced by the result, while retaining the same asymptotic bounds for the increment and the decrement operations.	4-bit;best, worst and average case;bitwise operation;blum axioms;brute-force search;computational complexity theory;data structure;increment and decrement operators;model of computation;time complexity	Mohammed Z Rahman;J. Ian Munro	2008	Algorithmica	10.1007/s00453-008-9247-2	model of computation;gray code;least significant bit;logarithm;combinatorics;discrete mathematics;systems modeling;8-n-1;subtraction;bit error rate;data structure;bit array;dirty bit;bit manipulation;computer science;bit numbering;parity bit;mathematics;most significant bit;bit field;upper and lower bounds;sign bit;programming language;find first set;bitwise operations in c;counting;algorithm	Theory	15.965005347618844	26.7973028846671	89023
79ec4c76bd0bea1b2ea0effbdcd8d94d3b78857e	algorithms for dominating set in disk graphs: breaking the logn barrier - (extended abstract)	dominating set;randomized algorithm	We consider the problem of finding a lowest cost dominating set in a given disk graph containing n disks. The problem has been extensively studied on subclasses of disk graphs, yet the best known approximation for disk graphs has remained O(log n) – a bound that is asymptotically no better than the general case. We improve the status quo in two ways: for the unweighted case, we show how to obtain a PTAS using the framework recently proposed (independently) by Mustafa and Ray [16] and by Chan and Har-Peled [4]; for the weighted case where each input disk has an associated rational weight with the objective of finding a minimum cost dominating set, we give a randomized algorithm that obtains a dominating set whose weight is within a factor 2 ∗ n) of a minimum cost solution, with high probability – the technique follows the framework proposed recently by Varadarajan [19].	approximation;disk storage;dominating set;ptas reduction;randomized algorithm;srinidhi varadarajan;with high probability	Matt Gibson;Imran A. Pirwani	2010		10.1007/978-3-642-15775-2_21	mathematical optimization;combinatorics;discrete mathematics;dominating set;computer science;mathematics;maximal independent set;randomized algorithm;algorithm	Theory	23.621142850056675	20.703314940673135	89118
4db0e64a78e8b4e909d2490202324c53fffd6feb	improved shortest path algorithms for nearly acyclic graphs	strongly connected component;feedback vertex set;time complexity;shortest path algorithm;dijkstra s algorithm;directed graph;priority queue;single source shortest path;data structure;hybrid algorithm	Dijkstra’s algorithm solves the single-source shortest path problem on any directed graph in O(m + n log n) worst-case time when a Fibonacci heap is used as the frontier set data structure. Here n is the number of vertices and m is the number of edges in the graph. If the graph is nearly acyclic, then other algorithms can achieve a time complexity lower than that of Dijkstra’s algorithm. Abuaiadh and Kingston gave a single source shortest path algorithm for nearly acyclic graphs with O(m + n log t) worst-case time complexity, where the new parameter t is the number of delete-min operations performed in priority queue manipulation. For nearly acyclic graphs, the value of t is expected to be small, allowing the algorithm to outperform Dijkstra’s algorithm. Takaoka, using a different definition for acyclicity, gave an algorithm with O(m+n log k) worstcase time complexity. In this algorithm, the new parameter k is the maximum cardinality of the strongly connected components in the graph. This thesis presents several new shortest path algorithms that define trigger vertices, from which efficient computation of shortest paths through underlying acyclic structures in the graph is possible. Various definitions for trigger vertices are considered. One definition decomposes a graph into a unique set of acyclic structures, where each single trigger vertex dominates a single corresponding acyclic structure. This acyclic decomposition can be computed in O(m) time, thus allowing the single source problem to be solved in O(m + r log r) worst-case time, where r is the resulting number of trigger vertices in the graph. For nearly acyclic graphs, the value of r is small and single-source can be solved in close to O(m) worst-case time. It is possible to define both monodirectional and bidirectional variants of this acyclic decomposition. This thesis also presents decompositions in which multiple trigger vertices dominate a single acyclic structure. The trigger vertices of such decompositions constitute feedback vertex sets. If trigger vertices are defined as a set of precomputed feedback vertices, then the all-pairs shortest path problem can be solved in O(mn + nr) worst-case time. This allows all-pairs to be solved in O(mn) worst-case time when a feedback vertex set smaller than the square root of the number of edges is known. For suitable graph types, these new algorithms offer an improvement on the time complexity of previous algorithms.	best, worst and average case;computation;data structure;directed acyclic graph;directed graph;feedback vertex set;fibonacci heap;graph (discrete mathematics);maxima and minima;precomputation;priority queue;shortest path problem;strongly connected component;time complexity;vertex (graph theory);zhu–takaoka string matching algorithm	Shane Saunders;Tadao Takaoka	2001	Electr. Notes Theor. Comput. Sci.	10.1016/S1571-0661(04)80888-0	mathematical optimization;suurballe's algorithm;combinatorics;discrete mathematics;bidirectional search;independent set;dijkstra's algorithm;widest path problem;data structure;longest path problem;a* search algorithm;floyd–warshall algorithm;computer science;pathfinding;hopcroft–karp algorithm;yen's algorithm;cycle graph;johnson's algorithm;mathematics;path;shortest path problem;programming language;distance;k shortest path routing;directed acyclic graph;shortest path faster algorithm;matching	Theory	20.0407728397452	24.085677875661197	89292
178576f7602d2d979c67c588c339ec165d15e0d0	a simple parallel algorithm for the maximal independent set problem	graph theory;algoritmo paralelo;teoria grafo;nc;parallel algorithm;metodo monte carlo;algorithm performance;methode monte carlo;68o0;05b99;05c99;theorie graphe;algorithme parallele;68e10;aleatorizacion;62k99;maximal independent set;68099;resultado algoritmo;pairwise independences;monte carlo method;performance algorithme;randomisation;ensemble independant maximal;62e25;randomization;parallel computations;randomizing algorithms	Two basic design strategies are used to develop a very simple and fast parallel algorithms for the maximal independent set (MIS) problem. The first strategy consists of assigning identical copies of a simple algorithm to small local portions of the problem input. The algorithm is designed so that when the copies are executed in parallel the correct problem output is produced very quickly. A very simple Monte Carlo algorithm for the MIS problem is presented which is based upon this strategy. The second strategy is a general and powerful technique for removing randomization from algorithms. This strategy is used to convert the Monte Carlo algorithm for this MIS problem into a simple deterministic algorithm with the same parallel running time.	independent set (graph theory);maximal independent set;maximal set;parallel algorithm	Michael Luby	1986	SIAM J. Comput.	10.1137/0215074	nc;randomization;mathematical optimization;combinatorics;graph theory;mathematics;parallel algorithm;maximal independent set;algorithm;monte carlo method	Theory	18.158770041343235	28.12902068297899	89457
7301791ae4284d2199f61404d196edcddd54df8a	graph search algorithms and maximum bipartite matching algorithm on the hypercube network model	graph search;parallel calculus;graph theory;algoritmo paralelo;hypercube;graphe biparti;teoria grafo;busqueda aucho primer;parallel algorithm;algoritmo busqueda;time complexity;information retrieval;algorithme recherche;recherche profondeur d abord;bipartite matching;search algorithm;theorie graphe;algorithme parallele;complexite temps;calculo paralelo;recherche information;network model;grafico bipartido;depth first search;recherche largeur d abord;recuperacion informacion;complejidad tiempo;busqueda profundidad primer;breadth first search;bipartite graph;calcul parallele;hipercubo	"""The parallel algorithms for graph search techniques such as breadth-first search (BFS) and depth-first search (DFS) have been studied by several researchers [2,3,5,7]. In this paper, two parallel search algorithms, one for BFS and the other for DFS, based on the hypercube network model are proposed. In addition, we devise a parallel algorithm to find the maximum matching in bipartite graphs. Both search technique~ have the lower bound of time complexity f~(n 2) for sequential algorithms in the worst case, and the fast known sequential maximum bipartite matching algorithm needs O(n 2""""5) time [4], where n is the number of vertices in the graph. Our parallel algorithms take O(n log n) time for the two graph search techniques and O(n 1""""S log n)"""	best, worst and average case;breadth-first search;depth-first search;graph traversal;matching (graph theory);network model;parallel algorithm;search algorithm;time complexity;vertex (graph theory)	Jang-Ping Sheu;Nan-Ling Kuo;Gen-Huey Chen	1990	Parallel Computing	10.1016/0167-8191(90)90151-X	folded cube graph;combinatorics;grid network;discrete mathematics;bipartite graph;breadth-first search;computer science;graph theory;3-dimensional matching;hopcroft–karp algorithm;hypercube graph;mathematics;blossom algorithm;assignment problem;best-first search;algorithm;matching;search algorithm	AI	18.85347741495149	27.99919510720987	89473
cdc677ebd122ab0192e708e5fc996976ba5a60d6	randomized and approximation algorithms for blue-red matching	approximate algorithm;maximum matching;polynomial time	We introduce the Blue-Red Matching problem: given a graph with red and blue edges, and a bound w, find a maximum matching consisting of at most w edges of each color. We show that Blue-Red Matching is at least as hard as the problem Exact Matching (Papadimitriou and Yannakakis, 1982), for which it is still open whether it can be solved in polynomial time. We present an RNC algorithm for this problem as well as two fast approximation algorithms. We finally show the applicability of our results to the problem of routing and assigning wavelengths to a maximum number of requests in all-optical rings.	approximation algorithm;randomized algorithm	Christos Nomikos;Aris Pagourtzis;Stathis Zachos	2007		10.1007/978-3-540-74456-6_63	time complexity;mathematical optimization;combinatorics;discrete mathematics;computer science;3-dimensional matching;edge cover;mathematics;algorithm;matching	Theory	23.811745949564557	22.75185049136814	89509
2c3d922037509edd8c8d495e2b41d4f388c66baf	on total traffic domination in non-complete graphs	dominance;elektroteknik och elektronik;gestion red;flujo optimo;gestion trafic;multi hour optimization;reseau;traffic management;red;dominancia;gestion reseau;gestion trafico;network optimization;coaccion capacidad;flot optimal;contrainte capacite;network management;capacity constraint;complete graph;optimal flow;traffic matrix;traffic matrices domination;network	Given an undirected graph G(V,E), a set of traffic matrices H and one additional traffic matrix h, we say that H totally dominates h if, for each capacity reservation u supporting H, u also supports h using the same routing pattern. It has been shown that if |H|=1 (H={h@?}, say) and G is a complete graph, H totally dominates h if, and only if, h@?>=h component-wise. In this paper we give a generalized condition for |H|>=1 and any undirected graph.	dominating set	Pablo Pavón-Mariño;Michal Pióro	2011	Oper. Res. Lett.	10.1016/j.orl.2010.11.007	network management;active traffic management;telecommunications;computer science;mathematics;dominance;subgroup;complete graph;algorithm	Theory	22.744200644884423	30.000239513442796	89535
aa79b18a6f5cbeaa543f5645ce9ff2e3adefc22d	on regular expression matching and deterministic finite automata		Given a regular expression R and a string T the regular expression matching problem is to determine if T matches any string in the language generated by R. The best known solution to the problem uses linear space and O ( nm log logn log3/2 n +n+m ) time in the worst-case [2], where m and n are the lengths of R and T , respectively. A common misconception is that we can solve the problem efficiently by building a deterministic finite automaton (DFA) for R using 2O(m) space and then run it on T in O(n) time [1]. However, this analysis completely ignores issues of addressing into exponential sized data structures. An address in a DFA of size 2Ω(m) requires Ω(m) bits. Hence, on a standard unit-cost word RAM with word length Θ(logn) [3], we need at least Ω(m/ logn) time to simply write an address in the DFA. It follows that traversing the DFA for R uses at least Ω(nm/ logn+n+m) worst-case time (note that we do not even include DFA construction time). This bound can only be O(n) when m = O(logn) and is never better than the above best known bound. BODY Even ignoring construction time, deterministic finite automata do not solve regular expression matching in worst-case linear time.	automata theory;best, worst and average case;data structure;deterministic finite automaton;finite-state machine;random-access memory;regular expression;time complexity	Philip Bille	2015	TinyToCS		combinatorics;discrete mathematics;nondeterministic finite automaton;theoretical computer science;mathematics;string searching algorithm	Theory	12.91840991267465	26.75831477638775	89603
531f7cb32aa139dfc31d8b987744498b100c7a6c	constructing huffman trees in parallel	least weight subsequence;68p20;parallel algorithm;fonction poids;huffman coding;trees mathematics;huffman codes;algorithme parallele;huffman code;arbre mathematiques;codigo huffman;arbol binario;code huffman;soussuite moindre poids;arbre binaire;funcion peso;weight function;fonction concave;parallel algorithms;binary tree	We present a parallel algorithm for the Huffman coding problem. We reduce the Huffman coding problem to the concave least weight subsequence problem and give a parallel algorithm that solves the latter problem in $O(\sqrt n \log n)$ time with $n$ processors on a concurrent read exclusive write parameter random-access machine (CREW PRAM). This leads to the first sublinear time $o(n^2)$-total work parallel algorithm for Huffman coding. This reduction of the Huffman coding problem to the CLWS problem also yields an alternative $O(n \log n)$-time (or linear-time, for a sorted input sequence) algorithm for Huffman coding.	huffman coding	Lawrence L. Larmore;Teresa M. Przytycka	1995	SIAM J. Comput.	10.1137/S0097539792233245	arithmetic;shannon–fano coding;canonical huffman code;variable-length code;computer science;theoretical computer science;mathematics;algorithm;statistics;huffman coding	Theory	15.880810886109956	26.73899270141991	89644
c277fecf2b4070a1449a67c50e8efc08430055fd	faster algorithms for minimum cycle basis in directed graphs	cycle basis;shortest path;68w40;shortest paths;metodo monte carlo;digraph;65c05;vector space;methode monte carlo;digrafo;05c20;anneau;punto;calculo automatico;vecteur;product;68wxx;input;computing;calcul automatique;algorithme monte carlo;multiplication matrice;point;dot;producto;dijkstra s algorithm;directed graph;cycle graphe;monte carlo method;ensemble contour;68r10;graphe oriente;monte carlo algorithm;edge graph;entree ordinateur;edge set;algorithme dijkstra;randomized algorithm;arete graphe;produit;algoritmo dijstra;grafo orientado;minimum cycle basis;matrix multiplication;vector;dijstra algorithm;cycle graph;espace vectoriel;fast matrix multiplication;ring;entrada ordenador;58a25;randomization;espacio vectorial;68w20;arista grafico;anillo;ciclo diagrama;digraphe	We consider the problem of computing a minimum cycle basis in a directed graph. The input to this problem is a directed graph G whose edges have nonnegative weights. A cycle in this graph is actually a cycle in the underlying undirected graph with edges traversable in both directions. A {−1, 0, 1} edge incidence vector is associated with each cycle: edges traversed by the cycle in the right direction get 1 and edges traversed in the opposite direction get −1. The vector space over Q generated by these vectors is the cycle space of G. A set of cycles is called a cycle basis of G if it forms a basis for this vector space. We seek a cycle basis where the sum of weights of the cycles is minimum. The current fastest algorithm for computing a minimum cycle basis in a directed graph with m edges and n vertices runs in Õ(mω+1n) time, where ω < 2.376 is the exponent of matrix multiplication. We present an O(m3n + m2n2 logn) algorithm. We obtain our algorithm by using fast matrix multiplication over rings and an efficient extension of Dijkstra’s algorithm to compute a shortest cycle in G whose dot product with a function on its edge set is nonzero. We also present a simple O(m2n + mn2 logn) Monte Carlo algorithm. The problem of computing a minimum cycle basis in an undirected graph has been well studied. In this problem a {0, 1} edge incidence vector is associated with each cycle and the vector space over Z2 generated by these vectors is the cycle space of the graph. The fastest known algorithm for computing a minimum cycle basis in an undirected graph runs in O(m2n+mn2 logn) time and our randomized algorithm for directed graphs matches this running time.	cycle (graph theory);cycle basis;cycle space;deterministic algorithm;dijkstra's algorithm;directed graph;fastest;graph (discrete mathematics);incidence matrix;matrix multiplication;monte carlo algorithm;randomized algorithm;time complexity	Ramesh Hariharan;Telikepalli Kavitha;Kurt Mehlhorn	2008	SIAM J. Comput.	10.1137/060670730	cycle space;mathematical optimization;combinatorics;discrete mathematics;feedback arc set;directed graph;pseudoforest;cycle graph;mathematics;path;butterfly graph;cycle basis;algorithm;strength of a graph	Theory	21.349189762545652	29.309649048724417	89915
99359db5690900dd43ecb92c381cc71a6dac4cc1	improved low-density subset sum algorithms	knapsack cryptosystems;subset sum problems;subset sum problem;lattices;lattice basis reduction;polynomial time algorithm;subject classifications 11y16;polynomial time;low density	The general subset sum problem is NP-complete. However, there are two algorithms, one due to Brickell and the other to Lagarias and Odlyzko, which in polynomial time solve almost all subset sum problems of sufficiently low density. Both methods rely on basis reduction algorithms to find short non-zero vectors in special lattices. The Lagarias-Odlyzko algorithm would solve almost all subset sum problems of density<0.6463 ... in polynomial time if it could invoke a polynomial-time algorithm for finding the shortest non-zero vector in a lattice. This paper presents two modifications of that algorithm, either one of which would solve almost all problems of density<0.9408 ... if it could find shortest non-zero vectors in lattices. These modifications also yield dramatic improvements in practice when they are combined with known lattice basis reduction algorithms.	algorithm;jeffrey lagarias;lattice reduction;np-completeness;polynomial;subset sum problem;time complexity	Matthijs J. Coster;Antoine Joux;Brian A. LaMacchia;Andrew M. Odlyzko;Claus-Peter Schnorr;Jacques Stern	1992	computational complexity	10.1007/BF01201999	time complexity;mathematical optimization;combinatorics;discrete mathematics;computer science;lattice;mathematics;subset sum problem;algorithm;lattice problem	Theory	12.200594555401672	22.89975990752724	89947
4f8535b380a9c21ece22049280bbdf4ab765d17b	approximate string matching using compressed suffix arrays	sufijo;approximate string matching;text;espace lineaire;suffix;edit distance;interrogation base donnee;espacio lineal;chaine caractere;interrogacion base datos;texte;suffix array;combinatorial problem;probleme combinatoire;problema combinatorio;hamming distance;indexing;pattern matching;indexation;estructura datos;cadena caracter;distance hamming;indizacion;structure donnee;appariement chaine;concordance forme;suffixe;string matching;texto;linear space;data structure;database query;distancia hamming;character string	Let T be a text of length n and P be a pattern of length m, both strings over a fixed finite alphabet A. The k-difference (k-mismatch, respectively) problem is to find all occurrences of P in T that have edit distance (Hamming distance, respectively) at most k from P. In this paper we investigate a well-studied case in which k = 1 and T is fixed and preprocessed into an indexing data structure so that any pattern query can be answered faster [16-19]. This paper gives a solution using O(n) bits indexing data structure with O(m log 2  n) query time. To the best of our knowledge, this is the first result which requires linear indexing space. The results can be extended for the k-difference problem with k > 1.	approximate string matching	Trinh N. D. Huynh;Wing-Kai Hon;Tak Wah Lam;Wing-Kin Sung	2004		10.1007/978-3-540-27801-6_33	search engine indexing;combinatorics;hamming distance;edit distance;approximate string matching;data structure;string;computer science;theoretical computer science;pattern matching;database;mathematics;programming language;algorithm;linear space;string searching algorithm	Theory	13.957430295918673	26.922753461419518	89990
e53df7585bc2f27d15bf299210f432814b9a3d04	practical fpt implementations and applications (invited talk)	estensibilidad;recouvrement graphe;algoritmo paralelo;cubierta grafo;haute performance;pulga de dna;parallel algorithm;securite;methode noyau;puce a dna;branching;bioinformatique;algorithme parallele;optimisation combinatoire;combinatorial problem;probleme combinatoire;graph covering;problema combinatorio;ramificacion;metodo nucleo;safety;recouvrement ensemble;dna chip;alto rendimiento;ramification;kernel method;extensibilite;scalability;bioinformatica;set covering;cubierta conjunto;combinatorial optimization;seguridad;high performance;optimizacion combinatoria;bioinformatics	When combined with high performance computational architectures, methods born of FPT can be used as a practical basis for launching systematic attacks on large-scale combinatorial problems of significance. Efficient sequential techniques for kernelization and highly parallel algorithms for branching will be discussed. The importance of maintaining a balanced decomposition of the search space turns out to be critical to achieving scalability. Applications abound, perhaps most notably in high-throughput computational biology. A toolchain will be described that transforms immense quantities of mRNA microarray data into instances of the clique problem, which are then solved via vertex cover to derive sets of putatively co-regulated genes. This makes it possible to narrow the search for cis and trans regulatory structures on scales that were previously unthinkable.	parameterized complexity	Michael A. Langston	2004		10.1007/978-3-540-28639-4_26	kernel method;combinatorics;scalability;dna microarray;branching;combinatorial optimization;computer science;artificial intelligence;theoretical computer science;mathematics;parallel algorithm;ramification;algorithm	Crypto	16.051892273827587	21.541806381123923	90356
72178b683162e111c8239b9d913e482c8aac4a4f	practical approximation schemes for maximum induced-subgraph problems on k_{3, 3}-free or k_5-free graphs	approximation scheme;polynomial time approximation scheme;planar graph	We show that for an integer k  2 and an n-vertex graph G without a K 3,3 (resp., K 5) minor, we can compute k induced subgraphs of G with treewidth  3k–4 (resp.,  6k–7) in O(kn) (resp., O(kn+n 2)) time such that each vertex of G appears in exactly k – 1 of these subgraphs. This leads to practical polynomial-time approximation schemes for various maximum induced-subgraph problems on graphs without a K 3,3 or K 5 minor. The result extends a well-known result of Baker that there are practical polynomial-time approximation schemes for various maximum induced-subgraph problems on planar graphs.	approximation	Zhi-Zhong Chen	1996		10.1007/3-540-61440-0_134	1-planar graph;mathematical optimization;combinatorics;discrete mathematics;polynomial-time approximation scheme;computer science;k-minimum spanning tree;mathematics;hardness of approximation;minimax approximation algorithm;approximation algorithm;planar graph	Theory	23.160955931589957	22.20722329183119	90598
63e0c7cd8ed827ba1806429ff85a7869d1f064e1	simultaneous source location	graph theory;location problem;probleme localisation;teoria grafo;approximation algorithm;localization;problema np duro;localizacion;theorie graphe;optimisation combinatoire;np hard problem;aleatorizacion;localisation;probleme np difficile;algoritmo aproximacion;randomisation;treewidth;problema localizacion;anchura arbol;algorithme approximation;randomization;combinatorial optimization;largeur arborescente;optimizacion combinatoria	We consider the problem of Simultaneous Source Location – selecting locations for sources in a capacitated graph such that a given set of demands can be satisfied. We give an exact algorithm for trees and show how this can be combined with a result of Räcke to give a solution that exceeds edge capacities by at most O(log n log logn), where n is the number of nodes. On graphs of bounded treewidth, we show the problem is still NP-Hard, but we are able to give a PTAS with at most O(1+ ) violation of the capacities, or a (k+1)-approximation with exact capacities, where k is the treewidth and can be made arbitrarily small.	exact algorithm;np-hardness;ptas reduction;treewidth	Konstantin Andreev;Charles Garrod;Bruce M. Maggs;Adam Meyerson	2004		10.1007/978-3-540-27821-4_2	randomization;mathematical optimization;combinatorics;internationalization and localization;combinatorial optimization;graph theory;np-hard;mathematics;treewidth;approximation algorithm;algorithm	Theory	20.903630548548232	26.38481956048044	90653
39a17d65451cf80e73ec1b46270b28111eba4fdf	lift and project relaxations for the matching and related polytopes	covering;complet;polyhedral combinatorics;polyedre;combinatorics;apareamiento perfecto;procedimiento;combinatoria;poliedro;nudo;perfect matching;combinatoire;recubrimiento;projection method;overlay;polyhedron;index;pairing;methode;result;recouvrement;relajacion;algorithme;politope;algorithm;completo;probleme recouvrement;problema recubrimiento;indice;matching;indexation;68r10;resultado;relaxation;resultat;couplage parfait;noeud;sequential tightening procedures;emparejamiento;covering problem;grafo completo;appariement;complete graph;graphe complet;metodo;node;method;complete;procedure;algoritmo;polytope;perfect match	We compare lift and project methods given by Lovasz and Schrijver (the N+ and N procedures) and by Balas, Ceria and Cornuejols (the disjunctive procedure) when working on the matching, perfect matching and covering polytopes. When the underlying graph is the complete graph of n=2s + 1 nodes we obtain that the disjunctive index for all problems is s2, the N+-index for the matching and perfect matching problems is s (extending a result by Stephen and Tuncel), the N-index for the perfect matching problem is s, and the N+ and N indices for the covering problem and the N-index for the matching problem are strictly greater than s.		Néstor E. Aguilera;Silvia M. Bianchi;Graciela L. Nasini	2004	Discrete Applied Mathematics	10.1016/S0166-218X(03)00337-8	complete;matching;polytope;procedure;mathematical optimization;factor-critical graph;combinatorics;discrete mathematics;method;polyhedral combinatorics;3-dimensional matching;relaxation;pairing;mathematics;overlay;projection method;node;complete graph;algorithm;matching;polyhedron	ML	23.281817885959704	29.897743116982372	90954
81e21f15d78a6be2e85d589076d6f4efd9bcb65e	exact solution in linear time of networks of constraints using perfect relaxation	exact solution;linear time			Francesca Rossi;Ugo Montanari	1989			time complexity;mathematical optimization;computer science	ML	19.046752713624958	25.269085243202806	91133
4e6e099a375d45e0ee34cdb2f473a296c5f29a1c	the extremal values of the wiener index of a tree with given degree sequence	optimisation;characteristic;combinatorics;maximo;tree;optimizacion;05c05;bepress selected works;combinatoria;arbol;combinatoire;index;maximum;caracteristica;sequence degre;degree sequence;extreme value;wiener index;indice;informatique theorique;68r10;distancia;arbre;caracteristique;optimization;tree wiener index degree sequence;distance;computer theory;informatica teorica	The Wiener index of a graph is the sum of the distances between all pairs of vertices, it has been one of the main descriptors that correlate a chemical compound’s molecular graph with experimentally gathered data regarding the compound’s characteristics. In [1], the tree that minimizes the Wiener index among trees of given maximal degree is studied. We characterize trees that achieve the maximum and minimum Wiener index, given the number of vertices and the degree sequence. Keyword: tree, Wiener index, degree sequence	degree (graph theory);experiment;maxima and minima;maximal set;molecular graph;wiener index	Hua Wang	2008	Discrete Applied Mathematics	10.1016/j.dam.2007.11.005	mathematical optimization;combinatorics;discrete mathematics;wiener index;topology;extreme value theory;mathematics;tree;characteristic;distance	Theory	23.82720556346675	31.063387895676243	91417
6bc37cf46c4c9bbe5268a4241945d33ff68cb6f4	reducing dense virtual networks for fast embedding	heuristics network virtualization virtual network embedding;time complexity indium phosphide iii v semiconductor materials measurement manganese virtualization engines;virtual network embedding;virtualisation computational complexity computer networks numerical analysis telecommunication traffic;network virtualization;numerical evaluation virtual network embedding time complexity virtual links embedding cost embedding time;heuristics	Virtual network embedding has been intensively studied for a decade. The time complexity of most conventional methods has been reduced to the cube of the number of links. Since customers are likely to request a dense virtual network that connects every node pair directly (|E| = O(|V|2)) based on a traffic matrix, the time complexity is actually O(|E|3 = |V|6). If we were allowed to reduce this dense network into a sparse one before embedding, the time complexity could be decreased to O(|V|3); the time gap can be a million times for |V| = 100. The network reduction, however, combines several virtual links into a broader link, which makes the embedding cost (solution quality) much worse. This paper analytically and empirically investigates the trade-off between the embedding time and cost for the virtual network reduction. We define two simple reduction algorithms and analyze them with several interesting theorems. The analysis indicates that the embedding cost increases only linearly with exponential decay of embedding time. Thorough numerical evaluation justifies the desirability of the trade-off.	algorithm;cloud computing;cobham's thesis;experiment;exponential time hypothesis;numerical analysis;preprocessor;sparse matrix;time complexity	Toru Mano;Takeru Inoue;Kimihiro Mizutani;Osamu Akashi	2016	IEEE INFOCOM 2016 - The 35th Annual IEEE International Conference on Computer Communications	10.1109/INFOCOM.2016.7524412	real-time computing;computer science;theoretical computer science;operating system;heuristics;distributed computing;computer network	HPC	22.263613338713224	18.3674623509547	91725
486e405b546d46b6aa27e2db8f5ea607d5df2b78	parameterized dominating set problem in chordal graphs: complexity and lower bound	graph theory;grafo triangular;teoria grafo;parameterized complexity;graphe intervalle;interval graph;grafo intervalo;theorie graphe;optimisation combinatoire;upper bound;dominating set;interval graphs;borne inferieure;graphe triangule;conjunto dominando;borne superieure;combinatorial optimization;lower bound;chordal graphs;cota superior;ensemble dominant;cota inferior;optimizacion combinatoria;chordal graph	In this paper, we study the parameterized dominating set problem in chordal graphs. The goal of the problem is to determine whether a given chordal graph G = (V ,E) contains a dominating set of size k or not, where k is an integer parameter. We show that the problem is W[1]-hard and it cannot be solved in time |V |o( √ k) unless 3SAT can be solved in subexponential time. In addition, we show that the upper bound of this problem can be improved to O(2 √ k|V |2 √ ) when the underlying graph G is an interval graph.	directed graph;dominating set;graph (discrete mathematics);parameterized complexity;time complexity	Chunmei Liu;Yinglei Song	2009	J. Comb. Optim.	10.1007/s10878-008-9141-5	mathematical optimization;combinatorics;discrete mathematics;topology;combinatorial optimization;graph theory;mathematics;upper and lower bounds	Theory	21.384973106372897	26.691229775022922	91765
5e6265860c8a5261e6a9994fd1b3d5cc6584fb0b	algorithms for routing around a rectangle	graph theory;teoria grafo;routing;theorie graphe;algorithme;algorithm;encaminamiento;rectangle;acheminement;algoritmo	Frank, A., T. Nishizeki, N. Saito, H. Suzuki and E. Tardos, Algorithms for routing around a rectangle, Discrete Applied Mathematics 40 (1992) 363-378. Simple efficient algorithms are given for three routing problems around a rectangle. The algorithms find routing in two or three layers for two-terminal nets specified on the sides of a rectangle. All algorithms run in linear time. One of the three routing problems is the minimum area routing previously considered by LaPaugh and Gonzalez and Lee. The algorithms they developed run in time O(n’) and O(n) respectively. Our simple linear time algorithm is based on a theorem of Okamura and Seymour and on a data structure developed by Suzuki, Ishiguro and Nishizeki.	algorithm;data structure;itakura–saito distance;routing;time complexity	András Frank;Takao Nishizeki;Nobuji Saito;Hitoshi Suzuki;Éva Tardos	1992	Discrete Applied Mathematics	10.1016/0166-218X(92)90007-W	mathematical optimization;routing;combinatorics;graph theory;mathematics;algorithm	Theory	20.414639153429412	26.32210965732586	91794
fa1a28e2886545e7a7565b86597994268024a0eb	a linear time algorithm for finding all hinge vertices of a permutation graph	camino mas corto;graph theory;shortest path;shortest paths;temps lineaire;linear time algorithm;plus court chemin;theorie graphe;permutation;algorithme;permutation graph;linear time;permutacion;communication cost;algorithms;permutation graphs;neighborhood;hinge vertices	If the distance of any two vertices becomes longer after a vertex u is removed, then u is called a hinge vertex. The fault of a hinge vertex will increase the overall communication cost in a network. Therefore, finding the set of all hinge vertices in a graph can be used to identify critical nodes in a real network. We shall propose a linear time algorithm for finding all hinge vertices of a permutation graph.	algorithm;time complexity;vertex (geometry)	Ting-Yem Ho;Yue-Li Wang;Ming-Tsan Juan	1996	Inf. Process. Lett.	10.1016/0020-0190(96)00092-0	loop;time complexity;graph power;combinatorics;discrete mathematics;independent set;topology;feedback vertex set;level structure;graph center;degree;graph theory;permutation graph;hypercube graph;cycle graph;vertex;path graph;mathematics;biconnected graph;permutation;path;shortest path problem;wheel graph;bound graph;distance;neighbourhood;algorithm	Theory	21.669780261926928	29.362905939913105	92040
dff2418cdaef4f86651b60176860d11f175c2e86	efficient approximation schemes for maximization problems on k3,3-free or k5-free graphs	graph theory;subgrafo;temps polynomial;maximization;theorie graphe;algorithme;sous graphe;graphe planaire;vertex graph;polynomial time;approximation scheme;algorithms;subgraph;grafo planario;maximizacion;vertice grafo;polynomial time approximation scheme;planar graph;sommet graphe;maximisation;tiempo polinomial	We show that for any integerk?2 and anyn-vertex graphGwithout aK3,3(orK5) minor, one can computekinduced subgraphs ofGwith treewidth no more than 3k?4 (respectively, 6k?7) inO(kn) (respectively,O(kn+n2)) time such that each vertex ofGappears in exactlyk?1 of these subgraphs. This leads topracticalpolynomial-time approximation schemes for various maximum induced-subgraph problems on graphs without aK3,3(respectively,K5) minor. The result extends the well-known practical polynomial-time approximation schemes of Baker for various maximum induced-subgraph problems onplanargraphs.	approximation;expectation–maximization algorithm	Zhi-Zhong Chen	1998	J. Algorithms	10.1006/jagm.1997.0894	time complexity;mathematical optimization;combinatorics;discrete mathematics;polynomial-time approximation scheme;graph theory;vertex;mathematics;algorithm;planar graph	Theory	21.459203806944007	26.571878040817108	92067
323bf000c690d53f4078b531ffad88e74612888a	on-line suffix tree construction with reduced branching	branching;suffix tree;sliding window;construction algorithm	Classical suffix tree construction algorithms by McCreight and Ukkonen spend most of the time looking up the right branch to follow from the current node. However, not all these slow branching operations are necessary. A significant portion of them is used for implicit suffix link simulation and can be avoided by replacing the traditional top-down descent with bottom-up climbing. We describe the bottom-up approach and analyze its costs and benefits. An experimental evaluation on two standard data corpora shows that bottom-up climbing removes forty to sixty six percent of branching operations and consequently saves twenty one to thirty two percent of construction time. However, a theoretical analysis of the worst-case behavior reveals that the time complexity of the bottom-up approach is superlinear. This is remedied by a combination of both approaches that removes nearly as many branching operations as the bottom-up climb, but still runs in linear time like the top-down descent.	suffix tree	Martin Senft;Tomás Dvorák	2012	J. Discrete Algorithms	10.1016/j.jda.2012.01.001	sliding window protocol;mathematical optimization;combinatorics;branching;computer science;theoretical computer science;mathematics;algorithm	Theory	11.429716880184275	27.317172695826127	92113
fd1df8f0cbf289c1ec97261ac86d672093e12231	complexity of ising polynomials	partition function;statistical mechanics;tutte polynomial;external field;potts model;phase transition;computational complexity;82b20;primary 05c31;polynomial time;ising model;secondary 05c85	This paper deals with the partition function of the Ising model from statistical mechanics, which is used to study phase transitions in physical systems. A special case of interest is that of the Ising model with constant energies and external field. One may consider such an Ising system as a simple graph together with vertex and edge weights. When these weights are considered indeterminates, the partition function for the constant case is a trivariate polynomial Z(G; x, y, z). This polynomial was studied with respect to its approximability by L. generalizes a bivariate polynomial Z(G; t, y), which was studied in by D. Andrén and K. Markström in [1]. We consider the complexity of Z(G; t, y) and Z(G; x, y, z) in comparison to that of the Tutte polynomial, which is well-known to be closely related to the Potts model in the absence of an external field. We show that Z(G; x, y, z) is #P-hard to evaluate at all points in Q 3 , except those in an exceptional set of low dimension, even when restricted to simple graphs which are bipartite and planar. A counting version of the Exponential Time Hypothesis, #ETH, was introduced by H. Dell, T. Husfeldt and M. Wahlén in [6] in order to study the complexity of the Tutte polynomial. In analogy to their results, we give under #ETH a dichotomy theorem stating that evaluations of Z(G; t, y) either take exponential time in the number of vertices of G to compute, or can be done in polynomial time. Finally, we give an algorithm for computing Z(G; x, y, z) in polynomial time on graphs of bounded clique-width, which is not known in the case of the Tutte polynomial.	approximation algorithm;clique-width;exponential time hypothesis;graph (discrete mathematics);ising model;p (complexity);partition function (mathematics);planar graph;potts model;time complexity;tutte polynomial	Tomer Kotek	2012	Combinatorics, Probability & Computing	10.1017/S0963548312000259	phase transition;ising model;time complexity;chromatic polynomial;combinatorics;potts model;discrete mathematics;statistical mechanics;stable polynomial;degree of a polynomial;mathematics;tutte polynomial;matrix polynomial;partition function;computational complexity theory;square-free polynomial;algebra	Theory	21.129058721667484	22.87307903268584	92367
6d9b1fda7ae3f24e0a4abb08bc2969ee093a7b91	learning dfa from simple examples	collusion;exact identification;pac learning;artificial intelligent;characteristic sets;kolmogorov complexity;learning from examples;dfa inference;grammatical inference;uniform distribution;canonical representation	Efficient learning of DFA is a challenging research problem in grammatical inference. It is known that both exact and approximate (in the PAC sense) identifiability of DFA is hard. Pitt has posed the following open research problem: “Are DFA PAC-identifiable if examples are drawn from the uniform distribution, or some other known simple distribution?” (Pitt, in Lecture Notes in Artificial Intelligence, 397, pp. 18–44, Springer-Verlag, 1989). We demonstrate that the class of DFA whose canonical representations have logarithmic Kolmogorov complexity is efficiently PAC learnable under the Solomonoff Levin universal distribution (m). We prove that the class of DFA is efficiently learnable under the PACS (PAC learning with simple examples) model (Denis, D'Halluin & Gilleron, STACS'96—Proceedings of the 13th Annual Symposium on the Theoretical Aspects of Computer Science, pp. 231–242, 1996) wherein positive and negative examples are sampled according to the universal distribution conditional on a description of the target concept. Further, we show that any concept that is learnable under Gold's model of learning from characteristic samples, Goldman and Mathias' polynomial teachability model, and the model of learning from example based queries is also learnable under the PACS model.	approximation algorithm;artificial intelligence;grammar induction;integrated circuit layout design protection;kolmogorov complexity;lecture notes in computer science;open research;physics and astronomy classification scheme;polynomial;probably approximately correct learning;ray solomonoff;springer (tank)	Rajesh Parekh;Vasant Honavar	2001	Machine Learning	10.1023/A:1010822518073	canonical form;computer science;artificial intelligence;machine learning;mathematics;uniform distribution;probably approximately correct learning;algorithm;statistics	Theory	10.060282522080671	19.828528978530134	92397
48fdad6cf2e341f55d961ca201b45cdd5f6575e2	edit distance of run-length coded strings	time complexity;edit distance;data structure	We give an algorithm for measuring the similarity of runlength coded strings. In run-length coding, not all individual symbols in a string are listed. Instead, one run of identical consecutive symbols is coded by giving one representative symbol together with its multiplicity. If the strings under consideration consist of long runs of identical symbols, significant reductions in memory and access time can be achieved by run-length coding. Our algorithm determines the minimum coat sequence of edit operations needed to transform one string into another. It uses as basic data structure an edit matrix similar to the classical algorithm of Wagner and Fischer [1]. However, dependh-ig on the particular pair of strings to be compared, only a part of this edit matrix usually needs to be computed. In the worst case, our algorithm has a time complexity of O(n . m), where n and m give the lengths of the strings to be compared. In the best case, the time complexity is O(k. /), where k and/ are the numbers of runs of identical symbols in the two strings under comparison,	access time;best, worst and average case;class diagram;data structure;edit distance;michael j. fischer;run-length encoding;time complexity;wagner–fischer algorithm	Horst Bunke;János Csirik	1992		10.1145/143559.143625	damerau–levenshtein distance;edit distance;wagner–fischer algorithm;string-to-string correction problem;jaro–winkler distance	Theory	13.513812330284138	27.13756549137692	92498
2e7cf2bb1eb225807a7c8226d4b839349acaf24e	on monochromatic component size for improper colourings	maximum degree;optimisation;coloracion grafo;graph partition;combinatorics;algoritmo busqueda;local search algorithm;optimizacion;combinatoria;algorithme recherche;metacoloration;search algorithm;combinatoire;monocromatico;68wxx;monochromatic components;metacolouring;algorithm;partition graphe;graph partitioning;coloration graphe;monochromatic;informatique theorique;68r10;metacoloring;monochromatique;optimization;algoritmo optimo;algorithme optimal;optimal algorithm;graph colouring;computer theory;graphe colore;informatica teorica;05c15	This paper concerns improper λ-colourings of graphs and focuses on the sizes of the monochromatic components (i.e., components of the subgraphs induced by the colour classes). Consider the following three simple operations, which should, heuristically, help reduce monochromatic component size: (a) assign to a vertex the colour that is least popular among its neighbours; (b) change the colours of any two adjacent differently coloured vertices, if doing so reduces the number of monochromatic edges; and (c) change the colour of a vertex, if by so doing you can reduce the size of the largest monochromatic component containing it without increasing the number of monochromatic edges. If a colouring cannot be further improved by these operations, then we regard it as locally optimal. We show that, for such a locally optimal 2-colouring of a graph of maximum degree 4, the maximum monochromatic component size is O(2(2 log2 n) 1/2 ). The operation set (a)–(c) appears to be one of the simplest that achieves a o(n) bound on monochromatic component size. Recent work by Alon, Ding, Oporowski and Vertigan, and then Haxell, Szabó and Tardos, has shown that some algorithms can do much better, achieving a constant bound on monochromatic component size. However, the simplicity of our operation set, and of the associated local search algorithm, make the algorithm, and our locally optimal colourings, of interest in their own right. ∗Some of the work of this paper was done while Farr was visiting: Department of Applied Computing, University of Dundee; Institut für Computergraphik und Algorithmen, Technische Universität Wien; and Department of Computer Science, Royal Holloway, University of London.	binary logarithm;color;computer science;emoticon;graph coloring;heuristic;local optimum;local search (optimization);monochrome;search algorithm	Keith Edwards;Graham Farr	2005	Discrete Applied Mathematics	10.1016/j.dam.2004.10.005	mathematical optimization;combinatorics;discrete mathematics;graph partition;mathematics;geometry;algorithm	Theory	22.09475766120516	26.25199108372209	92948
1e9f615f591597a79b4871644d73eacad64cc364	bijections for refined restricted permutations	permutation statistics;dyck path;fixed point;upper bound;longest increasing subsequence;pattern avoidance;generating function;bijection;restricted permutations;restricted permutation	We present a bijection between 321and 132-avoiding permutations that preserves the number of fixed points and the number of excedances. This gives a simple combinatorial proof of recent results of Robertson, Saracino and Zeilberger [10], and the first author [4]. We also show that our bijection preserves additional statistics, which extends the previous	fixed point (mathematics)	Sergi Elizalde;Igor Pak	2004	J. Comb. Theory, Ser. A	10.1016/j.jcta.2003.10.009	partial permutation;generating function;combinatorics;discrete mathematics;bijection;topology;longest increasing subsequence;mathematics;fixed point;upper and lower bounds	Theory	15.356247774775836	20.717178119255383	93000
cb950ab3de114a810c14550563c68c637f810b14	linear time bounds for median computations	arithmetic operations;algebraic algorithms;linear functionals;upper bound;modular arithmetic;linear time;upper and lower bounds;rational functions;lower bound	New upper and lower bounds are presented for the maximum number of comparisons, f(i,n), required to select the i-th largest of n numbers. An upper bound is found, by an analysis of a new selection algorithm, to be a linear function of n:  f(i,n) ≤ 103n/18 < 5.73n, for 1 ≤ i ≤ n.  A lower bound is shown deductively to be:  f(i,n) ≥ n+min(i,n−i+l) + [log<subscrpt>2</subscrpt>(n)] − 4,  for 2 ≤ i ≤ n−1,  or, for the case of computing medians:  f([n/2],n) ≥ 3n/2 − 3	computation;linear function;selection algorithm;time complexity	Manuel Blum;Robert W. Floyd;Vaughan R. Pratt;Ronald L. Rivest;Robert E. Tarjan	1972		10.1145/800152.804904	mathematical optimization;combinatorics;discrete mathematics;mathematics;upper and lower bounds;algorithm	Theory	23.97088790945248	25.9276624459179	93108
1e6d19c53f4851e1227ca6e22c72b5dd4ddca5b8	logical complexity of graphs: a survey	random graph;qa mathematics;zero one law;computational complexity;logic in computer science;first order logic	We discuss the definability of finite graphs in first-order logic with two relation symbols for adjacency and equality of vertices. The logical depth D(G) of a graphG is equal to the minimum quantifier depth of a sentence defining G up to isomorphism. The logical width W (G) is the minimum number of variables occurring in such a sentence. The logical length L(G) is the length of a shortest defining sentence. We survey known estimates for these graph parameters and discuss their relations to other topics (such as the efficiency of the Weisfeiler-Lehman algorithm in isomorphism testing, the evolution of a random graph, quantitative characteristics of the zero-one law, or the contribution of Frank Ramsey to the research on Hilbert’s Entscheidungsproblem). Also, we trace the behavior of the descriptive complexity of a graph as the logic becomes more restrictive (for example, only definitions with a bounded number of variables or quantifier alternations are allowed) or more expressible (after powering with counting quantifiers).	algorithm;descriptive complexity theory;entscheidungsproblem;first-order logic;first-order predicate;level of measurement;logical depth;quantifier (logic);random graph	Oleg Pikhurko;Oleg Verbitsky	2004	CoRR		graph power;random regular graph;pathwidth;random graph;combinatorics;discrete mathematics;universal graph;null graph;graph property;graph canonization;degree;clique-width;distance-regular graph;first-order logic;mathematics;voltage graph;geometry;graph;graph isomorphism;graph homomorphism;computational complexity theory;complement graph;line graph;algorithm;string graph;coxeter graph;algebra	Theory	21.639528295300753	32.302624670116145	93121
6b0b2051983a6336846fdf800d899d06292cb37f	the expected advantage of asynchrony	expected advantage	This paper expands on the APRAM model introduced in [CZ89]. It introduces a model under which processes may proceed at different and varying speeds. Using this model the implicit costs of synchronization can be studied. The merit of the model is exhibited by analyzing two key algorithms, parallel summation along an implicit binary tree and recursive doubling, and demonstrating that both asynchronous algorithms perform better then their synchronous counterparts in asynchronous settings.	alok r. chaturvedi;analysis of algorithms;asynchronous i/o;asynchrony (computer programming);binary tree;computation;computer engineering;distributed computing;fault-tolerant computer system;klara kedem;list of sega arcade system boards;marc snir;podc;parallel algorithm;parallel computing;period-doubling bifurcation;pointer jumping;spaa;shared memory;symposium on theory of computing	Richard Cole;Ofer Zajicek	1990		10.1145/97444.97673	generalization;synchronization;combinatorics;computer science;theoretical computer science;mathematics;algorithm	DB	10.340687788802143	31.825961000005908	93185
f135cae08a3e6753229ff9dfc57b172f1004ac25	spelling approximate repeated or common motifs using a suffix tree	algorithm complexity;complejidad algoritmo;biologia molecular;optimisation combinatoire;suffix tree;complexite algorithme;informatique theorique;pattern matching;molecular biology;concordance forme;combinatorial optimization;optimizacion combinatoria;computer theory;biologie moleculaire;informatica teorica	"""We present in this paper two algorithms. The rst one extracts repeated motifs from a sequence deened over an alphabet. For instance, may be equal to fA, C, G, Tg and the sequence represents an encoding of a DNA macromolecule. The motifs searched correspond to words over the same alphabet which occur a minimum number q of times in the sequence with at most e mismatches each time (q is called the quorum constraint). The second algorithm extracts common motifs from a set of N 2 sequences. In this case, the motifs must occur, again with at most e mismatches, in 1 q N distinct sequences of the set. In both cases, the words representing the motifs may never be present exactly in the sequences. We therefore speak of the motifs, repeated in a sequence or common to a set of them, as being \external"""" objects and denote them by the expression \valid models"""" if they verify the quorum constraint q. The approach we introduce here for nding all valid models corresponding to either repeated or common motifs starts by building a suux tree of the sequence(s) and then, after some further preprocess-ing, uses this tree to simply \spell"""" the models. Assuming an alphabet of xed size, the total time needed is O(nN 2 V(e; k)) using O(nN 2 =w) space, where n is the (average) length of the sequence(s), k is the length of the models sought or is the length of the longest possible valid models, w is the size of a word machine and V(e; k) is the number of words of length k at a Hamming distance at most e from another k-length word. V(e; k) may be majored by k e jj e. This improves on an algorithm by Waterman 23]. It is also a better time bound than our previous approach 15] for the common motifs problem whenever N < kjj, and a better space bound when N=w < k. It is a better time and space bound in absolute for the repeated motifs problem. The complexities obtained in this second case are O(nV(e; k)) and O(n) respectively. Finally, we suggest how to extend these algorithms to deal with gaps."""	dna microarray;hamming distance;preprocessor;sequence motif;smith–waterman algorithm;suffix tree	Marie-France Sagot	1998		10.1007/BFb0054337	arithmetic;mathematical optimization;combinatorics;combinatorial optimization;computer science;artificial intelligence;machine learning;pattern matching;mathematics;programming language;algorithm;statistics	Theory	14.40423398902663	26.083743823131627	93191
bf6d491e0135fbcf3170f0d8efe4199dfad70522	an algorithm for reliability analysis of planar graphs	graph theory;teoria grafo;algorithm complexity;complejidad algoritmo;theorie graphe;connected graph;construction graphe;grafico planario;complexite algorithme;q175 m41 o616 no 141 85;graphe planaire;vertex graph;construccion diagrama;reliability analysis;cuspide grafico;graphe connexe;graph construction;planar graph;sommet graphe;grafico connexo	The K-terminal reliability problem can be stated as follows: given an undirected graph G = (V, E ) whose arcs are erased independently with known probabilities, and a subset K of V , compute the probability that K remains connected. This problem is known to be #P-hard [l], and, not surprisingly, the best algorithm known to date has complexity that is strictly exponential in IV( [4]. The case of planar G has received attention recently. Even though this restriction of the general problem is still #P-hard, even when IKI = 2 [ l l ] , the complexity of the case K = V is still unknown. In this paper we present an algorithm for computing K-terminal reliability of planar graphs, whose complexity is at most strictly exponential in the square root of I VI, a large improvement over the general case. Our algorithm uses elements of two closely related reliability algorithms, those of Rosenthal [12] and Fratta and Montanari [6], as well as Miller’s version of the planar separator theorem [lo] and pertinent properties of planar graphs. This paper is organized as follows: in Section 2 we describe the Rosenthal, Fratta and Montanari approach; Sections 3 and 4 present relevant characteristics of planar graphs and Section 5 describes our algorithm. Section 6 analyzes the complexity of this algorithm and Section 7 presents some extensions.	algorithm;graph (discrete mathematics);p (complexity);planar graph;planar separator theorem;relevance;reliability engineering;sharp-p-complete;time complexity	Daniel Blenstock	1986	Networks	10.1002/net.3230160407	combinatorics;discrete mathematics;topology;connectivity;graph theory;vertex;mathematics;chordal graph;planar graph	Theory	23.272483275424136	28.458410341195876	93370
8ed63d7b09189b87b9097e8ee81b1509b4ebb5dd	nearly optimal language compression using extractors	graphe biparti;complexite calcul;grafo bipartido;complexite kolmogorov;recursive function;complejidad computacion;kolmogorov complexity;computational complexity;informatique theorique;funcion recursiva;polynomial time;fonction recursive;bipartite graph;computer theory;informatica teorica	We show two sets of results applying the theory of extractors to resource-bounded Kolmogorov complexity: Most strings in easy sets have nearly optimal polynomial-time CD complexity. This extends work of Sipser Sip83] and Buhrman and Fortnow BF97]. We use extractors to extract the randomness of strings. In particular we show how to get from an arbitrary string, an incompressible string which encodes almost as much polynomial-time CND complexity as the original string.	incompressible string;kolmogorov complexity;michael sipser;polynomial;randomness;time complexity	Lance Fortnow;Sophie Laplante	1998		10.1007/BFb0028551	kolmogorov structure function;time complexity;combinatorics;discrete mathematics;bipartite graph;computer science;mathematics;computational complexity theory;algorithm	Theory	10.435488947930232	25.19531166325302	93523
d3c261b5626cf98677aa51b89ea5dfa63eeee634	a network-flow-based scheduler: design, performance history, and experimental analysis	experimental analysis;search space;program design;graphs;computer experiment;scheduling;matroids;network flow	We describe a program that schedules physician attending teamsat Denver Health Medical Center. The program uses network flowtechniques to prune an exponentially sized search space. Wedescribe the program design, its performance history at thehospital, and experiments on a simplified version of theprogram.	experiment;flow network;scheduling (computing)	Harold N. Gabow;Tadayoshi Kohno	2001	ACM Journal of Experimental Algorithmics	10.1145/945394.945397	matroid;mathematical optimization;combinatorics;flow network;simulation;computer experiment;computer science;theoretical computer science;program design language;mathematics;graph;scheduling;algorithm;experimental analysis of behavior	PL	20.203654658757667	19.735134983567505	93593
96b57486a64c1b1f126bf73d8c549dad50603193	efficient algorithms and data structures for massive data sets	minimum cut;priority queue;lower bound;external memory;maximal independent set;computer model;minimum spanning tree;planar graph;vertex cover;data structure;interval graph	For many algorithmic problems, traditional algorithms that optimise on the number of instructions executed prove expensive on I/Os. Novel and very different design techniques, when applied to these problems, can produce algorithms that are I/O efficient. This thesis adds to the growing chorus of such results. The computational models we use are the external memory model and the W-Stream model. On the external memory model, we obtain the following results. (1) An I/O efficient algorithm for computing minimum spanning trees of graphs that improves on the performance of the best known algorithm. (2) The first external memory version of soft heap, an approximate meldable priority queue. (3) Hard heap, the first meldable external memory priority queue that matches the amortised I/O performance of the known external memory priority queues, while allowing a meld operation at the same amortised cost. (4) I/O efficient exact, approximate and randomised algorithms for the minimum cut problem, which has not been explored before on the external memory model. (5) Some lower and upper bounds on I/Os for interval graphs. On the W-Stream model, we obtain the following results. (1) Algorithms for various tree problems and list ranking that match the performance of the best known algorithms and are easier to implement than them. (2) Pass efficient algorithms for sorting, and the maximal independent set problems, that improve on the best known algorithms. (3) Pass efficient algorithms for the graphs problems of finding vertex-colouring, approximate single source shortest paths, maximal matching, and approximate weighted vertex cover. (4) Lower bounds on passes for list ranking and maximal matching. We propose two variants of the W-Stream model, and design algorithms for the maximal independent set, vertex-colouring, and planar graph single source shortest paths problems on those models.	approximation algorithm;boolean algebra;chorusos;computation;computational model;file spanning;graph coloring;independent set (graph theory);input/output;list ranking;matching (graph theory);maximal independent set;maximal set;meld (software);minimum cut;minimum spanning tree;planar graph;priority queue;shortest path problem;soft heap;sorting;vertex cover	Alka	2010	CoRR			Theory	19.58609594058465	24.966255147564663	93712
a8a6ae95d71a6def399f6a0ad9d1636aea161dc4	erdos-renyi sequences and deterministic construction of expanding cayley graphs		Given a finite group G by its multiplication table as input, we give a deterministic polynomial-time construction of a directed Cayley graph on G with O(log |G|) generators, which has a rapid mixing property and a constant spectral expansion. We prove a similar result in the undirected case, and give a new deterministic polynomialtime construction of an expanding Cayley graph with O(log |G|) generators, for any group G given by its multiplication table. This gives a completely different and elementary proof of a result of Wigderson and Xiao [10]. For any finite group G given by a multiplication table, we give a deterministic polynomialtime construction of a cube generating sequence that gives a distribution on G which is arbitrarily close to the uniform distribution. This derandomizes the well-known construction of Erdös-Rényi sequences [2].	graph (discrete mathematics);magma;polynomial;randomized algorithm;time complexity;whole earth 'lectronic link	Vikraman Arvind;Partha Mukhopadhyay;Prajakta Nimbhorkar	2011		10.1007/978-3-642-29344-3_4	combinatorics;discrete mathematics;cayley graph;mathematics;cayley transform;algebra	Theory	11.019544767289831	23.537745868126965	93807
1485a61dfb5a8421ef007ed3db615df4cd855963	approximation of conp sets by np-complete sets and its applications		It is said that a set L1 in a class C1 is an approximation of a set L2 in a class C2 if L1 is a subset of L2. An approximation L1 is said to be optimal if there is no approximation L1 g such that L1  L1 g and L1 g L1 is infinite. When the class C1 = P and C2 = NP, it is known that there is no optimal approximation under a quite general condition unless P = NP. In this paper we discuss the case where C1 = the class of NP-complete sets and C2 = coNP. A similar result as above that shows the difficulty of the optimal approximation is obtained. Approximating coNP sets by NP-complete sets plays an important role in the efficient generation of test instances for combinatorial algorithms. © 1999 Scripta Technica, Syst Comp Jpn, 30(7): 4754, 1999	algorithm;approximation;co-np;combinatorial optimization;digi-comp i;np-completeness;p versus np problem;rough set	Shuichi Miyazaki;Kazuo Iwama	1999	Systems and Computers in Japan	10.1002/(SICI)1520-684X(19990630)30:7%3C47::AID-SCJ6%3E3.0.CO;2-7	np-complete;machine learning;discrete mathematics;artificial intelligence;computer science	Theory	16.497424318649433	19.4361052562549	93874
957d21d269054dcfc36cc8f98dae3ab944487920	the shortest vector problem in lattices with many cycles	lenstra lenstra lavasz algorithm;shortest vector problem;probleme vecteur le plus court;probleme np complet;number theory;enrejado;algorithme lll;treillis;theorie nombre;problema np completo;lll algorithm;teoria numeros;np complete problem;lattice	In this paper we investigate how the complexity of the shortest vector problem in a lattice Λ depends on the cycle structure of the additive group Z/Λ. We give a proof that the shortest vector problem is NP-complete in the max-norm for n-dimensional lattices Λ where Z/Λ has n−1 cycles. We also give experimental data that show that the LLL algorithm does not perform significantly better on lattices with a high number of cycles.	lattice problem;lenstra–lenstra–lovász lattice basis reduction algorithm;np-completeness;utility functions on indivisible goods	Mårten Trolin	2001		10.1007/3-540-44670-2_14	combinatorics;number theory;np-complete;lattice;mathematics;algorithm;lattice problem	Theory	18.148219926579284	26.64702880604036	93926
4fad4759c80162408132fa20b41154626594eeaa	symmetry-based search space reduction for grid maps	search space;artificial intelligent	In this paper we explore a symmetry-based search space reduction technique which can speed up optimal pathfinding on undirected uniform-cost grid maps by up to 38 times. Our technique decomposes grid maps into a set of empty rectangles, removing from each rectangle all interior nodes and possibly some from along the perimeter. We then add a series of macro-edges between selected pairs of remaining perimeter nodes to facilitate provably optimal traversal through each rectangle. We also develop a novel online pruning technique to further speed up search. Our algorithm is fast, memory efficient and retains the same optimality and completeness guarantees as searching on an unmodified grid map.	a* search algorithm;algorithm;branching factor;connectivity (graph theory);graph (discrete mathematics);heuristic (computer science);map;pathfinding;perimeter;the australian;tree traversal	Daniel Harabor;Adi Botea;Philip Kilby	2011	CoRR		mathematical optimization;combinatorics;discrete mathematics;searching the conformational space for docking;computer science;artificial intelligence;machine learning;mathematics	HPC	20.415817647482857	23.278515300218032	93997
edd7f99e8c8e599b6ad89d08ea9aa799ac9d7a8c	dna tile assembly model for 0–1 knapsack problem	dna;self assembly;knapsack problem self assembly dna tile tile assembly model;parallel computing ability;optimisation;biocomputing;knapsack problems;nondeterministic guess system;dna tile;sticky end association;0 1 knapsack problem;tile assembly model;knapsack problem;optimisation biocomputing combinatorial mathematics computational complexity knapsack problems;computational complexity;dna tiles;polynomial time;parallel computer;dna computing;tiles;combinatorial optimization;comparator system;dna tile assembly model;dna self assembly model;combinatorial mathematics;adder system;combinatorial optimization dna tile assembly model 0 1 knapsack problem np complete problem dna self assembly model parallel computing ability dna computing sticky end association nondeterministic guess system adder system comparator system polynomial time;np complete problem	Research results shows that reasonable solution for NP-complete problem could be achieved using DNA self-assembly model, in which the parallel computing ability of DNA computation could be get a full play. In DNA computing paradigm, the information is encoded in DNA tiles, which can be self-assembled via sticky-end associations. In this paper, the DNA self-assembly model for 0–1 knapsack problem is constructed. This model is composed of three units: nondeterministic guess system, adder system and comparator system. Results shows that the three systems can be carried out in polynomial time with optimal 0(1) distinct tile types in parallel. All of these demonstrate the feasibility of DNA tiles self-assembly for NP-problems.	adder (electronics);algorithm;comparator;computation;dna computing;knapsack problem;np-completeness;parallel computing;polynomial;programming paradigm;self-assembly;sticky bit;time complexity	Yanfeng Wang;Weili Lu;Xuncai Zhang;Guangzhao Cui	2010	2010 IEEE Fifth International Conference on Bio-Inspired Computing: Theories and Applications (BIC-TA)	10.1109/BICTA.2010.5645332	continuous knapsack problem;combinatorics;theoretical computer science;mathematics;algorithm	Robotics	15.949826852210535	22.145716497461514	94014
b2e5f2ebcdf1b2e9aa91fda9992bed67dd09e93b	towards a syntactic characterization of ptas	optimization problem;polynomial time	The class PTAS is defined to consist of all NP optimization problems that permit polynomial-time approximation schemes. This paper explores the possibility that a core of PTAS may be characterized through syntactic classes endowed with restrictions on the structure of the input instances. Recent work in approximability of NP-hard problems has led to the identification of a syntactic class called MAX SNP as the core of APX, the class of constant-factor approximable NP optimization problems. This has enhanced our understanding of these classes from both an algorithmic and a complexity-theoretic point of view. Our work is motivated by the hope that a similar understanding can be attained for PTAS. We argue that while the core of APX is the purely syntactic class MAX SNP, in the case of PTAS we must identify the core in terms of syntactic prescriptions for the problem definition augmented with structural restrictions on the input instances. Specifically, we propose such a unified framework based on syntactic classes restricted to instances exhibiting a planar structure. A variety of known and some new results follow directly from this framework, thereby lending credence to our hypothesis that there exists some common structural underpinnings for problems in PTAS.	apx;approximation algorithm;mathematical optimization;max;np (complexity);np-hardness;optimization problem;ptas reduction;polynomial;polynomial-time approximation scheme;snp (complexity);theory;time complexity;unified framework	Sanjeev Khanna;Rajeev Motwani	1996		10.1145/237814.237979	time complexity;optimization problem;mathematical optimization;combinatorics;discrete mathematics;computer science;mathematics;algorithm	Theory	18.750401634779234	18.769514411435352	94072
2ad330a5e4acf95fe777af9f00b9dc51bf36a37e	a constant-time optimal parallel string-matching algorithm	parallel algorithm;timing optimization;string matching	Given a pattern string, we describe a way to preprocess it. We design a constant-time optimal parallel algorithm for finding all occurences of the (preprocessed) pattern in any given text.	parallel algorithm;preprocessor;string searching algorithm	Zvi Galil	1992		10.1145/129712.129720	mathematical optimization;commentz-walter algorithm;computer science;theoretical computer science;machine learning;boyer–moore string search algorithm;parallel algorithm;string searching algorithm	Theory	13.12861686563364	27.771687186289313	94398
5d7177d43132bcb46555e7c9259934905555ec1b	omg! orthologs in multiple genomes - competing graph-theoretical formulations		From the set of all pairwise homologies, weighted by sequence similarities, among a set of genomes, we seek disjoint orthology sets of genes, in which each element is orthogonal to all other genes (on a different genome) in the same set. In a graph-theoretical formulation, where genes are vertices and weighted edges represent homologies, we suggest three criteria, with three different biological motivations, for evaluating the partition of genes produced by deletion of a subset of edges: i) minimum weight edge removal, ii) minimum degree-zero vertex creation, and iii) maximum number of edges in the transitive closure of the graph after edge deletion. For each of the problems, all either proved or conjectured to be NP-hard, we suggest approximate and heuristic algorithms of finding orthology sets satisfying the criteria, and show how to incorporate genomes that have a whole genome duplication event in their immediate lineage. We apply this to ten flowering plant genomes, involving 160,000 different genes in given pairwise homologies. We evaluate the results in a number of ways and recommend criterion iii) as best suited to applications to multiple gene order alignment.	algorithmic game theory;approximation algorithm;auction algorithm;block code;graph theory;heuristic;homology (biology);lineage (evolution);matching (graph theory);minimum weight;np-hardness;phylogenetics;sms language;transitive closure	Chunfang Zheng;Krister M. Swenson;Eric Lyons;David Sankoff	2011		10.1007/978-3-642-23038-7_30	combinatorics;discrete mathematics;bioinformatics;mathematics;genetics	Comp.	18.311556994154746	22.265527827755527	94402
82d575119b68a6942123b6a646624813d92832f6	a space efficient persistent implementation of an index for dna sequences	molecular biology;indexation;dna sequence	Due to newly developed high-throughput technologies for DNA sequencing, the number of fully sequenced species increases rapidly. String databases holding these sequences are very large. On the eld of molecular biology the handling of large string data which cannot be broken in words is a great challenge. Hereby the most important string operation is the approximate substring match. This type of match is essential for many applications in biology. The suÆx tree has been established as the most predestined in-memory data structure supporting approximate substring matches on DNA sequences. They are belonging to the wider class of suÆx structures. The central issue of the paper is the theoretically evaluation of the suÆx structures with the aim to reveal the most suitable structure for a persistent implementation on the disk. I will show that this is a variant of a suÆx tree. Further, the paper addresses the question in which way this data structure can be stored on disk, and how fast access can be achieved. In this connection I want to introduce a space eÆcient implementation by using a tree coding scheme which optimizes disk saving and therefore preventing unneeded disk access. Here the most important design issue lies in the representation of di erent variants of nodes of the tree. I will present an implementation in C programming language and show that the implementation in a rst evaluation step compares favourably to other implementations.	approximation algorithm;high-throughput computing;in-memory database;memory management;persistent data structure;programming language;runtime system;string operations;substring;throughput	Gabriele Witterstein	2003			implementation;suffix tree;data mining;coding (social sciences);theoretical computer science;computer science;dna sequencing;substring;indexation;suffix;data structure;bioinformatics	Theory	10.606061926470232	28.58616681646321	94404
7bdb6b8746e9c6678f2fafb07aa2de96f8fc4a14	finding frequent items in data streams	count sketch algorithm;frequent item;streaming algorithm;data stream;approximation algorithm;echantillonnage;stockage donnee;fonction objectif;approximation;sampling;objective function;data storage;codificacion;informatique theorique;estructura datos;item frequent;algorithme courant;coding;algoritmo aproximacion;almacenamiento datos;funcion objetivo;structure donnee;frequent items;algorithme approximation;muestreo;data structure;codage;computer theory;informatica teorica	We present a 1-pass algorithm for estimating the most frequent items in a data stream using limited storage space. Our method relies on a data structure called a COUNT SKETCH, which allows us to reliably estimate the frequencies of frequent items in the stream. Our algorithm achieves better space bounds than the previously known best algorithms for this problem for several natural distributions on the item frequencies. In addition, our algorithm leads directly to a 2-pass algorithm for the problem of estimating the items with the largest (absolute) change in frequency between two data streams. To our knowledge, this latter problem has not been previously studied in the literature.		Moses Charikar;Kevin C. Chen;Martin Farach-Colton	2004	Theor. Comput. Sci.	10.1016/S0304-3975(03)00400-6	sampling;mathematical optimization;data structure;computer science;approximation;computer data storage;data mining;database;mathematics;streaming algorithm;coding;approximation algorithm;algorithm	ECom	14.640740293703692	24.28472646648594	94494
a45883cd62d734b2bb8fe2fd1e9a40e1248ad7dd	minimum-weight spanning tree algorithms a survey and empirical study	empirical study;performance evaluation;minimum weight spanning tree;linear time algorithm;ninimum spanning tree;linear time;comparative study;minimum spanning tree;network optimisation;graph algorithm;linear time algorithms;graph algorithms;data structure;combinatorial optimisation	"""The minimum-weight spanning tree problem is one of the most typical and well-known problems of combinatorial optimisation. E$cient solution techniques had been known for many years. However, in the last two decades asymptotically faster algorithms have been invented. Each new algorithm brought the time bound one step closer to linearity and """"nally Karger, Klein and Tarjan proposed the only known expected linear-time method. Modern algorithms make use of more advanced data structures and appear to be more complicated to implement.Most authors and practitioners refer to these but still use the classical ones, which are considerably simpler but asymptotically slower. The paper """"rst presents a survey of the classical methods and the more recent algorithmic developments. Modern algorithms are then compared with the classical ones and their relative performance is evaluated through extensive empirical tests, using reasonably large-size problem instances. Randomly generated problem instances used in the tests range from small networks having 512 nodes and 1024 edges to quite large ones with 16 384 nodes and 524 288 edges. The purpose of the comparative study is to investigate the conjecture that modern algorithms are also easy to apply and have constants of proportionality small enough to make them competitive in practice with the older ones."""	algorithm;apply;binary heap;combinatorial optimization;data structure;fastest;lowest common ancestor;mathematical optimization;minimum spanning tree;minimum-weight triangulation;prim's algorithm;time complexity;verification and validation	Cüneyt F. Bazlamaçci;Khalil S. Hindi	2001	Computers & OR	10.1016/S0305-0548(00)00007-1	time complexity;euclidean minimum spanning tree;mathematical optimization;combinatorics;kruskal's algorithm;data structure;spanning tree;prim's algorithm;combinatorial optimization;computer science;expected linear time mst algorithm;minimum spanning tree;comparative research;gomory–hu tree;connected dominating set;k-minimum spanning tree;mathematics;reverse-delete algorithm;distributed minimum spanning tree;empirical research;algorithm	Theory	21.107997391979456	21.43661170880864	94704
a5833731a05bf6bbbfdd9728ec71f76382f80e4e	simple balanced binary search trees		Efficient implementations of sets and maps (dictionaries) are important in computer science, and balanced binary search trees are the basis of the best practical implementations. Pedagogically, however, they are often quite complicated, especially with respect to deletion. I present complete code (with justification and analysis not previously available in the literature) for a purely-functional implementation based on AA trees, which is the simplest treatment of the subject of which I am aware.	2–3–4 tree;aa tree;computer science;data structure;dictionary;fits;final exam;immutable object;imperative programming;invariant (computer science);map;recursion;red–black tree	Prabhakar Ragde	2014		10.4204/EPTCS.170.6	combinatorics;binary search tree;geometry of binary search trees;computer science;theoretical computer science;mathematics;weight-balanced tree;ternary search tree;algorithm	Logic	14.767973006221968	28.832798874343183	94831
e3993453a95887b4a18aebcc5b9a43dd14ca12d9	tight bounds for oblivious routing in the hypercube	oblivious routing;maximum degree;upper bound;community networks	We prove that in anyN-node communication network with maximum degreed, any deterministic oblivious algorithm for routing an arbitrary permutation requires Ω(√N/d) parallel communication steps in the worst case. This is an improvement upon the Ω(√N/d 3/2) bound obtained by Borodin and Hopcroft. For theN-node hypercube, in particular, we show a matching upper bound by exhibiting a deterministic oblivious algorithm that routes any permutation in Θ(√N/logN) steps. The best previously known upper bound was Θ(√N). Our algorithm may be practical for smallN (up to about 214 nodes).	algorithm;best, worst and average case;routing;telecommunications network	Christos Kaklamanis;Danny Krizanc;Thanasis Tsantilas	1990	Mathematical systems theory	10.1007/BF02090400	mathematical optimization;combinatorics;discrete mathematics;mathematics;upper and lower bounds	Theory	21.50236523002117	22.343027914929248	95026
b476199393cb45956c1db44f29064c54dd228954	on the compatibility of quartet trees	algorithms;design;nonnumerical algorithms and problems;trees;theory	Phylogenetic tree reconstruction is a fundamental biological problem. Quartet trees, trees over four species, are the minimal informational unit for phylogenetic classification. While every phylogenetic tree over $n$ species defines ${n \choose 4}$ quartets, not every set of quartets is compatible with some phylogenetic tree. Here we focus on the compatibility of quartet sets. We provide several results addressing the question of what can be inferred about the compatibility of a set from its subsets. Most of our results use probabilistic arguments to prove the sought characteristics. In particular we show that there are quartet sets $Q$ of size $m=c n \log n$ in which every subset of cardinality $c' n/ \log n$ is compatible, and yet no fraction of more than $1/3+\epsilon$ of $Q$ is compatible. On the other hand, in contrast to the classical result stating when $Q$ is the densest, i.e., $m={n \choose 4}$ and the compatibility of any set of three quartets implies full compatibility, we show that even for $m...		Noga Alon;Sagi Snir;Raphael Yuster	2014	SIAM J. Discrete Math.	10.1137/130941043	combinatorics;discrete mathematics;mathematics;algorithm	Theory	18.333898867845193	22.22754139851672	95088
3360a05338e34a95c5798981b9bdfc5da581e2d7	on the difficulty of manhattan channel routing	modele manhattan;complexite calcul;complejidad calculo;limite inferior;geometrie algorithmique;routing;probleme np complet;lower bounds;np completeness;sistema informatico;computational geometry;circuit vlsi;combinatorial problems;computer system;computing complexity;vlsi channel routing;probleme combinatoire;vlsi circuit;problema combinatorio;computational complexity;geometria algoritmica;problema np completo;systeme informatique;encaminamiento;combinatory problem;circuito vlsi;limite inferieure;lower bound;np complete problem;acheminement	We show that channel routing in the Manhattan model remains diicult even when all nets are single-sided. Given a set of n single-sided nets, we consider the problem of determining the minimum number of tracks required to obtain a dogleg-free routing. In addition to showing that the decision version of the problem is NP-complete, we show that there are problems requiring at least d + (p n) tracks, where d is the density. This existential lower bound does not follow from any of the known lower bounds in the literature.	channel router;decision problem;np-completeness;routing	Ronald I. Greenberg;Joseph JáJá;Sridhar Krishnamurthy	1992	Inf. Process. Lett.	10.1016/0020-0190(92)90214-G	combinatorics;np-complete;computational geometry;computer science;artificial intelligence;destination-sequenced distance vector routing;mathematics;algorithm	Theory	19.106502580966037	26.838593866684473	95094
707aac89db419e9b4d4471dd96b977cd294845be	a sublinear algorithm for two-dimensional string matching	two dimensional shape;forme bidimensionnelle;chaine caractere;forma bidimensional;boyer moore algorithm;two dimensional string matching;reconnaissance caractere;cadena caracter;sublinear algorithm;string matching;character recognition;reconocimiento caracter;character string	Abstract   A simple algorithm based on the Boyer-Moore idea is presented for two-dimensional string matching. The algorithm examines a strip of columns at a time, and the shift of the pattern is based on a string of several characters on a row. The expected running time is shown to be sublinear for random texts and patterns. The algorithm is easy to implement, and it works well in practice.	string searching algorithm	Jorma Tarhio	1996	Pattern Recognition Letters	10.1016/0167-8655(96)00055-4	combinatorics;discrete mathematics;approximate string matching;commentz-walter algorithm;string;computer science;boyer–moore string search algorithm;mathematics;string metric;algorithm;rabin–karp algorithm;string searching algorithm	Vision	14.037655151944158	27.03528863625845	95318
98ac668c8fb6d0cfbffed4967dc7ee3e745e1df6	parameterized algorithms for feedback vertex set	graph theory;teoria grafo;feedback vertex set;theorie graphe;grafo extremal;extremal graph theory;graphe extremal;parameterized algorithm;vertex graph;extremal graph;vertice grafo;sommet graphe	We present an algorithm for the parameterized feedback vertex set problem that runs in time O((2 lg k + 2 lg lg k + 18)n). This improves the previous O(max{12, (4 lg k)}n) algorithm by Raman et al. by roughly a 2 factor (n ∈ O(n) is the time needed to multiply two n × n matrices). Our results are obtained by developing new combinatorial tools and employing results from extremal graph theory. We also show that for several special classes of graphs the feedback vertex set problem can be solved in time cn for some constant c. This includes, for example, graphs of genus O(lg n).	algorithm;carrier-to-noise ratio;extremal graph theory;feedback vertex set;genus (mathematics);raman scattering	Iyad A. Kanj;Michael J. Pelsmajer;Marcus Schaefer	2004		10.1007/978-3-540-28639-4_21	combinatorics;extremal graph theory;discrete mathematics;feedback arc set;topology;feedback vertex set;vertex cover;degree;graph theory;cycle graph;vertex;mathematics;windmill graph;neighbourhood;circulant graph	Theory	23.624741059279874	25.699326657940748	95379
4d6b588cb436d7af8d0504ec9fc8b1fffc1f1647	cubesort: an optimal sorting algorithm for feasible parallel computers	cube connected cycles;asymptotic optimality;time use;parallel computer;parallel machines;sorting algorithm	This paper studies the problem of sorting N items on a P processor parallel machine, where N≥P. The central result of the  paper is a new algorithm, called cubesort, that sorts N=P1+1/k items in O(k P1/k log P) time using a P processor shuffle-exchange. Thus for any positive constant k, cubesort provides an asymptotically optimal  speed-up over sequential sorting. Cubesort also sorts N = P log P items using a P processor shuffle-exchange in O(log3 P/loglog P) time. Both of these results are faster than any previously published algorithms for the given problems. Cubesort  also provides asymptotically optimal sorting algorithms for a wide range of parallel computers, including the cube-connected  cycles and the hypercube. An important extension of the central result is an algorithm that simulates a single step of a Priority-CRCW  PRAM with N processors and N words of memory on a P processor shuffle-exchange machine in O(k P1/k log P) time, where N=P1+1/k.  	cubesort;parallel computing;sorting algorithm	Robert Cypher;Jorge L. C. Sanz	1988		10.1007/BFb0040412	combinatorics;computer science;theoretical computer science;bitonic sorter;distributed computing;parallel algorithm;cost efficiency	HPC	12.491447510200661	31.79492315337286	95389
0c3a1d071c404e1f01a70f9da2551a64dae89e25	maximum skew-symmetric flows and matchings	camino mas corto;acoplamiento grafo;shortest path;graphe biparti;network flow matching;grafo simetrico;maximum flow;digraph;symmetric graph;grafo bipartido;metodo minimax;minimax method;digrafo;plus court chemin;maximum matching;graph matching;couplage graphe;flujo red;skew symmetric graph;matching;methode minimax;b matching;network flow;bipartite graph;flot reseau;graphe symetrique;digraphe	The maximum integer skew-symmetric flow problem (MSFP) generalizes both the maximum flow and maximum matching problems. It was introduced by Tutte [28] in terms of selfconjugate flows in antisymmetrical digraphs. He showed that for these objects there are natural analogs of classical theoretical results on usual network flows, such as the flow decomposition, augmenting path, and max-flow min-cut theorems. We give unified and shorter proofs for those theoretical results. We then extend to MSFP the shortest augmenting path method of Edmonds and Karp [7] and the blocking flow method of Dinits [4], obtaining algorithms with similar time bounds in general case. Moreover, in the cases of unit arc capacities and unit “node capacities” the blocking skew-symmetric flow algorithm has time bounds similar to those established in [8, 21] for Dinits’ algorithm. In particular, this implies an algorithm for finding a maximum matching in a nonbipartite graph in O( √ nm) time, which matches the time bound for the algorithm of Micali and Vazirani [25]. Finally, extending a clique compression technique of Feder and Motwani [9] to particular skew-symmetric graphs, we speed up the implied maximum matching algorithm to run in O( √ nm log(n2/m)/ log n) time, improving the best known bound for dense nonbipartite graphs. Also other theoretical and algorithmic results on skew-symmetric flows and their applications are presented.	blocking (computing);dinic's algorithm;edmonds' algorithm;edmonds–karp algorithm;flow network;matching (graph theory);max-flow min-cut theorem;maximum flow problem;minimum cut;olami–feder–christensen model;simplex algorithm	Andrew V. Goldberg;Alexander V. Karzanov	2004	Math. Program.	10.1007/s10107-004-0505-z	mathematical optimization;combinatorics;discrete mathematics;push–relabel maximum flow algorithm;hopcroft–karp algorithm;mathematics;edmonds–karp algorithm;matching	Theory	22.036587116363023	21.638996506037603	95442
aa7be764d8068dd7674e58a6ce88d6f49ae774a5	graph searching on some subclasses of chordal graphs	graph search;edge searching;interval graph;split graphs;node searching;graph searching problem;star like graphs;interval graphs;polynomial algorithm;chordal graph	In the graph-searching problem, initially a graph with all the edges contaminated is presented. The objective is to obtain a state of the graph in which all the edges are simultaneously cleared by using the least number of searchers. Two variations of the graph-searching problem are considered. One is edge searching, in which an edge is cleared by moving a searcher along this edge, and the other is node searching, in which an edge is cleared by concurrently having searchers on both of its two endpoints. We present a uniform approach to solve the above two variations on several subclasses of chordal graphs. For edge searching, we give an O(mn 2 ) -time algorithm on split graphs (i.e., 1-starlike graphs), an O(m+n) -time algorithm on interval graphs, and an O(mn k ) -time algorithm on k -starlike graphs (a generalization of split graphs), for a fixed k\geq 2 , where m and n are the numbers of edges and vertices in the input graph, respectively. There is no polynomial algorithm known previously for any of the above problems. In addition, we also show that the edge-searching problem remains NP-complete on chordal graphs. For node searching, we give an O(mn k ) -time algorithm on k -starlike graphs for a fixed k \geq 1 . This result implies that the pathwidth problem on k -starlike graphs can also be solved in this time bound which greatly improves the previous results.	algorithm;graph traversal;karp's 21 np-complete problems;pathwidth;peripheral;polynomial;time complexity;vertex (geometry)	Sheng-Lung Peng;Chuan Yi Tang;Ming-Tat Ko;Chin-Wen Ho;Tsan-sheng Hsu	2000	Algorithmica	10.1007/s004530010026	1-planar graph;outerplanar graph;block graph;pathwidth;edge contraction;split graph;combinatorics;discrete mathematics;interval graph;topology;computer science;simplex graph;graph coloring;symmetric graph;mathematics;voltage graph;distance-hereditary graph;complement graph;graph operations;chordal graph;indifference graph;line graph;algorithm;string graph;coxeter graph;planar graph	Theory	24.483071535114988	23.77830417249617	95511
067f75310d8f430a3dc2cd0c84e31ba5ce707252	some observations on maximum weight stable sets in certain p	metodo polinomial;subgrafo;camino grafo;temps polynomial;complexite calcul;graph path;problema np duro;polynomial time algorithm;np hard problem;complejidad computacion;probleme np difficile;polynomial method;sous graphe;computational complexity;polynomial time;chemin graphe;stable set;subgraph;methode polynomiale;p5 free graphs;tiempo polinomial;maximum weight stable set problem	Abstract   The maximum weight stable set problem (MWS) is the weighted version of the maximum stable set problem (MS), which is NP-hard. The class of  P  5 -free graphs – i.e., graphs with no induced path of five vertices – is the unique minimal class, defined by forbidding a single connected subgraph, for which the computational complexity of MS is an open question. At the same time, it is known that MS can be efficiently solved for    (    P    5    ,  F  )   -free graphs, where  F  is any graph of five vertices different to a  C  5 . In this paper we introduce some observations on  P  5 -free graphs, and apply them to introduce certain subclasses of such graphs for which one can efficiently solve MWS. That extends or improves some known results, and implies – together with other known results – that MWS can be efficiently solved for    (    P    5    ,  F  )   -free graphs where  F  is any graph of five vertices different to a  C  5 .		Raffaele Mosca	2008	European Journal of Operational Research	10.1016/j.ejor.2006.12.011	1-planar graph;time complexity;pathwidth;mathematical optimization;split graph;combinatorics;discrete mathematics;cograph;independent set;graph product;longest path problem;computer science;hopcroft–karp algorithm;pancyclic graph;metric dimension;np-hard;clique-sum;trapezoid graph;mathematics;maximal independent set;modular decomposition;computational complexity theory;induced path;chordal graph;indifference graph;algorithm	Theory	22.047468385180288	26.861117608758086	95635
4a2822a768d7dab8443a7b82fc0879a99858230e	linear expected-time algorithms for connectivity problems (extended abstract)	random graph;deterministic space complete problems;lower bounds;polynomial over gf 2;boolean expression;graph connectivity;directed graph;space complexity;graph algorithm;connected component;set of assignments;cycle free problem	Researchers in recent years have developed many graph algorithms that are fast in the worst case, but little work has been done on graph algorithms that are fast on the average. (Exceptions include the work of Angluin and Valiant [1], Karp [7], and Schnorr [9].) In this paper we analyze the expected running time of four algorithms for solving graph connectivity problems. Our goal is to exhibit algorithms whose expected time is within a constant factor of optimum and to shed light on the properties of random graphs.  In Section 2 we develop and analyze a simple algorithm that finds the connected components of an undirected graph with n vertices in O(n) expected time. In Sections 3 and 4 we describe algorithms for finding the strong components of a directed graph and the blocks of an undirected graph in O(n) expected time. The time required for these three problems is Ω(m) in the worst case, where m is the number of edges in the graph, since all edges must be examined; but our results show that only O(n) edges must be examined on the average.*@@@@  In Section 5 we present an algorithm for finding a minimum weight spanning forest in an undirected graph with edge weights in O(m) expected time.	algorithm;average-case complexity;best, worst and average case;connected component (graph theory);connectivity (graph theory);directed graph;file spanning;graph (discrete mathematics);graph theory;list of algorithms;minimum spanning tree;minimum weight;random graph;time complexity	Richard M. Karp;Robert E. Tarjan	1980		10.1145/800141.804686	graph power;random graph;mathematical optimization;factor-critical graph;combinatorics;discrete mathematics;connected component;directed graph;graph bandwidth;boolean expression;null graph;graph property;connectivity;simplex graph;cycle graph;cubic graph;mathematics;voltage graph;graph;windmill graph;dspace;butterfly graph;graph minor;quartic graph;complement graph;semi-symmetric graph;line graph;strength of a graph;coxeter graph	Theory	21.49979630581564	22.572781448116675	95668
62ce420c4efd1c7d09af62a4fb3ac4aae5bc4e38	note on structural properties and sizes of eigenspaces of min-max functions	metodo minimax;minimax method;problema np duro;separability;decision problem;np hard problem;graph connectivity;separabilidad;probleme np difficile;conectividad grafo;methode minimax;separabilite;connectivite graphe;structural properties	In this paper, we study relationships among structural properties of min-max functions and complexity on deciding the sizes of the eigenspaces of minmax functions. We show that strong connectivity implies inseparability; Olsder’s separated systems are separable. A relaxed condition under which inseparability remains equivalent to the balance condition is also presented. The decision problem of whether the size of the eigenspace of a min-max function is greater than one is then report to be NP-hard based on the connection between the separability of a min-max function and the size of the eigenspace of its skeleton.	co-np;database normalization;decision problem;disjunctive normal form;linear separability;maxima and minima;minimax;np-hardness;yang	Qianchuan Zhao;Da-Zhong Zheng	2003		10.1007/978-3-540-44928-7_52	mathematical optimization;combinatorics;discrete mathematics;computer science;artificial intelligence;connectivity;decision problem;np-hard;mathematics;statistics	DB	22.164287046314673	26.492982709831796	95672
b5da4101b9fbfaa13a40e5a3cb26c33d871df16b	a fast prize-collecting steiner forest algorithm for functional analyses in biological networks		The Prize-collecting Steiner Forest (PCSF) problem is NPhard, requiring extreme computational effort to find exact solutions for large inputs. We introduce a new heuristic algorithm for PCSF which preserves the quality of solutions obtained by previous heuristic approaches while reducing the runtime by a factor of 10 for larger graphs. By decreasing the draw on computational resources, this algorithm affords systems biologists the opportunity to analyze larger biological networks faster and narrow their analyses to individual patients.	algorithm;analysis of algorithms;biological network;command & conquer:yuri's revenge;computation;computational resource;decade (log scale);geo-imputation;heuristic (computer science);interactome;steiner tree problem;subnetwork;systems biology;x image extension	Murodzhon Akhmedov;Alexander LeNail;Francesco Bertoni;Ivo Kwee;Ernest Fraenkel;Roberto Montemanni	2017		10.1007/978-3-319-59776-8_22	discrete mathematics;mathematical optimization;heuristic (computer science);biological network;computer science;algorithm;heuristic;graph	PL	19.432301747221338	19.60567691007038	95697
49111dc2d17c87d5e19bb9b0571a9709d1a63bd1	polynomial time sat decision for complementation-invariant clause-sets, and sign-non-singular matrices	polynomial time	We study complement-invariant clause-sets F, where for every clause C ∈ F we have C = {x : x ∈ C} ∈ F, i.e., F is closed under elementwise complementation of clauses. The reduced deficiency of a clauseset F is defined as δr(F) := 1/2 (δ(F) - n(F)), where δ(F) = c(F) - n(F) is the difference of the number of clauses and the number of variables, while the maximal reduced deficiency is δr* (F) := maxF′⊆F δr(F′) ≥ 0. We show polynomial time SAT decision for complement-invariant clausesets F with δr* (F) = 0, exploiting the (non-trivial) decision algorithm for sign-non-singular (SNS) matrices given by [Robertson, Seymour, Thomas 1999; McCuaig 2004]. As an application, hypergraph 2-colourability decision is considered. Minimally unsatisfiable complement-invariant clausesets F fulfil δr (F) = δr* (F), and thus we immediately obtain polynomial time decidability of minimally unsatisfiable complement-invariant clausesets F with δr (F) = 0, but we also givemore direct algorithms and characterisations (especially for sub-classes). The theory of autarkies is the basis for all these considerations.	time complexity	Oliver Kullmann	2007		10.1007/978-3-540-72788-0_30	time complexity;combinatorics;discrete mathematics;computer science;mathematics;minimal polynomial;algorithm	Theory	22.288840585361182	24.227134979261102	95804
3e1a20be8e526c8010d2f914326df7a65e87abed	greedy packet scheduling	camino mas corto;parallel computing;reseau information;shortest path;shortest paths;communication networks;algorithme glouton;routing;routage;information network;parallel computation;90b12;calculo paralelo;90b35;68q22;scheduling;chemin plus court;greedy algorithm;ordonamiento;packet scheduling;calcul parallele;68m10;ordonnancement;94a05;red informacion	"""Scheduling packets to be forwarded over a link is an important subtask of the routing process both in parallel computing and in communication networks. This paper investigates the simple class of greedy scheduling algorithms, namely, algorithms that always forward a packet if they can. It is rst proved that for various \natural"""" classes of routes, the time required to complete the transmission of a set of packets is bounded by the number of packets, k, and the maximal route length, d, for any greedy algorithm (including the arbitrary scheduling policy). Next, tight time bounds of d + k 1 are proved for a speci c greedy algorithm on the class of shortest paths in n-vertex networks. Finally it is shown that when the routes are arbitrary, the time achieved by various \natural"""" greedy algorithms can be as bad as (d p k+ k), for any k, and even for d = (n). IBM T.J. Watson Research Center, P.O. Box 704, Yorktown Heights, NY 10598, and Faculty of Electrical Engineering, The Technion, Haifa 32000, Israel. cidon@ee.technion.ac.il IBM T.J. Watson Research Center P.O. Box 704, Yorktown Heights, NY 10598, kutten@watson.ibm.com Laboratory for Computer Science, MIT, Cambridge, MA 02139. Partially supported by IBM graduate fellowship. mansour@theory.lcs.mit.edu Department of Applied Mathematics and Computer Science, The Weizmann Institute, Rehovot 76100, Israel. peleg@wisdom.weizmann.ac.il. Supported in part by an Allon Fellowship, by a Walter and Elise Haas Career Development Award and by a Bantrell Fellowship. Part of the work was done while visiting IBM T.J. Watson Research Center."""	electrical engineering;greedy algorithm;mit computer science and artificial intelligence laboratory;maximal set;network packet;parallel computing;precedence effect;routing;scheduling (computing);shortest path problem;telecommunications network;thomas j. watson research center	Israel Cidon;Shay Kutten;Yishay Mansour;David Peleg	1995	SIAM J. Comput.	10.1137/S0097539791217695	greedy randomized adaptive search procedure;mathematical optimization;routing;greedy algorithm;computer science;theoretical computer science;mathematics;distributed computing;shortest path problem;scheduling;algorithm	Theory	19.05454419359648	30.94189325106784	95973
4291336c0384da67f2fe944fa7eb82c76367d415	estimating reliability of workers for cooperative distributed computing	probability;message complexity estimating reliability cooperative distributed computing internet supercomputing interconnected computers network supercomputing randomized synchronous algorithms crash prone processors average probability synchronous distributed algorithm approximation theory fractional polynomial;distributed processing;probability computational complexity distributed processing internet polynomial approximation;worker reliability estimation internet supercomputing distributed computing;internet;computational complexity;program processors computer crashes estimation complexity theory computational modeling algorithm design and analysis computers;polynomial approximation	Internet supercomputing is an approach to solving partitionable, computation-intensive problems by harnessing the power of a vast number of interconnected computers. For the problem of using network supercomputing to perform a large collection of independent tasks, prior work introduced a decentralized approach and provided randomized synchronous algorithms that perform all tasks correctly with high probability, while dealing with misbehaving or crash-prone processors. The main weaknesses of existing algorithms is that they assume either that the average probability of a non-crashed processor returning incorrect results is inferior to 12, or that the probability of returning incorrect results is known to each processor. Here we present a randomized synchronous distributed algorithm that tightly estimates the probability of each processor returning correct results. Starting with the set P of n processors, let F be the set of processors that crash. Our algorithm estimates the probability pi of returning a correct result for each processor i ∈ P - F, making the estimates available to all these processors. The estimation is based on the (ε, δ)-approximation, where each estimated probability p̃i of pi obeys the bound Pr[pi(1 - ε) ≤ p̃i ≤ pi(1 + ε)] > 1 - δ, for any constants δ > 0 and ε > 0 chosen by the user. An important aspect of this algorithm is that each processor terminates without global coordination. We assess the efficiency of the algorithm in three adversarial models as follows. For the model where the number of non-crashed processors P - F is linearly bounded the time complexity T (n) of the algorithm is O(log n), work complexity W(n) is O(n log n), and message complexity M(n) is O(n log2 n). For the model where P - F is bounded by a fractional polynomial we have T(n) = O(n1-a log n log log n), W(n) = O(n log n log log n), and M(n) = O(n log2 n log log n). For the model where P - F is bounded by a poly-logarithm we have T(n) = O(n), W(n) = O(n poly log n), and M(n) = O(n log2 n poly log n). All bounds are shown to hold with high probability.	amortized analysis;binary logarithm;central processing unit;computation;computer;crash (computing);distributed algorithm;distributed computing;linear bounded automaton;polynomial;randomized algorithm;supercomputer;time complexity;with high probability	Seda Davtyan;Kishori M. Konwar;Alexander A. Shvartsman	2013	2013 IEEE 12th International Symposium on Parallel and Distributed Computing	10.1109/ISPDC.2013.22	parallel computing;real-time computing;the internet;computer science;theoretical computer science;probability;distributed computing;computational complexity theory	Theory	11.609592368432885	31.431556004439724	95984
bcb3f39af7b609ce0a96f780bf59d956b9b2ed5a	a primal-dual method for approximating tree cover with two weights	primal dual method	The tree cover (TC) problem is to compute a minimum weight connected edge set, given a connected and edge-weighted graph G, such that its vertex set forms a vertex cover for G. Unlike related problems of vertex cover or edge dominating set, weighted TC is not yet known to be approximable in polynomial time as well as the unweighted version is. Moreover, the best approximation algorithm known so far for weighted TC is far from practical in its efficiency. In this paper we consider a restricted version of weighted TC, as a first step towards better approximation of general TC, where only two edge weights differing by at least a factor of 2 are available. It will be shown that a factor 2 approximation can be attained efficiently (in the complexity of max flow) in this case by a primal-dual method. Even under the limited weights as such, the primal-dual arguments used will be seen to be quite involved, having a nontrivial style of dual assignments as an essential part, unlike the case of uniform weights.		Takashi Doi;Toshihiro Fujito	2004	Electronic Notes in Discrete Mathematics	10.1016/j.endm.2004.03.027	mathematical optimization;combinatorics;discrete mathematics;mathematics	Theory	24.04914603453081	19.80287182751849	96178
ceb03bd6ae03281d80acfdcb66dc7960d58faed5	matching polynomials of series-parallel graphs	series parallel graph;erew model;tree contraction;matching polynomials;series parallel graphs;parallel algorithms	We present an efficient algorithm for computing the matching polynomial of a series-parallel graph in O(n2) time. This algorithm improves on the previous result of O(n3). We also present a cost-optimal parallel algorithm for computing the matching polynomial of a series-parallel graph using an EREW PRAM computer with the number of processors p less than n2/ log n.		Lih-Hsing Hsu	1993	Parallel Processing Letters	10.1142/S0129626493000034	1-planar graph;claw-free graph;outerplanar graph;block graph;folded cube graph;factor-critical graph;combinatorics;discrete mathematics;parallel computing;bipartite graph;computer science;3-dimensional matching;theoretical computer science;hopcroft–karp algorithm;cubic graph;graph coloring;mathematics;voltage graph;blossom algorithm;parallel algorithm;tutte polynomial;line graph;coxeter graph;matching	Theory	19.23900430474658	28.496718482230253	96315
66475e9b77b1812c4f7e62799065364397282921	sparse approximation is provably hard under coherent dictionaries	unique games;complexity;sparse approximation;pcp	It is well known that sparse approximation problem is NP-hard under general dictionaries. Several algorithms have been devised and analyzed in the past decade under various assumptions on the coherence μ   of the dictionary represented by an M×NM×N matrix from which a subset of k   column vectors is selected. All these results assume μ=O(k−1)μ=O(k−1). This article is an attempt to bridge the big gap between the negative result of NP-hardness under general dictionaries and the positive results under this restrictive assumption. In particular, it suggests that the aforementioned assumption might be asymptotically the best one can make to arrive at any efficient algorithmic result under well-known conjectures of complexity theory. In establishing the results, we make use of a new simple multilayered PCP which is tailored to give a matrix with small coherence combined with our reduction.	coherent;dictionary;sparse approximation	Ali Çivril	2017	J. Comput. Syst. Sci.	10.1016/j.jcss.2016.07.001	combinatorics;complexity;computer science;theoretical computer science;sparse approximation;mathematics;algorithm	Theory	11.139518094572463	19.777972946065788	96333
919cf82c3ddd55b9912cdef1338182e74e5e23e9	composite pattern discovery for pcr application	busqueda informacion;genetique;approximation asymptotique;optimisation;optimizacion;genetica;information retrieval;chaine caractere;multiplaje;multiplexing;genetics;pattern discovery;multiplexage;recherche information;human genome;decouverte connaissance;cadena caracter;genome;descubrimiento conocimiento;optimization;nested pcr;asymptotic approximation;genoma;aproximacion asintotica;character string;knowledge discovery	We consider the problem of finding pairs of short patterns such that, in a given input sequence of length n, the distance between each pair’s patterns is at least α. The problem was introduced in [1] and is motivated by the optimization of multiplexed nested PCR. We study algorithms for the following two cases; the special case when the two patterns in the pair are required to have the same length, and the more general case when the patterns can have different lengths. For the first case we present an O(αn log log n) time and O(n) space algorithm, and for the general case we give an O(αn log n) time and O(n) space algorithm. The algorithms work for any alphabet size and use asymptotically less space than the algorithms presented in [1]. For alphabets of constant size we also give an O(n √ n log n) time algorithm for the general case. We demonstrate that the algorithms perform well in practice and present our findings for the human genome. In addition, we study an extended version of the problem where patterns in the pair occur at certain positions at a distance at most α, but do not occur α-close anywhere else, in the input sequence.	algorithm;composite pattern;experiment;hamming distance;mathematical optimization;multiplexing;suffix tree;time complexity	Stanislav Angelov;Shunsuke Inenaga	2005		10.1007/11575832_19	nested polymerase chain reaction;human genome;string;computer science;artificial intelligence;calculus;mathematics;knowledge extraction;algorithm;multiplexing;genome	Theory	15.43127972340212	25.749896473156273	96813
d8e7974e19bf66ffcc36f8a3c9861e3e7f0938b7	dynamic compressed representation of texts with rank/select	rank;index;data structure;select;compression	Given an n-length text T over a σ-size alphabet, we present a compressed representation of T which supports retrieving queries of rank/select/access and updating queries of insert/delete. For a measure of compression, we use the empirical entropy H(T), which defines a lower bound nH(T) bits for any algorithm to compress T of n log σ bits. Our representation takes this entropy bound of T, i.e., nH(T) ≤ n log σ bits, and an additional bits less than the text size, i.e., o(n log σ) + O(n) bits. In compressed space of nH(T) + o(n log σ) + O(n) bits, our representation supports O(log n) time queries for a log n-size alphabet and its extension provides O((1+ ) log n) time queries for a σ-size alphabet.	algorithm;burrows–wheeler transform;data compression;entropy (information theory);ti-nspire series	Sunho Lee;Kunsoo Park	2009	JCSE		speech recognition;theoretical computer science;pattern recognition;mathematics	Theory	12.170294098721763	27.246720284484113	96862
cd44e5074b33f5af6248d810db49faa7b54c24e2	parameterized algorithms for non-separating trees and branchings in digraphs	parameterized complexity;fixed parameter tractable;branching;partitioning problem;linear vertex kernel;spanning tree;exponential time algorithm	A well known result in graph algorithms, due to Edmonds, states that given a digraph D and a positive integer $$\ell $$ ℓ , we can test whether D contains $$\ell $$ ℓ arc-disjoint out-branchings in polynomial time. However, if we ask whether there exists an out-branching and an in-branching which are arc-disjoint, then the problem becomes NP-complete. In fact, even deciding whether a digraph D contains an out-branching which is arc-disjoint from some spanning tree in the underlying undirected graph remains NP-complete. In this paper we formulate some natural optimization questions around these problems and initiate its study in the realm of parameterized complexity. More precisely, the problems we study are the following: Arc-Disjoint Branchings and Non-Disconnecting Out-Branching. In Arc-Disjoint Branchings (Non-Disconnecting Out-Branching), a digraph D and a positive integer k are given as input and the goal is to test whether there exist an out-branching and in-branching (respectively, a spanning tree in the underlying undirected graph) that differ on at least k arcs. We obtain the following results for these problems. Non-Disconnecting Out-Branching is fixed parameter tractable (FPT) and admits a linear vertex kernel. Arc-Disjoint Branchings is FPT on strong digraphs. The algorithm for Non-Disconnecting Out-Branching runs in time $$2^{\mathcal {O}(k)}n^{\mathcal {O}(1)}$$ 2 O ( k ) n O ( 1 ) and the approach we use to obtain this algorithms seems useful in designing other moderately exponential time algorithms for edge/arc partitioning problems. Non-Disconnecting Out-Branching is fixed parameter tractable (FPT) and admits a linear vertex kernel. Arc-Disjoint Branchings is FPT on strong digraphs.	cobham's thesis;directed graph;edmonds' algorithm;existential quantification;file spanning;graph (discrete mathematics);graph theory;karp's 21 np-complete problems;kernel (operating system);mathematical optimization;np-completeness;parameterized complexity;polynomial;spanning tree;time complexity	Jørgen Bang-Jensen;Saket Saurabh;Sven Simonsen	2015	Algorithmica	10.1007/s00453-015-0037-3	parameterized complexity;mathematical optimization;combinatorics;discrete mathematics;branching;spanning tree;computer science;mathematics	Theory	22.315190386536784	23.052638630075702	97119
90ae02d2d3aa4a2dbcb7329938546a6dc13fc529	on central spanning trees of a graph	spanning tree	We consider the collection of all spanning trees of a graph with distance between them based on the size of the symmetric difference of their edge sets. A central spanning tree of a graph is one for which the maximal distance to all other spanning trees is minimal. We prove that the problem of constructing a central spanning tree is algorithmically difficult and leads to an NP-complete problem.	algorithm;edge dominating set;file spanning;maximal set;np-completeness;spanning tree	Sergei L. Bezrukov;Firoz Kaderali;Werner Poguntke	1995		10.1007/3-540-61576-8_73	euclidean minimum spanning tree;combinatorics;geometric graph theory;kruskal's algorithm;feedback arc set;minimum degree spanning tree;spanning tree;minimum spanning tree;graph factorization;connected dominating set;k-minimum spanning tree;trémaux tree;reverse-delete algorithm;distributed minimum spanning tree;cycle basis;tree;shortest-path tree	Theory	23.69451066584686	26.816789866741658	97191
017991185d08bc0ca3593e691ec70e2fddd310e0	testing consistency of quartet topologies: a parameterized approach	property testing;evolutionary tree;randomized algorithm;quartet method	Property testing considers the following task: given a function @j over a domain D, a property P and a parameter 0<@e<1, by querying function values of f over o(|D|) elements in D, determine if @j satisfies P or differs from any one which satisfies P in at least @e|D| elements. We focus on consistency of quartet topologies. Given a set Q of quartet topologies over an n-taxon set and an upper bound k on the number of quartets whose topologies are missing, we present a non-adaptive property tester with one-sided error, which runs in O(1.7321^kkn^3/@e) time and uses O(kn^3/@e) queries, to test if Q is consistent with an evolutionary tree.		Maw-Shang Chang;Chuang-Chieh Lin;Peter Rossmanith	2013	Inf. Process. Lett.	10.1016/j.ipl.2013.08.006	combinatorics;discrete mathematics;phylogenetic tree;computer science;property testing;mathematics;randomized algorithm;algorithm	SE	18.82350508567994	22.419024645649735	97252
9e26077af15c3bf6ab9d96c3e11e146354e29fe4	fast algorithms for the maximum convolution problem	algorithme rapide;optimisation;optimizacion;convolution;aproximacion probabilista;convolucion;probabilistic approach;maximum convulution;probabilistic analysis;approche probabiliste;fast algorithm;optimization;algoritmo rapido	We describe two algorithms for solving the maximum convolution problem, i.e. the calculation of c k := maxfa k?i + b i j 0 i n ? 1g for all k with respect to given se-1) of real numbers. Our rst algorithm with expected running time O(n logn) is mainly of theoretical interest while our second algorithm allows a simpler, more practicable implementation and showed quite fast performance in numerical experiments. maximum convolution; probabilistic analysis	algorithm;convolution;experiment;fast fourier transform;numerical analysis;probabilistic analysis of algorithms;time complexity	Michael R. Bussieck;Hannes Hassler;Gerhard J. Woeginger;Uwe T. Zimmermann	1994	Oper. Res. Lett.	10.1016/0167-6377(94)90048-5	mathematical optimization;combinatorics;probabilistic analysis of algorithms;mathematics;convolution;algorithm	Logic	14.264869086180296	30.798744099672945	97312
43b789de47ea72196f21da4747340ebae1ebdcbf	consistency of orthology and paralogy constraints in the presence of gene transfers		Orthology and paralogy relations are often inferred by methods based on gene similarity, which usually yield a graph depicting the relationships between gene pairs. Such relation graphs are known to frequently contain errors, as they cannot be explained via a gene tree that both contains the depicted orthologs/paralogs, and that is consistent with a species tree S. This idea of detecting errors through inconsistency with a species tree has mostly been studied in the presence of speciation and duplication events only. In this work, we ask: could the given set of relations be consistent if we allow lateral gene transfers in the evolutionary model? We formalize this question and provide a variety of algorithmic results regarding the underlying problems. Namely, we show that deciding if a relation graph R is consistent with a given species network N is NP-hard, and that it is W[1]-hard under the parameter “minimum number of transfers”. However, we present an FPT algorithm based on the degree of the DS-tree associated with R. We also study analogous problems in the case that the transfer highways on a species tree are unknown. 1998 ACM Subject Classification F.2 Analysis of algorithms and problem complexity	algorithm;analysis of algorithms;gene regulatory network;homology (biology);lateral thinking;models of dna evolution;parameterized complexity;sensor	Mark Jones;Manuel Lafond;Céline Scornavacca	2017	CoRR		combinatorics;bioinformatics;theoretical computer science	Theory	17.742930534385092	21.995062725259533	97324
61855097d01e8e4f8db65aaec60f4ea36cb27a12	maintaining minimum spanning forests in dynamic graphs	delecion;graph theory;68w40;teoria grafo;foret graphe;information retrieval;arbre maximal;pregunta documental;algorithme deterministe;trees mathematics;theorie graphe;dynamic graph;question documentaire;query languages;theorem proving;algorithme;algorithm;deterministic algorithms;dynamic graphs;arbol maximo;recherche information;computational complexity;data structures;estructura datos;05c85;minimum spanning tree;query;68p05;graph algorithm;bosque grafo;algorithms;68q25;structure donnee;spanning tree;recuperacion informacion;spanning forest;algorithme graphe;foret maximale;forest graph;data structure;problem solving;graphe dynamique;deletion;algoritmo	We present the first fully dynamic algorithm for maintaining a minimum spanning forest in time o( √ n) per operation. To be precise, the algorithm uses O(n1/3 logn) amortized time per update operation. The algorithm is fairly simple and deterministic. An immediate consequence is the first fully dynamic deterministic algorithm for maintaining connectivity and bipartiteness in amortized time O(n1/3 logn) per update, with O(1) worst case time per query.	amortized analysis;best, worst and average case;circuit restoration;data structure;deterministic algorithm;dynamic problem (algorithms);file spanning;graph (discrete mathematics);link/cut tree;maxima and minima;microsoft solutions framework;minimum spanning tree;phylogenetic tree;sorting;time complexity;tree (data structure)	Monika Henzinger;Valerie King	2001	SIAM J. Comput.	10.1137/S0097539797327209	combinatorics;discrete mathematics;amortized analysis;data structure;spanning tree;computer science;proof of o(log*n) time complexity of union–find;minimum spanning tree;mathematics;automated theorem proving;computational complexity theory;algorithm;query language	Theory	19.07633679045537	27.634287904140066	97381
0b2a811c6272298f34f21aa52162d8c7816f4206	a learning theory approach to noninteractive database privacy	noninteractive database privacy;learning theory	In this article, we demonstrate that, ignoring computational constraints, it is possible to release synthetic databases that are useful for accurately answering large classes of queries while preserving differential privacy. Specifically, we give a mechanism that privately releases synthetic data useful for answering a class of queries over a discrete domain with error that grows as a function of the size of the smallest net approximately representing the answers to that class of queries. We show that this in particular implies a mechanism for counting queries that gives error guarantees that grow only with the VC-dimension of the class of queries, which itself grows at most logarithmically with the size of the query class.  We also show that it is not possible to release even simple classes of queries (such as intervals and their generalizations) over continuous domains with worst-case utility guarantees while preserving differential privacy. In response to this, we consider a relaxation of the utility guarantee and give a privacy preserving polynomial time algorithm that for any halfspace query will provide an answer that is accurate for some small perturbation of the query. This algorithm does not release synthetic data, but instead another data structure capable of representing an answer for each query. We also give an efficient algorithm for releasing synthetic data for the class of interval queries and axis-aligned rectangles of constant dimension over discrete domains.	algorithm;apache axis;best, worst and average case;data structure;database;differential privacy;interactivity;linear programming relaxation;p (complexity);polynomial;statistical learning theory;synthetic data;synthetic intelligence;vc dimension	Avrim Blum;Katrina Ligett;Aaron Roth	2013	J. ACM	10.1145/2450142.2450148	computer science;theoretical computer science;learning theory;data mining;database;mathematics;spatial query	DB	12.661465560419977	24.13576093212133	97590
521d637d1b4742d1cb999a9fd351a0e906ef7b9f	amortized dynamic cell-probe lower bounds from four-party communication		"""This paper develops a new technique for proving amortized, randomized cell-probe lower bounds on dynamic data structure problems. We introduce a new randomized nondeterministic four-party communication model that enables """"accelerated"""", error-preserving simulations of dynamic data structures. We use this technique to prove an Ω(n(log n/log log n)2) cell-probe lower bound for the dynamic 2D weighted orthogonal range counting problem (2D-ORC) with n/poly log n updates and n queries, that holds even for data structures with exp(-Ω̃(n)) success probability. This result not only proves the highest amortized lower bound to date, but is also tight in the strongest possible sense, as a matching upper bound can be obtained by a deterministic data structure with worst-case operational time. This is the first demonstration of a """"sharp threshold"""" phenomenon for dynamic data structures. Our broader motivation is that cell-probe lower bounds for exponentially small success facilitate reductions from dynamic to static data structures. As a proof-of-concept, we show that a slightly strengthened version of our lower bound would imply an Ω((log n/log log n)2) lower bound for the static 3D-ORC problem with O(n logO(1) n) space. Such result would give a near quadratic improvement over the highest known static cell-probe lower bound, and break the long standing Ω(log n) barrier for static data structures."""	amortized analysis;best, worst and average case;counting problem (complexity);data structure;dynamic data;dynamization;randomized algorithm;range searching;simulation	Omri Weinstein;Huacheng Yu	2016	2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS)	10.1109/FOCS.2016.41	mathematical optimization;combinatorics;discrete mathematics;mathematics;algorithm	Theory	12.880907480464145	23.641807024685924	97661
6374162dc969f48ccef800671f34bc9ba5ccfec0	error analysis of two parallel algorithms for solving linear recurrence systems	parallelisme;preuve programme;program proof;parallel algorithm;calcul erreur;recurrence;algorithme;algorithm;error analysis;parallelism;paralelismo;recurrencia;prueba programa;calculo error;linear recurrence systems;algoritmo;parallel algorithms	In the paper we present an error analysis of two parallel algorithms for solving linear recurrence systems. We prove that the computed solution is the exact solution of the problem with slightly perturbed input data	linear difference equation;parallel algorithm	Przemyslaw Stpiczynski	1993	Parallel Computing	10.1016/0167-8191(93)90074-U	combinatorics;parallel computing;computer science;theoretical computer science;mathematics;parallel algorithm;algorithm	HPC	13.868255754997156	30.83945964064187	97679
3246d075fc9ff8f3dab19da7c31ef7e7c3ead71c	balanced max 2-sat might not be the hardest	unique games conjecture;max 2 sat;datavetenskap datalogi;inapproximability;computer science	We show that, assuming the Unique Games Conjecture, it is NP-hard to approximate MAX2SAT within αLLZ-+ε, where 0.9401 < αLLZ- < 0.9402 is the believed approximation ratio of the algorithm of Lewin, Livnat and Zwick [28].  This result is surprising considering the fact that balanced instances of MAX2SAT, i.e., instances where each variable occurs positively and negatively equally often, can be approximated within 0.9439. In particular, instances in which roughly 68% of the literals are unnegated variables and 32% are negated appear less amenable to approximation than instances where the ratio is 50% - 50%.	2-satisfiability;approximation algorithm;karloff–zwick algorithm;unique games conjecture	Per Austrin	2006	Electronic Colloquium on Computational Complexity (ECCC)	10.1145/1250790.1250818	mathematical optimization;combinatorics;discrete mathematics;computer science;mathematics;unique games conjecture;algorithm	Theory	17.945564860688947	18.646555814207098	98166
be9f42ae66b634635f2f937667e2e6052972483d	alignments with non-overlapping moves, inversions and tandem duplications in o ( n 4) time	chevauchement;insertion;dynamic programming;string to string comparison block operations scoring schemes;programacion dinamica;alignement sequence;time complexity;probleme np complet;inversion;dynamic programming algorithm;tandem duplication;bioinformatique;dynamic program;overlap;alineacion secuencia;imbricacion;optimisation combinatoire;transposition;complexite temps;insercion;duplication;biological sequence alignment;programmation dynamique;duplicacion;algorithme evolutionniste;problema np completo;algoritmo evolucionista;sequence alignment;bioinformatica;evolutionary algorithm;complejidad tiempo;combinatorial optimization;np complete problem;optimizacion combinatoria;transposicion;bioinformatics	Sequence alignment is a central problem in bioinformatics. The classical dynamic programming algorithm aligns two sequences by optimizing over possible insertions, deletions and substitution. However, other evolutionary events can be observed, such as inversions, tandem duplications or moves (transpositions). It has been established that the extension of the problem to move operations is NP-complete. Previous work has shown that an extension restricted to non-overlapping inversions can be solved in O(n3) with a restricted scoring scheme. In this paper, we show that the alignment problem extended to non-overlapping moves can be solved in O(n5) for general scoring schemes, O(n4 log n) for concave scoring schemes and O(n4) for restricted scoring schemes. Furthermore, we show that the alignment problem extended to nonoverlapping moves, inversions and tandem duplications can be solved with the same time complexities. Finally, an example of an alignment with non-overlapping moves is provided.	inversion (discrete mathematics);tandem computers	Christian Ledergerber;Christophe Dessimoz	2007		10.1007/978-3-540-73545-8_17	mathematical optimization;combinatorics;combinatorial optimization;computer science;dynamic programming;evolutionary algorithm;mathematics;algorithm	Vision	16.186407980458966	24.03533761304523	98178
f2b5abc56d7de1d2f3c2e63efb37fdccc5b9dc66	fast sorting algorithms on uniform ladders (multiple shift-register loops)	period;uniform ladder bubble sort demuth s algorithm loading magnetic bubbles merging odd even transposition sort period shift register loops sorting switches;shift register loops;bubble sort;sorting;uniform ladder;loading;magnetic bubbles;ieee computer society;odd even transposition sort;merging;demuth s algorithm;switches;sorting algorithm;article	This paper presents two sorting algorithms on the uniform ladder (a new storage device based on charged coupled devices, or magnetic bubbles implementation, proposed by Chen et al.). It is assumed that control and comparison timings are negligible when compared to the relatively slow bubble movements. The first algorithm (Algorithm 1) enables the sorting process on a single ladder to be completely embedded in the input/output time (whereas Chen's algorithm (SLISO) has a 20 percent unoverlapped sorting time in the load-sort-unload process). When one ladder cannot accommodate all the input records and two or more ladders are needed, Algorithm 2 attains a negligible unoverlapped sorting time (which can be removed with a minor modification in the system hardware and hence in Algorithm 2). In comparison, Algorithm 2 obviates the need for explicit merging of the ladders, which is required in Chen's algorithm (MLISO). This implies that unlike the MLISO scheme, ladders are not tied up for merging, and can be recycled once their contents are outputted. Therefore, in a real processing environment, the number of ladders required by Algorithm 2 may even be less than the theoretical minimum which can be attained by the MLISO scheme.	embedded system;entity–relationship model;input/output;shift register;sorting algorithm	Francis Y. L. Chin;K. Samson Fok	1980	IEEE Transactions on Computers	10.1109/TC.1980.1675633	proxmap sort;pigeonhole sort;counting sort;parallel computing;hybrid algorithm;network switch;computer science;sorting;period;theoretical computer science;bubble sort;external sorting;sorting algorithm;block sort;in-place algorithm;comparison sort;algorithm;quantum sort;stooge sort	Theory	12.248087369407964	30.501194746616214	98315
bb3a380a464eb2211e3a8c8aada12009834b4bd6	improvements on reductions among different variants of svp and cvp	reduction;gapsvp;search svp;optimization svp;lattice	It is well known that Search SVP is equivalent to Optimization SVP. However, the classical reduction from Search SVP to Optimization SVP by Kannan needs polynomial times of calls to the oracle that solves Optimization SVP. In this paper, a new rank-preserving reduction is presented with only one call to the Optimization SVP oracle. The idea also leads to a similar direct reduction from Search CVP to Optimization CVP with only one call to the corresponding oracle. Both of the reductions above can be generalized for    $$l_p$$         l   p          norm with    $$p\in \mathbb {Z}^{+} $$         p   ∈       Z     +           .#R##N##R##N#On the other hand, whether the search and optimization variants of approximate SVP are computationally equivalent is an outstanding open problem. Recently, Cheng gave a reduction from Search    $$\text {SVP}_\gamma $$         SVP   γ          to Optimization    $$\text {SVP}_{\gamma ^{'}}$$         SVP     γ       '             , where    $$\gamma ^{'}=\gamma ^{\frac{1}{n(n-1)\log _{2}\gamma n}}$$           γ       '       =     γ     1     n     (   n   -   1   )       log   2     γ   n                is much smaller than    $$\gamma $$       γ       . We slightly improve the reduction by making    $$\gamma ^{'}=\gamma ^{\frac{O(\log _{2}n)}{n(n-1)\log _{2}\gamma n}}$$           γ       '       =     γ       O   (     log   2     n   )       n     (   n   -   1   )       log   2     γ   n               . In addition, a reduction from Search    $$\text {CVP}_\gamma $$         CVP   γ          to Optimization    $$\text {CVP}_{\gamma ^{'}}$$         CVP     γ       '              with    $$\gamma ^{'}=\gamma ^{\frac{1}{n\lceil n/2 + \log _{2}\gamma \cdot \text {dist}(t,\mathcal {L}(B))\rceil }}$$           γ       '       =     γ     1     n   i¾?   n   /   2   +     log   2     γ   ·   dist     (   t   ,   L     (   B   )     )     i¾?                is also presented.	lattice problem	Gengran Hu;Yanbin Pan	2013		10.1007/978-3-319-05149-9_3	speech recognition;reduction;lattice;algorithm	ML	12.160194291197346	23.905156824809712	98582
8276746c1fc36e2a2fb57bb2cecff32d6aaee690	fast polynomial-space algorithms using inclusion-exclusion	inclusion-exclusion;fixed parameter tractability;exact algorithms;polynomial space;steiner tree	Given a graph with n vertices, k terminals and positive integer weights not larger than c, we compute a minimum Steiner Tree in $\mathcal{O}^{\star}(2^{k}c)$ time and $\mathcal{O}^{\star}(c)$ space, where the $\mathcal{O}^{\star}$ notation omits terms bounded by a polynomial in the input-size. We obtain the result by defining a generalization of walks, called branching walks, and combining it with the Inclusion-Exclusion technique. Using this combination we also give $\mathcal{O}^{\star}(2^{n})$ -time polynomial space algorithms for Degree Constrained Spanning Tree, Maximum Internal Spanning Tree and #Spanning Forest with a given number of components. Furthermore, using related techniques, we also present new polynomial space algorithms for computing the Cover Polynomial of a graph, Convex Tree Coloring and counting the number of perfect matchings of a graph.	algorithm;degree-constrained spanning tree;graph coloring;pspace;polynomial;steiner tree problem	Jesper Nederlof	2012	Algorithmica	10.1007/s00453-012-9630-x	mathematical optimization;combinatorics;discrete mathematics;topology;mathematics;geometry	Theory	23.577553225463546	22.23359599226193	98598
02ef17464b9c946b06b54671f6dffa15bb00730e	obdds in heuristic search	diagrama binaria decision;diagramme binaire decision;algoritmo busqueda;algorithm complexity;algorithm analysis;algorithme recherche;complejidad algoritmo;search algorithm;approche heuristique;optimisation combinatoire;heuristic search;search trees;complexite algorithme;enfoque heuristico;analyse algorithme;heuristic approach;combinatorial optimization;breadth first search;analisis algoritmo;lower bound;optimizacion combinatoria;binary decision diagram	The use of a lower bound estimate in the search has a tremendous impact on the size of the resulting search trees, whereas OBDDs can be used to e ciently describe sets of states based on their binary encoding. This paper combines these two ideas to a new algorithm BDDA . It challenges both, the breadthrst search using OBDDs and the traditional A algorithm. The problem with A is that in many applications areas the set of states is too huge to be kept in main memory. In contrary, brute-force breadthrst search using OBDDs unnecessarily expands several nodes. Therefore, we exhibit a new trade-o between time and space requirements and tackle the most important problem in heuristic search to overcome space limitations while avoiding a strong penalty in time. We evaluate our approach in the (n 1)-Puzzle and within Sokoban.	admissible heuristic;algorithm;binary file;computer data storage;iteration;requirement;search tree;sokoban	Stefan Edelkamp;Frank Reffel	1998		10.1007/BFb0095430	beam search;mathematical optimization;combinatorics;breadth-first search;combinatorial optimization;computer science;mathematics;upper and lower bounds;binary decision diagram;algorithm;search algorithm	AI	15.980884370021272	26.69618466331386	98650
15aecc1f3af1361e1d8296109dbe0d0e8ba8e7a4	making bound consistency as effective as arc consistency	arc consistency;polynomial algorithm	We study under what conditions bound consistency (BC) and arc consistency (AC), two forms of propagation used in constraint solvers, are equivalent to each other. We show that they prune exactly the same values when the propagated constraint is connected row convex / closed under median and its complement is row convex. This characterization is exact for binary constraints. Since row convexity depends on the order of the values in the domains, we give polynomial algorithms for computing orders under which BC and AC are equivalent, if any.	algorithm;consistency model;intension;local consistency;polynomial;propagator;software propagation	Christian Bessiere;Thierry Petit;Bruno Zanuttini	2003			mathematical optimization;combinatorics;discrete mathematics;row equivalence;computer science;mathematics;local consistency	AI	22.929723880715184	24.13223839726962	98662
f01ed7780538a8980a724058e9ada420e8bc4235	optimized lossless compression of remote sensing image files using splay trees	geophysical image processing;splay trees;image coding;trees mathematics data compression geophysical image processing image coding remote sensing search problems;data compression;binary search trees;photogrammetrie und bildanalyse;compression algorithms;self adjustment;trees mathematics;optimized lossless compression;remote sensing image files;image coding transforms binary search trees compression algorithms adaptive coding probability distribution;adaptive coding;remote sensing;probability distribution;transforms;search problems;binary search trees optimized lossless compression remote sensing image files splay trees amortized complexity self adjustment;amortized complexity	In this paper will be applied concepts like amortized complexity or self-adjustment to binary search trees. Motivation comes from the fact that the search trees have multiples drawbacks. It will be developed and analyzed the splay tree, a self-adjusting form of binary search trees.	amortized analysis;lossless compression;search tree;splay tree	Radu Radescu;Andreea Honciuc;Mihai Datcu	2012	2012 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2012.6350398	data compression;random binary tree;potential method;optimal binary search tree;computer vision;red–black tree;binary search tree;geometry of binary search trees;computer science;theoretical computer science;machine learning;splay tree;k-d tree;weight-balanced tree;ternary search tree;statistics	Embedded	15.481708894855089	29.77561929287034	98923
37bd5d48e9474e41bc083e983ee8da509275b46a	graph matching and clustering using spectral partitions	graph partition;inexact graph matching;graph matching;laplacian matrix;spectral clustering;graph partitioning;graph simplification;spectral method;parallel computer;fiedler vector;graph spectral clustering	Although inexact graph-matching is a problem of potentially exponential complexity, the problem may be simplified by decomposing the graphs to be matched into smaller subgraphs. If this is done, then the process may cast into a hierarchical framework and hence rendered suitable for parallel computation. In this paper we describe a spectral method which can be used to partition graphs into nonoverlapping subgraphs. In particular, we demonstrate how the Fiedler-vector of the Laplacian matrix can be used to decompose graphs into non-overlapping neighbourhoods that can be used for the purposes of both matching and clustering. 2005 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.	algebraic connectivity;cluster analysis;computation;edit distance;graph (discrete mathematics);induced subgraph;laplacian matrix;matching (graph theory);parallel computing;pattern recognition;spectral method;time complexity	Huaijun Qiu;Edwin R. Hancock	2006	Pattern Recognition	10.1016/j.patcog.2005.06.014	outerplanar graph;graph energy;mathematical optimization;split graph;factor-critical graph;combinatorics;discrete mathematics;graph bandwidth;laplacian matrix;null graph;graph property;graph partition;clique-width;comparability graph;cubic graph;mathematics;voltage graph;butterfly graph;spectral graph theory;complement graph;line graph;strength of a graph;coxeter graph;adjacency matrix	Vision	20.464603881921917	30.45411188089192	98930
7200db72db5848f15854a6f61f40735360c498fb	self-bounded prediction suffix tree via approximate string matching		Lemma 5. Let !(i, k) = ((1 ⇠)k i exp( )/i!)1/2. Given binary sequence y 1 1 and arbitrary suffix tree T equipped with Hamming distance metric, t 1 i=1 ✏ k=0 P s:s2T ,d(s,y 1 t i )=k ! 2 (i, k)  e + e (1 ⇠) (t, ( 2 + ⇠))/ (t) for all > 0, ✏ 0, and 0 < ⇠ < 1. Proof. Given a binary sequence of length i, there are at most i k possible suffixes that are exactly k-Hamming distance away from the original sequence if i k, otherwise 0.2 Therefore the sum of ! can be bounded by the number of possible approximate suffixes as t 1 X	approximate string matching;approximation algorithm;bitstream;exptime;hamming distance;prediction suffix tree	Dongwoo Kim;Christian J. Walder	2018			suffix tree;machine learning;pattern recognition;approximate string matching;mathematics;suffix;artificial intelligence;bounded function	ML	12.881241860580372	26.28432417501278	99036
51f63fa4d3665f892a0093096770931a4df5b324	satisfiability threshold for random regular nae-sat	satisfiability threshold;replica symmetry breaking;condensation;constraint satisfaction problem;survey propagation	We consider the random regular <i>k</i>-nae-sat problem with <i>n</i> variables each appearing in exactly <i>d</i> clauses. For all <i>k</i> exceeding an absolute constant <i>k</i><sub>0</sub>, we establish explicitly the satisfiability threshold <i>d</i><sub>*</sub> ∈ <i>d</i><sub>*</sub>(<i>k</i>). We prove that for <i>d</i> < <i>d</i><sub>*</sub> the problem is satisfiable with high probability while for <i>d</i> > <i>d</i><sub>*</sub> the problem is unsatisfiable with high probability. If the threshold <i>d</i><sub>*</sub> lands exactly on an integer, we show that the problem is satisfiable with probability bounded away from both zero and one. This is the first result to locate the exact satisfiability threshold in a random constraint satisfaction problem exhibiting the condensation phenomenon identified by Krzakał a et al. (2007). Our proof verifies the onestep replica symmetry breaking formalism for this model. We expect our methods to be applicable to a broad range of random constraint satisfaction problems and combinatorial problems on random graphs.	boolean satisfiability problem;constraint satisfaction problem;random graph;semantics (computer science);symmetry breaking;with high probability	Jian Ding;Allan Sly;Nike Sun	2014		10.1145/2591796.2591862	combinatorics;discrete mathematics;computer science;mathematics;constraint satisfaction problem;algorithm;condensation	Theory	11.566346707339589	19.337244545822077	99391
01ae802556fc8a4fd3310fc1fbbe312bd5c7b96c	on learning graphs with edge-detecting queries		We consider the problem of learning a general graph G = (V,E) using edge-detecting queries, where the number of vertices |V | = n is given to the learner. The information theoretic lower bound gives m log n for the number of queries, where m = |E| is the number of edges. In case the number of edges m is also given to the learner, Angluin-Chen’s Las Vegas algorithm Angluin and Chen (2008) runs in 4 rounds and detects the edges in O(m log n) queries. In the other harder case where the number of edges m is unknown, their algorithm runs in 5 rounds and asks O(m log n + √ m log n) queries. There have been two open problems: (i) can the number of queries be reduced to O(m log n) in the second case, and, (ii) can the number of rounds be reduced without substantially increasing the number of queries (in both cases). For the first open problem (when m is unknown) we give two algorithms. The first is an O(1)-round Las Vegas algorithm that asks m log n + √ m(log n) log n queries for any constant k where log n = log k · · · log n. The second is an O(log∗ n)-round Las Vegas algorithm that asks O(m log n) queries. This solves the first open problem for any practical n, for example, n < 2. We also show that no deterministic algorithm can solve this problem in a constant number of rounds. To solve the second problem we study the case when m is known. We first show that any non-adaptive Monte Carlo algorithm (one-round) must ask at least Ω(m log n) queries, and any two-round Las Vegas algorithm must ask at least m4/3−o(1) log n queries on average. We then give two two-round Monte Carlo algorithms, the first asks O(m log n) queries for any n and m, and the second asks O(m log n) queries when n > 2. Finally, we give a 3-round Monte Carlo algorithm that asks O(m log n) queries for any n and m.	deterministic algorithm;edge dominating set;entity–relationship model;las vegas algorithm;log-space reduction;monte carlo algorithm;sensor;theory	Hasan Abasi;Nader H. Bshouty	2018	CoRR		discrete mathematics;monte carlo algorithm;las vegas algorithm;open problem;vertex (geometry);upper and lower bounds;mathematics;binary logarithm;graph	Theory	13.858833830424793	20.795387211728304	99421
da7e3f7f15f660b30b542fbd92659a5d664f60ed	on the complexity of random quantum computations and the jones polynomial		There is a natural relationship between Jones polynomials and quantum computation. We use this relationship to show that the complexity of evaluating relative-error approximations of Jones polynomials can be used to bound the classical complexity of approximately simulating random quantum computations. We prove that random quantum computations cannot be classically simulated up to a constant total variation distance, under the assumption that (1) the Polynomial Hierarchy does not collapse and (2) the average-case complexity of relative-error approximations of the Jones polynomial matches the worst-case complexity over a constant fraction of random links. Our results provide a straightforward relationship between the approximation of Jones polynomials and the complexity of random quantum computations.	approximation error;average-case complexity;best, worst and average case;computation;jones calculus;jones polynomial;polynomial hierarchy;quantum computing;simulation;worst-case complexity	Ryan L. Mann;Michael J. Bremner	2017	CoRR		discrete mathematics;bracket polynomial;structural complexity theory;combinatorics;quantum algorithm;quantum complexity theory;polynomial;quantum computer;polynomial hierarchy;mathematics;ph	Theory	10.858394656140172	21.0029862347063	99619
9f74fae4acbeda1c3712338d0acaeb7d7ad31880	a constant-factor approximation algorithm for co-clustering		Co-clustering is the simultaneous partitioning of the rows and columns of a matrix such that the blocks induced by the row/column partitions are good clusters. Motivated by several applications in text mining, market-basket analysis, and bioinformatics, this problem has attracted a lot of attention in the past few years. Unfortunately, to date, most of the algorithmic work on this problem has been heuristic in nature. In this work we obtain the first approximation algorithms for the co-clustering problem. Our algorithms are simple and provide constant-factor approximations to the optimum. We also show that co-clustering is NP-hard, thereby complementing our algorithmic result. ACM Classification: F.2.0 AMS Classification: 68W25	acm computing classification system;apx;affinity analysis;approximation algorithm;biclustering;bioinformatics;cluster analysis;co-np;column (database);heuristic;np-hardness;order of approximation;simultaneous multithreading;text mining	Aris Anagnostopoulos;Anirban Dasgupta;Ravi Kumar	2012	Theory of Computing	10.4086/toc.2012.v008a026	mathematical optimization;combinatorics;machine learning;mathematics;algorithm	DB	20.28219041348836	18.676164495699147	99638
ec414204729bc4c1cc3edbc7e1f70eb06543cf4f	beating the random ordering is hard: every ordering csp is approximation resistant	computer and information science;68q17;unique games conjecture;feedback arc set;data och informationsvetenskap;hardness of approximation;integrality gaps;maximum acyclic subgraph	We prove that, assuming the Unique Games Conjecture (UGC), every problem in the class of ordering constraint satisfaction problems (OCSP) where each constraint has constant arity is approximation resistant. In other words, we show that if ρ is the expected fraction of constraints satisfied by a random ordering, then obtaining a ρ′ approximation, for any ρ′ > ρ is UG-hard. For the simplest ordering CSP, the Maximum Acyclic Subgraph (MAS) problem, this implies that obtaining a ρ-approximation, for any constant ρ > 1/2 is UG-hard. Specifically, for every constant ε > 0 the following holds: given a directed graph G that has an acyclic subgraph consisting of a fraction (1− ε) of its edges, it is UG-hard to find one with more than (1/2 + ε) of its edges. Note that it is trivial to find an acyclic subgraph with 1/2 the edges, by taking either the forward or backward edges in an arbitrary ordering of the vertices of G. The MAS problem has been well studied and beating the random ordering for MAS has been a basic open problem. An OCSP of arity k is specified by a subset Π ⊆ Sk of permutations on {1, 2, . . . , k}. An instance of such an OCSP is a set V and a collection of constraints each of which is an ordered k-tuple of V . The objective is to find a global linear ordering of V while maximizing the number of constraints ordered as in Π. A random ordering of V is expected to satisfy a ρ = |Π| k! fraction. We show that, for any fixed k, it is hard to obtain a ρ′-approximation for Π-OCSP for any ρ′ > ρ. The result is in fact stronger: we show that for every Λ ⊆ Π ⊆ Sk, and an arbitrarily small ε, it is hard to distinguish instances where a (1 − ε) fraction of the constraints can be ordered according to Λ; from instances where at most a ρ + ε fraction can be ordered as in Π. A special case of our result is that the Betweenness problem is hard to approximate beyond a factor 1/3. The results naturally generalize to OCSPs which assign a payoff to the different permutations. Finally, our results imply (unconditionally) that a simple semidefinite relaxation for MAS does not suffice to obtain a better approximation. ∗Computer Science Department, Carnegie Mellon University, Pittsburgh, PA 15213. Some of this work was done while visiting the School of Mathematics, Institute for Advanced Study, Princeton, NJ. Research supported in part by a Packard Fellowship, and NSF grants CCF-0343672 and CCF-0963975. Email: guruswami@cmu.edu †School of Computer Science and Communication, KTH, Sweden. Email: johanh@kth.se. Research supported by ERC grant 226203. ‡College of Computing, Georgia Institute of Technology, Atlanta, GA. Some of this work was done while visiting Princeton University and when at Microsoft Research New England, Cambridge, MA. Supported in part by NSF grant CCF-0343672. Email: praghave@cc.gatech.edu §Department of Computer Science, Princeton University, NJ. Supported by NSF grants MSPA-MCS 0528414, and ITR 0205594. Email: {rajsekar,moses}@cs.princeton.edu ¶Preliminary versions appeared in the Proceedings of the 49th Annual IEEE Symposium on Foundations of Computer Science, 2008 [14] and the 24th IEEE Conference on Computational Complexity, 2009 [6].	approximation algorithm;betweenness;computational complexity theory;constraint satisfaction problem;directed acyclic graph;directed graph;email;ibm notes;linear programming relaxation;microsoft research;online certificate status protocol;relaxation (approximation);software release life cycle;symposium on foundations of computer science;three utilities problem;unique games conjecture;user-generated content	Venkatesan Guruswami;Johan Håstad;Rajsekar Manokaran;Prasad Raghavendra;Moses Charikar	2011	Electronic Colloquium on Computational Complexity (ECCC)	10.1137/090756144	mathematical optimization;combinatorics;discrete mathematics;feedback arc set;computer science;mathematics;hardness of approximation;unique games conjecture;algorithm;algebra	Theory	20.164791254185403	19.344178124586634	99654
95f3d153600a17477a2e3b33d6cd36361f8198ea	distance oracles for unweighted graphs: breaking the quadratic barrier with constant additive error	shortest path;qa mathematics;weighted graph;data structure	Thorup and Zwick, in the seminal paper [Journal of ACM, 52(1), 2005, pp 1-24], showed that a weighted undirected graph on n vertices can be preprocessed in subcubic time to design a data structure which occupies only subquadratic space, and yet, for any pair of vertices, can answer distance query approximately in constant time. The data structure is termed as approximate distance oracle. Subsequently, there has been improvement in their preprocessing time, and presently the best known algorithms [4, 3] achieve expected O(n) preprocessing time for these oracles. For a class of graphs, these algorithms indeed run in Θ(n) time. In this paper, we are able to break this quadratic barrier at the expense of introducing a (small) constant additive error for unweighted graphs. In achieving this goal, we have been able to preserve the optimal size-stretch trade offs of the oracles. One of our algorithms can be extended to weighted graphs, where the additive error becomes 2 · wmax(u, v) here wmax(u, v) is the heaviest edge in the shortest path between vertices u, v.	additive model;approximation algorithm;data structure;distance oracle;emoticon;graph (discrete mathematics);karloff–zwick algorithm;oracle machine;preprocessor;shortest path problem;time complexity;utility functions on indivisible goods;vertex (geometry)	Surender Baswana;Akshay Gaur;Sandeep Sen;Jayant Upadhyay	2008		10.1007/978-3-540-70575-8_50	mathematical optimization;combinatorics;discrete mathematics;data structure;computer science;mathematics;shortest path problem;programming language;distance;algorithm;statistics	Theory	20.81229977787506	22.13235634177235	99823
b9d0df068ddfad3a6f09f3c527d35a8d2524e115	on the most imbalanced orientation of a graph	graph theory;orientation;complexity;mixed integer programming;cutting plane algorithm;(in)approximability;cactus	We study the problem of orienting the edges of a graph such that the minimum over all the vertices of the absolute difference between the outdegree and the indegree of a vertex is maximized. We call this minimum the imbalance of the orientation, i.e. the higher it gets, the more imbalanced the orientation is. The studied problem is denoted by \({{\mathrm{\textsc {MaxIm}}}}\). We first characterize graphs for which the optimal objective value of \({{\mathrm{\textsc {MaxIm}}}}\) is zero. Next we show that \({{\mathrm{\textsc {MaxIm}}}}\) is generally NP-hard and cannot be approximated within a ratio of \(\frac{1}{2}+\varepsilon \) for any constant \(\varepsilon >0\) in polynomial time unless \(\texttt {P}=\texttt {NP}\) even if the minimum degree of the graph \(\delta \) equals 2. Then we describe a polynomial-time approximation algorithm whose ratio is almost equal to \(\frac{1}{2}\). An exact polynomial-time algorithm is also derived for cacti. Finally, two mixed integer linear programming formulations are presented. Several valid inequalities are exhibited with the related separation algorithms. The performance of the strengthened formulations is assessed through several numerical experiments.		Walid Ben-Ameur;Antoine Glorieux;José Neto	2018	J. Comb. Optim.	10.1007/s10878-017-0117-1	mathematical optimization;combinatorics;discrete mathematics;mathematics	NLP	23.702688480497486	19.891024099385724	99850
e5ace4c17f9621d8bb0ca04a4324762216e3c521	pseudorandom functions: three decades later		In 1984, Goldreich, Goldwasser and Micali formalized the concept of pseudorandom functions and proposed a construction based on any length-doubling pseudorandom generator. Since then, pseudorandom functions have turned out to be an extremely influential abstraction, with applications ranging from message authentication to barriers in proving computational complexity lower bounds. In this tutorial we survey various incarnations of pseudorandom functions, giving self-contained proofs of key results from the literature. Our main focus is on feasibility results and constructions, as well as on limitations of (and induced by) pseudorandom functions. Along the way we point out some open questions that we believe to be within reach of current techniques. I have set up on a Manchester computer a small programme using only 1000 units of storage, whereby the machine supplied with one sixteen figure number replies with another within two seconds. I would defy anyone to learn from these replies sufficient about the programme to be able to predict any replies to untried values. A. TURING (from [GGM84]) ∗This survey appeared in the book Tutorials on the Foundations of Cryptography, published in honor of Oded Goldreichs 60th birthday. †Dept. of Computer Science and Engineering and Institute of Theoretical Computer Science and Communications, Chinese University of Hong Kong. andrejb@cse.cuhk.edu.hk ‡Efi Arazi School of Computer Science, IDC Herzliya. alon.rosen@idc.ac.il	computational complexity theory;computer engineering;cryptography;data center;message authentication;period-doubling bifurcation;pseudorandom function family;pseudorandom generator;pseudorandom number generator;pseudorandomness;theoretical computer science	Andrej Bogdanov;Alon Rosen	2017	Electronic Colloquium on Computational Complexity (ECCC)	10.1007/978-3-319-57048-8_3	mathematics;discrete mathematics;pseudorandom generator;pseudorandom function family;computational complexity theory;pseudorandom number generator;abstraction;ranging;message authentication code;linear cryptanalysis	Theory	11.796222831392589	21.264845939620805	99904
3714f2a16ffcdc658da9bf890b410bb5ffdf8284	reachability determination in acyclic petri nets by cell enumeration approach	computacion informatica;linear diophantine equations on bounded integer set;diophantine equation;grupo de excelencia;cell enumeration;upper bound;arrangement of hyperplanes;ciencias basicas y experimentales;discrete geometry;petri nets;petri net;reachability analysis	Reachability is one of the most important behavioral properties of Petri nets. We propose in this paper a novel approach for solving the fundamental equation in the reachability analysis of acyclic Petri nets, which has been known to be NP-complete. More specifically, by adopting a revised version of the cell enumeration method for an arrangement of hyperplanes in discrete geometry, we develop an efficient solution scheme to identify firing count vector solution(s) to the fundamental equation on a bounded integer set, with a complexity bound ofO((nu)n−m), where n is the number of transitions,m is the number of places and u is the upper bound of the number of firings for all individual transitions. © 2011 Elsevier Ltd. All rights reserved.	cell (microprocessor);directed acyclic graph;np-completeness;petri net;reachability	Duan Li;Xiaoling Sun;Jianjun Gao;Shenshen Gu;Xiaojin Zheng	2011	Automatica	10.1016/j.automatica.2011.06.017	discrete geometry;combinatorics;discrete mathematics;stochastic petri net;mathematics;petri net;algebra	Logic	19.401677591157224	27.32790137735778	100077
cc895f29c09575d4abc5c6817702abb54db21277	on the complexity of reinforcement in graphs		We show that the decision problem for p-reinforcement, p-total reinforcement, total restrained reinforcement, and k-rainbow reinforcement are NP-hard for bipartite graphs.	decision problem	Nader Jafari Rad	2016	Discussiones Mathematicae Graph Theory	10.7151/dmgt.1898	mathematics;discrete mathematics;combinatorics;reinforcement;graph	ML	23.299767139333948	25.254888092194623	100131
4448941bd4bbbadf11dc3a52d522bbf294aa7018	constructing iceberg lattices from frequent closures using generators	iceberg lattices;frequent generators;concept lattices;association rules;transversal hypergraph;pattern mining;hypergraphs;precedence relation;frequent closed itemsets;concept analysis;association rule;formal concept analysis	Frequent closures (FCIs) and generators (FGs) as well as the precedence relation on FCIs are key components in the de nition of a variety of association rule bases. Although their joint computation has been studied in concept analysis, no scalable algorithm exists for the task at present. We propose here to reverse a method from the latter eld using a fundamental property of hypergraph theory. The goal is to extract the precedence relation from a more common mining output, i.e. closures and generators. The resulting order computation algorithm proves to be highly e cient, bene ting from peculiarities of generator families in typical mining datasets. Due to its genericity, the new algorithm ts an arbitrary FCI/FG-miner.	algorithm;association rule learning;computation;data mining;discrete mathematics;formal concept analysis;full configuration interaction;generic programming;job control (unix);polynomial;scalability	Laszlo Szathmary;Petko Valtchev;Amedeo Napoli;Robert Godin	2008		10.1007/978-3-540-88411-8_15	discrete mathematics;association rule learning;computer science;formal concept analysis;machine learning;data mining;mathematics;algorithm	ML	20.970185865978713	32.144110085667066	100135
eb96484bd5f6b98b782337de4d05b82cbdf1bc1f	heuristic graph coloring algorithm	graph coloring		algorithm;graph coloring;heuristic	Jihad Mohamad Jaam;Ahmad Hasnah	2002	Egyptian Computer Science Journal		control engineering;engineering;list coloring;mathematical optimization;graph power;complete coloring;greedy coloring;edge coloring;fractional coloring;voltage graph;graph coloring	Theory	24.556720108063303	28.572358195279662	100229
7d038648e5df37a89c12aa3b9a10b635f7cf3154	the limits of depth reduction for arithmetic formulas: it's all about the top fan-in	lower bounds;68w30;68q17;arithmetic circuits;depth reduction;12y05	In recent years, a very exciting and promising method for proving lower bounds for arithmetic circuits has been proposed. This method combines the method of depth reduction developed in the works of Agrawal and Vinay [FOCS, IEEE, Piscataway, NJ, 2008, pp. 67--75], Koiran [Theoret. Comput. Sci., 448 (2012), pp. 56--65], and Tavenas [Inform. and Comput., 240 (2015), pp, 2--11], and the use of the shifted partial derivative complexity measure developed in the works of Kayal [Electronic Colloquium on Computational Complexity, TR12-081, 2012] and Gupta et al. [J. ACM, 61 (2014), 33]. These results inspired a flurry of other beautiful results and strong lower bounds for various classes of arithmetic circuits, in particular a recent work of Kayal, Saha, and Saptharishi [STOC, ACM, New York, 2014, pp. 146--153] showing superpolynomial lower bounds for regular arithmetic formulas via an improved depth reduction for these formulas. It was left as an intriguing question if these methods could prove superpolynomial l...		Mrinal Kumar;Shubhangi Saraf	2015	SIAM J. Comput.	10.1137/140999220	mathematical optimization;combinatorics;computer science;calculus;mathematics;algorithm;algebra	Theory	11.818642287028387	21.29496150259538	100246
7f4a3b9f60dcc13aee084bea8f0393cc77fc2114	cardinality of relations and relational approximation algorithms	relation algebra;approximation algorithm	First, we discuss three specific classes of relations, which allow to model the essential constituents of graphs, such as vertices and (directed or undirected) edges. Based on Kawahara’s characterisation of the cardinality of relations we then derive fundamental properties on their cardinalities. As main applications of these results, we formally verify four relational programs, which implement approximation algorithms by using the assertion-based method and relation-algebraic calculations.	approximation algorithm;assertion (software development);cardinality (data modeling);graph (discrete mathematics);linear algebra;vertex (graph theory)	Rudolf Berghammer;Peter Höfner;Insa Stucke	2016	J. Log. Algebr. Meth. Program.	10.1016/j.jlamp.2015.12.001	combinatorics;discrete mathematics;mathematics;algorithm	Theory	20.65595301291934	32.011360888516265	100262
19953d9c09c731e4b905e66be80915713db5d3a4	an õ(n2.5)-time algorithm for online topological ordering	topological ordering ams subject classification. 68w01;online algorithm;68w40;directed acyclic graph	We present an Õ(n)-time algorithm for maintaining the topological order of a directed acyclic graph with n vertices while inserting m edges. This is an improvement over the previous result of O(n) by Ajwani, Friedrich, and Meyer.	algorithm;directed acyclic graph;topological sorting	Hsiao-Fei Liu;Kun-Mao Chao	2008	CoRR			Theory	24.55495326680891	27.35778327433258	100556
959e70116a93959b4a4adf1ccce952b83700d051	approximation performance of the (1+1) evolutionary algorithm for the minimum degree spanning tree problem	会议论文	Evolutionary algorithms (EAs) are stochastic heuristic algorithms which are often successfully used for solving many optimization problems. However, the rigorous theoretical analysis results on the behavior of EAs on combinatorial optimizations are comparatively scarce, especially for NP-hard optimization problems. In this paper, we theoretically investigate the approximation performance of the (1+1) EA, a simple version of EAs on the minimum degree spanning tree (MDST) problem which is a classical NP-hard optimization problem. We show that the (1+1) EA can obtain an approximate solution for the MDST problem with maximum degree at most (O(varDelta _{opt}+log n)) in expected polynomial runtime (O(m^2n^3+mlog n)), where (varDelta _{opt}) is the maximal degree of the optimal spanning tree. It implies that EAs can obtain solutions with guaranteed performance on the MDST problem.	approximation;evolutionary algorithm;file spanning;like button;spanning tree;steiner tree problem	Xiaoyun Xia;Yuren Zhou	2015		10.1007/978-3-662-49014-3_45	edmonds' algorithm;euclidean minimum spanning tree;mathematical optimization;kruskal's algorithm;minimum degree spanning tree;spanning tree;steiner tree problem;prim's algorithm;expected linear time mst algorithm;minimum spanning tree;gomory–hu tree;connected dominating set;k-minimum spanning tree;reverse-delete algorithm;distributed minimum spanning tree;shortest-path tree	Theory	21.412460346798117	18.577182828610933	100798
055cc877e0214304094e1238190f7864c6826b21	the many facets of upper domination		This paper studies Upper Domination, i.e., the problem of computing the maximum cardinality of a minimal dominating set in a graph with respect to classical and parameterised complexity as well as approximability.	analysis of algorithms;approximation algorithm;degree (graph theory);dominating set;graph coloring;parameterized complexity;pathwidth	Cristina Bazgan;Ljiljana Brankovic;Katrin Casel;Henning Fernau;Klaus Jansen;Kim-Manuel Klein;Michael Lampis;Mathieu Liedloff;Jérôme Monnot;Vangelis Th. Paschos	2018	Theor. Comput. Sci.	10.1016/j.tcs.2017.05.042	discrete mathematics;cardinality;combinatorics;domination analysis;mathematics;dominating set;graph	AI	23.831655929771767	24.17968689606058	100900
3fdb43f4ab0cfb3883b02838dfb4651df0a2b307	hardness of approximating the closest vector problem with pre-processing	closest vector problem;lattices cryptography polynomials mathematics application software computer science gaussian processes equations computer applications educational institutions;computational complexity;approximation hardness closest vector problem complexity assumption nearest codeword problem;hardness of approximation	We show that, unless NP/spl sube/DTIME(2/sup poly log(n)/) the closest vector problem with pre-processing, for /spl lscr//sub p/ norm for any p /spl ges/ 1, is hard to approximate within a factor of (log n)/sup 1/p - /spl epsi//' /P for any /spl epsi/ > 0. This improves the previous best factor of 3/sup 1/p/ - /spl epsi/ due to Regev (2004). Our results also imply that under the same complexity assumption, the nearest codeword problem with pre-processing is hard to approximate within a factor of (log n)/sup 1 - /spl epsi//' for any /spl epsi/ > 0.	lattice problem	Mikhail Alekhnovich;Subhash Khot;Guy Kindler;Nisheeth K. Vishnoi	2005		10.1109/SFCS.2005.40	mathematical optimization;combinatorics;discrete mathematics;computer science;theoretical computer science;mathematics;hardness of approximation;computational complexity theory;algorithm	Theory	11.510621082194383	23.10399657854275	100999
f2e1f74e9a594cdaff27c54d62e99a62f4db8d7b	efficient transitive closure of sparse matrices over closed semirings	linguistique;typed feature logic;efficiency;cerradura transitiva;large data sets;calculo automatico;sparse matrix multiplication;computing;calcul automatique;multiplication matrice;parsing;semi anneau;eficacia;linguistica;matrice creuse;fermeture transitive;semirings;dijkstra s algorithm;informatique theorique;estructura datos;algorithme dijkstra;efficacite;transitive closure;structure donnee;sparse matrix;data structure;sparse matrices;matriz dispersa;computer theory;informatica teorica;linguistics	This paper surveys several alternative data structures and algorithms for multiplying sparse upper-triangular matrices over closed semirings, and evaluates their efficiency in computing transitive closures of matrices over the Boolean semiring. Two new variants are introduced that outperform previously known methods on a collection of large data-sets drawn from linguistic applications.	sparse matrix;transitive closure	Gerald Penn	2006	Theor. Comput. Sci.	10.1016/j.tcs.2005.11.008	combinatorics;discrete mathematics;data structure;sparse matrix;computer science;kleene algebra;mathematics;programming language;algorithm	ECom	14.300482866492903	30.12765624691413	101200
f11b097ef3ac6aeb199500874d441d9591f862b2	minimal perfect hashing: a competitive method for indexing internal memory	hash chain;indexing internal memory;move to front;minimal perfect hashing;hash table;indexation;comparative method;hash function	A perfect hash function (PHF) is an injective function that maps keys from a set S to unique values. Since no collisions occur, each key can be retrieved from a hash table with a single probe. A minimal perfect hash function (MPHF) is a PHF with the smallest possible range, that is, the hash table size is exactly the number of keys in S. MPHFs are widely used for memory efficient storage and fast retrieval of items from static sets. Differently from other hashing schemes, MPHFs completely avoid the problem of wasted space and wasted time to deal with collisions. Until recently, the amount of space to store an MPHF description for practical implementations found in the literature was O(logn) bits per key and therefore similar to the overhead of space of other hashing schemes. Recent results on MPHFs presented in the literature changed this scenario: an MPHF can now be described by approximately 2.6 bits per key. The objective of this paper is to show that MPHFs are, after the new recent results, a good option to index internal memory when static key sets are involved and both successful and unsuccessful searches are allowed. We have shown that MPHFs provide the best tradeoff between space usage and lookup time when compared with other open addressing and chaining hash schemes such as linear hashing, quadratic hashing, double hashing, dense hashing, cuckoo hashing, sparse hashing, hopscotch hashing, chaining with move to front heuristic and exact fit. We considered lookup time for successful and unsuccessful searches in two scenarios: (i) the MPHF description fits in the CPU cache and (ii) the MPHF description does not fit entirely in the CPU cache. Considering lookup time, the minimal perfect hashing outperforms the other hashing schemes in the two scenarios and, in the first scenario, the performance is better even when the compared methods leave more than 80% of the hash table entries free. Considering space overhead (the amount of used space other than the key-value pairs), the minimal perfect hashing is within a factor of O(logn) bits lower than the other hashing schemes for both scenarios.	computer data storage;perfect hash function	Fabiano C. Botelho;Anísio Lacerda;Guilherme Vale Menezes;Nivio Ziviani	2011	Inf. Sci.	10.1016/j.ins.2009.12.003	feature hashing;hopscotch hashing;hash table;double hashing;hash function;linear hashing;perfect hash function;extendible hashing;dynamic perfect hashing;open addressing;computer science;consistent hashing;theoretical computer science;tabulation hashing;universal hashing;hash chain;comparative method;database;mathematics;k-independent hashing;distributed computing;cuckoo hashing;locality preserving hashing;coalesced hashing;programming language;suha;2-choice hashing;locality-sensitive hashing	DB	10.244626230210864	29.42936916916808	101427
8df88bb9fa609294754b3618ca8df4727ed19aab	d-width: a more natural measure for directed tree width	graphe non oriente;non directed graph;descomposicion grafo;digraph;metodo minimax;minimax method;digrafo;desir;deseo;grafo no orientado;informatique theorique;directed graph;graphe oriente;methode minimax;grafo orientado;tree decomposition;treewidth;anchura arbol;largeur arborescente;desire;graph decomposition;computer theory;decomposition graphe;informatica teorica;digraphe	Due to extensive research on tree-width for undirected graphs and due to its many applications in various fields it has been a natural desire for many years to generalize the idea of tree decomposition to directed graphs, but since many parameters in tree-width behave very differently in the world of digraphs, the generalization is still in its preliminary steps.#R##N##R##N#In this paper, after surveying the main work that has been done on this topic, we propose a new simple definition for directed tree-width and prove a special case of the min-max theorem (duality theorem) relating haven order, bramble number, and tree-width on digraphs. We also compare our definition with previous definitions and study the behavior of some tree-width like parameters such as brambles and havens on digraphs.	treewidth	Mohammad Ali Safari	2005		10.1007/11549345_64	combinatorics;discrete mathematics;directed graph;mathematics;algorithm	Theory	21.475783250026012	28.18766449700828	101464
7a677f2ceef554cde5724b6ff79b6ec95f01e024	wireless network design via 3-decompositions	heuristique;lettre alphabet;arbre graphe;graph node;assignment;asignacion;maximum degree;wireless networks;network design;metric space;nombre entier;approximate algorithm;espace metrique;procesamiento informacion;arbre steiner;aplicacion;ad hoc wireless network;maximo;nudo grafo;algorithm analysis;heuristica;tree graph;best approximation;steiner trees;05bxx;approximation algorithms;05c05;espacio metrico;metodo descomposicion;approximation algorithm;power efficiency;aproximacion;arbre maximal;wireless network;methode decomposition;usuario;assignation;conception;utilisateur;maximum;ad hoc network;reseau;potencia;68wxx;input;red;probleme steiner;approximation;connected graph;unit;integer;decomposition method;graph connectivity;54e35;arbol maximo;informatique theorique;domaine puissance;entero;ensemble contour;68r10;information processing;conectividad grafo;algoritmo aproximacion;entree ordinateur;edge set;diseno;distancia;approximation scheme;mejor aproximacion;puissance;design;letra alfabeto;analyse algorithme;user;power range;heuristics;spanning tree;entrada ordenador;algorithme approximation;letter;arbol grafo;traitement information;application;connectivite graphe;68w25;steiner tree;graphe connexe;power;analisis algoritmo;distance;noeud graphe;network;mobile network;unite;unidad;computer theory;grafo conexo;meilleure approximation;informatica teorica	We consider some network design problems with applications for wireless networks. The input for these problems is a metric space (X, d) and a finite subset U ⊆ X of terminals. In the Steiner Tree with Minimum Number of Steiner Points (STMSP) problem, the goal is to find a minimum size set S ⊆ X − U of points so that the unit-disc graph of S + U is connected. Let ∆ be the smallest integer so that for any finite V ⊆ X for which the unit-disc graph is connected, this graph contains a spanning tree with maximum degree ≤ ∆. The best known approximation ratio for STMSP was ∆ − 1 [10]. We improve this ratio to ⌊(∆ + 1)/2⌋ + 1 + ε. In the Minimum Power Spanning Tree (MPST) problem, V = X is finite, and the goal is to find a “range assignment” {p(v) : v ∈ V } on the nodes so that the edge set {uv ∈ E : d(uv) ≤ min{p(u), p(v)}} contains a spanning tree, and ∑ v∈V p(v) is minimized. We consider a particular case {0, 1}-MPST of MPST when the distances are in {0, 1}; here the goal is to find a minimum size set S ⊆ V of ”active” nodes so that the graph (V,E0 + E1(S)) is connected, where E0 = {uv : d(uv) = 0}, and E1(S) is the set the edges in E1 = {uv : d(uv) = 1} with both endpoints in S. We will show that the (5/3+ ε)-approximation scheme for MPST of [1] achieves a ratio 3/2 for {0, 1}-distances. This answers an open question posed in [9]. ∗Part of this work was done as a part of author’s M.Sc. Thesis at The Open University of Israel.	approximation algorithm;file spanning;network planning and design;spanning tree;steiner tree problem	Zeev Nutov;Ariel Yaroshevitch	2009	Inf. Process. Lett.	10.1016/j.ipl.2009.07.013	wireless ad hoc network;combinatorics;discrete mathematics;minimum degree spanning tree;steiner tree problem;computer science;connectivity;wireless network;mathematics;approximation algorithm;algorithm	Theory	21.29807821748801	27.630431068481563	101682
46aa6c7128911927ab713acfc85f5a8fc85d16cd	text indexing and dictionary matching with one error	text;indexation texte;complexite calcul;pregunta documental;texte;probleme concordance dictionnaire;question documentaire;algorithme;algorithm;probleme intersection chemin arborescence;complejidad computacion;hamming distance;indexing;computational complexity;tree path intersection problem;pattern matching;indexation;indizacion;query;concordance forme;dictionary matching problem;texto;text indexing;algoritmo	The indexing problem is where a text is preprocessed and subsequent queries of the form “Find all occurrences of pattern P in the text” are answered in time proportional to the length of the query and the number of occurrences. In the dictionary matching problem a set of patterns is preprocessed and subsequent queries of the form “Find all occurrences of dictionary patterns in text T” are answered in time proportional to the length of the text and the number of occurrences.There exist efficient worst-case solutions for the indexing problem and the dictionary matching problem, but none that find approximate occurrences of the patterns, i.e., where the pattern is within a bound edit (or Hamming) distance from the appropriate text location.In this paper we present a uniform deterministic solution to both the indexing and the general dictionary matching problem with one error. We preprocess the data in time O(nlog2n), where n is the text size in the indexing problem and the dictionary size in the dictionary matching problem. Our query time for the indexing problem is O(mlognloglogn+tocc), where m is the query string size and tocc is the number of occurrences. Our query time for the dictionary matching problem is O(nlog3dloglogd+tocc), where n is the text size and d the dictionary size. The time bounds above apply to both bounded and unbounded alphabets.	dictionary	Amihood Amir;Dmitry Keselman;Gad M. Landau;Moshe Lewenstein;Noa Lewenstein;Michael Rodeh	2000	J. Algorithms	10.1006/jagm.2000.1104	search engine indexing;hamming distance;computer science;theoretical computer science;pattern matching;pattern recognition;mathematics;computational complexity theory;algorithm	Theory	14.171341785940296	26.761795091841705	101711
0e2c744e9dcb18e7be4c73af6fbd80b4899d4501	a syntactic pr approach to telugu handwritten character recognition	approximate string matching;branch and bound technique;discrete curve;trie	This paper shows a character recognition mechanism based on a syntactic PR approach that uses the trie data structure for efficient recognition. It uses approximate matching of the string for classification. During the preprocessing an input character image is transformed into a skeletonized image and discrete curves are found using a 3 x 3 pixel region. A trie, which we call as a sequence trie is used for a look up approach at a lower level to encode a discrete curve pattern of pixels. The sequence of such discrete curves from the input pattern is looked up in the sequence trie. The encoding of several such sequence numbers for the thinned character constructs a pattern string. Approximate string matching is used to compare the encoded pattern string from a template character with the pattern string obtained from the input character. We consider the approximate matching of the string instead of the exact matching to make the approach robust in the presence of noise. Another trie data structure (called pattern trie) is used for the efficient storage and retrieval for approximate matching of the string. We make use of the trie since it takes O(m) in worst case where m is the length of the longest string in the trie. For the approximate string matching we use look ahead with a branch and bound scheme in the trie. Here we apply our method on 43 Telugu characters from the basic Telugu characters for demonstration. The proposed approach has recognised all the test characters given here correctly, however more extensive testing on realistic data is required.	approximate string matching;approximation algorithm;best, worst and average case;branch and bound;data structure;encode;handwriting recognition;optical character recognition;pixel;preprocessor;regular expression;string searching algorithm;syntactic predicate;trie	Samit Kumar Pradhan;Atul Negi	2012		10.1145/2432553.2432579	approximate string matching;string;theoretical computer science;pattern recognition;mathematics;hash array mapped trie;string metric;x-fast trie;algorithm	Theory	12.529126021647475	27.69553216925806	102012
3561f0cb777c48c0c6ad3e37faa8d8187ea890c1	an o(logn)-competitive algorithm for online constrained forest problems	competitive algorithm;prize-collecting steiner tree problem;source-sink pair;forest problem;online version;2-approximation algorithm;offline problem;online problem;generalized steiner tree problem;log n	In the generalized Steiner tree problem, we find a minimumcost set of edges to connect a given set of source-sink pairs. In the online version of this problem, the source-sink pairs arrive over time. Agrawal, Klein, and Ravi [1] give a 2-approximation algorithm for the offline problem; Berman and Coulston [3] give an O(logn)-competitive algorithm for the online problem. Goemans and Williamson [4] subsequently generalized the offline algorithm of Agrawal et al. to handle a large class of problems they called constrained forest problems, and other problems, such as the prize-collecting Steiner tree problem. In this paper, we show how to combine the ideas of Goemans and Williamson and those of Berman and Coulston to give an O(logn)-competitive algorithm for online constrained forest problems, including an online version of the prize-collecting Steiner tree problem.	approximation algorithm;combinatorica;discrete mathematics;graph (discrete mathematics);log-space reduction;network planning and design;online algorithm;online and offline;plotkin bound;siam journal on computing;steiner tree problem;symposium on discrete algorithms;symposium on theory of computing;theoretical computer science	Jiawei Qian;David P. Williamson	2011		10.1007/978-3-642-22006-7_4	mathematical optimization;combinatorics;steiner tree problem;mathematics;algorithm	Theory	22.012411815587228	19.428768856597433	102091
2c4c0989993f6ccc8ea4133967a7eef2da3d53fb	a probabilistic algorithm for updating files over a communication link	reseau communication;stochastic process;operation copie attachement;probabilistic algorithm;algorithme;algorithm;copying attaching operation;processus stochastique;error probability;proceso estocastico;red de comunicacion;communication network;algoritmo	Consider two people, P and Q, connected with a bidirectional communication link. Let P have a binary string 2 and let Q have y, The string y can be obtained from x by a small number k of editing operations: inserting/deleting a bit and copying/moving/deleting a block (substring). Our problem is to communicate the string y to P using a small number of bits in both directions. We present a probabilistic algorithm for that task aud prove that for any Z, y and E > 0 the algorithm communicates y (the string that Q has) to P with probability 1 E using poly(log 1~1, log ]y], loge-l, k) bits. The running time is poly(]z], ]y],log~-l).	randomized algorithm;substring;time complexity	Alexandre V. Evfimievski	1998		10.1016/S0304-3975(98)00019-X	stochastic process;computer science;artificial intelligence;theoretical computer science;probability of error;mathematics;randomized algorithm;algorithm;telecommunications network;statistics	Theory	17.438720874697328	31.69853362619005	102255
e90790bf13097ba177636f37e5565261ad48d2cd	computing with time-varying data: sequential complexity and parallel speed-up	parallel algorithm;time varying data	We discuss the design of sequential and parallel algorithms working on a time-increasing data set, within two paradigms of computation. In Paradigm 1 the process terminates when all the data currently arrived have been treated, independently of future arrivals. In Paradigm 2 an endless process is divided in stages, and in each stage the computation is carried out on the data set updated up to the previous stage. A problem may be unsolvable because no algorithm is fast enough to cope with the increasing data set. The computational cost of succeeding algorithms is studied in a new perspective, in the sequential RAM and parallel PRAM models, with the running time possibly tending to infinity for proper values of the parameters. It is shown that the traditional time bounds of parallel versus sequential computation (i.e., speed-up and slow-down under the so-called Brent's principle) do not hold, and new bounds are provided. Several problems are examined in the new paradigms, and the new algorithms are compared with the known ones designed for time-invariant data. Optimal sequential and parallel algorithms are also defined, and given whenever possible. In particular it is shown that some problems do not gain anything from a parallel solution, while others can be practically solved only in parallel. Paradigm 1 is the most innovative, and the relative results on parallel speed-up and scaling are probably unexpected. Paradigm 2 opens a new perspective in dynamic algorithms, because processing batches of data may be more efficient than processing single incoming data on-line.	algorithmic efficiency;computation;dynamic problem (algorithms);image scaling;online and offline;parallel algorithm;parallel computing;programming paradigm;random-access memory;time complexity;time-invariant system	Fabrizio Luccio;Linda Pagli	1998	Theory of Computing Systems	10.1007/s002240000074	combinatorics;computer science;theoretical computer science;analysis of parallel algorithms;mathematics;distributed computing;parallel algorithm;algorithm;cost efficiency	Theory	10.169238042903162	31.17476506925909	102276
491ad6d93839b13330fed4243c9a4b685fdd6610	easiness assumptions and hardness tests: trading time for zero error	probabilistic algorithm	We propose a new approach towards derandomization in the uniform setting where it is computationally hard to nd possible mistakes in the simulation of a given probabilistic algo rithm The approach consists in combining both easiness and hardness complexity assumptions if a derandomization method based on an easiness assumption fails then we obtain a certain hardness test that can be used to remove error in BPP algorithms As an application we prove that every RP algorithm can be simulated by a zero error probabilistic algorithm running in expected subexponential time that appears correct in nitely often i o to every e cient adversary A similar result by Impagliazzo and Wigderson FOCS states that BPP allows de terministic subexponential time simulations that appear correct with respect to any e ciently sampleable distribution i o under the assumption that EXP BPP in contrast our result does not rely on any unproven assumptions As another application of our techniques we get the following gap theorem for ZPP either every RP algorithm can be simulated by a deter ministic subexponential time algorithm that appears correct i o to every e cient adversary or EXP ZPP In particular this implies that if ZPP is somewhat easy e g ZPP DTIME n c for some xed constant c then RP is subexponentially easy in the uniform setting described above	adversary (cryptography);bpp (complexity);computational complexity theory;dtime;exptime;gap theorem;rp (complexity);randomized algorithm;simulation;symposium on foundations of computer science;time complexity;zpp (complexity)	Valentine Kabanets	2001	J. Comput. Syst. Sci.	10.1006/jcss.2001.1763	combinatorics;discrete mathematics;computer science;mathematics;randomized algorithm;algorithm	Theory	10.607924534255991	20.94985341494325	102441
660cc8f643f973b7a08193e7ae65535b32d43059	approximating the minimum degree spanning tree to within one from the optimal degree	approximate algorithm;directed graph;polynomial time;spanning tree;minimum degree	We consider the problem of constructing a spanning tree for a graph G = (V,E) with n vertices whose maximal degree is the smallest among all spanning trees of G. This problem is easily shown to be NP-hard. We describe an iterative polynomial time approximation algorithm for this problem. This algorithm computes a spanning tree whose maximal degree is at most O(Δ + log n), where Δ is the degree of some optimal tree. The result is generalized to the case where only some vertices need to be connected (Steiner case) and to the case of directed graphs. It is then shown that our algorithm can be refined to produce a spanning tree of degree at most Δ + 1. Unless P = NP, this is the best bound achievable in polynomial time.	approximation algorithm;directed graph;file spanning;iterative method;maximal set;np-hardness;p versus np problem;polynomial;spanning tree;steiner tree problem;time complexity;vertex (geometry)	Martin Fürer;Balaji Raghavachari	1992			time complexity;euclidean minimum spanning tree;mathematical optimization;combinatorics;discrete mathematics;kruskal's algorithm;directed graph;minimum degree spanning tree;spanning tree;steiner tree problem;prim's algorithm;degree;minimum spanning tree;frequency partition of a graph;gomory–hu tree;degree of a polynomial;k-ary tree;connected dominating set;k-minimum spanning tree;mathematics;trémaux tree;reverse-delete algorithm;distributed minimum spanning tree;cycle basis;tree;shortest-path tree	Theory	23.764465137149706	21.380214153067953	102525
1749cdf32bfbad2c47b8dd8817d2fb66191a2a7c	close to uniform prime number generation with fewer random bits		"""In this paper, we analyze several variants of a simple method for generating prime numbers with fewer random bits. To generate a prime <inline-formula> <tex-math notation=""""LaTeX"""">$p$ </tex-math></inline-formula> less than <inline-formula> <tex-math notation=""""LaTeX"""">$x$ </tex-math></inline-formula>, the basic idea is to fix a constant <inline-formula> <tex-math notation=""""LaTeX"""">$q\propto x^{1- \varepsilon }$ </tex-math></inline-formula>, pick a uniformly random <inline-formula> <tex-math notation=""""LaTeX"""">$a< q$ </tex-math></inline-formula> coprime to <inline-formula> <tex-math notation=""""LaTeX"""">$q$ </tex-math></inline-formula>, and choose <inline-formula> <tex-math notation=""""LaTeX"""">$p$ </tex-math></inline-formula> of the form <inline-formula> <tex-math notation=""""LaTeX"""">$a+t\cdot q$ </tex-math></inline-formula>, where only <inline-formula> <tex-math notation=""""LaTeX"""">$t$ </tex-math></inline-formula> is updated if the primality test fails. We prove that variants of this approach provide prime generation algorithms requiring a few random bits and whose output distribution is close to uniform, under less and less expensive assumptions: first a relatively strong conjecture by H. Montgomery, made precise by Friedlander and Granville; then the Extended Riemann Hypothesis; and finally fully unconditionally using the Barban–Davenport–Halberstam theorem. We argue that this approach has a number of desirable properties compared with the previous algorithms, at least in an asymptotic sense. In particular: 1) it uses much fewer random bits than both the “trivial algorithm” (testing random numbers less than <inline-formula> <tex-math notation=""""LaTeX"""">$x$ </tex-math></inline-formula> for primality) and Maurer’s almost uniform prime generation algorithm; 2) the distance of its output distribution to uniform can be made arbitrarily small, unlike algorithms like PRIMEINC (studied by Brandt and Damgård), which we show exhibit significant biases; and 3) all quality measures (number of primality tests, output entropy, randomness, and so on) can be obtained under standard conjectures or even unconditionally, whereas most previous nontrivial algorithms can only be proved based on stronger, less standard assumptions like the Hardy-Littlewood prime tuple conjecture. Note, however, that our analysis involves non-explicit constants, and therefore does not establish the superiority of our approach for concrete parameter sizes."""	algorithm;analysis of algorithms	Pierre-Alain Fouque;Mehdi Tibouchi	2011	IEEE Transactions on Information Theory	10.1007/978-3-662-43948-7_82	provable prime;combinatorics;discrete mathematics;mathematics	Theory	11.03697128651545	23.927470029190378	102792
a04cf5bbdba000285b50d966e58388e7b44a8cba	size-constrained tree partitioning: a story on approximation algorithm design for the multicast k-tree routing problem	optical network;approximate algorithm;best approximation;approximation algorithm;capacitated multicast tree routing;multicast tree;tree partitioning;performance ratio	In the Multicast k-Tree Routing Problem, a data copy is sent from the source node to at most k destination nodes in every transmission. The goal is to minimize the total cost of sending data to all destination nodes, which is measured as the sum of the costs of all routing trees. This problem was formulated out of optical networking and has applications in general multicasting. Several approximation algorithms, with increasing performance, have been proposed in the last several years; The most recent ones are heavily relied on a tree partitioning technique. In this paper, we present a further improved approximation algorithm along the line. The algorithm has a worst case performance ratio of 5 4 ρ + 3 2 , where ρ denotes the best approximation ratio for the Steiner Minimum Tree problem. The proofs of the technical routing lemmas also provide some insights on why such a performance ratio could be the best possible that one can get using this tree partitioning technique.		Zhipeng Cai;Randy Goebel;Guohui Lin	2009		10.1007/978-3-642-02026-1_34	mathematical optimization;combinatorics;tree rotation;vantage-point tree;exponential tree;steiner tree problem;computer science;destination-sequenced distance vector routing;k-ary tree;interval tree;k-minimum spanning tree;mathematics;distributed computing;fractal tree index;tree;tree traversal;approximation algorithm	Theory	23.364544226717385	18.977949766496582	102880
56defb15883d0d2193a9966fec3ecae3bed8027c	on the hardness of approximating max-satisfy	sistema lineal;assignment;systeme equation;asignacion;approximate algorithm;procesamiento informacion;algorithm analysis;complexite calcul;approximation algorithms;approximation algorithm;assignation;problema np duro;ecuacion lineal;satisfiability;linear system;np hard problem;complejidad computacion;sistema ecuacion;probleme np difficile;computational complexity;informatique theorique;equation system;information processing;algoritmo aproximacion;analyse algorithme;linear equations;systeme lineaire;linear equation;algorithme approximation;traitement information;hardness of approximation;analisis algoritmo;equation lineaire;computer theory;informatica teorica	Max-Satisfy is the problem of finding an assignment that satisfies the maximum number of equations in a system of linear equations over Q. We prove that unless NP⊂BPP Max-Satisfy cannot be efficiently approximated within an approximation ratio of 1/n1−2, if we consider systems of n linear equations with at most n variables and 2 > 0 is an arbitrarily small constant. Previously, it was known that the problem is NP-hard to approximate within a ratio of 1/n, but 0 < α < 1 was some specific constant that could not be taken to be arbitrarily close to 1.	approximation algorithm;linear equation;np-hardness;system of linear equations	Uriel Feige;Daniel Reichman	2004	Electronic Colloquium on Computational Complexity (ECCC)	10.1016/j.ipl.2005.08.012	combinatorics;computer science;calculus;mathematics;linear equation;approximation algorithm;algorithm	Theory	18.662201784515144	25.482709858227924	103430
ce152a735c3640215fbf02b4abe3a25df9ad6df2	space-efficient implementations of graph search methods	graph search;command language;waiting list;efficient implementation;graph representation;depth first search;breadth first search	Several space-efficmnt implementations of the two most common and useful kinds of graph search, namely, breadth-first search and depth-first search, are discussed. A straightforward implementation of each method requires n bits and n + O(1) pointers of auxiliary storage, where n is the number of vertices in the graph. We devise methods that need only 2n + m bits, of which m are read-only, where rn is the number of edges in the graph. We save space by folding the queue or stack required by the search into the graph representation; two of our methods for depth-first search are variants of the Deutsch-Schorr-Waite list-marking algorithm. Our algorithms are expressed in a version of Dijkstra's guarded command language.	algorithm;auxiliary memory;breadth-first search;depth-first search;graph (abstract data type);graph traversal;guarded command language;item unique identification;read-only memory;vertex (graph theory)	Robert E. Tarjan	1983	ACM Trans. Math. Softw.	10.1145/356044.356048	beam search;graph power;mathematical optimization;factor-critical graph;combinatorics;discrete mathematics;bidirectional search;directed graph;graph bandwidth;breadth-first search;null graph;graph labeling;theoretical computer science;simplex graph;aperiodic graph;cycle graph;mathematics;voltage graph;graph;best-first search;quartic graph;complement graph;line graph;algorithm;strength of a graph;search algorithm	Graphics	16.110062616938034	29.030371840828305	103526
8d682d4e9dae59e6df5ec2108039d88e5c4b9629	a practical heuristic for finding graph minors		We present a heuristic algorithm for finding a graph H as a minor of a graph G that is practical for sparse G and H with hundreds of vertices. We also explain the practical importance of finding graph minors in mapping quadratic pseudo-boolean optimization problems onto an adiabatic quantum annealer.	algorithm;computation;graph minor;heuristic (computer science);ising model;iteration;mathematical optimization;quantum annealing;simulated annealing;sparse matrix;tree decomposition;vertex (geometry);vertex model	Jun Cai;William G. Macready;Aidan Roy	2014	CoRR		graph power;edge contraction;petersen graph;mathematical optimization;factor-critical graph;combinatorics;discrete mathematics;graph bandwidth;wagner graph;null graph;distance-regular graph;simplex graph;cubic graph;mathematics;voltage graph;graph;butterfly graph;graph minor;complete graph;quartic graph;complement graph;line graph;strength of a graph;coxeter graph	ML	23.58853997455502	25.413631325696993	103703
a3b7967aa7d9f9c0329cfda8be7492c65fd88f87	from few components to an eulerian graph by adding arcs	fixed-parameter tractable;polynomial-size problem kernel;arc addition;combined parameter;eulerian graph;connected component;multigraph eulerian;underlying undirected multigraph;eulerian extension;parameter combination;input multigraph	Eulerian Extension (EE) is the problem to make an arcweighted directed multigraph Eulerian by adding arcs of minimum total cost. EE is NP-hard and has been shown fixed-parameter tractable with respect to the number of arc additions. Complementing this result, on the way to answering an open question, we show that EE is fixedparameter tractable with respect to the combined parameter “number of connected components in the underlying undirected multigraph” and “sum of indeg(v)− outdeg(v) over all vertices v in the input multigraph where this value is positive.” Moreover, we show that EE is unlikely to admit a polynomial-size problem kernel for this parameter combination and for the parameter “number of arc additions”.	arcs (computing);algorithm;cobham's thesis;connected component (graph theory);connectivity (graph theory);eulerian path;experiment;graph (discrete mathematics);many-one reduction;multigraph;np-hardness;parameterized complexity;polynomial;turing	Manuel Sorge;René van Bevern;Rolf Niedermeier;Mathias Weller	2011		10.1007/978-3-642-25870-1_28	mathematical optimization;combinatorics;discrete mathematics;route inspection problem;mathematics	AI	22.686249621956065	22.950263105760538	103896
3cde38a5a8c6dada97ea1d53479b0dbf451d7dce	approximation complexity of max-cut on power law graphs		In this paper we study the MAX-CUT problem on power law graphs (PLGs) with power law exponent β. We prove some new approximability results on that problem. In particular we show that there exist polynomial time approximation schemes (PTAS) for MAX-CUT on PLGs for the power law exponent β in the interval (0, 2). For β > 2 we show that for some ε > 0, MAX-CUT is NP-hard to approximate within approximation ratio 1 + ε, ruling out the existence of a PTAS in this case. Moreover we give an approximation algorithm with improved constant approximation ratio for the case of β > 2.	approximation algorithm;existential quantification;max;maximum cut;ptas reduction;polynomial;polynomial-time approximation scheme;time complexity	Mikael Gast;Mathias Hauptmann;Marek Karpinski	2016	CoRR		mathematical optimization;combinatorics;discrete mathematics;mathematics	Theory	22.347067424512726	20.11064085201547	104095
1f4ea12d55680d175a0f4919b912518407b5e137	the maximum agreement forest problem: approximation algorithms and computational experiments	arbre phylogenetique;maximum degree;forests;approximate algorithm;matematicas aplicadas;algorithm performance;mathematiques appliquees;phylogeny;complexite calcul;phylogenese;approximation algorithm;interdisciplinar;problema np duro;arbol filogenetico;foret;apx complete;np hard problem;complejidad computacion;approche evolutionaire;evolutionary trees;phylogenetic tree;computer experiment;probleme np difficile;computational complexity;resultado algoritmo;informatique theorique;filogenesis;algoritmo aproximacion;performance algorithme;agreement forest;evolutionary process;algorithme approximation;performance ratio;applied mathematics;bosque;computer theory;informatica teorica	There are various techniques for reconstructing phylogenetic trees from data, and in this context the problem of determining how distant two such trees are from each other arises naturally. Various metrics for measuring the distance between two phylogenies have been defined. Another way of comparing two trees T and U is to compute the so called maximum agreement forest of these trees. Informally, the number of components of an agreement forest tells how many edges from each of T and U need to be cut so that the resulting forests agree, after performing all forced edge contractions. This problem is NP-hard even when the input trees have maximum degree 2. Hein et al. [J. Hein, T. Jiang, L. Wang, K. Zhang, On the complexity of comparing evolutionary trees, Discrete Applied Mathematics 71 (1996) 153–169] presented an approximation algorithm for it, claimed to have performance ratio 3. We show that the performance ratio of the algorithm proposed by Hein et al. is 4, and we also present two new 3-approximation algorithms for this problem. We show how to modify one of the algorithms into a (d + 1)-approximation algorithm for trees with bounded degree d , d ≥ 2. Finally, we report on some computational experiments comparing the performance of the algorithms presented in this paper. c © 2006 Elsevier B.V. All rights reserved.	approximation algorithm;computation;degree (graph theory);experiment;phylogenetic tree;phylogenetics	Estela Maris Rodrigues;Marie-France Sagot;Yoshiko Wakabayashi	2007	Theor. Comput. Sci.	10.1016/j.tcs.2006.12.011	combinatorics;phylogenetic tree;computer science;mathematics;weight-balanced tree;approximation algorithm;algorithm;statistics	Theory	21.543268382860457	26.795521531622327	104335
d2518cfc97f8b905c936a0060d7254877c8b8a54	symmetric (and generic) algorithms for height balanced trees	generic algorithm		algorithm;self-balancing binary search tree	Coenraad Bron	1990	Structured Programming			Theory	16.858843514121695	29.28502014915391	104347
d21a1b58b69f3b8ff754cd6ac552ad9e54524f61	a distributed algorithm for allocation of resources to process groups with acyclic graph.	distributed algorithm			Yutaka Wada;Zixue Cheng;Tongjun Huang	1997			distributed algorithm;suurballe's algorithm;computer science;theoretical computer science;machine learning;distributed computing;reverse-delete algorithm;distributed minimum spanning tree	HPC	17.661734538781353	29.85584537618388	104702
ff2a847fe03746d3051c108ea298242de3ccde85	some results concerning the complexity of restricted colorings of graphs	complexite;coloracion grafo;temps polynomial;probleme np complet;complejidad;nombre chromatique;chromatic number;complexity;indice chromatique;numero indice cromatico;coloration graphe;polynomial time;chromatic index;problema np completo;coloration restreinte;np complete problem;graph colouring;tiempo polinomial	Abstract   We consider the complexity of restricted colorings of a graph in which each vertex (or edge) receives one color from a list of permissible colors associated with that vertex (edge). Since the problem is strongly NP-complete, we assume various restrictions imposed on the number and form of permissible colors and the structure of a graph. In this way we obtain some evidence for comparing the complexity of the restricted vertex coloring problem versus that of edge coloring and arrive at a number of results about special cases that are either positive (polynomial solvability) or negative (NP-completeness proofs).		Marek Kubale	1992	Discrete Applied Mathematics	10.1016/0166-218X(92)90202-L	time complexity;combinatorics;discrete mathematics;complexity;np-complete;topology;feedback vertex set;fractional coloring;vertex cover;edge space;complete coloring;edge cover;edge coloring;graph coloring;vertex;mathematics;list coloring;greedy coloring;neighbourhood;algorithm	Theory	23.20206085241789	25.198702389231865	104858
09d3d667530c4f0cb50a124bc45c0f5834aeb572	practical loss-resilient codes	algoweb_ldpc;real time;differential equation;linear time;ldpc codes;bipartite graph;software implementation	We present randomized constructions of linear-time encodable and decodable codes that can transmit over lossy channels at rates extremely close to capacity. The encoding and decoding algorithms for these codes have fast and simple software implementations. Partial implementations of our algorithms are faster by orders of magnitude than the best software implementations of any previous algorithm for this problem. We expect these codes will be extremely useful for applications such as real-time audio and video transmission over the Internet, where lossy channels are common and fast decoding is a requirement. Despite the simplicity of the algorithms, their design and analysis are mathematically intricate. The design requires the careful choice of a random irregular bipartite graph, where the structure of the irregular graph is extremely important. We model the progress of the decoding algorithm by a set of differential equations. The solution to these equations can then be expressed as polynomials in one variable Digital Equipment Corporation, Systems Research Center, Palo Alto, CA, and Computer Science Division, University of California at Berkeley. A substantial portion of this research done while at the International Computer Science Institute. Research supported in part by National Science Foundation operating grant NCR-9416101, and United States-Israel Binational Science Foundation grant No. 92-00226. yDigital Equipment Corporation, Systems Research Center, Palo Alto, CA. A substantial portion of this research done while at the Computer Science Department, UC Berkeley, under the National Science Foundation grant No. CCR-9505448. International Computer Science Institute Berkeley, and Institut f ̈ ur Informatik der Universit ̈ at Bonn, Germany. Research supported by a Habilitationsstipendium of the Deutsche Forschungsgemeinschaft, Grant Sh 57/1–1. xDepartment of Mathematics, M.I.T. Supported by an NSF mathematical sciences postdoc. A substantial portion of this research done while visiting U.C. Berkeley. Research done while at the International Computer Science Institute. with coefficients determined by the graph structure. Based on these polynomials, we design a graph structure that guarantees successful decoding with high probability.	bsd;certificate authority;code;computer science;ibm notes;lossy compression;palo;polynomial;randomized algorithm;real-time clock;time complexity;uc browser;with high probability	Michael Luby;Michael Mitzenmacher;Amir H. Shokrollahi;Daniel A. Spielman;Volker Stemann	1997		10.1145/258533.258573	combinatorics;discrete mathematics;low-density parity-check code;bipartite graph;sequential decoding;computer science;theoretical computer science;factor graph;mathematics;error floor;differential equation;algorithm	Theory	12.929250831771808	21.65407732798363	104873
74b366e9eeddb6e14124d5c80b4c1ba44b4dc2cd	minimax trees in linear time with applications	004;data structures data compression prefix free coding	A minimax tree is similar to a Huffman tree except that, instead of minimizing the weighted average of the leaves' depths, it minimizes the maximum of any leaf's weight plus its depth. Golumbic (1976) introduced minimax trees and gave a Huffman-like, $\mathcal{O}{n \log n}$-time algorithm for building them. Drmota and Szpankowski (2002) gave another $\mathcal{O}{n \log n}$-time algorithm, which takes linear time when the weights are already sorted by their fractional parts. In this paper we give the first linear-time algorithm for building minimax trees for unsorted real weights.	minimax;time complexity	Pawel Gawrychowski;Travis Gagie	2009		10.1007/978-3-642-10217-2_28	mathematical optimization;combinatorics;discrete mathematics;mathematics	Theory	18.797734898707354	24.74862336849275	105006
1570d81fd827c1c40e39e8bfd0888ca4eea86b34	is quantum search practical?	search algorithm;satisfiability;quantum computing quantum entanglement databases polynomials algorithm design and analysis query processing runtime concurrent computing parallel processing performance evaluation;search problems quantum computing;quantum physics;quantum algorithm;quantum;algorithms;web search;search problems;grover quantum search algorithm;quantum computing;grover s algorithm;quantum algorithms web search	Gauging a quantum algorithm's practical significance requires weighing it against the best conventional techniques applied to useful instances of the same problem. The authors show that several commonly suggested applications of Grover's quantum search algorithm fail to offer computational improvements over the best conventional algorithms.	best, worst and average case;boolean satisfiability problem;computation;computer;heuristic (computer science);open research;parallel computing;quantum algorithm;quantum circuit;quantum computing;requirement;scalability;search algorithm;simulation;time complexity	George F. Viamontes;Igor L. Markov;John P. Hayes	2005	Computing in Science & Engineering	10.1109/MCSE.2005.53	grover's algorithm;combinatorics;quantum information;quantum;computer science;theoretical computer science;mathematics;quantum computer;quantum algorithm for linear systems of equations;quantum algorithm;algorithm;one-way quantum computer;quantum mechanics;quantum phase estimation algorithm;quantum sort;satisfiability;search algorithm	Theory	11.945946364307984	24.162205867015118	105246
aca88caafabe1f3a18657295c125776c1d660523	exact and heuristic algorithms for the weighted feedback arc set problem: a special case of the skew-symmetric quadratic assignment problem	systeme grande taille;aplicacion;graph flow;aproximacion;heuristic method;exact solution;metodo heuristico;large scale system;solucion exacta;systeme asservi;flujo grafo;approximation;algorithme;algorithm;flot graphe;directed graph;grafico orientado;graphe oriente;servomecanismo;quadratic assignment problem;methode heuristique;feedback system;solution exacte;application;heuristic algorithm;sistema gran escala;algoritmo	This paper presents algorithms for finding exact and approximate solutions for the weighted feedback arc set problem, determining a minimum-cardinality set o f arcs that breaks all cycles in a directed graph. This is also the problem of finding a group rank ordering given the rank orders for each member of a group. The algorithms have many other applications. Mathematically, the problem is that of finding a permutation matrix P that maximizes the sum of the elements above the principal diagonal of P‘ WP where W is a skew-symmetric matrix of order n. This is a special case of the KooprnansBeckmann quadratic assignment problem. Computational experience is reported for a sample of randomly generated problems, including comparisons with results obtained using algorithms three other authors have developed for solving the more general quadratic assignment problem. For our sample, using randomly generated problems, the computational time required for calculating all exact solutions for each problem is approximately T ( n ) = c2.232”, where c = 9.8764B 6 and T ( 2 0 ) = 93 s for the Cray X-MP/48 supercomputer. For our sample, the computational time required for calculating one approximate solution is approximately T ( n ) = an4.’, where a = 3.0361 E 8 and T ( 2 5 0 ) = 206 s for the Cray.	approximation algorithm;computation;cray x-mp;directed graph;feedback arc set;heuristic;procedural generation;quadratic assignment problem;supercomputer;time complexity	Merrill M. Flood	1990	Networks	10.1002/net.3230200102	heuristic;mathematical optimization;combinatorics;directed graph;computer science;generalized assignment problem;approximation;feedback;mathematics;algorithm;quadratic assignment problem	Theory	18.291872300828935	27.911497267029375	105556
d619d077f98c4d55d412daf724fd836975736dbc	linear-time modular decomposition and efficient transitive orientation of comparability graphs	directed hypergraphs;edit distance;linear time algorithm;permutation graph;evolutionary trees;modular decomposition;linear time;recombination;bottleneck optimality;computational biology;linear space;partial order	A module of an undirected graph is a set X of nodes such for each node x not in X, either every member of X is adjacent to x , or no member of X is adjacent to x. There is a canonical linear-space representation for the modules of a graph, called the modular decomposition. The modular decomposition facilitates solution of a number of combinatorial problems on certain classes of graphs, and algorithms for computing it have a lengthy history. Closely related to modular decomposition is the transitive orientation problem, which is the problem of assigning a direction to each edge of a graph so that the resulting digraph is transitive, if such an assignment is possible. We give the lirst linear-time algorithm for modular decomposition, and a new bound of 0 (ri +m logn) on transitive orientation and the problem of recognizing permutation graphs and two-dimensional partial orders.	algorithm;directed graph;graph (discrete mathematics);modular decomposition;time complexity;transitive reduction	Ross M. McConnell;Jeremy P. Spinrad	1994			partially ordered set;time complexity;mathematical optimization;combinatorics;discrete mathematics;phylogenetic tree;edit distance;comparability graph;permutation graph;mathematics;modular decomposition;recombination;linear space	Theory	22.909438030607998	25.960840796270112	105667
4bf9c7190baea215b736556f6ed52fa65fdc1892	a black box for online approximate pattern matching	online algorithm;approximate pattern matching;approximate matching;sliding window	We present a deterministic black box solution for online approximate matching. Given a pattern of length  m and a streaming text of length  n that arrives one character at a time, the task is to report the distance between the pattern and a sliding window of the text as soon as the new character arrives. Our solution requires $O(\Sigma_{j=1}^{\log_2{m}} T(n,2^{j-1})/n)$ time for each input character, where  T ( n , m ) is the total running time of the best offline algorithm. The types of approximation that are supported include exact matching with wildcards, matching under the Hamming norm, approximating the Hamming norm,  k -mismatch and numerical measures such as the  L  2 and  L  1 norms. For these examples, the resulting online algorithms take  O (log2 m ), $O(\sqrt{m\log{m}})$,  O (log2 m / i¾? 2), $O(\sqrt{k \log k} \log{m})$,  O (log2 m ) and $O(\sqrt{m\log{m}})$ time per character respectively. The space overhead is  O ( m ) which we show is optimal.	black box;pattern matching	Raphaël Clifford;Klim Efremenko;Benny Porat;Ely Porat	2008		10.1007/978-3-540-69068-9_15	sliding window protocol;online algorithm;combinatorics;computer science;theoretical computer science;machine learning;mathematics	Theory	13.224238542856813	26.97750634423522	105879
fa97afdbf4ff151e04a95f2ba4c01182fa10ea02	on finding a longest common palindromic subsequence		Abstract Recently, Chowdhury et al. [5] proposed the longest common palindromic subsequence problem . It is a variant of the well-known LCS problem, which refers to finding a palindromic LCS between two strings T 1 and T 2 . In this paper, we present a new O ( n + R 2 ) -time algorithm where n = | T 1 | = | T 2 | and R is the number of matches between T 1 and T 2 . We also show that the average running time of our algorithm is O ( n 4 / | Σ | 2 ) , where Σ is the alphabet of T 1 and T 2 . This improves the previously best algorithms whose running times are O ( n 4 ) and O ( R 2 log 2 ⁡ n log ⁡ log ⁡ n ) .	longest increasing subsequence	Sang Won Bae;Inbok Lee	2018	Theor. Comput. Sci.	10.1016/j.tcs.2017.02.018	subsequence;discrete mathematics;combinatorics;palindrome;alphabet;mathematics	ECom	13.556380063797084	26.83262514957177	106026
a4d6df28fcf7cc4b1b0ab7635b47c713d3aa088c	approximating vertex cover in dense hypergraphs	maximum degree;vertex cover;random polynomials;approximation algorithms;k partite k uniform hypergraphs;approximation hardness;sampling technique;dense hypergraphs;polynomial time;data structure;set cover	We consider the minimum vertex cover problem in hypergraphs in which every hyperedge has size k (also known as minimum hitting set problem, or minimum set cover with element frequency k). Simple algorithms exist that provide k-approximations, and this is believed to be the best possible approximation achievable in polynomial time. We show how to exploit density and regularity properties of the input hypergraph to break this barrier. In particular, we provide a randomized polynomial-time algorithm with approximation factor k/(1+(k−1) d̄ k∆ ), where d̄ and ∆ are the average and maximum degree, respectively, and ∆ must be Ω(nk−1/ log n). The proposed algorithm generalizes the recursive sampling technique of Imamura and Iwama (SODA’05) for vertex cover in dense graphs. As a corollary, we obtain an approximation factor k/(2 − 1/k) for subdense regular hypergraphs, which is shown to be the best possible under the unique games conjecture.	approximation;degree (graph theory);polynomial;rp (complexity);randomized algorithm;recursion;sampling (signal processing);set cover problem;time complexity;unique games conjecture;vertex cover	Jean Cardinal;Marek Karpinski;Richard Schmied;Claus Viehmann	2012	J. Discrete Algorithms	10.1016/j.jda.2012.01.003	time complexity;sampling;mathematical optimization;combinatorics;discrete mathematics;data structure;vertex cover;computer science;edge cover;mathematics;set cover problem;approximation algorithm;algorithm	Theory	21.18961181726842	22.008754770629803	106238
f33f3ba632632e17237597f44a1203babbcac707	on the exact complexity of polyomino packing		We show that the problem of deciding whether a collection of polyominoes, each fitting in a 2×O(log n) rectangle, can be packed into a 3×n box does not admit a 2o(n/ log n)-time algorithm, unless the Exponential Time Hypothesis fails. We also give an algorithm that attains this lower bound, solving any instance of polyomino packing with total area n in 2O(n/ log n) time. This establishes a tight bound on the complexity of Polyomino Packing, even in a very restricted case. In contrast, for a 2× n box, we show that the problem can be solved in strongly subexponential time. 2012 ACM Subject Classification Theory of computation → Problems, reductions and completeness, Mathematics of computing → Combinatorial algorithms	algorithm;exptime;exponential time hypothesis;set packing;theory of computation	Hans L. Bodlaender;Tom C. van der Zanden	2018		10.4230/LIPIcs.FUN.2018.9	combinatorics;computer science;discrete mathematics;rectangle;exponential time hypothesis;binary logarithm;polyomino;upper and lower bounds	Theory	18.157021747039604	19.663812196068562	106364
9748ece84f95f242df6b0f736a90334da559e513	an alternative for the implementation of kruskal's minimal spanning tree algorithm	minimal spanning tree	Abstract   An application of the bucket sort in Kruskal's minimal spanning tree algorithm is proposed. The modified algorithm is very fast if the edge costs are from a distribution which is close to uniform. This is due to the fact that the sorting phase then takes for an  m  edge graph an O( m ) average time. The O( m  log  m ) worst case occurs when there is a strong peak in the distribution of the edge costs.	file spanning;kruskal's algorithm;minimum spanning tree	Jyrki Katajainen;Olli Nevalainen	1983	Sci. Comput. Program.	10.1016/0167-6423(83)90011-4	euclidean minimum spanning tree;mathematical optimization;kruskal's algorithm;minimum degree spanning tree;spanning tree;prim's algorithm;computer science;minimum spanning tree;borůvka's algorithm;reverse-delete algorithm;distributed minimum spanning tree;shortest-path tree	Theory	20.22178471413187	25.754919296522	106530
21a5d568ab605b1bb620b3bfd88476092fb3229d	approximability of manipulating elections	approximate algorithm;approximation scheme;fully polynomial time approximation scheme;polynomial approximation	In this paper, we set up a framework to study approximation of manipulation, control, and bribery in elections. We show existence of approximation algorithms (even fully polynomial time approximation schemes) as well as obtain inapproximability results.	approximation algorithm;hardness of approximation;polynomial;time complexity	Eric Brelsford;Piotr Faliszewski;Edith Hemaspaandra;Henning Schnoor;Ilka Schnoor	2008			mathematical optimization;approximation error;polynomial-time approximation scheme;computer science;approximation;matrix polynomial;hardness of approximation;minimax approximation algorithm;approximation algorithm	AI	17.13244504980904	19.428361343499887	106618
8fe1c7f5eae67cf0ecb3413e3247c6ec8c1ddb33	an innovative technique for backbone network design	arbre graphe;graph theory;algorithme ford fulkerson;methode branch and bound;network design;teoria grafo;tree graph;spine algorithm design and analysis computer networks routing cost function network topology process design pricing delay effects communication networks;reseau ordinateur;telecommunication network;theorie graphe;camino optimo;computer networks;computer network;chemin optimal;minimizacion costo;branch and bound method;minimisation cout;optimal path;cost minimization;metodo branch and bound;red telecomunicacion;reseau telecommunication;red ordenador;delays computer networks;sparsely connected networks innovative technique minimum cost backbone network design time delay performance criterion multiple hop communication paths branch and bound method network topology depth first search cost heuristic;arbol grafo;delays	Absfruct-A new algorithm is proposed for solving the minimum cost backbone network design problem. The method includes time delay as the performance criteria and multiple hop communication paths. This algorithm combines a branch and bound method with the Ford-Fulkerson algorithm. The branch and bound method is used to find the network topology. The Ford-Fulkerson algorithm is then applied to determine the optimal paths between each pair of communicating nodes. A depth first search using a cost heuristic that is a linear function of the maximum and current depth in the tree is used. Whenever a feasible topology is found the Ford-Fulkerson algorithm is applied to find the optimal paths. The resulting network is retained if it is the current optimum cost. This algorithm has been compared with a branch and bound algorithm and it was empirically found to produce globally optimal solutions. For large sparcely connected networks, computation times were reduced by a factor of over one thousand.	branch and bound;broadcast delay;computation;depth-first search;dijkstra's algorithm;ford–fulkerson algorithm;heuristic;internet backbone;linear function;maxima and minima;network planning and design;network topology	Nanda Gopal Chattopadhyay;Thomas W. Morgan;Aswatnarayan Raghuram	1989	IEEE Trans. Systems, Man, and Cybernetics	10.1109/21.44028	out-of-kilter algorithm;mathematical optimization;suurballe's algorithm;network planning and design;floyd–warshall algorithm;computer science;graph theory;brooks–iyengar algorithm;distributed computing;tree;algorithm;telecommunications network	EDA	18.297010542467387	31.48465160009089	106804
16741d50faaff06a434e7fe6f99cb4a2da0f99f0	the connectivity of boolean satisfiability: computational and structural dichotomies	heuristique;probleme satisfiabilite;formule booleenne;hypercube;34d09;algorithm complexity;subgrafo;temps polynomial;heuristica;seuil;benchmark;complejidad algoritmo;satisfiabilite;05c40;threshold;polynomial;satisfiability;pspace completeness;68wxx;composante connexe;boolean satisfiability;polynomial time algorithm;graph connectivity;68q17;complexite algorithme;sous graphe;computational complexity;dicotomia;polinomio;problema satisfactibilidad;dichotomie;68r10;conectividad grafo;03d15;benchmarks;polynomial time;algorithme polynomial;68q15;68q25;dichotomy;heuristics;umbral;subgraph;connected component;polynome;connectivite graphe;satisfiability problem;dichotomy theorems;algorithme heuristique;heuristic algorithm;structural properties;pspace;tiempo polinomial;hipercubo	Boolean satisfiability problems are an important benchmark for questions about complexity, algorithms, heuristics and threshold phenomena. Recent work on heuristics, and the satisfiability threshold has centered around the structure and connectivity of the so lution space. Motivated by this work, we study structural and connectivity-related properties of t he space of solutions of Boolean satisfiability problems and establish various dichotomies in Schaefer’s f ramework. On the structural side, we obtain dichotomies for the kinds o f subgraphs of the hypercube that can be induced by the solutions of Boolean formulas, as well as for t he diameter of the connected components of the solution space. On the computational side, we establi h dichotomy theorems for the complexity of the connectivity andst-connectivity questions for the graph of solutions of Boole an formulas. Our results assert that the intractable side of the computation l dichotomies is PSPACE-complete, while the tractable side which includes but is not limited to all prob lems with polynomial time algorithms for satisfiability is in P for thest-connectivity question, and in coNP for the connectivity qu estion. The diameter of components can be exponential for the PSPACE-co mplete cases, whereas in all other cases it is linear; thus, small diameter and tractability of the co nnectivity problems are remarkably aligned. The crux of our results is an expressibility theorem showing that in the tractable cases, the subgraphs induced by the solution space possess certain good structur al properties, whereas in the intractable cases, the subgraphs can be arbitrary.	algorithm;benchmark (computing);boolean expression;boolean satisfiability problem;co-np;cobham's thesis;computation;connected component (graph theory);feasible region;heuristic (computer science);pspace;pspace-complete;polynomial;time complexity	Parikshit Gopalan;Phokion G. Kolaitis;Elitza N. Maneva;Christos H. Papadimitriou	2006	SIAM J. Comput.	10.1137/07070440X	dichotomy;mathematical optimization;combinatorics;discrete mathematics;maximum satisfiability problem;mathematics;boolean satisfiability problem;algorithm	Theory	21.39684679142939	25.672274256056138	106822
485dfdd938331cce2362c48bf8e7c3c828342c98	complexity of splits reconstruction for low-degree trees	data structure;discrete mathematics;biological activity;inverse problem;drug design;maximum degree;wiener index	Given a vertex-weighted tree T , the split of an edge xy in T is min{sx(xy), sy(xy)} where su(uv) is the sum of all weights of vertices that are closer to u than to v in T . Given a set of weighted vertices V and a multiset of splits S, we consider the problem of constructing a tree on V whose splits correspond to S. The problem is known to be NP-complete, even when all vertices have unit weight and the maximum vertex degree of T is required to be no more than 4. We show that • the problem is strongly NP-complete when T is required to be a path, • the problem is NP-complete when all vertices have unit weight and the maximum degree of T is required to be no more than 3, and • it remains NP-complete when all vertices have unit weight and T is required to be a caterpillar with unbounded hair length and maximum degree at most 3. We also design polynomial time algorithms for • the variant where T is required to be a path and the number of distinct vertex weights is constant, and • the variant where all vertices have unit weight and T has a constant number of leaves. The latter algorithm is not only polynomial when the number of leaves, k, is a constant, but also fixed-parameter tractable when parameterized by k. Finally, we shortly discuss the problem when the vertex weights are not given but can be freely chosen by an algorithm. The considered problem is related to building libraries of chemical compounds used for drug design and discovery. In these inverse problems, the goal is to generate chemical compounds having desired structural properties, as there is a strong correlation between structural properties, such as the Wiener index, which is closely connected to the considered problem, and biological activity.	algorithm;ambiguous name resolution;autodesk maya;degree (graph theory);emoticon;karp's 21 np-complete problems;library (computing);linear algebra;polynomial;strong np-completeness;time complexity;vertex (geometry);wiener index	Serge Gaspers;Mathieu Liedloff;Maya Jakobine Stein;Karol Suchan	2011		10.1007/978-3-642-25870-1_16	combinatorics;discrete mathematics;mathematics;geometry;neighbourhood;algorithm	Theory	18.767443874645906	22.30756651529153	106876
297d0a1e5a0064da7857ef2e29290d1c34ff19c8	parallel evaluation of the determinant and of the inverse of a matrix	calcul matriciel;algoritmo paralelo;parallel algorithm;algorithmique;multiprocessor;complexite calcul;complejidad calculo;computing complexity;matrix inversion;inversion matriz;algorithme parallele;polynome caracteristique;algorithmics;algoritmica;determinante;determinant;inversion matrice;matrix calculus;multiprocesador;characteristic polynomial;calculo de matrices;polinomio caracteristico;multiprocesseur	Abstract   We decrease (from( n  2.876 )  to  o( n  2.851 )) the current record bound on the number of processors required in O(log 2  n ) step parallel arithmetic algorithms over rationals for the exact evaluation of the inverse and all coefficients of the characteristic polynomial of an  n  ×  n  rational, real, or complex matrix  A . For an integer input matrix  A , the evaluation involves only  d -bit numbers where either  d  = O(log  p ) if the computation is modulo a prime  p  or  d  = O( n  log ‖ A ‖) in the general case; the Boolean cost of computing det  A  is further decreased in a randomized parallel algorithm.		Zvi Galil;Victor Y. Pan	1989	Inf. Process. Lett.	10.1016/0020-0190(89)90173-7	combinatorics;discrete mathematics;multiprocessing;determinant;matrix calculus;computer science;mathematics;parallel algorithm;characteristic polynomial;algorithmics;algorithm	DB	13.89072578500197	31.39587369413488	106900
0ffd02a92320d439221ea9bf28aae793f38789ba	network a randomized automatic pixel purity index technique			pixel;purity (quantum mechanics);randomized algorithm	Mahmoud Maghrbay;Reda A. Ammar;Sanguthevar Rajasekaran	2010			parallel computing;computer science;pixel	Vision	11.606974645442943	32.08325561109278	107042
2506705c0645bfbf951574c064ff1f5ec4c42d8b	solving the 2-disjoint paths problem in nearly linear time	graphe non oriente;non directed graph;camino grafo;graph path;inversion;temps lineaire;problema np duro;tiempo lineal;arete disjointe;np hard problem;probleme np difficile;disjoint edge;grafo no orientado;informatique theorique;directed graph;linear time;graphe oriente;chemin graphe;grafo orientado;arista disyuntiva;disjoint paths;computer theory;informatica teorica	Given four distinct vertices s 1 , s 2 , t 1 , and t 2  of a graph G, the 2-disjoint paths problem is to determine two disjoint paths, p 1  from s 1  to t 1  and p 2  from s 2  to t 2 , if such paths exist. Disjoint can mean vertex- or edge-disjoint. Both, the edge- and the vertex-disjoint version of the problem, are NP-hard in the case of directed graphs. For undirected graphs, we show that the O(mn)-time algorithm of Shiloach can be modified so as to solve the 2-(vertex-)disjoint paths problem in O(n + mα(m, n)) time, where m is the number of edges in G, n is the number of vertices in G, and α denotes the inverse of the Ackermann function. Our result also improves the running time for the 2-edge-disjoint paths problem on undirected graphs as well as the running times for the decision versions of the 2-vertex- and the 2-edge-disjoint paths problem on dags.	time complexity	Torsten Tholey	2004		10.1007/978-3-540-24749-4_31	inversion;time complexity;combinatorics;discrete mathematics;directed graph;floyd–warshall algorithm;computer science;np-hard;mathematics;algorithm	Theory	20.778929140348403	26.645319030658573	107272
270eeb676948dd4f2b68d5e4bd5560b209c88d90	approximation algorithms for channel allocation problems in broadcast networks	hipergrafico;approximate algorithm;image processing;allocation canal;resource allocation;approximation algorithm;speech processing;canal transmision;asignacion canal;packing;tratamiento palabra;procesamiento imagen;traitement parole;individual object;traitement image;canal transmission;transmission channel;diffusion information;diffusion donnee;algorithme reparti;information dissemination;algoritmo aproximacion;difusion dato;pattern recognition;number;algoritmo repartido;asignacion recurso;hypergraph;reconnaissance forme;difusion informacion;information system;data broadcast;reconocimiento patron;allocation ressource;algorithme approximation;channel allocation;nombre;distributed algorithm;systeme information;numero;garnissage;hypergraphe;relleno;sistema informacion	We study two packing problems that arise in the area of dissemination-based information systems; a second theme is the study of distributed approximation algorithms. The problems considered have the property that the space occupied by a collection of objects together could be signi cantly less than the sum of the sizes of the individual objects. In the Channel Allocation Problem, there are users who request subsets of items. There are a xed number of channels that can carry an arbitrary amount of information. Each user must get all of the requested items from one channel, i.e., all the data items of each request must be broadcast on some channel. The load on any channel is the number of items that are broadcast on that channel; the objective is to minimize the maximum load on any channel. We present approximation algorithms for this problem and also show that the problem is MAX-SNP hard. The second problem is the Edge Partitioning Problem addressed by Goldschmidt, Hochbaum, Levin, and Olinick (Networks, 41:13-23, 2003 ). Each channel here can deliver information to at most k users, and we aim to minimize the total load on all channels. We present an O(n){approximation algorithm and also show that the algorithm can be made fully distributed with the same approximation guarantee; we also generalize to the case of hypergraphs.	approximation algorithm;genetic algorithm;information system;max;partition problem;snp (complexity);set packing	Rajiv Gandhi;Samir Khuller;Aravind Srinivasan;Nan Wang	2003	Networks	10.1007/978-3-540-45198-3_5	distributed algorithm;mathematical optimization;numero sign;telecommunications;image processing;resource allocation;computer science;theoretical computer science;speech processing;distributed computing;grammatical number;approximation algorithm;information system;algorithm	Theory	20.401461518196342	28.098714851741747	107334
9f323daa2ea7640e212460eaa7c25c1b4ac1170f	encoding 2d range maximum queries	indexing model;journal article;cartesian trees;effective entropy;range maximum queries;independent identically distributed;upper and lower bounds;data structure;encoding model	a Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong b Department of Computer Science and Engineering, Polytechnic Institute of New York University, Brooklyn, NY 11201, USA c Mathematics and Computer Science, Wesleyan University, Middletown, CT 06459, USA d Department of Computer Science, University of Leicester, Leicester, UK e School of Computer Science and Engineering, Seoul National University, Seoul, South Korea f Department of Computer Science, Rutgers University, Camden, NJ 08102, USA	computational complexity theory;computer science;data structure;olap cube;online analytical processing;range minimum query;relevance;space–time tradeoff	Mordecai J. Golin;John Iacono;Danny Krizanc;Rajeev Raman;S. Srinivasa Rao	2016	Theor. Comput. Sci.	10.1016/j.tcs.2015.10.012	combinatorics;discrete mathematics;data structure;computer science;theoretical computer science;mathematics;upper and lower bounds;programming language	Theory	15.16053799145933	24.64634059565808	107516
d92eb00111558b50cc14cf22ab9cdd044de68050	a run-time efficient implementation of compressed pattern matching automata	theoretical computer science;computer science all;huffman codes;prefix code;efficient implementation;pattern matching;biochemistry genetics and molecular biology all;compression ratio;high speed	We present a run-time efficient implementation of automata for compressed pattern matching (CPM), where a text is given as a truncation-free collage system $\langle{\mathcal D},{\mathcal S}\rangle$ such that variable sequence $\mathcal S$ is encoded by any prefix code. We experimentally show that a combination of recursive-pairing compression and byte-oriented Huffman coding allows both a high compression ratio and a high speed CPM.	automaton;compressed pattern matching	Tetsuya Matsumoto;Kazuhito Hagio;Masayuki Takeda	2008		10.1007/978-3-540-70844-5_21	prefix code;computer science;theoretical computer science;algorithm	Logic	12.072402278519231	28.071914289769133	107530
2cbf86acf894459988ce849f3d2cef294d842727	relaxed balance using standard rotations	arbre graphe;control relajado;arbre recherche;approximation asymptotique;ajustamiento modelo;equilibrado;tree graph;relaxed control;ajustement modele;search trees;arbol investigacion;equilibre;model matching;estructura datos;balancing;commande relaxee;structure donnee;rotacion;asymptotic approximation;equilibrium;equilibrio;arbol grafo;algoritmo optimo;search tree;rotation;algorithme optimal;optimal algorithm;data structure;equilibrage;parallel applications;aproximacion asintotica	Abstract. In search trees with relaxed balance, rebalancing transformations need not be connected with updates, but may be delayed. For standard AVL tree rebalancing, we prove that even though the rebalancing operations are uncoupled from updates, their total number is bounded by O(M log (M+N)) , where M is the number of updates to an AVL tree of initial size N . Hence, relaxed balancing of AVL trees comes at no extra cost asymptotically. Furthermore, our scheme differs from most other relaxed balancing schemes in an important aspect: No rebalancing transformation can be done in the wrong direction, i.e., no performed rotation can make the tree less balanced. Moreover, each performed rotation indeed corresponds to a real imbalance situation in the tree. Finally, and perhaps most importantly, our structure is capable of forgetting registered imbalance if later updates happen to improve the situation. Our results are of theoretical interest and have possible sequential and parallel applications.	avl tree;detailed balance;parallel computing;self-balancing binary search tree	Kim S. Larsen;Eljas Soisalon-Soininen;Peter Widmayer	2001	Algorithmica	10.1007/s00453-001-0059-x	mathematical optimization;combinatorics;data structure;rotation;computer science;mathematics;search tree;programming language;tree;algorithm;avl tree	Theory	16.32784945600572	27.765694948502396	107617
947a70093242e216ddba93d136f9a8d60abe4ac8	optimal disk merge patterns	cost function	Our purpose is to study the optimal way to merge n initially sorted runs, stored on a disk like device, into a unique sorted file. This problem is equivalent to finding a tree with n leaves which minimizes a certain cost function (see Knuth [1]). We shall study some properties of those optimal trees, in the hope of finding efficient ways for constructing them. In particular, if all the initial runs have the same length, an algorithm for constructing the best merge pattern is described ; its running time is proportional to n 2 and it requires space proportional to n. A special case is also analyzed in which the problem is solved in time and space proportional to n, and which provides some insight into the asymptotic behaviour of optimal merge trees.	algorithm;computer science;francis;jean;loss function;merge sort;optimal stopping;the art of computer programming;time complexity	Maurice Schlumberger;Jean Vuillemin	1973	Acta Informatica	10.1007/BF00288649	mathematical optimization;computer science;theoretical computer science;mathematics;merge algorithm;algorithm;polyphase merge sort;loss function	Theory	14.586871233190607	28.032389342152197	107941
0742ce718e6d15361f62659658992e75d1bd77cc	the complexity of determining existence a hamiltonian cycle is $o(n^3)$	hamiltonian cycle;discrete mathematics;computational complexity;bipartite graph;data structure	The Hamiltonian cycle problem in digraph is mapped into a matching cover bipartite graph. Based on this mapping, it is proved that determining existence a Hamiltonian cycle in graph is O(n). Abstract. Hamiltonian Cycle, Z-mapping graph, complexity, decision, matching covered, optimization Hamiltonian Cycle, Z-mapping graph, complexity, decision, matching covered, optimization	complexity;directed graph;hamiltonian path problem;mathematical optimization	Guohun Zhu	2007	CoRR		hamiltonian path;graph power;edge-transitive graph;folded cube graph;complete bipartite graph;factor-critical graph;combinatorics;discrete mathematics;topology;data structure;bipartite graph;computer science;3-dimensional matching;simplex graph;aperiodic graph;hypercube graph;foster graph;cycle graph;graph factorization;mathematics;voltage graph;distance-hereditary graph;hamiltonian path problem;programming language;computational complexity theory;cycle basis;biregular graph;quartic graph	Theory	24.038040144247326	27.0902314508955	108023
ed87837a6e52b35d89f8bb974cb8b83d5b51b952	approximating minimum cost source location problems with local vertex-connectivity demands	location problem;approximate algorithm	The source location problem is a problem of computing a minimum cost source set in an undirected graph so that the connectivity between the source set and a vertex is at least the demand of the vertex. In this paper, the connectivity between a source set S and a vertex v is defined as the maximum number of paths between v and S no two of which have common vertex except v. We propose an O(d∗ log d∗)-approximation algorithm for the problem with maximum demand d∗. We also define a variant of the source location problem and propose an approximation algorithm for it.	approximation algorithm;graph (discrete mathematics);greedy algorithm;hiroshi ishii (computer scientist);iterative method;k-vertex-connected graph;rounding	Takuro Fukunaga	2011		10.1007/978-3-642-20877-5_42	mathematical optimization;combinatorics;discrete mathematics;feedback vertex set;vertex cover;vertex;mathematics;1-center problem	Theory	24.008094345418243	19.269063898625067	108090
0e6f860802d524c54cf365c261c3a4d6f0ae3a7b	single pass sparsification in the streaming model with edge deletions		In this paper we give a construction of cut sparsifiers of Benc zúr and Karger in thedynamicstreaming setting in a single pass over the data stream. Previous const ructions either required multiple passes or were unable to handle edge deletions. We use Õ(1/ǫ) time for each stream update and̃ O(n/ǫ) time to construct a sparsifier. Our ǫ-sparsifiers have O(n log n/ǫ) edges. The main tools behind our result are an application of sketching techniques of Ahn et al.[SODA’12] to estimate edge connectivity together with a novel application of sampling with limited independence an d sparse recovery to produce the edges of the sparsifier.	compressed sensing;const (computer programming);k-edge-connected graph;sampling (signal processing);sparse matrix	Ashish Goel;Mikhail Kapralov;Ian Post	2012	CoRR		telecommunications;computer science;algorithm	Theory	20.567521125449467	24.214601623724107	108124
3a8ace6854bbe79573c4120c25d2f9dc5543372b	algorithms for 2-route cut problems	approximate algorithm;fault tolerant;network flow	In this paper we study approximation algorithms for multiroute cut problems in undirected graphs. In these problems the goal is to find a minimum cost set of edges to be removed from a given graph such that the edge-connectivity (or node-connectivity) between certain pairs of nodes is reduced below a given threshold K. In the usual cut problems the edge connectivity is required to be reduced below 1 (i.e. disconnected). We consider the case of K = 2 and obtain poly-logarithmic approximation algorithms for fundamental cut problems including singlesource, multiway-cut, multicut, and sparsest cut. These cut problems are dual to multi-route flows that are of interest in fault-tolerant networks flows. Our results show that the flow-cut gap between 2-route cuts and 2-route flows is poly-logarithmic in undirected graphs with arbitrary capacities. 2-route cuts are also closely related to well-studied feedback problems and we obtain results on some new variants. Multi-route cuts pose interesting algorithmic challenges. The new techniques developed here are of independent technical interest, and may have applications to other cut and partitioning problems.	approximation algorithm;cut (graph theory);fault tolerance;graph (discrete mathematics);k-edge-connected graph	Chandra Chekuri;Sanjeev Khanna	2008		10.1007/978-3-540-70575-8_39	mathematical optimization;maximum cut;fault tolerance;combinatorics;discrete mathematics;flow network;minimum cut;cut;computer science;gomory–hu tree;mathematics	Theory	22.892798850310808	20.44989922725346	108187
503c37bb69eae85505686adb0cf02e447c4a446a	efficient ranking of lyndon words and decoding lexicographically minimal de bruijn sequence	lyndon word;efficient algorithm;deterministic finite automaton;necklaces;68r15;68w32;de bruijn sequence;68q45;68r05	We give efficient algorithms for ranking Lyndon words of length n over an alphabet of size σ. The rank of a Lyndon word is its position in the sequence of lexicographically ordered Lyndon words of the same length. The outputs are integers of exponential size, and complexity of arithmetic operations on such large integers cannot be ignored. Our model of computations is the word-RAM, in which basic arithmetic operations on (large) numbers of size at most σ take O(n) time. Our algorithm for ranking Lyndon words makes O(n) arithmetic operations (this would imply directly cubic time on word-RAM). However, using an algebraic approach we are able to reduce the total time complexity on the word-RAM to O(n log σ). We also present an O(n log σ)-time algorithm that generates the Lyndon word of a given length and rank in lexicographic order. Finally we use the connections between Lyndon words and lexicographically minimal de Bruijn sequences (theorem of Fredricksen and Maiorana) to develop the first polynomial-time algorithm for decoding minimal de Bruijn sequence of any rank n (it determines the position of an arbitrary word of length n within the de Bruijn sequence).	algorithm;computation;de bruijn graph;joe 90;lexicographical order;linear algebra;polynomial;random-access memory;time complexity	Tomasz Kociumaka;Jakub Radoszewski;Wojciech Rytter	2016	SIAM J. Discrete Math.	10.1137/15M1043248	arithmetic;combinatorics;discrete mathematics;de bruijn sequence;deterministic finite automaton;lyndon word;mathematics;algebra	Theory	12.816186736118933	26.706500579445013	108303
45ef9086509e1d1f285b8c237faaef4d1559f4dc	minimum cost homomorphisms to proper interval graphs and bigraphs	interval graph;discrete mathematics;artificial intelligent;optimization problem;polynomial time;connected component	For graphs G and H , a mapping f : V (G)→V (H) is a homomorphism of G to H if uv ∈ E(G) implies f(u)f(v) ∈ E(H). If, moreover, each vertex u ∈ V (G) is associated with costs ci(u), i ∈ V (H), then the cost of the homomorphism f is ∑ u∈V (G) cf(u)(u). For each fixed graph H , we have the minimum cost homomorphism problem, written as MinHOM(H). The problem is to decide, for an input graph G with costs ci(u), u ∈ V (G), i ∈ V (H), whether there exists a homomorphism of G to H and, if one exists, to find one of minimum cost. Minimum cost homomorphism problems encompass (or are related to) many well studied optimization problems. We describe a dichotomy of the minimum cost homomorphism problems for graphs H , with loops allowed. When each connected component of H is either a reflexive proper interval graph or an irreflexive proper interval bigraph, the problem MinHOM(H) is polynomial time solvable. In all other cases the problem MinHOM(H) is NP-hard. This solves an open problem from an earlier paper. Along the way, we prove a new characterization of the class of proper interval bigraphs. 1 Motivation and Terminology We consider finite undirected and directed graphs without multiple edges, but with loops allowed. For a directed or undirected graph H, V (H) (E(H)) Corresponding author. Department of Computer Science, Royal Holloway University of London, Egham, Surrey TW20 OEX, UK, gutin@cs.rhul.ac.uk and Department of Computer Science, University of Haifa, Israel School of Computing Science, Simon Fraser University, Burnaby, B.C., Canada, V5A 1S6, pavol@cs.sfu.ca Department of Computer Science, Royal Holloway University of London, Egham, Surrey TW20 OEX, UK, arash@cs.rhul.ac.uk Department of Computer Science, Royal Holloway University of London, Egham, Surrey TW20 OEX, UK, anders@cs.rhul.ac.uk	bigraph;computer science;connected component (graph theory);decision problem;directed graph;graph (discrete mathematics);mathematical optimization;multiple edges;np-hardness;polynomial;school of computing (robert gordon university);time complexity	Gregory Gutin;Pavol Hell;Arash Rafiey;Anders Yeo	2006	CoRR		time complexity;optimization problem;mathematical optimization;combinatorics;discrete mathematics;connected component;interval graph;mathematics;algorithm	Theory	20.574026344277737	19.969779755334145	108377
3764bd3dfd66b2ef0181a0f5ff3d0c65ec17db0f	multiple matching of rectangular patterns	frecuencia aparicion;traitement liste;frequence apparition;text;algoritmo busqueda;algorithme recherche;efficient algorithm;search algorithm;tratamiento lista;dynamic method;rectangular shape;texte;dictionnaire;occurrence frequency;methode dynamique;pattern matching;dictionaries;metodo dinamico;concordance forme;string matching;texto;diccionario;list processing;forma rectangular;forme rectangulaire;aspect ratio	We describe the first worst-case efficient algorithm for simultaneously matching multiple rectangular patterns of varying sizes and aspect ratios in a rectangular text. Efficient means significantly more efficient asymptotically than applying known algorithms that handle one height (or width or aspect ratio) at a time for each height. Our algorithm features an interesting use of multidimensional range searching, as well as new adaptations of several known techniques for two-dimensional string matching. We also extend our algorithm to a dynamic setting where the set of patterns can change over time.		Ramana M. Idury;Alejandro A. Schäffer	1995	Inf. Comput.	10.1006/inco.1995.1030	combinatorics;aspect ratio;computer science;pattern matching;mathematics;geometry;programming language;algorithm;string searching algorithm;search algorithm	Logic	14.556081809054614	26.59680020369461	108467
3704ec8429a3064acc38cc0cee502cea826eb749	an improved randomized approximation algorithm for max tsp	graphe non oriente;traveling salesman problem;approximation asymptotique;algoritmo aleatorizado;randomized algorithms;non directed graph;approximate algorithm;circuito hamiltoniano;approximation algorithms;circuit hamiltonien;travelling salesman problem;approximation algorithm;randomized approximation algorithm;algorithme randomise;hamiltonian circuit;problema viajante comercio;max tsp;probleme commis voyageur;grafo no orientado;algoritmo aproximacion;randomized algorithm;graph algorithm;asymptotic approximation;algorithme approximation;graph algorithms;aproximacion asintotica	We present an O(n3)-time randomized approximation algorithm for the maximum traveling salesman problem whose expected approximation ratio is asymptotically 251 331 , where n is the number of vertices in the input (undirected) graph. This improves the previous best.	approximation algorithm;graph (discrete mathematics);randomized algorithm;travelling salesman problem	Zhi-Zhong Chen;Lusheng Wang	2005	J. Comb. Optim.	10.1007/s10878-005-1779-7	mathematical optimization;combinatorics;discrete mathematics;polynomial-time approximation scheme;computer science;mathematics;randomized algorithm;travelling salesman problem;approximation algorithm;algorithm	Theory	21.001065023813926	26.447701203895402	108479
3b100e4353c89e24017bd4ef1de26f0e31baed2e	optimal branch-decomposition of planar graphs in o(n3) time	descomposicion grafo;branching;automaton;refinement method;automata;ramificacion;graphe planaire;automate;ramification;methode raffinement;grafo planario;metodo afinamiento;planar graph;graph decomposition;decomposition graphe	We give an O(n) time algorithm for constructing a minimum-width branch-decomposition of a given planar graph with n vertices. This is achieved through a refinement to the previously best known algorithm of Seymour and Thomas, which runs in O(n) time.		Qian-Ping Gu;Hisao Tamaki	2005		10.1007/11523468_31	outerplanar graph;combinatorics;discrete mathematics;computer science;artificial intelligence;mathematics;automaton;algorithm	Theory	22.237103610493072	27.93145745081944	108484
80338d9f4e58899c545ec7b342bde948a3006f74	the maximum colorful arborescence problem parameterized by the structure of its color hierarchy graph		Let G = (V, A) be a vertex-colored arc-weighted directed acyclic graph (DAG) rooted in some vertex r. The color hierarchy graph H(G) of G is defined as follows: V (H(G)) is the color set C of G, and H(G) has an arc from c to c if G has an arc from a vertex of color c to a vertex of color c. We study the Maximum Colorful Arborescence (MCA) problem, which takes as input a DAG G such that H(G) is also a DAG, and aims at finding in G a maximum-weight arborescence rooted in r in which no color appears more than once. The MCA problem models the de novo inference of unknown metabolites by mass spectrometry experiments. Although the problem has been introduced ten years ago (under a different name), it was only recently pointed out that a crucial additional property in the problem definition was missing: by essence, H(G) must be a DAG. In this paper, we further investigate MCA under this new light and provide new algorithmic results for this problem, with a specific focus on fixed-parameter tractability (FPT) issues for different structural parameters of H(G). In particular, we show there exists an O(3 ∗ H) time algorithm for solving MCA, where nH is the number of vertices of indegree at least two in H(G), thereby improving the O(3) algorithm from Böcker et al. [Proc. ECCB ’08]. We also prove that MCA is W[2]-hard relatively to the treewidth Ht of the underlying undirected graph of H(G), and further show that it is FPT relatively to Ht + lC , where lC := |V | − |C|. 2012 ACM Subject Classification F.2.2 Nonnumerical Algorithms and Problems, G.2.1 Combinatorics, G.2.2 Graph Theory	algorithm;de novo transcriptome assembly;directed acyclic graph;directed graph;european conference on computational biology;experiment;graph (discrete mathematics);graph theory;parameterized complexity;treewidth	Guillaume Fertin;Julien Fradin;Christian Komusiewicz	2017	CoRR		combinatorics;discrete mathematics;directed acyclic graph;arc (geometry);arborescence;hierarchy;vertex (geometry);parameterized complexity;graph;treewidth;mathematics	Theory	23.054528186375805	22.854174484543613	108587
711acf7e0a7e317387f0405828cc8c3ded7aeb97	black-box complexity: breaking the $o(n \log n)$ barrier of leadingones	runtime analysis;query complexity;theory;algorithms;black box complexity	We show that the unrestricted black-box complexity of the n-dimensional XORand permutation-invariant LeadingOnes function class is O(n log(n)/ log logn). This shows that the recent natural looking O(n logn) bound is not tight. The black-box optimization algorithm leading to this bound can be implemented in a way that only 3-ary unbiased variation operators are used. Hence our bound is also valid for the unbiased black-box complexity recently introduced by Lehre and Witt. The bound also remains valid if we impose the additional restriction that the black-box algorithm does not have access to the objective values but only to their relative order (ranking-based black-box complexity).	algorithm;black box;mathematical optimization	Benjamin Doerr;Carola Doerr	2011		10.1007/978-3-642-35533-2_18	mathematical optimization;combinatorics;discrete mathematics;average-case complexity;worst-case complexity;mathematics;theory;algorithm;statistics	Theory	14.40821172635909	20.04380944976593	108825
535d86377d4980c3144f815e3390e15f242aa421	tight bounds on the solutions of multidimensional divide-and-conquer maximin recurrences	arbre graphe;graph theory;methode diviser pour regner;partition trees;teoria grafo;algorithm analysis;tree graph;complexite calcul;relation recurrence;recurrence relations;exact solution;metodo dividir para vencer;theorie graphe;analysis of algorithm;analysis of algorithms;upper bound;complejidad computacion;computational complexity;divide and conquer method;recurrence relation;analyse algorithme;arbol grafo;relacion recurrencia;divide and conquer;analisis algoritmo	Abstract   In this paper, upper bounds are presented for the solution of the following multidimensional divide-and-conquer maximin recurrence   G(n)=  max  n     1   +n     2   +⋯+n     p   =n  n     1   ,    n     2   ,…,n     p   ⩾1  ∑  i=1  p  G(n     i   )+  smin  1⩽i⩽p       (k)   f(n     i   )  ,   where   p⩾2,    1⩽k    f   is an arbitrary nondecreasing function, and “  smin     1⩽i⩽p      (k)     f(n     i   )  ” denotes “the sum of the smallest   k   numbers among   f(n     1   ),f(n     2   ),…,   and   f(n     p   )”. All the presented upper bounds are at most   ⌈  log     2   k⌉+p   times the exact solution of   G(n)  . The derivation of the upper bounds is based on properties of partition trees. For   k=1   and   k=p−1   we obtain, respectively, two of the recurrences previously studied by Alonso et al. (SIAM J. Discrete Math. 8 (1995) 428–447). In both of these two cases, our results improve theirs.	minimax;recurrence relation;wald's maximin model	Biing-Feng Wang	2000	Theor. Comput. Sci.	10.1016/S0304-3975(98)00334-X	combinatorics;recurrence relation;graph theory;mathematics;algorithm	ECom	20.623549323538064	27.039743876336193	108827
f09ab9704e7809d785fc5b12ba5ab658a3d4b5fc	autoscaling bloom filter: controlling trade-off between true and false positives		A Bloom filter is a simple data structure supporting membership queries on a set. The standard Bloom filter does not support the delete operation, therefore, many applications use a counting Bloom filter to enable deletion. This paper proposes a generalization of the counting Bloom filter approach, called “autoscaling Bloom filters”, which allows adjustment of its capacity with probabilistic bounds on false positives and true positives. In essence, the autoscaling Bloom filter is a binarized counting Bloom filter with an adjustable binarization threshold. We present the mathematical analysis of the performance as well as give a procedure for minimization of the false positive rate.	applications-by-forms;autoscaling;bloom filter;brainfuck;data structure;hash function;mathematical optimization	Denis Kleyko;Abbas Rahimi;Evgeny Osipov	2017	CoRR		arithmetic;quotient filter;theoretical computer science;mathematics;algorithm	DB	11.8329893272098	24.940128464437514	108832
4a4d0c84af28415622882f819e8a510533911fda	persistency in the assignment and transportation problems	assignment;graph theory;assignment problem;asignacion;graphe biparti;teoria grafo;problema transporte;transportation problem;grafo bipartido;probleme transport;persistency;assignation;theorie graphe;optimisation combinatoire;programacion lineal;linear programming;tâche appariement;tarea apareamiento;programmation lineaire;combinatorial optimization;bipartite graph;matching task;optimizacion combinatoria	Let G = (U, V, E) be a bipartite graph with weights of its edges cij. For the assignment and transportation problem given by such a graph we propose efficient procedures for partitioning the edge set E into three classes: E0 is the set of edges/j with x/j = 0 for each optimum solution (0persistent edges); E1 is the set of edges with x/j > 0 and constant for each optimum (1-persistent edges) and Ew is the set of edges such that there are two optimum solutions x, x' with x/j ~ x~. (weakly persistent edges).	transportation theory (mathematics)	Katarína Cechlárová	1998	Math. Meth. of OR	10.1007/BF01194399	transportation theory;mathematical optimization;combinatorics;discrete mathematics;feedback arc set;topological graph;multiple edges;bipartite graph;combinatorial optimization;linear programming;graph theory;force-directed graph drawing;mixed graph;multigraph;hypercube graph;cycle graph;assignment;mathematics;assignment problem;path;complement graph;strength of a graph;matching	ML	22.59104997198884	28.481114442310442	108962
4b978629d04c6a46c5c4b41d2eb91680c272d939	how to eliminate a graph	complete graph;elimination problem;graphs g;graph operation;vertex elimination;parameterized complexity;polynomial time;computational complexity;related clique problem;input graph	Vertex elimination is a graph operation that turns the neighborhood of a vertex into a clique and removes the vertex itself. It has widely known applications within sparse matrix computations. We define the Elimination problem as follows: given two graphs G and H, decide whether H can be obtained from G by |V (G)| − |V (H)| vertex eliminations. We study the parameterized complexity of the Elimination problem. We show that Elimination is W [1]-hard when parameterized by |V (H)|, even if both input graphs are split graphs, and W [2]-hard when parameterized by |V (G)| − |V (H)|, even if H is a complete graph. On the positive side, we show that Elimination admits a kernel with at most 5|V (H)| vertices in the case when G is connected and H is a complete graph, which is in sharp contrast to the W [1]-hardness of the related Clique problem. We also study the case when either G or H is tree. The computational complexity of the problem depends on which graph is assumed to be a tree: we show that Elimination can be solved in polynomial time when H is a tree, whereas it surprisingly remains NP-complete when G is a tree.	clique problem;computation;computational complexity theory;graph operations;parameterized complexity;polynomial;sparse matrix;time complexity	Petr A. Golovach;Pinar Heggernes;Pim van 't Hof;Fredrik Manne;Daniël Paulusma;Michal Pilipczuk	2012		10.1007/978-3-642-34611-8_32	graph power;split graph;combinatorics;discrete mathematics;feedback vertex set;vertex cover;degree;simplex graph;cycle graph;vertex;clique-sum;mathematics;tree-depth;bound graph;complement graph;chordal graph;neighbourhood;algorithm;shortest-path tree	DB	22.613900742455478	23.50218935883806	109000
edbfafa5ed6ed1eb41f91f129e53791cb2ff9306	graph-based decision for gödel-dummett logics	graphs;sequent calculus;matrix computation;decision procedure;countermodels;decision procedures;godel dummett logic	We present a graph-based decision procedure for Gödel-Dummett logics and an algorithm to compute countermodels. A formula is transformed into a conditional bicolored graph in which we detect some specific cycles and alternating chains using matrix computations. From an instance graph containing no such cycle, (resp. no (n + 1)-alternating chain) we extract a countermodel in LC, (resp. LC n ).	algorithm;computation;decision problem;gödel	Dominique Larchey-Wendling	2006	Journal of Automated Reasoning	10.1007/s10817-006-9047-9	combinatorics;discrete mathematics;null graph;clique-width;simplex graph;mathematics;voltage graph;numerical linear algebra;graph;programming language;sequent calculus;complement graph;algorithm;adjacency matrix	Logic	21.456394031433668	32.18803811208415	109094
16e5f81fc9017a40401c9675934884ada6b952a3	on approximating multi-criteria tsp	traveling salesman problem;approximate algorithm;multi criteria optimization;004;triangle inequality;randomized approximation algorithm;traveling salesman;objective function;data structure;approximation algorithms traveling salesman multi criteria optimization	We present approximation algorithms for almost all variants of the multicriteria traveling salesman problem (TSP), whose performances are independent of the number k of criteria and come close to the approximation ratios obtained for TSP with a single objective function. We present randomized approximation algorithms for multi-criteria maximum traveling salesman problems (Max-TSP). For multi-criteria Max-STSP, where the edge weights have to be symmetric, we devise an algorithm that achieves an approximation ratio of 2/3 − ε. For multi-criteria Max-ATSP, where the edge weights may be asymmetric, we present an algorithm with an approximation ratio of 1/2 − ε. Our algorithms work for any fixed number k of objectives. To get these ratios, we introduce a decomposition technique for cycle covers. These decompositions are optimal in the sense that no decomposition can always yield more than a fraction of 2/3 and 1/2, respectively, of the weight of a cycle cover. Furthermore, we present a deterministic algorithm for bi-criteria Max-STSP that achieves an approximation ratio of 61/243 ≈ 1/4. Finally, we present a randomized approximation algorithm for the asymmetric multicriteria minimum TSP with triangle inequality (Min-ATSP). This algorithm achieves a ratio of log n + ε. For this variant of multi-criteria TSP, this is the first approximation algorithm we are aware of. If the distances fulfil the γ-triangle inequality, its ratio is 1/(1 − γ) + ε. 1. Multi-Criteria Traveling Salesman Problem Traveling Salesman Problem. The traveling salesman problem (TSP) is one of the most famous combinatorial optimization problems. Given a graph, the goal is to find a Hamiltonian cycle (also called a tour) of maximum or minimum weight (Max-TSP or Min-TSP). An instance of Max-TSP is a complete graph G = (V, E) with edge weights w : E → Q+. The goal is to find a Hamiltonian cycle of maximum weight. The weight of a Hamiltonian cycle (or, more general, of any set of edges) is the sum of the weights of its edges. If G is undirected, we have Max-STSP (symmetric TSP). If G is directed, we obtain Max-ATSP (asymmetric TSP). An instance of Min-TSP is also a complete graph G with edge weights w that fulfil the triangle inequality: w(u, v) ≤ w(u, x)+w(x, v) for all u, v, x ∈ V . The goal is to find a tour of minimum weight. We have Min-STSP if G is undirected and Min-ATSP if G is directed. In this paper, we only consider the latter. If we restrict the instances to fulfil the γ-triangle inequality (w(u, v) ≤ γ · (w(u, x) + w(x, v)) for all distinct u, v, x ∈ V	approximation algorithm;combinatorial optimization;deterministic algorithm;directed graph;edge cycle cover;emoticon;graph (discrete mathematics);graph theory;hamiltonian path;mathematical optimization;maxima and minima;minimum weight;optimization problem;order of approximation;performance;randomized algorithm;social inequality;travelling salesman problem	Bodo Manthey	2009		10.4230/LIPIcs.STACS.2009.1853	mathematical optimization;combinatorics;discrete mathematics;polynomial-time approximation scheme;christofides algorithm;data structure;computer science;mathematics;travelling salesman problem;minimax approximation algorithm;approximation algorithm;bottleneck traveling salesman problem	Theory	23.916278049236425	19.877156617279145	109160
69db9663890c9d637e4c17c0dc19b7cb6832e702	on smoothed k-cnf formulas and the walksat algorithm	arbitrary instance f;n variable;k-cnf formulas f;k-cnf formula;k ln;random k-cnfs;whp unsatisfiable;smoothing operation;polynomial time whp;walksat algorithm;lower density;lower bound;polynomial time	In this paper we study the model of ε-smoothed k-CNF formulas. Starting from an arbitrary instance F with n variables and m = dn clauses, apply the ε-smoothing operation of flipping the polarity of every literal in every clause independently at random with probability ε. Keeping ε and k fixed, and letting the density d = m/n grow, it is rather easy to see that for d ≥ ε ln 2, F becomes whp unsatisfiable after smoothing. We show that a lower density that behaves roughly like ε suffices for this purpose. We also show that our bound on d is nearly best possible in the sense that there are k-CNF formulas F of slightly lower density that whp remain satisfiable after smoothing. One consequence of our proof is a new lower bound of Ω(2/k) on the density up to which Walksat solves random k-CNFs in polynomial time whp. We are not aware of any previous rigorous analysis showing that Walksat is successful at densities that are increasing as a function of k. University of Edinburgh . E-mail: acoghlan@inf.ed.ac.uk. The Weizmann Institute. E-mail: uriel.feige@weizmann.ac.il. Supported in part by The Israel Science Foundation (grant No. 873/08) Carnegie Mellon University. E-mail: alan@random.math.cmu.edu. Supported in part by NSF grant DMS0753472 Tel-Aviv University. E-mail: krivelev@post.tau.ac.il. Research supported in part by a USA-Israel BSF Grant, by a grant from the Israel Science Foundation, and by Pazy Memorial Award. Tel-Aviv University. E-mail: vilenchi@post.tau.ac.il.	bean scripting framework;conjunctive normal form;genetic algorithm;ibm notes;literal (mathematical logic);mail (macos);polynomial;smoothing;time complexity;walksat	Amin Coja-Oghlan;Uriel Feige;Alan M. Frieze;Michael Krivelevich;Dan Vilenchik	2009				Theory	11.211450802975913	21.563578194026068	109210
370a1740aa4d524774bef1f91ba47b0c1ee77399	time-space tradeoffs for undirected st-connectivity on a graph automata	undirected st connectivity;graphe non oriente;graph theory;probabilistic automaton;teoria grafo;non directed graph;lower bounds;68q05;theorie graphe;algorithme;algorithm;jag graph;graph connectivity;grafo no orientado;conectividad grafo;automate probabiliste;68q15;automata probabilista;time space tradeoffs;68q25;connectivite graphe;algoritmo	Undirected st-connectivity is an important problem in computing. There are algorithms for this problem that use O (n) time and ones that use O (logn) space. The main result of this paper is that, in a very natural structured model, these upper bounds are not simultaneously achievable. Any probabilistic jumping automaton for graphs (JAG) requires either space Ω(log n/ log logn) or time n(1+Ω(1/ log logn)) to solve undirected st-connectivity.	algorithm;automaton;graph (discrete mathematics);st-connectivity	Jeff Edmonds	1998	SIAM J. Comput.	10.1137/S0097539794277135	combinatorics;discrete mathematics;connectivity;graph theory;probabilistic automaton;mathematics;algorithm	Theory	19.828137176061418	28.07268271419978	109298
b2e31dac80a70a1ecddb47b09556003e3ece6928	lessons from the congested clique applied to mapreduce	distributed algorithms;graph coloring;mapreduce;congested clique	The main results of this paper are (I) a simulation algorithm which, under quite general constraints, transforms algorithms running on the Congested Clique into alg rithms running in the MapReduce model, and (II) a distributedO(∆)-coloring algorithm running on the Congested Clique which h as an expected running time of O(1) rounds, if∆ ≥ Θ(log n); andO(log log log n) rounds otherwise. Applying the simulation theorem to the Congested CliqueO(∆)-coloring algorithm yields anO(1)-roundO(∆)-coloring algorithm in the MapReduce model. Our simulation algorithm illustrates a natural correspond ence between per-node bandwidth in the Congested Clique model and memory per machine in the MapReduce model. In the Co ngested Clique (and more generally, any network in theCONGEST model), the major impediment to constructing fast algorith ms is theO(log n) restriction on message sizes. Similarly, in the MapReduce model, the com bined restrictions on memory per machine and total system memory have a dominant effect on algorithm design. In showing a fairly general simulation algorithm, we highlight the similarities and differences between these m odels.	algorithm design;graph coloring;mapreduce;provable security;simulation;stateless protocol;time complexity	James Hegeman;Sriram V. Pemmaraju	2015	Theor. Comput. Sci.	10.1016/j.tcs.2015.09.029	distributed algorithm;combinatorics;computer science;theoretical computer science;graph coloring;mathematics;distributed computing	Metrics	20.08710943851074	21.865082486269984	109350
6706dd69dcdd90974f2b43a0a37614b43991c6f9	randomized group testing both query-optimal and minimal adaptive	algorithmic learning theory;parallel queries;randomized algorithm;group testing;bioinformatics	The classical group testing problem asks to determine at most d defective elements in a set of n elements, by queries to subsets that return Yes if the subset contains some defective, and No if the subset is free of defectives. By the entropy lower bound, log2 ∑d	binary logarithm;randomized algorithm	Peter Damaschke;Azam Sheikh Muhammad	2012		10.1007/978-3-642-27660-6_18	combinatorics;discrete mathematics;algorithmic learning theory;group testing;computer science;mathematics;randomized algorithm;algorithm	Theory	14.382455692660056	22.97616651194132	109515
08202d8f76900d68f66f13fb0e804ada471ee70e	nearly-linear work parallel sdd solvers, low-diameter decomposition, and low-stretch subgraphs	linear systems;low stretch spanning trees;low diameter decomposition;low stretch subgraphs;parallel algorithms	We present the design and analysis of a nearly-linear work parallel algorithm for solving symmetric diagonally dominant (SDD) linear systems. On input an SDD n-by-n matrix A with m nonzero entries and a vector b, our algorithm computes a vector $\tilde{x}$ such that $\|\tilde{x} - A^{+}b\|_{A} \leq\varepsilon\cdot\|{A^{+}b}\|_{A}$ in $O(m\log^{O(1)}{n}\log {\frac{1}{\varepsilon}})$ work and $O(m^{1/3+\theta}\log\frac{1}{\varepsilon})$ depth for any θ>0, where A + denotes the Moore-Penrose pseudoinverse of A. The algorithm relies on a parallel algorithm for generating low-stretch spanning trees or spanning subgraphs. To this end, we first develop a parallel decomposition algorithm that in O(mlog O(1) n) work and polylogarithmic depth, partitions a graph with n nodes and m edges into components with polylogarithmic diameter such that only a small fraction of the original edges are between the components. This can be used to generate low-stretch spanning trees with average stretch O(n α ) in O(mlog O(1) n) work and O(n α ) depth for any α>0. Alternatively, it can be used to generate spanning subgraphs with polylogarithmic average stretch in O(mlog O(1) n) work and polylogarithmic depth. We apply this subgraph construction to derive a parallel linear solver. By using this solver in known applications, our results imply improved parallel randomized algorithms for several problems, including single-source shortest paths, maximum flow, minimum-cost flow, and approximate maximum flow.	approximation algorithm;diagonally dominant matrix;file spanning;ibm 7030 stretch;linear system;maximum flow problem;minimum-cost flow problem;moore–penrose pseudoinverse;numerical linear algebra;parallel algorithm;polylogarithmic function;randomized algorithm;shortest path problem;solver;spanning tree;sparse matrix	Guy E. Blelloch;Anupam Gupta;Ioannis Koutis;Gary L. Miller;Richard Peng;Kanat Tangwongsan	2013	Theory of Computing Systems	10.1007/s00224-013-9444-5	combinatorics;discrete mathematics;computer science;mathematics;parallel algorithm;linear system;algorithm	Theory	20.073473598295994	30.44480898533557	109799
aec82df55ce3bf98cdb29267d1eff953fb361492	computational complexity of the interleaving distance		The interleaving distance is arguably the most prominent distance measure in topological data analysis. In this paper, we provide bounds on the computational complexity of determining the interleaving distance in several settings. We show that the interleaving distance is NP-hard to compute for persistence modules valued in the category of vector spaces. In the specific setting of multidimensional persistent homology we show that the problem is at least as hard as a matrix invertibility problem. Furthermore, this allows us to conclude that the interleaving distance of interval decomposable modules depends on the characteristic of the field. Persistence modules valued in the category of sets are also studied. As a corollary, we obtain that the isomorphism problem for Reeb graphs is graph isomorphism complete. 2012 ACM Subject Classification Theory of computation → Problems, reductions and completeness	computational complexity theory;forward error correction;graph isomorphism problem;homology (biology);np-hardness;persistence (computer science);persistent homology;theory of computation;topological data analysis	Håvard Bakke Bjerkevik;Magnus Bakke Botnan	2018		10.4230/LIPIcs.SoCG.2018.13	discrete mathematics;mathematics;isomorphism;combinatorics;graph isomorphism;vector space;computational complexity theory;topological data analysis;category of sets;interleaving;persistent homology	Theory	18.39989638772123	22.74515752007143	109973
142f40d87a79cc625d31d2888f8af5bc9a4f1176	a cost-optimal erew breadth-first algorithm for ordered trees, with applications	encodings;parallel algorithm;ordered trees;binary trees;integrated circuit design;cost optimization;traversals;time use;model of computation;erew pram;breadth first search;game playing;encoding;automated theorem proving;parallel algorithms;binary tree	Encoding the shape of a tree is a basic step in a number of algorithms in integrated circuit design, automated theorem proving, and game playing. We propose a simple cost-optimal encoding algorithm for ordered trees and show that our encoding can be used to obtain an optimal Breadth-First traversal of ordered trees. Specifically, with an n-node ordered tree as input our algorithms run in O(logn) time using O(n/logn) processors in the EREW-PRAM model of computation. We then show that the Breadth-First algorithm can be used to produce new encodings of binary and ordered trees.	algorithm;cost efficiency	Rong Lin;Stephan Olariu	1995	Parallel Algorithms Appl.	10.1080/10637199508915484	discrete mathematics;parallel computing;binary search tree;binary tree;computer science;theoretical computer science;mathematics;parallel algorithm;weight-balanced tree;algorithm	ML	15.760456153958277	29.39199870211915	110026
de301c057a7746c8cc5d955058ddea2acffeb807	worst-case comparison of valid inequalities for the tsp	metodo caso peor;polyhedral combinatorics;polyedre;desigualdad;inequality;travelling salesman problem;poliedro;inegalite;polyhedron;valid inequalities;mathematiques combinatoires;worst case analysis;problema viajante comercio;probleme commis voyageur;travelling salesman;methode cas pire;inegalite valide;worst case method;combinatorial mathematics	We consider most of the known classes of valid inequalities for the graphical travelling salesman polyhedron and compute the worst-case improvement resulting from their addition to the subtour polyhedron. For example, we show that the comb inequalities cannot improve the subtour bound by a factor greater than ~. The corresponding factor for the class of clique tree inequalities is 8, while it is 4 for the path configuration inequalities.	best, worst and average case;comb filter;graphical user interface;polyhedron;travelling salesman problem;tree decomposition	Michel X. Goemans	1995	Math. Program.	10.1007/BF01585563	mathematical optimization;combinatorics;calculus;mathematics;travelling salesman problem	Theory	22.160114890424254	25.810311290596758	110097
d42841f72d7aa6dc4de315de3feaf6aec4463fcf	the point-range tree: a data structure for indexing intervals	multi dimensional;indexation;data structure	<italic>In this paper, we propose a data structure, the Point-Range Tree (PR-Tree), specifically designed for indexing intervals. With the PR-Tree, a point data can be queried against a set of intervals to determine which of those intervals overlap the point. The PR-tree allows dynamic insertions and deletions while it maintains itself balanced. A balanced PR-Tree takes &Ogr;</italic>(log <italic>n</italic>) <italic>time for search. Insertion, deletion, and storage space have worst case requirements of &Ogr;</italic>(<italic>n</italic> log <italic>n</italic> + <italic>m</italic>), <italic>&Ogr;</italic>(<italic>n</italic> log<supscrpt>2</supscrpt> <italic>n</italic> + <italic>m</italic>), <italic>and &Ogr;</italic>(<italic>n</italic> log <italic>n</italic>), <italic>respectively, where n is the total number of intervals in the tree, and m the number of nodes visited during insertion and deletion. A modified version of the PR-Tree is also developed to minimize space usage. An additional advantage of the PR-Tree is that it can be easily extended to multi-dimensional domains.</italic>	best, worst and average case;data structure;insertion sort;range tree;requirement;space–time tradeoff	Moez Chaabouni;Soon Myoung Chung	1993		10.1145/170791.170897	segment tree;data structure;computer science;data mining;interval tree;programming language;algorithm	Theory	14.229331373029604	28.35724013125188	110431
8948e74670995c41f03d42e505d7300519a1eab1	fast approximate pcps for multidimensional bin-packing problems	bin packing problem;bin packing;algorithm performance;algorithm complexity;complejidad algoritmo;randomised algorithms;problema relleno;algorithme randomise;operations research;optimisation combinatoire;complexite algorithme;recherche operationnelle;resultado algoritmo;informatique theorique;performance algorithme;probleme remplissage;algorithme approximation;combinatorial optimization;investigacion operacional;optimizacion combinatoria;computer theory;informatica teorica	We consider approximate PCPs for multidimensional bin-pac king problems. In particular, we show how a verifier can be quickly conv inced that a set of multidimensional blocks can be packed into a small number of bins. The running time of the verifier is bounded by O(T (n)), whereT (n) is the time required to test for heaviness . We give heaviness testers that can test heaviness of an element in the domain[1; : : : ; n]d in timeO((log n)d). We also also give approximate PCPs with efficient verifiers for recursive bin packing a d multidimensional routing.	approximation algorithm;bin packing problem;karp's 21 np-complete problems;recursion;routing;set packing;time complexity	Tugkan Batu;Ronitt Rubinfeld;Patrick White	1999		10.1007/978-3-540-48413-4_25	mathematical optimization;combinatorics;bin packing problem;combinatorial optimization;computer science;mathematics;algorithm	Theory	17.430097205636805	25.73487727322022	110449
8e51a0038c0c1d08368d40dc5254c80bc9ac70ba	nearly tight approximation bounds for vertex cover on dense k-uniform k-partite hypergraphs	vertex cover;approximation algorithms;k partite k uniform hypergraphs;approximation hardness;dense hypergraphs;set cover	We establish almost tight upper and lower approximation bounds for the Vertex Cover problem on dense k-uniform k-partite hypergraphs.	approximation;vertex cover	Marek Karpinski;Richard Schmied;Claus Viehmann	2015	J. Discrete Algorithms	10.1016/j.jda.2014.12.010	mathematical optimization;combinatorics;discrete mathematics;vertex cover;computer science;mathematics;set cover problem;approximation algorithm;algorithm	Theory	23.099048843808152	21.52703350370795	110504
2f0ab8040034e128175d424616b504d22cac485c	composable and versatile privacy via truncated cdp		We propose truncated concentrated differential privacy (tCDP), a refinement of differential privacy and of concentrated differential privacy. This new definition provides robust and efficient composition guarantees, supports powerful algorithmic techniques such as privacy amplification via sub-sampling, and enables more accurate statistical analyses. In particular, we show a central task for which the new definition enables exponential accuracy improvement.	algorithm;differential privacy;exponential time hypothesis;leftover hash lemma;refinement (computing);sampling (signal processing);time complexity	Mark Bun;Cynthia Dwork;Guy N. Rothblum;Thomas Steinke	2018		10.1145/3188745.3188946	differential privacy;discrete mathematics;stability (learning theory);computer science;exponential function	Crypto	15.480271479462573	18.486407280845317	110506
9c22c71052255b0774e12ccc16f9cf16f71566a4	unshuffling a square is np-hard	np completeness;square shuffle	A shuffle of two strings is formed by interleaving the characters into a new string, keeping the characters of each string in order. A string is a square if it is a shuffle of two identical strings. There is a known polynomial time dynamic programming algorithm to determine if a given string z is the shuffle of two given strings x, y; however, it has been an open question whether there is a polynomial time algorithm to determine if a given string z is a square. We resolve this by proving that this problem is NP-complete via a many-one reduction from 3Partition.	algorithm;dynamic programming;forward error correction;many-one reduction;np-completeness;p (complexity);polynomial;string (computer science);time complexity	Samuel R. Buss;Michael Soltys	2014	J. Comput. Syst. Sci.	10.1016/j.jcss.2013.11.002	combinatorics;discrete mathematics;np-complete;computer science;mathematics;string metric;algorithm;string searching algorithm	Theory	13.757029647234342	26.555860553939244	110518
360c68aa690790053a3373ce09282cb242fac34f	efficient parallel algorithms for string editing and related problems	tratamiento datos;algoritmo paralelo;shortest path;shortest paths;edit distances;run time computers;parallel algorithm;algorithmique;algorithm complexity;algorithm analysis;cascading divide and conquer;complejidad algoritmo;spelling correction;efficiency;operacion edicion;edit distance;transformacion;data processing;parallel programming;traitement donnee;editing operation;analysis of algorithm;parallel computation;algorithme parallele;analysis of algorithms;longest common subsequence;approximate string searching;complexite algorithme;response time computers;algorithmics;algoritmica;parallel computer;editing routines computers;cost effectiveness;algorithms;68q25;operation edition;analyse algorithme;transformation;computer systems performance;divide and conquer;analisis algoritmo;grid graphs;string to string correction	The string editing problem for input strings x and y consists of transforming x into y by performing a series of weighted edit operations on x of overall minimum cost. An edit operation on x can be the deletion of a symbol from x, the insertion of a symbol in x or the substitution of a symbol of:c with another symbol. This problem has a well-known OClxlJyl) time sequential solution. Efficient PRAM parallel algorithms Cor the string editing problem are given. If m = min(lxj, Iyl) and n = max(lxl,lyl), then the CREW bound is O(logmlogn} time with G(mn/lagm) processors. The CReW bound is O(logn(loglogm)2) time with O(mn(loglogm) processors. In all algorithms, space is O(mn).	central processing unit;parallel algorithm	Alberto Apostolico;Mikhail J. Atallah;Lawrence L. Larmore;Scott McFaddin	1990	SIAM J. Comput.	10.1137/0219066	transformation;mathematical optimization;combinatorics;divide and conquer algorithms;cost-effectiveness analysis;edit distance;data processing;computer science;analysis of algorithms;theoretical computer science;longest common subsequence problem;mathematics;parallel algorithm;efficiency;shortest path problem;programming language;algorithmics;algorithm	Theory	13.552831714418367	28.89056628318605	110594
17048bf86dd355ca758d18d92d018e538f5468f3	an analysis of new south wales electronic vote counting	splay trees;amortized analysis;search trees	We re-examine the 2012 local government elections in New South Wales, Australia. The count was conducted electronically using a randomised form of the Single Transferable Vote (STV). It was already well known that randomness does make a difference to outcomes in some seats. We describe how the process could be amended to include a demonstration that the randomness was chosen fairly.  Second, and more significantly, we found an error in the official counting software, which caused a mistake in the count in the council of Griffith, where candidate Rina Mercuri narrowly missed out on a seat. We believe the software error incorrectly decreased Mercuri's winning probability to about 10%---according to our count she should have won with 91% probability.  The NSW Electoral Commission (NSWEC) corrected their code when we pointed out the error, and made their own announcement.  We have since investigated the 2016 local government election (held after correcting the error above) and found two new errors. We notified the NSWEC about these errors a few days after they posted the results.	assignment (computer science);computer;fairness measure;list of sega arcade system boards;random seed;randomness;recursive internetwork architecture (rina);software bug	Andrew Conway;Michelle L. Blom;Lee Naish;Vanessa Teague	2017		10.1145/3014812.3014837	amortized analysis;computer science;splay tree;operations research;computer security;algorithm;statistics	AI	13.495647608124006	22.817750976389817	110686
5da4659a30a9f1a00ac3dbc8fb1138799e069a23	simple, fast, and efficient natural language adaptive compression	busqueda informacion;lenguaje natural;metodo adaptativo;information retrieval;real time;langage naturel;chaine caractere;methode adaptative;delai transmission;transmission time;scenario;argumento;recherche information;relacion compresion;natural language;temps reel;cadena caracter;adaptive method;script;tiempo real;compression ratio;taux compression;plazo transmision;character string	One of the most successful natural language compression methods is word-based Huffman. However, such a two-pass semi-static compressor is not well suited to many interesting real-time transmission scenarios. A one-pass adaptive variant of Huffman exists, but it is character-oriented and rather complex. In this paper we implement word-based adaptive Huffman compression, showing that it obtains very competitive compression ratios. Then, we show how End-Tagged Dense Code, an alternative to word-based Huffman, can be turned into a faster and much simpler adaptive compression method which obtains almost the same compression ratios.	adaptive compression;algorithm;byte;code word;data rate units;huffman coding;megabyte;natural language;netbsd gzip / freebsd gzip;real-time clock;semiconductor industry;bzip2	Nieves R. Brisaboa;Antonio Fariña;Gonzalo Navarro;José R. Paramá	2004		10.1007/978-3-540-30213-1_34	data compression;transmission time;speech recognition;canonical huffman code;string;computer science;artificial intelligence;scenario;compression ratio;lossless compression;linguistics;natural language;programming language;information retrieval;algorithm;huffman coding	ML	10.854853245743922	27.590829246524866	110885
f607fc4d993c06328d1f5fb1cd7cbb79bcf6662c	overlaps help: improved bounds for group testing with interval queries	genetique;ordered set;publikationer;estrategia optima;genetica;interrogation base donnee;interrogacion base datos;bioinformatique;konferensbidrag;ensemble ordonne;genetics;optimal strategy;combinatorial problem;probleme combinatoire;problema combinatorio;artiklar;borne inferieure;rapporter;bioinformatica;strategie optimale;database query;lower bound;group testing;conjunto ordenado;cota inferior;bioinformatics	Given a finite ordered set of items and an unknown distinguished subset P of up to p positive elements, identify the items in P by asking the least number of queries of the type “does the subset Q intersect P?”, where Q is a subset of consecutive elements of {1, 2, ..., n}. This problem arises e.g. in computational biology, in a particular method for determining splice sites. We consider time-efficient algorithms where queries are arranged in a fixed number s of stages: in each stage, queries are performed in parallel. In a recent paper we devised query-optimal strategies in the special cases p=1 or s=2, subject to lower-order terms. Exploiting new ideas we are now able to provide a much neater argument that allows doubling the general lower bound for any p≥ 2 and s≥ 3. Moreover, we provide new strategies that match this new bound up to the constant of the main term. The new query scheme shows an effective use of overlapping queries within a stage. Remarkably, this contrasts with the known results for s ≤ 2 where optimal strategies were implemented by disjoint queries.		Ferdinando Cicalese;Peter Damaschke;Libertad Tansini;Sören Werth	2005		10.1007/11533719_94	combinatorics;group testing;artificial intelligence;machine learning;database;mathematics;upper and lower bounds;algorithm;statistics	HCI	15.363387225335094	25.221844508029914	111011
3cce468f5c192c89cdaa2d4a160ca53815ea533a	tree rebalancing in optimal time and space	optimisation;equilibrado;arbre recherche binaire;programme tri;optimizacion;programa ordenacion;database searching mathematical models;sort routine;arbol investigacion binaria;binary search tree;estructura datos;linear time;balancing;optimization;structure donnee;algoritmo optimo;algorithme optimal;optimal algorithm;data structure;equilibrage	A simple algorithm is given which takes an arbitrary binary search tree and rebalances it to form another of optimal shape, using time linear in the number of nodes and only a constant amount of space (beyond that used to store the initial tree). This algorithm is therefore optimal in its use of both time and space. Previous algorithms were optimal in at most one of these two measures, or were not applicable to all binary search trees. When the nodes of the tree are stored in an array, a simple addition to this algorithm results in the nodes being stored in sorted order in the initial portion of the array, again using linear time and constant space.	algorithm;array data structure;search tree;time complexity	Quentin F. Stout;Bette L. Warren	1986	Commun. ACM	10.1145/6592.6599	random binary tree;optimal binary search tree;cartesian tree;time complexity;segment tree;red–black tree;mathematical optimization;combinatorics;binary search tree;tree rotation;binary expression tree;exponential tree;data structure;computer science;order statistic tree;range tree;self-balancing binary search tree;k-d tree;k-ary tree;interval tree;mathematics;fractal tree index;search tree;programming language;tree sort;tree traversal;algorithm;avl tree	Theory	15.521162993712345	27.693625764766598	111169
035bbd7907d4d4f02bf045c54e74a462557a9b06	families with infants: speeding up algorithms for np-hard problems using fft	chromatic number;traveling salesman;fast fourier transform;np hard problem;algorithms;counting perfect matchings	Assume that a group of <i>n</i> people is going to an excursion and our task is to seat them into buses with several constraints each saying that a pair of people does not want to see each other in the same bus. This is a well-known graph coloring problem (with <i>n</i> being the number of vertices) and it can be solved in <i>O</i>*(2<sup><i>n</i></sup>) time by the inclusion-exclusion principle as shown by Björklund, Husfeldt, and Koivisto in 2009. Another approach to solve this problem in <i>O</i>*(2<sup><i>n</i></sup>) time is to use the Fast Fourier Transform (FFT). For this, given a graph <i>G</i> one constructs a polynomial <i>P<sub>G</sub></i>(<i>x</i>) of degree <i>O</i>*(2<sup><i>n</i></sup>) with the following property: <i>G</i> is <i>k</i>-colorable if and only if the coefficient of <i>x<sup>m</sup></i> (for some particular value of <i>m</i>) in the <i>k</i>-th power of <i>P</i>(<i>x</i>) is nonzero. Then, it remains to compute this coefficient using FFT.  Assume now that we have additional constraints: the group of people contains several infants and these infants should be accompanied by their relatives in a bus. We show that if the number of infants is linear, then the problem can be solved in <i>O</i>*((2 − ϵ)<sup><i>n</i></sup>) time, where ϵ is a positive constant independent of <i>n</i>. We use this approach to improve known bounds for several NP-hard problems (the traveling salesman problem, the graph coloring problem, the problem of counting perfect matchings) on graphs of bounded average degree, as well as to simplify the proofs of several known results.	algorithm;coefficient;degree (graph theory);fast fourier transform;graph coloring;np-hardness;polynomial;travelling salesman problem	Alexander Golovnev;Alexander S. Kulikov;Ivan Mihajlin	2016	ACM Trans. Algorithms	10.1145/2847419	fast fourier transform;mathematical optimization;combinatorics;discrete mathematics;computer science;np-hard;mathematics;travelling salesman problem;algorithm	Theory	23.57499617674988	21.29668720195365	111408
297b285a0efa32d65f5f830e260e5b835f7488ec	detecting sharp drops in pagerank and a simplified local partitioning algorithm	linear time	We show that whenever there is a sharp drop in the numerical rank defined by a personalized PageRank vector, the location of the drop reveals a cut with small conductance. We then show that for any cut in the graph, and for many starting vertices within that cut, an approximate personalized PageRank vector will have a sharp drop sufficient to produce a cut with conductance nearly as small as the original cut. Using this technique, we produce a nearly linear time local partitioning algorithm whose analysis is simpler than previous algorithms.	approximation algorithm;conductance (graph);numerical analysis;pagerank;personalization;time complexity	Reid Andersen;Fan Chung Graham	2007		10.1007/978-3-540-72504-6_1	time complexity;mathematical optimization;maximum cut;combinatorics;discrete mathematics;cut;computer science;mathematics;algorithm	Theory	22.364532540279672	21.360610672425373	111594
32fbf037d8f7ce2ae711909928eb3cfaa2f0bd5a	near-popular matchings in the roommates problem	popular matching;roommates problem;68r10	Our input is a graph G = (V,E) where each vertex ranks its neighbors in a strict order of preference. The problem is to compute a matching in G that captures the preferences of the vertices in a popular way. Matching M is more popular than matching M ′ if the number of vertices that prefer M to M ′ is more than those that prefer M ′ to M . The unpopularity factor of M measures by what factor any matching can be more popular than M . We show that G always admits a matching whose unpopularity factor is O(log |V |) and such a matching can be computed in linear time. In our problem the optimal matching would be a least unpopularity factor matching we show that computing such a matching is NP-hard. In fact, for any > 0, it is NP-hard to compute a matching whose unpopularity factor is at most 4/3− of the optimal. ? Supported by IMPECS (the Indo-German Max Planck Center for Computer Science). A preliminary version of this work appeared in the 19th Annual European Symposium on Algorithms (ESA) 2011, LNCS 6942, pages	algorithm;esa;graph (discrete mathematics);indo;lecture notes in computer science;matching (graph theory);np-hardness;optimal matching;time complexity;vertex (geometry)	Chien-Chung Huang;Telikepalli Kavitha	2013	SIAM J. Discrete Math.	10.1137/110852838	mathematical optimization;combinatorics;discrete mathematics;3-dimensional matching;mathematics;stable roommates problem	Theory	21.210485631713908	19.93657815264288	111666
36add2d32766fa3514adc8cdc53685dcf23f99fd	polynomially improved efficiency for fast parallel single-source lexicographic depth-first search, breadth-first search, and topological-first search	parallel algorithm;code optimization;time use;single source shortest path;data flow analysis;greedy algorithm;transitive closure;depth first search;breadth first search;divide and conquer	Although lexicographic (lex) variants of greedy algorithms are often P -complete, NC -algorithms are known for the following lex-search problems: lexicographic depth-first search (lex-dfs) for dags [12], [17], lexicographic breadth-first search (lex-bfs) for digraphs [12], [17], and lexicographic topological-first search (lex-tfs) for dags [12]. For the all-sources version of the problem for dense digraphs, the lex-dfs (lex-bfs, lex-tfs) in [12] is (within a log factor of) work-optimal with respect to the all-sources sequential solution that performs a dfs (bfs, tfs) from every vertex. By contrast, to solve the single-source lexicographic version on inputs of size n , all known NC -algorithms perform work that is at least an n factor away from the work performed by their sequential counterparts. We present parallel algorithms that solve the single-source version of these lex-search problems in O(log  2 n) time using M(n) processors on the EREW PRAM. (M(n) denotes the number of processors required to multiply two n\times n integer matrices in O(log  n) time and has O(n 2.376 ) as tightest currently known bound.) They all offer a polynomial improvement in work-efficiency over that of their corresponding best previously known and close the gap between the requirements of the best known parallel algorithms for the lex and the nonlex versions of the problems. Key to the efficiency of these algorithms is the novel idea of a lex-splitting tree and lex-conquer subgraphs of a dag G from source s . These structures provide a divide-and-conquer skeleton from which NC -algorithms for several lexicographic search problems emerge, in particular, an algorithm that places in the class NC the lex-dfs for reducible flow graphs—an interesting class of graphs which arise naturally in connection with code optimization and data flow analysis [4], [19]. A notable aspect of these algorithms is that they solve the lex-search problem instance at hand by efficiently transforming solutions of appropriate instances of (nonlex) path problems. This renders them potentially capable of transferring significant algorithmic advances—such as Driscoll et al.'s [14] single-source shortest paths algorithm and Ullman and Yannakakis' [34] transitive closure algorithm—from fundamental (nonlex) path problems to lex-search problems.	central processing unit;computational problem;data-flow analysis;dataflow;depth-first search;dijkstra's algorithm;directed acyclic graph;directed graph;greedy algorithm;interval (graph theory);lex (software);lexicographic breadth-first search;lexicographical order;lexicography;longest path problem;mathematical optimization;parallel algorithm;parallel computing;parallel random-access machine;polynomial;program optimization;rendering (computer graphics);requirement;search problem;sethi–ullman algorithm;shortest path problem;sparse matrix;surround sound;time complexity;topological sorting;transitive closure	Pilar de la Torre;Clyde P. Kruskal	2001	Theory of Computing Systems	10.1007/s00224-001-1008-4	mathematical optimization;combinatorics;breadth-first search;computer science;lexicographic breadth-first search;mathematics;programming language;algorithm	Theory	18.542503043680338	24.4076981242437	111720
101f14189ae328a3ccf2bd671e176b5bfb6593dd	approximation of partial capacitated vertex cover	vertex cover;approximate algorithm	We study the partial capacitated vertex cover problem (pcvc) in which the input consists of a graph G and a covering requirement L. Each edge e in G is associated with a demand (or load) `(e), and each vertex v is associated with a (soft) capacity c(v) and a weight w(v). A feasible solution is an assignment of edges to vertices such that the total demand of assigned edges is at least L. The weight of a solution is ∑ v α(v)w(v), where α(v) is the number of copies of v required to cover the demand of the edges that are assigned to v. The goal is to find a solution of minimum weight. We consider three variants of pcvc. In pcvc with separable demands the only requirement is that total demand of edges assigned to v is at most α(v)c(v). In pcvc with inseparable demands there is an additional requirement that if an edge is assigned to v then it must be assigned to one of its copies. The third variant is the unit demand version. We present 3-approximation algorithms for both pcvc with inseparable demands and pcvc with separable demands. We also present a 2-approximation for pcvc with unit demands. Our analyses rely on the local ratio technique and sophisticated charging schemes.	algorithm;approximation;minimum weight;separable polynomial;vertex cover	Reuven Bar-Yehuda;Guy Flysher;Julián Mestre;Dror Rawitz	2007		10.1007/978-3-540-75520-3_31	mathematical optimization;combinatorics;discrete mathematics;vertex cover;computer science;mathematics;algorithm	Theory	23.377202033954987	19.16850814654576	111942
c39fd4ead3fb944b736ef5b61abf13d060061571	on optimal preprocessing for contraction hierarchies	contraction hierarchies;shortest paths;apx hardness;planar graphs	For some graph classes, most notably real-world road networks, shortest path queries can be answered very efficiently if the graph is preprocessed into a <i>contraction hierarchy</i>. The preprocessing algorithm <i>contracts</i> nodes in some order, adding new edges (<i>shortcuts</i>) in the process. While preprocessing and query algorithm work for any contraction ordering, it is desirable to use one that produces as few shortcuts as possible.  It is known that the problem of minimizing the size (number of edges) of a given graph's contraction hierarchy is APX-hard. Also, any graph can be processed into a contraction hierarchy with at most <i>O</i>(<i>nh</i> log <i>D</i>) edges, where <i>n</i>, <i>D</i>, and <i>h</i> are the number of nodes, the diameter, and the <i>highway dimension</i> of the original graph, respectively.  In this paper we show that the <i>O</i>(<i>nh</i> log <i>D</i>) bound is tight for a wide range of parameters <i>n</i>, <i>D</i>, and <i>h</i>. We also show that planar graphs, despite having highway dimension Ω(√<i>n</i>), can be preprocessed into a graph of size <i>O</i>(<i>n</i> log <i>n</i>). Finally, we present a simpler proof of APX-hardness.	apx;algorithm;contraction hierarchies;data pre-processing;graph (discrete mathematics);greedy algorithm;heuristic (computer science);planar graph;preprocessor;shortest path problem	Nikola Milosavljevic	2012		10.1145/2442942.2442949	graph power;edge contraction;combinatorics;discrete mathematics;multiple edges;graph bandwidth;null graph;degree;multigraph;hypercube graph;cycle graph;path graph;mathematics;voltage graph;distance-hereditary graph;path;butterfly graph;graph minor;complement graph;graph operations;line graph;algorithm;strength of a graph;planar graph	Theory	21.174012307966674	23.178097328150113	112138
3ea4ad1554c7cb6e6d573e064a68895d1815bab5	stronger virtual connections in hex	virtual connection connection games hex h search;color;games color vectors monte carlo methods educational institutions data structures;vectors;data structures;games;proof number search implementation virtual connections connection games hex havannah cell to cell connection strategies computational bottleneck automated players automated solvers anshelevich h search algorithm trivial connections and rule or rule fastvc search alpha beta player wolve monte carlo tree search player mohex;monte carlo methods;search problems computer games	For connection games such as Hex or Y or Havannah, finding guaranteed cell-to-cell connection strategies can be a computational bottleneck. In automated players and solvers, sets of such virtual connections are often found with Anshelevich's H-search algorithm: initialize trivial connections, and then repeatedly apply an AND-rule (for combining connections in series) and an OR-rule (for combining connections in parallel). We present FastVC Search, a new algorithm for finding such connections. FastVC Search is more effective than H-search when finding a representative set of connections quickly is more important than finding a larger set of connections slowly. We tested FastVC Search in an alpha-beta player Wolve, a Monte Carlo tree search player MoHex, and a proof number search implementation called Solver. It does not strengthen Wolve, but it significantly strengthens MoHex and Solver.	computation;computational complexity theory;diplexer;heuristic;hex;mathematical optimization;monte carlo method;monte carlo tree search;move-to-front transform;power dividers and directional couplers;search algorithm;series and parallel circuits;solver;time complexity;veritas cluster server	Jakub Pawlewicz;Ryan B. Hayward;Philip Henderson;Broderick Arneson	2015	IEEE Transactions on Computational Intelligence and AI in Games	10.1109/TCIAIG.2014.2345398	games;simulation;data structure;computer science;artificial intelligence;theoretical computer science;machine learning;mathematics;monte carlo tree search;algorithm;statistics;monte carlo method	AI	15.810859066273917	30.809595421074604	112189
d22cfddaf2d912b13e5ff1ce8e9584c721cf40fc	faster algorithms for markov decision processes with low treewidth	linear time;low treewidth;constant treewidth;time o;faster algorithm;amortized logarithmic time;present decremental algorithm;cdot k;treewidth k;almost-sure reachability set computation;markov decision process	We consider two core algorithmic problems for probabilistic verification: the maximal end-component decomposition and the almostsure reachability set computation for Markov decision processes (MDPs). For MDPs with treewidth k, we present two improved static algorithms for both the problems that run in time O(n · k · 2) and O(m · logn · k), respectively, where n is the number of states and m is the number of edges, significantly improving the previous known O(n · k · √ n · k) bound for low treewidth. We also present decremental algorithms for both problems for MDPs with constant treewidth that run in amortized logarithmic time, which is a huge improvement over the previously known algorithms that require amortized linear time.	amortized analysis;approximation algorithm;computation;markov chain;markov decision process;maximal set;reachability;time complexity;treewidth	Krishnendu Chatterjee;Jakub Lacki	2013		10.1007/978-3-642-39799-8_36	mathematical optimization;combinatorics;discrete mathematics;mathematics	Logic	19.98352585120814	23.182706219014598	112233
92917ca8f16d312a694201108b9654dfe420e756	on the power of two-point based sampling		MIT Laboratory for Computer Science, Cambridge, Massachusetts 02139 Received March 19, 1986 The purpose of this note is to present a new sampling technique and to demon- strate some of its properties. The new technique consists of picking two elements at random, and deterministically generating (from them) a long sequence of pairwise-independent elements. The sequence is guaranteed to intersect, with high probability, any set of non-negligible density.	power of two;sampling (signal processing)	Benny Chor;Oded Goldreich	1989	J. Complexity	10.1016/0885-064X(89)90015-0	computer science;mathematics;algorithm;statistics	Theory	13.440531068799109	21.992750419461608	112347
0b3b26887d88e4942ad08265f4fbc708badddd46	layered clusters of tightness set functions	graph theory;leakproofness;optimisation;teoria grafo;cluster;optimizacion;algorithme glouton;amas;fonction serrage;weighting;forme stratifiee;ponderacion;classification;theorie graphe;monotonie;fonction liaison;grafo;hermeticidad;layered structure;graphe pondere;tightness function;particion;monotonicity;graph;graphe;image sequence;linkage function;partition;layered pattern;greedy algorithm;algoritmo gloton;secuencia imagen;optimization;mecanisme articule;monton;ponderation;monotonia;weighted graph;mecanismo articulado;clasificacion;sequence image;linkage mechanism;structured data;etancheite	A method for structural clustering is proposed involving data on subset-to-entity linkages that can be calculated with structural data such as graphs or sequences or images. The method is based on the layered structure of the problem of maximization of a set function de ned as the minimum value of linkages between a set and its elements and referred to as the tightness function. When the linkage function is monotone, the layered cluster can be easily found with a greedy type algorithm.	cluster analysis;expectation–maximization algorithm;graph (discrete mathematics);greedy algorithm;linkage (software);monotone	Boris G. Mirkin;Ilya B. Muchnik	2002	Appl. Math. Lett.	10.1016/S0893-9659(01)00109-4	partition;mathematical optimization;combinatorics;greedy algorithm;monotonic function;biological classification;data model;graph theory;weighting;mathematics;graph;algorithm;cluster	DB	21.868626704079634	29.199644602360934	112501
13a6e172d6e6dea18e7f3772227df67e77ac030b	on the np-completeness of some graph cluster measures	graph theory;cluster algorithm;optimisation;analyse amas;cluster graph;teoria grafo;subgrafo;optimizacion;relative density;usu;probleme np complet;graph clustering;grafo monton;prise decision;classification;theorie graphe;graphe amas;decision problem;cluster analysis;mathematical programming;sous graphe;computational complexity;controle qualite;analisis cluster;problema np completo;optimization;subgraph;toma decision;quality control;programmation mathematique;programacion matematica;clasificacion;np complete problem;control calidad;fitness function	Graph clustering is the problem of identifying sparsely connected dense subgraphs (clusters) in a given graph. Proposed clustering algorithms usually optimize various fitness functions that measure the quality of a cluster within the graph. Examples of such cluster measures include the conductance, the local and relative densities, and single cluster editing. We prove that the decision problems associated with the optimization tasks of finding the clusters that are optimal with respect to these fitness measures are NP-complete.	algorithm;approximation;cluster analysis;computer cluster;conductance (graph);connected component (graph theory);decision problem;fitness function;mathematical optimization;np-completeness	Jirí Síma;Satu Elisa Schaeffer	2006		10.1007/11611257_51	relative density;quality control;combinatorics;discrete mathematics;np-complete;biological classification;computer science;graph theory;decision problem;clustering coefficient;mathematics;cluster analysis;computational complexity theory;fitness function;complement graph;algorithm;strength of a graph	ML	21.739267687464896	29.111284415787793	112557
863e6ccb7e1d402fce0a09b662ddd9a29e368995	cut and count and representative sets on branch decompositions	computing science	Recently, new techniques have been introduced to speed up dynamic programming algorithms on tree decompositions for connectivity problems: the ‘Cut and Count’ method and a method called the rank-based approach, based on representative sets and Gaussian elimination. These methods respectively give randomised and deterministic algorithms that are single exponential in the treewidth, and polynomial, respectively linear in the number of vertices. In this paper, we adapt these methods to branch decompositions yielding algorithms, both randomised and deterministic, that are in many cases faster than when tree decompositions would be used. In particular, we obtain the currently fastest randomised algorithms for several problems on planar graphs. When the involved weights are O(nO(1)), we obtain faster randomised algorithms on planar graphs for Steiner Tree, Connected Dominating Set, Feedback Vertex Set and TSP, and a faster deterministic algorithm for TSP. When considering planar graphs with arbitrary real weights, we obtain faster deterministic algorithms for all four mentioned problems. 1998 ACM Subject Classification F.2.2 Nonnumerical Algorithms and Problems, G.2.2 Graph Theory, I.2.8 Problem Solving, Control Methods, and Search	connected dominating set;deterministic algorithm;dynamic programming;fastest;feedback vertex set;gaussian elimination;graph theory;planar graph;polynomial;problem solving;randomized algorithm;steiner tree problem;time complexity;treewidth	Willem J. A. Pino;Hans L. Bodlaender;Johan M. M. van Rooij	2016		10.4230/LIPIcs.IPEC.2016.27	combinatorics;discrete mathematics;mathematics;algorithm	Theory	20.25529424509602	20.792793616889416	112623
dbcbc822e536f4f3e52b76525fdfeeae15a11c25	a faster algorithm for finding edge-disjoint branchings	graph theory;algorithmique;algorithm analysis;theorie graphe;algorithmics;directed graph;graphe oriente;analyse algorithme;multigraphe	Abstract   An O(k 2 mn) algorithm is proposed for finding k edge-disjoint branchings in a directed multigraph with m edges and n vertices. With appropriate preprocessing the time bound can be reduced to O(kmn + k 3  n  2 ). Our proposed algorithm runs faster than any previously known algorithm and provides yet another constructive proof of Edmonds' Branchings Theorem.	algorithm	Po Tong;Eugene L. Lawler	1983	Inf. Process. Lett.	10.1016/0020-0190(83)90073-X	combinatorics;discrete mathematics;directed graph;graph theory;mathematics;algorithmics;algorithm	DB	20.159149684898544	27.23232585826445	112685
bc759e6f47358f294dd406c935ead61dcce86320	constant-degree graph expansions that preserve treewidth	optimisation sous contrainte;arbre graphe;graphe non oriente;constrained optimization;graph theory;maximum degree;teoria grafo;non directed graph;circuit graphe;algorithmique;sub cubic graph;subgrafo;grado grafo;temps polynomial;graph expansion;tree graph;circuito grafo;theorie graphe;constant degree;upper bound;optimizacion con restriccion;graph connectivity;graphe cubique;algorithmics;algoritmica;graph circuit;sous graphe;grafo no orientado;conectividad grafo;polynomial time;graph algorithm;algorithms;tree decomposition;treewidth;degre graphe;anchura arbol;subgraph;arbol grafo;borne superieure;largeur arborescente;computational efficiency;connectivite graphe;grafo cubico;graph degree;cota superior;cubic graph;expansion;tiempo polinomial	Many hard algorithmic problems dealing with graphs, circuits, formulas and constraints admit polynomial-time upper bounds if the underlying graph has small treewidth. The same problems often encourage reducing the maximal degree of vertices to simplify theoretical arguments or address practical concerns. Such degree reduction can be performed through a sequence of splittings of vertices, resulting in an expansion of the original graph. We observe that the treewidth of a graph may increase dramatically if the splittings are not performed carefully. In this context we address the following natural question: is it possible to reduce the maximum degree to a constant without substantially increasing the treewidth? We answer the above question affirmatively. We prove that any simple undirected graph G=(V,E) admits an expansion G′=(V′,E′) with the maximum degree ≤3 and tw(G′)≤tw(G)+1, where tw(⋅) is the treewidth of a graph. Furthermore, such an expansion will have no more than 2|E|+|V| vertices and 3|E| edges; it can be computed efficiently from a tree-decomposition of G. We also construct a family of examples for which the increase by 1 in treewidth cannot be avoided.	directed graph;graph (discrete mathematics);maximal set;polynomial;time complexity;tree decomposition;treewidth;vertex (geometry);vertex (graph theory)	Igor L. Markov;Yaoyun Shi	2009	Algorithmica	10.1007/s00453-009-9312-5	1-planar graph;outerplanar graph;time complexity;graph power;pathwidth;mathematical optimization;constrained optimization;combinatorics;discrete mathematics;topology;degree;clique-width;connectivity;graph theory;cubic graph;clique-sum;path graph;mathematics;distance-hereditary graph;geometry;tree-depth;trémaux tree;circle graph;upper and lower bounds;treewidth;partial k-tree;algorithmics;graph minor;bound graph;complement graph;tree;book embedding;algorithm;strength of a graph;tree decomposition	Theory	21.774007721164477	25.549572827166294	112868
97647126521c907f07bb6cd239c395f219f99229	a simple recursive tree oblivious ram		Oblivious RAM (ORAM) has received increasing attention in the past few years. The goal of oblivious RAM is to enable a client, that can locally store only a small (preferably constant) amount of data, to store remotely N data items, and access them while hiding the identities of the items that are being accessed. Most of the earlier ORAM constructions were based on the hierarchical data structure of Goldreich and Ostrovsky [3]. Shi et al. [9] introduced a binary tree ORAM, which is simpler and more efficient than the classical hierarchical ORAM. Gentry et al. [2] have followed them and improved the scheme. In this work, we improve these two constructions. Our scheme asymptotically outperforms all previous tree based ORAM schemes that have constant client memory, with an overhead of O(log N log logN) per operation for a O(N) storage server. Although the best known asymptotic result for ORAM is due to the hierarchical structure of Kushilevitz et al. [6] (O( log N log logN )), tree based ORAM constructions are much simpler.	binary tree;data structure;file server;hierarchical database model;oblivious ram;overhead (computing);random-access memory;recursion (computer science);server (computing)	Benny Pinkas;Tzachy Reinman	2014	IACR Cryptology ePrint Archive			Crypto	12.293547889385676	30.04451720341608	112916
aa8a9bd3f0af136f8e7e28511585c317bf7bba72	on the linfinity-norm of extreme points for crossing supermodular directed network lps		  We discuss extensions of Jain’s framework for network design [7] that go beyond undirected graphs. The main problem is approximating  a minimum cost set of directed edges that covers a crossing supermodular function. We show that iterated rounding gives a  factor 3 approximation, where factor 4 was previously known and factor 2 was conjectured. Our bound is tight for the simplest  interpretation of iterated rounding. We also show that (the simplest version of) iterated rounding has unbounded approximation  ratio when the problem is extended to mixed graphs.    	supermodular function	Harold N. Gabow	2005		10.1007/11496915_29		Theory	22.916314418021745	20.429327501073914	112991
53c9c7129e3c08d0d506528debbda71192948753	flexible identification of structural objects in nucleic acid sequences: palindromes, mirror repeats, pseudoknots and triple helices	algorithmique;nucleotides;biologia molecular;algorithmics;algoritmica;informatique theorique;pattern matching;molecular biology;triple helix;concordance forme;computer theory;biologie moleculaire;informatica teorica	This paper presents algorithms for exibly identifying structural objects in nucleic acid sequences. These objects are palindromes, mirror repeats, pseudoknots and triple helices. We further explore here the idea of a model against which the words in a sequence are compared for nding these structural objects 17]. In the present case, models are words deened over the alphabet of nucleotides that have both direct and inverse occurrences in the sequence. Moreover, errors (substitutions , deletions and insertions) are allowed between a model and its inverse occurrences. Helix stems may therefore present bulges or interior loops, and mirror repeats need not be exact. Reasonably eecient performance comes from the fact that the parts composing the structures are kept separated until the end and that ltering for valid occurrences (occurrences that may form part of such a structure) can be done in O(n) time where n is the length of the sequence. The time complexity for the searching phase (that is, before the structural parts are put together at the end) of both algorithms presented here (one for palin-dromes and mirror repeats, the other for pseudoknots and triple helices) is then O(nk(e + 1)(1 + minfdmax ?dmin + 1 + e; k e j j e g)) where n is the length of the sequence, dmax and dmin are, respectively, the maximal and minimal length of a hairpin loop, k is either the maximum length kmax of a model, is a xed length or represents the maximum value of a range of lengths, e is the maximum number of errors allowed (substitutions , deletions and insertions) and j j is the size of the alphabet of nucleotides.	algorithm;maximal set;time complexity	Marie-France Sagot;Alain Viari	1997		10.1007/3-540-63220-4_62	combinatorics;nucleotide;computer science;bioinformatics;pattern matching;mathematics;programming language;triple helix;algorithmics;algorithm	Theory	14.457215250642122	26.114147886731242	112995
7e793f454f1349c14d1910c9636e0a95ab9e16d2	edit distance with block deletions	dynamic programming;approximation algorithms;text processing;edit distance;np complete;block operations	Several variants of the edit distance problem with block deletions are considered. Polynomial time optimal algorithms are presented for the edit distance with block deletions allowing character insertions and character moves, but without block moves. We show that the edit distance with block moves and block deletions is NP-complete (Nondeterministic Polynomial time problems in which any given solution to such problem can be verified in polynomial time, and any NP problem can be converted into it in polynomial time), and that it can be reduced to the problem of non-recursive block moves and block deletions within a constant factor.	ansi escape code;algorithm;dynamic programming;edit distance;np (complexity);np-completeness;p (complexity);polynomial;recursion;time complexity	Dana Shapira;James A. Storer	2011	Algorithms	10.3390/a4010040	mathematical optimization;combinatorics;discrete mathematics;np-complete;edit distance;computer science;dynamic programming;mathematics;string-to-string correction problem;approximation algorithm;algorithm	Theory	13.821477958050568	26.82828051304493	113100
4803617c084e656b55712652b4418d457c4d46d2	new computational result on harmonious trees	discrete mathematics;hybrid algorithm	Graham and Sloane proposed in 1980 a conjecture stating that every tree has a harmonious labelling, a graph labelling closely related to additive base. Very limited results on this conjecture are known. In this paper, we proposed a computational approach to this conjecture by checking trees with limited size. With a hybrid algorithm, we are able to show that every tree with at most 31 nodes is graceful, extending the best previous result in this direction.	computation;graham scan;hybrid algorithm;utility functions on indivisible goods	Wenjie Fang	2011	CoRR		combinatorics;discrete mathematics;hybrid algorithm;mathematics;algorithm	Theory	23.769100553144142	24.87423295884587	113106
250b97ca6a30de95505437ce0b9597fbf8f479e5	optimal dynamic sequence representations	68p20;succinct data structures;68p10;strings;68p05;68p30;mathsf rank and mathsf select	We describe a data structure that supports access, rank and select queries, as well as symbol insertions and deletions, on a string S[1, n] over alphabet [1..σ] in time O(lgn/ lg lgn), which is optimal. The time is worst-case for the queries and amortized for the updates. This complexity is better than the best previous ones by a Θ(1 + lg σ/ lg lgn) factor. Our structure uses nH0(S) + O(n + σ(lg σ + lg 1+ε n)) bits, where H0(S) is the zero-order entropy of S and 0 < ε < 1 is any constant. This space redundancy over nH0(S) is also better, almost always, than that of the best previous dynamic structures, o(n lg σ)+O(σ(lg σ+lgn)). We can also handle general alphabets in optimal time, which has been an open problem in dynamic sequence representations.	amortized analysis;best, worst and average case;data structure	Gonzalo Navarro;Yakov Nekrich	2014	SIAM J. Comput.	10.1137/130908245	succinct data structure;combinatorics;discrete mathematics;computer science;theoretical computer science;mathematics	Theory	12.318141786214712	26.602738264655905	113130
fa7b35ff59b82e1043c7a4d0d43430cb51749bcd	parallel comparability graph recognition and modular decomposition	parallel algorithm;parallel random access machine;modular decomposition;time use	A parallelization of the algorithm of Golumbic for recognizing comparability graphs is proposed for the concurrent parallel random access machine (CRCW PRAM). Parallel algorithms for finding a transitive orientation and the modular decomposition of any undirected graph are deduced from an extension of the theory of Golumbic toward modular decomposition. The algorithms for recognizing and transitively orienting comparability graphs run in O(log n) time using d m processors and the modular decomposition algorithm runs in O(log n) time using n^3 processors (n, m and d respectively denote the number of vertices, the number of edges and the maximal degree of the undirected input graph).	modular decomposition	Michel Morvan;Laurent Viennot	1996		10.1007/3-540-60922-9_15	combinatorics;discrete mathematics;computer science;theoretical computer science;trivially perfect graph;comparability graph;mathematics;parallel algorithm;parallel random-access machine;modular decomposition	Vision	19.044066270993813	29.15539229567	113169
12448b873e5769e4a31b70505681148c3a9f274a	generating labeled planar graphs uniformly at random	recuento;dynamic programming;graph theory;random graph;metodo polinomial;comptage;enumeration;programacion dinamica;teoria grafo;decomposition;temps polynomial;dynamique;enumeracion;edge detection;metodo descomposicion;random generation;echantillonnage;grafo aleatorio;methode decomposition;enumeration counting;dynamic program;graphe aleatoire;algorithme deterministe;denombrement;sampling algorithm;probabilistic approach;polynomial;theorie graphe;contaje;68wxx;composante connexe;dinamica;deteccion contorno;recurrence;sampling;connected graph;polynomial time algorithm;detection contour;decomposition method;deterministic algorithms;graphe etiquete;dynamics;polynomial method;informatique theorique;enfoque probabilista;approche probabiliste;recurrencia;graphe planaire;polinomio;68r10;programmation dynamique;polynomial time;counting;labeled planar graph;labelled graph;echantillon;algorithme polynomial;05cxx;etiqueta;sample;etiquette;grafo planario;muestreo;methode polynomiale;label;connected component;polynome;49lxx;grafo marcado;graphe connexe;muestra;planar graph;computer theory;tiempo polinomial;grafo conexo;graphe marque;informatica teorica	We present an expected polynomial time algorithm to generat e a labeled planar graph uniformly at random. To generate the plan ar graphs, we derive recurrence formulas that count all such graphs with vertices and edges, based on a decomposition into 1-, 2-, and 3-connected components. For 3-connected graphs we apply a recent random generation algorithm by Scha effer and a counting formula by Mullin and Schellenberg.	algorithm;p (complexity);planar graph;polynomial;procedural generation;recurrence relation	Manuel Bodirsky;Clemens Gröpl;Mihyun Kang	2003		10.1007/3-540-45061-0_84	1-planar graph;time complexity;random regular graph;pathwidth;random graph;sampling;dynamics;combinatorics;discrete mathematics;connected component;decomposition method;edge detection;etiquette;dense graph;sample;connectivity;graph theory;hopcroft–karp algorithm;metric dimension;dynamic programming;clique-sum;mathematics;modular decomposition;decomposition;label;enumeration;chordal graph;indifference graph;counting;book embedding;algorithm;planar graph;polynomial	Theory	23.24475804730377	31.479416159464574	113418
a2644b67381bdb4ec4aa740cf8e8bc2a0a7e7bb8	distance constrained labeling on graphs with bounded neighborhood diversity		We study the complexity of a group of distance-constrained graph labeling problems when parameterized by the neighborhood diversity (nd), which is a natural graph parameter between vertex cover and clique width. Neighborhood diversity has been used to generalize and speed up FPT algorithms previously parameterized by vertex cover, as is also demonstrated by our paper. We show that the Uniform Channel Assignment problem is fixed parameter tractable when parameterized by nd and the largest weight and that every L(p1, p2, . . . , pk)-labeling problem is FPT when parameterized by nd, maximum pi and k. These results furthermore yield an FPT algorithms for L(p1, p2, . . . , pk)-labeling and Channel Assignment problems when parameterized by vertex cover size, answering an open question of Fiala et al.: Parameterized complexity of coloring problems: Treewidth versus vertex cover, generalizing their results beyond L(2, 1)-labeling, and tightening the complexity gap to clique width, where the problems are already hard for constant clique width. 1998 ACM Subject Classification G.2.2 Graph Theory	algorithm;assignment problem;clique-width;cobham's thesis;graph coloring;graph labeling;graph theory;parameterized complexity;treewidth;vertex cover	Jirí Fiala;Tomas Gavenciak;Dusan Knop;Martin Koutecký;Jan Kratochvíl	2015	CoRR		graph labeling;discrete mathematics;combinatorics;mathematics;vertex cover;parameterized complexity;clique-width;treewidth;bounded function;graph	Theory	22.618246821394948	21.84365405350505	113422
90c856c5765ff93038d1842c0bd2e5e353b9bb35	beaches of islands of tractability : algorithms for parsimony and minimum perfect phylogeny haplotyping problems	genetics;evolutionary trees;perfect phylogeny haplotyping;data structure	The problem Parsimony Haplotyping (PH) asks for the smallest set of haplotypes which can explain a given set of genotypes, and the problem Minimum Perfect Phylogeny Haplotyping (MPPH) asks for the smallest such set which also allows the haplotypes to be embedded in a perfect phylogeny evolutionary tree, a well-known biologically-motivated data structure. For PH we extend recent work of [16] by further mapping the interface between “easy” and “hard” instances, within the framework of (k, l)-bounded instances. By exploring, in the same way, the tractability frontier of MPPH we provide the first concrete, positive results for this problem, and the algorithms underpinning these results offer new insights about how MPPH might be further tackled in the future. In both PH and MPPH intriguing open problems remain.	algorithm;data structure;embedded system;occam's razor;phylogenetic tree;phylogenetics	Leo van Iersel;Judith Keijsper;Steven Kelk;Leen Stougie	2006		10.1007/11851561_8	biology;phylogenetic tree;data structure;computer science;bioinformatics;genetics;algorithm	Theory	17.945317070282726	22.08566587791867	113522
0328319c4aa6140f165e46adc2db476d126b1c8a	on the negation-limited circuit complexity of sorting and inverting k-tonic sequences	boolean circuits;secuencia binaria;logica booleana;diseno circuito;binary sequence;puerta logica;complexite calcul;sorting;inversion;circuit design;circuito logico;tria;circuit complexity;porte logique;upper bound;complejidad computacion;circuit logique;computational complexity;triage;logique booleenne;conception circuit;borne superieure;sequence binaire;boolean logic;logic circuit;logic gate;cota superior	A binary sequence x1, ..., xn is called k-tonic if it contains at most k changes between 0 and 1, i.e., there are at most k indices such that xi ≠xi+1. A sequence ¬x1, ..., ¬xn is called an inversion of x1, ..., xn. In this paper, we investigate the size of a negation-limited circuit, which is a Boolean circuit with a limited number of NOT gates, that sorts or inverts k-tonic input sequences. We show that if k = O(1) and t = O(loglogn), a k-tonic sequence of length n can be sorted by a circuit with t NOT gates whose size is O((n logn)/ 2ct) where c > 0 is some constant. This generalizes a similar upper bound for merging by Amano, Maruoka and Tarui [4], which corresponds to the case k = 2. We also show that a k-tonic sequence of length n can be inverted by a circuit with O(k logn) NOT gates whose size is O(kn) and depth is O(k log2n). This reduces the size of the negation-limited inverter of size O(n logn) by Beals, Nishino and Tanaka [6] when k = o(logn). If k = O(1), our inverter has size O(n) and depth O(log2n) and contains O(logn) NOT gates. For this case, the size and the number of NOT gates are optimal up to a constant factor.	circuit complexity;sorting	Takayuki Sato;Kazuyuki Amano;Akira Maruoka	2006		10.1007/11809678_13	arithmetic;combinatorics;discrete mathematics;logic gate;computer science;mathematics;algorithm	Theory	16.605335077059717	26.527024457569258	113632
4d4f26053511e41611601ee031f3ef8b402b1fd6	expected number of breakpoints after t random reversals in genomes with duplicate genes	metodo momento;optimisation;mathematics;combinatorics;distance method;moment method;optimizacion;comparative genomics;combinatoria;momentskattning;edit distance;combinatoire;discrete mathematics;breakpoint;permutation;natural sciences;distance measurement;method of moments estimate;informatique theorique;medicion distancia;methode moment;genome;permutacion;forvantat avstand;gene order;distancia;diskret matematik;method of moment;vandning;optimization;brytpunkt;genoma;genomic distance;matematik;evolutionart avstand;expected distance;reversal;distance;mesure de distance;computer theory;informatica teorica	In comparative genomics, one wishes to deduce the evolutionary distance between different species by studying their genomes. Using gene order information, we seek the number of times the gene order has changed between two species. One approach is to compute the method of moments estimate of this edit distance from a measure of dissimilarity called the breakpoint measure. In this paper, we extend the formulae and bounds of this estimate on gene permutations to genomes with duplicate genes. c © 2007 Elsevier B.V. All rights reserved.	breakpoint;edit distance	Niklas Eriksen	2008	Discrete Applied Mathematics	10.1016/j.dam.2007.10.023	combinatorics;edit distance;mathematics;permutation;breakpoint;comparative genomics;distance;statistics;genome	Theory	22.708671665537096	31.40667226653064	113667
d324d1a06c279de87363169458429caaa462d145	capacitated m ring star problem under diameter constrained reliability		Abstract   We add random link failures into a celebrated robust network design problem, called Capacitated  m -Ring Star Problem (CmRSP), where  m  rings should connect terminal nodes to a source node at minimum cost. The result is a novel   NP  -Hard network optimization problem, called Capacitated  m -Ring Star Problem Diameter Constrained Reliability, or CmRSP-DCR for short. The hardness of the CmRSP-DCR is formally proved. Then, we heuristically address a GRASP resolution, discuss results and trends for future work.		Gabriel Bayá;Antonio Mauttone;Franco Robledo;Pablo Romero;Gerardo Rubino	2016	Electronic Notes in Discrete Mathematics	10.1016/j.endm.2016.01.004	mathematical optimization;combinatorics;mathematics;distributed computing	Theory	23.106902599287555	18.661588376485717	113711
62e9a9e1f7226e68e918ad6b2cc02b11ec1625d0	the complexity of string partitioning	finite alphabet;suffix-free;np-completeness;equality-free;oligo design;collision-aware oligo design;string partition problem;string w;various definition;interesting restriction;important problem;gene synthesis;string partitioning;contemporary synthetic biology;prefix-free;factor-free;integer k	Given a string w over a finite alphabet Σ and an integer K, can w be partitioned into strings of length at most K, such that there are no collisions? We refer to this question as the string partition problem and show it is NP-complete for various definitions of collision and for a number of interesting restrictions including |Σ| = 2. This establishes the hardness of an important problem in contemporary synthetic biology, namely, oligo design for gene synthesis.	artificial gene synthesis;np-completeness;oligo primer analysis software;partition problem;synthetic biology;synthetic intelligence	Anne Condon;Ján Manuch;Chris Thachuk	2012	J. Discrete Algorithms	10.1007/978-3-642-31265-6_13	combinatorics;discrete mathematics;string kernel;mathematics;algorithm;string searching algorithm	Theory	15.966767747007871	22.400924713357817	113743
712a079c2cb0b80817cd0b13395e488b7c70e0f3	dual-issue scheduling with spills for binary trees	dependence graph;approximation;bounding box;binary tree	We describe an algorithm that finds a minimum cost schedule, including spili code, for a register-constrained machine that can issue up to one arithmetic operation and one memory access operation at a time, under the restrictions that the dependence graph is a full binary tree, and all operations have unit processing times. This problem is a generalization of two problems whose efficient solutions are well understood: optimal dual-issue scheduling without spills, solved by ,Bernstein, J&e and Rodeh (1987), and optimal single-issue scheduling with spill code, solved by Sethi and Ullman (1970), both assuming a binary dependence graph and a fixed number of registers. We show that the algorithm’s complexity is O(nk) where n is the number of operations to be scheduled and k is the number of spills in the schedule. The cost of an optimal schedule (i.e. its length) is p + 2k + (Al where p is the number of registers used! k is the number of spills and IAl is the number of arithmetic operations. We show that this is the cost of all schedules that are in “contiguous form” and therefore that all minimum-spill contiguous form schedules have minimum cost.	binary tree;phil bernstein;processor register;regular expression;schedule (computer science);scheduling (computing);sethi–ullman algorithm	Waleed Meleis;Edward S. Davidson	1999			random binary tree;optimal binary search tree;red–black tree;mathematical optimization;combinatorics;discrete mathematics;bounding interval hierarchy;binary search tree;tree rotation;binary expression tree;binary tree;computer science;minimum bounding box;treap;approximation;self-balancing binary search tree;k-ary tree;interval tree;mathematics;geometry;weight-balanced tree;ternary search tree;threaded binary tree;bounding volume hierarchy;tree traversal;algorithm	Theory	12.281789415369694	32.24428610557935	113789
f82c4328649bb37902b8cd6a2fc3fb54108f5dd4	a fast bit-parallel algorithm for computing the subset partial order	algorithme rapide;graph theory;distributed system;algoritmo paralelo;teoria grafo;systeme reparti;parallel algorithm;relation ordre partiel;combinatorics;algorithm analysis;time complexity;combinatoria;teoria conjunto;combinatoire;theorie ensemble;set theory;theorie graphe;parallel computation;algorithme parallele;complexite temps;calculo paralelo;sistema repartido;partial ordering;fast algorithm;relacion orden parcial;complejidad tiempo;calcul parallele;algoritmo rapido;partial order	A given collection of sets has a natural partial order induced by the subset relation. Let the size N of the collection be defined as the sum of the cardinalities of the sets that comprise it. Algorithms have recently been discovered that compute the partial order in worst-case time O(N 2 / log N) . This paper gives implementations of a variant of a previously proposed algorithm which exploit bit-parallel operations on a RAM with $ \Theta(log N) $ -bit words. One is shown to have a worst-case complexity of O(N 2 log log N / log 2 N) operations. This is the first known o(N 2 / log N) worst case or randomized expected running time for this problem.	best, worst and average case;cardinality (data modeling);parallel algorithm;random-access memory;randomized algorithm;time complexity;worst-case complexity	Paul Pritchard	1999	Algorithmica	10.1007/PL00009272	partially ordered set;combinatorics;graph theory;mathematics;algorithm	Theory	17.493469504271136	27.720266750251398	113914
a4bc24da66aa5f638f91d786bc6b39b30547f24c	on stable cutsets in claw-free graphs and planar graphs	graph theory;claw free graph;teoria grafo;grado grafo;temps polynomial;complexite calcul;probleme np complet;by product;theorie graphe;complejidad computacion;computational complexity;sous produit;informatique theorique;graphe planaire;subproducto;polynomial time;grafo linea;problema np completo;degre graphe;grafo planario;line graph;graphe ligne;planar graph;np complete problem;graph degree;computer theory;tiempo polinomial;informatica teorica	To decide whether a line graph (hence a claw-free graph) of maximum degree five admits a stable cutset has been proven to be an nP-complete problem. The same result has been known for K4-free graphs. Here we show how to decide this problem in polynomial time for (claw, K4)-free graphs and for a claw-free graph of maximum degree at most four. As a by-product we prove that the stable cutset problem is polynomially solvable for claw-free planar graphs, and for planar line graphs. now, the computational complexity of the stable cutset problem restricted to claw-free graphs and claw-free planar graphs is known for all bounds on the maximum degree.#R##N##R##N#Moreover, we prove that the stable cutset problem remains nP-complete for K4-free planar graphs of maximum degree five.	claw-free graph	Van Bang Le;Raffaele Mosca;Haiko Müller	2005		10.1007/11604686_15	strong perfect graph theorem;1-planar graph;claw-free graph;outerplanar graph;time complexity;pathwidth;combinatorics;discrete mathematics;cograph;np-complete;topology;graph product;longest path problem;dense graph;graph theory;pancyclic graph;forbidden graph characterization;metric dimension;clique-sum;mathematics;maximal independent set;modular decomposition;treewidth;computational complexity theory;partial k-tree;chordal graph;indifference graph;book embedding;line graph;algorithm;planar graph	Theory	22.00311592070832	27.171914943831542	114264
cb056c87da9ec9bc72eacc1c3931318843a13afb	an optimal rebuilding strategy for an incremental tree problem	average distance;satisfiability;numerical approximation;steiner tree problem	"""This paper is devoted to the following incremental problem. Initially, a graph and a distinguished subset of vertices, called initial group, are given. This group is connected by an initial tree. The incremental part of the input is given by an on-line sequence of vertices of the graph, not yet in the current group, revealed on-line one after one. The goal is to connect each new member to the current tree, while satisfying a quality constraint: the average distance between members in each constructed tree must be kept in a given range compared to the best possible one. Under this quality constraint, our objectives are to minimize the number of critical stages and the number of elementary changes of the sequence of constructed trees. We call """"critical"""" a stage where the inclusion of a new member implies heavy changes in the current tree. Otherwise, the new member is just added by connecting it with a (well chosen) path to the current tree. In both cases, updating a tree implies a certain number of elementary changes (that we define). We propose a strategy leading to at most O(log i) critical stages (i is the number of new members) and to at most a constant average number of elementary changes per stage. We also prove that there exists situations where Ω(log i) critical stages are necessary to any algorithm to maintain the quality constraint. Our strategy is then worst case optimal in order of magnitude for the number of critical stages criterion and induces a constant number of elementary changes in average per stage."""		Nicolas Thibault;Christian Laforest	2007	Journal of Interconnection Networks	10.1142/S0219265907001916	left-child right-sibling binary tree;segment tree;mathematical optimization;combinatorics;discrete mathematics;tree rotation;minimum degree spanning tree;steiner tree problem;computer science;range tree;gomory–hu tree;k-ary tree;interval tree;k-minimum spanning tree;mathematics;fractal tree index;search tree;tree traversal;avl tree;satisfiability	Theory	19.072050089782056	26.130527935473467	114319
22350344f606e7d947ef6db94093992b0ebafd1a	undirected st-connectivity in log-space	pac learning;phylogenetic reconstruction;connected graph;hidden markov models;evolutionary trees;space complexity	We present a deterministic, log-space algorithm that solves st-connectivity in undirected graphs. The previous bound on the space complexity of undirected st-connectivity was log4/3 obtained by Armoni, Ta-Shma, Wigderson and Zhou [9]. As undirected st-connectivity is complete for the class of problems solvable by symmetric, non-deterministic, log-space computations (the class SL), this algorithm implies that SL = L (where L is the class of problems solvable by deterministic log-space computations). Independent of our work (and using different techniques), Trifonov [45] has presented an O(log n log log n)-space, deterministic algorithm for undirected st-connectivity.Our algorithm also implies a way to construct in log-space a fixed sequence of directions that guides a deterministic walk through all of the vertices of any connected graph. Specifically, we give log-space constructible universal-traversal sequences for graphs with restricted labelling and log-space constructible universal-exploration sequences for general graphs.	computation;connectivity (graph theory);constructible function;dspace;decision problem;deterministic algorithm;graph (discrete mathematics);l (complexity);log-space reduction;sl (complexity);st-connectivity;tree traversal	Omer Reingold	2004		10.1145/1060590.1060647	mathematical optimization;combinatorics;discrete mathematics;phylogenetic tree;level structure;computer science;connectivity;mathematics;graph;dspace;probably approximately correct learning;algorithm;hidden markov model	Theory	20.397061482518026	24.91202604786448	114386
11af7606b7338331d89bc63cbc8dc733590eb8a1	approximations for aligned coloring and spillage minimization in interval and chordal graphs	interval graph;register allocation;chordal graph	We consider the problem of aligned coloring of interval and chordal graphs. These problems have substantial applications to register allocation in compilers and have recently been proven NP-Hard. We provide the first constant approximations: a 4 3 -approximation for interval graphs and a 3 2 -approximation for chordal graphs. We extend our techniques to the problem of minimizing spillage in these graph types.	approximation;compiler;graph coloring;register allocation	Douglas E. Carroll;Adam Meyerson;Brian Tagiku	2009		10.1007/978-3-642-03685-9_3	1-planar graph;outerplanar graph;block graph;pathwidth;mathematical optimization;split graph;combinatorics;discrete mathematics;interval graph;lexicographic breadth-first search;graph coloring;mathematics;distance-hereditary graph;treewidth;chordal graph;indifference graph	Theory	23.7184880636885	24.353966946517136	114610
b557c85a2b5c101900d43b2a38dbed05ce58d169	faster combinatorial algorithms for determinant and pfaffian	calcul matriciel;algorithme rapide;subtraction;dynamic programming;graph theory;grafo aciclico;metodo polinomial;programacion dinamica;probleme sac a dos;teoria grafo;combinatorics;sistema pfaff;temps polynomial;combinatorial algorithm;producto matriz;dynamic programming algorithm;matrice antisymetrique;sustraccion;summation;soustraction;methode gauss;graphe acyclique;systeme pfaff;sumacion;problema mochila;theorie graphe;division;acyclic graph;matriz antisimetrica;symmetric matrices;pfaffian;algorithm;combinatorial problem;knapsack problem;probleme combinatoire;problema combinatorio;polynomial method;gauss method;fast algorithm;determinante;graph;programmation dynamique;polynomial time;time use;determinant;gaussian elimination;matrix;matrix multiplication;metodo gauss;matrix calculus;pfaff system;produit matrice;methode polynomiale;sommation;algoritmo rapido;calculo de matrices;matrix product;antisymmetric matrix;tiempo polinomial	Computation of a determinant is a very classical problem. The related concept is a Pfaffian of a matrix defined for skew-symmetric matrices. The classical algorithm for computing the determinant is Gaussian elimination. It needs O(n 3) additions, subtractions, multiplications and divisions. The algorithms of Mahajan and Vinay compute determinant and Pfaffian in a completely non-classical and combinatorial way, by reducing these problems to summation of paths in some acyclic graphs. The attractive feature of these algorithms is that they are division-free. We present a novel algebraic view of these algorithms: a relation to a pseudo-polynomial dynamic-programming algorithm for the knapsack problem. The main phase of Mahajan-Vinay algorithm can be interpreted as a computation of an algebraic version of the knapsack problem, which is an alternative to the graph-theoretic approach used in the original algorithm. Our main results show how to implement Mahajan-Vinay algorithms without divisions, in time $\tilde{O}(n^{3.03})$ using the fast matrix multiplication algorithm.	computation;directed acyclic graph;dynamic programming;gaussian elimination;graph theory;knapsack problem;linear algebra;matrix multiplication algorithm;polynomial	Anna Urbanska	2008	Algorithmica	10.1007/s00453-008-9240-9	combinatorics;matrix multiplication;graph theory;dynamic programming;calculus;mathematics;geometry;algorithm	Theory	18.62113442582313	30.059438793003913	114625
ccab3fbfdcffef9afc8731927aae72d4aea4ac10	optimal regular expressions for permutations		The permutation language Pn consists of all words that are permutations of a fixed alphabet of size n. Using divide-and-conquer, we construct a regular expression Rn that specifies Pn. We then give explicit bounds for the length of Rn, which we find to be 4nn−(lgn)/4+Θ(1), and use these bounds to show that Rn has minimum size over all regular expressions specifying Pn.		Antonio Molina Lovett;Jeffrey Shallit	2018	CoRR			Theory	13.30907189705705	26.637531550650486	114868
057d338326c1738cd21fbf59ef30c3a5f019fd8a	on the optimality of planar and geometric approximation schemes	graph theory;tsp;time approximation;approximation theory;planar approximation scheme;planar logic problem;computer science logic polynomials history;unit disk graph;geometric approximation scheme;graph theory approximation theory;exponential tune hypothesis;approximation scheme;planar logic problem planar approximation scheme geometric approximation scheme time approximation maximum independent set tsp planar graph exponential tune hypothesis eth unit disk graph;eth;planar graph;maximum independent set	We show for several planar and geometric problems that the best known approximation schemes are essentially optimal with respect to the dependence on epsi. For example, we show that the 2<sup>O(1/epsi)</sup>ldrn time approximation schemes for planar maximum independent set and for TSP on a metric defined bv a planar graph are essentially optimal: if there is a delta>0 such that any of these problems admits a 2<sup>O((1/epsi)</sup> <sup>1-delta</sup> <sup>)</sup>n<sup>O(1)</sup> time PTAS, then the exponential tune hypothesis (ETH) fails. It is known that maximum independent set on unit disk graphs and the planar logic problems MPSAT. TMIN, TMAX admit n<sup>O(1/epsi)</sup> time approximation schemes. We show that they are optimal in the sense that if there is a delta>0 such that any of these problems admits a 2<sup>(1/epsi)</sup> <sup>O(1)</sup> n<sup>O((1/epsi)</sup> <sup>1-delta</sup> <sup>)</sup> time PTAS, then ETH fails.	approximation;epsi;independent set (graph theory);ptas reduction;planar graph;time complexity;unit disk graph	Dániel Marx	2007	48th Annual IEEE Symposium on Foundations of Computer Science (FOCS'07)	10.1109/FOCS.2007.50	mathematical optimization;unit disk graph;combinatorics;discrete mathematics;independent set;graph theory;mathematics;planar graph;approximation theory	Theory	23.028740826637947	21.675621734035143	115035
fcec35176184691627e9a0d9080eea7d3968591b	computing exact bounds on elements of an inverse interval matrix is np-hard	matrice intervalle;temps polynomial;best approximation;problema np duro;calculo automatico;computing;calcul automatique;upper bound;np hard problem;inverse matrix;matriz intervalo;probleme np difficile;matrice inverse;interval matrix;polynomial time;borne superieure;matriz inversa;cota superior;tiempo polinomial	"""For a given interval matrix, it would be valuable to have a practical method for determining the family of matrices which are inverses of its members. Since the exact family of inverse matrices can be difficult to find or to describe, effort is often applied to developing methods for determining matrix families with interval structure which """"best"""" approximate or contain it. A common approach is to seek exact bounds on individual elements. In this paper, we show that computing exact bounds is NP-hard; therefore any algorithm will have at least exponential-time worst-case computational cost unless P = NP."""	np-hardness	Gregory E. Coxson	1999	Reliable Computing	10.1023/A:1009901405160	combinatorics;computing;calculus;mathematics;algorithm	HPC	18.642394472296072	25.29565869971691	115114
7241e7e6f57ee8f0828039382d046d6230cf8a52	approximating multilinear monomial coefficients and maximum multilinear monomials in multivariate polynomials	approximate algorithm;approximation algorithms;upper bound;monomial coefficients;multivariate polynomial;polynomial time;approximation scheme;monomial testing;inapproximability;multivariate polynomials;maximum multilinear monomials;lower bound;arithmetic circuit	This paper is our third step towards developing a theory of testing monomials in multivariate polynomials and concentrates on two problems: (1) How to compute the coefficients of multilinear monomials; and (2) how to find a maximum multilinear monomial when the input is a ΠΣΠ polynomial. We first prove that the first problem is #P-hard and then devise a O(3s(n)) upper bound for this problem for any polynomial represented by an arithmetic circuit of size s(n). Later, this upper bound is improved to O(2) for ΠΣΠ polynomials. We then design fully polynomial-time randomized approximation schemes for this problem for ΠΣ polynomials. On the negative side, we prove that, even for ΠΣΠ polynomials with terms of degree ≤ 2, the first problem cannot be approximated at all for any approximation factor ≥ 1, nor ”weakly approximated” in a much relaxed setting, unless P=NP. For the second problem, we first give a polynomial time λ-approximation algorithm for ΠΣΠ polynomials with terms of degrees no more a constant λ ≥ 2. On the inapproximability side, we give a n lower bound, for any ǫ > 0, on the approximation factor for ΠΣΠ polynomials. When terms in these polynomials are constrained to degrees ≤ 2, we prove a 1.0476 lower bound, assuming P 6= NP ; and a higher 1.0604 lower bound, assuming the Unique Games Conjecture.	approximation algorithm;arithmetic circuit complexity;coefficient;hardness of approximation;monomial;p (complexity);p versus np problem;polynomial ring;polynomial-time approximation scheme;polynomial-time reduction;randomized algorithm;regular expression;sharp-p-complete;theory;time complexity;unique games conjecture	Zhixiang Chen;Bin Fu	2010		10.1007/978-3-642-17458-2_26	gegenbauer polynomials;difference polynomials;polynomial matrix;combinatorics;discrete mathematics;symmetric polynomial;discrete orthogonal polynomials;elementary symmetric polynomial;classical orthogonal polynomials;mathematics;complete homogeneous symmetric polynomial;macdonald polynomials;monomial basis;upper and lower bounds;monomial;approximation algorithm;algorithm;algebra	Theory	14.496761089681065	20.2626599655389	115161
2dd3ff45e5802361d8234ba229b4ce7897b4a026	on the compatibility of quartet trees		Phylogenetic tree reconstruction is a fundamental biological problem. Quartet trees, trees over four species, are the minimal informational unit for phylogenetic classification. While every phylogenetic tree over n species defines ( n 4 ) quartets, not every set of quartets is compatible with some phylogenetic tree. Here we focus on the compatibility of quartet sets. We provide several results addressing the question of what can be inferred about the compatibility of a set from its subsets. Most of our results use probabilistic arguments to prove the sought characteristics. In particular we show that there are quartet sets Q of size m = cn logn in which every subset of cardinality c′n/ logn is compatible, and yet no fraction of more than 1/3 + of Q is compatible. On the other hand, in contrast to the classical result stating when Q is the densest, i.e., m = ( n 4 ) and the compatibility of any set of three quartets implies full compatibility, we show that even for m = Θ (( n 4 )) there are (very) incompatible sets for which every subset of large constant cardinality is compatible. Our final result relates to the conjecture of Bandelt and Dress regarding the maximum quartet distance between trees. We provide asymptotic upper and lower bounds for this value.	carrier-to-noise ratio;phylogenesis;phylogenetic nomenclature;phylogenetic tree;phylogenetics;quartet distance	Noga Alon;Sagi Snir;Raphael Yuster	2014		10.1137/1.9781611973402.40	combinatorics;discrete mathematics;mathematics;algorithm	Theory	18.373422556996253	22.253284228126233	115190
89cc43e027e977afcbe919e40105d5e4541210b2	computational determination of (3, 11) and (4, 7) cages	minimum order;graph theory;lower bounds;heuristic methods cage;cage;backtrack search;journal article;keywords backtrack search;girth;hill climbing;regular graphs;lower bound;regular graph	A (k,g)-graph is a k-regular graph of girth g, and a (k,g)-cage is a (k,g)-graph of minimum order. We show that a (3,11)-graph of order 112 found by Balaban in 1973 is minimal and unique. We also show that the order of a (4,7)-cage is 67 and find one example. Finally, we improve the lower bounds on the orders of (3,13)-cages and (3,14)-cages to 202 and 260, respectively. The methods used were a combination of heuristic hill-climbing and an innovative backtrack search.	computation	Geoffrey Exoo;Brendan D. McKay;Wendy J. Myrvold;Jacqueline Nadon	2011	J. Discrete Algorithms	10.1016/j.jda.2010.11.001	combinatorics;discrete mathematics;moore graph;regular graph;graph theory;hill climbing;mathematics;upper and lower bounds;cage;algorithm	Theory	23.58412576779964	25.784850004575127	115223
0265a0a6ff4b78e980990744658c8879609ab5e6	efficient algorithms for universal portfolios	universal portfolios;investments;non financial applications;mathematics;data compression;competitive algorithm;efficient algorithm;best constant rebalanced portfolio;competitive algorithm universal portfolios constant rebalanced portfolio investment strategy wealth best constant rebalanced portfolio performance guarantees universal algorithm non uniform random walks non financial applications data compression language modelin;competitive algorithms;language modelin;portfolios;investment;stock markets;performance guarantees;efficient implementation;engineering profession;random walk;non uniform random walks;wealth;investment strategy;universal algorithm;best constant;data compression stock markets competitive algorithms investment;computer science;sampling methods;constant rebalanced portfolio;language model;portfolios sampling methods computer science investments mathematics laboratories data compression engineering profession	A constant rebalanced portfolio is an investment strategy that keeps the same distribution of wealth among a set of stocks from day to day. There has been much work on Cover’s Universal algorithm, which is competitive with the best constant rebalanced portfolio determined in hindsight (Cover, 1991, Helmbold et al, 1998, Blum and Kalai, 1999, Foster and Vohra, 1999, Vovk, 1998, Cover and Ordentlich, 1996a, Cover, 1996c). While this algorithm has good performance guarantees, all known implementations are exponential in the number of stocks, restricting the number of stocks used in experiments (Helmbold et al, 1998, Cover and Ordentlich, 1996a, Ordentlich and Cover, 1996b, Cover, 1996c, Blum and Kalai, 1999). We present an efficient implementation of the Universal algorithm that is based on non-uniform random walks that are rapidly mixing (Applegate and Kannan, 1991, Lovasz and Simonovits, 1992, Frieze and Kannan, 1999). This same implementation also works for non-financial applications of the Universal algorithm, such as data compression (Cover, 1996c) and language modeling (Chen et al, 1999).	algorithm;approximation;blum axioms;covering space;data compression;entity–relationship model;experiment;language model;money;polynomial;randomized algorithm;time complexity;word lists by frequency	Adam Tauman Kalai;Santosh Vempala	2000		10.1109/SFCS.2000.892136	actuarial science;investment;computer science;mathematics;language model	Theory	11.475786625318422	20.570825018267282	115232
01e22a4e1fb4b9320f4cb4510877a1e04ef0bf2a	making a tournament k-arc-strong by reversing or deorienting arcs	torneo;optimisation;deorienting arcs;tournament;optimizacion;algorithme combinatoire;informatique theorique;directed graph;tournoi;graphe oriente;grafo orientado;optimization;connectivity;semicomplete digraph;arc reversal flows;computer theory;informatica teorica	We prove that every tournament T = (V; A) on n¿ 2k + 1 vertices can be made k-arc-strong by reversing no more than k(k + 1)=2 arcs. This is best possible as the transitive tournament needs this many arcs to be reversed. We show that the number of arcs we need to reverse in order to make a tournament k-arc-strong is closely related to the number of arcs we need to reverse just to achieve inand out-degree at least k. We also consider, for general digraphs, the operation of deorienting an arc which is not part of a 2-cycle. That is we replace an arc xy such that yx is not an arc by the 2-cycle xyx. We prove that for every tournament T on at least 2k + 1 vertices, the number of arcs we need to reverse in order to obtain a k-arc-strong tournament from T is equal to the number of arcs one needs to deorient in order to obtain a k-arc-strong digraph from T . Finally, we discuss the relations of our results to related problems and conjectures. ? 2003 Elsevier B.V. All rights reserved.	directed graph;reversing: secrets of reverse engineering;vertex (geometry)	Jørgen Bang-Jensen;Anders Yeo	2004	Discrete Applied Mathematics	10.1016/S0166-218X(03)00438-4	combinatorics;directed graph;connectivity;mathematics;tournament;algorithm	AI	23.523616600949506	30.62453885990527	115299
06363dc9e87dec80a8aa5d4e380ef84a15e2bc2e	the square root phenomenon in planar graphs	general graph;planar problem;best algorithm;square root;various planar problem;planar graph;algorithmic side;time algorithm;n-vertex planar graph;square root phenomenon;exponential-time algorithm	Most of the classical NP-hard problems remain NP-hard when restricted to planar graphs, and only exponential-time algorithms are known for the exact solution of these planar problems. However, in many cases, the exponential-time algorithms on planar graphs are significantly faster than the algorithms for general graphs: for example, 3-Coloring can be solved in time $2^{O(sqrt{n})}$ in an n-vertex planar graph, whereas only 2O(n)-time algorithms are known for general graphs. For various planar problems, we often see a square root appearing in the running time of the best algorithms, e.g., the running time is often of the form $2^{O(sqrt{n})}$, $n^{O(sqrt{k})}$, or $2^{O(sqrt{k})}cdot n$. By now, we have a good understanding of why this square root appears. On the algorithmic side, most of these algorithms rely on the notion of treewidth and its relation to grid minors in planar graphs (but sometimes this connection is not obvious and takes some work to exploit). On the lower bound side, under a complexity assumption called Exponential Time Hypothesis (ETH), we can show that these algorithms are essentially best possible, and therefore the square root has to appear in the running time.	planar graph	Dániel Marx	2013		10.1007/978-3-642-38756-2_1	mathematical optimization;combinatorics;discrete mathematics;mathematics;geometry	Theory	21.721461457656	22.308265727841153	115407
dae69413af39d890ebaa6af48dcaf0b6e1fd77ab	algebraic graph rewriting using a single pushout	graph rewriting	We show how graph rewriting can be described with a single pushout in a suitable category of graphs, and compare our result with the conventional approach which uses double pushouts.	graph rewriting	Pim van den Broek	1991		10.1007/3-540-53982-4_6	discrete mathematics;universal graph;directed graph;topology;null graph;graph property;computer science;clique-width;simplex graph;comparability graph;mathematics;voltage graph;distance-hereditary graph;graph;vertex-transitive graph;confluence;complement graph;line graph;graph rewriting	NLP	21.606851559735055	32.038287090557944	115760
001bbb0b135e6197b008b68a14515e6732153c1d	distributed processing of the transitive-closure queries	distributed processing;transitive closure	 Given two vertices u and v in a directed graph, a reachability query asks if there is a path from u to v.	directed graph;reachability;transitive closure	Ghassan Z. Qadah;Jung J. Kim	1990			transitive closure;theoretical computer science;distributed computing;computer science	DB	17.97569467815983	30.781625619166302	115853
cb7c8b463da4b0e2e0127a1d026814d97d182f5f	lower bounds on the obdd size of two fundamental functions' graphs	fonction booleenne;borne exponentielle;diagrama binaria decision;diagramme binaire decision;procesamiento informacion;storage access;algorithm analysis;fonction poids;complexite calcul;obdd;boolean function;analysis of algorithm;ordered binary decision diagram;analysis of algorithms;complejidad computacion;theory of computing;model checking;computational complexity;theory of computation;data structures;funcion booliana;informatique theorique;almacenamiento;estructura datos;information processing;acces memoire;borne inferieure;stockage;funcion peso;graph algorithm;acceso memoria;analyse algorithme;structure donnee;theorie calcul;weight function;algorithme graphe;traitement information;data structure;storage;analisis algoritmo;lower bound;cota inferior;computer theory;binary decision diagram;informatica teorica	Ordered Binary Decision Diagrams (OBDDs) are a data structure for Boolean functions which supports many useful operations. Among others it finds applications in CAD, model checking, and symbolic graph algorithms. Nevertheless, many simple functions are known to have exponential OBDD size with respect to their number of variables. In order to investigate the limits of symbolic graph algorithms which work on OBDD-represented graph instances, it is useful to have simply-structured graphs whose OBDD representation has exponential size. Therefore, we consider two fundamental functions with exponential lower bounds on their OBDD size and transfer these results to their corresponding graphs. Concretely, we consider the Indirect Storage Access function and the Hidden Weighted Bit function. © 2006 Elsevier B.V. All rights reserved.	algorithm;binary decision diagram;computer-aided design;data structure;graph theory;list of algorithms;model checking;time complexity	Daniel Sawitzki	2007	Inf. Process. Lett.	10.1016/j.ipl.2006.08.004	model checking;combinatorics;weight function;data structure;computer science;analysis of algorithms;mathematics;boolean function;upper and lower bounds;computational complexity theory;binary decision diagram;algorithm	Theory	17.199351562086047	26.495124864399404	115938
584839a208635af49938e2b94ff2bec2a0bcaec8	sampling algorithms for differential batch retrieval problems (extended abstract)	sampling algorithms;extended abstract;retrieval problems;differential batch	Without Abstract	algorithm	Dan E. Willard	1984		10.1007/3-540-13345-3_48	theoretical computer science;machine learning;information retrieval	Theory	15.929945908407207	20.698523951459716	116120
fbd647a0fbaf1663d400c87b27b883b7a7398c4c	on the complexity of deriving position specific score matrices from positive and negative sequences	matrice poids;complexite;position specific scoring matrix;temps polynomial;complejidad;biologia molecular;problema np duro;profile;complexity;weight matrix;computational molecular biology;np hard problem;probleme np difficile;informatique theorique;molecular biology;polynomial time;motif sequence;sequence motif;np hard;matrice score position specifique;profil;position specific score matrix;computer theory;biologie moleculaire;tiempo polinomial;informatica teorica	Position-specific score matrices (PSSMs) have been applied to various problems in computational molecular biology. In this paper, we study the following problem: given positive examples (sequences) and negative examples (sequences), find a PSSM which correctly discriminates between positive and negative examples. We prove that this problem is solved in polynomial time if the size of a PSSM is bounded by a constant. On the other hand, we prove that this problem is NP-hard if the size is not bounded. We also prove hardness results for deriving multiple PSSMs and related problems.		Tatsuya Akutsu;Hideo Bannai;Satoru Miyano;Sascha Ott	2007	Discrete Applied Mathematics	10.1016/j.dam.2004.10.011	calculus;np-hard;mathematics;algorithm	Vision	16.328707640428142	23.890177609005296	116380
c62a3d209a5da38c9c60dfb21a040022a21961a9	separating clique tree and bipartition inequalities in polynominal time		Abst rac t . Many important cutting planes have been discovered for the traveling salesman problem. Among them are the clique tree inequalities and the more general bipartition inequalities. Little is known in the way of exact algorithms for separating these inequalities in polynomial time in the size of the fractional point x ~ which is being separated. An algorithm is presented here that separates bipartition inequalities of a fixed number of handles and teeth, which includes clique tree inequalities of a fixed number of handles and teeth, in polynomial time.	algorithm;time complexity;travelling salesman problem;tree decomposition	Robert D. Carr	1995		10.1007/3-540-59408-6_40	mathematical optimization;combinatorics;discrete mathematics;mathematics	Theory	24.191599369739887	23.215855443707344	116417
e4453b736acf1be13295879473d12f11f106668e	the complexity of a periodic sequence over gf(p)	complexite;algoritmo busqueda;suite mathematique;algorithm analysis;algorithme recherche;complejidad;search algorithm;complexity;registro dispersion;analyse algorithme;sequence;registre decalage;shift register;analisis algoritmo;sucesion matematica	Abstract   Some algorithms for finding the complexity and the connection polynomial of the shortest linear feedback shift register that generates a periodic sequence over GF( p ) are proposed.		Shenquan Xie	1988	Discrete Applied Mathematics	10.1016/0166-218X(88)90035-2	combinatorics;complexity;calculus;sequence;mathematics;shift register;algorithm;search algorithm	Vision	17.06851241053387	26.129029599281257	116444
b569a9906d86c554eb80d669912cbbd90830abf3	a faster fptas for the subset-sums ratio problem		The Subset-Sums Ratio problem (SSR) is an optimization problem in which, given a set of integers, the goal is to find two subsets such that the ratio of their sums is as close to 1 as possible. In this paper we develop a new FPTAS for the SSR problem which builds on techniques proposed in [D. Nanongkai, Simple FPTAS for the subset-sums ratio problem, Inf. Proc. Lett. 113 (2013)]. One of the key improvements of our scheme is the use of a dynamic programming table in which one dimension represents the difference of the sums of the two subsets. This idea, together with a careful choice of a scaling parameter, yields an FPTAS that is several orders of magnitude faster than the best currently known scheme of [C. Bazgan, M. Santha, Z. Tuza, Efficient approximation algorithms for the Subset-Sums Equality problem, J. Comp. System Sci. 64 (2) (2002)].	approximation algorithm;correctness (computer science);dynamic programming;image scaling;mathematical optimization;optimization problem;polynomial-time approximation scheme;subset sum problem	Nikolaos Melissinos;Aris Pagourtzis	2018		10.1007/978-3-319-94776-1_50	discrete mathematics;dynamic programming;combinatorial optimization;scaling;orders of magnitude (numbers);mathematics;optimization problem;integer	Theory	20.91869773797802	19.132372717296406	116539
df46647242c8a5c2734ed6411ba8241175614188	social hash partitioner: a scalable distributed hypergraph partitioner		We design and implement a distributed algorithm for balanced k-way hypergraph partitioning that minimizes fanout, a fundamental hypergraph quantity also known as the communication volume and (k − 1)-cut metric, by optimizing a novel objective called probabilistic fanout. This choice allows a simple local search heuristic to achieve comparable solution quality to the best existing hypergraph partitioners. Our algorithm is arbitrarily scalable due to a careful design that controls computational complexity, space complexity, and communication. In practice, we commonly process hypergraphs with billions of vertices and hyperedges in a few hours. We explain how the algorithm’s scalability, both in terms of hypergraph size and bucket count, is limited only by the number of machines available. We perform an extensive comparison to existing distributed hypergraph partitioners and find that our approach is able to optimize hypergraphs roughly 100 times bigger on the same set of machines. We call the resulting tool Social Hash Partitioner (SHP), and accompanying this paper, we open-source the most scalable version based on recursive bisection.	communication complexity;computation;computational complexity theory;computational science;dspace;distributed algorithm;empirical risk minimization;experiment;fan-out;graph partition;heuristic;local search (optimization);open-source software;partition problem;recursion;scalability;shapefile	Igor Kabiljo;Brian Karrer;Mayank Pundir;Sergey Pupyrev;Alon Shalita;Yaroslav Akhremtsev;Alessandro Presta	2017	PVLDB	10.14778/3137628.3137650	distributed algorithm;hypergraph;data mining;scalability;database;hash function;computer science;probabilistic logic;constraint graph;local search (optimization);computational complexity theory	DB	20.934643840087332	19.128334987198862	116765
42c43001e09e4dcc9ea37cb4bf7893978b0af680	a lower bound on the transposition diameter	05a05;mathematiques discretes;maximo;genome rearrangement;sorting;matematicas discretas;discrete mathematics;68p10;maximum;tria;permutation;transposition;05c35;triage;genome;permutacion;borne inferieure;permutations;92d15;genoma;lower bound;cota inferior;transposicion;transposition diameter	Sorting permutations by transpositions is an important and difficult problem in genome rearrangements. The transposition diameter TD(n) is the maximum transposition distance among all pairs of permutations in Sn. It was previously conjectured [H. Eriksson et al., Discrete Math., 241 (2001), pp. 289–300] that TD(n) ≤ n+1 2 . This conjecture was disproved by Elias and Hartman [IEEE/ACM Trans. Comput. Biol. Bioinform., 3 (2006), pp. 369–379] by showing TD(n) ≥ n+1 2 + 1. In this paper we improved the lower bound to TD(n) ≥ 17 33 n+ 1 33 via computation.	computation;diameter (protocol);discrete mathematics;sorting;source-to-source compiler	Linyuan Lu;Yiting Yang	2010	SIAM J. Discrete Math.	10.1137/080741860	combinatorics;mathematics;permutation;algorithm;algebra	Theory	18.089192748539666	26.489207083358668	116976
21cff89a75d7b973865f97914cf23e6f0d0bd8a7	improved linear time approximation algorithms for weighted matchings	graph theory;acoplamiento grafo;teoria grafo;approximate algorithm;approximation algorithm;theorie graphe;graph matching;couplage graphe;graphe pondere;linear time;algoritmo aproximacion;weighted graph;algorithme approximation;performance ratio	The weighted matching problem is to find a matching in a weighted graph that has maximum weight. The fastest known algorithm for this problem has running time O(nm + n log n). Many real world problems require graphs of such large size that this running time is too costly. We present a linear time approximation algorithm for the weighted matching problem with a performance ratio of 2 3 − ε. This improves the previously best performance ratio of 1 2 .	approximation algorithm;fastest;matching (graph theory);time complexity	Doratha E. Drake Vinkemeier;Stefan Hougardy	2003		10.1007/978-3-540-45198-3_2	time complexity;mathematical optimization;factor-critical graph;combinatorics;discrete mathematics;graph theory;3-dimensional matching;mathematics;blossom algorithm;approximation algorithm;matching	Theory	21.001089139519017	25.607118429057657	117355
e9ca955457f79b957da95a2c670566288fd5facb	generalizing a theorem of wilber on rotations in binary search trees to encompass unordered binary trees	rotations;wilber s theorem;binary search trees;off line lower bound;unordered binary trees	Wilber’s logarithmic lower bound, concerning off-line binary search tree access costs, is generalized to encompass binary trees that are not constrained to satisfy the search tree property. Rotation operations in this extended model can be preceded by subtree swaps. A separation between the power of processing with search trees versus unordered trees is demonstrated.	aronszajn tree;binary tree;gimp;online and offline;search tree;tree (data structure)	Michael L. Fredman	2011	Algorithmica	10.1007/s00453-011-9489-2	random binary tree;optimal binary search tree;red–black tree;mathematical optimization;combinatorics;discrete mathematics;binary search tree;binary expression tree;geometry of binary search trees;binary tree;rotation;computer science;scapegoat tree;self-balancing binary search tree;k-d tree;mathematics;uniform binary search;weight-balanced tree;ternary search tree;threaded binary tree;metric tree	Theory	16.517268513566673	27.921770434541294	117414
e59dd09be00f3904c898c22f384d3701ae09c67b	communication-efficient parallel multiway and approximate minimum cut computation	algoritmo paralelo;parallel algorithm;randomised algorithms;algorithme randomise;algorithme parallele;combinatorial problem;probleme combinatoire;problema combinatorio;informatique theorique;algoritmo optimo;algorithme optimal;optimal algorithm;algorithm design;minimum cut;computer theory;informatica teorica	We examine diierent variants of minimum cut problems on undi-rected weighted graphs on the p-processor bulk synchronous parallel BSP model of Valiant. This model and the corresponding cost measure guide algorithm designers to develop work eecient algorithms that need only very little communication. Karger and Stein have presented a recursive contraction algorithm to solve minimum cut problems. They suggest a PRAM implementation of their algorithm working in polylogarithmic time, but being not work-optimal. Typically the problem size n is much larger than the number of processors p on real-world parallel computers (p << n). For this setting we present improved BSP implementations of the algorithm of Karger and Stein. For the case of multiway cut and approximate minimum cut we reach optimal, communication eecient results. A nice eeect, beside the optimality, is that communication is eecient for a large spectrum of BSP-parameters. In the case of the minimal cut problem our results are close to optimal.	analysis of algorithms;approximation algorithm;bulk synchronous parallel;central processing unit;computation;computer;max-flow min-cut theorem;minimum cut;parallel computing;polylogarithmic function;primitive recursive function;recursion;time complexity	Friedhelm Meyer auf der Heide;Gabriel Terán Martinez	1998		10.1007/BFb0054332	algorithm design;mathematical optimization;maximum cut;combinatorics;minimum cut;computer science;mathematics;parallel algorithm;algorithm	Theory	18.303357666247223	26.83110181617474	117435
dce427aea84f7b513a8f2c728b3c1b6ea5266d97	suffix array construction in external memory using d-critical substrings	suffix array;external memory;sorting algorithm	We present a new suffix array construction algorithm that aims to build, in external memory, the suffix array for an input string of length n measured in the magnitude of tens of Giga characters over a constant or integer alphabet. The core of this algorithm is adapted from the framework of the original internal memory SA-DS algorithm that samples fixed-size d-critical substrings. This new external-memory algorithm, called EM-SA-DS, uses novel cache data structures to construct a suffix array in a sequential scanning manner with good data spatial locality: data is read from or written to disk sequentially. On the assumed external-memory model with RAM capacity Ω((nB)0.5), disk capacity O(n), and size of each I/O block B, all measured in log n-bit words, the I/O complexity of EM-SA-DS is O(n/B). This work provides a general cache-based solution that could be further exploited to develop external-memory solutions for other suffix-array-related problems, for example, computing the longest-common-prefix array, using a modern personal computer with a typical memory configuration of 4GB RAM and a single disk.	computer data storage;data structure;input/output;locality of reference;out-of-core algorithm;personal computer;principle of locality;random-access memory;substring;suffix array	Ge Nong;Wai Hong Chan;Sen Zhang;Xiao Feng Guan	2014	ACM Trans. Inf. Syst.	10.1145/2518175	generalized suffix tree;parallel computing;computer science;theoretical computer science;operating system;sorting algorithm;database;compressed suffix array;programming language;algorithm	OS	12.161476312202273	29.09750767789889	117475
52961ff506248d42543f5d751e8563601484d2e7	a polynomial restriction lemma with applications		"""A polynomial threshold function (PTF) of degree <i>d</i> is a boolean function of the form <i>f</i>=<i>sgn</i>(<i>p</i>), where <i>p</i> is a degree-<i>d</i> polynomial, and <i>sgn</i> is the sign function. The main result of the paper is an almost optimal bound on the probability that a random restriction of a PTF is not close to a constant function, where a boolean function <i>g</i> is called Î´-close to constant if, for some <i>v</i>â{1,â1}, we have <i>g</i>(<i>x</i>)=<i>v</i> for all but at most Î´ fraction of inputs. We show for every PTF <i>f</i> of degree <i>d</i>â¥ 1, and parameters 0<Î´, <i>r</i>â¤ 1/16, that  <table class=""""display dcenter""""><tr style=""""vertical-align:middle""""><td class=""""dcell""""><i>Pr</i><sub>Ïâ¼Â <i>R</i><sub><i>r</i></sub></sub>Â [<i>f</i><sub>Ï</sub>Â is not Â Î´Â -close to constant]Â â¤Â </td><td class=""""dcell"""">â</td><td class=""""dcell""""><table style=""""border:0;border-spacing:1;border-collapse:separate;"""" class=""""cellpadding0""""><tr><td class=""""hbar""""></td></tr><tr><td style=""""text-align:center;white-space:nowrap"""" ><i>r</i></td></tr></table></td><td class=""""dcell"""">Â·Â (log<i>r</i><sup>â1</sup>Â Â·Â logÎ´<sup>â1</sup>)<sup><i>O</i>(<i>d</i><sup>2</sup>)</sup>,Â Â </td></tr></table> where Ïâ¼ <i>R</i><sub><i>r</i></sub> is a random restriction leaving each variable, independently, free with probability <i>r</i>, and otherwise assigning it 1 or â1 uniformly at random. In fact, we show a more general result for random <em>block</em> restrictions: given an arbitrary partitioning of input variables into <i>m</i> blocks, a random block restriction picks a uniformly random block ââ [<i>m</i>] and assigns 1 or â1, uniformly at random, to all variable outside the chosen block â. We prove the Block Restriction Lemma saying that a PTF <i>f</i> of degree <i>d</i> becomes Î´-close to constant when hit with a random block restriction, except with probability at most <i>m</i><sup>â1/2</sup> Â· (log<i>m</i>Â· logÎ´<sup>â1</sup>)<sup><i>O</i>(<i>d</i><sup>2</sup>)</sup>. As an application of our Restriction Lemma, we prove lower bounds against constant-depth circuits with PTF gates of any degree 1â¤ <i>d</i>âª âlog<i>n</i>/loglog<i>n</i>, generalizing the recent bounds against constant-depth circuits with linear threshold gates (LTF gates) proved by Kane and Williams (<em>STOC</em>, 2016) and Chen, Santhanam, and Srinivasan (<em>CCC</em>, 2016). In particular, we show that there is an <i>n</i>-variate boolean function <i>F</i><sub><i>n</i></sub> â <i>P</i> such that every depth-2 circuit with PTF gates of degree <i>d</i>â¥ 1 that computes <i>F</i><sub><i>n</i></sub> must have at least (<i>n</i><sup>3/2+1/<i>d</i></sup>)Â· (log<i>n</i>)<sup>â<i>O</i>(<i>d</i><sup>2</sup>)</sup> wires. For constant depths greater than 2, we also show average-case lower bounds for such circuits with super-linear number of wires. These are the first super-linear bounds on the number of wires for circuits with PTF gates. We also give short proofs of the optimal-exponent average sensitivity bound for degree-<i>d</i> PTFs due to Kane (<em>Computational Complexity</em>, 2014), and the Littlewood-Offord type anticoncentration bound for degree-<i>d</i> multilinear polynomials due to Meka, Nguyen, and Vu (<em>Theory of Computing</em>, 2016). Finally, we give <em>derandomized</em> versions of our Block Restriction Lemma and Littlewood-Offord type anticoncentration bounds, using a pseudorandom generator for PTFs due to Meka and Zuckerman (<em>SICOMP</em>, 2013)."""	ac0;align (company);best, worst and average case;block cipher;boolean algebra;computation;computational complexity theory;constant function;degree (graph theory);em (typography);entity–relationship model;polynomial hierarchy;pseudorandom generator;pseudorandomness;randomized algorithm	Valentine Kabanets;Daniel M. Kane;Zhenjian Lu	2017		10.1145/3055399.3055470	combinatorics;random variate;boolean function;multilinear map;discrete mathematics;binary logarithm;pseudorandom generator;sign function;polynomial;constant function;mathematics	Theory	10.969226242224607	22.54054628200652	117599
7718020de933cf7abf7e229c7d605be01b956cc4	robust quantum algorithms computing or with epsilon-biased oracles	estimation phase;tecnologia electronica telecomunicaciones;0367l;pregunta documental;biased oracle;algorithme;upper bound;phase estimation;quantum algorithm;borne inferieure;query;algorithms;parameter estimation;estimation parametre;calcul quantique;tecnologias;grupo a;borne superieure;quantum computing;lower bound;oracle;requete;cota superior;cota inferior	This paper considers the quantum query complexity of e-biased oracles that return the correct value with probability only 1/2 + e. In particular, we show a quantum algorithm to compute N-bit OR functions with O(√N/e) queries to e-biased oracles. This improves the known upper bound of O(√N/e2) and matches the known lower bound; we answer the conjecture raised by the paper yamashita affirmatively. We also show a quantum algorithm to cope with the situation in which we have no knowledge about the value of e. This contrasts with the corresponding classical situation, where it is almost hopeless to construct a bounded error algorithm without knowing the value of e.	quantum algorithm	Tomoya Suzuki;Shigeru Yamashita;Masaki Nakanishi;Katsumasa Watanabe	2007	IEICE Transactions	10.1093/ietisy/e90-d.2.395	artificial intelligence;machine learning;mathematics;upper and lower bounds;algorithm;statistics	Theory	14.83434659710596	24.50674699492696	117664
088339afcb9e06cb1c192f6fbc816aa26b17f91a	congested clique algorithms for the minimum cut problem		We provide three different approaches to the minimum cut problem in the congested clique model of distributed computing. In this model, n nodes of the graph, each of which knows its own edges, can communicate in synchronous rounds; per round each node can send B-bits to each other node, where typically B=O(log n). At the end, each node should know its own part of the output, e.g., which side of the cut it is on. Our first algorithm is an O(1) round algorithm that finds a 1+o(1) approximation of the minimum cut. If the min-cut size is O(n^1/3 ), the algorithm finds an exact min-cut. This algorithm combines Kargeru0027s random sampling and his contraction algorithm; Nagamochi--Ibaraki--Nishizeki--Poljaku0027s k--connectivity certificates; and Ahn--Guha--McGregoru0027s algorithm for finding those certificates in the streaming model. To get an efficient implementation, we provide an algorithm that can solve simultaneously polynomially many instances of the MST problem in O(1) rounds. Our second algorithm is an O(log^3 n) round exact algorithm, based on the Karger-Stein approach. Its time complexity improves when larger messages are allowed. To implement this algorithm we present a general method to perform divide and conquer algorithms in the congested clique model. Our third algorithm is an O(log^2 n) round exact algorithm based on Kargeru0027s state of the art sequential exact min-cut algorithm, which works via tree-packing.	approximation;distributed computing;exact algorithm;maxima and minima;minimum cut;sampling (signal processing);set packing;time complexity	Mohsen Ghaffari;Krzysztof Nowicki	2018		10.1145/3212734.3212750	divide and conquer algorithms;clique;time complexity;minimum cut;approximation algorithm;sampling (statistics);binary logarithm;computer science;algorithm;exact algorithm	Theory	21.616481781640843	21.46672943660263	117700
ccfcb101ab0faf8eb96873b9c0e6a42c64676288	nonoblivious hashing	perfect hashing;o 1 probe search;dictionary problem;oblivious and nonoblivious search;upper and lower bounds;model of computation	Nonoblivious hashing, where information gathered from unsuccessful probes is used to modify subsequent probe strategy, is introduced and used to obtain the following results for static lookup on full tables:<list><item>(1) An <italic>O</italic>(1)-time worst-case scheme that uses only logarithmic additional memory, (and no memory when the domain size is linear in the table size), which improves upon previously linear space requirements. </item><item>(2) An almost sure <italic>O</italic>(1)-time probabilistic worst-case scheme, which uses no additional memory and which improves upon previously logarithmic time requirements. </item><item>(3) Enhancements to hashing: (1) and (2) are solved for multikey recors, where search can be performed under  any key  in time <italic>O</italic>(1); these schemes also permit properties, such as nearest neighbor and rank, to be determined in logarithmic time. </item></list>	best, worst and average case;cryptographic hash function;lookup table;requirement;time complexity	Amos Fiat;Moni Naor;Jeanette P. Schmidt;Alan Siegel	1992	J. ACM	10.1145/146585.146591	model of computation;combinatorics;perfect hash function;dynamic perfect hashing;computer science;theoretical computer science;machine learning;mathematics;upper and lower bounds;programming language;2-choice hashing	Theory	12.325329196987527	26.576128305787627	117711
e519a8d036e8a36f99e39aff0859f03389a8be5f	randomized selection with tripartitioning	computational complexity;smallest of;data structure	We show that several versions of Floyd and Rivest’s algorithm Select [Comm. ACM 18 (1975) 173] for finding the kth smallest of n elements require at most n + min{k, n − k} + o(n) comparisons on average, even when equal elements occur. This parallels our recent analysis of another variant due to Floyd and Rivest [Comm. ACM 18 (1975) 165–172]. Our computational results suggest that both variants perform well in practice, and may compete with other selection methods, such as Hoare’s Find or quickselect with median-of-3 pivots.	computation;hoare logic;parallels desktop for mac;quickselect;randomized algorithm	Krzysztof C. Kiwiel	2004	CoRR		combinatorics;data structure;computer science;artificial intelligence;mathematics;programming language;computational complexity theory;algorithm	Theory	13.071778572417223	22.904363768033885	118076
54d2b2dd21f21a043e239b88ffc24d70f0ca2228	contention resolution in hashing based shared memory simulations	distributed memory;distributed system;random graph;pram;systeme reparti;shared memory;exclusive read exclusive write;memoria compartida;distributed processing;simulation;machine parallele;distributed memory machine;simulacion;algorithme randomise;68q05;parallel random access machine;ordinateur optique;optical computer;hashing;sistema repartido;simulation technique;computadora optica;randomized shared memory simulations;contention resolution;randomized algorithm;erew;68q10;parallel machines;68q25;memoire repartie;traitement reparti;memoire partagee;tratamiento repartido;dmm	Contention Resolution in Hashing Based Shared Memory Simulations by Artur Czumaj (joint work with F. Meyer auf der Heide and Volker Stemann) We investigate the problem of simulating shared memory on the Distributed Memory Machine (DMM). Our approach uses multiple copies of shared memory cells distributed among the memory modules of the DMM via universal hashing. Thus the main problem is to design strategies that resolve contention in the memory modules. Developing ideas from random graphs and very fast randomized algorithms, we present new simulation techniques that enable us to improve the previously best results exponentially. Three randomized simulations are presented, all achieving better time-performance that previously known. The best of them simulates an n-processor CRCW PRAM ion an n-processor DMM with delay O(log log log n log∗ n), with high probability. We also present a general technique that can be used to turn these simulations to timeprocessor optimal ones, in the case of EREW PRAMs to be simulated. We obtain a timeprocessor optimal simulation of an (n log log log n log∗ n)-processor EREW PRAM on an nprocessor DMM with O(log log log n log∗ n) delay. We further demonstrate that the simulations obtained can not be significantly improved using our techniques. We show an Ω(log log log n/ log log log log n) lower bound on the expected delay for a class of PRAM simulations, called topological simulations, that cover all previously known simulations as well as the simulations presented in the paper. Gossiping and Broadcasting vs. Computing Boolean Functions in Processor Networks by Martin Dietzfelbinger The gossip problem for an undirected graph G is the following: Assume each node initially has a piece of information, which is atomic, but replicable. In one round a node can send all the information it has gathered so far to one of its neighbors or can receive such information from one of its neighbors. How many rounds are necessary and sufficient so that all nodes get all the information? This problem has been studied intensively over some decades and has been completely or partially solved for many graph classes. The aim of this talk is to show that results from gossiping theory are closely related to the problem of computing Boolean functions in networks of processors under certain communication modes, in particular in the problem of synchronizing the processors, which corresponds to computing the OR function, as follows: initially, each processor has one input bit; finally, all processors know the output value. At the first glance, this task seems simpler than gossiping, since no atomic messages have to be preserved and since the algorithms that are allowed are more flexible in that they may use different communication patterns for different inputs. In the talk it is shown that for certain types of communication protocols (“telegraph mode”: in one step a processor either can receive a message or send a message or do nothing, with prior knowledge whether a message will arrive or not), best algorithms for computing the OR and best gossip algorithms are the same. This result can be extended to different tasks and communication modes. For example, the problem of computing a Boolean function with the output appearing at one of the nodes is closely related to the problem of broadcasting one atomic message from one	central processing unit;computer simulation;dimm;digital molecular matter (dmm);directed graph;distributed memory;graph (discrete mathematics);hash function;information;parallel random-access machine;parameter (computer programming);random graph;randomized algorithm;shared memory;universal hashing;with high probability	Volker Stemann	1995	SIAM J. Comput.	10.1137/S009753979529564X	random graph;shared memory;parallel computing;hash function;distributed memory;computer science;theoretical computer science;distributed computing;parallel random-access machine;optical computing;randomized algorithm;algorithm	Theory	17.43622230614405	31.660630324805144	118171
12e0c453ed0fa693cac001dddb4cfdd5d6e3b31e	quantum algorithms for learning symmetric juntas via adversary bound	complexity theory;boolean functions;input variables;testing;symmetric matrices;vectors;group testing quantum query complexity computational learning theory;nonadaptive quantum complexity quantum algorithms symmetric junta learning problem adversary bound boolean function oracle access bernstein vazirani problem combinatorial group testing problem quantum query complexity optimal quantum query algorithms or function exact half function upper bound randomised complexity;quantum computing combinatorial mathematics computational complexity learning artificial intelligence;quantum query complexity;tin;computational learning theory;group testing;complexity theory testing symmetric matrices vectors boolean functions input variables tin	In this paper, we study the following variant of the junta learning problem. We are given oracle access to a Boolean function f on n variables that only depends on k variables, and, when restricted to them, equals some predefined function h. The task is to identify the variables the function depends on. This is a generalisation of the Bernstein-Vazirani problem (when h is the XOR function) and the combinatorial group testing problem (when h is the OR function). We analyse the general case using the adversary bound, and give an alternative formulation for the quantum query complexity of this problem. We construct optimal quantum query algorithms for the cases when h is the OR function (complexity is square root of k) or the exact-half function (complexity is the fourth power root of k). The first algorithm resolves an open problem from. For the case when h is the majority function, we prove an upper bound of the fourth power root of k. We obtain a quartic improvement when compared to the randomised complexity (if h is the exact-half or the majority function), and a quadratic one when compared to the non-adaptive quantum complexity (for all functions considered in the paper).	adversary (cryptography);quantum algorithm	Aleksandrs Belovs	2014		10.1109/CCC.2014.11	time complexity;circuit complexity;parameterized complexity;combinatorics;discrete mathematics;group testing;average-case complexity;ph;tin;quantum complexity theory;computer science;theoretical computer science;structural complexity theory;machine learning;worst-case complexity;complexity index;mathematics;software testing;boolean function;computational learning theory;asymptotic computational complexity;game complexity;quantum algorithm;simon's problem;symmetric matrix	Theory	10.338140798154601	22.308048817168988	118177
cd9d00ce85ba12c224b2c011d1518cdc89722379	some formulations for the group steiner tree problem	combinatorial optimization problem;chip;extended formulation;steiner tree problem	Abstract   The group Steiner tree problem consists of, given a graph  G , a collection   R   of subsets of   V  (  G  )   and a cost   c  (  e  )   for each edge of  G , find a minimum-cost subgraph that connects at least one vertex from each   R  ∈  R  . It is a generalization of the well-known Steiner tree problem that arises naturally in the design of VLSI chips. In this paper, we study a polyhedron associated with this problem and some extended formulations. We give facet defining inequalities and explore the relationship between the group Steiner tree problem and other combinatorial optimization problems.	steiner tree problem	Carlos Eduardo Ferreira;Fernando M. de Oliveira Filho	2004	Electronic Notes in Discrete Mathematics	10.1016/j.endm.2004.06.020	chip;optimization problem;mathematical optimization;combinatorics;discrete mathematics;steiner tree problem;gomory–hu tree;k-ary tree;mathematics;steiner system	Theory	24.566711231705824	23.10827926485309	118226
71d3720bb50ea4464f97de28968f6e58d456c72e	parallel sorting with limited bandwidth	algoritmo paralelo;largeur bande;pram;parallel algorithm;metodo monte carlo;memoria acceso directo;parallel sorting;largeur bande limitee;sorting;routing;limitation;routage;methode monte carlo;tria;bsp;parallel computation;algorithme parallele;permutation;random access machine;bulk synchronous parallel model;68q17;calculo paralelo;limitacion;limited bandwidth;memoire acces direct;monte carlo method;triage;anchura banda;permutacion;las vegas algorithm;logp;random access memory ram;bandwidth;bsp mdel;calcul parallele;algorithm las vegas;68w10	We study the problem of sorting on a parallel computer with limited communication bandwidth. By using the PRAM(m) model, where p processors communicate through a globally shared memory which can service m requests per unit time, we focus on the trade-off between the amount of local computation and the amount of interprocessor communication required for parallel sorting algorithms. Our main result is a lower bound of $\Omega(\frac{n \log m}{m \log n})$ on the time required to sort n numbers on the exclusive-read and queued-read variants of the PRAM(m). We also show that Leighton's Columnsort can be used to give an asymptotically matching upper bound in the case where m grows as a fractional power of n. The bounds are of a surprising form in that they have little dependence on the parameter p. This implies that attempting to distribute the workload across more processors while holding the problem size and the size of the shared memory fixed will not improve the optimal running time of sorting in this model. We also show that both the lower and the upper bounds can be adapted to bridging models that address the issue of limited communication bandwidth: the LogP model and the bulk-synchronous parallel (BSP) model. The lower bounds provide further convincing evidence that efficient parallel algorithms for sorting rely strongly on high communication bandwidth.	sorting	Micah Adler;John W. Byers;Richard M. Karp	2000	SIAM J. Comput.	10.1137/S0097539797315884	binary space partitioning;routing;parallel computing;las vegas algorithm;computer science;sorting;mathematics;distributed computing;permutation;parallel algorithm;bandwidth;algorithm;statistics;monte carlo method	Theory	11.036128994266608	31.215695647127433	118273
f9fb8cc13e31f1f41f323c41cc7348e0bc1cb82d	the list-decoding size of fourier-sparse boolean functions	list decoding;fourier sparse functions list decoding learning theory property testing;004;property testing;fourier sparse functions;learning theory	A function defined on the Boolean hypercube is k-Fourier-sparse if it has at most k nonzero Fourier coefficients. For a function f : Fn2 → R and parameters k and d, we prove a strong upper bound on the number of k -Fourier-sparse Boolean functions that disagree with f on at most d inputs. Our bound implies that the number of uniform and independent random samples needed for learning the class of k-Fourier-sparse Boolean functions on n variables exactly is at most O (n · k log k).#R##N##R##N#As an application, we prove an upper bound on the query complexity of testing Booleanity of Fourier-sparse functions. Our bound is tight up to a logarithmic factor and quadratically improves on a result due to Gur and Tamuz (Chicago J. Theor. Comput. Sci., 2013).	list decoding;sparse	Ishay Haviv;Oded Regev	2015		10.4230/LIPIcs.CCC.2015.58	list decoding;combinatorics;discrete mathematics;boolean network;boolean expression;computer science;machine learning;learning theory;property testing;complexity index;mathematics;algorithm;parity function;statistics	Theory	10.918898914788572	22.330181769786044	118298
adb267aa2a66b8839ae77615a7835f258e3d825c	on the subnet prune and regraft distance		Phylogenetic networks are rooted directed acyclic graphs that represent evolutionary relationships between species whose past includes reticulation events such as hybridisation and horizontal gene transfer. To search the space of phylogenetic networks, the popular tree rearrangement operation rooted subtree prune and regraft (rSPR) was recently generalised to phylogenetic networks. This new operation called subnet prune and regraft (SNPR) induces a metric on the space of all phylogenetic networks as well as on several widely-used network classes. In this paper, we investigate several problems that arise in the context of computing the SNPRdistance. For a phylogenetic tree T and a phylogenetic network N , we show how this distance can be computed by considering the set of trees that are embedded in N and then use this result to characterise the SNPR-distance between T and N in terms of agreement forests. Furthermore, we analyse properties of shortest SNPR-sequences between two phylogenetic networks N and N ′, and answer the question whether or not any of the classes of tree-child, reticulation-visible, or tree-based networks isometrically embeds into the class of all phylogenetic networks under SNPR.	directed acyclic graph;embedded system;phylogenetic network;phylogenetic tree;phylogenetics;prune and search;subnetwork;tree (data structure);tree rearrangement	Jonathan Klawitter;Simone Linz	2018	CoRR		subnet;phylogenetic tree;discrete mathematics;horizontal gene transfer;directed acyclic graph;combinatorics;tree (data structure);mathematics;tree rearrangement;phylogenetic network	Theory	18.350219514862708	22.543510347728922	118357
20ef9f21a1cba168712483c616be20214c7cf11d	how to construct pseudorandom and super pseudorandom permutations from one single pseudorandom function	pseudorandom permutation generator;single pseudorandom function;public function;pseudorandom function generator;open problem;pseudorandom permutations generator;important new idea;i time;super pseudorandom permutation generator;super pseudorandom function generator	In this paper we will solve two open problems concerning pseudorandom permutations generators.#R##N##R##N#1. We will see that it is possible to obtain a pseudorandom permutation generator with only three rounds of DES - like permutation and a single pseudorandom function. This will solve an open problem of [6].#R##N##R##N#2. We will see that it is possible to obtain a super pseudorandom permutation generator with a single pseudorandom function. This will solve an open problem of [5]. For this we will use only four rounds of DES - like permutation.#R##N##R##N#For example, we will see that if ζ denotes the rotation of one bit, ψ(f,f,fo ζ o f) is a pseudorandom function generator. And ψ(f,f,f,f,o ζ o f) is a super pseudorandom function generator.#R##N##R##N#Here the number of rounds used is optimal. It should be noted that here we introduce an important new idea in that we do not use a composition of f, i times, but f o ζ o f for the last round, where ζ is a fixed and public function.	pseudorandom function family;pseudorandom number generator;pseudorandomness	Jacques Patarin	1992		10.1007/3-540-47555-9_22	arithmetic;pseudorandom generators for polynomials;theoretical computer science;pseudorandom function family;pseudorandom permutation;mathematics;pseudorandom generator;pseudorandomness;pseudorandom generator theorem;algorithm	Crypto	10.799501331953692	23.863428544907503	118754
8b4b91e604432c1a56663e0c98fec94b2ea0b059	dynamic structures for top- k queries on uncertain data	score function;building block;dynamic data structure;query answering;uncertain data	In an uncertain data set S = (S, p, f) where S is the ground set consisting of n elements, p : S → [0, 1] a probability function, and f : S → R a score function, each element i ∈ S with score f(i) appears independently with probability p(i). The top-k query on S asks for the set of k elements that has the maximum probability of appearing to be the k elements with the highest scores in a random instance of S. Computing the top-k answer on a fixed S is known to be easy. In this paper, we consider the dynamic problem, that is, how to maintain the top-k query answer when S changes, including element insertion and deletions in the ground set S, changes in the probability function p and the score function f . We present a fully dynamic data structure that handles an update in O(k log k log n) time, and answers a top-j query in O(log n+j) time for any j ≤ k. The structure has O(n) size and can be constructed in O(n log k) time. As a building block of our dynamic structure, we present an algorithm for the all-top-k problem, that is, computing the top-j answers for all j = 1, . . . , k, which may be of independent interest.	algorithm;data structure;dynamic data;dynamic problem (algorithms);uncertain data	Jiang Chen;Ke Yi	2007		10.1007/978-3-540-77120-3_38	theoretical computer science;data mining;database;mathematics;score	Theory	12.806761640642097	25.089597729599763	118848
bd0f264765635e43ea584818ba634d81be89d8d5	the complexity of colouring by locally semicomplete digraphs	prueba;complexite;combinatorics;34d09;digraph;np completude;mathematiques discretes;matematicas discretas;combinatoria;np completeness;complejidad;combinatoire;discrete mathematics;digrafo;complexity;homomorphism;preuve;dicotomia;polynomial algorithm;digraph homomorphism;directed graph;dichotomie;graphe oriente;locally semicomplete digraphs;homomorphisme;algorithme polynomial;grafo orientado;dichotomy;homomorfismo;proof;digraphe	In this paper we establish a dichotomy theorem for the complexity of homomorphisms to fixed locally semicomplete digraphs. It is also shown that the same dichotomy holds for list homomorphisms. The polynomial algorithms follow from a different, shorter proof of a result by Gutjahr, Welzl and Woeginger.	algorithm;directed graph;emo welzl;polynomial	Jørgen Bang-Jensen;Gary MacGillivray;Jacobus Swarts	2010	Discrete Mathematics	10.1016/j.disc.2010.03.033	dichotomy;combinatorics;discrete mathematics;directed graph;mathematics;algebra	Theory	22.87926456062092	30.55527198339597	119031
1822618726c4beb99cf3443a806c4d0b8f3cd79b	optimal parallel encoding and decoding algorithms for trees	integrated circuit design;trees;euler tour;erew pram;encoding;parallel algorithms	Encoding the shape of a binary tree is a basic step in a number of algorithms in integrated circuit design, automated theorem proving, and game playing. We propose cost-optimal parallel algorithms to solve the binary tree encoding/decoding problem. Specifically, we encode the relevant shape information of an n-node binary tree in a 2n bitstring. Conversely, given an arbitrary 2n bitstring we reconstruct the shape of the corresponding binary tree, if such a tree exists. All our algorithms run in O(log n) time using O(n/log n) processors in the EREW-PRAM model of computation.	algorithm	Stephan Olariu;James L. Schwing;Jingyuan Zhang	1992	Int. J. Found. Comput. Sci.	10.1142/S0129054192000024	random binary tree;optimal binary search tree;cartesian tree;red–black tree;combinatorics;tree rotation;binary expression tree;truncated binary encoding;binary tree;computer science;treap;theoretical computer science;order statistic tree;range tree;self-balancing binary search tree;k-ary tree;interval tree;parallel algorithm;threaded binary tree;tree traversal;algorithm;encoding;avl tree;eulerian path;integrated circuit design	Logic	15.762860914922532	29.428577583276226	119040
1fae233ef85525004733457b72a4c3251739f11b	optimal incremental sorting		Let A be a set of size m. Obtaining the first k ≤ m elements of A in ascending order can be done in optimal O(m+k log k) time. We present an algorithm (online on k) which incrementally gives the next smallest element of the set, so that the first k elements are obtained in optimal time for any k. We also give a practical algorithm with the same complexity on average, which improves in practice the existing online algorithm. As a direct application, we use our technique to implement Kruskal’s Minimum Spanning Tree algorithm, where our solution is competitive with the best current implementations. We finally show that our technique can be applied to several other problems, such as obtaining an interval of the sorted sequence and implementing heaps.	heap (data structure);kruskal's algorithm;minimum spanning tree;online algorithm;sorting	Rodrigo Paredes;Gonzalo Navarro	2006		10.1137/1.9781611972863.16	mathematical optimization;combinatorics;mathematics;reverse-delete algorithm;algorithm	Theory	13.859140966393005	25.07120192520226	119152
d21fe8196941778b1e82d6eb4cb4bd8f207165d7	fast algorithms for the unit cost editing distance between trees	arbre graphe;algorithme rapide;algorithmique;tree graph;edit distance;distance measurement;algorithmics;algoritmica;medicion distancia;fast algorithm;arbol grafo;algoritmo rapido;mesure de distance	1. IVoblem. 2. Editing operations. 2.1 Definitions. 2.2. Mappings. 2.3. Left-to-right postorder traversal notation-the default. 3. Basic algorithm. 4. A simple improved algorithm. 4.1 Relevance-Computing for short distances first. 4.2. A simple algorithm. 5. Improving the simple algorithm--l”he second improvement. 5.1. Strategies and inspiration. 51.1. Basic strategies. 51.2. Inspiration: Landau-Vishkin algorithm. 5.1.3. Problems in applying this approach to trees. 5.1.4. Lemmas to achieve second improvement. 5.2. Monotonicity. 5.3. Proper forests and quarantined subtrees. 5.4. Traversal orderings and continuation. 5.5. Up and down. 5.6. Preprocessing. 6. Algorithms. 6.1. Encoding of distance array. 6.2. One stage of the algorithm. 7. Overall resource analysis of Algorithms 2 and 3. 8. Conclusion.	analysis of algorithms;binary search algorithm;continuation;dynamic programming;fast fourier transform;frequency-hopping spread spectrum;preprocessor;relevance;suffix tree;surround sound;time complexity;tree (data structure);tree traversal	Dennis Shasha;Kaizhong Zhang	1990	J. Algorithms	10.1016/0196-6774(90)90011-3	combinatorics;topology;edit distance;mathematics;algorithmics;tree;algorithm	Theory	15.858189895709083	28.214705970779914	119248
6bcefe0af15ecf6a48d255ef43c5f32e33d4e5dd	linear-space data structures for range minority query in arrays	minority;least frequent element;data structures;higher dimensions;linear space;data structure;range queries	We consider range queries that search for low-frequency elements (least frequent elements and $$\alpha $$ α -minorities) in arrays. An $$\alpha $$ α -minority of a query range has multiplicity no greater than an $$\alpha $$ α fraction of the elements in the range. Our data structure for the least frequent element range query problem requires $$O(n)$$ O ( n ) space, $$O(n^{3/2})$$ O ( n 3 / 2 ) preprocessing time, and $$O(\sqrt{n})$$ O ( n ) query time. A reduction from boolean matrix multiplication to this problem shows the hardness of simultaneous improvements in both preprocessing time and query time. Our data structure for the $$\alpha $$ α -minority range query problem requires $$O(n)$$ O ( n ) space, supports queries in $$O(1/\alpha )$$ O ( 1 / α ) time, and allows $$\alpha $$ α to be specified at query time.	data structure;matrix multiplication;preprocessor;range query (data structures);range query (database)	Timothy M. Chan;Stephane Durocher;Matthew Skala;Bryan T. Wilkinson	2012	Algorithmica	10.1007/s00453-014-9881-9	range query;data structure;computer science;theoretical computer science;data mining;database;mathematics;programming language;linear space	Theory	13.937898393834043	28.937390657917504	119507
11912f61d0bff94aecb080f5e9841d46d9c20424	subgraphs decomposable into two trees and k-edge-connected subgraphs	arbre graphe;graph theory;complet;teoria grafo;peso;decomposition;subgrafo;inegalite triangulaire;tree graph;desigualdad;inequality;bord;triangle inequality;arbre maximal;edge connected subgraph;inegalite;problema np duro;satisfiability;triangular inequality;theorie graphe;costo;borde;connected graph;minimo;np hard problem;completo;decomposable graph;weight;arbol maximo;edge;probleme np difficile;sous graphe;minimum;graphe decomposable;poids;68r10;edge graph;arete graphe;spanning tree;descomposicion;subgraph;arbol grafo;grafo completo;complete graph;graphe complet;sous graphe connexe par arete;graphe connexe;arista grafico;complete;cout;grafo conexo	Given a complete graph with edge weights that satisfy the triangle inequality, we bound the minimum cost of a subgraph which is the union of two spanning trees in terms of the minimum cost of a k-edge-connected subgraph, for k ≤ 4.	file spanning;social inequality	Refael Hassin;Asaf Levin	2003	Discrete Applied Mathematics	10.1016/S0166-218X(02)00194-4	combinatorics;discrete mathematics;topology;graph theory;reconstruction conjecture;triangle inequality;subgraph isomorphism problem;graph factorization;mathematics;induced subgraph isomorphism problem	Theory	24.513508763747513	30.788905585511188	119641
561ae32b211a6e82fa18c5dfef12d30374706557	complexity of ascent finding problem		An edge-ordering of a graph G = (V, E) is a one-to-one function f from E to a subset of the set of positive integers. A path P in G is called an f-ascent if f increases along the edge sequence of P . The height h(f) of f is the maximum length of an f -ascent in G. This problem was studied rather extensively from graph theoretical point of view in connection with graph invariant known as altitude of graph. In this paper, we prove that for a given edge-ordering f of a graph G the problem of determining the value of h(f) is NP-hard. In particular, the problem of deciding, whether there is an f -ascent containing all the vertices of G is NP-complete. For that reason we present an algorithm for solving this problem with average time complexity O(n2), where n is the number of vertices of G.	algorithm;graph (discrete mathematics);graph property;graph theory;np-completeness;np-hardness;one-to-one (data model);time complexity;times ascent;vertex (geometry)	Ján Katrenic;Gabriel Semanisin	2008			discrete mathematics;combinatorics;computer science;time complexity;graph property;vertex (geometry);integer;graph	Theory	23.223745710123023	23.77604703451781	119777
d56d2deec921ddf8429fbe701a7df86f00294bd1	on the value of multiple read/write streams for approximating frequency moments	frequency moments;complexity theory;communication complexity data streams frequency moments lower bounds;frequency computer science data engineering approximation algorithms costs sorting databases statistics monitoring traffic control;approximation algorithms;lower bounds;communication complexity;skeleton;data streams;approximation theory;computational modeling;computational complexity;constant factor approximation;multiple read write stream model;deterministic algorithm computing frequency moments multiple read write stream model randomized read write stream algorithm constant factor approximation;approximation methods;computational complexity approximation theory;deterministic algorithm computing;algorithm design and analysis;randomized read write stream algorithm;data models	Recently, an extension of the standard data stream model has been introduced in which an algorithm can create and manipulate multiple read/write streams in addition to its input data stream. Like the data stream model, the most important parameter for this model is the amount of internal memory used by such an algorithm. The other key parameters are the number of streams the algorithm uses and the number of passes it makes on these streams. We consider how the addition of these multiple read/write streams impacts the ability of algorithms to approximate the frequency moments of the input stream. We show that any randomized read/write stream algorithm with a fixed number of streams and a sublogarithmic number of passes that produces a constant factor approximation of the k-th frequency moment Fk of an input sequence of length of at most N from {1, . . . , N} requires space Ω(N) for any δ > 0. For comparison, it is known that with a single read-only data stream there is a randomized constant-factor approximation for Fk using Õ(N ) space and that there is a deterministic algorithm computing Fk exactly using 3 read/write streams, O(log N) passes, and O(log N) space. Therefore, although the ability to manipulate multiple read/write streams can add substantial power to the data stream model, with a sub-logarithmic number of passes this does not significantly improve the ability to approximate higher frequency moments efficiently. Our lower bounds also apply to (1 + )-approximations of Fk for ≥ 1/N .	apply;approximation algorithm;computer data storage;deterministic algorithm;randomized algorithm;read-only memory;stream (computing);stream cipher;streaming algorithm	Paul Beame;Dang-Trinh Huynh-Ngoc	2008	Electronic Colloquium on Computational Complexity (ECCC)	10.1109/FOCS.2008.52	data modeling;algorithm design;parallel computing;computer science;theoretical computer science;communication complexity;distributed computing;data stream mining;computational complexity theory;computational model;skeleton;approximation algorithm;algorithm;approximation theory	Theory	12.501917465194653	24.84841413167032	119797
1034fd7447a802f7cafa8d989c458579f7343a2f	approximating the crossing number of toroidal graphs	graph theory;edge width;approximate algorithm;graph drawing;crossing minimization;approximation algorithm;toroidal graph;approximation;polynomial time;toroidal grid;vlsi layout;crossing number;np complete problem	CrossingNumber is one of the most challenging algorithmic problems in topological graph theory, with applications to graph drawing and VLSI layout. No polynomial time constant approximation algorithm is known for this NP-complete problem. We prove that a natural approach to planar drawing of toroidal graphs (used already by Pach and Tóth in [20]) gives a polynomial time constant approximation algorithm for the crossing number of toroidal graphs with bounded degree. In this proof we present a new “grid” theorem on toroidal graphs.	approximation algorithm;carsten thomassen;crossing number (graph theory);cubic function;embedded system;graph drawing;iteration;lecture notes in computer science;np-completeness;planar graph;polyhedron;polynomial;springer (tank);time complexity;topological graph theory;toroidal graph	Petr Hlinený;Gelasio Salazar	2007		10.1007/978-3-540-77120-3_15	1-planar graph;outerplanar graph;lattice graph;time complexity;pathwidth;topological graph theory;mathematical optimization;combinatorics;discrete mathematics;graph embedding;np-complete;independent set;graph bandwidth;graph theory;forbidden graph characterization;approximation;toroidal graph;graph automorphism;cubic graph;graph coloring;mathematics;voltage graph;tutte polynomial;graph drawing;crossing number;approximation algorithm;book embedding;line graph;algorithm;coxeter graph;planar graph	Theory	23.924743331253694	25.394961463146267	119909
1cecf44d4235be745469edb2ce64f0209fb12278	finding hidden independent sets in interval graphs	verification;use;conjunto independiente;graph theory;metodo adaptativo;intervalo;hidden information;problem;teoria grafo;graphe intervalle;aplicacion;asymptotic optimality;interval graph;independent set;algoritmo adaptativo;battleship;technology;grafo intervalo;competitive algorithms;pregunta documental;conception;gene finding;1 dimensional;methode adaptative;probleme;theorie graphe;adaptive algorithms;intervalle;informacion;question documentaire;algorithme;algorithm;utilisation;algorithme competitif;adaptive algorithm;interval;interval graphs;ensemble independant;algorithme adaptatif;games;uso;adaptive method;68r10;technologie;diseno;query;number;design;problema;verificacion;algoritmo optimo;nombre;algorithme optimal;application;optimal algorithm;information;numero;algoritmo;tecnologia	Consider a game in a given set of intervals (and their implied interval graph G) in which the adversary chooses an independent set X in G. The goal is to discover this hidden independent set X by making the fewest queries of the form “Is point p covered by an interval in X?” Our interest in this problem stems from two applications: experimental gene discovery with PCR technology and the game of Battleship (in a 1-dimensional setting). We provide adaptive algorithms for both the verification scenario (given an independent set, is it X?) and the discovery scenario (find X without any information). Under some assumptions, these algorithms use an asymptotically optimal number of queries in every instance.	adaptive algorithm;adversary (cryptography);asymptotically optimal algorithm;battleship;best, worst and average case;bioinformatics;computation;gene prediction;independent set (graph theory);information theory;time complexity;verification and validation	Therese C. Biedl;Brona Brejová;Erik D. Demaine;Angèle M. Hamel;Alejandro López-Ortiz;Tomás Vinar	2004	Theor. Comput. Sci.	10.1016/S0304-3975(03)00422-5	interval;games;design;combinatorics;verification;interval graph;independent set;information;computer science;graph theory;mathematics;algorithm;technology	Theory	16.67734899826575	25.0393598401012	119982
3ddac15bd47bc0745db4297d30be71af43adf0bb	greed is good: approximating independent sets in sparse and bounded-degree graphs	search problem;graph theory;algoritmo paralelo;performance guarantee;approximation algorithme;independent set problem;teoria grafo;parallel algorithm;approximate algorithm;complexite calcul;independent set;independence number;aproximacion;problema investigacion;theorie graphe;algorithme parallele;approximation;theorem proving;algorithme;grafo;algorithm;demonstration theoreme;complejidad computacion;programacion lineal;computational complexity;parallel and distributed algorithms;graph;graphe;linear programming;programmation lineaire;greedy algorithm;heuristics;demostracion teorema;algorithme approximation;performance ratio;probleme recherche;algoritmo	Theminimum-degree greedy algorithm, or Greedy for short, is a simple and well-studied method for finding independent sets in graphs. We show that it achieves a performance ratio of (Δ+2)/3 for approximating independent sets in graphs with degree bounded by Δ. The analysis yields a precise characterization of the size of the independent sets found by the algorithm as a function of the independence number, as well as a generalization of Turán’s bound. We also analyze the algorithm when run in combination with a known preprocessing technique, and obtain an improved $$(2\bar d + 3)/5$$ performance ratio on graphs with average degree $$\bar d$$ , improving on the previous best $$(\bar d + 1)/2$$ of Hochbaum. Finally, we present an efficient parallel and distributed algorithm attaining the performance guarantees of Greedy.	distributed algorithm;graph (discrete mathematics);greedy algorithm;independent set (graph theory);preprocessor;sparse matrix	Magnús M. Halldórsson;Jaikumar Radhakrishnan	1997	Algorithmica	10.1007/BF02523693	combinatorics;independent set;computer science;linear programming;graph theory;mathematics;algorithm	Theory	19.517723510995086	26.372449270381043	120094
3082a63db51766534c119860554dddd8812ae083	uniform generation of a motzkin word	motzkin word;uniform generation	Alonso, L., Uniform generation of a Motzkin word, Theoretical Computer Science 134 (1994) 529-536. We present a simple and efficient algorithm which builds randomly and uniformly a Motzkin word of size n. The average complexity of this algorithm is O(n). The basic idea is to choose a slightly enlarged probability space where uniformity can be achieved and then to filter out the desired words without destorying uniformity.	algorithm;circuit complexity;computational complexity theory;microsoft word for mac;randomness;theoretical computer science	Laurent Alonso	1994	Theor. Comput. Sci.	10.1016/0304-3975(94)00086-7	arithmetic;combinatorics;discrete mathematics;mathematics	AI	11.098661425906386	23.824636237375174	120199
9362fe1a70d3f4ee531c107cee648eff9a0d31cd	probabilistic proximity search: fighting the curse of dimensionality in metric spaces	probabilidad error;complexite;proximity searching;erreur;metric space;approximate algorithm;espace metrique;tiempo busqueda;algoritmo busqueda;recherche proximite;high dimensionality;inegalite triangulaire;approximation algorithms;espacio metrico;information retrieval;algorithme recherche;approximation algorithm;triangle inequality;performance;complejidad;search algorithm;vector space;dimension;probabilistic algorithm;metric;curse of dimensionality;complexity;temps recherche;probabilistic approach;vecteur;costo;probabilistic algorithms;distance based indexing;vecino mas cercano;histogram;nearest neighbor searching;proximity search;proximite;proximidad;histogramme;indexing;recherche information;enfoque probabilista;approche probabiliste;indexation;proximity;indizacion;algoritmo aproximacion;tecnica;distancia;plus proche voisin;nearest neighbour;metrico;error probability;vector;nearest neighbor search;recuperacion informacion;espace vectoriel;error;rendimiento;algorithme approximation;histograma;search time;espacio vectorial;metric spaces;technique;probabilite erreur;metrique;distance;variance;variancia;cout	"""Proximity searches become very difficult on """"high dimensional"""" metric spaces, that is, those whose histogram of distances has a large mean and/or a small variance. This so-called """"curse of dimensionality"""", well known in vector spaces, is also observed in metric spaces. The search complexity grows sharply with the dimension and with the search radius. We present a general probabilistic framework applicable to any search algorithm and whose net effect is to reduce the search radius. The higher the dimension, the more effective the technique. We illustrate empirically its practical performance on a particular class of algorithms, where large improvements in the search time are obtained at the cost of a very small error probability."""	curse of dimensionality;proximity search (text)	Edgar Chávez;Gonzalo Navarro	2003	Inf. Process. Lett.	10.1016/S0020-0190(02)00344-7	convex metric space;mathematical optimization;combinatorics;proximity search;metric space;computer science;mathematics;geometry;distance;approximation algorithm;algorithm	DB	15.79633956307459	25.831930469674912	120304
8fbc0fcb09bf85dfa58b8ef1483e8b30708a5d46	on the symmetric angle-restricted nearest neighbor problem	procesamiento informacion;algorithm analysis;asymptotic optimality;monotone matrix searching;vecino mas cercano;informatique theorique;nearest neighbor;information processing;plus proche voisin;algorithms;geographic nearest neighbor;nearest neighbour;analyse algorithme;traitement information;algoritmo optimo;algorithme optimal;optimal algorithm;divide and conquer;analisis algoritmo;recherche matrice monotone;computer theory;informatica teorica	We present an O(n log n) time divide-and-conquer algorithm for solving the symmetric angle-restricted nearest neighbor (SARNN) problem for a set of n points in the plane under any Lp metric, 1 ≤ p ≤ ∞. This algorithm is asymptotically optimal (within a multiplicative constant) for any constant p ≥ 1.	nearest neighbor search	Alok Aggarwal;Youngcheul Wee	2004	Inf. Process. Lett.	10.1016/j.ipl.2004.07.003	nearest neighbor graph;combinatorics;divide and conquer algorithms;information processing;computer science;machine learning;mathematics;nearest neighbor search;k-nearest neighbors algorithm;algorithm	DB	16.688502980271746	26.183691973112314	120333
1827b4e7593b38ef06bfe07f689a46049edba6b0	quantum searching amidst uncertainty	busqueda informacion;probabilidad error;approximation asymptotique;base donnee;algoritmo busqueda;asymptotic optimality;variable aleatoire;incertidumbre;uncertainty;information retrieval;algorithme recherche;search algorithm;probability of error;interrogation base donnee;database;variable aleatoria;interrogacion base datos;base dato;quantum computation;recherche information;quantum algorithm;random variable;error probability;incertitude;asymptotic approximation;calcul quantique;algoritmo optimo;algorithme optimal;optimal algorithm;calculo cuantico;database query;probabilite erreur;aproximacion asintotica	Consider a database most of whose entries are marked but the precise fraction of marked entries is not known. What is known is that the fraction of marked entries is 1 − ǫ, where ǫ is a random variable that is uniformly distributed in the range (0, ǫ0) .The problem is to try to select a marked item from the database in a single query. If the algorithm selects a marked item, it succeeds, else if it selects an unmarked item, it makes an error. How low can we make the probability of error? The best possible classical algorithm can lower the probability of error to O ( ǫ0 ) . The best known quantum algorithms for this problem could also only lower the probability of error to O ( ǫ0 ) . Using a recently invented quantum search technique, this paper gives an algorithm that reduces the probability of error to O ( ǫ0 ) . The algorithm is asymptotically optimal.	asymptotically optimal algorithm;conditional (computer programming);quantum algorithm	Lov K. Grover	2005		10.1007/11560319_2	computer science;probability of error;calculus;mathematics;algorithm;statistics	Theory	14.950944392724704	24.73037843201164	120394
a82e30c8ba60e09ad3ae3a7e134b77561edd68ad	broadword computing and fibonacci code speed up compressed suffix arrays	succinct data structure;suffix array;indexation;lexicographic order;sequence analysis;data structure	The suffix array of a string  s  of length  n  over the alphabet  Σ  is the permutation that gives us the lexicographic order of all suffixes of  s  . This popular index can be used to solve many problems in sequence analysis. In practice, one limitation of this data structure is its size of  n  log n  bits, while the size of the text is  n  log| Σ  | bits. For this reason compressed suffix arrays (CSAs) were introduced. The size of these CSAs is asymptotically less than or equal to the text size if the text is compressible, while maintaining  O  (log  ***   n  ) access time to the elements (0  ***  ≤ 1). The goal of a good CSA implementation is to provide fast access time to the elements while using minimal space for the CSA. Both access time and space depend on the choice of a self-delimiting code for compression. We show that the Fibonacci code is superior to the Elias  ***  code for strings that are low compressible. Our second contribution are two new broadword methods that support the decoding of Fibonacci encoded numbers on 64 bit architectures. Furthermore, our experiments show that the use of known broadword methods speed up the decoding of Elias  ***  code for strings that are high compressible, like XML. Finally, we provide a new efficient C++ library for succinct data structures which includes a generic CSA for further experiments.	fibonacci coding	Simon Gog	2009		10.1007/978-3-642-02011-7_16	generalized suffix tree;succinct data structure;mathematical optimization;combinatorics;data structure;computer science;theoretical computer science;sequence analysis;lexicographical order;mathematics;compressed suffix array;programming language;algorithm	HPC	12.019303250654007	27.90891704866855	120480
5a27c1b1c7d3196798fb1fabe8d176907fc6eac7	decomposing graphs with symmetries	graph theory;morphisme;morfismo;teoria grafo;descomposicion grafo;efficient algorithm;theorie graphe;symetrie;symmetry;rewriting systems;rewrite systems;morphism;simetria;systeme reecriture;graph decomposition;decomposition graphe	While decomposing graphs in simpler items greatly helps to design more efficient algorithms, some classes of graphs can not be handled using the classical techniques. We show here that a graph having enough symmetries can be factored into simpler blocks through a standard morphism and that the inverse process may be formalized as a pullback rewriting system.	algorithm;computer science;graph (discrete mathematics);graph rewriting;linear algebra;modular decomposition;rewriting	Michel Bauderon;Frédérique Carrère	2002		10.1007/3-540-45832-8_6	combinatorics;discrete mathematics;graph theory;mathematics;symmetry;indifference graph;algorithm;morphism;algebra	Theory	21.45868598098255	31.961439624968914	120602
b1c5525b6ba15fd837da2c3b036b71520d4bd842	finding minimal perfect hash functions	technical report;hash function;computer science;graph model	A heuristic is given for finding minimal perfect hash functions without extensive searching. The procedure is to construct a set of graph (or hypergraph) models for the dictionary, then choose one of the models for use in constructing the minimal perfect hashing function. The construction of this function relies on a backtracking algorithm for numbering the vertices of the graph. Careful selection of the graph model limits the time spent searching. Good results have been obtained for dictionaries of up to 181 words. Using the same techniques, non-minimal perfect has functions have been found for sets of up to 667 words.	algorithm;backtracking;dictionary;heuristic;perfect hash function	Gary Haggard;Kevin Karplus	1986		10.1145/5600.5899	claw-free graph;hash table;double hashing;factor-critical graph;hash function;perfect hash function;dynamic perfect hashing;null graph;computer science;technical report;theoretical computer science;trivially perfect graph;distance-hereditary graph;k-independent hashing;quartic graph;complement graph;swifft	Theory	19.152713114793446	23.9596191662784	120614
dda743251afc80465e606ef878df6f3eb3b61533	on the exact cryptographic hardness of finding a nash equilibrium		The exact hardness of computing a Nash equilibrium is a fundamental open question in algorithmic game theory. This problem is complete for the complexity class PPAD. It is well known that problems in PPAD cannot be NP-complete unless NP = coNP. Therefore, a natural direction is to reduce the hardness of PPAD to the hardness of problems used in cryptography. Bitansky, Paneth, and Rosen [FOCS 2015] prove the hardness of PPAD assuming the existence of quasi-polynomially hard indistinguishability obfuscation and sub-exponentially hard one-way functions. This leaves open the possibility of basing PPAD hardness on simpler, polynomially hard, computational assumptions. We make further progress in this direction and reduce PPAD hardness directly to polynomially hard assumptions. Our first result proves hardness of PPAD assuming the existence of polynomially hard indistinguishability obfuscation (iO) and one-way permutations. While this improves upon Bitansky et al.’s work, it does not give us a reduction to simpler, polynomially hard computational assumption because constructions of iO inherently seems to require assumptions with sub-exponential hardness. In contrast, public key functional encryption is a much simpler primitive and does not suffer from this drawback. Our second result shows that PPAD hardness can be based on polynomially hard public key functional encryption and oneway permutations. Our results further demonstrate the power of polynomially hard public key functional encryption which is believed to be weaker than indistinguishability obfuscation. ∗University of California, Berkeley, sanjamg@berkeley.edu †University of California, Berkeley, omkant@berkeley.edu ‡University of California, Berkeley, akshayaram@berkeley.edu	algorithmic game theory;bsd;ciphertext indistinguishability;co-np;complexity class;computational hardness assumption;functional encryption;np (complexity);np-completeness;nash equilibrium;one-way function;ppad (complexity);permutation pattern;public-key cryptography;symposium on foundations of computer science;time complexity	Sanjam Garg;Omkant Pandey;Akshayaram Srinivasan	2015	IACR Cryptology ePrint Archive		mathematical optimization;cryptography;nash equilibrium;mathematics	Crypto	10.704154380877569	19.97668529042095	120678
c3de452e373cceaf23cb028075215f01dbf216b2	approximating tree edit distance through string edit distance for binary tree codes	binary tree code;binary tree representation;tree edit distance;string edit distance;binary tree	This article proposes an approximation of the tree edit distance through the string edit distance for binary tree codes, instead of for Euler strings introduced by Akutsu (2006). Here, a binary tree code is a string obtained by traversing a binary tree representation with two kinds of dummy nodes of a tree in preorder. Then, we show that σ/2 ≤ τ ≤ (h + 1)σ + h, where τ is the tree edit distance between trees, and σ is the string edit distance between their binary tree codes and h is the minimum height of the trees.	approximation;approximation algorithm;binary tree;code;download;dummy variable (statistics);email;euler;fundamenta informaticae;graph edit distance;parsing;tree (data structure)	Taku Aratsu;Kouichi Hirata;Tetsuji Kuboyama	2010	Fundam. Inform.	10.3233/FI-2010-282	random binary tree;optimal binary search tree;left-child right-sibling binary tree;red–black tree;combinatorics;tree rotation;binary expression tree;exponential tree;binary tree;computer science;trie;order statistic tree;range tree;self-balancing binary search tree;pattern recognition;k-ary tree;interval tree;mathematics;tree structure;search tree;programming language;threaded binary tree;string-to-string correction problem;tree traversal;algorithm;avl tree	Theory	15.838973500814483	28.71796105195761	120726
2f65249960194c796727eea4f78e6e13573a2e94	the maximum acyclic subgraph problem and degree-3 graphs	grafo aciclico;maximum degree;approximate algorithm;subgrafo;combinatorial algorithm;graphe acyclique;acyclic graph;optimisation combinatoire;sous graphe;directed graph;graphe oriente;grafo orientado;subgraph;combinatorial optimization;lower bound;optimizacion combinatoria	We study the problem of finding a maximum acyclic subgraph of a given directed graph in which the maximum total degree (in plus out) is 3. For these graphs, we present: (i) a simple combinatorial algorithm that achieves an 11/12-approximation (the previous best factor was 2/3 [1]), (ii) a lower bound of 125/126 on approximability, and (iii) an approximation-preserving reduction from the general case: if for any Ɛ > 0, there exists a (17/18 + Ɛ)-approximation algorithm for the maximum acyclic subgraph problem in graphs with maximum degree 3, then there is a (1/2+δ)-approximation algorithm for general graphs for some δ > 0. The problem of finding a better-than-half approximation for general graphs is open.	directed acyclic graph	Alantha Newman	2001		10.1007/3-540-44666-4_18	strong perfect graph theorem;mathematical optimization;combinatorics;discrete mathematics;cograph;directed graph;longest path problem;combinatorial optimization;hopcroft–karp algorithm;subgraph isomorphism problem;mathematics;maximal independent set;induced subgraph isomorphism problem;upper and lower bounds;chordal graph;indifference graph;directed acyclic graph	Theory	21.588558534226756	26.579526322001804	120835
529b5531a0d6948d8f3d2a51b4ab47cc106ed7ad	maximum s-t-flow with k crossings in o(k3n log n) time	maximum flow;planar graph algorithm;planar case;maximum cut problem;planar graph;time o;maximum s-t-flow problem;worst case;fastest maximum flow;planar instance;k3n log n;k crossing	There is a large body of results on planar graph algorithms that are more efficient than the best known algorithm for general graphs [13]. Maximum flow [1] is but one example. More drastically, the maximum cut problem is polynomially solvable for planar instances but NP-complete in general [12, 8, 5].  However, little is known about nearly planar graphs. This is unsatisfactory since the nearly planar case is particularly important in practice. Think for example of road networks with bridges and tunnels.  We present a preflow push algorithm that solves the maximum s-t-flow problem in a network with n vertices and m edges and embedded with k crossings in time O(k3n log n) worst case. To our knowledge there is only one previous result that relates asymptotic running time to a topological parameter of the graph such that the running time is polynomial in this parameter.  Compared with the currently fastest maximum flow algorithms this reduces the worst case running time by a factor of m/k3 ignoring logarithmic factors. Therefore, it is particularly favorable for very sparse or nearly planar graphs.	best, worst and average case;decision problem;embedded system;fastest;flow network;graph theory;maximum cut;maximum flow problem;np-completeness;planar graph;polynomial;push–relabel maximum flow algorithm;sparse matrix;time complexity	Jan M. Hochstein;Karsten Weihe	2007				Theory	21.85644628323969	22.029723349060212	120840
8d28bdb5053e32633b9524d1f4449a639775895d	a simple heuristic for minimum connected dominating set in graphs	connected dominating set;minimum connected dominating set;approximation algorithm;minor;2 independent set	Let α2(G), γ(G) and γc(G) be the 2-independence number, the domination number, and the connected domination number of a graph G respectively. Then α2(G) ≤ γ (G) ≤ γc(G). In this paper , we present a simple heuristic for Minimum Connected Dominating Set in graphs. When running on a graph G excluding Km (the complete graph of order m) as a minor, the heuristic produces a connected dominating set of cardinality at most 7α2(G) - 4 if m = 3, or at most if m ≥ 4. In particular, if running on a planar graph G, the heuristic outputs a connected dominating set of cardinality at most 15α2(G) - 5.	connected dominating set;heuristic	Peng-Jun Wan;Khaled M. Alzoubi;Ophir Frieder	2003	Int. J. Found. Comput. Sci.	10.1142/S0129054103001753	combinatorics;discrete mathematics;topology;bidimensionality;dominating set;connected dominating set;mathematics;maximal independent set;bound graph;approximation algorithm;algorithm;minor	Logic	24.51519277369804	21.654499371387683	120963
4cfb1d5ce9d6cc3993ab409ab972893249f1f891	stability in circular arc graphs	graphe non oriente;graph theory;teoria grafo;non directed graph;algorithmique;algoritmo busqueda;circular arc graph;algorithme recherche;grafico no orientado;reduction;search algorithm;theorie graphe;algorithmics;algoritmica;informatique theorique;reduccion;computer theory;informatica teorica	An algorithm is presented which finds a maximum stable set of a family of n arcs on a circle in 0( n log n) time given the arcs as an unordered list of their endpoints or in O(n) time if they are already sorted. If we are given only the circular arc graph without a circular arc representation for it, then a maximum stable set can be found in O(n + Se) time where II, e, and 6 are the number of vertices, edges, and minimum vertex degree, respectively. Our algorithms are based on a simple neighborhood reduction theorem which allows one to reduce any circular arc graph to a	algorithm;degree (graph theory);html element;time complexity	Martin Charles Golumbic;Peter L. Hammer	1988	J. Algorithms	10.1016/0196-6774(88)90023-5	combinatorics;reduction;graph theory;cycle graph;mathematics;geometry;algorithmics;algorithm;search algorithm	Theory	21.78813741447406	27.674200521441755	121026
f1e0ae383d380009c58b06a26602a16332b55445	bounds on universal sequences	graph theory;teoria grafo;complexity theory;clase complejidad;05c40;theorie graphe;conectividad diagrama;graph connectivity;classe complexite;complexity class;informatique theorique;68r10;68q15;connectivite graphe;universal sequences;computer theory;informatica teorica	Universal sequences for graphs, a concept introduced by Aleliunas [M. are studied. By letting U(d, n) denote the minimum length of a universal sequence for d-regular undirected graphs with n nodes, the latter paper has proved the upper bound U(d, n)= O(d2r log n) using a probabilistic argument. Here a lower bound of U(2, n)-(n log n) is proved from which U(d, n)-l'I(n log n) for all d is deduced. Also, for complete graphs U(n-1, n)=gl(n log n/log log n). An explicit construction of universal sequences for cycles (d 2) of length r/O(lgn) is given.	algorithm;brute-force search;connectivity (graph theory);expect;graph (discrete mathematics);nc (complexity);nonlinear system;parity bit;polynomial;traverse	Amotz Bar-Noy;Allan Borodin;Mauricio Karchmer;Nathan Linial;Michael Werman	1989	SIAM J. Comput.	10.1137/0218018	complexity class;combinatorics;connectivity;graph theory;mathematics;algorithm	Theory	22.70607927447879	31.67992183784476	121063
20e1c4090c110edcd028e711df0bdbb959cebd80	[k1, k2]-anonymization of shortest paths	k anonymity;shortest path;privacy preserving;edge weight;social networks	Privacy preserving network publishing has been studied extensively in recent years. Although more works have adopted un-weighted graphs to model network relationships, weighted graph modeling can provide deeper analysis of the degree of relationships. Previous works on weighted graph privacy have concentrated on preserving the shortest path characteristic between pairs of vertices. Two common types of privacy have been proposed. One type of privacy tried to add random noise edge weights to the graph but still maintain the same shortest path. The other privacy, k-shortest path privacy, minimally perturbed edge weights so that there exists k shortest paths. However, the k-shortest path privacy only considers anonymizing same fixed number of shortest paths for all pairs of source and destination vertices. In this work, we present a new concept called [k1, k2]-shortest path privacy to allow different number of shortest paths for different pairs of vertices. A published network graph with [k1, k2]-shortest path privacy has at least k' indistinguishable shortest paths between the source and destination vertices, where k1 ≦ k' ≦ k2. A heuristic algorithm based on modifying only Non-Visited (NV) edges is proposed and experimental results showing the feasibility and characteristics of the proposed approach are presented.	algorithm;heuristic (computer science);noise (electronics);nv network;privacy;shortest path problem;vertex (geometry)	Yu-Chuan Tsai;Shyue-Liang Wang;Tzung-Pei Hong;Hung-Yu Kao	2014		10.1145/2684103.2684160	widest path problem;constrained shortest path first;longest path problem;floyd–warshall algorithm;average path length;pathfinding;theoretical computer science;euclidean shortest path;yen's algorithm;johnson's algorithm;path graph;distributed computing;path;shortest path problem;distance;k shortest path routing;shortest path faster algorithm;social network	Theory	24.212404252070314	20.536527035825618	121160
c24a007bf5ddb30513e072ca3e6600c20ae04fd5	minimal separators in p4-sparse graphs	cographs;graphe minimal;separador;temps lineaire;graphe parfait;p 4;perfect graphs;p4 sparse graphs;perfect graph;calculo automatico;tiempo lineal;space time;espacio tiempo;computing;grafo minimo;calcul automatique;separation;separacion;p4 sparse graph;cographe;linear time;separator;linear space;separateur;minimal separation;espace temps;minimal graph;graphe p4 creux	"""In this paper, we determine the minimal separators of P""""4-sparse graphs and establish bounds on their number. Specifically, we show that a P""""4-sparse graph G on n vertices and m edges has fewer than 2n/3 minimal separators of total description size at most 4m/3. The bound on the number of minimal separators is tight and is also tight for the class of cographs, a well known subclass of the P""""4-sparse graphs. Our results enable us to present a linear-time and linear-space algorithm for computing the number of minimal separators of a given P""""4-sparse graph; the algorithm can be modified to report the minimal separators of the input graph in linear time and space as well."""		Stavros D. Nikolopoulos;Leonidas Palios	2006	Discrete Mathematics	10.1016/j.disc.2005.12.008	separator;combinatorics;computing;discrete mathematics;level structure;perfect graph;space time;mathematics;algorithm;linear space	Theory	23.377890503359794	27.85565668594451	121227
e6535d361b57da5733e0a25fea9c29499e8c122b	string comparison and lyndon-like factorization using v-order in linear time	unique maximal factorization families;lyndon-like factorization;linear-time algorithm;linear time;previous work;lyndon factorization;string comparison	In this paper we extend previous work on Unique Maximal Factorization Families (UMFFs) and a total (but non-lexicographic) ordering of strings called V-order. We describe linear-time algorithms for string comparison and Lyndon factorization based on V-order. We propose extensions of these algorithms to other forms of order.	comparison of programming languages (string functions);time complexity	David E. Daykin;Jacqueline W. Daykin;William F. Smyth	2011		10.1007/978-3-642-21458-5_8	dixon's factorization method;congruence of squares;combinatorics;discrete mathematics;lyndon word;mathematics;factorization of polynomials;string searching algorithm;algebra	Crypto	14.485648551299239	29.506328941461295	121369
23d23da80c158a92ebc5edfbe8dc28aa774eba0c	semisupervised clustering, and-queries and locally encodable source coding		Source coding is the canonical problem of data compression in information theory. In a locally encodable source coding, each compressed bit depends on only few bits of the input. In this paper, we show that a recently popular model of semisupervised clustering is equivalent to locally encodable source coding. In this model, the task is to perform multiclass labeling of unlabeled elements. At the beginning, we can ask in parallel a set of simple queries to an oracle who provides (possibly erroneous) binary answers to the queries. The queries cannot involve more than two (or a fixed constant number ∆ of) elements. Now the labeling of all the elements (or clustering) must be performed based on the (noisy) query answers. The goal is to recover all the correct labelings while minimizing the number of such queries. The equivalence to locally encodable source codes leads us to find lower bounds on the number of queries required in variety of scenarios. We are also able to show fundamental limitations of pairwise ‘same cluster’ queries and propose pairwise AND queries, that provably performs better in many situations.	cluster analysis;code;data compression;information theory;multiclass classification;oracle database;semi-supervised learning;turing completeness	Arya Mazumdar;Soumyabrata Pal	2017			equivalence (measure theory);information theory;machine learning;oracle;artificial intelligence;computer science;cluster analysis;data compression;source code;binary number;pairwise comparison	ML	11.406627323575274	25.705798829424925	121691
cf2f1f255b3e5de99757466050120ca36f4108f2	improved algorithms for finding level ancestors in dynamic trees	arbre graphe;algorithm complexity;tree graph;complejidad algoritmo;optimisation combinatoire;graph connectivity;complexite algorithme;informatique theorique;conectividad grafo;rooted tree;arbol grafo;combinatorial optimization;algoritmo optimo;algorithme optimal;optimal algorithm;connectivite graphe;optimizacion combinatoria;computer theory;informatica teorica	Given a node x at depth d in a rooted tree Level Ancestor(x; i) returns the ancestor to x in depth d - i. We show how to maintain a tree under addition of new leaves so that updates and level ancestor queries are being performed in worst case constant time. Given a forest of trees with n nodes where edges can be added, m queries and updates take O(mα(m, n)) time. This solves two open problems (P.F. Dietz, Finding level-ancestors in dynamic trees, LNCS, 519:32-40, 1991). In a tree with node weights, min(x, y) report the node with minimum weight on the path between the nodes x and y. We can substitute the Level Ancestor query with min, without increasing the complexity for updates and queries. Previously such results have been known only for special cases (e.g. R. E. Tarjan. Applications of path compression on balanced trees. J. ACM, 26(4):690-715, 1979).	algorithm;link/cut tree	Stephen Alstrup;Jacob Holm	2000		10.1007/3-540-45022-X_8	mathematical optimization;combinatorics;combinatorial optimization;connectivity;mathematics;tree;algorithm;lowest common ancestor	Theory	20.28460781822518	26.176658226766836	121957
0304c8bd44de2bd525f082e75d952dd9bc9263d3	optimization of approximate decision rules relative to length		In the paper, a modified dynamic programming approach for optimization of decision rules relative to length is studied. Experimental results connected with length of approximate decision rules, size of a directed acyclic graph, and accuracy of classifiers, are presented.	approximation algorithm	Beata Zielosko;Krzysztof Zabinski	2018		10.1007/978-3-319-99987-6_13	directed acyclic graph;mathematical optimization;dynamic programming;decision rule;mathematics	ECom	17.987979061895185	24.69616701970128	121998
7476708cc8d9fdc7653eb046e0be7757c1b46765	improved average complexity for comparison-based sorting		This paper studies the average complexity on the number of comparisons for sorting algorithms. Its information-theoretic lower bound is n lg n− 1.4427n+O(log n). For many efficient algorithms, the first n lg n term is easy to achieve and our focus is on the (negative) constant factor of the linear term. The current best value is −1.3999 for the MergeInsertion sort. Our new value is −1.4106, narrowing the gap by some 25%. An important building block of our algorithm is “two-element insertion,” which inserts two numbers A and B, A < B, into a sorted sequence T . This insertion algorithm is still sufficiently simple for rigorous mathematical analysis and works well for a certain range of the length of T for which the simple binary insertion does not, thus allowing us to take a complementary approach with the binary insertion.	information theory;sorting algorithm;term (logic)	Kazuo Iwama;Junichi Teruyama	2017		10.1007/978-3-319-62127-2_41	combinatorics;discrete mathematics;mathematics;algorithm	Theory	11.690879181273198	24.358655908657184	122032
55dff3f9f8948c82b8d29d965d63e9078e147f46	it's all a matter of degree: using degree information to optimize multiway joins	004;joins degree mapreduce	We optimize multiway equijoins on relational tables using degree information. We give a new bound that uses degree information to more tightly bound the maximum output size of a query. On real data, our bound on the number of triangles in a social network can be up to 95 times tighter than existing worst case bounds. We show that using only a constant amount of degree information, we are able to obtain join algorithms with a running time that has a smaller exponent than existing algorithms - for any database instance. We also show that this degree information can be obtained in nearly linear time, which yields asymptotically faster algorithms in the serial setting and lower communication algorithms in the MapReduce setting.#R##N##R##N#In the serial setting, the data complexity of join processing can be expressed as a function O(IN^x + OUT) in terms of input size IN and output size OUT in which x depends on the query. An upper bound for x is given by fractional hypertreewidth. We are interested in situations in which we can get algorithms for which x is strictly smaller than the fractional hypertreewidth. We say that a join can be processed in subquadratic time if x < 2. Building on the AYZ algorithm for processing cycle joins in quadratic time, for a restricted class of joins which we call 1-series-parallel graphs, we obtain a complete decision procedure for identifying subquadratic solvability (subject to the 3-SUM problem requiring quadratic time). Our 3-SUM based quadratic lower bound is tight, making it the only known tight bound for joins that does not require any assumption about the matrix multiplication exponent omega. We also give a MapReduce algorithm that meets our improved communication bound and handles essentially optimal parallelism.		Manas Joglekar;Christopher Ré	2016		10.4230/LIPIcs.ICDT.2016.11	computer science;database;distributed computing	DB	12.315129158144586	25.58741644459811	122048
bcd97c3883138cfee153a57ce14d5fd2cb2ed2fa	optimal algorithms for locating the longest and shortest segments satisfying a sum or an average constraint	online algorithm;plus court segment;procesamiento informacion;algorithm analysis;manipulacion dato;algorithme en ligne;data stream;linear time algorithm;estimacion promedio;algorithme temps lineaire;biology;biologia;algoritmo en linea;satisfiability;plus long segment;informatique theorique;linear time;information processing;analyse sequence;algorithms;analyse algorithme;sequence analysis;longest segment;mean estimation;data handling;traitement information;algoritmo optimo;estimation moyenne;algorithme optimal;optimal algorithm;shortest segment;maniement donnee;analisis algoritmo;biologie;computer theory;informatica teorica	We study several fundamental problems arising from biological sequence analysis. Given a sequence of real numbers, two linear-time algorithms, one for locating the “longest” sum-constrained segment, and the other for locating the “shorte constrained segment. These two algorithms are based on the same framework and run in an online manner, hence they of handling data stream inputs. Our algorithms also yield online linear-time solutions for finding the longest and shortest constrained segments by a simple reduction.  2005 Elsevier B.V. All rights reserved.	algorithm;sequence analysis;time complexity	Kuan-Yu Chen;Kun-Mao Chao	2005	Inf. Process. Lett.	10.1016/j.ipl.2005.08.006	time complexity;online algorithm;combinatorics;information processing;computer science;group method of data handling;sequence analysis;calculus;mathematics;algorithm;satisfiability	Theory	16.14729052744676	24.36326129433912	122144
352f7fbae61ad1d8461ae4680c84021b0fbaf19b	tight bounds for the partial-sums problem	lower and upper bound;partial sums;upper bound;model of computation;lower bound	We close the gaps between known lower and upper bounds for the online partial-sums problem in the RAM and group models of computation. If elements are chosen from an abstract group, we prove an Ω(lg <i>n</i>) lower bound on the number of algebraic operations that must be performed, matching a well-known upper bound. In the RAM model with <i>b</i>-bit memory registers, we consider the well-studied case when the elements of the array can be changed additively by Δ-bit integers. We give a RAM algorithm that achieves a running time of Θ(1 + lg <i>n</i> / lg(<i>b</i> / Δ)) and prove a matching lower bound in the cell-probe model. Our lower bound is for the amortized complexity, and makes minimal assumptions about the relations between <i>n</i>, <i>b</i>, and Δ. The best previous lower bound was Ω(lg <i>n</i> = (lg lg <i>n</i>+lg <i>b</i>)), and the best previous upper bound matched only in the special case <i>b</i> = Θ(lg <i>n</i>) and Δ = <i>O</i>(lg lg <i>n</i>).	algorithm;amortized analysis;cell-probe model;linear algebra;model of computation;random-access memory;time complexity;whole earth 'lectronic link	Mihai Patrascu;Erik D. Demaine	2004			arithmetic;combinatorics;mathematics;upper and lower bounds;algorithm	Theory	11.769246976717278	24.655318724569266	122476
a063f47b1bd8af7f2c59a263e4034307c2934cbc	recursive lattice reduction	selected works;time complexity;geometry of numbers;hermite factor;lattice reduction;recursive reduction;bepress;complete lattice	Lattice reduction is known to be a very powerful tool in modern cryptanalysis. In the literature, there are many lattice reduction algorithms that have been proposed with various time complexity (from quadratic to subexponential). These algorithms can be utilized to find a short vector of a lattice with a small norm. Over time, shorter vector will be found by incorporating these methods. In this paper, we take a different approach by presenting a methodology that can be applied to any lattice reduction algorithms, with the implication that enables us to find a shorter vector (i.e. a smaller solution) while requiring shorter computation time. Instead of applying a lattice reduction algorithm to a complete lattice, we work on a sublattice with a smaller dimension chosen in the function of the lattice reduction algorithm that is being used. This way, the lattice reduction algorithm will be fully utilized and hence, it will produce a better solution. Furthermore, as the dimension of the lattice becomes smaller, the time complexity will be better. Hence, our methodology provides us with a new direction to build a lattice that is resistant to lattice reduction attacks. Moreover, based on this methodology, we also propose a recursive method for producing an optimal approach for lattice reduction with optimal computational time, regardless of the lattice reduction algorithm used. We evaluate our technique by applying it to break the lattice challenge by producing the shortest vector known so far. Our results outperform the existing known results and hence, our results achieve the record in the lattice challenge problem.	algorithm;computation;cryptanalysis;cryptosystem;lattice reduction;recursion (computer science);time complexity	Thomas Plantard;Willy Susilo	2010		10.1007/978-3-642-15317-4_21	time complexity;mathematical optimization;combinatorics;discrete mathematics;lattice reduction;complete lattice;geometry of numbers;mathematics;algorithm;lattice problem	Crypto	12.232593769389707	23.17823628065788	122682
33c7fb45d39301ef916c691648e29edc94a534b7	degree bounded network design with metric costs	network design;05c40;graph connectivity;68r10;edge splitting off;68w25;68m10	Given a complete undirected graph, a cost function on edges, and a degree bound $B$, the degree bounded network design problem is to find a minimum cost simple subgraph with maximum degree $B$ satisfying given connectivity requirements. Even for a simple connectivity requirement such as finding a spanning tree, computing a feasible solution for the degree bounded network design problem is already NP-hard, and thus there is no polynomial factor approximation algorithm for this problem. In this paper, we show that when the cost function satisfies the triangle inequality, there are constant factor approximation algorithms for various degree bounded network design problems. In global edge-connectivity, there is a $(2+\frac{1}{k})$-approximation algorithm for the minimum bounded degree $k$-edge-connected subgraph problem. In local edge-connectivity, there is a 4-approximation algorithm for the minimum bounded degree Steiner network problem when $r_{\max}$ is even, and a 5.5-approximation algorithm when $r_{\max}$ is odd, where $r_{\max}$ is the maximum connectivity requirement. In global vertex-connectivity, there is a $(2+\frac{k-1}{n}+\frac{1}{k})$-approximation algorithm for the minimum bounded degree $k$-vertex-connected subgraph problem when $n\geq2k$, where $n$ is the number of vertices. For spanning tree, there is a $(1+\frac{1}{B-1})$-approximation algorithm for the minimum bounded degree spanning tree problem. These approximation algorithms return solutions with the smallest possible maximum degree, and in most cases the cost guarantee is obtained by comparing to the optimal cost when there are no degree constraints. This demonstrates that degree constraints can be incorporated into network design problems with metric costs. Our algorithms can be seen as a generalization of Christofides' algorithm for the metric traveling salesman problem. The main technical tool is a simplicity-preserving edge splitting-off operation, which is used to “short-cut” vertices with high degree while maintaining connectivity requirements and preserving simplicity of the solutions.		Yuk Hei Chan;Wai Shing Fung;Lap Chi Lau;Chun Kong Yung	2011	SIAM J. Comput.	10.1137/090746495	mathematical optimization;network planning and design;combinatorics;discrete mathematics;computer science;connectivity;mathematics;algorithm	Theory	23.448180549277886	19.6270768816179	122719
565ef33b331ffd4f877cd0d0671d4320e47e9ff7	an improved query time for succinct dynamic dictionary matching		In this work, we focus on building an efficient succinct dy- namic dictionary that significantly improves the query time of the current best known results. The algorithm that we propose suffers from only a O((log log n) 2 ) multiplicative slowdown in its query time and a O( 1 log n) slowdown for insertion and deletion operations, where n is the sum of all of the patterns' lengths, the size of the alphabet is polylog(n )a nd � ∈ (0, 1). For general alphabet the query time is O((log log n )l ogσ), where σ is the size of the alphabet. A byproduct of this paper is an Aho-Corasick automaton that can be constructed with only a compact working space, which is the first of its type to the best of our knowledge.	dictionary	Guy Feigenblat;Ely Porat;Ariel Shiftan	2014		10.1007/978-3-319-07566-2_13	combinatorics;theoretical computer science;mathematics;algorithm	Theory	12.61446347937312	26.830625668189764	122740
1c0f88a54be81a7086c22ed9ccdd0aa3338775bb	edge-coloring bipartite graphs	graph theory;edge coloring;maximum degree;graphe biparti;coloracion grafo;teoria grafo;time complexity;best approximation;grafo bipartido;theorie graphe;algorithme;algorithm;complexite temps;coloration graphe;complejidad tiempo;bipartite graph;graph colouring;qa075 electronic computers computer science;algoritmo;perfect match	Given a bipartite graph G with n nodes, m edges and maximum degree ∆, we find an edge coloring for G using ∆ colors in time T +O(m log ∆), where T is the time needed to find a perfect matching in a k-regular bipartite graph with O(m) edges and k ≤ ∆. Together with best known bounds for T this implies an O(m log ∆ + m ∆ log m ∆ log ∆) edge-coloring algorithm which improves on the O(m log ∆+ m ∆ log m ∆ log ∆) algorithm of Hopcroft and Cole. Our algorithm can also be used to find a (∆ + 2)-edge-coloring for G in time O(m log ∆). The previous best approximation algorithm with the same time bound needed ∆ + log ∆ colors.	approximation algorithm;color;edge coloring;graph coloring;matching (graph theory)	Ajai Kapoor;Romeo Rizzi	2000	J. Algorithms	10.1006/jagm.1999.1058	time complexity;combinatorics;discrete mathematics;bipartite graph;graph theory;edge coloring;mathematics;algorithm	Theory	22.040512589644873	26.971239266933306	122770
37f232d95980e948d12ccc0b47e896a0b8426ac0	revisiting dynamic programming for finding optimal subtrees in trees	arbre graphe;graph node;graphe non oriente;dynamic programming;programacion dinamica;non directed graph;informe tecnico;nudo grafo;tree graph;dynamic programming algorithm;heuristic method;problema np duro;heuristic dynamic programming;metodo heuristico;dynamic program;optimisation combinatoire;np hard problem;probleme np difficile;grafo no orientado;programmation dynamique;heuristics;technical report;methode heuristique;computer analysis;arbol grafo;combinatorial optimization;algoritmo optimo;algorithme optimal;optimal algorithm;rapport technique;noeud graphe;optimizacion combinatoria	In this paper we revisit an existing dynamic programming algorithm for finding optimal subtrees in edge weighted trees. This algorithm was sketched by Maffioli in a technical report in 1991. First, we adapt this algorithm for the application to trees that can have both node and edge weights. Second, we extend the algorithm such that it does not only deliver the values of optimal trees, but also the trees themselves. Finally, we use our extended algorithm for developing heuristics for the k-cardinality tree problem in undirected graphs G with node and edge weights. This NP-hard problem consists of finding in the given graph a tree with exactly k edges such that the sum of the node and the edge weights is minimal. In order to show the usefulness of our heuristics we conduct an extensive computational analysis that concerns most of the existing problem instances. Our results show that with growing problem size the proposed heuristics reach the performance of state-of-the-art metaheuristics. Therefore, this study can be seen as a cautious note on the scaling of metaheuristics.	dynamic programming;tree (data structure)	Christian Blum	2007	European Journal of Operational Research	10.1016/j.ejor.2005.11.005	mathematical optimization;combinatorics;combinatorial optimization;computer science;dynamic programming;mathematics;algorithm	Theory	21.727451939638037	18.617281234522586	122919
ef3fada76354d2a596d8d6a167e531a0eac74cf5	fixed hypercube embedding	minimisation;graph theory;complexite;hypercube;minimization;teoria grafo;multiprocessor;probleme np complet;reduction;complejidad;minimizacion;complexity;theorie graphe;reduccion;problema np completo;graph embedding;multiprocesador;np complete problem;multiprocesseur	In this paper, we consider the problem of embedding a given graph into a fixed-size hypercube. This work is related to other recent research on hypercube embedding. In [1], Bhat describes an order IVl log2 IVI algorithm for testing whether a given graph G = (V, E) is exactly an n-cube. Deciding whether a graph has a distance-preserving embedding into a hypercube can be done in polynomial time [3,7]. On the other hand, the present authors have shown that the problem of deciding whether a given graph is embeddable into any dimensioned hypercube is NP-complete [5]. Here we are concerned with the important practical problem of deciding whether a graph can be embedded into a fixed-size hypercube, thereby in a sense filling the gap between Bhat's result and that in [4]. We show that this problem is NP-complete. Our reduction uses the well-known 3-partition problem [4], which incidently was used in [6] to	3-partition problem;algorithm;binary logarithm;embedded system;karp's 21 np-complete problems;np-completeness;polynomial;time complexity;whole earth 'lectronic link	George Cybenko;David W. Krumme;K. N. Venkataraman	1987	Inf. Process. Lett.	10.1016/0020-0190(87)90090-1	minimisation;folded cube graph;combinatorics;grid network;discrete mathematics;complexity;multiprocessing;graph embedding;np-complete;topology;reduction;graph theory;hypercube graph;mathematics;algorithm;hypercube	Theory	20.82440166435238	27.45001988024064	122992
680f90b0e819c6c01df5aa9741b455c2610210a7	a practical method for the minimum genus of a graph: models and experiments		We consider the problem of the minimum genus of a graph, a fundamental measure of non-planarity. We propose the first formulations of this problem as an integer linear program (ILP) and as a satisfiability problem (SAT). These allow us to develop the first working implementations of general algorithms for the problem, other than exhaustive search. We investigate several different ways to speed-up and strengthen the formulations; our experimental evaluation shows that our approach performs well on small to medium-sized graphs with small genus, and compares favorably to other approaches.	algorithm;boolean satisfiability problem;brute-force search;cognitive dimensions of notations;computation;computational resource;crossing number (graph theory);exact algorithm;experiment;fast fourier transform;forbidden graph characterization;general-purpose modeling;genus (mathematics);graph (discrete mathematics);graph theory;jart armin;linear programming;planar graph;polyhedron;problem domain;solver;toroidal graph;whole earth 'lectronic link	Stephan Beyer;Markus Chimani;Ivo Hedtke;Michal Kotrbcík	2016		10.1007/978-3-319-38851-9_6	mathematical optimization;combinatorics;discrete mathematics;mathematics;algorithm	Logic	20.747142230470704	20.825687231928462	123153
6311506c0fcf833c15bd2e8c8246d8d461e2f05e	decycling number of crossed cubes cqn		A subset of vertices of a graph G is called a decycling set of G if its deletion results in an acyclic subgraph. The cardinality of a minimum decycling set is called the decycling number of G. This paper presents an approach to construct an acyclic subgraph of CQ_n and proves that for any integer n ≥ 2, the decycling number of CQ_n is 2^n-1 ⋅(1-c/n-1), c∊[0,1].	directed acyclic graph;marching cubes	Xirong Xu;Soomro Pir Dino;Huifeng Zhang;Huijun Jiang;Cong Liu	2017	2017 13th International Conference on Computational Intelligence and Security (CIS)	10.1109/CIS.2017.00039	computer science;mathematical optimization;cube;cardinality;combinatorics;vertex (geometry);approximation algorithm;computational intelligence;hypercube;graph;integer	DB	24.592564470937607	27.38067926363888	123303
b20019445625f28cacd41550ed37c788d45c28c6	on a fast version of a pseudorandom generator	large family;original family;strong pseudorandom property;new family;pseudorandom sequence;earlier paper;ind n;fast algorithm;pseudorandom generator;good pseudorandom property;fast version;discrete logarithm	In an earlier paper I constructed a large family of pseudorandom sequences by using the discrete logarithm. While the sequences in this construction have strong pseudorandom properties, they can be generated very slowly since no fast algorithm is known to compute ind n. The purpose of this paper is to modify this family slightly so that the members of the new family can be generated much faster, and they have almost as good pseudorandom properties as the sequences in the original family.	pseudorandom generator;pseudorandomness	Katalin Gyarmati	2005	Electronic Notes in Discrete Mathematics	10.1016/j.endm.2005.07.047	linear congruential generator;lavarand;random seed;pseudorandom number generator;lagged fibonacci generator;self-shrinking generator	Theory	10.817604687925682	24.04633565189253	123317
a199d32065d95e3a33310c0d36f2f7c0e34ca25d	a theoretical framework of hybrid approaches to max sat	arithmetique ordinateur;performance guarantee;approximate algorithm;theoretical framework;algorithmique;satisfiability;computer arithmetic;hybrid approach;algorithmics;algoritmica;informatique theorique;aritmetica ordenador;computer theory;informatica teorica	MAX SAT (the maximum satisfiability problem) is stated as follows: given a set of clauses with weights, find a truth assignment that maximizes the sum of the weights of the satisfied clauses. In this paper, we present a theoretical framewok of hybrid approaches combining the algorithms of Goemans-Williamson and Yannakakis. This framework leads to a unified analysis of the performance guarantees of proposed algorithms and also leads to a better approximation algorithm with performance guarantee 0.770, if we use a refinement of Yannakakis' algorithm.	max;maximum satisfiability problem	Takao Asano;Kuniaki Hori;Takao Ono;Tomio Hirata	1997		10.1007/3-540-63890-3_18	combinatorics;computer science;artificial intelligence;theoretical computer science;machine learning;mathematics;algorithmics;algorithm;satisfiability	Logic	14.649074558254648	18.641810581958065	123359
cb11837becb2dbb11978a47bf8380a33361f0430	chess endgames: 6-man data and strategy	data depth;statistique;metodo estadistico;chess;analisis datos;chess game;data;metric;estrategia;statistical method;depth;endnotes;strategy;ajedrez;data analysis;methode statistique;informatique theorique;profundidad;statistics;analyse donnee;metrico;jeu echecs;pubications;profondeur;endgame;strategie;metrique;move count;conversion;estadistica;computer theory;goal;informatica teorica	While Nalimov’s endgame tables for Western Chess are the most used today, their Depth-to-Mate metric is not the most efficient or effective in use. The authors have developed and used new programs to create tables to alternative metrics and recommend better strategies for endgame play.	computation;data mining;eugene nalimov;evolutionary governance theory;raid;snapshot (computer storage);terabyte;unix	M. S. Bourzutschky;Jeanette Tamplin;Guy Haworth	2005	Theor. Comput. Sci.	10.1016/j.tcs.2005.09.043	metric;strategy;computer science;artificial intelligence;mathematics;data analysis;algorithm;statistics;data	Theory	16.128274009959977	25.069987202496108	123401
eb9d3af6ad1dc3a0ec7684099587095935bfe448	simple and efficient greedy algorithms for hamilton cycles in random intersection graphs	graph theory;random graph;metodo polinomial;mecanique hamiltonienne;teoria grafo;temps polynomial;algorithme glouton;grafo aleatorio;graphe aleatoire;probabilistic approach;theorie graphe;polynomial time algorithm;mecanica hamiltoniana;polynomial method;hamilton cycle;enfoque probabilista;approche probabiliste;cycle graphe;edge graph;polynomial time;greedy algorithm;arete graphe;algoritmo gloton;cycle graph;intersection graphs;methode polynomiale;hamiltonian mechanics;arista grafico;tiempo polinomial;ciclo diagrama	In this work we consider the problem of finding Hamilton Cycles in graphs derived from the uniform random intersection graphs model Gn, m, p. In particular, (a) for the case m=nα, α>1 we give a result that allows us to apply (with the same probability of success) any algorithm that finds a Hamilton cycle with high probability in a Gn, k graph (i.e. a graph chosen equiprobably form the space of all graphs with k edges), (b) we give an expected polynomial time algorithm for the case p = constant and $m \leq \alpha {\sqrt{{n}\over {{\rm log}n}}}$ for some constant α, and (c) we show that the greedy approach still works well even in the case $m = o({{n}\over{{\rm log}n}})$ and p just above the connectivity threshold of Gn, m, p (found in [21]) by giving a greedy algorithm that finds a Hamilton cycle in those ranges of m, p with high probability.	algorithm	Christoforos Raptopoulos;Paul G. Spirakis	2005		10.1007/11602613_50	hamiltonian path;time complexity;random graph;combinatorics;greedy algorithm;discrete mathematics;graph theory;cycle graph;mathematics;hamiltonian mechanics;algorithm	Theory	23.24346094896647	31.37391143700706	123518
0f6c91cda09471b8c9c2da763ffac9320a336a85	a note on the use of independent sets for the k-sat problem	independent set;upper bounds;upper bound;independent sets of variables;k sat problem	An independent set of variables is one in which no two variables occur in the same clause in a given k-SAT instance. Recently, independent sets have obtained more attention. Due to a simple observation we prove that a k-SAT instance over n variables with independent set of size i can be solved in time O(φ2(k−1)(n − i)) where φk(n) denotes an upper bound on the complexity of solving k-SAT over n variables.	boolean satisfiability problem;independent set (graph theory)	Konstantin Kutzkov	2006	JSAT		combinatorics;discrete mathematics;pairwise independence;independent set;multiple-scale analysis;mathematics;upper and lower bounds;algorithm	Theory	22.271143892919945	23.92286712375101	123632
9fbd447235fe1a35255a47275c26d59d14ed71b9	a toroidal systolic array for warshall's algorithm	cographs;list ranking;systolic array;tree contraction;erew pram;tree representable graphs;parallel algorithms	Computing the transitive closure of an acyclic binary relation, and reducing a nonsingular square matrix to row echelon form are both instances of the algebraic path problem (see JR]). In this paper we modify the systolic array developed in [S] by changing the underlying topology to a toms. This simple change improves the 5N-5 computation time (where N is the size of the underlying set of the relation) to 4N-3. A further modification yields a systolic array for Gaussian elimination with a computation time of 3.5N (where N is the size of one side of the matrix).	computation;directed acyclic graph;floyd–warshall algorithm;gaussian elimination;linear algebra;row echelon form;systolic array;the matrix;time complexity;toroidal graph;transitive closure	Richard T. Denman;Travis B. Rowell	1991		10.1145/327164.327221	parallel computing;systolic array;computer science;parallel algorithm	Theory	18.558420494995296	30.15081011048985	123661
d8bb3c2be5cb0a9cb06d2a3d13de2c332d2b3558	selection from structured data sets	structured data	A large body of work studies the complexity of selecting the j-th largest element in an arbitrary set of n elements (a.k.a. the select(j) operation). In this work, we study the complexity of select in data that is partially structured by an initial preprocessing stage and in a data structure that is dynamically maintained. We provide lower and upper bounds in the comparison based model. For preprocessing, we show that making at most α(n) · n comparisons during preprocessing (before the rank j is provided) implies that select(j) must make at least (2 + )(n/e2) comparisons in the worst case, where > 2. For dynamically maintained data structures, we show that if the amortized number of comparisons executed with each insert operation is bounded by i(n), then select(j) must make at least (2 + )(n/e2) comparisons in the worst case, no matter how costly the other data structure operations are. When only insert is used, we provide a lower bound on the complexity of findmedian. This lower bound is much higher than the complexity of maintaining the minimum, thus formalizing the intuitive difference between findmin and findmedian. Finally, we present a new explicit adversary for comparison based algorithms and use it to show adversary lower bounds for selection problems. We demonstrate the power of this adversary by improving the best known lower bound for the findany operation in a data structure and by slightly improving the best adversary lower bound for sorting.	adversary (cryptography);algorithm;amortized analysis;best, worst and average case;data structure;electronic colloquium on computational complexity;in-place algorithm;international standard serial number;preprocessor;sorted array;sorting	Erez Petrank;Guy N. Rothblum	2004	Electronic Colloquium on Computational Complexity (ECCC)		combinatorics;data model;computer science;theoretical computer science;mathematics;distributed computing;algorithm	Theory	12.124588627855925	25.78230470606361	123727
81455e783d74cc5b92689f22420547d0f877ce43	improving the upper bound on the length of the shortest reset word		We improve the best upper bound on the length of the shortest reset words of synchronizing automata. The new bound is (15617n 3 + 7500n 2 + 9375n − 31250)/93750. So far, the best general upper bound was (n 3 − n)/6 − 1 obtained by Pin and Frankl in 1983. Despite a number of efforts, the bound remained unchanged for about 34 years. The new upper bound improves the coefficient 1/6 at n 3 by 4/46875. A word is avoiding for a state q if after reading the word, the automaton cannot be in q. We prove an upper bound on the length of a word that is either avoiding or compressing a subset. Then we show a quadratic bound on the length of the shortest avoiding words, and, using the approach of Trahtman from 2011, we improve the general upper bound on the length of the shortest reset words. For all the bounds, there exist polynomial algorithms finding a word of length not exceeding the bound. We deal with deterministic finite complete (semi)automata A (Q, Σ, δ), where Q is the set of states, Σ is the input alphabet, and δ : Q × Σ → Q is the transition function. We extend δ to a function Q × Σ * → Q in the usual way. Throughout the paper, by n we denote the number of states |Q|. By Σ ≤i we denote the set of all words over Σ of length at most i. Given a state q ∈ Q and a word w ∈ Σ * we write shortly qw = δ(Q, w), and given a subset S ⊆ Q we write Sw for the image {qw | q ∈ S}. Then Sw −1 is the preimage {q ∈ Q | qw ∈ S}. The rank of a word w ∈ Σ * is the cardinality |Qw|. A word is reset or synchronizing if it has rank 1. An automaton is synchronizing if it admits a reset word. The reset threshold rt(A) is the length of the shortest reset words. We say that a word w ∈ Σ * compresses a subset S ⊆ Q if |Sw| < |S|. A word w ∈ Σ * avoids a state q ∈ Q if q / ∈ Qw. The famous Černý conjecture ([8]) is one of the most long standing open problems in automata theory. It states …	algorithm;automata theory;automaton;coefficient;data compression;existential quantification;polynomial;quantum well;synchronizing word	Marek Szykula	2018		10.4230/LIPIcs.STACS.2018.56	combinatorics;conjecture;synchronizing;discrete mathematics;mathematics;polynomial;upper and lower bounds	Logic	12.599566286906025	29.282999688320864	123741
e7b9377e588c59f190901fffcc7ea6c1cd346d1f	a quantitative analysis of minimal window search	theoretical model;quantitative analysis;analytical model	Use of minimal windows enhances the aB algorithm in practical applications as well as in the search of artificially constructed game trees. Nevertheless, there exists no theoretical model to measure the strengths and weaknesses of minimal window search. In particular, it is not known which tree ordering properties are favorable for minimal window search. This paper presents a quantitative analysis of minimal window search based on recursive equations which assess the influence of static node values on the dynamic search process. The analytical model is computationally simple, easily extendible and gives a realistic estimate of the expected search time for averagely ordered game trees. 1 In t roduc t ion	algorithm;extensibility;microsoft windows;recursion;theory;window function	Alexander Reinefeld;T. Anthony Marsland	1987			quantitative analysis	Vision	10.407679109259428	27.079511537503787	123762
d82005995bbe4ea1fc49b6c47a6073b6059c872d	non-approximability of weighted multiple sequence alignment for arbitrary metrics	alphabet binaire;alignement sequence;score function;algorithm analysis;complexite calcul;biologia matematica;approximation algorithm;funcion arbitraria;biologie mathematique;alineacion secuencia;approximation hardness;alignement sequence multiple;complejidad computacion;mathematical biology;computational complexity;sum of pairs score;weighted sums;informatique theorique;algoritmo aproximacion;algorithms;analyse algorithme;sequence alignment;multiple sequence alignment;arbitrary function;algorithme approximation;computational biology;fonction arbitraire;analisis algoritmo;computer theory;informatica teorica	We prove that the multiple sequence alignment problem with weighted sum-of-pairs score is APX-hard for arbitrary metric scoring functions over the binary alphabet. This holds even when the weights are restricted to zero and one.	apx;approximation algorithm;multiple sequence alignment;scoring functions for docking;weight function	Bodo Manthey	2005	Inf. Process. Lett.	10.1016/j.ipl.2005.04.010	combinatorics;multiple sequence alignment;computer science;sequence alignment;mathematics;score;computational complexity theory;approximation algorithm;algorithm	ML	16.277464553406524	24.15406773568152	123883
a84d4a3715b75f6f5f25cfdecee339c26e72b120	a multi-stack method for the fast generation of permutations with minimal length increasing subsequences	graph theory;teoria grafo;complexite calcul;listing algorithms;combinatorial problems;theorie graphe;permutation;algorithme;algorithm;combinatorial problem;complejidad computacion;probleme combinatoire;problema combinatorio;constant amortized time;computational complexity;permutacion;algoritmo	Abstract   A multi-stack, O( n ) space, constant amortized time algorithm is presented for listing all permutations of the integers 1, 2, …,  n  that contain a subsequence of length  k  where all of the elements in the subsequence are in increasing order.	stack (abstract data type)	Dominique Roelants van Baronaigien	1999	Inf. Process. Lett.	10.1016/S0020-0190(98)00208-7	combinatorics;longest increasing subsequence;graph theory;mathematics;permutation;computational complexity theory;longest alternating subsequence;algorithm	DB	17.01930263352304	26.75313455229698	123945
b02514b7aff88f84f358f94d62026aac7d5b9381	batch dynamic algorithms for two graph problems	parallel algorithm;parallel computer;connected component;data structure;on line algorithm	We provide a new paradigm to treat insertion and deletion of a batch of edges in a graph, that makes use of the sparsification technique developed for on-line algorithms. In particular, we refer to the problems of minimal spanning forest (MSF), connected components (CC) and k-vertex-connectivity (k-VC). Our batch algorithms, improve of a log-factor over the classical one-by-one algorithms, for k- VC. This improvement is limited to batches of properly large size for MSF and CC. In parallel computation we discuss MSF and CC under single edge updating, batch insertions and batch updating, extending the use of the sparsification data structure 10 the CRCVV PRAM model and providing efficient parallel algorithms for these problems (the batch algorithms for CC are indeed work-optimal). We also discuss the difficulty of obtaining a parallel dynamic algorithm for the k-vertex connectivity problem.	dynamic problem (algorithms);graph theory	Paolo Ferragina;Fabrizio Luccio	1994		10.1007/3-540-58184-7_143	tarjan's strongly connected components algorithm;connected component;data structure;computer science;theoretical computer science;machine learning;in-place algorithm;distributed computing;parallel algorithm;cost efficiency	Theory	19.060405487077556	28.647349630384358	123957
06457c683e2277c868347afbba652f092f01306d	algorithms and complexity	data structure;discrete mathematics;computer graphic	material may be reproduced for any educational purpose, multiple copies may be made for classes, etc. Charges, if any, for reproduced copies must be just enough to recover reasonable costs of reproduction. Reproduction for commercial purposes is prohibited. This cover page must be included in all distributed copies. It may be taken at no charge by all interested persons. Comments and corrections are welcome, and should be sent to		Herbert S. Wilf	1986		10.1007/3-540-62592-5	complete;randomized algorithms as zero-sum games;parameterized complexity;probabilistic analysis of algorithms;complexity;average-case complexity;decision tree model;quantum complexity theory;worst-case complexity;asymptotic computational complexity;game complexity	Theory	13.26316494939951	19.448491043925358	124057
1a450e8d2dc3a382a3d24ea23a1108061a986e82	chain-splay trees, or, how to achieve and prove loglogn-competitiveness by splaying	splay trees;arbre recherche;arbre recherche binaire;procesamiento informacion;algorithm analysis;dynamique;05c05;competitividad;competitive algorithms;chain;68wxx;dinamica;algorithme competitif;recherche binaire;arbol investigacion;dynamics;arbol investigacion binaria;arbol binario;binary search tree;data structures;informatique theorique;chaine;estructura datos;recherche arborescence;arbre binaire;information processing;splay tree;competitiveness;68p05;competitive analysis;cadena;analyse algorithme;structure donnee;rotacion;traitement information;search tree;rotation;competitivite;data structure;analisis algoritmo;analyse competitive;dynamic optimization;computer theory;binary tree;informatica teorica	We present an extension of the splay technique, the chain-splay. Chain-splay trees 'splay' the accessed element to the root as classical splay trees do, but also perform some local 'house-keeping' splay operations below the accessed element. We prove that chain-splay is loglogN-competitive to any off-line algorithm that maintains a binary search tree with rotations. This result is the nearest point to the dynamic optimality that splay trees have reached since 1983.	splay tree	George F. Georgakopoulos	2008	Inf. Process. Lett.	10.1016/j.ipl.2007.10.001	optimal binary search tree;data structure;geometry of binary search trees;computer science;artificial intelligence;splay tree;mathematics;programming language;algorithm	DB	16.12666418099032	27.37322627237802	124147
071e17e11afb7fd7bda42bb79ee6204191175d7d	faster generation of random spanning trees	linear algebra;linear systems;graph theory;electrical flows;continuous linear algebraic method;spanning trees;random walks on graphs;electrical network;sparse graph;combinatorial partitioning technique;algorithmic graph theory random spanning tree undirected graph sparse graph discrete random walk based technique continuous linear algebraic method electrical network sparse linear system solver combinatorial partitioning technique;random variables;trees mathematics;electrical flows spanning trees random walks on graphs;undirected graph;manganese;electrical engineering and computer science;computational modeling;trajectory;thesis;computational complexity;tree graphs partitioning algorithms graph theory computer science linear systems sparse matrices random processes;random walk;random spanning tree;algorithmic graph theory;discrete random walk based technique;trees mathematics computational complexity linear algebra;spanning tree;markov processes;sparse linear system;sparse linear system solver	In this paper, we set forth a new algorithm for generating approximately uniformly random spanning trees in undirected graphs. We show how to sample from a distribution that is within a multiplicative $(1+\delta)$ of uniform in expected time $\TO(m\sqrt{n}\log 1/\delta)$. This improves the sparse graph case of the best previously known worst-case bound of $O(\min \{mn, n^{2.376}\})$, which has stood for twenty years. To achieve this goal, we exploit the connection between random walks on graphs and electrical networks, and we use this to introduce a new approach to the problem that integrates discrete random walk-based techniques with continuous linear algebraic methods. We believe that our use of electrical networks and sparse linear system solvers in conjunction with random walks and combinatorial partitioning techniques is a useful paradigm that will find further applications in algorithmic graph theory.	algorithm;average-case complexity;best, worst and average case;file spanning;graph (discrete mathematics);graph theory;linear algebra;linear system;programming paradigm;sparse graph code;sparse matrix	Jonathan A. Kelner;Aleksander Madry	2009	2009 50th Annual IEEE Symposium on Foundations of Computer Science	10.1109/FOCS.2009.75	random graph;mathematical optimization;combinatorics;discrete mathematics;spanning tree;graph theory;linear algebra;mathematics	Theory	19.782071546616653	22.727840922534156	124156
636796fbc01b5587610849b8db059617a490ef7f	steiner 2-edge connected subgraph polytopes on series-parallel graphs	arbre graphe;article accepte pour publication ou publie;graph theory;transportation network;red transporte;arbre steiner;subgrafo;tree graph;theorie graphe;series parallel graph;connected graph;politope;sous graphe;90c27;05c85;polytopes;subgraph;arbol grafo;steiner 2 edge connected subgraphs;steiner tree;graphe connexe;connected subgraphs;series parallel graphs;reseau transport;polytope;grafo conexo	Given a graph G = (V,E) with weights on its edges and a set of specified nodes S ⊆ V , the Steiner 2-edge survivable network problem is to find a minimum weight subgraph of G such that between every two nodes of S there are at least two edge-disjoint paths. This problem has applications to the design of reliable communication and transportation networks. In this paper, we give a complete linear description of the polytope associated with the solutions to this problem when the underlying graph is series-parallel. We also discuss related polyhedra.	directed graph;minimum weight;polyhedron;series-parallel graph;steiner tree problem	Mourad Baïou;Ali Ridha Mahjoub	1997	SIAM J. Discrete Math.	10.1137/S0895480193259813	polytope;factor-critical graph;combinatorics;discrete mathematics;topology;steiner tree problem;graph theory;subgraph isomorphism problem;graph factorization;mathematics;distance-hereditary graph;induced subgraph isomorphism problem	Theory	23.07124481070325	29.475791160505285	124241
1915b61e41519dea0008959159b2a2692352ff68	flow switching approach to the maximum flow problem: i	maximum flow;network flow	An alternate approach to the maximum flow problem, based upon the switching or red~strlbuUon of flows in the local neighborhoods of a succession of cuts, is presented The switching process redistributes the flow locally by tracing only locahzed dtrected paths and can augment flow along several paths from source to terminal These features appear to be advantageous for large and dense networks The concepts of the flow switching approach are presented m part I together with a Flow Switching Algorithm (FSA) and Its proof. Part II wdl present the computational comparison of the FSA and the labehng method	algorithm;computation;flow network;maximum flow problem;succession	Bharat Kinariwala;A. G. Rao	1977	J. ACM	10.1145/322033.322042	pipe network analysis;maximum flow problem;flow network;multi-commodity flow problem;computer science;mathematics;algorithm	Theory	20.795232029073254	24.437669766682845	124242
d8a847dbfafc2901a49df047fd7155a8bc774cbe	inverse max + sum spanning tree problem under hamming distance by modifying the sum-cost vector			file spanning;hamming distance;minimum spanning tree	Xiucui Guan;Xinyan He;Panos M. Pardalos;Binwu Zhang	2017	J. Global Optimization	10.1007/s10898-017-0546-5		Theory	23.461420932698594	26.668620763015426	124558
aabbe51198069feffdbb8bacd37e24ae1ff320b5	revisiting connected dominating sets: an optimal local algorithm?		In this paper we consider the classical Connected Dominating Set (CDS) problem. Twenty years ago, Guha and Khuller developed two algorithms for this problem - a centralized greedy approach with an approximation guarantee of $H(Delta)+2$ , and a local greedy approach with an approximation guarantee of $2 (H(Delta)+1)$ (where $H()$ is the harmonic function, and $Delta$ is the maximum degree in the graph). A local greedy algorithm uses significantly less information about the graph, and can be useful in a variety of contexts. However, a fundamental question remained - can we get a local greedy algorithm with the same performance guarantee as the global greedy algorithm without the penalty of the multiplicative factor of “2” in the approximation factor? In this paper, we answer that question in the affirmative.	local algorithm	Samir Khuller;Sheng Yang	2018		10.1109/ITA.2018.8503201	local algorithm;degree (graph theory);combinatorics;greedy coloring;dominating set;greedy randomized adaptive search procedure;multiplicative function;greedy algorithm;connected dominating set;mathematical optimization;mathematics	Theory	22.729505911298098	20.488606463693518	124600
a3d5dbb06f73ce19f446e0c00f3f3a159e5c6fbd	the sat-unsat transition in the adversarial sat problem		Adversarial SAT (AdSAT) is a generalization of the satisfiability (SAT) problem in which two players try to make a boolean formula true (resp. false) by controlling their respective sets of variables. AdSAT belongs to a higher complexity class in the polynomial hierarchy than SAT and therefore the nature of the critical region and the transition are not easily paralleled to those of SAT and worth of independent study. AdSAT also provides an upper bound for the transition threshold of the quantum satisfiability problem (QSAT). We present a complete algorithm for AdSAT, show that 2-AdSAT is in P, and then study two stochastic algorithms (simulated annealing and its improved variant) and compare their performances in detail for 3-AdSAT. Varying the density of clauses α we find a sharp SAT-UNSAT transition at a critical value whose upper bound is αc ∼ 1.5, thus providing a much stricter upper bound for the QSAT transition than those previously found.	algorithm;boolean satisfiability problem;complexity class;performance;polynomial hierarchy;simulated annealing;true quantified boolean formula	Marco Bardoscia;Daniel Nagaj;Antonello Scardicchio	2013	CoRR	10.1103/PhysRevE.89.032128	combinatorics;discrete mathematics;mathematics;algorithm	AI	11.861754683275255	18.783454093267846	124769
e71ca16c5886a0a6ba31272b3501e9abf73ce5ac	on derandomizing local distributed algorithms		The gap between the known randomized and deterministic local distributed algorithms underlies arguably the most fundamental and central open question in distributed graph algorithms. In this paper, we combine the method of conditional expectation with network decompositions to obtain a generic and clean recipe for derandomizing LOCAL algorithms. This leads to significant improvements on a number of problems, in cases resolving known open problems. Two main results are: - An improved deterministic distributed algorithm for hypergraph maximal matching, improving on Fischer, Ghaffari, and Kuhn [FOCS '17]. This yields improved algorithms for edge-coloring, maximum matching approximation, and low out-degree edge orientation. The last result gives the first positive resolution in the Open Problem 11.10 in the book of Barenboim and Elkin. - Improved randomized and deterministic distributed algorithms for the Lovász Local Lemma, which get closer to a conjecture of Chang and Pettie [FOCS '17].	approximation;directed graph;distributed algorithm;edge coloring;graph coloring;graph theory;matching (graph theory);maximal set;michael j. fischer;randomized algorithm;symposium on foundations of computer science	Mohsen Ghaffari;David G. Harris;Fabian Kuhn	2018	2018 IEEE 59th Annual Symposium on Foundations of Computer Science (FOCS)	10.1109/FOCS.2018.00069	combinatorics;hypergraph;open problem;lovász local lemma;distributed algorithm;discrete mathematics;matching (graph theory);conjecture;conditional expectation;mathematics;approximation algorithm;mathematical optimization	Theory	21.524226933537708	20.860908448265818	124796
a6bee6c6afd53530fa99ad334a15d60c44659e5f	edge-augmentation of hypergraphs	graph theory;hipergrafico;teoria grafo;aumentacion;metodo minimax;minimax method;augmentation;theorie graphe;optimisation combinatoire;graph connectivity;increase;conectividad grafo;methode minimax;hypergraph;combinatorial optimization;connectivite graphe;hypergraphe;optimizacion combinatoria	Let G be a connected hypergraph. We give a min-max relation for the minimum number of edges that are required to increase the connectivity by 1. We also show that the corresponding algorithmic problem can be solved in polynomial time.	maxima and minima;time complexity	Eddie Cheng	1999	Math. Program.	10.1007/s101070050032	mathematical optimization;combinatorics;discrete mathematics;combinatorial optimization;connectivity;graph theory;mathematics	Theory	21.828467346957193	27.056989885840967	124930
31ec9234187a9d3c60360437b14b36e6c419e963	an improved approximation algorithm for maximum edge 2-coloring in simple graphs	approximate algorithm;approximation algorithms;derandomization;maximum b matching;maximum edge t coloring;randomization	We present a polynomial-time approximation algorithm for legally coloring as many edges of a given simple graph as possible using two colors. It achieves an approximation ratio of 468 575 . This improves on the previous best (trivial) ratio of 45 .	approximation algorithm;color;graph (discrete mathematics);graph coloring;polynomial;time complexity	Zhi-Zhong Chen;Ruka Tanahashi;Lusheng Wang	2008	J. Discrete Algorithms	10.1016/j.jda.2007.08.002	randomization;mathematical optimization;combinatorics;discrete mathematics;fractional coloring;complete coloring;edge coloring;mathematics;minimax approximation algorithm;greedy coloring;approximation algorithm	Theory	23.698395639643767	22.327006393676523	124994
4acaac30d33e77a303cd45e780ac96a21809b07c	interval routing in some planar networks	esquema;camino mas corto;graph theory;embedding;intervalo;shortest path;internal routing;interval routing scheme;reseau communication;teoria grafo;plane;communication networks;graphe intervalle;subgrafo;interval graph;routing;bord;grafo intervalo;concepcion optimal;articulo;isometrie;temps lineaire;conception optimale;plan;routage;cell;plus court chemin;compacite;mensajeria;reseau;result;tiempo lineal;maillage;theorie graphe;plane quandragulation;design optimization;intervalle;messagerie;demi plan;rectilinearite;routage reseau;internal connection;red;network routing;schema;borde;grid;grafo;quadrangulation planaire;semiplano;interval;celdarada;triangulacion;half plane;isometria;edge;community networks;sous graphe;rejilla;graphe planaire;message handling;graph;graphe;linear time;plano;plongement;circuit;optimal design;grille;resultado;grid pattern;connexion interne;triangulation;resultat;compactness;rectilinearidad;interval routing;inmersion;subgraph;grafo planario;circuito;conexion interna;plane quadrangulation;cellule;isometry;red de comunicacion;article;scheme;communication network;rectilinearity;planar graph;celula;network;routage interne;compacidad;enrutamiento	In this article, we design optimal or near optimal interval routing schemes (IRS, for short) with small compactness for several classes of plane quadrangulations and triangulations (by optimality or near optimality we mean that messages are routed via shortest or almost shortest paths). We show that the subgraphs of the rectilinear grid bounded by simple circuits allow optimal IRS with at most two circular intervals per edge (2-IRS). We extend this result to all plane quadrangulations in which all inner vertices have degrees¿ 4. Namely, we establish that every such graph has an optimal IRS with at most seven linear intervals per edge (7-LIRS). This leads to a 7-LIRS with the stretch factor 2 for all plane triangulations in which all inner vertices have degrees¿ 6. All routing schemes can be implemented in linear time. c © 2002 Elsevier Science B.V. All rights reserved.	regular grid;routing;shortest path problem;time complexity;vertex (geometry)	Victor Chepoi;Alexis Rollin	2003	Theor. Comput. Sci.	10.1016/S0304-3975(02)00067-1	routing;combinatorics;topology;computer science;graph theory;mathematics;geometry	Theory	22.834079551423606	29.701106042527513	125304
06387edb384148d3575042da8aab955b71e9cfaf	mergeable dictionaries	amortized analysis.;data structures	A data structure is presented for the Mergeable Dictionary abstract data type, which supports the following operations on a collection of disjoint sets of totally ordered data: PredecessorSearch, Split and Merge. While Predecessor-Search and Split work in the normal way, the novel operation is Merge. While in a typical mergeable dictionary (e.g. 2-4 Trees), the Merge operation can only be performed on sets that span disjoint intervals in keyspace, the structure here has no such limitation, and permits the merging of arbitrarily interleaved sets. Tarjan and Brown present a data structure [4] which can handle arbitrary Merge operations in O(log n) amortized time per operation if the set of operations is restricted to exclude the Split operation. In the presence of Split operations, the amortized time complexity of their structure becomes Ω(n). A data structure which supports both Split and Merge operations in O(log n) amortized time per operation was given by Farach and Thorup [6]. In contrast, our data structure supports all operations, including Split and Merge, in O(log n) amortized time, thus showing that interleaved Merge operations can be supported at no additional cost vis-à-vis disjoint Merge operations.	abstract data type;amortized analysis;data structure;dictionary;time complexity	John Iacono;Özgür Özkan	2010		10.1007/978-3-642-14165-2_15	computer science;theoretical computer science;database;merge algorithm;algorithm;polyphase merge sort	Theory	14.16590652651948	28.547086795766525	125417
3f33571a7bf53638083c2bdb040f894cce991ae0	on the intercluster distance of a tree metric	distance minimale;approximate algorithm;metrique arbre;routing;approximation algorithm;arbre maximal;routage;average distance;tree metric;minimal distance;optimization problem;arbol maximo;probleme optimisation;informatique theorique;graph;algoritmo aproximacion;spanning tree;algorithme approximation;distancia minima;computer theory;informatica teorica;enrutamiento	For two vertex clusters of a tree metric, we show that the sum of the average intracluster distances is always less than or equal to twice of the average intercluster distance. We show the feature in a more general form of weighted distance. This feature provides a 2-approximation algorithm for the minimum average intercluster distance spanning tree problem, which is a generalization of the minimum routing cost spanning tree or minimum average distance spanning tree problem. The results in this paper can be further generalized to more than two clusters. © 2006 Elsevier B.V. All rights reserved.	algorithm;file spanning;minimum spanning tree;routing;steiner tree problem	Bang Ye Wu	2006	Theor. Comput. Sci.	10.1016/j.tcs.2006.07.056	euclidean minimum spanning tree;optimization problem;mathematical optimization;routing;combinatorics;kruskal's algorithm;topology;vantage-point tree;minimum degree spanning tree;spanning tree;prim's algorithm;computer science;minimum spanning tree;interval tree;k-minimum spanning tree;mathematics;graph;distributed minimum spanning tree;approximation algorithm;shortest-path tree	Theory	22.0771634987838	28.57627622420206	125514
2c96b6e62a0075407d5df03d08f290c5a8eaea29	p systems with local graph productions	graph grammar;graph products;p system;directed graph;membrane computing;recursively enumerable language	P systems (membrane systems) of various types so far mainly have been considered as computing devices working on multisets or strings. In this paper we investigate P systems with local graph productions generating weakly connected directed graphs. At least when equipped with a priority relation on the rules, such P systems can generate any recursively enumerable language of weakly connected directed graphs with only one membrane.	connectivity (graph theory);directed graph;p system;recursively enumerable language;string (computer science)	Rudolf Freund;Marion Oswald	2004	New Generation Computing	10.1007/BF03037287	block graph;wait-for graph;directed graph;null graph;graph property;computer science;regular graph;membrane computing;clique-width;comparability graph;aperiodic graph;symmetric graph;voltage graph;distance-hereditary graph;graph;modular decomposition;vertex-transitive graph;complement graph;strongly connected component;recursively enumerable language;line graph;algorithm;string graph;coxeter graph;p system	Theory	21.87857535627134	31.47935546247003	125633
45c6467fcb06f18a267b1f24de10c5bc080a3578	a slice theoretic approach for embedding problems on digraphs		We say that a digraph H can be covered by k paths if there exist k directed paths $$\\mathfrak {p}_1,\\mathfrak {p}_2,\\ldots ,\\mathfrak {p}_k$$ such that $$H=\\cup _{i=1}^k \\mathfrak {p}_i$$. In this work we devise parameterized algorithms for embedding problems on digraphs in the setting in which the host digraph G has directed pathwidth w and the pattern digraph H can be covered by k paths. More precisely, we show that the subgraph isomorphism, subgraph homeomorphism, and two other related embedding problems can each be solved in time $$2^{Ok\\cdot w \\log k\\cdot w} \\cdot |H|^{Ok\\cdot w}\\cdot |G|^{Ok\\cdot w}$$. We note in particular that for constant values of w and k, our algorithm runs in polynomial time with respect to the size of the pattern digraph H. Therefore for the classes of digraphs considered in this work our results yield an exponential speedup with respect to the best general algorithm for the subgraph isomorphism problem which runs in time $$O^*2^{|H|}\\cdot |G|^{ tw H}$$ where $$ tw H$$ is the undirected treewidth of H, and an exponential speedup with respect to the best general algorithm for the subgraph homeomorphism problem which runs in time $$|G|^{O|H|}$$.	directed graph	Mateus de Oliveira Oliveira	2015		10.1007/978-3-662-53174-7_26	topology	Theory	21.628656608462283	23.266333892817485	125650
b68bd320465b8832733e2fd72a052b6f3cec2d7c	on supergraphs satisfying cmso properties		Let CMSO denote the counting monadic second order logic of graphs. We give a constructive proof that for some computable function f , there is an algorithm A that takes as input a CMSO sentence φ, a positive integer t, and a connected graph G of maximum degree at most ∆, and determines, in time f(|φ|, t) · 2O(∆·t) · |G|O(t), whether G has a supergraph G′ of treewidth at most t such that G′ |= φ. The algorithmic metatheorem described above sheds new light on certain unresolved questions within the framework of graph completion algorithms. In particular, using this metatheorem, we provide an explicit algorithm that determines, in time f(d) ·2O(∆·d) · |G|O(d), whether a connected graph of maximum degree ∆ has a planar supergraph of diameter at most d. Additionally, we show that for each fixed k, the problem of determining whether G has an k-outerplanar supergraph of diameter at most d is strongly uniformly fixed parameter tractable with respect to the parameter d. This result can be generalized in two directions. First, the diameter parameter can be replaced by any contraction-closed effectively CMSO-definable parameter p. Examples of such parameters are vertex-cover number, dominating number, and many other contraction-bidimensional parameters. In the second direction, the planarity requirement can be relaxed to bounded genus, and more generally, to bounded local treewidth. 1998 ACM Subject Classification F.2 Analysis of Algorithms and Problem Complexity, F.4 Mathematical Logic and Formal Languages	algorithm;analysis of algorithms;cobham's thesis;computable function;connectivity (graph theory);linear temporal logic to büchi automaton;logic of graphs;monadic predicate calculus;planar graph;treewidth;vertex cover	Mateus de Oliveira Oliveira	2017		10.4230/LIPIcs.CSL.2017.33	connectivity;discrete mathematics;planarity testing;combinatorics;degree (graph theory);computer science;metatheorem;computable function;treewidth;bounded function;epigraph	Theory	22.0446924841452	23.821757343502334	125756
44cdfa3d8635f305da85d75e256b8294c38c7998	on the enumeration of tree decompositions		Many intractable computational problems on graphs admit tractable algorithms when applied to trees or forests. Tree decomposition extracts a tree structure from a graph by grouping nodes into bags, where each bag corresponds to a single node in of the tree. The corresponding operation on hypergraphs is that of a generalized hypertree decomposition [10], which entails a tree decomposition of the primal graph (which has the same set of nodes, and an edge between every two nodes that co-occur in a hyperedge) and an assignment of a hyperedge cover to each bag [11]. Tree decomposition and generalized hypertree decomposition have a plethora of applications, including join optimization in databases [7, 10, 21], constraint-satisfaction problems [17], computation of Nash equilibria in games [10], analysis of probabilistic graphical models [18], and weighted model counting [16,19]. Past research has focused on obtaining a “good” tree decomposition for the given graph, where goodness is typically measured by means of the width—the maximal cardinality of a bag. Nevertheless, finding a tree decomposition of a minimal width is NP-hard [2]. Moreover, in various applications the measure of goodness is different from (though related to) the width [11,16]. Abseher et al. [1] empirically showed that the execution cost of dynamic programming algorithms over a tree decomposition is highly sensitive to features of the tree decomposition other than mere width; in particular, tree decompositions of the same width may entail highly diverging running times on the same problem instance. In this paper, we describe our ongoing effort on the task of enumerating all (or a subset of) the tree decompositions of a graph. Such algorithms have been proposed in the past for small graphs (representing database queries), without complexity guarantees [15, 21]. Our main result so far is an enumeration algorithm that runs in incremental polynomial time, and our current efforts are on a practical and effective implementation.	algorithm;cobham's thesis;computation;computational problem;database;decomposition method (constraint satisfaction);dynamic programming;graph (discrete mathematics);graphical model;mathematical optimization;maximal set;np-hardness;nash equilibrium;time complexity;tree decomposition;tree structure	Nofar Carmeli;Batya Kenig;Benny Kimelfeld	2016			combinatorics;graph enumeration;enumeration;mathematics	ML	22.051340192994385	22.67789686488394	125887
eebebb67f0755181c7484b8aaf15ddd291f7051a	function matching: algorithms, applications, and a lower bound	text;color index;function matching;register allocation;convolution;efficient algorithm;localization;convolucion;plan randomise;localizacion;texte;probabilistic approach;approche deterministe;deterministic approach;aleatorizacion;plan aleatorizado;localisation;modelo 2 dimensiones;randomized design;pattern matching;enfoque probabilista;approche probabiliste;parameterized matching;enfoque determinista;borne inferieure;modele 2 dimensions;randomized algorithm;randomisation;protein folding;false positive;randomization;texto;lower bound;two dimensional model;cota inferior;color indexing	We introduce a new matching criterion – function matching – that captures several different applications. The function matching problem has as its input a text T of length n over alphabet ΣT and a pattern P = P [1]P [2] · · · P [m] of length m over alphabet ΣP . We seek all text locations i for which, for some function f : ΣP → ΣT (f may also depend on i), the m-length substring that starts at i is equal to f(P [1])f(P [2]) · · · f(P [m]). We give a randomized algorithm which, for any given constant k, solves the function matching problem in time O(n log n) with probability 1 nk of declaring a false positive. We give a deterministic algorithm whose time is O(n|ΣP | log m) and show that it is almost optimal in the newly formalized convolutions model. Finally, a variant of the third problem is solved by means of two-dimensional parameterized matching, for which we also give an efficient algorithm.	convolution;deterministic algorithm;loss function;parameterized complexity;randomized algorithm;substring	Amihood Amir;Yonatan Aumann;Richard Cole;Moshe Lewenstein;Ely Porat	2003		10.1007/3-540-45061-0_72	randomization;protein folding;combinatorics;discrete mathematics;internationalization and localization;type i and type ii errors;computer science;artificial intelligence;machine learning;pattern matching;color index;mathematics;convolution;upper and lower bounds;randomized algorithm;deterministic system;completely randomized design;register allocation;algorithm;statistics	Theory	14.721250729610764	26.078466705744678	125950
9556717dec56b7f93308ba50c9abaedceb811df2	parallel algorithms for the minimum cut and the minimum length tree layout problems	parallel algorithm;linear arrangement;minimum cut	The minimum cut and minimum length linear arrangement problems usually occur in solving wiring problems and have a lot in common with job sequencing questions. Both problems are NP-complete for general graphs and in P for trees. We present here two parallel algorithms for the CREW PRAM. The rst solves the minimum length linear arrangement problem for trees and the second solves the minimum cut arrangement for trees. We prove that the rst problem belongs to NC for trees, and the second problem is in NC for bounded degree trees. To the best of our knowledge, these are the rst parallel algorithms for the minimum length and the minimum cut linear arrangement problems.	job shop scheduling;minimum cut;nc (complexity);np-completeness;parallel algorithm;wiring	Josep Díaz;Alan Gibbons;Grammati E. Pantziou;Maria J. Serna;Paul G. Spirakis;Jacobo Torán	1997	Theor. Comput. Sci.	10.1016/S0304-3975(96)00274-5	mathematical optimization;combinatorics;minimum cut;computer science;gomory–hu tree;mathematics;parallel algorithm;algorithm;minimum k-cut	Theory	24.2722170868376	22.96208333312466	126051
54574d5193b3e372bab10ae05fab9e2d5f114eab	wavelength conversion in shortest-path all-optical networks	shortest path	We consider all-optical networks with shortest-path routing that use wavelengthdivision multiplexing and employ wavelength conversion at specific nodes in order to maximize their capacity usage. We present efficient algorithms for deciding whether a placement of wavelength converters allows the network to run at maximum capacity, and for finding an optimal wavelength assignment when such a placement of converters is known. Our algorithms apply to both undirected and directed networks. Furthermore, we show that the problem of finding an optimal placement of converters is MAX SNP-hard in both undirected and directed networks. Finally, we give a linear-time algorithm for finding an optimal placement of converters in undirected triangle-free networks, and show that the problem remains NP-hard in bidirected triangle-free planar networks.	algorithm;apollonian network;graph (discrete mathematics);max;np-hardness;routing;snp (complexity);shortest path problem;time complexity;wavelength-division multiplexing	Thomas Erlebach;Stamatis Stefanakos	2003		10.1007/978-3-540-24587-2_61	decidability;time complexity;optical path;routing;telecommunications;image processing;computer science;np-hard;wavelength;speech processing;mathematics;shortest path problem;algorithm;telecommunications network;wavelength-division multiplexing	Theory	24.556423841209156	22.871033933338786	126339
c7c04537278c1899ab23e52743bf41f4d7b5a6bb	average case analysis of five two-dimensional bubble sorting algorithms	average case analysis;sorting algorithm	For each of five generalizations of the odcl-even transposition sort to a sorting algorithm on a /77X W mesh of processors. we demonstrate that with “high probability,” the number of steps required to sort a random permutation of N numbers is @(N).	best, worst and average case;central processing unit;random permutation;sorting algorithm	Serap A. Savari	1993		10.1145/165231.157381	computer science;sorting algorithm;programming language;algorithm	Theory	12.100025862711485	31.497260437269954	126501
baebdca65ed477b261015fe6a0503fdd48f7714d	an efficient heuristic to identify threshold logic functions	linear separable functions;threshold logic gate;threshold functions	A fast method to identify the given Boolean function as a threshold function with weight assignment is introduced. It characterizes the function based on the parameters that have been defined in the literature. The proposed method is capable to quickly characterize all functions that have less than eight inputs and has been shown to operate fast for functions with as many as forty inputs. Furthermore, comparisons with other existing heuristic methods show huge increase in the number of threshold functions identified, and drastic reduction in time and complexity.	heuristic	Ashok Kumar Palaniswamy;Spyros Tragoudas	2012	JETC	10.1145/2287696.2287702	algorithm	EDA	15.306190001732547	21.698742905700584	126706
26193dd9b4f179527f4bc1a2de77055b9b26153b	planar l-drawings of directed graphs		We study planar drawings of directed graphs in the L-drawing standard. We provide necessary conditions for the existence of these drawings and show that testing for the existence of a planar L-drawing is an NP-complete problem. Motivated by this result, we focus on upwardplanar L-drawings. We show that directed st-graphs admitting an upward(resp. upward-rightward-) planar L-drawing are exactly those admitting a bitonic (resp. monotonically increasing) st-ordering. We give a lineartime algorithm that computes a bitonic (resp. monotonically increasing) st-ordering of a planar st-graph or reports that there exists none.	algorithm;directed graph;np-completeness;planar graph	Steven Chaplick;Markus Chimani;Sabine Cornelsen;Giordano Da Lozzo;Martin Nöllenburg;Maurizio Patrignani;Ioannis G. Tollis;Alexander Wolff	2017		10.1007/978-3-319-73915-1_36	discrete mathematics;combinatorics;directed graph;mathematics;existential quantification;planar straight-line graph;monotonic function;graph;planar	Theory	23.075839711514277	24.702978282083315	126721
a57e92e66f6e1e7b57010b92f14cd34227056eff	hardness of approximating the shortest vector problem in lattices	lattice theory;approximate algorithm;shortest vector problem;bch codes;randomised algorithms;lattices tensile stress polynomials paper technology history books gaussian processes geometry linear programming computer science;analysis of algorithm;polynomial time algorithm;approximation theory;theory of computing;computational complexity;problem complexity;ny;hardness factor shortest vector problem lattices np hard problem randomized reduction closest vector problem bch codes augmented tensor product;hardness of approximation;lattice theory computational complexity randomised algorithms approximation theory tensors bch codes;tensors	Let p > 1 be any fixed real. We show that assuming NP /spl nsube/ RP, it is hard to approximate the shortest vector problem (SVP) in l/sub p/ norm within an arbitrarily large constant factor. Under the stronger assumption NP /spl nsube/ RTIME(2/sup poly(log n)/), we show that the problem is hard to approximate within factor 2/sup log n1/2 - /spl epsi// where n is the dimension of the lattice and /spl epsi/> 0 is an arbitrarily small constant. This greatly improves all previous results in l/sub p/ norms with 1 < p < /spl infin/. The best results so far gave only a constant factor hardness, namely, 2/sup 1/p/ - /spl epsi/ by Micciancio and p/sup 1 - /spl epsi// in high l/sub p/ norms by Khot. We first give a new (randomized) reduction from closest vector problem (CVP) to SVP that achieves some constant factor hardness. The reduction is based on BCH codes. Its advantage is that the SVP instances produced by the reduction behave well under the augmented tensor product, a new variant of tensor product that we introduce. This enables us to boost the hardness factor to 2/sup log n1/2-/spl epsi//.	lattice problem	Subhash Khot	2004		10.1109/FOCS.2004.31	mathematical optimization;combinatorics;discrete mathematics;tensor;computer science;lattice;mathematics;hardness of approximation;bch code;computational complexity theory;algorithm;lattice problem;algebra;approximation theory	Theory	11.731190815802131	23.122677677336043	126852
f77a262cc7cf11f3f53abe05f5a53a3994b1fc3c	a tree-based approach for computing double-base chains	complexity analysis;elliptic curve cryptography;double base number system;scalar multiplication	We introduce a tree-based method to find short Double-Base chains. As compared to the classical greedy approach, this new method is not only simpler to implement and faster, experimentally it also returns shorter chains on average. The complexity analysis shows that the average length of a chain returned by this tree-based approach is log2 n 4.6419 · This tends to suggest that the average length of DB-chains generated by the greedy approach is not O(log n/ log log n). We also discuss generalizations of this method, namely to compute Step Multi-Base Representation chains involving more than 2 bases and extended DB-chains having nontrivial coefficients.	analysis of algorithms;binary logarithm;coefficient;experiment;greedy algorithm	Christophe Doche;Laurent Habsieger	2008		10.1007/978-3-540-70500-0_32	mathematical optimization;combinatorics;discrete mathematics;computer science;scalar multiplication;mathematics;elliptic curve cryptography;computer security;algorithm	Logic	14.00787516629092	23.80998846712168	126864
97259404cc0880f5f92f6f36e04a0a038ef8d59e	can rare sat formulas be easily recognized? on the efficiency of message passing algorithms for k-sat at large clause-to-variable ratios	satisfiability;phase transition;polynomial time;critical value;statistical mechanics;random graph;statistical physics;computational complexity;hardness of approximation;uniform distribution;variable ratio	For large clause-to-variable ratios, typical K-SAT instances drawn from the uniform distribution have no solution. We argue, based on statistical mechanics calculations using the replica and cavity methods, that rare satisfiable instances from the uniform distribution are very similar to typical instances drawn from the so-called planted distribution, where instances are chosen uniformly between the ones that admit a given solution. It then follows, from a recent article by Feige, Mossel and Vilenchik (2006 Complete convergence of message passing algorithms for some satisfiability problems Proc. Random 2006 pp 339–50), that these rare instances can be easily recognized (in O(log N) time and with probability close to 1) by a simple message-passing algorithm. PACS numbers: 89.20.Ff, 05.70.Fh, 75.10.Nr	algorithm;boolean satisfiability problem;message passing;picture archiving and communication system	Fabrizio Altarelli;Rémi Monasson;Francesco Zamponi	2006	CoRR		phase transition;time complexity;random graph;reinforcement;combinatorics;discrete mathematics;critical value;statistical mechanics;probability;mathematics;uniform distribution;hardness of approximation;computational complexity theory;algorithm;satisfiability	Theory	11.502131182673892	19.62348731104338	126940
01f5f614d4fc8ab95e144cfbd1ea9b426b78f243	lp rounding for k-centers with non-uniform hard capacities	lp rounding approximation algorithms k center non uniform capacities hard capacities;hard capacities;graph theory;graph theory approximation theory computational complexity facility location;approximation algorithms;approximation theory;k center;computational complexity;lp rounding;facility location problem lp rounding approach classical k center problem nonuniform hard capacity graph capacity constraints maximum distance minimization np hard problem optimal factor 2 approximation algorithms general capacity function first constant factor approximation algorithm nonuniform soft capacity 11 approximation algorithm;non uniform capacities;facility location;approximation methods approximation algorithms polynomials awards activities standards educational institutions computer science	In this paper we consider a generalization of the classical k-center problem with capacities. Our goal is to select k centers in a graph, and assign each node to a nearby center, so that we respect the capacity constraints on centers. The objective is to minimize the maximum distance a node has to travel to get to its assigned center. This problem is NP-hard, even when centers have no capacity restrictions and optimal factor 2 approximation algorithms are known. With capacities, when all centers have identical capacities, a 6 approximation is known with no better lower bounds than for the infinite capacity version. While many generalizations and variations of this problem have been studied extensively, no progress was made on the capacitated version for a general capacity function. We develop the first constant factor approximation algorithm for this problem. Our algorithm uses an LP rounding approach to solve this problem, and works for the case of non-uniform hard capacities, when multiple copies of a node may not be chosen and can be extended to the case when there is a hard bound on the number of copies of a node that may be selected. Finally, for non-uniform soft capacities we present a much simpler 11-approximation algorithm, which we find as one more evidence that hard capacities are much harder to deal with.	apx;approximation algorithm;facility location problem;graph (discrete mathematics);linear programming relaxation;metric k-center;np-hardness;rounding	Marek Cygan;Mohammad Taghi Hajiaghayi;Samir Khuller	2012	2012 IEEE 53rd Annual Symposium on Foundations of Computer Science	10.1109/FOCS.2012.63	mathematical optimization;combinatorics;discrete mathematics;graph theory;facility location problem;mathematics;computational complexity theory;approximation algorithm;algorithm;approximation theory	Theory	24.02641243185334	19.693459893692953	127011
5e379d1e2679995fcdb6ddbd58a1a9c0c5ea800a	the constant inapproximability of the parameterized dominating set problem	fpt inapproximability;dominating set;w 1;eth	We prove that there is no fpt-algorithm that can approximate the dominating set problem with any constant ratio, unless FPT = W[1]. Our hardness reduction is built on the second author's recent W[1]-hardness proof of the biclique problem [25]. This yields, among other things, a proof without the PCP machinery that the classical dominating set problem has no polynomial time constant approximation under the exponential time hypothesis.	approximation algorithm;dominating set;exptime;exponential time hypothesis;hardness of approximation;parameterized complexity;polynomial;reduction (complexity);time complexity	Yijia Chen;Bingkai Lin	2016	2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS)	10.1109/FOCS.2016.61	combinatorics;discrete mathematics;dominating set;mathematics;algorithm	Theory	17.41215119422711	20.05803775467357	127053
bb4930c8ef784a819d9f50b63b7abcb14054da17	nearest neighbor representations of boolean functions		Lower and upper bounds are given for the number of prototypes required for various nearest neighbor representations of Boolean functions.		Péter Hajnal;Zhihao Liu;György Turán	2006			combinatorics;boolean function;nearest-neighbor chain algorithm;k-nearest neighbors algorithm;nearest neighbor graph;mathematics;nearest neighbor search	Theory	16.550055336899515	29.149846391026948	127075
3acf8a01590d24b7df2a114169e78d5a453901cd	the max-shift algorithm for approximate string matching	dynamic programming;alignement;location problem;approximate string matching;programacion dinamica;probleme localisation;edit distance;dynamic program;codificacion;pattern matching;coding;programmation dynamique;alineamiento;problema localizacion;concordance forme;algorithme corde;alignment;codage	1 Dept. of Computer Science, King’s College London London WC2R 2LS, England 2 School of Computing, Curtin University of Technology GPO Box 1987 U, WA. {csi,pinzon}@dcs.kcl.ac.uk www.dcs.kcl.ac.uk/staff/csi, www.dcs.kcl.ac.uk/pg/pinzon 3 ESA 6037: Dept. of Vegetal Physiology ABISS, Université de Rouen 76821 Mont Saint Aignan Cedex, France 4 School of Computing, Curtin University of Technology GPO Box 1987 U, WA. lm@dir.univ-rouen.fr www.dir.univ-rouen.fr/~lm	algorithm;approximate string matching;approximation algorithm;computer science;esa;group policy;pattern matching;preprocessor;school of computing (robert gordon university)	Costas S. Iliopoulos;Laurent Mouchard;Yoan J. Pinzón	2001		10.1007/3-540-44688-5_2	combinatorics;edit distance;approximate string matching;commentz-walter algorithm;computer science;substring;theoretical computer science;machine learning;dynamic programming;pattern matching;mathematics;coding;programming language;bitap algorithm;algorithm	Theory	17.577756611103247	26.84062627712254	127153
3f9b99c42122b58b1b8916bc6e964bba145d1f4b	minimum weight convex steiner partitions	distributed system;metodo caso peor;problema arbol steiner;equi partition;network design;euclidean theory;systeme reparti;fairness;minimal spanning tree;online scheduling;convex partition;probleme arbre steiner;coeficiente dilatacion;non clairvoyant algorithm;coefficient of expansion;sistema repartido;euclidean minimum spanning tree;particion;graphe planaire;coefficient dilatation;stretch factor;grafo linea;partition;theorie euclidienne;methode cas pire;steiner tree problem;grafo planario;line graph;graphe ligne;worst case method;precedences;arbre maximal minimal;planar graph;teoria euclidiana;steiner points	New tight bounds are presented on the minimum length of planar straight line graphs connecting n given points in the plane and having convex faces. Specifically, we show that the convex Steiner partition of n points in the plane is at most O(log n/log log n) times longer than their Euclidean minimum spanning tree (EMST), and this bound is best possible. Without allowing Steiner points, the corresponding bound is known to be Θ(log n), attained for n points lying along a pseudo-triangle. We also show that the convex Steiner partition of n points along a pseudo-triangle is at most O(log log n) times longer than the EMST, and this bound is also best possible. Our methods are constructive and lead to polynomial-time algorithms for computing convex Steiner partitions within these bounds in both cases.	algorithm;file spanning;minimum spanning tree;minimum weight;polynomial;pseudotriangle;steiner tree problem;time complexity	Adrian Dumitrescu;Csaba D. Tóth	2008	Algorithmica	10.1007/s00453-009-9329-9	partition;thermal expansion;euclidean minimum spanning tree;mathematical optimization;network planning and design;combinatorics;topology;convex combination;steiner tree problem;minimum spanning tree;mathematics;geometry;convex set;line graph;planar graph	Theory	21.46488351995254	27.2920440705089	127191
20b94be09db05677d9a530cd530f51c70742740b	on-line maximum-order induces hereditary subgraph problems	article accepte pour publication ou publie;competition;algorithm complexity;algorithm analysis;heredity;approximation algorithms;information retrieval;complejidad algoritmo;approximation algorithm;graph clique;satisfiability;communication conference;on line algorithms;maximum clique;complexite algorithme;recherche information;informatique theorique;analyse algorithme;hereditary property;recuperacion informacion;clique graphe;algorithme approximation;analisis algoritmo;competitive ratio;computer theory;maximum independent set;informatica teorica	We first study the competitivity ratio for the on-line version of the problem of finding a maximum-order induced subgraph satisfying some hereditary property, under the hypothesis that the input graph is revealed by clusters. Then, we focus ourselves on two of the most known instantiations of this problem, the maximum independent set and the maximum clique.		Marc Demange;Xavier Paradon;Vangelis Th. Paschos	2000		10.1007/3-540-44411-4_21	mathematical optimization;combinatorics;competition;computer science;heredity;subgraph isomorphism problem;mathematics;induced subgraph isomorphism problem;maximum common subgraph isomorphism problem;approximation algorithm;algorithm;satisfiability	Theory	18.50761028805547	25.520135230754498	127275
92336b787316f4d79d7cc0dcbc12962137083710	anderson localization casts clouds over adiabatic quantum optimization	statistical mechanics;quantum computer;anderson localization;np complete problem;local minima;excited states	Understanding NP-complete problems is a central topic in computer science. This is why adiabatic quantum optimization has attracted so much attention, as it provided a new approach to tackle NP-complete problems using a quantum computer. The efficiency of this approach is limited by small spectral gaps between the ground and excited states of the quantum computer's Hamiltonian. We show that the statistics of the gaps can be analyzed in a novel way, borrowed from the study of quantum disordered systems in statistical mechanics. It turns out that due to a phenomenon similar to Anderson localization, exponentially small gaps appear close to the end of the adiabatic algorithm for large random instances of NP-complete problems. This implies that unfortunately, adiabatic quantum optimization fails: the system gets trapped in one of the numerous local minima.	adiabatic quantum computation;mathematical optimization;tag cloud	Boris Altshuler;Hari Krovi;Jérémie Roland	2009	CoRR			Robotics	12.35920259675428	18.73358159245907	127476
21394b2d6d8443e11decda5dbaf6394d96218a2b	randomized parallel list ranking for distributed memory multiprocessors	distributed memory;ranking algorithm;local computation	We present a randomized parallel list ranking algorithm for distributed memory multiprocessors. A simple version requires, with high probability, log(3p) + log ln(n) = ~ O(log p + log log n) communication rounds (h-relations with h = ~ O( p )) and ~ O( p ) local computation. An improved version requires, with high probability, only r (4k + 6) log( 2 3 p) + 8 = ~ O(k log p) communication rounds where k = minfi 0j ln n ( 2 3 p)g. Note that k < ln (n) is an extremely small number. For n 10 100 and p 4, the value of k is at most 2. For a given number of processors, p, the number of communication rounds required is, for all practical purposes, independent of n. For n 10 100 and 4 p 2048, the number of communication rounds in our algorithm is bounded, with high probability, by 118. We conjecture that the actual number of communications rounds will not exceed 50.	central processing unit;computation;distributed memory;list ranking;randomized algorithm;with high probability	Frank Dehne;Siang Wun Song	1996		10.1007/BFb0027774	distributed shared memory;parallel computing;computer science;theoretical computer science;distributed computing	Theory	11.732928851758	31.354581666284645	127529
88377380ad5a795a83bda2177b05165517782c37	answering n2+o(1) counting queries with differential privacy is hard	traitor tracing;68q17;cryptography;68q25;differential privacy	A central problem in differentially private data analysis is how to design efficient algorithms capable of answering large numbers of counting queries on a sensitive database. Counting queries are of the form “What fraction of individual records in the database satisfy the property $q$?” We prove that if one-way functions exist, then there is no algorithm that takes as input a database $D \in (\{0,1\}^d)^n$, and $k = \tilde{\Theta}(n^2)$ arbitrary efficiently computable counting queries, runs in time $\mathrm{poly}(d, n)$, and returns an approximate answer to each query, while satisfying differential privacy. We also consider the complexity of answering “simple” counting queries, and make some progress in this direction by showing that the above result holds even when we require that the queries are computable by constant-depth $(AC^0)$ circuits. Our result is almost tight because it is known that $\tilde{\Omega}(n^2)$ counting queries can be answered efficiently while satisfying differential privacy. Mor...	differential privacy	Jonathan Ullman	2016	SIAM J. Comput.	10.1137/130928121	computer science;cryptography;theoretical computer science;data mining;database;mathematics;differential privacy	Theory	12.59708541238246	24.269731750409445	128106
c493a544a5de767eeac6f1df9cd9b422eabd41e5	immediate parallel solution of the longest common subsequence problem.	longest common subsequence		longest common subsequence problem	David M. Champion;Jerome Rothstein	1987			longest increasing subsequence;computer science;longest common subsequence problem;longest alternating subsequence	Theory	13.410464044414297	27.547037002390574	128108
2b8243a9dcf58e6c864dab92e4342ffdf0e9059f	an optimal minimum spanning tree algorithm	decision tree;linear time;minimum spanning tree;graph algorithm;spanning tree;optimal complexity;graph algorithms	We establish that the algorithmic complexity of the minimumspanning tree problem is equal to its decision-tree complexity.Specifically, we present a deterministic algorithm to find aminimum spanning tree of a graph with <i>n</i> vertices and<i>m</i> edges that runs in time<i>O</i>(<i>T</i><sup>*</sup>(<i>m,n</i>)) where<i>T</i><sup>*</sup> is the minimum number of edge-weightcomparisons needed to determine the solution. The algorithm isquite simple and can be implemented on a pointer machine.Althoughour time bound is optimal, the exact function describing it is notknown at present. The current best bounds known for<i>T</i><sup>*</sup> are <i>T</i><sup>*</sup>(<i>m,n</i>) =Ω(<i>m</i>) and <i>T</i><sup>*</sup>(<i>m,n</i>) =<i>O</i>(<i>m</i> ∙ α(<i>m,n</i>)), where α is acertain natural inverse of Ackermann's function.Even under theassumption that <i>T</i><sup>*</sup> is superlinear, we show thatif the input graph is selected from <i>G</i><sub><i>n,m</i></sub>,our algorithm runs in linear time with high probability, regardlessof <i>n</i>, <i>m</i>, or the permutation of edge weights. Theanalysis uses a new martingale for <i>G</i><sub><i>n,m</i></sub>similar to the edge-exposure martingale for<i>G</i><sub><i>n,p</i></sub>.	ackermann function;computational complexity theory;decision tree model;deterministic algorithm;edge dominating set;file spanning;graph theory;list of algorithms;minimum spanning tree;pointer machine;time complexity;with high probability	Seth Pettie;Vijaya Ramachandran	2002	J. ACM	10.1145/505241.505243	time complexity;euclidean minimum spanning tree;mathematical optimization;combinatorics;discrete mathematics;kruskal's algorithm;minimum degree spanning tree;spanning tree;prim's algorithm;computer science;minimum spanning tree;decision tree;gomory–hu tree;k-ary tree;k-minimum spanning tree;mathematics;reverse-delete algorithm;distributed minimum spanning tree;algorithm;shortest-path tree	Theory	19.774392418858888	25.52906939817123	128284
7b80deb807ff6463818dfc79994cb22b1f46d012	approximating the minimum clique cover and other hard problems in subtree filament graphs	grafo triangular;circle graph;approximate algorithm;graphe intervalle;combinatorics;temps polynomial;circular arc graph;interval graph;probleme np complet;combinatoria;grafo intervalo;approximation algorithm;graphe cordal;combinatoire;polygone;graph clique;polygon;informatique theorique;graphe intersection;vertex graph;algoritmo aproximacion;polynomial time;graphe triangule;graph algorithm;recouvrement clique;poligono;graphe cercle;np complete;problema np completo;clique graphe;intersection graphs;covering problem;algorithme graphe;algorithme approximation;subtree filament graph;graphe filament sous arbre;vertice grafo;sommet graphe;np complete problem;graphe arc circulaire;computer theory;chordal graph;tiempo polinomial;informatica teorica;clique cover	Subtree filament graphs are the intersection graphs of subtree filaments in a tree. This class of graphs contains subtree overlap graphs, interval filament graphs, chordal graphs, circle graphs, circular-arc graphs, cocomparability graphs, and polygon-circle graphs. In this paper we show that, for circle graphs, the clique cover problem is NP-complete and the h-clique cover problem for fixed h is solvable in polynomial time. We then present a general scheme for developing approximation algorithms for subtree filament graphs, and give approximation algorithms developed from the scheme for the following problems which are NP-complete on circle graphs and therefore on subtree filament graphs: clique cover, vertex colouring, maximum k-colourable subgraph, and maximum h-coverable subgraph. © 2006 Elsevier B.V. All rights reserved.	approximation algorithm;clique cover;decision problem;graph (discrete mathematics);graph coloring;np-completeness;polynomial;time complexity;tree (data structure)	J. Mark Keil;Lorna Stewart	2006	Discrete Applied Mathematics	10.1016/j.dam.2006.03.003	strong perfect graph theorem;1-planar graph;block graph;pathwidth;split graph;combinatorics;discrete mathematics;cograph;np-complete;interval graph;topology;graph product;longest path problem;dense graph;clique problem;metric dimension;polygon;clique-sum;trapezoid graph;mathematics;maximal independent set;modular decomposition;treewidth;partial k-tree;chordal graph;indifference graph;approximation algorithm	Theory	22.309581083442538	27.535928143036898	128308
e92b80c8ac86d0e77a917fc15bed2ac1a6c21440	minimization by the d algorithm	minimisation;minimization;optimisation;programmation;optimizacion;redundancia;logic;minimizacion;pla;programacion;algorithme;algorithm;algorritmo;d algorithm;redundancy;pla d algorithm logic minimization;optimization;programming;redondance	"""Programmed logic arrays [1], [2] are common in computer design. A form of this method of design has been used since the beginnings of computers, in telephone relay networks [3]. Optimization of such realizations of functions were begun by Karnaugh [4], Quine [5], McCluskey [6], and Roth [7]. Substantial use was mnade of such programs by Preiss [8] and Perlman [9]. Despite the existence of exact procedures, """"fast,"""" """"approximate"""" procedures have been widely used. A new approximate procedure, using the D algorithm [1], [10], [11] is introduced here. It gets around a large computation, in complementation, using prior methods. Running programs """"verify"""" this expectation."""	approximation algorithm;computation;computer architecture;karnaugh map;quine (computing);realms of the haunting;relay	J. Paul Roth	1986	IEEE Transactions on Computers	10.1109/TC.1986.1676790	minimisation;programming;mathematical optimization;parallel computing;computer science;theoretical computer science;mathematics;redundancy;logic;algorithm;statistics	Visualization	14.524348303499506	18.668766186982296	128405
85a7b94c11f1591ae9e4eeb9c754813d46c69f84	on finding lowest common ancestors in trees	dag;code optimization;register allocation;straight line program;polynomial complete;program optimization;lowest common ancestor;on line algorithm;random access	Trees in an n node forest are to be merged according to instructions in a given sequence, while other instructions in the sequence ask for the lowest common ancestor of pairs of nodes. We show that any sequence of O(n) instructions can be processed “on line” in O(n log n) steps on a random access computer.  If we can accept our answer “off-line”, that is, no answers need to be produced until the entire sequence of instructions has been seen seen, then we may perform the task in O(n G(n)) steps, where G(n) is the number of times we must apply log2 to n to obtain a number less than or equal to zero.  A third algorithm solves a problem of intermediate complexity. We require the answers on line, but we suppose that all tree merging instructions precede the information requests. This algorithm requires O(n log log n) time.  We apply the first on line algorithm to a problem in code optimization, that of computing immediate dominators in a reducible flow graph. We show how this computation can be performed in O(n log n) steps.	access computer;algorithm;apply;binary logarithm;computation;dominator (graph theory);lowest common ancestor;mathematical optimization;np-intermediate;online and offline;program optimization;random access	Alfred V. Aho;John E. Hopcroft;Jeffrey D. Ullman	1973		10.1145/800125.804056	mathematical optimization;combinatorics;computer science;theoretical computer science;program optimization;mathematics;programming language;algorithm	Theory	14.285979906950354	27.773969285293198	128541
8d0e3e94bbf7babef80f58edabcb5b7317ffba7a	a study on the tie-set graph theory and network flow optimization problems	graph theory;gestion informacion;optimisation;teoria grafo;optimizacion;ring network;teoria conjunto;theorie ensemble;set theory;theorie graphe;information network management;red mallada cerrada;optimization problem;flujo red;reseau maille;red anillo;information management;reseau anneau;algorithme reparti;tie set graph;meshed network;algoritmo repartido;optimization;information system;gestion information;network flow;graph and network theory;distributed algorithm;systeme information;flot reseau;sistema informacion	Abstract#R##N##R##N#Aiming at establishing a firm basic theory to ring-based information network management systems, our paper proposes a tie-set graph theory. We define a binary vector representing a tie-set in a biconnected undirected graph G=(V,E) as a tie-set vector. The set of tie-set vectors forms a vector space over the proposed law of composition, then a basis of the vector space, μ linear independent tie-set vectors, is defined as a tie-set basis. The essential key concept in our theory is a tie-set graph, which has a one-to-one correspondence to a tie-set basis and represents a relation between two tie-set vectors of the basis. Some important properties of tie-set graphs and their application to survivable mesh networks in modern high-speed backbone networks are also presented. Furthermore, as a general approach to network flow optimization problems, tie-set flow vector space is proposed based on the tie-set graph theory. A distributed algorithm for the network flow optimization problems and its application are also presented in this paper. Copyright © 2004 John Wiley & Sons, Ltd.	flow network;graph theory;mathematical optimization	Toshio Koide;Haruki Kubo;Hitoshi Watanabe	2004	I. J. Circuit Theory and Applications	10.1002/cta.275	cycle space;graph power;optimization problem;distributed algorithm;ring network;combinatorics;flow network;directed graph;graph bandwidth;null graph;graph property;edge space;computer science;artificial intelligence;connectivity;graph theory;mathematics;voltage graph;graph;moral graph;information management;random geometric graph;cycle basis;complement graph;information system;line graph;algorithm;strength of a graph;set theory	Theory	22.450014071002624	29.405840916197995	128660
8fc81efdadca421c66738c43942abb5e6505db1d	the quickest multicommodity flow problem	multicommodity flow;algorithmique;temps polynomial;approximation algorithm;quickest multicommodity flow;flot arrivee;aproximacion polinomial;flujo red;algorithmics;algoritmica;approximation polynomiale;algoritmo aproximacion;polynomial time;network flow;algorithme approximation;flot reseau;polynomial approximation;tiempo polinomial	Traditionally, flows over time are solved in time-expanded networks which contain one copy of the original network for each discrete time step. While this method makes available the whole algorithmic toolbox developed for static flows, its main and often fatal drawback is the enormous size of the time-expanded network. In particular, this approach usually does not lead to efficient algorithms with running time polynomial in the input size since the size of the time-expanded network is only pseudo-polynomial. We present two different approaches for coping with this difficulty. Firstly, inspired by the work of Ford and Fulkerson on maximal s-t-flows over time (or ‘maximal dynamic s-t-flows’), we show that static, lengthbounded flows lead to provably good multicommodity flows over time. These solutions not only feature a simple structure but can also be computed very efficiently in polynomial time. Secondly, we investigate ‘condensed’ time-expanded networks which rely on a rougher discretization of time. Unfortunately, there is a natural tradeoff between the roughness of the discretization and the quality of the achievable solutions. However, we prove that a solution of arbitrary precision can be computed in polynomial time through an appropriate discretization leading to a condensed time expanded network of polynomial size. In particular, this approach yields a fully polynomial time approximation scheme for the quickest multicommodity flow problem and also for more general problems.	algorithm;arbitrary-precision arithmetic;discretization;flow network;fulkerson prize;information;maximal set;polynomial;polynomial-time approximation scheme;time complexity	Lisa Fleischer;Martin Skutella	2002		10.1007/3-540-47867-1_4	time complexity;mathematical optimization;combinatorics;flow network;multi-commodity flow problem;computer science;mathematics;algorithmics;approximation algorithm;algorithm	Theory	21.820724426156392	21.392554271473042	128750
06112f17193e5f36a33accba66b11a8007cb49f0	error compensation in leaf root problems	algorithmique;subgrafo;correction erreur;algorithmics;algoritmica;sous graphe;error correction;forbidden subgraphs;error compensation;edge graph;arete graphe;correccion error;subgraph;arista grafico	The k-Leaf Root problem is a particular case of graph power problems. Here, we study “error correction” versions of k-Leaf Root—that is, for instance, adding or deleting at most l edges to generate a graph that has a k-leaf root. We provide several NP-completeness results in this context, and we show that the NP-complete Closest 3Leaf Root problem (the error correction version of 3-Leaf Root) is fixed-parameter tractable with respect to the number of edge modifications in the given graph. Thus, we provide the seemingly first nontrivial positive algorithmic results in the field of error compensation for leaf root problems with k > 2. To this end, as a result of independent interest, we develop a forbidden subgraph characterization of graphs with 3-leaf roots.	cobham's thesis;error detection and correction;forbidden graph characterization;graph power;karp's 21 np-complete problems;np-completeness;parameterized complexity;root-finding algorithm	Michael Dom;Jiong Guo;Falk Hüffner;Rolf Niedermeier	2004		10.1007/978-3-540-30551-4_35	mathematical optimization;combinatorics;discrete mathematics;error detection and correction;computer science;mathematics;algorithmics;statistics	Theory	22.021523362122576	25.526023271099433	128874
164d8177cf405709fe77f7a80201cda0fe40b07d	an exact characterization of symmetric functions in qac0[2]	fonction parite;parite;circuit theory;complexite;teoria circuito;period;synthese circuit;concepcion circuito;language class;partity function;temps polynomial;theorie circuit;complexite calcul;symmetric function;funcion simetrica;clase complejidad;circuit design;periodo;complejidad;fonction symetrique;complexity;parity;optimisation combinatoire;complejidad computacion;periode;classe complexite;complexity class;computational complexity;informatique theorique;classe langage;polynomial time;borne inferieure;characterization;paridad;sintesis circuito;conception circuit;caracterisation;combinatorial optimization;caracterizacion;lower bound;circuit synthesis;cota inferior;optimizacion combinatoria;computer theory;clase lenguaje;tiempo polinomial;informatica teorica	qAC 0 2] is the class of languages computable by circuits of constant depth and quasi-polynomial (2 log O(1) n) size with unbounded fan-in AND, OR, and PARITY gates. Symmetric functions are those functions that are invariant under permutations of the input variables. Thus a symmetric function f n : f0; 1g n ! f0; 1g can also be seen as a function f n : f0; 1; ; ng ! f0; 1g. We give the following characterization of symmetric functions in qAC 0 2], according to how f n (x) changes as x grows from 0 to n. A symmetric function f = (f n) n2N is in qAC 0 2] if and only if f n has period 2 t(n) = log O(1) n except within both ends of length log O(1) n.	computable function;emoticon;fan-in;parity bit;polynomial;quasi-polynomial	Chi-Jen Lu	1998		10.1007/3-540-68535-9_20	time complexity;complexity class;complexity;network analysis;parity;combinatorial optimization;computer science;period;circuit design;calculus;mathematics;upper and lower bounds;computational complexity theory;algorithm;symmetric function	Theory	16.85231747184133	26.038567989293874	128896
201e990ca8eafbd6e2fe0e736a42768c6382243b	resource bounded measure	performance evaluation;resource bounded measurability;zero one law;decidable languages;decision problem;complexity class;algebra;computational complexity;space complexity;writing;lifting equipment;computer science;productivity;decidability computational complexity relational algebra;martingales;probability measure;relational algebra;decidability;algebra computational complexity lifting equipment computer science performance evaluation writing productivity;algebra resource bounded measurability complexity class decision problems decidable languages space complexity martingales;decision problems	A general theory of resource-bounded measurability and measure is developed. Starting from any feasible probability measure ν on the Cantor space C (the set of all decision problems) and any suitable complexity class C ⊆ C, the theory identifies the subsets of C that are ν-measurable in C and assigns measures to these sets, thereby endowing C with internal measure-theoretic structure. Classes C to which the theory applies include various exponential time and space complexity classes, the class of all decidable languages, and the Cantor space C itself, on which the resource-bounded theory is shown to agree with the classical theory. The sets that are ν-measurable in C are shown to form an algebra relative to which ν-measure is well-behaved (monotone, additive, etc.). This algebra is also shown to be complete (subsets of measure 0 sets are measurable) and closed under sufficiently uniform infinitary unions and intersections, and ν-measure in C is shown to have the appropriate additivity and monotone convergence properties with respect to such infinitary operations. A generalization of the classical Kolmogorov zero-one law is proven, showing that when ν is any feasible coin-toss (i.e., product) probability measure on C, every set that is ν-measurable in C and (like most complexity classes) invariant under finite alterations must have ν-measure 0 or ν-measure 1 in C. The theory is presented here is based on resource-bounded martingale splitting operators, which are type-2 functionals, each of which maps N × Dν into Dν × Dν , where Dν is the set of all ν-martingales. This type-2 aspect of the theory appears to be essential for general ν-measure in complexity classes C, but the sets of ν-measure 0 or 1 in C are shown to be characterized by the success conditions for martingales (type-1 functions) that have been used in resource-bounded measure to date. ∗This research was supported in part by National Science Foundation Grants 9157382 (with matching funds from Rockwell, Microware Systems Corporation, and Amoco Foundation) and 9610461. Much of the research was performed during several visits at the California Institute of Technology, and much of the writing took place during a sabbatical at Cornell University.	cantor;complexity class;dspace;decision problem;exptime;map;microware;resource bounded measure;theory;time complexity;utility functions on indivisible goods;monotone	Jack H. Lutz	1998		10.1109/CCC.1998.694611	combinatorics;discrete mathematics;computer science;decision problem;mathematics;algorithm	Theory	12.614319234055422	20.458305675877646	129167
19033c2f50a515e2892dcabddf7791f784c28f1c	finding a maximum likelihood tree is hard	criterio optimalidad;teoria demonstracion;recouvrement graphe;maxima parsimonia;arbre phylogenetique;cubierta grafo;vertex cover;tecnologia electronica telecomunicaciones;theorie preuve;computacion informatica;reconstruction graphe;maximum likelihood;proof theory;probleme np complet;heuristic method;maximum vraisemblance;grupo de excelencia;metodo heuristico;program verification;arbol filogenetico;verificacion programa;graph covering;phylogenetic tree;graph reconstruction;ciencias basicas y experimentales;reconstruccion grafo;theory;approximate vertex cover;algorithme evolutionniste;algorithms;problema np completo;algoritmo evolucionista;optimality criterion;tree reconstruction;intractability;maximum de parcimonie;methode heuristique;evolutionary algorithm;critere optimalite;tecnologias;verification programme;correctness proof;maxima verosimilitud;np complete problem;maximum parsimony	Maximum likelihood (ML) is an increasingly popular optimality criterion for selecting evolutionary trees [Felsenstein 1981]. Finding optimal ML trees appears to be a very hard computational task, but for tractable cases, ML is the method of choice. In particular, algorithms and heuristics for ML take longer to run than algorithms and heuristics for the second major character based criterion, maximum parsimony (MP). However, while MP has been known to be NP-complete for over 20 years [Foulds and Graham, 1982; Day et al. 1986], such a hardness result for ML has so far eluded researchers in the field.An important work by Tuffley and Steel [1997] proves quantitative relations between the parsimony values of given sequences and the corresponding log likelihood values. However, a direct application of their work would only give an exponential time reduction from MP to ML. Another step in this direction has recently been made by Addario-Berry et al. [2004], who proved that ancestral maximum likelihood (AML) is NP-complete. AML “lies in between” the two problems, having some properties of MP and some properties of ML. Still, the AML proof is not directly applicable to the ML problem.We resolve the question, showing that “regular” ML on phylogenetic trees is indeed intractable. Our reduction follows the vertex cover reductions for MP [Day et al. 1986] and AML [Addario-Berry et al. 2004], but its starting point is an approximation version of vertex cover, known as gap vc. The crux of our work is not the reduction, but its correctness proof. The proof goes through a series of tree modifications, while controlling the likelihood losses at each step, using the bounds of Tuffley and Steel [1997]. The proof can be viewed as correlating the value of any ML solution to an arbitrarily close approximation to vertex cover.	algorithm;approximation;cobham's thesis;computation;correctness (computer science);graham scan;heuristic (computer science);maximum parsimony (phylogenetics);np-completeness;occam's razor;optimality criterion;phylogenetic tree;phylogenetics;regular expression;time complexity;vertex cover	Benny Chor;Tamir Tuller	2006	J. ACM	10.1145/1183907.1183909	mathematical optimization;combinatorics;phylogenetic tree;np-complete;vertex cover;computer science;evolutionary algorithm;proof theory;mathematics;maximum likelihood;maximum parsimony;theory;algorithm;statistics	Theory	17.44181814340243	22.72269597322177	129199
6ba65a2242dda2bdf4063e8ec35fee0ab8a37614	a functional approach to external graph algorithms	lenguaje programacion;methode diviser pour regner;acoplamiento grafo;optimisation;external graph;optimizacion;programming language;gollete estrangulamiento;optimization technique;arbre maximal;metodo dividir para vencer;randomised algorithms;algorithme deterministe;algorithme randomise;graph matching;functional programming;approche deterministe;connected graph;deterministic approach;couplage graphe;deterministic algorithms;goulot etranglement;maximal independent set;side effect;arbol maximo;duplication;estructura datos;divide and conquer method;enfoque determinista;minimum spanning tree;duplicacion;graph algorithm;langage programmation;optimization;structure donnee;programmation fonctionnelle;spanning tree;external memory algorithms;connected component;programacion funcional;data structure;bottleneck;graphe connexe;grafo conexo	We present a new approach for designing external graph algorithms and use it to design simple, deterministic and randomized external algorithms for computing connected components, minimum spanning forests, bottleneck minimum spanning forests, maximal independent sets (randomized only), and maximal matchings in undirected graphs. Our I/ O bounds compete with those of previous approaches. We also introduce a semi-external model, in which the vertex set but not the edge set of a graph fits in main memory. In this model we give an improved connected components algorithm, using new results for external grouping and sorting with duplicates. Unlike previous approaches, ours is purely functional—without side effects—and is thus amenable to standard checkpointing and programming language optimization techniques. This is an important practical consideration for applications that may take hours to run.	adjacency list;application checkpointing;block size (cryptography);computational complexity theory;computer data storage;connected component (graph theory);dynamic problem (algorithms);fits;file spanning;functional approach;functional programming;graph (discrete mathematics);graph theory;input/output;kaplan–meier estimator;kathleen fisher;lars arge;lars bak (computer programmer);list of algorithms;matching (graph theory);mathematical optimization;maximal independent set;maximal set;programming language;randomized algorithm;semiconductor industry;sorting;spanning tree;vertex (graph theory)	James Abello;Adam L. Buchsbaum;Jeffery Westbrook	2001	Algorithmica	10.1007/s00453-001-0088-5	combinatorics;discrete mathematics;feedback arc set;connected component;data structure;spanning tree;computer science;connectivity;minimum spanning tree;connected dominating set;mathematics;maximal independent set;reverse-delete algorithm;deterministic system;functional programming;side effect;algorithm;gene duplication;matching	Theory	18.53865826110513	27.77542381814622	129518
331cf2aaa0b79d8118205f26ae7f9ce770eb6cf8	a polynomial time approximation scheme for dense min 2sat	minimisation;minimization;temps polynomial;everywhere dense min eq;minimizacion;aproximacion polinomial;everywhere dense min 2sat;approximation polynomiale;polynomial time;polynomial time approximation scheme;polynomial approximation;tiempo polinomial	It is proved that everywhere-dense Min 2SAT and everywhere- dense MIN EQ both have polynomial time approximation schemes.	2-satisfiability;maxima and minima;polynomial;polynomial-time approximation scheme;time complexity	Cristina Bazgan;Wenceslas Fernandez de la Vega	1999		10.1007/3-540-48321-7_6	time complexity;minimisation;mathematical optimization;combinatorics;discrete mathematics;polynomial-time approximation scheme;computer science;wilkinson's polynomial;mathematics;matrix polynomial;algorithm;statistics	Theory	20.848918806904795	25.660067675424557	129534
b143b65d35980327e7bd4b788ecb2db9838d66e1	the complexity of detecting fixed-density clusters	cluster computing;temps polynomial;complexite calcul;probleme np complet;fixed parameter problems;density based clustering;complejidad computacion;fonction densite;density function;computational complexity;clustering;informatique theorique;funcion densidad;polynomial time;graph algorithm;algorithme polynomial;problema np completo;algorithme graphe;graph algorithms;density functional;np complete problem;computer theory;tiempo polinomial;informatica teorica	We study the complexity of finding a subgraph of a certain size and a certain density, where density is measured by the average degree. Let γ: N → Q+ be any density function, i.e., γ is computable in polynomial time and satisfies γ(k) ≤ k-1 for all k ∈ N. Then γ-CLUSTER is the problem of deciding, given an undirected graph G and a natural number k, whether there is a subgraph of G on k vertices that has average degree at least γ(k). For γ(k) = k - 1, this problem is the same as the well-known CLIQUE problem, and thus NP-complete. In contrast to this, the problem is known to be solvable in polynomial time for γ(k)=2. We ask for the possible functions γ such that γ-CLUSTER remains NP-complete or becomes solvable in polynomial time. We show a rather sharp boundary: γ CLUSTER is NP-complete if γ = 2+Ω(1/k1-e) for some e > 0 and has a polynomial-time algorithm for γ=2+O(1/k). The algorithm also shows that for γ = 2+O(1/k1-o;(1)), γ-CLUSTER is solvable in subexponential time 2no(1).	sensor	Klaus Holzapfel;Sven Kosub;Moritz G. Maaß;Hanjo Täubig	2006	Discrete Applied Mathematics	10.1016/j.dam.2006.01.005	time complexity;probability density function;combinatorics;discrete mathematics;np-complete;computer cluster;mathematics;cluster analysis;computational complexity theory;algorithm	ML	21.995797026513788	26.65955283261073	129640
7d989cdc660aa1af5704d1a1f31be08dbebc7115	the complexity of the np-class	travelling salesman problem;computational complexity;data structure;geometric structure	This paper presents a novel and straight formulation, and gives a complete insight towards the understanding of the complexity of the problems of the so called NP-Class. In particular, this paper focuses in the Searching of the Optimal Geometrical Structures and the Travelling Salesman Problems. The main results are the polynomial reduction procedure and the solution to the Noted Conjecture of the NP-Class.	np (complexity);polynomial;travelling salesman problem	Carlos Barron-Romero	2010	CoRR		complete;mathematical optimization;combinatorics;data structure;computer science;mathematics;travelling salesman problem;computational complexity theory;algorithm;3-opt	Theory	20.46338779201415	20.714418420131707	129780
509e22941c68bbd61bbaa101242dd577acb0794e	np is as easy as detecting unique solutions	satisfiability;np complete problem	For all known NP-complete problems the number of solutions in instances having solutions may vary over an exponentially large range. Furthermore, most of the well-known ones, such as satisfiability, are parsimoniously interreducible, and these can have any number of solutions between zero and an exponentially large number. It is natural to ask whether the inherent intractability of NP-complete problems is caused by this wide variation. In this paper we give a negative answer to this using randomized reductions. We show that the problems of distinguishing between instances of SAT having zero or one solution, or finding solutions to instances of SAT having unique solutions, are as hard as SAT itself. Several corollaries about the difficulty of specific problems follow. For example if the parity of the number of solutions of SAT can be computed in RP then NP = RP. Some further problems can be shown to be hard for NP or DP via randomized reductions.	karp's 21 np-complete problems;many-one reduction;np (complexity);occam's razor;rp (complexity);randomized algorithm;whole earth 'lectronic link	Leslie G. Valiant;Vijay V. Vazirani	1985		10.1145/22145.22196	combinatorics;discrete mathematics;np-complete;computer science;mathematics;algorithm;satisfiability	Theory	10.842726219839054	19.252249270243265	129811
da5ec0d2e2a752c4c38d772fa34623d4ccff677f	a generalization of maximal independent sets	conjunto independiente;graph theory;hopfield model;maximum independent set problem;mis problem;modele hopfield;teoria grafo;fonction energie;hopfield network;algorithme glouton;modelo hopfield;independent set;problema np duro;greedy algorithms;reseau;theorie graphe;polynomial factorization;energy function;red;np hard problem;ensemble independant;maximal independent set;probleme np difficile;polynomial algorithm;algorithme polynomial;funcion energia;greedy algorithm;algoritmo gloton;network	Abstract   We generalize the concept of maximal-independent set in the following way. For a nonnegative integer  k  we define a  k - insulated set  of a graph  G  as a subset  S  of its vertices such that each vertex in  S  is adjacent to at most  k  other vertices in  S  and each vertex not in  S  is adjacent to at least  k +1 vertices in  S . We show that it is NP-hard to approximate a maximum  k -insulated set within a polynomial factor and describe a polynomial algorithm which approximates a maximum  k -insulated set in an  n -vertex graph to within the factor of  cnk /log 2  n , for a constant  c >0. We also give an O( kn  2 ) algorithm which finds an arbitrary  k -insulated set.	maximal independent set;maximal set	Arun K. Jagota;Giri Narasimhan;Lubomír Soltés	2001	Discrete Applied Mathematics	10.1016/S0166-218X(00)00215-8	mathematical optimization;combinatorics;greedy algorithm;discrete mathematics;independent set;feedback vertex set;metric k-center;graph theory;cycle graph;vertex;mathematics;neighbourhood;algorithm	Theory	21.64694337656058	27.137694953915386	130182
45bb3a2a40fa9449a0ab8fb1ce4c70750a431e45	permuting sparse rectangular matrices into block-diagonal form	calcul scientifique;graph partitioning by vertex separator;matriz bloque;hipergrafico;sparse rectangular matrices;analisis numerico;graphe biparti;empirical study;65f20;general and miscellaneous mathematics computing and information science;rectangular matrix;65f05;grafo bipartido;05c65;coarse grain parallelism;partitioning;combinatorial problems;matrice diagonale;matriz rectangular;qr factorization;65y05;05c50;analyse numerique;factorizacion lu;factorization;computacion cientifica;65k05;matrice creuse;numerical analysis;graph partitioning;matrices;matriz diagonal;mathematical programming;matrice bloc;block diagonalization;parallelisme gros grain;05c85;block matrix;partitionnement;vertex separation;hypergraph;hypergraph partitioning;subdivision;sparse matrix;separation sommet graphe;65f50;coarse grained;scientific computation;singly bordered block diagonal form;bipartite graph;05c90;programmation mathematique;programming coarse grain parallelism sparse rectangular matrices singly bordered block diagonal form doubly bordered block diagonal form graph partitioning by vertex separator hypergraph partitioning;programacion matematica;factorisation lu;lu factorization;sparse matrices;matriz dispersa;hypergraphe;doubly bordered block diagonal form;diagonal matrix;matrice rectangulaire	We investigate the problem of permuting a sparse rectangular matrix into blockdiagonal form. Block-diagonal form of a matrix grants an inherent parallelism for solving the deriving problem, as recently investigated in the context of mathematical programming, LU factorization, and QR factorization. To represent the nonzero structure of a matrix, we propose bipartite graph and hypergraph models that reduce the permutation problem to those of graph partitioning by vertex separator and hypergraph partitioning, respectively. Our experiments on a wide range of matrices, using the state-of-the-art graph and hypergraph partitioning tools MeTiS and PaToH, revealed that the proposed methods yield very effective solutions both in terms of solution quality and runtime.	apple desktop bus;experiment;graph (abstract data type);graph partition;lu decomposition;metis;mathematical optimization;parallel computing;qr decomposition;sandy bridge;sparse matrix;vertex separator	Cevdet Aykanat;Ali Pinar;Ümit V. Çatalyürek	2004	SIAM J. Scientific Computing	10.1137/S1064827502401953	combinatorics;discrete mathematics;sparse matrix;graph partition;mathematics;incidence matrix;algebra	HPC	19.842160545996226	30.575719701624806	130376
48ecb954d63dc71bf30707d2853ca17f66376d65	new bounds on the obdd-size of integer multiplication via universal hashing	hachage;fonction booleenne;diagrama binaria decision;diagramme binaire decision;nombre entier;boolean function;satisfiability;ordered binary decision diagram;upper bound;integer;hashing;funcion booliana;entero;hash function;multiplicacion;multiplication;binary decision diagram	Ordered binary decision diagrams (OBDDs) nowadays belong to the most common representation types for Boolean functions. Although they allow important operations such as satisfiability test and equality test to be performed efficiently, their limitation lies in the fact that they may require exponential size for important functions. Bryant [8] has shown that any OBDD-representation of the function MULn−1,n, which computes the middle bit of the product of two n-bit numbers, requires at least 2 nodes. In this paper a stronger bound of 2/61 is proven by a new technique, using a recently found universal family of hash functions [23]. As a result, one cannot hope anymore to find reasonable small OBDDs even for the multiplication of relatively short integers, since for only a 64-bit multiplication millions of nodes are required. Further, a first non-trivial upper bound of 7/3 · 2 for the OBDD size of MULn−1,n is provided.	64-bit computing;binary decision diagram;hash function;time complexity;universal hashing	Philipp Woelfel	2000	Electronic Colloquium on Computational Complexity (ECCC)	10.1007/3-540-44693-1_49	combinatorics;discrete mathematics;hash function;computer science;mathematics;algorithm	Theory	10.446687565308554	24.710598883152294	130483
64ae56670daed909028061d772cc747992fdebfc	on the benefits of adaptivity in property testing of dense graphs	modelizacion;graph theory;metodo adaptativo;adaptive testing;maximum degree;graphe biparti;teoria grafo;high density;grado grafo;complexite calcul;grafo bipartido;algoritmo adaptativo;property testing;methode adaptative;theorie graphe;upper bound;modelisation;densite elevee;adaptive algorithm;adaptivity;complejidad computacion;query complexity;algorithme adaptatif;computational complexity;adaptive method;borne inferieure;densidad elevada;bipartiteness;degre graphe;graph model;borne superieure;bipartite graph;modeling;lower bound;graph degree;cota superior;cota inferior	We consider the question of whether adaptivity can improve the complexity of property testing algorithms in the dense graphs model. It is known that there can be at most a quadratic gap between adaptive and non-adaptive testers in this model, but it was not known whether any gap indeed exists. In this work we reveal such a gap. Specifically, we focus on the well studied property of bipartiteness. Bogdanov and Trevisan (IEEE Symposium on Computational Complexity, pp. 75–81, 2004) proved a lower bound of Ω(1/ε 2) on the query complexity of non-adaptive testing algorithms for bipartiteness. This lower bound holds for graphs with maximum degree O(ε n). Our main result is an adaptive testing algorithm for bipartiteness of graphs with maximum degree O(ε n) whose query complexity is $\tilde{O}(1/\epsilon ^{3/2})$ . A slightly modified version of our algorithm can be used to test the combined property of being bipartite and having maximum degree O(ε n). Thus we demonstrate that adaptive testers are stronger than non-adaptive testers in the dense graphs model. We note that the upper bound we obtain is tight up-to polylogarithmic factors, in view of the Ω(1/ε 3/2) lower bound of Bogdanov and Trevisan for adaptive testers. In addition we show that $\tilde{O}(1/\epsilon ^{3/2})$ queries also suffice when (almost) all vertices have degree $\Omega(\sqrt{ \epsilon }\cdot n)$ . In this case adaptivity is not necessary.	adaptive grammar;algorithm;computational complexity theory;decision tree model;degree (graph theory);polylogarithmic function;property testing;whole earth 'lectronic link	Mira Gonen;Dana Ron	2008	Algorithmica	10.1007/s00453-008-9237-4	mathematical optimization;combinatorics;graph theory;mathematics;geometry;upper and lower bounds;algorithm	Theory	21.275327818630657	25.831871719699247	130500
4d32808c33ef9af2184276a80b99e3b355f100b7	deterministic simulation of idealized parallel computers on more realistic ones	idealized parallel computers;idealized parallel computer;deterministic simulation;parallel computer;point to point;shared memory;lower bound	We describe a non-uniform deterministic simulation of PRAMs on module parallel computers (MPCs) and on processor networks of bounded degree. The simulating machines have the same number n of processors as the simulated PRAM, and if the size of the PRAMu0027s shared memory is polynomial in n, each PRAM step is simulated by O(log n) MPC steps or by O((log n)2) steps of the bounded-degree network. This improves upon a previous result by Upfal and Wigderson. We also prove an Ω((log n)2/log log n) lower bound on the number of steps needed to simulate one PRAM step on a bounded-degree network under the assumption that the communication in the network is point-to-point.	parallel computing;simulation	Helmut Alt;Torben Hagerup;Kurt Mehlhorn;Franco P. Preparata	1987	SIAM J. Comput.	10.1137/0216053	shared memory;parallel processing;programming;parallel computing;deterministic simulation;point-to-point;computer science;theoretical computer science;mathematics;distributed computing;upper and lower bounds;programming language;functional programming;function;algorithm;polynomial	Theory	10.532270032619978	32.10679871745395	130788
db2e849f304a2009e541acec57bc03c3a0a501c2	an o(n3/2sqrt(log n)) algorithm for sorting by reciprocal translocations	genetique;analisis datos;genetica;sorting;tria;genetics;combinatorial problem;data analysis;probleme combinatoire;problema combinatorio;sorting by reversals;triage;genome;pattern recognition;analyse donnee;reconnaissance forme;genoma;reconocimiento patron	We prove that sorting by reciprocal translocations can be done in $O(n^{3/2}\sqrt{\log (n)})$ for an n-gene genome. Our algorithm is an adaptation of the Tannier et. al algorithm for sorting by reversals. This improves over the O(n3) algorithm for sorting by reciprocal translocations given by Bergeron et al.	algorithm;sorting	Michal Ozery-Flato;Ron Shamir	2006		10.1007/11780441_24	computer science;sorting;data analysis;genetics;algorithm;genome	Theory	16.276496288011487	25.044187885837463	130811
f812eb15fcdf7043c7567d1470f8f79b7db2aa39	clustered planarity: small clusters in eulerian graphs	connected graph;polynomial time algorithm;eulerian graph	We present several polynomial-time algorithms for c-planarity testing for clustered graphs with clusters of size at most three. The most general result concerns a special class of Eulerian graphs, namely graphs obtained froma fixed-size 3-connected graph bymultiplying and then subdividing edges. We further give algorithms for 3-connected graphs, and for graphs with small faces. The last result applies with no restrictions on the cluster size.	clustered planarity;eulerian path	Eva Jelínková;Jan Kára;Jan Kratochvíl;Martin Pergel;Ondrej Suchý;Tomás Vyskocil	2007		10.1007/978-3-540-77537-9_30	1-planar graph;block graph;pathwidth;topological graph theory;split graph;combinatorics;discrete mathematics;cograph;topology;graph product;dense graph;connectivity;pancyclic graph;graph coloring;trapezoid graph;mathematics;maximal independent set;modular decomposition;treewidth;partial k-tree;chordal graph;indifference graph;book embedding;line graph;eulerian path	NLP	24.206886284537354	27.823108619965105	130863
35cf406a46044ead53848b2eab0d35282c21156d	algorithms for the minimum non-separating path and the balanced connected bipartition problems on grid graphs	approximate algorithm;approximation algorithm;balanced connected partition;algorithm;exact algorithm;polynomial time;approximation scheme;fully polynomial time approximation scheme;non separating path;data structure;grid graphs	For given a pair of nodes in a graph, the minimum non-separating path problem looks for a minimum weight path between the two nodes such that the remaining graph after removing the path is still connected. The balanced connected bipartition (BCP2) problem looks for a way to bipartition a graph into two connected subgraphs with their weights as equal as possible. In this paper we present an O(N logN) time algorithm for finding a minimum weight non-separating path between two given nodes in a grid graph of N nodes with positive weight. This result leads to a 5/4-approximation algorithm for the BCP2 problem on grid graphs, which is the currently best ratio achieved in polynomial time. We also developed an exact algorithm for the BCP2 problem on grid graphs. Based on the exact algorithm and a rounding B.Y. Wu National Chung Cheng University, ChiaYi, Taiwan 621, R.O.C. E-mail: bangye@cs.ccu.edu.tw 2 technique, we show an approximation scheme, which is a fully polynomial time approximation scheme for fixed number of rows.	exact algorithm;graph (discrete mathematics);lattice graph;minimum weight;polynomial;polynomial-time approximation scheme;rounding;time complexity	Bang Ye Wu	2013	J. Comb. Optim.	10.1007/s10878-012-9481-z	1-planar graph;time complexity;mathematical optimization;suurballe's algorithm;combinatorics;discrete mathematics;polynomial-time approximation scheme;widest path problem;data structure;longest path problem;computer science;mathematics;tree-depth;modular decomposition;shortest path problem;reverse-delete algorithm;strong orientation;approximation algorithm;shortest path faster algorithm;algorithm;minimum k-cut	Theory	24.32180282026033	21.91580424827221	130933
fb5dfbe7b5575559f2f8e27b18ed8c855df191d3	fast parallel molecular solution for dna-based computing: the 0-1 knapsack problem	satisfiability;deoxyribonucleic acid;knapsack problem;hamiltonian path problem	It is shown first by Adleman that deoxyribonucleic acid (DNA) strand could be employed towards calculating solution to an instance of the NP-complete Hamiltonian Path Problem (HPP). Lipton also demonstrated that Adleman's techniques could be used to solve the satisfiability (SAT) problem. In this paper, it is demonstrated how the DNA operations presented by Adleman and Lipton can be used to develop the DNA-based algorithm for solving the 0-1 Knapsack Problem.	knapsack problem	Sientang Tsai	2009		10.1007/978-3-642-03095-6_40	continuous knapsack problem;mathematical optimization;computer science;cutting stock problem;hamiltonian path problem;knapsack problem;dna;algorithm;satisfiability	HPC	16.023811189626734	22.17238235323213	131048
597fe612377ab64ed692edd315fb1923a05d7843	a band and bound technique for simple random algorithms	randomized algorithm		algorithm	Vernon Rego	1990			mathematical optimization;combinatorics;computer science;theoretical computer science;randomized algorithm	Theory	16.69962181742754	20.884548640410607	131085
73d85578548dd3f0a77187dba131619840de4eb9	testing matrix rank, optimally		We show that for the problem of testing if a matrix A ∈ Fn×n has rank at most d, or requires changing an -fraction of entries to have rank at most d, there is a non-adaptive query algorithm making Õ(d/ ) queries. Our algorithm works for any field F. This improves upon the previous O(d/ ) bound (Krauthgamer and Sasson, SODA ’03), and bypasses an Ω(d/ ) lower bound of (Li, Wang, and Woodruff, KDD ’14) which holds if the algorithm is required to read a submatrix. Our algorithm is the first such algorithm which does not read a submatrix, and instead reads a carefully selected non-adaptive pattern of entries in rows and columns of A. We complement our algorithm with a matching Ω̃(d/ ) query complexity lower bound for non-adaptive testers over any field. We also give tight bounds of Θ̃(d) queries in the sensing model for which query access comes in the form of 〈Xi,A〉 := tr(Xi A); perhaps surprisingly these bounds do not depend on . Testing rank is only one of many tasks in determining if a matrix has low intrinsic dimensionality. We next develop a novel property testing framework for testing numerical properties of a real-valued matrix A more generally, which includes the stable rank, Schatten-p norms, and SVD entropy. Specifically, we propose a bounded entry model, where A is required to have entries bounded by 1 in absolute value. Such a model provides a meaningful framework for testing numerical quantities and avoids trivialities caused by single entries being arbitrarily large. It is also well-motivated by recommendation systems. We give upper and lower bounds for a wide range of problems in this model, and discuss connections to the sensing model above. We obtain several results for estimating the operator norm that may be of independent interest. For example, we show that if the stable rank is constant, ‖A‖F = Ω(n), and the singular value gap σ1(A)/σ2(A) = (1/ ) γ for any constant γ > 0, ∗A full version of the paper is available at https://arxiv. org/abs/1810.08171. †Carnegie Mellon University. Email: ninamf@cs.cmu.edu. ‡Nanyang Technological University. Email: yili@ntu.edu.sg. §Carnegie Mellon University. Email: dwoodruf@cs.cmu.edu. ¶Carnegie Mellon University. Email: hongyanz@cs.cmu.edu. Corresponding author. then the operator norm can be estimated up to a (1± )-factor non-adaptively by querying O(1/ ) entries. This should be contrasted to adaptive methods such as the power method, or previous non-adaptive sampling schemes based on matrix Bernstein inequalities which read a 1/ 2 × 1/ 2 submatrix and thus make Ω(1/ ) queries. Similar to our non-adaptive algorithm for testing rank, our scheme instead reads a carefully selected pattern of entries.	adaptive algorithm;adaptive sampling;column (database);data mining;decision tree model;email;numerical analysis;phil bernstein;power iteration;property testing;recommender system;sampling (signal processing);singular value decomposition	Maria-Florina Balcan;Yi Li;David P. Woodruff;Hongyang Zhang	2019		10.1137/1.9781611975482.46	combinatorics;discrete mathematics;rank (linear algebra);row and column spaces;matrix (mathematics);mathematics;singular value decomposition;bounded function;upper and lower bounds;property testing;absolute value	ML	13.563016636373579	20.475212665910444	131147
50ff2a6e179d25afe7ab55cb6aa45d09a27c2405	average-case hardness of rip certification		The restricted isometry property (RIP) for design matrices gives guarantees for optimal recovery in sparse linear models. It is of high interest in compressed sensing and statistical learning. This property is particularly important for computationally efficient recovery methods. As a consequence, even though it is in general NP-hard to check that RIP holds, there have been substantial efforts to find tractable proxies for it. These would allow the construction of RIP matrices and the polynomial-time verification of RIP given an arbitrary matrix. We consider the framework of average-case certifiers, that never wrongly declare that a matrix is RIP, while being often correct for random instances. While there are such functions which are tractable in a suboptimal parameter regime, we show that this is a computationally hard task in any better regime. Our results are based on a new, weaker assumption on the problem of detecting dense subgraphs.	algorithmic efficiency;best, worst and average case;cobham's thesis;compressed sensing;linear model;machine learning;np-hardness;polynomial;restricted isometry property;sensor;sparse matrix;time complexity	Tengyao Wang;Quentin Berthet;Yaniv Plan	2016			mathematical optimization;theoretical computer science;mathematics;algorithm	ML	17.574085182271887	18.531194588388875	131343
281d7a3f87efc2660d5e7991eea7c286775c2c59	capturing topology in graph pattern matching	data graph;real-life data;bounded number;strong simulation;cubic-time algorithm;earlier extension;graph simulation;synthetic data;pattern graph;capturing topology;graph pattern matching	Graph pattern matching is often defined in terms of subgraph isomorphism, an np-complete problem. To lower its complexity, various extensions of graph simulation have been considered instead. These extensions allow pattern matching to be conducted in cubic-time. However, they fall short of capturing the topology of data graphs, i.e., graphs may have a structure drastically different from pattern graphs they match, and the matches found are often too large to understand and analyze. To rectify these problems, this paper proposes a notion of strong simulation, a revision of graph simulation, for graph pattern matching. (1) We identify a set of criteria for preserving the topology of graphs matched. We show that strong simulation preserves the topology of data graphs and finds a bounded number of matches. (2) We show that strong simulation retains the same complexity as earlier extensions of simulation, by providing a cubic-time algorithm for computing strong simulation. (3) We present the locality property of strong simulation, which allows us to effectively conduct pattern matching on distributed graphs. (4) We experimentally verify the effectiveness and efficiency of these algorithms, using real-life data and synthetic data.	algorithm;cubic function;experiment;locality of reference;np-completeness;pattern matching;real life;simulation;subgraph isomorphism problem;synthetic data	Shuai Ma;Yang Cao;Wenfei Fan;Jinpeng Huai;Tianyu Wo	2011	PVLDB	10.14778/2095686.2095690	1-planar graph;pathwidth;split graph;factor-critical graph;combinatorics;discrete mathematics;null model;bipartite graph;graph product;graph theory;theoretical computer science;forbidden graph characterization;comparability graph;mathematics;maximal independent set;modular decomposition;partial k-tree;chordal graph;indifference graph;book embedding	DB	22.61488813844978	23.894388888251928	131651
fc322b00f45aca610564cded6985ad0c6ce9d146	pipelined search on coarse grained networks	parallelisme;distributed system;algoritmo paralelo;complexite;systeme reparti;parallel algorithm;algoritmo busqueda;multiprocessor;time complexity;queuing theory;algorithme recherche;sistema informatico;complejidad;search algorithm;computer system;average case analysis;complexity;algorithme parallele;parallelism;sistema repartido;paralelismo;systeme informatique;coarse grained;multiprocesador;multiprocesseur	The time complexity of searching a sorted list ofn elements in parallel on a coarse grained network of diameterD and consisting ofN processors (wheren may be much larger thanN) is studied. The worst case period and latency of a sequence of pipeline search operation are easity seen to be Ω(logn−logN) and Ω(D+logn−logN), respectively. Since forn=N 1+Ω(1) the worst-case period is Ω(logn) (which can be achieved by a single processor), coarse-grained networks appear to be unsuitable for the search problem. By contrast, it is demonstrated using standard queuing theory techniques that a constant expected period can be achieved provided thatn=O(N2 N ).	best, worst and average case;central processing unit;queueing theory;search problem;sorting algorithm;time complexity	Selim G. Akl;Frank Dehne	1989	International Journal of Parallel Programming	10.1007/BF01379185	time complexity;parallel computing;complexity;real-time computing;multiprocessing;computer science;parallel algorithm;queueing theory;algorithm;search algorithm	HPC	10.851102050232958	32.06231849728928	131827
f370fdd0d5bd1dd57ef0018c8bdb2cf2e5247442	on the complexity of clustered-level planarity and t-level planarity		In this paper we study two problems related to the drawing of level graphs, that is, T -LEVEL PLANARITY and CLUSTERED-LEVEL PLANARITY. We show that both problems are NP-complete in the general case and that they become polynomial-time solvable when restricted to proper instances.	clustered planarity;decision problem;np-completeness;polynomial;regular expression;time complexity	Patrizio Angelini;Giordano Da Lozzo;Giuseppe Di Battista;Fabrizio Frati;Vincenzo Roselli	2014	CoRR		combinatorics;discrete mathematics;level structure;planarity testing;force-directed graph drawing;mathematics;bound graph;neighbourhood;algorithm	Theory	23.97671214093352	25.169854416639808	131948
063633c7401710baf1bf74081cd73f4c36e2351b	p is a proper subset of np	complexity class;computational complexity;polynomial time;np complete problem	The purpose of this article is to examine and limit the conditions in which the P complexity class could be equivalent to the NP complexity class. Proof is provided by demonstrating that as the number of clauses in a NP-complete problem approaches infinity, the number of input sets processed per computation performed also approaches infinity when solved by a polynomial time solution. It is then possible to determine that the only deterministic optimization of a NP-complete problem that could prove P = NP would be one that examines no more than a polynomial number of input sets for a given problem. It is then shown that subdividing the set of all possible input sets into a polynomial-sized search partition is a problem in the FEXP complexity class. By demonstrating that at least one NP-complete problem exists such that the set of all possible input sets for that problem cannot be partitioned in polynomial time, it then will follow that P 6= NP.	complexity class;computation;mathematical optimization;np (complexity);np-completeness;p versus np problem;polynomial;time complexity	Jerrald Meek	2008	CoRR		co-np;complete;time complexity;complexity class;p;mathematical optimization;combinatorics;discrete mathematics;function problem;fp;polynomial-time reduction;np;np-complete;apx;computer science;structural complexity theory;karp–lipton theorem;sparse language;p versus np problem;cook–levin theorem;mathematics;up;np-easy;pseudo-polynomial time;co-np-complete;computational complexity theory;algorithm;pspace-complete	Theory	16.218835219234897	19.458863665353853	132042
b00f32a84ef4f72b75d9b8a06785a1ea39b70752	selección aleatoria de árboles generadores en gráficas	matrices de transicion en cadenas de markov;paseos aleatorios sobre graficas;distribuciones de probabilidad en;seleccion aleatoria de arboles generadores;distribuciones de probabilidad en vecindades de vertices;article;palabras clave seleccion aleatoria de arboles generadores paseos aleatorios sobre graficas matrices de transicion en cadenas de markov distribuciones de probabilidad en vecindades de vertices	Random selection of spanning trees on graphs has been treated extensively in technical literature. Popular randomized algorithms have time complexity varying from to , where is the order of a graph, namely, the number of vertices. In this work, we introduce effective and efficient procedures to select spanning trees using random walks with the purpose to balance the diameter of the selected tree, the valencies of its inner vertices, and the number of leaves at its yield. We describe several ways to form transition matrices of Markov chains in terms of probability distributions on the neighborhood of any visited vertex along the random walk.	file spanning;markov chain;randomized algorithm;time complexity;vertex (geometry)	Sergio Luis Pérez-Pérez;Guillermo Morales-Luna;Feliu Sagols	2012	Computación y Sistemas		combinatorics;calculus;mathematics;geometry	Theory	23.963149662073914	31.996066219743604	132234
84c987d6fa31b2b6c1dd498cef69ec0f2f36ef42	exact output rate of peres's algorithm for random number generation	random number generation;analysis of algorithms;von neumann s method;peres s method;elias s method	An exact computation of the output rate of Peres@?s algorithm is reported. The algorithm, recursively defined, converts independent flips of a biased coin into unbiased coin flips at rates that approach the information-theoretic upper bound, as the input size and the recursion depth tend to infinity. However, only the limiting rate with respect to the input size is known for each recursion depth. We compute the exact output rate for each fixed-length input and compare it with another asymptotically optimal method by Elias.	algorithm;peres–horodecki criterion;random number generation	Sung-il Pae	2013	Inf. Process. Lett.	10.1016/j.ipl.2012.12.012	combinatorics;discrete mathematics;random number generation;computer science;analysis of algorithms;mathematics;algorithm	AI	14.302158717954034	23.082826687670547	132325
90bf5625a54d8d3d928e51e1892eb3214a2cca92	parallel algorithms for relational coarsest partition problems	parallel algorithm;parallel algorithms partitioning algorithms algorithm design and analysis algebra phase change random access memory logic functions concurrent computing polynomials computer society hardware;labeled transition system;labeled transition systems;data processing;computations;sequential analysis;computer programming;crew pram processors parallel algorithms relational coarsest partition problems concurrent systems verification p complete label transition system sequential algorithm;formal verification;coarsest partition problems;concurrent systems;computational complexity;time use;input output processing;algorithms;random access computer storage;analysis of concurrent systems;read write memories;formal verification parallel algorithms computational complexity;bisimulation checking;parallel processing;parallel processors;concurrent engineering;problem solving;parallel algorithms	Relational Coarsest Partition Problems (RCPPs) play a vital role in verifying concurrent systems. It is known that RCPPS are Ρ-complete and hence it may not be possible to design polylog time parallel algorithms for these problems. In this paper, we present two efficient parallel algorithms for RCPP, in which its associated label transition system is assumed to have m transitions and n states. The first algorithm runs in O(n1+∈) time using m/n∈ CREW PRAAM processors, for any fixed ∈ < 1. This algorithm is analogous and optimal with respect to the sequential algorithm of Kanellakis and Smolka. The second algorithm runs in O(n log n) time using m/n log n CREW PRAM processor. This algorithm is analogous and nearly optimal with respect to the sequential algorithm of Paige and Tarjan. Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-93-71. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/493 Parallel Algorithms for Relational Coarsest Partition Problems MS-CIS-93-71 GRASP LAB 354 Sangut hevar Rajasekaran Insup Lee University of Pennsylvania School of Engineering and Applied Science Computer and 1nformat.ion Science Depart,ment Philadelphia, PA 19104-6389	central processing unit;concurrency (computer science);crew scheduling;grasp;information and computer science;information science;microsoft windows;parallel algorithm;parallel random-access machine;sequential algorithm;sorting;time complexity;transition system;verification and validation	Sanguthevar Rajasekaran;Insup Lee	1998	IEEE Trans. Parallel Distrib. Syst.	10.1109/71.707548	parallel processing;parallel computing;data processing;computer science;theoretical computer science;database;distributed computing;parallel algorithm;programming language;algorithm	Logic	15.5424267581665	31.561468948252767	132433
44872b740da3a5a9392a8d82d5503a74a5e11785	primality proving via one round in ecpp and one iteration in aks	primality testing;computational number theory;temps polynomial;algorithm analysis;elliptic curve;time complexity;heuristic method;metodo heuristico;securite donnee;courbe elliptique;approche deterministe;algorithme;deterministic approach;iterative methods;number theory;complexite temps;0210d;agrawal kayal saxena primalit testing algorithm;curva eliptica;methode iterative;cryptography;elliptic curve primality proving;enfoque determinista;polynomial time;cryptographie;algorithms;theorie nombre;analyse algorithme;methode heuristique;complejidad tiempo;security of data;analisis algoritmo;tiempo polinomial	In August 2002, Agrawal, Kayal and Saxena announced the first deterministic and polynomial-time primality-testing algorithm. For an input n, the Agarwal-Kayal-Saxena (AKS) algorithm runs in time $\tilde{O}(\log^{7.5} n)$ (heuristic time $\tilde{O}(\log^6 n)$ ). Verification takes roughly the same amount of time. On the other hand, the Elliptic Curve Primality Proving algorithm (ECPP) runs in random heuristic time $\tilde{O}(\log^{6} n)$ (some variant has heuristic time complexity $\tilde{O}(\log^4 n)$ ) and generates certificates which can be easily verified. However, it is hard to analyze the provable time complexity of ECPP even for a small portion of primes. More recently, Berrizbeitia gave a variant of the AKS algorithm, in which some primes (of density $O({ 1 / {\log}^2\,n })$ ) cost much less time to prove than a general prime does. Building on these celebrated results, this paper explores the possibility of designing a randomized primality-proving algorithm based on the AKS algorithm. We first generalize Berrizbeitia's algorithm to one which has higher density ( $\Omega( {1/{\log}\log n }) $ ) of primes whose primality can be proved in time complexity $\tilde{O}(\log^{4} n)$ . For a general prime, one round of ECPP is deployed to reduce its primality proof to the proof of a random easily proved prime, thus we achieve heuristic time complexity $\tilde{O}(\log^{4} n)$ for all primes.	aks primality test;deterministic algorithm;dijkstra's algorithm;elliptic curve primality;heuristic;iteration;neeraj kayal;primality certificate;provable security;randomized algorithm;time complexity	Qi Cheng	2006	Journal of Cryptology	10.1007/s00145-006-0406-9	time complexity;number theory;discrete mathematics;computer science;mathematics;algorithm;computational number theory;algebra	Crypto	12.21901164499281	22.96707542687691	132504
843bb91b38dd41a38f4de04ffa7ea97749e63342	new results on induced matchings	arbre graphe;graph theory;strong matchings;teoria grafo;graphe intervalle;temps polynomial;tree graph;time complexity;matchings;complexite calcul;interval graph;grafo intervalo;graphe parfait;mise en correspondence;perfect graphs;perfect graph;theorie graphe;analysis of algorithm;induced matchings;algorithme;algorithm;complejidad computacion;trapezoid graph;computational complexity;matching;polynomial time;graphe trapezoide;graphe cocomparabilite;arbol grafo;cocomparability graph;bipartite graph;design and analysis of algorithms;planar graph;algoritmo;tiempo polinomial	A matching in a graph is a set of edges no two of which share a common vertex. A matching M is an induced matching if no edge connects two edges of M . The problem of nding a maximum induced matching is known to be NP-Complete in general and speci cally for bipartite graphs and for 3-regular planar graphs. The problem has been shown to be polynomial for several classes of graphs. In this paper we generalize the results to wider classes of graphs, and improve the time complexity of previously known results. ? 2000 Elsevier Science B.V. All rights reserved.	induced subgraph;matching (graph theory);np-completeness;planar graph;polynomial;time complexity	Martin Charles Golumbic;Moshe Lewenstein	2000	Discrete Applied Mathematics	10.1016/S0166-218X(99)00194-8	strong perfect graph theorem;1-planar graph;time complexity;pathwidth;combinatorics;discrete mathematics;cograph;independent set;topology;bipartite graph;vertex cover;longest path problem;dense graph;graph theory;3-dimensional matching;hopcroft–karp algorithm;edge cover;graph coloring;clique-sum;mathematics;blossom algorithm;maximal independent set;modular decomposition;chordal graph;indifference graph;matching	Theory	22.061497145132414	26.76352759048799	132507
0a3688035860751d5cabf5a0da24d577779f7f3f	an efficient algorithm for generating necklaces with fixed density	arbre graphe;tratamiento datos;optimal solution;cat algorithm;complexite;computer program;solution optimale;generate;collier a densite fixe;collier;tree graph;lyndon word;efficient algorithm;coefficient binomial;complejidad;necklaces;recubrimiento;data processing;overlay;05 04;traitement donnee;68r15;complexity;recursividad;fixed density;recouvrement;algorithme;algorithm;fixed density necklace;recursivite;solucion optima;coeficiente binomial;mot lyndon;lyndon words;recursive algorithm;necklace;binomial coefficient;arbol grafo;68r05;programa computador;recursivity;difference covers;programme ordinateur;algoritmo	A k-ary necklace is an equivalence class of t-ary strings under rotation. A necklace of fixed density is a necklace where the number of zeroes is fixed. We present a fast, simple, recursive algorithm for generating (i.e., listing) fixed density X--ary necklaces or aperiodic necklaces. The algorithm is optimal in the sense that it runs in time proportional to the number of necklaces produced.	algorithm;recursion (computer science);turing completeness	Joe Sawada;Frank Ruskey	1999	SIAM J. Comput.	10.1137/S0097539798344112	recursion;combinatorics;discrete mathematics;complexity;data processing;computer science;lyndon word;mathematics;overlay;binomial coefficient;tree;algorithm;recursion	Theory	16.69261205963154	26.261156020393003	132716
9f8d688c6053d9b3d8bdd49d5211225dd7c0cf56	a minimal spanning tree algorithm applied to spatial cluster analysis	recursive partitioning;minimal spanning tree;cluster analysis;community networks;spatial clustering	Abstract   Abstract  Many applications, such as geographical regionalization and communication network districting, have problems that can be formulated as the clustering of  n  objects with contiguity constraints. In this paper, we present an algorithm based on recursive partitioning of a minimal spanning tree and a tool implementing it named SKATER. We also present some simulations where SKATER has been successfully used.	algorithm;cluster analysis;file spanning;minimum spanning tree	Juliano Palmieri Lage;Renato Assunção;Edna Afonso Reis	2001	Electronic Notes in Discrete Mathematics	10.1016/S1571-0653(04)00250-1	mathematical optimization;combinatorics;spanning tree;minimum spanning tree;machine learning;mathematics;cluster analysis;distributed minimum spanning tree;recursive partitioning	Theory	21.27091527038178	30.01247183043845	132805
9e00c34bb0b885d48adbf4669659dc8699be5a80	on the generation of permutations in magnetic bubble memories	generation of permutations;magnetic bubble memory generation of permutations;magnetic bubble memory	A better algorithm is presented for the generation of an arbitrary permutation in a model of magnetic bubble memories that have been investigated previously.	algorithm;bubble bobble	C. L. Chen	1978	IEEE Transactions on Computers	10.1109/TC.1978.1675022	parallel computing;computer hardware;computer science;operating system;bubble memory;mathematics;algorithm	Visualization	12.048765793293365	31.582359061793596	132818
169b49c17dcd351ee28f9e49fd97b708a788309a	polynomial threshold functions: structure, approximation and pseudorandomness	probabilistic method;approximation theory;computational complexity	We study the computational power of polynomial threshold functions, that is, threshold functions of real polynomials over the boolean cube. We provide two new results bounding the computational power of this model. Our first result shows that low-degree polynomial threshold functions cannot approximate any function with many influential variables. We provide a couple of examples where this technique yields tight approximation bounds. Our second result relates to constructing pseudorandom generators fooling lowdegree polynomial threshold functions. This problem has received attention recently, where Diakonikolas et al [13] proved that k-wise independence suffices to fool linear threshold functions. We prove that any low-degree polynomial threshold function, which can be represented as a function of a small number of linear threshold functions, can also be fooled by k-wise independence. We view this as an important step towards fooling general polynomial threshold functions, and we discuss a plausible approach achieving this goal based on our techniques. Our results combine tools from real approximation theory, hyper-contractive inequalities and probabilistic methods. In particular, we develop several new tools in approximation theory which may be of independent interest. ∗School of Computer Science, Raymond and Beverly Sackler Faculty of Exact Sciences, Tel Aviv University, Tel Aviv, Israel. Email: idobene@tau.ac.il †Weizmann Institute of Science, Rehovot, Israel. Email: shachar.lovett@weizmann.ac.il. Research supported by the Israel Science Foundation (grant 1300/05) ‡Centre for Mathematical Sciences, Wilberforce Road, Cambridge CB3 0WB, UK. Email: a.yadin@statslab.cam.ac.uk	approximation algorithm;approximation theory;boolean algebra;computation;computer science;email;polynomial;pseudorandom generator;pseudorandomness	Ido Ben-Eliezer;Shachar Lovett;Ariel Yadin	2009	CoRR		combinatorics;discrete mathematics;probabilistic method;mathematics;matrix polynomial;computational complexity theory;algorithm;statistics;approximation theory	Theory	11.561730682625406	21.830081489062586	132984
0285c8624e18a6cee37afdf16eccfa9223d5793c	solving the lpn problem in cube-root time		In this paper it is shown that given a sufficient number of (noisy) random binary linear equations, the Learning from Parity with Noise (LPN) problem can be solved in essentially cube root time in the number of unknowns. The techniques used to recover the solution are known from fast correlation attacks on stream ciphers. As in fast correlation attacks, the performance of the algorithm depends on the number of equations given. It is shown that if this number exceeds a certain bound, and the bias of the noisy equations is polynomial in number of unknowns n, the running time of the algorithm is reduced to 2 n 3 +o(n) compared to the brute force checking of all 2 n possible solutions. The mentioned bound is explicitly given and it is further shown that when this bound is exceeded, the complexity of the approach can even be further reduced.	algorithm;brute-force search;correlation attack;linear equation;polynomial;stream cipher;time complexity	Urs Wagner	2012	CoRR		combinatorics;discrete mathematics;mathematics;algorithm	Theory	10.70290902264473	23.815350957428038	133047
9e9052e2dd19987ec328466271df3036a436744d	asynchronous p systems		In this paper, the authors propose a new approach to fully asynchronous P systems, and a matching complexity measure, both inspired from the field of distributed algorithms. The authors validate the proposed approach by implementing several well-known distributed depth-first search (DFS) and breadth-first search (BFS) algorithms. Empirical results show that the proposed P algorithms have shorter descriptions and achieve a performance comparable to the corresponding distributed algorithms.	approximation algorithm;blum axioms;breadth-first search;computational complexity theory;depth-first search;distributed algorithm;p system	Tudor Balanescu;Radu Nicolescu;Huiling Wu	2011	IJNCR	10.4018/jncr.2011040101	computer science;theoretical computer science;machine learning;distributed computing	AI	19.482012518580156	21.076094551323237	133194
4ded0c58a99d0e6d04474e3186e4e0eb1b753eaf	session details: session 1d: graph algorithms			algorithm;list of algorithms	Peter Robinson	2018		10.1145/3258698	theoretical computer science;distributed computing;computer science;broadcasting;graph	ML	17.905223715802848	30.646197725509815	133345
0738f4cc71f9552c8d2c883f9be399786aa5728a	the complexity of some problems on subsequences and supersequences	longest common subsequence;computational complexity;shortest common supersequence	The complexity of finding the Longest Common Subsequence (LCS) and the Shortest Common Supersequence (SCS) of an arbRrary number of sequences IS considered We show that the yes/no version of the LCS problem is NP-complete for sequences over an alphabet of size 2, and that the yes/no SCS problem is NPcomplete for sequences over an alphabet of size 5	longest common subsequence problem;np-completeness	David Maier	1978	J. ACM	10.1145/322063.322075	shortest common supersequence;longest increasing subsequence;computer science;longest common subsequence problem;computational complexity theory;longest alternating subsequence;algorithm	Theory	15.36281093752697	23.000691508451734	133514
f95959ae0a7ee93c1a1d982d4e16810241e9c9ad	subgraphs generating algorithm for obtaining set of node-disjoint paths in terrain-based mesh graphs	battlefield simulation games;generic algorithm;simulation game;terrain based graph;node disjoint paths;mesh graph;subgraphs generating;terrain modeling;disjoint paths	In the article an algorithm (SGDP) for solving node-disjoint shortest K paths problem in mesh graphs is presented. The mesh graph can represent e.g. a discrete terrain model in a battlefield simulation. Arcs in the graph geographically link adjacent nodes only. The algorithm is based on an iterative subgraph generating procedure inside the mesh graph (for finding a single path from among K paths single subgraph is generated iteratively) and the usage of different strategies to find (and improve) the solution. Some experimental results with a discussion of the complexity and accuracy of the algorithm are shown in detail.	algorithm	Zbigniew Tarapata;Stefan Wroclawski	2010		10.1007/978-3-642-16958-8_37	block graph;lattice graph;suurballe's algorithm;factor-critical graph;cograph;universal graph;genetic algorithm;graph property;floyd–warshall algorithm;computer science;artificial intelligence;theoretical computer science;graph factorization;voltage graph;distance-hereditary graph;distributed computing;butterfly graph;complement graph;line graph	Theory	24.106762576286595	28.824360177063546	133621
f282d505a7a2fb45da6c5b95fefee40f838a6d23	representing sets with constant time equality testing	tratamiento datos;insertion;delecion;lenguaje programacion;metodo caso peor;compilateur;programming language;time complexity;sistema informatico;teoria conjunto;data processing;theorie ensemble;traitement donnee;worse case method;computer system;set theory;compiler;complexite temps;insercion;estructura datos;methode cas pire;space complexity;langage programmation;structure donnee;systeme informatique;complexite espace;complejidad tiempo;data structure;compilador;deletion	Most data structures for sets minimize the time spent on the operations insert, delete, and member. Using (I tree-based representation scheme, a sequence of N operations has complexity O(ZogN) per operation. However, if we slso sllow the operation equal(Si,Sj), where Si and Sj are two different sets, the complexity can be O(N) per operation, since set equality testing can require linear time. In this paper we present two tree-based representation schemes thst support the operation GqUal(S;*Sj) in constant time. The first scheme is very simple and establishes an upper bound of O(ZogN + L) pet operation for IL sequence of N operations, where k is the number of sets. A variation on this scheme uses hashing snd gives an expected complexity of U(k) per operation. Our second set representation achieves O(logN) amortiPed complexity per operation.	amortized analysis;data structure;time complexity;zobrist hashing	Daniel M. Yellin	1990		10.1016/0196-6774(92)90044-D	insertion;time complexity;combinatorics;compiler;discrete mathematics;data structure;data processing;computer science;proof of o(log*n) time complexity of union–find;mathematics;dspace;programming language;algorithm;set theory	Theory	15.86477463539197	27.350737238116686	133872
66c62f4924fb74969adff77804d795ea43477c41	on the intersection of independence systems	independence systems;k systems;stochastic probing	We study the properties of the intersection of independence systems. An independence system (V, I) is a k-system if for all S ⊆ V the ratio of the cardinality of the largest to the smallest maximal independent subset of S is at most k. We show that the intersection of a k1-system and a k2-system is a (k1 + k2)-system. As an application of our results, we show an improved approximation algorithm for stochastic probing with deadlines over k-systems.	approximation algorithm;maximal set;stochastic gradient descent	Julián Mestre	2015	Oper. Res. Lett.	10.1016/j.orl.2014.10.004	combinatorics;discrete mathematics;mathematics;algorithm	ECom	16.9547050953825	20.70781110373967	133873
4ce4384f1cd62d9c6502fe7eb46ba5c699727500	word-packing algorithms for dynamic connectivity and dynamic sets		We examine several (dynamic) graph and set intersection problems in the word-RAM model with word size w. We begin with Dynamic Connectivity where we need to maintain a fully dynamic graph G = (V,E) with n = |V | vertices while supporting (s, t)-connectivity queries. To do this, we provide a new simplified worst-case solution for the famous Dynamic Connectivity (which is interesting on its own merit), and then show how in the word-RAM model the query and update cost can be reduced to O( √ n · logn w log( w logn )), assuming w < n 1−Ω(1). Since w = Ω(log n), this bound is always O( √ n) and it is o( √ n) when w = ω(log n). We then examine the task of maintaining a family F of dynamic sets where insertions and deletions into the sets are allowed, while enabling a set intersection reporting query on sets S1, S2 ∈ F where we wish to report all of the elements in S1 ∩ S2. We first show that given a known upper-bound d on the size of any set, we can maintain F so that a set intersection reporting query costs O( d w/log2w ) expected time, and updates cost O(logw) expected time. Using this algorithm we can list all triangles of a graph G = (V,E) in O( mα w/log2w + t) expected time where m = |E|, α is the arboricity of G, and t is the size of the output. This is comparable with known algorithms that run in O(mα) time. Next, we provide an incremental data structure on F that supports intersection proof queries in which given S1 and S2 we wish to determine if they intersect, and if they do we require a proof (an element in the intersection). Both queries and insertions of elements into sets take O( √ N w/log2w ) expected time, where N = ∑ S∈F |S|. Finally, we provide time/space tradeoffs for the fully dynamic set intersection listing problem so that using M words of space each update costs O( √ M logN) expected time, each reporting query costs O( √ logN √ M √ op+ 1) expected time where op is the size of the output, and each proof query costs O( √ logN √ M +logN) expected time.	algorithm;arboricity;average-case complexity;best, worst and average case;data structure alignment;dynamic connectivity;random-access machine;random-access memory;set packing	Casper Kejlberg-Rasmussen;Tsvi Kopelowitz;Seth Pettie;Ely Porat	2014	CoRR		mathematical optimization;combinatorics;discrete mathematics;mathematics;algorithm	Theory	13.857457270996198	24.598464906410413	133937
e581a72dd47c6c7224d4741d22c07b2f91c06055	influence of tree topology restrictions on the complexity of haplotyping with missing data	evolutionary model;fixed parameter tractable;phylogenetic tree;theoretical analysis;computational complexity;perfect phylogeny haplotyping;exact algorithm;polynomial time;missing data;computational biology	Haplotyping, also known as haplotype phase prediction, is the problem of predicting likely haplotypes from genotype data. One fast haplotyping method is based on an evolutionary model where a perfect phylogenetic tree is sought that explains the observed data. Unfortunately, when data entries are missing as is often the case in laboratory data, the resulting incomplete perfect phylogeny haplotyping problem  ipph  is NP-complete and no theoretical results are known concerning its approximability, fixed-parameter tractability, or exact algorithms for it. Even radically simplified versions, such as the restriction to phylogenetic trees consisting of just two directed paths from a given root, are still NP-complete; but here a fixed-parameter algorithm is known. We show that such drastic and ad hoc simplifications are not necessary to make  ipph  fixed-parameter tractable: We present the first theoretical analysis of an algorithm, which we develop in the course of the paper, that works for arbitrary instances of  ipph  . On the negative side we show that restricting the topology of perfect phylogenies does not always reduce the computational complexity: while the incomplete directed perfect phylogeny problem is well-known to be solvable in polynomial time, we show that the same problem restricted to path topologies is NP-complete.	missing data	Michael Elberfeld;Ilka Schnoor;Till Tantau	2009		10.1007/978-3-642-02017-9_23	time complexity;combinatorics;phylogenetic tree;missing data;computer science;bioinformatics;mathematics;computational complexity theory;algorithm	ECom	17.694408137138538	22.143555604928125	133981
ccd6cbed9b392fce099b5daa945295cce17aede9	modular sieves for directed hamiltonian cycles		In this paper we make further progress on the following problem: can we detect if an n-vertex directed graph has a Hamiltonian cycle much faster than in 2 time? We first show that the number of Hamiltonian cycles in a directed n-vertex graph can be counted modulo prime powers p in (2−Ω(1)) expected time, for any constant prime p and k < (1 − λ)n/p for any arbitrary fixed λ > 0 . Previously a (2 − Ω(1)) algorithm was known only for counting modulo two [Björklund and Husfeldt, FOCS 2013]. We then give three applications of the counting result and the techniques used. I. We show that when we know that there are at most d Hamiltonian cycles in a directed graph for some arbitrarily large known constant d, we can detect the presence of a Hamiltonian cycle by a Las Vegas algorithm running in (2 − Ω(1)) expected time. Previously this was known to hold only for d < 1.035 [Björklund, Dell, and Husfeldt, ICALP 2015]. In particular we can count the Hamiltonian cycles in a directed graph of average outdegree d, in 2 poly(d) expected time. Such bounds were previously only known with a doubly-exponential dependency on the degree [Cygan and Pilipczuk, ICALP 2013]. II. We show that we can count the Hamiltonian cycles modulo 2 for any positive integer k in ∑k−1 i=0 ( n i ) 1.619 time, deterministically and using only polynomial space. As a direct consequence this result gives the same bounds but without any use of randomization as the detection algorithm from [Björklund, Dell, and Husfeldt, ICALP 2015]. III. We show that a Hamiltonian cycle in a directed graph of inplus outdegree at every vertex bounded by d can be detected by a Monte Carlo algorithm running in 2 ) time, using only polynomial space. This improves over the previously best algorithm that had an exponential dependency on d in the denominator of the Ω-expression [Björklund et al. TALG 2012].	average-case complexity;directed graph;hamiltonian (quantum mechanics);hamiltonian path;icalp;las vegas algorithm;modulo operation;monte carlo algorithm;pspace;polynomial;symposium on foundations of computer science;time complexity	Andreas Björklund;Ioannis Koutis	2016	CoRR		combinatorics;discrete mathematics;mathematics	Theory	20.742676165137105	23.164005339491162	134501
38ced6bb6c93ad7c904b743f79bdc46e265d83a8	a 3/2-approximation algorithm for augmenting the edge-connectivity of a graph from 1 to 2 using a subset of a given edge set	graph theory;teoria grafo;approximate algorithm;best approximation;problema np duro;theorie graphe;optimisation combinatoire;connected graph;np hard problem;graph connectivity;probleme recouvrement;problema recubrimiento;probleme np difficile;conectividad grafo;edge graph;arete graphe;covering problem;combinatorial optimization;connectivite graphe;graphe connexe;arista grafico;optimizacion combinatoria;grafo conexo	We consider the following problem: given a connected graph G = (V, ƐE) and an additional edge set E, find a minimum size subset of edges F ⊆ E suchth at (V, Ɛ ∪ F) is 2-edge connected. This problem is NP-hard. For a long time, 2 was the best approximation ratio known. Recently, Nagamochi reported a (1.875 + Ɛ)-approximation algorithm. We give a new algorithm with a better approximation ratio of 3/2 and a practical running time.	algorithm;edge dominating set	Guy Even;Jon Feldman;Guy Kortsarz;Zeev Nutov	2001		10.1007/3-540-44666-4_13	mathematical optimization;combinatorics;discrete mathematics;combinatorial optimization;connectivity;graph theory;mathematics;algorithm	Theory	21.568983748323443	26.845894234671366	134526
76a069160f116f54bca07f0521b54c92cc0ebc84	poly-logarithmic adaptive algorithms require revealing primitives	compare swap;fetch inc;renaming;collect;atomic snapshot	Abstract This paper studies the step complexity of adaptive algorithms, depending on the revealing properties of the primitives used, namely, how many processes are revealed when concurrently applying the primitive (in specific situations). When only 0 - or 1 -revealing primitives are used, e.g., reads, writes, testu0026set , compareu0026swap and LL/SC , then for any collect algorithm there is an execution in which k processes collectively perform Ω ( k 2 ) steps, provided k ∈ O ( log log n ) . This implies that any adaptive collect algorithm has an Ω ( k ) amortized (and hence, worst-case) step complexity in an execution with total contention k ∈ O ( log log n ) . The lower bound applies for snapshot and renaming, both one-shot and long-lived. While there are snapshot algorithms whose step complexity is polylogarithmic in n using only reads and writes, there is no adaptive algorithm whose step complexity is polylogarithmic in the contention, even when compareu0026swap and LL/SC are used. Primitives like fetchu0026inc are more revealing, and using them admits snapshot algorithms with O ( log k ) step complexity, where k is the total or the point contention. These algorithms combine a renaming algorithm with a mechanism for propagating values so they can be quickly collected. The main implication of these results is that the step complexity of adaptive algorithms depends on the revealing power of the primitives used. Even conditional primitives that allow to solve consensus for any number of processes, like compareu0026swap and LL/SC , do not improve the step complexity of adaptive algorithms.	adaptive algorithm;amortized analysis;best, worst and average case;ll grammar;ll parser;load-link/store-conditional;polylogarithmic function;snapshot (computer storage);test-and-set	Hagit Attiya;Arie Fouren	2017	J. Parallel Distrib. Comput.	10.1016/j.jpdc.2017.05.010	adaptive algorithm;parallel computing;distributed computing;snapshot (computer storage);swap (finance);logarithm;algorithm;log-log plot;computer science;upper and lower bounds	Theory	11.133646236644436	31.06862635671554	134648
eb873efd1d6a77d1c61efa252a2ec12988393aa0	parameterized certificate dispersal and its variants	parameterized complexity;turing kernelization;asymmetric cryptography;algorithmic graph theory	"""Given a directed graph G and a set R of vertex pairs, the Minimum Certificate Dispersal problem asks for an assignment of arcs to vertices (""""terminals"""") such that, for each ( u , v ) ? R , a u-v-path can be constructed using only arcs assigned to u or v. Herein, the total number k of assignments should be minimal. The problem is well motivated in key-exchange protocols for asymmetric cryptography. We provide a first parameterized complexity analysis of this NP-hard problem and its variant ChainedMinimum Certificate Dispersal, where, instead of pairs of terminals, a set of paths (""""chains"""") that should be constructed, is prescribed.Although polynomial-time solvable for constant values of k, the former variant seems much harder, surfacing in the proof that it is W1-hard with respect to k while ChainedMinimum Certificate Dispersal yields a polynomial-size problem kernel. We even show fixed-parameter tractability of the latter with respect to the stronger parameter """"number t of terminals"""". In particular, while there is no polynomial-size kernel with respect to t, the problem admits a polynomial-size Turing kernel. Towards answering the question whether Minimum Certificate Dispersal can be solved in polynomial time when t is constant, we show polynomial-time solvability for at most two requests (comprising all instances with t ? 2 ) using an algorithm for the Strong Distance problem, which asks for a minimum-size subdigraph in which two given vertices are strongly connected. Finally, we emphasize the hardness of Minimum Certificate Dispersal by proving it NP-hard for very restricted sets of instances, thereby excluding many parameters and combinations from consideration for efficient multivariate algorithms."""		Valentin Garnero;Mathias Weller	2016	Theor. Comput. Sci.	10.1016/j.tcs.2016.02.001	parameterized complexity;combinatorics;discrete mathematics;computer science;mathematics;certificate;public-key cryptography;algorithm;algebra	ECom	12.032062169765318	20.045253399386922	134788
2289325400975d7ff46af7ce6c25d03b60c16cd5	efficient parallel algorithms for planar st-graphs	camino mas corto;parallel computing;graph theory;algoritmo paralelo;shortest path;algorithms erew pram;representation graphique;multi selection;teoria grafo;parallel algorithm;algoritmo busqueda;digraph;subgrafo;crew pram;camino grafo;sorting;graph path;representacion grafica;algorithme recherche;recherche profondeur d abord;search algorithm;all pairs shortest path;partitioning;plus court chemin;theorie graphe;algorithme parallele;first depth search;graph connectivity;graphe pondere;sous graphe;informatique theorique;graphe planaire;conectividad grafo;chemin graphe;merging;depth first search;weighted graph;subgraph;grafo planario;connectivite graphe;graphics;planar graph;computer theory;informatica teorica;digraphe	Planar st-graphs find applications in a number of areas. In this paper, we present efficient parallel algorithms for solving several fundamental problems on planar st-graphs. The problems we consider include all-pairs shortest paths in weighted planar st-graphs, single-source shortest paths in weighted planar layered digraphs (which can be reduced to single-source shortest paths in certain special planar st-graphs), and depth-first search in planar st-graphs. Our parallel shortest path techniques exploit the specific geometric and graphic structures of planar stgraphs, and involve schemes for partitioning planar st-graphs into subgraphs in a way that ensures that the resulting path length matrices have a monotonieity property (1, 2]. The parallel algorithms we obtain are considerable improvement over the previously best known solutions (when they are applied to these st-graph problems), and are in fact relatively simple. The parallel computational models we use are the CREW PRAM and EREW PRAM.	computational model;depth-first search;parallel algorithm;parallel random-access machine;shortest path problem	Mikhail J. Atallah	1997		10.1007/3-540-63890-3_25	combinatorics;computer science;graph theory;theoretical computer science;planar straight-line graph;mathematics;shortest path problem;algorithm	Theory	18.912339270386408	28.374191278435575	134829
13f5189ffd3b0e9cba46c1aafd186ad122c52ee4	an algebraic approach to complexity of data stream computations	algebraic approach;data stream computation;data stream;relative error;computational complexity;process model;lower bound	We consider a basic problem in the general data streaming model, namely, to estimate a vector f ∈ Z that is arbitrarily updated (i.e., incremented or decremented) coordinatewise. The estimate f̂ ∈ Z must satisfy ‖f̂ − f‖∞ ≤ ǫ‖f‖1, that is, ∀i (|f̂i − fi| ≤ ǫ‖f‖1). It is known to have Õ(ǫ) randomized space upper bound [4], Ω(ǫ log(ǫn)) space lower bound [2] and deterministic space upper bound of Ω̃(ǫ) bits. We show that any deterministic algorithm for this problem requires space Ω(ǫ−2(log‖f‖1)) bits.	complexity;computation;dspace;deterministic algorithm;linear algebra;randomized algorithm	Sumit Ganguly	2007	CoRR		approximation error;combinatorics;discrete mathematics;process modeling;mathematics;upper and lower bounds;computational complexity theory;algorithm	Theory	13.606271569892227	23.94169081523044	134911
666b6e9e45666c39ae0a440df60c1276fac2d198	randomized rounding for routing and covering problems: experiments and improvements	randomized rounding;greedy heuristic;satisfiability;rounding errors;hybrid approach;experimental evaluation;covering problem;data structure	Following previous theoretical work by Srinivasan (FOCS 2001) and the first author (STACS 2006) and a first experimental evaluation on random instances (ALENEX 2009), we investigate how the recently developed different approaches to generate randomized roundings satisfying disjoint cardinality constraints behave when used in two classical algorithmic problems, namely low-congestion routing in networks and max-coverage problems in hypergraphs. We generally find that all randomized rounding algorithms work well, much better than what is guaranteed by existing theoretical work. The derandomized versions produce again significantly better rounding errors, with running times still negligible compared to the one for solving the corresponding LP. It thus seems worth preferring them over the randomized variants. The data created in these experiments lets us propose and investigate the following new ideas. For the low-congestion routing problems, we suggest to solve a second LP, which yields the same congestion, but aims at producing a solution that is easier to round. Experiments show that this reduces the rounding errors considerably, both in combination with randomized and derandomized rounding. For the max-coverage instances, we generally observe that the greedy heuristics also performs very good. We develop a strengthened method of derandomized rounding, and a simple greedy/rounding hybrid approach using greedy and LP-based rounding elements, and observe that both these improvements yield again better solutions than both earlier approaches on their own. For an important special case of max-coverage, namely unit disk max-domination, we also develop a PTAS. Contrary to all other algorithms investigated, it performs not much better in experiments than in theory. In consequence, unless extremely good solutions are to be obtained with huge computational resources, greedy, LP-based rounding or hybrid approaches are preferable.	computation;computational resource;covering problems;dominating set;experiment;greedy algorithm;heuristic (computer science);network congestion;ptas reduction;randomized algorithm;randomized rounding;referring expression generation;round-off error;routing;stacs;symposium on foundations of computer science	Benjamin Doerr;Marvin Künnemann;Magnus Wahlström	2010		10.1007/978-3-642-13193-6_17	greedy randomized adaptive search procedure;mathematical optimization;randomized rounding;combinatorics;greedy algorithm;data structure;computer science;mathematics;programming language;algorithm;satisfiability	Theory	21.080519544025478	19.03253513687954	134961
3e72195e3c2b8226294327e3fa72e34597a1146c	edge-packing in planar graphs	approximate algorithm;graph isomorphism;technical report departmental;historical collection till dec 2001;computational complexity;linear time;polynomial time;planar graph	Maximum G Edge-Packing (EPack G ) is the problem of finding the maximum number of edge-disjoint isomorphic copies of a fixed guest graph G in a host graph H . This paper investigates the computational complexity of edge-packing for planar guests and planar hosts. Edge-packing is solvable in polynomial time when both G and H are trees. Edge-packing is solvable in linear time when H is outerplanar and G is either a 3-cycle or a k -star (a graph isomorphic to K 1,k ). Edge-packing is NP-complete when H is planar and G is either a cycle or a tree with $\geq 3$ edges. A strategy for developing polynomial-time approximation algorithms for planar hosts is exemplified by a linear-time approximation algorithm that finds a k -star edge-packing of size at least half the optimal.	a* search algorithm;approximation algorithm;computational complexity theory;connected component (graph theory);cycle (graph theory);decision problem;edge dominating set;np-completeness;outerplanar graph;planar graph;polynomial;set packing;time complexity	Lenwood S. Heath;John Paul C. Vergara	1998	Theory of Computing Systems	10.1007/s002240000107	1-planar graph;outerplanar graph;time complexity;graph power;pathwidth;combinatorics;discrete mathematics;polyhedral graph;branch-decomposition;graph embedding;graph bandwidth;graph canonization;computer science;theoretical computer science;forbidden graph characterization;graph coloring;planar straight-line graph;subgraph isomorphism problem;mathematics;tutte polynomial;graph isomorphism;butterfly graph;cycle basis;graph minor;book embedding;algorithm;planar graph	Theory	24.17291103806487	25.45714222766115	134984
fcc1170f70c4f947be1795713d2cdbf1c72c21ad	computational completeness of path-structured graph-controlled insertion-deletion systems		A graph-controlled insertion-deletion (GCID) system is a regulated extension of an insertion-deletion system. It has several components and each component contains some insertion-deletion rules. These components are the vertices of a directed control graph. A rule is applied to a string in a component and the resultant string is moved to the target component specified in the rule, describing the arcs of the control graph. We investigate which combinations of size parameters (the maximum number of components, the maximal length of the insertion string, the maximal length of the left context for insertion, the maximal length of the right context for insertion; a similar three restrictions with respect to deletion) are sufficient to maintain the computational completeness of such restricted systems with the additional restriction that the control graph is a path, thus, these results also hold for ins-del P systems.	computation;insertion sort	Henning Fernau;Lakshmanan Kuppusamy;Indhumathi Raman	2017		10.1007/978-3-319-60134-2_8	combinatorics;completeness (statistics);vertex (geometry);computational resource;graph;computer science	Logic	18.114631300398994	23.62944445620044	135023
db3e6a63193bee8f2ab6364d9261849be27ff826	on parallel evaluation of game trees	and or tree;parallel algorithm;parallel speedup;agr;pruning;α β pruning;lower bound;bgr	A class of parallel algorithms for evaluating game trees is presented. These algorithms parallelize a standard sequential algorithm for evaluating AND/OR trees and the α-β pruning procedure for evaluating MIN/MAX trees. It is shown that, uniformly on all instances of uniform AND/OR trees, the parallel AND/OR tree algorithm achieves an asymptotic linear speedup using a polynomial number of processors in the height of the tree. The analysis of linear speedup using more than a linear number of processors is due to J. Harting. A numerical lower bound rigorously establishes a good speedup for the uniform AND/OR trees with parameters that are typical in practice. The performance of the parallel α-β algorithm on best-ordered MIN/MAX trees is analyzed.	alpha–beta pruning;central processing unit;list of algorithms;max;numerical analysis;parallel algorithm;polynomial;sequential algorithm;speedup	Richard M. Karp;Yanjun Zhang	1998	J. ACM	10.1145/293347.293353	mathematical optimization;speedup;computer science;theoretical computer science;pruning;machine learning;parallel algorithm;weight-balanced tree;upper and lower bounds;and–or tree	Theory	10.140704926030157	31.78625234123596	135196
07d9d384a3103ad72b1b296079852faa929ed8c2	integer programming-based method for grammar-based tree compression and its application to pattern extraction of glycan tree structures	approximate algorithm;data compression;right hand side;polysaccharides;computational biology bioinformatics;computer experiment;context free grammar;tree structure;algorithms;combinatorial libraries;integer program;computational biology;computer appl in life sciences;production rule;microarrays;bioinformatics	A bisection-type algorithm for the grammar-based compression of tree-structured data has been proposed recently. In this framework, an elementary ordered-tree grammar (EOTG) and an elementary unordered-tree grammar (EUTG) were defined, and an approximation algorithm was proposed. In this paper, we propose an integer programming-based method that finds the minimum context-free grammar (CFG) for a given string under the condition that at most two symbols appear on the right-hand side of each production rule. Next, we extend this method to find the minimum EOTG and EUTG grammars for given ordered and unordered trees, respectively. Then, we conduct computational experiments for the ordered and unordered artificial trees. Finally, we apply our methods to pattern extraction of glycan tree structures. We propose integer programming-based methods that find the minimum CFG, EOTG, and EUTG for given strings, ordered and unordered trees. Our proposed methods for trees are useful for extracting patterns of glycan tree structures.	approximation algorithm;compression;computation;context-free grammar;context-free language;experiment;integer (number);integer programming;nut hypersensitivity;pattern recognition;polysaccharides;production (computer science);tree (data structure);trees (plant)	Yang Zhao;Morihiro Hayashida;Tatsuya Akutsu	2010		10.1186/1471-2105-11-S11-S4	data compression;id/lp grammar;computer experiment;dna microarray;grammar induction;computer science;bioinformatics;theoretical computer science;polysaccharide;tree structure;context-free grammar;grammar-based code;stochastic context-free grammar;algorithm	NLP	14.49639489094918	26.01603852414724	135347
7595a5af534e773d83a673ffc63f3092b1c0288b	vertex disjoint paths on clique-width bounded graphs	arbre graphe;graphe lineaire;camino grafo;tree graph;graph path;probleme np complet;vertex;temps lineaire;graph clique;tiempo lineal;grafo lineal;informatique theorique;linear time;chemin graphe;problema np completo;vertice;clique graphe;arbol grafo;grafo completo;complete graph;graphe complet;np complete problem;disjoint paths;linear graph;computer theory;informatica teorica	We show that I vertex disjoint paths between I pairs of vertices can be found in linear time for co-graphs but is NP-complete for graphs of NLC-width at most 4 and clique-width at most 7. This is the first inartificial graph problem known to be NP-complete on graphs of bounded clique-width but solvable in linear time on co-graphs and graphs of bounded tree-width.	clique-width	Frank Gurski;Egon Wanke	2004		10.1007/978-3-540-24698-5_16	1-planar graph;clique;time complexity;pathwidth;vertex;combinatorics;discrete mathematics;cograph;np-complete;interval graph;independent set;topology;bipartite graph;longest path problem;metric dimension;vertex;clique-sum;trapezoid graph;mathematics;maximal independent set;linear equation;modular decomposition;treewidth;partial k-tree;complete graph;chordal graph;indifference graph;tree;neighbourhood	Theory	21.782572551313585	27.48662276774918	135446
457fffe3a48aa115708b5a32010d13db67026277	minimum cost flows with minimum quantities	graphe biparti;flow;procesamiento informacion;temps polynomial;algorithm analysis;complexite calcul;grafo bipartido;probleme np complet;aproximacion;oleada;minimum cost flow;approximation;series parallel graph;41a10;complejidad computacion;graphe serie parallele;computational complexity;informatique theorique;exact algorithm;information processing;polynomial time;borne inferieure;coste;approximation scheme;algorithme polynomial;fully polynomial time approximation scheme;problema np completo;analyse algorithme;flot;traitement information;grafo completo;complete graph;graphe complet;bipartite graph;analisis algoritmo;lower bound;np complete problem;cota inferior;computer theory;cout;tiempo polinomial;informatica teorica	We consider a variant of the well-known minimum cost flow problem where the flow on each arc in the network is restricted to be either zero or above a given lower bound. The problem was recently shown to be weakly NP-complete even on series-parallel graphs. We start by showing that the problem is strongly NP-complete and cannot be approximated in polynomial time (unless P=NP) up to any polynomially computable function even when the graph is bipartite and the given instance is guaranteed to admit a feasible solution. Moreover, we present a pseudo-polynomial-time exact algorithm and a fully polynomial-time approximation scheme (FPTAS) for the problem on series-parallel graphs.		Sven Oliver Krumke;Clemens Thielen	2011	Inf. Process. Lett.	10.1016/j.ipl.2011.03.007	time complexity;mathematical optimization;combinatorics;polynomial-time approximation scheme;np-complete;flow;bipartite graph;minimum-cost flow problem;input/output;information processing;computer science;approximation;calculus;mathematics;upper and lower bounds;computational complexity theory;complete graph;algorithm	DB	20.608826511344738	26.472415655858768	135649
abbf2b279d4b56aef53aca0bfbc5447532fd908a	a parallel ring ordering algorithm for efficient one-sided jacobi svd computations	calcul matriciel;algorithm performance;complexite calcul;sorting;etude experimentale;implementation;singular value decomposition;tria;parallel computation;algorithme;algorithm;ejecucion;complejidad computacion;calculo paralelo;indexing;computational complexity;resultado algoritmo;methode jacobi;indexation;triage;indizacion;performance algorithme;metodo jacobi;matrix calculus;calcul parallele;estudio experimental;calculo de matrices;jacobi method;algoritmo	In this paper we give evidence to show that in one-sided Jacobi SVD computation the sorting of column norms in each sweep is very important. An e cient parallel ring Jacobi ordering for computing singular value decomposition is described. This ordering can generate n(n 1)=2 di erent index pairs and sort column norms at the same time. The one-sided Jacobi SVD algorithm using this parallel ordering converges in about the same number of sweeps as the sequential cyclic Jacobi algorithm. The issue of equivalence of orderings for one-sided Jacobi is also discussed. We show how an ordering which does not sort column norms into order may still perform e ciently as long as it can generate the same index pairs at the same step as one which does sorting. Some experimental results on a Fujitsu AP1000 are presented.	algorithm;computation;converge;jacobi method;singular value decomposition;sorting;turing completeness	Bing Bing Zhou;Richard P. Brent	1997	J. Parallel Distrib. Comput.	10.1006/jpdc.1997.1304	search engine indexing;combinatorics;jacobi method;jacobi method for complex hermitian matrices;matrix calculus;computer science;sorting;jacobi eigenvalue algorithm;calculus;mathematics;jacobi rotation;computational complexity theory;implementation;singular value decomposition;algorithm	HPC	13.779859142843168	30.87237934614214	135676
e7cd4fafa540c104b7631347d6e89bbd40b0bab6	a connection between circular colorings and periodic schedules	28cxx;numero cromatico;computadora;graph theory;period;optimisation;communication donnee;coloracion grafo;periodic scheduling;teoria grafo;nombre entier;combinatorics;translating;digraph;circular chromatic number;optimizacion;ordinateur;combinatoria;vertex;possibilite;edge weighted graph;periodo;combinatoire;nombre chromatique;digrafo;modele calcul;graph coloring;chromatic number;horario;numero real;computer;theorie graphe;data communication;input;methode calcul;parallel computation;metodo calculo;traduction;integer;enquete;graphe pondere;calculo paralelo;coloration graphe;arbre marque;grafo pondero;periode;graph coloring problem;borne electrique;timed marked graph;informatique theorique;scheduling;conexion;directed graph;entero;vertex graph;68r10;graphe oriente;raccordement;edge graph;entree ordinateur;borne electrico;68xx;schedule;coste;parallel computer;communication cost;superficie;05cxx;termination;arete graphe;area;68q10;traduccion;grafo orientado;optimization;28bxx;vertice;encuesta;terminaison;weighted graph;real number;entrada ordenador;nombre reel;survey;calcul parallele;connection;minty s theorem;vertice grafo;computing method;ordonnancement;horaire;arista grafico;sommet graphe;reglamento;graph colouring;computer theory;68m20;cout;graphe colore;informatica teorica;digraphe;05c15	We show that there is a curious connection between circular colorings of edge-weighted digraphs and periodic schedules of timed marked graphs. Circular coloring of an edgeweighted digraphwas introduced byMohar [B. Mohar, Circular colorings of edge-weighted graphs, J. Graph Theory 43 (2003) 107–116]. This kind of coloring is a very natural generalization of several well-known graph coloring problems including the usual circular coloring [X. Zhu, Circular chromatic number: A survey, DiscreteMath. 229 (2001) 371–410] and the circular coloring of vertex-weighted graphs [W. Deuber, X. Zhu, Circular coloring of weighted graphs, J. Graph Theory 23 (1996) 365–376]. Timed marked graphs E G [R.M. Karp, R.E. Miller, Properties of a model for parallel computations: Determinancy, termination, queuing, SIAM J. Appl. Math. 14 (1966) 1390–1411] are used, in computer science, tomodel the data movement in parallel computations, where a vertex represents a task, an arc uv with weight cuv represents a data channel with communication cost, and tokens on arc uv represent the input data of task vertex v. Dynamically, if vertex u operates at time t , then u removes one token from each of its in-arc; if uv is an out-arc of u, then at time t + cuv vertex u places one token on arc uv. Computer scientists are interested in designing, for each vertex u, a sequence of time instants {fu(1), fu(2), fu(3), . . .} such that vertex u starts its kth operation at time fu(k) and each in-arc of u contains at least one token at that time. The set of functions {fu : u ∈ V (E G)} is called a schedule of E G. Computer scientists are particularly interested in periodic schedules. Given a timed marked graph E G, they ask if there exist a period p > 0 and real numbers xu such that E G has a periodic schedule of the form fu(k) = xu + p(k − 1) for each vertex u and any positive integer k. In this note we demonstrate an unexpected connection between circular colorings and periodic schedules. The aim of this note is to provide a possibility of translating problems and methods from one area of graph coloring to another area of computer science. © 2008 Elsevier B.V. All rights reserved.	c date and time functions;channel (communications);circular coloring;computation;computer science;computer scientist;execution unit;existential quantification;graph coloring;graph theory;marked graph;schedule (computer science)	Hong-Gwa Yeh	2009	Discrete Applied Mathematics	10.1016/j.dam.2008.10.003	combinatorics;discrete mathematics;directed graph;fractional coloring;graph theory;complete coloring;graph coloring;mathematics;greedy coloring;algorithm	Theory	17.98717838115059	28.918627997663855	135681
2030b42b1ee5d4347ac7ad4faba3a736f09b910a	testing the expansion of a graph	maximum degree;probability;maximo;grado grafo;loi probabilite;voisinage;ley probabilidad;vertex;maximum;eigenvalues;68wxx;algorithme;algorithm;informatique theorique;probability distribution;probabilidad;68r10;probabilite;vertice;degre graphe;graph degree;computer theory;expansion;algoritmo;informatica teorica	We study the problem of testing the expansion of graphs with bounded degree d in sublinear time. A graph is said to be an αexpander if every vertex set U ⊂ V of size at most 1 2 |V | has a neighborhood of size at least α|U |. We show that the algorithm proposed by Goldreich and Ron [9] (ECCC-2000) for testing the expansion of a graph distinguishes with high probability between α-expanders of degree bound d and graphs which are -far from having expansion at least Ω(α). This improves a recent result of Czumaj and Sohler [3] (FOCS-07) who showed that this algorithm can distinguish between α-expanders of degree bound d and graphs which are -far from having expansion at least Ω(α/ log n). It also improves a recent result of Kale and Seshadhri [11] (ECCC-2007) who showed that this algorithm can distinguish between α-expanders and graphs which are -far from having expansion at least Ω(α) with twice the maximum degree. Finally, our result shows that the conjecture of Goldreich and Ron [9], on testing the second eigenvalue of the graph, holds when the second eigenvalue lies in a certain interval of constant size. Our methods combine the techniques of [3], [9] and [11]. Department of Mathematics, U.C. Berkeley. Research supported in part by NSF grant #DMS-0605166. Microsoft Research	algorithm;degree (graph theory);ibm notes;microsoft research;time complexity;with high probability	Asaf Nachmias;Asaf Shapira	2007	Inf. Comput.	10.1016/j.ic.2009.09.002	probability distribution;vertex;combinatorics;discrete mathematics;topology;eigenvalues and eigenvectors;probability;mathematics;statistics	Theory	23.154670235132933	31.23350992658331	135696
8fda8aa47c017bb72ca32f42336beb213e92b320	designing a truthful mechanism for a spanning arborescence bicriteria problem	graph theory;bicriteria network design;network design;multiagent system;reseau communication;settore inf 01 informatica;teoria grafo;arborescence;digraph;arborescencia;cost function;algorithmic mechanism design;truthful single minded mechanisms;egoisme;arbre maximal;digrafo;selfishness;treeing;satisfiability;multi parameter agents;theorie graphe;funcion logaritmica;logarithmic function;arbol maximo;community networks;egoismo;directed graph;graphe oriente;fonction logarithmique;grafo orientado;spanning tree;sistema multiagente;red de comunicacion;communication network;systeme multiagent;digraphe	Let a communication network be modelled by a directed graph G=(V,E) of n nodes and m edges, and assume that each edge is owned by a selfish agent, which privately holds a pair of values associated with the edge, namely its cost and its length. In this paper we analyze the problem of designing a truthful mechanism for computing a spanning arborescence of G rooted at a fixed node r ∈V having minimum cost (as computed w.r.t. the cost function) among all the spanning arborescences rooted at r which satisfy the following constraint: for each node, the distance from r (as computed w.r.t. the length function) must not exceed a fixed bound associated with the node. First, we prove that the problem is hard to approximate within better than a logarithmic factor, unless NP admits slightly superpolynomial time algorithms. Then, we provide a truthful single-minded mechanism for the problem, which guarantees an approximation factor of (1+e)(n–1), for any e>0.		Davide Bilò;Luciano Gualà;Guido Proietti	2006		10.1007/11922377_3	mathematical optimization;combinatorics;directed graph;artificial intelligence;graph theory;mathematics;distributed computing;algorithm	ECom	19.218139000405408	32.131814850350935	135721
7b183fa9c00792fcb32a6745f16a5fdc4c5c2a96	on survivable set connectivity		In the Set Connectivity problem, we are given an n-node edge-weighted undirected graph and a collection of h set pairs (Si, Ti), where Si and Ti are subsets of nodes. The goal is to compute a min-cost subgraph H so that, for each set pair (Si, Ti), there exists at least one path in H between some node in Si and some node in Ti. In this paper, we initiate the study of the Survivable Set Connectivity problem (SSC), i.e., the generalization of Set Connectivity where we are additionally given an integer requirement ki ≥ 1 for each set pair (Si, Ti), and we want to find a min-cost subgraph H so that there are at least ki edge-disjoint paths in H between Si and Ti. We achieve the following main results: • We show that there is no poly-logarithmic approximation for SSC unless NP has a quasi-polynomial time algorithm. This result is based on a reduction from the Minimum Label Cover problem, and the result holds even for the special case where Si = {r} for all i, i.e., for the high-connectivity variant of the classical Group Steiner Tree problem. More precisely, we prove an approximability lower bound of 2 1− n for SSC, for any constant > 0, which is almost polynomial on n. A technical novelty of our proof is the first use of a padding scheme technique for an edge-connectivity problem on undirected graphs. (Prior to our results, the applications of this technique only pertain to either node-connectivity problems or problems on directed graphs). • We present a bicriteria approximation algorithm for SSC that computes a solution H of cost at most ∗This work was partially done while the first and third authors were working at IDSIA. Partially supported by the ERC Starting Grant NEWNET 279352, Swiss National Science Foundation project 20002	approximation algorithm;directed graph;graph (discrete mathematics);k-edge-connected graph;maxima and minima;p (complexity);padding (cryptography);polynomial;quasi-polynomial;sql server compact;steiner tree problem;time complexity	Parinya Chalermsook;Fabrizio Grandoni;Bundit Laekhanukit	2015		10.1137/1.9781611973730.3	mathematical optimization;combinatorics;discrete mathematics;mathematics;geometry;algorithm	Theory	23.04921468773501	19.86431803887397	135845
1a499e011e1728988b6f5bc3d3c924ed7166b265	a probabilistic approach to problems parameterized above or below tight bounds	kernel;probabilistic method;fixed parameter tractable;probabilistic approach;above tight bounds;upper bound;hypercontractive inequality;lower bound;parameterized problems	We introduce a new approach for establishing fixed-parameter tractability of problems parameterized above tight lower bounds or below tight upper bounds. To illustrate the approach we consider three problems of this type of unknown complexity that were introduced by Mahajan, Raman and Sikdar (J. Comput. Syst. Sci. 75, 2009). We show that a generalization of one of the problems and non-trivial special cases of the other two are fixed-parameter tractable.	cobham's thesis;parameterized complexity;raman scattering	Gregory Gutin;Eun Jung Kim;Stefan Szeider;Anders Yeo	2011	J. Comput. Syst. Sci.	10.1016/j.jcss.2010.06.001	mathematical optimization;combinatorics;discrete mathematics;mathematics;upper and lower bounds	Theory	17.10547564236352	19.83296057819653	135913
3b383401427f0d557510e43f07922d961270bab2	r-by-c crozzle: an np-hard problem	crozzle;complexity;set covering problem;polynomial time algorithm;np hard problem;np hard	"""In an Australian magazine, a monetary prize is awarded to the person with the best answer to a word puzzle called a Crozzle. The placement of words into a ten by fifteen grid obtaining the highest score is the best answer. Various search techniques have been employed to solve this problem, yet no one has shown whether there is a polynomial-time algorithm to find the best Crozzle. This paper creates a similar word puzzle, called R-by-C Crozzle, by lifting the constraint on the grid size. R-by-C Crozzle is not in NP, but there exists a polynomial reduction to it from the exact 3-set cover problem. Thus the R-by-C Crozzle is NP-hard. This paper also explores any implications that the complexity of the R-by-C Crozzle might have on the complexity of the original problem. I N T R O D U C T I O N The Crozzle is a word puzzle that appears in The Australian Women's Weekly magazine. The object of the game is to place words from a given list into a ten by fifteen empty grid such that the score of the arrangement is maximal. Only words from the current word list can be used, and they can appear at most once in the grid. The re.suiting structure of words must be one interlocking grid. This means that no word can be free-standing. Also, a blank must separate words in a single column or row. The scoring rules give each word appearing in the answer ten points, and each letter in the intersection of two words scores additional points according to Table I. Different search algorithms have been utilized and modified to find the best arrangement [2,3]; however, no one has established the complexity of the Crozzle problem. Problems are classified by their computational complexity into sets which include P, NP, NP-complete and NP-hard. The following are brief descriptions of these well-known classes. For a rigorous mathematical description of complexity theory, the reader is referred to """"Permission to make digital/hard copy of all or part of this material without fee is granted provided that copies are not made or distributed for profit or commercial advantage, the ACM copyright/server notice, the title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery, Inc.(ACM). To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee."""" © 1996 ACM 0-89791-820-7 96 0002 3.50 Table I Scores for intersection letters a, b, c, d, e, f . . . . . : . : : o , . : . : . . . . . . . . . 2 g, h, i,j , k, 1 . . . . . . . . . . . . . . . . . . . . . . . 4 m, n, o, p, q, r . . . . . . . . . . . . . . . . . . . . 8 s, t, u, v, w, x . . . . . . . . . . . . . . . . . . . . . 16 y . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 z . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 Garey and Johnson's work [1] in which they use Turing machines as the model of computation. P is the class of problems that have polynomialtime algorithms to find the solutions, whereas.problems in NP must have p01ynomial-time algorithms to check the validity of possible solutions. It is easily shown that P is subset of NP, which means that a problem that has been shown not to be in NP will never have a polynomial time . , 4"""	computational complexity theory;computer science;information;lambda lifting;maximal set;michael garey;microsoft word for mac;model of computation;np (complexity);np-completeness;np-hardness;p (complexity);polynomial;search algorithm;server (computing);the australian;time complexity;turing machine;yahoo! answers	Michelle Gower;Ralph W. Wilkerson	1996		10.1145/331119.331148	time complexity;continuous knapsack problem;quadratic unconstrained binary optimization;mathematical optimization;max-3sat;function problem;fp;polynomial-time reduction;set packing;np;apx;computer science;cutting stock problem;karp–lipton theorem;p versus np problem;np-hard;cook–levin theorem;valiant–vazirani theorem;np-easy;pseudo-polynomial time;co-np-complete	Theory	16.998489542589567	18.802654228677863	136116
abfd22a0d8ce859fa3c84e8e61b662d3abbb0a57	space-efficient construction algorithm for the circular suffix tree	space-efficient construction algorithm;n log;succinct space;n denotes;n log n;suffix tree;construction algorithm;circular dictionary;log n;circular suffix tree;efficient construction algorithm	Hon et al. (2011) recently proposed a variant of suffix tree, called circular suffix tree, and showed that it can be compressed into succinct space and can be used to solve the circular dictionary matching problem efficiently. Although there are several efficient construction algorithms for the suffix tree in the literature, none of them can be applied directly to construct circular suffix tree due to the different nature of the patterns being indexed. Here, we give the first construction algorithm for the circular suffix tree, which takes O(n log n) time and requires O(n log s + d log n)$ bits of working space, where n denotes the total length of the patterns in the dictionary, d denotes the number of patterns, and s denotes the alphabet size.	algorithm;suffix tree	Wing-Kai Hon;Tsung-Han Ku;Rahul Shah;Sharma V. Thankachan	2013		10.1007/978-3-642-38905-4_15	generalized suffix tree;longest common substring problem;k-ary tree;compressed suffix array	Theory	13.198438626639723	27.17601702570902	136166
eab31bda1610dab0064fb81ab806f7c9488f6cca	nontrivial lower bounds for some np-problems on directed graphs	graph theory;turing machine;complexity class;directed graph;linear time;lower bound;np complete problem	NP-complete problems are believed to be not in P. But only a very few NP-complete problems, and none concerning graph theory, are proved to have a nontrivial time lower bound (i.e. not to be solvable in linear time on a DTM (i.e. deterministic Turing machine). A problem L  NP is linearly NP-complete if any problem in Ntime (n) can be reduced to it in linear time on a DTM. It follows from the separation result between deterministic and nondeterministic linear-time complexity classes [PPST83], that a linearly NP-complete problem has a nontrivial time lower bound. We present in this paper the first natural problems on graphs which are linearly NP-complete.		Solomampionona Ranaivoson	1990		10.1007/3-540-54487-9_68	1-planar graph;implicit graph;outerplanar graph;time complexity;pathwidth;complexity class;mathematical optimization;combinatorics;discrete mathematics;feedback arc set;np-complete;directed graph;null graph;graph property;computer science;turing machine;clique-width;graph theory;pancyclic graph;comparability graph;graph automorphism;aperiodic graph;mathematics;voltage graph;graph;np-easy;upper and lower bounds;complement graph;book embedding;line graph;algorithm;circulant graph	Theory	22.335958224912744	24.1365879961175	136401
411eea5497009efa1955a4ad6ae045323ddd207f	reconstructing a minimum spanning tree after deletion of any node	graph node;algoritmo paralelo;ajustamiento modelo;reseau communication;parallel algorithm;crew pram;nudo grafo;arbre maximal;telecommunication network;algorithme parallele;ajustement modele;miniaturisation;arbol maximo;community networks;red telecomunicacion;nodal network;model matching;minimum spanning tree;reseau telecommunication;time use;micromaquina;micromachine;miniaturization;spanning tree;miniaturizacion;reseau nodal;red de comunicacion;communication network;noeud graphe	Abstract. Updating a minimum spanning tree (MST) is a basic problem for communication networks. In this paper we consider single node deletions in MSTs. Let G=(V,E) be an undirected graph with n nodes and m edges, and let T be the MST of G . For each node v in V , the node replacement for v is the minimum weight set of edges R(v) that connect the components of T-v . We present a sequential algorithm and a parallel algorithm that find R(v) for all V simultaneously. The sequential algorithm takes O(m log n) time, but only O(m α (m,n)) time when the edges of E are presorted by weight. The parallel algorithm takes O(log 2 n) time using m processors on a CREW PRAM.	central processing unit;file spanning;graph (discrete mathematics);minimum spanning tree;minimum weight;parallel algorithm;sequential algorithm;telecommunications network	B. Das;Michael C. Loui	2001	Algorithmica	10.1007/s00453-001-0061-3	combinatorics;minimum degree spanning tree;spanning tree;telecommunications;computer science;minimum spanning tree;mathematics;distributed minimum spanning tree;algorithm;telecommunications network;shortest-path tree	Theory	21.206039215245752	28.59257163652843	136616
fafc426e2d213de79a8cc123273e4064f5d62032	constructions used in associative parallel algorithms for directed graphs	the single source shortest path;transitive closure of a directed graph;access data by contents;spanning tree;adjacency matrix	The paper selects constructions that are used to implement a group of algorithms for directed graphs on a model of associative parallel systems with vertical processing the STAR---machine. Moreover, a new implementation on the STAR---machine of Dijkstra's algorithm for finding the single---source shortest paths is proposed.	parallel algorithm	Anna Nepomniaschaya	2015		10.1007/978-3-319-21909-7_19	transitive reduction;combinatorics;discrete mathematics;longest path problem;spanning tree;floyd–warshall algorithm;theoretical computer science;yen's algorithm;mathematics;k shortest path routing;directed acyclic graph;shortest path faster algorithm;adjacency matrix	Theory	18.41706655198632	29.498659649310863	136719
3c1a79ceee4b7f35593c67022fc70df75924ac95	constrained content distribution and communication scheduling for several restricted classes of graphs	dynamic programming;optimisation;restricted graph class;communication scheduling;complexity theory;communication scheduling broadcast strategy tree network intersecting cliques mutual exclusion;optimal offline broadcast strategies;broadcast strategy;geometric aspects;broadcasting tree graphs processor scheduling scheduling algorithm scientific computing intelligent networks computer science constraint optimization computer networks distributed computing;intersecting cliques;trees mathematics;data mining;mutual exclusion;indexes;content distribution;constrained content distribution;geometric aspects constrained content distribution communication scheduling restricted graph class content distribution communication optimization intersecting cliques optimal offline broadcast strategies mutual exclusion constraints;mutual exclusion constraints;trees mathematics optimisation;tree network;communication optimization;bipartite graph;algorithm design and analysis	In this paper we address several problems regarding content distribution (broadcast) and communication optimization and scheduling for some restricted classes of graphs (trees, intersecting cliques). For the broadcast problem in trees we introduce some new extensions and present some new algorithmic results for determining optimal offline broadcast strategies. The communication scheduling problems are also addressed from an offline algorithmic perspective, considering mutual exclusion constraints or geometric aspects.	algorithm;clique (graph theory);digital distribution;mathematical optimization;mutual exclusion;online and offline;scheduling (computing)	Mugurel Ionut Andreica;Nicolae Tapus	2008	2008 10th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing	10.1109/SYNASC.2008.50	database index;algorithm design;combinatorics;bipartite graph;mutual exclusion;computer science;theoretical computer science;dynamic programming;distributed computing;programming language;algorithm	Theory	15.707861809195105	31.212269545610337	137290
88a0fc5d243effb9806fe2862f1da3818a55e4d7	on algorithm for constructing of decision trees with minimal depth	decision tree;algorithm	An algorithm is considered which for a given decision table constructs a decision tree with minimal depth. The class of all information systems (finite and infinite) is described for which this algorithm has polynomial time complexity depending on the number of columns (attributes) in decision tables.	algorithm;decision tree	Mikhail Ju. Moshkov;Igor Chikalov	2000	Fundam. Inform.	10.3233/FI-2000-41302	mathematical optimization;combinatorics;discrete mathematics;element distinctness problem;c4.5 algorithm;influence diagram;decision tree model;decision tree learning;weighted product model;computer science;decision tree;alternating decision tree;incremental decision tree;mathematics;id3 algorithm;algorithm;weighted sum model;decision stump;decision matrix	Theory	17.456179297883956	24.699084797964204	137294
959e043aedc74035532cf6a69148f46f223332c9	a subquadratic-time algorithm for decremental single-source shortest paths	drntu science mathematics discrete mathematics algorithms;datenstrukturen;conference paper;theoretische informatik	We study dynamic (1 + )-approximation algorithms for the single-source shortest paths problem in an unweighted undirected n-node m-edge graph under edge deletions. The fastest algorithm for this problem is an algorithm with O(n) total update time and constant query time by Bernstein and Roditty (SODA 2011). In this paper, we improve the total update time to O(n + m) while keeping the query time constant. This running time is essentially tight when m = Ω(n) since we need Ω(m) time even in the static setting. For smaller values of m, the running time of our algorithm is subquadratic, and is the first that breaks through the quadratic time barrier. In obtaining this result, we develop a fast algorithm for what we call center cover data structure. We also make non-trivial extensions to our previous techniques called lazyupdate and monotone Even-Shiloach trees (ICALP 2013 and FOCS 2013). As by-products of our new techniques, we obtain two new results for the decremental all-pairs shortestpaths problem. Our first result is the first approximation algorithm whose total update time is faster than Õ(mn) for all values of m. Our second result is a new trade-off between the total update time and the additive approximation guarantee.	approximation algorithm;command & conquer:yuri's revenge;data structure;emoticon;emulator;fastest;graph (discrete mathematics);icalp;karloff–zwick algorithm;lazy evaluation;line graph;list of algorithms;nl (complexity);numerical aperture;operational amplifier;order of approximation;phil bernstein;pseudocode;rounding;shortest path problem;sparse matrix;symposium on foundations of computer science;time complexity;utility functions on indivisible goods;monotone	Monika Henzinger;Sebastian Krinninger;Danupon Nanongkai	2014		10.1137/1.9781611973402.79	mathematical optimization;combinatorics;mathematics;geometry;algorithm	Theory	20.66317392982935	22.12011313688546	137319
e563d357c7094edadde3bd948747d6e0fce148de	adaptive pattern matching	langage fonctionnel;reseau discrimination;68q40;traversal order;68q42;lenguaje funcional;ordre parcours;functional programming;adaptive algorithms;fonction selection;tipificacion;algorithms and complexity;discrimination nets;algorithme adaptatif;programming theory;typing;indexing;pattern matching;reecriture;selection function;finite automata;typage;theorie programmation;68q25;concordance forme;automate fini;68n15;rewriting;functional language;reescritura;funcion seleccion	Pattern matching is an important operation used in many applications such as functional programming, rewriting and rule-based expert systems. By preprocessing the patterns into a DFA-like automaton, we can rapidly select the matching pattern(s) in a single scan of the relevant portions of the input term. This automaton is typically based on left-to-right traversal (of the patterns) or its variants. By adapting the traversal order to suit the set of input patterns, it is possible to considerably reduce the space and matching time requirements of the automaton. The design of such adaptive automata is the focus of this paper. In this context we study several important problems that have remained open even for automata based on left-to-right traversals. Such problems include upper and lower bounds on space complexity, construction of optimal dag automata and impact of typing in pattern matching. An interesting consequence of our results is that lazy pattern matching in typed systems (such as ML) is computationally hard whereas it can be done eeciently in untyped systems.	automaton;dspace;directed acyclic graph;expert system;functional programming;lazy evaluation;logic programming;pattern matching;preprocessor;requirement;rewriting;tree traversal	R. C. Sekar;Ramadoss Ramesh;I. V. Ramakrishnan	1995	SIAM J. Comput.	10.1137/S0097539793246252	search engine indexing;combinatorics;discrete mathematics;rewriting;computer science;theoretical computer science;pattern matching;mathematics;programming language;functional programming;algorithm	Theory	15.202214593209236	27.128134335632538	137508
15e71de323270f5c38a4967f9f8cd74861746706	interval completion with few edges	fpt algorithm;interval graph;branching;false negative;search trees;interval graphs;physical mapping;edge completion;greedy algorithm;performance bounds;sparse matrix computation;profile minimization;physical map;np complete problem	We present an algorithm with runtime O(k(2k)n3 * m) for the following NP-complete problem: Given an arbitrary graph G on n vertices and m edges, can we obtain an interval graph by adding at most k new edges to G? This resolves the long-standing open question, first posed by Kaplan, Shamir and Tarjan, of whether this problem could be solved in time f(k) * n(O(1)).The problem has applications in Physical Mapping of DNA and in Profile Minimization for Sparse Matrix Computations. For the first application, our results show tractability for the case of a small number k of false negative errors, and for the second, a small number k of zero elements in the envelope.  Our algorithm performs bounded search among possible ways of adding edges to a graph to obtain an interval graph, and combines this with a greedy algorithm when graphs of a certain structure are reached by the search. The presented result is surprising, as it was not believed that a bounded search tree algorithm would suffice to answer the open question affirmatively.	computation;forbidden graph characterization;forbidden subgraph problem;greedy algorithm;kaplan–meier estimator;list of algorithms;np-completeness;parameterized complexity;polynomial;search tree;software deployment;sparse matrix;time complexity	Pinar Heggernes;Christophe Paul;Jan Arne Telle;Yngve Villanger	2007		10.1145/1250790.1250847	graph power;mathematical optimization;combinatorics;greedy algorithm;discrete mathematics;kruskal's algorithm;np-complete;interval graph;independent set;multiple edges;graph bandwidth;level structure;branching;degree;computer science;pseudoforest;hopcroft–karp algorithm;mixed graph;hypercube graph;cycle graph;graph coloring;mathematics;path;complement graph;line graph;algorithm;strength of a graph;matching	Theory	22.864043221162945	23.03308868794575	137544
ff8a125a7a334cf6e22c607c58e32e62434a3d2e	an efficient parallel algorithm for updating minimum spanning trees	parallel algorithm;minimum spanning tree	A new parallel algorithm for updating the minimum spanning tree of an n-vertex graph following the addition of a new vertex is presented. The algorithm runs in O(log n) time using O(n) processors on a concurrent-read-exclusive-write parallel random access machine. The algorithm is superior to previous algorithms on this model, that either obtain O(log n) time performance using O(n*) processors, or employ O(n) processors but have a time complexity of 0(log2 n).	central processing unit;computation;directed graph;exclusive or;file spanning;minimum spanning tree;parallel algorithm;parallel random-access machine;random access;recursion;sparse matrix;time complexity	Peter J. Varman;Kshitij Doshi	1988	Theor. Comput. Sci.	10.1016/0304-3975(88)90035-7	combinatorics;kruskal's algorithm;minimum degree spanning tree;spanning tree;prim's algorithm;computer science;theoretical computer science;minimum spanning tree;mathematics;distributed computing;parallel algorithm;reverse-delete algorithm;distributed minimum spanning tree;freivalds' algorithm;rabin–karp algorithm	Theory	18.0440163218357	28.659716618974926	137589
e5be8535805073612316e2af77b69f8e13ed0f4b	improved algorithms for mst and metric-tsp interdiction		We consider the MST-interdiction problem: given a multigraph G = (V,E), edge weights {we ≥ 0}e∈E , interdiction costs {ce ≥ 0}e∈E , and an interdiction budget B ≥ 0, the goal is to remove a set R ⊆ E of edges of total interdiction cost at most B so as to maximize the w-weight of an MST of G−R := (V,E \R). Our main result is a 4-approximation algorithm for this problem. This improves upon the previousbest 14-approximation [30]. Notably, our analysis is also significantly simpler and cleaner than the one in [30]. Whereas [30] uses a greedy algorithm with an involved analysis to extract a good interdiction set from an over-budget set, we utilize a generalization of knapsack called the tree knapsack problem that nicely captures the key combinatorial aspects of this “extraction problem.” We prove a simple, yet strong, LP-relative approximation bound for tree knapsack, which leads to our improved guarantees for MST interdiction. Our algorithm and analysis are nearly tight, as we show that one cannot achieve an approximation ratio better than 3 relative to the upper bound used in our analysis (and the one in [30]). Our guarantee for MST-interdiction yields an 8-approximation for metric-TSP interdiction (improving over the 28-approximation in [30]). We also show that the maximum-spanning-tree interdiction problem is at least as hard to approximate as the minimization version of densest-k-subgraph.	approximation algorithm;dijkstra's algorithm;expected linear time mst algorithm;file spanning;greedy algorithm;knapsack problem;multigraph;travelling salesman problem	André Linhares;Chaitanya Swamy	2017		10.4230/LIPIcs.ICALP.2017.32	combinatorics;mathematics;knapsack problem;multigraph;interdiction;algorithm;greedy algorithm;upper and lower bounds	Theory	22.485047824713128	20.752942081799777	138097
73ee4f78547297c6007aac1aa2ff141d0ce09764	the moser-tardos lopsidependency criterion can be stronger than shearer's criterion		The Lopsided Lovász Local Lemma (LLLL) is a probabilistic tool which is a cornerstone of the probabilistic method of combinatorics, which shows that it is possible to avoid a collection of “bad” events as long as their probabilities and interdependencies are sufficiently small. The strongest possible criterion that can be stated in these terms is due to Shearer (1985), although it is technically difficult to apply to constructions in combinatorics. The original formulation of the LLLL was non-constructive; a seminal algorithm of Moser & Tardos (2010) gave an efficient constructive algorithm for nearly all applications of it, including applications to k-SAT instances with a bounded number of occurrences per variables. Harris (2015) later gave an alternate criterion for this algorithm to converge, which appeared to give stronger bounds than the standard LLL. Unlike the LLL criterion or its variants, this criterion depends in a fundamental way on the decomposition of bad-events into variables. In this note, we show that the criterion given by Harris can be stronger in some cases even than Shearer’s criterion. We construct k-SAT formulas with bounded variable occurrence, and show that the criterion of Harris is satisfied while the criterion of Shearer is violated. In fact, there is an exponentially growing gap between the bounds provable from any form of the LLLL and from the bound shown by Harris.	boolean satisfiability problem;clique problem;converge;eisenstein's criterion;harris affine region detector;interdependence;lenstra–lenstra–lovász lattice basis reduction algorithm;moser spindle;provable security	David G. Harris	2016	CoRR		calculus;mathematics;algorithm;statistics	Theory	16.05300816858061	18.88519332980714	138112
cdc18d2f315287f3bbd3fba46a9dedbd3d4cf8a5	on independent vertex sets in subclasses of apple-free graphs	conjunto independiente;graphe non oriente;grafo triangular;cograph;efficient algorithms;claw free graph;non directed graph;descomposicion grafo;subgrafo;temps polynomial;independent set;efficient algorithm;qa mathematics;graph clique;cografo;apple free graphs;nearly perfect graphs;graphe fini;perfect graph;finite graph;grafo finito;qa76 electronic computers computer science computer software;maximum weight independent set problem;ensemble independant;sous graphe;grafo no orientado;modular decomposition;cographe;polynomial time;graphe triangule;nearly chordal graphs;clique graphe;subgraph;graph decomposition;decomposition graphe;chordal graph;tiempo polinomial;clique separators	In a finite undirected graph, an apple consists of a chordless cycle of length at least 4, and an additional vertex which is not in the cycle and sees exactly one of the cycle vertices. A graph is apple-free if it contains no induced subgraph isomorphic to an apple. Apple-free graphs are a common generalization of chordal graphs, claw-free graphs and cographs and occur in various papers. The Maximum Weight Independent Set (MWS) problem is efficiently solvable on chordal graphs, on cographs as well as on claw-free graphs. In this paper, we obtain partial results on some subclasses of apple-free graphs where our results show that the MWS problem is solvable in polynomial time. The main tool is a combination of clique separators with modular decomposition. Our algorithms are robust in the sense that there is no need to recognize whether the input graph is in the given graph class; the algorithm either solves the MWS problem correctly or detects that the input graph is not in the given class.	algorithm;claw-free graph;claw-free permutation;cograph;decision problem;expect;forbidden subgraph problem;graph (discrete mathematics);independent set (graph theory);induced path;induced subgraph;modular decomposition;time complexity	Andreas Brandstädt;Tilo Klembt;Vadim V. Lozin;Raffaele Mosca	2008	Algorithmica	10.1007/s00453-008-9176-0	1-planar graph;claw-free graph;clique;block graph;time complexity;pathwidth;split graph;combinatorics;discrete mathematics;cograph;universal graph;interval graph;independent set;topology;perfect graph;pancyclic graph;forbidden graph characterization;comparability graph;mathematics;distance-hereditary graph;tree-depth;maximal independent set;modular decomposition;treewidth;partial k-tree;chordal graph;indifference graph;line graph	Theory	22.27042978670057	27.10019660747388	138136
6b34c3ef0677cf8db82bbbe1b9f1bc03b821f5c5	linear layouts in submodular systems		Linear layout of graphs/digraphs is one of the classical and important optimization problems that have many practical applications. Recently Tamaki proposed an O(mn)-time and O(n)space algorithm for testing whether the pathwidth (or vertex separation) of a given digraph with n vertices and m edges is at most k. In this paper, we show that linear layout of digraphs with an objective function such as cutwidth, minimum linear arrangement, vertex separation (or pathwidth) and sum cut can be formulated as a linear layout problem on a submodular system (V, f) and then propose a simple framework of search tree algorithms for finding a linear layout (a sequence of V ) with a bounded width that minimizes a given cost function. According to our framework, we obtain an O(kmn)-time and O(n+m)-space algorithm for testing whether the pathwidth of a given digraph is at most k.	algorithm;directed graph;mathematical optimization;optimization problem;pathwidth;search tree;submodular set function	Hiroshi Nagamochi	2012		10.1007/978-3-642-35261-4_50	combinatorics;computer science;discrete mathematics;submodular set function;vertex (geometry);pathwidth;bounded function;search tree;optimization problem;graph	Theory	24.092527165033285	18.42691760530424	138262
400d39e6d4e09a73397e4ff9fcd16f2998d39bad	regular expression constrained sequence alignment	estensibilidad;dynamic programming;no determinismo;expresion regular;alignement sequence;proteine;automata estado finito;nondeterministic finite automaton;protein sequence;chaine caractere;regular language;probabilistic approach;alineacion secuencia;lenguaje racional;non determinism;non determinisme;pattern matching;enfoque probabilista;approche probabiliste;cadena caracter;langage rationnel;pattern recognition;expression reguliere;dynamic program ming;analyse combinatoire;proteina;finite automaton;sequence alignment;extensibilite;scalability;reconnaissance forme;automate fini;reconocimiento patron;protein;regular expression;analisis combinatorio;combinatorial analysis;character string	We introduce regular expression constrained sequence alignment as the problem of finding the maximum alignment score between given strings S1 and S2 over all alignments such that in these alignments there exists a segment where some substring s1 of S1 is aligned to some substring s2 of S2, and both s1 and s2 match a given regular expression R, i.e. s1, s2 ∈ L(R) where L(R) is the regular language described by R. For complexity results we assume, without loss of generality, that n = |S1| |m| = |S2|. A motivation for the problem is that protein sequences can be aligned in a way that known motifs guide the alignments. We present an O(nmr) time algorithm for the regular expression constrained sequence alignment problem where r = O(t4), and t is the number of states of a nondeterministic finite automaton N that accepts L(R). We use in our algorithm a nondeterministic weighted finite automaton M that we construct from N . M has O(t2) states where the transition-weights are obtained from the given costs of edit operations, and state-weights correspond to optimum alignment scores we compute using the underlying dynamic programming solution for sequence alignment. If we are given a deterministic finite automaton D accepting L(R) with td states then our construction creates a deterministic finite automaton Md with t 2 d states. In this case, our algorithm takes O(t2 d nm) time. Using Md results in faster computation than using M when td < t 2. If we only want to compute the optimum score, the space required by our algorithm is O(t2n) (O(t2 d m) if we use a given Md ). If we also want to compute an optimal alignment then our algorithm uses O(t2m + t|s1||s2|) space (O(t2 dm + t2 d |s1||s2|) space if we use a given Md ) where s1 and s2 are substrings of S1 and S2, respectively, s1, s2 ∈ L(R), and s1 and s2 are aligned together in the optimal alignment that we construct. We also show that our method generalizes for the case of the problem with affine gap penalties, and for finding optimal regular expression constrained local sequence alignments. © 2007 Elsevier B.V. All rights reserved.	algorithm;computation;deterministic finite automaton;dynamic programming;finite-state machine;motif;nondeterministic finite automaton;peptide sequence;regular expression;regular language;sequence alignment;substring	Abdullah N. Arslan	2005		10.1007/11496656_28	combinatorics;scalability;nondeterministic finite automaton;regular language;string;computer science;theoretical computer science;dynamic programming;pattern matching;protein sequencing;sequence alignment;mathematics;finite-state machine;programming language;regular expression;algorithm	Comp.	15.092963983937306	25.569309740122698	138301
d2d0af21f4deca68787f8fae3487965af34f7b37	c-perfect hashing schemes for binary trees, with applications to parallel memories	hachage;distributed memory;securite;memoria compartida;metodo arborescente;hashing;arbol binario;tolerancia;tree structure;estructura datos;arbre binaire;safety;tree structured method;structure donnee;tolerance;methode arborescente;memoire repartie;grafo completo;complete graph;graphe complet;seguridad;data structure;structured data;binary tree	We study the problem of mapping tree-structured data to an ensemble of parallel memory modules. We are given a “conflict tolerance” c, and we seek the smallest ensemble that will allow us to store any nvertex rooted binary tree with no more than c tree-vertices stored on the same module. Our attack on this problem abstracts it to a search for the smallest c-perfect universal graph for complete binary trees. We construct such a graph which witnesses that only O ( c(1−1/c) · 2 ) memory modules are needed to obtain the required bound on conflicts, and we prove that Ω ( 2 ) memory modules are necessary. These bounds are tight to within constant factors when c is fixed—as it is with the motivating application.	binary tree;dimm;perfect hash function	Gennaro Cordasco;Alberto Negro;Vittorio Scarano;Arnold L. Rosenberg	2003		10.1007/978-3-540-45209-6_125	combinatorics;hash function;distributed memory;data structure;binary tree;data model;computer science;artificial intelligence;theoretical computer science;operating system;machine learning;database;mathematics;distributed computing;tree structure;programming language;complete graph;algorithm	Theory	17.42986786910265	28.51264300925002	138397
3d1c2229a28b273c4530d3788bc27fd9f477e489	an efficient trie construction for natural language dictionaries	arbre graphe;lenguaje natural;database system;natural language dictionary;data compression;tree graph;information retrieval;dictionnaire langage naturel;langage naturel;trie structures;recherche information;natural language;natural language dictionaries;digital search;key retrieval algorithm;compresion dato;recuperacion informacion;arbol grafo;data structure;compression donnee	A trie structure is frequently used for various applications, such as natural language dictionaries, database systems and compilers. However, the total number of nodes of the trie becomes large and it takes a lot of spaces for a huge set of keys. The space cost becomes a serious problem if long strings, or compound words, are stored in the trie. In order to resolve this disadvantage, this paper presents a compression method for these long strings by using trie arc for single words. The concept of the compression scheme to be presented is to replace long strings into the corresponding leaf node numbers of the trie. The double array structure is introduced because a fast backward tracing of the trie is required in this approach. The theoretical and experimental observations show that the method presented is more practical than existing ones.	dictionary;natural language;trie	Toru Sumitomo;Kazuhiro Morita;Masao Fuketa;Hidekazu Tokunaga;Jun-ichi Aoe	2002	Int. J. Comput. Math.	10.1080/00207160211286	data compression;data structure;radix tree;computer science;theoretical computer science;database;mathematics;natural language;hash array mapped trie;programming language;x-fast trie;tree;algorithm	Theory	11.338310512800675	28.018955631412332	138454
fb0f9757975b0121a73f73f7dfc8cec764f6532c	categorized bottleneck-minisum path problems on networks	camino mas corto;graph theory;shortest path;categorisation;complexite calcul;gollete estrangulamiento;probleme np complet;np completeness;complexity;reseau;theorie graphe;red;goulot etranglement;categorizacion;computational complexity;chemin plus court;problema np completo;bottleneck;np complete problem;network;categorization	We introduce two path problems on a network, where edges of the network are grouped into categories. The value of a path is calculated by first taking maximum (or sum) of lengths of edges of the path within each category and second taking sum (or maximum) of the obtained values among categories. It is shown that both problems are NP-complete even if all edges of the network have lengths equal to 1. Several special cases of the problems are also investigated, some of which are shown to be solvable in polynomial time and some of which are proved to be NP-complete.	categorization	Igor Averbakh;Oded Berman	1994	Oper. Res. Lett.	10.1016/0167-6377(94)90043-4	combinatorics;discrete mathematics;np-complete;longest path problem;computer science;graph theory;mathematics;algorithm	Theory	22.964718786821425	28.0553141728414	138474
4410914f018b5b82ffe64de0d23263d10f4710ce	dynamic expression trees	dynamic data structure;linear space;series parallel;binary tree	We present a technique for dynamically maintaining a collection of arithmetic expressions represented by binary trees (whose leaves are variables and whose internal nodes are operators). A query operation asks for the value of an expression (associated with the root of a tree). Update operations include changing the value of a variable and combining or decomposing expressions by linking or cutting the corresponding trees. Our dynamic data structure uses linear space and supports queries and updates in logarithmic time. An important application is the dynamic maintenance of maximum flow and shortest path in series-parallel digraphs under a sequence of vertex and edge insertions, series and parallel compositions, and their respective inverses. Queries include reporting the maximum flow or shortestst-path in a series-parallel subgraph.	binary tree;data structure;dynamic data;induced subgraph;linker (computing);maximum flow problem;series and parallel circuits;series-parallel graph;shortest path problem;time complexity;variable (computer science)	Robert F. Cohen;Roberto Tamassia	1995	Algorithmica	10.1007/BF01190506	mathematical optimization;combinatorics;series and parallel circuits;discrete mathematics;binary expression tree;binary tree;computer science;theoretical computer science;mathematics;geometry;algorithm;linear space	Theory	15.996758696668602	28.86220792310324	138760
3b2123a4d1fe61ffa3b0de188328bd4b00662e69	dynamic integer sets with optimal rank, select, and predecessor search	integer data structures;random access memory;standards;integer data structures dynamic data structures;random access storage computational complexity data structures;probes;polynomials;dynamic fusion node dynamic integer sets optimal rank predecessor search data structure w bit word ram w bit integers dynamic predecessor polynomial time;indexes;computational modeling;dynamic data structures;data structures;data structures standards random access memory polynomials indexes probes computational modeling	We present a data structure representing a dynamic set S of w-bit integers on a w-bit word RAM. With S = n and w ≥ logn and space O(n), we support the following standard operations in O(log n/ log w) time: . insert(x) sets S = S ∪ {x}. .delete(x) sets S = S \ {x}. . predecessor(x) returns max{y ∈ S \ y <; x}. . successor(x) returns min{y ∈ S y ≥ x}. . rank(x) returns # {y ∈ S \ y <; x}. . select(i) returns y ∈ S with rank(y) = i, if any. Our O (log n/ log w) bound is optimal for dynamic rank and select, matching a lower bound of Fredman and Saks [STOC'99]. When the word length is large, our time bound is also optimal for dynamic predecessor, matching a static lower bound of Beame and Fich [STOC' 99] whenever log n/ log w = O (log w/ log log w). Technically, the most interesting aspect of our data structure is that it supports all the above operations in constant time for sets of size n = wO(1). This resolves a main open problem of Ajtai, Komlos, and Fredman [FOCS'83]. Ajtai et al. presented such a data structure in Yao's abstract cell-probe model with w-bit cells/words, but pointed out that the functions used could not be implemented. As a partial solution to the problem, Fredman and Willard [STOC'90] introduced a fusion node that could handle queries in constant time, but used polynomial time on the updates. We call our small set data structure a dynamic fusion node as it does both queries and updates in constant time.	cell-probe model;data structure;polynomial;random-access memory;symposium on theory of computing;time complexity;yao graph	Mihai Patrascu;Mikkel Thorup	2014	2014 IEEE 55th Annual Symposium on Foundations of Computer Science	10.1109/FOCS.2014.26	database index;mathematical optimization;combinatorics;discrete mathematics;data structure;computer science;theoretical computer science;mathematics;programming language;computational model;algorithm;statistics;polynomial	Theory	12.49230371373803	25.142279366022553	138771
4411f3bc2c2239441a75b77314c99a2757ebe767	derandomizing homomorphism testing in general groups	desciframiento;prueba;complexite;43a38;temps polynomial;fourier transform;decodage;homomorphism testing;decoding;analyse fourier;linearity testing;derandomisation;complejidad;derandomization;transitivite;cayley graph;randomness;complexity;polynomial;desaleatorizacion;generador;homomorphism;generator;probabilistically checkable proofs;preuve;construccion;propagacion;fourier transformation;belief propagation;polinomio;68r10;transformation fourier;polynomial time;homomorphisme;grafo cayley;fourier analysis;caractere aleatoire;analisis fourier;code;homomorfismo;polynome;proof;generateur;construction;codigo;propagation;transformacion fourier;expansion;tiempo polinomial;graphe cayley	The main result of this paper is a near-optimal derandomization of the affine homomorphism test of Blum, Luby, and Rubinfeld [J. Comput. System Sci., 47 (1993), pp. 549-595].#R##N##R##N#We show that for any groups G and G, and any expanding generating set S of G, the natural deramdomized version of the BLR test in which we pick an element x randomly from G and y randomly from S and test whether $f(x)\cdot f(y)=f(x\cdot y)$, performs nearly as well (depending of course on the expansion) as the original test. Moreover, we show that the underlying homomorphism can be found by the natural local “belief propagation decoding.”#R##N##R##N#We note that the original BLR test uses $2\log_2 |G|$ random bits, whereas the derandomized test uses only $(1+o(1))\log_2 |G|$ random bits. This factor of 2 savings in the randomness complexity translates to a near quadratic savings in the length of the tables in the related locally testable codes (and possibly probabilistically checkable proofs which may use them).#R##N##R##N#Our result is a significant generalization of recent results that either refer to the special case of the groups $G=Z_p^m$ and $G =Z_p$ or are nonconstructive. We use simple combinatorial arguments and the transitivity of Cayley graphs (and this analysis gives optimal results up to constant factors). Previous techniques used the Fourier transform, a method which seems unextendable to general groups (and furthermore gives suboptimal bounds).#R##N##R##N#Finally, we provide a polynomial time (in $|G|$) construction of a (somewhat) small ($|G|^{\epsilon}$) set of expanding generators for every group $G$, which yield efficient testers of randomness $(1+\epsilon) \log |G|$ for $G$. This result follows from a simple derandomization of a known probabilistic construction.	randomized algorithm	Amir Shpilka;Avi Wigderson	2006	SIAM J. Comput.	10.1137/S009753970444658X	fourier transform;mathematical optimization;combinatorics;discrete mathematics;computer science;mathematics;algorithm;algebra	Theory	11.104052405357457	23.32684501671736	138830
a4a358b0546fe50b28315cf1a1a08e0adb5698a3	improved approximation of maximum vertex coverage problem on bipartite graphs	90c59;vertex coverage;transversal number;90c27;lp rounding;90c05	Given a simple undirected graph G and a positive integer s, the maximum vertex coverage problem (MVC) is the problem of finding a set U of s vertices of G such that the number of edges having at least one endpoint in U is as large as possible. The problem is NP-hard even in bipartite graphs, as shown in two recent papers [N. Apollonio and B. Simeone, Discrete Appl. Math., 165 (2014), pp. 37–48; G. Joret and A. Vetta, Reducing the Rank of a Matroid, preprint, arXiv:1211.4853v1 [cs.DS], 2012]. By exploiting the structure of the fractional optimal solutions of a linear programming formulation for the maximum coverage problem, we provide a 4/5-approximation algorithm for the problem. The algorithm immediately extends to the weighted version of MVC.	algorithm;approximation;communication endpoint;graph (discrete mathematics);linear programming formulation;matroid;maximum coverage problem;model–view–controller;np-hardness	Nicola Apollonio;Bruno Simeone	2014	SIAM J. Discrete Math.	10.1137/130931059	mathematical optimization;combinatorics;discrete mathematics;maximum coverage problem;vertex cover;vertex;mathematics	Theory	23.13067239439392	19.741522816347477	138852
8efe3d68d6136ca5de286308c913cf60ccf42096	maintaining densest subsets efficiently in evolving hypergraphs		In this paper we study the densest subgraph problem, which plays a key role in many graph mining applications. The goal of the problem is to find a subset of nodes that induces a graph with maximum average degree. The problem has been extensively studied in the past few decades under a variety of different settings. Several exact and approximation algorithms were proposed. However, as normal graph can only model objects with pairwise relationships, the densest subgraph problem fails in identifying communities under relationships that involve more than 2 objects, e.g., in a network connecting authors by publications.  We consider in this work the densest subgraph problem in hypergraphs, which generalizes the problem to a wider class of networks in which edges might have different cardinalities and contain more than 2 nodes. We present two exact algorithms and a near-linear time r-approximation algorithm for the problem, where r is the maximum cardinality of an edge in the hypergraph. We also consider the dynamic version of the problem, in which an adversary can insert or delete an edge from the hypergraph in each round and the goal is to maintain efficiently an approximation of the densest subgraph. We present two dynamic approximation algorithms in this paper with amortized polog update time, for any ε > 0. For the case when there are only insertions, the approximation ratio we maintain is r(1+ε), while for the fully dynamic case, the ratio is r2(1+ε). Extensive experiments are performed on large real datasets to validate the effectiveness and efficiency of our algorithms.	adversary (cryptography);amortized analysis;approximation algorithm;data transfer object;experiment;graph (discrete mathematics);structure mining;time complexity	Shuguang Hu;Xiaowei Wu;T.-H. Hubert Chan	2017		10.1145/3132847.3132907	discrete mathematics;hypergraph;data mining;cardinality;approximation algorithm;computer science;constraint graph;adversary;mathematical optimization;pairwise comparison;graph	Theory	21.718626405578114	20.615826418482257	139334
60314cb7720370afedc8d7ea44d11df1720301b4	fully-dynamic min-cut	finite fields;exponential sums;sparse polynomials	We show that we can maintain up to polylogarithmic edge connectivity for a fully-dynamic graph in <italic>\tilde O(\sqrt{n})</italic> time per edge insertion or deletion. Within logarithmic factors, this matches the best time bound for 1-edge connectivity. Previously, no <italic>o(n)</italic> bound was known for edge connectivity above <italic>3</italic>, and even for <italic>3</italic>-edge connectivity, the best update time was <italic>O(n^{2/3})</italic>, dating back to FOCS'92. Our algorithm maintains a concrete min-cut in terms of a pointer to a tree spanning one side of the cut plus ability to list the cut edges in <italic>O(\log n)</italic> time per edge. By dealing with polylogarithmic edge connectivity, we immediately get a sampling based expected factor <italic>(1+o(1))</italic> approximation to general edge connectivity in <italic>\tilde O(\sqrt{n})</italic> time per edge insertion or deletion. This algorithm also maintains a pointer to one side of a min-cut, but if we want to list the cut edges in <italic>O(\log n)</italic> time per edge, the update time increases to <italic>\tilde O(\sqrt{m})</italic>.	algorithm;approximation;bridge (graph theory);file spanning;k-edge-connected graph;minimum cut;pointer (computer programming);polylogarithmic function;sampling (signal processing)	Mikkel Thorup	2001		10.1145/380752.380804	combinatorics;discrete mathematics;mathematics;finite field;algorithm	Theory	20.787685389919265	24.25960728075025	139365
b69f4efc4437b76fbba42bfa8c8bedba694fe5a1				approximation algorithm;binary logarithm;computational complexity theory;greedy algorithm;heuristic;np-hardness;p (complexity);polynomial;sieve of eratosthenes;social inequality;strong np-completeness;time complexity	Tamar Cohen;Liron Yedidsion	2018	Math. Oper. Res.	10.1287/moor.2017.0904		Theory	17.38721158500783	20.106911707070324	139389
8a6203dc95749895ca80c0cb13d5d40bfea10d35	self-adjusting binary trees	search trees;efficient implementation;probability distribution;priority queue;information need;data structure;binary tree	We use the idea of self-adjusting trees to create new, simple data structures for priority queues (which we call heaps) and search trees. Unlike other efficient implementations of these data structures, self-adjusting trees have no balance condition. Instead, whenever the tree is accessed, certain adjustments take place. (In the case of heaps, the adjustment is a sequence of exchanges of children, in the case of search trees the adjustment is a sequence of rotations.) Self-adjusting trees are efficient in an amortized sense: any particular operation may be slow but any sequence of operations must be fast.  Self-adjusting trees have two advantages over the corresponding balanced trees in both applications. First, they are simpler to implement because there are fewer cases in the algorithms. Second, they are more storage-efficient because no balance information needs to be stored. Furthermore, a self-adjusting search tree has the remarkable property that its running time (for any sufficiently long sequence of search operations) is within a constant factor of the running time for the same set of searches on any fixed binary tree. It follows that a self-adjusting tree is (up to a constant factor) as fast as the optimal fixed tree for a particular probability distribution of search requests, even though the distribution is unknown.	algorithm;amortized analysis;binary tree;data structure;heap (data structure);information needs;priority queue;self-balancing binary search tree;time complexity	Daniel Dominic Sleator;Robert E. Tarjan	1983		10.1145/800061.808752	random binary tree;optimal binary search tree;probability distribution;information needs;finger tree;red–black tree;combinatorics;discrete mathematics;binary search tree;tree rotation;data structure;geometry of binary search trees;binary tree;computer science;treap;theoretical computer science;range tree;self-balancing binary search tree;k-d tree;interval tree;mathematics;search tree;weight-balanced tree;ternary search tree;programming language;tree traversal;priority queue;metric tree;avl tree	Theory	14.544540732462044	28.080249426507564	139474
5248a473b0f32ef3e0d0035f3c8d703bafe56b0a	digraph minors and algorithms (abstract only)	sorting by reversals;matching;alternating cycle decomposition;experimental results;column generation	The Graph Minors project, originated by Robertson and Seymour, was very successful. It resulted in many theoretical advances (e.g., a proof of Wagner's conjecture), but it also has algorithmic applications, and some of the methods have been successfully used in practical computation. It now appears possible to extend part of the project to directed graphs. The speaker will review the basics of the Graph Minors theory, and then will discuss generalizations to directed graphs, algorithmic consequences, and open problems.	algorithm;computation;directed graph;graph coloring;robertson–seymour theorem	Robin Thomas	2000			column generation;matching;mathematical optimization;combinatorics;discrete mathematics;mathematics;algorithm	Theory	22.654582441796368	22.529731164799735	139662
3eddff16ac2dc0bc3eae644acada418da111a7f8	a stable minimum storage sorting algorithm	sorting algorithm		sorting algorithm	Robert B. K. Dewar	1974	Inf. Process. Lett.	10.1016/0020-0190(74)90004-0	computer science;bitonic sorter;sorting algorithm;mathematics;algorithm	Theory	12.6892879597087	31.705320766020485	139678
931efdb8741e1541d499108bd7b0f5460483ee69	fixed-parameter tractability of the maximum agreement supertree problem	data mining;databases;upper bound;phylogeny;computational complexity;np hard problem;reductions;binary tree;organisms;time complexity;parameterized complexity;phylogenetics;algorithms;genetics;bioinformatics;binary trees	Given a ground set L of labels and a collection of trees whose leaves are bijectively labelled by some elements of L, the Maximum Agreement Supertree problem (SMAST) is the following: find a tree T on a largest label set L ⊆ L that homeomorphically contains every input tree restricted to L. The problem finds applications in phylogenetics, databases and data mining. In this paper we focus on the parameterized complexity of this NP-hard problem. We consider different combinations of parameters for SMAST as well as particular cases, providing both FPT algorithms and intractability results.	algorithm;data mining;database;np-hardness;parameterized complexity;phylogenetics;supertree	Sylvain Guillemot;Vincent Berry	2007		10.1007/978-3-540-73437-6_28	combinatorics;discrete mathematics;mathematics;algorithm	Theory	18.18198168401379	22.054751890726955	139900
6f874240cf91fa712ed94c02bdf24d5066346945	perfect (0, ±1)-matrices and perfect bidirected graphs	numero cromatico;graph theory;perfect matrix;1 polytopes;teoria grafo;l matrices;graphe bioriente;temps polynomial;perfectness;nombre chromatique;chromatic number;matrix algebra;plusm;0 1 matrices;theorie graphe;0;relajacion;stability;algorithme;politope;algorithm;bidirected graph;0 1 polytopes;directed graph;matrice parfaite;graphe oriente;polynomial time;stable set;relaxation;grafo orientado;stabilite;algebre matricielle;estabilidad;algoritmo;bidirected graphs;polytope;tiempo polinomial	Perfect (0, ± 1)-matrices and perfect bidirected graphs are independently defined but are closely related. In this paper, we discuss these relations and give several characterizations of perfectness. The generalized stable set problem associated with bidirected graphs is a generalization of the maximum weighted stable set problem. We also give a brief proof that if a given bidirected graph is perfect, then the problem can be solved in polynomial time.		Akihisa Tamura	2000	Theor. Comput. Sci.	10.1016/S0304-3975(99)00203-0	strong perfect graph theorem;time complexity;polytope;combinatorics;discrete mathematics;bidirected graph;directed graph;stability;graph theory;trivially perfect graph;relaxation;mathematics;geometry;perfect power;algorithm;algebra	ECom	22.02330760956475	27.52907339515487	140142
5e099fd1f8ebf89b6bbe461cbe4a36a74eaa4cd4	grid proximity graphs: logs, gigs and girls		This paper discusses three types of proximity graphs called LOGs, GIGs and GIRLs, defined on unit grids. We show that it can be decided in linear time whether a LOG graph is a GIG graph. We also show that it is NP-complete to recognize LOGs and GIGs, and explore the relationship between these graph classes and their properties. Enumeration results and open problems are also presented.	directed acyclic graph;global information grid;graph coloring;karp's 21 np-complete problems;linear arboricity;time complexity;universal quantification	River Allen;Laurie J. Heyer;Rahnuma Islam Nishat;Sue Whitesides	2013			grid;combinatorics;time complexity;discrete mathematics;enumeration;computer science;graph	Metrics	24.255661174425544	25.791406572867047	140288
1f3b2b0a76c53d3089743d5f7178af62a20d058b	an experimental study on approximating k shortest simple paths	approximated k shortest;algorithms;simple theoretical observation;cases exact path;experimentation;new variant;second path;k shortest simple path;path approximation;approximation algorithm;approximating k;problem solving, control methods, and search;log2 n;original algorithm;experimental study;heuristic;k;performance;n log n;simple path	We have conducted an extensive experimental study on approximation algorithms for computing k shortest simple paths in weighted directed graphs. Very recently, Bernstein [2] presented an algorithm that computes a 1 + e approximated k shortest simple paths in O(e-1k(m + n log n) log2 n) time. We have implemented Bernstein's algorithm and tested it on synthetic inputs and real world graphs (road maps). Our results reveal that Bernstein's algorithm has a practical value in many scenarios. Moreover, it produces in most of the cases exact paths rather than approximated. We also present a new variant for Bernstein's algorithm. We prove that our new variant has the same upper bounds for the running time and approximation as Bernstein's original algorithm. We have implemented and tested this variant as well. Our testing show that this variant, which is based on a simple theoretical observation, is better than Bernstein's algorithm in practice.		Asaf Frieder;Liam Roditty	2011		10.1007/978-3-642-23719-5_37	mathematical optimization;combinatorics;floyd–warshall algorithm;mathematics;bernstein polynomial;shortest path faster algorithm;algorithm	Theory	19.739052759755506	20.870948449776233	140477
6c9ac3505e8999d50be85da572f8a64d886213fa	routing flow through a strongly connected graph	switching networks;mathematics;maximum flow;algorithm analysis;feasible flow problem;technology;recherche profondeur d abord;flot donnee;physical sciences;flujo datos;applied;software engineering;analysis of algorithm;journal article;reseau commutation;connected graph;analysis of algorithms;first depth search;flujo red;science technology;linear time;aprovisionamiento;approvisionnement;analyse algorithme;depth first search;supply;encaminamiento;graphe fluence;computer science;forwarding;network flow;maximum flow problem;data flow;strongly connected graph;grafo fluencia;graphe connexe;flot reseau;analisis algoritmo;fluence graph;acheminement;grafo conexo	It is shown that, for every strongly connected network in which every edge has capacity at least Δ , linear time suffices to send flow from source vertices, each with a given supply, to sink vertices, each with a given demand, provided that the total supply equals the total demand and is bounded by Δ . This problem arises in a maximum-flow algorithm of Goldberg and Rao, the binary blocking flow algorithm.	dinic's algorithm;maximum flow problem;routing;strongly connected component;time complexity;vertex (geometry)	Thomas Erlebach;Torben Hagerup	2001	Algorithmica	10.1007/s00453-001-0082-y	maximum flow problem;mathematical optimization;combinatorics;flow network;computer science;connectivity;theoretical computer science;mathematics;algorithm	Theory	21.624752833315164	29.613615650079947	140502
9dda0953a772503d790f01c7c68b2413791496f5	optimization over k-set polytopes and efficient k-set enumeration	algoritmo busqueda;geometrie algorithmique;geometria combinatoria;algorithme recherche;search algorithm;computational geometry;linear functionals;combinatorial geometry;informatique theorique;estructura datos;geometria computacional;structure donnee;geometrie combinatoire;algoritmo optimo;algorithme optimal;optimal algorithm;data structure;computer theory;informatica teorica	We present two versions of an algorithm based on the reverse search technique for enumerating all k-sets of a point set in R d. The key elements include the notion of a k-set polytope and the optimization of a linear function over a k-set polytope.	algorithm;linear function;mathematical optimization	Artur Andrzejak;Komei Fukuda	1999		10.1007/3-540-48447-7_1	polytope;edge;combinatorics;uniform k 21 polytope;convex polytope;data structure;polyhedral combinatorics;computational geometry;computer science;birkhoff polytope;mathematics;geometry;programming language;vertex enumeration problem;polytope model;search algorithm	Crypto	16.897410947830913	27.183963228711455	140566
af21c3a8ddb1c7a74cbbd77e61594da7e54ffded	ip lookup using the novel idea of scalar prefix search with fast table updates		Recently, we have proposed a new prefix lookup algorithm which would use the prefixes as scalar numbers. This algorithm could be applied to different tree structures such as Binary Search Tree and some other balanced trees like RB-tree, AVL-tree and B-tree with minor modifications in the search, insert and/or delete procedures to make them capable of finding the prefixes of an incoming string e.g. an IP address. As a result, the search procedure complexity would be O(log n) where n is the number of prefixes stored in the tree. More important, the search complexity would not depend on the address length w i.e. 32 for IPv4 and 128 for IPv6. Here, it is assumed that interface to memory is wide enough to access the prefix and some simple operations like comparison can be done in O(1) even for the word length w. Moreover, insertion and deletion procedures of this algorithm are much simpler and faster than its competitors. In what follows, we report the software implementation results of this algorithm and compare it with other solutions for both IPv4 and IPv6. It also reports on a simple hardware implementation of the algorithm for IPv4. Comparison results show better lookup and update performances or superior storage requirements for Scalar Prefix Search in both average and worst cases. key words: scalar prefix, LPM, LMP, SP-BT, search, update	avl tree;algorithm;b-tree;longest prefix match;lookup table;performance;red–black tree;requirement;scalar processor;self-balancing binary search tree	Mohammad Behdadfar;Hossein Saidi;Masoud Reza Hashemi;Ali Ghiasian;Hamid Alaei	2010	IEICE Transactions		discrete mathematics;computer science;trie;theoretical computer science;ternary search tree;algorithm	Networks	11.644650552621252	28.560669112048885	140582
8245247f3e4897fa9d268502ee5048ff6b7a2e42	performance engineering case study: heap construction	algorithm performance;randomised algorithms;algorithme deterministe;algorithme randomise;computer architecture;deterministic algorithms;architecture ordinateur;resultado algoritmo;informatique theorique;performance algorithme;hierarchie memoire;arquitectura ordenador;memory hierarchy;jerarquia memoria;computer theory;informatica teorica	The behaviour of three methods for constructing a binary heap is studied. The methods considered are the original one proposed by Williams 1964], in which elements are repeatedly inserted into a single heap; the improvement by Floyd 1964], in which small heaps are repeatedly merged to bigger heaps; and a recent method proposed, e. g., by Fadel et al. 1999] in which a heap is built layerwise. Both the worst-case number of instructions and that of cache misses are analysed. It is well-known that Floyd's method has the best instruction count. Let N denote the size of the heap to be constructed, B the number of elements that t into a cache line, and let c and d be some positive constants. Our analysis shows that, under reasonable assumptions, repeated insertion and layerwise construction both incur at most cN=B cache misses, whereas repeated merging, as programmed by Floyd, can incur more than (dN log 2 B)=B cache misses. However, for our memory-tuned versions of repeated insertion and repeated merging the number of cache misses incurred is close to the optimal bound N=B.	best, worst and average case;binary heap;cpu cache;performance engineering	Jesper Bojesen;Jyrki Katajainen;Maz Spork	1999		10.1007/3-540-48318-7_24	binomial heap;fibonacci heap;parallel computing;heap;min-max heap;skew heap;double-ended priority queue;computer science;artificial intelligence;binary heap;operating system;d-ary heap;database;mathematics;algorithm	Arch	14.363356557172974	29.570046075843976	140744
be9ae2aec9e384b3d14861f4c5e36fcc29a51449	multifacility ordered median problems on networks: a further analysis	metodo polinomial;location theory;location problem;theorie localisation;probleme localisation;temps polynomial;equipement collectif;approximation algorithm;complexity;mediane;median;network analysis;dominating set;equipamiento colectivo;structure reseau;polynomial method;teoria localizacion;facility;finite dominating sets;algoritmo aproximacion;polynomial time;algorithms;mediana;problema localizacion;network structure;algorithme approximation;methode polynomiale;analyse circuit;analisis circuito;ensemble dominant;tiempo polinomial	In this paper, we address the ordered p-median problem, which includes as special cases most of the classical multifacility location problems discussed in the literature. Finite dominating sets (FDS) are known for particular instances of this problem: p-median, p-center, and p-centdian. We find an FDS for the ordered p-median problem. This set allows us to gain a better insight into the general FDS structure of network location problems. This FDS is later used to present the first polynomial time algorithm for p-facility ordered median problems on tree networks. This result is combined with some approximation algorithms to give an O(log M log log M) approximate solution of these problems on general networks, where M is the number of vertices. © 2003 Wiley Periodicals, Inc.	approximation algorithm;computational complexity theory;exact algorithm;family computer disk system;john d. wiley;np-hardness;p (complexity);polynomial;time complexity	Jörg Kalcsics;Stefan Nickel;Justo Puerto	2003	Networks	10.1002/net.10053	time complexity;combinatorics;complexity;location theory;dominating set;network analysis;computer science;calculus;mathematics;median;approximation algorithm;algorithm	Theory	21.356974348874168	27.206261678169156	141489
4c52a655a71a118a0e10d37d9067ff75d92097f3	efficient sampling of random permutations	distributed data;coarse grained parallelism;permutation routing;random permutation;external memory;external memory algorithms;uniformly generated communication matrix;coarse grained;random permutations;random shuffling	We show how to uniformly distribute data at random (not to be confounded with permutation routing) in two settings that are able to deal with massive data: coarse grained parallelism and external memory. In contrast to previously known work for parallel setups, our method is able to fulfill the three criteria of uniformity, work-optimality and balance among the processors simultaneously. To guarantee the uniformity we investigate the matrix of communication requests between the processors. We show that its distribution is a generalization of the multivariate hypergeometric distribution and we give algorithms to sample it efficiently in the two settings.	benchmark (computing);bottleneck (engineering);central processing unit;circuit complexity;computation;expect;irix;linux;microsoft outlook for mac;overhead (computing);parallel algorithm;parallel computing;randomized algorithm;randomness;routing;sparc;sampling (signal processing);sequential algorithm;shared memory;simulation;subroutine;the matrix;time complexity	Jens Gustedt	2008	J. Discrete Algorithms	10.1016/j.jda.2006.11.002	combinatorics;random permutation;discrete mathematics;parallel computing;theoretical computer science;mathematics	Theory	11.49149071982139	31.444174299981654	141550
19456b860fecd9d734c74e8bbf21b600295d8197	forward chaining algorithm for solving the shortest path problem in arbitrary deterministic environment in linear time - applied for the tower of hanoi problem	shortest path;linear time;agent architecture;shortest path problem	The paper presents an application of an algorithm which solves the shortest path problem in arbitrary deterministic environment in linear time, using OFF ROUTE acting method and emotional agent architecture. The complexity of the algorithm in general does not depend on the number of states n, but only on the length of the shortest path, in the worst case the complexity can be at most O(n). The algorithm is applied for the Tower of Hanoi problem.	algorithm;forward chaining;shortest path problem;time complexity;tower of hanoi	Silvana Petruseva	2009		10.1007/978-3-642-04617-9_84	private network-to-network interface;mathematical optimization;combinatorics;canadian traveller problem;widest path problem;constrained shortest path first;longest path problem;shortest job next;average path length;pathfinding;euclidean shortest path;machine learning;yen's algorithm;mathematics;shortest path problem;distance;k shortest path routing;shortest path faster algorithm	Theory	20.424985802810976	29.154984650932487	141628
84865d310ebff13d2138955e444e54f6e9bf9079	a sufficient connectivity condition for generic rigidity in the plane	mixed connectivity;optimisation;plane;combinatorics;optimizacion;combinatoria;rigid graph;combinatoire;plan;globally rigid graph;satisfiability;condition suffisante;connected graph;graph connectivity;condicion suficiente;informatique theorique;redundantly rigid graph;68r10;plano;conectividad grafo;optimization;sufficient condition;connectivite graphe;graphe connexe;computer theory;grafo conexo;informatica teorica	A graph G = (V,E) is said to be 6-mixed-connected if G−U−D is connected for all sets U ⊆ V and D ⊆ E which satisfy 2|U | + |D| ≤ 5. In this note we prove that 6-mixed-connected graphs are (redundantly globally) rigid in the plane. This improves on a previous result of Lovász and Yemini.	graph (discrete mathematics)	Bill Jackson;Tibor Jordán	2009	Discrete Applied Mathematics	10.1016/j.dam.2008.12.003	mathematical optimization;combinatorics;discrete mathematics;topology;connectivity;mathematics	Theory	24.089832715918654	30.785836855165638	141762
b7cd94008477aaaf516da3c28fccb0f1dd123195	the complexity of speedrunning video games.		Speedrunning is a popular activity in which the goal is to finish a video game as fast as possible. Players around the world spend hours each day on live stream, perfecting their skills to achieve a world record in well-known games such as Super Mario Bros, Castlevania or Mega Man. But human execution is not the only factor in a successful speed run. Some common techniques such as damage boosting or routing require careful planning to optimize time gains. In this paper, we show that optimizing these mechanics is in fact a profound algorithmic problem, as they lead to novel generalizations of the well-known NP-hard knapsack and feedback arc set problems. We show that the problem of finding the optimal damage boosting locations in a game admits an FPTAS and is FPT in k + r, the number k of enemy types in the game and r the number of health refill locations. However, if the player is allowed to lose a life to regain health, the problem becomes hard to approximate within a factor 1/2 but admits a (1/2− )-approximation with two lives. Damage boosting can also be solved in pseudo-polynomial time. As for routing, we show various hardness results, including W [2]-hardness in the time lost in a game, even on bounded treewidth stage graphs. On the positive side, we exhibit an FPT algorithm for stage graphs of bounded treewidth and bounded in-degree. 2012 ACM Subject Classification Theory of computation → Design and analysis of algorithms, Theory of computation → Approximation algorithms analysis, Theory of computation → Parameterized complexity and exact algorithms	analysis of algorithms;approximation algorithm;boosting (machine learning);directed graph;feedback arc set;mathematical optimization;mega man;np-hardness;parameterized complexity;polynomial;polynomial-time approximation scheme;pseudo-polynomial time;random number generation;routing;speedrun;streaming media;theory of computation;time complexity;treewidth;whole earth 'lectronic link	Manuel Lafond	2018		10.4230/LIPIcs.FUN.2018.27	discrete mathematics;computer science;theoretical computer science	Theory	20.922240403083844	20.61353545948298	141794
81b7caf8a6d8c5554cf419bb497f2c0b9f2dd38f	fast lempel-ziv decompression in linear space		We consider the problem of decompressing the Lempel-Ziv 77 representation of a string S ∈ [σ] using a working space as close as possible to the size z of the input. The folklore solution for the problem runs in optimal O(n) time but requires random access to the whole decompressed text. A better solution is to convert LZ77 into a grammar of size O(z log(n/z)) and then stream S in optimal linear time. In this paper, we show that O(n) time and O(z) working space can be achieved for constant-size alphabets. On larger alphabets, we describe (i) a trade-off achieving O(n log σ) time and O(z log1−δ σ) space for any 0 ≤ δ ≤ 1, and (ii) a solution achieving optimal O(n) time and O(z log logn) space. Our solutions can, more generally, extract any specified subsequence of S with little overheads on top of the optimal running time and working space. As an immediate corollary, we show that our techniques yield improved results for pattern matching problems on LZ77-compressed text.	data compression;lz77 and lz78;lempel–ziv–stac;pattern matching;random access;time complexity;workspace	Philip Bille;Mikko Berggren Ettienne;Travis Gagie;Inge Li Gørtz;Nicola Prezza	2018	CoRR		subsequence;discrete mathematics;linear space;combinatorics;time complexity;sigma;pattern matching;binary logarithm;mathematics;random access	Theory	12.526254684604591	26.916065441029534	141795
3910087db6ec39bd9f9ca3e6fbbccd354553f082	quantum communication-query tradeoffs		For any function f : X × Y → Z, we prove that Q(f) ·Q(f) · (logQ(f) + log |Z|) ≥ Ω(log |X |). Here, Q(f) denotes the bounded-error communication complexity of f using an entanglementassisted two-way qubit channel, and Q(f) denotes the number of quantum queries needed to learn x with high probability given oracle access to the function fx(y) def = f(x, y). We show that this tradeoff is close to the best possible. We also give a generalization of this tradeoff for distributional query complexity. As an application, we prove an optimal Ω(log q) lower bound on the Q complexity of determining whether x + y is a perfect square, where Alice holds x ∈ Fq, Bob holds y ∈ Fq, and Fq is a finite field of odd characteristic. As another application, we give a new, simpler proof that searching an ordered size-N database requires Ω(logN/ log logN) quantum queries. (It was already known that Θ(logN) queries are required.) ∗Department of Computer Science, University of Texas at Austin, whoza@utexas.edu	alice and bob;communication complexity;computer science;decision tree model;quantum information science;qubit;with high probability	William M. Hoza	2017	CoRR		qubit;combinatorics;discrete mathematics;mathematics;binary logarithm;communication complexity;upper and lower bounds;square number;finite field	Theory	10.942321682520168	22.624165286135717	141950
02ec9a024ffc323ae42bd87d48f8db900f1a4e37	approximating minimum max-stretch spanning trees on unweighted graphs	arbre graphe;spanning trees;approximate algorithm;tree graph;05c05;low stretch;approximation algorithm;aproximacion;arbre maximal;68wxx;probleme np;approximation;terme;spanners;arbol maximo;68r10;05c85;algoritmo aproximacion;distancia;ams subject classification;spanning tree;algorithme approximation;arbol grafo;68w25;05c12;distance	Given a graph <i>G</i> and a spanning tree <i>T</i> of <i>G</i>, we say that <i>T</i> is a <i>tree t-spanner</i> of <i>G</i> if the distance between every pair of vertices in <i>T</i> is at most <i>t</i> times their distance in <i>G.</i> The problem of finding a tree <i>t</i>-spanner minimizing <i>t</i> is referred to as the <i>Minimum Max-Stretch spanning Tree (MMST)</i> problem. This paper concerns the MMST problem on unweighted graphs. The problem is known to be NP-hard, and the paper presents an <i>O</i>(log <i>n</i>) approximation algorithm for it.	approximation algorithm;file spanning;mmst;np-hardness;spanning tree	Yuval Emek;David Peleg	2004		10.1137/060666202	combinatorics;discrete mathematics;spanning tree;mathematics;approximation algorithm;algorithm	Theory	21.451525381065903	27.103801740923473	142050
5d9a2b379e9d16e6f9231502d527f44fa24c0f5d	a polylogarithmic approximation for computing non-metric terminal steiner trees	desviacion;arbre graphe;approximate algorithm;procesamiento informacion;arbre steiner;algorithm analysis;tree graph;approximation algorithms;05c05;approximation algorithm;group steiner tree;aproximacion;deviation;calculo automatico;computing;approximation;calcul automatique;informatique theorique;terminal steiner tree;information processing;algoritmo aproximacion;coste;analyse algorithme;algorithme approximation;arbol grafo;traitement information;68w25;steiner tree;analisis algoritmo;computer theory;cout;informatica teorica	The main contribution of this short note is to provide improved bounds on the approximability of constructing terminal Steiner trees in arbitrary undirected graphs. Technically speaking, our results are obtained by relating this computational task to that of computing group Steiner trees. As a secondary objective, we make a concentrated effort to distinguish between the factor by which constructed trees exceed the optimal backbone cost and between the deviation from the optimal terminal linking cost. ∗Blavatnik School of Computer Science, Tel-Aviv University, Tel-Aviv 69978, Israel. Email: iftgam@tau.ac.il. Supported by the Israel Science Foundation, by the European Commission under the Integrated Project QAP funded by the IST directorate as Contract Number 015848, by a European Research Council (ERC) Starting Grant, and by the Wolfson Family Charitable Trust. †Department of Statistics, University of Haifa, Haifa 31905, Israel. Email: segevd@stat.haifa.ac.il.	approximation algorithm;computer science;email;graph (discrete mathematics);internet backbone;polylogarithmic function;quadratic assignment problem;steiner tree problem	Iftah Gamzu;Danny Segev	2010	Inf. Process. Lett.	10.1016/j.ipl.2010.07.006	combinatorics;computing;discrete mathematics;computer science;approximation;mathematics;deviation;approximation algorithm;algorithm	Theory	20.903937932565793	26.740042160011626	142058
81c14d1f056c7057812471d362aa444f691c3da3	randomized fixed-parameter algorithms for the closest string problem	randomized algorithms;fixed parameter algorithms;the closest string problem;computational biology	Given a set $$S = \{s_1, s_2, \ldots , s_n\}$$ S = { s 1 , s 2 , … , s n } of strings of equal length $$L$$ L and an integer $$d$$ d , the closest string problem (CSP) requires the computation of a string $$s$$ s of length $$L$$ L such that $$d(s, s_i) \le d$$ d ( s , s i ) ≤ d for each $$s_i \in S$$ s i ∈ S , where $$d(s, s_i)$$ d ( s , s i ) is the Hamming distance between $$s$$ s and $$s_i$$ s i . The problem is NP-hard and has been extensively studied in the context of approximation algorithms and fixed-parameter algorithms. Fixed-parameter algorithms provide the most practical solutions to its real-life applications in bioinformatics. In this paper we develop the first randomized fixed-parameter algorithms for CSP. Not only are the randomized algorithms much simpler than their deterministic counterparts, their time complexities are also significantly better than the previously best known (deterministic) algorithms.	approximation algorithm;bioinformatics;closest string;computation;hamming distance;np-hardness;parameterized complexity;randomized algorithm;real life	Zhi-Zhong Chen;Bin Ma;Lusheng Wang	2014	Algorithmica	10.1007/s00453-014-9952-y	mathematical optimization;combinatorics;computer science;mathematics;randomized algorithm;algorithm	Theory	15.86923873682927	22.775595271053287	142226
bc6b6d194ad30de3802b626e5a2a9b71ef200e3e	balanced spanning trees in complete and incomplete star graphs	multiprocessor interconnection networks;architecture systeme;star graph;reseau interconnecte;personalized broadcast;arbre maximal;tree graphs broadcasting computer society multiprocessor interconnection networks hypercubes computer science intelligent networks parallel architectures parallel processing information management;pipeline processing systems;trees mathematics;balanced spanning tree;interconnection network;graphe communication;computer architecture;parallel architectures;arbol maximo;data structures;trees mathematics multiprocessor interconnection networks parallel architectures;interconnection networks;communication graph;grafo estrella;algorithms;graphe etoile;arquitectura sistema;systeme parallele;spanning tree;parallel system;parallel processing systems;parallel architecture;system architecture;grafo completo;complete graph;red interconectada;graphe complet;interconnected power system;sistema paralelo;problem solving;near balanced spanning tree balanced spanning trees incomplete star graphs complete star graphs personalized broadcast problem interconnection network spanning tree complete star graph asymptotically balanced spanning tree;grafo comunicacion	Efficiently solving the personalized broadcast problem in an interconnection network typically relies on finding an appropriate spanning tree in the network. In this paper, we show how to construct in a complete star graph an asymptotically balanced spanning tree, and in an incomplete star graph a near-balanced spanning tree. In both cases, the tree is shown to have the minimum height. In the literature, this problem has only been considered for the complete star graph, and the constructed tree is about 4/3 times taller than the one proposed in this paper.	file spanning;interconnection;personalization;spanning tree	Tzung-Shi Chen;Yu-Chee Tseng;Jang-Ping Sheu	1996	IEEE Trans. Parallel Distrib. Syst.	10.1109/71.508251	euclidean minimum spanning tree;kruskal's algorithm;parallel computing;tree rotation;data structure;minimum degree spanning tree;spanning tree;star;prim's algorithm;computer science;theoretical computer science;minimum spanning tree;gomory–hu tree;k-ary tree;interval tree;connected dominating set;k-minimum spanning tree;distributed computing;tree-depth;trémaux tree;reverse-delete algorithm;distributed minimum spanning tree;complete graph;tree;tree decomposition;shortest-path tree;systems architecture	Theory	18.219079983778336	30.961454670708992	142248
b80df1590be71bd70ff8883502877a4c41f6610c	deletion in abstract voronoi diagrams in expected linear time		Updating an abstract Voronoi diagram in linear time, after deletion of one site, has been an open problem for a long time. Similarly for various concrete Voronoi diagrams of generalized sites, other than points. In this paper we present a simple, expected linear-time algorithm to update an abstract Voronoi diagram after deletion. We introduce the concept of a Voronoi-like diagram, a relaxed version of a Voronoi construct that has a structure similar to an abstract Voronoi diagram, without however being one. Voronoi-like diagrams serve as intermediate structures, which are considerably simpler to compute, thus, making an expected linear-time construction possible. We formalize the concept and prove that it is robust under an insertion operation, thus, enabling its use in incremental constructions. 2012 ACM Subject Classification Theory of computation → Computational geometry	algorithm;computational geometry;theory of computation;time complexity;voronoi diagram	Kolja Junginger;Evanthia Papadopoulou	2018		10.4230/LIPIcs.SoCG.2018.50	discrete mathematics;mathematics;diagram;time complexity;combinatorics;voronoi diagram;open problem	Theory	13.554888370676162	24.074469350206886	142315
53f8bc7eff14083951dc214455fafa79ac5d86e5	an efficient polynomial time approximation scheme for the vertex cover p 3 problem on planar graphs			planar graph;polynomial;polynomial-time approximation scheme;time complexity;vertex cover	Yongtang Shi;Jianhua Tu	2019	Discussiones Mathematicae Graph Theory	10.7151/dmgt.2060		Theory	23.744780833092	24.061013644899404	142368
126fac13e0c0d8d9107a9672134478bb5c2ad521	repetitions in strings: algorithms and combinatorics	mot sturmien;05xx;algorithme rapide;regularite;repetition;combinatorics;algorithmique;regularidad;combinatoria;tandem repeats;redaction;combinatoire;regularity;consecutive repeats;periodicite;68wxx;runs;periodicity;sturmian word;repetitions;periodicidad;racine carree;algorithmics;algoritmica;informatique theorique;fast algorithm;compresion;factorisation;superficie;writing;tandem repeat;squares;algorithms;area;repeticion;68p30;square root;compression;point of view;cubes;68r05;algoritmo rapido;redaccion;computer theory;informatica teorica;raiz cuadrada	The article is an overview of basic issues related to repetitions in strings, concentrating on algorithmic and combinatorial aspects. This area is important both from theoretical and practical point of view. Repetitions are highly periodic factors (substrings) in strings and are related to periodicities, regularities, and compression. The repetitive structure of strings leads to higher compression rates, and conversely, some compression techniques are at the core of fast algorithms for detecting repetitions. There are several types of repetitions in strings: squares, cubes, and maximal repetitions also called runs. For these repetitions, we distinguish between the factors (sometimes qualified as distinct) and their occurrences (also called positioned factors). The combinatorics of repetitions is a very intricate area, full of open problems. For example we know that the number of (distinct) primitively-rooted squares in a string of length n is no more than 2n−Θ(log n), conjecture to be n, and that their number of occurrences can be Θ(n log n). Similarly we know that there are at most 1.029n and at least 0.944n maximal repetitions and the conjecture is again that the exact bound is n. We know almost everything about the repetitions in Sturmian words, but despite the simplicity of these words, the results are nontrivial. One of the main motivations for writing this text is the development during the last couple of years of new techniques and results about repetitions. We report both the progress which has been achieved and which we expect to happen.	algorithm;data compression;graph coloring;maximal set;olap cube;sensor;substring;time complexity	Maxime Crochemore;Lucian Ilie;Wojciech Rytter	2009	Theor. Comput. Sci.	10.1016/j.tcs.2009.08.024	arithmetic;combinatorics;mathematics;algorithmics;algorithm;tandem repeat;algebra	Theory	12.891436973840857	29.273806329276585	142476
4af9ea5e8b6115b828dc6fc98a47c8c795a8c17b	analysing deletions in competitive self-adjusting linear list	competitive self-adjusting linear list;analysing deletions	There has been a great deal of work on self-adjusting algorithms for linear lists. Almost all of the prior work focused only on successful searches, but in [3] we designed and analysed self-adjusting algorithms for a linear list which were efficient for both successful and unsuccessful searches as well as insertions. Analysis of deletions is listed as an open question. This paper presents an improved version of MP which is also able to handle deletions efficiently, and proves that the new MP algorithm is competitive to offline adversaries when considering successful searches, unsuccessful searches, insertions, and deletions.		Lucas Chi Kwong Hui;Charles U. Martel	1994		10.1007/3-540-58325-4_209	theoretical computer science;machine learning;algorithm	Theory	13.220043302054714	24.6402449095021	142558
927c5be76f29634664ed71caef91b2103bd5cd00	3-hitting set on bounded degree hypergraphs: upper and lower bounds on the kernel size	kernel;parameterized complexity;lower bounds;upper bounds;upper bound;hitting set;upper and lower bounds;lower bound	We study upper and lower bounds on the kernel size for the 3-hitting set problem on hypergraphs of degree at most 3, denoted 33-hs. We first show that, unless P=NP, 3-3-hs on 3-uniform hypergraphs does not have a kernel of size at most 35k/19 > 1.8421k. We then give a 4k − k kernel for 3-3-hs that is computable in time O(k). This result improves the upper bound of 4k on the kernel size for 33-hs, given by Wahlström. We also show that the upper bound results on the kernel size for 3-3-hs can be generalized to the 3-hs problem on hypergraphs of bounded degree ∆, for any integer-constant ∆ > 3.	computable function;kernel (operating system);p versus np problem	Iyad A. Kanj;Fenghui Zhang	2011		10.1007/978-3-642-19754-3_17	mathematical optimization;combinatorics;discrete mathematics;mathematics;upper and lower bounds	Theory	22.625046470530283	22.165002985648393	142623
f3da17c64bb96677a6342ccb73fe49e116e18415	errors in improved polynomial algorithm for 3 sat proposed by narendra chaudhari		There are errors in the algorithm proposed by Narendra Chaudhari [2] purporting to solve the 3-sat problem in polynomial time. The present paper presents an instance for which the algorithm outputs erroneous results. Introduction: In his paper [1], Narendra Chaudhari describes an algorithm to find a solution for the 3-sat problem in polynomial time. In a second paper [2], he improves on his previous algorithm. The following paper describes two instances of the 3-sat problem on which the improved algorithm fails to give the expected output.	algorithm;boolean satisfiability problem;polynomial;time complexity	Ritesh Vispute	2011	CoRR		mathematical optimization;theoretical computer science;mathematics;algorithm	AI	10.27633905565466	18.471026272254804	142914
b78e68c5f02ea4ab124862c22c36dda413d0b41d	scan-first search and sparse certificates: an improved parallel algorithms for k-vertex connectivity	graphe non oriente;certificat graphe;search problem;algoritmo paralelo;pram;non directed graph;parallel algorithm;recherche balayage;05c40;problema investigacion;operations research;graphs;algorithme parallele;algorithme en direct;90b12;graph connectivity;68q22;grafo no orientado;conectividad grafo;technical report;probleme recherche;connectivite graphe;industrial engineering;vertex connectivity;parallel algorithms	Given a graph $G = (V,E)$, a certificate of k-vertex connectivity is an edge subset $E' \subset E$ such that the subgraph $(V,E')$ is k-vertex connected if and only if G is k-vertex connected. Let n and m denote the number of vertices and edges. A certificate is called sparse if it contains $O(kn)$ edges.For undirected graphs, this paper introduces a graph search called the scan-first search, and shows that a certificate with at most $k(n - 1)$ edges can be computed by executing scan-first search k times in sequence on subgraphs of G. For each of the parallel, distributed, and sequential models of computation, the complexity of scan-first search matches the best complexity of any graph search on that model. In particular, the parallel scan-first search runs in $O(\log n)$ time using $C(n,m)$ processors on a CRCW PRAM, where $C(n,m)$ is the number of processors needed to find a spanning tree in each connected component in $O(\log n)$ time, and the parallel certificate algorithm runs in $O(k\log n)$ time us...	k-vertex-connected graph;parallel algorithm;sparse	Joseph Cheriyan;Ming-Yang Kao;Ramakrishna Thurimella	1993	SIAM J. Comput.	10.1137/0222013	mathematical optimization;combinatorics;computer science;theoretical computer science;mathematics;parallel algorithm;algorithm	Theory	18.960109893616533	28.33453536323047	143107
6a2c7f5c6359292fe48dc3f7911fc10311ceef27	polylogarithmic fully retroactive priority queues via hierarchical checkpointing		Since the introduction of retroactive data structures at SODA 2004 [1], a major open question has been the difference between partial retroactivity (where updates can be made in the past) and full retroactivity (where queries can also be made in the past). In particular, for priority queues, partial retroactivity is possible in O(log m) time per operation on a m-operation timeline, but the best previously known fully retroactive priority queue has cost Θ( √ m log m) time per operation. We address this open problem by providing a general logarithmicoverhead transformation from partial to full retroactivity called “hierarchical checkpointing,” provided that the given data structure is “time-fusible” (multiple structures with disjoint timespans can be fused into a timeline supporting queries of the present). As an application, we construct a fully retroactive priority queue which can insert an element, delete the minimum element, and find the minimum element, at any point in time, in O(log m) amortized time per update and O(log m log log m) time per query, using O(m log m) space. Our data structure also supports the operation of determining the time at which an element was deleted in O(log m) time.	amortized analysis;application checkpointing;data structure;polylogarithmic function;priority queue;retroactive data structures;timeline	Erik D. Demaine;Tim Kaler;Quanquan C. Liu;Aaron Sidford;Adam Yedidia	2015		10.1007/978-3-319-21840-3_22	binary search algorithm;binary search tree;priority queue;discrete mathematics;retroactive data structures;computer science;timeline	Theory	13.801924607303782	25.098586013276694	143131
7a7eb44a27106e952561e75ebc13f6c8b18f7594	lp-branching algorithms based on biased graphs		We give a combinatorial condition for the existence of efficient, LP-based FPT algorithms for a broad class of graph-theoretical optimisation problems. Our condition is based on the notion of biased graphs known from matroid theory. Specifically, we show that given a biased graph Ψ = ( G , Β ), where Β is a class of balanced cycles in G , the problem of finding a set X of at most k vertices in G which intersects every unbalanced cycle in G admits an FPT algorithm using an LP-branching approach, similar to those previously seen for VCSP problems (Wahlstrom, SODA 2014). Our algorithm has two parts. First we define a local problem , where we are additionally given a root vertex v 0 ∈ V and asked only to delete vertices X (excluding v 0 ) so that the connected component of v 0 in G - X contains no unbalanced cycle. We show that this local problem admits a persistent, half-integral LP-relaxation with a polynomial-time solvable separation oracle, and can therefore be solved in FPT time via LP-branching, assuming only oracle membership queries for the class of balanced cycles in G. We then show that solutions to this local problem can be used to tile the graph, producing an optimal solution to the original, global problem as well. This framework captures many of the problems previously solved via the VCSP approach to LP-branching, as well as new generalisations, such as Group Feedback Vertex Set for infinite groups (e.g., for graphs whose edges are labelled by matrices). A major advantage compared to previous work is that it is immediate to check the applicability of the result for a given problem, whereas testing applicability of the VCSP approach for a specific VCSP, requires determining the existence of an embedding language with certain algebraically defined properties, which is not known to be decidable in general.	algorithm;biased graph	Magnus Wahlström	2017			competitive analysis;online algorithm;mathematical optimization;combinatorics;discrete mathematics;computer science;mathematics;algorithm	Theory	23.144134902380724	23.012866621262496	143174
83fa5a15faa19d80e2777425f37009163caaa33b	polynomial kernels for 3-leaf power graph modification problems	delecion;optimisation;kernel;fpt;combinatorics;tree;optimizacion;05c05;leaf power;combinatoria;arbol;temps lineaire;combinatoire;tiempo lineal;graph modification problems;polynomial;potencia;68wxx;algorithme;algorithm;data structures;informatique theorique;polinomio;estructura datos;ensemble contour;linear time;complecion;68r10;modificacion;edge graph;edge set;68p05;arbre;arete graphe;algorithms;puissance;optimization;structure donnee;polynome;completion;data structure;power;arista grafico;computer theory;modification;deletion;algoritmo;informatica teorica	A graph G=(V,E) is a 3-leaf power iff there exists a tree T the leaf set of which is V and such that uv@?E iff u and v are at distance at most 3 in T. The 3-leaf power graph edge modification problems, i.e. edition (also known as the closest 3-leaf power), completion and edge-deletion are FPT when parameterized by the size of the edge set modification. However, polynomial kernels were known for none of these three problems. For each of them, we provide kernels with O(k^3) vertices that can be computed in linear time. We thereby answer an open problem first mentioned by Dom et al. (2004) [8].	polynomial	Stéphane Bessy;Christophe Paul;Anthony Perez	2010	Discrete Applied Mathematics	10.1016/j.dam.2010.07.002	time complexity;graph power;combinatorics;discrete mathematics;kernel;completion;data structure;edge cover;power;mathematics;tree;bound graph;algorithm;polynomial	ML	20.17681134094662	26.284973705650614	143176
4ac50fe9cd3d90571c529780b38a6cd4bccb54f3	a nearly optimal lower bound on the approximate degree of ac^0		The approximate degree of a Boolean function f: {-1, 1}^n &#x21a6; {-1, 1} is the least degree of a real polynomial that approximates f pointwise to error at most 1/3. We introduce a generic method for increasing the approximate degree of a given function, while preserving its computability by constant-depth circuits.Specifically, we show how to transform any Boolean function f with approximate degree d into a function F on O(n polylog(n)) variables with approximate degree at least D = &#x3a9;(n^{1/3} d^{2/3}). In particular, if d = n^{1-&#x3a9;(1), then D is polynomially larger than d. Moreover, if f is computed by a constant-depth polynomial-size Boolean circuit, then so is F.By recursively applying our transformation, for any constant &#x3b4; 0 we exhibit an AC&#xb0; function of approximate degree &#x3a9;(n^{1-&#x3b4;}). This improves over the best previous lower bound of &#x3a9;(n^{2/3}) due to Aaronson and Shi (J. ACM 2004), and nearly matches the trivial upper bound of n that holds for any function. Our lower bounds also apply to (quasipolynomial-size) DNFs of polylogarithmic width.We describe several applications of these results. We give:&#x2022; For any constant &#x3b4; 0, an &#x3a9;(n^{1-&#x3b4;}) lower bound on the quantum communication complexity of a function in AC&#xb0;.&#x2022; A Boolean function f with approximate degree at least C(f)^{2-o(1), where C(f) is the certificate complexity of f. This separation is optimal up to the o(1) term in the exponent.&#x2022; Improved secret sharing schemes with reconstruction procedures in AC&#xb0;.	approximation algorithm;boolean circuit;certificate (complexity);communication complexity;computability;polylogarithmic function;polynomial;quantum channel;quasi-polynomial;recursion;ruth aaronson bari;secret sharing	Mark Bun;Justin Thaler	2017	2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS)	10.1109/FOCS.2017.10	combinatorics;boolean function;discrete mathematics;certificate;mathematics;recursion;boolean circuit;#p-complete;polynomial;pointwise;upper and lower bounds	Theory	11.359237657983176	21.978760514718594	143247
a00ef60d5eca3a15085f182e8650901a0f7fd2af	approximating minimum-cost connected t-joins	traveling salesman problem;approximation algorithms;primal dual method;lp rounding;t path tsp;s;prize collecting problems;t joins	We design and analyse approximation algorithms for the minimum-cost connected T-join problem: given an undirected graph G=(V,E) with nonnegative costs on the edges, and a set of nodes T⊆V, find (if it exists) a spanning connected subgraph H of minimum cost such that every node in T has odd degree and every node not in T has even degree; H may have multiple copies of any edge of G. Two well-known special cases are the TSP (T=∅) and the s,t path TSP (T={s,t}). Recently, An et al. (Proceedings of the 44th Annual ACM Symposium on Theory of Computing, pp. 875–886, 2012) improved on the long-standing $\frac{5}{3}$ approximation guarantee for the latter problem and presented an algorithm based on LP rounding that achieves an approximation guarantee of $\frac{1+\sqrt{5}}{2}\approx1.61803$ . We show that the methods of An et al. extend to the minimum-cost connected T-join problem. They presented a new proof for a $\frac{5}{3}$ approximation guarantee for the s,t path TSP; their proof extends easily to the minimum-cost connected T-join problem. Next, we improve on the approximation guarantee of $\frac{5}{3}$ by extending their LP-rounding algorithm to get an approximation guarantee of $\frac{13}{8}=1.625$ for all |T|≥4. Finally, we focus on the prize-collecting version of the problem, and present a primal-dual algorithm that is “Lagrangian multiplier preserving” and that achieves an approximation guarantee of $3-\frac{4}{|T|}$ when |T|≥4. Our primal-dual algorithm is a generalization of the known primal-dual 2-approximation for the prize-collecting s,t path TSP. Furthermore, we show that our analysis is tight by presenting instances with |T|≥4 such that the cost of the solution found by the algorithm is exactly $3-\frac{4}{|T|}$ times the cost of the constructed dual solution.	approximation algorithm;augmented lagrangian method;degree (graph theory);diffusing update algorithm;file spanning;graph (discrete mathematics);lagrange multiplier;lagrangian relaxation;rounding;symposium on theory of computing;tsp (time series processor)	Joseph Cheriyan;Zachary Friggstad;Zhihan Gao	2013	Algorithmica	10.1007/s00453-013-9850-8	mathematical optimization;combinatorics;computer science;mathematics;travelling salesman problem;approximation algorithm;algorithm	Theory	22.62139318029053	19.302018955708107	143329
4321ad5b080c2d7ac266ebdac0a199d415984d37	efficient unbalanced merge-sort	settore inf 01 informatica;design of algorithms;experimental analysis;sorting;data structures;space complexity;sorting algorithm;data structure	Sorting algorithms based on successive merging of ordered subsequences are widely used, due to their efficiency and to their intrinsically parallelizable structure. Among them, the merge–sort algorithm emerges indisputably as the most prominent method. In this paper we present a variant of merge–sort that proceeds through arbitrary merges between pairs of quasi-ordered subsequences, no matter which their size may be. We provide a detailed analysis, showing that a set of n elements can be sorted by performing at most nblog nc key comparisons. Our method has the same optimal asymptotic time and space complexity as compared to previous known unbalanced merge–sort algorithms, but experimental results show that it behaves significantly better in practice. 2005 Elsevier Inc. All rights reserved.	dspace;dictionary;insertion sort;merge sort;sorting algorithm;unbalanced circuit	Enrico Nardelli;Guido Proietti	2006	Inf. Sci.	10.1016/j.ins.2005.04.008	mathematical optimization;combinatorics;data structure;computer science;sorting;theoretical computer science;machine learning;sorting algorithm;mathematics;dspace;programming language;algorithm;experimental analysis of behavior	DB	11.992922571898903	26.446407280851407	143352
c02ed4cff88db413650af46b6d278ee9e50590eb	toward the krw composition conjecture: cubic formula lower bounds via communication complexity	004;formula lower bounds;communication complexity;formula lower bounds communication complexity karchmer wigderson games krw composition conjecture;karchmer wigderson games;krw composition conjecture	One of the major challenges of the research in circuit complexity is proving super-polynomial lower bounds for de-Morgan formulas. Karchmer, Raz, and Wigderson [KRW95] suggested to approach this problem by proving the conjecture that formula complexity behaves “as expected” with respect to the composition of functions f g. They showed that this conjecture, if proved, would imply super-polynomial formula lower bounds. The first step toward proving the KRW conjecture was made by Edmonds et. al. [EIRS01], who proved an analogue of the conjecture for the composition of “universal relations”. In this work, we extend the argument of [EIRS01] further to f g for any function f but where g = ⊕n, that is, we prove the KRW conjecture for the the special case where g is the parity function and f is an arbitrary function. While this case was already proved implicitly in H̊astad’s work on random restrictions [H̊as98], our proof uses an entirely different approach based on communication complexity, and seems more likely to be generalizable to other cases of the conjecture. In addition, our proof gives a new structural result, which roughly says that the naive way for computing f g is the only optimal way. Along the way, we obtain a new proof of the state-of-the-art formula lower bound of n3−o(1) of [H̊as98]. ∗Weizmann Institute, Rehovot, Israel. irit.dinur@weizmann.ac.il. This research was supported in part by ERC grant number 239986. †Department of Computer Science, Haifa University, Haifa 31905, Israel. ormeir@cs.haifa.ac.il. This research was done while Meir was supported by Irit Dinur’s ERC grant number 239986.	circuit complexity;communication complexity;computer science;cubic function;de morgan's laws;jack edmonds;parity function;polynomial;ran raz	Irit Dinur;Or Meir	2016		10.4230/LIPIcs.CCC.2016.3	combinatorics;discrete mathematics;computer science;communication complexity;mathematics;quantum mechanics	Theory	10.61808859555445	21.279087038836792	143363
6d809da72c7717a3d0ee27df726a1e94f0523ca6	complexity of list coloring problems with a fixed total number of colors	arbre graphe;list;graphe non oriente;graph theory;metodo polinomial;complexite;coloracion grafo;teoria grafo;non directed graph;lista;tree graph;complejidad;complexity;theorie graphe;algorithme;algorithm;coloration graphe;polynomial method;grafo no orientado;polynomial algorithm;list coloring;liste;arbol grafo;methode polynomiale;bipartite graph;graph colouring;algoritmo	We study list coloring problems where the total number k of colors on all lists is #xed. Such problems are known to be NP-Complete even for planar bipartite graphs and k = 3. We give polynomial algorithms for some special cases of these problems. ? 2002 Elsevier Science B.V. All rights reserved.	algorithm;cardinality (data modeling);color;decision problem;graph coloring;list coloring;np-completeness;p (complexity);polynomial hierarchy;time complexity	Sylvain Gravier;Daniel Kobler;Wieslaw Kubiak	2002	Discrete Applied Mathematics	10.1016/S0166-218X(01)00179-2	brooks' theorem;combinatorics;discrete mathematics;complexity;fractional coloring;bipartite graph;list;graph theory;complete coloring;edge coloring;mathematics;list coloring;greedy coloring;tree;algorithm	Theory	21.42458069347564	26.899215809531245	143780
3de13b4cca0250520811f6f76ff2e6b00e3dbaf7	generation of maximum independent sets of a bipartite graph and maximum cliques of a circular-arc graph	grafo aciclico;graphe biparti;algorithm analysis;circular arc graph;time complexity;grafo bipartido;sistema informatico;graph clique;graphe acyclique;computer system;acyclic graph;probleme combinatoire;complexite temps;maximum clique;ensemble independant;problema combinatorio;directed graph;graphe oriente;grafo orientado;analyse algorithme;systeme informatique;technical report;combinatory problem;clique graphe;complejidad tiempo;bipartite graph;analisis algoritmo;maximum independent set	Abstract   We present an efficient algorithm for generating all maximum independent sets of a bipartite graph. Its time complexity is  O ( n  2.5  + (output size)), where  n  is the number of vertices of a given graph. As its application, we develop an algorithm for generating all maximum cliques of a circular-arc graph. When the graph is given in the form of a family of  n  arcs on a circle, this algorithm runs in  O ( n  3.5  + (output size)) time.		Toshinobu Kashiwabara;Sumio Masuda;Kazuo Nakajima;Toshio Fujisawa	1992	J. Algorithms	10.1016/0196-6774(92)90012-2	clique;time complexity;graph power;edge-transitive graph;factor-critical graph;combinatorics;discrete mathematics;independent set;directed graph;graph bandwidth;bipartite graph;null graph;degree;distance-regular graph;technical report;simplex graph;cubic graph;mathematics;voltage graph;circle graph;butterfly graph;intersection number;biregular graph;quartic graph;complement graph;directed acyclic graph;line graph;algorithm;circulant graph;matching	Theory	21.798569861089717	27.51300040564684	143782
71da4c7d6c7e09dd2ac284d6d7ad442d52a0feff	the algorithmic complexity of colour switching	switching;optimisation;graphe biparti;coloracion grafo;design of algorithms;algorithm complexity;temps polynomial;optimizacion;metodo reduccion;grafo bipartido;complejidad algoritmo;sistema informatico;probleme np dur;problema np duro;computer system;np hard problem;probleme combinatoire;problema combinatorio;coloration graphe;complexite algorithme;conmutacion;polynomial time;methode reduction;optimization;systeme informatique;combinatory problem;channel allocation;combinatorial optimization;bipartite graph;reduction method;commutation;graph colouring;tiempo polinomial	Chee, Y.M. and A. Lim, The algorithmic complexity of colour switching, Information Processing Letters 43 (1992) 63-68. A new graph colouring related problem called the colour switching problem is introduced. By giving a reduction from the colour switching problem to the weighted matching problem on bipartite graphs, we show that the colour switching problem can be solved in O(n3/* log n) time. The colour switching problem has many useful applications in computer science and engineering. We outline an application to the channel allocation problem in mobile cellular radio systems.	analysis of algorithms;computational complexity theory;computer science;expanded memory;graph coloring;information processing letters;mobile phone	Yeow Meng Chee;Andrew Lim	1992	Inf. Process. Lett.	10.1016/0020-0190(92)90013-L	time complexity;combinatorics;bipartite graph;combinatorial optimization;computer science;np-hard;mathematics;algorithm	Theory	20.603719890369234	28.00907356119454	143822
995630e37fcca240a50d09098666851ffe444df0	algorithms for updating minimal spanning trees	minimal spanning tree;article	The problem of finding the minimal spanning tree on an undirected weighted graph has been investigated by many people and O(nZ) algorithms are well known. P. M. Spira and A. Pan (Siam J. Computing 4 (1975), 375-380) present an O(n) algorithm for updating the minimal spanning tree if a new vertex is inserted into the graph. In this paper, we present another O(n) algorithm simpler than that presented by Spira and Pan for the insertion of a vertex. Spira and Pan further show that the deletion of a vertex requires O(3) steps. If all the vertices are considered, O(ns) steps may be used. The algorithm which we present here takes only O(n2) steps and labels the vertices of the graph in such a way that any vertex may be deleted from the graph and the minimal spanning tree can be updated in constant time. Similar results are obtained for the insertion and the deletion of an edge.	algorithm;branch and bound;file spanning;graph (discrete mathematics);minimum spanning tree;time complexity;travelling salesman problem;vertex (geometry);vertex (graph theory)	Francis Y. L. Chin;David Houck	1978	J. Comput. Syst. Sci.	10.1016/0022-0000(78)90022-3	euclidean minimum spanning tree;combinatorics;discrete mathematics;kruskal's algorithm;feedback vertex set;minimum degree spanning tree;spanning tree;prim's algorithm;computer science;minimum spanning tree;cycle graph;vertex;connected dominating set;k-minimum spanning tree;mathematics;reverse-delete algorithm;distributed minimum spanning tree;neighbourhood;algorithm;shortest-path tree	Theory	20.66928900886081	26.24967205163184	144264
d7ce2b01344f1168a6fe4094a9d7d610680c6d0f	improved fpt algorithms for weighted independent set in bull-free graphs		Very recently, Thomasse, Trotignon and Vuyskovic (WG 2014) have given an FPT algorithm for Weighted Independent Set in bull- free graphs parameterized by the weight of the solution, running in time 2 O(k 5 ) ·n 9 . In this article we improve this running time to 2 O(k 2 ) ·n 7 .A s a byproduct, we also improve the previous Turing-kernel for this problem from O(k 5 )t oO(k 2 ). Furthermore, for the subclass of bull-free graphs without holes of length at most 2p − 1f orp ≥ 3, we speed up the running time to 2 O(k·k 1 p−1 ) · n 7 .A sp grows, this running time is asymptotically tight in terms of k, since we prove that for each integer p ≥ 3, Weighted Independent Set cannot be solved in time 2 o(k) · n O(1) in the class of {bull, C4 ,...,C 2p−1}-free graphs unless the ETH fails.	algorithm;independent set (graph theory);parameterized complexity	Henri Perret du Cray;Ignasi Sau	2014		10.1007/978-3-319-13524-3_24	combinatorics;discrete mathematics;mathematics;algorithm	Theory	21.723881496109602	22.0708351334776	144430
35cfa56895378e2f4f72fb7913b242d7f0a9a113	super mario bros. is harder/easier than we thought	004;video games computational complexity pspace	Mario is back! In this sequel, we prove that solving a generalized level of Super Mario Bros. is PSPACE-complete, strengthening the previous NP-hardness result (FUN 2014). Both our PSPACE-hardness and the previous NP-hardness use levels of arbitrary dimensions and require either arbitrarily large screens or a game engine that remembers the state of off-screen sprites. We also analyze the complexity of the less general case where the screen size is constant, the number of on-screen sprites is constant, and the game engine forgets the state of everything substantially off-screen, as in most, if not all, Super Mario Bros. video games. In this case we prove that the game is solvable in polynomial time, assuming levels are explicitly encoded; on the other hand, if levels can be represented using run-length encoding, then the problem is weakly NP-hard (even if levels have only constant height, as in the video games). All of our hardness proofs are also resilient to known glitches in Super Mario Bros., unlike the previous NP-hardness proof. 1998 ACM Subject Classification F.1.3 Complexity Measures and Classes	decision problem;display size;game engine;glitch;np-hardness;pspace;pspace-complete;polynomial;run-length encoding;sprite (computer graphics);time complexity	Erik D. Demaine;Giovanni Viglietta;Aaron Williams	2016		10.4230/LIPIcs.FUN.2016.13	combinatorics;simulation;computer science;mathematics;algorithm	Theory	13.021254975772777	18.672464604433078	144448
46ac7260ece7bc25fdbd5b8a2e6acfc14d2c25db	models and algorithms for computing the common labelling of a set of attributed graphs	softassign;graduated assignment;consistent multiple isomorphism;graph matching;inconsistent labelling;common graph labelling;multiple graph matching;optimal algorithm;np complete problem	In some methodologies, it is needed a consistent common labelling between the vertices of a graph set, for instance, to compute a representative of a set of graphs. This is an NP-complete problem with an exponential computational cost depending on the number of nodes and the number of graphs. In the current paper, we present two new methodologies to compute a sub-optimal common labelling. The former focuses in extending the Graduated Assignment algorithm, although the methodology could be applied to other probabilistic graph-matching algorithms. The latter goes one step further and computes the common labelling whereby a new iterative sub-optimal algorithm. Results show that the new methodologies improve the state of the art obtaining more precise results than the most recent method with similar computational cost.	algorithm;cluster analysis;computation;computational complexity theory;computer memories inc.;database;disk mirroring;distance matrix;graph (discrete mathematics);graph isomorphism;graph labeling;pattern recognition;probabilistic automaton;prototype;random graph;relay;time complexity;video content analysis	Albert Solé-Ribalta;Francesc Serratosa	2011	Computer Vision and Image Understanding	10.1016/j.cviu.2010.12.007	mathematical optimization;combinatorics;discrete mathematics;universal graph;np-complete;independent set;computer science;mathematics;graph homomorphism;line graph;matching	Vision	19.530648209828374	20.673954272695525	144478
992d30a7045579670e0de4edc501c3e834080932	on the linear complexity profile of the power generator	macquarie university institutional repository;blum integer;complexity theory;rsa generator;researchonline;lattices;digital repository;random number generation;macquarie university;linear complexity;linear complexity profile;cryptography power generator linear complexity profile lower bound pseudo random numbers blum integer blum blum shub generator lattice reduction attacks pseudorandom number generator rsa generator;cryptography computational complexity random number generation;lattice reduction;computational complexity;cryptography;lattice reduction attacks;power generation;random numbers;pseudorandom number generator;h infinity control;power generator;blum blum shub generator;lower bound;pseudo random numbers	We obtain a lower bound on the linear complexity profile of the power generator of pseudo-random numbers modulo a Blum integer. A different method is also proposed to estimate the linear complexity profile of the Blum-Blum-Shub (1986) generator. In particular, these results imply that lattice reduction attacks on such generators are not feasible.		Frances Griffin;Igor E. Shparlinski	2000	IEEE Trans. Information Theory	10.1109/18.868485	linear congruential generator;discrete mathematics;digital library;blum integer;lattice reduction;random number generation;cryptography;theoretical computer science;mathematics;pseudorandom number generator;algorithm;statistics	Theory	10.113429271415715	23.653194778162447	144580
f02a447d8182e55db32e10071103b75b645b23bf	a note on minimal d-separation trees for structural learning	structure learning;directed acyclic graph;grafo aciclico;conditional independence;bayesian network;relation ordre partiel;digraph;subgrafo;separation tree;geometrie algorithmique;computational geometry;digrafo;eficacia test;minimal d separation tree;graphe acyclique;intelligence artificielle;acyclic graph;reseau bayes;independence test;triangulacion;red bayes;sous graphe;partial ordering;structural learning;directed graph;tree structure;graphe oriente;bayes network;arbre minimal;clique tree;artificial intelligence;geometria computacional;grafo orientado;arbol minimo;triangulation;relacion orden parcial;inteligencia artificial;test independance;test independencia;subgraph;minimal tree;minimal triangulation;test efficiency;efficacite test;partial order;digraphe	Structural learning of a Bayesian network is often decomposed into problems related to its subgraphs, although many approaches without decomposition were proposed. In 2006, Xie, Geng and Zhao proposed using a d-separation tree to improve the power of conditional independence tests and the efficiency of structural learning. In our research note, we study a minimal d-separation tree under a partial ordering, by which the maximal efficiency can be obtained. Our results demonstrate that a minimal d-separation tree of a directed acyclic graph (DAG) can be constructed by searching for the clique tree of a minimal triangulation of the moral graph for the DAG.	bayesian network;separation kernel	Binghui Liu;Jianhua Guo;Bing-Yi Jing	2010	Artif. Intell.	10.1016/j.artint.2010.01.002	partially ordered set;combinatorics;directed graph;computational geometry;computer science;machine learning;gomory–hu tree;bayesian network;k-ary tree;mathematics;moral graph;tree;directed acyclic graph;algorithm;tree decomposition	AI	23.68980113813019	30.30292213886183	144593
86aab5981653c73c8536bdb16f257a5624e31549	approximating majority depth	data depth;range counting;median level;majority depth	We consider the problem of approximating the majority depth (Liu and Singh, 1993) of a point q with respect to an n-point set, S, by random sampling. At the heart of this problem is a data structures question: How can we preprocess a set of n lines so that we can quickly test whether a randomly selected vertex in the arrangement of these lines is above or below the median level. We describe a Monte-Carlo data structure for this problem that can be constructed in O(n log n) time, can answer queries O((log n)) expected time, and answers correctly with high probability.	average-case complexity;data structure;fastest;monte carlo method;partition type;preprocessor;randomness;sampling (signal processing);with high probability	Dan Chen;Pat Morin	2012	Comput. Geom.	10.1016/j.comgeo.2013.06.005	combinatorics;discrete mathematics;mathematics;algorithm	Theory	14.180007423585288	22.932333549880767	144773
120a60ed02b8bceeae76ad37b1fe2df4fb83d829	parameterized tsp: beating the average		In the Travelling Salesman Problem (TSP), we are given a complete graph Kn together with an integer weighting w on the edges of Kn, and we are asked to find a Hamilton cycle of Kn of minimum weight. Let h(w) denote the average weight of a Hamilton cycle of Kn for the weighting w. Vizing (1973) asked whether there is a polynomialtime algorithm which always finds a Hamilton cycle of weight at most h(w). He answered this question in the affirmative and subsequently Rublineckii (1973) and others described several other TSP heuristics satisfying this property. In this paper, we prove a considerable generalisation of Vizing’s result: for each fixed k, we give an algorithm that decides whether, for any input edge weighting w of Kn, there is a Hamilton cycle of Kn of weight at most h(w)−k (and constructs such a cycle if it exists). For k fixed, the running time of the algorithm is polynomial in n, where the degree of the polynomial does not depend on k (i.e. the generalised Vizing problem is fixed-parameter tractable with respect to the parameter k).	algorithm;cobham's thesis;hamiltonian path;heuristic (computer science);human body weight;minimum weight;parameterized complexity;polynomial;time complexity;travelling salesman problem;vadim g. vizing	Gregory Gutin;Viresh Patel	2014	CoRR		mathematical optimization;combinatorics;discrete mathematics;mathematics;algorithm	Theory	23.27355576383727	20.778609181667495	144921
180ec6138e899815a1fa092e0d9b43ab6a9dd712	a linear time algorithm for finding a minimum spanning tree with non-terminal set vnt on outerplanar graphs			algorithm;minimum spanning tree;outerplanar graph;time complexity	Shin-ichi Nakayama;Shigeru Masuyama	2017	IEICE Transactions		combinatorics;discrete mathematics;computer science;time complexity;minimum spanning tree;outerplanar graph;distributed minimum spanning tree;minimum degree spanning tree;spanning tree;graph	Theory	23.919679337060632	27.183757067342533	144971
9f2854d9cfcf459fcd7fa0f39cca2ef8f662c881	vertex sparsification and universal rounding algorithms	electrical engineering and computer science;thesis	Suppose we are given a gigantic communication network, but are only interested in a small number of nodes (clients). There are many routing problems we could be asked to solve for our clients. Is there a much smaller network that we could write down on a sheet of paper and put in our pocket that approximately preserves all the relevant communication properties of the original network? As we will demonstrate, the answer to this question is YES, and we call this smaller network a vertex sparsifier. In fact, if we are asked to solve a sequence of optimization problems characterized by cuts or flows, we can compute a good vertex sparsifier ONCE and discard the original network. We can run our algorithms (or approximation algorithms) on the vertex sparsifier as a proxy and still recover approximately optimal solutions in the original network. This novel pattern saves both space (because the network we store is much smaller) and time (because our algorithms run on a much smaller graph). Additionally, we apply these ideas to obtain a master theorem for graph partitioning problems as long as the integrality gap of a standard linear programming relaxation is bounded on trees, then the integrality gap is at most a logarithmic factor larger for general networks. This result implies optimal bounds for many well studied graph partitioning problems as a special case, and even yields optimal bounds for more challenging problems that had not been studied before. Morally, these results are all based on the idea that even though the structure of optimal solutions can be quite complicated, these solution values can be approximated by crude (even linear) functions. Thesis Supervisor: F. Thomson Leighton Title: Professor	approximation algorithm;graph partition;linear programming relaxation;master theorem;mathematical optimization;proxy server;rounding;routing;telecommunications network	Ankur Moitra	2011			mathematical optimization;combinatorics;mathematics;algorithm	Theory	22.86502523815831	18.65698336518874	145005
166b07a7e90fe20220742a45a53c1e6ef52e680a	two classical queries versus one quantum query	potential difference;decision problem;quantum computer;quantum physics;computational complexity;polynomial time	In this note we study the power of so called query-limited computers. We compare the strength of a classical computer that is allowed to ask two NP-questions to an oracle with the strength of a quantum computer that is allowed only one such query. It is shown that any decision problem that requires two parallel (non-adaptive) sat-queries on a classical computer can be solved exactly by a quantum computer using only one sat-oracle call in polynomial time. Such a simulation is impossible for a one-query classical computer, unless P=NP. The result also does not hold if we replace the sat-oracle by a general black-box. It is conjectured that in general two serial (adaptive) classical sat-queries can not be simulated by one quantum query. These results give an example of how a quantum computer is probably more powerful than a classical computer and they also highlight the potential differences between quantum complexity results for general oracles when compared to results for more structured problems. ∗Centre for Quantum Computation, Clarendon Laboratory, Department of Physics, University of Oxford, Parks Road, Oxford OX1 3PU, United Kingdom. Quantum Computing and Advanced Systems Research, C.W.I., P.O.Box 94079, NL–1090 GB Amsterdam, The Netherlands. Email address: wimvdam@qubit.org This work was supported by the European TMR Research Network ERP-4061PL95-1412, Hewlett-Packard, and the Institute for Logic, Language and Computation (Amsterdam).	black box;computation;computer;decision problem;email;oracle machine;p versus np problem;polynomial;quantum computing;simulation;time complexity;triple modular redundancy	Wim van Dam	1998	CoRR		time complexity;quantum operation;no-teleportation theorem;discrete mathematics;quantum information;voltage;d-wave two;quantum t-design;quantum complexity theory;theoretical computer science;quantum capacity;decision problem;mathematics;quantum computer;computational complexity theory;quantum process;quantum algorithm;physics;algorithm;one-way quantum computer;quantum mechanics;quantum sort	Theory	12.058909988620707	21.517153441607004	145013
90f0e0aef28c8c6dd53294676c4faf65845a7362	steiner trees, connected domination and strongly chordal graphs	strongly chordal graph;steiner tree	We consider Steiner tree problems and connected dominating set problems for several classes of graphs. We give a polynomial algorithm and a min-max theorem for the cardinality Steiner problem in strongly chordal graphs and a polynomial algorithm for the weighted connected dominating set problem in series-parallel graphs. We establish simple direct transformations between Steiner problems and connected domination problems for several classes of graphs and establish related NP-completeness results.	dominating set;steiner tree problem	Kevin White;Martin Farber;William R. Pulleyblank	1985	Networks	10.1002/net.3230150109	mathematical optimization;combinatorics;interval graph;steiner tree problem;k-tree;computer science;mathematics;distance-hereditary graph;treewidth;chordal graph;indifference graph;algorithm;tree decomposition	Theory	24.214633431326178	23.844131057130383	145043
89bb0b8ed75f5d8b30b7c55b0864b297a7715fa0	fast matrix rank algorithms and applications	exact linear algebra;matrix rank;randomized algorithm;combinatorial optimization	We consider the problem of computing the rank of an <i>m</i> × <i>n</i> matrix <i>A</i> over a field. We present a randomized algorithm to find a set of <i>r</i> = rank(<i>A</i>) linearly independent columns in <i>Õ</i>(|<i>A</i>| + <i>r</i><sup>ω</sup>) field operations, where |<i>A</i>| denotes the number of nonzero entries in <i>A</i> and ω < 2.38 is the matrix multiplication exponent. Previously the best known algorithm to find a set of <i>r</i> linearly independent columns is by Gaussian elimination, with deterministic running time <i>O</i>(<i>mnr</i><sup>ω-2</sup>). Our algorithm is faster when <i>r</i> < max{<i>m</i>,<i>n</i>}, for instance when the matrix is rectangular. We also consider the problem of computing the rank of a matrix dynamically, supporting the operations of rank one updates and additions and deletions of rows and columns. We present an algorithm that updates the rank in <i>Õ</i>(<i>mn</i>) field operations. We show that these algorithms can be used to obtain faster algorithms for various problems in exact linear algebra, combinatorial optimization and dynamic data structure.	column (database);combinatorial optimization;data structure;dynamic data;gaussian elimination;linear algebra;mathematical optimization;matrix multiplication;randomized algorithm;the matrix;time complexity	Ho Yee Cheung;Tsz Chiu Kwok;Lap Chi Lau	2012	J. ACM	10.1145/2528404	mathematical optimization;gaussian elimination;combinatorics;discrete mathematics;rank;combinatorial optimization;computer science;mathematics;randomized algorithm;augmented matrix;rank;algorithm;low-rank approximation	Theory	16.561894975446176	25.970187800124354	145055
0a8ae548627a3435e3865c25283e3e8b605229ce	the expected additive weight of trees	arbre graphe;algorithmique;algorithm analysis;fonction poids;tree graph;random tree;weighting;search algorithm;ponderacion;grafico planario;algorithmics;algoritmica;informatique theorique;graphe planaire;funcion peso;analyse algorithme;ponderation;weight function;arbol grafo;analisis algoritmo;planar graph;computer theory;informatica teorica	We consider a general additive weight of random trees which depends on the structure of the subtrees, on weight functions defined on the number of internal and external nodes and on the degrees of the nodes appearing in the tree and its subtrees. Choosing particular weight functions, the corresponding weight is an important parameter appearing in the analysis of sorting and searching algorithms. For a simply generated family of rooted planar trees ℱ, we shall derive a general approach to the computation of the average weight of a tree Tεℱ with n nodes and m leaves for arbitrary weight functions. This general result implies exact and asymptotic expressions for many types of average weights of a tree Tεℱ with n nodes if the weight functions are arbitrary polynomials in the number of nodes and leaves with coefficients depending on the node degrees.	coefficient;computation;human body weight;polynomial;search algorithm;sorting;tree (data structure);utility functions on indivisible goods;weight function	Rainer Kemp	1989	Acta Informatica	10.1007/BF00289158	combinatorics;discrete mathematics;weight function;topology;weighting;mathematics;search tree;algorithmics;tree;planar graph;search algorithm	Theory	19.85849673815179	25.684047333144488	145111
4797cb616969bc2c76cd3b276627ddb4bd3ef126	a fast string search algorithm for deep packet classification	packet classification;time complexity;search algorithm;boyer moore algorithm;string search;deep packet classification;numerical simulation	In this paper, we propose a string search algorithm that requires reduced time complexity. It also requires a small amount of memory, and shows better performance than any other algorithm for deep packet classification based on their payload data. The proposed algorithm is based on Boyer–Moore algorithm but requires a much reduced number of operations. In addition, our algorithm’s memory requirement is lower than Boyer–Moore algorithm without sacrificing its speed. We have done time complexity analysis and verified its time complexities through extensive numerical simulations. These simulations show that our algorithm’s performance is better for long text, long pattern, and large alphabet set and even its worst case time complexity linearly depends on the length of the text. q 2004 Elsevier B.V. All rights reserved.	analysis of algorithms;best, worst and average case;boyer–moore string search algorithm;network packet;numerical analysis;payload (computing);simulation;string searching algorithm;time complexity	A. N. M. Ehtesham Rafiq;M. Watheq El-Kharashi;Fayez Gebali	2004	Computer Communications	10.1016/j.comcom.2004.06.005	computer simulation;time complexity;average-case complexity;commentz-walter algorithm;a* search algorithm;computer science;theoretical computer science;machine learning;boyer–moore string search algorithm;worst-case complexity;fsa-red algorithm;best-first search;dinic's algorithm;nondeterministic algorithm;algorithm;difference-map algorithm;rabin–karp algorithm;binary search algorithm;string searching algorithm;search algorithm;population-based incremental learning	HPC	11.523884334679135	28.740873375131965	145489
ee5e2390a1fddfbd6e9fd81adbaaff5968cbd264	on information flow and sorting : new upper and lower bounds for vlsi circuits	sorting very large scale integration upper bound merging circuit optimization wire delay effects contracts zinc read only memory;complexity theory;sorting;very large scale integration;nickel;delay effects;contracts;wire;upper bound;manganese;information flow;tuning;awards activities;integrated circuit modeling;distributed databases;merging;zinc;upper and lower bounds;tv;data flow;read only memory;averaging method;lower bound;circuit optimization	This work comprises two parts: lower bounds and upper bounds in VLSI circuits. The upper bounds are for the sorting problem: we describe a large number of constructions for sorting N numbers in the range [0,M] for the standard VLSI bit model. Among other results, we attain: • VLSI sorter constructions that are within a constant factor of optimal size for almost all number ranges M (including M = N), and running times T. • A fundamentally new merging network for sorting numbers in a bit model. • New organizational approaches for optimal tuning of merging networks and the proper management of data flow. The lower bounds apply to a variety of problems. We present two new techniques for establishing lower bounds on the information flow in VLSI circuits. They are: • An averaging technique, which is easy to apply to a variety of problems, including a long standing question regarding the AT2 complexity for sorting. • A technique for constructing fooling sets in instances where our averaging method is unlikely to provide an adequate bound.	bitonic sorter;dataflow;information flow;sorting;very-large-scale integration	Richard Cole;Alan Siegel	1985	26th Annual Symposium on Foundations of Computer Science (sfcs 1985)	10.1109/SFCS.1985.39	combinatorics;computer science;theoretical computer science;mathematics;upper and lower bounds;distributed database;algorithm;statistics	Theory	11.451380551991711	30.299837421739877	145637
7dfff128bc471942630f9fa0b56de1b10b92bbe3	a heuristic quasi-polynomial algorithm for discrete logarithm in finite fields of small characteristic		In the present work, we present a new discrete logarithm algorithm, in the same vein as in recent works by Joux, using an asymptotically more efficient descent approach. The main result gives a quasi-polynomial heuristic complexity for the discrete logarithm problem in finite field of small characteristic. By quasi-polynomial, we mean a complexity of type $n^{O(\log n)}$ where $n$ is the bit-size of the cardinality of the finite field. Such a complexity is smaller than any $L(\varepsilon)$ for $\epsilon>0$. It remains super-polynomial in the size of the input, but offers a major asymptotic improvement compared to $L(1/4+o(1))$.	algorithm;discrete logarithm;heuristic;polynomial;quasi-polynomial	Razvan Barbulescu;Pierrick Gaudry;Antoine Joux;Emmanuel Thomé	2014		10.1007/978-3-642-55220-5_1	logarithm;combinatorics;mathematical analysis;discrete mathematics;iterated logarithm;baby-step giant-step;mathematics	Crypto	11.572082914462461	20.94334095692325	145678
ad4192fa08508e43fca1d94c8aec928d2be5f905	a probabilistic distributed algorithm for set intersection and its analysis (preliminary version)	set intersection;preliminary version;probabilistic algorithm;markov chain;distributed algorithm	A Probabilistic algorithm for checking set disjointness and performing set intersection of two sets stored at different machines is presented. The algorithm is intended to minimize the amount of communication between the machines. If n is the total number of elements and k is the number of bits required to represent each of the elements, then it is shown that the expected running time of the set disjointness algorithm is O(log log n) rounds, each round consisting of exchanging one message with O(n+k) bits and performing O(n) steps of local computation (all the constants are small). The analysis of the algorithm involves approximating Markov chains by deterministic models.	distributed algorithm;randomized algorithm	Thomas G. Kurtz;Udi Manber	1985		10.1007/BFb0015761	distributed algorithm;discrete mathematics;probabilistic analysis of algorithms;theoretical computer science;distributed computing	Theory	16.90387841111595	31.97578756539569	145966
c7d89dd3db9ee9452640bb9726e1bd4a9842c79a	finding optimal subgraphs by local search	satisfiability;polynomial time;weighted graph;local search	We consider the local search problem of finding a vertex induced subgraph on a vertexweighted graph that satisfies a fixed graph property l-I and maximizes the sum of its vertices’ weights. We show that the problem is complete for the class PLS of polynomial-time local search problems if Il is any nontrivial and hereditary graph property, such as planar, acyclic, complete, bipartite and chordal.	complete (complexity);directed acyclic graph;graph property;induced subgraph;local search (constraint satisfaction);local search (optimization);planar graph;polynomial;search problem;time complexity	Shinichi Shimozono	1997	Theor. Comput. Sci.	10.1016/S0304-3975(96)00135-1	outerplanar graph;time complexity;graph power;edge-transitive graph;mathematical optimization;factor-critical graph;combinatorics;discrete mathematics;graph bandwidth;null graph;graph property;computer science;regular graph;local search;simplex graph;forbidden graph characterization;cycle graph;graph factorization;mathematics;voltage graph;distance-hereditary graph;windmill graph;butterfly graph;crossing number;complement graph;line graph;algorithm;circulant graph;satisfiability	Theory	23.34898282330747	25.953493964031313	145986
92130bedd6fa651a12096a0e35e92fe6df2893da	estimating dominance norms of multiple data streams	upper envelope;recuento;internet protocol;metodo caso peor;dominance;base donnee;distribution donnee;algorithmique;protocolo internet;surveillance;data stream;protocole internet;database;base dato;enumeration counting;denombrement;data distribution;grid;funcion logaritmica;vigilancia;logarithmic function;monitoring;dominancia;marginal distribution;algorithmics;algoritmica;rejilla;fonction logarithmique;enveloppe superieure;methode cas pire;grille;ley marginal;monitorage;monitoreo;worst case method;loi marginale	There is much focus in the algorithms and database communities on designing tools to manage and mine data streams. Typically, data streams consist of multiple signals. Formally, a stream of multiple signals is (i, ai,j) where i’s correspond to the domain, j’s index the different signals and ai,j ≥ 0 give the value of the jth signal at point i. We study the problem of finding norms that are cumulative of the multiple signals in the data stream. For example, consider the max-dominance norm, defined as i maxj{ai,j}. It may be thought as estimating the norm of the “upper envelope” of the multiple signals, or alternatively, as estimating the norm of the “marginal” distribution of tabular data streams. It is used in applications to estimate the “worst case influence” of multiple processes, for example in IP traffic analysis, electrical grid monitoring and financial domain. In addition, it is a natural measure, generalizing the union of data streams or counting distinct elements in data streams. We present the first known data stream algorithms for estimating max-dominance of multiple signals. In particular, we use workspace and time-per-item that are both sublinear (in fact, poly-logarithmic) in the input size. In contrast other notions of dominance on streams a, b — min-dominance ( i minj{ai,j}), countdominance (|{i|ai > bi}|) or relative-dominance ( i ai/max{1, bi} ) — are all impossible to estimate accurately with sublinear space.	best, worst and average case;hilbert space;information;marginal model;maxima and minima;streaming algorithm;t-norm;table (information);traffic analysis;workspace	Graham Cormode;S. Muthukrishnan	2003		10.1007/978-3-540-39658-1_16	internet protocol;marginal distribution;logarithm;computer science;data mining;mathematics;dominance;grid;algorithmics;algorithm;statistics	DB	14.631454210238687	24.166731854093936	145991
e272a0fc578371dda4fed957f59c41f41de8f726	enumerating maximal frequent sets using irredundant dualization	hipergrafico;complejidad espacio;fiabilidad;reliability;primal dual method;duality;intelligence artificielle;methode primale duale;data mining;dualite;transaction data;query complexity;metodo primal dual;association rule;fouille donnee;fiabilite;space complexity;artificial intelligence;dualidad;hypergraph;inteligencia artificial;experimental evaluation;complexite espace;busca dato;hypergraphe	In this paper, we give a new algorithm for enumerating all maximal frequent sets using dualization. Frequent sets in transaction data has been used for computing association rules. Maximal frequent sets are important in representing frequent sets in a compact form, thus many researchers have proposed algorithms for enumerating maximal frequent sets. Among these algorithms, some researchers proposed algorithms for enumerating both maximal frequent sets and minimal infrequent sets in a primal-dual way by using a computation of the minimal transversal for a hypergraph, or in other words, hypergraph dualization. We give an improvement for this kind of algorithms in terms of the number of queries of frequency and the space complexity. Our algorithm checks each minimal infrequent set just once, while the existing algorithms check more than once, possibly so many times. Our algorithm does not store the minimal infrequent sets in memory, while the existing algorithms have to store them. The main idea of the improvement is that minimal infrequent sets computed from maximal frequent sets by dualization is still minimal infrequent even if we add a set to the current maximal frequent sets. We analyze the query complexity and the space complexity of our algorithm theoretically, and experimentally evaluate the algorithm to show that the computation time on average is in the order of the multiplication of the number of maximal frequent sets and the number of minimal infrequent sets.	maximal set	Ken Satoh;Takeaki Uno	2003		10.1007/978-3-540-39644-4_22	combinatorics;discrete mathematics;duality;association rule learning;computer science;transaction data;data mining;reliability;mathematics;dspace;algorithm	ML	17.39476816381798	27.390882070120398	146084
ca4a2cf9afe1ace626dfa1abb7a2293e29150ee9	simple approximation algorithms for maxnaesp and hypergraph 2-colorability	approximation algorithms;hypergraph;2-colorability;set splitting;maxnaesp;max-cut	Hypergraph 2-colorability, also known as set splitting, is a widely studied problem in graph theory. In this paper we study the maximization version  of the same. We recast the problem as a special type of satisfiability problem and give approximation algorithms for it. Our  results are valid for hypergraph 2-colorability, set splitting and MAX-CUT (which is a special case of hypergraph 2-colorability) because the reductions are approximation preserving. Here we study the MAXNAESP problem, the optimal solution to which is  a truth assignment of the literals that maximizes the number of clauses satisfied. As a main result of the paper, we show  that any locally optimal solution (a solution is locally optimal if its value cannot be increased by complementing assignments  to literals and pairs of literals) is guaranteed a performance ratio of         \frac12 +  Î</font >  \frac{1} {2} + \in      . This is an improvement over the ratio of         \frac12 \frac{1} {2}     attributed to another local improvement heuristic for MAX-CUT [6]. In fact we provide a bound of         \frackk + 1 \frac{k} {{k + 1}}     for this problem, where k ≥ 3 is the minimum number of literals in a clause. Such locally optimal algorithms appear to subsume typical greedy algorithms  that have been suggested for problems in the general domain of satisfiability. It should be noted that the NAESP problem where  each clause has exactly two literals, is equivalent to MAX-CUT. However, obtaining good approximation ratios using semi-definite  programming techniques [3] appears difficult. Also, the randomized rounding algorithm as well as the simple randomized algorithm both [4] yield a bound of         \frac12 \frac{1} {2}     for the MAXNAESP problem. In contrast to this, the algorithm proposed in this paper obtains a bound of         \frac12 +  Î</font >  \frac{1} {2} + \in      for this problem.  	approximation algorithm	Daya Ram Gaur;Ramesh Krishnamurti	2001	J. Comb. Optim.	10.1023/A:1011453115618	mathematical optimization;maximum cut;randomized rounding;yield;combinatorics;greedy algorithm;graph theory;mathematics;randomized algorithm;completely randomized design;computational complexity theory;algorithm;satisfiability	Theory	17.95577073935933	18.598994497630418	146309
7a419529f96615ced2fc53f37b590f8430012c63	on edge orienting methods for graph coloring	numero cromatico;chromatic graph;algoritmo busqueda;digraph;graph method;numerical method;algorithme recherche;search algorithm;nombre chromatique;digrafo;edge orienting;longest path;graph coloring;chromatic number;metodo grafo;methode graphe;optimisation combinatoire;busca local;graphe chromatique;metodo numerico;tabu search algorithm;tabu search;combinatorial optimization;local search;methode numerique;recherche locale;busqueda tabu;recherche tabou;optimizacion combinatoria;grafo cromatico;digraphe	We consider the problem of orienting the edges of a graph so that the length of a longest path in the resulting digraph is minimum. As shown by Gallai, Roy and Vitaver, this edge orienting problem is equivalent to finding the chromatic number of a graph. We study various properties of edge orienting methods in the context of local search for graph coloring. We then exploit these properties to derive four tabu search algorithms, each based on a different neighborhood. We compare these algorithms numerically to determine which are the most promising and to give potential research directions.	directed graph;graph coloring;local search (optimization);longest path problem;numerical analysis;search algorithm;tabu search	Bernard Gendron;Alain Hertz;Patrick St-Louis	2007	J. Comb. Optim.	10.1007/s10878-006-9019-3	gallai–hasse–roy–vitaver theorem;graph power;edge contraction;mathematical optimization;combinatorics;graph bandwidth;fractional coloring;null graph;longest path problem;numerical analysis;combinatorial optimization;tabu search;local search;simplex graph;edge cover;edge coloring;moser spindle;graph coloring;graph factorization;mathematics;voltage graph;distance-hereditary graph;butterfly graph;list coloring;complement graph;line graph;algorithm;strength of a graph;friendship graph;search algorithm	AI	21.8061844319611	27.68427049321325	146329
eab1c6eb44f51ef7960ab97c37f7654945ec9628	sorting streamed multisets	streaming algorithms;pire cas;memoire;entropia;procesamiento informacion;algorithm analysis;multiensemble;streaming algorithm;sorting;relacion orden;ordering;68p10;tria;68wxx;input;on line algorithms;algorithme;algorithm;relation ordre;memoria;informatique theorique;entropie;triage;information processing;entree ordinateur;algorithms;analyse algorithme;entropy;entrada ordenador;traitement information;on line algorithm;analisis algoritmo;memory;computer theory;algoritmo;informatica teorica	Sorting is a classic problem and one to which many others reduce easily. In the streaming model, however, we are allowed only one pass over the input and sublinear memory, so in general we cannot sort. In this paper we show that, to determine the sorted order of a multiset s of size n containing σ distinct elements using one pass and o(n log σ) bits of memory, it is generally necessary and sufficient that its entropy H = o(log σ). Specifically, if s = {s1, . . . , sn} and si1 , . . . , sin is the stable sort of s, then we can compute i1, . . . , in in one pass using O((H+1)n) time, O(σ) words plus O((H+1)n) bits of memory, and a simple combination of classic techniques. On the other hand, in the worst case it takes Ω(Hn) bits of memory to compute any sorted ordering of s in one pass.	best, worst and average case;entropy (information theory);sorting algorithm;streaming media	Travis Gagie	2007		10.1016/j.ipl.2008.06.006	proxmap sort;entropy;information processing;computer science;artificial intelligence;theoretical computer science;mathematics;streaming algorithm;algorithm	Theory	15.369667407273596	26.743629298800645	146341
2cecc34e76fb8fa07231b191ffb8f1d82f8eed78	efficient algorithms for graph manipulation [h] (algorithm 447)	efficient algorithm;analysis of algorithm;graphs;analysis of algorithms;graph manipulation;connected component;random access	Efficient algorithms are presented for partitioning a graph into connected components, biconnected components and simple paths. The algorithm for partitioning of a graph into simple paths of iterative and each iteration produces a new path between two vertices already on paths. (The start vertex can be specified dynamically.) If V is the number of vertices and E is the number of edges, each algorithm requires time and space proportional to max (V, E) when executed on a random access computer.	access computer;algorithm;biconnected component;biconnected graph;connected component (graph theory);iteration;random access;vertex (geometry)	John E. Hopcroft;Robert E. Tarjan	1973	Commun. ACM	10.1145/362248.362272	biconnected component;suurballe's algorithm;combinatorics;discrete mathematics;connected component;independent set;graph bandwidth;level structure;floyd–warshall algorithm;graph labeling;computer science;graph partition;analysis of algorithms;theoretical computer science;pseudoforest;hopcroft–karp algorithm;hypercube graph;cycle graph;path graph;mathematics;biconnected graph;k-vertex-connected graph;path;graph;programming language;complement graph;random access;strength of a graph;matching	Theory	19.62021039724693	28.02823596374532	146345
bc53ab1e39b863f1e9748bddf16dfe6f8474e08d	two algorithms for incremental construction of directed acyclic word graphs	search graphs;incremental algorithms;content addressable search;data structures;directed acyclic word graph;lexicon building	In this paper we present two algorithms for building lexicons in Directed Acyclic Word-Graphs (DAWGs). The two algorithms, one for deterministic and the other for non-deterministic DAWGs, can be used instead of the traditional subset construction method. Although the proposed algorithms do not produce the optimal DAWG (i.e., the one with the minimum number of states), they are simple, fast and able to build the DAWG incrementally, as new words are added to the lexicon. Thus, building large lexicons in a DAWG structure becomes an easy task, even for a modest computer.	algorithm;directed acyclic graph	Kyriakos N. Sgarbas;Nikos Fakotakis;George K. Kokkinakis	1995	International Journal on Artificial Intelligence Tools	10.1142/S0218213095000188	natural language processing;data structure;computer science;machine learning;directed acyclic word graph;algorithm	DB	13.340050493811619	27.764253860865963	146467
79c3065c223b622e3755669c75ecdea9343b0da9	a generalized birthday problem	65d10;60c05;birthday problem;approximation;41a10;mathematical modeling	"""A generalized version ofthe birthday problem is as follows. Suppose each member of a population independently receives a number randomly selected from 1, 2, 3, x }, and a random sample of size n is to be taken. If 0 < p < 1, what is the smallest value of n so that the probability that at least two of the sample have the same number is at least p? Both empirical modeling and approximation techniques are used to determine n as a function of x when p is fixed. An error analysis of the approximation is presented. Key words, birthday problem, mathematical modeling, approximation AMS(MOS) subject classifications. 60C05, 41A 10, 65D 10 Introduction. The birthday problem is well known to most students of elementary probability. One version of the problem is as follows 2, p. 207 ]. Suppose a group of n randomly selected individuals is assembled. What is the probability that at least two of them will have the same birthday? The solution is simple to compute and is interesting in that it seems contrary to our intuition. A slight variation of the birthday problem has to do with social security numbers. It may be a common practice for a teacher to identify students by the last four digits of their social security number. How large would the class have to be in order for the probability to be at least one-half that at least two students have the same last four digits? This is the birthday problem with the exception that in place of 365 birthdays we have 10,000 possible """"last four digits."""" The social security problem suggests the following generalization of the birthday problem: Suppose each member of a population independently receives a number randomly selected from l, 2, 3, x } and suppose a random sample of size n is to be selected from the population. If 0 < p < 1, find the smallest value of n so that the probability that at least two of the sample have matching numbers is at least p. As before the solution is simple to compute. First, consider the complementary problem. Let q p and find the smallest value of n so that the probability that no two of the sample have the same item is at most q. Then define the function x(x1)(x2)...(xn+ 1) f(n) xn Received by the editors July 18, 1990; accepted for publication September 17, 1990. f Department of Mathematics, Baylor University, Waco, Texas 76798-7328. 265 D ow nl oa de d 12 /0 8/ 14 to 1 28 .5 9. 22 6. 54 . R ed is tr ib ut io n su bj ec t t o SI A M li ce ns e or c op yr ig ht ; s ee h ttp :// w w w .s ia m .o rg /jo ur na ls /o js a. ph p"""	approximation;command & conquer:yuri's revenge;error analysis (mathematics);mathematical model;numerical aperture;operational amplifier;population;randomness;social security	Frank H. Mathis	1991	SIAM Review	10.1137/1033051	combinatorics;approximation;calculus;mathematical model;mathematics;birthday problem;statistics	Crypto	13.306983512734266	21.822745785723086	146510
65ea2b1ad7787b98ffab0f5dc73aa750aa84c066	a technique for two-dimensional pattern matching	two dimensional structure;algorithm complexity;algorithm analysis;information retrieval;complejidad algoritmo;search strategy;algorithme;algorithm;complexite algorithme;recherche information;pattern matching;structure 2 dimensions;strategie recherche;pattern recognition;analyse algorithme;recuperacion informacion;concordance forme;reconnaissance forme;reconocimiento patron;string matching;estructura 2 dimensiones;analisis algoritmo;estrategia investigacion;algoritmo	By reducing an array matching problem to a string matching problem in a natural way, it is shown that efficient string matching algorithms can be applied to arrays, assuming that a linear preprocessing is made on the text.	matching (graph theory);pattern matching;preprocessor;string searching algorithm	Rui Feng Zhu;Tadao Takaoka	1989	Commun. ACM	10.1145/66451.66459	template matching;approximate string matching;commentz-walter algorithm;computer science;artificial intelligence;3-dimensional matching;pattern matching;optimal matching;mathematics;programming language;string metric;algorithm;string searching algorithm	Theory	14.691184820486136	26.73488226650494	146545
211a34bc1ef86b550be3af4fc95df3d5a556c1b0	parallel algorithms for the edge-coloring and edge-coloring update problems	graphe non oriente;algoritmo paralelo;edge coloring;mise a jour;non directed graph;parallel algorithm;algorithm complexity;multiprocessor;complejidad algoritmo;algorithme parallele;complexite algorithme;grafo no orientado;indexation;puesta al dia;multiprocesador;updating;multiprocesseur	with D 1 1 colors for the edge-coloring problem. However, Holyer has shown that deciding whether a graph requires D or D 1 1 colors is NP-complete [10]. For a multigraph G, Shannon showed that x9(G) # 3D/2 [16]. A number of parallel algorithms exist for the edge-coloring problem. Lev et al. [14] presented parallel edge-coloring algorithms with D colors for bipartite multigraphs. When D 5 2, their algorithm requires O(log D log n) time and O(nD) processors. Otherwise, their algorithm requires O(log D log2 n) time and O(nD) processors on the EREW PRAM. Gibbons et al. [5] and Gibbons and Rytter [6] suggested algorithms for some other special graphs such as trees, outerplanar graphs, and Halin graphs. For trees, their algorithm requires O(log n) time and O(n) processors; for outplanar graphs, their algorithm requires O(log3 n) time and O(n2) processors; for Halin graphs, their algorithm requires O(log2 n) time and O(n) processors. All of their algorithms run in the CREW PRAM, and the number of colors used is at most D. For planar graphs, Chrobak and Yung [3] presented an edge-coloring algorithm with maxhD, 19j colors. Their algorithm runs in O(log2 n) time and uses O(n) processors on the EREW PRAM [3]. Later, Chrobak and Nishizeki [4] improved the algorithm in [3] by reducing the number of colors to maxhD, 8j. Their algorithm requires O(log3 n) time and O(n2) processors [4]. He [9] also discussed the edge-coloring problem with D colors for planar graphs, his algorithm has the same complexity as that of [3]. For general graphs, the pioneer work was done by Karloff and Shmoys [13]. They presented an edge-coloring algorithm with D 1 1 colors for simple graphs, which requires O(D6 log4 n) time and O(n2D) processors, assuming the fastest known algorithm for finding maximal independent sets [7] is used. They also gave a randomized edge-coloring algorithm with D 1 20D1/21« colors, (« # 1/4), which runs in O(logO(1) n) expected time and uses O(nO(1)) processors (independent of «). For multigraphs, Upfal once presented an O(log3 Dn) time algorithm with 3D/2 colors by using O(Dn) processors (appears in [13]). For the special class of multigraphs of D 5 3, Karloff and Shmoys [13] presented an algorithm which runs in O(log n) time and uses O(n) processors. In this paper we study two problems. The first problem JOURNAL OF PARALLEL AND DISTRIBUTED COMPUTING 32, 66–73 (1996) ARTICLE NO. 0005	average-case complexity;binary logarithm;central processing unit;color;distributed computing;edge coloring;fastest;graph coloring;maximal set;multigraph;np-completeness;outerplanar graph;parallel algorithm;parallel random-access machine;planar graph;randomized algorithm;regular expression;shannon (unit)	Weifa Liang;Xiaojun Shen;Qing Hu	1996	J. Parallel Distrib. Comput.	10.1006/jpdc.1996.0005	mathematical optimization;combinatorics;parallel computing;multiprocessing;computer science;theoretical computer science;edge coloring;mathematics;distributed computing;parallel algorithm;algorithm	Theory	18.088582159436825	28.42657923977241	146591
5e5ead387baf1f9b8962733e994dff0cd7ab4d33	on approximating the longest path in a graph	search problem;modelizacion;graph theory;random graph;optimisation;teoria grafo;approximate algorithm;complexity theory;optimizacion;approximation numerique;complexite calcul;maximization;camino hamiltoniano;approximation algorithm;grafo aleatorio;probleme np dur;longest path;problema np duro;graphe aleatoire;problema investigacion;theorie graphe;aproximacion numerica;theorem proving;algorithme;polynomial time algorithm;modelisation;algorithm;hamiltonian graph;demonstration theoreme;np hard problem;complejidad computacion;long paths;computational complexity;chemin hamiltonien;greedy algorithm;optimization;numerical approximation;demostracion teorema;algorithme approximation;performance ratio;probleme recherche;modeling;maximizacion;maximisation;hamiltonian path;algoritmo	We consider the problem of approximating the longest path in undirected graphs. In an attempt to pin down the best achievable performance ratio of an approximation algorithm for this problem, we present both positive and negative results. First, a simple greedy algorithm is shown to find long paths in dense graphs. We then consider the problem of finding paths in graphs that are guaranteed to have extremely long paths. We devise an algorithm that finds paths of a logarithmic length in Hamiltonian graphs. This algorithm works for a much larger class of graphs (weakly Hamiltonian), where the result is the best possible. Since the hard case appears to be that of sparse graphs, we also consider sparse random graphs. Here we show that a relatively long path can be obtained, thereby partially answering an open problem of Broderet al. To explain the difficulty of obtaining better approximations, we also prove hardness results. We show that, for any ε<1, the problem of finding a path of lengthn-n ε in ann-vertex Hamiltonian graph isNP-hard. We then show that no polynomial-time algorithm can find a constant factor approximation to the longest-path problem unlessP=NP. We conjecture that the result can be strengthened to say that, for some constant δ>0, finding an approximation of ration δ is alsoNP-hard. As evidence toward this conjecture, we show that if any polynomial-time algorithm can approximate the longest path to a ratio of $$2^{O(\log ^{1 - \varepsilon } n)} $$ , for any ε>0, thenNP has a quasi-polynomial deterministic time simulation. The hardness results apply even to the special case where the input consists of bounded degree graphs.	approximation algorithm;dtime;graph (discrete mathematics);greedy algorithm;hamiltonian (quantum mechanics);hamiltonian path;longest path problem;polynomial;quasi-polynomial;random graph;simulation;sparse matrix;time complexity	David R. Karger;Rajeev Motwani;G. D. S. Ramkumar	1997	Algorithmica	10.1007/BF02523689	hamiltonian path;gallai–hasse–roy–vitaver theorem;block graph;graph power;mathematical optimization;combinatorics;discrete mathematics;graph bandwidth;longest path problem;distance-regular graph;graph theory;simplex graph;hypercube graph;cubic graph;path graph;mathematics;voltage graph;distance-hereditary graph;path;shortest path problem;butterfly graph;approximation algorithm;line graph;algorithm;strength of a graph;coxeter graph	Theory	21.43344368935252	25.79426706087826	146636
d83cdf1bf7aa4c85d70f43f7be6fde3e86a0cd2c	lattice basis reduction: improved practical algorithms and solving subset sum problems	subset sum problem;lattice basis reduction	We report on improved practical algorithms for lattice basis reduction. We present a variant of the L 3-algorithm with deep insertions and a practical algorithm for blockwise Korkine-Zolotarev reduction, a concept extending L 3-reduction, that has been introduced by Schnorr (1987). Empirical tests show that the strongest of these algorithms solves almost all subset sum problems with up to 58 random weights of arbitrary bit length within at most a few hours on a UNISYS 6000/70 or within a couple of minutes on a SPARC 2 computer.	algorithm;lattice reduction;subset sum problem	Claus-Peter Schnorr;M. Euchner	1991		10.1007/3-540-54458-5_51	mathematical optimization;combinatorics;discrete mathematics;lattice reduction;computer science;mathematics;subset sum problem;algorithm;lattice problem	Theory	12.01828160569224	23.098500875038877	146702
f98b73585f66c309c4c709ed6eaec0ef4b8a1546	random matroids	deterministic space complete problems;lower bounds;polynomial over gf 2;boolean expression;space complexity;set of assignments;cycle free problem	We introduce a new random structure generalizing matroids. These random matroids allow us to develop general techniques for solving hard combinatorial optimization problems with random inputs.	combinatorial optimization;mathematical optimization;matroid	John H. Reif;Paul G. Spirakis	1980		10.1145/800141.804688	mathematical optimization;combinatorics;discrete mathematics;graphic matroid;random element;boolean expression;random compact set;computer science;mathematics;dspace;programming language;algorithm	Theory	17.51448914677193	25.326838524955516	146768
e967b05b5362cddc674a8fc58b38db38687537e5	an exact algorithm for sparse matrix bipartitioning	parallel computing;partitioning;graphs;exact algorithm;parallel;hypergraph;sparse matrix;vector multiplication;branch and bound;sparse matrix vector multiplication	The sparse matrix partitioning problem arises when minimizing communication in parallel sparse matrix-vector multiplications. Since the problem is NP-hard, heuristics are usually employed to find solutions. Here, we present a purely combinatorial branch-and-bound method for computing optimal bipartitionings of sparse matrices, in the sense that they have the lowest communication volume out of all possible bipartitionings obeying a certain load balance constraint. The method is based on a way of partitioning similar to the recently proposed medium-grain heuristic, which reduces the number of solutions to be considered in the branch-and-bound method. We applied the proposed optimal bipartitioner to find the optimal communication volume of all matrices of the University of Florida sparse matrix collection with 1000 nonzeros or less. For 85% of the matrices, an optimal bipartitioning was found within a single day of computation and for 58% even within a second. We also present optimal results for selected larger matrices, up to 129,042 nonzeros. The optimal bipartitionings and corresponding communication volumes are made publicly available in a benchmark collection. ∗This paper has been published in Journal of Parallel and Distributed Computing, vol. 85 (2015), pp. 79-90, doi:10.1016/j.jpdc.2015.06.005 †Scientific Computing Group, Centrum Wiskunde & Informatica, P.O. Box 94079, 1090 GB Amsterdam, The Netherlands, D.M.Pelt@cwi.nl ‡Mathematical Institute, Utrecht University, P.O. Box 80010, 3508 TA Utrecht, The Netherlands, R.H.Bisseling@uu.nl	benchmark (computing);branch and bound;central processing unit;column (database);computation;distributed computing;exact algorithm;heuristic (computer science);np-hardness;obedience (human behavior);parallel computing;partition problem;search tree;sparse matrix;the matrix;time complexity;tree (data structure)	Daniel M. Pelt;Rob H. Bisseling	2015	J. Parallel Distrib. Comput.	10.1016/j.jpdc.2015.06.005	mathematical optimization;combinatorics;parallel computing;sparse matrix;theoretical computer science;sparse approximation;parallel;mathematics;graph;matrix-free methods;branch and bound	HPC	20.792706258359523	19.33863195775317	146965
d4a8ef9be7a4bcd4f45ad54fc0ef1fd6591959de	computing the external geodesic diameter of a simple polygon	shortest path;algorithm complexity;geometrie algorithmique;complejidad algoritmo;computational geometry;polygone;espacio 2 dimensiones;diametre;diameter;algorithme;geodesique;algorithm;complexite algorithme;geodesic;poligon;two dimensional space;geodesico;geometria algoritmica;diametro;espace 2 dimensions;polygono;algoritmo	Given a simple polygonP ofn vertices, we present an algorithm that finds the pair of points on the boundary ofP that maximizes theexternal shortest path between them. This path is defined as theexternal geodesic diameter ofP. The algorithm takes0(n 2) time and requires0(n) space. Surprisingly, this problem is quite different from that of computing theinternal geodesic diameter ofP. While the internal diameter is determined by a pair of vertices ofP, this is not the case for the external diameter. Finally, we show how this algorithm can be extended to solve theall external geodesic furthest neighbours problem. Gegeben sei ein einfaches PolygonP mitn Ecken. Wir geben einen Algorithmus an, der ein Punktepaar auf der Begrenzung vonP liefert, welches die Länge des kürzesten Weges maximiert, der im Äußeren des Polygons verläuft. Den Weg bezeichnen wir als den äußeren geodätischen Durchmesser vonP. Unser Algorithmus benötigt 0(n 2) Zeit und erfordert 0(n) Speicherplatz. Zu unserer Überraschung ist das Problem von dem, der Berechnung des inneren geodätischen Durchmessers vonP völlig verschieden. Während der innere Durchmesser immer in Ecken vonP endet, muß dies für den äußeren Durchmesser nicht der Fall sein. Schließlich zeigen wir noch, daß der Algorithmus so erweitert werden kann, daß er das Problem der entferntesten äußeren geodätischen Nachbarn löst.	algorithm;shortest path problem;vertex (geometry)	David Samuel;Godfried T. Toussaint	1990	Computing	10.1007/BF02247961	combinatorics;topology;computational geometry;polygon;diameter;mathematics;geometry	Theory	23.59796059545539	28.56297359643961	147167
4acad1a804a3f858298a7834bc5d2f5c80071a35	improved tree decomposition based algorithms for domination-like problems	arbre graphe;dynamic programming;dynamique processus;graph theory;ajustamiento modelo;programacion dinamica;teoria grafo;combinatorics;tree graph;complexite calcul;on line;combinatoria;en linea;combinatoire;dynamic program;dinamica proceso;theorie graphe;dominating set;ajustement modele;complejidad computacion;computational complexity;model matching;estructura datos;programmation dynamique;en ligne;structure donnee;tree decomposition;arbol grafo;process dynamics;data structure	"""We present an improved dynamic programming strategy for DOMINATING set and related problems on graphs that are given together with a tree decomposition of width k. We obtain an O(4kn) algorithm for dominating set, where n is the number of nodes of the tree decomposition. This result improves the previously best known algorithm of Telle and Proskurowski running in time O(9kn). The key to our result is an argument on a certain """"monotonicity"""" in the table updating process during dynamic programming.Moreover, various other domination-like problems as discussed by Telle and Proskurowski are treated with our technique.We gain improvements on the base of the exponential term in the running time ranging between 55% and 68% in most of these cases. These results mean significant breakthroughs concerning practical implementations."""	algorithm;dominating set;tree decomposition	Jochen Alber;Rolf Niedermeier	2002		10.1007/3-540-45995-2_52	combinatorics;data structure;dominating set;computer science;graph theory;dynamic programming;mathematics;computational complexity theory;tree;algorithm;tree decomposition	Theory	18.918908670827758	26.25494590489224	147478
3f87270042fd9de5bb8da319510caa1875d85574	a simple and practical linear-work parallel algorithm for connectivity	graph connectivity;experiments;parallel algorithms	Graph connectivity is a fundamental problem in computer science with many important applications. Sequentially, connectivity can be done in linear work easily using breadth-first search or depth-first search. There have been many parallel algorithms for connectivity, however the simpler parallel algorithms require super-linear work, and the linear-work polylogarithmic-depth parallel algorithms are very complicated and not amenable to implementation. In this work, we address this gap by describing a simple and practical expected linear-work, polylogarithmic depth parallel algorithm for graph connectivity. Our algorithm is based on a recent parallel algorithm for generating low-diameter graph decompositions by Miller et al., which uses parallel breadth-first searches. We discuss a (modest) variant of their decomposition algorithm which preserves the theoretical complexity while leading to simpler and faster implementations. We experimentally compare the connectivity algorithms using both the original decomposition algorithm and our modified decomposition algorithm. We also experimentally compare against the fastest existing parallel connectivity implementations (which are not theoretically linear-work and polylogarithmic-depth) and show that our implementations are competitive for various input graphs. In addition, we compare our implementations to sequential connectivity algorithms and show that on 40 cores we achieve good speedup relative to the sequential implementations for many input graphs. We discuss the various optimizations used in our implementations and present an extensive experimental analysis of the performance. Our algorithm is the first parallel connectivity algorithm that is both theoretically and practically efficient.	breadth-first search;computer science;connectivity (graph theory);depth-first search;experiment;fastest;parallel algorithm;polylogarithmic function;speedup	Julian Shun;Laxman Dhulipala;Guy E. Blelloch	2014		10.1145/2612669.2612692	combinatorics;parallel computing;computer science;connectivity;theoretical computer science;distributed computing;parallel algorithm	ML	18.919799216874676	21.27817286245279	147533
6523238e7a60636829af0f4d71776afc382bc1c4	parameterized searching with mismatches for run-length encoded strings - (extended abstract)	run length encoding	  Two strings y and y′ of equal length over respective alphabets Σ  y   and Σ  y′ are said to parameterized match if there exists a bijection π: Σ  y   →Σ  y′ such that π(y ) = y′ , i.e., renaming each character of y according to its corresponding element under π yields y′. (Here we assume that all symbols of both alphabets are used somewhere.) Two natural problems are then parameterized matching, which consists of finding all positions of some text x where a pattern y parameterized matches a substring of x , and approximate parameterized matching, which seeks, at each location of x , a bijection π maximizing the number of parameterized matches at that location.    	run-length encoding	Alberto Apostolico;Péter L. Erdös;Alpár Jüttner	2010		10.1007/978-3-642-16321-0_38		Theory	13.568328120580576	26.27335209222388	147600
944fa1402b90ebed93bf9d42495ef6b015221f0d	three-player entangled xor games are np-hard to approximate	entangled games;pcp theorem;68q10;xor games;bell inequalities;81p68	We show that for any $\varepsilon>0$ the problem of finding a factor $(2-\varepsilon)$ approximation to the entangled value of a three-player XOR game is NP-hard. Equivalently, the problem of approximating the largest possible quantum violation of a tripartite Bell correlation inequality to within any multiplicative constant is NP-hard. These results are the first constant-factor hardness of approximation results for entangled games or quantum violations of Bell inequalities shown under the sole assumption that P$\neq$NP. They can be thought of as an extension of H\aastad's optimal hardness of approximation results for MAX-E3-LIN2 [J. ACM, 48 (2001), pp. 798--859] to the entangled-player setting. The key technical component of our work is a soundness analysis of a plane-vs-point low-degree test against entangled players. This extends and simplifies the analysis of the multilinearity test by Ito and Vidick [Proceedings of the $53$rd FOCS, IEEE, Piscataway, NJ, 2012, pp. 243--252]. Our results demonstrate t...	approximation algorithm;exclusive or;hardness of approximation;np-hardness	Thomas Vidick	2016	SIAM J. Comput.	10.1137/140956622	mathematical optimization;combinatorics;discrete mathematics;computer science;quantum pseudo-telepathy;pcp theorem;mathematics;algorithm;algebra	Theory	11.948296356166102	21.168929309155423	147847
657f6099cf5c929477dc50b1d4bb5cbbf408001b	the resolution complexity of random graph k-colorability	numero cromatico;resolution proof;preuve resolution;random graph;resolution proofs;borne exponentielle;coloracion grafo;proof complexity;grafo aleatorio;nombre chromatique;graph coloring;chromatic number;graphe aleatoire;random graphs;upper bound;fonction densite;coloration graphe;density function;proof systems;informatique theorique;funcion densidad;edge graph;borne inferieure;arete graphe;davis putnam;borne superieure;complexite preuve;lower bound;arista grafico;cota superior;graph colouring;cota inferior;computer theory;systeme preuve;graphe colore;informatica teorica	We consider the resolution proof complexity of propositional formulas which encode random instances of graph k-colorability. We obtain a tradeoff between the graph density and the resolution proof complexity. For random graphs with linearly many edges we obtain linear-exponential lower bounds on the length of resolution refutations. For any > 0, we obtain sub-exponential lower bounds of the form 2n δ for some δ > 0 for non-k-colorability proofs of graphs with n vertices and O(n 3 2 − 1 k − ) edges. We obtain sharper lower bounds for Davis-Putnam-DPLL proofs and for proofs in a system considered by McDiarmid. We also show that very simple algorithms achieve qualitatively similar running times.	dpll algorithm;davis–putnam algorithm;encode;graph coloring;proof complexity;putnam model;random graph;resolution (logic);time complexity	Paul Beame;Joseph C. Culberson;David G. Mitchell;Cristopher Moore	2004	Discrete Applied Mathematics	10.1016/j.dam.2005.05.004	random regular graph;random graph;combinatorics;discrete mathematics;null graph;cycle graph;mathematics;upper and lower bounds;complement graph;line graph;algorithm	Theory	21.127262992165353	26.26764165115369	147899
ab3ab3bb2dcfdfb7c179a1df7d78ce11c744b9a7	faster computation of genome mappability with one mismatch		The genome mappability problem refers to cataloging repetitive occurrences of every substring of length m in a genome, and its k-mappability variant extends this to approximate repeats by allowing up to k mismatches. This problem is formulated as follows: Given a sequence S[1, n] of length n over the constant DNA alphabet Σ = {A, C, G, T}, and two integers k and m ≤ n, output an integer array Fk, such that: Fk[i] = |{j ≠ i|dH(S[i, i + m − 1], S[j, j + m − 1]) ≤ k}| where dH(•,•) represents the hamming distance. Derrien et al. [PLoS one 2012] represented this problem within the framework of genome analysis. In this work we present a provably efficient algorithm for 1-mappability with O(n log n) worst case run time and O(n) spece. The fundamental technique is the heavy path decomposition on the suffix tree (ST) of S, and the entire work is based on the framework by Thankachan et al. [RECOMB 2018]. The previous best known run time is O(n log n log log n) [Alzamel et al., COCOA 2017].	approximation algorithm;best, worst and average case;computation;hamming distance;pathwidth;research in computational molecular biology;run time (program lifecycle phase);substring;suffix tree	Sahar Hooshmand;Paniz Abedin;Daniel Gibney;Srinivas Aluru;Sharma V. Thankachan	2018	2018 IEEE 8th International Conference on Computational Advances in Bio and Medical Sciences (ICCABS)	10.1109/ICCABS.2018.8541897	suffix tree;heavy path decomposition;time complexity;combinatorics;hamming distance;genome;bioinformatics;substring;computer science;log-log plot;integer	Theory	14.229987675759828	25.69999449940412	147946
4c1b663f497a7cce121c8982dd2818ba44462384	faster two dimensional pattern matching with rotations	metodo caso peor;text;time complexity;texte;analysis of algorithm;combinatorial problem;probleme combinatoire;complexite temps;problema combinatorio;modelo 2 dimensiones;pattern matching;modele 2 dimensions;methode cas pire;concordance forme;complejidad tiempo;texto;worst case method;two dimensional model	The most efficient currently known algorithms for two dimensional matching with rotation have a worst case time complexity of O(n 2 m 3 ), where the size of the text is n 2  and the size of the pattern is m 2 . In this paper we present two algorithms for the two dimensional rotated matching problem whose running time is O(n 2 m 2 ). The preprocessing time of the first algorithms is O(m 5 ) and the preprocessing time of the second algorithm is O(m 4 ).	pattern matching	Amihood Amir;Oren Kapah;Dekel Tsur	2004		10.1007/978-3-540-27801-6_31	time complexity;computer science;pattern matching;calculus;mathematics;programming language;algorithm;rabin–karp algorithm	Vision	14.497315993811426	26.81014140389606	147962
1c4dc4cb8d55e71bbb63b056114fd03737b351c1	on the algorithmic effectiveness of digraph decompositions and complexity measures	parameterized complexity;digraph decompositions;computer and information science;polynomial algorithm;treewidth;constant width;data och informationsvetenskap	We place our focus on the gap between treewidth's success in producing fixed-parameter polynomial algorithms for hard graph problems, and specifically Hamiltonian Circuit and Max Cut, and the failure of its directed variants (directed treewidth (Johnson et al., 2001 [13]), DAG-width (Obdrzalek, 2006 [14]) and Kelly-width (Hunter and Kreutzer, 2007 [15]) to replicate it in the realm of digraphs. We answer the question of why this gap exists by giving two hardness results: we show that Directed Hamiltonian Circuit is W[2]-hard when the parameter is the width of the input graph, for any of these widths, and that Max Di Cut remains NP-hard even when restricted to DAGs, which have the minimum possible width under all these definitions. Along the way, we extend our reduction for Directed Hamiltonian Circuit to show that the related Minimum Leaf Outbranching problem is also W[2]-hard when naturally parameterized by the number of leaves of the solution, even if the input graph has constant width. All our results also apply to directed pathwidth and cycle rank.	complexity;directed graph	Michael Lampis;Georgia Kaouri;Valia Mitsou	2011	Discrete Optimization	10.1016/j.disopt.2010.03.010	parameterized complexity;mathematical optimization;combinatorics;discrete mathematics;mathematics;tree-depth;treewidth;algorithm	Theory	22.76295552868367	22.592243843110445	148124
63377d5d495e04a9529312f639f3c9bf46fb63fd	an efficient bottom-up distance between trees	largest common forest;graph algorithms;subtree isomorphism;tree pattern matching;metric space;design and analysis of algorithms;pattern matching;combi- natorial problems;edit distance;tree isomorphism;algorithm design and analysis;computer science;software measurement;pattern analysis;application software;bottom up;tree graphs	A new bottom-updistancemeasur e for labeled trees, which is basedon the largest commonforest of the trees andhasthethreefoldadvantageof independenceof particular editcosts,low complexity, andcoverageof orderedand unorderedtrees,is introducedandrelatedin thispaperwith other distancemeasur espublishedin the literature. Algorithmsfor computingthebottom-updistancein timelinear in thenumberof nodesare givenin full detail.	top-down and bottom-up design	Gabriel Valiente	2001		10.1109/SPIRE.2001.10019	algorithm design;combinatorics;application software;edit distance;metric space;computer science;theoretical computer science;machine learning;pattern matching;top-down and bottom-up design;weight-balanced tree;programming language;software measurement;tree	Theory	16.753927906507343	29.504979141163584	148150
08c523e973b249d303ed5b234d4ccd5fe69ed882	greedy set cover estimations	discrete mathematics;set covering problem;greedy algorithm;set cover	More precise estimation of the greedy algorithm complexity for a special case of the set cover problem is given in this paper. Introduction The greedy heuristic is the most used for optimization problems. The general approach is as follows: repeatedly, a procedure that minimizes (maximizes) the local increase of the objective function is applied. In some cases the greedy strategy guarantees the optimal solution (minimal spanning trees, the shortest path, etc.), in some others it provides acceptable approximations (e.g. disjunctive forms and tests). Typically, the greedy algorithms use simple structures that require minimal computational resource – time and memory. Consider the set cover problem. Given a finite set } , , { 1 n a a A L = and a family of subsets of A , } , , { 1 m A A F L = , such that every element of A belongs to at least one subset from F , F covers A . The problem is in finding a collection F C ⊆ of minimal size, that covers A . The set cover problem is one of the most typical NP-complete problems. It has proven that there is no constant factor approximation to this problem (unless P=NP) [2]. The problem can be represented in terms of (0,1)-matrices. The elements of A correspond to the columns, and each subset from F corresponds to a row of the matrix. The problem is in finding the minimal number of rows that cover at least one “1” in each column. There are known reasonable approximation greedy algorithms for this problem. Consider the following set cover greedy algorithm: – at the first step the algorithm selects the row that contains the maximal number of “1”’s (covers maximal number of elements of A ). At the current step the row, which covers the greatest number of uncovered yet elements, has been selected. It is clear that continuing this process (at most till the last row selection) all elements will be covered. Probably it may occur before. We consider the scheme [1] where a part of elements of A has been covered by the greedy steps, and then, each uncovered element is being covered by taking some new row having “1” in that column. The problem is in estimating the number of all selected rows (the size of cover). We consider the estimation given for a special case in [1], and give a more precise formula for this case. Formula Improvement Given a (0,1)-matrix of size n m× (m is the number of rows). Each column contains at least m γ “1”’s (special case). The problem is to find minimal number of rows that contain at least one “1” on each column. Then the number of “1”’s of the whole matrix is not less than mn γ , and there is a row with at least n γ “1”s. At the first step the greedy algorithm selects the row with maximal number of “1”’s, therefore the selected row will cover at least n γ elements. Let we have done k similar steps, and let the number of uncovered yet elements does not exceed n k δ . The estimate ) 1 ( 1 γ − δ ≤ δ + k k n n is the main related result, obtained in [1], Part 3, Chapter 3.5, p. 136-137.The table below outlines the part of rows, selected during the first k greedy steps. The shaded columns do not contain “1”’s and hence the corresponding elements are not covered yet.	approximation;column (database);computation;computational resource;disjunctive normal form;file spanning;greedy algorithm;heuristic;karp's 21 np-complete problems;mathematical optimization;maximal set;optimization problem;p versus np problem;row (database);set cover problem;shading;shortest path problem;the matrix	Hakob Aslanyan	2011	CoRR		greedy randomized adaptive search procedure;mathematical optimization;combinatorics;greedy algorithm;discrete mathematics;set packing;vertex cover;mathematics;set cover problem;greedy coloring;algorithm	Theory	24.478089273770863	19.072043835463514	148151
45154ae011a3e4afebfc6d5d9dfa70379caf6113	an experimental comparison of orthogonal compaction algorithms (extended abstract)	evaluation performance;orthogonality;performance evaluation;evaluacion prestacion;etat actuel;flujo red;graphe planaire;state of the art;estado actual;network flow;grafo planario;flot reseau;orthogonalite;planar graph;ortogonalidad	We present an experimental study in which we compare the state-of-the-art methods for compacting orthogonal graph layouts. Given the shape of a planar orthogonal drawing, the task is to place the vertices and the bends on grid points so that the total area or the total edge length is minimised. We compare four constructive heuristics based on rectangular dissection and on turn-regularity, also in combination with two improvement heuristics based on longest paths and network flows, and an exact method which is able to compute provable optimal drawings of minimum total edge length. We provide a performance evaluation in terms of quality and running time. The test data consists of two test-suites already used in previous experimental research. In order to get hard instances, we randomly generated an additional set of planar graphs.	algorithm;data compaction;experiment;face (geometry);graph drawing;heuristic (computer science);in the beginning... was the command line;longest path problem;mind;performance evaluation;planar graph;procedural generation;provable security;run time (program lifecycle phase);test data;test suite;time complexity;video post-processing	Gunnar W. Klau;Karsten Klein;Petra Mutzel	2000		10.1007/3-540-44541-2_5	combinatorics;flow network;orthogonality;mathematics;algorithm;planar graph	Theory	19.75339961212222	26.389057192709156	148197
8be5e209da2631216d15b6051bff971f111659bf	a space efficient algorithm for the constrained heaviest common subsequence problem	efficient algorithm;heaviest common subsequence;longest common subsequence;constrained longest common subsequence;constrained heaviest common subsequence	Let Σ be an alphabet. For each letter in Σ a positive weight is assigned to it. The weight of a string <i>S</i> over Σ is defined as the sum of the weights of the letters in <i>S.</i> For two strings <i>X, Y</i>, and a constrained string <i>P</i> over an alphabet Σ, the constrained heaviest common subsequence problem for two strings <i>X</i> and <i>Y</i> with respect to <i>P</i> is to find a sequence <i>Z</i> such that <i>Z</i> is the heaviest, i.e., having the largest weight, common subsequence for <i>X</i> and <i>Y</i> and <i>P</i> is a subsequence of <i>Z.</i> In this paper an <i>O</i>(|<i>X</i>||<i>Y</i>||<i>Z</i>|) time and <i>O</i>((|<i>X</i>|+|<i>Y</i>|)|<i>P</i>|) space algorithm for the constrained heaviest common subsequence problem of two strings is presented, where |<i>X</i>|, |<i>Y</i>|, and |<i>P</i>| denote the lengths of string <i>X, Y, P</i>, respectively.	algorithm	Rao Li	2008		10.1145/1593105.1593164	combinatorics;discrete mathematics;longest increasing subsequence;longest common subsequence problem;mathematics;longest alternating subsequence;algorithm	Theory	13.634963193861621	26.239967103358364	148205
de92c69a70747ac594840663159878dca7fc634b	broadcasting in bounded degree graphs	plazo difusion;networks;interconnection;de bruijn digraphs;telecommunication network;radiodifusion;05c99;delai diffusion;graphs;grado diagrama;red telecomunicacion;68r10;interconnexion;reseau telecommunication;dissemination time;degre graphe;broadcasting;68m10;bounded degree graphs;radiodiffusion;94a05;interconeccion;graph degree	Broadcasting is an information dissemination process in which a message is to be sent from a single originator to all members of a network by placing calls over the communication lines of the network. Several previous papers have investigated methods to construct sparse graphs (networks) in which this process can be completed in minimum time from any originator. The graphs produced by these methods contain high degree vertices. In [20] and in [4] we began an investigation of graphs with fixed maximum degree in which broadcasting can be completed in near minimum time. In this paper we continue this investigation by giving lower bounds and constructing bounded degree graphs which allow rapid broadcasting. Our constructions use ideas developed by Jerrum and Skyum [16] which allow one to move from a graph with good average case behaviour to one with good worst case behaviour. In addition, we use de Bruijn digraphs [7], minimum broadcast graphs, and sparse broadcast graphs [3]. The resulting graphs yield the best broadcasting time known for bounded degree graphs. We also obtain asymptotic upper and lower bounds for broadcasting time, as the maximum degree increases.	best, worst and average case;de bruijn graph;sparse matrix;vertex (graph theory)	Jean-Claude Bermond;Pavol Hell;Arthur L. Liestman;Joseph G. Peters	1992	SIAM J. Discrete Math.	10.1137/0405002	combinatorics;discrete mathematics;interconnection;mathematics;geometry;graph;chordal graph;broadcasting;indifference graph;algorithm;telecommunications network;algebra	Theory	18.751365091288115	31.716434188689004	148449
308d8fdaabb3a4b6f24c33621be06db36dbf03cf	k-best suffix arrays	large dictionary;k-best suffix array;single order;log n;lookup time;sublinear time;example application;suffix array;novel index;standard suffix array sort;click through rate;kd tree;indexation;figure of merit	Suppose we have a large dictionary of strings. Each entry starts with a figure of merit (popularity). We wish to find the kbest matches for a substring, s, in a dictinoary, dict. That is, grep s dict | sort –n | head –k, but we would like to do this in sublinear time. Example applications: (1) web queries with popularities, (2) products with prices and (3) ads with click through rates. This paper proposes a novel index, k-best suffix arrays, based on ideas borrowed from suffix arrays and kdtrees. A standard suffix array sorts the suffixes by a single order (lexicographic) whereas k-best suffix arrays are sorted by two orders (lexicographic and popularity). Lookup time is between log N and sqrt N. 1 Standard Suffix Arrays This paper will introduce k-best suffix arrays, which are similar to standard suffix arrays (Manber and Myers, 1990), an index that makes it convenient to compute the frequency and location of a substring, s, in a long sequence, corpus. A suffix array, suf, is an array of all N suffixes, sorted alphabetically. A suffix, suf[i], also known as a semi-infinite string, is a string that starts at po siti n j in the corpus and continues to the end of the corpus. In practical implementations, a suffix is a 4 byte integer, j. In this way, an int (constant space) denotes a long string ( N bytes). The make_standard_suf program below creates a standard suffix array. The program starts with a corpus, a global variable containing a long string of N characters. The program allocates the suffix array suf and initializes it to a vector of N ints (suffixes) ranging from 0 to N−1. The suffix array is sorted by lexicographic order and returned. int* make_standard_suf () { int N = strlen(corpus); int* suf = (int*)malloc(N * sizeof(int)); for (int i=0; i<N; i++) suf[i] = i; qsort(suf, N, sizeof(int), lexcomp); return suf;} int lexcomp(int* a, int* b) { return strcmp(corpus + *a, corpus + *b);} This program is simple to describe (but inefficient , at least in theory) because trcmp can take O( N) time in the worst case (where the corpus contains two copies of an arbitrarily long string). See http://cm.bell-labs.com/cm/cs/who/doug/ssort.c for an implementation of the O( N log N) Manber and Myers algorithm. However, in practice, when the corpus is a dictionary of relatively short entries (such as web queries), the worst case is unlikely t o come up. In which case, the simple make_suf program above is good enough, and maybe even better than the O( N log N) solution. 1.1 Standard Suffix Array Lookup To compute the frequency and locations of a substring s, use a pair of binary searches to find and j, the locations of the first and last suffix in the suffix array that start with s. Each suffix between i and j point to a location of s in the corpus. The frequency is simply: j − i + 1. Here is some simple code. We show how to find the first suffix. The last suffix is left as an exercise. As above, we ignore the unlikely worst	algorithm;best, worst and average case;byte;c string handling;data structure;dictionary;emoticon;global variable;int (x86 instruction);lexicographical order;lexicography;lookup table;principle of good enough;query string;semiconductor industry;shared nothing architecture;string (computer science);substring;suffix array;text corpus;time complexity	Kenneth Ward Church;Bo Thiesson;Robert Ragno	2007			generalized suffix tree;longest common substring problem;figure of merit;speech recognition;click-through rate;computer science;k-d tree;compressed suffix array;fm-index;algorithm	Theory	14.012393221797801	27.83422281101217	148472
0aeb2a75a616ea74bc5da32c7e3db0e0af6b6ea4	using the incompressibility method to obtain local lemma results for ramsey-type problems	objet;lovasz local lemma;methode connexion;procesamiento informacion;algorithm analysis;data compression;ramsey theory;object;incompressibility;68wxx;algorithme;algorithm;informatique theorique;conexion;raccordement;information processing;borne inferieure;algorithms;68p30;analyse algorithme;theorie ramsey;compresion dato;traitement information;connection;objeto;analisis algoritmo;lower bound;compression donnee;nombre ramsey;numero ramsey;ramsey number;cota inferior;computer theory;algoritmo;informatica teorica	We reveal a connection between the incompressibility method and the Lovász local lemma in the context of Ramsey theory. We obtain bounds by repeatedly encoding objects of interest and thereby compressing strings. The method is demonstrated on the example of van der Waerden numbers. It applies to lower bounds of Ramsey numbers, large transitive subtournaments and other Ramsey phenomena as well.	incompressibility method;ramsey's theorem;string (computer science);transitive reduction	Pascal Schweitzer	2009	Inf. Process. Lett.	10.1016/j.ipl.2008.09.030	data compression;combinatorics;ramsey's theorem;lovász local lemma;information processing;connection;object;calculus;ramsey theory;mathematics;upper and lower bounds;algorithm	Theory	16.577645668629124	26.474726217647962	148553
e604a94bc0a4df9a2997297c2049d9ba6b81f34a	tree compression with top trees revisited	top trees;xml compression;grammar compression;tree compression	We revisit tree compression with top trees (Bille et al. [2]), and present several improvements to the compressor and its analysis. By significantly reducing the amount of information stored and guiding the compression step using a RePair-inspired heuristic, we obtain a fast compressor achieving good compression ratios, addressing an open problem posed by [2]. We show how, with relatively small overhead, the compressed file can be converted into an in-memory representation that supports basic navigation operations in worst-case logarithmic time without decompression. We also show a much improved worst-case bound on the size of the output of top-tree compression (answering an open question posed in a talk on this algorithm by Weimann in 2012).	algorithm;best, worst and average case;data compression;directed acyclic graph;file spanning;heuristic;in-memory database;locality of reference;naivety;overhead (computing);provable security;requirement;time complexity;top tree	Lorenz Hübschle-Schneider;Rajeev Raman	2015		10.1007/978-3-319-20086-6_2	computer science;theoretical computer science;lossless compression;algorithm	Theory	12.2554252815522	27.138212980475256	148580
787e0a24234e7fed0961636084fd43abfe6b7d2f	maximum weight clique algorithms for circular-arc graphs and circle graphs	dynamic programming;clique graph;algorithm analysis;circular arc graph;graphs;graph;graphe;programmation dynamique;graphe cercle;algorithms;analyse algorithme;clique graphe	Circle graphs and circular-arc graphs are the intersection graphs of chords and arcs in a circle. In this paper we present algorithms for finding maximum weight cliques in these graphs. The running times of the algorithms are $O(n^2 + m\log \log n)$ for circle graphs and $O(mn)$ for circular-arc graphs. Our algorithms are based on the scanning of appropriate endpoint sequences and efficient bookkeeping of results for subproblems.		Wen-Lian Hsu	1985	SIAM J. Comput.	10.1137/0214018	strong perfect graph theorem;1-planar graph;block graph;pathwidth;split graph;combinatorics;discrete mathematics;triangle-free graph;cograph;interval graph;topology;graph product;clique problem;pancyclic graph;clique-sum;trapezoid graph;mathematics;maximal independent set;circle graph;modular decomposition;graph;treewidth;partial k-tree;chordal graph;indifference graph;algorithm	Theory	22.716542840488966	27.389692347278626	148600
625c7318d65822d7cfd7d6d3f38dab0fa090d40b	graph layout for applications in compiler construction	compiler construction;graphe non oriente;graph theory;teoria grafo;gramatica grafo;compiler visualization;compilerbau;non directed graph;compilateur;graph drawing;technische informatik;visualizacion;fisheye view;dependence graph;estiramiento;control flow graph;compiler;theorie graphe;etirage;algorithme;algorithm;visualization;analyse syntaxique;grammaire graphe;drawing;visualisation;compound drawing;analisis sintaxico;force directed placement;graph grammar;grafo no orientado;syntactic analysis;directed graph;estructura datos;graphe oriente;hierarchical layout;graph layout;grafo orientado;structure donnee;graph visualization;data structure;compilador;algoritmo	We address graph visualization from the viewpoint of com piler construction Most data structures in compilers are large dense graphs such as annotated control ow graph syntax trees dependency graphs Our main focus is the animation and interactive exploration of these graphs Fast layout heuristics and powerful browsing methods are needed We give a survey of layout heuristics for general directed and undirected graphs and present the browsing facilities that help to manage large structured graphs	compiler;data structure;graph (discrete mathematics);graph drawing;heuristic (computer science)	Georg Sander	1999	Theor. Comput. Sci.	10.1016/S0304-3975(98)00270-9	combinatorics;visualization;data structure;computer science;clique-width;graph theory;theoretical computer science;mathematics;graph;graph drawing;programming language;graph operations;algorithm	PL	21.91696318912196	29.88880878550904	148664
0edd8f52ada9f5bf947e31458fba11c9deefccef	shortest paths in the tower of hanoi graph and finite automata	triangle sierpinski;camino mas corto;tour hanoi;configuracion;shortest path;mathematiques discretes;aplicacion;05bxx;matematicas discretas;automata estado finito;maquina estado finito;efficient algorithm;torre hanoi;discrete mathematics;plus court chemin;graphe fini;average distance;finite graph;calculo automatico;68wxx;computing;hanoi tower;moyenne;algorithme;grafo finito;calcul automatique;unit;algorithm;28a80;sierpin ski gasket;promedio;sierpinski gasket;68r10;finite automata;chemin plus court;distancia;average;finite automaton;automate fini;machine etat fini;configuration;application;68r05;finite state machine;distance;unite;unidad;tower of hanoi;algoritmo	Abstract. We present efficient algorithms for constructing a shortest path between two configurations in the Tower of Hanoi graph, and for computing the length of the shortest path. The key element is a finite-state machine which decides, after examining on the average only a small number of the largest discs (asymptotically, 63 38 ≈ 1.66), whether the largest disc will be moved once or twice. This solves a problem raised by Andreas Hinz, and results in a better understanding of how the shortest path is determined. Our algorithm for computing the length of the shortest path is typically about twice as fast as the existing algorithm. We also use our results to give a new derivation of the average distance 466 885 between two random points on the Sierpiński gasket of unit side.	algorithm;apollonian gasket;automaton;finite-state machine;shortest path problem;sierpinski triangle;tower of hanoi	Dan Romik	2006	SIAM J. Discrete Math.	10.1137/050628660	combinatorics;computing;discrete mathematics;unit;average path length;euclidean shortest path;yen's algorithm;mathematics;geometry;finite-state machine;shortest path problem;configuration;distance;distance;k shortest path routing;shortest path faster algorithm;algorithm	Theory	19.471126776629582	29.20631817676807	148673
1bea39edc2d5e77fb23eba2ac53ab7fe90f1bb90	a new reduction from search svp to optimization svp		It is well known that search SVP is equivalent to optimization SVP. However, the former reduction from search SVP to optimization SVP by Kannan needs polynomial times calls to the oracle that solves the optimization SVP. In this paper, a new rank-preserving reduction is presented with only one call to the optimization SVP oracle. It is obvious that the new reduction needs the least calls, and improves Kannan’s classical result. What’s more, the idea also leads a similar direct reduction from search CVP to optimization CVP with only one call to the oracle.	lattice problem;mathematical optimization;oracle database;polynomial	Gengran Hu;Yanbin Pan	2012	CoRR		parallel computing;real-time computing;algorithm	PL	12.104265478477647	24.003309934336293	148840
54e79162db7dda628f905288dfe74389c4e8f221	good code design with combinatorial approximation algorithms	approximate algorithm		approximation algorithm	Klaus-Uwe Koschnick	1990			mathematical optimization;combinatorics;theoretical computer science;mathematics;approximation algorithm	Theory	17.496644687636405	20.073142525398364	148957
8a4b3365dcab647552975de72e1fc91b4941f818	near-linear time approximation schemes for some implicit fractional packing problems		We consider several implicit fractional packing problems and obtain faster implementations of approximation schemes based on multiplicative-weight updates. This leads to new algorithms with near-linear running times for some fundamental problems in combinatorial optimization. We highlight two concrete applications. The first is to find the maximum fractional packing of spanning trees in a capacitated graph; we obtain a (1− )-approximation in Õ ( m/ 2 ) time, where m is the number of edges in the graph. Second, we consider the LP relaxation of the weighted unsplittable flow problem on a path and obtain a (1− )-approximation in Õ ( n/ 2 ) time, where n is the number of demands.	algorithm;approximation;combinatorial optimization;file spanning;flow network;fractional fourier transform;lagrangian relaxation;linear programming relaxation;mathematical optimization;set packing;time complexity	Chandra Chekuri;Kent Quanrud	2017			mathematical optimization;combinatorics;discrete mathematics;computer science;machine learning;mathematics;geometry;approximation algorithm;locality-sensitive hashing	Theory	22.419204171452474	19.189817582411887	149009
5e5d6cef86f58e5567fe1352322589ef0af7dfb2	large deviation bounds for decision trees and sampling lower bounds for ac0-circuits	lower bounds;small depth circuits;concentration bounds;vegetation decision trees hamming weight noise data structures random variables complexity theory;distribution complexity large deviation bounds decision trees sampling lower bounds bounded depth ac 0 circuits statistical distance data structure lower bounds switching lemma;concentration bounds small depth circuits lower bounds sampling distributions;statistical analysis;sampling distributions;computational complexity;decision trees;statistical analysis computational complexity decision trees	There has been considerable interest lately in the complexity of distributions. Recently, Lovett and Viola (CCC 2011) showed that the statistical distance between a uniform distribution over a good code, and any distribution which can be efficiently sampled by a small bounded-depth AC0 circuit, is inverse-polynomially close to one. That is, such distributions are very far from each other. We strengthen their result, and show that the distance is in fact exponentially close to one. This allows us to strengthen the parameters in their application for data structure lower bounds for succinct data structures for codes. From a technical point of view, we develop new large deviation bounds for functions computed by small depth decision trees, which we then apply to obtain bounds for AC0 circuits via the switching lemma. We show that if such functions are Lipschitz on average in a certain sense, then they are in fact Lipschitz almost everywhere. This type of result falls into the extensive line of research which studies large deviation bounds for the sum of random variables, where while not independent, exhibit large deviation bounds similar to these obtained by independent random variables.	ac0;code;contraction mapping;data structure;decision tree;kolmogorov complexity;maxima and minima;polynomial;randomness extractor;sampling (signal processing);switching lemma;time complexity	Chris Beck;Russell Impagliazzo;Shachar Lovett	2012	2012 IEEE 53rd Annual Symposium on Foundations of Computer Science	10.1109/FOCS.2012.82	mathematical optimization;combinatorics;discrete mathematics;computer science;decision tree;mathematics;sampling distribution;computational complexity theory;statistics	Theory	11.066928848904912	21.349301060261276	149013
37c900e959825934b086082e3f466d12c225000a	odes: an overlapping dense sub-graph algorithm	algorithme;grafo;algorithm;graph;graphe;graph algorithm;algoritmo	SUMMARY Enumeration of the dense sub-graphs of a graph is of interest in community discovery and membership problems, including dense sub-graphs that overlap each other. Described herein is ODES (Overlapping DEnse Sub-graphs), pthreads parallelized software to extract all overlapping maximal sub-graphs whose densities are greater than or equal to a specified cutoff density of at least 1/2.   AVAILABILITY AND IMPLEMENTATION http://dense.sf.net		James Long;Chris Hartman	2010	Bioinformatics	10.1093/bioinformatics/btq514	mathematical optimization;combinatorics;discrete mathematics;mathematics;graph	Comp.	21.865236948888068	30.84258725772066	149325
9930a1cf34927de2f3df53b2891df706a1dae45a	nearly maximum flows in nearly linear time	graph theory;optimisation;approximation algorithms;routing;routing approximation algorithms vegetation vectors equations approximation methods laplace equations;vegetation;iterative methods;laplace equations;vectors;computational complexity;optimisation computational complexity graph theory iterative methods;graph conductance nearly maximum flows maximum flow problem undirected capacitated graph congestion approximators single commodity demand routing residual excess function minimization flow congestion residual demand descent process flow routing nearly minimal congestion iteration laplacian system nearly linear time construction optimal single commodity flow undirected graph α competitive oblivious routing tree;approximation methods	We introduce a new approach to the maximum flow problem in undirected, capacitated graphs using congestion-approximators: easy-to-compute functions that approximate the congestion required to route single-commodity demands in a graph to within some factor α. Our algorithm maintains an arbitrary flow that may have some residual excess and deficits, while taking steps to minimize a potential function measuring the congestion of the current flow plus an over-estimate of the congestion required to route the residual demand. Since the residual term over-estimates, the descent process gradually moves the contribution to our potential function from the residual term to the congestion term, eventually achieving a flow routing the desired demands with nearly minimal congestion after Õ(α<sup>2</sup>ε<sup>-2</sup> log<sup>2</sup> n) iterations. Our approach is similar in spirit to that used by Spielman and Teng (STOC 2004) for solving Laplacian systems, and we summarize our approach as trying to do for ℓ<sub>∞</sub>-flows what they do for ℓ<sub>∞</sub>-flows. Together with a nearly linear time construction of a no(1)-congestion-approximator, we obtain 1 + ε-optimal singlecommodity flows undirected graphs in time m<sup>1+o</sup>(1)ε<sup>-2</sup>, yielding the fastest known algorithm for that problem. Our requirements of a congestion-approximator are quite low, suggesting even faster and simpler algorithms for certain classes of graphs. For example, an α-competitive oblivious routing tree meets our definition, even without knowing how to route the tree back in the graph. For graphs of conductance φ, a trivial φ<sup>-1</sup>-congestionapproximator gives an extremely simple algorithm for finding Õ(mφ<sup>-1</sup>).	approximation algorithm;conductance (graph);fastest;flow network;graph (discrete mathematics);iteration;maximum flow problem;network congestion;offset binary;requirement;routing;symposium on theory of computing;time complexity	Jonah Sherman	2013	2013 IEEE 54th Annual Symposium on Foundations of Computer Science	10.1109/FOCS.2013.36	mathematical optimization;routing;combinatorics;discrete mathematics;computer science;graph theory;mathematics;iterative method;computational complexity theory;approximation algorithm;algorithm;vegetation	Theory	22.914027570332678	21.484331521422448	149329
049400564b462503f00b035f3512d39db3625306	a new linear kernel for undirected planar feedback vertex set: smaller and simpler		We show that any instance I of the Feedback Vertex Set problem in undirected planar graphs can be reduced to an equivalent instance I such that (i) the size of the instance and the size of the minimum feedback vertex set do not increase, (ii) and the size of the minimum feedback vertex set in I is at least 1 29 of the number of vertices in I.T his implies a2 9k kernel for this problem with parameter k being the size of the feedback vertex set. Our result improves the previous results of 97k and 112k.	feedback vertex set;graph (discrete mathematics)	Mingyu Xiao	2014		10.1007/978-3-319-07956-1_26	mathematical optimization;combinatorics;discrete mathematics;feedback arc set;feedback vertex set;vertex cover;vertex model;vertex;mathematics;vertex;neighbourhood	Theory	23.440648326540973	21.766581244613803	149347
e1d987fc0653a65ae6b934bb38cb512b32308f07	minimum average congestion of enhanced and augmented hypercubes into complete binary trees	embedding;hypercube;complete binary tree;optimisation;interconnection;combinatorics;optimizacion;05c05;combinatoria;combinatoire;moyenne;interconexion;parallel architectures;average edge congestion;architecture parallele;arbol binario;informatique theorique;promedio;plongement;arbre binaire;interconnexion;fixed interconnection parallel architecture;average;optimization;augmented hypercube;inmersion;parallel architecture;enhanced hypercube;folded hypercube;computer theory;binary tree;informatica teorica;hipercubo	We study the embedding problem of enhanced and augmented hypercubes into complete binary trees. Using tree traversal techniques, we compute minimum average edge-congestion of enhanced and augmented hypercubes into complete binary trees.	binary tree;fibonacci heap;marching cubes;network congestion;olap cube;tree traversal	Paul D. Manuel	2011	Discrete Applied Mathematics	10.1016/j.dam.2010.12.001	combinatorics;discrete mathematics;binary search tree;topology;binary tree;mathematics	Theory	18.476638035473304	29.38218141186368	149460
1f4bf0edba08980eafa2a22aba7a7c0e323785e5	a tagless marking that is linear over subtrees	tree;graph traversal;garbage collection	Abstract   A new tagless marking algorithm traverses and marks nodes of a directed binary graph, initiating tests for cycles only as need arises. Because trees do not trigger any tests, the algorithm takes linear time on any graph subtree.	item unique identification;tree (data structure)	G. Lyon	1988	Inf. Process. Lett.	10.1016/0020-0190(88)90077-4	combinatorics;discrete mathematics;mathematics;algorithm	DB	16.21804670765206	29.113475656049182	149491
d920a7d95d8ee116b1c6b910f32a36a0deb6f828	on the worst case of a minimal spanning tree algorithm for euclidean space	euclidean space;k-d tree.;minimal spanning tree;k d tree	This paper concerns the worst case running time of the minimal spanning tree algorithm presented by Bentley and Friedman.	algorithm;file spanning;minimum spanning tree	Jyrki Katajainen	1983	BIT		segment tree;euclidean minimum spanning tree;combinatorics;discrete mathematics;kruskal's algorithm;tree rotation;topology;vantage-point tree;minimum degree spanning tree;spanning tree;prim's algorithm;euclidean space;minimum spanning tree;range tree;k-d tree;k-ary tree;interval tree;connected dominating set;k-minimum spanning tree;mathematics;search tree;reverse-delete algorithm;distributed minimum spanning tree;avl tree;shortest-path tree	Theory	20.15745628597222	25.945205394238172	149609
5870a7ac1f68a2a435a56bad5ee7d620b1927ab3	minimum tree decompositions with a given tree as a factor			steiner tree problem	Anna Lladó;Susana-Clara López	2005	Australasian J. Combinatorics		order statistic tree;tree rotation;gomory–hu tree;k-ary tree;avl tree;artificial intelligence;mathematics;vantage-point tree;interval tree;segment tree;pattern recognition	Theory	16.909089097667763	28.934995432098127	149793
d047ac8e91bba527d22868d6f0eb820e53ab8973	lca queries in directed acyclic graphs	calcul matriciel;directed acyclic graph;grafo aciclico;producto matriz;interrogation base donnee;interrogacion base datos;graphe acyclique;automaton;acyclic graph;automata;directed graph;automate;matriz booleana;graphe oriente;lowest common ancestor;datavetenskap datalogi;grafo orientado;matrix multiplication;matrix calculus;produit matrice;database query;calculo de matrices;matrix product;matrice booleenne;boolean matrix	We present two methods for finding a lowest common ancestor (LCA) for each pair of vertices of a directed acyclic graph (dag) on n vertices and m edges. The first method is surprisingly natural and solves the all-pairs LCA problem for the input dag on n vertices and m edges in time O(nm). As a corollary, we obtain an O(n)-time algorithm for finding genealogical distances considerably improving the previously known O(n) timebound for this problem. The second method relies on a novel reduction of the all-pairs LCA problem to the problem of finding maximum witnesses for Boolean matrix product. We solve the latter problem and hence also the all-pairs LCA problem in time O(n 1 4−ω ), where ω = 2.376 is the exponent of the fastest known matrix multiplication algorithm. This improves the previously known O(n w+3 2 ) time-bound for the general all-pairs LCA problem in dags.	directed acyclic graph;fastest;lowest common ancestor;matrix multiplication algorithm;vertex (geometry)	Miroslaw Kowaluk;Andrzej Lingas	2005		10.1007/11523468_20	combinatorics;discrete mathematics;matrix multiplication;computer science;artificial intelligence;mathematics;automaton;directed acyclic graph;algorithm	Theory	17.180712831194683	26.994316437495524	149880
7770be2e614fa64bc689c456162900b0c72cc982	on end-vertices of lexicographic breadth first searches	interval graph;complexity;interval graphs;lexicographic breadth first search;graph classes;graph algorithm;simplicial vertex;cocomparability graphs;breadth first search;at free graphs;chordal graphs;chordal graph	Recently Lexicographic Breadth First Search (LBFS) has received considerable attention and has often been employed in amulti-sweep fashion. One variant of LBFS called LBFS+ breaks ties by choosing the last vertex of the tied set in a previous LBFS. This has motivated the study of vertices that may appear last in an LBFS (called end-vertices). In this paper, we present various theoretical and algorithmic results concerning end-vertices. © 2009 Elsevier B.V. All rights reserved.	breadth-first search;lexicographical order;vertex (geometry)	Derek G. Corneil;Ekkehard Köhler;Jean-Marc Lanlignel	2010	Discrete Applied Mathematics	10.1016/j.dam.2009.10.001	pathwidth;mathematical optimization;combinatorics;discrete mathematics;complexity;interval graph;breadth-first search;lexicographic breadth-first search;mathematics;chordal graph;indifference graph;algorithm	AI	22.799152269778343	23.346075406515084	149902
0e12a6d608dc44ed8ef7423d20024c9229409c90	upper bound on the communication complexity of private information retrieval	base donnee;information retrieval;private information retrieval;communication complexity;database;base dato;upper bound;recherche information;informatique theorique;recuperacion informacion;computer theory;informatica teorica	We construct a scheme for private information retrieval with k databases and communication complexity O(n 1=(2k?1)).	communication complexity;database;personally identifiable information;private information retrieval	Andris Ambainis	1996	IACR Cryptology ePrint Archive	10.1007/3-540-63165-8_196	private information retrieval;computer science;theoretical computer science;data mining;communication complexity;database;upper and lower bounds;computer security;information retrieval	Theory	16.06112304757555	26.168122456472886	149960
b264901b43afb34b0a45f97b87f4cc3fe858e627	bitwise-parallel reduction for connection tests	game theory;combinatorial games;fully connected player;connection tests;connection game;color;y reduction bitwise parallelism combinatorial game connection game hex y;board fills;testing;games business process re engineering testing color encoding monte carlo methods arrays;monte carlo playouts;combinatorial game;arrays;y;monte carlo methods combinatorial mathematics computer games game theory;monte carlo method;games;hex;hexagonal connection games;bitwise parallel reduction;bitwise parallelism;parallel implementation;combinatorial game bitwise parallel reduction connection tests hexagonal connection games fully connected player board fills monte carlo playouts;monte carlo;computer games;business process re engineering;encoding;combinatorial mathematics;monte carlo methods;y reduction;business process	This paper introduces bitwise-parallel reduction (BPR), an efficient method for performing connection tests in hexagonal connection games such as Hex and Y. BPR is based on a known property of Y that games can be reduced to a single value indicating the fully connected player (if any) through a sequence of reduction operations. We adapt this process for bitwise-parallel implementation and demonstrate its benefit over a range of board sizes. BPR is by far the fastest known method if connection tests only need to be performed once per game, for example, to evaluate board fills following Monte Carlo playouts.	64-bit computing;algorithm;bitwise operation;cell (microprocessor);fastest;hard coding;hex;monte carlo method;monte carlo tree search;playout;schmidt decomposition	Cameron Browne;Stephen Tavener	2012	IEEE Transactions on Computational Intelligence and AI in Games	10.1109/TCIAIG.2012.2195003	game theory;simulation;computer science;theoretical computer science;algorithm;statistics;monte carlo method	EDA	15.80457741359733	30.82828282931733	149965
42839a8b869229d7762bac728e7e59e362d59dff	parameterized complexity of independence and domination on geometric graphs	conjunto independiente;graph theory;geometric graph;teoria grafo;parameterized complexity;independent set;fixed parameter tractable;theorie graphe;methode calcul;metodo calculo;dominating set;ensemble independant;segment droite;segmento recta;conjunto dominando;line segment;intersection graphs;computing method;ensemble dominant;maximum independent set	We investigate the parameterized complexity of Maximum Independent Set and Dominating Set restricted to certain geometric graphs. We show that Dominating Set is W[1]-hard for the intersection graphs of unit squares, unit disks, and line segments. For Maximum Independent Set, we show that the problem is W[1]-complete for unit segments, but fixed-parameter tractable if the segments are axis-parallel.	apache axis;cobham's thesis;dominating set;independent set (graph theory);parameterized complexity	Dániel Marx	2006		10.1007/11847250_14	combinatorics;discrete mathematics;independent set;topology;bidimensionality;dominating set;graph theory;mathematics;maximal independent set;indifference graph	Theory	23.66492833886778	28.965155594465923	149981
1e3b7a007de1eb6e520c3f7dfc9a003fb6193488	a capacity-rounding algorithm for the minimum-cost circulation problem: a dual framework of the tardos algorithm	thesaurus;research resource;researcher;organization;search engine;science and technology;paper;integrated search;maximum flow;idea;jst;technical term;institute;japan science and technology agency;graph flow;sparkingarticle;expanding;database;magazine;patent;technical trend;reseau;chemical substance;journal;professional;red;flujo grafo;linking;algorithme;algorithm;j global;search;algorritmo;probleme combinatoire;research and development;problema combinatorio;bobliography;ｊｇｌｏｂａｌ;computational complexity;flot graphe;comprehensive search;material;funding;polynomial algorithm;a capacity rounding algorithm for the minimum cost circulation problem a dual framework of the tardos algorithm;facility;r d;flot cout minimum;jglobal;jdream;gene;ｊ ｇｌｏｂａｌ;imagination;research project;combinatory problem;related search;network flow;ｊｓｔ;article;network;shortest path problem;linkcenter	Recently, E. Tardos gave a strongly polynomial algorithm for the minimum-cost circulation problem and solved the open problem posed in 1972 by J. Edmonds and R.M. Karp. Her algorithm runs in O(m2T(m, n) logm) time, wherem is the number of arcs,n is the number of vertices, andT(m, n) is the time required for solving a maximum flow problem in a network withm arcs andn vertices. In the present paper, taking an approach that is a dual of Tardos's, we also give a strongly polynomial algorithm for the minimum-cost circulation problem. Our algorithm runs in O(m2S(m, n) logm) time and reduces the computational complexity, whereS(m, n) is the time required for solving a shortest path problem with a fixed origin in a network withm arcs,n vertices, and a nonnegative arc length function. The complexity is the same as that of Orlin's algorithm, recently developed by efficiently implementing the Edmonds-Karp scaling algorithm.	algorithm;circulation problem;rounding	Satoru Fujishige	1986	Math. Program.	10.1007/BF01580882	mathematical optimization;computer science;artificial intelligence;mathematics;operations research;algorithm;research	Theory	20.419115333545804	26.478194655389416	150081
14e7570db2d8588e93b2747a13bd364322338604	path-width and three-dimensional straight-line grid drawings of graphs	arbre graphe;p 720 straight line;camino grafo;tree graph;z 250 geometry;graph path;rapport aspect;estiramiento;three dimensional;etirage;series parallel graph;g 070 area edge length;graphe serie parallele;drawing;relacion dimensional;p 060 3d;graphe planaire;vertex graph;grafo linea;chemin graphe;grafo planario;line graph;arbol grafo;graphe ligne;series parallel;vertice grafo;planar graph;sommet graphe;aspect ratio	We prove that every n-vertex graph G with path-width pw(G) has a three-dimensional straight-line grid drawing with O(pw(G) ·n) volume. Thus for graphs with bounded path-width the volume is O(n), and it follows that for graphs with bounded tree-width, such as series-parallel graphs, the volume is O(n log n). No better bound than O(n) was previously known for drawings of series-parallel graphs. For planar graphs we obtain three-dimensional drawings with O(n) volume and O( √ n) aspect ratio, whereas all previous constructions with O(n) volume have Θ(n) aspect ratio.	pathwidth;planar graph;series-parallel graph;treewidth	Vida Dujmovic;Pat Morin;David R. Wood	2002		10.1007/3-540-36151-0_5	1-planar graph;pathwidth;combinatorics;aspect ratio;topology;mathematics;geometry;line graph;drawing;planar graph	Theory	23.93744121659971	29.28215251946413	150164
8a3ce5d61c70b74714692ba56dea2931d60f8c7c	reassembling trees for the traveling salesman	traveling salesman problem;approximation algorithm;90c27;spanning tree;s t path tsp	Many recent approximation algorithms for different variants of the traveling salesman problem (asymmetric TSP, graph TSP, s-t-path TSP) exploit the well-known fact that a solution of the natural linear programming relaxation can be written as convex combination of spanning trees. The main argument then is that randomly sampling a tree from such a distribution and then completing the tree to a tour at minimum cost yields a better approximation guarantee than simply taking a minimum cost spanning tree (as in Christofides’ algorithm). We argue that an additional step can help: reassembling the spanning trees before sampling. Exchanging two edges in a pair of spanning trees can improve their properties under certain conditions. We demonstrate the usefulness for the metric s-t-path TSP by devising a deterministic polynomial-time algorithm that improves on Sebő’s previously best approximation ratio of 8 5 . keywords: traveling salesman problem, s-t-path TSP, approximation algorithm, spanning tree	approximation algorithm;christofides algorithm;file spanning;linear programming relaxation;minimum spanning tree;polynomial;randomness;sampling (signal processing);time complexity;travelling salesman problem;whole earth 'lectronic link	Jens Vygen	2016	SIAM J. Discrete Math.	10.1137/15M1010531	euclidean minimum spanning tree;mathematical optimization;combinatorics;kruskal's algorithm;christofides algorithm;minimum degree spanning tree;spanning tree;combinatorial optimization;minimum spanning tree;mathematics;travelling salesman problem;distributed minimum spanning tree;approximation algorithm;algorithm;shortest-path tree	Theory	21.710641547276175	19.132416486012	150291
166b403258a81a1ea28352ca213d34cb149c2c8b	efficient algorithms for the longest path problem	efficient algorithms;algorithmique;camino grafo;graph path;efficient algorithm;camino hamiltoniano;temps lineaire;linear time algorithm;longest path;tiempo lineal;longest path problem;graphe pondere;grafo pondero;graph classes;algorithmics;hamiltonian path problem;algoritmica;linear time;chemin graphe;chemin hamiltonien;weighted graph;threshold graph;hamiltonian path	The longest path problem is to find a longest path in a given graph While the graph classes in which the Hamiltonian path problem can be solved efficiently are widely investigated, very few graph classes are known where the longest path problem can be solved efficiently For a tree, a simple linear time algorithm for the longest path problem is known We first generalize the algorithm, and it then solves the longest path problem efficiently for weighted trees, block graphs, ptolemaic graphs, and cacti We next propose three new graph classes that have natural interval representations, and show that the longest path problem can be solved efficiently on those classes As a corollary, it is also shown that the problem can be solved efficiently on threshold graphs.	algorithm;longest path problem	Ryuhei Uehara;Yushi Uno	2004		10.1007/978-3-540-30551-4_74	hamiltonian path;gallai–hasse–roy–vitaver theorem;mathematical optimization;combinatorics;discrete mathematics;canadian traveller problem;widest path problem;fast path;longest path problem;mathematics;hamiltonian path problem;path;shortest path problem;algorithmics;induced path	Theory	20.4980416694756	27.052248305118024	150309
60025a6e901e9530917f5641b1a2285fda69e13b	the complexity of finding uniform sparsest cuts in various graph classes	parameterized complexity;sparsest cut;clique width;unit interval graph;treewidth	Given an undirected graph G=(V,E), the (uniform, unweighted) sparsest cut problem is to find a vertex subset [email protected]?V minimizing |E(S,[email protected]?)|/(|S||[email protected]?|). We show that this problem is NP-complete, and give polynomial time algorithms for various graph classes. In particular, we show that the sparsest cut problem can be solved in linear time for unit interval graphs, and in cubic time for graphs of bounded treewidth. For cactus graphs and outerplanar graphs this can be improved to linear time and quadratic time, respectively. For graphs of clique-width k for which a short decomposition is given, we show that the problem can be solved in time O(n^2^k^+^1), where n is the number of vertices in the input graph. We also establish that a running time of the form n^O^(^k^) is optimal in this case, assuming that the Exponential Time Hypothesis holds.	apx;approximation algorithm;cut (graph theory);interval arithmetic;linear programming relaxation;np-completeness;pathwidth;polynomial;pseudo-polynomial time;time complexity;treewidth;vertex (geometry)	Paul S. Bonsma;Hajo Broersma;Viresh Patel;Artem V. Pyatkin	2012	J. Discrete Algorithms	10.1016/j.jda.2011.12.008	1-planar graph;outerplanar graph;block graph;pathwidth;parameterized complexity;mathematical optimization;split graph;combinatorics;discrete mathematics;interval graph;independent set;longest path problem;clique-width;pancyclic graph;comparability graph;clique-sum;mathematics;tree-depth;modular decomposition;treewidth;partial k-tree;chordal graph;indifference graph;book embedding;line graph	Theory	24.115854437662474	24.235519971544925	150493
8694c9c389116dab6524a0468c647d9301a67208	fault-tolerant spanners for doubling metrics: better and simpler		In STOC’95 Arya et al. [2] conjectured that for any constant dimensional n-point Euclidean space, a (1+ǫ)-spanner with constant degree, diameter O(log n) and weight O(log n) ·ω(MST ) can be built in O(n · log n) time. Recently Elkin and Solomon [10] (technical report, April 2012) proved this conjecture of Arya et al. in the affirmative. In fact, the proof of [10] is more general in two ways. First, it applies to arbitrary doubling metrics. Second, it provides a complete tradeoff between the three involved parameters that is tight (up to constant factors) in the entire range. Subsequently, Chan et al. [5] (technical report, July 2012]) provided another proof for Arya et al.’s conjecture, which is simpler than the proof of Elkin and Solomon [10]. Moreover, Chan et al. [5] also showed that one can build a fault-tolerant (FT) spanner with similar properties. Specifically, they showed that there exists a k-FT (1 + ǫ)-spanner with degree O(k), diameter O(log n) and weight O(k · logn) · ω(MST ). The running time of the construction of [5] was not analyzed. In this work we improve the results of Chan et al. [5], using a simpler proof. Specifically, we present a simple proof which shows that a k-FT (1 + ǫ)-spanner with degree O(k), diameter O(log n) and weight O(k · log n) · ω(MST ) can be built in O(n · (logn+ k)) time. Similarly to the constructions of [10] and [5], our construction applies to arbitrary doubling metrics. However, in contrast to the construction of Elkin and Solomon [10], our construction fails to provide a complete (and tight) tradeoff between the three involved parameters. The construction of Chan et al. [5] has this drawback too. For random point sets in R, we “shave” a factor of logn from the weight bound. Specifically, in this case our construction provides within the same time O(n · (log n + k)), a k-FT (1 + ǫ)-spanner with degree O(k), diameter O(log n) and weight that is with high probability O(k) · ω(MST ). ∗Department of Computer Science and Applied Mathematics, The Weizmann Institute of Science, Rehovot 76100, Israel. E-mail: shay.solomon@weizmann.ac.il. Part of this work was done while this author was a graduate student in the Department of Computer Science, Ben-Gurion University of the Negev, under the support of the Clore Fellowship grant No. 81265410, the BSF grant No. 2008430, and the ISF grant No. 87209011.	arbitrary-precision arithmetic;bean scripting framework;chan's algorithm;computer science;fault tolerance;period-doubling bifurcation;time complexity;with high probability	Shay Solomon	2012	CoRR		mathematical optimization;combinatorics;calculus;mathematics;algorithm	Theory	12.098327852777723	21.995636163726182	150660
ec4af73f0a807538394be857b1dacbf6d21621b0	graph orientations optimizing the number of light or heavy vertices	latter problem;heavy vertex;fixed non-negative integer;graph orientation problem;maximum independent set;undirected graph;maximize 0-light;computational complexity;different value;minimum vertex cover;minimize 1-heavy	This paper introduces four graph orientation problems named Maximize W -Light, Minimize W -Light, Maximize W -Heavy, and Minimize W -Heavy, where W can be any fixed non-negative integer. In each problem, the input is an undirected, unweighted graph G and the objective is to assign a direction to every edge in G so that the number of vertices with outdegree at most W or at least W in the resulting directed graph is maximized or minimized. A number of results on the computational complexity and polynomial-time approximability of these problems for different values of W and various special classes of graphs are derived. In particular, it is shown that Maximize 0-Light and Minimize 1-Heavy are identical to Maximum Independent Set and Minimum Vertex Cover, respectively, so by allowing the value of W to vary, we obtain a new generalization of the two latter problems. Submitted: May 2014 Reviewed: July 2015 Revised: August 2015 Accepted: September 2015 Final: September 2015 Published: October 2015 Article type: Regular paper Communicated by: K. Kawarabayashi A preliminary version of this article appeared in Proceedings of the 2 International Symposium on Combinatorial Optimization (ISCO 2012), volume 7422 of Lecture Notes in Computer Science, pp. 332–343, Springer-Verlag Berlin Heidelberg, 2012. This research was funded by KAKENHI grant numbers 23500020, 25330018, 26330017, and 26540005 and The Hakubi Project at Kyoto University. E-mail addresses: asahiro@is.kyusan-u.ac.jp (Yuichi Asahiro) jj@kuicr.kyoto-u.ac.jp (Jesper Jansson) miyano@ces.kyutech.ac.jp (Eiji Miyano) hirotaka@econ.kyushu-u.ac.jp (Hirotaka Ono) 442 Asahiro et al. Graph Orientations Optimizing Light or Heavy Vertices	approximation algorithm;combinatorial optimization;computational complexity theory;directed graph;graph (discrete mathematics);independent set (graph theory);lecture notes in computer science;optimizing compiler;orientation (graph theory);shadow volume;springer (tank);strong orientation;time complexity;vertex (geometry);vertex cover	Yuichi Asahiro;Jesper Jansson;Eiji Miyano;Hirotaka Ono	2015	J. Graph Algorithms Appl.	10.7155/jgaa.00371	loop;gallai–hasse–roy–vitaver theorem;graph power;mathematical optimization;combinatorics;discrete mathematics;feedback arc set;independent set;directed graph;graph bandwidth;level structure;graph center;null graph;degree;graph toughness;regular graph;distance-regular graph;cycle graph;vertex;mathematics;graph;bound graph;complement graph;strength of a graph	Theory	21.321519366420937	19.913494534127864	150684
d241ced84cf6aabac1c68f9c28a0a4ab520fe8ef	an information-theoretic approach to time bounds for on-line computation (preliminary version)	deterministic space complete problems;time complexity;lower bounds;polynomial over gf 2;boolean expression;computational complexity;space complexity;set of assignments;information theoretic;lower bound;cycle free problem	Static, descriptional complexity (program size) [16, 9] can be used to obtain lower bounds on dynamic, computational complexity (such as running time). We describe and discuss this “information-theoretic approach” in the following section. Paul introduced it in [13], to obtain restricted lower bounds on the time complexity of sorting. We use the approach here to obtain lower time bounds for on-line simulation of one abstract storage unit by another. A major goal of our work is to promote the approach.	computation;computational complexity theory;descriptive complexity theory;information theory;online and offline;sorting;time complexity;web-based simulation	Wolfgang J. Paul;Joel I. Seiferas;Janos Simon	1980		10.1145/800141.804685	complete;time complexity;complexity class;mathematical optimization;combinatorics;discrete mathematics;average-case complexity;ph;decision tree model;boolean expression;quantum complexity theory;computer science;structural complexity theory;worst-case complexity;mathematics;dspace;upper and lower bounds;computational complexity theory;asymptotic computational complexity;algorithm;descriptive complexity theory	Theory	14.537415293641375	24.709946113237926	150854
2c433699fc16c305ceebe2ce0928d415af07271d	hybridization number on three rooted binary trees is ept	rooted phylogenetic tree;rooted phylogenetic network;reticulate evolution;fixed parameter tractability;hybridization number;05c85;92d15;agreement forest	Phylogenetic networks are leaf-labeled directed acyclic graphs that are used to describe nontreelike evolutionary histories and are thus a generalization of phylogenetic trees. The hybridization number of a phylogenetic network is the sum of all in-degrees minus the number of nodes plus one. The hybridization number problem takes as input a collection of rooted binary phylogenetic trees and asks to construct a phylogenetic network that contains an embedding of each of the input trees and has the smallest possible hybridization number. We present an algorithm for the hybridization number problem on three binary phylogenetic trees on n leaves that runs in time O(ckpoly(n)) with k the hybridization number of an optimal network and c some (astronomical) constant. For the case of two trees, an algorithm with running time O(3.18kn) was proposed before, whereas an algorithm with running time O(ckpoly(n)), also called an EPT algorithm, had prior to this article remained elusive for more than two trees. The algorithm for two trees uses the close connection to acyclic agreement forests to achieve a linear exponent in the running time, while previous algorithms for more than two trees (explicitly or implicitly) relied on a brute force search through all possible underlying network topologies, leading to running times that are not O(ckpoly(n)) for any c. The connection to acyclic agreement forests is much weaker for more than two trees, so even given the right agreement forest, the reconstruction of the network poses major challenges. We prove novel structural results that allow us to reconstruct a network without having to guess the underlying topology. Our techniques generalize to more than three input trees with the exception of one key lemma that maps nodes in the network to tree nodes in order to minimize the amount of guessing involved in constructing the network. The main open problem therefore is to prove results that establish such a mapping for more than three trees.	algorithm;brute-force search;directed acyclic graph;map;network topology;phylogenesis;phylogenetic network;phylogenetic tree;phylogenetics;recursion (computer science);second level address translation;time complexity	Leo van Iersel;Steven Kelk;Nela Lekic;Chris Whidden;Norbert Zeh	2016	SIAM J. Discrete Math.	10.1137/15M1036579	combinatorics;discrete mathematics;mathematics	Theory	18.646016472943256	22.265212875742964	150887
3964ad0f99492b8cafb2c74801d6ce7a99255da3	extractor-based time-space lower bounds for learning		A matrix <i>M</i>: <i>A</i> × <i>X</i> → {−1,1} corresponds to the following learning problem: An unknown element <i>x</i> ∈ <i>X</i> is chosen uniformly at random. A learner tries to learn <i>x</i> from a stream of samples, (<i>a</i><sub>1</sub>, <i>b</i><sub>1</sub>), (<i>a</i><sub>2</sub>, <i>b</i><sub>2</sub>) …, where for every <i>i</i>, <i>a</i><sub><i>i</i></sub> ∈ <i>A</i> is chosen uniformly at random and <i>b</i><sub><i>i</i></sub> = <i>M</i>(<i>a</i><sub><i>i</i></sub>,<i>x</i>).   Assume that <i>k</i>, <i>l</i>, <i>r</i> are such that any submatrix of <i>M</i> of at least 2<sup>−<i>k</i></sup> · |<i>A</i>| rows and at least 2<sup>−<i>l</i></sup> · |<i>X</i>| columns, has a bias of at most 2<sup>−<i>r</i></sup>. We show that any learning algorithm for the learning problem corresponding to <i>M</i> requires either a memory of size at least Ω(<i>k</i> · <i>l</i> ), or at least 2<sup>Ω(<i>r</i>)</sup> samples. The result holds even if the learner has an exponentially small success probability (of 2<sup>−Ω(<i>r</i>)</sup>).   In particular, this shows that for a large class of learning problems, any learning algorithm requires either a memory of size at least Ω((log|<i>X</i>|) · (log|<i>A</i>|)) or an exponential number of samples, achieving a tight Ω((log|<i>X</i>|) · (log|<i>A</i>|)) lower bound on the size of the memory, rather than a bound of Ω(min{(log|<i>X</i>|)<sup>2</sup>,(log|<i>A</i>|)<sup>2</sup>}) obtained in previous works by Raz [FOCS’17] and Moshkovitz and Moshkovitz [ITCS’18].   Moreover, our result implies all previous memory-samples lower bounds, as well as a number of new applications.   Our proof builds on the work of Raz [FOCS’17] that gave a general technique for proving memory samples lower bounds.	algorithm;column (database);commitment ordering;randomness extractor;time complexity	Sumegha Garg;Ran Raz;Avishay Tal	2017	Electronic Colloquium on Computational Complexity (ECCC)	10.1145/3188745.3188962	combinatorics;discrete mathematics;exponential function;mathematics;row;extractor;exponential growth;upper and lower bounds;matrix (mathematics)	Theory	13.355129599169532	20.482219117653926	150955
2b560eb549dde4a9c073efff23233cc459137433	partitioning algorithms for transportation graphs and their applications to routing			algorithm;routing	Cavit Aydin;Doug Ierardi	1997			discrete mathematics;link-state routing protocol;theoretical computer science;computer science;graph	Theory	20.539602926513616	30.06124235531272	150969
1e7ed33b6be0e128de03fc40b1d7d2db5ec9c027	cgmgraph/cgmlib: implementing and testing cgm graph algorithms on pc clusters	distributed system;graphe biparti;foret graphe;systeme reparti;protocole transmission;graph method;grafo bipartido;equation euler;metodo grafo;approche deterministe;methode graphe;deterministic approach;protocolo transmision;hierarchical classification;sistema repartido;efficient implementation;diffusion donnee;ecuacion euler;enfoque determinista;difusion dato;pc cluster;classification hierarchique;graph algorithm;bosque grafo;data broadcast;first integral;forest graph;bipartite graph;clasificacion jerarquizada;euler equation;transmission protocol	In this paper, we present CGMgraph, the first integrated library of parallel graph methods for PC clusters based on CGM algorithms. CGMgraph implements parallel methods for various graph problems. Our implementations of deterministic list ranking, Euler tour, connected components, spanning forest, and bipartite graph detection are, to our knowledge, the first efficient implementations for PC clusters. Our library also includes CGMlib, a library of basic CGM tools such as sorting, prefix sum, one to all broadcast, all to one gather, h-Relation, all to all broadcast, array balancing, and CGM partitioning. Both libraries are available for download at http: //cgm.dehne.net.	algorithm;computer cluster;list of algorithms	Albert Chan;Frank Dehne	2003		10.1007/978-3-540-39924-7_20	combinatorics;bipartite graph;computer science;theoretical computer science;operating system;distributed computing;deterministic system;euler equations;algorithm	DB	18.6379717491299	29.304868810212373	151160
b6beef3a6b88b036b99fc0d4940dd86423b05376	intersection representations of matrices by subtrees and unicycles on graphs	unicycle graph;data clustering;intersection representation of a matrix;hypergraph;intersection graphs;adjacency matrix;computational biology;biological network clustering;intersection graph;database management system;biological network	Consider a 0-1 matrix M(i,j) with columns C={c1, c2,..., cm}, and rows R, or – equivalently – a hypergraph M(R,C) having M as its adjacency matrix (where R are the vertices and C are the hyperedges). Denote ri={cj| cj∈C and M(i,j)=1}. We consider the following two problems: (a) Is there a graph H, with vertex set C, such that every vertex subgraph H(ri) of H is a tree and the intersection of every two such trees is also a tree? (b) Is there a graph H, with vertex set C, such that every H(ri) is a unicycle and the intersection of every two and every three unicycles is a tree? These questions occur in application areas such as database management systems and computational biology; e.g., in the latter they arise in the context of the analysis of biological networks, primarily for the purpose of data clustering. We describe algorithms to find such intersection representations of a matrix M (and equivalently of the hypergraph M), when they exist.	adjacency matrix;algorithm;biological network;cluster analysis;column (database);computation;computational biology;database;emoticon;regular expression;tree (data structure);vertex (graph theory)	Fanica Gavril;Ron Y. Pinter;Shmuel Zaks	2008	J. Discrete Algorithms	10.1016/j.jda.2007.08.001	biological network;combinatorics;discrete mathematics;computer science;theoretical computer science;mathematics;cluster analysis;intersection graph;adjacency matrix	Theory	23.974012912780246	27.683082737156795	151459
1736944e5f848558bd3264b1ce8839bd91010b1c	faster exponential time algorithms for the shortest vector problem	algorithm analysis;shortest vector problem;exact solution;upper bound;sieving algorithms;cryptography;space complexity;software implementations;upper and lower bounds	"""We present new faster algorithms for the exact solution of the shortest vector problem in arbitrary lattices. Our main result shows that the shortest vector in any <i>n</i>-dimensional lattice can be found in time 2<sup>3.199<i>n</i></sup> (and space 2<sup>1.325<i>n</i></sup>), or in space 2<sup>1.095<i>n</i></sup> (and still time 2<sup><i>O(n)</i></sup>). This improves the best previously known algorithm by Ajtai, Kumar and Sivakumar [Proceedings of STOC 2001] which was shown by Nguyen and Vidick [J. Math. Crypto. 2(2):181--207] to run in time 2<sup>5.9<i>n</i></sup> and space 2<sup>2.95<i>n</i></sup>. We also present a practical variant of our algorithm which provably uses an amount of space proportional to τ<i>n</i>, the """"kissing"""" constant in dimension <i>n</i>. No upper bound on the running time of our second algorithm is currently known, but experimentally the algorithm seems to perform fairly well in practice, with running time 2<sup>0.52<i>n</i></sup>, and space complexity 2<sup>0.2<i>n</i></sup>."""	algorithm;dspace;experiment;lattice problem;symposium on theory of computing;time complexity	Panagiotis Voulgaris;Daniele Micciancio	2009		10.1137/1.9781611973075.119	mathematical optimization;combinatorics;discrete mathematics;mathematics;geometry;upper and lower bounds;shortest path faster algorithm;algorithm	Theory	12.485207568233893	22.84878139984866	151484
300eec22026c6cd5fb7bfcfe6aad61a1709369c7	average linear time and compressed space construction of the burrows-wheeler transform		The Burrows-Wheeler Transform is a text permutation that has revolutionized the fields of pattern matching and text compression, bridging the gap existing between the two. In this paper we approach the BWT-construction problem generalizing a well-known algorithm—based on backward search and dynamic strings manipulation—to work in a context-wise fashion, using automata on words. Let n, σ, and Hk be the text length, the alphabet size, and the k-th order empirical entropy of the text, respectively. Moreover, let H∗ k = min{Hk + 1, dlog σe}. Under the word RAM model with word size w ∈ Θ(logn), our algorithm builds the BWT in average O(nH∗ k ) time using nH∗ k + o(nH∗ k ) bits of space, where k = logσ(n/ log 2 n) − 1. We experimentally show that our algorithm has very good performances (essentially linear time) on DNA sequences, using about 2.6 bits per input symbol in RAM.	algorithm;alphabet (formal languages);automata theory;automaton;backward induction;best, worst and average case;bioinformatics;bridging (networking);burrows–wheeler transform;data compression;data structure;discrete logarithm;entropy (information theory);experiment;load balancing (computing);pattern matching;performance;random-access memory;semiconductor industry;time complexity;workspace;worst-case complexity;xml	Alberto Policriti;Nicola Gigante;Nicola Prezza	2015		10.1007/978-3-319-15579-1_46	speech recognition;theoretical computer science;mathematics;algorithm	Theory	12.38769382448116	27.212747749197057	151684
27dd0f4af31ca591d6320fb622f8d9f8ff19e4b0	directed subset feedback vertex set is fixed-parameter tractable	directed graphs;important separators;subset feedback vertex set;parameterized algorithms;qa75 electronic computers computer science szamitastechnika szamitogeptudomany;qa74 analysis analizis	Given a graph <i>G</i> and an integer <i>k</i>, the <scp>Feedback Vertex Set</scp> (FVS) problem asks if there is a vertex set <i>T</i> of size at most <i>k</i> that hits all cycles in the graph. The first fixed-parameter algorithm for FVS in undirected graphs appeared in a monograph of Mehlhorn in 1984. The fixed-parameter tractability (FPT) status of FVS in directed graphs was a long-standing open problem until Chen et al. (STOC ’08, JACM ’08) showed that it is fixed-parameter tractable by giving a 4<sup><i>k</i></sup><i>k</i>! · <i>n</i><sup><i>O</i>(1)</sup> time algorithm. There are two subset versions of this problems: We are given an additional subset <i>S</i> of vertices (resp., edges), and we want to hit all cycles passing through a vertex of <i>S</i> (resp., an edge of <i>S</i>); the two variants are known to be equivalent in the parameterized sense. Recently, the <scp>Subset</scp> FVS problem in undirected graphs was shown to be FPT by Cygan et al. (ICALP’11, SIDMA’13) and independently by Kakimura et al. (SODA ’12). We generalize the result of Chen et al. (STOC ’08, JACM ’08) by showing that a <scp>Subset</scp> FVS in directed graphs can be solved in time 2<sup><i>O</i>(<i>k</i><sup>3</sup>)</sup>ċ<i>n</i><sup><i>O</i>(1)</sup> (i.e., FPT parameterized by size <i>k</i> of the solution). By our result, we complete the picture for FVS problems and their subset versions in undirected and directed graphs. The technique of random sampling of important separators was used by Marx and Razgon (STOC ’11, SICOMP ’14) to show that <scp>Undirected Multicut</scp> is FPT, and it was generalized by Chitnis et al. (SODA ’12, SICOMP ’13) to directed graphs to show that <scp>Directed Multiway Cut</scp> is FPT. In addition to proving the FPT of a <scp>Directed Subset</scp> FVS, we reformulate the random sampling of important separators technique in an abstract way that can be used with a general family of transversal problems. We believe this general approach will be useful for showing the FPT of other problems in directed graphs. Moreover, we modify the probability distribution used in the technique to achieve better running time; in particular, this gives an improvement from 2<sup>2<i>O</i>(<i>k</i>)</sup> to 2<sup><i>O</i>(<i>k</i><sup>2</sup>)</sup> in the parameter dependence of the <scp>Directed Multiway Cut</scp> algorithm of Chitnis et al. (SODA ’12, SICOMP ’13).	algorithm;cobham's thesis;directed graph;entity–relationship model;feedback vertex set;graph (discrete mathematics);iterative compression;iterative method;journal of the acm;kernelization;monte carlo method;multiway branch;parameterized complexity;polynomial kernel;siam journal on computing;sampling (signal processing);search tree;symposium on theory of computing;time complexity;vertex (graph theory)	Rajesh Hemant Chitnis;Marek Cygan;Mohammad Taghi Hajiaghayi;Dániel Marx	2011	ACM Trans. Algorithms	10.1145/2700209	mathematical optimization;combinatorics;discrete mathematics;directed graph;mathematics;algorithm	Theory	20.88536396536135	21.526460222700436	151846
f1980d51f54fbf43bb3f897f7e90948cc34422e3	tree compatibility and inferring evoluationary history	evolutionary history;algorithmique;algorithm complexity;tree;tree compatibility problem;complejidad algoritmo;arbol;probleme compatibilite arbre;algorithme;algorithm;complexite algorithme;algorithmics;algoritmica;arbre;algoritmo	The Tree Compatibility Problem is a problem concerned with evolutionary history construction. For a species set S, an S-labelled tree is a tree T along with a labelling L : S + V(T). The Tree Compatibility Problem asks whether a set 7 of S-labelled trees, each of which describes the evolution of S, is compatible in the sense that there is a single tree T’ from which each tree T E I can be derived by a sequence of edge contractions. Polynomial time algorithms for the Tree Compatibility problem have been known to exist for a long time, with the best being the recent O(n2k) algorithm by Gusfield to determine compatibility of k trees on n species. Recently Gusfield found an O(nk) algorithm for a restricted case of the Tree Compatibility Problem. In this paper, we will present an O(nk) algorithm for the Tree Compatibility Problem, thus achieving linear time. The Perfect Phylogeny Problem is another problem in evolutionary history construction, which has been recently shown to be in P for special cases but NP-Complete for the general case. We will show that we can characterize each instance to the Perfect Phylogeny Problem as an instance to the Tree Compatibility Problem, and thus derive an algorithm for the Perfect Phylogeny Problem. Our algorithm is superior to the other algorithms for Perfect Phylogeny Problem for many realistic cases, such as when the species are defined by molecular data such as aminoacid sequences.	algorithm;karp's 21 np-complete problems;p (complexity);phylogenetic tree;phylogenetics;time complexity	Tandy J. Warnow	1993		10.1006/jagm.1994.1018	optimal binary search tree;red–black tree;mathematical optimization;combinatorics;tree rotation;vantage-point tree;binary tree;computer science;order statistic tree;range tree;tree rearrangement;gomory–hu tree;k-ary tree;interval tree;mathematics;tree structure;search tree;tree;algorithmics;tree traversal;algorithm	Theory	17.4949777833272	23.039745255770345	151966
0da43276c8926778f2656d348edbcb633cefa336	two strategies for solving the vertex cover problem on a transputer network	vertex cover;branch and bound	 In this article, we present an implementation of a distributed branch and bound algorithmsolving the Vertex Cover problem on a network of up to 63 Transputers.We implemented two different strategies: The first parallelization of our branch andbound algorithm is fully distributed. Every processor performs the same algorithm but ona different part of the solution tree. In this case it is necessary to distribute subproblemsamong the processors to achieve a well balanced workload.Our second... 	transputer;vertex cover	Reinhard Lüling;Burkhard Monien	1989		10.1007/3-540-51687-5_40	mathematical optimization;combinatorics;discrete mathematics;feedback vertex set;vertex cover;edge cover;vertex;neighbourhood	Theory	18.08655034660215	28.425369572143087	152057
689dcaa4bd250e2ad2d0f8cf5afd43f06db24388	online data structures in external memory	algorithm performance;almacenamiento informacion;algoritmo adaptativo;geometrie algorithmique;sistema informatico;computational geometry;computer system;input output;organizacion memoria;adaptive algorithm;information storage;algorithme adaptatif;resultado algoritmo;informatique theorique;estructura datos;organisation memoire;performance algorithme;algorithme em;stockage information;geometria computacional;structure donnee;systeme informatique;computer application;algoritmo em;memory organization;external memory;information system;em algorithm;data structure;article;systeme information;computer theory;sistema informacion;informatica teorica	The data sets for many of today’s computer applications are too large to fit within the computer’s internal memory and must instead be stored on external storage devices such as disks. A major performance bottleneck can be the input/output communication (or I/O) between the external and internal memories. In this paper we discuss a variety of online data structures for external memory, some very old and some very new, such as hashing (for dictionaries), B-trees (for dictionaries and 1-D range search), buffer trees (for batched dynamic problems), interval trees with weight-balanced B-trees (for stabbing queries), priority search trees (for 3-sided 2-D range search), and R-trees and other spatial structures. We also discuss several open problems along the way.	auxiliary memory;b-tree;computational geometry;computer data storage;data structure;dictionary;external storage;graph coloring;input/output;r-tree;range searching;ray casting;string (computer science)	Jeffrey Scott Vitter	1999		10.1007/3-540-48523-6_10	input/output;data structure;expectation–maximization algorithm;computational geometry;computer science;artificial intelligence;operating system;geometry;memory organisation;programming language;information system;algorithm	DB	12.319324047051992	29.93602146391058	152088
2182f02ce14aadcd6eafeb85a86be9ba006fb2c5	parameterized complexity of the list coloring reconfiguration problem with graph parameters		Let G be a graph such that each vertex has its list of available colors, and assume that each list is a subset of the common set consisting of k colors. For two given list colorings of G, we study the problem of transforming one into the other by changing only one vertex color assignment at a time, while at all times maintaining a list coloring. This problem is known to be PSPACEcomplete even for bounded bandwidth graphs and a fixed constant k. In this paper, we study the fixed-parameter tractability of the problem when parameterized by several graph parameters. We first give a fixed-parameter algorithm for the problem when parameterized by k and the modular-width of an input graph. We next give a fixed-parameter algorithm for the shortest variant which computes the length of a shortest transformation when parameterized by k and the size of a minimum vertex cover of an input graph. As corollaries, we show that the problem for cographs and the shortest variant for split graphs are fixed-parameter tractable even when only k is taken as a parameter. On the other hand, we prove that the problem is W[1]-hard when parameterized only by the size of a minimum vertex cover of an input graph. 1998 ACM Subject Classification G.2.2 Graph Theory	algorithm;cobham's thesis;cograph;color;graph (discrete mathematics);graph coloring;graph theory;list coloring;parameterized complexity;vertex cover	Tatsuhiko Hatanaka;Takehiro Ito;Xiao Zhou	2017		10.4230/LIPIcs.MFCS.2017.51	graph power;parameterized complexity;mathematical optimization;combinatorics;discrete mathematics;fractional coloring;complete coloring;edge coloring;graph coloring;graph factorization;voltage graph;list coloring;greedy coloring	Theory	23.978810315013224	23.160112904596478	152148
546b014e849376aff61b21070657ca11a1398852	approximation algorithms for the shortest total path length spanning tree problem	camino mas corto;approximation ratio;graph theory;shortest path;teoria grafo;approximate algorithm;time complexity;rapport approximation;aproximacion;arbre maximal;plus court chemin;theorie graphe;shortest path spanning tree problem;approximation;algorithme;algorithm;complexite temps;arbol maximo;spanning tree;complejidad tiempo;spst problem;algoritmo	Given an undirected graph with a nonnegative weight on each edge, the shortest total path length spanning tree problem is to nd a spanning tree of the graph such that the total path length summed over all pairs of the vertices is minimized. In this paper, we present several approximation algorithms for this problem. Our algorithms achieve approximation ratios of 2, 15/8, and 3/2 in time O(n + f(G)); O(n), and O(n) respectively, in which f(G) is the time complexity for computing all-pairs shortest paths of the input graph G and n is the number of vertices of G. Furthermore, we show that the approximation ratio of (4=3 + ) can be achieved in polynomial time for any constant ¿ 0. ? 2000 Elsevier Science B.V. All rights reserved.	approximation algorithm;file spanning;graph (discrete mathematics);minimum spanning tree;polynomial;shortest path problem;shortest total path length spanning tree;time complexity;vertex (geometry)	Bang Ye Wu;Kun-Mao Chao;Chuan Yi Tang	2000	Discrete Applied Mathematics	10.1016/S0166-218X(00)00185-2	time complexity;graph power;euclidean minimum spanning tree;mathematical optimization;combinatorics;widest path problem;graph bandwidth;minimum degree spanning tree;longest path problem;spanning tree;graph theory;minimum spanning tree;approximation;path graph;connected dominating set;k-minimum spanning tree;mathematics;trémaux tree;path;shortest path problem;cycle basis;induced path;distance;tree;k shortest path routing;algorithm;shortest-path tree	Theory	21.41053222480541	27.09072921694642	152222
97e37d02554a7e720c8e208d6d65667d3960b22e	on balanced edge connectivity and applications to some bottleneck augmentation problems in networks	algorithm complexity;temps polynomial;gollete estrangulamiento;aumentacion;probleme np complet;complejidad algoritmo;augmentation;reseau;red;optimisation combinatoire;decision problem;polynomial time algorithm;goulot etranglement;complexite algorithme;increase;polynomial time;rational number;problema np completo;weight function;combinatorial optimization;bottleneck;np complete problem;network;optimizacion combinatoria;tiempo polinomial	Let w i : V V ! Q, i = 1; 2 be two weight functions on the possible edges of a directed or undirected graph with vertex set V such that for the cut function, the inequality w2 (T) := X i2T j6 2T w 2 (ij) 0; holds for every T 2 2 V. We consider the computation of the value ~ (w 1 ; w 2 ; k) deened by ~ (w 1 ; w 2 ; k) := minfl j w1 (T) + l w2 (T) k 8 ; T V g : We show that the associated decision problem is NP-complete, but for a class of instances we can give a polynomial time algorithm. This class is closely related to the following bottleneck augmentation problem. Consider a network N = (V; E; c) with a rational valued capacity function c: V V ! Q + , and let k be a positive, rational number. Consider the problem of nding a capacity function c 0 : V V ! Q + such that, in the resulting network N 0 = (V; E; c + c 0) the edge connectivity number c+c 0 is at least k, and the maximal increase c 0 (ij) is minimal. We give an algorithm which computes such an augmentation in strongly polynomial time.	algorithm;computation;decision problem;graph (discrete mathematics);k-edge-connected graph;maximal set;np-completeness;p (complexity);polynomial;social inequality;time complexity	Michael R. Bussieck	1996	Math. Meth. of OR	10.1007/BF01680371	time complexity;mathematical optimization;combinatorics;weight function;np-complete;combinatorial optimization;decision problem;mathematics;algorithm;rational number	Theory	21.681691240080895	27.141016324001694	152243
74fab726ebb74f79bec8e13435bbfb57df0ee1cf	power of quantum computing with restricted postselections		We consider restricted versions of postBQP where postselection probabilities can be efficiently calculated by classical or quantum computers. We show that such restricted postBQP classes are in AWPP(⊆ PP = postBQP). This result suggests that postselecting an event with an exponentially small probability does not necessarily boost BQP to PP. The best upperbound of BQP is AWPP, and therefore restricted postBQP classes are other examples of complexity classes “slightly above BQP”. In [C. M. Lee and J. Barrett, arXiv:1412.8671], it was shown that the computational capacity of a general probabilistic theory which satisfies the tomographic locality is in AWPP. Our result therefore implies that quantum physics with restricted postselections is an example of a super quantum theory which seems to be outside of their general probabilistic theory but its computational capacity is also in AWPP. Although it is physically natural to expect that such restricted postBQP classes are not equivalent to BQP, we show that UP ∩ coUP, which is unlikely to be in BQP, is contained in the restricted postBQP classes. Finally, we also consider another restricted class of postBQP, where the postselected probability depends only on the size of inputs. We show that this restricted version is contained in APP, where AWPP ⊆ APP ⊆ PP = postBQP.	awpp (complexity);app store;bqp;barrett reduction;complexity class;computer;locality of reference;pp (complexity);postbqp;postselection;quantum computing;quantum mechanics	Tomoyuki Morimae;Harumichi Nishimura	2015	CoRR		theoretical computer science;mathematics	Theory	10.645032849016355	20.43997757402058	152494
f6c2c1bb459fe61c70e3db65053c55baff31aa00	an improved algorithm for the longest common subsequence problem	lcs;longest common subsequence;beam search;algorithms;heuristic function;bioinformatics	The Longest Common Subsequence problem seeks a longest subsequence of every member of a given set of strings. It has applications, among others, in data compression, FPGA circuit minimization, and bioinformatics. The problem is NP-hard for more than two input strings, and the existing exact solutions are impractical for large input sizes. Therefore, several approximation and (meta) heuristic algorithms have been proposed which aim at finding good, but not necessarily optimal, solutions to the problem. In this paper, we propose a new algorithm based on the constructive beam search method. We have devised a novel heuristic, inspired by the probability theory, intended for domains where the input strings are assumed to be independent. Special data structures and dynamic programming methods are developed to reduce the time complexity of the algorithm. The proposed algorithm is compared with the state-of-the-art over several standard benchmarks including random and real biological sequences. Extensive experimental results show that the proposed algorithm outperforms the state-of-the-art by giving higher quality solutions with less computation time for most of the experimental cases. & 2011 Elsevier Ltd. All rights reserved.	approximation;beam search;benchmark (computing);bioinformatics;blum axioms;circuit minimization for boolean functions;code;computation;data compression;data structure;deterministic algorithm;dynamic programming;field-programmable gate array;heuristic (computer science);hyper-heuristic;longest common subsequence problem;time complexity	Sayyed Rasoul Mousavi;Farzaneh Tabataba	2012	Computers & OR	10.1016/j.cor.2011.02.026	beam search;mathematical optimization;hunt–mcilroy algorithm;combinatorics;longest increasing subsequence;computer science;longest common subsequence problem;mathematics;heuristic function;longest alternating subsequence;algorithm	AI	15.463567847947681	22.200579480211825	152649
a1a9aaa45ce5e603ecaa5bdff9e7c2bdb7d170de	new subquadratic approximation algorithms for the girth		We consider the problem of approximating the girth, g, of an unweighted and undirected graph G “ pV,Eq with n nodes andm edges. A seminal result of Itai and Rodeh [SICOMP’78] gave an additive 1-approximation in Opnq time, and the main open question is thus how well we can do in subquadratic time. In this paper we present two main results. The first is a p1` ε,Op1qq-approximation in truly subquadratic time. Specifically, for any k ě 2 our algorithm returns a cycle of length 2 rg{2s ` 2 Q	approximation algorithm;girth (graph theory);graph (discrete mathematics);time complexity;utility functions on indivisible goods	Søren Dahlgaard;Mathias Bæk Tejs Knudsen;Morten Stöckel	2017	CoRR		mathematical optimization;combinatorics;discrete mathematics;mathematics;algorithm	Theory	20.96427442852642	22.226243216439492	153026
3170b85b8d657fbf6be2438f6cd4752b0b9b561b	efficient search for approximate nearest neighbor in high dimensional spaces	high dimensionality;search algorithm;euclidean space;approximate nearest neighbor;nearest neighbor search;random projection;data structure	We address the problem of designing data structures that allow efficient search for approximate nearest neighbors. More specifically, given a database consisting of a set of vectors in some high dimensional Euclidean space, we want to construct a space-efficient data struct,ure t,hat would allow us to search, given a query vector, for t.he closest or nearly closest vector in the database. We also address t.hii problem when distances are measured by the L1 norm, and in t.he Hamming cube. Sign%cant.ly improving and extending recent results of Kleinberg, we const,ruct data structures whose size is polynomial in the size of t,he database, and search algorithms t,hat run in time nearly linear or nearly quadratic in the dimension (depending on the case; the extra factors are polylogarit.hmic in the size of the database).	approximation algorithm;const (computer programming);data structure;hamming distance;lambda cube;polynomial;search algorithm;spaces;struct (c programming language);taxicab geometry	Eyal Kushilevitz;Rafail Ostrovsky;Yuval Rabani	1998		10.1145/276698.276877	nearest-neighbor chain algorithm;ball tree;combinatorics;best bin first;data structure;computer science;euclidean space;machine learning;pattern recognition;mathematics;cover tree;nearest neighbor search;fixed-radius near neighbors;search algorithm	Theory	12.88326187027001	26.150854615655998	153070
77c30326c0e770c66fd9c11dcd6744a2d49b8767	computing optimal repairs for functional dependencies		"""We investigate the complexity of computing an optimal repair of an inconsistent database, in the case where integrity constraints are Functional Dependencies (FDs). We focus on two types of repairs: an optimal subset repair (optimal S-repair) that is obtained by a minimum number of tuple deletions, and an optimal update repair (optimal U-repair) that is obtained by a minimum number of value (cell) updates. For computing an optimal S-repair, we present a polynomial-time algorithm that succeeds on certain sets of FDs and fails on others. We prove the following about the algorithm. When it succeeds, it can also incorporate weighted tuples and duplicate tuples. When it fails, the problem is NP-hard, and in fact, APX-complete (hence, cannot be approximated better than some constant). Thus, we establish a dichotomy in the complexity of computing an optimal S-repair. We present general analysis techniques for the complexity of computing an optimal U-repair, some based on the dichotomy for S-repairs. We also draw a connection to a past dichotomy in the complexity of finding a """"most probable database"""" that satisfies a set of FDs with a single attribute on the left hand side; the case of general FDs was left open, and we show how our dichotomy provides the missing generalization and thereby settles the open problem."""	apx;approximation algorithm;data integrity;functional dependency;np-hardness;time complexity;type class	Ester Livshits;Benny Kimelfeld;Sudeepa Roy	2018		10.1145/3196959.3196980	tuple;open problem;discrete mathematics;computer science;data integrity;functional dependency	DB	14.550070525770442	21.2665117611183	153082
8711f396d77cdc400d8d802c1dd41b2770653c60	automating the detection and simplification of junctions in road networks	graph theory;graphic methods;networks;dendrogramme;generic algorithm;road network;automated map generalization;analisis grupo;junction simplification;reseau;cartographie;detection;minimum intervention;algorithme;cartografia;cluster analysis;methode graphique;level of detail;route;roads;analyse groupe;theory;teoria;carretera;cartography;dendrograma;algorithms;spatial clustering;dendrograms;theorie;digital mapping;algoritmo	A road network is cartographically drawn in varying levels of detail depending on the resolution or scale of the output graphic. In automated generalization, the challenge is in deriving generalized forms of a road network, appropriate for the intended target scale and to achieve this with minimum intervention from the user. This paper presents a method for the detection and simplification of road junctions as part of that process. Road junctions within the network are identified using a combination of spatial clustering and graph theory. The junctions are simplified using a combination of contractions and restrictions of the graph. Consideration is given to ways in which attribute and cartometric information can be used both to modify the behaviour of the algorithm, and to influence the choice of other generalization algorithms. This algorithm is considered to be part of a growing number of generalization algorithms that can be used to derive generalized products from single detailed database. The success and limitations are discussed and future developments are proposed.	algorithm;cartography;cluster analysis;contraction mapping;graph theory;level of detail;text simplification	William A. Mackaness;Gordon A. Mackechnie	1999	GeoInformatica	10.1023/A:1009807927991	route;genetic algorithm;digital mapping;computer science;artificial intelligence;graph theory;machine learning;level of detail;mathematics;cluster analysis;dendrogram;theory;algorithm;cartography	AI	21.319818350203757	30.10892120403408	153110
4bc3aee7513a954a8f4aed8f64fd145e2b87220f	pattern matching in lempel-ziv compressed strings: fast, simple, and deterministic	lempel ziv;asymptotic optimality;pattern matching;space complexity;compression;data structure	Countless variants of the Lempel-Ziv compression are widely used in many real-life applications. This paper is concerned with a natural modification of the classical pattern matching problem inspired by the popularity of such compression methods: given an uncompressed pattern s[1. . m] and a Lempel-Ziv representation of a string t[1. . N ], does s occur in t? Farach and Thorup [6] gave a randomized O(n log 2 N n +m) time solution for this problem, where n is the size of the compressed representation of t. Building on the methods of [4] and [7], we improve their result by developing a faster and fully deterministic O(n log N n + m) time algorithm with the same space complexity. Note that for highly compressible texts, log N n might be of order n, so for such inputs the improvement is very significant. A (tiny) fragment of our method can be used to give an asymptotically optimal solution for the substring hashing problem considered by Farach and Muthukrishnan [5].	asymptotically optimal algorithm;dspace;lempel–ziv–stac;lempel–ziv–welch;pattern matching;randomized algorithm;real life;substring	Pawel Gawrychowski	2011		10.1007/978-3-642-23719-5_36	mathematical optimization;combinatorics;discrete mathematics;data structure;computer science;theoretical computer science;pattern matching;mathematics;dspace;programming language;compression;algorithm	Theory	12.455559959578043	27.2854999064744	153301
7eedde0dea5a9504cf3b11b44f7e5692fae2ea6f	a series of algorithmic results related to the iterated hairpin completion	iterated hairpin completion;68t20;algorithmique;tree;time complexity;05c05;efficient algorithm;arbol;segment tree;hairpin completion;upper bound;resolucion problema;conception algorithme;complexite temps;algorithmics;minimum distance;algoritmica;informatique theorique;complecion;palabra;dna computation;arbre;hairpin completion distance;word;dna computing;trie;complejidad tiempo;borne superieure;completion;algorithm design;calculo adn;problem solving;resolution probleme;mot;cota superior;computer theory;calcul adn;informatica teorica	In this paper we propose efficient algorithmic solutions for the computation of the hairpin completion distance between two given words, for the computation of a minimumdistance common hairpin completion ancestor of two given words (i.e., a word fromwhich we can obtain the two given words by iterated hairpin completion, such that the sum of the hairpin completion distances from this word to the two given words is minimum), and, respectively, for the computation of an arbitrary hairpin completion ancestor of two given words. In all the cases we improve the upper bounds known for time complexity of solving these problems. Then we show how the algorithms designed for these three initial problems can be modified to solve a series of related problems. © 2010 Elsevier B.V. All rights reserved.	algorithm;binary logarithm;computation;data structure;decision problem;dynamic programming;expect;iterated function;iteration;segment tree;time complexity;trie	Florin Manea	2010	Theor. Comput. Sci.	10.1016/j.tcs.2010.06.014	time complexity;segment tree;algorithm design;combinatorics;completion;computer science;trie;word;mathematics;tree;upper and lower bounds;algorithmics;dna computing;algorithm	AI	15.927100300236667	25.692854616204123	153448
c43ed43caf10d5bb8457e77553a4cfc7587f3dbd	more on average case vs approximation complexity	hardness of approximation	We consider the problem to determine the maximal number of satisfiable equations in a linear system chosen at random. We make several plausible conjectures about the average case hardness of this problem for some natural distributions on the instances, and relate them to several interesting questions in the theory of approximation algorithms and in cryptography. Namely we show that our conjectures imply the following facts: ◦ Feige’s hypothesis about the hardness of refuting a random 3CNF is true, which in turn implies inapproximability within a constant for several combinatorial problems, for which no NP-hardness of approximation is known. ◦ It is hard to approximate the NEAREST CODEWORD within factor n 1-ε . ◦ It is hard to estimate the rigidity of a matrix. More exactly, it is hard to distinguish between matrices of low rigidity and random ones. ◦ There exists a secure public-key (probabilistic) cryptosystem, based on the intractability of decoding of random binary codes. ◦ Feige’s hypothesis about the hardness of refuting a random 3CNF is true, which in turn implies inapproximability within a constant for several combinatorial problems, for which no NP-hardness of approximation is known. ◦ It is hard to approximate the NEAREST CODEWORD within factor n 1-ε . ◦ It is hard to estimate the rigidity of a matrix. More exactly, it is hard to distinguish between matrices of low rigidity and random ones. ◦ There exists a secure public-key (probabilistic) cryptosystem, based on the intractability of decoding of random binary codes.	approximation algorithm;best, worst and average case;binary code;boolean satisfiability problem;cryptosystem;hardness of approximation;linear system;maximal set;np-hardness;public-key cryptography	Michael Alekhnovich	2003	computational complexity	10.1007/s00037-011-0029-x	combinatorics;discrete mathematics;computer science;mathematics;hardness of approximation;algorithm	Theory	11.259960062632087	20.151593883673716	153629
c572e90a23f99f193332df2227ca665356a9056c	performance characterization of the tree quorum algorithm	distributed algorithms;distributed system;note;system structure;systeme reparti;logical tree structure performance characterization tree quorum algorithm tree structure fault tolerant solution distributed mutual exclusion performance characteristics refinement algorithm;fault tolerant;fault tolerant solution;sorting;availability;structure arborescente;tree quorum algorithm;tria;tree data structures;computer networks;performance characterization;exclusion mutual;mutual exclusion;sistema repartido;structure systeme;logical tree structure;fault tolerant systems;estructura arborescente;costs tree data structures centralized control distributed algorithms performance analysis algorithm design and analysis decision making fault tolerant systems availability computer networks;tree structure;triage;performance analysis;communication cost;performance characteristics;distributed mutual exclusion;centralized control;exclusion mutuelle;refinement algorithm;algoritmo optimo;algorithme optimal;optimal algorithm;tree data structures distributed algorithms;algorithm design and analysis;estructura sistema	We present a new method to determine whether a convex region contains any integer points. The method is designed for array subscript analysis in parallel programs. The general problem is whether a system of linear qualities and inequalities has an integer solution. A set of known techniques is used to transform the problem to that of finding whether a convex region contains any integer points. The main result of the paper is a set of new search procedures that identify an integer solution in a convex region, or prove that no integer solutions exist. They are based on the geometrid properties of convex regions that are not empty, but also do not contain any integer points. The results contribute to exact and efficient dependence and synchronization analysis of parallel programs. Index TermSubscript analysis, dependence testing, integer programming, parallelizing compilers, parallel program analysis, synchronization analysis.	algorithm;automatic parallelization;compiler;convex set;integer programming;program analysis	Her-Kun Chang;Shyan-Ming Yuan	1995	IEEE Trans. Parallel Distrib. Syst.	10.1109/71.388047	algorithm design;distributed algorithm;availability;fault tolerance;vantage-point tree;mutual exclusion;computer science;sorting;trie;theoretical computer science;order statistic tree;machine learning;incremental decision tree;k-ary tree;interval tree;distributed computing;fractal tree index;tree structure;search tree;tree;distributed minimum spanning tree;tree traversal;algorithm	Theory	15.684555286728221	31.777192471934523	153681
52fb71211f3c529f0a104e33541e12fd3308f3c0	asymptotic expected number of nash equilibria of two-player normal form games	two player games;statistical mechanics;nash equilibrium;normal form games;nash equilibria;random games;computational complexity;disordered systems;disordered system;normal form game	* We are indebted to Robin Pemantle for suggestions that led to the development of Theorem 2, and to Jeroen Swinkels for a very helpful conversation. We have benefitted from the comments of seminar participants at the Games 2002. McLennan gratefully acknowledges the hospitality of the Abdus Salam International Center for Theoretical Physics and the Institute for Social and Economic Research. We benefitted from the suggestions of the editor and two referees.	algorithm;algorithmic efficiency;best, worst and average case;complexity class;computational complexity theory;connected component (graph theory);np-hardness;nash equilibrium;stellar classification;time complexity	Andrew McLennan;Johannes Berg	2005	Games and Economic Behavior	10.1016/j.geb.2004.10.008	mathematical optimization;combinatorics;best response;trembling hand perfect equilibrium;economics;statistical mechanics;mathematics;correlated equilibrium;mathematical economics;nash equilibrium	ECom	18.871923887302152	18.766311471519707	153790
cc9e2321bd6b1c46fa4847b8b8af60fbd0555c03	continuous monitoring of distributed data streams over a time-based sliding window	004;algorithms distributed data streams communication efficiency frequent items	The past decade has witnessed many interesting algorithms for maintaining statistics over a data stream.#R##N##R##N#This paper initiates a theoretical study of algorithms for monitoring distributed data streams over a time-based sliding window (which contains a variable number of items and possibly out-of-order items). The concern is how to minimize the communication between individual streams and the root, while allowing the root, at any time, to be able to report the global statistics of all streams within a given error bound. #R##N##R##N#This paper presents communication-efficient algorithms for three classical statistics, namely, basic counting, frequent items and quantiles. The worst-case communication cost over a window is#R##N#$O(\frac{k}{\varepsilon} \log \frac{\varepsilon N}{k})$ bits for basic counting and $O(\frac{k}{\varepsilon} \log \frac{N}{k})$ words for the remainings, where $k$ is the number of distributed data streams, $N$ is the total number of items in the streams that arrive or expire in the window, and $\varepsilon < 1$ is the desired error bound. Matching and nearly matching lower bounds are also obtained.		Ho-Leung Chan;Tak Wah Lam;Lap-Kei Lee;Hing-Fung Ting	2010		10.4230/LIPIcs.STACS.2010.2453	computer science;theoretical computer science;data mining;mathematics;distributed computing	DB	13.0563485449122	24.43997611610316	153945
bf43e7ab8cc118b101c06af8378272a8070df962	an optimal search tree: complete binary search tree	search trees;binary search tree;data structure;binary tree	Definitions and properties of a complete binary tree and a binary search tree are well known [1]. This short paper discusses a method to construct a binary search tree which is also a complete binary tree. We call such a tree a Complete Binary Search Tree, a CBS Tree for short. Two basic advantages of a CBS tree are that it is a search tree with minimum height and that it can be placed in an array. The latter allows for implementation of the data structure without the use of pointers.	binary tree;data structure;monte carlo tree search;pointer (computer programming);search tree	Narayan Murthy	1989		10.1145/75427.1030225	random binary tree;optimal binary search tree;cartesian tree;left-child right-sibling binary tree;red–black tree;binary search tree;tree rotation;binary expression tree;data structure;binary tree;stern–brocot tree;computer science;treap;theoretical computer science;order statistic tree;range tree;machine learning;self-balancing binary search tree;data mining;k-ary tree;interval tree;search tree;ternary search tree;programming language;threaded binary tree;tree traversal;avl tree	Comp.	15.452504203666397	28.128733836025372	153955
1601626b346c0c3915ef3d51f853ef9b20804369	fast algorithms for min independent dominating set	exponential algorithms;min independent dominating set;approximation algorithms;communication conference;dominating set;approximate solution;fast algorithm;exact algorithms	We first devise a branching algorithm that computes a minimum independent dominating set with running time O∗(1.3351n) = O∗(20.417n) and polynomial space. This improves upon the best state of the art algorithms for this problem. We then study approximation of the problem by moderately exponential time algorithms and show that it can be approximated within ratio 1 + ε, for any ε > 0, in a time smaller than the one of exact computation and exponentially decreasing with ε. We also propose approximation algorithms with better running times for ratios greater than 3 in general graphs and give improvedmoderately exponential time approximation results in triangle-free andbipartite graphs. These latter results are based upon a new bound on the number of maximal independent sets of a given size in these graphs, which is a result interesting per se. © 2012 Elsevier B.V. All rights reserved.	approximation algorithm;computation;dominating set;independent set (graph theory);maxima and minima;maximal independent set;maximal set;pspace;time complexity	Nicolas Bourgeois;Federico Della Croce;Bruno Escoffier;Vangelis Th. Paschos	2013	Discrete Applied Mathematics	10.1016/j.dam.2012.01.003	mathematical optimization;combinatorics;discrete mathematics;apx;dominating set;mathematics;maximal independent set;approximation algorithm	Theory	22.094591932189584	21.457364900517433	153965
05f5e91e0ece147098766aa434864e67023496e1	computing variance for interval data is np-hard	interval data;upper bound;lower bound	When we have only interval ranges [<i>x<inf>i</inf>,</i> <i>x<inf>i</inf></i>] of sample values <i>x</i><inf>1</inf>,…, <i>x<inf>n</inf>,</i> what is the interval [<i>V, V</i>] of possible values for the variance <i>V</i> of these values? We prove that the problem of computing the upper bound <i>V</i> is NP-hard. We provide a feasible (quadratic time) algorithm for computing the lower bound <i>V</i> on the variance of interval data. We also provide a feasible algorithm that computes <i>V</i> under reasonable easily verifiable conditions.	algorithm;formal verification;np-hardness;time complexity	Scott Ferson;Lev Ginzburg;Vladik Kreinovich;Luc Longpré;Monica Aviles	2002	SIGACT News	10.1145/564585.564604	mathematical optimization;combinatorics;mathematics;upper and lower bounds;statistics	DB	14.820245697935725	19.723471263674462	153972
efee269d2970de0781294a87b284f57b318ffb7d	confluently persistent tries for efficient version control	general techniques;hash table;version control;data structure	We consider a data-structural problem motivated by version control of a hierarchical directory structure in a system like Subversion. The model is that directories and files can be moved and copied between two arbitrary versions in addition to being added or removed in an arbitrary version. Equivalently, we wish to maintain a confluently persistent trie (where internal nodes represent directories, leaves represent files, and edge labels represent path names), subject to copying a subtree between two arbitrary versions, adding a new child to an existing node, and deleting an existing subtree in an arbitrary version. Our first data structure represents an n-node degree-Δ trie with O(1) “fingers” in each version while supporting finger movement (navigation) and modifications near the fingers (including subtree copy) in O(lg Δ) time and space per operation. This data structure is essentially a locality-sensitive version of the standard practice—path copying—costing O(dlg Δ) time and space for modification of a node at depth d, which is expensive when performing many deep but nearby updates. Our second data structure supporting finger movement in O(lg Δ) time and no space, while modifications take O(lg n) time and space. This data structure is substantially faster for deep updates, i.e., unbalanced tries. Both of these data structures are functional, which is a stronger property than confluent persistence. Without this stronger property, we show how both data structures can be sped up to support movement in O(lg lg Δ), which is essentially optimal. Along the way, we present a general technique for global rebuilding of fully persistent data structures, which is nontrivial because amortization and persistence do not usually mix. In particular, this technique improves the best previous result for fully persistent arrays and obtains the first efficient fully persistent hash table.	confluence (abstract rewriting);directory (computing);functional programming;hash table;locality of reference;locality-sensitive hashing;persistence (computer science);persistent data structure;subversion;tree (data structure);trie;unbalanced circuit;version control	Erik D. Demaine;Stefan Langerman;Eric Price	2008	Algorithmica	10.1007/s00453-008-9274-z	hash table;combinatorics;data structure;computer science;revision control;theoretical computer science;database;algorithm	Theory	13.22634791483156	25.478547642342495	154061
e4350656463922d21177251458d25cef68a6763d	cutting and partitioning a graph aifter a fixed pattern (extended abstract)	graph aifter;fixed pattern;extended abstract	Without Abstract		Mihalis Yannakakis;Paris C. Kanellakis;Stavros S. Cosmadakis;Christos H. Papadimitriou	1983		10.1007/BFb0036950	graph bandwidth;strength of a graph	Theory	24.471261838465743	26.542793340251123	154147
377571a4e7153f2607619ccab6ae32534f638b03	deterministic sample sort for gpus	cluster computing;sorting;random sampling;gpu;data dependence;computational complexity;sorting algorithm;data structure;parallel algorithms	We present and evaluate GPU Bucket Sort, a parallel deterministic sample sort algorithm for many-core GPUs. Our method is considerably faster than Thrust Merge (Satish et.al., Proc. IPDPS 2009), the best comparison-based sorting algorithm for GPUs, and it is as fast as the new randomized sample sort for GPUs by Leischner et.al. (to appear in Proc. IPDPS 2010). Our deterministic sample sort has the advantage that bucket sizes are guaranteed and therefore its running time does not have the input data dependent fluctuations that can occur for randomized sample sort.	best, worst and average case;bucket sort;graphics processing unit;international parallel and distributed processing symposium;manycore processor;merge sort;randomized algorithm;samplesort;sorting algorithm;thrust;time complexity	Frank Dehne;Hamidreza Zaboli	2012	Parallel Processing Letters	10.1142/S0129626412500089	merge sort;adaptive sort;sampling;insertion sort;counting sort;parallel computing;bucket sort;selection sort;data structure;computer cluster;computer science;sorting;theoretical computer science;timsort;sorting algorithm;in-place algorithm;distributed computing;parallel algorithm;integer sorting;programming language;computational complexity theory;algorithm;stooge sort	Theory	10.674986195753208	31.398134338046496	154191
583c3b7aa66d8429eb3be1d57c9fed2fd7c71f6b	speeding-up hirschberg and hunt-szymanski lcs algorithms	dynamic programming;automata;parallel processing;divide and conquer;computer science;longest common subsequence;pattern matching	Two algorithms are presented that solve the problem of recovering the longest common subsequence of two strings instead of just its length. The first algorithm is an improvement of Hirschberg’s divide-and-conquer algorithm. The second algorithm is an improvement of Hunt-Szymanski algorithm based on an efficient computation of all dominant match points. These two algorithms use bit-vector operations and are shown to work very efficiently in practice.	8k resolution;bit-level parallelism;computation;dijkstra's algorithm;hs algorithm;hirschberg's algorithm;ibm 1400 series;longest common subsequence problem;parallel computing;pattern matching;temporary variable;test and evaluation master plan;weatherstar	Maxime Crochemore;Costas S. Iliopoulos;Yoan J. Pinzón	2001	Fundam. Inform.	10.1109/SPIRE.2001.10031	hunt–mcilroy algorithm;divide and conquer algorithms;longest increasing subsequence;computer science;theoretical computer science;longest common subsequence problem;mathematics;distributed computing;programming language;longest alternating subsequence;hirschberg's algorithm;algorithm	DB	13.04141109105692	28.33568275540297	154348
92fefd02d9d1dbe8a64f741edb0d4666c504a83c	some remarks on computing the square parts of integers	numero entero;algorithm complexity;time complexity;complejidad algoritmo;integer number;algorithme;algorithm;factorization;complexite temps;complexite algorithme;factorizacion;informatique theorique;factorisation;complejidad tiempo;computer theory;algoritmo;informatica teorica	Abstract   Let  n  be a positive integer, and suppose   n =   Π   p     i      a      i    is its prime factorization. Let   θ(n) =   Π   p     i      a      i      − 1   , so that   n  θ  (n)   is the largest squarefree factor of  n . We show that π is deterministic polynomial time Turing reducible to ϕ, the Euler function. We also show that θ is reducible to λ, the Carmichael function. We survey other recent work on computing the square part of an integer and give upper and lower bounds on the complexity of solving the problem.		Susan Landau	1988	Inf. Comput.	10.1016/0890-5401(88)90028-4	combinatorics;calculus;mathematics;factorization;algorithm;algebra	Logic	18.29199377629322	26.73948451216581	154519
33e33ad835f5cd6f4ecb2650e81b691f025c18c2	reputation games for undirected graphs	game theory;fractional optimization;nash equilibria;pagerank	J. Hopcroft and D. Sheldon originally introduced network reputation games to investigate the self-interested behavior of web authors who want to maximize their PageRank on a directed web graph by choosing their outlinks in a game theoretic manner. They give best response strategies for each player and characterize properties of web graphs which are Nash equilibria. In this paper we consider three different models for PageRank games on undirected graphs such as certain social networks. In undirected graphs players may delete links at will, but typically cannot add links without the other player’s permission. In the deletion-model players are free to delete any of their bidirectional links but may not add links. We study the problem of determining whether the given graph represents a Nash equilibrium or not in this model. We give an O(n) time algorithm for a tree, and a parametric O(2n) time algorithm for general graphs, where k is the maximum vertex degree in any biconnected component of the graph. In the request-delete-model players are free to delete any bidirectional links and add any directed links, since these additions can be done unilaterally and can be viewed as requests for bidirected links. For this model we give an O(n) time algorithm for verifying Nash equilibria in trees. Finally, in the add-delete-model we allow a node to make arbitrary deletions and the addition of a single bidirectional link if it would increase the page rank of the other player also. In this model we give a parametric algorithm for verifying Nash equilibria in general graphs and characterize so called α-insensitive Nash Equilibria. We also give a result showing a large class of graphs where there is an edge addition that causes the PageRank of both of its endpoints to increase, suggesting convergence towards complete subgraphs. ∗School of Informatics, Kyoto University and School of Computer Science, McGill University avis@cs.mcgill.ca †School of Informatics, Kyoto University 1 ar X iv :1 20 5. 66 83 v2 [ cs .D M ] 1 D ec 2 01 2	algorithm;approximation;biconnected component;bidirectional search;computer science;degree (graph theory);doom;graph (discrete mathematics);informatics;junichi iijima;nash equilibrium;pagerank;social network;theory;time complexity;usability;verification and validation;webgraph	David Avis;Kazuo Iwama;Daichi Paku	2014	Discrete Applied Mathematics	10.1016/j.dam.2013.09.022	game theory;epsilon-equilibrium;mathematical optimization;combinatorics;discrete mathematics;mathematics;distributed computing;indifference graph;algorithm;nash equilibrium	Theory	21.325441546071442	20.207151117799988	154553
525884a1fb337e31d68c7432c8c27597d1ee4228	finding your seat versus tossing a coin		In a classroom of n seats and n students, the first student sits at random, whereas every other student must sit at her/his seat, but may sit randomly if her/his seat is already taken. The probability that a student finds her/his seat is given by a simple formula. Two entertaining proofs are given.	randomness	Yared Nigussie	2014	The American Mathematical Monthly	10.4169/amer.math.monthly.121.06.545	calculus;algebra;mathematics;mathematical proof	Theory	13.352719131337354	19.04264543381853	154565
f06ee64974fc5163d65d7e84e94a9500489fe298	approximation algorithms for channel assignment with constraints	approximate algorithm;algorithm performance;algorithm analysis;generic model;approximation algorithm;telecommunication network;satisfiability;optimisation combinatoire;channel assignment problem;resultado algoritmo;red telecomunicacion;informatique theorique;reseau telecommunication;cellular network;performance algorithme;analyse algorithme;weighted graph;algorithme approximation;combinatorial optimization;article;analisis algoritmo;channel assignment;optimizacion combinatoria;computer theory;informatica teorica	Cellular networks are generally modeled as node-weighted graphs, where the nodes represent cells and the edges represent the possibility of radio interference. An algorithm for the channel assignment problem must assign as many channels as the weight indicates to every node, such that any two channels assigned to the same node satisfy the co-site constraint, and any two channels assigned to adjacent nodes satisfy the inter-site constraint. We describe several approximation algorithms for channel assignment with arbitrary co-site and inter-site constraints for odd cycles and so-called hexagon graphs that are often used to model cellular networks. The algorithms given for odd cycles are optimal for some values of constraints, and have performance ratio at most 1 + 1=(n ? 1) for all other cases, where n is the length of the cycle. Our main result is an algorithm of performance ratio at most 4=3 + for hexagon graphs with arbitrary co-site and inter-site constraints.	approximation algorithm;assignment problem;catastrophic interference;centralized computing;distributed algorithm;interference (communication)	Jeannette C. M. Janssen;Lata Narayanan	1999		10.1007/3-540-46632-0_33	cellular network;mathematical optimization;combinatorics;combinatorial optimization;computer science;mathematics;approximation algorithm;algorithm;telecommunications network;satisfiability	Metrics	20.951735065570883	28.63411783667146	154582
e5b563467bbb72dd8453127d287b268f69243f73	an unexpected meeting of four seemingly unrelated problems: graph testing, dna complex screening, superimposed codes and secure key distribution	dna;modelizacion;graph theory;criblage;teoria grafo;relation equivalence;screening;theorie graphe;optimisation combinatoire;modelisation;pooling designs;equivalence relation;criptografia;cryptography;test activite;proceedings paper;borne inferieure;depistage;descubrimiento;cernido;graph testing;cryptographie;medical screening;combinatorial optimization;screening test;modeling;relacion equivalencia;article;lower bound;prueba actividad;group testing;cota inferior;optimizacion combinatoria;key distribution;superimposed codes	This paper discusses the relation among four problems: graph testing, DNA complex screening, superimposed codes and secure key distribution. We prove a surprising equivalence relation among these four problems, and use this equivalence to improve current results on graph testing. In the rest of this paper, we give a lower bound for the minimum number of tests on DNA complex screening model.	code;key distribution;turing completeness	H. B. Chen;Dingzhu Du;Frank K. Hwang	2007	J. Comb. Optim.	10.1007/s10878-007-9067-3	combinatorics;group testing;combinatorial optimization;cryptography;graph theory;mathematics;equivalence relation;key distribution;dna;algorithm	Theory	16.561497749891377	24.699667179297897	154727
a5421572ea4c02b6b90fe0daebfb787a0bc36d53	round compression for parallel graph algorithms in strongly sublinear space		The Massive Parallel Computation (MPC) model is a theoretical framework for popular parallel and distributed platforms such as MapReduce, Hadoop, or Spark. We consider the task of computing a large matching or small vertex cover in this model when the space per machine is n for δ ∈ (0, 1), where n is the number of vertices in the input graph. A direct simulation of classic PRAM and distributed algorithms from the 1980s results in algorithms that require at least a logarithmic number of MPC rounds. We give the first algorithm that breaks this logarithmic barrier and runs in Õ( √ logn) rounds, as long as the total space is at least slightly superlinear in the number of vertices. The result is obtained by repeatedly compressing several rounds of a natural peeling algorithm to a logarithmically smaller number of MPC rounds. Each time we show that it suffices to consider a low–degree subgraph, in which local neighborhoods can be explored with exponential speedup. Our techniques are relatively simple and can also be used to accelerate the simulation of distributed algorithms for bounded–degree graphs and finding a maximal independent set in bounded–arboricity graphs.	apache hadoop;arboricity;computation;distributed algorithm;graph (discrete mathematics);independent set (graph theory);list of algorithms;mapreduce;maximal independent set;spark;simulation;speedup;time complexity;vertex (geometry);vertex cover	Krzysztof Onak	2018	CoRR		distributed algorithm;discrete mathematics;combinatorics;vertex (geometry);speedup;logarithm;vertex cover;sublinear function;algorithm;maximal independent set;mathematics;binary logarithm	Theory	20.17286475104694	22.31324338864445	154982
412afb52753d862c2ba7c0f9e25fdf767d7fd3f5	planar graphs: logical complexity and parallel isomorphism tests	canonical form;first order;computational complexity;logic in computer science;planar graph	We prove that every triconnected planar graph is definable by a first order sentence that uses at most 15 variables and has quantifier depth at most 11 log2 n+43. As a consequence, a canonic form of such graphs is computable in AC1 by the 14-dimensional Weisfeiler-Lehman algorithm. This provides another way to show that the planar graph isomorphism is solvable in AC1.	algorithm;binary logarithm;computable function;decision problem;graph isomorphism;planar graph;quantifier (logic)	Oleg Verbitsky	2007		10.1007/978-3-540-70918-3_58	1-planar graph;outerplanar graph;pathwidth;canonical form;combinatorics;discrete mathematics;polyhedral graph;universal graph;graph embedding;independent set;level structure;graph property;graph canonization;computer science;forbidden graph characterization;graph automorphism;first-order logic;graph coloring;planar straight-line graph;subgraph isomorphism problem;mathematics;graph isomorphism;graph homomorphism;computational complexity theory;butterfly graph;book embedding;algorithm;planar graph	Theory	21.46429944037597	32.06256346943433	155296
9d61a5432c919c1b124af585c18ad859a9bf3a12	computational complexity of quantum correlation: quantum discord is np-complete	physical sciences and mathematics	This Letter studies the computational complexity of quantum discord–a measure of quantum correlation beyond quantum entanglement, and proves the NP-completeness of computing quantum discord. Therefore quantum discord is commonly believed computationally intractable in the sense that the running time of any algorithm for quantum discord scales at least exponentially with the dimension of the Hilbert space, which imposes serious fundamental limitations on the future development of quantum discord. As by-products several entanglement measures, namely entanglement cost, entanglement of formation, relative entropy of entanglement, and squashed entanglement are NP-hard (or NP-complete) to compute. These complexity-theoretic results are directly applicable in common randomness distillation, quantum state merging, entanglement distillation, superdense coding, quantum teleportation, etc. In addition, I prove the NP-completeness of two relevant problems: linear optimization over classical states and determining whether there are classical states in a given convex set of states, providing strong evidence that working with classical states is generically computationally intractable.	algorithm;computational complexity theory;convex set;entanglement distillation;hilbert space;kullback–leibler divergence;linear programming;mathematical optimization;np-completeness;np-hardness;quantum correlation;quantum discord;quantum entanglement;quantum relative entropy;quantum state;quantum teleportation;randomness;squashed entanglement;state-merging;superdense coding;time complexity	Yichen Huang	2013	CoRR	10.1088/1367-2630/16/3/033027	squashed entanglement;quantum information;multipartite entanglement;quantum teleportation;amplitude damping channel;quantum network;quantum discord;quantum capacity;open quantum system;quantum convolutional code;qubit;quantum channel;quantum electrodynamics;superdense coding;classical capacity;quantum entanglement;condensed matter physics;quantum algorithm;physics;quantum mechanics	Theory	12.292284335999803	18.805560246278166	155347
31908a0307f2fef67d82ea4719622dc9c514979c	efficient random sampling of binary and unary-binary trees via holonomic equations		We present a new uniform random sampler for binary trees with n internal nodes consuming 2n + Θ(log(n)) random bits on average. This makes it quasi-optimal and out-performs the classical Remy algorithm. We also present a sampler for unary-binary trees with n nodes taking Θ(n) random bits on average. Both are the first linear-time algorithms to be optimal up to a constant.	algorithm;binary tree;bitwise operation;elementary;expect;linear algebra;monte carlo method;sampling (signal processing);time complexity;unary operation	Axel Bacher;Olivier Bodini;Alice Jacquot	2017	Theor. Comput. Sci.	10.1016/j.tcs.2017.07.009	random binary tree;mathematical optimization;combinatorics;discrete mathematics;mathematics	Theory	14.095574611522819	23.553825120855997	155505
02eec885665621e9b369bd568f96e0216e038d37	on the complexity and approximation of binary evidence in lifted inference	complexity;lifted inference	Lifted inference algorithms exploit symmetries in probabilistic models to speed up inference. They show impressive performance when calculating unconditional probabilities in relational models, but often resort to non-lifted inference when computing conditional probabilities. The reason is that conditioning on evidence breaks many of the model’s symmetries, which can preempt standard lifting techniques. Recent theoretical results show, for example, that conditioning on evidence which corresponds to binary relations is #P-hard, suggesting that no lifting is to be expected in the worst case. In this paper, we balance this negative result by identifying the Boolean rank of the evidence as a key parameter for characterizing the complexity of conditioning in lifted inference. In particular, we show that conditioning on binary evidence with bounded Boolean rank is efficient. This opens up the possibility of approximating evidence by a low-rank Boolean matrix factorization, which we investigate both theoretically and empirically.	algorithm;approximation;approximation algorithm;best, worst and average case;boolean algebra;complexity;lambda lifting;machine learning;p (complexity);sharp-p;speedup	Guy Van den Broeck;Adnan Darwiche	2013			discrete mathematics;complexity;computer science;machine learning;mathematics;statistics	ML	10.146759916956938	20.652170804644204	155603
335544b6f54a9ac8eea9356d932ecd2b77c20b31	on the time complexity of dijkstra's three-state mutual exclusion algorithm	distributed computing;self stabilization;lower bound;mutual exclusion;analysis of algorithms;time complexity	In this letter we give a lower bound on the worst-case time complexity of Dijkstrau0027s three-state mutual exclusion algorithm by specifying a concrete behavior of the algorithm. We also show that our result is more accurate than the known best bound.		Masahiro Kimoto;Tatsuhiro Tsuchiya;Tohru Kikuno	2009	IEICE Transactions		maekawa's algorithm;time complexity;self-stabilization;suurballe's algorithm;mutual exclusion;a* search algorithm;computer science;pathfinding;artificial intelligence;analysis of algorithms;calculus;yen's algorithm;worst-case complexity;mathematics;upper and lower bounds;suzuki-kasami algorithm;shortest path faster algorithm;algorithm	Visualization	18.814705857457973	27.12152722182274	155641
7dbaecb1b30fbe59b140ce34c717068e07dad02c	k-monotonicity is not testable on the hypercube		We continue the study of k-monotone Boolean functions in the property testing model, initiated by Canonne et al. (ITCS 2017). A function f : {0, 1} → {0, 1} is said to be kmonotone if it alternates between 0 and 1 at most k times on every ascending chain. Such functions represent a natural generalization of (1-)monotone functions, and have been recently studied in circuit complexity, PAC learning, and cryptography. In property testing, the fact that 1-monotonicity can be locally tested with polyn queries led to a previous conjecture that k-monotonicity can be tested with poly(n) queries. In this work we disprove the conjecture, and show that even 2-monotonicity requires an exponential in √ n number of queries. Furthermore, even the apparently easier task of distinguishing 2-monotone functions from functions that are far from being n.01-monotone also requires an exponential number of queries. Our results follow from constructions of families that are hard for a canonical tester that picks a random chain and queries all points on it. Our techniques rely on a simple property of the violation graph and on probabilistic arguments necessary to understand chain tests. ∗Purdue University. Email: elena-g@purdue.edu. Research supported in part by NSF CCF-1649515. †Purdue University. Email: akumar@purdue.edu. Research supported in part by NSF CCF-1649515 and NSF CCF-1618918. ‡Duquesne University. Email: wimmerk@duq.edu.	circuit complexity;cryptography;email;ibm notes;non-monotonic logic;probably approximately correct learning;property testing;time complexity;monotone	Elena Grigorescu;Akash Kumar;Karl Wimmer	2017	Electronic Colloquium on Computational Complexity (ECCC)		combinatorics;discrete mathematics;mathematics;algorithm	Theory	10.376686779504931	21.230379600349945	155656
496a16a523056bd89072826ebacf30fb8c911f3c	using statistical encoding to achieve tree succinctness never seen before		We propose a new succinct representation of labeled trees which represents a tree T using |T|H_k(T) number of bits (plus some order terms), where |T|H_k(T) denotes the k-th order (tree label) entropy, as defined by Ferragina at al. 2005. Our representation employs a new, simple method of partitioning the tree, which preserves both tree shape and node degrees. Previously, the only representation that used |T|H_k(T) bits was based on XBWT, a transformation that linearizes tree labels into a single string, combined with compression boosting. The proposed representation is much simpler than the one based on XBWT, which used additional linear space (bounded by 0.01n) hidden in the smaller order terms notion, as an artifact of using zeroth order entropy coder; our representation uses sublinear additional space (for reasonable values of k and size of the label alphabet {sigma}). The proposed representation can be naturally extended to a succinct data structure for trees, which uses |T|H_k(T) plus additional O(|T|k log_{sigma}/ log_{sigma} |T| + |T| log log_{sigma} |T|/ log_{sigma} |T|) bits and supports all the usual navigational queries in constant time. At the cost of increasing the query time to O(log log |T|/ log |T|) we can further reduce the space redundancy to O(|T| log log |T|/ log_{sigma} |T|) bits, assuming k u003c= log_{sigma} |T|. This is a major improvement over representation based on XBWT: even though XBWT-based representation uses |T|H_k(T) bits, the space needed for structure supporting navigational queries is much larger: (...)		Michal Ganczorz	2018	CoRR		combinatorics;boosting (machine learning);sigma;mathematics;succinctness;linear space;discrete mathematics;log-log plot;bounded function;succinct data structure;sublinear function	NLP	11.591258523885946	26.743355411023686	155746
135ee48f0e7da3700e3c86a79a4bd1bc84b76ac5	fully polynomial-time parameterized computations for graphs and matrices of low treewidth		We investigate the complexity of several fundamental polynomial-time solvable problems on graphs and on matrices, when the given instance has low treewidth; in the case of matrices, we consider the treewidth of the graph formed by non-zero entries. In each of the considered cases, the best known algorithms working on general graphs run in polynomial time; however, the exponent of the polynomial is large. Therefore, our main goal is to construct algorithms with running time of the form poly(<i>k</i>)ṡ <i>n</i> or poly(<i>k</i>)ṡ <i>n</i>log <i>n</i>, where <i>k</i> is the width of the tree decomposition given on the input. Such procedures would outperform the best known algorithms for the considered problems already for moderate values of the treewidth, like <i>O</i>(<i>n</i><sup>1/<i>c</i></sup>) for a constant <i>c</i>.  Our results include the following:  — an algorithm for computing the determinant and the rank of an <i>n</i>×<i>n</i> matrix using <i>O</i>(<i>k</i><sup>3</sup>ṡ<i>n</i>) time and arithmetic operations;  —an algorithm for solving a system of linear equations using <i>O</i>(<i>k</i><sup>3</sup>ṡ<i>n</i>) time and arithmetic operations;  —an <i>O</i>(<i>k</i><sup>3</sup>ṡ<i>n</i> log <i>n</i>)-time randomized algorithm for finding the cardinality of a maximum matching in a graph;  —an <i>O</i>(<i>k</i><sup>4</sup>ṡ<i>n</i> log<sup>2</sup> <i>n</i>)-time randomized algorithm for constructing a maximum matching in a graph;  —an <i>O</i>(<i>k</i><sup>2</sup>ṡ<i>n</i> log <i>n</i>)-time algorithm for finding a maximum vertex flow in a directed graph.  Moreover, we give an approximation algorithm for treewidth with time complexity suited to the running times as above. Namely, the algorithm, when given a graph <i>G</i> and integer <i>k</i>, runs in time <i>O</i>(<i>k</i><sup>7</sup>ṡ<i>n</i> log <i>n</i>) and either correctly reports that the treewidth of <i>G</i> is larger than <i>k</i>, or constructs a tree decomposition of <i>G</i> of width <i>O</i>(<i>k</i><sup>2</sup>).  The above results stand in contrast with the recent work of Abboud et al. (SODA 2016), which shows that the existence of algorithms with similar running times is unlikely for the problems of finding the diameter and the radius of a graph of low treewidth.	approximation algorithm;decision problem;directed graph;distance (graph theory);linear equation;matching (graph theory);polynomial;randomized algorithm;system of linear equations;time complexity;tree decomposition;treewidth	Fedor V. Fomin;Daniel Lokshtanov;Saket Saurabh;Michal Pilipczuk;Marcin Wrochna	2017		10.1145/3186898	1-planar graph;competitive analysis;online algorithm;mathematical optimization;combinatorics;discrete mathematics;computer science;mathematics;tree-depth;partial k-tree;algorithm;tree decomposition	Theory	21.576631163294113	21.98227451566528	155793
3ac3e7fba71fb52f6019527c8f51e264b31e1fc5	a unifying method for the design of algorithms canonizing combinatorial objects		We devise a unified framework for the design of canonization algorithms. Using hereditarily finite sets, we define a general notion of combinatorial objects that includes graphs, hypergraphs, relational structures, codes, permutation groups, tree decompositions, and so on. Our approach allows for a systematic transfer of the techniques that have been developed for isomorphism testing to canonization. We use it to design a canonization algorithm for general combinatorial objects. This result gives new fastest canonization algorithms with an asymptotic running time matching the best known isomorphism algorithm for the following types of objects: hypergraphs, hypergraphs of bounded color class size, permutation groups (up to permutational isomorphism) and codes that are explicitly given (up to code equivalence).	algorithm;code;fastest;time complexity;turing completeness;unified framework	Pascal Schweitzer;Daniel Wiebking	2018	CoRR		discrete mathematics;equivalence (measure theory);combinatorics;constraint graph;class size;isomorphism;permutation group;bounded function;hereditarily finite set;mathematics;algorithm;graph	Theory	18.5186118807902	23.686781042006803	155898
32c512745c3d3519eadb714ed4322b3fdced3e3d	the complexity of valued constraint satisfaction		We survey recent results on the broad family of problems that can be cast as valued constraint satisfaction problems. We discuss general methods for analysing the complexity of such problems, give examples of tractable cases, and identify general features of the complexity landscape.	approximation algorithm;cobham's thesis;combinatorial optimization;communicating sequential processes;exclusive or;linear algebra;linear programming relaxation;mathematical optimization;np-completeness;np-hardness;olami–feder–christensen model;weighted constraint satisfaction problem	Peter Jeavons;Andrei A. Krokhin;Stanislav Zivny	2014	Bulletin of the EATCS		mathematical optimization;combinatorics;discrete mathematics;mathematics;complexity of constraint satisfaction;constraint satisfaction problem	AI	16.171101098734894	19.467786254268795	156081
00e3a2db20ee94268240202ccb3672e532189c77	a nearly optimal time-space lower bound for directed st-connectivity on the nnjag model	directed graph;lower bound	Directed st-connectivity is the problem of detecting whether or not there is a path from a distinguished node s to a distinguished node t in a directed graph. We prove a timelog2(*) space lower bound of T = 2Q( IOSIOSn ‘x&for the JAG model of Cook and Rackoff [CR80] where n is the number of nodes in the input graph and S and T are the space and time used. The same bound holds for the stronger (node-named) NNJAG model of Peon Poo93a] and improves the previJ ous bound of ST = Q(n / log n) by Barnes and Edmonds [Edm93a, BE93]. It also almost matches the known upper bound of T = 2°t’0g2(”/sJ)x7nn (WZ being the number of edges in the graph) which is implicitly stated in Barnes et al. [BBRS92] and is implementable on the JAG model.	directed graph;edmonds' algorithm;sensor;st-connectivity;winzip	Jeff Edmonds;Chung Keung Poon	1995		10.1145/225058.225103	combinatorics;directed graph;mathematics;upper and lower bounds	Theory	20.83137895183041	22.97805206833189	156103
d254a722509f097a8db7625ed0b10c36f1364dda	new parallel domain extenders for uowhf	mascara;algoritmo paralelo;secuencial;parallel algorithm;sequential;longitud;length;algorithme parallele;condition suffisante;sequentiel;condicion suficiente;longueur;arbol binario;criptografia;cryptography;arbre binaire;cryptographie;hash function;masque;sufficient condition;mask;large classes;binary tree	We present two new parallel algorithms for extending the domain of a UOWHF. The first algorithm is complete binary tree based construction and has less key length expansion than Sarkar’s construction which is the previously best known complete binary tree based construction. But only disadvantage is that here we need more key length expansion than that of Shoup’s sequential algorithm. But it is not too large as in all practical situations we need just two more masks than Shoup’s. Our second algorithm is based on non-complete l-ary tree and has the same optimal key length expansion as Shoup’s which has the most efficient key length expansion known so far. Using the recent result [9], we can also prove that the key length expansion of this algorithm and Shoup’s sequential algorithm are the minimum possible for any algorithms in a large class of “natural” domain extending algorithms. But its parallelizability performance is less efficient than complete tree based constructions. However if l is getting larger, then the parallelizability of the construction is also getting near to that of complete tree based constructions. We also give a sufficient condition for valid domain extension in sequential domain extension.	binary tree;key size;parallel algorithm;sequential algorithm;universal one-way hash function	Wonil Lee;Donghoon Chang;Sangjin Lee;Soo Hak Sung;Mridul Nandi	2003		10.1007/978-3-540-40061-5_13	combinatorics;discrete mathematics;hash function;binary tree;computer science;cryptography;length;mathematics;parallel algorithm;mask;algorithm	Crypto	10.297373634606787	24.893536915258203	156165
16219edab32c17446588e20e634f7139cdf970d9	combining efficient preprocessing and incremental maxsat reasoning for maxclique in large graphs		This work is supported by NSFC Grant No. 61370184, No. 61272014, and No. 61472147. The third author was supported by Mobility Grant PRX16/00215 of the Ministerio de Educacion, Cultura y Deporte, the Generalitat de Catalunya grant AGAUR 2014-SGR-118, and the MINECO-FEDER project RASO TIN2015-71799-C2-1-P.	preprocessor	Hua Jiang;Chu Min Li;Felip Manyà	2016		10.3233/978-1-61499-672-9-939	maximum satisfiability problem;artificial intelligence;machine learning;computer science;preprocessor;graph	AI	20.059874686994604	20.486138056511958	156296
8a981e63c582ad6a55fbdeac5e3b12975bf4786a	on making directed graphs transitive	kernelization and data reduction;fixed parameter tractability;np hardness;graph modification problem;hierarchical structure detection	We present the first thorough theoretical analysis of the Transitivity Editing problem on digraphs. Herein, the task is to perform a minimum number of arc insertions or deletions in order to make a given digraph transitive. This problem has recently been identified as important for the detection of hierarchical structure in molecular characteristics of disease. Mixing up Transitivity Editing with the companion problems on undirected graphs, it has been erroneously claimed to be NP-hard. We correct this error by presenting a first proof of NPhardness, which also extends to the restricted cases where the input digraph is acyclic or has maximum degree four. Moreover, we improve previous fixed-parameter algorithms, now achieving a running time of O(2.57 +n) for an n-vertex digraph if k arc modifications are sufficient to make it transitive. In particular, providing an O(k)-vertex problem kernel, we positively answer an open question from the literature. In case of digraphs with maximum degree d, an O(k · d)-vertex problem kernel can be shown. We also demonstrate that if the input digraph contains no “diamond structure”, then one can always find an optimal solution that exclusively performs arc deletions. Most of our results (including NPhardness) can be transferred to the Transitivity Deletion problem, where only arc deletions are allowed.	algorithm;cubic function;directed acyclic graph;directed graph;graph (discrete mathematics);graph operations;kernel (operating system);kernelization;mixing (mathematics);np-hardness;polynomial;quasi-quotation;relevance;time complexity;vertex-transitive graph	Mathias Weller;Christian Komusiewicz;Rolf Niedermeier;Johannes Uhlmann	2012	J. Comput. Syst. Sci.	10.1016/j.jcss.2011.07.001	combinatorics;discrete mathematics;mathematics;algorithm	Theory	23.57038673437141	23.378260289910642	156349
1039770ac6ef68559412c5f63a79a75a3a07d2bb	finding skew partitions efficiently	graph theory;teoria grafo;descomposicion grafo;temps polynomial;asymmetry;skew partition problem;algoritmo recursivo;graphe parfait;perfect graph;cutset;asymetrie;theorie graphe;algorithme;polynomial time algorithm;algorithm;algorithme recursif;particion;polynomial time;partition;probleme partition asymetrique;asimetria;conexidad;recursive algorithm;connexite;connectedness;graph decomposition;decomposition graphe;algoritmo;tiempo polinomial;ensemble chemin isolant	A skew partition as defined by Chvatal is a partition of the vertex set of a graph into four nonempty parts A,B,C,D such that there are all possible edges between A and B and no edges between C and D. We present a polynomial-time algorithm for testing whether a graph admits a skew partition. Our algorithm solves the more general list skew partition problem, where the input contains, for each vertex, a list containing some of the labels A,B,C,D of the four parts. Our polynomial-time algorithm settles the complexity of the original partition problem proposed by Chvatal in 1985 and answers a recent question of Feder, Hell, Klein, and Motwani.		Celina M. H. de Figueiredo;Sulamita Klein;Yoshiharu Kohayakawa;Bruce A. Reed	2000	J. Algorithms	10.1006/jagm.1999.1122	partition;time complexity;mathematical optimization;combinatorics;discrete mathematics;topology;perfect graph;social connectedness;graph partition;graph theory;frequency partition of a graph;mathematics;algorithm;asymmetry;recursion	DB	21.95774077839482	27.432501808589656	156756
e4a6fbf018717d64f1f6a23b699328d3e1a82cd7	a n^5/2 algorithm for maximum matchings in bipartite graphs	finite element methods;graph theory;complexity theory;bipartite graph sparse matrices algorithms transportation tree graphs computational complexity;presses;maximum matching;inspection;system on a chip;phase locked loops;tree graphs;large scale integration;computational complexity;data structures;transportation;algorithms;computational efficiency;bipartite graph;sparse matrices;algorithm design and analysis;partitioning algorithms	The present paper shows how to construct a maximum matching in a bipartite graph with n vertices and m edges in a number of computation steps proportional to (m+n) n.	algorithm;matching (graph theory)	John E. Hopcroft;Richard M. Karp	1971		10.1109/SWAT.1971.1	system on a chip;edge-transitive graph;algorithm design;transport;complete bipartite graph;factor-critical graph;combinatorics;discrete mathematics;phase-locked loop;inspection;data structure;bipartite graph;sparse matrix;computer science;graph theory;3-dimensional matching;theoretical computer science;hopcroft–karp algorithm;finite element method;mathematics;blossom algorithm;computational complexity theory;biregular graph;tree;algorithm;matching	Theory	20.535577361385766	31.201368918472667	156770
d48a5860a53c8284bdbd2747948aac50bd610ee4	finding frequent patterns in parallel point processes		We consider the task of finding frequent patterns in parallel point processes—also known as finding frequent parallel episodes in event sequences. This task can be seen as a generalization of frequent item set mining: the co-occurrence of items (or events) in transactions is replaced by their (imprecise) co-occurrence on a continuous (time) scale, meaning that they occur in a limited (time) span from each other. We define the support of an item set in this setting based on a maximum independent set approach allowing for efficient computation. Furthermore, we show how the enumeration and test of candidate sets can be made efficient by properly reducing the event sequences and exploiting perfect extension pruning. Finally, we study how the resulting frequent item sets/event sets can be filtered for closed and maximal sets.	64-bit computing;algorithm;command-line interface;compiler;computation;decision problem;experiment;filter (signal processing);independent set (graph theory);linux;list of intel core i5 microprocessors;maximal set;np-completeness;operating system;python;random-access memory;rendering (computer graphics);ubuntu;monotone	Christian Borgelt;David Picado-Muiño	2013		10.1007/978-3-642-41398-8_11	discrete mathematics;machine learning;data mining;mathematics	ML	13.29802604652175	28.180146131280978	156882
0ad8b5ade75c309f309f872d56b3dd6a33d57c7b	drawing graphs in euler diagrams	esquema euler;multicriteria optimization;multicriteria analysis;hipergrafico;optimisation;metodo diagramatico;optimizacion;graph method;euler scheme;on line;equation euler;en linea;localization;arbre maximal;schema euler;qa 76 software;localizacion;logical programming;embedded graph;metodo grafo;methode graphe;resolucion problema;computer programming;methode diagrammatique;hill climbing method;diagram method;localisation;arbol maximo;programmation logique;ecuacion euler;data visualization;metodo escalada;conexidad;visualisation donnee;en ligne;optimization;hill climbing;analisis multicriterio;spanning tree;hypergraph;methode escalade;connexite;analyse multicritere;connectedness;connected component;programacion logica;problem solving;resolution probleme;hypergraphe;g000 computing and mathematical sciences;euler equation	We describe a method for drawing graph-enhanced Euler diagrams using a three stage method. The first stage is to lay out the underlying Euler diagram using a multicriteria optimizing system. The second stage is to find suitable locations for nodes in the zones of the Euler diagram using a force based method. The third stage is to minimize edge crossings and total edge length by swapping the location of nodes that are in the same zone with a multicriteria hill climbing method. We show a working version of the software that draws spider diagrams. Spider diagrams represent logical expressions by superimposing graphs upon an Euler diagram. This application requires an extra step in the drawing process because the embedded graphs only convey information about the connectedness of nodes and so a spanning tree must be chosen for each maximally connected component. Similar notations to Euler diagrams enhanced with graphs are common in many applications and our method is generalizable to drawing Hypergraphs represented in the subset standard, or to drawing Higraphs where edges are restricted to connecting with only atomic nodes.	connected component (graph theory);embedded system;euler;euler–lagrange equation;file spanning;hill climbing;paging;spanning tree;state diagram	Paul Mutton;Peter Rodgers;Jean Flower	2004		10.1007/978-3-540-25931-2_9	combinatorics;discrete mathematics;connected component;spanning tree;spider diagram;computer science;artificial intelligence;hill climbing;computer programming;mathematics;geometry;data visualization;algorithm	AI	21.998922698319916	29.740754220063888	156915
ff9315e415e85dd1765774647c134d8ab44c4698	on improved time bounds for permutation graph problems	feedback vertex set;dynamic program;dominating set;permutation graph;polynomial time;structural properties;shortest path problem	For many problems on permutation graphs, polynomial time bounds were found by using different approaches as e.g. dynamic programming, structural properties of the intersection model, the reformulation as a shortest-path problem on suitable derived graphs and a geometric representation as points in the plane. Here we outline these approaches and apply them to two problems: minimum weight independent dominating set and maximum weight cycle-free subgraph (minimum weight feedback vertex set).		Andreas Brandstädt	1992		10.1007/3-540-56402-0_30	time complexity;mathematical optimization;combinatorics;discrete mathematics;feedback arc set;independent set;feedback vertex set;dominating set;graph property;regular graph;simplex graph;edge cover;graph automorphism;permutation graph;cycle graph;cubic graph;vertex;mathematics;voltage graph;distance-hereditary graph;circle graph;windmill graph;shortest path problem;cyclic permutation;butterfly graph;circulant graph	Theory	23.612782869287695	24.71785288624292	157083
43a63228b9e0e0a37364cb532e23d862e7b7bc42	complexity of total outer-connected domination problem in graphs	domination;apx complete;np complete;total outer connected domination;graph algorithms;chordal graph	A set D ? V is called a total dominating set of a graph G = ( V , E ) , if for all v ? V , | N G ( v ) ? D | ? 1 . A total dominating set D of G = ( V , E ) is called a total outer-connected dominating set if G V ? D is connected. The Minimum Total Outer-connected Domination problem for a graph G = ( V , E ) is to find a total outer-connected dominating set of minimum cardinality. The Total Outer-connected Domination Decision problem, the decision version of the Minimum Total Outer-connected Domination problem, is known to be NP-complete for general graphs. In this paper, we strengthen this NP-completeness result by showing that the Total Outer-connected Domination Decision problem remains NP-complete for chordal graphs, split graphs, and doubly chordal graphs. We prove that the Total Outer-connected Domination Decision problem can be solved in linear time for bounded tree-width graphs. We, then, propose a linear time algorithm for computing the total outer-connected domination number, the cardinality of a minimum total outer-connected dominating set, of a tree. We prove that the Minimum Total Outer-connected Domination problem cannot be approximated within a factor of ( 1 - e ) ln | V | for any e 0 , unless NP ? DTIME ( | V | O ( log log | V | ) ) . Finally, we show that the Minimum Total Outer-connected Domination problem is APX-complete for bounded degree graphs.	dominating set	Bhawani Sankar Panda;Arti Pandey	2016	Discrete Applied Mathematics	10.1016/j.dam.2015.05.009	combinatorics;discrete mathematics;np-complete;topology;domination analysis;dominating set;mathematics;maximal independent set;chordal graph;algorithm	Theory	23.801901881628467	20.769747999448924	157389
fb0731a4da8340926075a8181af7d79954e15a33	suffix array for large alphabet	word based compression;word based compression suffix array sorting burrows wheeler transform text compression;mathematics;suffix array sorting;data compression;block compression;application software;sorting;text analysis;text compression;testing;xml files compression;sorting data compression phased arrays xml testing mathematics physics application software encoding arithmetic;xml data compression text analysis transforms;alphabet coding;suffix array;physics;word based bwt;syllable based bwt;xbw software project;linear time;transforms;xml;textual files;arithmetic;compression ratio;burrows wheeler transform;encoding;textual files burrows wheeler transform block compression suffix array alphabet coding xbw software project xml files compression word based bwt syllable based bwt;phased arrays	Burrows-Wheeler Transform (BWT) is used as the main part in block compression which has a good balance of speed and compression ratio. Suffix arrays are used in the coding phase of BWT and we focus on creating them for an alphabet larger than 256 symbols. The motivation for this work has been software project XBW-an application for compression of large XML files using word- and syllable-based BWT. The role of BWT is to reorder input before applying other algorithms. We describe and implement three families of algorithms for encoding. Finally we present algorithm by Karkkainen and Sanders for constructing suffix arrays in linear time.	algorithm;burrows–wheeler transform;data compression;software project management;suffix array;syllable;time complexity;xml	Radovan Sesták;Jan Lansky;Michal Zemlicka	2008	Data Compression Conference (dcc 2008)	10.1109/DCC.2008.22	data compression;time complexity;text mining;application software;xml;speech recognition;computer science;sorting;theoretical computer science;burrows–wheeler transform;compression ratio;software testing;compressed suffix array;algorithm;encoding	HPC	11.580908709134683	28.181110173877325	157440
a93af9c043b62ae40441313c8ba9e55e93ea8bc9	on the expected longest length probe sequence for hashing with separate chaining	symbolic computation;separate chaining;computer and information science;table search;analysis of algorithm;analysis of algorithms;worst case search time;hashing;hash table;data och informationsvetenskap;analytic solution	Hashing is a widely used technique for data organization. Hash tables enable a fast search of the stored data and are used in a variety of applications ranging from software to network equipment and computer hardware. One of the main issues associated with hashing are collisions that cause an increase in the search time. A number of alternatives have been proposed to deal with collisions. One of them is separate chaining, in which for each hash value an independent list of the elements that have that value is stored. In this scenario, the worst case search time is given by the maximum length of that list across all hash values. This worst case is often referred to as Longest Length Probe Sequence (llps) in the literature. Approximations for the expected longest length probe sequence when the hash table is large have been proposed and an exact analytical solution has also been presented in terms of a set of recurring equations. In this paper, a novel analytical formulation of the expected longest length probe sequence is introduced. The new formulation does not require a recursive computation and can be easily implemented in a symbolic computation tool.	approximation;hash function;hash table;recursion;symbolic computation	Pedro Reviriego;Lars Holst;Juan Antonio Maestro	2011	J. Discrete Algorithms	10.1016/j.jda.2011.04.005	feature hashing;hopscotch hashing;closed-form expression;mathematical optimization;hash table;double hashing;combinatorics;symbolic computation;hash function;linear hashing;perfect hash function;extendible hashing;dynamic perfect hashing;primary clustering;open addressing;computer science;consistent hashing;analysis of algorithms;theoretical computer science;universal hashing;mathematics;k-independent hashing;distributed computing;cuckoo hashing;rolling hash;coalesced hashing;linear probing;programming language;suha;2-choice hashing;algorithm	Vision	10.35073929980624	29.54814346141569	157469
b8472209323244b6dbefd6e125f5a6497cf91122	optical grooming with grooming ratio eight	trafic;block design;optical network;multiplexage longueur onde;optimisation;exceptions;descomposicion grafo;combinatorics;subgrafo;optimizacion;combinatorial designs;05bxx;traffic grooming;ring network;combinatoria;exception;combinatoire;group divisible designs;trafico;conception;red fibra optica;plan bloc;optical networks;plan bloque;red anillo;sous graphe;informatique theorique;reseau anneau;68r10;reseau fibre optique;traffic;combinatorial design;coste;diseno;design;block designs;optical fiber network;optimization;subgraph;grafo completo;complete graph;graphe complet;multiplaje longitud onda;graph decomposition;computer theory;decomposition graphe;wavelength division multiplexing;cout;wavelength division multiplex;informatica teorica	Grooming uniform all-to-all traffic in optical ring networks with grooming ratio C requires the determination of graph decompositions of the complete graph into subgraphs each having at most C edges. The drop cost of such a grooming is the total number of vertices of nonzero degree in these subgraphs, and the grooming is optimal when the drop cost is minimum. The minimum possible drop cost is determined for grooming ratio 8, and this cost is shown to be realized with six exceptions, and 37 possible exceptions, the largest being 105.		Charles J. Colbourn;Gennian Ge;Alan C. H. Ling	2009	Discrete Applied Mathematics	10.1016/j.dam.2009.03.019	combinatorial design;combinatorics;telecommunications;mathematics;algorithm	ECom	22.986079135761813	30.035362947202117	157588
b3aaa04b61389f684ad5622a79a4ce866daedbd1	the external network problem with edge- or arc-connectivity requirements	distributed system;metodo polinomial;reseau communication;systeme reparti;digraph;qa mathematics;digrafo;combinatorial problem;probleme combinatoire;problema combinatorio;sistema repartido;community networks;polynomial method;polynomial algorithm;methode polynomiale;red de comunicacion;communication network;disjoint paths;digraphe	The connectivity of a communications network can often be enhanced if the nodes are able, at some expense, to form links using an external network. In this paper, we consider the problem of how to obtain a prescribed level of connectivity with a minimum number of nodes connecting to the external network.#R##N##R##N#Let D = (V,A) be a digraph. A subset X of vertices in V may be chosen, the so-called external vertices. An internal path is a normal directed path in D; an external path is a pair of internal paths p1=v1 ⋯ vs, p2=w1 ⋯ wt in D such that vs and w1 are external vertices ( the idea is that v1 can contact wt along this path using an external link from vt to w1 ). Then (D,X) is externally-k-arc-strong if for each pair of vertices u and v in V, there are k arc-disjoint paths ( which may be internal or external ) from u to v.#R##N##R##N#We present polynomial algorithms that, given a digraph D and positive integer k, will find a set of external vertices X of minimum size subject to the requirement that (D,X) must be externally-k-arc-strong.	requirement	Jan van den Heuvel;Matthew Richard Johnson	2004		10.1007/11527954_11	combinatorics;discrete mathematics;directed graph;computer science;mathematics;distributed computing;distance;algorithm;telecommunications network	Theory	22.23632749296147	30.228204645387084	157764
8340e7cfd59a7767d3c39a44c240b28008debea0	uniquely restricted matchings	intervalo tiempo;graph theory;acoplamiento grafo;adaptacion;graphe biparti;teoria grafo;algorithm complexity;subgrafo;algorithm analysis;grafo bipartido;probleme np complet;complejidad algoritmo;temps lineaire;perfect graph;tiempo lineal;time interval;theorie graphe;graph matching;analysis of algorithm;couplage graphe;complexite algorithme;sous graphe;adaptation;linear time;borne inferieure;problema np completo;analyse algorithme;subgraph;bipartite graph;analisis algoritmo;lower bound;np complete problem;cota inferior;intervalle temps;perfect match	A matching in a graph is a set of edges no two of which share a common vertex. In this paper we introduce a new, specialized type of matching which we call uniquely restricted matchings, originally motivated by the problem of determining a lower bound on the rank of a matrix having a specified zero/ non-zero pattern. A uniquely restricted matching is defined to be a matching M whose saturated vertices induce a subgraph which has only one perfect matching, namely M itself. We introduce the two problems of recognizing a uniquely restricted matching and of finding a maximum uniquely restricted matching in a given graph, and present algorithms and complexity results for certain special classes of graphs. We demonstrate that testing whether a given matching M is uniquely restricted can be done in O(|M||E|) time for an arbitrary graph G=(V,E) and in linear time for cacti, interval graphs, bipartite graphs, split graphs and threshold graphs. The maximum uniquely restricted matching problem is shown to be NP-complete for bipartite graphs, split graphs, and hence for chordal graphs and comparability graphs, but can be solved in linear time for threshold graphs, proper interval graphs, cacti and block graphs.	algorithm;matching (graph theory);np-completeness;regular expression;time complexity	Martin Charles Golumbic;Tirza Hirst;Moshe Lewenstein	2001	Algorithmica	10.1007/s00453-001-0004-z	strong perfect graph theorem;1-planar graph;apollonian network;time complexity;pathwidth;split graph;combinatorics;discrete mathematics;cograph;np-complete;interval graph;bipartite graph;longest path problem;dense graph;perfect graph;graph theory;3-dimensional matching;hopcroft–karp algorithm;metric dimension;mathematics;geometry;blossom algorithm;maximal independent set;modular decomposition;upper and lower bounds;partial k-tree;chordal graph;indifference graph;algorithm;matching;adaptation	Theory	22.394067801520322	26.585527314000917	157776
64172ba82d84d7b5531481c88e42d61671795cc1	lower bounds on the broadcasting and gossiping time of restricted protocols	protocols;mathematiques discretes;general and specific networks;matematicas discretas;lower bounding technique;gossiping;discrete mathematics;radiodifusion;protocole;informatique theorique;68r10;borne inferieure;reseau general et specifique;68m12;broadcasting;68r05;68m10;radiodiffusion;lower bound;echange total;cota inferior;computer theory;informatica teorica	In this paper we extend the technique provided in 6] to allow the determination of lower bounds on the broadcasting and gossiping time required by the so-called restrictedd protocols. Informally, a protocol is (i; o)-restricted at a given processor if every outgoing activation of an arc depends on at most i previous incoming activations and any incoming activation innuences at most o successive outgoing activations. Examples of restricted protocols are those running on bounded degree networks or systolic. We thus derive improved lower bounds on the broadcasting time in several well-known networks, such as Butterry, de Bruijn and Kautz graphs. Moreover, we derive the rst general lower bound on the gossiping time of d-bounded degree networks in the directed and half-duplex cases. Improved lower bounds on gossiping are also obtained for Butterry, de Bruijn and Kautz graphs. Finally, as a corollary we obtain the same lower bounds on s-systolic protocols obtained in 6]. All the results are also extended to other communication models, such as c-port and/or postal and optical. Bornes inffrieures du temps de diiusion de protocoles contraints RRsumm : Nous ggnnralisons les techniques introduites dans 6] aan de minorer le temps de diiusion et d''change total de protocoles satisfaisant certaines contraintes. Intuitivement un protocole est (i; o) contraint si un message mis ddpend uniquement des i derniers messages reeus et si un message reeu innuence au plus les o envois suivants. Cette contrainte est utile car elle apparaat la fois dans le cas des protocoles systoliques, et des protocoles fonctionnant dans des rrseaux de degrr bornn. Les rrsultats sont appliquus aan de ddterminer de nouvelles bornes inffrieures pour la diiusion et llechange total dans des rrseaux classiquess. Nous retrouvons aussi les rrsultats de 6] et prouvons la premiire borne inffrieure non triviale sur le temps de lllchange total dans les rrseaux de degrr bornn. Ennn nous montrons comment la technique introduite peut s'appliquer des moddles de communication divers (moddles postal ou optique)	de bruijn graph;duplex (telecommunications);estdomains;large eddy simulation;linear algebra;postal;reactions to the november 2015 paris attacks	Michele Flammini;Stéphane Pérennes	2004	SIAM J. Discrete Math.	10.1137/S0895480101386450	gossip;communications protocol;combinatorics;discrete mathematics;mathematics;upper and lower bounds;broadcasting;algorithm	Theory	17.977938458712526	32.00927248739239	157834
4ad37b765053375e623593a8d4acbf130397ec0d	kernelization using structural parameters on sparse graph classes		We prove that graph problems with nite integer index have linear kernels on graphs of bounded expansion when parameterized by the size of a modulator to constant-treedepth graphs. For nowhere dense graph classes, our result yields almost-linear kernels. We also argue that such a linear kernelization result with a weaker parameter would fail to include some of the problems covered by our framework. We only require the problems to have FII on graphs of constant treedepth. This allows to prove linear kernels also for problems such as LongestPath/Cycle, Exact-s, t-Path, Treewidth, and Pathwidth, which do not have FII on general graphs.		Jakub Gajarský;Petr Hlinený;Jan Obdrzálek;Sebastian Ordyniak;Felix Reidl;Peter Rossmanith;Fernando Sánchez Villaamil;Somnath Sikdar	2013		10.1007/978-3-642-40450-4_45	combinatorics;machine learning;pattern recognition;kernelization	Theory	22.58370211938045	24.246854841636008	157914
661af9b8ff975eabaafd20ee251aad9833edc66a	capacitated domination		We define an r-capacitated dominating set of a graph G = (V,E) as a set {v1, . . . , vk} ⊆ V such that there is a partition (V1, . . . , Vk) of V where for all i, vi ∈ Vi, vi is adjacent to all of Vi − {vi}, and |Vi| ≤ r + 1. kr(G) is the minimum cardinality of an r-capacitated dominating set. We show properties of kr, especially as regards the trivial lower bound |V |/(r+1). We calculate the value of the parameter in several graph families, and show that it is related to codes and polyominoes. The parameter is NP-complete in general to compute, but a greedy approach provides a linear-time algorithm for trees.	code;dominating set;greedy algorithm;np-completeness;time complexity	Wayne Goddard;Stephen T. Hedetniemi;James L. Huff;Alice A. McRae	2010	Ars Comb.			Theory	24.186516680992078	21.204412280743618	157982
9223267be81c2d8ae05bab9d04e1cd3ced534450	on digital search trees - a simple method for constructing balanced binary trees	digital search tree;binary search tree;experimental evaluation;data structure;binary tree	This paper presents digital search trees, a binary tree data structure that can produce well-balanced trees in the majority of cases. Digital search tree algorithms are reviewed, and a novel algorithm for building sorted trees is introduced. It was found that digital search trees are simple to implement because their code is similar to the code for ordinary binary search trees. Experimental evaluation was performed and the results are presented. It was found that digital search trees, in addition to being conceptually simpler, often outperform other popular balanced trees such as AVL or red-black trees. It was found that good performance of digital search trees is due to better exploitation of cache locality in modern computers.	avl tree;algorithm;binary tree;cpu cache;computer;data structure;experiment;locality of reference;red–black tree;self-balancing binary search tree;string (computer science);tab stop;tree (data structure)	Franjo Plavec;Zvonko G. Vranesic;Stephen Dean Brown	2007			random binary tree;optimal binary search tree;cartesian tree;red–black tree;binary search tree;tree rotation;binary expression tree;data structure;binary tree;computer science;treap;theoretical computer science;order statistic tree;scapegoat tree;machine learning;self-balancing binary search tree;k-d tree;k-ary tree;interval tree;search tree;weight-balanced tree;ternary search tree;programming language;threaded binary tree;tree traversal	Graphics	12.70790469285175	28.483752435188254	157984
190711ee2d07b1f3cd9c462c42814c19ea60e409	longest common extension		Given a word w of length n and i,j[n], the longest common extension is the longest substring starting at both i and j. In this note we estimate the average length of the longest common extension over all words w and all pairs (i,j), as well as the typical maximum length of the longest common extension.We also consider a variant of this problem, due to Blanchet-Sadri and Lazarow, in which the word is allowed to contain holes, which are special symbols functioning as jokers, i.e.are considered to be equal to any character. In particular, we estimate the average longest common extension over all words w with a small number of holes, extending a result by Blanchet-Sadri, Harred and Lazarow, and prove a similar result for words with holes appearing randomly.		Béla Bollobás;Shoham Letzter	2018	Eur. J. Comb.	10.1016/j.ejc.2017.07.019	combinatorics;mathematics;small number;arithmetic;substring;longest repeated substring problem	NLP	13.284871367635825	26.492863777411685	158077
8318b53a52a653caa92f404ea5d6fc7bb603b089	storing text using integer codes	word list;encoding scheme;integer code;storage space;storing text;two-byte integar code;performance statistic;facilitates word manipulation whilst;respective position;linear order	Traditionally, text is stored on computers as a stremn of characters. The goal of this research is to store text in a form that facilitates word manipulation whilst reducing storage space. A word list with syntactic linear ordering is stored and words in a text are given two-byte integer codes that point to their respective positions in this list. The implementation of the encoding scheme is described and the perfomnance statistics of ~lis encoding scheme is presented.	byte;computer;line code;while	Raja Noor Ainon	1986			computer science;theoretical computer science;total order;algorithm	Theory	10.161666972351235	28.220643872499394	158172
1b5d43d1c1c3ee986e20645f4d63e2bc637da483	storing the web in memory: space efficient language models with constant time retrieval	variable length code;hash function;language model	We present three novel methods of compactly storing very large n-gram language models. These methods use substantially less space than all known approaches and allow n-gram probabilities or counts to be retrieved in constant time, at speeds comparable to modern language modeling toolkits. Our basic approach generates an explicit minimal perfect hash function, that maps all n-grams in a model to distinct integers to enable storage of associated values. Extensions of this approach exploit distributional characteristics of n-gram data to reduce storage costs, including variable length coding of values and the use of tiered structures that partition the data for more efficient storage. We apply our approach to storing the full Google Web1T n-gram set and all 1-to-5 grams of the Gigaword newswire corpus. For the 1.5 billion n-grams of Gigaword, for example, we can store full count information at a cost of 1.66 bytes per n-gram (around 30% of the cost when using the current stateof-the-art approach), or quantized counts for 1.41 bytes per n-gram. For applications that are tolerant of a certain class of relatively innocuous errors (where unseen n-grams may be accepted as rare n-grams), we can reduce the latter cost to below 1 byte per n-gram.	byte;grams;language model;list of toolkits;lookup table;map;n-gram;perfect hash function;time complexity;variable-length code	David Guthrie;Mark Hepple	2010			hash function;variable-length code;computer science;theoretical computer science;machine learning;database;algorithm;language model	Web+IR	11.481005142786048	26.984941542003714	158203
10e8879767baa66f121dbbbdbbcceb1447620c27	counting and verifying maximal palindromes	linear time algorithm;close relationships	A palindrome is a symmetric string that reads the same forward and backward. Let Pals(w) denote the set of maximal palindromes of a string w in which each palindrome is represented by a pair (c, r), where c is the center and r is the radius of the palindrome. We say that two strings w and z are pal-distinct if Pals(w) 6= Pals(z). Firstly, we describe the number of pal-distinct strings, and show that we can enumerate all pal-distinct strings in time linear in the output size, for alphabets of size at most 3. These results follow from a close relationship between maximal palindromes and parameterized matching. Secondly, we present a linear time algorithm which finds a string w such that Pals(w) is identical to a given set of maximal palindromes.	algorithm;enumerated type;lexicographical order;maximal set;pattern matching;time complexity	I Tomohiro;Shunsuke Inenaga;Hideo Bannai;Masayuki Takeda	2010		10.1007/978-3-642-16321-0_13	combinatorics;discrete mathematics;computer science;mathematics;algorithm	Theory	13.755730818959927	26.56393865050711	158395
2acd6522cbb31372de572da784784e2b25781757	approximating good simultaneous diophantine approximations is almost np-hard	approximate algorithm;interactive proof systems;diophantine approximation;probabilistically checkable proofs;computational complexity	"""Given a real vector =(1; : : : ; d) and a real number """" > 0 a good Diophantine approximation to is a number Q such that kQQ mod Zk1 """", where k k1 denotes thè1-norm kxk1 := max 1id jxij for x = (x1; : : : ; x d). Lagarias 12] proved the NP-completeness of the corresponding decision problem, i.e., given a vector 2 Q d , a rational number """" > 0 and a number N 2 N+, decide whether there exists a number Q with 1 Q N and kQQ mod Zk1 """". We prove that, unless NP DTIME(n poly(log n)), there exists no polynomial time algorithm which computes on inputs 2 Q d and N 2 N + a number Q with 1 Q 2 log 0:5? d N and kQ mod Zk1 2 log 0:5? d min 1qN jjq mod Zk1; where is an arbitrary small positive constant. To put it in other words, it is almost NP{hard to approximate a minimum good Diophantine approximation to in polynomial-time within a factor 2 log 0:5? d for an arbitrary small positive constant. We also investigate the nonhomogeneous variant of the good Diophantine approximation problem, i.e., given vectors ; 2 Q d , a rational number """" > 0 and a number N 2 N + , decide whether there exists a number Q with 1 Q N and kQQ ? mod Zk1 """". This problem is particularly interesting since nding good nonhomo-geneous Diophantine approximations enables us to factor integers and compute discrete logarithms (see Schnorr 17]). We prove that the problem Good Nonhomogeneous Diophantine Approximation is NP-complete and even approximating it in polynomial-time within a factor 2 log 1? d for an arbitrary small positive constant is almost NP-hard. Our results follow from recent work in the theory of probabilistically checkable proofs 4] and 2-prover 1-round interactive proof-systems 7, 14]."""	approximation algorithm;decision problem;discrete logarithm;interactive proof system;jeffrey lagarias;maxima and minima;np-completeness;np-hardness;p (complexity);polynomial;probabilistically checkable proof;time complexity	Carsten Rössner;Jean-Pierre Seifert	1996		10.1007/3-540-61550-4_173	mathematical optimization;combinatorics;discrete mathematics;diophantine approximation;diophantine set;probabilistically checkable proof;pcp theorem;diophantine equation;mathematics;nexptime;computational complexity theory;algorithm;cornacchia's algorithm	Theory	14.608502482999706	20.350049852836502	158435
bec9f897d701a8326ccdfc97064f4ce2c07a732f	smoothsort, an alternative for sorting in situ		Like heapsort – which inspired it – smoothsort is an algorithm for sorting in situ. It is of order N⋅log N in the worst case, but of order N in the best case, with a smooth transition between the two. (Hence its name.)	algorithm;best, worst and average case;heapsort;smoothsort;sorting	Edsger W. Dijkstra	1982	Sci. Comput. Program.	10.1016/0167-6423(82)90016-8	smoothsort;mathematical optimization;algorithm	Theory	13.99671513798779	24.7453292289133	158467
b7f66ec5bf98ae97886903609883526e9eba3da6	generalised fine and wilf's theorem for arbitrary number of periods	lettre alphabet;greatest common divisor;theoreme fine et wilf s;period;combinatorics on words;fine and wilf s theorem;periodo;algorithme;algorithm;periode;informatique theorique;periods;letra alfabeto;word combinatorics;letter;combinatoire mot;computer theory;algoritmo;informatica teorica	The well known Fine and Wilf's theorem for words states that if a word has two periods and its length is at least as long as the sum of the two periods minus their greatest common divisor, then the word also has as period the greatest common divisor. We generalise this result for an arbitrary number of periods. Our bound is strictly better in some cases than previous generalisations. Moreover, we prove it optimal. We show also that any extrenal word is unique up to letter renaming and give an algorithm to compute both the bound and a corresponding extremal word.		Sorin Constantinescu;Lucian Ilie	2005	Theor. Comput. Sci.	10.1016/j.tcs.2005.01.007	combinatorics;period;mathematics;algorithm;algebra	ECom	10.529090996238082	25.832864409846742	158565
0082819cf597c6c7e606682fada9b9c55031b307	dynamic algorithms for the dyck languages	word problem;free group;monte carlo;algorithms and data structure;lower bound	We study dynamic membership problems for the Dyck languages, the class of strings of properly balanced parentheses. We also study the Dynamic Word problem for the free group. We present deterministic algorithms and data structures which maintain a string under replacements of symbols, insertions, and deletions of symbols, and language membership queries. Updates and queries are handled in polylogarithmic time. We also give both Las Vegasand Monte Carlo-type randomised algorithms to achieve better running times, and present lower bounds on the complexity for variants of the problems. ∗Basic Research in Computer Science, Centre of the Danish National Research Foundation. This work was partially supported by the ESPRIT II Basic Research Actions Program of the EC under contract no.  (project ALCOM II). Gudmund Frandsen was partially supported by CCI-Europe. Peter Bro Miltersen was partially supported by a grant from the Danish Natural Science Research Council, part of his research was done done at the Department of Computer Science, University of Toronto.	algorithm;bro;computer science;data structure;dyck language;dynamic problem (algorithms);monte carlo method;polylogarithmic function;symbol (formal);time complexity	Gudmund Skovbjerg Frandsen;Thore Husfeldt;Peter Bro Miltersen;Theis Rauhe;Søren Skyum	1995		10.1007/3-540-60220-8_54	word problem;combinatorics;computer science;theoretical computer science;free group;mathematics;upper and lower bounds;algorithm;monte carlo method	Theory	12.877957880844017	22.409884192300147	158618
bb2558b0f519ea921c4aff1197555153091f7177	the history heuristic and alpha-beta search enhancements in practice	jeu;algorithme αβ;arbre recherche;transposition tables;research tree;history minimax techniques iterative algorithms decision trees councils testing;play;interior nodes;metodo minimax;heuristic method;minimax method;search strategy;metodo heuristico;trees mathematics;indexing terms;algorithme;algorithm;minimax techniques;alpha beta search enhancements;arbol investigacion;methode minimax;strategie recherche;game trees history heuristic alpha beta search enhancements minimax trees interior nodes transposition tables;game trees;search problems;methode heuristique;history heuristic;juego;trees mathematics minimax techniques search problems;minimax trees;estrategia investigacion;algoritmo	Many enhancements to the alpha-beta algorithm have been proposed to help reduce the size of minimax trees. A recent enhancement, the history heuristic, is described that improves the order in which branches are considered at interior nodes. A comprehensive set of experiments is reported which tries all combinations of enhancements to determine which one yields the best performance. Previous work on assessing their performance has concentrated on the benefits of individual enhancements or a few combinations. However, each enhancement should not be taken in isolation; one would like to find the combination that provides the greatest reduction in tree size. Results indicate that the history heuristic and transposition tables significantly out-perform other alpha-beta enhancements in application generated game trees. For trees up to depth 8, when taken together, they account for over 99% of the possible reductions in tree size, with the other enhancements yielding insignificant gains.	algorithm;experiment;heuristic;minimax	Jonathan Schaeffer	1989	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/34.42858	games;mathematical optimization;combinatorics;index term;game tree;computer science;machine learning;mathematics;geometry;algorithm	Metrics	15.822840154582124	27.6121946506411	158658
3ce83adf31274cda0ab8d654d632eb323a6a4c22	special issue on computational methods for graph coloring and its generalizations	graph coloring;computational method		graph coloring	David S. Johnson;Anuj Mehrotra;Michael A. Trick	2008	Discrete Applied Mathematics	10.1016/j.dam.2007.10.007	graph power;factor-critical graph;combinatorics;discrete mathematics;fractional coloring;theoretical computer science;edge coloring;comparability graph;nowhere-zero flow;cubic graph;graph coloring;graph factorization;mathematics;voltage graph;distance-hereditary graph;graph;list coloring;greedy coloring;line graph	Theory	23.442309408872717	25.875734789388474	158689
8aed949fe88fc9f7bcf354a65ba4b4418fecb926	tracking the best of many experts	moyenne ponderee;pistage;camino grafo;weighted averaging;graph path;rastreo;intelligence artificielle;weighted average;directed graph;graphe oriente;chemin graphe;artificial intelligence;grafo orientado;inteligencia artificial;tracking	An algorithm is presented for online prediction that allows to track the best expert efficiently even if the number of experts is exponentially large, provided that the set of experts has a certain structure allowing efficient implementations of the exponentially weighted average predictor. As an example we work out the case where each expert is represented by a path in a directed graph and the loss of each expert is the sum of the weights over the edges in the path.	algorithm;directed graph;kerrison predictor	András György;Tamás Linder;Gábor Lugosi	2005		10.1007/11503415_14	directed graph;weighted arithmetic mean;computer science;artificial intelligence;machine learning;data mining;mathematics;tracking;algorithm	ML	20.78691677584964	27.6429546649992	158743
24fe7ac1691d5055520ebb6c97056edab46abf88	fast multi-dimensional approximate pattern matching	k md error;alphabet size;k m;size nd;search algorithm;size md;space cost;k mismatches;approximate string;fast multi-dimensional approximate pattern;time penalty	We address the problem of approximate string matching in d dimensions, that is, to nd a pattern of size m d in a text of size n d with at most k < m d errors (substitutions, insertions and deletions along any dimension). We use a novel and very exible error model, for which there exists only an algorithm to evaluate the similarity between two elements in two dimensions at O(m 4) time. We extend the algorithm to d dimensions, at O(d!m 2d) time and O(d!m 2d?1) space. We also give the rst search algorithm for such model, which is O(d!m d n d) time and O(d!m d n d?1) space. We show how to reduce the space cost to O(d!3 d m 2d?1) with little time penalty. Finally, we present the rst sublinear-time (on average) searching algorithm (i.e. not all text cells are inspected), which is O(kn d =m d?1) for k < (m=(d(log m ? log d))) d?1 , where is the alphabet size. After that error level the lter still remains better than dynamic programming for k m d?1 =(d(log m ? log d)) (d?1)=d. These are the rst search algorithms for the problem. As side-eeects we extend to d dimensions an already proposed algorithm for two-dimensional exact string matching, and we obtain a sublinear-time lter to search in d dimensions allowing k mismatches.	approximate string matching;dynamic programming;pattern matching;search algorithm;string searching algorithm;time complexity	Gonzalo Navarro;Ricardo A. Baeza-Yates	1999		10.1007/3-540-48452-3_18	time complexity;combinatorics;two-dimensional space;approximate string matching;computer science;artificial intelligence;pattern matching;mathematics;programming language;algorithm;string searching algorithm;search algorithm	Web+IR	13.812865039193886	27.157380597659575	158849
25700a964ca418af1a8df0f4cdbde0b38f51b678	computing connected components of simple undirected graphs based on generalized rough sets	k step upper approximation;journal;rough set;connected component;connection;definable set	This paper studies the problem of computing connected components of a simple undirected graph based on generalized rough sets. Definable sets of generalized rough sets are used to characterize the connection of graphs. Based on the concept of k-step-upper approximations, an algorithm is designed for computing the connected components of a simple undirected graph. Experiments show that the new proposed algorithm gets a better performance compared to the classical algorithm. Also, this algorithm can be easily to realize in parallel and requires less processors. 2012 Elsevier B.V. All rights reserved.	algorithm;approximation;central processing unit;connected component (graph theory);graph (discrete mathematics);parallel algorithm;rough set;set theory;uniprocessor system	Jinkun Chen;Jinjin Li;Yaojin Lin	2013	Knowl.-Based Syst.	10.1016/j.knosys.2012.07.013	connected component;rough set;definable set;connection;theoretical computer science;machine learning;connected dominating set;modular decomposition	AI	18.983457892152337	29.874290998813837	159035
0d5cf7d41e1131e32c3d49f64d4eed852c4b80f7	shallow circuits with high-powered inputs	sum of products;upper bound;computational complexity;high power;lower bound;arithmetic circuit	A polynomial identity testing algorithm must determine whether an input polynomial (given for instance by an arithmetic circuit) is identically equal to 0. In this paper, we show that a deterministic black-box identity testing algorithm for (high-degree) univariate polynomials would imply a lower bound on the arithmetic complexity of the permanent. The lower bounds that are known to follow from derandomization of (low-degree) multivariate identity testing are weaker. To obtain our lower bound it would be sufficient to derandomize identity testing for polynomials of a very specific norm: sums of products of sparse polynomials with sparse coefficients. This observation leads to new versions of the Shub-Smale τ -conjecture on integer roots of univariate polynomials. In particular, we show that a lower bound for the permanent would follow if one could give a polynomial upper bound on the number of real roots of sums of products of sparse polynomials (Descartes’ rule of signs gives such a bound for sparse polynomials and products thereof). In this third version of our paper we show that the same lower bound would follow even if one could only prove a slightly superpolynomial upper bound on the number of real roots. This is a consequence of a new result on reduction to depth 4 for arithmetic circuits which we establish in a companion paper. We also show that an even weaker bound on the number of real roots would suffice to obtain a lower bound on the size of depth 4 circuits computing the permanent. ∗UMR 5668 ENS Lyon, CNRS, UCBL, INRIA. †A part of this work was done during a visit to the Fields Institute. 1 en sl -0 04 77 02 3, v er si on 4 29 J ul 2 01 0	arithmetic circuit complexity;black box;blum blum shub;coefficient;computing the permanent;descartes' rule of signs;factorization of polynomials;polynomial identity testing;randomized algorithm;sparse matrix;time complexity	Pascal Koiran	2011			combinatorics;discrete mathematics;polynomial arithmetic;mathematics;arithmetic circuit complexity;upper and lower bounds;algorithm;algebra	Theory	10.8705090243485	22.091075677484014	159300
74fbe9b90cafd3399b11a434c02682747faf8d57	window-accumulated subsequence matching problem is linear	frequent pattern	Given two strings, text t of length n, and pattern p = pl . . . pk of length k, and given a natural number w, the subsequence matching problem consists in finding the number of size w windows of text t which contain pattern p as a subsequence, i.e. the letters ~1,. . . ,pk occur in the window, in the same order as in p, but not necessarily consecutively (they may be interleaved with other letters). Subsequence matching is used for finding frequent patterns and association rules in databases. We generalize the KnuthMorris-Pratt (KMP) pattern matching algorithm; we define a non-conventional kind of RAM, the MP-RAMS which model more closely the microprocessor operations; we design an O(n) on-line algorithm for solving the subsequence matching problem on MPRAMS.	association rule learning;database;knuth–morris–pratt algorithm;matching (graph theory);microprocessor;microsoft windows;online algorithm;online and offline;pattern matching;rams;random-access memory	Luc Boasson;Patrick Cégielski;Irène Guessarian;Yuri V. Matiyasevich	1999		10.1145/303976.304008	longest increasing subsequence;computer science;longest common subsequence problem;longest alternating subsequence	DB	13.531451127669433	27.791748625989708	159429
